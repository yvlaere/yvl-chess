{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5d7453",
   "metadata": {},
   "source": [
    "# NNUE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821225",
   "metadata": {},
   "source": [
    "Great source on NNUE: https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead68bd",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Stockfish has a lot of data available for NNUE training in the .binpack format. They have a repo for training NNUEs (nnue-pytorch) that enables efficient dataloading with this format. I don't want to use nnue-pytorch, i want to make my own NNUE training setup.\n",
    "\n",
    "The nnue-pytorch repo also has information on training datasets for NNUEs: https://github.com/official-stockfish/nnue-pytorch/wiki/Training-datasets. They explain how to make your own dataset and link some of the datasets they generated. I will use some of this data, because generating the data myself would be too time-consuming on my hardware.\n",
    "\n",
    "Currently using training data: test80-2024-01-jan-2tb7p.min-v2.v6.binpack.zst from https://huggingface.co/datasets/linrock/test80-2024/tree/main\n",
    "\n",
    "This file contains billions of positions with evaluations in the .binpack format. The stockfish tools branch has a tool to covert the .binpack data into .plain data (https://github.com/official-stockfish/Stockfish/blob/tools/docs/convert.md). I used this tool and stored the first 200M evaluated positions.\n",
    "\n",
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1c61",
   "metadata": {},
   "source": [
    "### Turn FEN into input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict_w = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "piece_dict_b = {'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K':11, 'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "def FEN_to_inputs(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer_w = np.zeros(768, dtype = np.float32)\n",
    "    input_layer_b = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                alt_pos = 63 - (position ^ 7)\n",
    "                input_layer_w[position + piece_dict_w[char]*64] = 1\n",
    "                input_layer_b[alt_pos + piece_dict_b[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer_w, dtype=torch.float32), torch.tensor(input_layer_b, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1938bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Black Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Side to Move: tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_993/2390657366.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(w_features)))\n",
      "/tmp/ipykernel_993/2390657366.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(b_features)))\n"
     ]
    }
   ],
   "source": [
    "# testing encoding\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "\n",
    "w_features, b_features, stm = FEN_to_inputs(fen1)\n",
    "print(\"White Features:\", sum(w_features))\n",
    "print(np.nonzero(np.array(w_features)))\n",
    "print(\"Black Features:\", sum(b_features))\n",
    "print(np.nonzero(np.array(b_features)))\n",
    "print(\"Side to Move:\", stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cdf67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test1 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 14, 15, 21, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "test2 = [ 8, 9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192, 199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510, 570, 573, 632, 639, 699, 764]\n",
    "\n",
    "np.sort(test1)\n",
    "np.sort(test2)\n",
    "print(np.array_equal(np.sort(test1), np.sort(test2)))\n",
    "\n",
    "test3 = [  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192, 199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,  570, 573, 632, 639, 699, 764]\n",
    "test4 = [632, 505, 570, 699, 764, 573, 510, 639, 432, 433, 434, 435, 436, 438, 439, 429, 8, 9, 10, 11, 12, 13, 14, 15, 192, 65, 130, 259, 324, 133, 70, 199]\n",
    "print(np.array_equal(np.sort(test3), np.sort(test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c0263",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Input: a sparse, binary array of length 768. Each element of the array represents a possible combination of piece type (6), piece_color (2) and position (64) (6*2*64 = 768).\n",
    "\n",
    "This is a very simple input feature (P feature set) set that will be improved upon later (HalfKP).\n",
    "\n",
    "The fully connected feedfoward network has 4 hidden layers: 768 -> 1024, 1024 -> 8, 8 -> 32 and 32 -> 1.\n",
    "\n",
    "The output is a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Split_NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split_NNUE, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)\n",
    "        cat_bw = torch.cat([b, w], dim=1)\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = (1 - stm) * cat_wb + stm * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Split_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_inputs(row[0].strip())\n",
    "                score, result = float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                buf.append((w_in, b_in, stm, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                yield w_in, b_in, stm, score, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data_full.csv'\n",
    "dataset = Custom_Split_Dataset(csv_path, shuffle_buffer = 100000)\n",
    "loader = DataLoader(dataset, batch_size = 1024, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa7cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Kaiming uniform for piecewise-linear (ReLU-like) activations:\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0.0, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | Avg Loss: 0.0174 | Grad Norm: 0.01097746\n",
      "Epoch 1 | Step 200 | Avg Loss: 0.0150 | Grad Norm: 0.00888000\n",
      "Epoch 1 | Step 300 | Avg Loss: 0.0142 | Grad Norm: 0.00825052\n",
      "Epoch 1 | Step 400 | Avg Loss: 0.0136 | Grad Norm: 0.00941511\n",
      "Epoch 1 | Step 500 | Avg Loss: 0.0135 | Grad Norm: 0.00920944\n",
      "Epoch 1 | Step 600 | Avg Loss: 0.0136 | Grad Norm: 0.00889536\n",
      "Epoch 1 | Step 700 | Avg Loss: 0.0139 | Grad Norm: 0.00965357\n",
      "Epoch 1 | Step 800 | Avg Loss: 0.0137 | Grad Norm: 0.01071357\n",
      "Epoch 1 | Step 900 | Avg Loss: 0.0142 | Grad Norm: 0.01017332\n",
      "Epoch 1 | Step 1000 | Avg Loss: 0.0138 | Grad Norm: 0.00877418\n",
      "Epoch 1 | Step 1100 | Avg Loss: 0.0134 | Grad Norm: 0.00932349\n",
      "Epoch 1 | Step 1200 | Avg Loss: 0.0134 | Grad Norm: 0.01010502\n",
      "Epoch 1 | Step 1300 | Avg Loss: 0.0134 | Grad Norm: 0.00997096\n",
      "Epoch 1 | Step 1400 | Avg Loss: 0.0133 | Grad Norm: 0.00883213\n",
      "Epoch 1 | Step 1500 | Avg Loss: 0.0132 | Grad Norm: 0.00950090\n",
      "Epoch 1 | Step 1600 | Avg Loss: 0.0130 | Grad Norm: 0.00887633\n",
      "Epoch 1 | Step 1700 | Avg Loss: 0.0133 | Grad Norm: 0.01218441\n",
      "Epoch 1 | Step 1800 | Avg Loss: 0.0131 | Grad Norm: 0.00976543\n",
      "Epoch 1 | Step 1900 | Avg Loss: 0.0132 | Grad Norm: 0.00831439\n",
      "Epoch 1 | Step 2000 | Avg Loss: 0.0134 | Grad Norm: 0.00871284\n",
      "Epoch 1 | Step 2100 | Avg Loss: 0.0132 | Grad Norm: 0.00898697\n",
      "Epoch 1 | Step 2200 | Avg Loss: 0.0132 | Grad Norm: 0.00951955\n",
      "Epoch 1 | Step 2300 | Avg Loss: 0.0134 | Grad Norm: 0.00898848\n",
      "Epoch 1 | Step 2400 | Avg Loss: 0.0133 | Grad Norm: 0.01090957\n",
      "Epoch 1 | Step 2500 | Avg Loss: 0.0134 | Grad Norm: 0.00926086\n",
      "Epoch 1 | Step 2600 | Avg Loss: 0.0135 | Grad Norm: 0.01002531\n",
      "Epoch 1 | Step 2700 | Avg Loss: 0.0137 | Grad Norm: 0.01152117\n",
      "Epoch 1 | Step 2800 | Avg Loss: 0.0134 | Grad Norm: 0.00912074\n",
      "Epoch 1 | Step 2900 | Avg Loss: 0.0135 | Grad Norm: 0.00973383\n",
      "Epoch 1 | Step 3000 | Avg Loss: 0.0135 | Grad Norm: 0.00950437\n",
      "Epoch 1 | Step 3100 | Avg Loss: 0.0132 | Grad Norm: 0.01079381\n",
      "Epoch 1 | Step 3200 | Avg Loss: 0.0128 | Grad Norm: 0.01027544\n",
      "Epoch 1 | Step 3300 | Avg Loss: 0.0128 | Grad Norm: 0.00936288\n",
      "Epoch 1 | Step 3400 | Avg Loss: 0.0130 | Grad Norm: 0.01041831\n",
      "Epoch 1 | Step 3500 | Avg Loss: 0.0125 | Grad Norm: 0.00884279\n",
      "Epoch 1 | Step 3600 | Avg Loss: 0.0127 | Grad Norm: 0.00894097\n",
      "Epoch 1 | Step 3700 | Avg Loss: 0.0129 | Grad Norm: 0.01459840\n",
      "Epoch 1 | Step 3800 | Avg Loss: 0.0131 | Grad Norm: 0.01013666\n",
      "Epoch 1 | Step 3900 | Avg Loss: 0.0131 | Grad Norm: 0.00956687\n",
      "Epoch 1 | Step 4000 | Avg Loss: 0.0132 | Grad Norm: 0.00889548\n",
      "Epoch 1 | Step 4100 | Avg Loss: 0.0131 | Grad Norm: 0.01031688\n",
      "Epoch 1 | Step 4200 | Avg Loss: 0.0128 | Grad Norm: 0.00864393\n",
      "Epoch 1 | Step 4300 | Avg Loss: 0.0129 | Grad Norm: 0.00896085\n",
      "Epoch 1 | Step 4400 | Avg Loss: 0.0132 | Grad Norm: 0.00963927\n",
      "Epoch 1 | Step 4500 | Avg Loss: 0.0131 | Grad Norm: 0.01078073\n",
      "Epoch 1 | Step 4600 | Avg Loss: 0.0127 | Grad Norm: 0.00949231\n",
      "Epoch 1 | Step 4700 | Avg Loss: 0.0128 | Grad Norm: 0.01025814\n",
      "Epoch 1 | Step 4800 | Avg Loss: 0.0130 | Grad Norm: 0.00974350\n",
      "Epoch 1 | Step 4900 | Avg Loss: 0.0131 | Grad Norm: 0.00872450\n",
      "Epoch 1 | Step 5000 | Avg Loss: 0.0130 | Grad Norm: 0.00889648\n",
      "Epoch 1 | Step 5100 | Avg Loss: 0.0131 | Grad Norm: 0.01196100\n",
      "Epoch 1 | Step 5200 | Avg Loss: 0.0128 | Grad Norm: 0.00918294\n",
      "Epoch 1 | Step 5300 | Avg Loss: 0.0132 | Grad Norm: 0.00874645\n",
      "Epoch 1 | Step 5400 | Avg Loss: 0.0134 | Grad Norm: 0.01061362\n",
      "Epoch 1 | Step 5500 | Avg Loss: 0.0132 | Grad Norm: 0.00881277\n",
      "Epoch 1 | Step 5600 | Avg Loss: 0.0131 | Grad Norm: 0.00985626\n",
      "Epoch 1 | Step 5700 | Avg Loss: 0.0131 | Grad Norm: 0.00949246\n",
      "Epoch 1 | Step 5800 | Avg Loss: 0.0134 | Grad Norm: 0.00952804\n",
      "Epoch 1 | Step 5900 | Avg Loss: 0.0132 | Grad Norm: 0.01297022\n",
      "Epoch 1 | Step 6000 | Avg Loss: 0.0133 | Grad Norm: 0.00978439\n",
      "Epoch 1 | Step 6100 | Avg Loss: 0.0129 | Grad Norm: 0.00933565\n",
      "Epoch 1 | Step 6200 | Avg Loss: 0.0130 | Grad Norm: 0.01045706\n",
      "Epoch 1 | Step 6300 | Avg Loss: 0.0136 | Grad Norm: 0.01003998\n",
      "Epoch 1 | Step 6400 | Avg Loss: 0.0131 | Grad Norm: 0.00990695\n",
      "Epoch 1 | Step 6500 | Avg Loss: 0.0134 | Grad Norm: 0.00891406\n",
      "Epoch 1 | Step 6600 | Avg Loss: 0.0135 | Grad Norm: 0.01110737\n",
      "Epoch 1 | Step 6700 | Avg Loss: 0.0137 | Grad Norm: 0.00975005\n",
      "Epoch 1 | Step 6800 | Avg Loss: 0.0134 | Grad Norm: 0.01042765\n",
      "Epoch 1 | Step 6900 | Avg Loss: 0.0135 | Grad Norm: 0.00984265\n",
      "Epoch 1 | Step 7000 | Avg Loss: 0.0131 | Grad Norm: 0.01146452\n",
      "Epoch 1 | Step 7100 | Avg Loss: 0.0130 | Grad Norm: 0.01028515\n",
      "Epoch 1 | Step 7200 | Avg Loss: 0.0126 | Grad Norm: 0.00976176\n",
      "Epoch 1 | Step 7300 | Avg Loss: 0.0130 | Grad Norm: 0.01057603\n",
      "Epoch 1 | Step 7400 | Avg Loss: 0.0128 | Grad Norm: 0.00981744\n",
      "Epoch 1 | Step 7500 | Avg Loss: 0.0129 | Grad Norm: 0.00860360\n",
      "Epoch 1 | Step 7600 | Avg Loss: 0.0133 | Grad Norm: 0.01254449\n",
      "Epoch 1 | Step 7700 | Avg Loss: 0.0130 | Grad Norm: 0.00964600\n",
      "Epoch 1 | Step 7800 | Avg Loss: 0.0131 | Grad Norm: 0.00888633\n",
      "Epoch 1 | Step 7900 | Avg Loss: 0.0132 | Grad Norm: 0.00897702\n",
      "Epoch 1 | Step 8000 | Avg Loss: 0.0136 | Grad Norm: 0.00921840\n",
      "Epoch 1 | Step 8100 | Avg Loss: 0.0133 | Grad Norm: 0.01020577\n",
      "Epoch 1 | Step 8200 | Avg Loss: 0.0131 | Grad Norm: 0.00882477\n",
      "Epoch 1 | Step 8300 | Avg Loss: 0.0132 | Grad Norm: 0.00997490\n",
      "Epoch 1 | Step 8400 | Avg Loss: 0.0131 | Grad Norm: 0.00970818\n",
      "Epoch 1 | Step 8500 | Avg Loss: 0.0129 | Grad Norm: 0.01097199\n",
      "Epoch 1 | Step 8600 | Avg Loss: 0.0135 | Grad Norm: 0.01105742\n",
      "Epoch 1 | Step 8700 | Avg Loss: 0.0132 | Grad Norm: 0.01052603\n",
      "Epoch 1 | Step 8800 | Avg Loss: 0.0134 | Grad Norm: 0.01061584\n",
      "Epoch 1 | Step 8900 | Avg Loss: 0.0132 | Grad Norm: 0.01042647\n",
      "Epoch 1 | Step 9000 | Avg Loss: 0.0133 | Grad Norm: 0.01079833\n",
      "Epoch 1 | Step 9100 | Avg Loss: 0.0133 | Grad Norm: 0.00943384\n",
      "Epoch 1 | Step 9200 | Avg Loss: 0.0133 | Grad Norm: 0.01301678\n",
      "Epoch 1 | Step 9300 | Avg Loss: 0.0129 | Grad Norm: 0.00951992\n",
      "Epoch 1 | Step 9400 | Avg Loss: 0.0132 | Grad Norm: 0.01169250\n",
      "Epoch 1 | Step 9500 | Avg Loss: 0.0132 | Grad Norm: 0.01163932\n",
      "Epoch 1 | Step 9600 | Avg Loss: 0.0133 | Grad Norm: 0.01035028\n",
      "Epoch 1 | Step 9700 | Avg Loss: 0.0132 | Grad Norm: 0.01080032\n",
      "Epoch 1 | Step 9800 | Avg Loss: 0.0132 | Grad Norm: 0.00950249\n",
      "Epoch 1 | Step 9900 | Avg Loss: 0.0133 | Grad Norm: 0.01154115\n",
      "Epoch 1 | Step 10000 | Avg Loss: 0.0133 | Grad Norm: 0.01048072\n",
      "Epoch 1 | Step 10100 | Avg Loss: 0.0137 | Grad Norm: 0.01137124\n",
      "Epoch 1 | Step 10200 | Avg Loss: 0.0138 | Grad Norm: 0.01175438\n",
      "Epoch 1 | Step 10300 | Avg Loss: 0.0136 | Grad Norm: 0.01001359\n",
      "Epoch 1 | Step 10400 | Avg Loss: 0.0134 | Grad Norm: 0.01037861\n",
      "Epoch 1 | Step 10500 | Avg Loss: 0.0137 | Grad Norm: 0.01056714\n",
      "Epoch 1 | Step 10600 | Avg Loss: 0.0138 | Grad Norm: 0.01153674\n",
      "Epoch 1 | Step 10700 | Avg Loss: 0.0135 | Grad Norm: 0.00903712\n",
      "Epoch 1 | Step 10800 | Avg Loss: 0.0135 | Grad Norm: 0.00952243\n",
      "Epoch 1 | Step 10900 | Avg Loss: 0.0132 | Grad Norm: 0.00934809\n",
      "Epoch 1 | Step 11000 | Avg Loss: 0.0133 | Grad Norm: 0.01091891\n",
      "Epoch 1 | Step 11100 | Avg Loss: 0.0136 | Grad Norm: 0.01021893\n",
      "Epoch 1 | Step 11200 | Avg Loss: 0.0134 | Grad Norm: 0.00940400\n",
      "Epoch 1 | Step 11300 | Avg Loss: 0.0136 | Grad Norm: 0.01046956\n",
      "Epoch 1 | Step 11400 | Avg Loss: 0.0130 | Grad Norm: 0.01093812\n",
      "Epoch 1 | Step 11500 | Avg Loss: 0.0132 | Grad Norm: 0.01292135\n",
      "Epoch 1 | Step 11600 | Avg Loss: 0.0132 | Grad Norm: 0.01134193\n",
      "Epoch 1 | Step 11700 | Avg Loss: 0.0130 | Grad Norm: 0.01045528\n",
      "Epoch 1 | Step 11800 | Avg Loss: 0.0131 | Grad Norm: 0.00849835\n",
      "Epoch 1 | Step 11900 | Avg Loss: 0.0129 | Grad Norm: 0.00957772\n",
      "Epoch 1 | Step 12000 | Avg Loss: 0.0128 | Grad Norm: 0.01030922\n",
      "Epoch 1 | Step 12100 | Avg Loss: 0.0129 | Grad Norm: 0.01042553\n",
      "Epoch 1 | Step 12200 | Avg Loss: 0.0129 | Grad Norm: 0.00841856\n",
      "Epoch 1 | Step 12300 | Avg Loss: 0.0130 | Grad Norm: 0.00935543\n",
      "Epoch 1 | Step 12400 | Avg Loss: 0.0131 | Grad Norm: 0.01114535\n",
      "Epoch 1 | Step 12500 | Avg Loss: 0.0134 | Grad Norm: 0.01096115\n",
      "Epoch 1 | Step 12600 | Avg Loss: 0.0131 | Grad Norm: 0.01020924\n",
      "Epoch 1 | Step 12700 | Avg Loss: 0.0130 | Grad Norm: 0.00979830\n",
      "Epoch 1 | Step 12800 | Avg Loss: 0.0132 | Grad Norm: 0.01069768\n",
      "Epoch 1 | Step 12900 | Avg Loss: 0.0134 | Grad Norm: 0.00971742\n",
      "Epoch 1 | Step 13000 | Avg Loss: 0.0133 | Grad Norm: 0.01131666\n",
      "Epoch 1 | Step 13100 | Avg Loss: 0.0132 | Grad Norm: 0.01095047\n",
      "Epoch 1 | Step 13200 | Avg Loss: 0.0130 | Grad Norm: 0.00945000\n",
      "Epoch 1 | Step 13300 | Avg Loss: 0.0130 | Grad Norm: 0.01053488\n",
      "Epoch 1 | Step 13400 | Avg Loss: 0.0131 | Grad Norm: 0.01007266\n",
      "Epoch 1 | Step 13500 | Avg Loss: 0.0131 | Grad Norm: 0.00860507\n",
      "Epoch 1 | Step 13600 | Avg Loss: 0.0130 | Grad Norm: 0.00998909\n",
      "Epoch 1 | Step 13700 | Avg Loss: 0.0127 | Grad Norm: 0.00936727\n",
      "Epoch 1 | Step 13800 | Avg Loss: 0.0128 | Grad Norm: 0.00840054\n",
      "Epoch 1 | Step 13900 | Avg Loss: 0.0135 | Grad Norm: 0.01012485\n",
      "Epoch 1 | Step 14000 | Avg Loss: 0.0134 | Grad Norm: 0.01025629\n",
      "Epoch 1 | Step 14100 | Avg Loss: 0.0130 | Grad Norm: 0.00954569\n",
      "Epoch 1 | Step 14200 | Avg Loss: 0.0131 | Grad Norm: 0.00995329\n",
      "Epoch 1 | Step 14300 | Avg Loss: 0.0129 | Grad Norm: 0.00945598\n",
      "Epoch 1 | Step 14400 | Avg Loss: 0.0129 | Grad Norm: 0.01115223\n",
      "Epoch 1 | Step 14500 | Avg Loss: 0.0129 | Grad Norm: 0.01015826\n",
      "Epoch 1 | Step 14600 | Avg Loss: 0.0130 | Grad Norm: 0.00916201\n",
      "Epoch 1 | Step 14700 | Avg Loss: 0.0132 | Grad Norm: 0.01097415\n",
      "Epoch 1 | Step 14800 | Avg Loss: 0.0129 | Grad Norm: 0.00883184\n",
      "Epoch 1 | Step 14900 | Avg Loss: 0.0129 | Grad Norm: 0.00989062\n",
      "Epoch 1 | Step 15000 | Avg Loss: 0.0132 | Grad Norm: 0.00942307\n",
      "Epoch 1 | Step 15100 | Avg Loss: 0.0130 | Grad Norm: 0.00959565\n",
      "Epoch 1 | Step 15200 | Avg Loss: 0.0129 | Grad Norm: 0.00977145\n",
      "Epoch 1 | Step 15300 | Avg Loss: 0.0127 | Grad Norm: 0.00874491\n",
      "Epoch 1 | Step 15400 | Avg Loss: 0.0128 | Grad Norm: 0.01182694\n",
      "Epoch 1 | Step 15500 | Avg Loss: 0.0130 | Grad Norm: 0.01191709\n",
      "Epoch 1 | Step 15600 | Avg Loss: 0.0133 | Grad Norm: 0.00983050\n",
      "Epoch 1 | Step 15700 | Avg Loss: 0.0136 | Grad Norm: 0.00992884\n",
      "Epoch 1 | Step 15800 | Avg Loss: 0.0139 | Grad Norm: 0.00958776\n",
      "Epoch 1 | Step 15900 | Avg Loss: 0.0139 | Grad Norm: 0.01016179\n",
      "Epoch 1 | Step 16000 | Avg Loss: 0.0138 | Grad Norm: 0.01111489\n",
      "Epoch 1 | Step 16100 | Avg Loss: 0.0139 | Grad Norm: 0.01100102\n",
      "Epoch 1 | Step 16200 | Avg Loss: 0.0138 | Grad Norm: 0.00956149\n",
      "Epoch 1 | Step 16300 | Avg Loss: 0.0138 | Grad Norm: 0.01118365\n",
      "Epoch 1 | Step 16400 | Avg Loss: 0.0134 | Grad Norm: 0.00985473\n",
      "Epoch 1 | Step 16500 | Avg Loss: 0.0133 | Grad Norm: 0.01084977\n",
      "Epoch 1 | Step 16600 | Avg Loss: 0.0131 | Grad Norm: 0.00971895\n",
      "Epoch 1 | Step 16700 | Avg Loss: 0.0129 | Grad Norm: 0.00986963\n",
      "Epoch 1 | Step 16800 | Avg Loss: 0.0131 | Grad Norm: 0.01123275\n",
      "Epoch 1 | Step 16900 | Avg Loss: 0.0136 | Grad Norm: 0.01128035\n",
      "Epoch 1 | Step 17000 | Avg Loss: 0.0131 | Grad Norm: 0.01072197\n",
      "Epoch 1 | Step 17100 | Avg Loss: 0.0131 | Grad Norm: 0.00955433\n",
      "Epoch 1 | Step 17200 | Avg Loss: 0.0130 | Grad Norm: 0.00954632\n",
      "Epoch 1 | Step 17300 | Avg Loss: 0.0130 | Grad Norm: 0.00963779\n",
      "Epoch 1 | Step 17400 | Avg Loss: 0.0131 | Grad Norm: 0.00924566\n",
      "Epoch 1 | Step 17500 | Avg Loss: 0.0129 | Grad Norm: 0.01011147\n",
      "Epoch 1 | Step 17600 | Avg Loss: 0.0128 | Grad Norm: 0.00939099\n",
      "Epoch 1 | Step 17700 | Avg Loss: 0.0127 | Grad Norm: 0.01041952\n",
      "Epoch 1 | Step 17800 | Avg Loss: 0.0129 | Grad Norm: 0.01064265\n",
      "Epoch 1 | Step 17900 | Avg Loss: 0.0132 | Grad Norm: 0.01111458\n",
      "Epoch 1 | Step 18000 | Avg Loss: 0.0130 | Grad Norm: 0.00964725\n",
      "Epoch 1 | Step 18100 | Avg Loss: 0.0128 | Grad Norm: 0.00981425\n",
      "Epoch 1 | Step 18200 | Avg Loss: 0.0130 | Grad Norm: 0.00980877\n",
      "Epoch 1 | Step 18300 | Avg Loss: 0.0131 | Grad Norm: 0.01058634\n",
      "Epoch 1 | Step 18400 | Avg Loss: 0.0129 | Grad Norm: 0.01138509\n",
      "Epoch 1 | Step 18500 | Avg Loss: 0.0132 | Grad Norm: 0.01007639\n",
      "Epoch 1 | Step 18600 | Avg Loss: 0.0133 | Grad Norm: 0.01097265\n",
      "Epoch 1 | Step 18700 | Avg Loss: 0.0131 | Grad Norm: 0.01211329\n",
      "Epoch 1 | Step 18800 | Avg Loss: 0.0134 | Grad Norm: 0.01130743\n",
      "Epoch 1 | Step 18900 | Avg Loss: 0.0134 | Grad Norm: 0.01017385\n",
      "Epoch 1 | Step 19000 | Avg Loss: 0.0131 | Grad Norm: 0.01097306\n",
      "Epoch 1 | Step 19100 | Avg Loss: 0.0136 | Grad Norm: 0.00897475\n",
      "Epoch 1 | Step 19200 | Avg Loss: 0.0137 | Grad Norm: 0.00938714\n",
      "Epoch 1 | Step 19300 | Avg Loss: 0.0132 | Grad Norm: 0.00988725\n",
      "Epoch 1 | Step 19400 | Avg Loss: 0.0126 | Grad Norm: 0.00866237\n",
      "Epoch 1 | Step 19500 | Avg Loss: 0.0127 | Grad Norm: 0.00951449\n",
      "Epoch 1 | Step 19600 | Avg Loss: 0.0126 | Grad Norm: 0.00990619\n",
      "Epoch 1 | Step 19700 | Avg Loss: 0.0127 | Grad Norm: 0.00965268\n",
      "Epoch 1 | Step 19800 | Avg Loss: 0.0125 | Grad Norm: 0.01002019\n",
      "Epoch 1 | Step 19900 | Avg Loss: 0.0125 | Grad Norm: 0.00963693\n",
      "Epoch 1 | Step 20000 | Avg Loss: 0.0127 | Grad Norm: 0.01006275\n",
      "Epoch 1 | Step 20100 | Avg Loss: 0.0126 | Grad Norm: 0.00914493\n",
      "Epoch 1 | Step 20200 | Avg Loss: 0.0126 | Grad Norm: 0.00916558\n",
      "Epoch 1 | Step 20300 | Avg Loss: 0.0131 | Grad Norm: 0.01041524\n",
      "Epoch 1 | Step 20400 | Avg Loss: 0.0132 | Grad Norm: 0.01020904\n",
      "Epoch 1 | Step 20500 | Avg Loss: 0.0133 | Grad Norm: 0.01183412\n",
      "Epoch 1 | Step 20600 | Avg Loss: 0.0137 | Grad Norm: 0.01030133\n",
      "Epoch 1 | Step 20700 | Avg Loss: 0.0134 | Grad Norm: 0.00787233\n",
      "Epoch 1 | Step 20800 | Avg Loss: 0.0130 | Grad Norm: 0.01138642\n",
      "Epoch 1 | Step 20900 | Avg Loss: 0.0131 | Grad Norm: 0.01040508\n",
      "Epoch 1 | Step 21000 | Avg Loss: 0.0132 | Grad Norm: 0.01044004\n",
      "Epoch 1 | Step 21100 | Avg Loss: 0.0129 | Grad Norm: 0.00975120\n",
      "Epoch 1 | Step 21200 | Avg Loss: 0.0132 | Grad Norm: 0.00976472\n",
      "Epoch 1 | Step 21300 | Avg Loss: 0.0129 | Grad Norm: 0.01125637\n",
      "Epoch 1 | Step 21400 | Avg Loss: 0.0130 | Grad Norm: 0.01106511\n",
      "Epoch 1 | Step 21500 | Avg Loss: 0.0134 | Grad Norm: 0.01254611\n",
      "Epoch 1 | Step 21600 | Avg Loss: 0.0134 | Grad Norm: 0.00987193\n",
      "Epoch 1 | Step 21700 | Avg Loss: 0.0133 | Grad Norm: 0.01153274\n",
      "Epoch 1 | Step 21800 | Avg Loss: 0.0136 | Grad Norm: 0.01100067\n",
      "Epoch 1 | Step 21900 | Avg Loss: 0.0135 | Grad Norm: 0.00918763\n",
      "Epoch 1 | Step 22000 | Avg Loss: 0.0134 | Grad Norm: 0.00981883\n",
      "Epoch 1 | Step 22100 | Avg Loss: 0.0135 | Grad Norm: 0.00999683\n",
      "Epoch 1 | Step 22200 | Avg Loss: 0.0134 | Grad Norm: 0.01131981\n",
      "Epoch 1 | Step 22300 | Avg Loss: 0.0135 | Grad Norm: 0.01205453\n",
      "Epoch 1 | Step 22400 | Avg Loss: 0.0137 | Grad Norm: 0.01018847\n",
      "Epoch 1 | Step 22500 | Avg Loss: 0.0136 | Grad Norm: 0.01038947\n",
      "Epoch 1 | Step 22600 | Avg Loss: 0.0134 | Grad Norm: 0.00902407\n",
      "Epoch 1 | Step 22700 | Avg Loss: 0.0136 | Grad Norm: 0.00994578\n",
      "Epoch 1 | Step 22800 | Avg Loss: 0.0138 | Grad Norm: 0.01088459\n",
      "Epoch 1 | Step 22900 | Avg Loss: 0.0141 | Grad Norm: 0.00995092\n",
      "Epoch 1 | Step 23000 | Avg Loss: 0.0134 | Grad Norm: 0.00925324\n",
      "Epoch 1 | Step 23100 | Avg Loss: 0.0135 | Grad Norm: 0.01504629\n",
      "Epoch 1 | Step 23200 | Avg Loss: 0.0134 | Grad Norm: 0.01027219\n",
      "Epoch 1 | Step 23300 | Avg Loss: 0.0134 | Grad Norm: 0.01038368\n",
      "Epoch 1 | Step 23400 | Avg Loss: 0.0139 | Grad Norm: 0.01064694\n",
      "Epoch 1 | Step 23500 | Avg Loss: 0.0143 | Grad Norm: 0.00982497\n",
      "Epoch 1 | Step 23600 | Avg Loss: 0.0144 | Grad Norm: 0.00995217\n",
      "Epoch 1 | Step 23700 | Avg Loss: 0.0141 | Grad Norm: 0.01034911\n",
      "Epoch 1 | Step 23800 | Avg Loss: 0.0141 | Grad Norm: 0.01172163\n",
      "Epoch 1 | Step 23900 | Avg Loss: 0.0143 | Grad Norm: 0.01134659\n",
      "Epoch 1 | Step 24000 | Avg Loss: 0.0140 | Grad Norm: 0.01076406\n",
      "Epoch 1 | Step 24100 | Avg Loss: 0.0141 | Grad Norm: 0.00996527\n",
      "Epoch 1 | Step 24200 | Avg Loss: 0.0143 | Grad Norm: 0.01102916\n",
      "Epoch 1 | Step 24300 | Avg Loss: 0.0140 | Grad Norm: 0.00946352\n",
      "Epoch 1 | Step 24400 | Avg Loss: 0.0139 | Grad Norm: 0.00993185\n",
      "Epoch 1 | Step 24500 | Avg Loss: 0.0140 | Grad Norm: 0.01023907\n",
      "Epoch 1 | Step 24600 | Avg Loss: 0.0141 | Grad Norm: 0.01255467\n",
      "Epoch 1 | Step 24700 | Avg Loss: 0.0147 | Grad Norm: 0.01067141\n",
      "Epoch 1 | Step 24800 | Avg Loss: 0.0139 | Grad Norm: 0.01144659\n",
      "Epoch 1 | Step 24900 | Avg Loss: 0.0137 | Grad Norm: 0.00990570\n",
      "Epoch 1 | Step 25000 | Avg Loss: 0.0139 | Grad Norm: 0.01214128\n",
      "Epoch 1 | Step 25100 | Avg Loss: 0.0139 | Grad Norm: 0.01105895\n",
      "Epoch 1 | Step 25200 | Avg Loss: 0.0142 | Grad Norm: 0.01044955\n",
      "Epoch 1 | Step 25300 | Avg Loss: 0.0144 | Grad Norm: 0.01133335\n",
      "Epoch 1 | Step 25400 | Avg Loss: 0.0142 | Grad Norm: 0.01202686\n",
      "Epoch 1 | Step 25500 | Avg Loss: 0.0142 | Grad Norm: 0.01042530\n",
      "Epoch 1 | Step 25600 | Avg Loss: 0.0141 | Grad Norm: 0.01080660\n",
      "Epoch 1 | Step 25700 | Avg Loss: 0.0140 | Grad Norm: 0.01146918\n",
      "Epoch 1 | Step 25800 | Avg Loss: 0.0144 | Grad Norm: 0.01082790\n",
      "Epoch 1 | Step 25900 | Avg Loss: 0.0143 | Grad Norm: 0.01152572\n",
      "Epoch 1 | Step 26000 | Avg Loss: 0.0141 | Grad Norm: 0.01065534\n",
      "Epoch 1 | Step 26100 | Avg Loss: 0.0136 | Grad Norm: 0.00937866\n",
      "Epoch 1 | Step 26200 | Avg Loss: 0.0138 | Grad Norm: 0.01080884\n",
      "Epoch 1 | Step 26300 | Avg Loss: 0.0142 | Grad Norm: 0.00899369\n",
      "Epoch 1 | Step 26400 | Avg Loss: 0.0138 | Grad Norm: 0.00937402\n",
      "Epoch 1 | Step 26500 | Avg Loss: 0.0137 | Grad Norm: 0.01208424\n",
      "Epoch 1 | Step 26600 | Avg Loss: 0.0133 | Grad Norm: 0.00986005\n",
      "Epoch 1 | Step 26700 | Avg Loss: 0.0139 | Grad Norm: 0.01083491\n",
      "Epoch 1 | Step 26800 | Avg Loss: 0.0141 | Grad Norm: 0.01038157\n",
      "Epoch 1 | Step 26900 | Avg Loss: 0.0140 | Grad Norm: 0.01053910\n",
      "Epoch 1 | Step 27000 | Avg Loss: 0.0141 | Grad Norm: 0.01125425\n",
      "Epoch 1 | Step 27100 | Avg Loss: 0.0141 | Grad Norm: 0.00939400\n",
      "Epoch 1 | Step 27200 | Avg Loss: 0.0137 | Grad Norm: 0.00941543\n",
      "Epoch 1 | Step 27300 | Avg Loss: 0.0137 | Grad Norm: 0.00981040\n",
      "Epoch 1 | Step 27400 | Avg Loss: 0.0142 | Grad Norm: 0.01157980\n",
      "Epoch 1 | Step 27500 | Avg Loss: 0.0143 | Grad Norm: 0.00999644\n",
      "Epoch 1 | Step 27600 | Avg Loss: 0.0139 | Grad Norm: 0.01062589\n",
      "Epoch 1 | Step 27700 | Avg Loss: 0.0140 | Grad Norm: 0.01240668\n",
      "Epoch 1 | Step 27800 | Avg Loss: 0.0142 | Grad Norm: 0.01017605\n",
      "Epoch 1 | Step 27900 | Avg Loss: 0.0142 | Grad Norm: 0.01027454\n",
      "Epoch 1 | Step 28000 | Avg Loss: 0.0140 | Grad Norm: 0.00958395\n",
      "Epoch 1 | Step 28100 | Avg Loss: 0.0140 | Grad Norm: 0.00991729\n",
      "Epoch 1 | Step 28200 | Avg Loss: 0.0140 | Grad Norm: 0.01091404\n",
      "Epoch 1 | Step 28300 | Avg Loss: 0.0135 | Grad Norm: 0.01319826\n",
      "Epoch 1 | Step 28400 | Avg Loss: 0.0137 | Grad Norm: 0.01125016\n",
      "Epoch 1 | Step 28500 | Avg Loss: 0.0139 | Grad Norm: 0.01050012\n",
      "Epoch 1 | Step 28600 | Avg Loss: 0.0138 | Grad Norm: 0.01193144\n",
      "Epoch 1 | Step 28700 | Avg Loss: 0.0135 | Grad Norm: 0.01026437\n",
      "Epoch 1 | Step 28800 | Avg Loss: 0.0136 | Grad Norm: 0.01066237\n",
      "Epoch 1 | Step 28900 | Avg Loss: 0.0136 | Grad Norm: 0.01129615\n",
      "Epoch 1 | Step 29000 | Avg Loss: 0.0136 | Grad Norm: 0.00919740\n",
      "Epoch 1 | Step 29100 | Avg Loss: 0.0136 | Grad Norm: 0.01100425\n",
      "Epoch 1 | Step 29200 | Avg Loss: 0.0139 | Grad Norm: 0.01133696\n",
      "Epoch 1 | Step 29300 | Avg Loss: 0.0135 | Grad Norm: 0.01000137\n",
      "Epoch 1 | Step 29400 | Avg Loss: 0.0135 | Grad Norm: 0.00910586\n",
      "Epoch 1 | Step 29500 | Avg Loss: 0.0139 | Grad Norm: 0.00948410\n",
      "Epoch 1 | Step 29600 | Avg Loss: 0.0138 | Grad Norm: 0.00945383\n",
      "Epoch 1 | Step 29700 | Avg Loss: 0.0140 | Grad Norm: 0.01088883\n",
      "Epoch 1 | Step 29800 | Avg Loss: 0.0141 | Grad Norm: 0.00964358\n",
      "Epoch 1 | Step 29900 | Avg Loss: 0.0142 | Grad Norm: 0.01240457\n",
      "Epoch 1 | Step 30000 | Avg Loss: 0.0143 | Grad Norm: 0.01043850\n",
      "Epoch 1 | Step 30100 | Avg Loss: 0.0140 | Grad Norm: 0.01162001\n",
      "Epoch 1 | Step 30200 | Avg Loss: 0.0143 | Grad Norm: 0.01007128\n",
      "Epoch 1 | Step 30300 | Avg Loss: 0.0144 | Grad Norm: 0.00983305\n",
      "Epoch 1 | Step 30400 | Avg Loss: 0.0143 | Grad Norm: 0.00888904\n",
      "Epoch 1 | Step 30500 | Avg Loss: 0.0140 | Grad Norm: 0.00961636\n",
      "Epoch 1 | Step 30600 | Avg Loss: 0.0137 | Grad Norm: 0.00973919\n",
      "Epoch 1 | Step 30700 | Avg Loss: 0.0140 | Grad Norm: 0.01002929\n",
      "Epoch 1 | Step 30800 | Avg Loss: 0.0136 | Grad Norm: 0.01184452\n",
      "Epoch 1 | Step 30900 | Avg Loss: 0.0140 | Grad Norm: 0.00878952\n",
      "Epoch 1 | Step 31000 | Avg Loss: 0.0136 | Grad Norm: 0.01123352\n",
      "Epoch 1 | Step 31100 | Avg Loss: 0.0137 | Grad Norm: 0.01090825\n",
      "Epoch 1 | Step 31200 | Avg Loss: 0.0135 | Grad Norm: 0.01027417\n",
      "Epoch 1 | Step 31300 | Avg Loss: 0.0135 | Grad Norm: 0.00994688\n",
      "Epoch 1 | Step 31400 | Avg Loss: 0.0138 | Grad Norm: 0.00969987\n",
      "Epoch 1 | Step 31500 | Avg Loss: 0.0139 | Grad Norm: 0.01066852\n",
      "Epoch 1 | Step 31600 | Avg Loss: 0.0137 | Grad Norm: 0.01034176\n",
      "Epoch 1 | Step 31700 | Avg Loss: 0.0136 | Grad Norm: 0.01008549\n",
      "Epoch 1 | Step 31800 | Avg Loss: 0.0141 | Grad Norm: 0.01093358\n",
      "Epoch 1 | Step 31900 | Avg Loss: 0.0142 | Grad Norm: 0.00976468\n",
      "Epoch 1 | Step 32000 | Avg Loss: 0.0140 | Grad Norm: 0.00938136\n",
      "Epoch 1 | Step 32100 | Avg Loss: 0.0140 | Grad Norm: 0.00971996\n",
      "Epoch 1 | Step 32200 | Avg Loss: 0.0142 | Grad Norm: 0.00935409\n",
      "Epoch 1 | Step 32300 | Avg Loss: 0.0145 | Grad Norm: 0.01107940\n",
      "Epoch 1 | Step 32400 | Avg Loss: 0.0143 | Grad Norm: 0.00926576\n",
      "Epoch 1 | Step 32500 | Avg Loss: 0.0141 | Grad Norm: 0.01235962\n",
      "Epoch 1 | Step 32600 | Avg Loss: 0.0140 | Grad Norm: 0.01098581\n",
      "Epoch 1 | Step 32700 | Avg Loss: 0.0141 | Grad Norm: 0.01074642\n",
      "Epoch 1 | Step 32800 | Avg Loss: 0.0143 | Grad Norm: 0.01010816\n",
      "Epoch 1 | Step 32900 | Avg Loss: 0.0141 | Grad Norm: 0.01016204\n",
      "Epoch 1 | Step 33000 | Avg Loss: 0.0139 | Grad Norm: 0.00964533\n",
      "Epoch 1 | Step 33100 | Avg Loss: 0.0136 | Grad Norm: 0.01108038\n",
      "Epoch 1 | Step 33200 | Avg Loss: 0.0136 | Grad Norm: 0.01030766\n",
      "Epoch 1 | Step 33300 | Avg Loss: 0.0137 | Grad Norm: 0.00909853\n",
      "Epoch 1 | Step 33400 | Avg Loss: 0.0137 | Grad Norm: 0.00980833\n",
      "Epoch 1 | Step 33500 | Avg Loss: 0.0139 | Grad Norm: 0.00908298\n",
      "Epoch 1 | Step 33600 | Avg Loss: 0.0140 | Grad Norm: 0.01150353\n",
      "Epoch 1 | Step 33700 | Avg Loss: 0.0139 | Grad Norm: 0.00945795\n",
      "Epoch 1 | Step 33800 | Avg Loss: 0.0138 | Grad Norm: 0.01037057\n",
      "Epoch 1 | Step 33900 | Avg Loss: 0.0134 | Grad Norm: 0.01147111\n",
      "Epoch 1 | Step 34000 | Avg Loss: 0.0134 | Grad Norm: 0.00959877\n",
      "Epoch 1 | Step 34100 | Avg Loss: 0.0131 | Grad Norm: 0.00947168\n",
      "Epoch 1 | Step 34200 | Avg Loss: 0.0129 | Grad Norm: 0.01033232\n",
      "Epoch 1 | Step 34300 | Avg Loss: 0.0131 | Grad Norm: 0.01099606\n",
      "Epoch 1 | Step 34400 | Avg Loss: 0.0134 | Grad Norm: 0.00868463\n",
      "Epoch 1 | Step 34500 | Avg Loss: 0.0137 | Grad Norm: 0.00964691\n",
      "Epoch 1 | Step 34600 | Avg Loss: 0.0138 | Grad Norm: 0.01202558\n",
      "Epoch 1 | Step 34700 | Avg Loss: 0.0134 | Grad Norm: 0.00914221\n",
      "Epoch 1 | Step 34800 | Avg Loss: 0.0135 | Grad Norm: 0.01267098\n",
      "Epoch 1 | Step 34900 | Avg Loss: 0.0135 | Grad Norm: 0.00807224\n",
      "Epoch 1 | Step 35000 | Avg Loss: 0.0134 | Grad Norm: 0.00955894\n",
      "Epoch 1 | Step 35100 | Avg Loss: 0.0137 | Grad Norm: 0.01039455\n",
      "Epoch 1 | Step 35200 | Avg Loss: 0.0142 | Grad Norm: 0.01072964\n",
      "Epoch 1 | Step 35300 | Avg Loss: 0.0139 | Grad Norm: 0.00991312\n",
      "Epoch 1 | Step 35400 | Avg Loss: 0.0139 | Grad Norm: 0.00996285\n",
      "Epoch 1 | Step 35500 | Avg Loss: 0.0140 | Grad Norm: 0.01172308\n",
      "Epoch 1 | Step 35600 | Avg Loss: 0.0142 | Grad Norm: 0.01041900\n",
      "Epoch 1 | Step 35700 | Avg Loss: 0.0141 | Grad Norm: 0.00967608\n",
      "Epoch 1 | Step 35800 | Avg Loss: 0.0136 | Grad Norm: 0.00934458\n",
      "Epoch 1 | Step 35900 | Avg Loss: 0.0140 | Grad Norm: 0.01099981\n",
      "Epoch 1 | Step 36000 | Avg Loss: 0.0134 | Grad Norm: 0.00943612\n",
      "Epoch 1 | Step 36100 | Avg Loss: 0.0134 | Grad Norm: 0.00952500\n",
      "Epoch 1 | Step 36200 | Avg Loss: 0.0140 | Grad Norm: 0.00984426\n",
      "Epoch 1 | Step 36300 | Avg Loss: 0.0141 | Grad Norm: 0.01067749\n",
      "Epoch 1 | Step 36400 | Avg Loss: 0.0144 | Grad Norm: 0.01111912\n",
      "Epoch 1 | Step 36500 | Avg Loss: 0.0143 | Grad Norm: 0.01178110\n",
      "Epoch 1 | Step 36600 | Avg Loss: 0.0141 | Grad Norm: 0.00961284\n",
      "Epoch 1 | Step 36700 | Avg Loss: 0.0142 | Grad Norm: 0.01011195\n",
      "Epoch 1 | Step 36800 | Avg Loss: 0.0142 | Grad Norm: 0.01101552\n",
      "Epoch 1 | Step 36900 | Avg Loss: 0.0143 | Grad Norm: 0.01123627\n",
      "Epoch 1 | Step 37000 | Avg Loss: 0.0144 | Grad Norm: 0.00994891\n",
      "Epoch 1 | Step 37100 | Avg Loss: 0.0142 | Grad Norm: 0.01115302\n",
      "Epoch 1 | Step 37200 | Avg Loss: 0.0139 | Grad Norm: 0.01018507\n",
      "Epoch 1 | Step 37300 | Avg Loss: 0.0136 | Grad Norm: 0.01106224\n",
      "Epoch 1 | Step 37400 | Avg Loss: 0.0140 | Grad Norm: 0.01058826\n",
      "Epoch 1 | Step 37500 | Avg Loss: 0.0138 | Grad Norm: 0.01288444\n",
      "Epoch 1 | Step 37600 | Avg Loss: 0.0137 | Grad Norm: 0.01078108\n",
      "Epoch 1 | Step 37700 | Avg Loss: 0.0138 | Grad Norm: 0.00958043\n",
      "Epoch 1 | Step 37800 | Avg Loss: 0.0142 | Grad Norm: 0.00957813\n",
      "Epoch 1 | Step 37900 | Avg Loss: 0.0140 | Grad Norm: 0.01228882\n",
      "Epoch 1 | Step 38000 | Avg Loss: 0.0143 | Grad Norm: 0.01028488\n",
      "Epoch 1 | Step 38100 | Avg Loss: 0.0144 | Grad Norm: 0.01155265\n",
      "Epoch 1 | Step 38200 | Avg Loss: 0.0143 | Grad Norm: 0.01129169\n",
      "Epoch 1 | Step 38300 | Avg Loss: 0.0143 | Grad Norm: 0.01307219\n",
      "Epoch 1 | Step 38400 | Avg Loss: 0.0143 | Grad Norm: 0.01062681\n",
      "Epoch 1 | Step 38500 | Avg Loss: 0.0143 | Grad Norm: 0.01009245\n",
      "Epoch 1 | Step 38600 | Avg Loss: 0.0141 | Grad Norm: 0.00968525\n",
      "Epoch 1 | Step 38700 | Avg Loss: 0.0142 | Grad Norm: 0.01019824\n",
      "Epoch 1 | Step 38800 | Avg Loss: 0.0143 | Grad Norm: 0.01111842\n",
      "Epoch 1 | Step 38900 | Avg Loss: 0.0139 | Grad Norm: 0.01024514\n",
      "Epoch 1 | Step 39000 | Avg Loss: 0.0137 | Grad Norm: 0.01128483\n",
      "Epoch 1 | Step 39100 | Avg Loss: 0.0141 | Grad Norm: 0.01123148\n",
      "Epoch 1 | Step 39200 | Avg Loss: 0.0140 | Grad Norm: 0.01010709\n",
      "Epoch 1 | Step 39300 | Avg Loss: 0.0139 | Grad Norm: 0.01031599\n",
      "Epoch 1 | Step 39400 | Avg Loss: 0.0140 | Grad Norm: 0.01030874\n",
      "Epoch 1 | Step 39500 | Avg Loss: 0.0138 | Grad Norm: 0.01025774\n",
      "Epoch 1 | Step 39600 | Avg Loss: 0.0138 | Grad Norm: 0.01031637\n",
      "Epoch 1 | Step 39700 | Avg Loss: 0.0142 | Grad Norm: 0.01270611\n",
      "Epoch 1 | Step 39800 | Avg Loss: 0.0141 | Grad Norm: 0.01230700\n",
      "Epoch 1 | Step 39900 | Avg Loss: 0.0139 | Grad Norm: 0.01069170\n",
      "Epoch 1 | Step 40000 | Avg Loss: 0.0137 | Grad Norm: 0.01101686\n",
      "Epoch 1 | Step 40100 | Avg Loss: 0.0138 | Grad Norm: 0.00908274\n",
      "Epoch 1 | Step 40200 | Avg Loss: 0.0138 | Grad Norm: 0.01097000\n",
      "Epoch 1 | Step 40300 | Avg Loss: 0.0137 | Grad Norm: 0.01087394\n",
      "Epoch 1 | Step 40400 | Avg Loss: 0.0136 | Grad Norm: 0.01024077\n",
      "Epoch 1 | Step 40500 | Avg Loss: 0.0136 | Grad Norm: 0.01247802\n",
      "Epoch 1 | Step 40600 | Avg Loss: 0.0136 | Grad Norm: 0.01063081\n",
      "Epoch 1 | Step 40700 | Avg Loss: 0.0135 | Grad Norm: 0.00995091\n",
      "Epoch 1 | Step 40800 | Avg Loss: 0.0133 | Grad Norm: 0.00972373\n",
      "Epoch 1 | Step 40900 | Avg Loss: 0.0135 | Grad Norm: 0.00934986\n",
      "Epoch 1 | Step 41000 | Avg Loss: 0.0140 | Grad Norm: 0.01040916\n",
      "Epoch 1 | Step 41100 | Avg Loss: 0.0139 | Grad Norm: 0.01027454\n",
      "Epoch 1 | Step 41200 | Avg Loss: 0.0144 | Grad Norm: 0.01160783\n",
      "Epoch 1 | Step 41300 | Avg Loss: 0.0140 | Grad Norm: 0.01060219\n",
      "Epoch 1 | Step 41400 | Avg Loss: 0.0138 | Grad Norm: 0.01086784\n",
      "Epoch 1 | Step 41500 | Avg Loss: 0.0137 | Grad Norm: 0.01028381\n",
      "Epoch 1 | Step 41600 | Avg Loss: 0.0140 | Grad Norm: 0.01254815\n",
      "Epoch 1 | Step 41700 | Avg Loss: 0.0142 | Grad Norm: 0.01074690\n",
      "Epoch 1 | Step 41800 | Avg Loss: 0.0142 | Grad Norm: 0.01174002\n",
      "Epoch 1 | Step 41900 | Avg Loss: 0.0145 | Grad Norm: 0.01100551\n",
      "Epoch 1 | Step 42000 | Avg Loss: 0.0147 | Grad Norm: 0.01083557\n",
      "Epoch 1 | Step 42100 | Avg Loss: 0.0145 | Grad Norm: 0.00926394\n",
      "Epoch 1 | Step 42200 | Avg Loss: 0.0147 | Grad Norm: 0.01201583\n",
      "Epoch 1 | Step 42300 | Avg Loss: 0.0148 | Grad Norm: 0.01105668\n",
      "Epoch 1 | Step 42400 | Avg Loss: 0.0148 | Grad Norm: 0.01087917\n",
      "Epoch 1 | Step 42500 | Avg Loss: 0.0144 | Grad Norm: 0.01064252\n",
      "Epoch 1 | Step 42600 | Avg Loss: 0.0141 | Grad Norm: 0.01045226\n",
      "Epoch 1 | Step 42700 | Avg Loss: 0.0139 | Grad Norm: 0.01032309\n",
      "Epoch 1 | Step 42800 | Avg Loss: 0.0138 | Grad Norm: 0.01153164\n",
      "Epoch 1 | Step 42900 | Avg Loss: 0.0134 | Grad Norm: 0.01170435\n",
      "Epoch 1 | Step 43000 | Avg Loss: 0.0137 | Grad Norm: 0.01103443\n",
      "Epoch 1 | Step 43100 | Avg Loss: 0.0141 | Grad Norm: 0.01006372\n",
      "Epoch 1 | Step 43200 | Avg Loss: 0.0140 | Grad Norm: 0.01034673\n",
      "Epoch 1 | Step 43300 | Avg Loss: 0.0138 | Grad Norm: 0.01109874\n",
      "Epoch 1 | Step 43400 | Avg Loss: 0.0140 | Grad Norm: 0.01237647\n",
      "Epoch 1 | Step 43500 | Avg Loss: 0.0142 | Grad Norm: 0.01035221\n",
      "Epoch 1 | Step 43600 | Avg Loss: 0.0141 | Grad Norm: 0.00875182\n",
      "Epoch 1 | Step 43700 | Avg Loss: 0.0137 | Grad Norm: 0.00999442\n",
      "Epoch 1 | Step 43800 | Avg Loss: 0.0139 | Grad Norm: 0.01047085\n",
      "Epoch 1 | Step 43900 | Avg Loss: 0.0138 | Grad Norm: 0.01045788\n",
      "Epoch 1 | Step 44000 | Avg Loss: 0.0140 | Grad Norm: 0.00960858\n",
      "Epoch 1 | Step 44100 | Avg Loss: 0.0139 | Grad Norm: 0.01067459\n",
      "Epoch 1 | Step 44200 | Avg Loss: 0.0142 | Grad Norm: 0.01114859\n",
      "Epoch 1 | Step 44300 | Avg Loss: 0.0143 | Grad Norm: 0.00905577\n",
      "Epoch 1 | Step 44400 | Avg Loss: 0.0144 | Grad Norm: 0.00963585\n",
      "Epoch 1 | Step 44500 | Avg Loss: 0.0144 | Grad Norm: 0.01067625\n",
      "Epoch 1 | Step 44600 | Avg Loss: 0.0141 | Grad Norm: 0.00900316\n",
      "Epoch 1 | Step 44700 | Avg Loss: 0.0146 | Grad Norm: 0.01067399\n",
      "Epoch 1 | Step 44800 | Avg Loss: 0.0146 | Grad Norm: 0.00827394\n",
      "Epoch 1 | Step 44900 | Avg Loss: 0.0143 | Grad Norm: 0.00979065\n",
      "Epoch 1 | Step 45000 | Avg Loss: 0.0141 | Grad Norm: 0.01244241\n",
      "Epoch 1 | Step 45100 | Avg Loss: 0.0138 | Grad Norm: 0.00989303\n",
      "Epoch 1 | Step 45200 | Avg Loss: 0.0139 | Grad Norm: 0.01070269\n",
      "Epoch 1 | Step 45300 | Avg Loss: 0.0136 | Grad Norm: 0.00900089\n",
      "Epoch 1 | Step 45400 | Avg Loss: 0.0140 | Grad Norm: 0.01068618\n",
      "Epoch 1 | Step 45500 | Avg Loss: 0.0138 | Grad Norm: 0.01107973\n",
      "Epoch 1 | Step 45600 | Avg Loss: 0.0139 | Grad Norm: 0.00958996\n",
      "Epoch 1 | Step 45700 | Avg Loss: 0.0143 | Grad Norm: 0.00974574\n",
      "Epoch 1 | Step 45800 | Avg Loss: 0.0138 | Grad Norm: 0.00995026\n",
      "Epoch 1 | Step 45900 | Avg Loss: 0.0131 | Grad Norm: 0.01069408\n",
      "Epoch 1 | Step 46000 | Avg Loss: 0.0136 | Grad Norm: 0.01165194\n",
      "Epoch 1 | Step 46100 | Avg Loss: 0.0136 | Grad Norm: 0.01032425\n",
      "Epoch 1 | Step 46200 | Avg Loss: 0.0135 | Grad Norm: 0.01124550\n",
      "Epoch 1 | Step 46300 | Avg Loss: 0.0139 | Grad Norm: 0.01011265\n",
      "Epoch 1 | Step 46400 | Avg Loss: 0.0140 | Grad Norm: 0.00950461\n",
      "Epoch 1 | Step 46500 | Avg Loss: 0.0137 | Grad Norm: 0.01151216\n",
      "Epoch 1 | Step 46600 | Avg Loss: 0.0137 | Grad Norm: 0.01010753\n",
      "Epoch 1 | Step 46700 | Avg Loss: 0.0139 | Grad Norm: 0.00965182\n",
      "Epoch 1 | Step 46800 | Avg Loss: 0.0136 | Grad Norm: 0.01242144\n",
      "Epoch 1 | Step 46900 | Avg Loss: 0.0138 | Grad Norm: 0.01214002\n",
      "Epoch 1 | Step 47000 | Avg Loss: 0.0138 | Grad Norm: 0.01064301\n",
      "Epoch 1 | Step 47100 | Avg Loss: 0.0144 | Grad Norm: 0.01036499\n",
      "Epoch 1 | Step 47200 | Avg Loss: 0.0140 | Grad Norm: 0.01237708\n",
      "Epoch 1 | Step 47300 | Avg Loss: 0.0143 | Grad Norm: 0.01031945\n",
      "Epoch 1 | Step 47400 | Avg Loss: 0.0146 | Grad Norm: 0.01078232\n",
      "Epoch 1 | Step 47500 | Avg Loss: 0.0145 | Grad Norm: 0.01152043\n",
      "Epoch 1 | Step 47600 | Avg Loss: 0.0145 | Grad Norm: 0.00984026\n",
      "Epoch 1 | Step 47700 | Avg Loss: 0.0147 | Grad Norm: 0.01211939\n",
      "Epoch 1 | Step 47800 | Avg Loss: 0.0145 | Grad Norm: 0.00978510\n",
      "Epoch 1 | Step 47900 | Avg Loss: 0.0141 | Grad Norm: 0.01089497\n",
      "Epoch 1 | Step 48000 | Avg Loss: 0.0138 | Grad Norm: 0.01087429\n",
      "Epoch 1 | Step 48100 | Avg Loss: 0.0140 | Grad Norm: 0.00984179\n",
      "Epoch 1 | Step 48200 | Avg Loss: 0.0138 | Grad Norm: 0.00931090\n",
      "Epoch 1 | Step 48300 | Avg Loss: 0.0138 | Grad Norm: 0.01094899\n",
      "Epoch 1 | Step 48400 | Avg Loss: 0.0140 | Grad Norm: 0.01321279\n",
      "Epoch 1 | Step 48500 | Avg Loss: 0.0140 | Grad Norm: 0.00933297\n",
      "Epoch 1 | Step 48600 | Avg Loss: 0.0138 | Grad Norm: 0.00966293\n",
      "Epoch 1 | Step 48700 | Avg Loss: 0.0144 | Grad Norm: 0.01006524\n",
      "Epoch 1 | Step 48800 | Avg Loss: 0.0137 | Grad Norm: 0.01054305\n",
      "Epoch 1 | Step 48900 | Avg Loss: 0.0138 | Grad Norm: 0.01016978\n",
      "Epoch 1 | Step 49000 | Avg Loss: 0.0139 | Grad Norm: 0.01061294\n",
      "Epoch 1 | Step 49100 | Avg Loss: 0.0137 | Grad Norm: 0.01032068\n",
      "Epoch 1 | Step 49200 | Avg Loss: 0.0140 | Grad Norm: 0.01150913\n",
      "Epoch 1 | Step 49300 | Avg Loss: 0.0144 | Grad Norm: 0.01009724\n",
      "Epoch 1 | Step 49400 | Avg Loss: 0.0144 | Grad Norm: 0.01210905\n",
      "Epoch 1 | Step 49500 | Avg Loss: 0.0140 | Grad Norm: 0.00997678\n",
      "Epoch 1 | Step 49600 | Avg Loss: 0.0141 | Grad Norm: 0.01030937\n",
      "Epoch 1 | Step 49700 | Avg Loss: 0.0140 | Grad Norm: 0.01202853\n",
      "Epoch 1 | Step 49800 | Avg Loss: 0.0137 | Grad Norm: 0.00969207\n",
      "Epoch 1 | Step 49900 | Avg Loss: 0.0140 | Grad Norm: 0.00925740\n",
      "Epoch 1 | Step 50000 | Avg Loss: 0.0140 | Grad Norm: 0.01050644\n",
      "Epoch 1 | Step 50100 | Avg Loss: 0.0141 | Grad Norm: 0.01304193\n",
      "Epoch 1 | Step 50200 | Avg Loss: 0.0140 | Grad Norm: 0.00988807\n",
      "Epoch 1 | Step 50300 | Avg Loss: 0.0141 | Grad Norm: 0.00960958\n",
      "Epoch 1 | Step 50400 | Avg Loss: 0.0143 | Grad Norm: 0.01025108\n",
      "Epoch 1 | Step 50500 | Avg Loss: 0.0144 | Grad Norm: 0.01108211\n",
      "Epoch 1 | Step 50600 | Avg Loss: 0.0140 | Grad Norm: 0.01139423\n",
      "Epoch 1 | Step 50700 | Avg Loss: 0.0137 | Grad Norm: 0.01055811\n",
      "Epoch 1 | Step 50800 | Avg Loss: 0.0139 | Grad Norm: 0.01108063\n",
      "Epoch 1 | Step 50900 | Avg Loss: 0.0140 | Grad Norm: 0.01173443\n",
      "Epoch 1 | Step 51000 | Avg Loss: 0.0139 | Grad Norm: 0.01040684\n",
      "Epoch 1 | Step 51100 | Avg Loss: 0.0138 | Grad Norm: 0.01072146\n",
      "Epoch 1 | Step 51200 | Avg Loss: 0.0138 | Grad Norm: 0.01009058\n",
      "Epoch 1 | Step 51300 | Avg Loss: 0.0139 | Grad Norm: 0.01001377\n",
      "Epoch 1 | Step 51400 | Avg Loss: 0.0139 | Grad Norm: 0.01200360\n",
      "Epoch 1 | Step 51500 | Avg Loss: 0.0141 | Grad Norm: 0.01185369\n",
      "Epoch 1 | Step 51600 | Avg Loss: 0.0145 | Grad Norm: 0.01084613\n",
      "Epoch 1 | Step 51700 | Avg Loss: 0.0140 | Grad Norm: 0.00950553\n",
      "Epoch 1 | Step 51800 | Avg Loss: 0.0142 | Grad Norm: 0.01195914\n",
      "Epoch 1 | Step 51900 | Avg Loss: 0.0147 | Grad Norm: 0.01046739\n",
      "Epoch 1 | Step 52000 | Avg Loss: 0.0143 | Grad Norm: 0.01109223\n",
      "Epoch 1 | Step 52100 | Avg Loss: 0.0144 | Grad Norm: 0.01044106\n",
      "Epoch 1 | Step 52200 | Avg Loss: 0.0144 | Grad Norm: 0.01266767\n",
      "Epoch 1 | Step 52300 | Avg Loss: 0.0142 | Grad Norm: 0.00962668\n",
      "Epoch 1 | Step 52400 | Avg Loss: 0.0139 | Grad Norm: 0.00943921\n",
      "Epoch 1 | Step 52500 | Avg Loss: 0.0138 | Grad Norm: 0.01220527\n",
      "Epoch 1 | Step 52600 | Avg Loss: 0.0140 | Grad Norm: 0.01025304\n",
      "Epoch 1 | Step 52700 | Avg Loss: 0.0138 | Grad Norm: 0.01023396\n",
      "Epoch 1 | Step 52800 | Avg Loss: 0.0136 | Grad Norm: 0.01015809\n",
      "Epoch 1 | Step 52900 | Avg Loss: 0.0136 | Grad Norm: 0.00918018\n",
      "Epoch 1 | Step 53000 | Avg Loss: 0.0137 | Grad Norm: 0.01011521\n",
      "Epoch 1 | Step 53100 | Avg Loss: 0.0137 | Grad Norm: 0.01092854\n",
      "Epoch 1 | Step 53200 | Avg Loss: 0.0137 | Grad Norm: 0.01090811\n",
      "Epoch 1 | Step 53300 | Avg Loss: 0.0138 | Grad Norm: 0.01046115\n",
      "Epoch 1 | Step 53400 | Avg Loss: 0.0140 | Grad Norm: 0.01171523\n",
      "Epoch 1 | Step 53500 | Avg Loss: 0.0140 | Grad Norm: 0.01015972\n",
      "Epoch 1 | Step 53600 | Avg Loss: 0.0138 | Grad Norm: 0.01082706\n",
      "Epoch 1 | Step 53700 | Avg Loss: 0.0139 | Grad Norm: 0.01446715\n",
      "Epoch 1 | Step 53800 | Avg Loss: 0.0140 | Grad Norm: 0.01107593\n",
      "Epoch 1 | Step 53900 | Avg Loss: 0.0142 | Grad Norm: 0.01056136\n",
      "Epoch 1 | Step 54000 | Avg Loss: 0.0146 | Grad Norm: 0.01119690\n",
      "Epoch 1 | Step 54100 | Avg Loss: 0.0145 | Grad Norm: 0.01025035\n",
      "Epoch 1 | Step 54200 | Avg Loss: 0.0144 | Grad Norm: 0.00980372\n",
      "Epoch 1 | Step 54300 | Avg Loss: 0.0146 | Grad Norm: 0.00964375\n",
      "Epoch 1 | Step 54400 | Avg Loss: 0.0145 | Grad Norm: 0.01142558\n",
      "Epoch 1 | Step 54500 | Avg Loss: 0.0147 | Grad Norm: 0.01101083\n",
      "Epoch 1 | Step 54600 | Avg Loss: 0.0145 | Grad Norm: 0.00941305\n",
      "Epoch 1 | Step 54700 | Avg Loss: 0.0144 | Grad Norm: 0.01045417\n",
      "Epoch 1 | Step 54800 | Avg Loss: 0.0144 | Grad Norm: 0.00967995\n",
      "Epoch 1 | Step 54900 | Avg Loss: 0.0143 | Grad Norm: 0.00956551\n",
      "Epoch 1 | Step 55000 | Avg Loss: 0.0146 | Grad Norm: 0.01030806\n",
      "Epoch 1 | Step 55100 | Avg Loss: 0.0147 | Grad Norm: 0.01021425\n",
      "Epoch 1 | Step 55200 | Avg Loss: 0.0143 | Grad Norm: 0.00974924\n",
      "Epoch 1 | Step 55300 | Avg Loss: 0.0142 | Grad Norm: 0.01231074\n",
      "Epoch 1 | Step 55400 | Avg Loss: 0.0139 | Grad Norm: 0.00976823\n",
      "Epoch 1 | Step 55500 | Avg Loss: 0.0143 | Grad Norm: 0.01008555\n",
      "Epoch 1 | Step 55600 | Avg Loss: 0.0140 | Grad Norm: 0.01003471\n",
      "Epoch 1 | Step 55700 | Avg Loss: 0.0141 | Grad Norm: 0.00962099\n",
      "Epoch 1 | Step 55800 | Avg Loss: 0.0142 | Grad Norm: 0.00968871\n",
      "Epoch 1 | Step 55900 | Avg Loss: 0.0141 | Grad Norm: 0.01005434\n",
      "Epoch 1 | Step 56000 | Avg Loss: 0.0140 | Grad Norm: 0.01073954\n",
      "Epoch 1 | Step 56100 | Avg Loss: 0.0145 | Grad Norm: 0.01344854\n",
      "Epoch 1 | Step 56200 | Avg Loss: 0.0148 | Grad Norm: 0.01021121\n",
      "Epoch 1 | Step 56300 | Avg Loss: 0.0142 | Grad Norm: 0.01150182\n",
      "Epoch 1 | Step 56400 | Avg Loss: 0.0144 | Grad Norm: 0.01024737\n",
      "Epoch 1 | Step 56500 | Avg Loss: 0.0140 | Grad Norm: 0.00982945\n",
      "Epoch 1 | Step 56600 | Avg Loss: 0.0141 | Grad Norm: 0.00968536\n",
      "Epoch 1 | Step 56700 | Avg Loss: 0.0144 | Grad Norm: 0.01165436\n",
      "Epoch 1 | Step 56800 | Avg Loss: 0.0149 | Grad Norm: 0.01054197\n",
      "Epoch 1 | Step 56900 | Avg Loss: 0.0149 | Grad Norm: 0.01130208\n",
      "Epoch 1 | Step 57000 | Avg Loss: 0.0145 | Grad Norm: 0.00953438\n",
      "Epoch 1 | Step 57100 | Avg Loss: 0.0139 | Grad Norm: 0.01049039\n",
      "Epoch 1 | Step 57200 | Avg Loss: 0.0140 | Grad Norm: 0.01124695\n",
      "Epoch 1 | Step 57300 | Avg Loss: 0.0140 | Grad Norm: 0.01052789\n",
      "Epoch 1 | Step 57400 | Avg Loss: 0.0139 | Grad Norm: 0.01151230\n",
      "Epoch 1 | Step 57500 | Avg Loss: 0.0139 | Grad Norm: 0.01119883\n",
      "Epoch 1 | Step 57600 | Avg Loss: 0.0138 | Grad Norm: 0.01043530\n",
      "Epoch 1 | Step 57700 | Avg Loss: 0.0139 | Grad Norm: 0.00949438\n",
      "Epoch 1 | Step 57800 | Avg Loss: 0.0137 | Grad Norm: 0.00930347\n",
      "Epoch 1 | Step 57900 | Avg Loss: 0.0138 | Grad Norm: 0.01133257\n",
      "Epoch 1 | Step 58000 | Avg Loss: 0.0137 | Grad Norm: 0.01014066\n",
      "Epoch 1 | Step 58100 | Avg Loss: 0.0140 | Grad Norm: 0.01000837\n",
      "Epoch 1 | Step 58200 | Avg Loss: 0.0138 | Grad Norm: 0.00982449\n",
      "Epoch 1 | Step 58300 | Avg Loss: 0.0141 | Grad Norm: 0.01035981\n",
      "Epoch 1 | Step 58400 | Avg Loss: 0.0140 | Grad Norm: 0.01160722\n",
      "Epoch 1 | Step 58500 | Avg Loss: 0.0143 | Grad Norm: 0.01066651\n",
      "Epoch 1 | Step 58600 | Avg Loss: 0.0141 | Grad Norm: 0.01103390\n",
      "Epoch 1 | Step 58700 | Avg Loss: 0.0135 | Grad Norm: 0.00981640\n",
      "Epoch 1 | Step 58800 | Avg Loss: 0.0139 | Grad Norm: 0.01057511\n",
      "Epoch 1 | Step 58900 | Avg Loss: 0.0142 | Grad Norm: 0.00989672\n",
      "Epoch 1 | Step 59000 | Avg Loss: 0.0139 | Grad Norm: 0.00983951\n",
      "Epoch 1 | Step 59100 | Avg Loss: 0.0140 | Grad Norm: 0.01165085\n",
      "Epoch 1 | Step 59200 | Avg Loss: 0.0142 | Grad Norm: 0.01219323\n",
      "Epoch 1 | Step 59300 | Avg Loss: 0.0141 | Grad Norm: 0.01131794\n",
      "Epoch 1 | Step 59400 | Avg Loss: 0.0138 | Grad Norm: 0.01084125\n",
      "Epoch 1 | Step 59500 | Avg Loss: 0.0137 | Grad Norm: 0.01075009\n",
      "Epoch 1 | Step 59600 | Avg Loss: 0.0142 | Grad Norm: 0.01030816\n",
      "Epoch 1 | Step 59700 | Avg Loss: 0.0136 | Grad Norm: 0.01006951\n",
      "Epoch 1 | Step 59800 | Avg Loss: 0.0136 | Grad Norm: 0.01016458\n",
      "Epoch 1 | Step 59900 | Avg Loss: 0.0138 | Grad Norm: 0.01028971\n",
      "Epoch 1 | Step 60000 | Avg Loss: 0.0141 | Grad Norm: 0.01023244\n",
      "Epoch 1 | Step 60100 | Avg Loss: 0.0144 | Grad Norm: 0.01020564\n",
      "Epoch 1 | Step 60200 | Avg Loss: 0.0144 | Grad Norm: 0.00899255\n",
      "Epoch 1 | Step 60300 | Avg Loss: 0.0141 | Grad Norm: 0.01078930\n",
      "Epoch 1 | Step 60400 | Avg Loss: 0.0141 | Grad Norm: 0.00950982\n",
      "Epoch 1 | Step 60500 | Avg Loss: 0.0139 | Grad Norm: 0.00990122\n",
      "Epoch 1 | Step 60600 | Avg Loss: 0.0136 | Grad Norm: 0.01108906\n",
      "Epoch 1 | Step 60700 | Avg Loss: 0.0139 | Grad Norm: 0.01014340\n",
      "Epoch 1 | Step 60800 | Avg Loss: 0.0140 | Grad Norm: 0.00896277\n",
      "Epoch 1 | Step 60900 | Avg Loss: 0.0142 | Grad Norm: 0.01120878\n",
      "Epoch 1 | Step 61000 | Avg Loss: 0.0140 | Grad Norm: 0.00980404\n",
      "Epoch 1 | Step 61100 | Avg Loss: 0.0141 | Grad Norm: 0.01105465\n",
      "Epoch 1 | Step 61200 | Avg Loss: 0.0142 | Grad Norm: 0.01148876\n",
      "Epoch 1 | Step 61300 | Avg Loss: 0.0139 | Grad Norm: 0.00926292\n",
      "Epoch 1 | Step 61400 | Avg Loss: 0.0141 | Grad Norm: 0.01111668\n",
      "Epoch 1 | Step 61500 | Avg Loss: 0.0142 | Grad Norm: 0.01054744\n",
      "Epoch 1 | Step 61600 | Avg Loss: 0.0142 | Grad Norm: 0.00951204\n",
      "Epoch 1 | Step 61700 | Avg Loss: 0.0141 | Grad Norm: 0.01226555\n",
      "Epoch 1 | Step 61800 | Avg Loss: 0.0141 | Grad Norm: 0.00952930\n",
      "Epoch 1 | Step 61900 | Avg Loss: 0.0145 | Grad Norm: 0.01060578\n",
      "Epoch 1 | Step 62000 | Avg Loss: 0.0148 | Grad Norm: 0.00954124\n",
      "Epoch 1 | Step 62100 | Avg Loss: 0.0147 | Grad Norm: 0.00945513\n",
      "Epoch 1 | Step 62200 | Avg Loss: 0.0144 | Grad Norm: 0.00992968\n",
      "Epoch 1 | Step 62300 | Avg Loss: 0.0139 | Grad Norm: 0.00954614\n",
      "Epoch 1 | Step 62400 | Avg Loss: 0.0136 | Grad Norm: 0.01073360\n",
      "Epoch 1 | Step 62500 | Avg Loss: 0.0135 | Grad Norm: 0.00878372\n",
      "Epoch 1 | Step 62600 | Avg Loss: 0.0140 | Grad Norm: 0.01126578\n",
      "Epoch 1 | Step 62700 | Avg Loss: 0.0141 | Grad Norm: 0.01110906\n",
      "Epoch 1 | Step 62800 | Avg Loss: 0.0142 | Grad Norm: 0.01030112\n",
      "Epoch 1 | Step 62900 | Avg Loss: 0.0140 | Grad Norm: 0.00944590\n",
      "Epoch 1 | Step 63000 | Avg Loss: 0.0148 | Grad Norm: 0.00972861\n",
      "Epoch 1 | Step 63100 | Avg Loss: 0.0145 | Grad Norm: 0.01030255\n",
      "Epoch 1 | Step 63200 | Avg Loss: 0.0143 | Grad Norm: 0.00977530\n",
      "Epoch 1 | Step 63300 | Avg Loss: 0.0145 | Grad Norm: 0.00947158\n",
      "Epoch 1 | Step 63400 | Avg Loss: 0.0142 | Grad Norm: 0.01102314\n",
      "Epoch 1 | Step 63500 | Avg Loss: 0.0137 | Grad Norm: 0.01022374\n",
      "Epoch 1 | Step 63600 | Avg Loss: 0.0133 | Grad Norm: 0.01015118\n",
      "Epoch 1 | Step 63700 | Avg Loss: 0.0135 | Grad Norm: 0.00971328\n",
      "Epoch 1 | Step 63800 | Avg Loss: 0.0133 | Grad Norm: 0.01067730\n",
      "Epoch 1 | Step 63900 | Avg Loss: 0.0134 | Grad Norm: 0.01031244\n",
      "Epoch 1 | Step 64000 | Avg Loss: 0.0135 | Grad Norm: 0.01110895\n",
      "Epoch 1 | Step 64100 | Avg Loss: 0.0135 | Grad Norm: 0.01033767\n",
      "Epoch 1 | Step 64200 | Avg Loss: 0.0135 | Grad Norm: 0.01257529\n",
      "Epoch 1 | Step 64300 | Avg Loss: 0.0138 | Grad Norm: 0.00956999\n",
      "Epoch 1 | Step 64400 | Avg Loss: 0.0143 | Grad Norm: 0.01076287\n",
      "Epoch 1 | Step 64500 | Avg Loss: 0.0143 | Grad Norm: 0.01081694\n",
      "Epoch 1 | Step 64600 | Avg Loss: 0.0142 | Grad Norm: 0.01126848\n",
      "Epoch 1 | Step 64700 | Avg Loss: 0.0141 | Grad Norm: 0.00955519\n",
      "Epoch 1 | Step 64800 | Avg Loss: 0.0139 | Grad Norm: 0.00966780\n",
      "Epoch 1 | Step 64900 | Avg Loss: 0.0138 | Grad Norm: 0.00858444\n",
      "Epoch 1 | Step 65000 | Avg Loss: 0.0138 | Grad Norm: 0.01092791\n",
      "Epoch 1 | Step 65100 | Avg Loss: 0.0137 | Grad Norm: 0.01048336\n",
      "Epoch 1 | Step 65200 | Avg Loss: 0.0136 | Grad Norm: 0.01260142\n",
      "Epoch 1 | Step 65300 | Avg Loss: 0.0136 | Grad Norm: 0.01050151\n",
      "Epoch 1 | Step 65400 | Avg Loss: 0.0135 | Grad Norm: 0.01167427\n",
      "Epoch 1 | Step 65500 | Avg Loss: 0.0136 | Grad Norm: 0.00967350\n",
      "Epoch 1 | Step 65600 | Avg Loss: 0.0135 | Grad Norm: 0.00975396\n",
      "Epoch 1 | Step 65700 | Avg Loss: 0.0133 | Grad Norm: 0.01198424\n",
      "Epoch 1 | Step 65800 | Avg Loss: 0.0135 | Grad Norm: 0.01068230\n",
      "Epoch 1 | Step 65900 | Avg Loss: 0.0138 | Grad Norm: 0.01148626\n",
      "Epoch 1 | Step 66000 | Avg Loss: 0.0137 | Grad Norm: 0.00934439\n",
      "Epoch 1 | Step 66100 | Avg Loss: 0.0137 | Grad Norm: 0.01006783\n",
      "Epoch 1 | Step 66200 | Avg Loss: 0.0138 | Grad Norm: 0.01307267\n",
      "Epoch 1 | Step 66300 | Avg Loss: 0.0139 | Grad Norm: 0.00961173\n",
      "Epoch 1 | Step 66400 | Avg Loss: 0.0141 | Grad Norm: 0.01054882\n",
      "Epoch 1 | Step 66500 | Avg Loss: 0.0144 | Grad Norm: 0.01297169\n",
      "Epoch 1 | Step 66600 | Avg Loss: 0.0144 | Grad Norm: 0.01134428\n",
      "Epoch 1 | Step 66700 | Avg Loss: 0.0141 | Grad Norm: 0.01127271\n",
      "Epoch 1 | Step 66800 | Avg Loss: 0.0133 | Grad Norm: 0.00913713\n",
      "Epoch 1 | Step 66900 | Avg Loss: 0.0135 | Grad Norm: 0.01022243\n",
      "Epoch 1 | Step 67000 | Avg Loss: 0.0139 | Grad Norm: 0.01079589\n",
      "Epoch 1 | Step 67100 | Avg Loss: 0.0138 | Grad Norm: 0.00973502\n",
      "Epoch 1 | Step 67200 | Avg Loss: 0.0138 | Grad Norm: 0.00958418\n",
      "Epoch 1 | Step 67300 | Avg Loss: 0.0136 | Grad Norm: 0.01197353\n",
      "Epoch 1 | Step 67400 | Avg Loss: 0.0138 | Grad Norm: 0.01074688\n",
      "Epoch 1 | Step 67500 | Avg Loss: 0.0139 | Grad Norm: 0.01068813\n",
      "Epoch 1 | Step 67600 | Avg Loss: 0.0145 | Grad Norm: 0.01044229\n",
      "Epoch 1 | Step 67700 | Avg Loss: 0.0143 | Grad Norm: 0.01301477\n",
      "Epoch 1 | Step 67800 | Avg Loss: 0.0142 | Grad Norm: 0.01009677\n",
      "Epoch 1 | Step 67900 | Avg Loss: 0.0140 | Grad Norm: 0.01256359\n",
      "Epoch 1 | Step 68000 | Avg Loss: 0.0137 | Grad Norm: 0.01011864\n",
      "Epoch 1 | Step 68100 | Avg Loss: 0.0138 | Grad Norm: 0.00989007\n",
      "Epoch 1 | Step 68200 | Avg Loss: 0.0138 | Grad Norm: 0.00964305\n",
      "Epoch 1 | Step 68300 | Avg Loss: 0.0138 | Grad Norm: 0.01007770\n",
      "Epoch 1 | Step 68400 | Avg Loss: 0.0139 | Grad Norm: 0.00930108\n",
      "Epoch 1 | Step 68500 | Avg Loss: 0.0141 | Grad Norm: 0.00885047\n",
      "Epoch 1 | Step 68600 | Avg Loss: 0.0141 | Grad Norm: 0.01139371\n",
      "Epoch 1 | Step 68700 | Avg Loss: 0.0138 | Grad Norm: 0.01075663\n",
      "Epoch 1 | Step 68800 | Avg Loss: 0.0142 | Grad Norm: 0.01055527\n",
      "Epoch 1 | Step 68900 | Avg Loss: 0.0143 | Grad Norm: 0.01033515\n",
      "Epoch 1 | Step 69000 | Avg Loss: 0.0146 | Grad Norm: 0.01068144\n",
      "Epoch 1 | Step 69100 | Avg Loss: 0.0144 | Grad Norm: 0.01179610\n",
      "Epoch 1 | Step 69200 | Avg Loss: 0.0140 | Grad Norm: 0.00993668\n",
      "Epoch 1 | Step 69300 | Avg Loss: 0.0138 | Grad Norm: 0.01077085\n",
      "Epoch 1 | Step 69400 | Avg Loss: 0.0139 | Grad Norm: 0.00902566\n",
      "Epoch 1 | Step 69500 | Avg Loss: 0.0143 | Grad Norm: 0.01169161\n",
      "Epoch 1 | Step 69600 | Avg Loss: 0.0142 | Grad Norm: 0.01058269\n",
      "Epoch 1 | Step 69700 | Avg Loss: 0.0143 | Grad Norm: 0.00778783\n",
      "Epoch 1 | Step 69800 | Avg Loss: 0.0140 | Grad Norm: 0.01232990\n",
      "Epoch 1 | Step 69900 | Avg Loss: 0.0142 | Grad Norm: 0.01048625\n",
      "Epoch 1 | Step 70000 | Avg Loss: 0.0143 | Grad Norm: 0.01095414\n",
      "Epoch 1 | Step 70100 | Avg Loss: 0.0145 | Grad Norm: 0.01013659\n",
      "Epoch 1 | Step 70200 | Avg Loss: 0.0140 | Grad Norm: 0.01178133\n",
      "Epoch 1 | Step 70300 | Avg Loss: 0.0140 | Grad Norm: 0.00984366\n",
      "Epoch 1 | Step 70400 | Avg Loss: 0.0138 | Grad Norm: 0.01265234\n",
      "Epoch 1 | Step 70500 | Avg Loss: 0.0144 | Grad Norm: 0.01003561\n",
      "Epoch 1 | Step 70600 | Avg Loss: 0.0142 | Grad Norm: 0.01054417\n",
      "Epoch 1 | Step 70700 | Avg Loss: 0.0139 | Grad Norm: 0.01020065\n",
      "Epoch 1 | Step 70800 | Avg Loss: 0.0135 | Grad Norm: 0.00968456\n",
      "Epoch 1 | Step 70900 | Avg Loss: 0.0137 | Grad Norm: 0.00942210\n",
      "Epoch 1 | Step 71000 | Avg Loss: 0.0142 | Grad Norm: 0.01106871\n",
      "Epoch 1 | Step 71100 | Avg Loss: 0.0142 | Grad Norm: 0.01064945\n",
      "Epoch 1 | Step 71200 | Avg Loss: 0.0145 | Grad Norm: 0.00966569\n",
      "Epoch 1 | Step 71300 | Avg Loss: 0.0141 | Grad Norm: 0.01044036\n",
      "Epoch 1 | Step 71400 | Avg Loss: 0.0140 | Grad Norm: 0.01001336\n",
      "Epoch 1 | Step 71500 | Avg Loss: 0.0141 | Grad Norm: 0.00945160\n",
      "Epoch 1 | Step 71600 | Avg Loss: 0.0146 | Grad Norm: 0.01020136\n",
      "Epoch 1 | Step 71700 | Avg Loss: 0.0145 | Grad Norm: 0.01093244\n",
      "Epoch 1 | Step 71800 | Avg Loss: 0.0143 | Grad Norm: 0.01088700\n",
      "Epoch 1 | Step 71900 | Avg Loss: 0.0140 | Grad Norm: 0.01177559\n",
      "Epoch 1 | Step 72000 | Avg Loss: 0.0145 | Grad Norm: 0.00999189\n",
      "Epoch 1 | Step 72100 | Avg Loss: 0.0142 | Grad Norm: 0.01042645\n",
      "Epoch 1 | Step 72200 | Avg Loss: 0.0139 | Grad Norm: 0.01032317\n",
      "Epoch 1 | Step 72300 | Avg Loss: 0.0141 | Grad Norm: 0.00997560\n",
      "Epoch 1 | Step 72400 | Avg Loss: 0.0143 | Grad Norm: 0.01178298\n",
      "Epoch 1 | Step 72500 | Avg Loss: 0.0142 | Grad Norm: 0.01257564\n",
      "Epoch 1 | Step 72600 | Avg Loss: 0.0143 | Grad Norm: 0.00998833\n",
      "Epoch 1 | Step 72700 | Avg Loss: 0.0140 | Grad Norm: 0.01127175\n",
      "Epoch 1 | Step 72800 | Avg Loss: 0.0142 | Grad Norm: 0.01037978\n",
      "Epoch 1 | Step 72900 | Avg Loss: 0.0145 | Grad Norm: 0.01046657\n",
      "Epoch 1 | Step 73000 | Avg Loss: 0.0150 | Grad Norm: 0.01183344\n",
      "Epoch 1 | Step 73100 | Avg Loss: 0.0148 | Grad Norm: 0.00950033\n",
      "Epoch 1 | Step 73200 | Avg Loss: 0.0146 | Grad Norm: 0.00983064\n",
      "Epoch 1 | Step 73300 | Avg Loss: 0.0145 | Grad Norm: 0.01135911\n",
      "Epoch 1 | Step 73400 | Avg Loss: 0.0142 | Grad Norm: 0.01066252\n",
      "Epoch 1 | Step 73500 | Avg Loss: 0.0137 | Grad Norm: 0.01006060\n",
      "Epoch 1 | Step 73600 | Avg Loss: 0.0139 | Grad Norm: 0.00811815\n",
      "Epoch 1 | Step 73700 | Avg Loss: 0.0139 | Grad Norm: 0.01173339\n",
      "Epoch 1 | Step 73800 | Avg Loss: 0.0140 | Grad Norm: 0.01024447\n",
      "Epoch 1 | Step 73900 | Avg Loss: 0.0138 | Grad Norm: 0.00996600\n",
      "Epoch 1 | Step 74000 | Avg Loss: 0.0138 | Grad Norm: 0.00937988\n",
      "Epoch 1 | Step 74100 | Avg Loss: 0.0141 | Grad Norm: 0.01177668\n",
      "Epoch 1 | Step 74200 | Avg Loss: 0.0139 | Grad Norm: 0.01000563\n",
      "Epoch 1 | Step 74300 | Avg Loss: 0.0140 | Grad Norm: 0.01035697\n",
      "Epoch 1 | Step 74400 | Avg Loss: 0.0142 | Grad Norm: 0.01352795\n",
      "Epoch 1 | Step 74500 | Avg Loss: 0.0142 | Grad Norm: 0.01107453\n",
      "Epoch 1 | Step 74600 | Avg Loss: 0.0145 | Grad Norm: 0.01072381\n",
      "Epoch 1 | Step 74700 | Avg Loss: 0.0143 | Grad Norm: 0.01014542\n",
      "Epoch 1 | Step 74800 | Avg Loss: 0.0142 | Grad Norm: 0.01013714\n",
      "Epoch 1 | Step 74900 | Avg Loss: 0.0141 | Grad Norm: 0.00926874\n",
      "Epoch 1 | Step 75000 | Avg Loss: 0.0147 | Grad Norm: 0.01085557\n",
      "Epoch 1 | Step 75100 | Avg Loss: 0.0145 | Grad Norm: 0.01072888\n",
      "Epoch 1 | Step 75200 | Avg Loss: 0.0141 | Grad Norm: 0.00916820\n",
      "Epoch 1 | Step 75300 | Avg Loss: 0.0139 | Grad Norm: 0.01071602\n",
      "Epoch 1 | Step 75400 | Avg Loss: 0.0139 | Grad Norm: 0.01175692\n",
      "Epoch 1 | Step 75500 | Avg Loss: 0.0136 | Grad Norm: 0.01071978\n",
      "Epoch 1 | Step 75600 | Avg Loss: 0.0139 | Grad Norm: 0.00933702\n",
      "Epoch 1 | Step 75700 | Avg Loss: 0.0142 | Grad Norm: 0.01153615\n",
      "Epoch 1 | Step 75800 | Avg Loss: 0.0143 | Grad Norm: 0.01194991\n",
      "Epoch 1 | Step 75900 | Avg Loss: 0.0144 | Grad Norm: 0.01126879\n",
      "Epoch 1 | Step 76000 | Avg Loss: 0.0144 | Grad Norm: 0.01086310\n",
      "Epoch 1 | Step 76100 | Avg Loss: 0.0142 | Grad Norm: 0.01010538\n",
      "Epoch 1 | Step 76200 | Avg Loss: 0.0142 | Grad Norm: 0.01008828\n",
      "Epoch 1 | Step 76300 | Avg Loss: 0.0145 | Grad Norm: 0.01087637\n",
      "Epoch 1 | Step 76400 | Avg Loss: 0.0142 | Grad Norm: 0.01073831\n",
      "Epoch 1 | Step 76500 | Avg Loss: 0.0142 | Grad Norm: 0.01131692\n",
      "Epoch 1 | Step 76600 | Avg Loss: 0.0140 | Grad Norm: 0.01046353\n",
      "Epoch 1 | Step 76700 | Avg Loss: 0.0144 | Grad Norm: 0.01055813\n",
      "Epoch 1 | Step 76800 | Avg Loss: 0.0142 | Grad Norm: 0.01074621\n",
      "Epoch 1 | Step 76900 | Avg Loss: 0.0143 | Grad Norm: 0.01019591\n",
      "Epoch 1 | Step 77000 | Avg Loss: 0.0144 | Grad Norm: 0.01140928\n",
      "Epoch 1 | Step 77100 | Avg Loss: 0.0142 | Grad Norm: 0.01145397\n",
      "Epoch 1 | Step 77200 | Avg Loss: 0.0141 | Grad Norm: 0.01011196\n",
      "Epoch 1 | Step 77300 | Avg Loss: 0.0143 | Grad Norm: 0.01177165\n",
      "Epoch 1 | Step 77400 | Avg Loss: 0.0143 | Grad Norm: 0.01083954\n",
      "Epoch 1 | Step 77500 | Avg Loss: 0.0145 | Grad Norm: 0.01080549\n",
      "Epoch 1 | Step 77600 | Avg Loss: 0.0149 | Grad Norm: 0.01370288\n",
      "Epoch 1 | Step 77700 | Avg Loss: 0.0151 | Grad Norm: 0.01014759\n",
      "Epoch 1 | Step 77800 | Avg Loss: 0.0150 | Grad Norm: 0.01301487\n",
      "Epoch 1 | Step 77900 | Avg Loss: 0.0148 | Grad Norm: 0.01143746\n",
      "Epoch 1 | Step 78000 | Avg Loss: 0.0145 | Grad Norm: 0.01169263\n",
      "Epoch 1 | Step 78100 | Avg Loss: 0.0143 | Grad Norm: 0.01111908\n",
      "Epoch 1 | Step 78200 | Avg Loss: 0.0145 | Grad Norm: 0.01345088\n",
      "Epoch 1 | Step 78300 | Avg Loss: 0.0149 | Grad Norm: 0.01182533\n",
      "Epoch 1 | Step 78400 | Avg Loss: 0.0149 | Grad Norm: 0.01111356\n",
      "Epoch 1 | Step 78500 | Avg Loss: 0.0147 | Grad Norm: 0.01034804\n",
      "Epoch 1 | Step 78600 | Avg Loss: 0.0145 | Grad Norm: 0.01033581\n",
      "Epoch 1 | Step 78700 | Avg Loss: 0.0144 | Grad Norm: 0.01049254\n",
      "Epoch 1 | Step 78800 | Avg Loss: 0.0147 | Grad Norm: 0.00983345\n",
      "Epoch 1 | Step 78900 | Avg Loss: 0.0146 | Grad Norm: 0.00958215\n",
      "Epoch 1 | Step 79000 | Avg Loss: 0.0147 | Grad Norm: 0.01277144\n",
      "Epoch 1 | Step 79100 | Avg Loss: 0.0143 | Grad Norm: 0.01165908\n",
      "Epoch 1 | Step 79200 | Avg Loss: 0.0140 | Grad Norm: 0.01027659\n",
      "Epoch 1 | Step 79300 | Avg Loss: 0.0139 | Grad Norm: 0.00897077\n",
      "Epoch 1 | Step 79400 | Avg Loss: 0.0143 | Grad Norm: 0.01209607\n",
      "Epoch 1 | Step 79500 | Avg Loss: 0.0139 | Grad Norm: 0.00944528\n",
      "Epoch 1 | Step 79600 | Avg Loss: 0.0142 | Grad Norm: 0.00904336\n",
      "Epoch 1 | Step 79700 | Avg Loss: 0.0146 | Grad Norm: 0.01069385\n",
      "Epoch 1 | Step 79800 | Avg Loss: 0.0149 | Grad Norm: 0.01113180\n",
      "Epoch 1 | Step 79900 | Avg Loss: 0.0150 | Grad Norm: 0.01253880\n",
      "Epoch 1 | Step 80000 | Avg Loss: 0.0146 | Grad Norm: 0.01119762\n",
      "Epoch 1 | Step 80100 | Avg Loss: 0.0144 | Grad Norm: 0.01016502\n",
      "Epoch 1 | Step 80200 | Avg Loss: 0.0142 | Grad Norm: 0.01215333\n",
      "Epoch 1 | Step 80300 | Avg Loss: 0.0142 | Grad Norm: 0.01157933\n",
      "Epoch 1 | Step 80400 | Avg Loss: 0.0143 | Grad Norm: 0.01142606\n",
      "Epoch 1 | Step 80500 | Avg Loss: 0.0141 | Grad Norm: 0.01036902\n",
      "Epoch 1 | Step 80600 | Avg Loss: 0.0142 | Grad Norm: 0.01058079\n",
      "Epoch 1 | Step 80700 | Avg Loss: 0.0142 | Grad Norm: 0.01143113\n",
      "Epoch 1 | Step 80800 | Avg Loss: 0.0136 | Grad Norm: 0.01121500\n",
      "Epoch 1 | Step 80900 | Avg Loss: 0.0133 | Grad Norm: 0.01357400\n",
      "Epoch 1 | Step 81000 | Avg Loss: 0.0138 | Grad Norm: 0.01046625\n",
      "Epoch 1 | Step 81100 | Avg Loss: 0.0139 | Grad Norm: 0.00995598\n",
      "Epoch 1 | Step 81200 | Avg Loss: 0.0139 | Grad Norm: 0.00927391\n",
      "Epoch 1 | Step 81300 | Avg Loss: 0.0135 | Grad Norm: 0.01015726\n",
      "Epoch 1 | Step 81400 | Avg Loss: 0.0138 | Grad Norm: 0.00920250\n",
      "Epoch 1 | Step 81500 | Avg Loss: 0.0140 | Grad Norm: 0.01049507\n",
      "Epoch 1 | Step 81600 | Avg Loss: 0.0138 | Grad Norm: 0.01148919\n",
      "Epoch 1 | Step 81700 | Avg Loss: 0.0138 | Grad Norm: 0.01188702\n",
      "Epoch 1 | Step 81800 | Avg Loss: 0.0142 | Grad Norm: 0.01132473\n",
      "Epoch 1 | Step 81900 | Avg Loss: 0.0142 | Grad Norm: 0.00893492\n",
      "Epoch 1 | Step 82000 | Avg Loss: 0.0143 | Grad Norm: 0.01088731\n",
      "Epoch 1 | Step 82100 | Avg Loss: 0.0138 | Grad Norm: 0.01007492\n",
      "Epoch 1 | Step 82200 | Avg Loss: 0.0136 | Grad Norm: 0.01018214\n",
      "Epoch 1 | Step 82300 | Avg Loss: 0.0135 | Grad Norm: 0.01053021\n",
      "Epoch 1 | Step 82400 | Avg Loss: 0.0139 | Grad Norm: 0.01052920\n",
      "Epoch 1 | Step 82500 | Avg Loss: 0.0139 | Grad Norm: 0.01003444\n",
      "Epoch 1 | Step 82600 | Avg Loss: 0.0138 | Grad Norm: 0.00980885\n",
      "Epoch 1 | Step 82700 | Avg Loss: 0.0138 | Grad Norm: 0.00986378\n",
      "Epoch 1 | Step 82800 | Avg Loss: 0.0139 | Grad Norm: 0.01034326\n",
      "Epoch 1 | Step 82900 | Avg Loss: 0.0142 | Grad Norm: 0.00932669\n",
      "Epoch 1 | Step 83000 | Avg Loss: 0.0144 | Grad Norm: 0.01106119\n",
      "Epoch 1 | Step 83100 | Avg Loss: 0.0140 | Grad Norm: 0.01123067\n",
      "Epoch 1 | Step 83200 | Avg Loss: 0.0139 | Grad Norm: 0.00967039\n",
      "Epoch 1 | Step 83300 | Avg Loss: 0.0140 | Grad Norm: 0.01205842\n",
      "Epoch 1 | Step 83400 | Avg Loss: 0.0142 | Grad Norm: 0.01294086\n",
      "Epoch 1 | Step 83500 | Avg Loss: 0.0145 | Grad Norm: 0.01290688\n",
      "Epoch 1 | Step 83600 | Avg Loss: 0.0140 | Grad Norm: 0.00881527\n",
      "Epoch 1 | Step 83700 | Avg Loss: 0.0141 | Grad Norm: 0.01273390\n",
      "Epoch 1 | Step 83800 | Avg Loss: 0.0144 | Grad Norm: 0.01126334\n",
      "Epoch 1 | Step 83900 | Avg Loss: 0.0144 | Grad Norm: 0.01125086\n",
      "Epoch 1 | Step 84000 | Avg Loss: 0.0141 | Grad Norm: 0.00969640\n",
      "Epoch 1 | Step 84100 | Avg Loss: 0.0140 | Grad Norm: 0.00984747\n",
      "Epoch 1 | Step 84200 | Avg Loss: 0.0140 | Grad Norm: 0.00902936\n",
      "Epoch 1 | Step 84300 | Avg Loss: 0.0145 | Grad Norm: 0.01050210\n",
      "Epoch 1 | Step 84400 | Avg Loss: 0.0146 | Grad Norm: 0.01178129\n",
      "Epoch 1 | Step 84500 | Avg Loss: 0.0142 | Grad Norm: 0.01135791\n",
      "Epoch 1 | Step 84600 | Avg Loss: 0.0140 | Grad Norm: 0.01177402\n",
      "Epoch 1 | Step 84700 | Avg Loss: 0.0139 | Grad Norm: 0.01121557\n",
      "Epoch 1 | Step 84800 | Avg Loss: 0.0142 | Grad Norm: 0.00941195\n",
      "Epoch 1 | Step 84900 | Avg Loss: 0.0141 | Grad Norm: 0.01228726\n",
      "Epoch 1 | Step 85000 | Avg Loss: 0.0136 | Grad Norm: 0.01185771\n",
      "Epoch 1 | Step 85100 | Avg Loss: 0.0140 | Grad Norm: 0.01016602\n",
      "Epoch 1 | Step 85200 | Avg Loss: 0.0141 | Grad Norm: 0.01056667\n",
      "Epoch 1 | Step 85300 | Avg Loss: 0.0139 | Grad Norm: 0.00938092\n",
      "Epoch 1 | Step 85400 | Avg Loss: 0.0137 | Grad Norm: 0.01167706\n",
      "Epoch 1 | Step 85500 | Avg Loss: 0.0135 | Grad Norm: 0.01162353\n",
      "Epoch 1 | Step 85600 | Avg Loss: 0.0135 | Grad Norm: 0.01099359\n",
      "Epoch 1 | Step 85700 | Avg Loss: 0.0135 | Grad Norm: 0.01142549\n",
      "Epoch 1 | Step 85800 | Avg Loss: 0.0139 | Grad Norm: 0.01154174\n",
      "Epoch 1 | Step 85900 | Avg Loss: 0.0138 | Grad Norm: 0.01031272\n",
      "Epoch 1 | Step 86000 | Avg Loss: 0.0140 | Grad Norm: 0.01048195\n",
      "Epoch 1 | Step 86100 | Avg Loss: 0.0140 | Grad Norm: 0.01166719\n",
      "Epoch 1 | Step 86200 | Avg Loss: 0.0140 | Grad Norm: 0.01083691\n",
      "Epoch 1 | Step 86300 | Avg Loss: 0.0143 | Grad Norm: 0.00972085\n",
      "Epoch 1 | Step 86400 | Avg Loss: 0.0141 | Grad Norm: 0.01066618\n",
      "Epoch 1 | Step 86500 | Avg Loss: 0.0141 | Grad Norm: 0.01054963\n",
      "Epoch 1 | Step 86600 | Avg Loss: 0.0140 | Grad Norm: 0.01136925\n",
      "Epoch 1 | Step 86700 | Avg Loss: 0.0137 | Grad Norm: 0.01076159\n",
      "Epoch 1 | Step 86800 | Avg Loss: 0.0142 | Grad Norm: 0.00888374\n",
      "Epoch 1 | Step 86900 | Avg Loss: 0.0141 | Grad Norm: 0.01237574\n",
      "Epoch 1 | Step 87000 | Avg Loss: 0.0142 | Grad Norm: 0.01042267\n",
      "Epoch 1 | Step 87100 | Avg Loss: 0.0142 | Grad Norm: 0.00945986\n",
      "Epoch 1 | Step 87200 | Avg Loss: 0.0139 | Grad Norm: 0.01032888\n",
      "Epoch 1 | Step 87300 | Avg Loss: 0.0143 | Grad Norm: 0.00955492\n",
      "Epoch 1 | Step 87400 | Avg Loss: 0.0144 | Grad Norm: 0.01022059\n",
      "Epoch 1 | Step 87500 | Avg Loss: 0.0146 | Grad Norm: 0.01136096\n",
      "Epoch 1 | Step 87600 | Avg Loss: 0.0147 | Grad Norm: 0.01157150\n",
      "Epoch 1 | Step 87700 | Avg Loss: 0.0148 | Grad Norm: 0.00920326\n",
      "Epoch 1 | Step 87800 | Avg Loss: 0.0145 | Grad Norm: 0.00866737\n",
      "Epoch 1 | Step 87900 | Avg Loss: 0.0142 | Grad Norm: 0.01085281\n",
      "Epoch 1 | Step 88000 | Avg Loss: 0.0142 | Grad Norm: 0.01152305\n",
      "Epoch 1 | Step 88100 | Avg Loss: 0.0144 | Grad Norm: 0.01198073\n",
      "Epoch 1 | Step 88200 | Avg Loss: 0.0143 | Grad Norm: 0.01151389\n",
      "Epoch 1 | Step 88300 | Avg Loss: 0.0140 | Grad Norm: 0.01097635\n",
      "Epoch 1 | Step 88400 | Avg Loss: 0.0140 | Grad Norm: 0.01065167\n",
      "Epoch 1 | Step 88500 | Avg Loss: 0.0139 | Grad Norm: 0.01149427\n",
      "Epoch 1 | Step 88600 | Avg Loss: 0.0136 | Grad Norm: 0.01007253\n",
      "Epoch 1 | Step 88700 | Avg Loss: 0.0141 | Grad Norm: 0.00981130\n",
      "Epoch 1 | Step 88800 | Avg Loss: 0.0141 | Grad Norm: 0.01134919\n",
      "Epoch 1 | Step 88900 | Avg Loss: 0.0140 | Grad Norm: 0.01029156\n",
      "Epoch 1 | Step 89000 | Avg Loss: 0.0141 | Grad Norm: 0.00989382\n",
      "Epoch 1 | Step 89100 | Avg Loss: 0.0141 | Grad Norm: 0.00924240\n",
      "Epoch 1 | Step 89200 | Avg Loss: 0.0146 | Grad Norm: 0.00879306\n",
      "Epoch 1 | Step 89300 | Avg Loss: 0.0145 | Grad Norm: 0.01217520\n",
      "Epoch 1 | Step 89400 | Avg Loss: 0.0144 | Grad Norm: 0.01015227\n",
      "Epoch 1 | Step 89500 | Avg Loss: 0.0143 | Grad Norm: 0.00953945\n",
      "Epoch 1 | Step 89600 | Avg Loss: 0.0141 | Grad Norm: 0.01042333\n",
      "Epoch 1 | Step 89700 | Avg Loss: 0.0134 | Grad Norm: 0.00908562\n",
      "Epoch 1 | Step 89800 | Avg Loss: 0.0134 | Grad Norm: 0.01014790\n",
      "Epoch 1 | Step 89900 | Avg Loss: 0.0140 | Grad Norm: 0.01033044\n",
      "Epoch 1 | Step 90000 | Avg Loss: 0.0144 | Grad Norm: 0.01182704\n",
      "Epoch 1 | Step 90100 | Avg Loss: 0.0142 | Grad Norm: 0.01200899\n",
      "Epoch 1 | Step 90200 | Avg Loss: 0.0144 | Grad Norm: 0.01273414\n",
      "Epoch 1 | Step 90300 | Avg Loss: 0.0143 | Grad Norm: 0.01006969\n",
      "Epoch 1 | Step 90400 | Avg Loss: 0.0146 | Grad Norm: 0.01051305\n",
      "Epoch 1 | Step 90500 | Avg Loss: 0.0145 | Grad Norm: 0.01100718\n",
      "Epoch 1 | Step 90600 | Avg Loss: 0.0145 | Grad Norm: 0.01107547\n",
      "Epoch 1 | Step 90700 | Avg Loss: 0.0143 | Grad Norm: 0.01208869\n",
      "Epoch 1 | Step 90800 | Avg Loss: 0.0139 | Grad Norm: 0.00913615\n",
      "Epoch 1 | Step 90900 | Avg Loss: 0.0135 | Grad Norm: 0.00984210\n",
      "Epoch 1 | Step 91000 | Avg Loss: 0.0134 | Grad Norm: 0.01057252\n",
      "Epoch 1 | Step 91100 | Avg Loss: 0.0136 | Grad Norm: 0.01081170\n",
      "Epoch 1 | Step 91200 | Avg Loss: 0.0142 | Grad Norm: 0.01123288\n",
      "Epoch 1 | Step 91300 | Avg Loss: 0.0147 | Grad Norm: 0.01127435\n",
      "Epoch 1 | Step 91400 | Avg Loss: 0.0144 | Grad Norm: 0.01136585\n",
      "Epoch 1 | Step 91500 | Avg Loss: 0.0138 | Grad Norm: 0.01103036\n",
      "Epoch 1 | Step 91600 | Avg Loss: 0.0141 | Grad Norm: 0.00947537\n",
      "Epoch 1 | Step 91700 | Avg Loss: 0.0142 | Grad Norm: 0.01068796\n",
      "Epoch 1 | Step 91800 | Avg Loss: 0.0137 | Grad Norm: 0.01029200\n",
      "Epoch 1 | Step 91900 | Avg Loss: 0.0139 | Grad Norm: 0.01073925\n",
      "Epoch 1 | Step 92000 | Avg Loss: 0.0141 | Grad Norm: 0.01029468\n",
      "Epoch 1 | Step 92100 | Avg Loss: 0.0141 | Grad Norm: 0.01143110\n",
      "Epoch 1 | Step 92200 | Avg Loss: 0.0142 | Grad Norm: 0.01027265\n",
      "Epoch 1 | Step 92300 | Avg Loss: 0.0146 | Grad Norm: 0.01070858\n",
      "Epoch 1 | Step 92400 | Avg Loss: 0.0143 | Grad Norm: 0.01044224\n",
      "Epoch 1 | Step 92500 | Avg Loss: 0.0144 | Grad Norm: 0.01123971\n",
      "Epoch 1 | Step 92600 | Avg Loss: 0.0141 | Grad Norm: 0.01107460\n",
      "Epoch 1 | Step 92700 | Avg Loss: 0.0142 | Grad Norm: 0.01162764\n",
      "Epoch 1 | Step 92800 | Avg Loss: 0.0139 | Grad Norm: 0.00959663\n",
      "Epoch 1 | Step 92900 | Avg Loss: 0.0138 | Grad Norm: 0.01121529\n",
      "Epoch 1 | Step 93000 | Avg Loss: 0.0137 | Grad Norm: 0.01103045\n",
      "Epoch 1 | Step 93100 | Avg Loss: 0.0139 | Grad Norm: 0.01078510\n",
      "Epoch 1 | Step 93200 | Avg Loss: 0.0138 | Grad Norm: 0.00969795\n",
      "Epoch 1 | Step 93300 | Avg Loss: 0.0136 | Grad Norm: 0.00896761\n",
      "Epoch 1 | Step 93400 | Avg Loss: 0.0136 | Grad Norm: 0.01073167\n",
      "Epoch 1 | Step 93500 | Avg Loss: 0.0138 | Grad Norm: 0.01277937\n",
      "Epoch 1 | Step 93600 | Avg Loss: 0.0138 | Grad Norm: 0.00875291\n",
      "Epoch 1 | Step 93700 | Avg Loss: 0.0136 | Grad Norm: 0.01030659\n",
      "Epoch 1 | Step 93800 | Avg Loss: 0.0141 | Grad Norm: 0.01123280\n",
      "Epoch 1 | Step 93900 | Avg Loss: 0.0142 | Grad Norm: 0.00953676\n",
      "Epoch 1 | Step 94000 | Avg Loss: 0.0144 | Grad Norm: 0.00992592\n",
      "Epoch 1 | Step 94100 | Avg Loss: 0.0143 | Grad Norm: 0.00946148\n",
      "Epoch 1 | Step 94200 | Avg Loss: 0.0141 | Grad Norm: 0.00931677\n",
      "Epoch 1 | Step 94300 | Avg Loss: 0.0140 | Grad Norm: 0.01046892\n",
      "Epoch 1 | Step 94400 | Avg Loss: 0.0141 | Grad Norm: 0.00994363\n",
      "Epoch 1 | Step 94500 | Avg Loss: 0.0138 | Grad Norm: 0.01126029\n",
      "Epoch 1 | Step 94600 | Avg Loss: 0.0139 | Grad Norm: 0.01103340\n",
      "Epoch 1 | Step 94700 | Avg Loss: 0.0137 | Grad Norm: 0.00911549\n",
      "Epoch 1 | Step 94800 | Avg Loss: 0.0137 | Grad Norm: 0.01252937\n",
      "Epoch 1 | Step 94900 | Avg Loss: 0.0136 | Grad Norm: 0.00850963\n",
      "Epoch 1 | Step 95000 | Avg Loss: 0.0139 | Grad Norm: 0.01066922\n",
      "Epoch 1 | Step 95100 | Avg Loss: 0.0142 | Grad Norm: 0.00990672\n",
      "Epoch 1 | Step 95200 | Avg Loss: 0.0142 | Grad Norm: 0.01028717\n",
      "Epoch 1 | Step 95300 | Avg Loss: 0.0139 | Grad Norm: 0.01023736\n",
      "Epoch 1 | Step 95400 | Avg Loss: 0.0142 | Grad Norm: 0.01035252\n",
      "Epoch 1 | Step 95500 | Avg Loss: 0.0138 | Grad Norm: 0.00870017\n",
      "Epoch 1 | Step 95600 | Avg Loss: 0.0138 | Grad Norm: 0.01107048\n",
      "Epoch 1 | Step 95700 | Avg Loss: 0.0138 | Grad Norm: 0.01163102\n",
      "Epoch 1 | Step 95800 | Avg Loss: 0.0136 | Grad Norm: 0.00968495\n",
      "Epoch 1 | Step 95900 | Avg Loss: 0.0133 | Grad Norm: 0.01000158\n",
      "Epoch 1 | Step 96000 | Avg Loss: 0.0134 | Grad Norm: 0.00926514\n",
      "Epoch 1 | Step 96100 | Avg Loss: 0.0132 | Grad Norm: 0.01002979\n",
      "Epoch 1 | Step 96200 | Avg Loss: 0.0132 | Grad Norm: 0.00917135\n",
      "Epoch 1 | Step 96300 | Avg Loss: 0.0136 | Grad Norm: 0.00957207\n",
      "Epoch 1 | Step 96400 | Avg Loss: 0.0137 | Grad Norm: 0.01078935\n",
      "Epoch 1 | Step 96500 | Avg Loss: 0.0140 | Grad Norm: 0.00857864\n",
      "Epoch 1 | Step 96600 | Avg Loss: 0.0139 | Grad Norm: 0.01060743\n",
      "Epoch 1 | Step 96700 | Avg Loss: 0.0143 | Grad Norm: 0.01077381\n",
      "Epoch 1 | Step 96800 | Avg Loss: 0.0143 | Grad Norm: 0.01104247\n",
      "Epoch 1 | Step 96900 | Avg Loss: 0.0142 | Grad Norm: 0.01023184\n",
      "Epoch 1 | Step 97000 | Avg Loss: 0.0140 | Grad Norm: 0.01101533\n",
      "Epoch 1 | Step 97100 | Avg Loss: 0.0140 | Grad Norm: 0.01079165\n",
      "Epoch 1 | Step 97200 | Avg Loss: 0.0143 | Grad Norm: 0.01083892\n",
      "Epoch 1 | Step 97300 | Avg Loss: 0.0147 | Grad Norm: 0.01016120\n",
      "Epoch 1 | Step 97400 | Avg Loss: 0.0143 | Grad Norm: 0.01234797\n",
      "Epoch 1 | Step 97500 | Avg Loss: 0.0145 | Grad Norm: 0.01144039\n",
      "Epoch 1 | Step 97600 | Avg Loss: 0.0145 | Grad Norm: 0.01257334\n",
      "Epoch 1 | Step 97700 | Avg Loss: 0.0148 | Grad Norm: 0.01228255\n",
      "Epoch 1 | Step 97800 | Avg Loss: 0.0147 | Grad Norm: 0.01106821\n",
      "Epoch 1 | Step 97900 | Avg Loss: 0.0151 | Grad Norm: 0.01147927\n",
      "Epoch 1 | Step 98000 | Avg Loss: 0.0144 | Grad Norm: 0.01045593\n",
      "Epoch 1 | Step 98100 | Avg Loss: 0.0144 | Grad Norm: 0.00943408\n",
      "Epoch 1 | Step 98200 | Avg Loss: 0.0141 | Grad Norm: 0.01034177\n",
      "Epoch 1 | Step 98300 | Avg Loss: 0.0143 | Grad Norm: 0.01031397\n",
      "Epoch 1 | Step 98400 | Avg Loss: 0.0142 | Grad Norm: 0.01000231\n",
      "Epoch 1 | Step 98500 | Avg Loss: 0.0141 | Grad Norm: 0.00995794\n",
      "Epoch 1 | Step 98600 | Avg Loss: 0.0139 | Grad Norm: 0.00980428\n",
      "Epoch 1 | Step 98700 | Avg Loss: 0.0142 | Grad Norm: 0.01104988\n",
      "Epoch 1 | Step 98800 | Avg Loss: 0.0143 | Grad Norm: 0.01115878\n",
      "Epoch 1 | Step 98900 | Avg Loss: 0.0144 | Grad Norm: 0.01174383\n",
      "Epoch 1 | Step 99000 | Avg Loss: 0.0145 | Grad Norm: 0.01123148\n",
      "Epoch 1 | Step 99100 | Avg Loss: 0.0143 | Grad Norm: 0.01121407\n",
      "Epoch 1 | Step 99200 | Avg Loss: 0.0141 | Grad Norm: 0.01074133\n",
      "Epoch 1 | Step 99300 | Avg Loss: 0.0140 | Grad Norm: 0.00993350\n",
      "Epoch 1 | Step 99400 | Avg Loss: 0.0135 | Grad Norm: 0.00929665\n",
      "Epoch 1 | Step 99500 | Avg Loss: 0.0134 | Grad Norm: 0.00987318\n",
      "Epoch 1 | Step 99600 | Avg Loss: 0.0136 | Grad Norm: 0.01315978\n",
      "Epoch 1 | Step 99700 | Avg Loss: 0.0136 | Grad Norm: 0.00988378\n",
      "Epoch 1 | Step 99800 | Avg Loss: 0.0134 | Grad Norm: 0.01102387\n",
      "Epoch 1 | Step 99900 | Avg Loss: 0.0138 | Grad Norm: 0.01038802\n",
      "Epoch 1 | Step 100000 | Avg Loss: 0.0139 | Grad Norm: 0.01126726\n",
      "Saving model at step100000\n",
      "Epoch 1 | Step 100100 | Avg Loss: 0.0139 | Grad Norm: 0.01014509\n",
      "Epoch 1 | Step 100200 | Avg Loss: 0.0137 | Grad Norm: 0.01000885\n",
      "Epoch 1 | Step 100300 | Avg Loss: 0.0139 | Grad Norm: 0.01318727\n",
      "Epoch 1 | Step 100400 | Avg Loss: 0.0139 | Grad Norm: 0.01045242\n",
      "Epoch 1 | Step 100500 | Avg Loss: 0.0143 | Grad Norm: 0.01098917\n",
      "Epoch 1 | Step 100600 | Avg Loss: 0.0147 | Grad Norm: 0.01014208\n",
      "Epoch 1 | Step 100700 | Avg Loss: 0.0151 | Grad Norm: 0.01135015\n",
      "Epoch 1 | Step 100800 | Avg Loss: 0.0147 | Grad Norm: 0.01114307\n",
      "Epoch 1 | Step 100900 | Avg Loss: 0.0150 | Grad Norm: 0.01081223\n",
      "Epoch 1 | Step 101000 | Avg Loss: 0.0147 | Grad Norm: 0.00941962\n",
      "Epoch 1 | Step 101100 | Avg Loss: 0.0146 | Grad Norm: 0.01027657\n",
      "Epoch 1 | Step 101200 | Avg Loss: 0.0141 | Grad Norm: 0.01040100\n",
      "Epoch 1 | Step 101300 | Avg Loss: 0.0146 | Grad Norm: 0.01017708\n",
      "Epoch 1 | Step 101400 | Avg Loss: 0.0144 | Grad Norm: 0.01236702\n",
      "Epoch 1 | Step 101500 | Avg Loss: 0.0140 | Grad Norm: 0.00930681\n",
      "Epoch 1 | Step 101600 | Avg Loss: 0.0135 | Grad Norm: 0.00955261\n",
      "Epoch 1 | Step 101700 | Avg Loss: 0.0139 | Grad Norm: 0.01171429\n",
      "Epoch 1 | Step 101800 | Avg Loss: 0.0135 | Grad Norm: 0.00885880\n",
      "Epoch 1 | Step 101900 | Avg Loss: 0.0137 | Grad Norm: 0.00941754\n",
      "Epoch 1 | Step 102000 | Avg Loss: 0.0138 | Grad Norm: 0.01046186\n",
      "Epoch 1 | Step 102100 | Avg Loss: 0.0138 | Grad Norm: 0.01020625\n",
      "Epoch 1 | Step 102200 | Avg Loss: 0.0135 | Grad Norm: 0.01006645\n",
      "Epoch 1 | Step 102300 | Avg Loss: 0.0138 | Grad Norm: 0.01082542\n",
      "Epoch 1 | Step 102400 | Avg Loss: 0.0138 | Grad Norm: 0.01105265\n",
      "Epoch 1 | Step 102500 | Avg Loss: 0.0141 | Grad Norm: 0.00888343\n",
      "Epoch 1 | Step 102600 | Avg Loss: 0.0145 | Grad Norm: 0.00934169\n",
      "Epoch 1 | Step 102700 | Avg Loss: 0.0144 | Grad Norm: 0.00987120\n",
      "Epoch 1 | Step 102800 | Avg Loss: 0.0139 | Grad Norm: 0.00940851\n",
      "Epoch 1 | Step 102900 | Avg Loss: 0.0142 | Grad Norm: 0.01060684\n",
      "Epoch 1 | Step 103000 | Avg Loss: 0.0139 | Grad Norm: 0.01089299\n",
      "Epoch 1 | Step 103100 | Avg Loss: 0.0140 | Grad Norm: 0.01002424\n",
      "Epoch 1 | Step 103200 | Avg Loss: 0.0138 | Grad Norm: 0.01259785\n",
      "Epoch 1 | Step 103300 | Avg Loss: 0.0134 | Grad Norm: 0.01006782\n",
      "Epoch 1 | Step 103400 | Avg Loss: 0.0138 | Grad Norm: 0.00926189\n",
      "Epoch 1 | Step 103500 | Avg Loss: 0.0142 | Grad Norm: 0.01276259\n",
      "Epoch 1 | Step 103600 | Avg Loss: 0.0141 | Grad Norm: 0.01042849\n",
      "Epoch 1 | Step 103700 | Avg Loss: 0.0139 | Grad Norm: 0.01185984\n",
      "Epoch 1 | Step 103800 | Avg Loss: 0.0141 | Grad Norm: 0.00964608\n",
      "Epoch 1 | Step 103900 | Avg Loss: 0.0141 | Grad Norm: 0.01154117\n",
      "Epoch 1 | Step 104000 | Avg Loss: 0.0140 | Grad Norm: 0.01030831\n",
      "Epoch 1 | Step 104100 | Avg Loss: 0.0141 | Grad Norm: 0.01029703\n",
      "Epoch 1 | Step 104200 | Avg Loss: 0.0139 | Grad Norm: 0.01065832\n",
      "Epoch 1 | Step 104300 | Avg Loss: 0.0139 | Grad Norm: 0.00974767\n",
      "Epoch 1 | Step 104400 | Avg Loss: 0.0142 | Grad Norm: 0.01173013\n",
      "Epoch 1 | Step 104500 | Avg Loss: 0.0143 | Grad Norm: 0.01004615\n",
      "Epoch 1 | Step 104600 | Avg Loss: 0.0143 | Grad Norm: 0.01120706\n",
      "Epoch 1 | Step 104700 | Avg Loss: 0.0141 | Grad Norm: 0.00925761\n",
      "Epoch 1 | Step 104800 | Avg Loss: 0.0140 | Grad Norm: 0.01011183\n",
      "Epoch 1 | Step 104900 | Avg Loss: 0.0140 | Grad Norm: 0.01121969\n",
      "Epoch 1 | Step 105000 | Avg Loss: 0.0141 | Grad Norm: 0.00992664\n",
      "Epoch 1 | Step 105100 | Avg Loss: 0.0142 | Grad Norm: 0.01101832\n",
      "Epoch 1 | Step 105200 | Avg Loss: 0.0143 | Grad Norm: 0.01008180\n",
      "Epoch 1 | Step 105300 | Avg Loss: 0.0144 | Grad Norm: 0.01155045\n",
      "Epoch 1 | Step 105400 | Avg Loss: 0.0144 | Grad Norm: 0.01063703\n",
      "Epoch 1 | Step 105500 | Avg Loss: 0.0135 | Grad Norm: 0.00944786\n",
      "Epoch 1 | Step 105600 | Avg Loss: 0.0137 | Grad Norm: 0.01040961\n",
      "Epoch 1 | Step 105700 | Avg Loss: 0.0140 | Grad Norm: 0.00981213\n",
      "Epoch 1 | Step 105800 | Avg Loss: 0.0140 | Grad Norm: 0.00918031\n",
      "Epoch 1 | Step 105900 | Avg Loss: 0.0140 | Grad Norm: 0.00876829\n",
      "Epoch 1 | Step 106000 | Avg Loss: 0.0143 | Grad Norm: 0.01029291\n",
      "Epoch 1 | Step 106100 | Avg Loss: 0.0144 | Grad Norm: 0.01117340\n",
      "Epoch 1 | Step 106200 | Avg Loss: 0.0145 | Grad Norm: 0.01007496\n",
      "Epoch 1 | Step 106300 | Avg Loss: 0.0144 | Grad Norm: 0.01023878\n",
      "Epoch 1 | Step 106400 | Avg Loss: 0.0143 | Grad Norm: 0.00972025\n",
      "Epoch 1 | Step 106500 | Avg Loss: 0.0143 | Grad Norm: 0.00967362\n",
      "Epoch 1 | Step 106600 | Avg Loss: 0.0134 | Grad Norm: 0.01138877\n",
      "Epoch 1 | Step 106700 | Avg Loss: 0.0135 | Grad Norm: 0.01054985\n",
      "Epoch 1 | Step 106800 | Avg Loss: 0.0139 | Grad Norm: 0.01484578\n",
      "Epoch 1 | Step 106900 | Avg Loss: 0.0139 | Grad Norm: 0.01023448\n",
      "Epoch 1 | Step 107000 | Avg Loss: 0.0139 | Grad Norm: 0.01037091\n",
      "Epoch 1 | Step 107100 | Avg Loss: 0.0141 | Grad Norm: 0.00967791\n",
      "Epoch 1 | Step 107200 | Avg Loss: 0.0146 | Grad Norm: 0.01133187\n",
      "Epoch 1 | Step 107300 | Avg Loss: 0.0150 | Grad Norm: 0.00937073\n",
      "Epoch 1 | Step 107400 | Avg Loss: 0.0150 | Grad Norm: 0.01046213\n",
      "Epoch 1 | Step 107500 | Avg Loss: 0.0149 | Grad Norm: 0.01084821\n",
      "Epoch 1 | Step 107600 | Avg Loss: 0.0148 | Grad Norm: 0.01105041\n",
      "Epoch 1 | Step 107700 | Avg Loss: 0.0146 | Grad Norm: 0.01025007\n",
      "Epoch 1 | Step 107800 | Avg Loss: 0.0148 | Grad Norm: 0.00949079\n",
      "Epoch 1 | Step 107900 | Avg Loss: 0.0147 | Grad Norm: 0.00917821\n",
      "Epoch 1 | Step 108000 | Avg Loss: 0.0146 | Grad Norm: 0.01001537\n",
      "Epoch 1 | Step 108100 | Avg Loss: 0.0149 | Grad Norm: 0.01069784\n",
      "Epoch 1 | Step 108200 | Avg Loss: 0.0146 | Grad Norm: 0.01193752\n",
      "Epoch 1 | Step 108300 | Avg Loss: 0.0144 | Grad Norm: 0.01110719\n",
      "Epoch 1 | Step 108400 | Avg Loss: 0.0146 | Grad Norm: 0.00908830\n",
      "Epoch 1 | Step 108500 | Avg Loss: 0.0142 | Grad Norm: 0.01305528\n",
      "Epoch 1 | Step 108600 | Avg Loss: 0.0140 | Grad Norm: 0.00867601\n",
      "Epoch 1 | Step 108700 | Avg Loss: 0.0140 | Grad Norm: 0.01007449\n",
      "Epoch 1 | Step 108800 | Avg Loss: 0.0143 | Grad Norm: 0.01043568\n",
      "Epoch 1 | Step 108900 | Avg Loss: 0.0144 | Grad Norm: 0.01022969\n",
      "Epoch 1 | Step 109000 | Avg Loss: 0.0144 | Grad Norm: 0.00919645\n",
      "Epoch 1 | Step 109100 | Avg Loss: 0.0139 | Grad Norm: 0.00938069\n",
      "Epoch 1 | Step 109200 | Avg Loss: 0.0141 | Grad Norm: 0.01114460\n",
      "Epoch 1 | Step 109300 | Avg Loss: 0.0145 | Grad Norm: 0.00919048\n",
      "Epoch 1 | Step 109400 | Avg Loss: 0.0146 | Grad Norm: 0.01015118\n",
      "Epoch 1 | Step 109500 | Avg Loss: 0.0145 | Grad Norm: 0.01037001\n",
      "Epoch 1 | Step 109600 | Avg Loss: 0.0142 | Grad Norm: 0.01101561\n",
      "Epoch 1 | Step 109700 | Avg Loss: 0.0140 | Grad Norm: 0.01137584\n",
      "Epoch 1 | Step 109800 | Avg Loss: 0.0141 | Grad Norm: 0.00967775\n",
      "Epoch 1 | Step 109900 | Avg Loss: 0.0139 | Grad Norm: 0.00904168\n",
      "Epoch 1 | Step 110000 | Avg Loss: 0.0140 | Grad Norm: 0.01089134\n",
      "Epoch 1 | Step 110100 | Avg Loss: 0.0138 | Grad Norm: 0.01015775\n",
      "Epoch 1 | Step 110200 | Avg Loss: 0.0137 | Grad Norm: 0.01161696\n",
      "Epoch 1 | Step 110300 | Avg Loss: 0.0138 | Grad Norm: 0.01031601\n",
      "Epoch 1 | Step 110400 | Avg Loss: 0.0140 | Grad Norm: 0.01131243\n",
      "Epoch 1 | Step 110500 | Avg Loss: 0.0138 | Grad Norm: 0.01051611\n",
      "Epoch 1 | Step 110600 | Avg Loss: 0.0138 | Grad Norm: 0.01142579\n",
      "Epoch 1 | Step 110700 | Avg Loss: 0.0145 | Grad Norm: 0.01029513\n",
      "Epoch 1 | Step 110800 | Avg Loss: 0.0144 | Grad Norm: 0.00975306\n",
      "Epoch 1 | Step 110900 | Avg Loss: 0.0143 | Grad Norm: 0.01180377\n",
      "Epoch 1 | Step 111000 | Avg Loss: 0.0142 | Grad Norm: 0.00970048\n",
      "Epoch 1 | Step 111100 | Avg Loss: 0.0143 | Grad Norm: 0.01014007\n",
      "Epoch 1 | Step 111200 | Avg Loss: 0.0143 | Grad Norm: 0.01060932\n",
      "Epoch 1 | Step 111300 | Avg Loss: 0.0144 | Grad Norm: 0.01041014\n",
      "Epoch 1 | Step 111400 | Avg Loss: 0.0143 | Grad Norm: 0.01070244\n",
      "Epoch 1 | Step 111500 | Avg Loss: 0.0142 | Grad Norm: 0.00991884\n",
      "Epoch 1 | Step 111600 | Avg Loss: 0.0142 | Grad Norm: 0.01039970\n",
      "Epoch 1 | Step 111700 | Avg Loss: 0.0145 | Grad Norm: 0.01054775\n",
      "Epoch 1 | Step 111800 | Avg Loss: 0.0141 | Grad Norm: 0.01165144\n",
      "Epoch 1 | Step 111900 | Avg Loss: 0.0141 | Grad Norm: 0.00980620\n",
      "Epoch 1 | Step 112000 | Avg Loss: 0.0141 | Grad Norm: 0.01068604\n",
      "Epoch 1 | Step 112100 | Avg Loss: 0.0141 | Grad Norm: 0.00978792\n",
      "Epoch 1 | Step 112200 | Avg Loss: 0.0144 | Grad Norm: 0.01139384\n",
      "Epoch 1 | Step 112300 | Avg Loss: 0.0143 | Grad Norm: 0.00999537\n",
      "Epoch 1 | Step 112400 | Avg Loss: 0.0139 | Grad Norm: 0.00959388\n",
      "Epoch 1 | Step 112500 | Avg Loss: 0.0140 | Grad Norm: 0.01142931\n",
      "Epoch 1 | Step 112600 | Avg Loss: 0.0145 | Grad Norm: 0.01121806\n",
      "Epoch 1 | Step 112700 | Avg Loss: 0.0143 | Grad Norm: 0.01036690\n",
      "Epoch 1 | Step 112800 | Avg Loss: 0.0141 | Grad Norm: 0.00985121\n",
      "Epoch 1 | Step 112900 | Avg Loss: 0.0147 | Grad Norm: 0.01115158\n",
      "Epoch 1 | Step 113000 | Avg Loss: 0.0145 | Grad Norm: 0.01140611\n",
      "Epoch 1 | Step 113100 | Avg Loss: 0.0148 | Grad Norm: 0.01119911\n",
      "Epoch 1 | Step 113200 | Avg Loss: 0.0148 | Grad Norm: 0.01465267\n",
      "Epoch 1 | Step 113300 | Avg Loss: 0.0146 | Grad Norm: 0.01004402\n",
      "Epoch 1 | Step 113400 | Avg Loss: 0.0141 | Grad Norm: 0.01100729\n",
      "Epoch 1 | Step 113500 | Avg Loss: 0.0141 | Grad Norm: 0.01061634\n",
      "Epoch 1 | Step 113600 | Avg Loss: 0.0140 | Grad Norm: 0.01036919\n",
      "Epoch 1 | Step 113700 | Avg Loss: 0.0140 | Grad Norm: 0.01124593\n",
      "Epoch 1 | Step 113800 | Avg Loss: 0.0139 | Grad Norm: 0.01126012\n",
      "Epoch 1 | Step 113900 | Avg Loss: 0.0146 | Grad Norm: 0.01094536\n",
      "Epoch 1 | Step 114000 | Avg Loss: 0.0141 | Grad Norm: 0.01011970\n",
      "Epoch 1 | Step 114100 | Avg Loss: 0.0139 | Grad Norm: 0.01112572\n",
      "Epoch 1 | Step 114200 | Avg Loss: 0.0144 | Grad Norm: 0.01072687\n",
      "Epoch 1 | Step 114300 | Avg Loss: 0.0142 | Grad Norm: 0.00995353\n",
      "Epoch 1 | Step 114400 | Avg Loss: 0.0143 | Grad Norm: 0.01288010\n",
      "Epoch 1 | Step 114500 | Avg Loss: 0.0142 | Grad Norm: 0.01091160\n",
      "Epoch 1 | Step 114600 | Avg Loss: 0.0141 | Grad Norm: 0.01009469\n",
      "Epoch 1 | Step 114700 | Avg Loss: 0.0141 | Grad Norm: 0.01182457\n",
      "Epoch 1 | Step 114800 | Avg Loss: 0.0138 | Grad Norm: 0.00961191\n",
      "Epoch 1 | Step 114900 | Avg Loss: 0.0143 | Grad Norm: 0.01074563\n",
      "Epoch 1 | Step 115000 | Avg Loss: 0.0144 | Grad Norm: 0.01068691\n",
      "Epoch 1 | Step 115100 | Avg Loss: 0.0144 | Grad Norm: 0.01122962\n",
      "Epoch 1 | Step 115200 | Avg Loss: 0.0142 | Grad Norm: 0.00987725\n",
      "Epoch 1 | Step 115300 | Avg Loss: 0.0143 | Grad Norm: 0.00997984\n",
      "Epoch 1 | Step 115400 | Avg Loss: 0.0139 | Grad Norm: 0.00996707\n",
      "Epoch 1 | Step 115500 | Avg Loss: 0.0138 | Grad Norm: 0.01110579\n",
      "Epoch 1 | Step 115600 | Avg Loss: 0.0138 | Grad Norm: 0.00923815\n",
      "Epoch 1 | Step 115700 | Avg Loss: 0.0138 | Grad Norm: 0.01114578\n",
      "Epoch 1 | Step 115800 | Avg Loss: 0.0140 | Grad Norm: 0.01174554\n",
      "Epoch 1 | Step 115900 | Avg Loss: 0.0138 | Grad Norm: 0.01014623\n",
      "Epoch 1 | Step 116000 | Avg Loss: 0.0139 | Grad Norm: 0.00964935\n",
      "Epoch 1 | Step 116100 | Avg Loss: 0.0140 | Grad Norm: 0.01116357\n",
      "Epoch 1 | Step 116200 | Avg Loss: 0.0142 | Grad Norm: 0.01039917\n",
      "Epoch 1 | Step 116300 | Avg Loss: 0.0146 | Grad Norm: 0.00957056\n",
      "Epoch 1 | Step 116400 | Avg Loss: 0.0146 | Grad Norm: 0.01092669\n",
      "Epoch 1 | Step 116500 | Avg Loss: 0.0142 | Grad Norm: 0.01052323\n",
      "Epoch 1 | Step 116600 | Avg Loss: 0.0140 | Grad Norm: 0.01010501\n",
      "Epoch 1 | Step 116700 | Avg Loss: 0.0142 | Grad Norm: 0.01027675\n",
      "Epoch 1 | Step 116800 | Avg Loss: 0.0145 | Grad Norm: 0.01005021\n",
      "Epoch 1 | Step 116900 | Avg Loss: 0.0146 | Grad Norm: 0.00982948\n",
      "Epoch 1 | Step 117000 | Avg Loss: 0.0141 | Grad Norm: 0.01014209\n",
      "Epoch 1 | Step 117100 | Avg Loss: 0.0143 | Grad Norm: 0.00979095\n",
      "Epoch 1 | Step 117200 | Avg Loss: 0.0140 | Grad Norm: 0.01106391\n",
      "Epoch 1 | Step 117300 | Avg Loss: 0.0141 | Grad Norm: 0.00995375\n",
      "Epoch 1 | Step 117400 | Avg Loss: 0.0140 | Grad Norm: 0.01045734\n",
      "Epoch 1 | Step 117500 | Avg Loss: 0.0140 | Grad Norm: 0.00956999\n",
      "Epoch 1 | Step 117600 | Avg Loss: 0.0142 | Grad Norm: 0.00919516\n",
      "Epoch 1 | Step 117700 | Avg Loss: 0.0146 | Grad Norm: 0.01152391\n",
      "Epoch 1 | Step 117800 | Avg Loss: 0.0144 | Grad Norm: 0.01136956\n",
      "Epoch 1 | Step 117900 | Avg Loss: 0.0142 | Grad Norm: 0.01048342\n",
      "Epoch 1 | Step 118000 | Avg Loss: 0.0145 | Grad Norm: 0.01074348\n",
      "Epoch 1 | Step 118100 | Avg Loss: 0.0147 | Grad Norm: 0.00989062\n",
      "Epoch 1 | Step 118200 | Avg Loss: 0.0145 | Grad Norm: 0.01021244\n",
      "Epoch 1 | Step 118300 | Avg Loss: 0.0142 | Grad Norm: 0.01035338\n",
      "Epoch 1 | Step 118400 | Avg Loss: 0.0145 | Grad Norm: 0.00893185\n",
      "Epoch 1 | Step 118500 | Avg Loss: 0.0143 | Grad Norm: 0.01019138\n",
      "Epoch 1 | Step 118600 | Avg Loss: 0.0142 | Grad Norm: 0.01113107\n",
      "Epoch 1 | Step 118700 | Avg Loss: 0.0143 | Grad Norm: 0.01055039\n",
      "Epoch 1 | Step 118800 | Avg Loss: 0.0138 | Grad Norm: 0.01026276\n",
      "Epoch 1 | Step 118900 | Avg Loss: 0.0140 | Grad Norm: 0.01213058\n",
      "Epoch 1 | Step 119000 | Avg Loss: 0.0139 | Grad Norm: 0.00993795\n",
      "Epoch 1 | Step 119100 | Avg Loss: 0.0138 | Grad Norm: 0.00939684\n",
      "Epoch 1 | Step 119200 | Avg Loss: 0.0139 | Grad Norm: 0.00913592\n",
      "Epoch 1 | Step 119300 | Avg Loss: 0.0139 | Grad Norm: 0.01083952\n",
      "Epoch 1 | Step 119400 | Avg Loss: 0.0135 | Grad Norm: 0.00965591\n",
      "Epoch 1 | Step 119500 | Avg Loss: 0.0135 | Grad Norm: 0.01072324\n",
      "Epoch 1 | Step 119600 | Avg Loss: 0.0136 | Grad Norm: 0.00971111\n",
      "Epoch 1 | Step 119700 | Avg Loss: 0.0138 | Grad Norm: 0.01106520\n",
      "Epoch 1 | Step 119800 | Avg Loss: 0.0140 | Grad Norm: 0.00983514\n",
      "Epoch 1 | Step 119900 | Avg Loss: 0.0144 | Grad Norm: 0.01084228\n",
      "Epoch 1 | Step 120000 | Avg Loss: 0.0144 | Grad Norm: 0.01036573\n",
      "Epoch 1 | Step 120100 | Avg Loss: 0.0142 | Grad Norm: 0.01038645\n",
      "Epoch 1 | Step 120200 | Avg Loss: 0.0140 | Grad Norm: 0.01222538\n",
      "Epoch 1 | Step 120300 | Avg Loss: 0.0138 | Grad Norm: 0.01100883\n",
      "Epoch 1 | Step 120400 | Avg Loss: 0.0140 | Grad Norm: 0.01017023\n",
      "Epoch 1 | Step 120500 | Avg Loss: 0.0141 | Grad Norm: 0.01069913\n",
      "Epoch 1 | Step 120600 | Avg Loss: 0.0142 | Grad Norm: 0.01081144\n",
      "Epoch 1 | Step 120700 | Avg Loss: 0.0140 | Grad Norm: 0.00910310\n",
      "Epoch 1 | Step 120800 | Avg Loss: 0.0136 | Grad Norm: 0.00897750\n",
      "Epoch 1 | Step 120900 | Avg Loss: 0.0136 | Grad Norm: 0.00855524\n",
      "Epoch 1 | Step 121000 | Avg Loss: 0.0138 | Grad Norm: 0.00946919\n",
      "Epoch 1 | Step 121100 | Avg Loss: 0.0135 | Grad Norm: 0.00972067\n",
      "Epoch 1 | Step 121200 | Avg Loss: 0.0138 | Grad Norm: 0.01020736\n",
      "Epoch 1 | Step 121300 | Avg Loss: 0.0140 | Grad Norm: 0.00925531\n",
      "Epoch 1 | Step 121400 | Avg Loss: 0.0140 | Grad Norm: 0.01017045\n",
      "Epoch 1 | Step 121500 | Avg Loss: 0.0141 | Grad Norm: 0.00868720\n",
      "Epoch 1 | Step 121600 | Avg Loss: 0.0142 | Grad Norm: 0.01089589\n",
      "Epoch 1 | Step 121700 | Avg Loss: 0.0143 | Grad Norm: 0.01044032\n",
      "Epoch 1 | Step 121800 | Avg Loss: 0.0142 | Grad Norm: 0.00984999\n",
      "Epoch 1 | Step 121900 | Avg Loss: 0.0140 | Grad Norm: 0.01017146\n",
      "Epoch 1 | Step 122000 | Avg Loss: 0.0140 | Grad Norm: 0.01057075\n",
      "Epoch 1 | Step 122100 | Avg Loss: 0.0142 | Grad Norm: 0.01107051\n",
      "Epoch 1 | Step 122200 | Avg Loss: 0.0138 | Grad Norm: 0.01173671\n",
      "Epoch 1 | Step 122300 | Avg Loss: 0.0140 | Grad Norm: 0.01019975\n",
      "Epoch 1 | Step 122400 | Avg Loss: 0.0139 | Grad Norm: 0.01175089\n",
      "Epoch 1 | Step 122500 | Avg Loss: 0.0139 | Grad Norm: 0.01059157\n",
      "Epoch 1 | Step 122600 | Avg Loss: 0.0138 | Grad Norm: 0.01165376\n",
      "Epoch 1 | Step 122700 | Avg Loss: 0.0137 | Grad Norm: 0.00994991\n",
      "Epoch 1 | Step 122800 | Avg Loss: 0.0139 | Grad Norm: 0.01153124\n",
      "Epoch 1 | Step 122900 | Avg Loss: 0.0136 | Grad Norm: 0.00823979\n",
      "Epoch 1 | Step 123000 | Avg Loss: 0.0136 | Grad Norm: 0.01011186\n",
      "Epoch 1 | Step 123100 | Avg Loss: 0.0141 | Grad Norm: 0.01007279\n",
      "Epoch 1 | Step 123200 | Avg Loss: 0.0139 | Grad Norm: 0.01004390\n",
      "Epoch 1 | Step 123300 | Avg Loss: 0.0139 | Grad Norm: 0.00958300\n",
      "Epoch 1 | Step 123400 | Avg Loss: 0.0135 | Grad Norm: 0.00996313\n",
      "Epoch 1 | Step 123500 | Avg Loss: 0.0135 | Grad Norm: 0.01086312\n",
      "Epoch 1 | Step 123600 | Avg Loss: 0.0139 | Grad Norm: 0.00976097\n",
      "Epoch 1 | Step 123700 | Avg Loss: 0.0138 | Grad Norm: 0.01018186\n",
      "Epoch 1 | Step 123800 | Avg Loss: 0.0141 | Grad Norm: 0.01095327\n",
      "Epoch 1 | Step 123900 | Avg Loss: 0.0139 | Grad Norm: 0.00935260\n",
      "Epoch 1 | Step 124000 | Avg Loss: 0.0144 | Grad Norm: 0.01098724\n",
      "Epoch 1 | Step 124100 | Avg Loss: 0.0143 | Grad Norm: 0.01033908\n",
      "Epoch 1 | Step 124200 | Avg Loss: 0.0143 | Grad Norm: 0.01049456\n",
      "Epoch 1 | Step 124300 | Avg Loss: 0.0142 | Grad Norm: 0.01075906\n",
      "Epoch 1 | Step 124400 | Avg Loss: 0.0143 | Grad Norm: 0.01120626\n",
      "Epoch 1 | Step 124500 | Avg Loss: 0.0142 | Grad Norm: 0.00884546\n",
      "Epoch 1 | Step 124600 | Avg Loss: 0.0140 | Grad Norm: 0.01009510\n",
      "Epoch 1 | Step 124700 | Avg Loss: 0.0139 | Grad Norm: 0.00974434\n",
      "Epoch 1 | Step 124800 | Avg Loss: 0.0140 | Grad Norm: 0.00975609\n",
      "Epoch 1 | Step 124900 | Avg Loss: 0.0141 | Grad Norm: 0.01043407\n",
      "Epoch 1 | Step 125000 | Avg Loss: 0.0142 | Grad Norm: 0.00939100\n",
      "Epoch 1 | Step 125100 | Avg Loss: 0.0142 | Grad Norm: 0.01098036\n",
      "Epoch 1 | Step 125200 | Avg Loss: 0.0143 | Grad Norm: 0.01019261\n",
      "Epoch 1 | Step 125300 | Avg Loss: 0.0141 | Grad Norm: 0.00953668\n",
      "Epoch 1 | Step 125400 | Avg Loss: 0.0143 | Grad Norm: 0.00981320\n",
      "Epoch 1 | Step 125500 | Avg Loss: 0.0142 | Grad Norm: 0.01060145\n",
      "Epoch 1 | Step 125600 | Avg Loss: 0.0142 | Grad Norm: 0.01042243\n",
      "Epoch 1 | Step 125700 | Avg Loss: 0.0140 | Grad Norm: 0.00966834\n",
      "Epoch 1 | Step 125800 | Avg Loss: 0.0141 | Grad Norm: 0.01137110\n",
      "Epoch 1 | Step 125900 | Avg Loss: 0.0141 | Grad Norm: 0.01024753\n",
      "Epoch 1 | Step 126000 | Avg Loss: 0.0140 | Grad Norm: 0.00995048\n",
      "Epoch 1 | Step 126100 | Avg Loss: 0.0145 | Grad Norm: 0.01191607\n",
      "Epoch 1 | Step 126200 | Avg Loss: 0.0146 | Grad Norm: 0.01182438\n",
      "Epoch 1 | Step 126300 | Avg Loss: 0.0147 | Grad Norm: 0.01285889\n",
      "Epoch 1 | Step 126400 | Avg Loss: 0.0143 | Grad Norm: 0.01090295\n",
      "Epoch 1 | Step 126500 | Avg Loss: 0.0146 | Grad Norm: 0.00994362\n",
      "Epoch 1 | Step 126600 | Avg Loss: 0.0143 | Grad Norm: 0.00991585\n",
      "Epoch 1 | Step 126700 | Avg Loss: 0.0145 | Grad Norm: 0.01106106\n",
      "Epoch 1 | Step 126800 | Avg Loss: 0.0138 | Grad Norm: 0.01018106\n",
      "Epoch 1 | Step 126900 | Avg Loss: 0.0138 | Grad Norm: 0.00898604\n",
      "Epoch 1 | Step 127000 | Avg Loss: 0.0143 | Grad Norm: 0.01074969\n",
      "Epoch 1 | Step 127100 | Avg Loss: 0.0143 | Grad Norm: 0.01087166\n",
      "Epoch 1 | Step 127200 | Avg Loss: 0.0143 | Grad Norm: 0.01187779\n",
      "Epoch 1 | Step 127300 | Avg Loss: 0.0140 | Grad Norm: 0.01230083\n",
      "Epoch 1 | Step 127400 | Avg Loss: 0.0137 | Grad Norm: 0.00993455\n",
      "Epoch 1 | Step 127500 | Avg Loss: 0.0136 | Grad Norm: 0.01006808\n",
      "Epoch 1 | Step 127600 | Avg Loss: 0.0140 | Grad Norm: 0.00968452\n",
      "Epoch 1 | Step 127700 | Avg Loss: 0.0141 | Grad Norm: 0.00999968\n",
      "Epoch 1 | Step 127800 | Avg Loss: 0.0142 | Grad Norm: 0.01113763\n",
      "Epoch 1 | Step 127900 | Avg Loss: 0.0145 | Grad Norm: 0.00847542\n",
      "Epoch 1 | Step 128000 | Avg Loss: 0.0143 | Grad Norm: 0.00944164\n",
      "Epoch 1 | Step 128100 | Avg Loss: 0.0136 | Grad Norm: 0.00961041\n",
      "Epoch 1 | Step 128200 | Avg Loss: 0.0140 | Grad Norm: 0.01041336\n",
      "Epoch 1 | Step 128300 | Avg Loss: 0.0139 | Grad Norm: 0.00971446\n",
      "Epoch 1 | Step 128400 | Avg Loss: 0.0138 | Grad Norm: 0.01094132\n",
      "Epoch 1 | Step 128500 | Avg Loss: 0.0138 | Grad Norm: 0.01011114\n",
      "Epoch 1 | Step 128600 | Avg Loss: 0.0142 | Grad Norm: 0.01149217\n",
      "Epoch 1 | Step 128700 | Avg Loss: 0.0140 | Grad Norm: 0.00890439\n",
      "Epoch 1 | Step 128800 | Avg Loss: 0.0143 | Grad Norm: 0.01080956\n",
      "Epoch 1 | Step 128900 | Avg Loss: 0.0142 | Grad Norm: 0.00910512\n",
      "Epoch 1 | Step 129000 | Avg Loss: 0.0145 | Grad Norm: 0.00963829\n",
      "Epoch 1 | Step 129100 | Avg Loss: 0.0143 | Grad Norm: 0.01053824\n",
      "Epoch 1 | Step 129200 | Avg Loss: 0.0145 | Grad Norm: 0.01135757\n",
      "Epoch 1 | Step 129300 | Avg Loss: 0.0142 | Grad Norm: 0.00996416\n",
      "Epoch 1 | Step 129400 | Avg Loss: 0.0146 | Grad Norm: 0.01238157\n",
      "Epoch 1 | Step 129500 | Avg Loss: 0.0145 | Grad Norm: 0.01032923\n",
      "Epoch 1 | Step 129600 | Avg Loss: 0.0145 | Grad Norm: 0.01050513\n",
      "Epoch 1 | Step 129700 | Avg Loss: 0.0144 | Grad Norm: 0.01047698\n",
      "Epoch 1 | Step 129800 | Avg Loss: 0.0147 | Grad Norm: 0.01128796\n",
      "Epoch 1 | Step 129900 | Avg Loss: 0.0147 | Grad Norm: 0.01079597\n",
      "Epoch 1 | Step 130000 | Avg Loss: 0.0146 | Grad Norm: 0.01129170\n",
      "Epoch 1 | Step 130100 | Avg Loss: 0.0140 | Grad Norm: 0.00983546\n",
      "Epoch 1 | Step 130200 | Avg Loss: 0.0142 | Grad Norm: 0.00983693\n",
      "Epoch 1 | Step 130300 | Avg Loss: 0.0143 | Grad Norm: 0.00979623\n",
      "Epoch 1 | Step 130400 | Avg Loss: 0.0138 | Grad Norm: 0.00912739\n",
      "Epoch 1 | Step 130500 | Avg Loss: 0.0142 | Grad Norm: 0.00849788\n",
      "Epoch 1 | Step 130600 | Avg Loss: 0.0147 | Grad Norm: 0.01042562\n",
      "Epoch 1 | Step 130700 | Avg Loss: 0.0148 | Grad Norm: 0.01098651\n",
      "Epoch 1 | Step 130800 | Avg Loss: 0.0147 | Grad Norm: 0.01043650\n",
      "Epoch 1 | Step 130900 | Avg Loss: 0.0145 | Grad Norm: 0.00971682\n",
      "Epoch 1 | Step 131000 | Avg Loss: 0.0144 | Grad Norm: 0.00968163\n",
      "Epoch 1 | Step 131100 | Avg Loss: 0.0141 | Grad Norm: 0.01043135\n",
      "Epoch 1 | Step 131200 | Avg Loss: 0.0144 | Grad Norm: 0.01038865\n",
      "Epoch 1 | Step 131300 | Avg Loss: 0.0146 | Grad Norm: 0.01093006\n",
      "Epoch 1 | Step 131400 | Avg Loss: 0.0146 | Grad Norm: 0.01229994\n",
      "Epoch 1 | Step 131500 | Avg Loss: 0.0144 | Grad Norm: 0.00915500\n",
      "Epoch 1 | Step 131600 | Avg Loss: 0.0143 | Grad Norm: 0.00953055\n",
      "Epoch 1 | Step 131700 | Avg Loss: 0.0145 | Grad Norm: 0.01104170\n",
      "Epoch 1 | Step 131800 | Avg Loss: 0.0145 | Grad Norm: 0.00975154\n",
      "Epoch 1 | Step 131900 | Avg Loss: 0.0139 | Grad Norm: 0.01026367\n",
      "Epoch 1 | Step 132000 | Avg Loss: 0.0137 | Grad Norm: 0.00945634\n",
      "Epoch 1 | Step 132100 | Avg Loss: 0.0140 | Grad Norm: 0.01111676\n",
      "Epoch 1 | Step 132200 | Avg Loss: 0.0146 | Grad Norm: 0.01098609\n",
      "Epoch 1 | Step 132300 | Avg Loss: 0.0145 | Grad Norm: 0.01042344\n",
      "Epoch 1 | Step 132400 | Avg Loss: 0.0148 | Grad Norm: 0.01115699\n",
      "Epoch 1 | Step 132500 | Avg Loss: 0.0148 | Grad Norm: 0.01081044\n",
      "Epoch 1 | Step 132600 | Avg Loss: 0.0146 | Grad Norm: 0.00993024\n",
      "Epoch 1 | Step 132700 | Avg Loss: 0.0148 | Grad Norm: 0.00960254\n",
      "Epoch 1 | Step 132800 | Avg Loss: 0.0147 | Grad Norm: 0.00968748\n",
      "Epoch 1 | Step 132900 | Avg Loss: 0.0146 | Grad Norm: 0.00880947\n",
      "Epoch 1 | Step 133000 | Avg Loss: 0.0145 | Grad Norm: 0.01189156\n",
      "Epoch 1 | Step 133100 | Avg Loss: 0.0143 | Grad Norm: 0.01050050\n",
      "Epoch 1 | Step 133200 | Avg Loss: 0.0143 | Grad Norm: 0.01024030\n",
      "Epoch 1 | Step 133300 | Avg Loss: 0.0145 | Grad Norm: 0.00967228\n",
      "Epoch 1 | Step 133400 | Avg Loss: 0.0144 | Grad Norm: 0.01040360\n",
      "Epoch 1 | Step 133500 | Avg Loss: 0.0139 | Grad Norm: 0.00936310\n",
      "Epoch 1 | Step 133600 | Avg Loss: 0.0144 | Grad Norm: 0.01006965\n",
      "Epoch 1 | Step 133700 | Avg Loss: 0.0140 | Grad Norm: 0.01037194\n",
      "Epoch 1 | Step 133800 | Avg Loss: 0.0138 | Grad Norm: 0.01016852\n",
      "Epoch 1 | Step 133900 | Avg Loss: 0.0138 | Grad Norm: 0.00945156\n",
      "Epoch 1 | Step 134000 | Avg Loss: 0.0136 | Grad Norm: 0.00887543\n",
      "Epoch 1 | Step 134100 | Avg Loss: 0.0137 | Grad Norm: 0.00991406\n",
      "Epoch 1 | Step 134200 | Avg Loss: 0.0138 | Grad Norm: 0.01011510\n",
      "Epoch 1 | Step 134300 | Avg Loss: 0.0140 | Grad Norm: 0.00929669\n",
      "Epoch 1 | Step 134400 | Avg Loss: 0.0141 | Grad Norm: 0.01128612\n",
      "Epoch 1 | Step 134500 | Avg Loss: 0.0143 | Grad Norm: 0.01235953\n",
      "Epoch 1 | Step 134600 | Avg Loss: 0.0141 | Grad Norm: 0.01008123\n",
      "Epoch 1 | Step 134700 | Avg Loss: 0.0140 | Grad Norm: 0.01210121\n",
      "Epoch 1 | Step 134800 | Avg Loss: 0.0139 | Grad Norm: 0.01016632\n",
      "Epoch 1 | Step 134900 | Avg Loss: 0.0142 | Grad Norm: 0.00974484\n",
      "Epoch 1 | Step 135000 | Avg Loss: 0.0141 | Grad Norm: 0.00920373\n",
      "Epoch 1 | Step 135100 | Avg Loss: 0.0144 | Grad Norm: 0.00972887\n",
      "Epoch 1 | Step 135200 | Avg Loss: 0.0145 | Grad Norm: 0.00955992\n",
      "Epoch 1 | Step 135300 | Avg Loss: 0.0144 | Grad Norm: 0.00917315\n",
      "Epoch 1 | Step 135400 | Avg Loss: 0.0148 | Grad Norm: 0.01028424\n",
      "Epoch 1 | Step 135500 | Avg Loss: 0.0147 | Grad Norm: 0.00934117\n",
      "Epoch 1 | Step 135600 | Avg Loss: 0.0144 | Grad Norm: 0.01008394\n",
      "Epoch 1 | Step 135700 | Avg Loss: 0.0144 | Grad Norm: 0.01044770\n",
      "Epoch 1 | Step 135800 | Avg Loss: 0.0145 | Grad Norm: 0.01219618\n",
      "Epoch 1 | Step 135900 | Avg Loss: 0.0145 | Grad Norm: 0.01013156\n",
      "Epoch 1 | Step 136000 | Avg Loss: 0.0141 | Grad Norm: 0.00985021\n",
      "Epoch 1 | Step 136100 | Avg Loss: 0.0143 | Grad Norm: 0.00947557\n",
      "Epoch 1 | Step 136200 | Avg Loss: 0.0142 | Grad Norm: 0.01080401\n",
      "Epoch 1 | Step 136300 | Avg Loss: 0.0139 | Grad Norm: 0.01005014\n",
      "Epoch 1 | Step 136400 | Avg Loss: 0.0137 | Grad Norm: 0.00915797\n",
      "Epoch 1 | Step 136500 | Avg Loss: 0.0136 | Grad Norm: 0.00878436\n",
      "Epoch 1 | Step 136600 | Avg Loss: 0.0135 | Grad Norm: 0.01000083\n",
      "Epoch 1 | Step 136700 | Avg Loss: 0.0136 | Grad Norm: 0.01068160\n",
      "Epoch 1 | Step 136800 | Avg Loss: 0.0138 | Grad Norm: 0.01308878\n",
      "Epoch 1 | Step 136900 | Avg Loss: 0.0138 | Grad Norm: 0.00939085\n",
      "Epoch 1 | Step 137000 | Avg Loss: 0.0141 | Grad Norm: 0.01149235\n",
      "Epoch 1 | Step 137100 | Avg Loss: 0.0138 | Grad Norm: 0.00900436\n",
      "Epoch 1 | Step 137200 | Avg Loss: 0.0140 | Grad Norm: 0.00971348\n",
      "Epoch 1 | Step 137300 | Avg Loss: 0.0141 | Grad Norm: 0.00906408\n",
      "Epoch 1 | Step 137400 | Avg Loss: 0.0142 | Grad Norm: 0.01228512\n",
      "Epoch 1 | Step 137500 | Avg Loss: 0.0140 | Grad Norm: 0.01088501\n",
      "Epoch 1 | Step 137600 | Avg Loss: 0.0142 | Grad Norm: 0.01065397\n",
      "Epoch 1 | Step 137700 | Avg Loss: 0.0145 | Grad Norm: 0.01082062\n",
      "Epoch 1 | Step 137800 | Avg Loss: 0.0143 | Grad Norm: 0.00928747\n",
      "Epoch 1 | Step 137900 | Avg Loss: 0.0142 | Grad Norm: 0.00836110\n",
      "Epoch 1 | Step 138000 | Avg Loss: 0.0143 | Grad Norm: 0.01160704\n",
      "Epoch 1 | Step 138100 | Avg Loss: 0.0145 | Grad Norm: 0.01024257\n",
      "Epoch 1 | Step 138200 | Avg Loss: 0.0144 | Grad Norm: 0.00984160\n",
      "Epoch 1 | Step 138300 | Avg Loss: 0.0146 | Grad Norm: 0.01118419\n",
      "Epoch 1 | Step 138400 | Avg Loss: 0.0143 | Grad Norm: 0.01075346\n",
      "Epoch 1 | Step 138500 | Avg Loss: 0.0139 | Grad Norm: 0.01000760\n",
      "Epoch 1 | Step 138600 | Avg Loss: 0.0143 | Grad Norm: 0.00915479\n",
      "Epoch 1 | Step 138700 | Avg Loss: 0.0144 | Grad Norm: 0.01173183\n",
      "Epoch 1 | Step 138800 | Avg Loss: 0.0139 | Grad Norm: 0.00904080\n",
      "Epoch 1 | Step 138900 | Avg Loss: 0.0138 | Grad Norm: 0.01198877\n",
      "Epoch 1 | Step 139000 | Avg Loss: 0.0137 | Grad Norm: 0.00946103\n",
      "Epoch 1 | Step 139100 | Avg Loss: 0.0141 | Grad Norm: 0.01040410\n",
      "Epoch 1 | Step 139200 | Avg Loss: 0.0146 | Grad Norm: 0.00908799\n",
      "Epoch 1 | Step 139300 | Avg Loss: 0.0145 | Grad Norm: 0.00872521\n",
      "Epoch 1 | Step 139400 | Avg Loss: 0.0146 | Grad Norm: 0.00924804\n",
      "Epoch 1 | Step 139500 | Avg Loss: 0.0143 | Grad Norm: 0.01188245\n",
      "Epoch 1 | Step 139600 | Avg Loss: 0.0146 | Grad Norm: 0.00926046\n",
      "Epoch 1 | Step 139700 | Avg Loss: 0.0146 | Grad Norm: 0.01082948\n",
      "Epoch 1 | Step 139800 | Avg Loss: 0.0148 | Grad Norm: 0.01092588\n",
      "Epoch 1 | Step 139900 | Avg Loss: 0.0145 | Grad Norm: 0.00993175\n",
      "Epoch 1 | Step 140000 | Avg Loss: 0.0140 | Grad Norm: 0.00799172\n",
      "Epoch 1 | Step 140100 | Avg Loss: 0.0138 | Grad Norm: 0.01038144\n",
      "Epoch 1 | Step 140200 | Avg Loss: 0.0136 | Grad Norm: 0.00822481\n",
      "Epoch 1 | Step 140300 | Avg Loss: 0.0138 | Grad Norm: 0.00994286\n",
      "Epoch 1 | Step 140400 | Avg Loss: 0.0139 | Grad Norm: 0.01066745\n",
      "Epoch 1 | Step 140500 | Avg Loss: 0.0145 | Grad Norm: 0.01101525\n",
      "Epoch 1 | Step 140600 | Avg Loss: 0.0147 | Grad Norm: 0.01160120\n",
      "Epoch 1 | Step 140700 | Avg Loss: 0.0145 | Grad Norm: 0.01015606\n",
      "Epoch 1 | Step 140800 | Avg Loss: 0.0143 | Grad Norm: 0.01082067\n",
      "Epoch 1 | Step 140900 | Avg Loss: 0.0144 | Grad Norm: 0.00960314\n",
      "Epoch 1 | Step 141000 | Avg Loss: 0.0145 | Grad Norm: 0.00933143\n",
      "Epoch 1 | Step 141100 | Avg Loss: 0.0144 | Grad Norm: 0.01026227\n",
      "Epoch 1 | Step 141200 | Avg Loss: 0.0143 | Grad Norm: 0.01124864\n",
      "Epoch 1 | Step 141300 | Avg Loss: 0.0145 | Grad Norm: 0.01116155\n",
      "Epoch 1 | Step 141400 | Avg Loss: 0.0143 | Grad Norm: 0.01049389\n",
      "Epoch 1 | Step 141500 | Avg Loss: 0.0140 | Grad Norm: 0.00961460\n",
      "Epoch 1 | Step 141600 | Avg Loss: 0.0139 | Grad Norm: 0.00913169\n",
      "Epoch 1 | Step 141700 | Avg Loss: 0.0140 | Grad Norm: 0.00925094\n",
      "Epoch 1 | Step 141800 | Avg Loss: 0.0139 | Grad Norm: 0.01005976\n",
      "Epoch 1 | Step 141900 | Avg Loss: 0.0134 | Grad Norm: 0.01112836\n",
      "Epoch 1 | Step 142000 | Avg Loss: 0.0138 | Grad Norm: 0.00973928\n",
      "Epoch 1 | Step 142100 | Avg Loss: 0.0141 | Grad Norm: 0.00980918\n",
      "Epoch 1 | Step 142200 | Avg Loss: 0.0144 | Grad Norm: 0.01051743\n",
      "Epoch 1 | Step 142300 | Avg Loss: 0.0144 | Grad Norm: 0.01121713\n",
      "Epoch 1 | Step 142400 | Avg Loss: 0.0144 | Grad Norm: 0.01027120\n",
      "Epoch 1 | Step 142500 | Avg Loss: 0.0140 | Grad Norm: 0.00907700\n",
      "Epoch 1 | Step 142600 | Avg Loss: 0.0135 | Grad Norm: 0.01317336\n",
      "Epoch 1 | Step 142700 | Avg Loss: 0.0136 | Grad Norm: 0.00942762\n",
      "Epoch 1 | Step 142800 | Avg Loss: 0.0136 | Grad Norm: 0.00969959\n",
      "Epoch 1 | Step 142900 | Avg Loss: 0.0134 | Grad Norm: 0.01074752\n",
      "Epoch 1 | Step 143000 | Avg Loss: 0.0139 | Grad Norm: 0.00984896\n",
      "Epoch 1 | Step 143100 | Avg Loss: 0.0141 | Grad Norm: 0.00993821\n",
      "Epoch 1 | Step 143200 | Avg Loss: 0.0143 | Grad Norm: 0.00880457\n",
      "Epoch 1 | Step 143300 | Avg Loss: 0.0141 | Grad Norm: 0.00969176\n",
      "Epoch 1 | Step 143400 | Avg Loss: 0.0142 | Grad Norm: 0.01068439\n",
      "Epoch 1 | Step 143500 | Avg Loss: 0.0142 | Grad Norm: 0.01070530\n",
      "Epoch 1 | Step 143600 | Avg Loss: 0.0140 | Grad Norm: 0.00978919\n",
      "Epoch 1 | Step 143700 | Avg Loss: 0.0140 | Grad Norm: 0.01015516\n",
      "Epoch 1 | Step 143800 | Avg Loss: 0.0141 | Grad Norm: 0.01035811\n",
      "Epoch 1 | Step 143900 | Avg Loss: 0.0142 | Grad Norm: 0.01150700\n",
      "Epoch 1 | Step 144000 | Avg Loss: 0.0139 | Grad Norm: 0.00897183\n",
      "Epoch 1 | Step 144100 | Avg Loss: 0.0141 | Grad Norm: 0.00977051\n",
      "Epoch 1 | Step 144200 | Avg Loss: 0.0138 | Grad Norm: 0.01089621\n",
      "Epoch 1 | Step 144300 | Avg Loss: 0.0144 | Grad Norm: 0.01016939\n",
      "Epoch 1 | Step 144400 | Avg Loss: 0.0146 | Grad Norm: 0.01041899\n",
      "Epoch 1 | Step 144500 | Avg Loss: 0.0146 | Grad Norm: 0.01079275\n",
      "Epoch 1 | Step 144600 | Avg Loss: 0.0144 | Grad Norm: 0.01059847\n",
      "Epoch 1 | Step 144700 | Avg Loss: 0.0147 | Grad Norm: 0.01106041\n",
      "Epoch 1 | Step 144800 | Avg Loss: 0.0147 | Grad Norm: 0.01094205\n",
      "Epoch 1 | Step 144900 | Avg Loss: 0.0143 | Grad Norm: 0.01054604\n",
      "Epoch 1 | Step 145000 | Avg Loss: 0.0140 | Grad Norm: 0.00909204\n",
      "Epoch 1 | Step 145100 | Avg Loss: 0.0141 | Grad Norm: 0.01217607\n",
      "Epoch 1 | Step 145200 | Avg Loss: 0.0143 | Grad Norm: 0.01004756\n",
      "Epoch 1 | Step 145300 | Avg Loss: 0.0145 | Grad Norm: 0.00986509\n",
      "Epoch 1 | Step 145400 | Avg Loss: 0.0145 | Grad Norm: 0.00935969\n",
      "Epoch 1 | Step 145500 | Avg Loss: 0.0146 | Grad Norm: 0.01052894\n",
      "Epoch 1 | Step 145600 | Avg Loss: 0.0146 | Grad Norm: 0.01012729\n",
      "Epoch 1 | Step 145700 | Avg Loss: 0.0143 | Grad Norm: 0.00983689\n",
      "Epoch 1 | Step 145800 | Avg Loss: 0.0140 | Grad Norm: 0.00978304\n",
      "Epoch 1 | Step 145900 | Avg Loss: 0.0141 | Grad Norm: 0.00888848\n",
      "Epoch 1 | Step 146000 | Avg Loss: 0.0142 | Grad Norm: 0.00976934\n",
      "Epoch 1 | Step 146100 | Avg Loss: 0.0141 | Grad Norm: 0.00976527\n",
      "Epoch 1 | Step 146200 | Avg Loss: 0.0140 | Grad Norm: 0.01015156\n",
      "Epoch 1 | Step 146300 | Avg Loss: 0.0140 | Grad Norm: 0.01120732\n",
      "Epoch 1 | Step 146400 | Avg Loss: 0.0144 | Grad Norm: 0.01036677\n",
      "Epoch 1 | Step 146500 | Avg Loss: 0.0142 | Grad Norm: 0.00999185\n",
      "Epoch 1 | Step 146600 | Avg Loss: 0.0137 | Grad Norm: 0.01148343\n",
      "Epoch 1 | Step 146700 | Avg Loss: 0.0138 | Grad Norm: 0.01115635\n",
      "Epoch 1 | Step 146800 | Avg Loss: 0.0141 | Grad Norm: 0.01090812\n",
      "Epoch 1 | Step 146900 | Avg Loss: 0.0141 | Grad Norm: 0.01175596\n",
      "Epoch 1 | Step 147000 | Avg Loss: 0.0147 | Grad Norm: 0.01037675\n",
      "Epoch 1 | Step 147100 | Avg Loss: 0.0149 | Grad Norm: 0.00929236\n",
      "Epoch 1 | Step 147200 | Avg Loss: 0.0150 | Grad Norm: 0.01083931\n",
      "Epoch 1 | Step 147300 | Avg Loss: 0.0146 | Grad Norm: 0.00922859\n",
      "Epoch 1 | Step 147400 | Avg Loss: 0.0146 | Grad Norm: 0.01059111\n",
      "Epoch 1 | Step 147500 | Avg Loss: 0.0143 | Grad Norm: 0.01148826\n",
      "Epoch 1 | Step 147600 | Avg Loss: 0.0140 | Grad Norm: 0.00995910\n",
      "Epoch 1 | Step 147700 | Avg Loss: 0.0141 | Grad Norm: 0.00886531\n",
      "Epoch 1 | Step 147800 | Avg Loss: 0.0141 | Grad Norm: 0.00978077\n",
      "Epoch 1 | Step 147900 | Avg Loss: 0.0142 | Grad Norm: 0.01030898\n",
      "Epoch 1 | Step 148000 | Avg Loss: 0.0144 | Grad Norm: 0.00947152\n",
      "Epoch 1 | Step 148100 | Avg Loss: 0.0142 | Grad Norm: 0.00887218\n",
      "Epoch 1 | Step 148200 | Avg Loss: 0.0142 | Grad Norm: 0.00810337\n",
      "Epoch 1 | Step 148300 | Avg Loss: 0.0140 | Grad Norm: 0.01170461\n",
      "Epoch 1 | Step 148400 | Avg Loss: 0.0143 | Grad Norm: 0.01001266\n",
      "Epoch 1 | Step 148500 | Avg Loss: 0.0143 | Grad Norm: 0.01115374\n",
      "Epoch 1 | Step 148600 | Avg Loss: 0.0146 | Grad Norm: 0.00992241\n",
      "Epoch 1 | Step 148700 | Avg Loss: 0.0146 | Grad Norm: 0.01032159\n",
      "Epoch 1 | Step 148800 | Avg Loss: 0.0146 | Grad Norm: 0.01156778\n",
      "Epoch 1 | Step 148900 | Avg Loss: 0.0143 | Grad Norm: 0.00898033\n",
      "Epoch 1 | Step 149000 | Avg Loss: 0.0141 | Grad Norm: 0.00998421\n",
      "Epoch 1 | Step 149100 | Avg Loss: 0.0142 | Grad Norm: 0.01058784\n",
      "Epoch 1 | Step 149200 | Avg Loss: 0.0143 | Grad Norm: 0.00974313\n",
      "Epoch 1 | Step 149300 | Avg Loss: 0.0141 | Grad Norm: 0.00830238\n",
      "Epoch 1 | Step 149400 | Avg Loss: 0.0140 | Grad Norm: 0.00940703\n",
      "Epoch 1 | Step 149500 | Avg Loss: 0.0140 | Grad Norm: 0.01005151\n",
      "Epoch 1 | Step 149600 | Avg Loss: 0.0139 | Grad Norm: 0.00998697\n",
      "Epoch 1 | Step 149700 | Avg Loss: 0.0139 | Grad Norm: 0.00997517\n",
      "Epoch 1 | Step 149800 | Avg Loss: 0.0141 | Grad Norm: 0.00897075\n",
      "Epoch 1 | Step 149900 | Avg Loss: 0.0134 | Grad Norm: 0.00916514\n",
      "Epoch 1 | Step 150000 | Avg Loss: 0.0138 | Grad Norm: 0.00880138\n",
      "Epoch 1 | Step 150100 | Avg Loss: 0.0139 | Grad Norm: 0.01075650\n",
      "Epoch 1 | Step 150200 | Avg Loss: 0.0139 | Grad Norm: 0.01087003\n",
      "Epoch 1 | Step 150300 | Avg Loss: 0.0143 | Grad Norm: 0.00937651\n",
      "Epoch 1 | Step 150400 | Avg Loss: 0.0142 | Grad Norm: 0.00926172\n",
      "Epoch 1 | Step 150500 | Avg Loss: 0.0144 | Grad Norm: 0.00826158\n",
      "Epoch 1 | Step 150600 | Avg Loss: 0.0144 | Grad Norm: 0.01007826\n",
      "Epoch 1 | Step 150700 | Avg Loss: 0.0140 | Grad Norm: 0.01020894\n",
      "Epoch 1 | Step 150800 | Avg Loss: 0.0140 | Grad Norm: 0.01059173\n",
      "Epoch 1 | Step 150900 | Avg Loss: 0.0138 | Grad Norm: 0.01080642\n",
      "Epoch 1 | Step 151000 | Avg Loss: 0.0137 | Grad Norm: 0.01036981\n",
      "Epoch 1 | Step 151100 | Avg Loss: 0.0137 | Grad Norm: 0.00811957\n",
      "Epoch 1 | Step 151200 | Avg Loss: 0.0137 | Grad Norm: 0.00993115\n",
      "Epoch 1 | Step 151300 | Avg Loss: 0.0137 | Grad Norm: 0.01060252\n",
      "Epoch 1 | Step 151400 | Avg Loss: 0.0137 | Grad Norm: 0.01000363\n",
      "Epoch 1 | Step 151500 | Avg Loss: 0.0137 | Grad Norm: 0.01240309\n",
      "Epoch 1 | Step 151600 | Avg Loss: 0.0138 | Grad Norm: 0.01022541\n",
      "Epoch 1 | Step 151700 | Avg Loss: 0.0138 | Grad Norm: 0.01046146\n",
      "Epoch 1 | Step 151800 | Avg Loss: 0.0141 | Grad Norm: 0.00977743\n",
      "Epoch 1 | Step 151900 | Avg Loss: 0.0142 | Grad Norm: 0.01065557\n",
      "Epoch 1 | Step 152000 | Avg Loss: 0.0140 | Grad Norm: 0.01095644\n",
      "Epoch 1 | Step 152100 | Avg Loss: 0.0138 | Grad Norm: 0.00983264\n",
      "Epoch 1 | Step 152200 | Avg Loss: 0.0138 | Grad Norm: 0.00998143\n",
      "Epoch 1 | Step 152300 | Avg Loss: 0.0140 | Grad Norm: 0.01122957\n",
      "Epoch 1 | Step 152400 | Avg Loss: 0.0144 | Grad Norm: 0.00969933\n",
      "Epoch 1 | Step 152500 | Avg Loss: 0.0142 | Grad Norm: 0.00887906\n",
      "Epoch 1 | Step 152600 | Avg Loss: 0.0142 | Grad Norm: 0.00919846\n",
      "Epoch 1 | Step 152700 | Avg Loss: 0.0141 | Grad Norm: 0.00882969\n",
      "Epoch 1 | Step 152800 | Avg Loss: 0.0142 | Grad Norm: 0.01284910\n",
      "Epoch 1 | Step 152900 | Avg Loss: 0.0141 | Grad Norm: 0.01039355\n",
      "Epoch 1 | Step 153000 | Avg Loss: 0.0138 | Grad Norm: 0.00948104\n",
      "Epoch 1 | Step 153100 | Avg Loss: 0.0137 | Grad Norm: 0.00937573\n",
      "Epoch 1 | Step 153200 | Avg Loss: 0.0135 | Grad Norm: 0.00931127\n",
      "Epoch 1 | Step 153300 | Avg Loss: 0.0136 | Grad Norm: 0.01098756\n",
      "Epoch 1 | Step 153400 | Avg Loss: 0.0143 | Grad Norm: 0.00911752\n",
      "Epoch 1 | Step 153500 | Avg Loss: 0.0142 | Grad Norm: 0.00873925\n",
      "Epoch 1 | Step 153600 | Avg Loss: 0.0140 | Grad Norm: 0.01054742\n",
      "Epoch 1 | Step 153700 | Avg Loss: 0.0139 | Grad Norm: 0.01084979\n",
      "Epoch 1 | Step 153800 | Avg Loss: 0.0140 | Grad Norm: 0.01055956\n",
      "Epoch 1 | Step 153900 | Avg Loss: 0.0145 | Grad Norm: 0.01010324\n",
      "Epoch 1 | Step 154000 | Avg Loss: 0.0145 | Grad Norm: 0.00957299\n",
      "Epoch 1 | Step 154100 | Avg Loss: 0.0147 | Grad Norm: 0.00986783\n",
      "Epoch 1 | Step 154200 | Avg Loss: 0.0143 | Grad Norm: 0.01013859\n",
      "Epoch 1 | Step 154300 | Avg Loss: 0.0141 | Grad Norm: 0.00998043\n",
      "Epoch 1 | Step 154400 | Avg Loss: 0.0138 | Grad Norm: 0.00959334\n",
      "Epoch 1 | Step 154500 | Avg Loss: 0.0136 | Grad Norm: 0.00999558\n",
      "Epoch 1 | Step 154600 | Avg Loss: 0.0138 | Grad Norm: 0.00926847\n",
      "Epoch 1 | Step 154700 | Avg Loss: 0.0140 | Grad Norm: 0.00944875\n",
      "Epoch 1 | Step 154800 | Avg Loss: 0.0144 | Grad Norm: 0.00922511\n",
      "Epoch 1 | Step 154900 | Avg Loss: 0.0147 | Grad Norm: 0.00950323\n",
      "Epoch 1 | Step 155000 | Avg Loss: 0.0145 | Grad Norm: 0.01092509\n",
      "Epoch 1 | Step 155100 | Avg Loss: 0.0144 | Grad Norm: 0.01352669\n",
      "Epoch 1 | Step 155200 | Avg Loss: 0.0142 | Grad Norm: 0.00901335\n",
      "Epoch 1 | Step 155300 | Avg Loss: 0.0142 | Grad Norm: 0.01022200\n",
      "Epoch 1 | Step 155400 | Avg Loss: 0.0140 | Grad Norm: 0.01016019\n",
      "Epoch 1 | Step 155500 | Avg Loss: 0.0139 | Grad Norm: 0.00824730\n",
      "Epoch 1 | Step 155600 | Avg Loss: 0.0140 | Grad Norm: 0.01004474\n",
      "Epoch 1 | Step 155700 | Avg Loss: 0.0135 | Grad Norm: 0.00979966\n",
      "Epoch 1 | Step 155800 | Avg Loss: 0.0133 | Grad Norm: 0.00988382\n",
      "Epoch 1 | Step 155900 | Avg Loss: 0.0134 | Grad Norm: 0.01013118\n",
      "Epoch 1 | Step 156000 | Avg Loss: 0.0136 | Grad Norm: 0.00954649\n",
      "Epoch 1 | Step 156100 | Avg Loss: 0.0141 | Grad Norm: 0.01064434\n",
      "Epoch 1 | Step 156200 | Avg Loss: 0.0143 | Grad Norm: 0.01140809\n",
      "Epoch 1 | Step 156300 | Avg Loss: 0.0144 | Grad Norm: 0.01081654\n",
      "Epoch 1 | Step 156400 | Avg Loss: 0.0148 | Grad Norm: 0.00839269\n",
      "Epoch 1 | Step 156500 | Avg Loss: 0.0143 | Grad Norm: 0.00963929\n",
      "Epoch 1 | Step 156600 | Avg Loss: 0.0144 | Grad Norm: 0.00854256\n",
      "Epoch 1 | Step 156700 | Avg Loss: 0.0141 | Grad Norm: 0.00911497\n",
      "Epoch 1 | Step 156800 | Avg Loss: 0.0139 | Grad Norm: 0.00888992\n",
      "Epoch 1 | Step 156900 | Avg Loss: 0.0141 | Grad Norm: 0.00915832\n",
      "Epoch 1 | Step 157000 | Avg Loss: 0.0142 | Grad Norm: 0.00986335\n",
      "Epoch 1 | Step 157100 | Avg Loss: 0.0140 | Grad Norm: 0.01020349\n",
      "Epoch 1 | Step 157200 | Avg Loss: 0.0140 | Grad Norm: 0.00966159\n",
      "Epoch 1 | Step 157300 | Avg Loss: 0.0139 | Grad Norm: 0.00870748\n",
      "Epoch 1 | Step 157400 | Avg Loss: 0.0143 | Grad Norm: 0.01106192\n",
      "Epoch 1 | Step 157500 | Avg Loss: 0.0141 | Grad Norm: 0.01103799\n",
      "Epoch 1 | Step 157600 | Avg Loss: 0.0142 | Grad Norm: 0.00975719\n",
      "Epoch 1 | Step 157700 | Avg Loss: 0.0141 | Grad Norm: 0.01089865\n",
      "Epoch 1 | Step 157800 | Avg Loss: 0.0139 | Grad Norm: 0.00944507\n",
      "Epoch 1 | Step 157900 | Avg Loss: 0.0138 | Grad Norm: 0.00888100\n",
      "Epoch 1 | Step 158000 | Avg Loss: 0.0142 | Grad Norm: 0.00984793\n",
      "Epoch 1 | Step 158100 | Avg Loss: 0.0139 | Grad Norm: 0.00820849\n",
      "Epoch 1 | Step 158200 | Avg Loss: 0.0140 | Grad Norm: 0.01009588\n",
      "Epoch 1 | Step 158300 | Avg Loss: 0.0141 | Grad Norm: 0.00963321\n",
      "Epoch 1 | Step 158400 | Avg Loss: 0.0146 | Grad Norm: 0.01231629\n",
      "Epoch 1 | Step 158500 | Avg Loss: 0.0142 | Grad Norm: 0.01020824\n",
      "Epoch 1 | Step 158600 | Avg Loss: 0.0142 | Grad Norm: 0.01003309\n",
      "Epoch 1 | Step 158700 | Avg Loss: 0.0142 | Grad Norm: 0.01153698\n",
      "Epoch 1 | Step 158800 | Avg Loss: 0.0140 | Grad Norm: 0.00985158\n",
      "Epoch 1 | Step 158900 | Avg Loss: 0.0142 | Grad Norm: 0.00833302\n",
      "Epoch 1 | Step 159000 | Avg Loss: 0.0145 | Grad Norm: 0.01053984\n",
      "Epoch 1 | Step 159100 | Avg Loss: 0.0144 | Grad Norm: 0.01030421\n",
      "Epoch 1 | Step 159200 | Avg Loss: 0.0144 | Grad Norm: 0.00978905\n",
      "Epoch 1 | Step 159300 | Avg Loss: 0.0143 | Grad Norm: 0.00893532\n",
      "Epoch 1 | Step 159400 | Avg Loss: 0.0144 | Grad Norm: 0.01011681\n",
      "Epoch 1 | Step 159500 | Avg Loss: 0.0145 | Grad Norm: 0.00987020\n",
      "Epoch 1 | Step 159600 | Avg Loss: 0.0141 | Grad Norm: 0.00925381\n",
      "Epoch 1 | Step 159700 | Avg Loss: 0.0142 | Grad Norm: 0.01032942\n",
      "Epoch 1 | Step 159800 | Avg Loss: 0.0146 | Grad Norm: 0.00999194\n",
      "Epoch 1 | Step 159900 | Avg Loss: 0.0144 | Grad Norm: 0.00901308\n",
      "Epoch 1 | Step 160000 | Avg Loss: 0.0142 | Grad Norm: 0.01189913\n",
      "Epoch 1 | Step 160100 | Avg Loss: 0.0143 | Grad Norm: 0.01229258\n",
      "Epoch 1 | Step 160200 | Avg Loss: 0.0143 | Grad Norm: 0.00963891\n",
      "Epoch 1 | Step 160300 | Avg Loss: 0.0140 | Grad Norm: 0.01048958\n",
      "Epoch 1 | Step 160400 | Avg Loss: 0.0141 | Grad Norm: 0.01068999\n",
      "Epoch 1 | Step 160500 | Avg Loss: 0.0144 | Grad Norm: 0.00975084\n",
      "Epoch 1 | Step 160600 | Avg Loss: 0.0140 | Grad Norm: 0.01130858\n",
      "Epoch 1 | Step 160700 | Avg Loss: 0.0142 | Grad Norm: 0.01004587\n",
      "Epoch 1 | Step 160800 | Avg Loss: 0.0140 | Grad Norm: 0.00941202\n",
      "Epoch 1 | Step 160900 | Avg Loss: 0.0138 | Grad Norm: 0.01011828\n",
      "Epoch 1 | Step 161000 | Avg Loss: 0.0138 | Grad Norm: 0.00969004\n",
      "Epoch 1 | Step 161100 | Avg Loss: 0.0141 | Grad Norm: 0.01183555\n",
      "Epoch 1 | Step 161200 | Avg Loss: 0.0137 | Grad Norm: 0.00975617\n",
      "Epoch 1 | Step 161300 | Avg Loss: 0.0141 | Grad Norm: 0.01022247\n",
      "Epoch 1 | Step 161400 | Avg Loss: 0.0140 | Grad Norm: 0.01010740\n",
      "Epoch 1 | Step 161500 | Avg Loss: 0.0140 | Grad Norm: 0.01026983\n",
      "Epoch 1 | Step 161600 | Avg Loss: 0.0142 | Grad Norm: 0.01003277\n",
      "Epoch 1 | Step 161700 | Avg Loss: 0.0145 | Grad Norm: 0.01042049\n",
      "Epoch 1 | Step 161800 | Avg Loss: 0.0143 | Grad Norm: 0.00969746\n",
      "Epoch 1 | Step 161900 | Avg Loss: 0.0139 | Grad Norm: 0.00844791\n",
      "Epoch 1 | Step 162000 | Avg Loss: 0.0140 | Grad Norm: 0.00874007\n",
      "Epoch 1 | Step 162100 | Avg Loss: 0.0143 | Grad Norm: 0.01168331\n",
      "Epoch 1 | Step 162200 | Avg Loss: 0.0144 | Grad Norm: 0.00954212\n",
      "Epoch 1 | Step 162300 | Avg Loss: 0.0142 | Grad Norm: 0.00831243\n",
      "Epoch 1 | Step 162400 | Avg Loss: 0.0137 | Grad Norm: 0.01004676\n",
      "Epoch 1 | Step 162500 | Avg Loss: 0.0133 | Grad Norm: 0.00996619\n",
      "Epoch 1 | Step 162600 | Avg Loss: 0.0138 | Grad Norm: 0.01010861\n",
      "Epoch 1 | Step 162700 | Avg Loss: 0.0139 | Grad Norm: 0.00990149\n",
      "Epoch 1 | Step 162800 | Avg Loss: 0.0142 | Grad Norm: 0.00933393\n",
      "Epoch 1 | Step 162900 | Avg Loss: 0.0143 | Grad Norm: 0.00805052\n",
      "Epoch 1 | Step 163000 | Avg Loss: 0.0142 | Grad Norm: 0.01040360\n",
      "Epoch 1 | Step 163100 | Avg Loss: 0.0143 | Grad Norm: 0.01169656\n",
      "Epoch 1 | Step 163200 | Avg Loss: 0.0141 | Grad Norm: 0.00934008\n",
      "Epoch 1 | Step 163300 | Avg Loss: 0.0141 | Grad Norm: 0.00976622\n",
      "Epoch 1 | Step 163400 | Avg Loss: 0.0143 | Grad Norm: 0.01031621\n",
      "Epoch 1 | Step 163500 | Avg Loss: 0.0146 | Grad Norm: 0.00920740\n",
      "Epoch 1 | Step 163600 | Avg Loss: 0.0143 | Grad Norm: 0.01062478\n",
      "Epoch 1 | Step 163700 | Avg Loss: 0.0142 | Grad Norm: 0.01107681\n",
      "Epoch 1 | Step 163800 | Avg Loss: 0.0144 | Grad Norm: 0.00806588\n",
      "Epoch 1 | Step 163900 | Avg Loss: 0.0149 | Grad Norm: 0.01114707\n",
      "Epoch 1 | Step 164000 | Avg Loss: 0.0145 | Grad Norm: 0.01006247\n",
      "Epoch 1 | Step 164100 | Avg Loss: 0.0147 | Grad Norm: 0.00922065\n",
      "Epoch 1 | Step 164200 | Avg Loss: 0.0150 | Grad Norm: 0.00922124\n",
      "Epoch 1 | Step 164300 | Avg Loss: 0.0147 | Grad Norm: 0.00913129\n",
      "Epoch 1 | Step 164400 | Avg Loss: 0.0144 | Grad Norm: 0.01120853\n",
      "Epoch 1 | Step 164500 | Avg Loss: 0.0141 | Grad Norm: 0.00875045\n",
      "Epoch 1 | Step 164600 | Avg Loss: 0.0142 | Grad Norm: 0.01017394\n",
      "Epoch 1 | Step 164700 | Avg Loss: 0.0141 | Grad Norm: 0.01210217\n",
      "Epoch 1 | Step 164800 | Avg Loss: 0.0141 | Grad Norm: 0.00985650\n",
      "Epoch 1 | Step 164900 | Avg Loss: 0.0142 | Grad Norm: 0.00922467\n",
      "Epoch 1 | Step 165000 | Avg Loss: 0.0141 | Grad Norm: 0.00977281\n",
      "Epoch 1 | Step 165100 | Avg Loss: 0.0139 | Grad Norm: 0.00921444\n",
      "Epoch 1 | Step 165200 | Avg Loss: 0.0141 | Grad Norm: 0.00894216\n",
      "Epoch 1 | Step 165300 | Avg Loss: 0.0143 | Grad Norm: 0.01008967\n",
      "Epoch 1 | Step 165400 | Avg Loss: 0.0146 | Grad Norm: 0.01095621\n",
      "Epoch 1 | Step 165500 | Avg Loss: 0.0143 | Grad Norm: 0.01020077\n",
      "Epoch 1 | Step 165600 | Avg Loss: 0.0141 | Grad Norm: 0.01061069\n",
      "Epoch 1 | Step 165700 | Avg Loss: 0.0139 | Grad Norm: 0.01022398\n",
      "Epoch 1 | Step 165800 | Avg Loss: 0.0138 | Grad Norm: 0.01207816\n",
      "Epoch 1 | Step 165900 | Avg Loss: 0.0140 | Grad Norm: 0.01053453\n",
      "Epoch 1 | Step 166000 | Avg Loss: 0.0139 | Grad Norm: 0.01022461\n",
      "Epoch 1 | Step 166100 | Avg Loss: 0.0140 | Grad Norm: 0.01007234\n",
      "Epoch 1 | Step 166200 | Avg Loss: 0.0141 | Grad Norm: 0.01142190\n",
      "Epoch 1 | Step 166300 | Avg Loss: 0.0145 | Grad Norm: 0.00937148\n",
      "Epoch 1 | Step 166400 | Avg Loss: 0.0148 | Grad Norm: 0.01001216\n",
      "Epoch 1 | Step 166500 | Avg Loss: 0.0145 | Grad Norm: 0.01006787\n",
      "Epoch 1 | Step 166600 | Avg Loss: 0.0145 | Grad Norm: 0.00962105\n",
      "Epoch 1 | Step 166700 | Avg Loss: 0.0144 | Grad Norm: 0.01111105\n",
      "Epoch 1 | Step 166800 | Avg Loss: 0.0143 | Grad Norm: 0.01040093\n",
      "Epoch 1 | Step 166900 | Avg Loss: 0.0142 | Grad Norm: 0.01054907\n",
      "Epoch 1 | Step 167000 | Avg Loss: 0.0142 | Grad Norm: 0.01061270\n",
      "Epoch 1 | Step 167100 | Avg Loss: 0.0142 | Grad Norm: 0.01017551\n",
      "Epoch 1 | Step 167200 | Avg Loss: 0.0147 | Grad Norm: 0.01186500\n",
      "Epoch 1 | Step 167300 | Avg Loss: 0.0148 | Grad Norm: 0.01161146\n",
      "Epoch 1 | Step 167400 | Avg Loss: 0.0144 | Grad Norm: 0.00993599\n",
      "Epoch 1 | Step 167500 | Avg Loss: 0.0146 | Grad Norm: 0.01148393\n",
      "Epoch 1 | Step 167600 | Avg Loss: 0.0148 | Grad Norm: 0.01359781\n",
      "Epoch 1 | Step 167700 | Avg Loss: 0.0148 | Grad Norm: 0.01091735\n",
      "Epoch 1 | Step 167800 | Avg Loss: 0.0146 | Grad Norm: 0.01004605\n",
      "Epoch 1 | Step 167900 | Avg Loss: 0.0143 | Grad Norm: 0.01009865\n",
      "Epoch 1 | Step 168000 | Avg Loss: 0.0142 | Grad Norm: 0.01158317\n",
      "Epoch 1 | Step 168100 | Avg Loss: 0.0146 | Grad Norm: 0.01105742\n",
      "Epoch 1 | Step 168200 | Avg Loss: 0.0148 | Grad Norm: 0.01030907\n",
      "Epoch 1 | Step 168300 | Avg Loss: 0.0145 | Grad Norm: 0.00969621\n",
      "Epoch 1 | Step 168400 | Avg Loss: 0.0143 | Grad Norm: 0.01011575\n",
      "Epoch 1 | Step 168500 | Avg Loss: 0.0145 | Grad Norm: 0.01212415\n",
      "Epoch 1 | Step 168600 | Avg Loss: 0.0148 | Grad Norm: 0.01063006\n",
      "Epoch 1 | Step 168700 | Avg Loss: 0.0144 | Grad Norm: 0.00926486\n",
      "Epoch 1 | Step 168800 | Avg Loss: 0.0144 | Grad Norm: 0.01003804\n",
      "Epoch 1 | Step 168900 | Avg Loss: 0.0144 | Grad Norm: 0.00959020\n",
      "Epoch 1 | Step 169000 | Avg Loss: 0.0138 | Grad Norm: 0.01225213\n",
      "Epoch 1 | Step 169100 | Avg Loss: 0.0137 | Grad Norm: 0.00945723\n",
      "Epoch 1 | Step 169200 | Avg Loss: 0.0140 | Grad Norm: 0.00915205\n",
      "Epoch 1 | Step 169300 | Avg Loss: 0.0138 | Grad Norm: 0.00969402\n",
      "Epoch 1 | Step 169400 | Avg Loss: 0.0138 | Grad Norm: 0.00959853\n",
      "Epoch 1 | Step 169500 | Avg Loss: 0.0138 | Grad Norm: 0.01002082\n",
      "Epoch 1 | Step 169600 | Avg Loss: 0.0139 | Grad Norm: 0.00947799\n",
      "Epoch 1 | Step 169700 | Avg Loss: 0.0142 | Grad Norm: 0.01029957\n",
      "Epoch 1 | Step 169800 | Avg Loss: 0.0140 | Grad Norm: 0.00939293\n",
      "Epoch 1 | Step 169900 | Avg Loss: 0.0138 | Grad Norm: 0.00881911\n",
      "Epoch 1 | Step 170000 | Avg Loss: 0.0137 | Grad Norm: 0.00962058\n",
      "Epoch 1 | Step 170100 | Avg Loss: 0.0137 | Grad Norm: 0.00928805\n",
      "Epoch 1 | Step 170200 | Avg Loss: 0.0138 | Grad Norm: 0.01003348\n",
      "Epoch 1 | Step 170300 | Avg Loss: 0.0145 | Grad Norm: 0.01041333\n",
      "Epoch 1 | Step 170400 | Avg Loss: 0.0144 | Grad Norm: 0.00914695\n",
      "Epoch 1 | Step 170500 | Avg Loss: 0.0144 | Grad Norm: 0.00977126\n",
      "Epoch 1 | Step 170600 | Avg Loss: 0.0144 | Grad Norm: 0.00931662\n",
      "Epoch 1 | Step 170700 | Avg Loss: 0.0144 | Grad Norm: 0.01065906\n",
      "Epoch 1 | Step 170800 | Avg Loss: 0.0143 | Grad Norm: 0.01117273\n",
      "Epoch 1 | Step 170900 | Avg Loss: 0.0145 | Grad Norm: 0.00980155\n",
      "Epoch 1 | Step 171000 | Avg Loss: 0.0140 | Grad Norm: 0.00960171\n",
      "Epoch 1 | Step 171100 | Avg Loss: 0.0138 | Grad Norm: 0.00962743\n",
      "Epoch 1 | Step 171200 | Avg Loss: 0.0143 | Grad Norm: 0.01110923\n",
      "Epoch 1 | Step 171300 | Avg Loss: 0.0143 | Grad Norm: 0.01102474\n",
      "Epoch 1 | Step 171400 | Avg Loss: 0.0145 | Grad Norm: 0.00974138\n",
      "Epoch 1 | Step 171500 | Avg Loss: 0.0147 | Grad Norm: 0.00906294\n",
      "Epoch 1 | Step 171600 | Avg Loss: 0.0143 | Grad Norm: 0.01239538\n",
      "Epoch 1 | Step 171700 | Avg Loss: 0.0143 | Grad Norm: 0.01130027\n",
      "Epoch 1 | Step 171800 | Avg Loss: 0.0144 | Grad Norm: 0.00979699\n",
      "Epoch 1 | Step 171900 | Avg Loss: 0.0146 | Grad Norm: 0.00972895\n",
      "Epoch 1 | Step 172000 | Avg Loss: 0.0148 | Grad Norm: 0.00974382\n",
      "Epoch 1 | Step 172100 | Avg Loss: 0.0144 | Grad Norm: 0.01142345\n",
      "Epoch 1 | Step 172200 | Avg Loss: 0.0140 | Grad Norm: 0.00994278\n",
      "Epoch 1 | Step 172300 | Avg Loss: 0.0141 | Grad Norm: 0.01145311\n",
      "Epoch 1 | Step 172400 | Avg Loss: 0.0136 | Grad Norm: 0.01029606\n",
      "Epoch 1 | Step 172500 | Avg Loss: 0.0136 | Grad Norm: 0.00898212\n",
      "Epoch 1 | Step 172600 | Avg Loss: 0.0135 | Grad Norm: 0.01159439\n",
      "Epoch 1 | Step 172700 | Avg Loss: 0.0133 | Grad Norm: 0.00858347\n",
      "Epoch 1 | Step 172800 | Avg Loss: 0.0134 | Grad Norm: 0.01122010\n",
      "Epoch 1 | Step 172900 | Avg Loss: 0.0136 | Grad Norm: 0.00997871\n",
      "Epoch 1 | Step 173000 | Avg Loss: 0.0136 | Grad Norm: 0.00864908\n",
      "Epoch 1 | Step 173100 | Avg Loss: 0.0138 | Grad Norm: 0.00983909\n",
      "Epoch 1 | Step 173200 | Avg Loss: 0.0146 | Grad Norm: 0.01088164\n",
      "Epoch 1 | Step 173300 | Avg Loss: 0.0145 | Grad Norm: 0.00916212\n",
      "Epoch 1 | Step 173400 | Avg Loss: 0.0144 | Grad Norm: 0.01091710\n",
      "Epoch 1 | Step 173500 | Avg Loss: 0.0145 | Grad Norm: 0.01048304\n",
      "Epoch 1 | Step 173600 | Avg Loss: 0.0146 | Grad Norm: 0.01114978\n",
      "Epoch 1 | Step 173700 | Avg Loss: 0.0145 | Grad Norm: 0.01012277\n",
      "Epoch 1 | Step 173800 | Avg Loss: 0.0142 | Grad Norm: 0.00908597\n",
      "Epoch 1 | Step 173900 | Avg Loss: 0.0147 | Grad Norm: 0.00945618\n",
      "Epoch 1 | Step 174000 | Avg Loss: 0.0146 | Grad Norm: 0.01033725\n",
      "Epoch 1 | Step 174100 | Avg Loss: 0.0147 | Grad Norm: 0.00991459\n",
      "Epoch 1 | Step 174200 | Avg Loss: 0.0145 | Grad Norm: 0.01033410\n",
      "Epoch 1 | Step 174300 | Avg Loss: 0.0142 | Grad Norm: 0.00975461\n",
      "Epoch 1 | Step 174400 | Avg Loss: 0.0148 | Grad Norm: 0.00956235\n",
      "Epoch 1 | Step 174500 | Avg Loss: 0.0145 | Grad Norm: 0.00964087\n",
      "Epoch 1 | Step 174600 | Avg Loss: 0.0145 | Grad Norm: 0.00960509\n",
      "Epoch 1 | Step 174700 | Avg Loss: 0.0146 | Grad Norm: 0.00925936\n",
      "Epoch 1 | Step 174800 | Avg Loss: 0.0148 | Grad Norm: 0.01059953\n",
      "Epoch 1 | Step 174900 | Avg Loss: 0.0146 | Grad Norm: 0.01104869\n",
      "Epoch 1 | Step 175000 | Avg Loss: 0.0146 | Grad Norm: 0.00893515\n",
      "Epoch 1 | Step 175100 | Avg Loss: 0.0146 | Grad Norm: 0.00991918\n",
      "Epoch 1 | Step 175200 | Avg Loss: 0.0144 | Grad Norm: 0.00998190\n",
      "Epoch 1 | Step 175300 | Avg Loss: 0.0142 | Grad Norm: 0.00953920\n",
      "Epoch 1 | Step 175400 | Avg Loss: 0.0142 | Grad Norm: 0.01048475\n",
      "Epoch 1 | Step 175500 | Avg Loss: 0.0141 | Grad Norm: 0.01099919\n",
      "Epoch 1 | Step 175600 | Avg Loss: 0.0140 | Grad Norm: 0.01128559\n",
      "Epoch 1 | Step 175700 | Avg Loss: 0.0147 | Grad Norm: 0.00964054\n",
      "Epoch 1 | Step 175800 | Avg Loss: 0.0147 | Grad Norm: 0.01015742\n",
      "Epoch 1 | Step 175900 | Avg Loss: 0.0144 | Grad Norm: 0.00919630\n",
      "Epoch 1 | Step 176000 | Avg Loss: 0.0142 | Grad Norm: 0.00960696\n",
      "Epoch 1 | Step 176100 | Avg Loss: 0.0142 | Grad Norm: 0.00962773\n",
      "Epoch 1 | Step 176200 | Avg Loss: 0.0143 | Grad Norm: 0.00934114\n",
      "Epoch 1 | Step 176300 | Avg Loss: 0.0139 | Grad Norm: 0.01066005\n",
      "Epoch 1 | Step 176400 | Avg Loss: 0.0141 | Grad Norm: 0.00940510\n",
      "Epoch 1 | Step 176500 | Avg Loss: 0.0142 | Grad Norm: 0.01123745\n",
      "Epoch 1 | Step 176600 | Avg Loss: 0.0141 | Grad Norm: 0.00997268\n",
      "Epoch 1 | Step 176700 | Avg Loss: 0.0142 | Grad Norm: 0.00906707\n",
      "Epoch 1 | Step 176800 | Avg Loss: 0.0146 | Grad Norm: 0.00961694\n",
      "Epoch 1 | Step 176900 | Avg Loss: 0.0149 | Grad Norm: 0.01153714\n",
      "Epoch 1 | Step 177000 | Avg Loss: 0.0147 | Grad Norm: 0.00960414\n",
      "Epoch 1 | Step 177100 | Avg Loss: 0.0143 | Grad Norm: 0.00992645\n",
      "Epoch 1 | Step 177200 | Avg Loss: 0.0145 | Grad Norm: 0.00941656\n",
      "Epoch 1 | Step 177300 | Avg Loss: 0.0146 | Grad Norm: 0.01146902\n",
      "Epoch 1 | Step 177400 | Avg Loss: 0.0146 | Grad Norm: 0.01059357\n",
      "Epoch 1 | Step 177500 | Avg Loss: 0.0143 | Grad Norm: 0.01157450\n",
      "Epoch 1 | Step 177600 | Avg Loss: 0.0142 | Grad Norm: 0.01059202\n",
      "Epoch 1 | Step 177700 | Avg Loss: 0.0140 | Grad Norm: 0.00915526\n",
      "Epoch 1 | Step 177800 | Avg Loss: 0.0142 | Grad Norm: 0.00886840\n",
      "Epoch 1 | Step 177900 | Avg Loss: 0.0142 | Grad Norm: 0.01119037\n",
      "Epoch 1 | Step 178000 | Avg Loss: 0.0147 | Grad Norm: 0.00914479\n",
      "Epoch 1 | Step 178100 | Avg Loss: 0.0146 | Grad Norm: 0.01179233\n",
      "Epoch 1 | Step 178200 | Avg Loss: 0.0147 | Grad Norm: 0.00978070\n",
      "Epoch 1 | Step 178300 | Avg Loss: 0.0142 | Grad Norm: 0.01012691\n",
      "Epoch 1 | Step 178400 | Avg Loss: 0.0146 | Grad Norm: 0.01004846\n",
      "Epoch 1 | Step 178500 | Avg Loss: 0.0145 | Grad Norm: 0.01092659\n",
      "Epoch 1 | Step 178600 | Avg Loss: 0.0143 | Grad Norm: 0.00991278\n",
      "Epoch 1 | Step 178700 | Avg Loss: 0.0145 | Grad Norm: 0.01094874\n",
      "Epoch 1 | Step 178800 | Avg Loss: 0.0146 | Grad Norm: 0.01214432\n",
      "Epoch 1 | Step 178900 | Avg Loss: 0.0148 | Grad Norm: 0.00923094\n",
      "Epoch 1 | Step 179000 | Avg Loss: 0.0148 | Grad Norm: 0.01050662\n",
      "Epoch 1 | Step 179100 | Avg Loss: 0.0146 | Grad Norm: 0.00850907\n",
      "Epoch 1 | Step 179200 | Avg Loss: 0.0147 | Grad Norm: 0.00961734\n",
      "Epoch 1 | Step 179300 | Avg Loss: 0.0147 | Grad Norm: 0.00983044\n",
      "Epoch 1 | Step 179400 | Avg Loss: 0.0144 | Grad Norm: 0.00950075\n",
      "Epoch 1 | Step 179500 | Avg Loss: 0.0150 | Grad Norm: 0.01101907\n",
      "Epoch 1 | Step 179600 | Avg Loss: 0.0148 | Grad Norm: 0.00995717\n",
      "Epoch 1 | Step 179700 | Avg Loss: 0.0149 | Grad Norm: 0.01087016\n",
      "Epoch 1 | Step 179800 | Avg Loss: 0.0148 | Grad Norm: 0.01066501\n",
      "Epoch 1 | Step 179900 | Avg Loss: 0.0147 | Grad Norm: 0.01020443\n",
      "Epoch 1 | Step 180000 | Avg Loss: 0.0147 | Grad Norm: 0.00968452\n",
      "Epoch 1 | Step 180100 | Avg Loss: 0.0146 | Grad Norm: 0.00869245\n",
      "Epoch 1 | Step 180200 | Avg Loss: 0.0148 | Grad Norm: 0.00933761\n",
      "Epoch 1 | Step 180300 | Avg Loss: 0.0149 | Grad Norm: 0.00941814\n",
      "Epoch 1 | Step 180400 | Avg Loss: 0.0146 | Grad Norm: 0.01043377\n",
      "Epoch 1 | Step 180500 | Avg Loss: 0.0147 | Grad Norm: 0.00947276\n",
      "Epoch 1 | Step 180600 | Avg Loss: 0.0143 | Grad Norm: 0.00943691\n",
      "Epoch 1 | Step 180700 | Avg Loss: 0.0142 | Grad Norm: 0.00891916\n",
      "Epoch 1 | Step 180800 | Avg Loss: 0.0144 | Grad Norm: 0.00880285\n",
      "Epoch 1 | Step 180900 | Avg Loss: 0.0143 | Grad Norm: 0.01107717\n",
      "Epoch 1 | Step 181000 | Avg Loss: 0.0145 | Grad Norm: 0.01149614\n",
      "Epoch 1 | Step 181100 | Avg Loss: 0.0146 | Grad Norm: 0.01219220\n",
      "Epoch 1 | Step 181200 | Avg Loss: 0.0149 | Grad Norm: 0.00997242\n",
      "Epoch 1 | Step 181300 | Avg Loss: 0.0147 | Grad Norm: 0.01098582\n",
      "Epoch 1 | Step 181400 | Avg Loss: 0.0151 | Grad Norm: 0.01060331\n",
      "Epoch 1 | Step 181500 | Avg Loss: 0.0143 | Grad Norm: 0.01145417\n",
      "Epoch 1 | Step 181600 | Avg Loss: 0.0145 | Grad Norm: 0.01155182\n",
      "Epoch 1 | Step 181700 | Avg Loss: 0.0145 | Grad Norm: 0.00964887\n",
      "Epoch 1 | Step 181800 | Avg Loss: 0.0142 | Grad Norm: 0.01035886\n",
      "Epoch 1 | Step 181900 | Avg Loss: 0.0141 | Grad Norm: 0.01181020\n",
      "Epoch 1 | Step 182000 | Avg Loss: 0.0141 | Grad Norm: 0.00936448\n",
      "Epoch 1 | Step 182100 | Avg Loss: 0.0140 | Grad Norm: 0.00913152\n",
      "Epoch 1 | Step 182200 | Avg Loss: 0.0142 | Grad Norm: 0.00938970\n",
      "Epoch 1 | Step 182300 | Avg Loss: 0.0144 | Grad Norm: 0.00930466\n",
      "Epoch 1 | Step 182400 | Avg Loss: 0.0146 | Grad Norm: 0.01049965\n",
      "Epoch 1 | Step 182500 | Avg Loss: 0.0142 | Grad Norm: 0.00825926\n",
      "Epoch 1 | Step 182600 | Avg Loss: 0.0140 | Grad Norm: 0.00890792\n",
      "Epoch 1 | Step 182700 | Avg Loss: 0.0138 | Grad Norm: 0.00980046\n",
      "Epoch 1 | Step 182800 | Avg Loss: 0.0140 | Grad Norm: 0.00824857\n",
      "Epoch 1 | Step 182900 | Avg Loss: 0.0144 | Grad Norm: 0.01030111\n",
      "Epoch 1 | Step 183000 | Avg Loss: 0.0141 | Grad Norm: 0.01109265\n",
      "Epoch 1 | Step 183100 | Avg Loss: 0.0143 | Grad Norm: 0.00937905\n",
      "Epoch 1 | Step 183200 | Avg Loss: 0.0141 | Grad Norm: 0.01069096\n",
      "Epoch 1 | Step 183300 | Avg Loss: 0.0143 | Grad Norm: 0.00999175\n",
      "Epoch 1 | Step 183400 | Avg Loss: 0.0140 | Grad Norm: 0.00914646\n",
      "Epoch 1 | Step 183500 | Avg Loss: 0.0140 | Grad Norm: 0.00901200\n",
      "Epoch 1 | Step 183600 | Avg Loss: 0.0142 | Grad Norm: 0.00832358\n",
      "Epoch 1 | Step 183700 | Avg Loss: 0.0143 | Grad Norm: 0.00977223\n",
      "Epoch 1 | Step 183800 | Avg Loss: 0.0143 | Grad Norm: 0.00977611\n",
      "Epoch 1 | Step 183900 | Avg Loss: 0.0140 | Grad Norm: 0.01010569\n",
      "Epoch 1 | Step 184000 | Avg Loss: 0.0140 | Grad Norm: 0.00994934\n",
      "Epoch 1 | Step 184100 | Avg Loss: 0.0142 | Grad Norm: 0.00993226\n",
      "Epoch 1 | Step 184200 | Avg Loss: 0.0145 | Grad Norm: 0.00925517\n",
      "Epoch 1 | Step 184300 | Avg Loss: 0.0144 | Grad Norm: 0.00920097\n",
      "Epoch 1 | Step 184400 | Avg Loss: 0.0144 | Grad Norm: 0.00872908\n",
      "Epoch 1 | Step 184500 | Avg Loss: 0.0143 | Grad Norm: 0.00865541\n",
      "Epoch 1 | Step 184600 | Avg Loss: 0.0141 | Grad Norm: 0.01004974\n",
      "Epoch 1 | Step 184700 | Avg Loss: 0.0140 | Grad Norm: 0.00924326\n",
      "Epoch 1 | Step 184800 | Avg Loss: 0.0142 | Grad Norm: 0.00959869\n",
      "Epoch 1 | Step 184900 | Avg Loss: 0.0141 | Grad Norm: 0.01099814\n",
      "Epoch 1 | Step 185000 | Avg Loss: 0.0141 | Grad Norm: 0.01226541\n",
      "Epoch 1 | Step 185100 | Avg Loss: 0.0141 | Grad Norm: 0.01004854\n",
      "Epoch 1 | Step 185200 | Avg Loss: 0.0139 | Grad Norm: 0.00931240\n",
      "Epoch 1 | Step 185300 | Avg Loss: 0.0136 | Grad Norm: 0.01002264\n",
      "Epoch 1 | Step 185400 | Avg Loss: 0.0138 | Grad Norm: 0.00964654\n",
      "Epoch 1 | Step 185500 | Avg Loss: 0.0140 | Grad Norm: 0.00972910\n",
      "Epoch 1 | Step 185600 | Avg Loss: 0.0141 | Grad Norm: 0.01058603\n",
      "Epoch 1 | Step 185700 | Avg Loss: 0.0140 | Grad Norm: 0.00993223\n",
      "Epoch 1 | Step 185800 | Avg Loss: 0.0137 | Grad Norm: 0.00929927\n",
      "Epoch 1 | Step 185900 | Avg Loss: 0.0143 | Grad Norm: 0.00805454\n",
      "Epoch 1 | Step 186000 | Avg Loss: 0.0140 | Grad Norm: 0.01118759\n",
      "Epoch 1 | Step 186100 | Avg Loss: 0.0141 | Grad Norm: 0.00927061\n",
      "Epoch 1 | Step 186200 | Avg Loss: 0.0140 | Grad Norm: 0.00927157\n",
      "Epoch 1 | Step 186300 | Avg Loss: 0.0139 | Grad Norm: 0.00966443\n",
      "Epoch 1 | Step 186400 | Avg Loss: 0.0135 | Grad Norm: 0.00935681\n",
      "Epoch 1 | Step 186500 | Avg Loss: 0.0136 | Grad Norm: 0.01068651\n",
      "Epoch 1 | Step 186600 | Avg Loss: 0.0139 | Grad Norm: 0.00915420\n",
      "Epoch 1 | Step 186700 | Avg Loss: 0.0143 | Grad Norm: 0.01119123\n",
      "Epoch 1 | Step 186800 | Avg Loss: 0.0142 | Grad Norm: 0.00863645\n",
      "Epoch 1 | Step 186900 | Avg Loss: 0.0146 | Grad Norm: 0.00961643\n",
      "Epoch 1 | Step 187000 | Avg Loss: 0.0145 | Grad Norm: 0.01034243\n",
      "Epoch 1 | Step 187100 | Avg Loss: 0.0142 | Grad Norm: 0.01002203\n",
      "Epoch 1 | Step 187200 | Avg Loss: 0.0148 | Grad Norm: 0.01018791\n",
      "Epoch 1 | Step 187300 | Avg Loss: 0.0148 | Grad Norm: 0.00998823\n",
      "Epoch 1 | Step 187400 | Avg Loss: 0.0147 | Grad Norm: 0.01122729\n",
      "Epoch 1 | Step 187500 | Avg Loss: 0.0146 | Grad Norm: 0.00966084\n",
      "Epoch 1 | Step 187600 | Avg Loss: 0.0145 | Grad Norm: 0.00970559\n",
      "Epoch 1 | Step 187700 | Avg Loss: 0.0146 | Grad Norm: 0.01063193\n",
      "Epoch 1 | Step 187800 | Avg Loss: 0.0149 | Grad Norm: 0.01044316\n",
      "Epoch 1 | Step 187900 | Avg Loss: 0.0151 | Grad Norm: 0.00971058\n",
      "Epoch 1 | Step 188000 | Avg Loss: 0.0149 | Grad Norm: 0.00933606\n",
      "Epoch 1 | Step 188100 | Avg Loss: 0.0149 | Grad Norm: 0.01125899\n",
      "Epoch 1 | Step 188200 | Avg Loss: 0.0147 | Grad Norm: 0.01082620\n",
      "Epoch 1 | Step 188300 | Avg Loss: 0.0146 | Grad Norm: 0.00932833\n",
      "Epoch 1 | Step 188400 | Avg Loss: 0.0142 | Grad Norm: 0.01202004\n",
      "Epoch 1 | Step 188500 | Avg Loss: 0.0144 | Grad Norm: 0.01019082\n",
      "Epoch 1 | Step 188600 | Avg Loss: 0.0145 | Grad Norm: 0.01087020\n",
      "Epoch 1 | Step 188700 | Avg Loss: 0.0144 | Grad Norm: 0.01015120\n",
      "Epoch 1 | Step 188800 | Avg Loss: 0.0142 | Grad Norm: 0.01007664\n",
      "Epoch 1 | Step 188900 | Avg Loss: 0.0142 | Grad Norm: 0.01057355\n",
      "Epoch 1 | Step 189000 | Avg Loss: 0.0144 | Grad Norm: 0.01040250\n",
      "Epoch 1 | Step 189100 | Avg Loss: 0.0146 | Grad Norm: 0.00986043\n",
      "Epoch 1 | Step 189200 | Avg Loss: 0.0146 | Grad Norm: 0.01075745\n",
      "Epoch 1 | Step 189300 | Avg Loss: 0.0146 | Grad Norm: 0.00961489\n",
      "Epoch 1 | Step 189400 | Avg Loss: 0.0144 | Grad Norm: 0.01213245\n",
      "Epoch 1 | Step 189500 | Avg Loss: 0.0141 | Grad Norm: 0.00968521\n",
      "Epoch 1 | Step 189600 | Avg Loss: 0.0141 | Grad Norm: 0.00964449\n",
      "Epoch 1 | Step 189700 | Avg Loss: 0.0139 | Grad Norm: 0.01022827\n",
      "Epoch 1 | Step 189800 | Avg Loss: 0.0141 | Grad Norm: 0.00862126\n",
      "Epoch 1 | Step 189900 | Avg Loss: 0.0138 | Grad Norm: 0.00938077\n",
      "Epoch 1 | Step 190000 | Avg Loss: 0.0139 | Grad Norm: 0.01105340\n",
      "Epoch 1 | Step 190100 | Avg Loss: 0.0139 | Grad Norm: 0.01083212\n",
      "Epoch 1 | Step 190200 | Avg Loss: 0.0142 | Grad Norm: 0.01102675\n",
      "Epoch 1 | Step 190300 | Avg Loss: 0.0147 | Grad Norm: 0.00994784\n",
      "Epoch 1 | Step 190400 | Avg Loss: 0.0148 | Grad Norm: 0.00875148\n",
      "Epoch 1 | Step 190500 | Avg Loss: 0.0147 | Grad Norm: 0.01053639\n",
      "Epoch 1 | Step 190600 | Avg Loss: 0.0146 | Grad Norm: 0.01115619\n",
      "Epoch 1 | Step 190700 | Avg Loss: 0.0144 | Grad Norm: 0.01040300\n",
      "Epoch 1 | Step 190800 | Avg Loss: 0.0141 | Grad Norm: 0.00945398\n",
      "Epoch 1 | Step 190900 | Avg Loss: 0.0142 | Grad Norm: 0.01030605\n",
      "Epoch 1 | Step 191000 | Avg Loss: 0.0141 | Grad Norm: 0.00974307\n",
      "Epoch 1 | Step 191100 | Avg Loss: 0.0142 | Grad Norm: 0.00920716\n",
      "Epoch 1 | Step 191200 | Avg Loss: 0.0141 | Grad Norm: 0.00952350\n",
      "Epoch 1 | Step 191300 | Avg Loss: 0.0140 | Grad Norm: 0.01098537\n",
      "Epoch 1 | Step 191400 | Avg Loss: 0.0141 | Grad Norm: 0.00921214\n",
      "Epoch 1 | Step 191500 | Avg Loss: 0.0144 | Grad Norm: 0.01096276\n",
      "Epoch 1 | Step 191600 | Avg Loss: 0.0141 | Grad Norm: 0.00958813\n",
      "Epoch 1 | Step 191700 | Avg Loss: 0.0141 | Grad Norm: 0.00999723\n",
      "Epoch 1 | Step 191800 | Avg Loss: 0.0142 | Grad Norm: 0.00981087\n",
      "Epoch 1 | Step 191900 | Avg Loss: 0.0143 | Grad Norm: 0.01037925\n",
      "Epoch 1 | Step 192000 | Avg Loss: 0.0146 | Grad Norm: 0.01047760\n",
      "Epoch 1 | Step 192100 | Avg Loss: 0.0144 | Grad Norm: 0.00986388\n",
      "Epoch 1 | Step 192200 | Avg Loss: 0.0143 | Grad Norm: 0.01144593\n",
      "Epoch 1 | Step 192300 | Avg Loss: 0.0142 | Grad Norm: 0.01142046\n",
      "Epoch 1 | Step 192400 | Avg Loss: 0.0147 | Grad Norm: 0.00975207\n",
      "Epoch 1 | Step 192500 | Avg Loss: 0.0151 | Grad Norm: 0.01033838\n",
      "Epoch 1 | Step 192600 | Avg Loss: 0.0145 | Grad Norm: 0.00932060\n",
      "Epoch 1 | Step 192700 | Avg Loss: 0.0142 | Grad Norm: 0.00828266\n",
      "Epoch 1 | Step 192800 | Avg Loss: 0.0141 | Grad Norm: 0.00896168\n",
      "Epoch 1 | Step 192900 | Avg Loss: 0.0147 | Grad Norm: 0.01140154\n",
      "Epoch 1 | Step 193000 | Avg Loss: 0.0147 | Grad Norm: 0.01000860\n",
      "Epoch 1 | Step 193100 | Avg Loss: 0.0145 | Grad Norm: 0.01018309\n",
      "Epoch 1 | Step 193200 | Avg Loss: 0.0146 | Grad Norm: 0.01220871\n",
      "Epoch 1 | Step 193300 | Avg Loss: 0.0147 | Grad Norm: 0.01056859\n",
      "Epoch 1 | Step 193400 | Avg Loss: 0.0144 | Grad Norm: 0.00860594\n",
      "Epoch 1 | Step 193500 | Avg Loss: 0.0148 | Grad Norm: 0.00980808\n",
      "Epoch 1 | Step 193600 | Avg Loss: 0.0150 | Grad Norm: 0.01189649\n",
      "Epoch 1 | Step 193700 | Avg Loss: 0.0145 | Grad Norm: 0.00955148\n",
      "Epoch 1 | Step 193800 | Avg Loss: 0.0144 | Grad Norm: 0.01180551\n",
      "Epoch 1 | Step 193900 | Avg Loss: 0.0144 | Grad Norm: 0.00944632\n",
      "Epoch 1 | Step 194000 | Avg Loss: 0.0148 | Grad Norm: 0.01009997\n",
      "Epoch 1 | Step 194100 | Avg Loss: 0.0148 | Grad Norm: 0.00875899\n",
      "Epoch 1 | Step 194200 | Avg Loss: 0.0145 | Grad Norm: 0.00948834\n",
      "Epoch 1 | Step 194300 | Avg Loss: 0.0143 | Grad Norm: 0.01008582\n",
      "Epoch 1 | Step 194400 | Avg Loss: 0.0144 | Grad Norm: 0.01079936\n",
      "Epoch 1 | Step 194500 | Avg Loss: 0.0140 | Grad Norm: 0.01153814\n",
      "Epoch 1 | Step 194600 | Avg Loss: 0.0144 | Grad Norm: 0.01109460\n",
      "Epoch 1 | Step 194700 | Avg Loss: 0.0141 | Grad Norm: 0.01044061\n",
      "Epoch 1 | Step 194800 | Avg Loss: 0.0142 | Grad Norm: 0.01026801\n",
      "Epoch 1 | Step 194900 | Avg Loss: 0.0146 | Grad Norm: 0.01015335\n",
      "Epoch 1 | Step 195000 | Avg Loss: 0.0144 | Grad Norm: 0.01067474\n",
      "Epoch 1 | Step 195100 | Avg Loss: 0.0146 | Grad Norm: 0.00926068\n",
      "Epoch 1 | Step 195200 | Avg Loss: 0.0147 | Grad Norm: 0.01098715\n",
      "Epoch 1 | Step 195300 | Avg Loss: 0.0146 | Grad Norm: 0.00986155\n",
      "Epoch 1 | Step 195400 | Avg Loss: 0.0151 | Grad Norm: 0.01033729\n",
      "Epoch 1 | Step 195500 | Avg Loss: 0.0149 | Grad Norm: 0.01062339\n",
      "Epoch 1 | Step 195600 | Avg Loss: 0.0148 | Grad Norm: 0.00956459\n",
      "Epoch 1 | Step 195700 | Avg Loss: 0.0146 | Grad Norm: 0.00954397\n",
      "Epoch 1 | Step 195800 | Avg Loss: 0.0145 | Grad Norm: 0.01028818\n",
      "Epoch 1 | Step 195900 | Avg Loss: 0.0144 | Grad Norm: 0.00924152\n",
      "Epoch 1 | Step 196000 | Avg Loss: 0.0143 | Grad Norm: 0.00945037\n",
      "Epoch 1 | Step 196100 | Avg Loss: 0.0145 | Grad Norm: 0.00891407\n",
      "Epoch 1 | Step 196200 | Avg Loss: 0.0145 | Grad Norm: 0.00976086\n",
      "Epoch 1 | Step 196300 | Avg Loss: 0.0143 | Grad Norm: 0.00854304\n",
      "Epoch 1 | Step 196400 | Avg Loss: 0.0141 | Grad Norm: 0.01036171\n",
      "Epoch 1 | Step 196500 | Avg Loss: 0.0142 | Grad Norm: 0.00957730\n",
      "Epoch 1 | Step 196600 | Avg Loss: 0.0140 | Grad Norm: 0.01026172\n",
      "Epoch 1 | Step 196700 | Avg Loss: 0.0141 | Grad Norm: 0.01045017\n",
      "Epoch 1 | Step 196800 | Avg Loss: 0.0143 | Grad Norm: 0.00968251\n",
      "Epoch 1 | Step 196900 | Avg Loss: 0.0140 | Grad Norm: 0.00916822\n",
      "Epoch 1 | Step 197000 | Avg Loss: 0.0145 | Grad Norm: 0.00956311\n",
      "Epoch 1 | Step 197100 | Avg Loss: 0.0145 | Grad Norm: 0.00967413\n",
      "Epoch 1 | Step 197200 | Avg Loss: 0.0143 | Grad Norm: 0.00900559\n",
      "Epoch 1 | Step 197300 | Avg Loss: 0.0140 | Grad Norm: 0.00929750\n",
      "Epoch 1 | Step 197400 | Avg Loss: 0.0139 | Grad Norm: 0.01083499\n",
      "Epoch 1 | Step 197500 | Avg Loss: 0.0139 | Grad Norm: 0.00904192\n",
      "Epoch 1 | Step 197600 | Avg Loss: 0.0143 | Grad Norm: 0.01023527\n",
      "Epoch 1 | Step 197700 | Avg Loss: 0.0146 | Grad Norm: 0.01085524\n",
      "Epoch 1 | Step 197800 | Avg Loss: 0.0145 | Grad Norm: 0.00965291\n",
      "Epoch 1 | Step 197900 | Avg Loss: 0.0150 | Grad Norm: 0.01274711\n",
      "Epoch 1 | Step 198000 | Avg Loss: 0.0148 | Grad Norm: 0.00935173\n",
      "Epoch 1 | Step 198100 | Avg Loss: 0.0143 | Grad Norm: 0.00983859\n",
      "Epoch 1 | Step 198200 | Avg Loss: 0.0144 | Grad Norm: 0.01056508\n",
      "Epoch 1 | Step 198300 | Avg Loss: 0.0142 | Grad Norm: 0.00926902\n",
      "Epoch 1 | Step 198400 | Avg Loss: 0.0143 | Grad Norm: 0.00929739\n",
      "Epoch 1 | Step 198500 | Avg Loss: 0.0144 | Grad Norm: 0.01027109\n",
      "Epoch 1 | Step 198600 | Avg Loss: 0.0145 | Grad Norm: 0.00958749\n",
      "Epoch 1 | Step 198700 | Avg Loss: 0.0144 | Grad Norm: 0.01161356\n",
      "Epoch 1 | Step 198800 | Avg Loss: 0.0144 | Grad Norm: 0.01029292\n",
      "Epoch 1 | Step 198900 | Avg Loss: 0.0144 | Grad Norm: 0.00889894\n",
      "Epoch 1 | Step 199000 | Avg Loss: 0.0144 | Grad Norm: 0.00929883\n",
      "Epoch 1 | Step 199100 | Avg Loss: 0.0143 | Grad Norm: 0.00964078\n",
      "Epoch 1 | Step 199200 | Avg Loss: 0.0146 | Grad Norm: 0.01197426\n",
      "Epoch 1 | Step 199300 | Avg Loss: 0.0146 | Grad Norm: 0.00868916\n",
      "Epoch 1 | Step 199400 | Avg Loss: 0.0149 | Grad Norm: 0.01048577\n",
      "Epoch 1 | Step 199500 | Avg Loss: 0.0148 | Grad Norm: 0.00997056\n",
      "Epoch 1 | Step 199600 | Avg Loss: 0.0150 | Grad Norm: 0.01038305\n",
      "Epoch 1 | Step 199700 | Avg Loss: 0.0149 | Grad Norm: 0.01055418\n",
      "Epoch 1 | Step 199800 | Avg Loss: 0.0146 | Grad Norm: 0.00988195\n",
      "Epoch 1 | Step 199900 | Avg Loss: 0.0140 | Grad Norm: 0.01055468\n",
      "Epoch 1 | Step 200000 | Avg Loss: 0.0143 | Grad Norm: 0.01100702\n",
      "Saving model at step200000\n",
      "Epoch 1 | Step 200100 | Avg Loss: 0.0140 | Grad Norm: 0.00884846\n",
      "Epoch 1 | Step 200200 | Avg Loss: 0.0140 | Grad Norm: 0.00975826\n",
      "Epoch 1 | Step 200300 | Avg Loss: 0.0140 | Grad Norm: 0.00889699\n",
      "Epoch 1 | Step 200400 | Avg Loss: 0.0142 | Grad Norm: 0.01017747\n",
      "Epoch 1 | Step 200500 | Avg Loss: 0.0145 | Grad Norm: 0.01083807\n",
      "Epoch 1 | Step 200600 | Avg Loss: 0.0143 | Grad Norm: 0.01069586\n",
      "Epoch 1 | Step 200700 | Avg Loss: 0.0144 | Grad Norm: 0.01004849\n",
      "Epoch 1 | Step 200800 | Avg Loss: 0.0147 | Grad Norm: 0.00977899\n",
      "Epoch 1 | Step 200900 | Avg Loss: 0.0146 | Grad Norm: 0.01070083\n",
      "Epoch 1 | Step 201000 | Avg Loss: 0.0145 | Grad Norm: 0.00968968\n",
      "Epoch 1 | Step 201100 | Avg Loss: 0.0145 | Grad Norm: 0.00908585\n",
      "Epoch 1 | Step 201200 | Avg Loss: 0.0146 | Grad Norm: 0.00929214\n",
      "Epoch 1 | Step 201300 | Avg Loss: 0.0144 | Grad Norm: 0.01054511\n",
      "Epoch 1 | Step 201400 | Avg Loss: 0.0142 | Grad Norm: 0.01114995\n",
      "Epoch 1 | Step 201500 | Avg Loss: 0.0141 | Grad Norm: 0.01206942\n",
      "Epoch 1 | Step 201600 | Avg Loss: 0.0145 | Grad Norm: 0.00942524\n",
      "Epoch 1 | Step 201700 | Avg Loss: 0.0142 | Grad Norm: 0.01106851\n",
      "Epoch 1 | Step 201800 | Avg Loss: 0.0140 | Grad Norm: 0.01022946\n",
      "Epoch 1 | Step 201900 | Avg Loss: 0.0141 | Grad Norm: 0.01154892\n",
      "Epoch 1 | Step 202000 | Avg Loss: 0.0142 | Grad Norm: 0.01047052\n",
      "Epoch 1 | Step 202100 | Avg Loss: 0.0142 | Grad Norm: 0.00908354\n",
      "Epoch 1 | Step 202200 | Avg Loss: 0.0143 | Grad Norm: 0.01131047\n",
      "Epoch 1 | Step 202300 | Avg Loss: 0.0140 | Grad Norm: 0.00877451\n",
      "Epoch 1 | Step 202400 | Avg Loss: 0.0140 | Grad Norm: 0.00950066\n",
      "Epoch 1 | Step 202500 | Avg Loss: 0.0143 | Grad Norm: 0.01069256\n",
      "Epoch 1 | Step 202600 | Avg Loss: 0.0144 | Grad Norm: 0.01079344\n",
      "Epoch 1 | Step 202700 | Avg Loss: 0.0145 | Grad Norm: 0.01045316\n",
      "Epoch 1 | Step 202800 | Avg Loss: 0.0153 | Grad Norm: 0.01176732\n",
      "Epoch 1 | Step 202900 | Avg Loss: 0.0146 | Grad Norm: 0.01081751\n",
      "Epoch 1 | Step 203000 | Avg Loss: 0.0145 | Grad Norm: 0.01050338\n",
      "Epoch 1 | Step 203100 | Avg Loss: 0.0144 | Grad Norm: 0.00956223\n",
      "Epoch 1 | Step 203200 | Avg Loss: 0.0145 | Grad Norm: 0.00883130\n",
      "Epoch 1 | Step 203300 | Avg Loss: 0.0141 | Grad Norm: 0.00967237\n",
      "Epoch 1 | Step 203400 | Avg Loss: 0.0144 | Grad Norm: 0.00973635\n",
      "Epoch 1 | Step 203500 | Avg Loss: 0.0141 | Grad Norm: 0.00850282\n",
      "Epoch 1 | Step 203600 | Avg Loss: 0.0144 | Grad Norm: 0.00934159\n",
      "Epoch 1 | Step 203700 | Avg Loss: 0.0146 | Grad Norm: 0.01021959\n",
      "Epoch 1 | Step 203800 | Avg Loss: 0.0148 | Grad Norm: 0.01031914\n",
      "Epoch 1 | Step 203900 | Avg Loss: 0.0144 | Grad Norm: 0.01037842\n",
      "Epoch 1 | Step 204000 | Avg Loss: 0.0143 | Grad Norm: 0.01042896\n",
      "Epoch 1 | Step 204100 | Avg Loss: 0.0146 | Grad Norm: 0.01270240\n",
      "Epoch 1 | Step 204200 | Avg Loss: 0.0146 | Grad Norm: 0.00984167\n",
      "Epoch 1 | Step 204300 | Avg Loss: 0.0145 | Grad Norm: 0.00922904\n",
      "Epoch 1 | Step 204400 | Avg Loss: 0.0144 | Grad Norm: 0.01008650\n",
      "Epoch 1 | Step 204500 | Avg Loss: 0.0143 | Grad Norm: 0.00924521\n",
      "Epoch 1 | Step 204600 | Avg Loss: 0.0146 | Grad Norm: 0.01040903\n",
      "Epoch 1 | Step 204700 | Avg Loss: 0.0148 | Grad Norm: 0.01107283\n",
      "Epoch 1 | Step 204800 | Avg Loss: 0.0143 | Grad Norm: 0.01017621\n",
      "Epoch 1 | Step 204900 | Avg Loss: 0.0144 | Grad Norm: 0.01003155\n",
      "Epoch 1 | Step 205000 | Avg Loss: 0.0143 | Grad Norm: 0.00972775\n",
      "Epoch 1 | Step 205100 | Avg Loss: 0.0144 | Grad Norm: 0.01078706\n",
      "Epoch 1 | Step 205200 | Avg Loss: 0.0141 | Grad Norm: 0.01075601\n",
      "Epoch 1 | Step 205300 | Avg Loss: 0.0144 | Grad Norm: 0.00923485\n",
      "Epoch 1 | Step 205400 | Avg Loss: 0.0147 | Grad Norm: 0.00984561\n",
      "Epoch 1 | Step 205500 | Avg Loss: 0.0145 | Grad Norm: 0.00942168\n",
      "Epoch 1 | Step 205600 | Avg Loss: 0.0140 | Grad Norm: 0.01062615\n",
      "Epoch 1 | Step 205700 | Avg Loss: 0.0144 | Grad Norm: 0.00973041\n",
      "Epoch 1 | Step 205800 | Avg Loss: 0.0148 | Grad Norm: 0.01158900\n",
      "Epoch 1 | Step 205900 | Avg Loss: 0.0148 | Grad Norm: 0.01154258\n",
      "Epoch 1 | Step 206000 | Avg Loss: 0.0146 | Grad Norm: 0.00980265\n",
      "Epoch 1 | Step 206100 | Avg Loss: 0.0146 | Grad Norm: 0.00843519\n",
      "Epoch 1 | Step 206200 | Avg Loss: 0.0143 | Grad Norm: 0.00851345\n",
      "Epoch 1 | Step 206300 | Avg Loss: 0.0147 | Grad Norm: 0.01071238\n",
      "Epoch 1 | Step 206400 | Avg Loss: 0.0145 | Grad Norm: 0.00909577\n",
      "Epoch 1 | Step 206500 | Avg Loss: 0.0147 | Grad Norm: 0.00937332\n",
      "Epoch 1 | Step 206600 | Avg Loss: 0.0147 | Grad Norm: 0.00981074\n",
      "Epoch 1 | Step 206700 | Avg Loss: 0.0150 | Grad Norm: 0.01031383\n",
      "Epoch 1 | Step 206800 | Avg Loss: 0.0151 | Grad Norm: 0.00973165\n",
      "Epoch 1 | Step 206900 | Avg Loss: 0.0145 | Grad Norm: 0.01052982\n",
      "Epoch 1 | Step 207000 | Avg Loss: 0.0145 | Grad Norm: 0.01014726\n",
      "Epoch 1 | Step 207100 | Avg Loss: 0.0146 | Grad Norm: 0.00935907\n",
      "Epoch 1 | Step 207200 | Avg Loss: 0.0145 | Grad Norm: 0.00986121\n",
      "Epoch 1 | Step 207300 | Avg Loss: 0.0146 | Grad Norm: 0.00870289\n",
      "Epoch 1 | Step 207400 | Avg Loss: 0.0147 | Grad Norm: 0.01001472\n",
      "Epoch 1 | Step 207500 | Avg Loss: 0.0146 | Grad Norm: 0.01006987\n",
      "Epoch 1 | Step 207600 | Avg Loss: 0.0140 | Grad Norm: 0.00915034\n",
      "Epoch 1 | Step 207700 | Avg Loss: 0.0142 | Grad Norm: 0.01013052\n",
      "Epoch 1 | Step 207800 | Avg Loss: 0.0145 | Grad Norm: 0.00991041\n",
      "Epoch 1 | Step 207900 | Avg Loss: 0.0145 | Grad Norm: 0.00935853\n",
      "Epoch 1 | Step 208000 | Avg Loss: 0.0145 | Grad Norm: 0.01008230\n",
      "Epoch 1 | Step 208100 | Avg Loss: 0.0146 | Grad Norm: 0.01064070\n",
      "Epoch 1 | Step 208200 | Avg Loss: 0.0147 | Grad Norm: 0.00937779\n",
      "Epoch 1 | Step 208300 | Avg Loss: 0.0146 | Grad Norm: 0.00977779\n",
      "Epoch 1 | Step 208400 | Avg Loss: 0.0147 | Grad Norm: 0.01079177\n",
      "Epoch 1 | Step 208500 | Avg Loss: 0.0148 | Grad Norm: 0.00923389\n",
      "Epoch 1 | Step 208600 | Avg Loss: 0.0150 | Grad Norm: 0.01198557\n",
      "Epoch 1 | Step 208700 | Avg Loss: 0.0147 | Grad Norm: 0.01009371\n",
      "Epoch 1 | Step 208800 | Avg Loss: 0.0144 | Grad Norm: 0.00964144\n",
      "Epoch 1 | Step 208900 | Avg Loss: 0.0148 | Grad Norm: 0.01161036\n",
      "Epoch 1 | Step 209000 | Avg Loss: 0.0147 | Grad Norm: 0.00954820\n",
      "Epoch 1 | Step 209100 | Avg Loss: 0.0149 | Grad Norm: 0.01058513\n",
      "Epoch 1 | Step 209200 | Avg Loss: 0.0150 | Grad Norm: 0.01363814\n",
      "Epoch 1 | Step 209300 | Avg Loss: 0.0148 | Grad Norm: 0.01202478\n",
      "Epoch 1 | Step 209400 | Avg Loss: 0.0150 | Grad Norm: 0.00926889\n",
      "Epoch 1 | Step 209500 | Avg Loss: 0.0147 | Grad Norm: 0.01459127\n",
      "Epoch 1 | Step 209600 | Avg Loss: 0.0141 | Grad Norm: 0.00954409\n",
      "Epoch 1 | Step 209700 | Avg Loss: 0.0140 | Grad Norm: 0.00939610\n",
      "Epoch 1 | Step 209800 | Avg Loss: 0.0143 | Grad Norm: 0.00878008\n",
      "Epoch 1 | Step 209900 | Avg Loss: 0.0145 | Grad Norm: 0.01203561\n",
      "Epoch 1 | Step 210000 | Avg Loss: 0.0143 | Grad Norm: 0.01051250\n",
      "Epoch 1 | Step 210100 | Avg Loss: 0.0141 | Grad Norm: 0.00962222\n",
      "Epoch 1 | Step 210200 | Avg Loss: 0.0149 | Grad Norm: 0.01000013\n",
      "Epoch 1 | Step 210300 | Avg Loss: 0.0144 | Grad Norm: 0.01087336\n",
      "Epoch 1 | Step 210400 | Avg Loss: 0.0143 | Grad Norm: 0.00852135\n",
      "Epoch 1 | Step 210500 | Avg Loss: 0.0143 | Grad Norm: 0.01045610\n",
      "Epoch 1 | Step 210600 | Avg Loss: 0.0147 | Grad Norm: 0.01072398\n",
      "Epoch 1 | Step 210700 | Avg Loss: 0.0143 | Grad Norm: 0.01133031\n",
      "Epoch 1 | Step 210800 | Avg Loss: 0.0145 | Grad Norm: 0.00954732\n",
      "Epoch 1 | Step 210900 | Avg Loss: 0.0143 | Grad Norm: 0.01077893\n",
      "Epoch 1 | Step 211000 | Avg Loss: 0.0136 | Grad Norm: 0.00973267\n",
      "Epoch 1 | Step 211100 | Avg Loss: 0.0137 | Grad Norm: 0.00987228\n",
      "Epoch 1 | Step 211200 | Avg Loss: 0.0139 | Grad Norm: 0.01110140\n",
      "Epoch 1 | Step 211300 | Avg Loss: 0.0139 | Grad Norm: 0.00872054\n",
      "Epoch 1 | Step 211400 | Avg Loss: 0.0140 | Grad Norm: 0.00961806\n",
      "Epoch 1 | Step 211500 | Avg Loss: 0.0142 | Grad Norm: 0.00937397\n",
      "Epoch 1 | Step 211600 | Avg Loss: 0.0140 | Grad Norm: 0.00901018\n",
      "Epoch 1 | Step 211700 | Avg Loss: 0.0142 | Grad Norm: 0.00905321\n",
      "Epoch 1 | Step 211800 | Avg Loss: 0.0141 | Grad Norm: 0.00874980\n",
      "Epoch 1 | Step 211900 | Avg Loss: 0.0143 | Grad Norm: 0.01078952\n",
      "Epoch 1 | Step 212000 | Avg Loss: 0.0150 | Grad Norm: 0.01015613\n",
      "Epoch 1 | Step 212100 | Avg Loss: 0.0150 | Grad Norm: 0.00906045\n",
      "Epoch 1 | Step 212200 | Avg Loss: 0.0148 | Grad Norm: 0.01053863\n",
      "Epoch 1 | Step 212300 | Avg Loss: 0.0144 | Grad Norm: 0.01089501\n",
      "Epoch 1 | Step 212400 | Avg Loss: 0.0145 | Grad Norm: 0.00969878\n",
      "Epoch 1 | Step 212500 | Avg Loss: 0.0144 | Grad Norm: 0.00998327\n",
      "Epoch 1 | Step 212600 | Avg Loss: 0.0146 | Grad Norm: 0.00957652\n",
      "Epoch 1 | Step 212700 | Avg Loss: 0.0140 | Grad Norm: 0.01123452\n",
      "Epoch 1 | Step 212800 | Avg Loss: 0.0144 | Grad Norm: 0.01052317\n",
      "Epoch 1 | Step 212900 | Avg Loss: 0.0144 | Grad Norm: 0.00869870\n",
      "Epoch 1 | Step 213000 | Avg Loss: 0.0143 | Grad Norm: 0.01046667\n",
      "Epoch 1 | Step 213100 | Avg Loss: 0.0141 | Grad Norm: 0.01125938\n",
      "Epoch 1 | Step 213200 | Avg Loss: 0.0141 | Grad Norm: 0.00912083\n",
      "Epoch 1 | Step 213300 | Avg Loss: 0.0141 | Grad Norm: 0.01041052\n",
      "Epoch 1 | Step 213400 | Avg Loss: 0.0136 | Grad Norm: 0.00953194\n",
      "Epoch 1 | Step 213500 | Avg Loss: 0.0140 | Grad Norm: 0.01042731\n",
      "Epoch 1 | Step 213600 | Avg Loss: 0.0142 | Grad Norm: 0.00996614\n",
      "Epoch 1 | Step 213700 | Avg Loss: 0.0145 | Grad Norm: 0.00995980\n",
      "Epoch 1 | Step 213800 | Avg Loss: 0.0145 | Grad Norm: 0.00908230\n",
      "Epoch 1 | Step 213900 | Avg Loss: 0.0142 | Grad Norm: 0.00933554\n",
      "Epoch 1 | Step 214000 | Avg Loss: 0.0144 | Grad Norm: 0.00991854\n",
      "Epoch 1 | Step 214100 | Avg Loss: 0.0147 | Grad Norm: 0.01023615\n",
      "Epoch 1 | Step 214200 | Avg Loss: 0.0147 | Grad Norm: 0.01175863\n",
      "Epoch 1 | Step 214300 | Avg Loss: 0.0150 | Grad Norm: 0.00923162\n",
      "Epoch 1 | Step 214400 | Avg Loss: 0.0148 | Grad Norm: 0.01051003\n",
      "Epoch 1 | Step 214500 | Avg Loss: 0.0144 | Grad Norm: 0.01001066\n",
      "Epoch 1 | Step 214600 | Avg Loss: 0.0138 | Grad Norm: 0.00942802\n",
      "Epoch 1 | Step 214700 | Avg Loss: 0.0140 | Grad Norm: 0.01024082\n",
      "Epoch 1 | Step 214800 | Avg Loss: 0.0137 | Grad Norm: 0.01019777\n",
      "Epoch 1 | Step 214900 | Avg Loss: 0.0144 | Grad Norm: 0.01256981\n",
      "Epoch 1 | Step 215000 | Avg Loss: 0.0144 | Grad Norm: 0.01024622\n",
      "Epoch 1 | Step 215100 | Avg Loss: 0.0146 | Grad Norm: 0.01064597\n",
      "Epoch 1 | Step 215200 | Avg Loss: 0.0142 | Grad Norm: 0.00996135\n",
      "Epoch 1 | Step 215300 | Avg Loss: 0.0144 | Grad Norm: 0.00991975\n",
      "Epoch 1 | Step 215400 | Avg Loss: 0.0141 | Grad Norm: 0.01057046\n",
      "Epoch 1 | Step 215500 | Avg Loss: 0.0144 | Grad Norm: 0.01016028\n",
      "Epoch 1 | Step 215600 | Avg Loss: 0.0144 | Grad Norm: 0.00994083\n",
      "Epoch 1 | Step 215700 | Avg Loss: 0.0142 | Grad Norm: 0.00861266\n",
      "Epoch 1 | Step 215800 | Avg Loss: 0.0141 | Grad Norm: 0.00926382\n",
      "Epoch 1 | Step 215900 | Avg Loss: 0.0138 | Grad Norm: 0.01043863\n",
      "Epoch 1 | Step 216000 | Avg Loss: 0.0138 | Grad Norm: 0.00836514\n",
      "Epoch 1 | Step 216100 | Avg Loss: 0.0141 | Grad Norm: 0.01181675\n",
      "Epoch 1 | Step 216200 | Avg Loss: 0.0143 | Grad Norm: 0.01032983\n",
      "Epoch 1 | Step 216300 | Avg Loss: 0.0143 | Grad Norm: 0.01027858\n",
      "Epoch 1 | Step 216400 | Avg Loss: 0.0141 | Grad Norm: 0.01056315\n",
      "Epoch 1 | Step 216500 | Avg Loss: 0.0140 | Grad Norm: 0.00979104\n",
      "Epoch 1 | Step 216600 | Avg Loss: 0.0142 | Grad Norm: 0.00951677\n",
      "Epoch 1 | Step 216700 | Avg Loss: 0.0144 | Grad Norm: 0.01245153\n",
      "Epoch 1 | Step 216800 | Avg Loss: 0.0144 | Grad Norm: 0.01034016\n",
      "Epoch 1 | Step 216900 | Avg Loss: 0.0143 | Grad Norm: 0.00962452\n",
      "Epoch 1 | Step 217000 | Avg Loss: 0.0144 | Grad Norm: 0.01011595\n",
      "Epoch 1 | Step 217100 | Avg Loss: 0.0145 | Grad Norm: 0.01044466\n",
      "Epoch 1 | Step 217200 | Avg Loss: 0.0147 | Grad Norm: 0.00940606\n",
      "Epoch 1 | Step 217300 | Avg Loss: 0.0149 | Grad Norm: 0.00883976\n",
      "Epoch 1 | Step 217400 | Avg Loss: 0.0147 | Grad Norm: 0.00920715\n",
      "Epoch 1 | Step 217500 | Avg Loss: 0.0148 | Grad Norm: 0.01061625\n",
      "Epoch 1 | Step 217600 | Avg Loss: 0.0150 | Grad Norm: 0.01167666\n",
      "Epoch 1 | Step 217700 | Avg Loss: 0.0147 | Grad Norm: 0.00966435\n",
      "Epoch 1 | Step 217800 | Avg Loss: 0.0151 | Grad Norm: 0.00903980\n",
      "Epoch 1 | Step 217900 | Avg Loss: 0.0150 | Grad Norm: 0.01027053\n",
      "Epoch 1 | Step 218000 | Avg Loss: 0.0149 | Grad Norm: 0.01079025\n",
      "Epoch 1 | Step 218100 | Avg Loss: 0.0144 | Grad Norm: 0.00918956\n",
      "Epoch 1 | Step 218200 | Avg Loss: 0.0145 | Grad Norm: 0.01032656\n",
      "Epoch 1 | Step 218300 | Avg Loss: 0.0145 | Grad Norm: 0.01057632\n",
      "Epoch 1 | Step 218400 | Avg Loss: 0.0146 | Grad Norm: 0.00892831\n",
      "Epoch 1 | Step 218500 | Avg Loss: 0.0144 | Grad Norm: 0.01072573\n",
      "Epoch 1 | Step 218600 | Avg Loss: 0.0141 | Grad Norm: 0.00913476\n",
      "Epoch 1 | Step 218700 | Avg Loss: 0.0141 | Grad Norm: 0.01022532\n",
      "Epoch 1 | Step 218800 | Avg Loss: 0.0142 | Grad Norm: 0.00859518\n",
      "Epoch 1 | Step 218900 | Avg Loss: 0.0146 | Grad Norm: 0.00903939\n",
      "Epoch 1 | Step 219000 | Avg Loss: 0.0144 | Grad Norm: 0.00957079\n",
      "Epoch 1 | Step 219100 | Avg Loss: 0.0148 | Grad Norm: 0.01016822\n",
      "Epoch 1 | Step 219200 | Avg Loss: 0.0148 | Grad Norm: 0.00973286\n",
      "Epoch 1 | Step 219300 | Avg Loss: 0.0146 | Grad Norm: 0.00950999\n",
      "Epoch 1 | Step 219400 | Avg Loss: 0.0149 | Grad Norm: 0.01080975\n",
      "Epoch 1 | Step 219500 | Avg Loss: 0.0144 | Grad Norm: 0.01063641\n",
      "Epoch 1 | Step 219600 | Avg Loss: 0.0144 | Grad Norm: 0.00910651\n",
      "Epoch 1 | Step 219700 | Avg Loss: 0.0143 | Grad Norm: 0.00966912\n",
      "Epoch 1 | Step 219800 | Avg Loss: 0.0147 | Grad Norm: 0.00944677\n",
      "Epoch 1 | Step 219900 | Avg Loss: 0.0148 | Grad Norm: 0.01080980\n",
      "Epoch 1 | Step 220000 | Avg Loss: 0.0145 | Grad Norm: 0.00948604\n",
      "Epoch 1 | Step 220100 | Avg Loss: 0.0146 | Grad Norm: 0.01011493\n",
      "Epoch 1 | Step 220200 | Avg Loss: 0.0142 | Grad Norm: 0.00952259\n",
      "Epoch 1 | Step 220300 | Avg Loss: 0.0145 | Grad Norm: 0.00997875\n",
      "Epoch 1 | Step 220400 | Avg Loss: 0.0145 | Grad Norm: 0.01111366\n",
      "Epoch 1 | Step 220500 | Avg Loss: 0.0144 | Grad Norm: 0.01141015\n",
      "Epoch 1 | Step 220600 | Avg Loss: 0.0141 | Grad Norm: 0.01174149\n",
      "Epoch 1 | Step 220700 | Avg Loss: 0.0144 | Grad Norm: 0.00959391\n",
      "Epoch 1 | Step 220800 | Avg Loss: 0.0140 | Grad Norm: 0.00921764\n",
      "Epoch 1 | Step 220900 | Avg Loss: 0.0140 | Grad Norm: 0.00813586\n",
      "Epoch 1 | Step 221000 | Avg Loss: 0.0141 | Grad Norm: 0.01141464\n",
      "Epoch 1 | Step 221100 | Avg Loss: 0.0142 | Grad Norm: 0.00963245\n",
      "Epoch 1 | Step 221200 | Avg Loss: 0.0143 | Grad Norm: 0.01147316\n",
      "Epoch 1 | Step 221300 | Avg Loss: 0.0143 | Grad Norm: 0.01110897\n",
      "Epoch 1 | Step 221400 | Avg Loss: 0.0146 | Grad Norm: 0.01108186\n",
      "Epoch 1 | Step 221500 | Avg Loss: 0.0147 | Grad Norm: 0.01081457\n",
      "Epoch 1 | Step 221600 | Avg Loss: 0.0144 | Grad Norm: 0.00949445\n",
      "Epoch 1 | Step 221700 | Avg Loss: 0.0146 | Grad Norm: 0.00928946\n",
      "Epoch 1 | Step 221800 | Avg Loss: 0.0152 | Grad Norm: 0.00971347\n",
      "Epoch 1 | Step 221900 | Avg Loss: 0.0145 | Grad Norm: 0.01344397\n",
      "Epoch 1 | Step 222000 | Avg Loss: 0.0147 | Grad Norm: 0.00967014\n",
      "Epoch 1 | Step 222100 | Avg Loss: 0.0143 | Grad Norm: 0.01033336\n",
      "Epoch 1 | Step 222200 | Avg Loss: 0.0141 | Grad Norm: 0.00927493\n",
      "Epoch 1 | Step 222300 | Avg Loss: 0.0144 | Grad Norm: 0.00836341\n",
      "Epoch 1 | Step 222400 | Avg Loss: 0.0146 | Grad Norm: 0.00930396\n",
      "Epoch 1 | Step 222500 | Avg Loss: 0.0145 | Grad Norm: 0.00922033\n",
      "Epoch 1 | Step 222600 | Avg Loss: 0.0140 | Grad Norm: 0.00919386\n",
      "Epoch 1 | Step 222700 | Avg Loss: 0.0141 | Grad Norm: 0.00874658\n",
      "Epoch 1 | Step 222800 | Avg Loss: 0.0145 | Grad Norm: 0.00958738\n",
      "Epoch 1 | Step 222900 | Avg Loss: 0.0145 | Grad Norm: 0.01019864\n",
      "Epoch 1 | Step 223000 | Avg Loss: 0.0147 | Grad Norm: 0.00862440\n",
      "Epoch 1 | Step 223100 | Avg Loss: 0.0147 | Grad Norm: 0.00849168\n",
      "Epoch 1 | Step 223200 | Avg Loss: 0.0144 | Grad Norm: 0.00937691\n",
      "Epoch 1 | Step 223300 | Avg Loss: 0.0141 | Grad Norm: 0.00971815\n",
      "Epoch 1 | Step 223400 | Avg Loss: 0.0142 | Grad Norm: 0.00936105\n",
      "Epoch 1 | Step 223500 | Avg Loss: 0.0140 | Grad Norm: 0.00944935\n",
      "Epoch 1 | Step 223600 | Avg Loss: 0.0143 | Grad Norm: 0.00973042\n",
      "Epoch 1 | Step 223700 | Avg Loss: 0.0143 | Grad Norm: 0.00858019\n",
      "Epoch 1 | Step 223800 | Avg Loss: 0.0140 | Grad Norm: 0.00898282\n",
      "Epoch 1 | Step 223900 | Avg Loss: 0.0139 | Grad Norm: 0.01182762\n",
      "Epoch 1 | Step 224000 | Avg Loss: 0.0138 | Grad Norm: 0.00915681\n",
      "Epoch 1 | Step 224100 | Avg Loss: 0.0143 | Grad Norm: 0.01122508\n",
      "Epoch 1 | Step 224200 | Avg Loss: 0.0143 | Grad Norm: 0.01048857\n",
      "Epoch 1 | Step 224300 | Avg Loss: 0.0146 | Grad Norm: 0.00970304\n",
      "Epoch 1 | Step 224400 | Avg Loss: 0.0144 | Grad Norm: 0.01065315\n",
      "Epoch 1 | Step 224500 | Avg Loss: 0.0143 | Grad Norm: 0.01146248\n",
      "Epoch 1 | Step 224600 | Avg Loss: 0.0144 | Grad Norm: 0.01222660\n",
      "Epoch 1 | Step 224700 | Avg Loss: 0.0148 | Grad Norm: 0.01035898\n",
      "Epoch 1 | Step 224800 | Avg Loss: 0.0148 | Grad Norm: 0.00924630\n",
      "Epoch 1 | Step 224900 | Avg Loss: 0.0149 | Grad Norm: 0.01121389\n",
      "Epoch 1 | Step 225000 | Avg Loss: 0.0151 | Grad Norm: 0.01079477\n",
      "Epoch 1 | Step 225100 | Avg Loss: 0.0145 | Grad Norm: 0.00995880\n",
      "Epoch 1 | Step 225200 | Avg Loss: 0.0142 | Grad Norm: 0.00952959\n",
      "Epoch 1 | Step 225300 | Avg Loss: 0.0141 | Grad Norm: 0.00917331\n",
      "Epoch 1 | Step 225400 | Avg Loss: 0.0144 | Grad Norm: 0.00915983\n",
      "Epoch 1 | Step 225500 | Avg Loss: 0.0142 | Grad Norm: 0.00974346\n",
      "Epoch 1 | Step 225600 | Avg Loss: 0.0143 | Grad Norm: 0.00871750\n",
      "Epoch 1 | Step 225700 | Avg Loss: 0.0146 | Grad Norm: 0.00943915\n",
      "Epoch 1 | Step 225800 | Avg Loss: 0.0146 | Grad Norm: 0.00901208\n",
      "Epoch 1 | Step 225900 | Avg Loss: 0.0147 | Grad Norm: 0.01012379\n",
      "Epoch 1 | Step 226000 | Avg Loss: 0.0144 | Grad Norm: 0.00894741\n",
      "Epoch 1 | Step 226100 | Avg Loss: 0.0144 | Grad Norm: 0.01092910\n",
      "Epoch 1 | Step 226200 | Avg Loss: 0.0148 | Grad Norm: 0.01014052\n",
      "Epoch 1 | Step 226300 | Avg Loss: 0.0143 | Grad Norm: 0.01002002\n",
      "Epoch 1 | Step 226400 | Avg Loss: 0.0143 | Grad Norm: 0.01002584\n",
      "Epoch 1 | Step 226500 | Avg Loss: 0.0147 | Grad Norm: 0.01061219\n",
      "Epoch 1 | Step 226600 | Avg Loss: 0.0146 | Grad Norm: 0.00917632\n",
      "Epoch 1 | Step 226700 | Avg Loss: 0.0142 | Grad Norm: 0.01343011\n",
      "Epoch 1 | Step 226800 | Avg Loss: 0.0140 | Grad Norm: 0.00962800\n",
      "Epoch 1 | Step 226900 | Avg Loss: 0.0137 | Grad Norm: 0.01077273\n",
      "Epoch 1 | Step 227000 | Avg Loss: 0.0138 | Grad Norm: 0.01054798\n",
      "Epoch 1 | Step 227100 | Avg Loss: 0.0142 | Grad Norm: 0.00923350\n",
      "Epoch 1 | Step 227200 | Avg Loss: 0.0142 | Grad Norm: 0.00985390\n",
      "Epoch 1 | Step 227300 | Avg Loss: 0.0143 | Grad Norm: 0.00962770\n",
      "Epoch 1 | Step 227400 | Avg Loss: 0.0147 | Grad Norm: 0.01107861\n",
      "Epoch 1 | Step 227500 | Avg Loss: 0.0148 | Grad Norm: 0.00917668\n",
      "Epoch 1 | Step 227600 | Avg Loss: 0.0150 | Grad Norm: 0.01207387\n",
      "Epoch 1 | Step 227700 | Avg Loss: 0.0147 | Grad Norm: 0.01047799\n",
      "Epoch 1 | Step 227800 | Avg Loss: 0.0145 | Grad Norm: 0.00955448\n",
      "Epoch 1 | Step 227900 | Avg Loss: 0.0143 | Grad Norm: 0.01079703\n",
      "Epoch 1 | Step 228000 | Avg Loss: 0.0146 | Grad Norm: 0.00953825\n",
      "Epoch 1 | Step 228100 | Avg Loss: 0.0144 | Grad Norm: 0.01024443\n",
      "Epoch 1 | Step 228200 | Avg Loss: 0.0144 | Grad Norm: 0.01041380\n",
      "Epoch 1 | Step 228300 | Avg Loss: 0.0148 | Grad Norm: 0.01104704\n",
      "Epoch 1 | Step 228400 | Avg Loss: 0.0147 | Grad Norm: 0.01049093\n",
      "Epoch 1 | Step 228500 | Avg Loss: 0.0146 | Grad Norm: 0.00887701\n",
      "Epoch 1 | Step 228600 | Avg Loss: 0.0146 | Grad Norm: 0.01163770\n",
      "Epoch 1 | Step 228700 | Avg Loss: 0.0147 | Grad Norm: 0.00973442\n",
      "Epoch 1 | Step 228800 | Avg Loss: 0.0144 | Grad Norm: 0.00972887\n",
      "Epoch 1 | Step 228900 | Avg Loss: 0.0141 | Grad Norm: 0.01051156\n",
      "Epoch 1 | Step 229000 | Avg Loss: 0.0141 | Grad Norm: 0.00833102\n",
      "Epoch 1 | Step 229100 | Avg Loss: 0.0144 | Grad Norm: 0.00931023\n",
      "Epoch 1 | Step 229200 | Avg Loss: 0.0145 | Grad Norm: 0.00990746\n",
      "Epoch 1 | Step 229300 | Avg Loss: 0.0147 | Grad Norm: 0.01024656\n",
      "Epoch 1 | Step 229400 | Avg Loss: 0.0143 | Grad Norm: 0.00977489\n",
      "Epoch 1 | Step 229500 | Avg Loss: 0.0142 | Grad Norm: 0.00975399\n",
      "Epoch 1 | Step 229600 | Avg Loss: 0.0143 | Grad Norm: 0.01038562\n",
      "Epoch 1 | Step 229700 | Avg Loss: 0.0141 | Grad Norm: 0.00999275\n",
      "Epoch 1 | Step 229800 | Avg Loss: 0.0141 | Grad Norm: 0.00927605\n",
      "Epoch 1 | Step 229900 | Avg Loss: 0.0140 | Grad Norm: 0.01110128\n",
      "Epoch 1 | Step 230000 | Avg Loss: 0.0139 | Grad Norm: 0.00936196\n",
      "Epoch 1 | Step 230100 | Avg Loss: 0.0140 | Grad Norm: 0.01174580\n",
      "Epoch 1 | Step 230200 | Avg Loss: 0.0139 | Grad Norm: 0.00948880\n",
      "Epoch 1 | Step 230300 | Avg Loss: 0.0136 | Grad Norm: 0.00978849\n",
      "Epoch 1 | Step 230400 | Avg Loss: 0.0136 | Grad Norm: 0.00794345\n",
      "Epoch 1 | Step 230500 | Avg Loss: 0.0135 | Grad Norm: 0.00943115\n",
      "Epoch 1 | Step 230600 | Avg Loss: 0.0141 | Grad Norm: 0.00865306\n",
      "Epoch 1 | Step 230700 | Avg Loss: 0.0140 | Grad Norm: 0.00948118\n",
      "Epoch 1 | Step 230800 | Avg Loss: 0.0141 | Grad Norm: 0.00871950\n",
      "Epoch 1 | Step 230900 | Avg Loss: 0.0142 | Grad Norm: 0.00985660\n",
      "Epoch 1 | Step 231000 | Avg Loss: 0.0141 | Grad Norm: 0.00878264\n",
      "Epoch 1 | Step 231100 | Avg Loss: 0.0141 | Grad Norm: 0.00802849\n",
      "Epoch 1 | Step 231200 | Avg Loss: 0.0144 | Grad Norm: 0.01023906\n",
      "Epoch 1 | Step 231300 | Avg Loss: 0.0145 | Grad Norm: 0.01321316\n",
      "Epoch 1 | Step 231400 | Avg Loss: 0.0144 | Grad Norm: 0.00830994\n",
      "Epoch 1 | Step 231500 | Avg Loss: 0.0145 | Grad Norm: 0.01122647\n",
      "Epoch 1 | Step 231600 | Avg Loss: 0.0143 | Grad Norm: 0.00964531\n",
      "Epoch 1 | Step 231700 | Avg Loss: 0.0147 | Grad Norm: 0.01127397\n",
      "Epoch 1 | Step 231800 | Avg Loss: 0.0145 | Grad Norm: 0.00874168\n",
      "Epoch 1 | Step 231900 | Avg Loss: 0.0146 | Grad Norm: 0.01044685\n",
      "Epoch 1 | Step 232000 | Avg Loss: 0.0145 | Grad Norm: 0.00894884\n",
      "Epoch 1 | Step 232100 | Avg Loss: 0.0147 | Grad Norm: 0.00900182\n",
      "Epoch 1 | Step 232200 | Avg Loss: 0.0146 | Grad Norm: 0.01097690\n",
      "Epoch 1 | Step 232300 | Avg Loss: 0.0144 | Grad Norm: 0.01035923\n",
      "Epoch 1 | Step 232400 | Avg Loss: 0.0142 | Grad Norm: 0.00907661\n",
      "Epoch 1 | Step 232500 | Avg Loss: 0.0141 | Grad Norm: 0.01033690\n",
      "Epoch 1 | Step 232600 | Avg Loss: 0.0142 | Grad Norm: 0.01071270\n",
      "Epoch 1 | Step 232700 | Avg Loss: 0.0146 | Grad Norm: 0.00928424\n",
      "Epoch 1 | Step 232800 | Avg Loss: 0.0144 | Grad Norm: 0.00941272\n",
      "Epoch 1 | Step 232900 | Avg Loss: 0.0146 | Grad Norm: 0.00900630\n",
      "Epoch 1 | Step 233000 | Avg Loss: 0.0148 | Grad Norm: 0.00975714\n",
      "Epoch 1 | Step 233100 | Avg Loss: 0.0147 | Grad Norm: 0.01122299\n",
      "Epoch 1 | Step 233200 | Avg Loss: 0.0149 | Grad Norm: 0.00939970\n",
      "Epoch 1 | Step 233300 | Avg Loss: 0.0144 | Grad Norm: 0.00936974\n",
      "Epoch 1 | Step 233400 | Avg Loss: 0.0141 | Grad Norm: 0.01073668\n",
      "Epoch 1 | Step 233500 | Avg Loss: 0.0146 | Grad Norm: 0.00872674\n",
      "Epoch 1 | Step 233600 | Avg Loss: 0.0144 | Grad Norm: 0.00947997\n",
      "Epoch 1 | Step 233700 | Avg Loss: 0.0143 | Grad Norm: 0.01019677\n",
      "Epoch 1 | Step 233800 | Avg Loss: 0.0141 | Grad Norm: 0.01006420\n",
      "Epoch 1 | Step 233900 | Avg Loss: 0.0141 | Grad Norm: 0.00863909\n",
      "Epoch 1 | Step 234000 | Avg Loss: 0.0145 | Grad Norm: 0.01059647\n",
      "Epoch 1 | Step 234100 | Avg Loss: 0.0144 | Grad Norm: 0.00924173\n",
      "Epoch 1 | Step 234200 | Avg Loss: 0.0148 | Grad Norm: 0.01097867\n",
      "Epoch 1 | Step 234300 | Avg Loss: 0.0144 | Grad Norm: 0.00888440\n",
      "Epoch 1 | Step 234400 | Avg Loss: 0.0144 | Grad Norm: 0.00976408\n",
      "Epoch 1 | Step 234500 | Avg Loss: 0.0144 | Grad Norm: 0.01049574\n",
      "Epoch 1 | Step 234600 | Avg Loss: 0.0144 | Grad Norm: 0.01106702\n",
      "Epoch 1 | Step 234700 | Avg Loss: 0.0143 | Grad Norm: 0.01080425\n",
      "Epoch 1 | Step 234800 | Avg Loss: 0.0145 | Grad Norm: 0.01108955\n",
      "Epoch 1 | Step 234900 | Avg Loss: 0.0146 | Grad Norm: 0.01151667\n",
      "Epoch 1 | Step 235000 | Avg Loss: 0.0145 | Grad Norm: 0.01092173\n",
      "Epoch 1 | Step 235100 | Avg Loss: 0.0146 | Grad Norm: 0.00941031\n",
      "Epoch 1 | Step 235200 | Avg Loss: 0.0144 | Grad Norm: 0.00823537\n",
      "Epoch 1 | Step 235300 | Avg Loss: 0.0143 | Grad Norm: 0.00956977\n",
      "Epoch 1 | Step 235400 | Avg Loss: 0.0143 | Grad Norm: 0.00976780\n",
      "Epoch 1 | Step 235500 | Avg Loss: 0.0141 | Grad Norm: 0.00953303\n",
      "Epoch 1 | Step 235600 | Avg Loss: 0.0143 | Grad Norm: 0.01041234\n",
      "Epoch 1 | Step 235700 | Avg Loss: 0.0143 | Grad Norm: 0.01130874\n",
      "Epoch 1 | Step 235800 | Avg Loss: 0.0143 | Grad Norm: 0.01017701\n",
      "Epoch 1 | Step 235900 | Avg Loss: 0.0143 | Grad Norm: 0.00952048\n",
      "Epoch 1 | Step 236000 | Avg Loss: 0.0146 | Grad Norm: 0.00974031\n",
      "Epoch 1 | Step 236100 | Avg Loss: 0.0146 | Grad Norm: 0.01141800\n",
      "Epoch 1 | Step 236200 | Avg Loss: 0.0142 | Grad Norm: 0.00961433\n",
      "Epoch 1 | Step 236300 | Avg Loss: 0.0140 | Grad Norm: 0.00907366\n",
      "Epoch 1 | Step 236400 | Avg Loss: 0.0143 | Grad Norm: 0.01096091\n",
      "Epoch 1 | Step 236500 | Avg Loss: 0.0142 | Grad Norm: 0.00984234\n",
      "Epoch 1 | Step 236600 | Avg Loss: 0.0138 | Grad Norm: 0.00914880\n",
      "Epoch 1 | Step 236700 | Avg Loss: 0.0139 | Grad Norm: 0.01008951\n",
      "Epoch 1 | Step 236800 | Avg Loss: 0.0141 | Grad Norm: 0.01112611\n",
      "Epoch 1 | Step 236900 | Avg Loss: 0.0143 | Grad Norm: 0.00905944\n",
      "Epoch 1 | Step 237000 | Avg Loss: 0.0144 | Grad Norm: 0.00904498\n",
      "Epoch 1 | Step 237100 | Avg Loss: 0.0143 | Grad Norm: 0.01032499\n",
      "Epoch 1 | Step 237200 | Avg Loss: 0.0148 | Grad Norm: 0.01102535\n",
      "Epoch 1 | Step 237300 | Avg Loss: 0.0146 | Grad Norm: 0.01086266\n",
      "Epoch 1 | Step 237400 | Avg Loss: 0.0147 | Grad Norm: 0.01011883\n",
      "Epoch 1 | Step 237500 | Avg Loss: 0.0147 | Grad Norm: 0.00954170\n",
      "Epoch 1 | Step 237600 | Avg Loss: 0.0146 | Grad Norm: 0.01035998\n",
      "Epoch 1 | Step 237700 | Avg Loss: 0.0143 | Grad Norm: 0.01140789\n",
      "Epoch 1 | Step 237800 | Avg Loss: 0.0144 | Grad Norm: 0.00953492\n",
      "Epoch 1 | Step 237900 | Avg Loss: 0.0143 | Grad Norm: 0.00870097\n",
      "Epoch 1 | Step 238000 | Avg Loss: 0.0146 | Grad Norm: 0.00990500\n",
      "Epoch 1 | Step 238100 | Avg Loss: 0.0146 | Grad Norm: 0.00970223\n",
      "Epoch 1 | Step 238200 | Avg Loss: 0.0149 | Grad Norm: 0.01033047\n",
      "Epoch 1 | Step 238300 | Avg Loss: 0.0145 | Grad Norm: 0.01022034\n",
      "Epoch 1 | Step 238400 | Avg Loss: 0.0145 | Grad Norm: 0.01021842\n",
      "Epoch 1 | Step 238500 | Avg Loss: 0.0144 | Grad Norm: 0.01000424\n",
      "Epoch 1 | Step 238600 | Avg Loss: 0.0145 | Grad Norm: 0.00927519\n",
      "Epoch 1 | Step 238700 | Avg Loss: 0.0146 | Grad Norm: 0.00857639\n",
      "Epoch 1 | Step 238800 | Avg Loss: 0.0147 | Grad Norm: 0.00928236\n",
      "Epoch 1 | Step 238900 | Avg Loss: 0.0143 | Grad Norm: 0.01023490\n",
      "Epoch 1 | Step 239000 | Avg Loss: 0.0144 | Grad Norm: 0.01073178\n",
      "Epoch 1 | Step 239100 | Avg Loss: 0.0142 | Grad Norm: 0.01100171\n",
      "Epoch 1 | Step 239200 | Avg Loss: 0.0139 | Grad Norm: 0.01109286\n",
      "Epoch 1 | Step 239300 | Avg Loss: 0.0141 | Grad Norm: 0.00948001\n",
      "Epoch 1 | Step 239400 | Avg Loss: 0.0143 | Grad Norm: 0.01036620\n",
      "Epoch 1 | Step 239500 | Avg Loss: 0.0145 | Grad Norm: 0.00905690\n",
      "Epoch 1 | Step 239600 | Avg Loss: 0.0145 | Grad Norm: 0.01038352\n",
      "Epoch 1 | Step 239700 | Avg Loss: 0.0143 | Grad Norm: 0.01049050\n",
      "Epoch 1 | Step 239800 | Avg Loss: 0.0142 | Grad Norm: 0.00939111\n",
      "Epoch 1 | Step 239900 | Avg Loss: 0.0143 | Grad Norm: 0.00820440\n",
      "Epoch 1 | Step 240000 | Avg Loss: 0.0143 | Grad Norm: 0.00961274\n",
      "Epoch 1 | Step 240100 | Avg Loss: 0.0141 | Grad Norm: 0.00917588\n",
      "Epoch 1 | Step 240200 | Avg Loss: 0.0141 | Grad Norm: 0.01057201\n",
      "Epoch 1 | Step 240300 | Avg Loss: 0.0147 | Grad Norm: 0.00947641\n",
      "Epoch 1 | Step 240400 | Avg Loss: 0.0148 | Grad Norm: 0.01036706\n",
      "Epoch 1 | Step 240500 | Avg Loss: 0.0146 | Grad Norm: 0.00908814\n",
      "Epoch 1 | Step 240600 | Avg Loss: 0.0146 | Grad Norm: 0.00855393\n",
      "Epoch 1 | Step 240700 | Avg Loss: 0.0146 | Grad Norm: 0.00932004\n",
      "Epoch 1 | Step 240800 | Avg Loss: 0.0146 | Grad Norm: 0.01103190\n",
      "Epoch 1 | Step 240900 | Avg Loss: 0.0147 | Grad Norm: 0.01022809\n",
      "Epoch 1 | Step 241000 | Avg Loss: 0.0145 | Grad Norm: 0.00926065\n",
      "Epoch 1 | Step 241100 | Avg Loss: 0.0145 | Grad Norm: 0.00915568\n",
      "Epoch 1 | Step 241200 | Avg Loss: 0.0145 | Grad Norm: 0.00912045\n",
      "Epoch 1 | Step 241300 | Avg Loss: 0.0144 | Grad Norm: 0.00949093\n",
      "Epoch 1 | Step 241400 | Avg Loss: 0.0145 | Grad Norm: 0.00910591\n",
      "Epoch 1 | Step 241500 | Avg Loss: 0.0144 | Grad Norm: 0.01087554\n",
      "Epoch 1 | Step 241600 | Avg Loss: 0.0144 | Grad Norm: 0.01025905\n",
      "Epoch 1 | Step 241700 | Avg Loss: 0.0145 | Grad Norm: 0.01045483\n",
      "Epoch 1 | Step 241800 | Avg Loss: 0.0147 | Grad Norm: 0.00949225\n",
      "Epoch 1 | Step 241900 | Avg Loss: 0.0150 | Grad Norm: 0.00888758\n",
      "Epoch 1 | Step 242000 | Avg Loss: 0.0148 | Grad Norm: 0.00994698\n",
      "Epoch 1 | Step 242100 | Avg Loss: 0.0144 | Grad Norm: 0.00963013\n",
      "Epoch 1 | Step 242200 | Avg Loss: 0.0142 | Grad Norm: 0.00885829\n",
      "Epoch 1 | Step 242300 | Avg Loss: 0.0143 | Grad Norm: 0.01057359\n",
      "Epoch 1 | Step 242400 | Avg Loss: 0.0141 | Grad Norm: 0.01052911\n",
      "Epoch 1 | Step 242500 | Avg Loss: 0.0142 | Grad Norm: 0.00974745\n",
      "Epoch 1 | Step 242600 | Avg Loss: 0.0143 | Grad Norm: 0.00906553\n",
      "Epoch 1 | Step 242700 | Avg Loss: 0.0143 | Grad Norm: 0.00966762\n",
      "Epoch 1 | Step 242800 | Avg Loss: 0.0143 | Grad Norm: 0.00915929\n",
      "Epoch 1 | Step 242900 | Avg Loss: 0.0142 | Grad Norm: 0.00953315\n",
      "Epoch 1 | Step 243000 | Avg Loss: 0.0139 | Grad Norm: 0.00917339\n",
      "Epoch 1 | Step 243100 | Avg Loss: 0.0138 | Grad Norm: 0.00941953\n",
      "Epoch 1 | Step 243200 | Avg Loss: 0.0140 | Grad Norm: 0.01008411\n",
      "Epoch 1 | Step 243300 | Avg Loss: 0.0143 | Grad Norm: 0.01122438\n",
      "Epoch 1 | Step 243400 | Avg Loss: 0.0144 | Grad Norm: 0.00999883\n",
      "Epoch 1 | Step 243500 | Avg Loss: 0.0145 | Grad Norm: 0.00768157\n",
      "Epoch 1 | Step 243600 | Avg Loss: 0.0145 | Grad Norm: 0.00934536\n",
      "Epoch 1 | Step 243700 | Avg Loss: 0.0143 | Grad Norm: 0.01116131\n",
      "Epoch 1 | Step 243800 | Avg Loss: 0.0139 | Grad Norm: 0.00846123\n",
      "Epoch 1 | Step 243900 | Avg Loss: 0.0143 | Grad Norm: 0.00953119\n",
      "Epoch 1 | Step 244000 | Avg Loss: 0.0145 | Grad Norm: 0.00914474\n",
      "Epoch 1 | Step 244100 | Avg Loss: 0.0145 | Grad Norm: 0.00848954\n",
      "Epoch 1 | Step 244200 | Avg Loss: 0.0141 | Grad Norm: 0.01065838\n",
      "Epoch 1 | Step 244300 | Avg Loss: 0.0145 | Grad Norm: 0.00976067\n",
      "Epoch 1 | Step 244400 | Avg Loss: 0.0143 | Grad Norm: 0.01075452\n",
      "Epoch 1 | Step 244500 | Avg Loss: 0.0142 | Grad Norm: 0.00887035\n",
      "Epoch 1 | Step 244600 | Avg Loss: 0.0140 | Grad Norm: 0.00858547\n",
      "Epoch 1 | Step 244700 | Avg Loss: 0.0138 | Grad Norm: 0.01030452\n",
      "Epoch 1 | Step 244800 | Avg Loss: 0.0141 | Grad Norm: 0.00886849\n",
      "Epoch 1 | Step 244900 | Avg Loss: 0.0139 | Grad Norm: 0.00848860\n",
      "Epoch 1 | Step 245000 | Avg Loss: 0.0139 | Grad Norm: 0.01137173\n",
      "Epoch 1 | Step 245100 | Avg Loss: 0.0143 | Grad Norm: 0.00972043\n",
      "Epoch 1 | Step 245200 | Avg Loss: 0.0145 | Grad Norm: 0.01096129\n",
      "Epoch 1 | Step 245300 | Avg Loss: 0.0142 | Grad Norm: 0.00988420\n",
      "Epoch 1 | Step 245400 | Avg Loss: 0.0142 | Grad Norm: 0.00939443\n",
      "Epoch 1 | Step 245500 | Avg Loss: 0.0144 | Grad Norm: 0.00923922\n",
      "Epoch 1 | Step 245600 | Avg Loss: 0.0143 | Grad Norm: 0.00893701\n",
      "Epoch 1 | Step 245700 | Avg Loss: 0.0143 | Grad Norm: 0.00910443\n",
      "Epoch 1 | Step 245800 | Avg Loss: 0.0148 | Grad Norm: 0.01028609\n",
      "Epoch 1 | Step 245900 | Avg Loss: 0.0148 | Grad Norm: 0.00896369\n",
      "Epoch 1 | Step 246000 | Avg Loss: 0.0149 | Grad Norm: 0.00954328\n",
      "Epoch 1 | Step 246100 | Avg Loss: 0.0145 | Grad Norm: 0.00981115\n",
      "Epoch 1 | Step 246200 | Avg Loss: 0.0143 | Grad Norm: 0.00914721\n",
      "Epoch 1 | Step 246300 | Avg Loss: 0.0144 | Grad Norm: 0.00947418\n",
      "Epoch 1 | Step 246400 | Avg Loss: 0.0143 | Grad Norm: 0.00939353\n",
      "Epoch 1 | Step 246500 | Avg Loss: 0.0145 | Grad Norm: 0.01066860\n",
      "Epoch 1 | Step 246600 | Avg Loss: 0.0146 | Grad Norm: 0.00927566\n",
      "Epoch 1 | Step 246700 | Avg Loss: 0.0145 | Grad Norm: 0.00953515\n",
      "Epoch 1 | Step 246800 | Avg Loss: 0.0143 | Grad Norm: 0.01014248\n",
      "Epoch 1 | Step 246900 | Avg Loss: 0.0142 | Grad Norm: 0.01173261\n",
      "Epoch 1 | Step 247000 | Avg Loss: 0.0144 | Grad Norm: 0.00917080\n",
      "Epoch 1 | Step 247100 | Avg Loss: 0.0145 | Grad Norm: 0.00932024\n",
      "Epoch 1 | Step 247200 | Avg Loss: 0.0147 | Grad Norm: 0.01021088\n",
      "Epoch 1 | Step 247300 | Avg Loss: 0.0146 | Grad Norm: 0.00930995\n",
      "Epoch 1 | Step 247400 | Avg Loss: 0.0142 | Grad Norm: 0.00884784\n",
      "Epoch 1 | Step 247500 | Avg Loss: 0.0144 | Grad Norm: 0.00906423\n",
      "Epoch 1 | Step 247600 | Avg Loss: 0.0144 | Grad Norm: 0.00913064\n",
      "Epoch 1 | Step 247700 | Avg Loss: 0.0145 | Grad Norm: 0.00994074\n",
      "Epoch 1 | Step 247800 | Avg Loss: 0.0150 | Grad Norm: 0.00999150\n",
      "Epoch 1 | Step 247900 | Avg Loss: 0.0147 | Grad Norm: 0.01045202\n",
      "Epoch 1 | Step 248000 | Avg Loss: 0.0150 | Grad Norm: 0.01103454\n",
      "Epoch 1 | Step 248100 | Avg Loss: 0.0148 | Grad Norm: 0.01028085\n",
      "Epoch 1 | Step 248200 | Avg Loss: 0.0148 | Grad Norm: 0.01002442\n",
      "Epoch 1 | Step 248300 | Avg Loss: 0.0149 | Grad Norm: 0.00955351\n",
      "Epoch 1 | Step 248400 | Avg Loss: 0.0142 | Grad Norm: 0.00897182\n",
      "Epoch 1 | Step 248500 | Avg Loss: 0.0140 | Grad Norm: 0.01202346\n",
      "Epoch 1 | Step 248600 | Avg Loss: 0.0144 | Grad Norm: 0.01329357\n",
      "Epoch 1 | Step 248700 | Avg Loss: 0.0146 | Grad Norm: 0.01152885\n",
      "Epoch 1 | Step 248800 | Avg Loss: 0.0150 | Grad Norm: 0.01069298\n",
      "Epoch 1 | Step 248900 | Avg Loss: 0.0147 | Grad Norm: 0.01168156\n",
      "Epoch 1 | Step 249000 | Avg Loss: 0.0146 | Grad Norm: 0.01021700\n",
      "Epoch 1 | Step 249100 | Avg Loss: 0.0146 | Grad Norm: 0.01023003\n",
      "Epoch 1 | Step 249200 | Avg Loss: 0.0144 | Grad Norm: 0.01073773\n",
      "Epoch 1 | Step 249300 | Avg Loss: 0.0143 | Grad Norm: 0.00984684\n",
      "Epoch 1 | Step 249400 | Avg Loss: 0.0144 | Grad Norm: 0.01085714\n",
      "Epoch 1 | Step 249500 | Avg Loss: 0.0145 | Grad Norm: 0.00912704\n",
      "Epoch 1 | Step 249600 | Avg Loss: 0.0142 | Grad Norm: 0.01014290\n",
      "Epoch 1 | Step 249700 | Avg Loss: 0.0145 | Grad Norm: 0.00941228\n",
      "Epoch 1 | Step 249800 | Avg Loss: 0.0143 | Grad Norm: 0.01102700\n",
      "Epoch 1 | Step 249900 | Avg Loss: 0.0141 | Grad Norm: 0.01108640\n",
      "Epoch 1 | Step 250000 | Avg Loss: 0.0143 | Grad Norm: 0.00939149\n",
      "Epoch 1 | Step 250100 | Avg Loss: 0.0141 | Grad Norm: 0.01005090\n",
      "Epoch 1 | Step 250200 | Avg Loss: 0.0139 | Grad Norm: 0.00873768\n",
      "Epoch 1 | Step 250300 | Avg Loss: 0.0136 | Grad Norm: 0.00994989\n",
      "Epoch 1 | Step 250400 | Avg Loss: 0.0135 | Grad Norm: 0.00986753\n",
      "Epoch 1 | Step 250500 | Avg Loss: 0.0135 | Grad Norm: 0.00802364\n",
      "Epoch 1 | Step 250600 | Avg Loss: 0.0139 | Grad Norm: 0.00961484\n",
      "Epoch 1 | Step 250700 | Avg Loss: 0.0142 | Grad Norm: 0.01125524\n",
      "Epoch 1 | Step 250800 | Avg Loss: 0.0139 | Grad Norm: 0.00930472\n",
      "Epoch 1 | Step 250900 | Avg Loss: 0.0141 | Grad Norm: 0.01059399\n",
      "Epoch 1 | Step 251000 | Avg Loss: 0.0138 | Grad Norm: 0.01035983\n",
      "Epoch 1 | Step 251100 | Avg Loss: 0.0132 | Grad Norm: 0.01184243\n",
      "Epoch 1 | Step 251200 | Avg Loss: 0.0134 | Grad Norm: 0.00984535\n",
      "Epoch 1 | Step 251300 | Avg Loss: 0.0137 | Grad Norm: 0.00827538\n",
      "Epoch 1 | Step 251400 | Avg Loss: 0.0138 | Grad Norm: 0.00910227\n",
      "Epoch 1 | Step 251500 | Avg Loss: 0.0137 | Grad Norm: 0.01042836\n",
      "Epoch 1 | Step 251600 | Avg Loss: 0.0141 | Grad Norm: 0.00927277\n",
      "Epoch 1 | Step 251700 | Avg Loss: 0.0143 | Grad Norm: 0.00958289\n",
      "Epoch 1 | Step 251800 | Avg Loss: 0.0141 | Grad Norm: 0.00959627\n",
      "Epoch 1 | Step 251900 | Avg Loss: 0.0145 | Grad Norm: 0.00810776\n",
      "Epoch 1 | Step 252000 | Avg Loss: 0.0141 | Grad Norm: 0.00901693\n",
      "Epoch 1 | Step 252100 | Avg Loss: 0.0145 | Grad Norm: 0.01180969\n",
      "Epoch 1 | Step 252200 | Avg Loss: 0.0141 | Grad Norm: 0.00943351\n",
      "Epoch 1 | Step 252300 | Avg Loss: 0.0144 | Grad Norm: 0.01079407\n",
      "Epoch 1 | Step 252400 | Avg Loss: 0.0145 | Grad Norm: 0.00902414\n",
      "Epoch 1 | Step 252500 | Avg Loss: 0.0147 | Grad Norm: 0.00945657\n",
      "Epoch 1 | Step 252600 | Avg Loss: 0.0144 | Grad Norm: 0.00948602\n",
      "Epoch 1 | Step 252700 | Avg Loss: 0.0142 | Grad Norm: 0.01069026\n",
      "Epoch 1 | Step 252800 | Avg Loss: 0.0141 | Grad Norm: 0.01032377\n",
      "Epoch 1 | Step 252900 | Avg Loss: 0.0145 | Grad Norm: 0.01087221\n",
      "Epoch 1 | Step 253000 | Avg Loss: 0.0142 | Grad Norm: 0.01070188\n",
      "Epoch 1 | Step 253100 | Avg Loss: 0.0144 | Grad Norm: 0.01100955\n",
      "Epoch 1 | Step 253200 | Avg Loss: 0.0143 | Grad Norm: 0.00991573\n",
      "Epoch 1 | Step 253300 | Avg Loss: 0.0143 | Grad Norm: 0.00943134\n",
      "Epoch 1 | Step 253400 | Avg Loss: 0.0147 | Grad Norm: 0.01101921\n",
      "Epoch 1 | Step 253500 | Avg Loss: 0.0145 | Grad Norm: 0.01016055\n",
      "Epoch 1 | Step 253600 | Avg Loss: 0.0144 | Grad Norm: 0.00997368\n",
      "Epoch 1 | Step 253700 | Avg Loss: 0.0141 | Grad Norm: 0.00910769\n",
      "Epoch 1 | Step 253800 | Avg Loss: 0.0139 | Grad Norm: 0.00972257\n",
      "Epoch 1 | Step 253900 | Avg Loss: 0.0137 | Grad Norm: 0.00935191\n",
      "Epoch 1 | Step 254000 | Avg Loss: 0.0139 | Grad Norm: 0.01041706\n",
      "Epoch 1 | Step 254100 | Avg Loss: 0.0138 | Grad Norm: 0.00838515\n",
      "Epoch 1 | Step 254200 | Avg Loss: 0.0141 | Grad Norm: 0.01054729\n",
      "Epoch 1 | Step 254300 | Avg Loss: 0.0144 | Grad Norm: 0.01102160\n",
      "Epoch 1 | Step 254400 | Avg Loss: 0.0148 | Grad Norm: 0.00952531\n",
      "Epoch 1 | Step 254500 | Avg Loss: 0.0143 | Grad Norm: 0.01065392\n",
      "Epoch 1 | Step 254600 | Avg Loss: 0.0143 | Grad Norm: 0.01034866\n",
      "Epoch 1 | Step 254700 | Avg Loss: 0.0142 | Grad Norm: 0.01067193\n",
      "Epoch 1 | Step 254800 | Avg Loss: 0.0145 | Grad Norm: 0.01036208\n",
      "Epoch 1 | Step 254900 | Avg Loss: 0.0143 | Grad Norm: 0.00867487\n",
      "Epoch 1 | Step 255000 | Avg Loss: 0.0142 | Grad Norm: 0.00968242\n",
      "Epoch 1 | Step 255100 | Avg Loss: 0.0140 | Grad Norm: 0.00951132\n",
      "Epoch 1 | Step 255200 | Avg Loss: 0.0140 | Grad Norm: 0.00919329\n",
      "Epoch 1 | Step 255300 | Avg Loss: 0.0140 | Grad Norm: 0.00975253\n",
      "Epoch 1 | Step 255400 | Avg Loss: 0.0143 | Grad Norm: 0.00999854\n",
      "Epoch 1 | Step 255500 | Avg Loss: 0.0142 | Grad Norm: 0.01022650\n",
      "Epoch 1 | Step 255600 | Avg Loss: 0.0144 | Grad Norm: 0.01050846\n",
      "Epoch 1 | Step 255700 | Avg Loss: 0.0146 | Grad Norm: 0.00966112\n",
      "Epoch 1 | Step 255800 | Avg Loss: 0.0145 | Grad Norm: 0.00892098\n",
      "Epoch 1 | Step 255900 | Avg Loss: 0.0142 | Grad Norm: 0.00880582\n",
      "Epoch 1 | Step 256000 | Avg Loss: 0.0143 | Grad Norm: 0.00911807\n",
      "Epoch 1 | Step 256100 | Avg Loss: 0.0143 | Grad Norm: 0.00854601\n",
      "Epoch 1 | Step 256200 | Avg Loss: 0.0143 | Grad Norm: 0.00938263\n",
      "Epoch 1 | Step 256300 | Avg Loss: 0.0141 | Grad Norm: 0.00960261\n",
      "Epoch 1 | Step 256400 | Avg Loss: 0.0145 | Grad Norm: 0.00859523\n",
      "Epoch 1 | Step 256500 | Avg Loss: 0.0147 | Grad Norm: 0.00948729\n",
      "Epoch 1 | Step 256600 | Avg Loss: 0.0140 | Grad Norm: 0.00868137\n",
      "Epoch 1 | Step 256700 | Avg Loss: 0.0140 | Grad Norm: 0.00909800\n",
      "Epoch 1 | Step 256800 | Avg Loss: 0.0139 | Grad Norm: 0.00970101\n",
      "Epoch 1 | Step 256900 | Avg Loss: 0.0141 | Grad Norm: 0.00923405\n",
      "Epoch 1 | Step 257000 | Avg Loss: 0.0145 | Grad Norm: 0.00977170\n",
      "Epoch 1 | Step 257100 | Avg Loss: 0.0144 | Grad Norm: 0.00970521\n",
      "Epoch 1 | Step 257200 | Avg Loss: 0.0146 | Grad Norm: 0.00841867\n",
      "Epoch 1 | Step 257300 | Avg Loss: 0.0142 | Grad Norm: 0.01042034\n",
      "Epoch 1 | Step 257400 | Avg Loss: 0.0141 | Grad Norm: 0.01039026\n",
      "Epoch 1 | Step 257500 | Avg Loss: 0.0145 | Grad Norm: 0.01062433\n",
      "Epoch 1 | Step 257600 | Avg Loss: 0.0142 | Grad Norm: 0.00866416\n",
      "Epoch 1 | Step 257700 | Avg Loss: 0.0144 | Grad Norm: 0.00929555\n",
      "Epoch 1 | Step 257800 | Avg Loss: 0.0146 | Grad Norm: 0.00979072\n",
      "Epoch 1 | Step 257900 | Avg Loss: 0.0147 | Grad Norm: 0.00917638\n",
      "Epoch 1 | Step 258000 | Avg Loss: 0.0146 | Grad Norm: 0.01077058\n",
      "Epoch 1 | Step 258100 | Avg Loss: 0.0145 | Grad Norm: 0.00933025\n",
      "Epoch 1 | Step 258200 | Avg Loss: 0.0141 | Grad Norm: 0.01090554\n",
      "Epoch 1 | Step 258300 | Avg Loss: 0.0139 | Grad Norm: 0.01040825\n",
      "Epoch 1 | Step 258400 | Avg Loss: 0.0136 | Grad Norm: 0.00944819\n",
      "Epoch 1 | Step 258500 | Avg Loss: 0.0141 | Grad Norm: 0.00902441\n",
      "Epoch 1 | Step 258600 | Avg Loss: 0.0147 | Grad Norm: 0.00924066\n",
      "Epoch 1 | Step 258700 | Avg Loss: 0.0147 | Grad Norm: 0.01144055\n",
      "Epoch 1 | Step 258800 | Avg Loss: 0.0148 | Grad Norm: 0.01140974\n",
      "Epoch 1 | Step 258900 | Avg Loss: 0.0146 | Grad Norm: 0.01064253\n",
      "Epoch 1 | Step 259000 | Avg Loss: 0.0145 | Grad Norm: 0.00964281\n",
      "Epoch 1 | Step 259100 | Avg Loss: 0.0147 | Grad Norm: 0.00842060\n",
      "Epoch 1 | Step 259200 | Avg Loss: 0.0143 | Grad Norm: 0.01146468\n",
      "Epoch 1 | Step 259300 | Avg Loss: 0.0142 | Grad Norm: 0.01094345\n",
      "Epoch 1 | Step 259400 | Avg Loss: 0.0142 | Grad Norm: 0.00886656\n",
      "Epoch 1 | Step 259500 | Avg Loss: 0.0142 | Grad Norm: 0.00795939\n",
      "Epoch 1 | Step 259600 | Avg Loss: 0.0141 | Grad Norm: 0.01032226\n",
      "Epoch 1 | Step 259700 | Avg Loss: 0.0141 | Grad Norm: 0.00986559\n",
      "Epoch 1 | Step 259800 | Avg Loss: 0.0141 | Grad Norm: 0.01044513\n",
      "Epoch 1 | Step 259900 | Avg Loss: 0.0142 | Grad Norm: 0.01037249\n",
      "Epoch 1 | Step 260000 | Avg Loss: 0.0142 | Grad Norm: 0.00939647\n",
      "Epoch 1 | Step 260100 | Avg Loss: 0.0140 | Grad Norm: 0.00828604\n",
      "Epoch 1 | Step 260200 | Avg Loss: 0.0140 | Grad Norm: 0.00888413\n",
      "Epoch 1 | Step 260300 | Avg Loss: 0.0140 | Grad Norm: 0.00891237\n",
      "Epoch 1 | Step 260400 | Avg Loss: 0.0143 | Grad Norm: 0.01057645\n",
      "Epoch 1 | Step 260500 | Avg Loss: 0.0146 | Grad Norm: 0.01053076\n",
      "Epoch 1 | Step 260600 | Avg Loss: 0.0147 | Grad Norm: 0.01171819\n",
      "Epoch 1 | Step 260700 | Avg Loss: 0.0147 | Grad Norm: 0.01033969\n",
      "Epoch 1 | Step 260800 | Avg Loss: 0.0148 | Grad Norm: 0.00892864\n",
      "Epoch 1 | Step 260900 | Avg Loss: 0.0146 | Grad Norm: 0.00902634\n",
      "Epoch 1 | Step 261000 | Avg Loss: 0.0143 | Grad Norm: 0.01111484\n",
      "Epoch 1 | Step 261100 | Avg Loss: 0.0143 | Grad Norm: 0.00837457\n",
      "Epoch 1 | Step 261200 | Avg Loss: 0.0146 | Grad Norm: 0.01099481\n",
      "Epoch 1 | Step 261300 | Avg Loss: 0.0143 | Grad Norm: 0.01070199\n",
      "Epoch 1 | Step 261400 | Avg Loss: 0.0146 | Grad Norm: 0.01090115\n",
      "Epoch 1 | Step 261500 | Avg Loss: 0.0144 | Grad Norm: 0.00938149\n",
      "Epoch 1 | Step 261600 | Avg Loss: 0.0147 | Grad Norm: 0.00923977\n",
      "Epoch 1 | Step 261700 | Avg Loss: 0.0147 | Grad Norm: 0.00915208\n",
      "Epoch 1 | Step 261800 | Avg Loss: 0.0141 | Grad Norm: 0.01220941\n",
      "Epoch 1 | Step 261900 | Avg Loss: 0.0145 | Grad Norm: 0.00837967\n",
      "Epoch 1 | Step 262000 | Avg Loss: 0.0144 | Grad Norm: 0.00950044\n",
      "Epoch 1 | Step 262100 | Avg Loss: 0.0147 | Grad Norm: 0.00860933\n",
      "Epoch 1 | Step 262200 | Avg Loss: 0.0144 | Grad Norm: 0.01114898\n",
      "Epoch 1 | Step 262300 | Avg Loss: 0.0142 | Grad Norm: 0.01043663\n",
      "Epoch 1 | Step 262400 | Avg Loss: 0.0140 | Grad Norm: 0.00921550\n",
      "Epoch 1 | Step 262500 | Avg Loss: 0.0144 | Grad Norm: 0.01137111\n",
      "Epoch 1 | Step 262600 | Avg Loss: 0.0142 | Grad Norm: 0.01075729\n",
      "Epoch 1 | Step 262700 | Avg Loss: 0.0142 | Grad Norm: 0.01019812\n",
      "Epoch 1 | Step 262800 | Avg Loss: 0.0145 | Grad Norm: 0.01143749\n",
      "Epoch 1 | Step 262900 | Avg Loss: 0.0147 | Grad Norm: 0.00907408\n",
      "Epoch 1 | Step 263000 | Avg Loss: 0.0142 | Grad Norm: 0.00938886\n",
      "Epoch 1 | Step 263100 | Avg Loss: 0.0145 | Grad Norm: 0.01132451\n",
      "Epoch 1 | Step 263200 | Avg Loss: 0.0144 | Grad Norm: 0.00979528\n",
      "Epoch 1 | Step 263300 | Avg Loss: 0.0143 | Grad Norm: 0.00945857\n",
      "Epoch 1 | Step 263400 | Avg Loss: 0.0146 | Grad Norm: 0.00924507\n",
      "Epoch 1 | Step 263500 | Avg Loss: 0.0145 | Grad Norm: 0.00896191\n",
      "Epoch 1 | Step 263600 | Avg Loss: 0.0148 | Grad Norm: 0.01098976\n",
      "Epoch 1 | Step 263700 | Avg Loss: 0.0150 | Grad Norm: 0.01018306\n",
      "Epoch 1 | Step 263800 | Avg Loss: 0.0148 | Grad Norm: 0.01144625\n",
      "Epoch 1 | Step 263900 | Avg Loss: 0.0147 | Grad Norm: 0.00845097\n",
      "Epoch 1 | Step 264000 | Avg Loss: 0.0149 | Grad Norm: 0.00924494\n",
      "Epoch 1 | Step 264100 | Avg Loss: 0.0151 | Grad Norm: 0.00931164\n",
      "Epoch 1 | Step 264200 | Avg Loss: 0.0151 | Grad Norm: 0.01025942\n",
      "Epoch 1 | Step 264300 | Avg Loss: 0.0150 | Grad Norm: 0.01029824\n",
      "Epoch 1 | Step 264400 | Avg Loss: 0.0149 | Grad Norm: 0.01002914\n",
      "Epoch 1 | Step 264500 | Avg Loss: 0.0146 | Grad Norm: 0.00888056\n",
      "Epoch 1 | Step 264600 | Avg Loss: 0.0145 | Grad Norm: 0.01055558\n",
      "Epoch 1 | Step 264700 | Avg Loss: 0.0148 | Grad Norm: 0.00929263\n",
      "Epoch 1 | Step 264800 | Avg Loss: 0.0149 | Grad Norm: 0.01036468\n",
      "Epoch 1 | Step 264900 | Avg Loss: 0.0150 | Grad Norm: 0.00936197\n",
      "Epoch 1 | Step 265000 | Avg Loss: 0.0150 | Grad Norm: 0.01014276\n",
      "Epoch 1 | Step 265100 | Avg Loss: 0.0147 | Grad Norm: 0.01146983\n",
      "Epoch 1 | Step 265200 | Avg Loss: 0.0147 | Grad Norm: 0.00871281\n",
      "Epoch 1 | Step 265300 | Avg Loss: 0.0142 | Grad Norm: 0.00925216\n",
      "Epoch 1 | Step 265400 | Avg Loss: 0.0143 | Grad Norm: 0.00999223\n",
      "Epoch 1 | Step 265500 | Avg Loss: 0.0151 | Grad Norm: 0.01018674\n",
      "Epoch 1 | Step 265600 | Avg Loss: 0.0147 | Grad Norm: 0.00979178\n",
      "Epoch 1 | Step 265700 | Avg Loss: 0.0147 | Grad Norm: 0.00995815\n",
      "Epoch 1 | Step 265800 | Avg Loss: 0.0145 | Grad Norm: 0.01052160\n",
      "Epoch 1 | Step 265900 | Avg Loss: 0.0144 | Grad Norm: 0.00822381\n",
      "Epoch 1 | Step 266000 | Avg Loss: 0.0142 | Grad Norm: 0.00879309\n",
      "Epoch 1 | Step 266100 | Avg Loss: 0.0145 | Grad Norm: 0.00900036\n",
      "Epoch 1 | Step 266200 | Avg Loss: 0.0145 | Grad Norm: 0.00942482\n",
      "Epoch 1 | Step 266300 | Avg Loss: 0.0142 | Grad Norm: 0.00784262\n",
      "Epoch 1 | Step 266400 | Avg Loss: 0.0144 | Grad Norm: 0.01093987\n",
      "Epoch 1 | Step 266500 | Avg Loss: 0.0147 | Grad Norm: 0.00875784\n",
      "Epoch 1 | Step 266600 | Avg Loss: 0.0147 | Grad Norm: 0.01002565\n",
      "Epoch 1 | Step 266700 | Avg Loss: 0.0145 | Grad Norm: 0.01160631\n",
      "Epoch 1 | Step 266800 | Avg Loss: 0.0148 | Grad Norm: 0.00869877\n",
      "Epoch 1 | Step 266900 | Avg Loss: 0.0143 | Grad Norm: 0.00898487\n",
      "Epoch 1 | Step 267000 | Avg Loss: 0.0145 | Grad Norm: 0.01007014\n",
      "Epoch 1 | Step 267100 | Avg Loss: 0.0146 | Grad Norm: 0.01112383\n",
      "Epoch 1 | Step 267200 | Avg Loss: 0.0145 | Grad Norm: 0.00928085\n",
      "Epoch 1 | Step 267300 | Avg Loss: 0.0139 | Grad Norm: 0.00904394\n",
      "Epoch 1 | Step 267400 | Avg Loss: 0.0138 | Grad Norm: 0.01117204\n",
      "Epoch 1 | Step 267500 | Avg Loss: 0.0144 | Grad Norm: 0.01001740\n",
      "Epoch 1 | Step 267600 | Avg Loss: 0.0143 | Grad Norm: 0.00969711\n",
      "Epoch 1 | Step 267700 | Avg Loss: 0.0140 | Grad Norm: 0.01009580\n",
      "Epoch 1 | Step 267800 | Avg Loss: 0.0142 | Grad Norm: 0.00946779\n",
      "Epoch 1 | Step 267900 | Avg Loss: 0.0143 | Grad Norm: 0.00895430\n",
      "Epoch 1 | Step 268000 | Avg Loss: 0.0144 | Grad Norm: 0.00850315\n",
      "Epoch 1 | Step 268100 | Avg Loss: 0.0142 | Grad Norm: 0.01020522\n",
      "Epoch 1 | Step 268200 | Avg Loss: 0.0142 | Grad Norm: 0.01025416\n",
      "Epoch 1 | Step 268300 | Avg Loss: 0.0143 | Grad Norm: 0.00861377\n",
      "Epoch 1 | Step 268400 | Avg Loss: 0.0145 | Grad Norm: 0.00971882\n",
      "Epoch 1 | Step 268500 | Avg Loss: 0.0146 | Grad Norm: 0.01040228\n",
      "Epoch 1 | Step 268600 | Avg Loss: 0.0146 | Grad Norm: 0.00932372\n",
      "Epoch 1 | Step 268700 | Avg Loss: 0.0143 | Grad Norm: 0.00944389\n",
      "Epoch 1 | Step 268800 | Avg Loss: 0.0145 | Grad Norm: 0.01062327\n",
      "Epoch 1 | Step 268900 | Avg Loss: 0.0144 | Grad Norm: 0.01114445\n",
      "Epoch 1 | Step 269000 | Avg Loss: 0.0145 | Grad Norm: 0.01128992\n",
      "Epoch 1 | Step 269100 | Avg Loss: 0.0143 | Grad Norm: 0.00926042\n",
      "Epoch 1 | Step 269200 | Avg Loss: 0.0143 | Grad Norm: 0.00899025\n",
      "Epoch 1 | Step 269300 | Avg Loss: 0.0145 | Grad Norm: 0.00923669\n",
      "Epoch 1 | Step 269400 | Avg Loss: 0.0147 | Grad Norm: 0.00951425\n",
      "Epoch 1 | Step 269500 | Avg Loss: 0.0148 | Grad Norm: 0.00963347\n",
      "Epoch 1 | Step 269600 | Avg Loss: 0.0146 | Grad Norm: 0.00948359\n",
      "Epoch 1 | Step 269700 | Avg Loss: 0.0142 | Grad Norm: 0.00959328\n",
      "Epoch 1 | Step 269800 | Avg Loss: 0.0145 | Grad Norm: 0.01085653\n",
      "Epoch 1 | Step 269900 | Avg Loss: 0.0143 | Grad Norm: 0.01017612\n",
      "Epoch 1 | Step 270000 | Avg Loss: 0.0146 | Grad Norm: 0.00932214\n",
      "Epoch 1 | Step 270100 | Avg Loss: 0.0146 | Grad Norm: 0.00973554\n",
      "Epoch 1 | Step 270200 | Avg Loss: 0.0149 | Grad Norm: 0.00906468\n",
      "Epoch 1 | Step 270300 | Avg Loss: 0.0148 | Grad Norm: 0.00907988\n",
      "Epoch 1 | Step 270400 | Avg Loss: 0.0142 | Grad Norm: 0.00986833\n",
      "Epoch 1 | Step 270500 | Avg Loss: 0.0146 | Grad Norm: 0.00918915\n",
      "Epoch 1 | Step 270600 | Avg Loss: 0.0145 | Grad Norm: 0.01180058\n",
      "Epoch 1 | Step 270700 | Avg Loss: 0.0145 | Grad Norm: 0.00867547\n",
      "Epoch 1 | Step 270800 | Avg Loss: 0.0142 | Grad Norm: 0.01048226\n",
      "Epoch 1 | Step 270900 | Avg Loss: 0.0138 | Grad Norm: 0.00942282\n",
      "Epoch 1 | Step 271000 | Avg Loss: 0.0136 | Grad Norm: 0.00921867\n",
      "Epoch 1 | Step 271100 | Avg Loss: 0.0141 | Grad Norm: 0.01070869\n",
      "Epoch 1 | Step 271200 | Avg Loss: 0.0143 | Grad Norm: 0.00994306\n",
      "Epoch 1 | Step 271300 | Avg Loss: 0.0140 | Grad Norm: 0.00911259\n",
      "Epoch 1 | Step 271400 | Avg Loss: 0.0141 | Grad Norm: 0.01012837\n",
      "Epoch 1 | Step 271500 | Avg Loss: 0.0146 | Grad Norm: 0.01003669\n",
      "Epoch 1 | Step 271600 | Avg Loss: 0.0143 | Grad Norm: 0.01010704\n",
      "Epoch 1 | Step 271700 | Avg Loss: 0.0141 | Grad Norm: 0.00925519\n",
      "Epoch 1 | Step 271800 | Avg Loss: 0.0143 | Grad Norm: 0.01043745\n",
      "Epoch 1 | Step 271900 | Avg Loss: 0.0145 | Grad Norm: 0.00858131\n",
      "Epoch 1 | Step 272000 | Avg Loss: 0.0142 | Grad Norm: 0.01059378\n",
      "Epoch 1 | Step 272100 | Avg Loss: 0.0143 | Grad Norm: 0.01056641\n",
      "Epoch 1 | Step 272200 | Avg Loss: 0.0144 | Grad Norm: 0.00998838\n",
      "Epoch 1 | Step 272300 | Avg Loss: 0.0148 | Grad Norm: 0.01020693\n",
      "Epoch 1 | Step 272400 | Avg Loss: 0.0145 | Grad Norm: 0.01055862\n",
      "Epoch 1 | Step 272500 | Avg Loss: 0.0144 | Grad Norm: 0.00997412\n",
      "Epoch 1 | Step 272600 | Avg Loss: 0.0147 | Grad Norm: 0.00983011\n",
      "Epoch 1 | Step 272700 | Avg Loss: 0.0150 | Grad Norm: 0.01301741\n",
      "Epoch 1 | Step 272800 | Avg Loss: 0.0150 | Grad Norm: 0.00899029\n",
      "Epoch 1 | Step 272900 | Avg Loss: 0.0143 | Grad Norm: 0.00821332\n",
      "Epoch 1 | Step 273000 | Avg Loss: 0.0139 | Grad Norm: 0.00839720\n",
      "Epoch 1 | Step 273100 | Avg Loss: 0.0138 | Grad Norm: 0.01010086\n",
      "Epoch 1 | Step 273200 | Avg Loss: 0.0142 | Grad Norm: 0.00960754\n",
      "Epoch 1 | Step 273300 | Avg Loss: 0.0141 | Grad Norm: 0.00902155\n",
      "Epoch 1 | Step 273400 | Avg Loss: 0.0141 | Grad Norm: 0.01111937\n",
      "Epoch 1 | Step 273500 | Avg Loss: 0.0141 | Grad Norm: 0.01035550\n",
      "Epoch 1 | Step 273600 | Avg Loss: 0.0139 | Grad Norm: 0.00908949\n",
      "Epoch 1 | Step 273700 | Avg Loss: 0.0143 | Grad Norm: 0.00899867\n",
      "Epoch 1 | Step 273800 | Avg Loss: 0.0142 | Grad Norm: 0.01016090\n",
      "Epoch 1 | Step 273900 | Avg Loss: 0.0142 | Grad Norm: 0.00986501\n",
      "Epoch 1 | Step 274000 | Avg Loss: 0.0145 | Grad Norm: 0.01025828\n",
      "Epoch 1 | Step 274100 | Avg Loss: 0.0143 | Grad Norm: 0.01061172\n",
      "Epoch 1 | Step 274200 | Avg Loss: 0.0139 | Grad Norm: 0.00933010\n",
      "Epoch 1 | Step 274300 | Avg Loss: 0.0143 | Grad Norm: 0.00872375\n",
      "Epoch 1 | Step 274400 | Avg Loss: 0.0142 | Grad Norm: 0.00829168\n",
      "Epoch 1 | Step 274500 | Avg Loss: 0.0139 | Grad Norm: 0.00962823\n",
      "Epoch 1 | Step 274600 | Avg Loss: 0.0138 | Grad Norm: 0.00894809\n",
      "Epoch 1 | Step 274700 | Avg Loss: 0.0142 | Grad Norm: 0.01057023\n",
      "Epoch 1 | Step 274800 | Avg Loss: 0.0142 | Grad Norm: 0.00931857\n",
      "Epoch 1 | Step 274900 | Avg Loss: 0.0147 | Grad Norm: 0.00966062\n",
      "Epoch 1 | Step 275000 | Avg Loss: 0.0147 | Grad Norm: 0.01263074\n",
      "Epoch 1 | Step 275100 | Avg Loss: 0.0147 | Grad Norm: 0.00966010\n",
      "Epoch 1 | Step 275200 | Avg Loss: 0.0148 | Grad Norm: 0.00955636\n",
      "Epoch 1 | Step 275300 | Avg Loss: 0.0145 | Grad Norm: 0.00974949\n",
      "Epoch 1 | Step 275400 | Avg Loss: 0.0145 | Grad Norm: 0.00871810\n",
      "Epoch 1 | Step 275500 | Avg Loss: 0.0143 | Grad Norm: 0.00839928\n",
      "Epoch 1 | Step 275600 | Avg Loss: 0.0144 | Grad Norm: 0.01109088\n",
      "Epoch 1 | Step 275700 | Avg Loss: 0.0140 | Grad Norm: 0.00928511\n",
      "Epoch 1 | Step 275800 | Avg Loss: 0.0143 | Grad Norm: 0.00962106\n",
      "Epoch 1 | Step 275900 | Avg Loss: 0.0148 | Grad Norm: 0.00966095\n",
      "Epoch 1 | Step 276000 | Avg Loss: 0.0148 | Grad Norm: 0.01051378\n",
      "Epoch 1 | Step 276100 | Avg Loss: 0.0148 | Grad Norm: 0.00873100\n",
      "Epoch 1 | Step 276200 | Avg Loss: 0.0146 | Grad Norm: 0.01098342\n",
      "Epoch 1 | Step 276300 | Avg Loss: 0.0146 | Grad Norm: 0.00975858\n",
      "Epoch 1 | Step 276400 | Avg Loss: 0.0151 | Grad Norm: 0.01014122\n",
      "Epoch 1 | Step 276500 | Avg Loss: 0.0148 | Grad Norm: 0.01157372\n",
      "Epoch 1 | Step 276600 | Avg Loss: 0.0140 | Grad Norm: 0.00896450\n",
      "Epoch 1 | Step 276700 | Avg Loss: 0.0139 | Grad Norm: 0.00923702\n",
      "Epoch 1 | Step 276800 | Avg Loss: 0.0141 | Grad Norm: 0.00994033\n",
      "Epoch 1 | Step 276900 | Avg Loss: 0.0146 | Grad Norm: 0.01033800\n",
      "Epoch 1 | Step 277000 | Avg Loss: 0.0145 | Grad Norm: 0.00950325\n",
      "Epoch 1 | Step 277100 | Avg Loss: 0.0140 | Grad Norm: 0.00991905\n",
      "Epoch 1 | Step 277200 | Avg Loss: 0.0139 | Grad Norm: 0.01110584\n",
      "Epoch 1 | Step 277300 | Avg Loss: 0.0140 | Grad Norm: 0.00976831\n",
      "Epoch 1 | Step 277400 | Avg Loss: 0.0142 | Grad Norm: 0.01059944\n",
      "Epoch 1 | Step 277500 | Avg Loss: 0.0143 | Grad Norm: 0.00995473\n",
      "Epoch 1 | Step 277600 | Avg Loss: 0.0141 | Grad Norm: 0.00980003\n",
      "Epoch 1 | Step 277700 | Avg Loss: 0.0141 | Grad Norm: 0.01200281\n",
      "Epoch 1 | Step 277800 | Avg Loss: 0.0141 | Grad Norm: 0.00888955\n",
      "Epoch 1 | Step 277900 | Avg Loss: 0.0140 | Grad Norm: 0.00950458\n",
      "Epoch 1 | Step 278000 | Avg Loss: 0.0144 | Grad Norm: 0.00973734\n",
      "Epoch 1 | Step 278100 | Avg Loss: 0.0146 | Grad Norm: 0.00849017\n",
      "Epoch 1 | Step 278200 | Avg Loss: 0.0145 | Grad Norm: 0.00830799\n",
      "Epoch 1 | Step 278300 | Avg Loss: 0.0143 | Grad Norm: 0.01190617\n",
      "Epoch 1 | Step 278400 | Avg Loss: 0.0143 | Grad Norm: 0.01091211\n",
      "Epoch 1 | Step 278500 | Avg Loss: 0.0142 | Grad Norm: 0.00884435\n",
      "Epoch 1 | Step 278600 | Avg Loss: 0.0141 | Grad Norm: 0.01013648\n",
      "Epoch 1 | Step 278700 | Avg Loss: 0.0142 | Grad Norm: 0.00917064\n",
      "Epoch 1 | Step 278800 | Avg Loss: 0.0142 | Grad Norm: 0.00918042\n",
      "Epoch 1 | Step 278900 | Avg Loss: 0.0147 | Grad Norm: 0.01053244\n",
      "Epoch 1 | Step 279000 | Avg Loss: 0.0145 | Grad Norm: 0.00871941\n",
      "Epoch 1 | Step 279100 | Avg Loss: 0.0146 | Grad Norm: 0.00895165\n",
      "Epoch 1 | Step 279200 | Avg Loss: 0.0146 | Grad Norm: 0.00940044\n",
      "Epoch 1 | Step 279300 | Avg Loss: 0.0148 | Grad Norm: 0.01154238\n",
      "Epoch 1 | Step 279400 | Avg Loss: 0.0148 | Grad Norm: 0.00954406\n",
      "Epoch 1 | Step 279500 | Avg Loss: 0.0150 | Grad Norm: 0.01027407\n",
      "Epoch 1 | Step 279600 | Avg Loss: 0.0152 | Grad Norm: 0.01026953\n",
      "Epoch 1 | Step 279700 | Avg Loss: 0.0151 | Grad Norm: 0.00999311\n",
      "Epoch 1 | Step 279800 | Avg Loss: 0.0146 | Grad Norm: 0.01028972\n",
      "Epoch 1 | Step 279900 | Avg Loss: 0.0153 | Grad Norm: 0.00903406\n",
      "Epoch 1 | Step 280000 | Avg Loss: 0.0151 | Grad Norm: 0.01175875\n",
      "Epoch 1 | Step 280100 | Avg Loss: 0.0145 | Grad Norm: 0.01068807\n",
      "Epoch 1 | Step 280200 | Avg Loss: 0.0143 | Grad Norm: 0.01138086\n",
      "Epoch 1 | Step 280300 | Avg Loss: 0.0146 | Grad Norm: 0.00897457\n",
      "Epoch 1 | Step 280400 | Avg Loss: 0.0148 | Grad Norm: 0.00924268\n",
      "Epoch 1 | Step 280500 | Avg Loss: 0.0143 | Grad Norm: 0.00889673\n",
      "Epoch 1 | Step 280600 | Avg Loss: 0.0146 | Grad Norm: 0.01015192\n",
      "Epoch 1 | Step 280700 | Avg Loss: 0.0148 | Grad Norm: 0.00875279\n",
      "Epoch 1 | Step 280800 | Avg Loss: 0.0147 | Grad Norm: 0.00979921\n",
      "Epoch 1 | Step 280900 | Avg Loss: 0.0146 | Grad Norm: 0.00860298\n",
      "Epoch 1 | Step 281000 | Avg Loss: 0.0148 | Grad Norm: 0.00893666\n",
      "Epoch 1 | Step 281100 | Avg Loss: 0.0145 | Grad Norm: 0.01190126\n",
      "Epoch 1 | Step 281200 | Avg Loss: 0.0146 | Grad Norm: 0.01127957\n",
      "Epoch 1 | Step 281300 | Avg Loss: 0.0147 | Grad Norm: 0.00910515\n",
      "Epoch 1 | Step 281400 | Avg Loss: 0.0149 | Grad Norm: 0.01055893\n",
      "Epoch 1 | Step 281500 | Avg Loss: 0.0149 | Grad Norm: 0.00948037\n",
      "Epoch 1 | Step 281600 | Avg Loss: 0.0147 | Grad Norm: 0.00963279\n",
      "Epoch 1 | Step 281700 | Avg Loss: 0.0144 | Grad Norm: 0.00945743\n",
      "Epoch 1 | Step 281800 | Avg Loss: 0.0147 | Grad Norm: 0.00955519\n",
      "Epoch 1 | Step 281900 | Avg Loss: 0.0148 | Grad Norm: 0.01211564\n",
      "Epoch 1 | Step 282000 | Avg Loss: 0.0145 | Grad Norm: 0.01037916\n",
      "Epoch 1 | Step 282100 | Avg Loss: 0.0145 | Grad Norm: 0.01046673\n",
      "Epoch 1 | Step 282200 | Avg Loss: 0.0142 | Grad Norm: 0.00825882\n",
      "Epoch 1 | Step 282300 | Avg Loss: 0.0146 | Grad Norm: 0.00917515\n",
      "Epoch 1 | Step 282400 | Avg Loss: 0.0145 | Grad Norm: 0.00913081\n",
      "Epoch 1 | Step 282500 | Avg Loss: 0.0146 | Grad Norm: 0.00947430\n",
      "Epoch 1 | Step 282600 | Avg Loss: 0.0145 | Grad Norm: 0.00991339\n",
      "Epoch 1 | Step 282700 | Avg Loss: 0.0143 | Grad Norm: 0.01057647\n",
      "Epoch 1 | Step 282800 | Avg Loss: 0.0142 | Grad Norm: 0.00929973\n",
      "Epoch 1 | Step 282900 | Avg Loss: 0.0146 | Grad Norm: 0.01114649\n",
      "Epoch 1 | Step 283000 | Avg Loss: 0.0150 | Grad Norm: 0.01013539\n",
      "Epoch 1 | Step 283100 | Avg Loss: 0.0144 | Grad Norm: 0.00973011\n",
      "Epoch 1 | Step 283200 | Avg Loss: 0.0144 | Grad Norm: 0.01008152\n",
      "Epoch 1 | Step 283300 | Avg Loss: 0.0147 | Grad Norm: 0.01016630\n",
      "Epoch 1 | Step 283400 | Avg Loss: 0.0147 | Grad Norm: 0.00995736\n",
      "Epoch 1 | Step 283500 | Avg Loss: 0.0146 | Grad Norm: 0.00846596\n",
      "Epoch 1 | Step 283600 | Avg Loss: 0.0145 | Grad Norm: 0.00972769\n",
      "Epoch 1 | Step 283700 | Avg Loss: 0.0144 | Grad Norm: 0.01167272\n",
      "Epoch 1 | Step 283800 | Avg Loss: 0.0144 | Grad Norm: 0.01222334\n",
      "Epoch 1 | Step 283900 | Avg Loss: 0.0146 | Grad Norm: 0.00967868\n",
      "Epoch 1 | Step 284000 | Avg Loss: 0.0144 | Grad Norm: 0.01037192\n",
      "Epoch 1 | Step 284100 | Avg Loss: 0.0144 | Grad Norm: 0.00839952\n",
      "Epoch 1 | Step 284200 | Avg Loss: 0.0143 | Grad Norm: 0.00859207\n",
      "Epoch 1 | Step 284300 | Avg Loss: 0.0144 | Grad Norm: 0.00939653\n",
      "Epoch 1 | Step 284400 | Avg Loss: 0.0142 | Grad Norm: 0.00914209\n",
      "Epoch 1 | Step 284500 | Avg Loss: 0.0141 | Grad Norm: 0.01022470\n",
      "Epoch 1 | Step 284600 | Avg Loss: 0.0147 | Grad Norm: 0.01008084\n",
      "Epoch 1 | Step 284700 | Avg Loss: 0.0149 | Grad Norm: 0.00974705\n",
      "Epoch 1 | Step 284800 | Avg Loss: 0.0148 | Grad Norm: 0.00934901\n",
      "Epoch 1 | Step 284900 | Avg Loss: 0.0148 | Grad Norm: 0.01070842\n",
      "Epoch 1 | Step 285000 | Avg Loss: 0.0150 | Grad Norm: 0.00986192\n",
      "Epoch 1 | Step 285100 | Avg Loss: 0.0150 | Grad Norm: 0.01039707\n",
      "Epoch 1 | Step 285200 | Avg Loss: 0.0148 | Grad Norm: 0.00777791\n",
      "Epoch 1 | Step 285300 | Avg Loss: 0.0143 | Grad Norm: 0.00918825\n",
      "Epoch 1 | Step 285400 | Avg Loss: 0.0142 | Grad Norm: 0.01088201\n",
      "Epoch 1 | Step 285500 | Avg Loss: 0.0140 | Grad Norm: 0.00843949\n",
      "Epoch 1 | Step 285600 | Avg Loss: 0.0142 | Grad Norm: 0.01091192\n",
      "Epoch 1 | Step 285700 | Avg Loss: 0.0140 | Grad Norm: 0.00810169\n",
      "Epoch 1 | Step 285800 | Avg Loss: 0.0141 | Grad Norm: 0.00940670\n",
      "Epoch 1 | Step 285900 | Avg Loss: 0.0138 | Grad Norm: 0.00978725\n",
      "Epoch 1 | Step 286000 | Avg Loss: 0.0139 | Grad Norm: 0.00838336\n",
      "Epoch 1 | Step 286100 | Avg Loss: 0.0140 | Grad Norm: 0.00841264\n",
      "Epoch 1 | Step 286200 | Avg Loss: 0.0142 | Grad Norm: 0.00896163\n",
      "Epoch 1 | Step 286300 | Avg Loss: 0.0142 | Grad Norm: 0.01011657\n",
      "Epoch 1 | Step 286400 | Avg Loss: 0.0142 | Grad Norm: 0.01164019\n",
      "Epoch 1 | Step 286500 | Avg Loss: 0.0142 | Grad Norm: 0.00936550\n",
      "Epoch 1 | Step 286600 | Avg Loss: 0.0141 | Grad Norm: 0.00918806\n",
      "Epoch 1 | Step 286700 | Avg Loss: 0.0143 | Grad Norm: 0.00998686\n",
      "Epoch 1 | Step 286800 | Avg Loss: 0.0148 | Grad Norm: 0.01048746\n",
      "Epoch 1 | Step 286900 | Avg Loss: 0.0152 | Grad Norm: 0.00899263\n",
      "Epoch 1 | Step 287000 | Avg Loss: 0.0150 | Grad Norm: 0.01081219\n",
      "Epoch 1 | Step 287100 | Avg Loss: 0.0147 | Grad Norm: 0.01024031\n",
      "Epoch 1 | Step 287200 | Avg Loss: 0.0150 | Grad Norm: 0.00972312\n",
      "Epoch 1 | Step 287300 | Avg Loss: 0.0147 | Grad Norm: 0.00963946\n",
      "Epoch 1 | Step 287400 | Avg Loss: 0.0143 | Grad Norm: 0.00894777\n",
      "Epoch 1 | Step 287500 | Avg Loss: 0.0150 | Grad Norm: 0.00983792\n",
      "Epoch 1 | Step 287600 | Avg Loss: 0.0149 | Grad Norm: 0.00957771\n",
      "Epoch 1 | Step 287700 | Avg Loss: 0.0147 | Grad Norm: 0.00955590\n",
      "Epoch 1 | Step 287800 | Avg Loss: 0.0144 | Grad Norm: 0.01032655\n",
      "Epoch 1 | Step 287900 | Avg Loss: 0.0147 | Grad Norm: 0.00846822\n",
      "Epoch 1 | Step 288000 | Avg Loss: 0.0146 | Grad Norm: 0.00850922\n",
      "Epoch 1 | Step 288100 | Avg Loss: 0.0145 | Grad Norm: 0.00948101\n",
      "Epoch 1 | Step 288200 | Avg Loss: 0.0141 | Grad Norm: 0.00987480\n",
      "Epoch 1 | Step 288300 | Avg Loss: 0.0143 | Grad Norm: 0.01027612\n",
      "Epoch 1 | Step 288400 | Avg Loss: 0.0145 | Grad Norm: 0.00988754\n",
      "Epoch 1 | Step 288500 | Avg Loss: 0.0144 | Grad Norm: 0.01002837\n",
      "Epoch 1 | Step 288600 | Avg Loss: 0.0148 | Grad Norm: 0.01014008\n",
      "Epoch 1 | Step 288700 | Avg Loss: 0.0150 | Grad Norm: 0.01005975\n",
      "Epoch 1 | Step 288800 | Avg Loss: 0.0148 | Grad Norm: 0.00962664\n",
      "Epoch 1 | Step 288900 | Avg Loss: 0.0145 | Grad Norm: 0.00931615\n",
      "Epoch 1 | Step 289000 | Avg Loss: 0.0146 | Grad Norm: 0.00896777\n",
      "Epoch 1 | Step 289100 | Avg Loss: 0.0150 | Grad Norm: 0.00952483\n",
      "Epoch 1 | Step 289200 | Avg Loss: 0.0149 | Grad Norm: 0.00854343\n",
      "Epoch 1 | Step 289300 | Avg Loss: 0.0146 | Grad Norm: 0.00944195\n",
      "Epoch 1 | Step 289400 | Avg Loss: 0.0142 | Grad Norm: 0.00922237\n",
      "Epoch 1 | Step 289500 | Avg Loss: 0.0141 | Grad Norm: 0.00936852\n",
      "Epoch 1 | Step 289600 | Avg Loss: 0.0144 | Grad Norm: 0.00950621\n",
      "Epoch 1 | Step 289700 | Avg Loss: 0.0146 | Grad Norm: 0.01099871\n",
      "Epoch 1 | Step 289800 | Avg Loss: 0.0147 | Grad Norm: 0.01062604\n",
      "Epoch 1 | Step 289900 | Avg Loss: 0.0148 | Grad Norm: 0.01067494\n",
      "Epoch 1 | Step 290000 | Avg Loss: 0.0150 | Grad Norm: 0.01017860\n",
      "Epoch 1 | Step 290100 | Avg Loss: 0.0150 | Grad Norm: 0.00880275\n",
      "Epoch 1 | Step 290200 | Avg Loss: 0.0147 | Grad Norm: 0.01058773\n",
      "Epoch 1 | Step 290300 | Avg Loss: 0.0144 | Grad Norm: 0.00779130\n",
      "Epoch 1 | Step 290400 | Avg Loss: 0.0140 | Grad Norm: 0.00995512\n",
      "Epoch 1 | Step 290500 | Avg Loss: 0.0141 | Grad Norm: 0.01022941\n",
      "Epoch 1 | Step 290600 | Avg Loss: 0.0144 | Grad Norm: 0.01152757\n",
      "Epoch 1 | Step 290700 | Avg Loss: 0.0141 | Grad Norm: 0.00853251\n",
      "Epoch 1 | Step 290800 | Avg Loss: 0.0141 | Grad Norm: 0.00898590\n",
      "Epoch 1 | Step 290900 | Avg Loss: 0.0141 | Grad Norm: 0.00962081\n",
      "Epoch 1 | Step 291000 | Avg Loss: 0.0147 | Grad Norm: 0.00906719\n",
      "Epoch 1 | Step 291100 | Avg Loss: 0.0142 | Grad Norm: 0.01043475\n",
      "Epoch 1 | Step 291200 | Avg Loss: 0.0146 | Grad Norm: 0.00901590\n",
      "Epoch 1 | Step 291300 | Avg Loss: 0.0147 | Grad Norm: 0.00945602\n",
      "Epoch 1 | Step 291400 | Avg Loss: 0.0151 | Grad Norm: 0.01208701\n",
      "Epoch 1 | Step 291500 | Avg Loss: 0.0147 | Grad Norm: 0.00978058\n",
      "Epoch 1 | Step 291600 | Avg Loss: 0.0145 | Grad Norm: 0.00801137\n",
      "Epoch 1 | Step 291700 | Avg Loss: 0.0149 | Grad Norm: 0.01057733\n",
      "Epoch 1 | Step 291800 | Avg Loss: 0.0150 | Grad Norm: 0.00999325\n",
      "Epoch 1 | Step 291900 | Avg Loss: 0.0148 | Grad Norm: 0.00824634\n",
      "Epoch 1 | Step 292000 | Avg Loss: 0.0150 | Grad Norm: 0.00953435\n",
      "Epoch 1 | Step 292100 | Avg Loss: 0.0149 | Grad Norm: 0.00935346\n",
      "Epoch 1 | Step 292200 | Avg Loss: 0.0146 | Grad Norm: 0.00887937\n",
      "Epoch 1 | Step 292300 | Avg Loss: 0.0145 | Grad Norm: 0.00996013\n",
      "Epoch 1 | Step 292400 | Avg Loss: 0.0142 | Grad Norm: 0.00983951\n",
      "Epoch 1 | Step 292500 | Avg Loss: 0.0140 | Grad Norm: 0.00978053\n",
      "Epoch 1 | Step 292600 | Avg Loss: 0.0143 | Grad Norm: 0.00922232\n",
      "Epoch 1 | Step 292700 | Avg Loss: 0.0142 | Grad Norm: 0.00908001\n",
      "Epoch 1 | Step 292800 | Avg Loss: 0.0145 | Grad Norm: 0.00971873\n",
      "Epoch 1 | Step 292900 | Avg Loss: 0.0150 | Grad Norm: 0.01038889\n",
      "Epoch 1 | Step 293000 | Avg Loss: 0.0148 | Grad Norm: 0.01090088\n",
      "Epoch 1 | Step 293100 | Avg Loss: 0.0147 | Grad Norm: 0.01060531\n",
      "Epoch 1 | Step 293200 | Avg Loss: 0.0151 | Grad Norm: 0.00959401\n",
      "Epoch 1 | Step 293300 | Avg Loss: 0.0147 | Grad Norm: 0.00973977\n",
      "Epoch 1 | Step 293400 | Avg Loss: 0.0148 | Grad Norm: 0.00827920\n",
      "Epoch 1 | Step 293500 | Avg Loss: 0.0143 | Grad Norm: 0.01012741\n",
      "Epoch 1 | Step 293600 | Avg Loss: 0.0144 | Grad Norm: 0.01245322\n",
      "Epoch 1 | Step 293700 | Avg Loss: 0.0143 | Grad Norm: 0.01112974\n",
      "Epoch 1 | Step 293800 | Avg Loss: 0.0143 | Grad Norm: 0.01014862\n",
      "Epoch 1 | Step 293900 | Avg Loss: 0.0144 | Grad Norm: 0.01219651\n",
      "Epoch 1 | Step 294000 | Avg Loss: 0.0142 | Grad Norm: 0.01159287\n",
      "Epoch 1 | Step 294100 | Avg Loss: 0.0141 | Grad Norm: 0.01062929\n",
      "Epoch 1 | Step 294200 | Avg Loss: 0.0142 | Grad Norm: 0.00868910\n",
      "Epoch 1 | Step 294300 | Avg Loss: 0.0145 | Grad Norm: 0.00840256\n",
      "Epoch 1 | Step 294400 | Avg Loss: 0.0145 | Grad Norm: 0.00914092\n",
      "Epoch 1 | Step 294500 | Avg Loss: 0.0147 | Grad Norm: 0.00985015\n",
      "Epoch 1 | Step 294600 | Avg Loss: 0.0147 | Grad Norm: 0.00978118\n",
      "Epoch 1 | Step 294700 | Avg Loss: 0.0151 | Grad Norm: 0.00989777\n",
      "Epoch 1 | Step 294800 | Avg Loss: 0.0150 | Grad Norm: 0.00977436\n",
      "Epoch 1 | Step 294900 | Avg Loss: 0.0151 | Grad Norm: 0.01003071\n",
      "Epoch 1 | Step 295000 | Avg Loss: 0.0148 | Grad Norm: 0.00983929\n",
      "Epoch 1 | Step 295100 | Avg Loss: 0.0145 | Grad Norm: 0.01006574\n",
      "Epoch 1 | Step 295200 | Avg Loss: 0.0148 | Grad Norm: 0.00948570\n",
      "Epoch 1 | Step 295300 | Avg Loss: 0.0144 | Grad Norm: 0.01059909\n",
      "Epoch 1 | Step 295400 | Avg Loss: 0.0146 | Grad Norm: 0.00922821\n",
      "Epoch 1 | Step 295500 | Avg Loss: 0.0145 | Grad Norm: 0.00978597\n",
      "Epoch 1 | Step 295600 | Avg Loss: 0.0146 | Grad Norm: 0.00874034\n",
      "Epoch 1 | Step 295700 | Avg Loss: 0.0143 | Grad Norm: 0.00851393\n",
      "Epoch 1 | Step 295800 | Avg Loss: 0.0144 | Grad Norm: 0.00984326\n",
      "Epoch 1 | Step 295900 | Avg Loss: 0.0146 | Grad Norm: 0.01038624\n",
      "Epoch 1 | Step 296000 | Avg Loss: 0.0148 | Grad Norm: 0.01039248\n",
      "Epoch 1 | Step 296100 | Avg Loss: 0.0148 | Grad Norm: 0.00985704\n",
      "Epoch 1 | Step 296200 | Avg Loss: 0.0152 | Grad Norm: 0.01083609\n",
      "Epoch 1 | Step 296300 | Avg Loss: 0.0156 | Grad Norm: 0.01120246\n",
      "Epoch 1 | Step 296400 | Avg Loss: 0.0153 | Grad Norm: 0.00930145\n",
      "Epoch 1 | Step 296500 | Avg Loss: 0.0147 | Grad Norm: 0.00995627\n",
      "Epoch 1 | Step 296600 | Avg Loss: 0.0146 | Grad Norm: 0.00826498\n",
      "Epoch 1 | Step 296700 | Avg Loss: 0.0146 | Grad Norm: 0.01102626\n",
      "Epoch 1 | Step 296800 | Avg Loss: 0.0146 | Grad Norm: 0.00973975\n",
      "Epoch 1 | Step 296900 | Avg Loss: 0.0146 | Grad Norm: 0.00918414\n",
      "Epoch 1 | Step 297000 | Avg Loss: 0.0150 | Grad Norm: 0.00881295\n",
      "Epoch 1 | Step 297100 | Avg Loss: 0.0152 | Grad Norm: 0.01039411\n",
      "Epoch 1 | Step 297200 | Avg Loss: 0.0149 | Grad Norm: 0.00911129\n",
      "Epoch 1 | Step 297300 | Avg Loss: 0.0147 | Grad Norm: 0.01086419\n",
      "Epoch 1 | Step 297400 | Avg Loss: 0.0147 | Grad Norm: 0.00929743\n",
      "Epoch 1 | Step 297500 | Avg Loss: 0.0152 | Grad Norm: 0.00821211\n",
      "Epoch 1 | Step 297600 | Avg Loss: 0.0151 | Grad Norm: 0.00923466\n",
      "Epoch 1 | Step 297700 | Avg Loss: 0.0152 | Grad Norm: 0.00943885\n",
      "Epoch 1 | Step 297800 | Avg Loss: 0.0152 | Grad Norm: 0.01059108\n",
      "Epoch 1 | Step 297900 | Avg Loss: 0.0153 | Grad Norm: 0.01011718\n",
      "Epoch 1 | Step 298000 | Avg Loss: 0.0150 | Grad Norm: 0.00742975\n",
      "Epoch 1 | Step 298100 | Avg Loss: 0.0148 | Grad Norm: 0.00924055\n",
      "Epoch 1 | Step 298200 | Avg Loss: 0.0142 | Grad Norm: 0.00920887\n",
      "Epoch 1 | Step 298300 | Avg Loss: 0.0145 | Grad Norm: 0.00943096\n",
      "Epoch 1 | Step 298400 | Avg Loss: 0.0144 | Grad Norm: 0.00949715\n",
      "Epoch 1 | Step 298500 | Avg Loss: 0.0145 | Grad Norm: 0.00959044\n",
      "Epoch 1 | Step 298600 | Avg Loss: 0.0147 | Grad Norm: 0.00833876\n",
      "Epoch 1 | Step 298700 | Avg Loss: 0.0147 | Grad Norm: 0.00859927\n",
      "Epoch 1 | Step 298800 | Avg Loss: 0.0146 | Grad Norm: 0.00875863\n",
      "Epoch 1 | Step 298900 | Avg Loss: 0.0145 | Grad Norm: 0.01300057\n",
      "Epoch 1 | Step 299000 | Avg Loss: 0.0141 | Grad Norm: 0.00909738\n",
      "Epoch 1 | Step 299100 | Avg Loss: 0.0145 | Grad Norm: 0.00964048\n",
      "Epoch 1 | Step 299200 | Avg Loss: 0.0144 | Grad Norm: 0.00958555\n",
      "Epoch 1 | Step 299300 | Avg Loss: 0.0143 | Grad Norm: 0.00969744\n",
      "Epoch 1 | Step 299400 | Avg Loss: 0.0143 | Grad Norm: 0.00906417\n",
      "Epoch 1 | Step 299500 | Avg Loss: 0.0143 | Grad Norm: 0.01171126\n",
      "Epoch 1 | Step 299600 | Avg Loss: 0.0143 | Grad Norm: 0.00950640\n",
      "Epoch 1 | Step 299700 | Avg Loss: 0.0140 | Grad Norm: 0.00902733\n",
      "Epoch 1 | Step 299800 | Avg Loss: 0.0141 | Grad Norm: 0.00916202\n",
      "Epoch 1 | Step 299900 | Avg Loss: 0.0142 | Grad Norm: 0.00818012\n",
      "Epoch 1 | Step 300000 | Avg Loss: 0.0143 | Grad Norm: 0.00940838\n",
      "Saving model at step300000\n",
      "Epoch 1 | Step 300100 | Avg Loss: 0.0140 | Grad Norm: 0.00824499\n",
      "Epoch 1 | Step 300200 | Avg Loss: 0.0142 | Grad Norm: 0.00828599\n",
      "Epoch 1 | Step 300300 | Avg Loss: 0.0139 | Grad Norm: 0.00917697\n",
      "Epoch 1 | Step 300400 | Avg Loss: 0.0137 | Grad Norm: 0.00877064\n",
      "Epoch 1 | Step 300500 | Avg Loss: 0.0138 | Grad Norm: 0.00851467\n",
      "Epoch 1 | Step 300600 | Avg Loss: 0.0140 | Grad Norm: 0.00925170\n",
      "Epoch 1 | Step 300700 | Avg Loss: 0.0136 | Grad Norm: 0.00848602\n",
      "Epoch 1 | Step 300800 | Avg Loss: 0.0138 | Grad Norm: 0.00968066\n",
      "Epoch 1 | Step 300900 | Avg Loss: 0.0139 | Grad Norm: 0.00982977\n",
      "Epoch 1 | Step 301000 | Avg Loss: 0.0139 | Grad Norm: 0.01212214\n",
      "Epoch 1 | Step 301100 | Avg Loss: 0.0140 | Grad Norm: 0.00878252\n",
      "Epoch 1 | Step 301200 | Avg Loss: 0.0140 | Grad Norm: 0.00917924\n",
      "Epoch 1 | Step 301300 | Avg Loss: 0.0145 | Grad Norm: 0.00806017\n",
      "Epoch 1 | Step 301400 | Avg Loss: 0.0144 | Grad Norm: 0.01122143\n",
      "Epoch 1 | Step 301500 | Avg Loss: 0.0146 | Grad Norm: 0.01053773\n",
      "Epoch 1 | Step 301600 | Avg Loss: 0.0143 | Grad Norm: 0.00999116\n",
      "Epoch 1 | Step 301700 | Avg Loss: 0.0146 | Grad Norm: 0.01042218\n",
      "Epoch 1 | Step 301800 | Avg Loss: 0.0146 | Grad Norm: 0.00925255\n",
      "Epoch 1 | Step 301900 | Avg Loss: 0.0143 | Grad Norm: 0.01083170\n",
      "Epoch 1 | Step 302000 | Avg Loss: 0.0143 | Grad Norm: 0.00946537\n",
      "Epoch 1 | Step 302100 | Avg Loss: 0.0141 | Grad Norm: 0.00792453\n",
      "Epoch 1 | Step 302200 | Avg Loss: 0.0142 | Grad Norm: 0.00914433\n",
      "Epoch 1 | Step 302300 | Avg Loss: 0.0142 | Grad Norm: 0.00986790\n",
      "Epoch 1 | Step 302400 | Avg Loss: 0.0143 | Grad Norm: 0.00993189\n",
      "Epoch 1 | Step 302500 | Avg Loss: 0.0142 | Grad Norm: 0.01000960\n",
      "Epoch 1 | Step 302600 | Avg Loss: 0.0143 | Grad Norm: 0.00965647\n",
      "Epoch 1 | Step 302700 | Avg Loss: 0.0142 | Grad Norm: 0.00831816\n",
      "Epoch 1 | Step 302800 | Avg Loss: 0.0144 | Grad Norm: 0.01063460\n",
      "Epoch 1 | Step 302900 | Avg Loss: 0.0143 | Grad Norm: 0.00949538\n",
      "Epoch 1 | Step 303000 | Avg Loss: 0.0141 | Grad Norm: 0.00983121\n",
      "Epoch 1 | Step 303100 | Avg Loss: 0.0145 | Grad Norm: 0.00943810\n",
      "Epoch 1 | Step 303200 | Avg Loss: 0.0145 | Grad Norm: 0.01209638\n",
      "Epoch 1 | Step 303300 | Avg Loss: 0.0145 | Grad Norm: 0.00824554\n",
      "Epoch 1 | Step 303400 | Avg Loss: 0.0145 | Grad Norm: 0.00883783\n",
      "Epoch 1 | Step 303500 | Avg Loss: 0.0142 | Grad Norm: 0.00936998\n",
      "Epoch 1 | Step 303600 | Avg Loss: 0.0142 | Grad Norm: 0.00960479\n",
      "Epoch 1 | Step 303700 | Avg Loss: 0.0143 | Grad Norm: 0.00874445\n",
      "Epoch 1 | Step 303800 | Avg Loss: 0.0140 | Grad Norm: 0.00865688\n",
      "Epoch 1 | Step 303900 | Avg Loss: 0.0144 | Grad Norm: 0.00883145\n",
      "Epoch 1 | Step 304000 | Avg Loss: 0.0145 | Grad Norm: 0.00883842\n",
      "Epoch 1 | Step 304100 | Avg Loss: 0.0143 | Grad Norm: 0.00994691\n",
      "Epoch 1 | Step 304200 | Avg Loss: 0.0144 | Grad Norm: 0.00951779\n",
      "Epoch 1 | Step 304300 | Avg Loss: 0.0143 | Grad Norm: 0.00954839\n",
      "Epoch 1 | Step 304400 | Avg Loss: 0.0141 | Grad Norm: 0.00902228\n",
      "Epoch 1 | Step 304500 | Avg Loss: 0.0141 | Grad Norm: 0.00939297\n",
      "Epoch 1 | Step 304600 | Avg Loss: 0.0142 | Grad Norm: 0.01050336\n",
      "Epoch 1 | Step 304700 | Avg Loss: 0.0140 | Grad Norm: 0.00885994\n",
      "Epoch 1 | Step 304800 | Avg Loss: 0.0140 | Grad Norm: 0.00936478\n",
      "Epoch 1 | Step 304900 | Avg Loss: 0.0138 | Grad Norm: 0.00798101\n",
      "Epoch 1 | Step 305000 | Avg Loss: 0.0141 | Grad Norm: 0.00980737\n",
      "Epoch 1 | Step 305100 | Avg Loss: 0.0142 | Grad Norm: 0.01008354\n",
      "Epoch 1 | Step 305200 | Avg Loss: 0.0141 | Grad Norm: 0.00980593\n",
      "Epoch 1 | Step 305300 | Avg Loss: 0.0141 | Grad Norm: 0.00932670\n",
      "Epoch 1 | Step 305400 | Avg Loss: 0.0140 | Grad Norm: 0.00980210\n",
      "Epoch 1 | Step 305500 | Avg Loss: 0.0144 | Grad Norm: 0.00966800\n",
      "Epoch 1 | Step 305600 | Avg Loss: 0.0144 | Grad Norm: 0.00944516\n",
      "Epoch 1 | Step 305700 | Avg Loss: 0.0140 | Grad Norm: 0.00879246\n",
      "Epoch 1 | Step 305800 | Avg Loss: 0.0145 | Grad Norm: 0.01076188\n",
      "Epoch 1 | Step 305900 | Avg Loss: 0.0142 | Grad Norm: 0.00917218\n",
      "Epoch 1 | Step 306000 | Avg Loss: 0.0145 | Grad Norm: 0.00898969\n",
      "Epoch 1 | Step 306100 | Avg Loss: 0.0146 | Grad Norm: 0.01099615\n",
      "Epoch 1 | Step 306200 | Avg Loss: 0.0140 | Grad Norm: 0.00819603\n",
      "Epoch 1 | Step 306300 | Avg Loss: 0.0142 | Grad Norm: 0.00847276\n",
      "Epoch 1 | Step 306400 | Avg Loss: 0.0143 | Grad Norm: 0.00874841\n",
      "Epoch 1 | Step 306500 | Avg Loss: 0.0143 | Grad Norm: 0.01186385\n",
      "Epoch 1 | Step 306600 | Avg Loss: 0.0142 | Grad Norm: 0.00951552\n",
      "Epoch 1 | Step 306700 | Avg Loss: 0.0143 | Grad Norm: 0.00984870\n",
      "Epoch 1 | Step 306800 | Avg Loss: 0.0142 | Grad Norm: 0.00813805\n",
      "Epoch 1 | Step 306900 | Avg Loss: 0.0143 | Grad Norm: 0.00900145\n",
      "Epoch 1 | Step 307000 | Avg Loss: 0.0143 | Grad Norm: 0.00936694\n",
      "Epoch 1 | Step 307100 | Avg Loss: 0.0148 | Grad Norm: 0.00933563\n",
      "Epoch 1 | Step 307200 | Avg Loss: 0.0145 | Grad Norm: 0.00951815\n",
      "Epoch 1 | Step 307300 | Avg Loss: 0.0145 | Grad Norm: 0.00970374\n",
      "Epoch 1 | Step 307400 | Avg Loss: 0.0145 | Grad Norm: 0.00933687\n",
      "Epoch 1 | Step 307500 | Avg Loss: 0.0147 | Grad Norm: 0.00860832\n",
      "Epoch 1 | Step 307600 | Avg Loss: 0.0144 | Grad Norm: 0.01023538\n",
      "Epoch 1 | Step 307700 | Avg Loss: 0.0143 | Grad Norm: 0.01101131\n",
      "Epoch 1 | Step 307800 | Avg Loss: 0.0143 | Grad Norm: 0.01001917\n",
      "Epoch 1 | Step 307900 | Avg Loss: 0.0142 | Grad Norm: 0.00920971\n",
      "Epoch 1 | Step 308000 | Avg Loss: 0.0147 | Grad Norm: 0.01085254\n",
      "Epoch 1 | Step 308100 | Avg Loss: 0.0145 | Grad Norm: 0.01050760\n",
      "Epoch 1 | Step 308200 | Avg Loss: 0.0140 | Grad Norm: 0.00993477\n",
      "Epoch 1 | Step 308300 | Avg Loss: 0.0141 | Grad Norm: 0.01008751\n",
      "Epoch 1 | Step 308400 | Avg Loss: 0.0144 | Grad Norm: 0.00912644\n",
      "Epoch 1 | Step 308500 | Avg Loss: 0.0145 | Grad Norm: 0.00849260\n",
      "Epoch 1 | Step 308600 | Avg Loss: 0.0141 | Grad Norm: 0.01073258\n",
      "Epoch 1 | Step 308700 | Avg Loss: 0.0143 | Grad Norm: 0.00920221\n",
      "Epoch 1 | Step 308800 | Avg Loss: 0.0144 | Grad Norm: 0.00985142\n",
      "Epoch 1 | Step 308900 | Avg Loss: 0.0144 | Grad Norm: 0.00918975\n",
      "Epoch 1 | Step 309000 | Avg Loss: 0.0143 | Grad Norm: 0.00878759\n",
      "Epoch 1 | Step 309100 | Avg Loss: 0.0141 | Grad Norm: 0.01084375\n",
      "Epoch 1 | Step 309200 | Avg Loss: 0.0142 | Grad Norm: 0.00991303\n",
      "Epoch 1 | Step 309300 | Avg Loss: 0.0143 | Grad Norm: 0.00913449\n",
      "Epoch 1 | Step 309400 | Avg Loss: 0.0143 | Grad Norm: 0.00976086\n",
      "Epoch 1 | Step 309500 | Avg Loss: 0.0141 | Grad Norm: 0.00957524\n",
      "Epoch 1 | Step 309600 | Avg Loss: 0.0140 | Grad Norm: 0.00917692\n",
      "Epoch 1 | Step 309700 | Avg Loss: 0.0143 | Grad Norm: 0.01030690\n",
      "Epoch 1 | Step 309800 | Avg Loss: 0.0142 | Grad Norm: 0.00890552\n",
      "Epoch 1 | Step 309900 | Avg Loss: 0.0144 | Grad Norm: 0.01177144\n",
      "Epoch 1 | Step 310000 | Avg Loss: 0.0145 | Grad Norm: 0.00939997\n",
      "Epoch 1 | Step 310100 | Avg Loss: 0.0145 | Grad Norm: 0.00909980\n",
      "Epoch 1 | Step 310200 | Avg Loss: 0.0150 | Grad Norm: 0.00798351\n",
      "Epoch 1 | Step 310300 | Avg Loss: 0.0146 | Grad Norm: 0.00958397\n",
      "Epoch 1 | Step 310400 | Avg Loss: 0.0145 | Grad Norm: 0.00912421\n",
      "Epoch 1 | Step 310500 | Avg Loss: 0.0143 | Grad Norm: 0.00987612\n",
      "Epoch 1 | Step 310600 | Avg Loss: 0.0144 | Grad Norm: 0.00873765\n",
      "Epoch 1 | Step 310700 | Avg Loss: 0.0142 | Grad Norm: 0.00965020\n",
      "Epoch 1 | Step 310800 | Avg Loss: 0.0144 | Grad Norm: 0.00775263\n",
      "Epoch 1 | Step 310900 | Avg Loss: 0.0144 | Grad Norm: 0.00768264\n",
      "Epoch 1 | Step 311000 | Avg Loss: 0.0144 | Grad Norm: 0.00960963\n",
      "Epoch 1 | Step 311100 | Avg Loss: 0.0145 | Grad Norm: 0.01010319\n",
      "Epoch 1 | Step 311200 | Avg Loss: 0.0150 | Grad Norm: 0.00955495\n",
      "Epoch 1 | Step 311300 | Avg Loss: 0.0151 | Grad Norm: 0.00955191\n",
      "Epoch 1 | Step 311400 | Avg Loss: 0.0149 | Grad Norm: 0.01116187\n",
      "Epoch 1 | Step 311500 | Avg Loss: 0.0143 | Grad Norm: 0.00961818\n",
      "Epoch 1 | Step 311600 | Avg Loss: 0.0147 | Grad Norm: 0.00982861\n",
      "Epoch 1 | Step 311700 | Avg Loss: 0.0147 | Grad Norm: 0.01013868\n",
      "Epoch 1 | Step 311800 | Avg Loss: 0.0143 | Grad Norm: 0.00945241\n",
      "Epoch 1 | Step 311900 | Avg Loss: 0.0142 | Grad Norm: 0.00927985\n",
      "Epoch 1 | Step 312000 | Avg Loss: 0.0144 | Grad Norm: 0.01001442\n",
      "Epoch 1 | Step 312100 | Avg Loss: 0.0149 | Grad Norm: 0.00987940\n",
      "Epoch 1 | Step 312200 | Avg Loss: 0.0147 | Grad Norm: 0.01049831\n",
      "Epoch 1 | Step 312300 | Avg Loss: 0.0146 | Grad Norm: 0.01074833\n",
      "Epoch 1 | Step 312400 | Avg Loss: 0.0145 | Grad Norm: 0.00972394\n",
      "Epoch 1 | Step 312500 | Avg Loss: 0.0147 | Grad Norm: 0.00788209\n",
      "Epoch 1 | Step 312600 | Avg Loss: 0.0150 | Grad Norm: 0.01131017\n",
      "Epoch 1 | Step 312700 | Avg Loss: 0.0151 | Grad Norm: 0.01162024\n",
      "Epoch 1 | Step 312800 | Avg Loss: 0.0148 | Grad Norm: 0.00925203\n",
      "Epoch 1 | Step 312900 | Avg Loss: 0.0145 | Grad Norm: 0.01030393\n",
      "Epoch 1 | Step 313000 | Avg Loss: 0.0144 | Grad Norm: 0.00927747\n",
      "Epoch 1 | Step 313100 | Avg Loss: 0.0142 | Grad Norm: 0.01030411\n",
      "Epoch 1 | Step 313200 | Avg Loss: 0.0145 | Grad Norm: 0.00937841\n",
      "Epoch 1 | Step 313300 | Avg Loss: 0.0145 | Grad Norm: 0.00888031\n",
      "Epoch 1 | Step 313400 | Avg Loss: 0.0140 | Grad Norm: 0.00947891\n",
      "Epoch 1 | Step 313500 | Avg Loss: 0.0143 | Grad Norm: 0.00882832\n",
      "Epoch 1 | Step 313600 | Avg Loss: 0.0141 | Grad Norm: 0.00961495\n",
      "Epoch 1 | Step 313700 | Avg Loss: 0.0141 | Grad Norm: 0.00947466\n",
      "Epoch 1 | Step 313800 | Avg Loss: 0.0143 | Grad Norm: 0.01046303\n",
      "Epoch 1 | Step 313900 | Avg Loss: 0.0141 | Grad Norm: 0.00922934\n",
      "Epoch 1 | Step 314000 | Avg Loss: 0.0140 | Grad Norm: 0.00943459\n",
      "Epoch 1 | Step 314100 | Avg Loss: 0.0141 | Grad Norm: 0.00801324\n",
      "Epoch 1 | Step 314200 | Avg Loss: 0.0138 | Grad Norm: 0.01015818\n",
      "Epoch 1 | Step 314300 | Avg Loss: 0.0137 | Grad Norm: 0.00851817\n",
      "Epoch 1 | Step 314400 | Avg Loss: 0.0140 | Grad Norm: 0.00819182\n",
      "Epoch 1 | Step 314500 | Avg Loss: 0.0143 | Grad Norm: 0.00991002\n",
      "Epoch 1 | Step 314600 | Avg Loss: 0.0144 | Grad Norm: 0.01021217\n",
      "Epoch 1 | Step 314700 | Avg Loss: 0.0145 | Grad Norm: 0.00969377\n",
      "Epoch 1 | Step 314800 | Avg Loss: 0.0147 | Grad Norm: 0.00930012\n",
      "Epoch 1 | Step 314900 | Avg Loss: 0.0149 | Grad Norm: 0.00964241\n",
      "Epoch 1 | Step 315000 | Avg Loss: 0.0150 | Grad Norm: 0.00892476\n",
      "Epoch 1 | Step 315100 | Avg Loss: 0.0147 | Grad Norm: 0.00939971\n",
      "Epoch 1 | Step 315200 | Avg Loss: 0.0147 | Grad Norm: 0.00950938\n",
      "Epoch 1 | Step 315300 | Avg Loss: 0.0147 | Grad Norm: 0.00908073\n",
      "Epoch 1 | Step 315400 | Avg Loss: 0.0145 | Grad Norm: 0.01222168\n",
      "Epoch 1 | Step 315500 | Avg Loss: 0.0146 | Grad Norm: 0.00900245\n",
      "Epoch 1 | Step 315600 | Avg Loss: 0.0145 | Grad Norm: 0.00871084\n",
      "Epoch 1 | Step 315700 | Avg Loss: 0.0146 | Grad Norm: 0.00946757\n",
      "Epoch 1 | Step 315800 | Avg Loss: 0.0149 | Grad Norm: 0.00925042\n",
      "Epoch 1 | Step 315900 | Avg Loss: 0.0148 | Grad Norm: 0.00813016\n",
      "Epoch 1 | Step 316000 | Avg Loss: 0.0149 | Grad Norm: 0.00945674\n",
      "Epoch 1 | Step 316100 | Avg Loss: 0.0150 | Grad Norm: 0.00930709\n",
      "Epoch 1 | Step 316200 | Avg Loss: 0.0148 | Grad Norm: 0.00936968\n",
      "Epoch 1 | Step 316300 | Avg Loss: 0.0148 | Grad Norm: 0.01001615\n",
      "Epoch 1 | Step 316400 | Avg Loss: 0.0145 | Grad Norm: 0.00876947\n",
      "Epoch 1 | Step 316500 | Avg Loss: 0.0143 | Grad Norm: 0.00775915\n",
      "Epoch 1 | Step 316600 | Avg Loss: 0.0145 | Grad Norm: 0.00888614\n",
      "Epoch 1 | Step 316700 | Avg Loss: 0.0146 | Grad Norm: 0.00954027\n",
      "Epoch 1 | Step 316800 | Avg Loss: 0.0147 | Grad Norm: 0.01039371\n",
      "Epoch 1 | Step 316900 | Avg Loss: 0.0142 | Grad Norm: 0.01012607\n",
      "Epoch 1 | Step 317000 | Avg Loss: 0.0147 | Grad Norm: 0.00855849\n",
      "Epoch 1 | Step 317100 | Avg Loss: 0.0153 | Grad Norm: 0.01151492\n",
      "Epoch 1 | Step 317200 | Avg Loss: 0.0154 | Grad Norm: 0.00936212\n",
      "Epoch 1 | Step 317300 | Avg Loss: 0.0150 | Grad Norm: 0.01050868\n",
      "Epoch 1 | Step 317400 | Avg Loss: 0.0148 | Grad Norm: 0.01197991\n",
      "Epoch 1 | Step 317500 | Avg Loss: 0.0147 | Grad Norm: 0.01034389\n",
      "Epoch 1 | Step 317600 | Avg Loss: 0.0145 | Grad Norm: 0.01005477\n",
      "Epoch 1 | Step 317700 | Avg Loss: 0.0147 | Grad Norm: 0.01013573\n",
      "Epoch 1 | Step 317800 | Avg Loss: 0.0145 | Grad Norm: 0.01032838\n",
      "Epoch 1 | Step 317900 | Avg Loss: 0.0145 | Grad Norm: 0.01003123\n",
      "Epoch 1 | Step 318000 | Avg Loss: 0.0144 | Grad Norm: 0.01061179\n",
      "Epoch 1 | Step 318100 | Avg Loss: 0.0148 | Grad Norm: 0.01075602\n",
      "Epoch 1 | Step 318200 | Avg Loss: 0.0147 | Grad Norm: 0.01060562\n",
      "Epoch 1 | Step 318300 | Avg Loss: 0.0144 | Grad Norm: 0.00992670\n",
      "Epoch 1 | Step 318400 | Avg Loss: 0.0143 | Grad Norm: 0.00867937\n",
      "Epoch 1 | Step 318500 | Avg Loss: 0.0144 | Grad Norm: 0.00934419\n",
      "Epoch 1 | Step 318600 | Avg Loss: 0.0144 | Grad Norm: 0.00979846\n",
      "Epoch 1 | Step 318700 | Avg Loss: 0.0147 | Grad Norm: 0.01081195\n",
      "Epoch 1 | Step 318800 | Avg Loss: 0.0147 | Grad Norm: 0.00979433\n",
      "Epoch 1 | Step 318900 | Avg Loss: 0.0149 | Grad Norm: 0.01004266\n",
      "Epoch 1 | Step 319000 | Avg Loss: 0.0150 | Grad Norm: 0.00896263\n",
      "Epoch 1 | Step 319100 | Avg Loss: 0.0150 | Grad Norm: 0.01070541\n",
      "Epoch 1 | Step 319200 | Avg Loss: 0.0153 | Grad Norm: 0.00871935\n",
      "Epoch 1 | Step 319300 | Avg Loss: 0.0148 | Grad Norm: 0.00980455\n",
      "Epoch 1 | Step 319400 | Avg Loss: 0.0147 | Grad Norm: 0.00880217\n",
      "Epoch 1 | Step 319500 | Avg Loss: 0.0150 | Grad Norm: 0.00882516\n",
      "Epoch 1 | Step 319600 | Avg Loss: 0.0152 | Grad Norm: 0.00982403\n",
      "Epoch 1 | Step 319700 | Avg Loss: 0.0154 | Grad Norm: 0.00830058\n",
      "Epoch 1 | Step 319800 | Avg Loss: 0.0155 | Grad Norm: 0.00938770\n",
      "Epoch 1 | Step 319900 | Avg Loss: 0.0155 | Grad Norm: 0.01091530\n",
      "Epoch 1 | Step 320000 | Avg Loss: 0.0151 | Grad Norm: 0.00917596\n",
      "Epoch 1 | Step 320100 | Avg Loss: 0.0151 | Grad Norm: 0.00978568\n",
      "Epoch 1 | Step 320200 | Avg Loss: 0.0148 | Grad Norm: 0.00913056\n",
      "Epoch 1 | Step 320300 | Avg Loss: 0.0150 | Grad Norm: 0.00877912\n",
      "Epoch 1 | Step 320400 | Avg Loss: 0.0148 | Grad Norm: 0.00941062\n",
      "Epoch 1 | Step 320500 | Avg Loss: 0.0152 | Grad Norm: 0.01098421\n",
      "Epoch 1 | Step 320600 | Avg Loss: 0.0152 | Grad Norm: 0.01166726\n",
      "Epoch 1 | Step 320700 | Avg Loss: 0.0149 | Grad Norm: 0.00969575\n",
      "Epoch 1 | Step 320800 | Avg Loss: 0.0147 | Grad Norm: 0.01077565\n",
      "Epoch 1 | Step 320900 | Avg Loss: 0.0147 | Grad Norm: 0.00899942\n",
      "Epoch 1 | Step 321000 | Avg Loss: 0.0147 | Grad Norm: 0.00870453\n",
      "Epoch 1 | Step 321100 | Avg Loss: 0.0151 | Grad Norm: 0.00845237\n",
      "Epoch 1 | Step 321200 | Avg Loss: 0.0149 | Grad Norm: 0.00870155\n",
      "Epoch 1 | Step 321300 | Avg Loss: 0.0148 | Grad Norm: 0.01030624\n",
      "Epoch 1 | Step 321400 | Avg Loss: 0.0146 | Grad Norm: 0.00816711\n",
      "Epoch 1 | Step 321500 | Avg Loss: 0.0149 | Grad Norm: 0.00848824\n",
      "Epoch 1 | Step 321600 | Avg Loss: 0.0148 | Grad Norm: 0.00903378\n",
      "Epoch 1 | Step 321700 | Avg Loss: 0.0149 | Grad Norm: 0.00972315\n",
      "Epoch 1 | Step 321800 | Avg Loss: 0.0150 | Grad Norm: 0.00956132\n",
      "Epoch 1 | Step 321900 | Avg Loss: 0.0149 | Grad Norm: 0.01101036\n",
      "Epoch 1 | Step 322000 | Avg Loss: 0.0150 | Grad Norm: 0.00854730\n",
      "Epoch 1 | Step 322100 | Avg Loss: 0.0144 | Grad Norm: 0.01052671\n",
      "Epoch 1 | Step 322200 | Avg Loss: 0.0143 | Grad Norm: 0.01188506\n",
      "Epoch 1 | Step 322300 | Avg Loss: 0.0145 | Grad Norm: 0.00922673\n",
      "Epoch 1 | Step 322400 | Avg Loss: 0.0146 | Grad Norm: 0.00951750\n",
      "Epoch 1 | Step 322500 | Avg Loss: 0.0146 | Grad Norm: 0.00970354\n",
      "Epoch 1 | Step 322600 | Avg Loss: 0.0144 | Grad Norm: 0.00853602\n",
      "Epoch 1 | Step 322700 | Avg Loss: 0.0143 | Grad Norm: 0.00868094\n",
      "Epoch 1 | Step 322800 | Avg Loss: 0.0146 | Grad Norm: 0.01017642\n",
      "Epoch 1 | Step 322900 | Avg Loss: 0.0148 | Grad Norm: 0.00944948\n",
      "Epoch 1 | Step 323000 | Avg Loss: 0.0144 | Grad Norm: 0.00918382\n",
      "Epoch 1 | Step 323100 | Avg Loss: 0.0146 | Grad Norm: 0.01080933\n",
      "Epoch 1 | Step 323200 | Avg Loss: 0.0145 | Grad Norm: 0.00989744\n",
      "Epoch 1 | Step 323300 | Avg Loss: 0.0144 | Grad Norm: 0.01084068\n",
      "Epoch 1 | Step 323400 | Avg Loss: 0.0143 | Grad Norm: 0.00996871\n",
      "Epoch 1 | Step 323500 | Avg Loss: 0.0146 | Grad Norm: 0.01113756\n",
      "Epoch 1 | Step 323600 | Avg Loss: 0.0150 | Grad Norm: 0.00933757\n",
      "Epoch 1 | Step 323700 | Avg Loss: 0.0150 | Grad Norm: 0.01088826\n",
      "Epoch 1 | Step 323800 | Avg Loss: 0.0144 | Grad Norm: 0.00802651\n",
      "Epoch 1 | Step 323900 | Avg Loss: 0.0142 | Grad Norm: 0.00875122\n",
      "Epoch 1 | Step 324000 | Avg Loss: 0.0146 | Grad Norm: 0.00971630\n",
      "Epoch 1 | Step 324100 | Avg Loss: 0.0145 | Grad Norm: 0.01027919\n",
      "Epoch 1 | Step 324200 | Avg Loss: 0.0144 | Grad Norm: 0.01067890\n",
      "Epoch 1 | Step 324300 | Avg Loss: 0.0144 | Grad Norm: 0.01017588\n",
      "Epoch 1 | Step 324400 | Avg Loss: 0.0143 | Grad Norm: 0.01052889\n",
      "Epoch 1 | Step 324500 | Avg Loss: 0.0143 | Grad Norm: 0.00956771\n",
      "Epoch 1 | Step 324600 | Avg Loss: 0.0145 | Grad Norm: 0.00990804\n",
      "Epoch 1 | Step 324700 | Avg Loss: 0.0146 | Grad Norm: 0.00941142\n",
      "Epoch 1 | Step 324800 | Avg Loss: 0.0150 | Grad Norm: 0.01004219\n",
      "Epoch 1 | Step 324900 | Avg Loss: 0.0150 | Grad Norm: 0.00961525\n",
      "Epoch 1 | Step 325000 | Avg Loss: 0.0147 | Grad Norm: 0.00889766\n",
      "Epoch 1 | Step 325100 | Avg Loss: 0.0151 | Grad Norm: 0.00923566\n",
      "Epoch 1 | Step 325200 | Avg Loss: 0.0145 | Grad Norm: 0.00979592\n",
      "Epoch 1 | Step 325300 | Avg Loss: 0.0143 | Grad Norm: 0.00960095\n",
      "Epoch 1 | Step 325400 | Avg Loss: 0.0143 | Grad Norm: 0.01146000\n",
      "Epoch 1 | Step 325500 | Avg Loss: 0.0144 | Grad Norm: 0.01118483\n",
      "Epoch 1 | Step 325600 | Avg Loss: 0.0146 | Grad Norm: 0.00963152\n",
      "Epoch 1 | Step 325700 | Avg Loss: 0.0144 | Grad Norm: 0.00970301\n",
      "Epoch 1 | Step 325800 | Avg Loss: 0.0142 | Grad Norm: 0.00930548\n",
      "Epoch 1 | Step 325900 | Avg Loss: 0.0140 | Grad Norm: 0.00938817\n",
      "Epoch 1 | Step 326000 | Avg Loss: 0.0141 | Grad Norm: 0.01000794\n",
      "Epoch 1 | Step 326100 | Avg Loss: 0.0143 | Grad Norm: 0.00915570\n",
      "Epoch 1 | Step 326200 | Avg Loss: 0.0136 | Grad Norm: 0.00826395\n",
      "Epoch 1 | Step 326300 | Avg Loss: 0.0135 | Grad Norm: 0.00893888\n",
      "Epoch 1 | Step 326400 | Avg Loss: 0.0138 | Grad Norm: 0.01018929\n",
      "Epoch 1 | Step 326500 | Avg Loss: 0.0140 | Grad Norm: 0.00956463\n",
      "Epoch 1 | Step 326600 | Avg Loss: 0.0143 | Grad Norm: 0.01034849\n",
      "Epoch 1 | Step 326700 | Avg Loss: 0.0142 | Grad Norm: 0.00945761\n",
      "Epoch 1 | Step 326800 | Avg Loss: 0.0143 | Grad Norm: 0.01082682\n",
      "Epoch 1 | Step 326900 | Avg Loss: 0.0147 | Grad Norm: 0.01031294\n",
      "Epoch 1 | Step 327000 | Avg Loss: 0.0148 | Grad Norm: 0.01025768\n",
      "Epoch 1 | Step 327100 | Avg Loss: 0.0147 | Grad Norm: 0.01010174\n",
      "Epoch 1 | Step 327200 | Avg Loss: 0.0150 | Grad Norm: 0.01005470\n",
      "Epoch 1 | Step 327300 | Avg Loss: 0.0145 | Grad Norm: 0.00875704\n",
      "Epoch 1 | Step 327400 | Avg Loss: 0.0146 | Grad Norm: 0.00952506\n",
      "Epoch 1 | Step 327500 | Avg Loss: 0.0145 | Grad Norm: 0.00923552\n",
      "Epoch 1 | Step 327600 | Avg Loss: 0.0143 | Grad Norm: 0.00963278\n",
      "Epoch 1 | Step 327700 | Avg Loss: 0.0141 | Grad Norm: 0.00903820\n",
      "Epoch 1 | Step 327800 | Avg Loss: 0.0140 | Grad Norm: 0.00932970\n",
      "Epoch 1 | Step 327900 | Avg Loss: 0.0139 | Grad Norm: 0.00990277\n",
      "Epoch 1 | Step 328000 | Avg Loss: 0.0140 | Grad Norm: 0.00908728\n",
      "Epoch 1 | Step 328100 | Avg Loss: 0.0138 | Grad Norm: 0.01117522\n",
      "Epoch 1 | Step 328200 | Avg Loss: 0.0139 | Grad Norm: 0.01006091\n",
      "Epoch 1 | Step 328300 | Avg Loss: 0.0142 | Grad Norm: 0.01001101\n",
      "Epoch 1 | Step 328400 | Avg Loss: 0.0139 | Grad Norm: 0.00977437\n",
      "Epoch 1 | Step 328500 | Avg Loss: 0.0142 | Grad Norm: 0.00842169\n",
      "Epoch 1 | Step 328600 | Avg Loss: 0.0143 | Grad Norm: 0.00953083\n",
      "Epoch 1 | Step 328700 | Avg Loss: 0.0145 | Grad Norm: 0.00999079\n",
      "Epoch 1 | Step 328800 | Avg Loss: 0.0146 | Grad Norm: 0.00971806\n",
      "Epoch 1 | Step 328900 | Avg Loss: 0.0151 | Grad Norm: 0.00903535\n",
      "Epoch 1 | Step 329000 | Avg Loss: 0.0145 | Grad Norm: 0.01004413\n",
      "Epoch 1 | Step 329100 | Avg Loss: 0.0146 | Grad Norm: 0.01083786\n",
      "Epoch 1 | Step 329200 | Avg Loss: 0.0148 | Grad Norm: 0.01048677\n",
      "Epoch 1 | Step 329300 | Avg Loss: 0.0145 | Grad Norm: 0.01017218\n",
      "Epoch 1 | Step 329400 | Avg Loss: 0.0145 | Grad Norm: 0.00874383\n",
      "Epoch 1 | Step 329500 | Avg Loss: 0.0139 | Grad Norm: 0.01066805\n",
      "Epoch 1 | Step 329600 | Avg Loss: 0.0146 | Grad Norm: 0.01129087\n",
      "Epoch 1 | Step 329700 | Avg Loss: 0.0143 | Grad Norm: 0.00915296\n",
      "Epoch 1 | Step 329800 | Avg Loss: 0.0142 | Grad Norm: 0.00899129\n",
      "Epoch 1 | Step 329900 | Avg Loss: 0.0144 | Grad Norm: 0.01291974\n",
      "Epoch 1 | Step 330000 | Avg Loss: 0.0143 | Grad Norm: 0.00937767\n",
      "Epoch 1 | Step 330100 | Avg Loss: 0.0142 | Grad Norm: 0.00828515\n",
      "Epoch 1 | Step 330200 | Avg Loss: 0.0141 | Grad Norm: 0.00891286\n",
      "Epoch 1 | Step 330300 | Avg Loss: 0.0142 | Grad Norm: 0.00953692\n",
      "Epoch 1 | Step 330400 | Avg Loss: 0.0145 | Grad Norm: 0.01021010\n",
      "Epoch 1 | Step 330500 | Avg Loss: 0.0145 | Grad Norm: 0.00879758\n",
      "Epoch 1 | Step 330600 | Avg Loss: 0.0148 | Grad Norm: 0.01301838\n",
      "Epoch 1 | Step 330700 | Avg Loss: 0.0150 | Grad Norm: 0.00975683\n",
      "Epoch 1 | Step 330800 | Avg Loss: 0.0153 | Grad Norm: 0.01024781\n",
      "Epoch 1 | Step 330900 | Avg Loss: 0.0152 | Grad Norm: 0.01106519\n",
      "Epoch 1 | Step 331000 | Avg Loss: 0.0150 | Grad Norm: 0.00924756\n",
      "Epoch 1 | Step 331100 | Avg Loss: 0.0152 | Grad Norm: 0.00912324\n",
      "Epoch 1 | Step 331200 | Avg Loss: 0.0151 | Grad Norm: 0.00949477\n",
      "Epoch 1 | Step 331300 | Avg Loss: 0.0147 | Grad Norm: 0.00936682\n",
      "Epoch 1 | Step 331400 | Avg Loss: 0.0146 | Grad Norm: 0.00924417\n",
      "Epoch 1 | Step 331500 | Avg Loss: 0.0153 | Grad Norm: 0.01048981\n",
      "Epoch 1 | Step 331600 | Avg Loss: 0.0154 | Grad Norm: 0.00874412\n",
      "Epoch 1 | Step 331700 | Avg Loss: 0.0152 | Grad Norm: 0.00973365\n",
      "Epoch 1 | Step 331800 | Avg Loss: 0.0150 | Grad Norm: 0.01140364\n",
      "Epoch 1 | Step 331900 | Avg Loss: 0.0146 | Grad Norm: 0.01131294\n",
      "Epoch 1 | Step 332000 | Avg Loss: 0.0147 | Grad Norm: 0.00915643\n",
      "Epoch 1 | Step 332100 | Avg Loss: 0.0148 | Grad Norm: 0.00948470\n",
      "Epoch 1 | Step 332200 | Avg Loss: 0.0149 | Grad Norm: 0.01058129\n",
      "Epoch 1 | Step 332300 | Avg Loss: 0.0148 | Grad Norm: 0.00990613\n",
      "Epoch 1 | Step 332400 | Avg Loss: 0.0145 | Grad Norm: 0.01002258\n",
      "Epoch 1 | Step 332500 | Avg Loss: 0.0145 | Grad Norm: 0.01203577\n",
      "Epoch 1 | Step 332600 | Avg Loss: 0.0148 | Grad Norm: 0.00889561\n",
      "Epoch 1 | Step 332700 | Avg Loss: 0.0147 | Grad Norm: 0.00939448\n",
      "Epoch 1 | Step 332800 | Avg Loss: 0.0146 | Grad Norm: 0.01155293\n",
      "Epoch 1 | Step 332900 | Avg Loss: 0.0151 | Grad Norm: 0.01073040\n",
      "Epoch 1 | Step 333000 | Avg Loss: 0.0147 | Grad Norm: 0.01076589\n",
      "Epoch 1 | Step 333100 | Avg Loss: 0.0145 | Grad Norm: 0.00859182\n",
      "Epoch 1 | Step 333200 | Avg Loss: 0.0146 | Grad Norm: 0.01177527\n",
      "Epoch 1 | Step 333300 | Avg Loss: 0.0150 | Grad Norm: 0.01029554\n",
      "Epoch 1 | Step 333400 | Avg Loss: 0.0148 | Grad Norm: 0.01180199\n",
      "Epoch 1 | Step 333500 | Avg Loss: 0.0149 | Grad Norm: 0.00905582\n",
      "Epoch 1 | Step 333600 | Avg Loss: 0.0148 | Grad Norm: 0.01021097\n",
      "Epoch 1 | Step 333700 | Avg Loss: 0.0145 | Grad Norm: 0.00984537\n",
      "Epoch 1 | Step 333800 | Avg Loss: 0.0147 | Grad Norm: 0.00882302\n",
      "Epoch 1 | Step 333900 | Avg Loss: 0.0146 | Grad Norm: 0.00989845\n",
      "Epoch 1 | Step 334000 | Avg Loss: 0.0146 | Grad Norm: 0.00920627\n",
      "Epoch 1 | Step 334100 | Avg Loss: 0.0150 | Grad Norm: 0.01243903\n",
      "Epoch 1 | Step 334200 | Avg Loss: 0.0153 | Grad Norm: 0.00976526\n",
      "Epoch 1 | Step 334300 | Avg Loss: 0.0150 | Grad Norm: 0.00953236\n",
      "Epoch 1 | Step 334400 | Avg Loss: 0.0147 | Grad Norm: 0.01132857\n",
      "Epoch 1 | Step 334500 | Avg Loss: 0.0143 | Grad Norm: 0.00916905\n",
      "Epoch 1 | Step 334600 | Avg Loss: 0.0144 | Grad Norm: 0.01005716\n",
      "Epoch 1 | Step 334700 | Avg Loss: 0.0147 | Grad Norm: 0.01128620\n",
      "Epoch 1 | Step 334800 | Avg Loss: 0.0147 | Grad Norm: 0.01082570\n",
      "Epoch 1 | Step 334900 | Avg Loss: 0.0146 | Grad Norm: 0.00966944\n",
      "Epoch 1 | Step 335000 | Avg Loss: 0.0150 | Grad Norm: 0.00891417\n",
      "Epoch 1 | Step 335100 | Avg Loss: 0.0149 | Grad Norm: 0.01059911\n",
      "Epoch 1 | Step 335200 | Avg Loss: 0.0145 | Grad Norm: 0.01104543\n",
      "Epoch 1 | Step 335300 | Avg Loss: 0.0144 | Grad Norm: 0.01103511\n",
      "Epoch 1 | Step 335400 | Avg Loss: 0.0145 | Grad Norm: 0.00999169\n",
      "Epoch 1 | Step 335500 | Avg Loss: 0.0145 | Grad Norm: 0.00891528\n",
      "Epoch 1 | Step 335600 | Avg Loss: 0.0150 | Grad Norm: 0.00861021\n",
      "Epoch 1 | Step 335700 | Avg Loss: 0.0147 | Grad Norm: 0.00917244\n",
      "Epoch 1 | Step 335800 | Avg Loss: 0.0147 | Grad Norm: 0.00709789\n",
      "Epoch 1 | Step 335900 | Avg Loss: 0.0146 | Grad Norm: 0.00928566\n",
      "Epoch 1 | Step 336000 | Avg Loss: 0.0145 | Grad Norm: 0.01084386\n",
      "Epoch 1 | Step 336100 | Avg Loss: 0.0145 | Grad Norm: 0.00886584\n",
      "Epoch 1 | Step 336200 | Avg Loss: 0.0146 | Grad Norm: 0.00874720\n",
      "Epoch 1 | Step 336300 | Avg Loss: 0.0142 | Grad Norm: 0.00918205\n",
      "Epoch 1 | Step 336400 | Avg Loss: 0.0146 | Grad Norm: 0.00833280\n",
      "Epoch 1 | Step 336500 | Avg Loss: 0.0144 | Grad Norm: 0.00976649\n",
      "Epoch 1 | Step 336600 | Avg Loss: 0.0140 | Grad Norm: 0.00925270\n",
      "Epoch 1 | Step 336700 | Avg Loss: 0.0144 | Grad Norm: 0.00879618\n",
      "Epoch 1 | Step 336800 | Avg Loss: 0.0146 | Grad Norm: 0.00949426\n",
      "Epoch 1 | Step 336900 | Avg Loss: 0.0143 | Grad Norm: 0.00928807\n",
      "Epoch 1 | Step 337000 | Avg Loss: 0.0144 | Grad Norm: 0.00996753\n",
      "Epoch 1 | Step 337100 | Avg Loss: 0.0148 | Grad Norm: 0.01070135\n",
      "Epoch 1 | Step 337200 | Avg Loss: 0.0147 | Grad Norm: 0.00875283\n",
      "Epoch 1 | Step 337300 | Avg Loss: 0.0146 | Grad Norm: 0.00827661\n",
      "Epoch 1 | Step 337400 | Avg Loss: 0.0145 | Grad Norm: 0.00927485\n",
      "Epoch 1 | Step 337500 | Avg Loss: 0.0146 | Grad Norm: 0.01017280\n",
      "Epoch 1 | Step 337600 | Avg Loss: 0.0145 | Grad Norm: 0.00933140\n",
      "Epoch 1 | Step 337700 | Avg Loss: 0.0145 | Grad Norm: 0.00910192\n",
      "Epoch 1 | Step 337800 | Avg Loss: 0.0146 | Grad Norm: 0.00930329\n",
      "Epoch 1 | Step 337900 | Avg Loss: 0.0142 | Grad Norm: 0.00750522\n",
      "Epoch 1 | Step 338000 | Avg Loss: 0.0147 | Grad Norm: 0.00998423\n",
      "Epoch 1 | Step 338100 | Avg Loss: 0.0146 | Grad Norm: 0.01074803\n",
      "Epoch 1 | Step 338200 | Avg Loss: 0.0145 | Grad Norm: 0.00848513\n",
      "Epoch 1 | Step 338300 | Avg Loss: 0.0144 | Grad Norm: 0.00837911\n",
      "Epoch 1 | Step 338400 | Avg Loss: 0.0146 | Grad Norm: 0.00940093\n",
      "Epoch 1 | Step 338500 | Avg Loss: 0.0146 | Grad Norm: 0.00903313\n",
      "Epoch 1 | Step 338600 | Avg Loss: 0.0143 | Grad Norm: 0.00840423\n",
      "Epoch 1 | Step 338700 | Avg Loss: 0.0144 | Grad Norm: 0.01028645\n",
      "Epoch 1 | Step 338800 | Avg Loss: 0.0140 | Grad Norm: 0.00855112\n",
      "Epoch 1 | Step 338900 | Avg Loss: 0.0145 | Grad Norm: 0.00962247\n",
      "Epoch 1 | Step 339000 | Avg Loss: 0.0145 | Grad Norm: 0.01124245\n",
      "Epoch 1 | Step 339100 | Avg Loss: 0.0143 | Grad Norm: 0.00895737\n",
      "Epoch 1 | Step 339200 | Avg Loss: 0.0141 | Grad Norm: 0.00944616\n",
      "Epoch 1 | Step 339300 | Avg Loss: 0.0138 | Grad Norm: 0.01022065\n",
      "Epoch 1 | Step 339400 | Avg Loss: 0.0141 | Grad Norm: 0.00882279\n",
      "Epoch 1 | Step 339500 | Avg Loss: 0.0138 | Grad Norm: 0.01081917\n",
      "Epoch 1 | Step 339600 | Avg Loss: 0.0140 | Grad Norm: 0.00830098\n",
      "Epoch 1 | Step 339700 | Avg Loss: 0.0134 | Grad Norm: 0.00814596\n",
      "Epoch 1 | Step 339800 | Avg Loss: 0.0138 | Grad Norm: 0.00888910\n",
      "Epoch 1 | Step 339900 | Avg Loss: 0.0139 | Grad Norm: 0.00846502\n",
      "Epoch 1 | Step 340000 | Avg Loss: 0.0145 | Grad Norm: 0.00909551\n",
      "Epoch 1 | Step 340100 | Avg Loss: 0.0146 | Grad Norm: 0.00899031\n",
      "Epoch 1 | Step 340200 | Avg Loss: 0.0144 | Grad Norm: 0.01070737\n",
      "Epoch 1 | Step 340300 | Avg Loss: 0.0140 | Grad Norm: 0.00919153\n",
      "Epoch 1 | Step 340400 | Avg Loss: 0.0142 | Grad Norm: 0.00889117\n",
      "Epoch 1 | Step 340500 | Avg Loss: 0.0143 | Grad Norm: 0.00949503\n",
      "Epoch 1 | Step 340600 | Avg Loss: 0.0144 | Grad Norm: 0.00920589\n",
      "Epoch 1 | Step 340700 | Avg Loss: 0.0147 | Grad Norm: 0.00904937\n",
      "Epoch 1 | Step 340800 | Avg Loss: 0.0143 | Grad Norm: 0.00910750\n",
      "Epoch 1 | Step 340900 | Avg Loss: 0.0143 | Grad Norm: 0.00873849\n",
      "Epoch 1 | Step 341000 | Avg Loss: 0.0142 | Grad Norm: 0.00962109\n",
      "Epoch 1 | Step 341100 | Avg Loss: 0.0144 | Grad Norm: 0.00917628\n",
      "Epoch 1 | Step 341200 | Avg Loss: 0.0141 | Grad Norm: 0.00924531\n",
      "Epoch 1 | Step 341300 | Avg Loss: 0.0141 | Grad Norm: 0.00944609\n",
      "Epoch 1 | Step 341400 | Avg Loss: 0.0143 | Grad Norm: 0.00927672\n",
      "Epoch 1 | Step 341500 | Avg Loss: 0.0146 | Grad Norm: 0.00893205\n",
      "Epoch 1 | Step 341600 | Avg Loss: 0.0143 | Grad Norm: 0.00899853\n",
      "Epoch 1 | Step 341700 | Avg Loss: 0.0142 | Grad Norm: 0.00945926\n",
      "Epoch 1 | Step 341800 | Avg Loss: 0.0144 | Grad Norm: 0.00913045\n",
      "Epoch 1 | Step 341900 | Avg Loss: 0.0148 | Grad Norm: 0.00944456\n",
      "Epoch 1 | Step 342000 | Avg Loss: 0.0148 | Grad Norm: 0.01047325\n",
      "Epoch 1 | Step 342100 | Avg Loss: 0.0152 | Grad Norm: 0.01084616\n",
      "Epoch 1 | Step 342200 | Avg Loss: 0.0147 | Grad Norm: 0.01052547\n",
      "Epoch 1 | Step 342300 | Avg Loss: 0.0144 | Grad Norm: 0.00922402\n",
      "Epoch 1 | Step 342400 | Avg Loss: 0.0143 | Grad Norm: 0.00925837\n",
      "Epoch 1 | Step 342500 | Avg Loss: 0.0141 | Grad Norm: 0.00965851\n",
      "Epoch 1 | Step 342600 | Avg Loss: 0.0144 | Grad Norm: 0.01005540\n",
      "Epoch 1 | Step 342700 | Avg Loss: 0.0147 | Grad Norm: 0.00954101\n",
      "Epoch 1 | Step 342800 | Avg Loss: 0.0146 | Grad Norm: 0.01104733\n",
      "Epoch 1 | Step 342900 | Avg Loss: 0.0145 | Grad Norm: 0.01019164\n",
      "Epoch 1 | Step 343000 | Avg Loss: 0.0145 | Grad Norm: 0.00918510\n",
      "Epoch 1 | Step 343100 | Avg Loss: 0.0144 | Grad Norm: 0.00925498\n",
      "Epoch 1 | Step 343200 | Avg Loss: 0.0142 | Grad Norm: 0.00978653\n",
      "Epoch 1 | Step 343300 | Avg Loss: 0.0145 | Grad Norm: 0.01077694\n",
      "Epoch 1 | Step 343400 | Avg Loss: 0.0145 | Grad Norm: 0.01069222\n",
      "Epoch 1 | Step 343500 | Avg Loss: 0.0145 | Grad Norm: 0.01026227\n",
      "Epoch 1 | Step 343600 | Avg Loss: 0.0147 | Grad Norm: 0.00962852\n",
      "Epoch 1 | Step 343700 | Avg Loss: 0.0147 | Grad Norm: 0.01090997\n",
      "Epoch 1 | Step 343800 | Avg Loss: 0.0149 | Grad Norm: 0.01067164\n",
      "Epoch 1 | Step 343900 | Avg Loss: 0.0151 | Grad Norm: 0.01051076\n",
      "Epoch 1 | Step 344000 | Avg Loss: 0.0145 | Grad Norm: 0.00958438\n",
      "Epoch 1 | Step 344100 | Avg Loss: 0.0146 | Grad Norm: 0.00952003\n",
      "Epoch 1 | Step 344200 | Avg Loss: 0.0148 | Grad Norm: 0.00982483\n",
      "Epoch 1 | Step 344300 | Avg Loss: 0.0148 | Grad Norm: 0.00985172\n",
      "Epoch 1 | Step 344400 | Avg Loss: 0.0143 | Grad Norm: 0.00918556\n",
      "Epoch 1 | Step 344500 | Avg Loss: 0.0145 | Grad Norm: 0.00956720\n",
      "Epoch 1 | Step 344600 | Avg Loss: 0.0149 | Grad Norm: 0.01034673\n",
      "Epoch 1 | Step 344700 | Avg Loss: 0.0147 | Grad Norm: 0.00984970\n",
      "Epoch 1 | Step 344800 | Avg Loss: 0.0146 | Grad Norm: 0.00879483\n",
      "Epoch 1 | Step 344900 | Avg Loss: 0.0142 | Grad Norm: 0.00872443\n",
      "Epoch 1 | Step 345000 | Avg Loss: 0.0141 | Grad Norm: 0.00789009\n",
      "Epoch 1 | Step 345100 | Avg Loss: 0.0144 | Grad Norm: 0.00939497\n",
      "Epoch 1 | Step 345200 | Avg Loss: 0.0146 | Grad Norm: 0.01033919\n",
      "Epoch 1 | Step 345300 | Avg Loss: 0.0143 | Grad Norm: 0.01022282\n",
      "Epoch 1 | Step 345400 | Avg Loss: 0.0140 | Grad Norm: 0.00841214\n",
      "Epoch 1 | Step 345500 | Avg Loss: 0.0144 | Grad Norm: 0.00984816\n",
      "Epoch 1 | Step 345600 | Avg Loss: 0.0145 | Grad Norm: 0.01039858\n",
      "Epoch 1 | Step 345700 | Avg Loss: 0.0144 | Grad Norm: 0.00898120\n",
      "Epoch 1 | Step 345800 | Avg Loss: 0.0146 | Grad Norm: 0.01024750\n",
      "Epoch 1 | Step 345900 | Avg Loss: 0.0142 | Grad Norm: 0.00884473\n",
      "Epoch 1 | Step 346000 | Avg Loss: 0.0146 | Grad Norm: 0.01000964\n",
      "Epoch 1 | Step 346100 | Avg Loss: 0.0147 | Grad Norm: 0.00917048\n",
      "Epoch 1 | Step 346200 | Avg Loss: 0.0144 | Grad Norm: 0.01035174\n",
      "Epoch 1 | Step 346300 | Avg Loss: 0.0142 | Grad Norm: 0.00904508\n",
      "Epoch 1 | Step 346400 | Avg Loss: 0.0145 | Grad Norm: 0.00943596\n",
      "Epoch 1 | Step 346500 | Avg Loss: 0.0147 | Grad Norm: 0.00876290\n",
      "Epoch 1 | Step 346600 | Avg Loss: 0.0143 | Grad Norm: 0.01173315\n",
      "Epoch 1 | Step 346700 | Avg Loss: 0.0138 | Grad Norm: 0.01463609\n",
      "Epoch 1 | Step 346800 | Avg Loss: 0.0139 | Grad Norm: 0.00873681\n",
      "Epoch 1 | Step 346900 | Avg Loss: 0.0139 | Grad Norm: 0.00974114\n",
      "Epoch 1 | Step 347000 | Avg Loss: 0.0144 | Grad Norm: 0.01088355\n",
      "Epoch 1 | Step 347100 | Avg Loss: 0.0147 | Grad Norm: 0.00993124\n",
      "Epoch 1 | Step 347200 | Avg Loss: 0.0142 | Grad Norm: 0.01097082\n",
      "Epoch 1 | Step 347300 | Avg Loss: 0.0141 | Grad Norm: 0.00952904\n",
      "Epoch 1 | Step 347400 | Avg Loss: 0.0141 | Grad Norm: 0.01102612\n",
      "Epoch 1 | Step 347500 | Avg Loss: 0.0142 | Grad Norm: 0.00978238\n",
      "Epoch 1 | Step 347600 | Avg Loss: 0.0144 | Grad Norm: 0.00868475\n",
      "Epoch 1 | Step 347700 | Avg Loss: 0.0147 | Grad Norm: 0.00986826\n",
      "Epoch 1 | Step 347800 | Avg Loss: 0.0147 | Grad Norm: 0.00941117\n",
      "Epoch 1 | Step 347900 | Avg Loss: 0.0145 | Grad Norm: 0.01150010\n",
      "Epoch 1 | Step 348000 | Avg Loss: 0.0148 | Grad Norm: 0.00888214\n",
      "Epoch 1 | Step 348100 | Avg Loss: 0.0146 | Grad Norm: 0.00953031\n",
      "Epoch 1 | Step 348200 | Avg Loss: 0.0148 | Grad Norm: 0.00934262\n",
      "Epoch 1 | Step 348300 | Avg Loss: 0.0147 | Grad Norm: 0.00905916\n",
      "Epoch 1 | Step 348400 | Avg Loss: 0.0144 | Grad Norm: 0.00980953\n",
      "Epoch 1 | Step 348500 | Avg Loss: 0.0144 | Grad Norm: 0.01108942\n",
      "Epoch 1 | Step 348600 | Avg Loss: 0.0147 | Grad Norm: 0.00930969\n",
      "Epoch 1 | Step 348700 | Avg Loss: 0.0148 | Grad Norm: 0.01110124\n",
      "Epoch 1 | Step 348800 | Avg Loss: 0.0148 | Grad Norm: 0.01118253\n",
      "Epoch 1 | Step 348900 | Avg Loss: 0.0148 | Grad Norm: 0.00861881\n",
      "Epoch 1 | Step 349000 | Avg Loss: 0.0145 | Grad Norm: 0.00847880\n",
      "Epoch 1 | Step 349100 | Avg Loss: 0.0143 | Grad Norm: 0.01029160\n",
      "Epoch 1 | Step 349200 | Avg Loss: 0.0143 | Grad Norm: 0.00874274\n",
      "Epoch 1 | Step 349300 | Avg Loss: 0.0146 | Grad Norm: 0.00918673\n",
      "Epoch 1 | Step 349400 | Avg Loss: 0.0148 | Grad Norm: 0.00912486\n",
      "Epoch 1 | Step 349500 | Avg Loss: 0.0146 | Grad Norm: 0.01185262\n",
      "Epoch 1 | Step 349600 | Avg Loss: 0.0149 | Grad Norm: 0.00915512\n",
      "Epoch 1 | Step 349700 | Avg Loss: 0.0146 | Grad Norm: 0.01050346\n",
      "Epoch 1 | Step 349800 | Avg Loss: 0.0150 | Grad Norm: 0.00962827\n",
      "Epoch 1 | Step 349900 | Avg Loss: 0.0144 | Grad Norm: 0.00958468\n",
      "Epoch 1 | Step 350000 | Avg Loss: 0.0143 | Grad Norm: 0.00837551\n",
      "Epoch 1 | Step 350100 | Avg Loss: 0.0147 | Grad Norm: 0.00846357\n",
      "Epoch 1 | Step 350200 | Avg Loss: 0.0149 | Grad Norm: 0.01106789\n",
      "Epoch 1 | Step 350300 | Avg Loss: 0.0149 | Grad Norm: 0.01220767\n",
      "Epoch 1 | Step 350400 | Avg Loss: 0.0149 | Grad Norm: 0.01011141\n",
      "Epoch 1 | Step 350500 | Avg Loss: 0.0150 | Grad Norm: 0.01142747\n",
      "Epoch 1 | Step 350600 | Avg Loss: 0.0150 | Grad Norm: 0.00873249\n",
      "Epoch 1 | Step 350700 | Avg Loss: 0.0149 | Grad Norm: 0.00980912\n",
      "Epoch 1 | Step 350800 | Avg Loss: 0.0149 | Grad Norm: 0.01018856\n",
      "Epoch 1 | Step 350900 | Avg Loss: 0.0150 | Grad Norm: 0.01096452\n",
      "Epoch 1 | Step 351000 | Avg Loss: 0.0152 | Grad Norm: 0.01046118\n",
      "Epoch 1 | Step 351100 | Avg Loss: 0.0149 | Grad Norm: 0.00903934\n",
      "Epoch 1 | Step 351200 | Avg Loss: 0.0148 | Grad Norm: 0.00925316\n",
      "Epoch 1 | Step 351300 | Avg Loss: 0.0145 | Grad Norm: 0.00902607\n",
      "Epoch 1 | Step 351400 | Avg Loss: 0.0146 | Grad Norm: 0.01127828\n",
      "Epoch 1 | Step 351500 | Avg Loss: 0.0144 | Grad Norm: 0.01002881\n",
      "Epoch 1 | Step 351600 | Avg Loss: 0.0147 | Grad Norm: 0.01143301\n",
      "Epoch 1 | Step 351700 | Avg Loss: 0.0150 | Grad Norm: 0.01009979\n",
      "Epoch 1 | Step 351800 | Avg Loss: 0.0153 | Grad Norm: 0.00964664\n",
      "Epoch 1 | Step 351900 | Avg Loss: 0.0151 | Grad Norm: 0.00819209\n",
      "Epoch 1 | Step 352000 | Avg Loss: 0.0148 | Grad Norm: 0.01034369\n",
      "Epoch 1 | Step 352100 | Avg Loss: 0.0152 | Grad Norm: 0.01014172\n",
      "Epoch 1 | Step 352200 | Avg Loss: 0.0150 | Grad Norm: 0.01057204\n",
      "Epoch 1 | Step 352300 | Avg Loss: 0.0148 | Grad Norm: 0.00892148\n",
      "Epoch 1 | Step 352400 | Avg Loss: 0.0148 | Grad Norm: 0.01030291\n",
      "Epoch 1 | Step 352500 | Avg Loss: 0.0149 | Grad Norm: 0.00917901\n",
      "Epoch 1 | Step 352600 | Avg Loss: 0.0150 | Grad Norm: 0.00869608\n",
      "Epoch 1 | Step 352700 | Avg Loss: 0.0147 | Grad Norm: 0.01065106\n",
      "Epoch 1 | Step 352800 | Avg Loss: 0.0146 | Grad Norm: 0.00933473\n",
      "Epoch 1 | Step 352900 | Avg Loss: 0.0144 | Grad Norm: 0.00977798\n",
      "Epoch 1 | Step 353000 | Avg Loss: 0.0143 | Grad Norm: 0.00940081\n",
      "Epoch 1 | Step 353100 | Avg Loss: 0.0144 | Grad Norm: 0.00827019\n",
      "Epoch 1 | Step 353200 | Avg Loss: 0.0146 | Grad Norm: 0.00961723\n",
      "Epoch 1 | Step 353300 | Avg Loss: 0.0145 | Grad Norm: 0.00866656\n",
      "Epoch 1 | Step 353400 | Avg Loss: 0.0144 | Grad Norm: 0.01097028\n",
      "Epoch 1 | Step 353500 | Avg Loss: 0.0140 | Grad Norm: 0.00813599\n",
      "Epoch 1 | Step 353600 | Avg Loss: 0.0142 | Grad Norm: 0.00993984\n",
      "Epoch 1 | Step 353700 | Avg Loss: 0.0141 | Grad Norm: 0.00808359\n",
      "Epoch 1 | Step 353800 | Avg Loss: 0.0144 | Grad Norm: 0.01127318\n",
      "Epoch 1 | Step 353900 | Avg Loss: 0.0145 | Grad Norm: 0.01116422\n",
      "Epoch 1 | Step 354000 | Avg Loss: 0.0144 | Grad Norm: 0.00990193\n",
      "Epoch 1 | Step 354100 | Avg Loss: 0.0144 | Grad Norm: 0.00962676\n",
      "Epoch 1 | Step 354200 | Avg Loss: 0.0146 | Grad Norm: 0.00998455\n",
      "Epoch 1 | Step 354300 | Avg Loss: 0.0148 | Grad Norm: 0.01106045\n",
      "Epoch 1 | Step 354400 | Avg Loss: 0.0148 | Grad Norm: 0.00998620\n",
      "Epoch 1 | Step 354500 | Avg Loss: 0.0147 | Grad Norm: 0.01005241\n",
      "Epoch 1 | Step 354600 | Avg Loss: 0.0146 | Grad Norm: 0.00996359\n",
      "Epoch 1 | Step 354700 | Avg Loss: 0.0144 | Grad Norm: 0.00913412\n",
      "Epoch 1 | Step 354800 | Avg Loss: 0.0145 | Grad Norm: 0.01063684\n",
      "Epoch 1 | Step 354900 | Avg Loss: 0.0144 | Grad Norm: 0.00842404\n",
      "Epoch 1 | Step 355000 | Avg Loss: 0.0146 | Grad Norm: 0.01056125\n",
      "Epoch 1 | Step 355100 | Avg Loss: 0.0141 | Grad Norm: 0.01053935\n",
      "Epoch 1 | Step 355200 | Avg Loss: 0.0142 | Grad Norm: 0.00906597\n",
      "Epoch 1 | Step 355300 | Avg Loss: 0.0140 | Grad Norm: 0.00880084\n",
      "Epoch 1 | Step 355400 | Avg Loss: 0.0143 | Grad Norm: 0.01066535\n",
      "Epoch 1 | Step 355500 | Avg Loss: 0.0143 | Grad Norm: 0.01074444\n",
      "Epoch 1 | Step 355600 | Avg Loss: 0.0146 | Grad Norm: 0.00915969\n",
      "Epoch 1 | Step 355700 | Avg Loss: 0.0142 | Grad Norm: 0.00994776\n",
      "Epoch 1 | Step 355800 | Avg Loss: 0.0142 | Grad Norm: 0.00976786\n",
      "Epoch 1 | Step 355900 | Avg Loss: 0.0142 | Grad Norm: 0.01080591\n",
      "Epoch 1 | Step 356000 | Avg Loss: 0.0148 | Grad Norm: 0.00868900\n",
      "Epoch 1 | Step 356100 | Avg Loss: 0.0145 | Grad Norm: 0.01064769\n",
      "Epoch 1 | Step 356200 | Avg Loss: 0.0146 | Grad Norm: 0.00841745\n",
      "Epoch 1 | Step 356300 | Avg Loss: 0.0149 | Grad Norm: 0.01017453\n",
      "Epoch 1 | Step 356400 | Avg Loss: 0.0151 | Grad Norm: 0.01044749\n",
      "Epoch 1 | Step 356500 | Avg Loss: 0.0145 | Grad Norm: 0.00875314\n",
      "Epoch 1 | Step 356600 | Avg Loss: 0.0144 | Grad Norm: 0.01052819\n",
      "Epoch 1 | Step 356700 | Avg Loss: 0.0141 | Grad Norm: 0.01045957\n",
      "Epoch 1 | Step 356800 | Avg Loss: 0.0139 | Grad Norm: 0.00928488\n",
      "Epoch 1 | Step 356900 | Avg Loss: 0.0140 | Grad Norm: 0.00956193\n",
      "Epoch 1 | Step 357000 | Avg Loss: 0.0139 | Grad Norm: 0.00968425\n",
      "Epoch 1 | Step 357100 | Avg Loss: 0.0141 | Grad Norm: 0.00961805\n",
      "Epoch 1 | Step 357200 | Avg Loss: 0.0142 | Grad Norm: 0.01072249\n",
      "Epoch 1 | Step 357300 | Avg Loss: 0.0141 | Grad Norm: 0.00958547\n",
      "Epoch 1 | Step 357400 | Avg Loss: 0.0140 | Grad Norm: 0.00921720\n",
      "Epoch 1 | Step 357500 | Avg Loss: 0.0138 | Grad Norm: 0.00914907\n",
      "Epoch 1 | Step 357600 | Avg Loss: 0.0141 | Grad Norm: 0.00905499\n",
      "Epoch 1 | Step 357700 | Avg Loss: 0.0142 | Grad Norm: 0.01063933\n",
      "Epoch 1 | Step 357800 | Avg Loss: 0.0143 | Grad Norm: 0.00973790\n",
      "Epoch 1 | Step 357900 | Avg Loss: 0.0143 | Grad Norm: 0.00918216\n",
      "Epoch 1 | Step 358000 | Avg Loss: 0.0141 | Grad Norm: 0.00868633\n",
      "Epoch 1 | Step 358100 | Avg Loss: 0.0142 | Grad Norm: 0.00964282\n",
      "Epoch 1 | Step 358200 | Avg Loss: 0.0137 | Grad Norm: 0.01000033\n",
      "Epoch 1 | Step 358300 | Avg Loss: 0.0137 | Grad Norm: 0.00883549\n",
      "Epoch 1 | Step 358400 | Avg Loss: 0.0140 | Grad Norm: 0.00946782\n",
      "Epoch 1 | Step 358500 | Avg Loss: 0.0144 | Grad Norm: 0.00956111\n",
      "Epoch 1 | Step 358600 | Avg Loss: 0.0144 | Grad Norm: 0.00992824\n",
      "Epoch 1 | Step 358700 | Avg Loss: 0.0146 | Grad Norm: 0.00965265\n",
      "Epoch 1 | Step 358800 | Avg Loss: 0.0146 | Grad Norm: 0.01142124\n",
      "Epoch 1 | Step 358900 | Avg Loss: 0.0149 | Grad Norm: 0.01045018\n",
      "Epoch 1 | Step 359000 | Avg Loss: 0.0148 | Grad Norm: 0.00954986\n",
      "Epoch 1 | Step 359100 | Avg Loss: 0.0146 | Grad Norm: 0.00810558\n",
      "Epoch 1 | Step 359200 | Avg Loss: 0.0146 | Grad Norm: 0.00989787\n",
      "Epoch 1 | Step 359300 | Avg Loss: 0.0142 | Grad Norm: 0.00853673\n",
      "Epoch 1 | Step 359400 | Avg Loss: 0.0143 | Grad Norm: 0.00992036\n",
      "Epoch 1 | Step 359500 | Avg Loss: 0.0141 | Grad Norm: 0.00929874\n",
      "Epoch 1 | Step 359600 | Avg Loss: 0.0144 | Grad Norm: 0.00797480\n",
      "Epoch 1 | Step 359700 | Avg Loss: 0.0144 | Grad Norm: 0.00930403\n",
      "Epoch 1 | Step 359800 | Avg Loss: 0.0149 | Grad Norm: 0.00876167\n",
      "Epoch 1 | Step 359900 | Avg Loss: 0.0146 | Grad Norm: 0.00821635\n",
      "Epoch 1 | Step 360000 | Avg Loss: 0.0146 | Grad Norm: 0.01055796\n",
      "Epoch 1 | Step 360100 | Avg Loss: 0.0148 | Grad Norm: 0.00997110\n",
      "Epoch 1 | Step 360200 | Avg Loss: 0.0145 | Grad Norm: 0.00883324\n",
      "Epoch 1 | Step 360300 | Avg Loss: 0.0138 | Grad Norm: 0.00887559\n",
      "Epoch 1 | Step 360400 | Avg Loss: 0.0142 | Grad Norm: 0.00971538\n",
      "Epoch 1 | Step 360500 | Avg Loss: 0.0142 | Grad Norm: 0.00886453\n",
      "Epoch 1 | Step 360600 | Avg Loss: 0.0145 | Grad Norm: 0.01144204\n",
      "Epoch 1 | Step 360700 | Avg Loss: 0.0146 | Grad Norm: 0.01158624\n",
      "Epoch 1 | Step 360800 | Avg Loss: 0.0147 | Grad Norm: 0.00972233\n",
      "Epoch 1 | Step 360900 | Avg Loss: 0.0143 | Grad Norm: 0.01138232\n",
      "Epoch 1 | Step 361000 | Avg Loss: 0.0142 | Grad Norm: 0.01031417\n",
      "Epoch 1 | Step 361100 | Avg Loss: 0.0145 | Grad Norm: 0.00966540\n",
      "Epoch 1 | Step 361200 | Avg Loss: 0.0144 | Grad Norm: 0.00748137\n",
      "Epoch 1 | Step 361300 | Avg Loss: 0.0148 | Grad Norm: 0.01098375\n",
      "Epoch 1 | Step 361400 | Avg Loss: 0.0149 | Grad Norm: 0.01076046\n",
      "Epoch 1 | Step 361500 | Avg Loss: 0.0146 | Grad Norm: 0.00968921\n",
      "Epoch 1 | Step 361600 | Avg Loss: 0.0145 | Grad Norm: 0.00902423\n",
      "Epoch 1 | Step 361700 | Avg Loss: 0.0143 | Grad Norm: 0.00870129\n",
      "Epoch 1 | Step 361800 | Avg Loss: 0.0146 | Grad Norm: 0.00835047\n",
      "Epoch 1 | Step 361900 | Avg Loss: 0.0146 | Grad Norm: 0.00952095\n",
      "Epoch 1 | Step 362000 | Avg Loss: 0.0150 | Grad Norm: 0.00876398\n",
      "Epoch 1 | Step 362100 | Avg Loss: 0.0148 | Grad Norm: 0.00863979\n",
      "Epoch 1 | Step 362200 | Avg Loss: 0.0143 | Grad Norm: 0.00918819\n",
      "Epoch 1 | Step 362300 | Avg Loss: 0.0141 | Grad Norm: 0.00900185\n",
      "Epoch 1 | Step 362400 | Avg Loss: 0.0142 | Grad Norm: 0.00916413\n",
      "Epoch 1 | Step 362500 | Avg Loss: 0.0145 | Grad Norm: 0.00733211\n",
      "Epoch 1 | Step 362600 | Avg Loss: 0.0146 | Grad Norm: 0.01028505\n",
      "Epoch 1 | Step 362700 | Avg Loss: 0.0144 | Grad Norm: 0.00891664\n",
      "Epoch 1 | Step 362800 | Avg Loss: 0.0144 | Grad Norm: 0.00945686\n",
      "Epoch 1 | Step 362900 | Avg Loss: 0.0143 | Grad Norm: 0.00962274\n",
      "Epoch 1 | Step 363000 | Avg Loss: 0.0142 | Grad Norm: 0.00903907\n",
      "Epoch 1 | Step 363100 | Avg Loss: 0.0142 | Grad Norm: 0.01012426\n",
      "Epoch 1 | Step 363200 | Avg Loss: 0.0147 | Grad Norm: 0.01097203\n",
      "Epoch 1 | Step 363300 | Avg Loss: 0.0147 | Grad Norm: 0.01022234\n",
      "Epoch 1 | Step 363400 | Avg Loss: 0.0150 | Grad Norm: 0.00996941\n",
      "Epoch 1 | Step 363500 | Avg Loss: 0.0145 | Grad Norm: 0.00859785\n",
      "Epoch 1 | Step 363600 | Avg Loss: 0.0144 | Grad Norm: 0.01049323\n",
      "Epoch 1 | Step 363700 | Avg Loss: 0.0146 | Grad Norm: 0.01042533\n",
      "Epoch 1 | Step 363800 | Avg Loss: 0.0147 | Grad Norm: 0.00976424\n",
      "Epoch 1 | Step 363900 | Avg Loss: 0.0142 | Grad Norm: 0.00924609\n",
      "Epoch 1 | Step 364000 | Avg Loss: 0.0148 | Grad Norm: 0.00986554\n",
      "Epoch 1 | Step 364100 | Avg Loss: 0.0146 | Grad Norm: 0.00920113\n",
      "Epoch 1 | Step 364200 | Avg Loss: 0.0142 | Grad Norm: 0.00966802\n",
      "Epoch 1 | Step 364300 | Avg Loss: 0.0148 | Grad Norm: 0.01008322\n",
      "Epoch 1 | Step 364400 | Avg Loss: 0.0149 | Grad Norm: 0.01127225\n",
      "Epoch 1 | Step 364500 | Avg Loss: 0.0147 | Grad Norm: 0.01013214\n",
      "Epoch 1 | Step 364600 | Avg Loss: 0.0149 | Grad Norm: 0.01098589\n",
      "Epoch 1 | Step 364700 | Avg Loss: 0.0150 | Grad Norm: 0.00904572\n",
      "Epoch 1 | Step 364800 | Avg Loss: 0.0148 | Grad Norm: 0.01024024\n",
      "Epoch 1 | Step 364900 | Avg Loss: 0.0149 | Grad Norm: 0.00997039\n",
      "Epoch 1 | Step 365000 | Avg Loss: 0.0147 | Grad Norm: 0.01029032\n",
      "Epoch 1 | Step 365100 | Avg Loss: 0.0147 | Grad Norm: 0.01071866\n",
      "Epoch 1 | Step 365200 | Avg Loss: 0.0146 | Grad Norm: 0.00894423\n",
      "Epoch 1 | Step 365300 | Avg Loss: 0.0146 | Grad Norm: 0.00999605\n",
      "Epoch 1 | Step 365400 | Avg Loss: 0.0147 | Grad Norm: 0.00988169\n",
      "Epoch 1 | Step 365500 | Avg Loss: 0.0143 | Grad Norm: 0.00939445\n",
      "Epoch 1 | Step 365600 | Avg Loss: 0.0144 | Grad Norm: 0.00921356\n",
      "Epoch 1 | Step 365700 | Avg Loss: 0.0145 | Grad Norm: 0.00998391\n",
      "Epoch 1 | Step 365800 | Avg Loss: 0.0148 | Grad Norm: 0.01032752\n",
      "Epoch 1 | Step 365900 | Avg Loss: 0.0148 | Grad Norm: 0.00950618\n",
      "Epoch 1 | Step 366000 | Avg Loss: 0.0148 | Grad Norm: 0.01009906\n",
      "Epoch 1 | Step 366100 | Avg Loss: 0.0145 | Grad Norm: 0.00975481\n",
      "Epoch 1 | Step 366200 | Avg Loss: 0.0144 | Grad Norm: 0.01081876\n",
      "Epoch 1 | Step 366300 | Avg Loss: 0.0146 | Grad Norm: 0.00921507\n",
      "Epoch 1 | Step 366400 | Avg Loss: 0.0145 | Grad Norm: 0.00955819\n",
      "Epoch 1 | Step 366500 | Avg Loss: 0.0145 | Grad Norm: 0.01036532\n",
      "Epoch 1 | Step 366600 | Avg Loss: 0.0146 | Grad Norm: 0.00924726\n",
      "Epoch 1 | Step 366700 | Avg Loss: 0.0143 | Grad Norm: 0.00852442\n",
      "Epoch 1 | Step 366800 | Avg Loss: 0.0141 | Grad Norm: 0.00943537\n",
      "Epoch 1 | Step 366900 | Avg Loss: 0.0143 | Grad Norm: 0.01146341\n",
      "Epoch 1 | Step 367000 | Avg Loss: 0.0146 | Grad Norm: 0.01033997\n",
      "Epoch 1 | Step 367100 | Avg Loss: 0.0147 | Grad Norm: 0.00964593\n",
      "Epoch 1 | Step 367200 | Avg Loss: 0.0146 | Grad Norm: 0.00925278\n",
      "Epoch 1 | Step 367300 | Avg Loss: 0.0141 | Grad Norm: 0.01072966\n",
      "Epoch 1 | Step 367400 | Avg Loss: 0.0142 | Grad Norm: 0.00894047\n",
      "Epoch 1 | Step 367500 | Avg Loss: 0.0145 | Grad Norm: 0.00931392\n",
      "Epoch 1 | Step 367600 | Avg Loss: 0.0148 | Grad Norm: 0.00900864\n",
      "Epoch 1 | Step 367700 | Avg Loss: 0.0145 | Grad Norm: 0.00792196\n",
      "Epoch 1 | Step 367800 | Avg Loss: 0.0144 | Grad Norm: 0.01011001\n",
      "Epoch 1 | Step 367900 | Avg Loss: 0.0145 | Grad Norm: 0.00839834\n",
      "Epoch 1 | Step 368000 | Avg Loss: 0.0148 | Grad Norm: 0.01057986\n",
      "Epoch 1 | Step 368100 | Avg Loss: 0.0145 | Grad Norm: 0.00885604\n",
      "Epoch 1 | Step 368200 | Avg Loss: 0.0146 | Grad Norm: 0.00994643\n",
      "Epoch 1 | Step 368300 | Avg Loss: 0.0148 | Grad Norm: 0.00862254\n",
      "Epoch 1 | Step 368400 | Avg Loss: 0.0149 | Grad Norm: 0.00911589\n",
      "Epoch 1 | Step 368500 | Avg Loss: 0.0149 | Grad Norm: 0.00907172\n",
      "Epoch 1 | Step 368600 | Avg Loss: 0.0145 | Grad Norm: 0.01012899\n",
      "Epoch 1 | Step 368700 | Avg Loss: 0.0146 | Grad Norm: 0.01288095\n",
      "Epoch 1 | Step 368800 | Avg Loss: 0.0145 | Grad Norm: 0.01095374\n",
      "Epoch 1 | Step 368900 | Avg Loss: 0.0142 | Grad Norm: 0.00925516\n",
      "Epoch 1 | Step 369000 | Avg Loss: 0.0142 | Grad Norm: 0.00833365\n",
      "Epoch 1 | Step 369100 | Avg Loss: 0.0143 | Grad Norm: 0.00938199\n",
      "Epoch 1 | Step 369200 | Avg Loss: 0.0141 | Grad Norm: 0.01034007\n",
      "Epoch 1 | Step 369300 | Avg Loss: 0.0145 | Grad Norm: 0.00880068\n",
      "Epoch 1 | Step 369400 | Avg Loss: 0.0149 | Grad Norm: 0.00970382\n",
      "Epoch 1 | Step 369500 | Avg Loss: 0.0148 | Grad Norm: 0.00979988\n",
      "Epoch 1 | Step 369600 | Avg Loss: 0.0148 | Grad Norm: 0.00932034\n",
      "Epoch 1 | Step 369700 | Avg Loss: 0.0147 | Grad Norm: 0.00795206\n",
      "Epoch 1 | Step 369800 | Avg Loss: 0.0144 | Grad Norm: 0.01096618\n",
      "Epoch 1 | Step 369900 | Avg Loss: 0.0148 | Grad Norm: 0.00953852\n",
      "Epoch 1 | Step 370000 | Avg Loss: 0.0142 | Grad Norm: 0.00972264\n",
      "Epoch 1 | Step 370100 | Avg Loss: 0.0139 | Grad Norm: 0.00885491\n",
      "Epoch 1 | Step 370200 | Avg Loss: 0.0140 | Grad Norm: 0.01062050\n",
      "Epoch 1 | Step 370300 | Avg Loss: 0.0140 | Grad Norm: 0.00875384\n",
      "Epoch 1 | Step 370400 | Avg Loss: 0.0139 | Grad Norm: 0.01096728\n",
      "Epoch 1 | Step 370500 | Avg Loss: 0.0137 | Grad Norm: 0.00886306\n",
      "Epoch 1 | Step 370600 | Avg Loss: 0.0135 | Grad Norm: 0.00862037\n",
      "Epoch 1 | Step 370700 | Avg Loss: 0.0138 | Grad Norm: 0.01024720\n",
      "Epoch 1 | Step 370800 | Avg Loss: 0.0140 | Grad Norm: 0.00943896\n",
      "Epoch 1 | Step 370900 | Avg Loss: 0.0140 | Grad Norm: 0.01100782\n",
      "Epoch 1 | Step 371000 | Avg Loss: 0.0142 | Grad Norm: 0.01017271\n",
      "Epoch 1 | Step 371100 | Avg Loss: 0.0145 | Grad Norm: 0.01338582\n",
      "Epoch 1 | Step 371200 | Avg Loss: 0.0145 | Grad Norm: 0.01230251\n",
      "Epoch 1 | Step 371300 | Avg Loss: 0.0145 | Grad Norm: 0.00921213\n",
      "Epoch 1 | Step 371400 | Avg Loss: 0.0145 | Grad Norm: 0.01009627\n",
      "Epoch 1 | Step 371500 | Avg Loss: 0.0144 | Grad Norm: 0.00920828\n",
      "Epoch 1 | Step 371600 | Avg Loss: 0.0143 | Grad Norm: 0.00907796\n",
      "Epoch 1 | Step 371700 | Avg Loss: 0.0142 | Grad Norm: 0.00919755\n",
      "Epoch 1 | Step 371800 | Avg Loss: 0.0144 | Grad Norm: 0.00851647\n",
      "Epoch 1 | Step 371900 | Avg Loss: 0.0141 | Grad Norm: 0.00837159\n",
      "Epoch 1 | Step 372000 | Avg Loss: 0.0137 | Grad Norm: 0.00870731\n",
      "Epoch 1 | Step 372100 | Avg Loss: 0.0138 | Grad Norm: 0.00979038\n",
      "Epoch 1 | Step 372200 | Avg Loss: 0.0143 | Grad Norm: 0.00904711\n",
      "Epoch 1 | Step 372300 | Avg Loss: 0.0147 | Grad Norm: 0.01008740\n",
      "Epoch 1 | Step 372400 | Avg Loss: 0.0150 | Grad Norm: 0.00983328\n",
      "Epoch 1 | Step 372500 | Avg Loss: 0.0147 | Grad Norm: 0.00893262\n",
      "Epoch 1 | Step 372600 | Avg Loss: 0.0144 | Grad Norm: 0.01147361\n",
      "Epoch 1 | Step 372700 | Avg Loss: 0.0144 | Grad Norm: 0.00859575\n",
      "Epoch 1 | Step 372800 | Avg Loss: 0.0141 | Grad Norm: 0.00911190\n",
      "Epoch 1 | Step 372900 | Avg Loss: 0.0141 | Grad Norm: 0.00838154\n",
      "Epoch 1 | Step 373000 | Avg Loss: 0.0139 | Grad Norm: 0.00989683\n",
      "Epoch 1 | Step 373100 | Avg Loss: 0.0145 | Grad Norm: 0.00932916\n",
      "Epoch 1 | Step 373200 | Avg Loss: 0.0146 | Grad Norm: 0.00937690\n",
      "Epoch 1 | Step 373300 | Avg Loss: 0.0144 | Grad Norm: 0.00952892\n",
      "Epoch 1 | Step 373400 | Avg Loss: 0.0144 | Grad Norm: 0.01146180\n",
      "Epoch 1 | Step 373500 | Avg Loss: 0.0144 | Grad Norm: 0.00910527\n",
      "Epoch 1 | Step 373600 | Avg Loss: 0.0144 | Grad Norm: 0.00925664\n",
      "Epoch 1 | Step 373700 | Avg Loss: 0.0143 | Grad Norm: 0.00862369\n",
      "Epoch 1 | Step 373800 | Avg Loss: 0.0146 | Grad Norm: 0.01082686\n",
      "Epoch 1 | Step 373900 | Avg Loss: 0.0144 | Grad Norm: 0.01104302\n",
      "Epoch 1 | Step 374000 | Avg Loss: 0.0144 | Grad Norm: 0.00843106\n",
      "Epoch 1 | Step 374100 | Avg Loss: 0.0143 | Grad Norm: 0.00777685\n",
      "Epoch 1 | Step 374200 | Avg Loss: 0.0141 | Grad Norm: 0.01004567\n",
      "Epoch 1 | Step 374300 | Avg Loss: 0.0140 | Grad Norm: 0.00899055\n",
      "Epoch 1 | Step 374400 | Avg Loss: 0.0141 | Grad Norm: 0.01067802\n",
      "Epoch 1 | Step 374500 | Avg Loss: 0.0142 | Grad Norm: 0.01015185\n",
      "Epoch 1 | Step 374600 | Avg Loss: 0.0141 | Grad Norm: 0.00997979\n",
      "Epoch 1 | Step 374700 | Avg Loss: 0.0149 | Grad Norm: 0.00949258\n",
      "Epoch 1 | Step 374800 | Avg Loss: 0.0146 | Grad Norm: 0.00901666\n",
      "Epoch 1 | Step 374900 | Avg Loss: 0.0145 | Grad Norm: 0.00941866\n",
      "Epoch 1 | Step 375000 | Avg Loss: 0.0142 | Grad Norm: 0.00868064\n",
      "Epoch 1 | Step 375100 | Avg Loss: 0.0141 | Grad Norm: 0.00990252\n",
      "Epoch 1 | Step 375200 | Avg Loss: 0.0143 | Grad Norm: 0.00865920\n",
      "Epoch 1 | Step 375300 | Avg Loss: 0.0144 | Grad Norm: 0.00892641\n",
      "Epoch 1 | Step 375400 | Avg Loss: 0.0142 | Grad Norm: 0.01052009\n",
      "Epoch 1 | Step 375500 | Avg Loss: 0.0143 | Grad Norm: 0.00865471\n",
      "Epoch 1 | Step 375600 | Avg Loss: 0.0146 | Grad Norm: 0.00999915\n",
      "Epoch 1 | Step 375700 | Avg Loss: 0.0145 | Grad Norm: 0.00986830\n",
      "Epoch 1 | Step 375800 | Avg Loss: 0.0151 | Grad Norm: 0.00961599\n",
      "Epoch 1 | Step 375900 | Avg Loss: 0.0150 | Grad Norm: 0.00901725\n",
      "Epoch 1 | Step 376000 | Avg Loss: 0.0146 | Grad Norm: 0.00785324\n",
      "Epoch 1 | Step 376100 | Avg Loss: 0.0151 | Grad Norm: 0.01003407\n",
      "Epoch 1 | Step 376200 | Avg Loss: 0.0155 | Grad Norm: 0.00943320\n",
      "Epoch 1 | Step 376300 | Avg Loss: 0.0156 | Grad Norm: 0.00942890\n",
      "Epoch 1 | Step 376400 | Avg Loss: 0.0155 | Grad Norm: 0.00951474\n",
      "Epoch 1 | Step 376500 | Avg Loss: 0.0154 | Grad Norm: 0.00967097\n",
      "Epoch 1 | Step 376600 | Avg Loss: 0.0150 | Grad Norm: 0.00929041\n",
      "Epoch 1 | Step 376700 | Avg Loss: 0.0153 | Grad Norm: 0.01132155\n",
      "Epoch 1 | Step 376800 | Avg Loss: 0.0154 | Grad Norm: 0.01242980\n",
      "Epoch 1 | Step 376900 | Avg Loss: 0.0154 | Grad Norm: 0.01006316\n",
      "Epoch 1 | Step 377000 | Avg Loss: 0.0153 | Grad Norm: 0.00981610\n",
      "Epoch 1 | Step 377100 | Avg Loss: 0.0150 | Grad Norm: 0.00930523\n",
      "Epoch 1 | Step 377200 | Avg Loss: 0.0147 | Grad Norm: 0.00980300\n",
      "Epoch 1 | Step 377300 | Avg Loss: 0.0141 | Grad Norm: 0.00935949\n",
      "Epoch 1 | Step 377400 | Avg Loss: 0.0143 | Grad Norm: 0.00985579\n",
      "Epoch 1 | Step 377500 | Avg Loss: 0.0145 | Grad Norm: 0.00836161\n",
      "Epoch 1 | Step 377600 | Avg Loss: 0.0145 | Grad Norm: 0.00894410\n",
      "Epoch 1 | Step 377700 | Avg Loss: 0.0145 | Grad Norm: 0.00823401\n",
      "Epoch 1 | Step 377800 | Avg Loss: 0.0144 | Grad Norm: 0.01116043\n",
      "Epoch 1 | Step 377900 | Avg Loss: 0.0147 | Grad Norm: 0.00976255\n",
      "Epoch 1 | Step 378000 | Avg Loss: 0.0147 | Grad Norm: 0.01002968\n",
      "Epoch 1 | Step 378100 | Avg Loss: 0.0149 | Grad Norm: 0.00948608\n",
      "Epoch 1 | Step 378200 | Avg Loss: 0.0144 | Grad Norm: 0.00761907\n",
      "Epoch 1 | Step 378300 | Avg Loss: 0.0148 | Grad Norm: 0.01011570\n",
      "Epoch 1 | Step 378400 | Avg Loss: 0.0145 | Grad Norm: 0.00992535\n",
      "Epoch 1 | Step 378500 | Avg Loss: 0.0145 | Grad Norm: 0.01012028\n",
      "Epoch 1 | Step 378600 | Avg Loss: 0.0147 | Grad Norm: 0.01000811\n",
      "Epoch 1 | Step 378700 | Avg Loss: 0.0150 | Grad Norm: 0.00902065\n",
      "Epoch 1 | Step 378800 | Avg Loss: 0.0151 | Grad Norm: 0.00857915\n",
      "Epoch 1 | Step 378900 | Avg Loss: 0.0149 | Grad Norm: 0.00913203\n",
      "Epoch 1 | Step 379000 | Avg Loss: 0.0148 | Grad Norm: 0.00967631\n",
      "Epoch 1 | Step 379100 | Avg Loss: 0.0151 | Grad Norm: 0.01099212\n",
      "Epoch 1 | Step 379200 | Avg Loss: 0.0147 | Grad Norm: 0.00961171\n",
      "Epoch 1 | Step 379300 | Avg Loss: 0.0147 | Grad Norm: 0.01038765\n",
      "Epoch 1 | Step 379400 | Avg Loss: 0.0149 | Grad Norm: 0.01013376\n",
      "Epoch 1 | Step 379500 | Avg Loss: 0.0147 | Grad Norm: 0.00939213\n",
      "Epoch 1 | Step 379600 | Avg Loss: 0.0146 | Grad Norm: 0.01036084\n",
      "Epoch 1 | Step 379700 | Avg Loss: 0.0147 | Grad Norm: 0.00986476\n",
      "Epoch 1 | Step 379800 | Avg Loss: 0.0147 | Grad Norm: 0.00875834\n",
      "Epoch 1 | Step 379900 | Avg Loss: 0.0146 | Grad Norm: 0.00943579\n",
      "Epoch 1 | Step 380000 | Avg Loss: 0.0144 | Grad Norm: 0.00867399\n",
      "Epoch 1 | Step 380100 | Avg Loss: 0.0145 | Grad Norm: 0.01022429\n",
      "Epoch 1 | Step 380200 | Avg Loss: 0.0145 | Grad Norm: 0.00907926\n",
      "Epoch 1 | Step 380300 | Avg Loss: 0.0146 | Grad Norm: 0.00850835\n",
      "Epoch 1 | Step 380400 | Avg Loss: 0.0146 | Grad Norm: 0.00939503\n",
      "Epoch 1 | Step 380500 | Avg Loss: 0.0144 | Grad Norm: 0.01126304\n",
      "Epoch 1 | Step 380600 | Avg Loss: 0.0145 | Grad Norm: 0.01139571\n",
      "Epoch 1 | Step 380700 | Avg Loss: 0.0148 | Grad Norm: 0.00997926\n",
      "Epoch 1 | Step 380800 | Avg Loss: 0.0154 | Grad Norm: 0.01014443\n",
      "Epoch 1 | Step 380900 | Avg Loss: 0.0154 | Grad Norm: 0.00926704\n",
      "Epoch 1 | Step 381000 | Avg Loss: 0.0154 | Grad Norm: 0.00993614\n",
      "Epoch 1 | Step 381100 | Avg Loss: 0.0150 | Grad Norm: 0.00972688\n",
      "Epoch 1 | Step 381200 | Avg Loss: 0.0146 | Grad Norm: 0.00858789\n",
      "Epoch 1 | Step 381300 | Avg Loss: 0.0143 | Grad Norm: 0.00921861\n",
      "Epoch 1 | Step 381400 | Avg Loss: 0.0144 | Grad Norm: 0.01057722\n",
      "Epoch 1 | Step 381500 | Avg Loss: 0.0148 | Grad Norm: 0.00869791\n",
      "Epoch 1 | Step 381600 | Avg Loss: 0.0149 | Grad Norm: 0.00886274\n",
      "Epoch 1 | Step 381700 | Avg Loss: 0.0148 | Grad Norm: 0.00863193\n",
      "Epoch 1 | Step 381800 | Avg Loss: 0.0145 | Grad Norm: 0.00825076\n",
      "Epoch 1 | Step 381900 | Avg Loss: 0.0147 | Grad Norm: 0.01034841\n",
      "Epoch 1 | Step 382000 | Avg Loss: 0.0145 | Grad Norm: 0.00905726\n",
      "Epoch 1 | Step 382100 | Avg Loss: 0.0145 | Grad Norm: 0.01075509\n",
      "Epoch 1 | Step 382200 | Avg Loss: 0.0150 | Grad Norm: 0.00986104\n",
      "Epoch 1 | Step 382300 | Avg Loss: 0.0152 | Grad Norm: 0.00872070\n",
      "Epoch 1 | Step 382400 | Avg Loss: 0.0149 | Grad Norm: 0.00824271\n",
      "Epoch 1 | Step 382500 | Avg Loss: 0.0150 | Grad Norm: 0.00993388\n",
      "Epoch 1 | Step 382600 | Avg Loss: 0.0144 | Grad Norm: 0.00940988\n",
      "Epoch 1 | Step 382700 | Avg Loss: 0.0146 | Grad Norm: 0.01017223\n",
      "Epoch 1 | Step 382800 | Avg Loss: 0.0145 | Grad Norm: 0.00968706\n",
      "Epoch 1 | Step 382900 | Avg Loss: 0.0142 | Grad Norm: 0.00957973\n",
      "Epoch 1 | Step 383000 | Avg Loss: 0.0144 | Grad Norm: 0.00892630\n",
      "Epoch 1 | Step 383100 | Avg Loss: 0.0145 | Grad Norm: 0.00985509\n",
      "Epoch 1 | Step 383200 | Avg Loss: 0.0146 | Grad Norm: 0.00999739\n",
      "Epoch 1 | Step 383300 | Avg Loss: 0.0145 | Grad Norm: 0.00985414\n",
      "Epoch 1 | Step 383400 | Avg Loss: 0.0143 | Grad Norm: 0.00839406\n",
      "Epoch 1 | Step 383500 | Avg Loss: 0.0140 | Grad Norm: 0.00828378\n",
      "Epoch 1 | Step 383600 | Avg Loss: 0.0144 | Grad Norm: 0.01005219\n",
      "Epoch 1 | Step 383700 | Avg Loss: 0.0142 | Grad Norm: 0.00915289\n",
      "Epoch 1 | Step 383800 | Avg Loss: 0.0141 | Grad Norm: 0.00991577\n",
      "Epoch 1 | Step 383900 | Avg Loss: 0.0139 | Grad Norm: 0.00965747\n",
      "Epoch 1 | Step 384000 | Avg Loss: 0.0139 | Grad Norm: 0.00967727\n",
      "Epoch 1 | Step 384100 | Avg Loss: 0.0142 | Grad Norm: 0.01020569\n",
      "Epoch 1 | Step 384200 | Avg Loss: 0.0143 | Grad Norm: 0.00950232\n",
      "Epoch 1 | Step 384300 | Avg Loss: 0.0141 | Grad Norm: 0.00946481\n",
      "Epoch 1 | Step 384400 | Avg Loss: 0.0145 | Grad Norm: 0.00909405\n",
      "Epoch 1 | Step 384500 | Avg Loss: 0.0147 | Grad Norm: 0.01060044\n",
      "Epoch 1 | Step 384600 | Avg Loss: 0.0150 | Grad Norm: 0.01001898\n",
      "Epoch 1 | Step 384700 | Avg Loss: 0.0149 | Grad Norm: 0.00904111\n",
      "Epoch 1 | Step 384800 | Avg Loss: 0.0149 | Grad Norm: 0.01050391\n",
      "Epoch 1 | Step 384900 | Avg Loss: 0.0147 | Grad Norm: 0.00819432\n",
      "Epoch 1 | Step 385000 | Avg Loss: 0.0144 | Grad Norm: 0.00937377\n",
      "Epoch 1 | Step 385100 | Avg Loss: 0.0143 | Grad Norm: 0.00974953\n",
      "Epoch 1 | Step 385200 | Avg Loss: 0.0145 | Grad Norm: 0.00923264\n",
      "Epoch 1 | Step 385300 | Avg Loss: 0.0145 | Grad Norm: 0.00902946\n",
      "Epoch 1 | Step 385400 | Avg Loss: 0.0145 | Grad Norm: 0.00952492\n",
      "Epoch 1 | Step 385500 | Avg Loss: 0.0144 | Grad Norm: 0.00925927\n",
      "Epoch 1 | Step 385600 | Avg Loss: 0.0145 | Grad Norm: 0.00869409\n",
      "Epoch 1 | Step 385700 | Avg Loss: 0.0147 | Grad Norm: 0.00939220\n",
      "Epoch 1 | Step 385800 | Avg Loss: 0.0144 | Grad Norm: 0.01005179\n",
      "Epoch 1 | Step 385900 | Avg Loss: 0.0141 | Grad Norm: 0.00868028\n",
      "Epoch 1 | Step 386000 | Avg Loss: 0.0143 | Grad Norm: 0.01022433\n",
      "Epoch 1 | Step 386100 | Avg Loss: 0.0141 | Grad Norm: 0.00983074\n",
      "Epoch 1 | Step 386200 | Avg Loss: 0.0139 | Grad Norm: 0.00931631\n",
      "Epoch 1 | Step 386300 | Avg Loss: 0.0140 | Grad Norm: 0.01020785\n",
      "Epoch 1 | Step 386400 | Avg Loss: 0.0140 | Grad Norm: 0.00952619\n",
      "Epoch 1 | Step 386500 | Avg Loss: 0.0140 | Grad Norm: 0.00840692\n",
      "Epoch 1 | Step 386600 | Avg Loss: 0.0139 | Grad Norm: 0.00953872\n",
      "Epoch 1 | Step 386700 | Avg Loss: 0.0141 | Grad Norm: 0.00824387\n",
      "Epoch 1 | Step 386800 | Avg Loss: 0.0138 | Grad Norm: 0.00912642\n",
      "Epoch 1 | Step 386900 | Avg Loss: 0.0145 | Grad Norm: 0.00866639\n",
      "Epoch 1 | Step 387000 | Avg Loss: 0.0139 | Grad Norm: 0.01019909\n",
      "Epoch 1 | Step 387100 | Avg Loss: 0.0141 | Grad Norm: 0.00950256\n",
      "Epoch 1 | Step 387200 | Avg Loss: 0.0139 | Grad Norm: 0.00985612\n",
      "Epoch 1 | Step 387300 | Avg Loss: 0.0137 | Grad Norm: 0.00897074\n",
      "Epoch 1 | Step 387400 | Avg Loss: 0.0144 | Grad Norm: 0.00943278\n",
      "Epoch 1 | Step 387500 | Avg Loss: 0.0142 | Grad Norm: 0.01027092\n",
      "Epoch 1 | Step 387600 | Avg Loss: 0.0147 | Grad Norm: 0.01070403\n",
      "Epoch 1 | Step 387700 | Avg Loss: 0.0148 | Grad Norm: 0.00962215\n",
      "Epoch 1 | Step 387800 | Avg Loss: 0.0147 | Grad Norm: 0.01011635\n",
      "Epoch 1 | Step 387900 | Avg Loss: 0.0146 | Grad Norm: 0.01008133\n",
      "Epoch 1 | Step 388000 | Avg Loss: 0.0146 | Grad Norm: 0.00868139\n",
      "Epoch 1 | Step 388100 | Avg Loss: 0.0148 | Grad Norm: 0.01057337\n",
      "Epoch 1 | Step 388200 | Avg Loss: 0.0146 | Grad Norm: 0.00949728\n",
      "Epoch 1 | Step 388300 | Avg Loss: 0.0146 | Grad Norm: 0.00962866\n",
      "Epoch 1 | Step 388400 | Avg Loss: 0.0153 | Grad Norm: 0.00963350\n",
      "Epoch 1 | Step 388500 | Avg Loss: 0.0146 | Grad Norm: 0.01112326\n",
      "Epoch 1 | Step 388600 | Avg Loss: 0.0151 | Grad Norm: 0.00851906\n",
      "Epoch 1 | Step 388700 | Avg Loss: 0.0148 | Grad Norm: 0.00951890\n",
      "Epoch 1 | Step 388800 | Avg Loss: 0.0151 | Grad Norm: 0.00859784\n",
      "Epoch 1 | Step 388900 | Avg Loss: 0.0149 | Grad Norm: 0.00857600\n",
      "Epoch 1 | Step 389000 | Avg Loss: 0.0147 | Grad Norm: 0.00869734\n",
      "Epoch 1 | Step 389100 | Avg Loss: 0.0148 | Grad Norm: 0.00818571\n",
      "Epoch 1 | Step 389200 | Avg Loss: 0.0145 | Grad Norm: 0.00992313\n",
      "Epoch 1 | Step 389300 | Avg Loss: 0.0149 | Grad Norm: 0.01061749\n",
      "Epoch 1 | Step 389400 | Avg Loss: 0.0149 | Grad Norm: 0.00897448\n",
      "Epoch 1 | Step 389500 | Avg Loss: 0.0151 | Grad Norm: 0.00805781\n",
      "Epoch 1 | Step 389600 | Avg Loss: 0.0147 | Grad Norm: 0.00960411\n",
      "Epoch 1 | Step 389700 | Avg Loss: 0.0148 | Grad Norm: 0.00871902\n",
      "Epoch 1 | Step 389800 | Avg Loss: 0.0146 | Grad Norm: 0.00819962\n",
      "Epoch 1 | Step 389900 | Avg Loss: 0.0147 | Grad Norm: 0.00978213\n",
      "Epoch 1 | Step 390000 | Avg Loss: 0.0145 | Grad Norm: 0.01027867\n",
      "Epoch 1 | Step 390100 | Avg Loss: 0.0151 | Grad Norm: 0.00781286\n",
      "Epoch 1 | Step 390200 | Avg Loss: 0.0148 | Grad Norm: 0.00861636\n",
      "Epoch 1 | Step 390300 | Avg Loss: 0.0151 | Grad Norm: 0.00995609\n",
      "Epoch 1 | Step 390400 | Avg Loss: 0.0155 | Grad Norm: 0.00959576\n",
      "Epoch 1 | Step 390500 | Avg Loss: 0.0150 | Grad Norm: 0.00881795\n",
      "Epoch 1 | Step 390600 | Avg Loss: 0.0151 | Grad Norm: 0.00920856\n",
      "Epoch 1 | Step 390700 | Avg Loss: 0.0149 | Grad Norm: 0.00918100\n",
      "Epoch 1 | Step 390800 | Avg Loss: 0.0148 | Grad Norm: 0.00955343\n",
      "Epoch 1 | Step 390900 | Avg Loss: 0.0145 | Grad Norm: 0.01112738\n",
      "Epoch 1 | Step 391000 | Avg Loss: 0.0146 | Grad Norm: 0.00978034\n",
      "Epoch 1 | Step 391100 | Avg Loss: 0.0147 | Grad Norm: 0.01087235\n",
      "Epoch 1 | Step 391200 | Avg Loss: 0.0149 | Grad Norm: 0.00924834\n",
      "Epoch 1 | Step 391300 | Avg Loss: 0.0150 | Grad Norm: 0.01088453\n",
      "Epoch 1 | Step 391400 | Avg Loss: 0.0147 | Grad Norm: 0.00954491\n",
      "Epoch 1 | Step 391500 | Avg Loss: 0.0149 | Grad Norm: 0.00905659\n",
      "Epoch 1 | Step 391600 | Avg Loss: 0.0150 | Grad Norm: 0.00937048\n",
      "Epoch 1 | Step 391700 | Avg Loss: 0.0149 | Grad Norm: 0.00871342\n",
      "Epoch 1 | Step 391800 | Avg Loss: 0.0149 | Grad Norm: 0.00879474\n",
      "Epoch 1 | Step 391900 | Avg Loss: 0.0147 | Grad Norm: 0.00944151\n",
      "Epoch 1 | Step 392000 | Avg Loss: 0.0146 | Grad Norm: 0.00993873\n",
      "Epoch 1 | Step 392100 | Avg Loss: 0.0146 | Grad Norm: 0.00902578\n",
      "Epoch 1 | Step 392200 | Avg Loss: 0.0144 | Grad Norm: 0.00965875\n",
      "Epoch 1 | Step 392300 | Avg Loss: 0.0143 | Grad Norm: 0.01007224\n",
      "Epoch 1 | Step 392400 | Avg Loss: 0.0148 | Grad Norm: 0.01160535\n",
      "Epoch 1 | Step 392500 | Avg Loss: 0.0145 | Grad Norm: 0.00860602\n",
      "Epoch 1 | Step 392600 | Avg Loss: 0.0148 | Grad Norm: 0.00841831\n",
      "Epoch 1 | Step 392700 | Avg Loss: 0.0148 | Grad Norm: 0.01075271\n",
      "Epoch 1 | Step 392800 | Avg Loss: 0.0149 | Grad Norm: 0.00960306\n",
      "Epoch 1 | Step 392900 | Avg Loss: 0.0146 | Grad Norm: 0.00966761\n",
      "Epoch 1 | Step 393000 | Avg Loss: 0.0149 | Grad Norm: 0.00832018\n",
      "Epoch 1 | Step 393100 | Avg Loss: 0.0147 | Grad Norm: 0.01072069\n",
      "Epoch 1 | Step 393200 | Avg Loss: 0.0148 | Grad Norm: 0.00817000\n",
      "Epoch 1 | Step 393300 | Avg Loss: 0.0148 | Grad Norm: 0.00925461\n",
      "Epoch 1 | Step 393400 | Avg Loss: 0.0149 | Grad Norm: 0.00869701\n",
      "Epoch 1 | Step 393500 | Avg Loss: 0.0151 | Grad Norm: 0.00801868\n",
      "Epoch 1 | Step 393600 | Avg Loss: 0.0149 | Grad Norm: 0.01037877\n",
      "Epoch 1 | Step 393700 | Avg Loss: 0.0148 | Grad Norm: 0.00899050\n",
      "Epoch 1 | Step 393800 | Avg Loss: 0.0150 | Grad Norm: 0.01196367\n",
      "Epoch 1 | Step 393900 | Avg Loss: 0.0150 | Grad Norm: 0.00910687\n",
      "Epoch 1 | Step 394000 | Avg Loss: 0.0150 | Grad Norm: 0.00954485\n",
      "Epoch 1 | Step 394100 | Avg Loss: 0.0150 | Grad Norm: 0.00953122\n",
      "Epoch 1 | Step 394200 | Avg Loss: 0.0150 | Grad Norm: 0.01002599\n",
      "Epoch 1 | Step 394300 | Avg Loss: 0.0150 | Grad Norm: 0.01168505\n",
      "Epoch 1 | Step 394400 | Avg Loss: 0.0147 | Grad Norm: 0.00907404\n",
      "Epoch 1 | Step 394500 | Avg Loss: 0.0150 | Grad Norm: 0.00934866\n",
      "Epoch 1 | Step 394600 | Avg Loss: 0.0146 | Grad Norm: 0.01011019\n",
      "Epoch 1 | Step 394700 | Avg Loss: 0.0147 | Grad Norm: 0.01015862\n",
      "Epoch 1 | Step 394800 | Avg Loss: 0.0145 | Grad Norm: 0.01031614\n",
      "Epoch 1 | Step 394900 | Avg Loss: 0.0143 | Grad Norm: 0.00977115\n",
      "Epoch 1 | Step 395000 | Avg Loss: 0.0142 | Grad Norm: 0.00828369\n",
      "Epoch 1 | Step 395100 | Avg Loss: 0.0144 | Grad Norm: 0.00999233\n",
      "Epoch 1 | Step 395200 | Avg Loss: 0.0145 | Grad Norm: 0.00963645\n",
      "Epoch 1 | Step 395300 | Avg Loss: 0.0145 | Grad Norm: 0.00964649\n",
      "Epoch 1 | Step 395400 | Avg Loss: 0.0143 | Grad Norm: 0.00872194\n",
      "Epoch 1 | Step 395500 | Avg Loss: 0.0144 | Grad Norm: 0.00953344\n",
      "Epoch 1 | Step 395600 | Avg Loss: 0.0144 | Grad Norm: 0.00977741\n",
      "Epoch 1 | Step 395700 | Avg Loss: 0.0143 | Grad Norm: 0.00882223\n",
      "Epoch 1 | Step 395800 | Avg Loss: 0.0145 | Grad Norm: 0.01020827\n",
      "Epoch 1 | Step 395900 | Avg Loss: 0.0145 | Grad Norm: 0.00961244\n",
      "Epoch 1 | Step 396000 | Avg Loss: 0.0145 | Grad Norm: 0.00929012\n",
      "Epoch 1 | Step 396100 | Avg Loss: 0.0142 | Grad Norm: 0.00920372\n",
      "Epoch 1 | Step 396200 | Avg Loss: 0.0147 | Grad Norm: 0.01042541\n",
      "Epoch 1 | Step 396300 | Avg Loss: 0.0147 | Grad Norm: 0.00855333\n",
      "Epoch 1 | Step 396400 | Avg Loss: 0.0146 | Grad Norm: 0.01005251\n",
      "Epoch 1 | Step 396500 | Avg Loss: 0.0144 | Grad Norm: 0.01102379\n",
      "Epoch 1 | Step 396600 | Avg Loss: 0.0145 | Grad Norm: 0.00946047\n",
      "Epoch 1 | Step 396700 | Avg Loss: 0.0144 | Grad Norm: 0.00833611\n",
      "Epoch 1 | Step 396800 | Avg Loss: 0.0148 | Grad Norm: 0.00944868\n",
      "Epoch 1 | Step 396900 | Avg Loss: 0.0149 | Grad Norm: 0.01029035\n",
      "Epoch 1 | Step 397000 | Avg Loss: 0.0151 | Grad Norm: 0.00861900\n",
      "Epoch 1 | Step 397100 | Avg Loss: 0.0147 | Grad Norm: 0.00831811\n",
      "Epoch 1 | Step 397200 | Avg Loss: 0.0149 | Grad Norm: 0.00908179\n",
      "Epoch 1 | Step 397300 | Avg Loss: 0.0149 | Grad Norm: 0.00936692\n",
      "Epoch 1 | Step 397400 | Avg Loss: 0.0151 | Grad Norm: 0.01040070\n",
      "Epoch 1 | Step 397500 | Avg Loss: 0.0152 | Grad Norm: 0.00905296\n",
      "Epoch 1 | Step 397600 | Avg Loss: 0.0147 | Grad Norm: 0.00796445\n",
      "Epoch 1 | Step 397700 | Avg Loss: 0.0145 | Grad Norm: 0.01110051\n",
      "Epoch 1 | Step 397800 | Avg Loss: 0.0144 | Grad Norm: 0.00960898\n",
      "Epoch 1 | Step 397900 | Avg Loss: 0.0144 | Grad Norm: 0.01022521\n",
      "Epoch 1 | Step 398000 | Avg Loss: 0.0146 | Grad Norm: 0.00938390\n",
      "Epoch 1 | Step 398100 | Avg Loss: 0.0144 | Grad Norm: 0.00838450\n",
      "Epoch 1 | Step 398200 | Avg Loss: 0.0143 | Grad Norm: 0.00887105\n",
      "Epoch 1 | Step 398300 | Avg Loss: 0.0144 | Grad Norm: 0.00966617\n",
      "Epoch 1 | Step 398400 | Avg Loss: 0.0146 | Grad Norm: 0.00899999\n",
      "Epoch 1 | Step 398500 | Avg Loss: 0.0148 | Grad Norm: 0.00878116\n",
      "Epoch 1 | Step 398600 | Avg Loss: 0.0143 | Grad Norm: 0.01023948\n",
      "Epoch 1 | Step 398700 | Avg Loss: 0.0145 | Grad Norm: 0.00902655\n",
      "Epoch 1 | Step 398800 | Avg Loss: 0.0143 | Grad Norm: 0.00881370\n",
      "Epoch 1 | Step 398900 | Avg Loss: 0.0148 | Grad Norm: 0.00878544\n",
      "Epoch 1 | Step 399000 | Avg Loss: 0.0148 | Grad Norm: 0.00858112\n",
      "Epoch 1 | Step 399100 | Avg Loss: 0.0143 | Grad Norm: 0.00853778\n",
      "Epoch 1 | Step 399200 | Avg Loss: 0.0147 | Grad Norm: 0.01162590\n",
      "Epoch 1 | Step 399300 | Avg Loss: 0.0145 | Grad Norm: 0.01064785\n",
      "Epoch 1 | Step 399400 | Avg Loss: 0.0147 | Grad Norm: 0.00904616\n",
      "Epoch 1 | Step 399500 | Avg Loss: 0.0151 | Grad Norm: 0.00897200\n",
      "Epoch 1 | Step 399600 | Avg Loss: 0.0157 | Grad Norm: 0.01160789\n",
      "Epoch 1 | Step 399700 | Avg Loss: 0.0155 | Grad Norm: 0.01071606\n",
      "Epoch 1 | Step 399800 | Avg Loss: 0.0155 | Grad Norm: 0.01236167\n",
      "Epoch 1 | Step 399900 | Avg Loss: 0.0150 | Grad Norm: 0.00829796\n",
      "Epoch 1 | Step 400000 | Avg Loss: 0.0150 | Grad Norm: 0.00890480\n",
      "Saving model at step400000\n",
      "Epoch 1 | Step 400100 | Avg Loss: 0.0149 | Grad Norm: 0.01024346\n",
      "Epoch 1 | Step 400200 | Avg Loss: 0.0145 | Grad Norm: 0.00803057\n",
      "Epoch 1 | Step 400300 | Avg Loss: 0.0147 | Grad Norm: 0.00991221\n",
      "Epoch 1 | Step 400400 | Avg Loss: 0.0147 | Grad Norm: 0.00990151\n",
      "Epoch 1 | Step 400500 | Avg Loss: 0.0145 | Grad Norm: 0.01024175\n",
      "Epoch 1 | Step 400600 | Avg Loss: 0.0142 | Grad Norm: 0.00985460\n",
      "Epoch 1 | Step 400700 | Avg Loss: 0.0143 | Grad Norm: 0.00846668\n",
      "Epoch 1 | Step 400800 | Avg Loss: 0.0141 | Grad Norm: 0.00803144\n",
      "Epoch 1 | Step 400900 | Avg Loss: 0.0140 | Grad Norm: 0.01081885\n",
      "Epoch 1 | Step 401000 | Avg Loss: 0.0144 | Grad Norm: 0.00922185\n",
      "Epoch 1 | Step 401100 | Avg Loss: 0.0141 | Grad Norm: 0.00920620\n",
      "Epoch 1 | Step 401200 | Avg Loss: 0.0143 | Grad Norm: 0.00894095\n",
      "Epoch 1 | Step 401300 | Avg Loss: 0.0143 | Grad Norm: 0.00875020\n",
      "Epoch 1 | Step 401400 | Avg Loss: 0.0143 | Grad Norm: 0.00954009\n",
      "Epoch 1 | Step 401500 | Avg Loss: 0.0143 | Grad Norm: 0.00807857\n",
      "Epoch 1 | Step 401600 | Avg Loss: 0.0147 | Grad Norm: 0.00957875\n",
      "Epoch 1 | Step 401700 | Avg Loss: 0.0146 | Grad Norm: 0.00887151\n",
      "Epoch 1 | Step 401800 | Avg Loss: 0.0141 | Grad Norm: 0.00794656\n",
      "Epoch 1 | Step 401900 | Avg Loss: 0.0141 | Grad Norm: 0.00837514\n",
      "Epoch 1 | Step 402000 | Avg Loss: 0.0144 | Grad Norm: 0.00941451\n",
      "Epoch 1 | Step 402100 | Avg Loss: 0.0146 | Grad Norm: 0.00995130\n",
      "Epoch 1 | Step 402200 | Avg Loss: 0.0149 | Grad Norm: 0.01038790\n",
      "Epoch 1 | Step 402300 | Avg Loss: 0.0143 | Grad Norm: 0.00833234\n",
      "Epoch 1 | Step 402400 | Avg Loss: 0.0141 | Grad Norm: 0.00934482\n",
      "Epoch 1 | Step 402500 | Avg Loss: 0.0144 | Grad Norm: 0.00808992\n",
      "Epoch 1 | Step 402600 | Avg Loss: 0.0142 | Grad Norm: 0.01105143\n",
      "Epoch 1 | Step 402700 | Avg Loss: 0.0143 | Grad Norm: 0.00901567\n",
      "Epoch 1 | Step 402800 | Avg Loss: 0.0140 | Grad Norm: 0.00874036\n",
      "Epoch 1 | Step 402900 | Avg Loss: 0.0141 | Grad Norm: 0.00895340\n",
      "Epoch 1 | Step 403000 | Avg Loss: 0.0145 | Grad Norm: 0.00972284\n",
      "Epoch 1 | Step 403100 | Avg Loss: 0.0144 | Grad Norm: 0.00798774\n",
      "Epoch 1 | Step 403200 | Avg Loss: 0.0145 | Grad Norm: 0.01067092\n",
      "Epoch 1 | Step 403300 | Avg Loss: 0.0146 | Grad Norm: 0.00827945\n",
      "Epoch 1 | Step 403400 | Avg Loss: 0.0147 | Grad Norm: 0.00953983\n",
      "Epoch 1 | Step 403500 | Avg Loss: 0.0145 | Grad Norm: 0.01162039\n",
      "Epoch 1 | Step 403600 | Avg Loss: 0.0143 | Grad Norm: 0.00868484\n",
      "Epoch 1 | Step 403700 | Avg Loss: 0.0146 | Grad Norm: 0.00968402\n",
      "Epoch 1 | Step 403800 | Avg Loss: 0.0145 | Grad Norm: 0.00994559\n",
      "Epoch 1 | Step 403900 | Avg Loss: 0.0146 | Grad Norm: 0.00749698\n",
      "Epoch 1 | Step 404000 | Avg Loss: 0.0147 | Grad Norm: 0.01108511\n",
      "Epoch 1 | Step 404100 | Avg Loss: 0.0148 | Grad Norm: 0.00900821\n",
      "Epoch 1 | Step 404200 | Avg Loss: 0.0146 | Grad Norm: 0.00967322\n",
      "Epoch 1 | Step 404300 | Avg Loss: 0.0146 | Grad Norm: 0.00948946\n",
      "Epoch 1 | Step 404400 | Avg Loss: 0.0147 | Grad Norm: 0.00901015\n",
      "Epoch 1 | Step 404500 | Avg Loss: 0.0151 | Grad Norm: 0.00868984\n",
      "Epoch 1 | Step 404600 | Avg Loss: 0.0157 | Grad Norm: 0.00904520\n",
      "Epoch 1 | Step 404700 | Avg Loss: 0.0154 | Grad Norm: 0.00904820\n",
      "Epoch 1 | Step 404800 | Avg Loss: 0.0154 | Grad Norm: 0.00995571\n",
      "Epoch 1 | Step 404900 | Avg Loss: 0.0148 | Grad Norm: 0.00917620\n",
      "Epoch 1 | Step 405000 | Avg Loss: 0.0149 | Grad Norm: 0.00881291\n",
      "Epoch 1 | Step 405100 | Avg Loss: 0.0148 | Grad Norm: 0.00948298\n",
      "Epoch 1 | Step 405200 | Avg Loss: 0.0148 | Grad Norm: 0.00976945\n",
      "Epoch 1 | Step 405300 | Avg Loss: 0.0150 | Grad Norm: 0.00979580\n",
      "Epoch 1 | Step 405400 | Avg Loss: 0.0149 | Grad Norm: 0.00890417\n",
      "Epoch 1 | Step 405500 | Avg Loss: 0.0148 | Grad Norm: 0.00913525\n",
      "Epoch 1 | Step 405600 | Avg Loss: 0.0149 | Grad Norm: 0.00897866\n",
      "Epoch 1 | Step 405700 | Avg Loss: 0.0150 | Grad Norm: 0.00917061\n",
      "Epoch 1 | Step 405800 | Avg Loss: 0.0150 | Grad Norm: 0.00815078\n",
      "Epoch 1 | Step 405900 | Avg Loss: 0.0150 | Grad Norm: 0.01102764\n",
      "Epoch 1 | Step 406000 | Avg Loss: 0.0143 | Grad Norm: 0.00925691\n",
      "Epoch 1 | Step 406100 | Avg Loss: 0.0145 | Grad Norm: 0.00936498\n",
      "Epoch 1 | Step 406200 | Avg Loss: 0.0143 | Grad Norm: 0.00816991\n",
      "Epoch 1 | Step 406300 | Avg Loss: 0.0145 | Grad Norm: 0.00861722\n",
      "Epoch 1 | Step 406400 | Avg Loss: 0.0147 | Grad Norm: 0.00973462\n",
      "Epoch 1 | Step 406500 | Avg Loss: 0.0148 | Grad Norm: 0.01018941\n",
      "Epoch 1 | Step 406600 | Avg Loss: 0.0148 | Grad Norm: 0.01003025\n",
      "Epoch 1 | Step 406700 | Avg Loss: 0.0148 | Grad Norm: 0.00925035\n",
      "Epoch 1 | Step 406800 | Avg Loss: 0.0150 | Grad Norm: 0.00979442\n",
      "Epoch 1 | Step 406900 | Avg Loss: 0.0148 | Grad Norm: 0.01063338\n",
      "Epoch 1 | Step 407000 | Avg Loss: 0.0150 | Grad Norm: 0.00946038\n",
      "Epoch 1 | Step 407100 | Avg Loss: 0.0150 | Grad Norm: 0.00846814\n",
      "Epoch 1 | Step 407200 | Avg Loss: 0.0150 | Grad Norm: 0.01071544\n",
      "Epoch 1 | Step 407300 | Avg Loss: 0.0152 | Grad Norm: 0.00937352\n",
      "Epoch 1 | Step 407400 | Avg Loss: 0.0149 | Grad Norm: 0.01037060\n",
      "Epoch 1 | Step 407500 | Avg Loss: 0.0147 | Grad Norm: 0.01025369\n",
      "Epoch 1 | Step 407600 | Avg Loss: 0.0149 | Grad Norm: 0.00827907\n",
      "Epoch 1 | Step 407700 | Avg Loss: 0.0149 | Grad Norm: 0.00816348\n",
      "Epoch 1 | Step 407800 | Avg Loss: 0.0147 | Grad Norm: 0.00862617\n",
      "Epoch 1 | Step 407900 | Avg Loss: 0.0147 | Grad Norm: 0.01017856\n",
      "Epoch 1 | Step 408000 | Avg Loss: 0.0145 | Grad Norm: 0.00830483\n",
      "Epoch 1 | Step 408100 | Avg Loss: 0.0146 | Grad Norm: 0.00964524\n",
      "Epoch 1 | Step 408200 | Avg Loss: 0.0145 | Grad Norm: 0.00921508\n",
      "Epoch 1 | Step 408300 | Avg Loss: 0.0145 | Grad Norm: 0.00865089\n",
      "Epoch 1 | Step 408400 | Avg Loss: 0.0142 | Grad Norm: 0.00815989\n",
      "Epoch 1 | Step 408500 | Avg Loss: 0.0143 | Grad Norm: 0.00907026\n",
      "Epoch 1 | Step 408600 | Avg Loss: 0.0141 | Grad Norm: 0.00749739\n",
      "Epoch 1 | Step 408700 | Avg Loss: 0.0144 | Grad Norm: 0.00954664\n",
      "Epoch 1 | Step 408800 | Avg Loss: 0.0147 | Grad Norm: 0.00883197\n",
      "Epoch 1 | Step 408900 | Avg Loss: 0.0148 | Grad Norm: 0.00999178\n",
      "Epoch 1 | Step 409000 | Avg Loss: 0.0146 | Grad Norm: 0.00974395\n",
      "Epoch 1 | Step 409100 | Avg Loss: 0.0144 | Grad Norm: 0.01009521\n",
      "Epoch 1 | Step 409200 | Avg Loss: 0.0146 | Grad Norm: 0.00939586\n",
      "Epoch 1 | Step 409300 | Avg Loss: 0.0154 | Grad Norm: 0.00994174\n",
      "Epoch 1 | Step 409400 | Avg Loss: 0.0154 | Grad Norm: 0.00912216\n",
      "Epoch 1 | Step 409500 | Avg Loss: 0.0151 | Grad Norm: 0.00902374\n",
      "Epoch 1 | Step 409600 | Avg Loss: 0.0152 | Grad Norm: 0.00951552\n",
      "Epoch 1 | Step 409700 | Avg Loss: 0.0149 | Grad Norm: 0.00822283\n",
      "Epoch 1 | Step 409800 | Avg Loss: 0.0144 | Grad Norm: 0.00773296\n",
      "Epoch 1 | Step 409900 | Avg Loss: 0.0144 | Grad Norm: 0.01034321\n",
      "Epoch 1 | Step 410000 | Avg Loss: 0.0143 | Grad Norm: 0.00811806\n",
      "Epoch 1 | Step 410100 | Avg Loss: 0.0143 | Grad Norm: 0.00868596\n",
      "Epoch 1 | Step 410200 | Avg Loss: 0.0144 | Grad Norm: 0.01152609\n",
      "Epoch 1 | Step 410300 | Avg Loss: 0.0143 | Grad Norm: 0.00834754\n",
      "Epoch 1 | Step 410400 | Avg Loss: 0.0143 | Grad Norm: 0.01020334\n",
      "Epoch 1 | Step 410500 | Avg Loss: 0.0143 | Grad Norm: 0.01024833\n",
      "Epoch 1 | Step 410600 | Avg Loss: 0.0143 | Grad Norm: 0.00846917\n",
      "Epoch 1 | Step 410700 | Avg Loss: 0.0143 | Grad Norm: 0.00844842\n",
      "Epoch 1 | Step 410800 | Avg Loss: 0.0142 | Grad Norm: 0.00764354\n",
      "Epoch 1 | Step 410900 | Avg Loss: 0.0143 | Grad Norm: 0.00978333\n",
      "Epoch 1 | Step 411000 | Avg Loss: 0.0146 | Grad Norm: 0.00821651\n",
      "Epoch 1 | Step 411100 | Avg Loss: 0.0145 | Grad Norm: 0.00965560\n",
      "Epoch 1 | Step 411200 | Avg Loss: 0.0147 | Grad Norm: 0.00951446\n",
      "Epoch 1 | Step 411300 | Avg Loss: 0.0150 | Grad Norm: 0.00947803\n",
      "Epoch 1 | Step 411400 | Avg Loss: 0.0151 | Grad Norm: 0.00922215\n",
      "Epoch 1 | Step 411500 | Avg Loss: 0.0152 | Grad Norm: 0.00868703\n",
      "Epoch 1 | Step 411600 | Avg Loss: 0.0150 | Grad Norm: 0.00927921\n",
      "Epoch 1 | Step 411700 | Avg Loss: 0.0149 | Grad Norm: 0.00905349\n",
      "Epoch 1 | Step 411800 | Avg Loss: 0.0151 | Grad Norm: 0.01007250\n",
      "Epoch 1 | Step 411900 | Avg Loss: 0.0149 | Grad Norm: 0.00895056\n",
      "Epoch 1 | Step 412000 | Avg Loss: 0.0147 | Grad Norm: 0.01034745\n",
      "Epoch 1 | Step 412100 | Avg Loss: 0.0148 | Grad Norm: 0.01068824\n",
      "Epoch 1 | Step 412200 | Avg Loss: 0.0144 | Grad Norm: 0.00861507\n",
      "Epoch 1 | Step 412300 | Avg Loss: 0.0147 | Grad Norm: 0.01127672\n",
      "Epoch 1 | Step 412400 | Avg Loss: 0.0147 | Grad Norm: 0.01212983\n",
      "Epoch 1 | Step 412500 | Avg Loss: 0.0145 | Grad Norm: 0.00820748\n",
      "Epoch 1 | Step 412600 | Avg Loss: 0.0146 | Grad Norm: 0.00872553\n",
      "Epoch 1 | Step 412700 | Avg Loss: 0.0145 | Grad Norm: 0.00863316\n",
      "Epoch 1 | Step 412800 | Avg Loss: 0.0146 | Grad Norm: 0.00925991\n",
      "Epoch 1 | Step 412900 | Avg Loss: 0.0146 | Grad Norm: 0.01059247\n",
      "Epoch 1 | Step 413000 | Avg Loss: 0.0146 | Grad Norm: 0.00993391\n",
      "Epoch 1 | Step 413100 | Avg Loss: 0.0146 | Grad Norm: 0.01025259\n",
      "Epoch 1 | Step 413200 | Avg Loss: 0.0144 | Grad Norm: 0.00924566\n",
      "Epoch 1 | Step 413300 | Avg Loss: 0.0147 | Grad Norm: 0.00916821\n",
      "Epoch 1 | Step 413400 | Avg Loss: 0.0145 | Grad Norm: 0.01037098\n",
      "Epoch 1 | Step 413500 | Avg Loss: 0.0147 | Grad Norm: 0.00920961\n",
      "Epoch 1 | Step 413600 | Avg Loss: 0.0148 | Grad Norm: 0.00879367\n",
      "Epoch 1 | Step 413700 | Avg Loss: 0.0148 | Grad Norm: 0.01142718\n",
      "Epoch 1 | Step 413800 | Avg Loss: 0.0147 | Grad Norm: 0.00913511\n",
      "Epoch 1 | Step 413900 | Avg Loss: 0.0149 | Grad Norm: 0.00950271\n",
      "Epoch 1 | Step 414000 | Avg Loss: 0.0149 | Grad Norm: 0.00810935\n",
      "Epoch 1 | Step 414100 | Avg Loss: 0.0148 | Grad Norm: 0.00947273\n",
      "Epoch 1 | Step 414200 | Avg Loss: 0.0148 | Grad Norm: 0.00884390\n",
      "Epoch 1 | Step 414300 | Avg Loss: 0.0150 | Grad Norm: 0.00891871\n",
      "Epoch 1 | Step 414400 | Avg Loss: 0.0153 | Grad Norm: 0.00986534\n",
      "Epoch 1 | Step 414500 | Avg Loss: 0.0152 | Grad Norm: 0.01058128\n",
      "Epoch 1 | Step 414600 | Avg Loss: 0.0150 | Grad Norm: 0.00968584\n",
      "Epoch 1 | Step 414700 | Avg Loss: 0.0150 | Grad Norm: 0.01256722\n",
      "Epoch 1 | Step 414800 | Avg Loss: 0.0150 | Grad Norm: 0.00863720\n",
      "Epoch 1 | Step 414900 | Avg Loss: 0.0150 | Grad Norm: 0.01124370\n",
      "Epoch 1 | Step 415000 | Avg Loss: 0.0149 | Grad Norm: 0.00876813\n",
      "Epoch 1 | Step 415100 | Avg Loss: 0.0145 | Grad Norm: 0.00960319\n",
      "Epoch 1 | Step 415200 | Avg Loss: 0.0141 | Grad Norm: 0.00827492\n",
      "Epoch 1 | Step 415300 | Avg Loss: 0.0144 | Grad Norm: 0.00909602\n",
      "Epoch 1 | Step 415400 | Avg Loss: 0.0146 | Grad Norm: 0.00946226\n",
      "Epoch 1 | Step 415500 | Avg Loss: 0.0145 | Grad Norm: 0.00929051\n",
      "Epoch 1 | Step 415600 | Avg Loss: 0.0146 | Grad Norm: 0.00826520\n",
      "Epoch 1 | Step 415700 | Avg Loss: 0.0147 | Grad Norm: 0.00830076\n",
      "Epoch 1 | Step 415800 | Avg Loss: 0.0146 | Grad Norm: 0.00816721\n",
      "Epoch 1 | Step 415900 | Avg Loss: 0.0151 | Grad Norm: 0.00990334\n",
      "Epoch 1 | Step 416000 | Avg Loss: 0.0150 | Grad Norm: 0.01156515\n",
      "Epoch 1 | Step 416100 | Avg Loss: 0.0151 | Grad Norm: 0.01013029\n",
      "Epoch 1 | Step 416200 | Avg Loss: 0.0145 | Grad Norm: 0.00897536\n",
      "Epoch 1 | Step 416300 | Avg Loss: 0.0146 | Grad Norm: 0.00930882\n",
      "Epoch 1 | Step 416400 | Avg Loss: 0.0147 | Grad Norm: 0.01028730\n",
      "Epoch 1 | Step 416500 | Avg Loss: 0.0143 | Grad Norm: 0.00884899\n",
      "Epoch 1 | Step 416600 | Avg Loss: 0.0142 | Grad Norm: 0.00901188\n",
      "Epoch 1 | Step 416700 | Avg Loss: 0.0146 | Grad Norm: 0.00949930\n",
      "Epoch 1 | Step 416800 | Avg Loss: 0.0151 | Grad Norm: 0.00929721\n",
      "Epoch 1 | Step 416900 | Avg Loss: 0.0150 | Grad Norm: 0.00894645\n",
      "Epoch 1 | Step 417000 | Avg Loss: 0.0150 | Grad Norm: 0.00839812\n",
      "Epoch 1 | Step 417100 | Avg Loss: 0.0147 | Grad Norm: 0.00813230\n",
      "Epoch 1 | Step 417200 | Avg Loss: 0.0148 | Grad Norm: 0.00933003\n",
      "Epoch 1 | Step 417300 | Avg Loss: 0.0151 | Grad Norm: 0.00863755\n",
      "Epoch 1 | Step 417400 | Avg Loss: 0.0151 | Grad Norm: 0.00919419\n",
      "Epoch 1 | Step 417500 | Avg Loss: 0.0150 | Grad Norm: 0.00995356\n",
      "Epoch 1 | Step 417600 | Avg Loss: 0.0150 | Grad Norm: 0.00956978\n",
      "Epoch 1 | Step 417700 | Avg Loss: 0.0151 | Grad Norm: 0.00984524\n",
      "Epoch 1 | Step 417800 | Avg Loss: 0.0148 | Grad Norm: 0.00962049\n",
      "Epoch 1 | Step 417900 | Avg Loss: 0.0148 | Grad Norm: 0.00918747\n",
      "Epoch 1 | Step 418000 | Avg Loss: 0.0148 | Grad Norm: 0.00858945\n",
      "Epoch 1 | Step 418100 | Avg Loss: 0.0149 | Grad Norm: 0.00913083\n",
      "Epoch 1 | Step 418200 | Avg Loss: 0.0149 | Grad Norm: 0.00857281\n",
      "Epoch 1 | Step 418300 | Avg Loss: 0.0148 | Grad Norm: 0.01012499\n",
      "Epoch 1 | Step 418400 | Avg Loss: 0.0148 | Grad Norm: 0.01060583\n",
      "Epoch 1 | Step 418500 | Avg Loss: 0.0148 | Grad Norm: 0.00928459\n",
      "Epoch 1 | Step 418600 | Avg Loss: 0.0147 | Grad Norm: 0.00991333\n",
      "Epoch 1 | Step 418700 | Avg Loss: 0.0153 | Grad Norm: 0.00811584\n",
      "Epoch 1 | Step 418800 | Avg Loss: 0.0152 | Grad Norm: 0.00927666\n",
      "Epoch 1 | Step 418900 | Avg Loss: 0.0152 | Grad Norm: 0.00864220\n",
      "Epoch 1 | Step 419000 | Avg Loss: 0.0150 | Grad Norm: 0.00845426\n",
      "Epoch 1 | Step 419100 | Avg Loss: 0.0148 | Grad Norm: 0.00923712\n",
      "Epoch 1 | Step 419200 | Avg Loss: 0.0147 | Grad Norm: 0.01046541\n",
      "Epoch 1 | Step 419300 | Avg Loss: 0.0149 | Grad Norm: 0.00859902\n",
      "Epoch 1 | Step 419400 | Avg Loss: 0.0151 | Grad Norm: 0.01182948\n",
      "Epoch 1 | Step 419500 | Avg Loss: 0.0148 | Grad Norm: 0.00884984\n",
      "Epoch 1 | Step 419600 | Avg Loss: 0.0149 | Grad Norm: 0.00938531\n",
      "Epoch 1 | Step 419700 | Avg Loss: 0.0148 | Grad Norm: 0.00992880\n",
      "Epoch 1 | Step 419800 | Avg Loss: 0.0142 | Grad Norm: 0.00913790\n",
      "Epoch 1 | Step 419900 | Avg Loss: 0.0140 | Grad Norm: 0.00864823\n",
      "Epoch 1 | Step 420000 | Avg Loss: 0.0139 | Grad Norm: 0.00786851\n",
      "Epoch 1 | Step 420100 | Avg Loss: 0.0141 | Grad Norm: 0.00990099\n",
      "Epoch 1 | Step 420200 | Avg Loss: 0.0143 | Grad Norm: 0.01018084\n",
      "Epoch 1 | Step 420300 | Avg Loss: 0.0145 | Grad Norm: 0.00805350\n",
      "Epoch 1 | Step 420400 | Avg Loss: 0.0147 | Grad Norm: 0.00908878\n",
      "Epoch 1 | Step 420500 | Avg Loss: 0.0149 | Grad Norm: 0.00980197\n",
      "Epoch 1 | Step 420600 | Avg Loss: 0.0146 | Grad Norm: 0.00948403\n",
      "Epoch 1 | Step 420700 | Avg Loss: 0.0149 | Grad Norm: 0.00936763\n",
      "Epoch 1 | Step 420800 | Avg Loss: 0.0151 | Grad Norm: 0.00885874\n",
      "Epoch 1 | Step 420900 | Avg Loss: 0.0151 | Grad Norm: 0.00851049\n",
      "Epoch 1 | Step 421000 | Avg Loss: 0.0149 | Grad Norm: 0.00868744\n",
      "Epoch 1 | Step 421100 | Avg Loss: 0.0150 | Grad Norm: 0.00961034\n",
      "Epoch 1 | Step 421200 | Avg Loss: 0.0148 | Grad Norm: 0.00751346\n",
      "Epoch 1 | Step 421300 | Avg Loss: 0.0146 | Grad Norm: 0.00950337\n",
      "Epoch 1 | Step 421400 | Avg Loss: 0.0144 | Grad Norm: 0.00946879\n",
      "Epoch 1 | Step 421500 | Avg Loss: 0.0144 | Grad Norm: 0.00963190\n",
      "Epoch 1 | Step 421600 | Avg Loss: 0.0146 | Grad Norm: 0.00864562\n",
      "Epoch 1 | Step 421700 | Avg Loss: 0.0146 | Grad Norm: 0.01106920\n",
      "Epoch 1 | Step 421800 | Avg Loss: 0.0149 | Grad Norm: 0.00978917\n",
      "Epoch 1 | Step 421900 | Avg Loss: 0.0153 | Grad Norm: 0.01088629\n",
      "Epoch 1 | Step 422000 | Avg Loss: 0.0150 | Grad Norm: 0.01007463\n",
      "Epoch 1 | Step 422100 | Avg Loss: 0.0144 | Grad Norm: 0.00944174\n",
      "Epoch 1 | Step 422200 | Avg Loss: 0.0143 | Grad Norm: 0.00955368\n",
      "Epoch 1 | Step 422300 | Avg Loss: 0.0142 | Grad Norm: 0.00866709\n",
      "Epoch 1 | Step 422400 | Avg Loss: 0.0143 | Grad Norm: 0.00896967\n",
      "Epoch 1 | Step 422500 | Avg Loss: 0.0146 | Grad Norm: 0.00967274\n",
      "Epoch 1 | Step 422600 | Avg Loss: 0.0144 | Grad Norm: 0.00855417\n",
      "Epoch 1 | Step 422700 | Avg Loss: 0.0144 | Grad Norm: 0.00908479\n",
      "Epoch 1 | Step 422800 | Avg Loss: 0.0147 | Grad Norm: 0.00903473\n",
      "Epoch 1 | Step 422900 | Avg Loss: 0.0147 | Grad Norm: 0.00919504\n",
      "Epoch 1 | Step 423000 | Avg Loss: 0.0150 | Grad Norm: 0.00850446\n",
      "Epoch 1 | Step 423100 | Avg Loss: 0.0148 | Grad Norm: 0.01049064\n",
      "Epoch 1 | Step 423200 | Avg Loss: 0.0149 | Grad Norm: 0.00810605\n",
      "Epoch 1 | Step 423300 | Avg Loss: 0.0148 | Grad Norm: 0.01139880\n",
      "Epoch 1 | Step 423400 | Avg Loss: 0.0148 | Grad Norm: 0.01154872\n",
      "Epoch 1 | Step 423500 | Avg Loss: 0.0147 | Grad Norm: 0.00883748\n",
      "Epoch 1 | Step 423600 | Avg Loss: 0.0147 | Grad Norm: 0.00802972\n",
      "Epoch 1 | Step 423700 | Avg Loss: 0.0148 | Grad Norm: 0.00912861\n",
      "Epoch 1 | Step 423800 | Avg Loss: 0.0145 | Grad Norm: 0.00943844\n",
      "Epoch 1 | Step 423900 | Avg Loss: 0.0142 | Grad Norm: 0.01008863\n",
      "Epoch 1 | Step 424000 | Avg Loss: 0.0145 | Grad Norm: 0.00981404\n",
      "Epoch 1 | Step 424100 | Avg Loss: 0.0143 | Grad Norm: 0.00883596\n",
      "Epoch 1 | Step 424200 | Avg Loss: 0.0144 | Grad Norm: 0.00839166\n",
      "Epoch 1 | Step 424300 | Avg Loss: 0.0146 | Grad Norm: 0.00780283\n",
      "Epoch 1 | Step 424400 | Avg Loss: 0.0146 | Grad Norm: 0.00870567\n",
      "Epoch 1 | Step 424500 | Avg Loss: 0.0145 | Grad Norm: 0.00987096\n",
      "Epoch 1 | Step 424600 | Avg Loss: 0.0145 | Grad Norm: 0.00886486\n",
      "Epoch 1 | Step 424700 | Avg Loss: 0.0149 | Grad Norm: 0.00938972\n",
      "Epoch 1 | Step 424800 | Avg Loss: 0.0148 | Grad Norm: 0.00907066\n",
      "Epoch 1 | Step 424900 | Avg Loss: 0.0150 | Grad Norm: 0.00911666\n",
      "Epoch 1 | Step 425000 | Avg Loss: 0.0148 | Grad Norm: 0.01020773\n",
      "Epoch 1 | Step 425100 | Avg Loss: 0.0151 | Grad Norm: 0.00858363\n",
      "Epoch 1 | Step 425200 | Avg Loss: 0.0151 | Grad Norm: 0.00884481\n",
      "Epoch 1 | Step 425300 | Avg Loss: 0.0146 | Grad Norm: 0.00925148\n",
      "Epoch 1 | Step 425400 | Avg Loss: 0.0149 | Grad Norm: 0.00781822\n",
      "Epoch 1 | Step 425500 | Avg Loss: 0.0148 | Grad Norm: 0.00886289\n",
      "Epoch 1 | Step 425600 | Avg Loss: 0.0148 | Grad Norm: 0.00826913\n",
      "Epoch 1 | Step 425700 | Avg Loss: 0.0147 | Grad Norm: 0.00891572\n",
      "Epoch 1 | Step 425800 | Avg Loss: 0.0149 | Grad Norm: 0.00958472\n",
      "Epoch 1 | Step 425900 | Avg Loss: 0.0146 | Grad Norm: 0.00878403\n",
      "Epoch 1 | Step 426000 | Avg Loss: 0.0145 | Grad Norm: 0.00866159\n",
      "Epoch 1 | Step 426100 | Avg Loss: 0.0144 | Grad Norm: 0.00901752\n",
      "Epoch 1 | Step 426200 | Avg Loss: 0.0146 | Grad Norm: 0.00923556\n",
      "Epoch 1 | Step 426300 | Avg Loss: 0.0142 | Grad Norm: 0.00918929\n",
      "Epoch 1 | Step 426400 | Avg Loss: 0.0143 | Grad Norm: 0.00876644\n",
      "Epoch 1 | Step 426500 | Avg Loss: 0.0141 | Grad Norm: 0.00872168\n",
      "Epoch 1 | Step 426600 | Avg Loss: 0.0140 | Grad Norm: 0.00767377\n",
      "Epoch 1 | Step 426700 | Avg Loss: 0.0142 | Grad Norm: 0.00841765\n",
      "Epoch 1 | Step 426800 | Avg Loss: 0.0144 | Grad Norm: 0.00836382\n",
      "Epoch 1 | Step 426900 | Avg Loss: 0.0148 | Grad Norm: 0.00854672\n",
      "Epoch 1 | Step 427000 | Avg Loss: 0.0147 | Grad Norm: 0.00888059\n",
      "Epoch 1 | Step 427100 | Avg Loss: 0.0144 | Grad Norm: 0.00838653\n",
      "Epoch 1 | Step 427200 | Avg Loss: 0.0147 | Grad Norm: 0.00817851\n",
      "Epoch 1 | Step 427300 | Avg Loss: 0.0150 | Grad Norm: 0.00855441\n",
      "Epoch 1 | Step 427400 | Avg Loss: 0.0148 | Grad Norm: 0.00970813\n",
      "Epoch 1 | Step 427500 | Avg Loss: 0.0151 | Grad Norm: 0.00929618\n",
      "Epoch 1 | Step 427600 | Avg Loss: 0.0149 | Grad Norm: 0.00926750\n",
      "Epoch 1 | Step 427700 | Avg Loss: 0.0150 | Grad Norm: 0.00952070\n",
      "Epoch 1 | Step 427800 | Avg Loss: 0.0149 | Grad Norm: 0.00925310\n",
      "Epoch 1 | Step 427900 | Avg Loss: 0.0148 | Grad Norm: 0.00944999\n",
      "Epoch 1 | Step 428000 | Avg Loss: 0.0146 | Grad Norm: 0.00938507\n",
      "Epoch 1 | Step 428100 | Avg Loss: 0.0147 | Grad Norm: 0.00873495\n",
      "Epoch 1 | Step 428200 | Avg Loss: 0.0147 | Grad Norm: 0.01043543\n",
      "Epoch 1 | Step 428300 | Avg Loss: 0.0148 | Grad Norm: 0.00987706\n",
      "Epoch 1 | Step 428400 | Avg Loss: 0.0147 | Grad Norm: 0.01016304\n",
      "Epoch 1 | Step 428500 | Avg Loss: 0.0143 | Grad Norm: 0.00810811\n",
      "Epoch 1 | Step 428600 | Avg Loss: 0.0145 | Grad Norm: 0.00915020\n",
      "Epoch 1 | Step 428700 | Avg Loss: 0.0145 | Grad Norm: 0.00898647\n",
      "Epoch 1 | Step 428800 | Avg Loss: 0.0148 | Grad Norm: 0.00944198\n",
      "Epoch 1 | Step 428900 | Avg Loss: 0.0145 | Grad Norm: 0.01012796\n",
      "Epoch 1 | Step 429000 | Avg Loss: 0.0147 | Grad Norm: 0.00810667\n",
      "Epoch 1 | Step 429100 | Avg Loss: 0.0148 | Grad Norm: 0.00836597\n",
      "Epoch 1 | Step 429200 | Avg Loss: 0.0145 | Grad Norm: 0.00950670\n",
      "Epoch 1 | Step 429300 | Avg Loss: 0.0146 | Grad Norm: 0.00906843\n",
      "Epoch 1 | Step 429400 | Avg Loss: 0.0147 | Grad Norm: 0.00962779\n",
      "Epoch 1 | Step 429500 | Avg Loss: 0.0149 | Grad Norm: 0.00943348\n",
      "Epoch 1 | Step 429600 | Avg Loss: 0.0149 | Grad Norm: 0.00924358\n",
      "Epoch 1 | Step 429700 | Avg Loss: 0.0148 | Grad Norm: 0.00906042\n",
      "Epoch 1 | Step 429800 | Avg Loss: 0.0148 | Grad Norm: 0.00885310\n",
      "Epoch 1 | Step 429900 | Avg Loss: 0.0148 | Grad Norm: 0.00804084\n",
      "Epoch 1 | Step 430000 | Avg Loss: 0.0147 | Grad Norm: 0.00950148\n",
      "Epoch 1 | Step 430100 | Avg Loss: 0.0150 | Grad Norm: 0.00939282\n",
      "Epoch 1 | Step 430200 | Avg Loss: 0.0144 | Grad Norm: 0.01049888\n",
      "Epoch 1 | Step 430300 | Avg Loss: 0.0147 | Grad Norm: 0.00890588\n",
      "Epoch 1 | Step 430400 | Avg Loss: 0.0144 | Grad Norm: 0.00916292\n",
      "Epoch 1 | Step 430500 | Avg Loss: 0.0144 | Grad Norm: 0.00906023\n",
      "Epoch 1 | Step 430600 | Avg Loss: 0.0142 | Grad Norm: 0.00986008\n",
      "Epoch 1 | Step 430700 | Avg Loss: 0.0143 | Grad Norm: 0.00891708\n",
      "Epoch 1 | Step 430800 | Avg Loss: 0.0144 | Grad Norm: 0.00849046\n",
      "Epoch 1 | Step 430900 | Avg Loss: 0.0145 | Grad Norm: 0.00855810\n",
      "Epoch 1 | Step 431000 | Avg Loss: 0.0145 | Grad Norm: 0.00947735\n",
      "Epoch 1 | Step 431100 | Avg Loss: 0.0147 | Grad Norm: 0.00885102\n",
      "Epoch 1 | Step 431200 | Avg Loss: 0.0151 | Grad Norm: 0.00855184\n",
      "Epoch 1 | Step 431300 | Avg Loss: 0.0151 | Grad Norm: 0.01105887\n",
      "Epoch 1 | Step 431400 | Avg Loss: 0.0146 | Grad Norm: 0.00972676\n",
      "Epoch 1 | Step 431500 | Avg Loss: 0.0145 | Grad Norm: 0.01229169\n",
      "Epoch 1 | Step 431600 | Avg Loss: 0.0147 | Grad Norm: 0.00947421\n",
      "Epoch 1 | Step 431700 | Avg Loss: 0.0146 | Grad Norm: 0.01111661\n",
      "Epoch 1 | Step 431800 | Avg Loss: 0.0148 | Grad Norm: 0.01022806\n",
      "Epoch 1 | Step 431900 | Avg Loss: 0.0150 | Grad Norm: 0.00954354\n",
      "Epoch 1 | Step 432000 | Avg Loss: 0.0150 | Grad Norm: 0.00861015\n",
      "Epoch 1 | Step 432100 | Avg Loss: 0.0147 | Grad Norm: 0.00950094\n",
      "Epoch 1 | Step 432200 | Avg Loss: 0.0143 | Grad Norm: 0.01029744\n",
      "Epoch 1 | Step 432300 | Avg Loss: 0.0151 | Grad Norm: 0.00793870\n",
      "Epoch 1 | Step 432400 | Avg Loss: 0.0148 | Grad Norm: 0.00850997\n",
      "Epoch 1 | Step 432500 | Avg Loss: 0.0149 | Grad Norm: 0.00934883\n",
      "Epoch 1 | Step 432600 | Avg Loss: 0.0153 | Grad Norm: 0.01098990\n",
      "Epoch 1 | Step 432700 | Avg Loss: 0.0153 | Grad Norm: 0.00993831\n",
      "Epoch 1 | Step 432800 | Avg Loss: 0.0154 | Grad Norm: 0.00890965\n",
      "Epoch 1 | Step 432900 | Avg Loss: 0.0152 | Grad Norm: 0.00901396\n",
      "Epoch 1 | Step 433000 | Avg Loss: 0.0152 | Grad Norm: 0.01218645\n",
      "Epoch 1 | Step 433100 | Avg Loss: 0.0147 | Grad Norm: 0.00896804\n",
      "Epoch 1 | Step 433200 | Avg Loss: 0.0147 | Grad Norm: 0.00886803\n",
      "Epoch 1 | Step 433300 | Avg Loss: 0.0148 | Grad Norm: 0.00866097\n",
      "Epoch 1 | Step 433400 | Avg Loss: 0.0149 | Grad Norm: 0.00766229\n",
      "Epoch 1 | Step 433500 | Avg Loss: 0.0146 | Grad Norm: 0.00808570\n",
      "Epoch 1 | Step 433600 | Avg Loss: 0.0149 | Grad Norm: 0.01119312\n",
      "Epoch 1 | Step 433700 | Avg Loss: 0.0148 | Grad Norm: 0.00876770\n",
      "Epoch 1 | Step 433800 | Avg Loss: 0.0144 | Grad Norm: 0.00945232\n",
      "Epoch 1 | Step 433900 | Avg Loss: 0.0146 | Grad Norm: 0.00936174\n",
      "Epoch 1 | Step 434000 | Avg Loss: 0.0150 | Grad Norm: 0.00932402\n",
      "Epoch 1 | Step 434100 | Avg Loss: 0.0148 | Grad Norm: 0.01000500\n",
      "Epoch 1 | Step 434200 | Avg Loss: 0.0148 | Grad Norm: 0.00972195\n",
      "Epoch 1 | Step 434300 | Avg Loss: 0.0146 | Grad Norm: 0.00936589\n",
      "Epoch 1 | Step 434400 | Avg Loss: 0.0150 | Grad Norm: 0.00962169\n",
      "Epoch 1 | Step 434500 | Avg Loss: 0.0146 | Grad Norm: 0.00833330\n",
      "Epoch 1 | Step 434600 | Avg Loss: 0.0144 | Grad Norm: 0.00819886\n",
      "Epoch 1 | Step 434700 | Avg Loss: 0.0144 | Grad Norm: 0.01040459\n",
      "Epoch 1 | Step 434800 | Avg Loss: 0.0144 | Grad Norm: 0.00825042\n",
      "Epoch 1 | Step 434900 | Avg Loss: 0.0147 | Grad Norm: 0.00933627\n",
      "Epoch 1 | Step 435000 | Avg Loss: 0.0146 | Grad Norm: 0.01078487\n",
      "Epoch 1 | Step 435100 | Avg Loss: 0.0143 | Grad Norm: 0.00880133\n",
      "Epoch 1 | Step 435200 | Avg Loss: 0.0143 | Grad Norm: 0.00956839\n",
      "Epoch 1 | Step 435300 | Avg Loss: 0.0143 | Grad Norm: 0.00785454\n",
      "Epoch 1 | Step 435400 | Avg Loss: 0.0146 | Grad Norm: 0.00833501\n",
      "Epoch 1 | Step 435500 | Avg Loss: 0.0144 | Grad Norm: 0.01024834\n",
      "Epoch 1 | Step 435600 | Avg Loss: 0.0142 | Grad Norm: 0.01155492\n",
      "Epoch 1 | Step 435700 | Avg Loss: 0.0147 | Grad Norm: 0.00960962\n",
      "Epoch 1 | Step 435800 | Avg Loss: 0.0147 | Grad Norm: 0.00958702\n",
      "Epoch 1 | Step 435900 | Avg Loss: 0.0148 | Grad Norm: 0.01020101\n",
      "Epoch 1 | Step 436000 | Avg Loss: 0.0146 | Grad Norm: 0.00868632\n",
      "Epoch 1 | Step 436100 | Avg Loss: 0.0147 | Grad Norm: 0.00868612\n",
      "Epoch 1 | Step 436200 | Avg Loss: 0.0149 | Grad Norm: 0.00801571\n",
      "Epoch 1 | Step 436300 | Avg Loss: 0.0149 | Grad Norm: 0.00956321\n",
      "Epoch 1 | Step 436400 | Avg Loss: 0.0153 | Grad Norm: 0.00933916\n",
      "Epoch 1 | Step 436500 | Avg Loss: 0.0149 | Grad Norm: 0.01013822\n",
      "Epoch 1 | Step 436600 | Avg Loss: 0.0149 | Grad Norm: 0.01081319\n",
      "Epoch 1 | Step 436700 | Avg Loss: 0.0149 | Grad Norm: 0.01198690\n",
      "Epoch 1 | Step 436800 | Avg Loss: 0.0149 | Grad Norm: 0.00891769\n",
      "Epoch 1 | Step 436900 | Avg Loss: 0.0148 | Grad Norm: 0.00885027\n",
      "Epoch 1 | Step 437000 | Avg Loss: 0.0144 | Grad Norm: 0.00841264\n",
      "Epoch 1 | Step 437100 | Avg Loss: 0.0141 | Grad Norm: 0.00870282\n",
      "Epoch 1 | Step 437200 | Avg Loss: 0.0146 | Grad Norm: 0.00897351\n",
      "Epoch 1 | Step 437300 | Avg Loss: 0.0148 | Grad Norm: 0.00976134\n",
      "Epoch 1 | Step 437400 | Avg Loss: 0.0150 | Grad Norm: 0.00836263\n",
      "Epoch 1 | Step 437500 | Avg Loss: 0.0151 | Grad Norm: 0.00831257\n",
      "Epoch 1 | Step 437600 | Avg Loss: 0.0150 | Grad Norm: 0.00927618\n",
      "Epoch 1 | Step 437700 | Avg Loss: 0.0149 | Grad Norm: 0.00892954\n",
      "Epoch 1 | Step 437800 | Avg Loss: 0.0145 | Grad Norm: 0.00917461\n",
      "Epoch 1 | Step 437900 | Avg Loss: 0.0144 | Grad Norm: 0.00842795\n",
      "Epoch 1 | Step 438000 | Avg Loss: 0.0144 | Grad Norm: 0.00835391\n",
      "Epoch 1 | Step 438100 | Avg Loss: 0.0145 | Grad Norm: 0.00875450\n",
      "Epoch 1 | Step 438200 | Avg Loss: 0.0147 | Grad Norm: 0.00977688\n",
      "Epoch 1 | Step 438300 | Avg Loss: 0.0147 | Grad Norm: 0.00879031\n",
      "Epoch 1 | Step 438400 | Avg Loss: 0.0146 | Grad Norm: 0.00933425\n",
      "Epoch 1 | Step 438500 | Avg Loss: 0.0144 | Grad Norm: 0.00965621\n",
      "Epoch 1 | Step 438600 | Avg Loss: 0.0139 | Grad Norm: 0.00946551\n",
      "Epoch 1 | Step 438700 | Avg Loss: 0.0138 | Grad Norm: 0.00859457\n",
      "Epoch 1 | Step 438800 | Avg Loss: 0.0139 | Grad Norm: 0.01382770\n",
      "Epoch 1 | Step 438900 | Avg Loss: 0.0139 | Grad Norm: 0.00883599\n",
      "Epoch 1 | Step 439000 | Avg Loss: 0.0142 | Grad Norm: 0.00986331\n",
      "Epoch 1 | Step 439100 | Avg Loss: 0.0141 | Grad Norm: 0.00916517\n",
      "Epoch 1 | Step 439200 | Avg Loss: 0.0143 | Grad Norm: 0.00856474\n",
      "Epoch 1 | Step 439300 | Avg Loss: 0.0147 | Grad Norm: 0.00846978\n",
      "Epoch 1 | Step 439400 | Avg Loss: 0.0146 | Grad Norm: 0.00896946\n",
      "Epoch 1 | Step 439500 | Avg Loss: 0.0149 | Grad Norm: 0.01049217\n",
      "Epoch 1 | Step 439600 | Avg Loss: 0.0148 | Grad Norm: 0.00858396\n",
      "Epoch 1 | Step 439700 | Avg Loss: 0.0154 | Grad Norm: 0.00907616\n",
      "Epoch 1 | Step 439800 | Avg Loss: 0.0153 | Grad Norm: 0.00930778\n",
      "Epoch 1 | Step 439900 | Avg Loss: 0.0151 | Grad Norm: 0.00956562\n",
      "Epoch 1 | Step 440000 | Avg Loss: 0.0154 | Grad Norm: 0.00949299\n",
      "Epoch 1 | Step 440100 | Avg Loss: 0.0149 | Grad Norm: 0.00930602\n",
      "Epoch 1 | Step 440200 | Avg Loss: 0.0147 | Grad Norm: 0.00946778\n",
      "Epoch 1 | Step 440300 | Avg Loss: 0.0143 | Grad Norm: 0.00912524\n",
      "Epoch 1 | Step 440400 | Avg Loss: 0.0140 | Grad Norm: 0.00779586\n",
      "Epoch 1 | Step 440500 | Avg Loss: 0.0138 | Grad Norm: 0.00803083\n",
      "Epoch 1 | Step 440600 | Avg Loss: 0.0142 | Grad Norm: 0.00908040\n",
      "Epoch 1 | Step 440700 | Avg Loss: 0.0142 | Grad Norm: 0.00767445\n",
      "Epoch 1 | Step 440800 | Avg Loss: 0.0139 | Grad Norm: 0.00899063\n",
      "Epoch 1 | Step 440900 | Avg Loss: 0.0137 | Grad Norm: 0.00793371\n",
      "Epoch 1 | Step 441000 | Avg Loss: 0.0140 | Grad Norm: 0.00896128\n",
      "Epoch 1 | Step 441100 | Avg Loss: 0.0146 | Grad Norm: 0.00849220\n",
      "Epoch 1 | Step 441200 | Avg Loss: 0.0147 | Grad Norm: 0.00834938\n",
      "Epoch 1 | Step 441300 | Avg Loss: 0.0147 | Grad Norm: 0.00826290\n",
      "Epoch 1 | Step 441400 | Avg Loss: 0.0151 | Grad Norm: 0.00925247\n",
      "Epoch 1 | Step 441500 | Avg Loss: 0.0151 | Grad Norm: 0.00955953\n",
      "Epoch 1 | Step 441600 | Avg Loss: 0.0151 | Grad Norm: 0.00905406\n",
      "Epoch 1 | Step 441700 | Avg Loss: 0.0148 | Grad Norm: 0.00907534\n",
      "Epoch 1 | Step 441800 | Avg Loss: 0.0148 | Grad Norm: 0.00909503\n",
      "Epoch 1 | Step 441900 | Avg Loss: 0.0148 | Grad Norm: 0.00996564\n",
      "Epoch 1 | Step 442000 | Avg Loss: 0.0145 | Grad Norm: 0.00943832\n",
      "Epoch 1 | Step 442100 | Avg Loss: 0.0142 | Grad Norm: 0.00800740\n",
      "Epoch 1 | Step 442200 | Avg Loss: 0.0138 | Grad Norm: 0.00824705\n",
      "Epoch 1 | Step 442300 | Avg Loss: 0.0143 | Grad Norm: 0.00896924\n",
      "Epoch 1 | Step 442400 | Avg Loss: 0.0145 | Grad Norm: 0.00916001\n",
      "Epoch 1 | Step 442500 | Avg Loss: 0.0145 | Grad Norm: 0.00943001\n",
      "Epoch 1 | Step 442600 | Avg Loss: 0.0148 | Grad Norm: 0.00935374\n",
      "Epoch 1 | Step 442700 | Avg Loss: 0.0147 | Grad Norm: 0.00744309\n",
      "Epoch 1 | Step 442800 | Avg Loss: 0.0143 | Grad Norm: 0.00877585\n",
      "Epoch 1 | Step 442900 | Avg Loss: 0.0144 | Grad Norm: 0.01014662\n",
      "Epoch 1 | Step 443000 | Avg Loss: 0.0140 | Grad Norm: 0.01003416\n",
      "Epoch 1 | Step 443100 | Avg Loss: 0.0143 | Grad Norm: 0.00977052\n",
      "Epoch 1 | Step 443200 | Avg Loss: 0.0146 | Grad Norm: 0.01045030\n",
      "Epoch 1 | Step 443300 | Avg Loss: 0.0151 | Grad Norm: 0.00915420\n",
      "Epoch 1 | Step 443400 | Avg Loss: 0.0151 | Grad Norm: 0.01042193\n",
      "Epoch 1 | Step 443500 | Avg Loss: 0.0148 | Grad Norm: 0.01035454\n",
      "Epoch 1 | Step 443600 | Avg Loss: 0.0151 | Grad Norm: 0.00874758\n",
      "Epoch 1 | Step 443700 | Avg Loss: 0.0149 | Grad Norm: 0.00843207\n",
      "Epoch 1 | Step 443800 | Avg Loss: 0.0149 | Grad Norm: 0.00813066\n",
      "Epoch 1 | Step 443900 | Avg Loss: 0.0153 | Grad Norm: 0.00887116\n",
      "Epoch 1 | Step 444000 | Avg Loss: 0.0153 | Grad Norm: 0.00925406\n",
      "Epoch 1 | Step 444100 | Avg Loss: 0.0153 | Grad Norm: 0.00988261\n",
      "Epoch 1 | Step 444200 | Avg Loss: 0.0149 | Grad Norm: 0.01062002\n",
      "Epoch 1 | Step 444300 | Avg Loss: 0.0151 | Grad Norm: 0.00913618\n",
      "Epoch 1 | Step 444400 | Avg Loss: 0.0152 | Grad Norm: 0.00855575\n",
      "Epoch 1 | Step 444500 | Avg Loss: 0.0150 | Grad Norm: 0.00924472\n",
      "Epoch 1 | Step 444600 | Avg Loss: 0.0150 | Grad Norm: 0.00913938\n",
      "Epoch 1 | Step 444700 | Avg Loss: 0.0150 | Grad Norm: 0.00954763\n",
      "Epoch 1 | Step 444800 | Avg Loss: 0.0153 | Grad Norm: 0.00956859\n",
      "Epoch 1 | Step 444900 | Avg Loss: 0.0154 | Grad Norm: 0.01148467\n",
      "Epoch 1 | Step 445000 | Avg Loss: 0.0152 | Grad Norm: 0.00983317\n",
      "Epoch 1 | Step 445100 | Avg Loss: 0.0150 | Grad Norm: 0.01091786\n",
      "Epoch 1 | Step 445200 | Avg Loss: 0.0150 | Grad Norm: 0.00864832\n",
      "Epoch 1 | Step 445300 | Avg Loss: 0.0148 | Grad Norm: 0.01032192\n",
      "Epoch 1 | Step 445400 | Avg Loss: 0.0151 | Grad Norm: 0.00978342\n",
      "Epoch 1 | Step 445500 | Avg Loss: 0.0151 | Grad Norm: 0.01011200\n",
      "Epoch 1 | Step 445600 | Avg Loss: 0.0152 | Grad Norm: 0.00970865\n",
      "Epoch 1 | Step 445700 | Avg Loss: 0.0151 | Grad Norm: 0.01041452\n",
      "Epoch 1 | Step 445800 | Avg Loss: 0.0148 | Grad Norm: 0.00992848\n",
      "Epoch 1 | Step 445900 | Avg Loss: 0.0147 | Grad Norm: 0.00994781\n",
      "Epoch 1 | Step 446000 | Avg Loss: 0.0150 | Grad Norm: 0.00949035\n",
      "Epoch 1 | Step 446100 | Avg Loss: 0.0144 | Grad Norm: 0.00830183\n",
      "Epoch 1 | Step 446200 | Avg Loss: 0.0140 | Grad Norm: 0.00834524\n",
      "Epoch 1 | Step 446300 | Avg Loss: 0.0146 | Grad Norm: 0.00895747\n",
      "Epoch 1 | Step 446400 | Avg Loss: 0.0143 | Grad Norm: 0.00836261\n",
      "Epoch 1 | Step 446500 | Avg Loss: 0.0146 | Grad Norm: 0.01068780\n",
      "Epoch 1 | Step 446600 | Avg Loss: 0.0149 | Grad Norm: 0.00802676\n",
      "Epoch 1 | Step 446700 | Avg Loss: 0.0148 | Grad Norm: 0.00949391\n",
      "Epoch 1 | Step 446800 | Avg Loss: 0.0147 | Grad Norm: 0.00794094\n",
      "Epoch 1 | Step 446900 | Avg Loss: 0.0145 | Grad Norm: 0.00830694\n",
      "Epoch 1 | Step 447000 | Avg Loss: 0.0153 | Grad Norm: 0.00967044\n",
      "Epoch 1 | Step 447100 | Avg Loss: 0.0151 | Grad Norm: 0.00755704\n",
      "Epoch 1 | Step 447200 | Avg Loss: 0.0148 | Grad Norm: 0.00804359\n",
      "Epoch 1 | Step 447300 | Avg Loss: 0.0151 | Grad Norm: 0.00752225\n",
      "Epoch 1 | Step 447400 | Avg Loss: 0.0150 | Grad Norm: 0.00972391\n",
      "Epoch 1 | Step 447500 | Avg Loss: 0.0147 | Grad Norm: 0.01051205\n",
      "Epoch 1 | Step 447600 | Avg Loss: 0.0143 | Grad Norm: 0.00914414\n",
      "Epoch 1 | Step 447700 | Avg Loss: 0.0147 | Grad Norm: 0.00867617\n",
      "Epoch 1 | Step 447800 | Avg Loss: 0.0151 | Grad Norm: 0.00970212\n",
      "Epoch 1 | Step 447900 | Avg Loss: 0.0152 | Grad Norm: 0.01118352\n",
      "Epoch 1 | Step 448000 | Avg Loss: 0.0155 | Grad Norm: 0.00981056\n",
      "Epoch 1 | Step 448100 | Avg Loss: 0.0155 | Grad Norm: 0.00980036\n",
      "Epoch 1 | Step 448200 | Avg Loss: 0.0154 | Grad Norm: 0.00908184\n",
      "Epoch 1 | Step 448300 | Avg Loss: 0.0153 | Grad Norm: 0.01001185\n",
      "Epoch 1 | Step 448400 | Avg Loss: 0.0151 | Grad Norm: 0.00895734\n",
      "Epoch 1 | Step 448500 | Avg Loss: 0.0148 | Grad Norm: 0.00878758\n",
      "Epoch 1 | Step 448600 | Avg Loss: 0.0149 | Grad Norm: 0.00788245\n",
      "Epoch 1 | Step 448700 | Avg Loss: 0.0146 | Grad Norm: 0.00975952\n",
      "Epoch 1 | Step 448800 | Avg Loss: 0.0150 | Grad Norm: 0.01057509\n",
      "Epoch 1 | Step 448900 | Avg Loss: 0.0151 | Grad Norm: 0.01082770\n",
      "Epoch 1 | Step 449000 | Avg Loss: 0.0145 | Grad Norm: 0.01035807\n",
      "Epoch 1 | Step 449100 | Avg Loss: 0.0149 | Grad Norm: 0.00952043\n",
      "Epoch 1 | Step 449200 | Avg Loss: 0.0151 | Grad Norm: 0.00858163\n",
      "Epoch 1 | Step 449300 | Avg Loss: 0.0152 | Grad Norm: 0.00923750\n",
      "Epoch 1 | Step 449400 | Avg Loss: 0.0147 | Grad Norm: 0.00840173\n",
      "Epoch 1 | Step 449500 | Avg Loss: 0.0149 | Grad Norm: 0.00999623\n",
      "Epoch 1 | Step 449600 | Avg Loss: 0.0148 | Grad Norm: 0.01004651\n",
      "Epoch 1 | Step 449700 | Avg Loss: 0.0146 | Grad Norm: 0.00878734\n",
      "Epoch 1 | Step 449800 | Avg Loss: 0.0148 | Grad Norm: 0.00913750\n",
      "Epoch 1 | Step 449900 | Avg Loss: 0.0146 | Grad Norm: 0.00898537\n",
      "Epoch 1 | Step 450000 | Avg Loss: 0.0146 | Grad Norm: 0.01068852\n",
      "Epoch 1 | Step 450100 | Avg Loss: 0.0146 | Grad Norm: 0.00929085\n",
      "Epoch 1 | Step 450200 | Avg Loss: 0.0145 | Grad Norm: 0.01071089\n",
      "Epoch 1 | Step 450300 | Avg Loss: 0.0143 | Grad Norm: 0.00897485\n",
      "Epoch 1 | Step 450400 | Avg Loss: 0.0142 | Grad Norm: 0.00804228\n",
      "Epoch 1 | Step 450500 | Avg Loss: 0.0145 | Grad Norm: 0.00889988\n",
      "Epoch 1 | Step 450600 | Avg Loss: 0.0141 | Grad Norm: 0.00969434\n",
      "Epoch 1 | Step 450700 | Avg Loss: 0.0143 | Grad Norm: 0.00872458\n",
      "Epoch 1 | Step 450800 | Avg Loss: 0.0142 | Grad Norm: 0.01069448\n",
      "Epoch 1 | Step 450900 | Avg Loss: 0.0146 | Grad Norm: 0.00939345\n",
      "Epoch 1 | Step 451000 | Avg Loss: 0.0144 | Grad Norm: 0.00880138\n",
      "Epoch 1 | Step 451100 | Avg Loss: 0.0146 | Grad Norm: 0.00923443\n",
      "Epoch 1 | Step 451200 | Avg Loss: 0.0148 | Grad Norm: 0.00876925\n",
      "Epoch 1 | Step 451300 | Avg Loss: 0.0150 | Grad Norm: 0.00897914\n",
      "Epoch 1 | Step 451400 | Avg Loss: 0.0150 | Grad Norm: 0.00852941\n",
      "Epoch 1 | Step 451500 | Avg Loss: 0.0147 | Grad Norm: 0.01180795\n",
      "Epoch 1 | Step 451600 | Avg Loss: 0.0146 | Grad Norm: 0.01008441\n",
      "Epoch 1 | Step 451700 | Avg Loss: 0.0148 | Grad Norm: 0.00817534\n",
      "Epoch 1 | Step 451800 | Avg Loss: 0.0150 | Grad Norm: 0.01028189\n",
      "Epoch 1 | Step 451900 | Avg Loss: 0.0150 | Grad Norm: 0.00905194\n",
      "Epoch 1 | Step 452000 | Avg Loss: 0.0152 | Grad Norm: 0.01036658\n",
      "Epoch 1 | Step 452100 | Avg Loss: 0.0153 | Grad Norm: 0.00875444\n",
      "Epoch 1 | Step 452200 | Avg Loss: 0.0154 | Grad Norm: 0.01046939\n",
      "Epoch 1 | Step 452300 | Avg Loss: 0.0152 | Grad Norm: 0.01050380\n",
      "Epoch 1 | Step 452400 | Avg Loss: 0.0151 | Grad Norm: 0.00938064\n",
      "Epoch 1 | Step 452500 | Avg Loss: 0.0151 | Grad Norm: 0.01039095\n",
      "Epoch 1 | Step 452600 | Avg Loss: 0.0148 | Grad Norm: 0.01045807\n",
      "Epoch 1 | Step 452700 | Avg Loss: 0.0148 | Grad Norm: 0.00835324\n",
      "Epoch 1 | Step 452800 | Avg Loss: 0.0146 | Grad Norm: 0.00859682\n",
      "Epoch 1 | Step 452900 | Avg Loss: 0.0145 | Grad Norm: 0.00913026\n",
      "Epoch 1 | Step 453000 | Avg Loss: 0.0143 | Grad Norm: 0.00930122\n",
      "Epoch 1 | Step 453100 | Avg Loss: 0.0147 | Grad Norm: 0.00986981\n",
      "Epoch 1 | Step 453200 | Avg Loss: 0.0150 | Grad Norm: 0.00829126\n",
      "Epoch 1 | Step 453300 | Avg Loss: 0.0147 | Grad Norm: 0.01023731\n",
      "Epoch 1 | Step 453400 | Avg Loss: 0.0146 | Grad Norm: 0.00814498\n",
      "Epoch 1 | Step 453500 | Avg Loss: 0.0146 | Grad Norm: 0.00858512\n",
      "Epoch 1 | Step 453600 | Avg Loss: 0.0143 | Grad Norm: 0.00844246\n",
      "Epoch 1 | Step 453700 | Avg Loss: 0.0148 | Grad Norm: 0.00969385\n",
      "Epoch 1 | Step 453800 | Avg Loss: 0.0147 | Grad Norm: 0.00893190\n",
      "Epoch 1 | Step 453900 | Avg Loss: 0.0143 | Grad Norm: 0.00948898\n",
      "Epoch 1 | Step 454000 | Avg Loss: 0.0150 | Grad Norm: 0.01028919\n",
      "Epoch 1 | Step 454100 | Avg Loss: 0.0149 | Grad Norm: 0.00941529\n",
      "Epoch 1 | Step 454200 | Avg Loss: 0.0150 | Grad Norm: 0.00987271\n",
      "Epoch 1 | Step 454300 | Avg Loss: 0.0154 | Grad Norm: 0.00993413\n",
      "Epoch 1 | Step 454400 | Avg Loss: 0.0155 | Grad Norm: 0.01084150\n",
      "Epoch 1 | Step 454500 | Avg Loss: 0.0152 | Grad Norm: 0.01042424\n",
      "Epoch 1 | Step 454600 | Avg Loss: 0.0153 | Grad Norm: 0.01139587\n",
      "Epoch 1 | Step 454700 | Avg Loss: 0.0154 | Grad Norm: 0.01003777\n",
      "Epoch 1 | Step 454800 | Avg Loss: 0.0154 | Grad Norm: 0.00923956\n",
      "Epoch 1 | Step 454900 | Avg Loss: 0.0153 | Grad Norm: 0.01045618\n",
      "Epoch 1 | Step 455000 | Avg Loss: 0.0154 | Grad Norm: 0.00830800\n",
      "Epoch 1 | Step 455100 | Avg Loss: 0.0156 | Grad Norm: 0.01029945\n",
      "Epoch 1 | Step 455200 | Avg Loss: 0.0149 | Grad Norm: 0.01000982\n",
      "Epoch 1 | Step 455300 | Avg Loss: 0.0148 | Grad Norm: 0.00979407\n",
      "Epoch 1 | Step 455400 | Avg Loss: 0.0150 | Grad Norm: 0.00854102\n",
      "Epoch 1 | Step 455500 | Avg Loss: 0.0152 | Grad Norm: 0.00862339\n",
      "Epoch 1 | Step 455600 | Avg Loss: 0.0149 | Grad Norm: 0.01217450\n",
      "Epoch 1 | Step 455700 | Avg Loss: 0.0145 | Grad Norm: 0.00850856\n",
      "Epoch 1 | Step 455800 | Avg Loss: 0.0144 | Grad Norm: 0.01081111\n",
      "Epoch 1 | Step 455900 | Avg Loss: 0.0143 | Grad Norm: 0.00838311\n",
      "Epoch 1 | Step 456000 | Avg Loss: 0.0139 | Grad Norm: 0.00878687\n",
      "Epoch 1 | Step 456100 | Avg Loss: 0.0142 | Grad Norm: 0.01015753\n",
      "Epoch 1 | Step 456200 | Avg Loss: 0.0142 | Grad Norm: 0.00942275\n",
      "Epoch 1 | Step 456300 | Avg Loss: 0.0142 | Grad Norm: 0.00925990\n",
      "Epoch 1 | Step 456400 | Avg Loss: 0.0142 | Grad Norm: 0.00814291\n",
      "Epoch 1 | Step 456500 | Avg Loss: 0.0144 | Grad Norm: 0.00854581\n",
      "Epoch 1 | Step 456600 | Avg Loss: 0.0142 | Grad Norm: 0.00939004\n",
      "Epoch 1 | Step 456700 | Avg Loss: 0.0144 | Grad Norm: 0.00972669\n",
      "Epoch 1 | Step 456800 | Avg Loss: 0.0148 | Grad Norm: 0.00858724\n",
      "Epoch 1 | Step 456900 | Avg Loss: 0.0148 | Grad Norm: 0.00889320\n",
      "Epoch 1 | Step 457000 | Avg Loss: 0.0145 | Grad Norm: 0.00765369\n",
      "Epoch 1 | Step 457100 | Avg Loss: 0.0144 | Grad Norm: 0.00915230\n",
      "Epoch 1 | Step 457200 | Avg Loss: 0.0140 | Grad Norm: 0.00972992\n",
      "Epoch 1 | Step 457300 | Avg Loss: 0.0140 | Grad Norm: 0.00915394\n",
      "Epoch 1 | Step 457400 | Avg Loss: 0.0144 | Grad Norm: 0.00806144\n",
      "Epoch 1 | Step 457500 | Avg Loss: 0.0147 | Grad Norm: 0.00895029\n",
      "Epoch 1 | Step 457600 | Avg Loss: 0.0148 | Grad Norm: 0.00822801\n",
      "Epoch 1 | Step 457700 | Avg Loss: 0.0147 | Grad Norm: 0.00837172\n",
      "Epoch 1 | Step 457800 | Avg Loss: 0.0145 | Grad Norm: 0.00963225\n",
      "Epoch 1 | Step 457900 | Avg Loss: 0.0143 | Grad Norm: 0.00984365\n",
      "Epoch 1 | Step 458000 | Avg Loss: 0.0144 | Grad Norm: 0.00901172\n",
      "Epoch 1 | Step 458100 | Avg Loss: 0.0145 | Grad Norm: 0.00904674\n",
      "Epoch 1 | Step 458200 | Avg Loss: 0.0147 | Grad Norm: 0.00987421\n",
      "Epoch 1 | Step 458300 | Avg Loss: 0.0147 | Grad Norm: 0.00830990\n",
      "Epoch 1 | Step 458400 | Avg Loss: 0.0146 | Grad Norm: 0.00940437\n",
      "Epoch 1 | Step 458500 | Avg Loss: 0.0147 | Grad Norm: 0.00758348\n",
      "Epoch 1 | Step 458600 | Avg Loss: 0.0147 | Grad Norm: 0.00823493\n",
      "Epoch 1 | Step 458700 | Avg Loss: 0.0151 | Grad Norm: 0.00894203\n",
      "Epoch 1 | Step 458800 | Avg Loss: 0.0152 | Grad Norm: 0.01069037\n",
      "Epoch 1 | Step 458900 | Avg Loss: 0.0150 | Grad Norm: 0.00879586\n",
      "Epoch 1 | Step 459000 | Avg Loss: 0.0150 | Grad Norm: 0.00917422\n",
      "Epoch 1 | Step 459100 | Avg Loss: 0.0149 | Grad Norm: 0.00940821\n",
      "Epoch 1 | Step 459200 | Avg Loss: 0.0148 | Grad Norm: 0.00908852\n",
      "Epoch 1 | Step 459300 | Avg Loss: 0.0146 | Grad Norm: 0.00841044\n",
      "Epoch 1 | Step 459400 | Avg Loss: 0.0145 | Grad Norm: 0.00833525\n",
      "Epoch 1 | Step 459500 | Avg Loss: 0.0146 | Grad Norm: 0.00936352\n",
      "Epoch 1 | Step 459600 | Avg Loss: 0.0145 | Grad Norm: 0.00897403\n",
      "Epoch 1 | Step 459700 | Avg Loss: 0.0146 | Grad Norm: 0.00945180\n",
      "Epoch 1 | Step 459800 | Avg Loss: 0.0145 | Grad Norm: 0.00815269\n",
      "Epoch 1 | Step 459900 | Avg Loss: 0.0143 | Grad Norm: 0.00841239\n",
      "Epoch 1 | Step 460000 | Avg Loss: 0.0146 | Grad Norm: 0.00935170\n",
      "Epoch 1 | Step 460100 | Avg Loss: 0.0145 | Grad Norm: 0.01113959\n",
      "Epoch 1 | Step 460200 | Avg Loss: 0.0143 | Grad Norm: 0.00727091\n",
      "Epoch 1 | Step 460300 | Avg Loss: 0.0143 | Grad Norm: 0.00943524\n",
      "Epoch 1 | Step 460400 | Avg Loss: 0.0146 | Grad Norm: 0.00796956\n",
      "Epoch 1 | Step 460500 | Avg Loss: 0.0146 | Grad Norm: 0.00887935\n",
      "Epoch 1 | Step 460600 | Avg Loss: 0.0142 | Grad Norm: 0.00914814\n",
      "Epoch 1 | Step 460700 | Avg Loss: 0.0142 | Grad Norm: 0.00761144\n",
      "Epoch 1 | Step 460800 | Avg Loss: 0.0140 | Grad Norm: 0.00814320\n",
      "Epoch 1 | Step 460900 | Avg Loss: 0.0142 | Grad Norm: 0.00845651\n",
      "Epoch 1 | Step 461000 | Avg Loss: 0.0143 | Grad Norm: 0.00852776\n",
      "Epoch 1 | Step 461100 | Avg Loss: 0.0144 | Grad Norm: 0.00784711\n",
      "Epoch 1 | Step 461200 | Avg Loss: 0.0142 | Grad Norm: 0.00878049\n",
      "Epoch 1 | Step 461300 | Avg Loss: 0.0144 | Grad Norm: 0.00983731\n",
      "Epoch 1 | Step 461400 | Avg Loss: 0.0143 | Grad Norm: 0.01056096\n",
      "Epoch 1 | Step 461500 | Avg Loss: 0.0143 | Grad Norm: 0.01106892\n",
      "Epoch 1 | Step 461600 | Avg Loss: 0.0143 | Grad Norm: 0.00954371\n",
      "Epoch 1 | Step 461700 | Avg Loss: 0.0142 | Grad Norm: 0.00894239\n",
      "Epoch 1 | Step 461800 | Avg Loss: 0.0142 | Grad Norm: 0.00976793\n",
      "Epoch 1 | Step 461900 | Avg Loss: 0.0142 | Grad Norm: 0.00913203\n",
      "Epoch 1 | Step 462000 | Avg Loss: 0.0141 | Grad Norm: 0.00840861\n",
      "Epoch 1 | Step 462100 | Avg Loss: 0.0142 | Grad Norm: 0.00854329\n",
      "Epoch 1 | Step 462200 | Avg Loss: 0.0143 | Grad Norm: 0.00936969\n",
      "Epoch 1 | Step 462300 | Avg Loss: 0.0140 | Grad Norm: 0.01037412\n",
      "Epoch 1 | Step 462400 | Avg Loss: 0.0141 | Grad Norm: 0.00771264\n",
      "Epoch 1 | Step 462500 | Avg Loss: 0.0141 | Grad Norm: 0.00814406\n",
      "Epoch 1 | Step 462600 | Avg Loss: 0.0142 | Grad Norm: 0.00889987\n",
      "Epoch 1 | Step 462700 | Avg Loss: 0.0143 | Grad Norm: 0.00889277\n",
      "Epoch 1 | Step 462800 | Avg Loss: 0.0144 | Grad Norm: 0.00777069\n",
      "Epoch 1 | Step 462900 | Avg Loss: 0.0148 | Grad Norm: 0.00945584\n",
      "Epoch 1 | Step 463000 | Avg Loss: 0.0149 | Grad Norm: 0.00872529\n",
      "Epoch 1 | Step 463100 | Avg Loss: 0.0147 | Grad Norm: 0.01023045\n",
      "Epoch 1 | Step 463200 | Avg Loss: 0.0153 | Grad Norm: 0.00956282\n",
      "Epoch 1 | Step 463300 | Avg Loss: 0.0149 | Grad Norm: 0.00914358\n",
      "Epoch 1 | Step 463400 | Avg Loss: 0.0149 | Grad Norm: 0.00898477\n",
      "Epoch 1 | Step 463500 | Avg Loss: 0.0149 | Grad Norm: 0.00997353\n",
      "Epoch 1 | Step 463600 | Avg Loss: 0.0147 | Grad Norm: 0.00938286\n",
      "Epoch 1 | Step 463700 | Avg Loss: 0.0144 | Grad Norm: 0.01005991\n",
      "Epoch 1 | Step 463800 | Avg Loss: 0.0147 | Grad Norm: 0.00768136\n",
      "Epoch 1 | Step 463900 | Avg Loss: 0.0141 | Grad Norm: 0.00963239\n",
      "Epoch 1 | Step 464000 | Avg Loss: 0.0141 | Grad Norm: 0.00800716\n",
      "Epoch 1 | Step 464100 | Avg Loss: 0.0146 | Grad Norm: 0.00853981\n",
      "Epoch 1 | Step 464200 | Avg Loss: 0.0149 | Grad Norm: 0.00953827\n",
      "Epoch 1 | Step 464300 | Avg Loss: 0.0149 | Grad Norm: 0.00867267\n",
      "Epoch 1 | Step 464400 | Avg Loss: 0.0151 | Grad Norm: 0.00855246\n",
      "Epoch 1 | Step 464500 | Avg Loss: 0.0147 | Grad Norm: 0.00919440\n",
      "Epoch 1 | Step 464600 | Avg Loss: 0.0148 | Grad Norm: 0.00912858\n",
      "Epoch 1 | Step 464700 | Avg Loss: 0.0148 | Grad Norm: 0.00794511\n",
      "Epoch 1 | Step 464800 | Avg Loss: 0.0149 | Grad Norm: 0.00866490\n",
      "Epoch 1 | Step 464900 | Avg Loss: 0.0147 | Grad Norm: 0.00867141\n",
      "Epoch 1 | Step 465000 | Avg Loss: 0.0149 | Grad Norm: 0.00942900\n",
      "Epoch 1 | Step 465100 | Avg Loss: 0.0147 | Grad Norm: 0.00896696\n",
      "Epoch 1 | Step 465200 | Avg Loss: 0.0148 | Grad Norm: 0.00843506\n",
      "Epoch 1 | Step 465300 | Avg Loss: 0.0147 | Grad Norm: 0.00827196\n",
      "Epoch 1 | Step 465400 | Avg Loss: 0.0149 | Grad Norm: 0.00848732\n",
      "Epoch 1 | Step 465500 | Avg Loss: 0.0149 | Grad Norm: 0.00941217\n",
      "Epoch 1 | Step 465600 | Avg Loss: 0.0148 | Grad Norm: 0.00881062\n",
      "Epoch 1 | Step 465700 | Avg Loss: 0.0147 | Grad Norm: 0.00827790\n",
      "Epoch 1 | Step 465800 | Avg Loss: 0.0149 | Grad Norm: 0.00903595\n",
      "Epoch 1 | Step 465900 | Avg Loss: 0.0153 | Grad Norm: 0.00974166\n",
      "Epoch 1 | Step 466000 | Avg Loss: 0.0151 | Grad Norm: 0.00909262\n",
      "Epoch 1 | Step 466100 | Avg Loss: 0.0151 | Grad Norm: 0.00896030\n",
      "Epoch 1 | Step 466200 | Avg Loss: 0.0151 | Grad Norm: 0.00813109\n",
      "Epoch 1 | Step 466300 | Avg Loss: 0.0148 | Grad Norm: 0.01000317\n",
      "Epoch 1 | Step 466400 | Avg Loss: 0.0150 | Grad Norm: 0.00869185\n",
      "Epoch 1 | Step 466500 | Avg Loss: 0.0148 | Grad Norm: 0.00918840\n",
      "Epoch 1 | Step 466600 | Avg Loss: 0.0146 | Grad Norm: 0.00763777\n",
      "Epoch 1 | Step 466700 | Avg Loss: 0.0149 | Grad Norm: 0.00957005\n",
      "Epoch 1 | Step 466800 | Avg Loss: 0.0146 | Grad Norm: 0.00933779\n",
      "Epoch 1 | Step 466900 | Avg Loss: 0.0147 | Grad Norm: 0.00884887\n",
      "Epoch 1 | Step 467000 | Avg Loss: 0.0149 | Grad Norm: 0.00919916\n",
      "Epoch 1 | Step 467100 | Avg Loss: 0.0147 | Grad Norm: 0.00830711\n",
      "Epoch 1 | Step 467200 | Avg Loss: 0.0148 | Grad Norm: 0.00940940\n",
      "Epoch 1 | Step 467300 | Avg Loss: 0.0149 | Grad Norm: 0.00891636\n",
      "Epoch 1 | Step 467400 | Avg Loss: 0.0149 | Grad Norm: 0.01043181\n",
      "Epoch 1 | Step 467500 | Avg Loss: 0.0144 | Grad Norm: 0.01043134\n",
      "Epoch 1 | Step 467600 | Avg Loss: 0.0148 | Grad Norm: 0.00985667\n",
      "Epoch 1 | Step 467700 | Avg Loss: 0.0147 | Grad Norm: 0.00919695\n",
      "Epoch 1 | Step 467800 | Avg Loss: 0.0149 | Grad Norm: 0.00978696\n",
      "Epoch 1 | Step 467900 | Avg Loss: 0.0150 | Grad Norm: 0.00880523\n",
      "Epoch 1 | Step 468000 | Avg Loss: 0.0145 | Grad Norm: 0.00981749\n",
      "Epoch 1 | Step 468100 | Avg Loss: 0.0143 | Grad Norm: 0.00864685\n",
      "Epoch 1 | Step 468200 | Avg Loss: 0.0146 | Grad Norm: 0.00960141\n",
      "Epoch 1 | Step 468300 | Avg Loss: 0.0147 | Grad Norm: 0.00893781\n",
      "Epoch 1 | Step 468400 | Avg Loss: 0.0150 | Grad Norm: 0.00742017\n",
      "Epoch 1 | Step 468500 | Avg Loss: 0.0148 | Grad Norm: 0.00842524\n",
      "Epoch 1 | Step 468600 | Avg Loss: 0.0150 | Grad Norm: 0.01055977\n",
      "Epoch 1 | Step 468700 | Avg Loss: 0.0148 | Grad Norm: 0.00839575\n",
      "Epoch 1 | Step 468800 | Avg Loss: 0.0144 | Grad Norm: 0.01026678\n",
      "Epoch 1 | Step 468900 | Avg Loss: 0.0147 | Grad Norm: 0.00938903\n",
      "Epoch 1 | Step 469000 | Avg Loss: 0.0143 | Grad Norm: 0.00944943\n",
      "Epoch 1 | Step 469100 | Avg Loss: 0.0148 | Grad Norm: 0.01002276\n",
      "Epoch 1 | Step 469200 | Avg Loss: 0.0149 | Grad Norm: 0.00947643\n",
      "Epoch 1 | Step 469300 | Avg Loss: 0.0147 | Grad Norm: 0.01109763\n",
      "Epoch 1 | Step 469400 | Avg Loss: 0.0148 | Grad Norm: 0.00798793\n",
      "Epoch 1 | Step 469500 | Avg Loss: 0.0147 | Grad Norm: 0.00807728\n",
      "Epoch 1 | Step 469600 | Avg Loss: 0.0147 | Grad Norm: 0.00894032\n",
      "Epoch 1 | Step 469700 | Avg Loss: 0.0147 | Grad Norm: 0.01009394\n",
      "Epoch 1 | Step 469800 | Avg Loss: 0.0147 | Grad Norm: 0.00807402\n",
      "Epoch 1 | Step 469900 | Avg Loss: 0.0150 | Grad Norm: 0.00852164\n",
      "Epoch 1 | Step 470000 | Avg Loss: 0.0148 | Grad Norm: 0.00836442\n",
      "Epoch 1 | Step 470100 | Avg Loss: 0.0149 | Grad Norm: 0.00924162\n",
      "Epoch 1 | Step 470200 | Avg Loss: 0.0146 | Grad Norm: 0.00917714\n",
      "Epoch 1 | Step 470300 | Avg Loss: 0.0146 | Grad Norm: 0.00953951\n",
      "Epoch 1 | Step 470400 | Avg Loss: 0.0145 | Grad Norm: 0.00904800\n",
      "Epoch 1 | Step 470500 | Avg Loss: 0.0147 | Grad Norm: 0.00929021\n",
      "Epoch 1 | Step 470600 | Avg Loss: 0.0145 | Grad Norm: 0.00969127\n",
      "Epoch 1 | Step 470700 | Avg Loss: 0.0146 | Grad Norm: 0.00861684\n",
      "Epoch 1 | Step 470800 | Avg Loss: 0.0142 | Grad Norm: 0.00874720\n",
      "Epoch 1 | Step 470900 | Avg Loss: 0.0143 | Grad Norm: 0.00979660\n",
      "Epoch 1 | Step 471000 | Avg Loss: 0.0142 | Grad Norm: 0.00877878\n",
      "Epoch 1 | Step 471100 | Avg Loss: 0.0146 | Grad Norm: 0.00810678\n",
      "Epoch 1 | Step 471200 | Avg Loss: 0.0145 | Grad Norm: 0.00872673\n",
      "Epoch 1 | Step 471300 | Avg Loss: 0.0145 | Grad Norm: 0.00895079\n",
      "Epoch 1 | Step 471400 | Avg Loss: 0.0145 | Grad Norm: 0.00929757\n",
      "Epoch 1 | Step 471500 | Avg Loss: 0.0144 | Grad Norm: 0.00945648\n",
      "Epoch 1 | Step 471600 | Avg Loss: 0.0145 | Grad Norm: 0.00913107\n",
      "Epoch 1 | Step 471700 | Avg Loss: 0.0145 | Grad Norm: 0.00990425\n",
      "Epoch 1 | Step 471800 | Avg Loss: 0.0148 | Grad Norm: 0.00885672\n",
      "Epoch 1 | Step 471900 | Avg Loss: 0.0150 | Grad Norm: 0.00840836\n",
      "Epoch 1 | Step 472000 | Avg Loss: 0.0149 | Grad Norm: 0.00943709\n",
      "Epoch 1 | Step 472100 | Avg Loss: 0.0143 | Grad Norm: 0.00895192\n",
      "Epoch 1 | Step 472200 | Avg Loss: 0.0142 | Grad Norm: 0.00836535\n",
      "Epoch 1 | Step 472300 | Avg Loss: 0.0142 | Grad Norm: 0.00906654\n",
      "Epoch 1 | Step 472400 | Avg Loss: 0.0144 | Grad Norm: 0.00892262\n",
      "Epoch 1 | Step 472500 | Avg Loss: 0.0142 | Grad Norm: 0.00754145\n",
      "Epoch 1 | Step 472600 | Avg Loss: 0.0146 | Grad Norm: 0.01020464\n",
      "Epoch 1 | Step 472700 | Avg Loss: 0.0144 | Grad Norm: 0.00850007\n",
      "Epoch 1 | Step 472800 | Avg Loss: 0.0145 | Grad Norm: 0.00893141\n",
      "Epoch 1 | Step 472900 | Avg Loss: 0.0144 | Grad Norm: 0.00867385\n",
      "Epoch 1 | Step 473000 | Avg Loss: 0.0146 | Grad Norm: 0.00956539\n",
      "Epoch 1 | Step 473100 | Avg Loss: 0.0146 | Grad Norm: 0.00873293\n",
      "Epoch 1 | Step 473200 | Avg Loss: 0.0141 | Grad Norm: 0.00896178\n",
      "Epoch 1 | Step 473300 | Avg Loss: 0.0146 | Grad Norm: 0.00982614\n",
      "Epoch 1 | Step 473400 | Avg Loss: 0.0144 | Grad Norm: 0.00994564\n",
      "Epoch 1 | Step 473500 | Avg Loss: 0.0145 | Grad Norm: 0.00947226\n",
      "Epoch 1 | Step 473600 | Avg Loss: 0.0142 | Grad Norm: 0.00930937\n",
      "Epoch 1 | Step 473700 | Avg Loss: 0.0146 | Grad Norm: 0.00855717\n",
      "Epoch 1 | Step 473800 | Avg Loss: 0.0149 | Grad Norm: 0.00921086\n",
      "Epoch 1 | Step 473900 | Avg Loss: 0.0150 | Grad Norm: 0.00837941\n",
      "Epoch 1 | Step 474000 | Avg Loss: 0.0150 | Grad Norm: 0.00780753\n",
      "Epoch 1 | Step 474100 | Avg Loss: 0.0145 | Grad Norm: 0.01064439\n",
      "Epoch 1 | Step 474200 | Avg Loss: 0.0148 | Grad Norm: 0.01048465\n",
      "Epoch 1 | Step 474300 | Avg Loss: 0.0148 | Grad Norm: 0.01212449\n",
      "Epoch 1 | Step 474400 | Avg Loss: 0.0152 | Grad Norm: 0.00881264\n",
      "Epoch 1 | Step 474500 | Avg Loss: 0.0152 | Grad Norm: 0.00849941\n",
      "Epoch 1 | Step 474600 | Avg Loss: 0.0150 | Grad Norm: 0.00974149\n",
      "Epoch 1 | Step 474700 | Avg Loss: 0.0151 | Grad Norm: 0.00983478\n",
      "Epoch 1 | Step 474800 | Avg Loss: 0.0153 | Grad Norm: 0.01010429\n",
      "Epoch 1 | Step 474900 | Avg Loss: 0.0150 | Grad Norm: 0.01121373\n",
      "Epoch 1 | Step 475000 | Avg Loss: 0.0150 | Grad Norm: 0.01074705\n",
      "Epoch 1 | Step 475100 | Avg Loss: 0.0151 | Grad Norm: 0.00881483\n",
      "Epoch 1 | Step 475200 | Avg Loss: 0.0151 | Grad Norm: 0.00810073\n",
      "Epoch 1 | Step 475300 | Avg Loss: 0.0148 | Grad Norm: 0.00788826\n",
      "Epoch 1 | Step 475400 | Avg Loss: 0.0146 | Grad Norm: 0.00930589\n",
      "Epoch 1 | Step 475500 | Avg Loss: 0.0141 | Grad Norm: 0.00999763\n",
      "Epoch 1 | Step 475600 | Avg Loss: 0.0139 | Grad Norm: 0.00850995\n",
      "Epoch 1 | Step 475700 | Avg Loss: 0.0141 | Grad Norm: 0.00798096\n",
      "Epoch 1 | Step 475800 | Avg Loss: 0.0141 | Grad Norm: 0.00903619\n",
      "Epoch 1 | Step 475900 | Avg Loss: 0.0143 | Grad Norm: 0.00846504\n",
      "Epoch 1 | Step 476000 | Avg Loss: 0.0142 | Grad Norm: 0.00783082\n",
      "Epoch 1 | Step 476100 | Avg Loss: 0.0144 | Grad Norm: 0.00981978\n",
      "Epoch 1 | Step 476200 | Avg Loss: 0.0141 | Grad Norm: 0.00868381\n",
      "Epoch 1 | Step 476300 | Avg Loss: 0.0147 | Grad Norm: 0.00814993\n",
      "Epoch 1 | Step 476400 | Avg Loss: 0.0146 | Grad Norm: 0.00843907\n",
      "Epoch 1 | Step 476500 | Avg Loss: 0.0149 | Grad Norm: 0.00775725\n",
      "Epoch 1 | Step 476600 | Avg Loss: 0.0151 | Grad Norm: 0.01027904\n",
      "Epoch 1 | Step 476700 | Avg Loss: 0.0152 | Grad Norm: 0.00796618\n",
      "Epoch 1 | Step 476800 | Avg Loss: 0.0153 | Grad Norm: 0.00954332\n",
      "Epoch 1 | Step 476900 | Avg Loss: 0.0150 | Grad Norm: 0.01102891\n",
      "Epoch 1 | Step 477000 | Avg Loss: 0.0147 | Grad Norm: 0.00977114\n",
      "Epoch 1 | Step 477100 | Avg Loss: 0.0144 | Grad Norm: 0.01119545\n",
      "Epoch 1 | Step 477200 | Avg Loss: 0.0145 | Grad Norm: 0.00910327\n",
      "Epoch 1 | Step 477300 | Avg Loss: 0.0148 | Grad Norm: 0.00783771\n",
      "Epoch 1 | Step 477400 | Avg Loss: 0.0147 | Grad Norm: 0.00943912\n",
      "Epoch 1 | Step 477500 | Avg Loss: 0.0148 | Grad Norm: 0.00963046\n",
      "Epoch 1 | Step 477600 | Avg Loss: 0.0152 | Grad Norm: 0.00897850\n",
      "Epoch 1 | Step 477700 | Avg Loss: 0.0152 | Grad Norm: 0.00934993\n",
      "Epoch 1 | Step 477800 | Avg Loss: 0.0151 | Grad Norm: 0.00895866\n",
      "Epoch 1 | Step 477900 | Avg Loss: 0.0148 | Grad Norm: 0.00905707\n",
      "Epoch 1 | Step 478000 | Avg Loss: 0.0153 | Grad Norm: 0.00937049\n",
      "Epoch 1 | Step 478100 | Avg Loss: 0.0154 | Grad Norm: 0.00847551\n",
      "Epoch 1 | Step 478200 | Avg Loss: 0.0154 | Grad Norm: 0.01014953\n",
      "Epoch 1 | Step 478300 | Avg Loss: 0.0152 | Grad Norm: 0.00843444\n",
      "Epoch 1 | Step 478400 | Avg Loss: 0.0151 | Grad Norm: 0.00872647\n",
      "Epoch 1 | Step 478500 | Avg Loss: 0.0153 | Grad Norm: 0.00869710\n",
      "Epoch 1 | Step 478600 | Avg Loss: 0.0152 | Grad Norm: 0.00914192\n",
      "Epoch 1 | Step 478700 | Avg Loss: 0.0150 | Grad Norm: 0.00808251\n",
      "Epoch 1 | Step 478800 | Avg Loss: 0.0147 | Grad Norm: 0.00895340\n",
      "Epoch 1 | Step 478900 | Avg Loss: 0.0148 | Grad Norm: 0.00797306\n",
      "Epoch 1 | Step 479000 | Avg Loss: 0.0147 | Grad Norm: 0.00806688\n",
      "Epoch 1 | Step 479100 | Avg Loss: 0.0152 | Grad Norm: 0.00847761\n",
      "Epoch 1 | Step 479200 | Avg Loss: 0.0146 | Grad Norm: 0.00816029\n",
      "Epoch 1 | Step 479300 | Avg Loss: 0.0144 | Grad Norm: 0.00950616\n",
      "Epoch 1 | Step 479400 | Avg Loss: 0.0147 | Grad Norm: 0.01001692\n",
      "Epoch 1 | Step 479500 | Avg Loss: 0.0153 | Grad Norm: 0.00903192\n",
      "Epoch 1 | Step 479600 | Avg Loss: 0.0150 | Grad Norm: 0.00925823\n",
      "Epoch 1 | Step 479700 | Avg Loss: 0.0150 | Grad Norm: 0.01130038\n",
      "Epoch 1 | Step 479800 | Avg Loss: 0.0150 | Grad Norm: 0.00949342\n",
      "Epoch 1 | Step 479900 | Avg Loss: 0.0153 | Grad Norm: 0.00987572\n",
      "Epoch 1 | Step 480000 | Avg Loss: 0.0156 | Grad Norm: 0.00921240\n",
      "Epoch 1 | Step 480100 | Avg Loss: 0.0153 | Grad Norm: 0.01010976\n",
      "Epoch 1 | Step 480200 | Avg Loss: 0.0153 | Grad Norm: 0.00898833\n",
      "Epoch 1 | Step 480300 | Avg Loss: 0.0153 | Grad Norm: 0.00821226\n",
      "Epoch 1 | Step 480400 | Avg Loss: 0.0147 | Grad Norm: 0.00777789\n",
      "Epoch 1 | Step 480500 | Avg Loss: 0.0144 | Grad Norm: 0.00973277\n",
      "Epoch 1 | Step 480600 | Avg Loss: 0.0144 | Grad Norm: 0.00891089\n",
      "Epoch 1 | Step 480700 | Avg Loss: 0.0142 | Grad Norm: 0.00964127\n",
      "Epoch 1 | Step 480800 | Avg Loss: 0.0144 | Grad Norm: 0.00944952\n",
      "Epoch 1 | Step 480900 | Avg Loss: 0.0150 | Grad Norm: 0.00818732\n",
      "Epoch 1 | Step 481000 | Avg Loss: 0.0149 | Grad Norm: 0.01010736\n",
      "Epoch 1 | Step 481100 | Avg Loss: 0.0152 | Grad Norm: 0.00990339\n",
      "Epoch 1 | Step 481200 | Avg Loss: 0.0150 | Grad Norm: 0.00925954\n",
      "Epoch 1 | Step 481300 | Avg Loss: 0.0152 | Grad Norm: 0.00964951\n",
      "Epoch 1 | Step 481400 | Avg Loss: 0.0152 | Grad Norm: 0.00939691\n",
      "Epoch 1 | Step 481500 | Avg Loss: 0.0151 | Grad Norm: 0.00851785\n",
      "Epoch 1 | Step 481600 | Avg Loss: 0.0150 | Grad Norm: 0.00840978\n",
      "Epoch 1 | Step 481700 | Avg Loss: 0.0149 | Grad Norm: 0.00889250\n",
      "Epoch 1 | Step 481800 | Avg Loss: 0.0151 | Grad Norm: 0.00873047\n",
      "Epoch 1 | Step 481900 | Avg Loss: 0.0147 | Grad Norm: 0.01011016\n",
      "Epoch 1 | Step 482000 | Avg Loss: 0.0142 | Grad Norm: 0.00894057\n",
      "Epoch 1 | Step 482100 | Avg Loss: 0.0147 | Grad Norm: 0.00829631\n",
      "Epoch 1 | Step 482200 | Avg Loss: 0.0146 | Grad Norm: 0.00872537\n",
      "Epoch 1 | Step 482300 | Avg Loss: 0.0144 | Grad Norm: 0.00875276\n",
      "Epoch 1 | Step 482400 | Avg Loss: 0.0149 | Grad Norm: 0.00938728\n",
      "Epoch 1 | Step 482500 | Avg Loss: 0.0152 | Grad Norm: 0.00915576\n",
      "Epoch 1 | Step 482600 | Avg Loss: 0.0152 | Grad Norm: 0.01056824\n",
      "Epoch 1 | Step 482700 | Avg Loss: 0.0154 | Grad Norm: 0.01019343\n",
      "Epoch 1 | Step 482800 | Avg Loss: 0.0149 | Grad Norm: 0.00904147\n",
      "Epoch 1 | Step 482900 | Avg Loss: 0.0149 | Grad Norm: 0.00893304\n",
      "Epoch 1 | Step 483000 | Avg Loss: 0.0148 | Grad Norm: 0.00818206\n",
      "Epoch 1 | Step 483100 | Avg Loss: 0.0147 | Grad Norm: 0.00999043\n",
      "Epoch 1 | Step 483200 | Avg Loss: 0.0148 | Grad Norm: 0.00792024\n",
      "Epoch 1 | Step 483300 | Avg Loss: 0.0148 | Grad Norm: 0.01059537\n",
      "Epoch 1 | Step 483400 | Avg Loss: 0.0146 | Grad Norm: 0.00729233\n",
      "Epoch 1 | Step 483500 | Avg Loss: 0.0147 | Grad Norm: 0.00876233\n",
      "Epoch 1 | Step 483600 | Avg Loss: 0.0142 | Grad Norm: 0.00818918\n",
      "Epoch 1 | Step 483700 | Avg Loss: 0.0143 | Grad Norm: 0.01007374\n",
      "Epoch 1 | Step 483800 | Avg Loss: 0.0147 | Grad Norm: 0.00913696\n",
      "Epoch 1 | Step 483900 | Avg Loss: 0.0144 | Grad Norm: 0.00919031\n",
      "Epoch 1 | Step 484000 | Avg Loss: 0.0146 | Grad Norm: 0.00969999\n",
      "Epoch 1 | Step 484100 | Avg Loss: 0.0148 | Grad Norm: 0.00891363\n",
      "Epoch 1 | Step 484200 | Avg Loss: 0.0148 | Grad Norm: 0.00823438\n",
      "Epoch 1 | Step 484300 | Avg Loss: 0.0148 | Grad Norm: 0.01014333\n",
      "Epoch 1 | Step 484400 | Avg Loss: 0.0147 | Grad Norm: 0.00830345\n",
      "Epoch 1 | Step 484500 | Avg Loss: 0.0147 | Grad Norm: 0.00929366\n",
      "Epoch 1 | Step 484600 | Avg Loss: 0.0148 | Grad Norm: 0.00943686\n",
      "Epoch 1 | Step 484700 | Avg Loss: 0.0146 | Grad Norm: 0.00859350\n",
      "Epoch 1 | Step 484800 | Avg Loss: 0.0147 | Grad Norm: 0.00926104\n",
      "Epoch 1 | Step 484900 | Avg Loss: 0.0146 | Grad Norm: 0.00733001\n",
      "Epoch 1 | Step 485000 | Avg Loss: 0.0145 | Grad Norm: 0.00938509\n",
      "Epoch 1 | Step 485100 | Avg Loss: 0.0143 | Grad Norm: 0.00798669\n",
      "Epoch 1 | Step 485200 | Avg Loss: 0.0139 | Grad Norm: 0.00893129\n",
      "Epoch 1 | Step 485300 | Avg Loss: 0.0141 | Grad Norm: 0.00727431\n",
      "Epoch 1 | Step 485400 | Avg Loss: 0.0143 | Grad Norm: 0.00852356\n",
      "Epoch 1 | Step 485500 | Avg Loss: 0.0145 | Grad Norm: 0.00762703\n",
      "Epoch 1 | Step 485600 | Avg Loss: 0.0140 | Grad Norm: 0.00859868\n",
      "Epoch 1 | Step 485700 | Avg Loss: 0.0143 | Grad Norm: 0.00957132\n",
      "Epoch 1 | Step 485800 | Avg Loss: 0.0145 | Grad Norm: 0.00863360\n",
      "Epoch 1 | Step 485900 | Avg Loss: 0.0142 | Grad Norm: 0.00929388\n",
      "Epoch 1 | Step 486000 | Avg Loss: 0.0144 | Grad Norm: 0.00910678\n",
      "Epoch 1 | Step 486100 | Avg Loss: 0.0147 | Grad Norm: 0.00966299\n",
      "Epoch 1 | Step 486200 | Avg Loss: 0.0148 | Grad Norm: 0.00734957\n",
      "Epoch 1 | Step 486300 | Avg Loss: 0.0148 | Grad Norm: 0.00830818\n",
      "Epoch 1 | Step 486400 | Avg Loss: 0.0152 | Grad Norm: 0.00848284\n",
      "Epoch 1 | Step 486500 | Avg Loss: 0.0149 | Grad Norm: 0.00876944\n",
      "Epoch 1 | Step 486600 | Avg Loss: 0.0148 | Grad Norm: 0.00878177\n",
      "Epoch 1 | Step 486700 | Avg Loss: 0.0147 | Grad Norm: 0.00947389\n",
      "Epoch 1 | Step 486800 | Avg Loss: 0.0145 | Grad Norm: 0.00852596\n",
      "Epoch 1 | Step 486900 | Avg Loss: 0.0149 | Grad Norm: 0.00879523\n",
      "Epoch 1 | Step 487000 | Avg Loss: 0.0149 | Grad Norm: 0.01096249\n",
      "Epoch 1 | Step 487100 | Avg Loss: 0.0150 | Grad Norm: 0.01061470\n",
      "Epoch 1 | Step 487200 | Avg Loss: 0.0146 | Grad Norm: 0.00965713\n",
      "Epoch 1 | Step 487300 | Avg Loss: 0.0149 | Grad Norm: 0.00963052\n",
      "Epoch 1 | Step 487400 | Avg Loss: 0.0148 | Grad Norm: 0.00868459\n",
      "Epoch 1 | Step 487500 | Avg Loss: 0.0142 | Grad Norm: 0.00997734\n",
      "Epoch 1 | Step 487600 | Avg Loss: 0.0140 | Grad Norm: 0.00960237\n",
      "Epoch 1 | Step 487700 | Avg Loss: 0.0141 | Grad Norm: 0.00845780\n",
      "Epoch 1 | Step 487800 | Avg Loss: 0.0143 | Grad Norm: 0.00916705\n",
      "Epoch 1 | Step 487900 | Avg Loss: 0.0143 | Grad Norm: 0.00996409\n",
      "Epoch 1 | Step 488000 | Avg Loss: 0.0143 | Grad Norm: 0.01051963\n",
      "Epoch 1 | Step 488100 | Avg Loss: 0.0144 | Grad Norm: 0.00864863\n",
      "Epoch 1 | Step 488200 | Avg Loss: 0.0143 | Grad Norm: 0.00854632\n",
      "Epoch 1 | Step 488300 | Avg Loss: 0.0144 | Grad Norm: 0.00895978\n",
      "Epoch 1 | Step 488400 | Avg Loss: 0.0145 | Grad Norm: 0.00922906\n",
      "Epoch 1 | Step 488500 | Avg Loss: 0.0146 | Grad Norm: 0.00892924\n",
      "Epoch 1 | Step 488600 | Avg Loss: 0.0145 | Grad Norm: 0.00948380\n",
      "Epoch 1 | Step 488700 | Avg Loss: 0.0151 | Grad Norm: 0.00839236\n",
      "Epoch 1 | Step 488800 | Avg Loss: 0.0150 | Grad Norm: 0.00766315\n",
      "Epoch 1 | Step 488900 | Avg Loss: 0.0153 | Grad Norm: 0.00882714\n",
      "Epoch 1 | Step 489000 | Avg Loss: 0.0150 | Grad Norm: 0.00901073\n",
      "Epoch 1 | Step 489100 | Avg Loss: 0.0147 | Grad Norm: 0.00962683\n",
      "Epoch 1 | Step 489200 | Avg Loss: 0.0145 | Grad Norm: 0.00995583\n",
      "Epoch 1 | Step 489300 | Avg Loss: 0.0145 | Grad Norm: 0.00881060\n",
      "Epoch 1 | Step 489400 | Avg Loss: 0.0148 | Grad Norm: 0.00914372\n",
      "Epoch 1 | Step 489500 | Avg Loss: 0.0147 | Grad Norm: 0.00942395\n",
      "Epoch 1 | Step 489600 | Avg Loss: 0.0146 | Grad Norm: 0.00893868\n",
      "Epoch 1 | Step 489700 | Avg Loss: 0.0149 | Grad Norm: 0.00898853\n",
      "Epoch 1 | Step 489800 | Avg Loss: 0.0147 | Grad Norm: 0.01129459\n",
      "Epoch 1 | Step 489900 | Avg Loss: 0.0151 | Grad Norm: 0.01014911\n",
      "Epoch 1 | Step 490000 | Avg Loss: 0.0151 | Grad Norm: 0.00928294\n",
      "Epoch 1 | Step 490100 | Avg Loss: 0.0152 | Grad Norm: 0.01130713\n",
      "Epoch 1 | Step 490200 | Avg Loss: 0.0155 | Grad Norm: 0.01017887\n",
      "Epoch 1 | Step 490300 | Avg Loss: 0.0155 | Grad Norm: 0.01048618\n",
      "Epoch 1 | Step 490400 | Avg Loss: 0.0152 | Grad Norm: 0.00879502\n",
      "Epoch 1 | Step 490500 | Avg Loss: 0.0152 | Grad Norm: 0.00859513\n",
      "Epoch 1 | Step 490600 | Avg Loss: 0.0150 | Grad Norm: 0.01087763\n",
      "Epoch 1 | Step 490700 | Avg Loss: 0.0149 | Grad Norm: 0.00884914\n",
      "Epoch 1 | Step 490800 | Avg Loss: 0.0147 | Grad Norm: 0.00764752\n",
      "Epoch 1 | Step 490900 | Avg Loss: 0.0149 | Grad Norm: 0.01044707\n",
      "Epoch 1 | Step 491000 | Avg Loss: 0.0151 | Grad Norm: 0.00826849\n",
      "Epoch 1 | Step 491100 | Avg Loss: 0.0148 | Grad Norm: 0.00867904\n",
      "Epoch 1 | Step 491200 | Avg Loss: 0.0148 | Grad Norm: 0.01015122\n",
      "Epoch 1 | Step 491300 | Avg Loss: 0.0148 | Grad Norm: 0.01036872\n",
      "Epoch 1 | Step 491400 | Avg Loss: 0.0147 | Grad Norm: 0.00869186\n",
      "Epoch 1 | Step 491500 | Avg Loss: 0.0146 | Grad Norm: 0.00789542\n",
      "Epoch 1 | Step 491600 | Avg Loss: 0.0146 | Grad Norm: 0.01060212\n",
      "Epoch 1 | Step 491700 | Avg Loss: 0.0147 | Grad Norm: 0.01061255\n",
      "Epoch 1 | Step 491800 | Avg Loss: 0.0145 | Grad Norm: 0.00785424\n",
      "Epoch 1 | Step 491900 | Avg Loss: 0.0144 | Grad Norm: 0.00930600\n",
      "Epoch 1 | Step 492000 | Avg Loss: 0.0142 | Grad Norm: 0.00979823\n",
      "Epoch 1 | Step 492100 | Avg Loss: 0.0145 | Grad Norm: 0.00798017\n",
      "Epoch 1 | Step 492200 | Avg Loss: 0.0145 | Grad Norm: 0.00879339\n",
      "Epoch 1 | Step 492300 | Avg Loss: 0.0145 | Grad Norm: 0.00910664\n",
      "Epoch 1 | Step 492400 | Avg Loss: 0.0144 | Grad Norm: 0.01168462\n",
      "Epoch 1 | Step 492500 | Avg Loss: 0.0148 | Grad Norm: 0.00887643\n",
      "Epoch 1 | Step 492600 | Avg Loss: 0.0150 | Grad Norm: 0.00939826\n",
      "Epoch 1 | Step 492700 | Avg Loss: 0.0147 | Grad Norm: 0.00948683\n",
      "Epoch 1 | Step 492800 | Avg Loss: 0.0144 | Grad Norm: 0.01037259\n",
      "Epoch 1 | Step 492900 | Avg Loss: 0.0145 | Grad Norm: 0.00865521\n",
      "Epoch 1 | Step 493000 | Avg Loss: 0.0149 | Grad Norm: 0.00987904\n",
      "Epoch 1 | Step 493100 | Avg Loss: 0.0149 | Grad Norm: 0.01152012\n",
      "Epoch 1 | Step 493200 | Avg Loss: 0.0152 | Grad Norm: 0.00997778\n",
      "Epoch 1 | Step 493300 | Avg Loss: 0.0150 | Grad Norm: 0.00912380\n",
      "Epoch 1 | Step 493400 | Avg Loss: 0.0152 | Grad Norm: 0.01033777\n",
      "Epoch 1 | Step 493500 | Avg Loss: 0.0150 | Grad Norm: 0.00894911\n",
      "Epoch 1 | Step 493600 | Avg Loss: 0.0152 | Grad Norm: 0.00903924\n",
      "Epoch 1 | Step 493700 | Avg Loss: 0.0149 | Grad Norm: 0.00918540\n",
      "Epoch 1 | Step 493800 | Avg Loss: 0.0152 | Grad Norm: 0.00782226\n",
      "Epoch 1 | Step 493900 | Avg Loss: 0.0153 | Grad Norm: 0.00811633\n",
      "Epoch 1 | Step 494000 | Avg Loss: 0.0147 | Grad Norm: 0.00937539\n",
      "Epoch 1 | Step 494100 | Avg Loss: 0.0147 | Grad Norm: 0.01145588\n",
      "Epoch 1 | Step 494200 | Avg Loss: 0.0149 | Grad Norm: 0.00967185\n",
      "Epoch 1 | Step 494300 | Avg Loss: 0.0147 | Grad Norm: 0.00915880\n",
      "Epoch 1 | Step 494400 | Avg Loss: 0.0144 | Grad Norm: 0.01010264\n",
      "Epoch 1 | Step 494500 | Avg Loss: 0.0146 | Grad Norm: 0.00877167\n",
      "Epoch 1 | Step 494600 | Avg Loss: 0.0145 | Grad Norm: 0.00898034\n",
      "Epoch 1 | Step 494700 | Avg Loss: 0.0143 | Grad Norm: 0.00810047\n",
      "Epoch 1 | Step 494800 | Avg Loss: 0.0149 | Grad Norm: 0.00935639\n",
      "Epoch 1 | Step 494900 | Avg Loss: 0.0149 | Grad Norm: 0.00955925\n",
      "Epoch 1 | Step 495000 | Avg Loss: 0.0151 | Grad Norm: 0.00799854\n",
      "Epoch 1 | Step 495100 | Avg Loss: 0.0154 | Grad Norm: 0.00859105\n",
      "Epoch 1 | Step 495200 | Avg Loss: 0.0152 | Grad Norm: 0.00956082\n",
      "Epoch 1 | Step 495300 | Avg Loss: 0.0151 | Grad Norm: 0.00978704\n",
      "Epoch 1 | Step 495400 | Avg Loss: 0.0151 | Grad Norm: 0.00824713\n",
      "Epoch 1 | Step 495500 | Avg Loss: 0.0148 | Grad Norm: 0.00936371\n",
      "Epoch 1 | Step 495600 | Avg Loss: 0.0149 | Grad Norm: 0.01128347\n",
      "Epoch 1 | Step 495700 | Avg Loss: 0.0150 | Grad Norm: 0.00997406\n",
      "Epoch 1 | Step 495800 | Avg Loss: 0.0146 | Grad Norm: 0.00997529\n",
      "Epoch 1 | Step 495900 | Avg Loss: 0.0145 | Grad Norm: 0.00990377\n",
      "Epoch 1 | Step 496000 | Avg Loss: 0.0145 | Grad Norm: 0.00925875\n",
      "Epoch 1 | Step 496100 | Avg Loss: 0.0147 | Grad Norm: 0.00917815\n",
      "Epoch 1 | Step 496200 | Avg Loss: 0.0146 | Grad Norm: 0.01022486\n",
      "Epoch 1 | Step 496300 | Avg Loss: 0.0147 | Grad Norm: 0.01012353\n",
      "Epoch 1 | Step 496400 | Avg Loss: 0.0144 | Grad Norm: 0.00787458\n",
      "Epoch 1 | Step 496500 | Avg Loss: 0.0143 | Grad Norm: 0.00902083\n",
      "Epoch 1 | Step 496600 | Avg Loss: 0.0145 | Grad Norm: 0.00864398\n",
      "Epoch 1 | Step 496700 | Avg Loss: 0.0151 | Grad Norm: 0.00830653\n",
      "Epoch 1 | Step 496800 | Avg Loss: 0.0152 | Grad Norm: 0.01102927\n",
      "Epoch 1 | Step 496900 | Avg Loss: 0.0148 | Grad Norm: 0.00863038\n",
      "Epoch 1 | Step 497000 | Avg Loss: 0.0146 | Grad Norm: 0.00891473\n",
      "Epoch 1 | Step 497100 | Avg Loss: 0.0144 | Grad Norm: 0.00770462\n",
      "Epoch 1 | Step 497200 | Avg Loss: 0.0147 | Grad Norm: 0.00924176\n",
      "Epoch 1 | Step 497300 | Avg Loss: 0.0145 | Grad Norm: 0.00857868\n",
      "Epoch 1 | Step 497400 | Avg Loss: 0.0143 | Grad Norm: 0.00896778\n",
      "Epoch 1 | Step 497500 | Avg Loss: 0.0144 | Grad Norm: 0.00885351\n",
      "Epoch 1 | Step 497600 | Avg Loss: 0.0144 | Grad Norm: 0.00951959\n",
      "Epoch 1 | Step 497700 | Avg Loss: 0.0143 | Grad Norm: 0.01079651\n",
      "Epoch 1 | Step 497800 | Avg Loss: 0.0145 | Grad Norm: 0.00961955\n",
      "Epoch 1 | Step 497900 | Avg Loss: 0.0141 | Grad Norm: 0.00908128\n",
      "Epoch 1 | Step 498000 | Avg Loss: 0.0142 | Grad Norm: 0.00901429\n",
      "Epoch 1 | Step 498100 | Avg Loss: 0.0142 | Grad Norm: 0.00963892\n",
      "Epoch 1 | Step 498200 | Avg Loss: 0.0143 | Grad Norm: 0.00861728\n",
      "Epoch 1 | Step 498300 | Avg Loss: 0.0143 | Grad Norm: 0.01065218\n",
      "Epoch 1 | Step 498400 | Avg Loss: 0.0146 | Grad Norm: 0.00857791\n",
      "Epoch 1 | Step 498500 | Avg Loss: 0.0143 | Grad Norm: 0.00905003\n",
      "Epoch 1 | Step 498600 | Avg Loss: 0.0145 | Grad Norm: 0.00877712\n",
      "Epoch 1 | Step 498700 | Avg Loss: 0.0149 | Grad Norm: 0.00807433\n",
      "Epoch 1 | Step 498800 | Avg Loss: 0.0151 | Grad Norm: 0.00954891\n",
      "Epoch 1 | Step 498900 | Avg Loss: 0.0148 | Grad Norm: 0.00879290\n",
      "Epoch 1 | Step 499000 | Avg Loss: 0.0149 | Grad Norm: 0.00901870\n",
      "Epoch 1 | Step 499100 | Avg Loss: 0.0153 | Grad Norm: 0.00990936\n",
      "Epoch 1 | Step 499200 | Avg Loss: 0.0152 | Grad Norm: 0.00919046\n",
      "Epoch 1 | Step 499300 | Avg Loss: 0.0149 | Grad Norm: 0.00927665\n",
      "Epoch 1 | Step 499400 | Avg Loss: 0.0149 | Grad Norm: 0.00862137\n",
      "Epoch 1 | Step 499500 | Avg Loss: 0.0150 | Grad Norm: 0.00930931\n",
      "Epoch 1 | Step 499600 | Avg Loss: 0.0147 | Grad Norm: 0.00884684\n",
      "Epoch 1 | Step 499700 | Avg Loss: 0.0148 | Grad Norm: 0.00871630\n",
      "Epoch 1 | Step 499800 | Avg Loss: 0.0150 | Grad Norm: 0.00851838\n",
      "Epoch 1 | Step 499900 | Avg Loss: 0.0154 | Grad Norm: 0.00978493\n",
      "Epoch 1 | Step 500000 | Avg Loss: 0.0156 | Grad Norm: 0.00895722\n",
      "Saving model at step500000\n",
      "Epoch 1 | Step 500100 | Avg Loss: 0.0151 | Grad Norm: 0.00741621\n",
      "Epoch 1 | Step 500200 | Avg Loss: 0.0148 | Grad Norm: 0.00908791\n",
      "Epoch 1 | Step 500300 | Avg Loss: 0.0150 | Grad Norm: 0.00935616\n",
      "Epoch 1 | Step 500400 | Avg Loss: 0.0148 | Grad Norm: 0.00918208\n",
      "Epoch 1 | Step 500500 | Avg Loss: 0.0149 | Grad Norm: 0.00859541\n",
      "Epoch 1 | Step 500600 | Avg Loss: 0.0148 | Grad Norm: 0.00742126\n",
      "Epoch 1 | Step 500700 | Avg Loss: 0.0149 | Grad Norm: 0.00927440\n",
      "Epoch 1 | Step 500800 | Avg Loss: 0.0146 | Grad Norm: 0.00807587\n",
      "Epoch 1 | Step 500900 | Avg Loss: 0.0143 | Grad Norm: 0.00900327\n",
      "Epoch 1 | Step 501000 | Avg Loss: 0.0149 | Grad Norm: 0.00945508\n",
      "Epoch 1 | Step 501100 | Avg Loss: 0.0147 | Grad Norm: 0.00989794\n",
      "Epoch 1 | Step 501200 | Avg Loss: 0.0145 | Grad Norm: 0.00912094\n",
      "Epoch 1 | Step 501300 | Avg Loss: 0.0143 | Grad Norm: 0.00828168\n",
      "Epoch 1 | Step 501400 | Avg Loss: 0.0145 | Grad Norm: 0.00841953\n",
      "Epoch 1 | Step 501500 | Avg Loss: 0.0143 | Grad Norm: 0.00871364\n",
      "Epoch 1 | Step 501600 | Avg Loss: 0.0144 | Grad Norm: 0.00899274\n",
      "Epoch 1 | Step 501700 | Avg Loss: 0.0139 | Grad Norm: 0.00883047\n",
      "Epoch 1 | Step 501800 | Avg Loss: 0.0135 | Grad Norm: 0.00910892\n",
      "Epoch 1 | Step 501900 | Avg Loss: 0.0136 | Grad Norm: 0.01024498\n",
      "Epoch 1 | Step 502000 | Avg Loss: 0.0138 | Grad Norm: 0.00815457\n",
      "Epoch 1 | Step 502100 | Avg Loss: 0.0138 | Grad Norm: 0.00846258\n",
      "Epoch 1 | Step 502200 | Avg Loss: 0.0139 | Grad Norm: 0.00875031\n",
      "Epoch 1 | Step 502300 | Avg Loss: 0.0139 | Grad Norm: 0.01036739\n",
      "Epoch 1 | Step 502400 | Avg Loss: 0.0144 | Grad Norm: 0.00802418\n",
      "Epoch 1 | Step 502500 | Avg Loss: 0.0142 | Grad Norm: 0.00942541\n",
      "Epoch 1 | Step 502600 | Avg Loss: 0.0140 | Grad Norm: 0.00949035\n",
      "Epoch 1 | Step 502700 | Avg Loss: 0.0137 | Grad Norm: 0.00878894\n",
      "Epoch 1 | Step 502800 | Avg Loss: 0.0137 | Grad Norm: 0.00910250\n",
      "Epoch 1 | Step 502900 | Avg Loss: 0.0138 | Grad Norm: 0.00877274\n",
      "Epoch 1 | Step 503000 | Avg Loss: 0.0140 | Grad Norm: 0.00782633\n",
      "Epoch 1 | Step 503100 | Avg Loss: 0.0141 | Grad Norm: 0.01091405\n",
      "Epoch 1 | Step 503200 | Avg Loss: 0.0143 | Grad Norm: 0.00952160\n",
      "Epoch 1 | Step 503300 | Avg Loss: 0.0147 | Grad Norm: 0.00889655\n",
      "Epoch 1 | Step 503400 | Avg Loss: 0.0142 | Grad Norm: 0.00977808\n",
      "Epoch 1 | Step 503500 | Avg Loss: 0.0147 | Grad Norm: 0.00994881\n",
      "Epoch 1 | Step 503600 | Avg Loss: 0.0149 | Grad Norm: 0.00958603\n",
      "Epoch 1 | Step 503700 | Avg Loss: 0.0150 | Grad Norm: 0.00858871\n",
      "Epoch 1 | Step 503800 | Avg Loss: 0.0148 | Grad Norm: 0.00911674\n",
      "Epoch 1 | Step 503900 | Avg Loss: 0.0144 | Grad Norm: 0.00950397\n",
      "Epoch 1 | Step 504000 | Avg Loss: 0.0150 | Grad Norm: 0.00935518\n",
      "Epoch 1 | Step 504100 | Avg Loss: 0.0146 | Grad Norm: 0.00951709\n",
      "Epoch 1 | Step 504200 | Avg Loss: 0.0147 | Grad Norm: 0.00959682\n",
      "Epoch 1 | Step 504300 | Avg Loss: 0.0150 | Grad Norm: 0.00860731\n",
      "Epoch 1 | Step 504400 | Avg Loss: 0.0148 | Grad Norm: 0.01003005\n",
      "Epoch 1 | Step 504500 | Avg Loss: 0.0151 | Grad Norm: 0.00923860\n",
      "Epoch 1 | Step 504600 | Avg Loss: 0.0150 | Grad Norm: 0.00964139\n",
      "Epoch 1 | Step 504700 | Avg Loss: 0.0152 | Grad Norm: 0.00983822\n",
      "Epoch 1 | Step 504800 | Avg Loss: 0.0155 | Grad Norm: 0.00945390\n",
      "Epoch 1 | Step 504900 | Avg Loss: 0.0154 | Grad Norm: 0.00961995\n",
      "Epoch 1 | Step 505000 | Avg Loss: 0.0147 | Grad Norm: 0.00830404\n",
      "Epoch 1 | Step 505100 | Avg Loss: 0.0143 | Grad Norm: 0.00805094\n",
      "Epoch 1 | Step 505200 | Avg Loss: 0.0147 | Grad Norm: 0.00868269\n",
      "Epoch 1 | Step 505300 | Avg Loss: 0.0146 | Grad Norm: 0.00843655\n",
      "Epoch 1 | Step 505400 | Avg Loss: 0.0148 | Grad Norm: 0.00805889\n",
      "Epoch 1 | Step 505500 | Avg Loss: 0.0151 | Grad Norm: 0.00977858\n",
      "Epoch 1 | Step 505600 | Avg Loss: 0.0152 | Grad Norm: 0.00845251\n",
      "Epoch 1 | Step 505700 | Avg Loss: 0.0155 | Grad Norm: 0.00893216\n",
      "Epoch 1 | Step 505800 | Avg Loss: 0.0156 | Grad Norm: 0.00986121\n",
      "Epoch 1 | Step 505900 | Avg Loss: 0.0152 | Grad Norm: 0.00846991\n",
      "Epoch 1 | Step 506000 | Avg Loss: 0.0154 | Grad Norm: 0.00998323\n",
      "Epoch 1 | Step 506100 | Avg Loss: 0.0151 | Grad Norm: 0.00917106\n",
      "Epoch 1 | Step 506200 | Avg Loss: 0.0148 | Grad Norm: 0.00891694\n",
      "Epoch 1 | Step 506300 | Avg Loss: 0.0149 | Grad Norm: 0.00805052\n",
      "Epoch 1 | Step 506400 | Avg Loss: 0.0148 | Grad Norm: 0.01028477\n",
      "Epoch 1 | Step 506500 | Avg Loss: 0.0147 | Grad Norm: 0.00829493\n",
      "Epoch 1 | Step 506600 | Avg Loss: 0.0145 | Grad Norm: 0.00884007\n",
      "Epoch 1 | Step 506700 | Avg Loss: 0.0149 | Grad Norm: 0.00839032\n",
      "Epoch 1 | Step 506800 | Avg Loss: 0.0151 | Grad Norm: 0.00857880\n",
      "Epoch 1 | Step 506900 | Avg Loss: 0.0151 | Grad Norm: 0.01016671\n",
      "Epoch 1 | Step 507000 | Avg Loss: 0.0153 | Grad Norm: 0.00972999\n",
      "Epoch 1 | Step 507100 | Avg Loss: 0.0151 | Grad Norm: 0.00984624\n",
      "Epoch 1 | Step 507200 | Avg Loss: 0.0148 | Grad Norm: 0.01078575\n",
      "Epoch 1 | Step 507300 | Avg Loss: 0.0149 | Grad Norm: 0.00929368\n",
      "Epoch 1 | Step 507400 | Avg Loss: 0.0143 | Grad Norm: 0.00742358\n",
      "Epoch 1 | Step 507500 | Avg Loss: 0.0151 | Grad Norm: 0.01197847\n",
      "Epoch 1 | Step 507600 | Avg Loss: 0.0151 | Grad Norm: 0.00787109\n",
      "Epoch 1 | Step 507700 | Avg Loss: 0.0153 | Grad Norm: 0.00938560\n",
      "Epoch 1 | Step 507800 | Avg Loss: 0.0155 | Grad Norm: 0.01147417\n",
      "Epoch 1 | Step 507900 | Avg Loss: 0.0152 | Grad Norm: 0.00845643\n",
      "Epoch 1 | Step 508000 | Avg Loss: 0.0150 | Grad Norm: 0.00929690\n",
      "Epoch 1 | Step 508100 | Avg Loss: 0.0150 | Grad Norm: 0.01015632\n",
      "Epoch 1 | Step 508200 | Avg Loss: 0.0148 | Grad Norm: 0.00955050\n",
      "Epoch 1 | Step 508300 | Avg Loss: 0.0147 | Grad Norm: 0.00989625\n",
      "Epoch 1 | Step 508400 | Avg Loss: 0.0148 | Grad Norm: 0.01112980\n",
      "Epoch 1 | Step 508500 | Avg Loss: 0.0150 | Grad Norm: 0.01077137\n",
      "Epoch 1 | Step 508600 | Avg Loss: 0.0149 | Grad Norm: 0.00907478\n",
      "Epoch 1 | Step 508700 | Avg Loss: 0.0148 | Grad Norm: 0.01130132\n",
      "Epoch 1 | Step 508800 | Avg Loss: 0.0148 | Grad Norm: 0.00935731\n",
      "Epoch 1 | Step 508900 | Avg Loss: 0.0144 | Grad Norm: 0.00785080\n",
      "Epoch 1 | Step 509000 | Avg Loss: 0.0146 | Grad Norm: 0.00859498\n",
      "Epoch 1 | Step 509100 | Avg Loss: 0.0144 | Grad Norm: 0.00828238\n",
      "Epoch 1 | Step 509200 | Avg Loss: 0.0143 | Grad Norm: 0.00822028\n",
      "Epoch 1 | Step 509300 | Avg Loss: 0.0142 | Grad Norm: 0.00874365\n",
      "Epoch 1 | Step 509400 | Avg Loss: 0.0144 | Grad Norm: 0.00888243\n",
      "Epoch 1 | Step 509500 | Avg Loss: 0.0141 | Grad Norm: 0.00886353\n",
      "Epoch 1 | Step 509600 | Avg Loss: 0.0139 | Grad Norm: 0.00881650\n",
      "Epoch 1 | Step 509700 | Avg Loss: 0.0140 | Grad Norm: 0.00838522\n",
      "Epoch 1 | Step 509800 | Avg Loss: 0.0143 | Grad Norm: 0.00874946\n",
      "Epoch 1 | Step 509900 | Avg Loss: 0.0143 | Grad Norm: 0.00847263\n",
      "Epoch 1 | Step 510000 | Avg Loss: 0.0138 | Grad Norm: 0.00755856\n",
      "Epoch 1 | Step 510100 | Avg Loss: 0.0140 | Grad Norm: 0.00980439\n",
      "Epoch 1 | Step 510200 | Avg Loss: 0.0143 | Grad Norm: 0.01044668\n",
      "Epoch 1 | Step 510300 | Avg Loss: 0.0143 | Grad Norm: 0.01000855\n",
      "Epoch 1 | Step 510400 | Avg Loss: 0.0145 | Grad Norm: 0.00789057\n",
      "Epoch 1 | Step 510500 | Avg Loss: 0.0145 | Grad Norm: 0.00848505\n",
      "Epoch 1 | Step 510600 | Avg Loss: 0.0151 | Grad Norm: 0.00938668\n",
      "Epoch 1 | Step 510700 | Avg Loss: 0.0151 | Grad Norm: 0.00961795\n",
      "Epoch 1 | Step 510800 | Avg Loss: 0.0149 | Grad Norm: 0.00671430\n",
      "Epoch 1 | Step 510900 | Avg Loss: 0.0150 | Grad Norm: 0.00868444\n",
      "Epoch 1 | Step 511000 | Avg Loss: 0.0150 | Grad Norm: 0.00858157\n",
      "Epoch 1 | Step 511100 | Avg Loss: 0.0153 | Grad Norm: 0.00902430\n",
      "Epoch 1 | Step 511200 | Avg Loss: 0.0153 | Grad Norm: 0.00907727\n",
      "Epoch 1 | Step 511300 | Avg Loss: 0.0153 | Grad Norm: 0.00860880\n",
      "Epoch 1 | Step 511400 | Avg Loss: 0.0150 | Grad Norm: 0.00790358\n",
      "Epoch 1 | Step 511500 | Avg Loss: 0.0152 | Grad Norm: 0.00882901\n",
      "Epoch 1 | Step 511600 | Avg Loss: 0.0148 | Grad Norm: 0.00960281\n",
      "Epoch 1 | Step 511700 | Avg Loss: 0.0147 | Grad Norm: 0.00836634\n",
      "Epoch 1 | Step 511800 | Avg Loss: 0.0146 | Grad Norm: 0.00805138\n",
      "Epoch 1 | Step 511900 | Avg Loss: 0.0149 | Grad Norm: 0.00838092\n",
      "Epoch 1 | Step 512000 | Avg Loss: 0.0148 | Grad Norm: 0.00779115\n",
      "Epoch 1 | Step 512100 | Avg Loss: 0.0146 | Grad Norm: 0.00856887\n",
      "Epoch 1 | Step 512200 | Avg Loss: 0.0147 | Grad Norm: 0.00999509\n",
      "Epoch 1 | Step 512300 | Avg Loss: 0.0149 | Grad Norm: 0.00847603\n",
      "Epoch 1 | Step 512400 | Avg Loss: 0.0152 | Grad Norm: 0.00970758\n",
      "Epoch 1 | Step 512500 | Avg Loss: 0.0150 | Grad Norm: 0.00908979\n",
      "Epoch 1 | Step 512600 | Avg Loss: 0.0152 | Grad Norm: 0.00971023\n",
      "Epoch 1 | Step 512700 | Avg Loss: 0.0146 | Grad Norm: 0.00867901\n",
      "Epoch 1 | Step 512800 | Avg Loss: 0.0144 | Grad Norm: 0.00904323\n",
      "Epoch 1 | Step 512900 | Avg Loss: 0.0147 | Grad Norm: 0.00841478\n",
      "Epoch 1 | Step 513000 | Avg Loss: 0.0151 | Grad Norm: 0.00985815\n",
      "Epoch 1 | Step 513100 | Avg Loss: 0.0148 | Grad Norm: 0.01042129\n",
      "Epoch 1 | Step 513200 | Avg Loss: 0.0146 | Grad Norm: 0.00806586\n",
      "Epoch 1 | Step 513300 | Avg Loss: 0.0145 | Grad Norm: 0.00846273\n",
      "Epoch 1 | Step 513400 | Avg Loss: 0.0143 | Grad Norm: 0.00802095\n",
      "Epoch 1 | Step 513500 | Avg Loss: 0.0144 | Grad Norm: 0.01092913\n",
      "Epoch 1 | Step 513600 | Avg Loss: 0.0143 | Grad Norm: 0.00720916\n",
      "Epoch 1 | Step 513700 | Avg Loss: 0.0143 | Grad Norm: 0.00961128\n",
      "Epoch 1 | Step 513800 | Avg Loss: 0.0148 | Grad Norm: 0.00837662\n",
      "Epoch 1 | Step 513900 | Avg Loss: 0.0148 | Grad Norm: 0.00878031\n",
      "Epoch 1 | Step 514000 | Avg Loss: 0.0149 | Grad Norm: 0.00930235\n",
      "Epoch 1 | Step 514100 | Avg Loss: 0.0146 | Grad Norm: 0.00834786\n",
      "Epoch 1 | Step 514200 | Avg Loss: 0.0148 | Grad Norm: 0.00925988\n",
      "Epoch 1 | Step 514300 | Avg Loss: 0.0151 | Grad Norm: 0.00915971\n",
      "Epoch 1 | Step 514400 | Avg Loss: 0.0154 | Grad Norm: 0.00965905\n",
      "Epoch 1 | Step 514500 | Avg Loss: 0.0152 | Grad Norm: 0.01067374\n",
      "Epoch 1 | Step 514600 | Avg Loss: 0.0149 | Grad Norm: 0.00964008\n",
      "Epoch 1 | Step 514700 | Avg Loss: 0.0151 | Grad Norm: 0.00927694\n",
      "Epoch 1 | Step 514800 | Avg Loss: 0.0155 | Grad Norm: 0.00882171\n",
      "Epoch 1 | Step 514900 | Avg Loss: 0.0156 | Grad Norm: 0.00990302\n",
      "Epoch 1 | Step 515000 | Avg Loss: 0.0155 | Grad Norm: 0.00794434\n",
      "Epoch 1 | Step 515100 | Avg Loss: 0.0152 | Grad Norm: 0.00989596\n",
      "Epoch 1 | Step 515200 | Avg Loss: 0.0150 | Grad Norm: 0.00887698\n",
      "Epoch 1 | Step 515300 | Avg Loss: 0.0148 | Grad Norm: 0.00878323\n",
      "Epoch 1 | Step 515400 | Avg Loss: 0.0149 | Grad Norm: 0.00987607\n",
      "Epoch 1 | Step 515500 | Avg Loss: 0.0151 | Grad Norm: 0.01146064\n",
      "Epoch 1 | Step 515600 | Avg Loss: 0.0152 | Grad Norm: 0.00984347\n",
      "Epoch 1 | Step 515700 | Avg Loss: 0.0152 | Grad Norm: 0.01174413\n",
      "Epoch 1 | Step 515800 | Avg Loss: 0.0150 | Grad Norm: 0.00906692\n",
      "Epoch 1 | Step 515900 | Avg Loss: 0.0155 | Grad Norm: 0.01021165\n",
      "Epoch 1 | Step 516000 | Avg Loss: 0.0152 | Grad Norm: 0.00910209\n",
      "Epoch 1 | Step 516100 | Avg Loss: 0.0148 | Grad Norm: 0.00846655\n",
      "Epoch 1 | Step 516200 | Avg Loss: 0.0149 | Grad Norm: 0.01077508\n",
      "Epoch 1 | Step 516300 | Avg Loss: 0.0151 | Grad Norm: 0.01045537\n",
      "Epoch 1 | Step 516400 | Avg Loss: 0.0151 | Grad Norm: 0.00872619\n",
      "Epoch 1 | Step 516500 | Avg Loss: 0.0151 | Grad Norm: 0.00923254\n",
      "Epoch 1 | Step 516600 | Avg Loss: 0.0152 | Grad Norm: 0.00843915\n",
      "Epoch 1 | Step 516700 | Avg Loss: 0.0152 | Grad Norm: 0.01030420\n",
      "Epoch 1 | Step 516800 | Avg Loss: 0.0149 | Grad Norm: 0.00913306\n",
      "Epoch 1 | Step 516900 | Avg Loss: 0.0151 | Grad Norm: 0.00833869\n",
      "Epoch 1 | Step 517000 | Avg Loss: 0.0151 | Grad Norm: 0.00759665\n",
      "Epoch 1 | Step 517100 | Avg Loss: 0.0148 | Grad Norm: 0.00904785\n",
      "Epoch 1 | Step 517200 | Avg Loss: 0.0146 | Grad Norm: 0.00769753\n",
      "Epoch 1 | Step 517300 | Avg Loss: 0.0148 | Grad Norm: 0.01022133\n",
      "Epoch 1 | Step 517400 | Avg Loss: 0.0143 | Grad Norm: 0.00786079\n",
      "Epoch 1 | Step 517500 | Avg Loss: 0.0145 | Grad Norm: 0.00896143\n",
      "Epoch 1 | Step 517600 | Avg Loss: 0.0147 | Grad Norm: 0.00989857\n",
      "Epoch 1 | Step 517700 | Avg Loss: 0.0148 | Grad Norm: 0.01127818\n",
      "Epoch 1 | Step 517800 | Avg Loss: 0.0143 | Grad Norm: 0.00768430\n",
      "Epoch 1 | Step 517900 | Avg Loss: 0.0143 | Grad Norm: 0.00970878\n",
      "Epoch 1 | Step 518000 | Avg Loss: 0.0142 | Grad Norm: 0.00925326\n",
      "Epoch 1 | Step 518100 | Avg Loss: 0.0145 | Grad Norm: 0.00897826\n",
      "Epoch 1 | Step 518200 | Avg Loss: 0.0146 | Grad Norm: 0.00997792\n",
      "Epoch 1 | Step 518300 | Avg Loss: 0.0148 | Grad Norm: 0.00869918\n",
      "Epoch 1 | Step 518400 | Avg Loss: 0.0149 | Grad Norm: 0.01014376\n",
      "Epoch 1 | Step 518500 | Avg Loss: 0.0148 | Grad Norm: 0.00888379\n",
      "Epoch 1 | Step 518600 | Avg Loss: 0.0147 | Grad Norm: 0.00803369\n",
      "Epoch 1 | Step 518700 | Avg Loss: 0.0145 | Grad Norm: 0.01111957\n",
      "Epoch 1 | Step 518800 | Avg Loss: 0.0142 | Grad Norm: 0.01014378\n",
      "Epoch 1 | Step 518900 | Avg Loss: 0.0142 | Grad Norm: 0.00788028\n",
      "Epoch 1 | Step 519000 | Avg Loss: 0.0146 | Grad Norm: 0.00855868\n",
      "Epoch 1 | Step 519100 | Avg Loss: 0.0144 | Grad Norm: 0.00887031\n",
      "Epoch 1 | Step 519200 | Avg Loss: 0.0144 | Grad Norm: 0.00860017\n",
      "Epoch 1 | Step 519300 | Avg Loss: 0.0144 | Grad Norm: 0.00921507\n",
      "Epoch 1 | Step 519400 | Avg Loss: 0.0142 | Grad Norm: 0.00702083\n",
      "Epoch 1 | Step 519500 | Avg Loss: 0.0145 | Grad Norm: 0.00840770\n",
      "Epoch 1 | Step 519600 | Avg Loss: 0.0142 | Grad Norm: 0.00855250\n",
      "Epoch 1 | Step 519700 | Avg Loss: 0.0142 | Grad Norm: 0.00771234\n",
      "Epoch 1 | Step 519800 | Avg Loss: 0.0142 | Grad Norm: 0.00791449\n",
      "Epoch 1 | Step 519900 | Avg Loss: 0.0147 | Grad Norm: 0.00845028\n",
      "Epoch 1 | Step 520000 | Avg Loss: 0.0146 | Grad Norm: 0.00925897\n",
      "Epoch 1 | Step 520100 | Avg Loss: 0.0145 | Grad Norm: 0.00864445\n",
      "Epoch 1 | Step 520200 | Avg Loss: 0.0148 | Grad Norm: 0.01006490\n",
      "Epoch 1 | Step 520300 | Avg Loss: 0.0147 | Grad Norm: 0.00817458\n",
      "Epoch 1 | Step 520400 | Avg Loss: 0.0148 | Grad Norm: 0.01145279\n",
      "Epoch 1 | Step 520500 | Avg Loss: 0.0148 | Grad Norm: 0.00988210\n",
      "Epoch 1 | Step 520600 | Avg Loss: 0.0147 | Grad Norm: 0.00826630\n",
      "Epoch 1 | Step 520700 | Avg Loss: 0.0149 | Grad Norm: 0.00936468\n",
      "Epoch 1 | Step 520800 | Avg Loss: 0.0151 | Grad Norm: 0.01004829\n",
      "Epoch 1 | Step 520900 | Avg Loss: 0.0147 | Grad Norm: 0.00943070\n",
      "Epoch 1 | Step 521000 | Avg Loss: 0.0144 | Grad Norm: 0.00990027\n",
      "Epoch 1 | Step 521100 | Avg Loss: 0.0143 | Grad Norm: 0.00852811\n",
      "Epoch 1 | Step 521200 | Avg Loss: 0.0143 | Grad Norm: 0.00825522\n",
      "Epoch 1 | Step 521300 | Avg Loss: 0.0145 | Grad Norm: 0.01006979\n",
      "Epoch 1 | Step 521400 | Avg Loss: 0.0144 | Grad Norm: 0.00943116\n",
      "Epoch 1 | Step 521500 | Avg Loss: 0.0145 | Grad Norm: 0.00971743\n",
      "Epoch 1 | Step 521600 | Avg Loss: 0.0148 | Grad Norm: 0.00834263\n",
      "Epoch 1 | Step 521700 | Avg Loss: 0.0151 | Grad Norm: 0.00894543\n",
      "Epoch 1 | Step 521800 | Avg Loss: 0.0148 | Grad Norm: 0.00827757\n",
      "Epoch 1 | Step 521900 | Avg Loss: 0.0150 | Grad Norm: 0.00918198\n",
      "Epoch 1 | Step 522000 | Avg Loss: 0.0152 | Grad Norm: 0.00878353\n",
      "Epoch 1 | Step 522100 | Avg Loss: 0.0148 | Grad Norm: 0.00929494\n",
      "Epoch 1 | Step 522200 | Avg Loss: 0.0143 | Grad Norm: 0.00993603\n",
      "Epoch 1 | Step 522300 | Avg Loss: 0.0143 | Grad Norm: 0.00812028\n",
      "Epoch 1 | Step 522400 | Avg Loss: 0.0144 | Grad Norm: 0.01035878\n",
      "Epoch 1 | Step 522500 | Avg Loss: 0.0147 | Grad Norm: 0.01070950\n",
      "Epoch 1 | Step 522600 | Avg Loss: 0.0146 | Grad Norm: 0.00826882\n",
      "Epoch 1 | Step 522700 | Avg Loss: 0.0145 | Grad Norm: 0.00927843\n",
      "Epoch 1 | Step 522800 | Avg Loss: 0.0143 | Grad Norm: 0.01051511\n",
      "Epoch 1 | Step 522900 | Avg Loss: 0.0147 | Grad Norm: 0.00914131\n",
      "Epoch 1 | Step 523000 | Avg Loss: 0.0147 | Grad Norm: 0.00848035\n",
      "Epoch 1 | Step 523100 | Avg Loss: 0.0145 | Grad Norm: 0.00805340\n",
      "Epoch 1 | Step 523200 | Avg Loss: 0.0148 | Grad Norm: 0.00836159\n",
      "Epoch 1 | Step 523300 | Avg Loss: 0.0149 | Grad Norm: 0.00854905\n",
      "Epoch 1 | Step 523400 | Avg Loss: 0.0147 | Grad Norm: 0.00889636\n",
      "Epoch 1 | Step 523500 | Avg Loss: 0.0149 | Grad Norm: 0.01020445\n",
      "Epoch 1 | Step 523600 | Avg Loss: 0.0146 | Grad Norm: 0.00887817\n",
      "Epoch 1 | Step 523700 | Avg Loss: 0.0146 | Grad Norm: 0.00776867\n",
      "Epoch 1 | Step 523800 | Avg Loss: 0.0142 | Grad Norm: 0.00848700\n",
      "Epoch 1 | Step 523900 | Avg Loss: 0.0143 | Grad Norm: 0.01041297\n",
      "Epoch 1 | Step 524000 | Avg Loss: 0.0143 | Grad Norm: 0.00885871\n",
      "Epoch 1 | Step 524100 | Avg Loss: 0.0142 | Grad Norm: 0.00915010\n",
      "Epoch 1 | Step 524200 | Avg Loss: 0.0143 | Grad Norm: 0.00817696\n",
      "Epoch 1 | Step 524300 | Avg Loss: 0.0146 | Grad Norm: 0.01058252\n",
      "Epoch 1 | Step 524400 | Avg Loss: 0.0146 | Grad Norm: 0.00992180\n",
      "Epoch 1 | Step 524500 | Avg Loss: 0.0143 | Grad Norm: 0.00860138\n",
      "Epoch 1 | Step 524600 | Avg Loss: 0.0143 | Grad Norm: 0.00948067\n",
      "Epoch 1 | Step 524700 | Avg Loss: 0.0144 | Grad Norm: 0.00937271\n",
      "Epoch 1 | Step 524800 | Avg Loss: 0.0146 | Grad Norm: 0.00843443\n",
      "Epoch 1 | Step 524900 | Avg Loss: 0.0146 | Grad Norm: 0.00878947\n",
      "Epoch 1 | Step 525000 | Avg Loss: 0.0148 | Grad Norm: 0.00839040\n",
      "Epoch 1 | Step 525100 | Avg Loss: 0.0149 | Grad Norm: 0.00895657\n",
      "Epoch 1 | Step 525200 | Avg Loss: 0.0149 | Grad Norm: 0.00782017\n",
      "Epoch 1 | Step 525300 | Avg Loss: 0.0149 | Grad Norm: 0.00793928\n",
      "Epoch 1 | Step 525400 | Avg Loss: 0.0149 | Grad Norm: 0.00870234\n",
      "Epoch 1 | Step 525500 | Avg Loss: 0.0150 | Grad Norm: 0.00870390\n",
      "Epoch 1 | Step 525600 | Avg Loss: 0.0153 | Grad Norm: 0.00898574\n",
      "Epoch 1 | Step 525700 | Avg Loss: 0.0151 | Grad Norm: 0.00908060\n",
      "Epoch 1 | Step 525800 | Avg Loss: 0.0149 | Grad Norm: 0.00847525\n",
      "Epoch 1 | Step 525900 | Avg Loss: 0.0144 | Grad Norm: 0.00864197\n",
      "Epoch 1 | Step 526000 | Avg Loss: 0.0145 | Grad Norm: 0.00987566\n",
      "Epoch 1 | Step 526100 | Avg Loss: 0.0145 | Grad Norm: 0.00923720\n",
      "Epoch 1 | Step 526200 | Avg Loss: 0.0148 | Grad Norm: 0.00939803\n",
      "Epoch 1 | Step 526300 | Avg Loss: 0.0144 | Grad Norm: 0.00797499\n",
      "Epoch 1 | Step 526400 | Avg Loss: 0.0143 | Grad Norm: 0.01084121\n",
      "Epoch 1 | Step 526500 | Avg Loss: 0.0147 | Grad Norm: 0.00840958\n",
      "Epoch 1 | Step 526600 | Avg Loss: 0.0146 | Grad Norm: 0.00904055\n",
      "Epoch 1 | Step 526700 | Avg Loss: 0.0144 | Grad Norm: 0.01034958\n",
      "Epoch 1 | Step 526800 | Avg Loss: 0.0144 | Grad Norm: 0.00817715\n",
      "Epoch 1 | Step 526900 | Avg Loss: 0.0148 | Grad Norm: 0.00860367\n",
      "Epoch 1 | Step 527000 | Avg Loss: 0.0147 | Grad Norm: 0.00921682\n",
      "Epoch 1 | Step 527100 | Avg Loss: 0.0149 | Grad Norm: 0.00906623\n",
      "Epoch 1 | Step 527200 | Avg Loss: 0.0151 | Grad Norm: 0.00899577\n",
      "Epoch 1 | Step 527300 | Avg Loss: 0.0153 | Grad Norm: 0.00902185\n",
      "Epoch 1 | Step 527400 | Avg Loss: 0.0157 | Grad Norm: 0.00869798\n",
      "Epoch 1 | Step 527500 | Avg Loss: 0.0154 | Grad Norm: 0.00968080\n",
      "Epoch 1 | Step 527600 | Avg Loss: 0.0153 | Grad Norm: 0.00802823\n",
      "Epoch 1 | Step 527700 | Avg Loss: 0.0153 | Grad Norm: 0.00781833\n",
      "Epoch 1 | Step 527800 | Avg Loss: 0.0152 | Grad Norm: 0.00984200\n",
      "Epoch 1 | Step 527900 | Avg Loss: 0.0150 | Grad Norm: 0.00747133\n",
      "Epoch 1 | Step 528000 | Avg Loss: 0.0149 | Grad Norm: 0.00810662\n",
      "Epoch 1 | Step 528100 | Avg Loss: 0.0145 | Grad Norm: 0.00860455\n",
      "Epoch 1 | Step 528200 | Avg Loss: 0.0142 | Grad Norm: 0.00858260\n",
      "Epoch 1 | Step 528300 | Avg Loss: 0.0141 | Grad Norm: 0.00927292\n",
      "Epoch 1 | Step 528400 | Avg Loss: 0.0142 | Grad Norm: 0.00829718\n",
      "Epoch 1 | Step 528500 | Avg Loss: 0.0141 | Grad Norm: 0.00818335\n",
      "Epoch 1 | Step 528600 | Avg Loss: 0.0144 | Grad Norm: 0.00872621\n",
      "Epoch 1 | Step 528700 | Avg Loss: 0.0146 | Grad Norm: 0.00982888\n",
      "Epoch 1 | Step 528800 | Avg Loss: 0.0148 | Grad Norm: 0.00873578\n",
      "Epoch 1 | Step 528900 | Avg Loss: 0.0147 | Grad Norm: 0.01053288\n",
      "Epoch 1 | Step 529000 | Avg Loss: 0.0146 | Grad Norm: 0.01029434\n",
      "Epoch 1 | Step 529100 | Avg Loss: 0.0150 | Grad Norm: 0.00955371\n",
      "Epoch 1 | Step 529200 | Avg Loss: 0.0151 | Grad Norm: 0.00946685\n",
      "Epoch 1 | Step 529300 | Avg Loss: 0.0149 | Grad Norm: 0.01009256\n",
      "Epoch 1 | Step 529400 | Avg Loss: 0.0149 | Grad Norm: 0.00835564\n",
      "Epoch 1 | Step 529500 | Avg Loss: 0.0148 | Grad Norm: 0.00860203\n",
      "Epoch 1 | Step 529600 | Avg Loss: 0.0145 | Grad Norm: 0.00901239\n",
      "Epoch 1 | Step 529700 | Avg Loss: 0.0150 | Grad Norm: 0.00846742\n",
      "Epoch 1 | Step 529800 | Avg Loss: 0.0149 | Grad Norm: 0.00983277\n",
      "Epoch 1 | Step 529900 | Avg Loss: 0.0145 | Grad Norm: 0.00889960\n",
      "Epoch 1 | Step 530000 | Avg Loss: 0.0146 | Grad Norm: 0.00821609\n",
      "Epoch 1 | Step 530100 | Avg Loss: 0.0150 | Grad Norm: 0.00928777\n",
      "Epoch 1 | Step 530200 | Avg Loss: 0.0151 | Grad Norm: 0.00957917\n",
      "Epoch 1 | Step 530300 | Avg Loss: 0.0147 | Grad Norm: 0.00865356\n",
      "Epoch 1 | Step 530400 | Avg Loss: 0.0146 | Grad Norm: 0.00800886\n",
      "Epoch 1 | Step 530500 | Avg Loss: 0.0143 | Grad Norm: 0.00934100\n",
      "Epoch 1 | Step 530600 | Avg Loss: 0.0150 | Grad Norm: 0.00851172\n",
      "Epoch 1 | Step 530700 | Avg Loss: 0.0148 | Grad Norm: 0.00931112\n",
      "Epoch 1 | Step 530800 | Avg Loss: 0.0149 | Grad Norm: 0.01004765\n",
      "Epoch 1 | Step 530900 | Avg Loss: 0.0145 | Grad Norm: 0.01076996\n",
      "Epoch 1 | Step 531000 | Avg Loss: 0.0146 | Grad Norm: 0.00841195\n",
      "Epoch 1 | Step 531100 | Avg Loss: 0.0143 | Grad Norm: 0.00872651\n",
      "Epoch 1 | Step 531200 | Avg Loss: 0.0143 | Grad Norm: 0.00704579\n",
      "Epoch 1 | Step 531300 | Avg Loss: 0.0144 | Grad Norm: 0.00971568\n",
      "Epoch 1 | Step 531400 | Avg Loss: 0.0144 | Grad Norm: 0.00883790\n",
      "Epoch 1 | Step 531500 | Avg Loss: 0.0143 | Grad Norm: 0.00784942\n",
      "Epoch 1 | Step 531600 | Avg Loss: 0.0143 | Grad Norm: 0.00976450\n",
      "Epoch 1 | Step 531700 | Avg Loss: 0.0147 | Grad Norm: 0.00923054\n",
      "Epoch 1 | Step 531800 | Avg Loss: 0.0147 | Grad Norm: 0.00922804\n",
      "Epoch 1 | Step 531900 | Avg Loss: 0.0146 | Grad Norm: 0.00923431\n",
      "Epoch 1 | Step 532000 | Avg Loss: 0.0149 | Grad Norm: 0.00894577\n",
      "Epoch 1 | Step 532100 | Avg Loss: 0.0150 | Grad Norm: 0.00884672\n",
      "Epoch 1 | Step 532200 | Avg Loss: 0.0152 | Grad Norm: 0.00976741\n",
      "Epoch 1 | Step 532300 | Avg Loss: 0.0148 | Grad Norm: 0.00928207\n",
      "Epoch 1 | Step 532400 | Avg Loss: 0.0148 | Grad Norm: 0.00913623\n",
      "Epoch 1 | Step 532500 | Avg Loss: 0.0144 | Grad Norm: 0.00950143\n",
      "Epoch 1 | Step 532600 | Avg Loss: 0.0147 | Grad Norm: 0.00946433\n",
      "Epoch 1 | Step 532700 | Avg Loss: 0.0145 | Grad Norm: 0.00983784\n",
      "Epoch 1 | Step 532800 | Avg Loss: 0.0140 | Grad Norm: 0.00866695\n",
      "Epoch 1 | Step 532900 | Avg Loss: 0.0140 | Grad Norm: 0.00871029\n",
      "Epoch 1 | Step 533000 | Avg Loss: 0.0139 | Grad Norm: 0.00817775\n",
      "Epoch 1 | Step 533100 | Avg Loss: 0.0141 | Grad Norm: 0.00867402\n",
      "Epoch 1 | Step 533200 | Avg Loss: 0.0144 | Grad Norm: 0.00869679\n",
      "Epoch 1 | Step 533300 | Avg Loss: 0.0146 | Grad Norm: 0.00941259\n",
      "Epoch 1 | Step 533400 | Avg Loss: 0.0149 | Grad Norm: 0.00974281\n",
      "Epoch 1 | Step 533500 | Avg Loss: 0.0147 | Grad Norm: 0.00918368\n",
      "Epoch 1 | Step 533600 | Avg Loss: 0.0146 | Grad Norm: 0.00838478\n",
      "Epoch 1 | Step 533700 | Avg Loss: 0.0145 | Grad Norm: 0.00878631\n",
      "Epoch 1 | Step 533800 | Avg Loss: 0.0144 | Grad Norm: 0.00900697\n",
      "Epoch 1 | Step 533900 | Avg Loss: 0.0145 | Grad Norm: 0.00819357\n",
      "Epoch 1 | Step 534000 | Avg Loss: 0.0145 | Grad Norm: 0.01012396\n",
      "Epoch 1 | Step 534100 | Avg Loss: 0.0145 | Grad Norm: 0.01006018\n",
      "Epoch 1 | Step 534200 | Avg Loss: 0.0143 | Grad Norm: 0.00900043\n",
      "Epoch 1 | Step 534300 | Avg Loss: 0.0144 | Grad Norm: 0.00758872\n",
      "Epoch 1 | Step 534400 | Avg Loss: 0.0145 | Grad Norm: 0.00895988\n",
      "Epoch 1 | Step 534500 | Avg Loss: 0.0143 | Grad Norm: 0.00865292\n",
      "Epoch 1 | Step 534600 | Avg Loss: 0.0145 | Grad Norm: 0.00981591\n",
      "Epoch 1 | Step 534700 | Avg Loss: 0.0141 | Grad Norm: 0.00863087\n",
      "Epoch 1 | Step 534800 | Avg Loss: 0.0144 | Grad Norm: 0.01058728\n",
      "Epoch 1 | Step 534900 | Avg Loss: 0.0144 | Grad Norm: 0.00958287\n",
      "Epoch 1 | Step 535000 | Avg Loss: 0.0145 | Grad Norm: 0.01083692\n",
      "Epoch 1 | Step 535100 | Avg Loss: 0.0147 | Grad Norm: 0.00922655\n",
      "Epoch 1 | Step 535200 | Avg Loss: 0.0148 | Grad Norm: 0.01037691\n",
      "Epoch 1 | Step 535300 | Avg Loss: 0.0142 | Grad Norm: 0.00807264\n",
      "Epoch 1 | Step 535400 | Avg Loss: 0.0141 | Grad Norm: 0.00981215\n",
      "Epoch 1 | Step 535500 | Avg Loss: 0.0140 | Grad Norm: 0.00874060\n",
      "Epoch 1 | Step 535600 | Avg Loss: 0.0139 | Grad Norm: 0.00833646\n",
      "Epoch 1 | Step 535700 | Avg Loss: 0.0143 | Grad Norm: 0.00977378\n",
      "Epoch 1 | Step 535800 | Avg Loss: 0.0144 | Grad Norm: 0.00876705\n",
      "Epoch 1 | Step 535900 | Avg Loss: 0.0142 | Grad Norm: 0.01012635\n",
      "Epoch 1 | Step 536000 | Avg Loss: 0.0143 | Grad Norm: 0.00861744\n",
      "Epoch 1 | Step 536100 | Avg Loss: 0.0139 | Grad Norm: 0.00838230\n",
      "Epoch 1 | Step 536200 | Avg Loss: 0.0142 | Grad Norm: 0.00960694\n",
      "Epoch 1 | Step 536300 | Avg Loss: 0.0145 | Grad Norm: 0.00969001\n",
      "Epoch 1 | Step 536400 | Avg Loss: 0.0141 | Grad Norm: 0.01048246\n",
      "Epoch 1 | Step 536500 | Avg Loss: 0.0146 | Grad Norm: 0.01002282\n",
      "Epoch 1 | Step 536600 | Avg Loss: 0.0143 | Grad Norm: 0.00901642\n",
      "Epoch 1 | Step 536700 | Avg Loss: 0.0141 | Grad Norm: 0.00925171\n",
      "Epoch 1 | Step 536800 | Avg Loss: 0.0140 | Grad Norm: 0.00899725\n",
      "Epoch 1 | Step 536900 | Avg Loss: 0.0141 | Grad Norm: 0.00852435\n",
      "Epoch 1 | Step 537000 | Avg Loss: 0.0139 | Grad Norm: 0.00883765\n",
      "Epoch 1 | Step 537100 | Avg Loss: 0.0141 | Grad Norm: 0.00777227\n",
      "Epoch 1 | Step 537200 | Avg Loss: 0.0141 | Grad Norm: 0.00938788\n",
      "Epoch 1 | Step 537300 | Avg Loss: 0.0141 | Grad Norm: 0.00934171\n",
      "Epoch 1 | Step 537400 | Avg Loss: 0.0144 | Grad Norm: 0.00886072\n",
      "Epoch 1 | Step 537500 | Avg Loss: 0.0145 | Grad Norm: 0.00893370\n",
      "Epoch 1 | Step 537600 | Avg Loss: 0.0146 | Grad Norm: 0.00880686\n",
      "Epoch 1 | Step 537700 | Avg Loss: 0.0148 | Grad Norm: 0.00891557\n",
      "Epoch 1 | Step 537800 | Avg Loss: 0.0146 | Grad Norm: 0.00894518\n",
      "Epoch 1 | Step 537900 | Avg Loss: 0.0147 | Grad Norm: 0.00945459\n",
      "Epoch 1 | Step 538000 | Avg Loss: 0.0150 | Grad Norm: 0.00772648\n",
      "Epoch 1 | Step 538100 | Avg Loss: 0.0151 | Grad Norm: 0.00966385\n",
      "Epoch 1 | Step 538200 | Avg Loss: 0.0149 | Grad Norm: 0.00928795\n",
      "Epoch 1 | Step 538300 | Avg Loss: 0.0150 | Grad Norm: 0.00889537\n",
      "Epoch 1 | Step 538400 | Avg Loss: 0.0149 | Grad Norm: 0.00878172\n",
      "Epoch 1 | Step 538500 | Avg Loss: 0.0152 | Grad Norm: 0.00963034\n",
      "Epoch 1 | Step 538600 | Avg Loss: 0.0153 | Grad Norm: 0.00830767\n",
      "Epoch 1 | Step 538700 | Avg Loss: 0.0154 | Grad Norm: 0.00854427\n",
      "Epoch 1 | Step 538800 | Avg Loss: 0.0153 | Grad Norm: 0.00912518\n",
      "Epoch 1 | Step 538900 | Avg Loss: 0.0152 | Grad Norm: 0.00772364\n",
      "Epoch 1 | Step 539000 | Avg Loss: 0.0153 | Grad Norm: 0.00920993\n",
      "Epoch 1 | Step 539100 | Avg Loss: 0.0156 | Grad Norm: 0.00972623\n",
      "Epoch 1 | Step 539200 | Avg Loss: 0.0157 | Grad Norm: 0.00976792\n",
      "Epoch 1 | Step 539300 | Avg Loss: 0.0152 | Grad Norm: 0.01036694\n",
      "Epoch 1 | Step 539400 | Avg Loss: 0.0151 | Grad Norm: 0.01147182\n",
      "Epoch 1 | Step 539500 | Avg Loss: 0.0150 | Grad Norm: 0.01007111\n",
      "Epoch 1 | Step 539600 | Avg Loss: 0.0150 | Grad Norm: 0.01046027\n",
      "Epoch 1 | Step 539700 | Avg Loss: 0.0146 | Grad Norm: 0.00878212\n",
      "Epoch 1 | Step 539800 | Avg Loss: 0.0146 | Grad Norm: 0.00836929\n",
      "Epoch 1 | Step 539900 | Avg Loss: 0.0146 | Grad Norm: 0.00778110\n",
      "Epoch 1 | Step 540000 | Avg Loss: 0.0151 | Grad Norm: 0.00899081\n",
      "Epoch 1 | Step 540100 | Avg Loss: 0.0151 | Grad Norm: 0.00805895\n",
      "Epoch 1 | Step 540200 | Avg Loss: 0.0152 | Grad Norm: 0.00949053\n",
      "Epoch 1 | Step 540300 | Avg Loss: 0.0152 | Grad Norm: 0.00858190\n",
      "Epoch 1 | Step 540400 | Avg Loss: 0.0153 | Grad Norm: 0.00963842\n",
      "Epoch 1 | Step 540500 | Avg Loss: 0.0151 | Grad Norm: 0.00823774\n",
      "Epoch 1 | Step 540600 | Avg Loss: 0.0153 | Grad Norm: 0.00910416\n",
      "Epoch 1 | Step 540700 | Avg Loss: 0.0154 | Grad Norm: 0.00966050\n",
      "Epoch 1 | Step 540800 | Avg Loss: 0.0155 | Grad Norm: 0.00898864\n",
      "Epoch 1 | Step 540900 | Avg Loss: 0.0155 | Grad Norm: 0.01224271\n",
      "Epoch 1 | Step 541000 | Avg Loss: 0.0153 | Grad Norm: 0.00821347\n",
      "Epoch 1 | Step 541100 | Avg Loss: 0.0149 | Grad Norm: 0.00981583\n",
      "Epoch 1 | Step 541200 | Avg Loss: 0.0152 | Grad Norm: 0.01052261\n",
      "Epoch 1 | Step 541300 | Avg Loss: 0.0154 | Grad Norm: 0.00924696\n",
      "Epoch 1 | Step 541400 | Avg Loss: 0.0151 | Grad Norm: 0.00865308\n",
      "Epoch 1 | Step 541500 | Avg Loss: 0.0147 | Grad Norm: 0.01084488\n",
      "Epoch 1 | Step 541600 | Avg Loss: 0.0149 | Grad Norm: 0.01090564\n",
      "Epoch 1 | Step 541700 | Avg Loss: 0.0149 | Grad Norm: 0.00830915\n",
      "Epoch 1 | Step 541800 | Avg Loss: 0.0146 | Grad Norm: 0.00933138\n",
      "Epoch 1 | Step 541900 | Avg Loss: 0.0145 | Grad Norm: 0.00986204\n",
      "Epoch 1 | Step 542000 | Avg Loss: 0.0149 | Grad Norm: 0.00923432\n",
      "Epoch 1 | Step 542100 | Avg Loss: 0.0147 | Grad Norm: 0.01026673\n",
      "Epoch 1 | Step 542200 | Avg Loss: 0.0148 | Grad Norm: 0.00709892\n",
      "Epoch 1 | Step 542300 | Avg Loss: 0.0148 | Grad Norm: 0.01121252\n",
      "Epoch 1 | Step 542400 | Avg Loss: 0.0150 | Grad Norm: 0.01056445\n",
      "Epoch 1 | Step 542500 | Avg Loss: 0.0148 | Grad Norm: 0.00866622\n",
      "Epoch 1 | Step 542600 | Avg Loss: 0.0147 | Grad Norm: 0.00877876\n",
      "Epoch 1 | Step 542700 | Avg Loss: 0.0150 | Grad Norm: 0.00884761\n",
      "Epoch 1 | Step 542800 | Avg Loss: 0.0149 | Grad Norm: 0.00919748\n",
      "Epoch 1 | Step 542900 | Avg Loss: 0.0151 | Grad Norm: 0.00824136\n",
      "Epoch 1 | Step 543000 | Avg Loss: 0.0149 | Grad Norm: 0.00947847\n",
      "Epoch 1 | Step 543100 | Avg Loss: 0.0151 | Grad Norm: 0.00893506\n",
      "Epoch 1 | Step 543200 | Avg Loss: 0.0152 | Grad Norm: 0.00844597\n",
      "Epoch 1 | Step 543300 | Avg Loss: 0.0155 | Grad Norm: 0.01018461\n",
      "Epoch 1 | Step 543400 | Avg Loss: 0.0151 | Grad Norm: 0.00928631\n",
      "Epoch 1 | Step 543500 | Avg Loss: 0.0151 | Grad Norm: 0.00989424\n",
      "Epoch 1 | Step 543600 | Avg Loss: 0.0148 | Grad Norm: 0.00941854\n",
      "Epoch 1 | Step 543700 | Avg Loss: 0.0148 | Grad Norm: 0.00929585\n",
      "Epoch 1 | Step 543800 | Avg Loss: 0.0146 | Grad Norm: 0.00970981\n",
      "Epoch 1 | Step 543900 | Avg Loss: 0.0147 | Grad Norm: 0.00892127\n",
      "Epoch 1 | Step 544000 | Avg Loss: 0.0148 | Grad Norm: 0.00857879\n",
      "Epoch 1 | Step 544100 | Avg Loss: 0.0148 | Grad Norm: 0.01061407\n",
      "Epoch 1 | Step 544200 | Avg Loss: 0.0147 | Grad Norm: 0.01076460\n",
      "Epoch 1 | Step 544300 | Avg Loss: 0.0150 | Grad Norm: 0.00975148\n",
      "Epoch 1 | Step 544400 | Avg Loss: 0.0151 | Grad Norm: 0.00961427\n",
      "Epoch 1 | Step 544500 | Avg Loss: 0.0149 | Grad Norm: 0.00995190\n",
      "Epoch 1 | Step 544600 | Avg Loss: 0.0149 | Grad Norm: 0.00986356\n",
      "Epoch 1 | Step 544700 | Avg Loss: 0.0147 | Grad Norm: 0.00766850\n",
      "Epoch 1 | Step 544800 | Avg Loss: 0.0147 | Grad Norm: 0.00930183\n",
      "Epoch 1 | Step 544900 | Avg Loss: 0.0146 | Grad Norm: 0.00906500\n",
      "Epoch 1 | Step 545000 | Avg Loss: 0.0147 | Grad Norm: 0.00832249\n",
      "Epoch 1 | Step 545100 | Avg Loss: 0.0148 | Grad Norm: 0.01018921\n",
      "Epoch 1 | Step 545200 | Avg Loss: 0.0151 | Grad Norm: 0.00866988\n",
      "Epoch 1 | Step 545300 | Avg Loss: 0.0152 | Grad Norm: 0.00869854\n",
      "Epoch 1 | Step 545400 | Avg Loss: 0.0150 | Grad Norm: 0.00874695\n",
      "Epoch 1 | Step 545500 | Avg Loss: 0.0146 | Grad Norm: 0.00953550\n",
      "Epoch 1 | Step 545600 | Avg Loss: 0.0144 | Grad Norm: 0.00964807\n",
      "Epoch 1 | Step 545700 | Avg Loss: 0.0143 | Grad Norm: 0.00944133\n",
      "Epoch 1 | Step 545800 | Avg Loss: 0.0141 | Grad Norm: 0.00785592\n",
      "Epoch 1 | Step 545900 | Avg Loss: 0.0141 | Grad Norm: 0.00790255\n",
      "Epoch 1 | Step 546000 | Avg Loss: 0.0145 | Grad Norm: 0.00813783\n",
      "Epoch 1 | Step 546100 | Avg Loss: 0.0144 | Grad Norm: 0.00848990\n",
      "Epoch 1 | Step 546200 | Avg Loss: 0.0145 | Grad Norm: 0.00893873\n",
      "Epoch 1 | Step 546300 | Avg Loss: 0.0146 | Grad Norm: 0.00866361\n",
      "Epoch 1 | Step 546400 | Avg Loss: 0.0144 | Grad Norm: 0.00985990\n",
      "Epoch 1 | Step 546500 | Avg Loss: 0.0141 | Grad Norm: 0.01046653\n",
      "Epoch 1 | Step 546600 | Avg Loss: 0.0141 | Grad Norm: 0.01035872\n",
      "Epoch 1 | Step 546700 | Avg Loss: 0.0147 | Grad Norm: 0.00955115\n",
      "Epoch 1 | Step 546800 | Avg Loss: 0.0145 | Grad Norm: 0.01223552\n",
      "Epoch 1 | Step 546900 | Avg Loss: 0.0145 | Grad Norm: 0.00884830\n",
      "Epoch 1 | Step 547000 | Avg Loss: 0.0142 | Grad Norm: 0.00949030\n",
      "Epoch 1 | Step 547100 | Avg Loss: 0.0144 | Grad Norm: 0.00831242\n",
      "Epoch 1 | Step 547200 | Avg Loss: 0.0145 | Grad Norm: 0.00955003\n",
      "Epoch 1 | Step 547300 | Avg Loss: 0.0147 | Grad Norm: 0.00919100\n",
      "Epoch 1 | Step 547400 | Avg Loss: 0.0146 | Grad Norm: 0.00924595\n",
      "Epoch 1 | Step 547500 | Avg Loss: 0.0147 | Grad Norm: 0.00797002\n",
      "Epoch 1 | Step 547600 | Avg Loss: 0.0145 | Grad Norm: 0.00928930\n",
      "Epoch 1 | Step 547700 | Avg Loss: 0.0144 | Grad Norm: 0.01119342\n",
      "Epoch 1 | Step 547800 | Avg Loss: 0.0146 | Grad Norm: 0.00962752\n",
      "Epoch 1 | Step 547900 | Avg Loss: 0.0150 | Grad Norm: 0.01031663\n",
      "Epoch 1 | Step 548000 | Avg Loss: 0.0151 | Grad Norm: 0.00953433\n",
      "Epoch 1 | Step 548100 | Avg Loss: 0.0151 | Grad Norm: 0.00869210\n",
      "Epoch 1 | Step 548200 | Avg Loss: 0.0148 | Grad Norm: 0.00987115\n",
      "Epoch 1 | Step 548300 | Avg Loss: 0.0145 | Grad Norm: 0.00956800\n",
      "Epoch 1 | Step 548400 | Avg Loss: 0.0147 | Grad Norm: 0.00891599\n",
      "Epoch 1 | Step 548500 | Avg Loss: 0.0149 | Grad Norm: 0.00909639\n",
      "Epoch 1 | Step 548600 | Avg Loss: 0.0144 | Grad Norm: 0.00926838\n",
      "Epoch 1 | Step 548700 | Avg Loss: 0.0147 | Grad Norm: 0.00852235\n",
      "Epoch 1 | Step 548800 | Avg Loss: 0.0152 | Grad Norm: 0.01131423\n",
      "Epoch 1 | Step 548900 | Avg Loss: 0.0150 | Grad Norm: 0.01061394\n",
      "Epoch 1 | Step 549000 | Avg Loss: 0.0152 | Grad Norm: 0.00849241\n",
      "Epoch 1 | Step 549100 | Avg Loss: 0.0149 | Grad Norm: 0.00839625\n",
      "Epoch 1 | Step 549200 | Avg Loss: 0.0148 | Grad Norm: 0.00894608\n",
      "Epoch 1 | Step 549300 | Avg Loss: 0.0148 | Grad Norm: 0.00855203\n",
      "Epoch 1 | Step 549400 | Avg Loss: 0.0148 | Grad Norm: 0.00874237\n",
      "Epoch 1 | Step 549500 | Avg Loss: 0.0151 | Grad Norm: 0.01007552\n",
      "Epoch 1 | Step 549600 | Avg Loss: 0.0150 | Grad Norm: 0.00799201\n",
      "Epoch 1 | Step 549700 | Avg Loss: 0.0145 | Grad Norm: 0.00963272\n",
      "Epoch 1 | Step 549800 | Avg Loss: 0.0148 | Grad Norm: 0.00917585\n",
      "Epoch 1 | Step 549900 | Avg Loss: 0.0148 | Grad Norm: 0.01040491\n",
      "Epoch 1 | Step 550000 | Avg Loss: 0.0144 | Grad Norm: 0.00779875\n",
      "Epoch 1 | Step 550100 | Avg Loss: 0.0145 | Grad Norm: 0.00840249\n",
      "Epoch 1 | Step 550200 | Avg Loss: 0.0145 | Grad Norm: 0.00934814\n",
      "Epoch 1 | Step 550300 | Avg Loss: 0.0142 | Grad Norm: 0.00895671\n",
      "Epoch 1 | Step 550400 | Avg Loss: 0.0145 | Grad Norm: 0.00904542\n",
      "Epoch 1 | Step 550500 | Avg Loss: 0.0148 | Grad Norm: 0.00707530\n",
      "Epoch 1 | Step 550600 | Avg Loss: 0.0151 | Grad Norm: 0.00834509\n",
      "Epoch 1 | Step 550700 | Avg Loss: 0.0151 | Grad Norm: 0.00855882\n",
      "Epoch 1 | Step 550800 | Avg Loss: 0.0150 | Grad Norm: 0.00796112\n",
      "Epoch 1 | Step 550900 | Avg Loss: 0.0148 | Grad Norm: 0.00797628\n",
      "Epoch 1 | Step 551000 | Avg Loss: 0.0148 | Grad Norm: 0.00844610\n",
      "Epoch 1 | Step 551100 | Avg Loss: 0.0145 | Grad Norm: 0.00715345\n",
      "Epoch 1 | Step 551200 | Avg Loss: 0.0144 | Grad Norm: 0.00967326\n",
      "Epoch 1 | Step 551300 | Avg Loss: 0.0143 | Grad Norm: 0.00859709\n",
      "Epoch 1 | Step 551400 | Avg Loss: 0.0144 | Grad Norm: 0.00846776\n",
      "Epoch 1 | Step 551500 | Avg Loss: 0.0143 | Grad Norm: 0.00883530\n",
      "Epoch 1 | Step 551600 | Avg Loss: 0.0148 | Grad Norm: 0.00963015\n",
      "Epoch 1 | Step 551700 | Avg Loss: 0.0148 | Grad Norm: 0.00919258\n",
      "Epoch 1 | Step 551800 | Avg Loss: 0.0146 | Grad Norm: 0.00993286\n",
      "Epoch 1 | Step 551900 | Avg Loss: 0.0145 | Grad Norm: 0.00788472\n",
      "Epoch 1 | Step 552000 | Avg Loss: 0.0148 | Grad Norm: 0.00882403\n",
      "Epoch 1 | Step 552100 | Avg Loss: 0.0146 | Grad Norm: 0.00940960\n",
      "Epoch 1 | Step 552200 | Avg Loss: 0.0145 | Grad Norm: 0.00943288\n",
      "Epoch 1 | Step 552300 | Avg Loss: 0.0144 | Grad Norm: 0.00893062\n",
      "Epoch 1 | Step 552400 | Avg Loss: 0.0143 | Grad Norm: 0.00917720\n",
      "Epoch 1 | Step 552500 | Avg Loss: 0.0143 | Grad Norm: 0.00899255\n",
      "Epoch 1 | Step 552600 | Avg Loss: 0.0150 | Grad Norm: 0.01024326\n",
      "Epoch 1 | Step 552700 | Avg Loss: 0.0150 | Grad Norm: 0.01013360\n",
      "Epoch 1 | Step 552800 | Avg Loss: 0.0149 | Grad Norm: 0.00917423\n",
      "Epoch 1 | Step 552900 | Avg Loss: 0.0148 | Grad Norm: 0.00805718\n",
      "Epoch 1 | Step 553000 | Avg Loss: 0.0145 | Grad Norm: 0.00782837\n",
      "Epoch 1 | Step 553100 | Avg Loss: 0.0139 | Grad Norm: 0.00807659\n",
      "Epoch 1 | Step 553200 | Avg Loss: 0.0145 | Grad Norm: 0.00983517\n",
      "Epoch 1 | Step 553300 | Avg Loss: 0.0149 | Grad Norm: 0.00808393\n",
      "Epoch 1 | Step 553400 | Avg Loss: 0.0152 | Grad Norm: 0.01053983\n",
      "Epoch 1 | Step 553500 | Avg Loss: 0.0149 | Grad Norm: 0.00993645\n",
      "Epoch 1 | Step 553600 | Avg Loss: 0.0150 | Grad Norm: 0.00919359\n",
      "Epoch 1 | Step 553700 | Avg Loss: 0.0150 | Grad Norm: 0.00899902\n",
      "Epoch 1 | Step 553800 | Avg Loss: 0.0151 | Grad Norm: 0.01006112\n",
      "Epoch 1 | Step 553900 | Avg Loss: 0.0147 | Grad Norm: 0.00852735\n",
      "Epoch 1 | Step 554000 | Avg Loss: 0.0145 | Grad Norm: 0.00827659\n",
      "Epoch 1 | Step 554100 | Avg Loss: 0.0149 | Grad Norm: 0.00886948\n",
      "Epoch 1 | Step 554200 | Avg Loss: 0.0145 | Grad Norm: 0.01071615\n",
      "Epoch 1 | Step 554300 | Avg Loss: 0.0147 | Grad Norm: 0.00875086\n",
      "Epoch 1 | Step 554400 | Avg Loss: 0.0147 | Grad Norm: 0.00966095\n",
      "Epoch 1 | Step 554500 | Avg Loss: 0.0152 | Grad Norm: 0.00852345\n",
      "Epoch 1 | Step 554600 | Avg Loss: 0.0152 | Grad Norm: 0.01133920\n",
      "Epoch 1 | Step 554700 | Avg Loss: 0.0146 | Grad Norm: 0.00816304\n",
      "Epoch 1 | Step 554800 | Avg Loss: 0.0145 | Grad Norm: 0.00886483\n",
      "Epoch 1 | Step 554900 | Avg Loss: 0.0146 | Grad Norm: 0.00988402\n",
      "Epoch 1 | Step 555000 | Avg Loss: 0.0150 | Grad Norm: 0.01075107\n",
      "Epoch 1 | Step 555100 | Avg Loss: 0.0149 | Grad Norm: 0.00914642\n",
      "Epoch 1 | Step 555200 | Avg Loss: 0.0147 | Grad Norm: 0.00909722\n",
      "Epoch 1 | Step 555300 | Avg Loss: 0.0148 | Grad Norm: 0.00938400\n",
      "Epoch 1 | Step 555400 | Avg Loss: 0.0147 | Grad Norm: 0.00880754\n",
      "Epoch 1 | Step 555500 | Avg Loss: 0.0151 | Grad Norm: 0.00995478\n",
      "Epoch 1 | Step 555600 | Avg Loss: 0.0151 | Grad Norm: 0.00920375\n",
      "Epoch 1 | Step 555700 | Avg Loss: 0.0149 | Grad Norm: 0.00849804\n",
      "Epoch 1 | Step 555800 | Avg Loss: 0.0148 | Grad Norm: 0.00878504\n",
      "Epoch 1 | Step 555900 | Avg Loss: 0.0152 | Grad Norm: 0.01052566\n",
      "Epoch 1 | Step 556000 | Avg Loss: 0.0152 | Grad Norm: 0.00990560\n",
      "Epoch 1 | Step 556100 | Avg Loss: 0.0152 | Grad Norm: 0.00918854\n",
      "Epoch 1 | Step 556200 | Avg Loss: 0.0150 | Grad Norm: 0.00908750\n",
      "Epoch 1 | Step 556300 | Avg Loss: 0.0146 | Grad Norm: 0.00926475\n",
      "Epoch 1 | Step 556400 | Avg Loss: 0.0149 | Grad Norm: 0.00985626\n",
      "Epoch 1 | Step 556500 | Avg Loss: 0.0152 | Grad Norm: 0.01019287\n",
      "Epoch 1 | Step 556600 | Avg Loss: 0.0152 | Grad Norm: 0.00942666\n",
      "Epoch 1 | Step 556700 | Avg Loss: 0.0148 | Grad Norm: 0.00953242\n",
      "Epoch 1 | Step 556800 | Avg Loss: 0.0151 | Grad Norm: 0.00899107\n",
      "Epoch 1 | Step 556900 | Avg Loss: 0.0153 | Grad Norm: 0.00942088\n",
      "Epoch 1 | Step 557000 | Avg Loss: 0.0147 | Grad Norm: 0.00872268\n",
      "Epoch 1 | Step 557100 | Avg Loss: 0.0152 | Grad Norm: 0.00987658\n",
      "Epoch 1 | Step 557200 | Avg Loss: 0.0151 | Grad Norm: 0.01067212\n",
      "Epoch 1 | Step 557300 | Avg Loss: 0.0154 | Grad Norm: 0.00906019\n",
      "Epoch 1 | Step 557400 | Avg Loss: 0.0156 | Grad Norm: 0.01027301\n",
      "Epoch 1 | Step 557500 | Avg Loss: 0.0156 | Grad Norm: 0.01214026\n",
      "Epoch 1 | Step 557600 | Avg Loss: 0.0159 | Grad Norm: 0.01014972\n",
      "Epoch 1 | Step 557700 | Avg Loss: 0.0155 | Grad Norm: 0.00915210\n",
      "Epoch 1 | Step 557800 | Avg Loss: 0.0152 | Grad Norm: 0.00916758\n",
      "Epoch 1 | Step 557900 | Avg Loss: 0.0151 | Grad Norm: 0.00804770\n",
      "Epoch 1 | Step 558000 | Avg Loss: 0.0146 | Grad Norm: 0.00956410\n",
      "Epoch 1 | Step 558100 | Avg Loss: 0.0147 | Grad Norm: 0.00877523\n",
      "Epoch 1 | Step 558200 | Avg Loss: 0.0147 | Grad Norm: 0.00815594\n",
      "Epoch 1 | Step 558300 | Avg Loss: 0.0148 | Grad Norm: 0.00852282\n",
      "Epoch 1 | Step 558400 | Avg Loss: 0.0149 | Grad Norm: 0.01014112\n",
      "Epoch 1 | Step 558500 | Avg Loss: 0.0149 | Grad Norm: 0.00982270\n",
      "Epoch 1 | Step 558600 | Avg Loss: 0.0147 | Grad Norm: 0.00755953\n",
      "Epoch 1 | Step 558700 | Avg Loss: 0.0147 | Grad Norm: 0.00896063\n",
      "Epoch 1 | Step 558800 | Avg Loss: 0.0149 | Grad Norm: 0.00889355\n",
      "Epoch 1 | Step 558900 | Avg Loss: 0.0151 | Grad Norm: 0.01091514\n",
      "Epoch 1 | Step 559000 | Avg Loss: 0.0149 | Grad Norm: 0.01082858\n",
      "Epoch 1 | Step 559100 | Avg Loss: 0.0150 | Grad Norm: 0.00775583\n",
      "Epoch 1 | Step 559200 | Avg Loss: 0.0149 | Grad Norm: 0.00913141\n",
      "Epoch 1 | Step 559300 | Avg Loss: 0.0151 | Grad Norm: 0.01074350\n",
      "Epoch 1 | Step 559400 | Avg Loss: 0.0152 | Grad Norm: 0.01097565\n",
      "Epoch 1 | Step 559500 | Avg Loss: 0.0149 | Grad Norm: 0.00815433\n",
      "Epoch 1 | Step 559600 | Avg Loss: 0.0151 | Grad Norm: 0.00819553\n",
      "Epoch 1 | Step 559700 | Avg Loss: 0.0149 | Grad Norm: 0.00983333\n",
      "Epoch 1 | Step 559800 | Avg Loss: 0.0151 | Grad Norm: 0.00857475\n",
      "Epoch 1 | Step 559900 | Avg Loss: 0.0149 | Grad Norm: 0.00905860\n",
      "Epoch 1 | Step 560000 | Avg Loss: 0.0146 | Grad Norm: 0.00951417\n",
      "Epoch 1 | Step 560100 | Avg Loss: 0.0149 | Grad Norm: 0.00820845\n",
      "Epoch 1 | Step 560200 | Avg Loss: 0.0149 | Grad Norm: 0.00803122\n",
      "Epoch 1 | Step 560300 | Avg Loss: 0.0147 | Grad Norm: 0.00933934\n",
      "Epoch 1 | Step 560400 | Avg Loss: 0.0145 | Grad Norm: 0.00941872\n",
      "Epoch 1 | Step 560500 | Avg Loss: 0.0145 | Grad Norm: 0.00817153\n",
      "Epoch 1 | Step 560600 | Avg Loss: 0.0144 | Grad Norm: 0.00966118\n",
      "Epoch 1 | Step 560700 | Avg Loss: 0.0141 | Grad Norm: 0.00946933\n",
      "Epoch 1 | Step 560800 | Avg Loss: 0.0142 | Grad Norm: 0.00764615\n",
      "Epoch 1 | Step 560900 | Avg Loss: 0.0141 | Grad Norm: 0.00899886\n",
      "Epoch 1 | Step 561000 | Avg Loss: 0.0143 | Grad Norm: 0.00832065\n",
      "Epoch 1 | Step 561100 | Avg Loss: 0.0149 | Grad Norm: 0.00848832\n",
      "Epoch 1 | Step 561200 | Avg Loss: 0.0151 | Grad Norm: 0.00861301\n",
      "Epoch 1 | Step 561300 | Avg Loss: 0.0147 | Grad Norm: 0.01047369\n",
      "Epoch 1 | Step 561400 | Avg Loss: 0.0148 | Grad Norm: 0.00941859\n",
      "Epoch 1 | Step 561500 | Avg Loss: 0.0147 | Grad Norm: 0.00735088\n",
      "Epoch 1 | Step 561600 | Avg Loss: 0.0150 | Grad Norm: 0.00859890\n",
      "Epoch 1 | Step 561700 | Avg Loss: 0.0147 | Grad Norm: 0.00822526\n",
      "Epoch 1 | Step 561800 | Avg Loss: 0.0147 | Grad Norm: 0.00937912\n",
      "Epoch 1 | Step 561900 | Avg Loss: 0.0146 | Grad Norm: 0.00822666\n",
      "Epoch 1 | Step 562000 | Avg Loss: 0.0150 | Grad Norm: 0.00944004\n",
      "Epoch 1 | Step 562100 | Avg Loss: 0.0147 | Grad Norm: 0.00888500\n",
      "Epoch 1 | Step 562200 | Avg Loss: 0.0152 | Grad Norm: 0.00946188\n",
      "Epoch 1 | Step 562300 | Avg Loss: 0.0150 | Grad Norm: 0.01022284\n",
      "Epoch 1 | Step 562400 | Avg Loss: 0.0150 | Grad Norm: 0.00875408\n",
      "Epoch 1 | Step 562500 | Avg Loss: 0.0152 | Grad Norm: 0.00822810\n",
      "Epoch 1 | Step 562600 | Avg Loss: 0.0146 | Grad Norm: 0.00826189\n",
      "Epoch 1 | Step 562700 | Avg Loss: 0.0150 | Grad Norm: 0.00939315\n",
      "Epoch 1 | Step 562800 | Avg Loss: 0.0153 | Grad Norm: 0.00856982\n",
      "Epoch 1 | Step 562900 | Avg Loss: 0.0153 | Grad Norm: 0.00780560\n",
      "Epoch 1 | Step 563000 | Avg Loss: 0.0150 | Grad Norm: 0.00830662\n",
      "Epoch 1 | Step 563100 | Avg Loss: 0.0148 | Grad Norm: 0.00829176\n",
      "Epoch 1 | Step 563200 | Avg Loss: 0.0146 | Grad Norm: 0.00887620\n",
      "Epoch 1 | Step 563300 | Avg Loss: 0.0144 | Grad Norm: 0.00844804\n",
      "Epoch 1 | Step 563400 | Avg Loss: 0.0143 | Grad Norm: 0.00937693\n",
      "Epoch 1 | Step 563500 | Avg Loss: 0.0140 | Grad Norm: 0.00883480\n",
      "Epoch 1 | Step 563600 | Avg Loss: 0.0144 | Grad Norm: 0.00988714\n",
      "Epoch 1 | Step 563700 | Avg Loss: 0.0146 | Grad Norm: 0.01004114\n",
      "Epoch 1 | Step 563800 | Avg Loss: 0.0146 | Grad Norm: 0.00976717\n",
      "Epoch 1 | Step 563900 | Avg Loss: 0.0146 | Grad Norm: 0.00802304\n",
      "Epoch 1 | Step 564000 | Avg Loss: 0.0144 | Grad Norm: 0.00805864\n",
      "Epoch 1 | Step 564100 | Avg Loss: 0.0145 | Grad Norm: 0.00797679\n",
      "Epoch 1 | Step 564200 | Avg Loss: 0.0147 | Grad Norm: 0.00868399\n",
      "Epoch 1 | Step 564300 | Avg Loss: 0.0142 | Grad Norm: 0.00960057\n",
      "Epoch 1 | Step 564400 | Avg Loss: 0.0146 | Grad Norm: 0.00825150\n",
      "Epoch 1 | Step 564500 | Avg Loss: 0.0145 | Grad Norm: 0.00917899\n",
      "Epoch 1 | Step 564600 | Avg Loss: 0.0145 | Grad Norm: 0.01103018\n",
      "Epoch 1 | Step 564700 | Avg Loss: 0.0148 | Grad Norm: 0.00924288\n",
      "Epoch 1 | Step 564800 | Avg Loss: 0.0143 | Grad Norm: 0.00988278\n",
      "Epoch 1 | Step 564900 | Avg Loss: 0.0144 | Grad Norm: 0.00822454\n",
      "Epoch 1 | Step 565000 | Avg Loss: 0.0144 | Grad Norm: 0.00935595\n",
      "Epoch 1 | Step 565100 | Avg Loss: 0.0154 | Grad Norm: 0.00923731\n",
      "Epoch 1 | Step 565200 | Avg Loss: 0.0154 | Grad Norm: 0.00918594\n",
      "Epoch 1 | Step 565300 | Avg Loss: 0.0149 | Grad Norm: 0.00891710\n",
      "Epoch 1 | Step 565400 | Avg Loss: 0.0151 | Grad Norm: 0.00990790\n",
      "Epoch 1 | Step 565500 | Avg Loss: 0.0147 | Grad Norm: 0.00882297\n",
      "Epoch 1 | Step 565600 | Avg Loss: 0.0147 | Grad Norm: 0.00988466\n",
      "Epoch 1 | Step 565700 | Avg Loss: 0.0149 | Grad Norm: 0.00800784\n",
      "Epoch 1 | Step 565800 | Avg Loss: 0.0146 | Grad Norm: 0.01038580\n",
      "Epoch 1 | Step 565900 | Avg Loss: 0.0144 | Grad Norm: 0.00776693\n",
      "Epoch 1 | Step 566000 | Avg Loss: 0.0145 | Grad Norm: 0.00805841\n",
      "Epoch 1 | Step 566100 | Avg Loss: 0.0146 | Grad Norm: 0.00894972\n",
      "Epoch 1 | Step 566200 | Avg Loss: 0.0147 | Grad Norm: 0.00833861\n",
      "Epoch 1 | Step 566300 | Avg Loss: 0.0147 | Grad Norm: 0.00959971\n",
      "Epoch 1 | Step 566400 | Avg Loss: 0.0146 | Grad Norm: 0.00889786\n",
      "Epoch 1 | Step 566500 | Avg Loss: 0.0150 | Grad Norm: 0.00830931\n",
      "Epoch 1 | Step 566600 | Avg Loss: 0.0147 | Grad Norm: 0.00854506\n",
      "Epoch 1 | Step 566700 | Avg Loss: 0.0148 | Grad Norm: 0.00836961\n",
      "Epoch 1 | Step 566800 | Avg Loss: 0.0145 | Grad Norm: 0.00813324\n",
      "Epoch 1 | Step 566900 | Avg Loss: 0.0142 | Grad Norm: 0.00892940\n",
      "Epoch 1 | Step 567000 | Avg Loss: 0.0146 | Grad Norm: 0.01041474\n",
      "Epoch 1 | Step 567100 | Avg Loss: 0.0144 | Grad Norm: 0.00932793\n",
      "Epoch 1 | Step 567200 | Avg Loss: 0.0147 | Grad Norm: 0.00932934\n",
      "Epoch 1 | Step 567300 | Avg Loss: 0.0148 | Grad Norm: 0.00822097\n",
      "Epoch 1 | Step 567400 | Avg Loss: 0.0149 | Grad Norm: 0.00837574\n",
      "Epoch 1 | Step 567500 | Avg Loss: 0.0152 | Grad Norm: 0.00970364\n",
      "Epoch 1 | Step 567600 | Avg Loss: 0.0151 | Grad Norm: 0.00841584\n",
      "Epoch 1 | Step 567700 | Avg Loss: 0.0149 | Grad Norm: 0.00816552\n",
      "Epoch 1 | Step 567800 | Avg Loss: 0.0145 | Grad Norm: 0.00942487\n",
      "Epoch 1 | Step 567900 | Avg Loss: 0.0148 | Grad Norm: 0.00900020\n",
      "Epoch 1 | Step 568000 | Avg Loss: 0.0148 | Grad Norm: 0.00875220\n",
      "Epoch 1 | Step 568100 | Avg Loss: 0.0147 | Grad Norm: 0.00982011\n",
      "Epoch 1 | Step 568200 | Avg Loss: 0.0149 | Grad Norm: 0.00868294\n",
      "Epoch 1 | Step 568300 | Avg Loss: 0.0154 | Grad Norm: 0.00754537\n",
      "Epoch 1 | Step 568400 | Avg Loss: 0.0152 | Grad Norm: 0.00942960\n",
      "Epoch 1 | Step 568500 | Avg Loss: 0.0152 | Grad Norm: 0.00885821\n",
      "Epoch 1 | Step 568600 | Avg Loss: 0.0153 | Grad Norm: 0.00924652\n",
      "Epoch 1 | Step 568700 | Avg Loss: 0.0149 | Grad Norm: 0.00905665\n",
      "Epoch 1 | Step 568800 | Avg Loss: 0.0147 | Grad Norm: 0.01056212\n",
      "Epoch 1 | Step 568900 | Avg Loss: 0.0147 | Grad Norm: 0.01039343\n",
      "Epoch 1 | Step 569000 | Avg Loss: 0.0147 | Grad Norm: 0.00875621\n",
      "Epoch 1 | Step 569100 | Avg Loss: 0.0143 | Grad Norm: 0.00858090\n",
      "Epoch 1 | Step 569200 | Avg Loss: 0.0144 | Grad Norm: 0.00879098\n",
      "Epoch 1 | Step 569300 | Avg Loss: 0.0146 | Grad Norm: 0.01033194\n",
      "Epoch 1 | Step 569400 | Avg Loss: 0.0150 | Grad Norm: 0.00958590\n",
      "Epoch 1 | Step 569500 | Avg Loss: 0.0147 | Grad Norm: 0.00783643\n",
      "Epoch 1 | Step 569600 | Avg Loss: 0.0147 | Grad Norm: 0.01033825\n",
      "Epoch 1 | Step 569700 | Avg Loss: 0.0148 | Grad Norm: 0.00899855\n",
      "Epoch 1 | Step 569800 | Avg Loss: 0.0147 | Grad Norm: 0.00870075\n",
      "Epoch 1 | Step 569900 | Avg Loss: 0.0146 | Grad Norm: 0.00866996\n",
      "Epoch 1 | Step 570000 | Avg Loss: 0.0145 | Grad Norm: 0.01027860\n",
      "Epoch 1 | Step 570100 | Avg Loss: 0.0144 | Grad Norm: 0.01060818\n",
      "Epoch 1 | Step 570200 | Avg Loss: 0.0147 | Grad Norm: 0.00991585\n",
      "Epoch 1 | Step 570300 | Avg Loss: 0.0151 | Grad Norm: 0.01114273\n",
      "Epoch 1 | Step 570400 | Avg Loss: 0.0154 | Grad Norm: 0.00917926\n",
      "Epoch 1 | Step 570500 | Avg Loss: 0.0149 | Grad Norm: 0.01024959\n",
      "Epoch 1 | Step 570600 | Avg Loss: 0.0150 | Grad Norm: 0.00858287\n",
      "Epoch 1 | Step 570700 | Avg Loss: 0.0146 | Grad Norm: 0.00887743\n",
      "Epoch 1 | Step 570800 | Avg Loss: 0.0148 | Grad Norm: 0.00826441\n",
      "Epoch 1 | Step 570900 | Avg Loss: 0.0149 | Grad Norm: 0.00909904\n",
      "Epoch 1 | Step 571000 | Avg Loss: 0.0149 | Grad Norm: 0.00919580\n",
      "Epoch 1 | Step 571100 | Avg Loss: 0.0148 | Grad Norm: 0.00854401\n",
      "Epoch 1 | Step 571200 | Avg Loss: 0.0146 | Grad Norm: 0.00959123\n",
      "Epoch 1 | Step 571300 | Avg Loss: 0.0146 | Grad Norm: 0.00972726\n",
      "Epoch 1 | Step 571400 | Avg Loss: 0.0150 | Grad Norm: 0.00922987\n",
      "Epoch 1 | Step 571500 | Avg Loss: 0.0147 | Grad Norm: 0.00956549\n",
      "Epoch 1 | Step 571600 | Avg Loss: 0.0152 | Grad Norm: 0.00930073\n",
      "Epoch 1 | Step 571700 | Avg Loss: 0.0146 | Grad Norm: 0.00814569\n",
      "Epoch 1 | Step 571800 | Avg Loss: 0.0143 | Grad Norm: 0.00803902\n",
      "Epoch 1 | Step 571900 | Avg Loss: 0.0146 | Grad Norm: 0.00978206\n",
      "Epoch 1 | Step 572000 | Avg Loss: 0.0148 | Grad Norm: 0.00888165\n",
      "Epoch 1 | Step 572100 | Avg Loss: 0.0149 | Grad Norm: 0.00790296\n",
      "Epoch 1 | Step 572200 | Avg Loss: 0.0150 | Grad Norm: 0.00843594\n",
      "Epoch 1 | Step 572300 | Avg Loss: 0.0148 | Grad Norm: 0.00876420\n",
      "Epoch 1 | Step 572400 | Avg Loss: 0.0149 | Grad Norm: 0.00875830\n",
      "Epoch 1 | Step 572500 | Avg Loss: 0.0147 | Grad Norm: 0.00820194\n",
      "Epoch 1 | Step 572600 | Avg Loss: 0.0149 | Grad Norm: 0.00928451\n",
      "Epoch 1 | Step 572700 | Avg Loss: 0.0149 | Grad Norm: 0.00724791\n",
      "Epoch 1 | Step 572800 | Avg Loss: 0.0149 | Grad Norm: 0.00977465\n",
      "Epoch 1 | Step 572900 | Avg Loss: 0.0147 | Grad Norm: 0.00974152\n",
      "Epoch 1 | Step 573000 | Avg Loss: 0.0146 | Grad Norm: 0.01020535\n",
      "Epoch 1 | Step 573100 | Avg Loss: 0.0146 | Grad Norm: 0.00781803\n",
      "Epoch 1 | Step 573200 | Avg Loss: 0.0143 | Grad Norm: 0.00901588\n",
      "Epoch 1 | Step 573300 | Avg Loss: 0.0142 | Grad Norm: 0.00767304\n",
      "Epoch 1 | Step 573400 | Avg Loss: 0.0142 | Grad Norm: 0.00815055\n",
      "Epoch 1 | Step 573500 | Avg Loss: 0.0146 | Grad Norm: 0.00969414\n",
      "Epoch 1 | Step 573600 | Avg Loss: 0.0147 | Grad Norm: 0.00976006\n",
      "Epoch 1 | Step 573700 | Avg Loss: 0.0146 | Grad Norm: 0.00988085\n",
      "Epoch 1 | Step 573800 | Avg Loss: 0.0146 | Grad Norm: 0.00932599\n",
      "Epoch 1 | Step 573900 | Avg Loss: 0.0147 | Grad Norm: 0.00862385\n",
      "Epoch 1 | Step 574000 | Avg Loss: 0.0150 | Grad Norm: 0.00977901\n",
      "Epoch 1 | Step 574100 | Avg Loss: 0.0147 | Grad Norm: 0.00793088\n",
      "Epoch 1 | Step 574200 | Avg Loss: 0.0145 | Grad Norm: 0.00975360\n",
      "Epoch 1 | Step 574300 | Avg Loss: 0.0147 | Grad Norm: 0.00868147\n",
      "Epoch 1 | Step 574400 | Avg Loss: 0.0146 | Grad Norm: 0.01042873\n",
      "Epoch 1 | Step 574500 | Avg Loss: 0.0146 | Grad Norm: 0.00843492\n",
      "Epoch 1 | Step 574600 | Avg Loss: 0.0146 | Grad Norm: 0.01002243\n",
      "Epoch 1 | Step 574700 | Avg Loss: 0.0146 | Grad Norm: 0.00900352\n",
      "Epoch 1 | Step 574800 | Avg Loss: 0.0147 | Grad Norm: 0.00837585\n",
      "Epoch 1 | Step 574900 | Avg Loss: 0.0148 | Grad Norm: 0.00894297\n",
      "Epoch 1 | Step 575000 | Avg Loss: 0.0148 | Grad Norm: 0.00928122\n",
      "Epoch 1 | Step 575100 | Avg Loss: 0.0146 | Grad Norm: 0.00893312\n",
      "Epoch 1 | Step 575200 | Avg Loss: 0.0146 | Grad Norm: 0.01060479\n",
      "Epoch 1 | Step 575300 | Avg Loss: 0.0148 | Grad Norm: 0.00888640\n",
      "Epoch 1 | Step 575400 | Avg Loss: 0.0149 | Grad Norm: 0.00916443\n",
      "Epoch 1 | Step 575500 | Avg Loss: 0.0148 | Grad Norm: 0.00965998\n",
      "Epoch 1 | Step 575600 | Avg Loss: 0.0149 | Grad Norm: 0.00973379\n",
      "Epoch 1 | Step 575700 | Avg Loss: 0.0147 | Grad Norm: 0.01012215\n",
      "Epoch 1 | Step 575800 | Avg Loss: 0.0147 | Grad Norm: 0.00942109\n",
      "Epoch 1 | Step 575900 | Avg Loss: 0.0146 | Grad Norm: 0.00890105\n",
      "Epoch 1 | Step 576000 | Avg Loss: 0.0148 | Grad Norm: 0.00877133\n",
      "Epoch 1 | Step 576100 | Avg Loss: 0.0147 | Grad Norm: 0.00814658\n",
      "Epoch 1 | Step 576200 | Avg Loss: 0.0147 | Grad Norm: 0.00878287\n",
      "Epoch 1 | Step 576300 | Avg Loss: 0.0146 | Grad Norm: 0.00900653\n",
      "Epoch 1 | Step 576400 | Avg Loss: 0.0144 | Grad Norm: 0.01056497\n",
      "Epoch 1 | Step 576500 | Avg Loss: 0.0144 | Grad Norm: 0.00965323\n",
      "Epoch 1 | Step 576600 | Avg Loss: 0.0148 | Grad Norm: 0.00887674\n",
      "Epoch 1 | Step 576700 | Avg Loss: 0.0147 | Grad Norm: 0.00834252\n",
      "Epoch 1 | Step 576800 | Avg Loss: 0.0147 | Grad Norm: 0.00942231\n",
      "Epoch 1 | Step 576900 | Avg Loss: 0.0146 | Grad Norm: 0.00742789\n",
      "Epoch 1 | Step 577000 | Avg Loss: 0.0143 | Grad Norm: 0.00806390\n",
      "Epoch 1 | Step 577100 | Avg Loss: 0.0141 | Grad Norm: 0.00999437\n",
      "Epoch 1 | Step 577200 | Avg Loss: 0.0141 | Grad Norm: 0.00982210\n",
      "Epoch 1 | Step 577300 | Avg Loss: 0.0144 | Grad Norm: 0.00896965\n",
      "Epoch 1 | Step 577400 | Avg Loss: 0.0145 | Grad Norm: 0.00806826\n",
      "Epoch 1 | Step 577500 | Avg Loss: 0.0146 | Grad Norm: 0.00902863\n",
      "Epoch 1 | Step 577600 | Avg Loss: 0.0145 | Grad Norm: 0.00958284\n",
      "Epoch 1 | Step 577700 | Avg Loss: 0.0145 | Grad Norm: 0.00910327\n",
      "Epoch 1 | Step 577800 | Avg Loss: 0.0147 | Grad Norm: 0.00895062\n",
      "Epoch 1 | Step 577900 | Avg Loss: 0.0144 | Grad Norm: 0.00802569\n",
      "Epoch 1 | Step 578000 | Avg Loss: 0.0145 | Grad Norm: 0.01022733\n",
      "Epoch 1 | Step 578100 | Avg Loss: 0.0148 | Grad Norm: 0.00854519\n",
      "Epoch 1 | Step 578200 | Avg Loss: 0.0147 | Grad Norm: 0.00915294\n",
      "Epoch 1 | Step 578300 | Avg Loss: 0.0141 | Grad Norm: 0.01020674\n",
      "Epoch 1 | Step 578400 | Avg Loss: 0.0142 | Grad Norm: 0.00742858\n",
      "Epoch 1 | Step 578500 | Avg Loss: 0.0139 | Grad Norm: 0.00806656\n",
      "Epoch 1 | Step 578600 | Avg Loss: 0.0144 | Grad Norm: 0.01019160\n",
      "Epoch 1 | Step 578700 | Avg Loss: 0.0149 | Grad Norm: 0.00867226\n",
      "Epoch 1 | Step 578800 | Avg Loss: 0.0147 | Grad Norm: 0.00775087\n",
      "Epoch 1 | Step 578900 | Avg Loss: 0.0146 | Grad Norm: 0.00972257\n",
      "Epoch 1 | Step 579000 | Avg Loss: 0.0142 | Grad Norm: 0.00898079\n",
      "Epoch 1 | Step 579100 | Avg Loss: 0.0143 | Grad Norm: 0.01012233\n",
      "Epoch 1 | Step 579200 | Avg Loss: 0.0144 | Grad Norm: 0.00918306\n",
      "Epoch 1 | Step 579300 | Avg Loss: 0.0145 | Grad Norm: 0.01107660\n",
      "Epoch 1 | Step 579400 | Avg Loss: 0.0144 | Grad Norm: 0.00780210\n",
      "Epoch 1 | Step 579500 | Avg Loss: 0.0147 | Grad Norm: 0.00938993\n",
      "Epoch 1 | Step 579600 | Avg Loss: 0.0151 | Grad Norm: 0.00837070\n",
      "Epoch 1 | Step 579700 | Avg Loss: 0.0147 | Grad Norm: 0.00834648\n",
      "Epoch 1 | Step 579800 | Avg Loss: 0.0145 | Grad Norm: 0.00936221\n",
      "Epoch 1 | Step 579900 | Avg Loss: 0.0142 | Grad Norm: 0.00924046\n",
      "Epoch 1 | Step 580000 | Avg Loss: 0.0147 | Grad Norm: 0.01004383\n",
      "Epoch 1 | Step 580100 | Avg Loss: 0.0147 | Grad Norm: 0.00998265\n",
      "Epoch 1 | Step 580200 | Avg Loss: 0.0146 | Grad Norm: 0.00892666\n",
      "Epoch 1 | Step 580300 | Avg Loss: 0.0143 | Grad Norm: 0.00912256\n",
      "Epoch 1 | Step 580400 | Avg Loss: 0.0143 | Grad Norm: 0.00966448\n",
      "Epoch 1 | Step 580500 | Avg Loss: 0.0148 | Grad Norm: 0.00929202\n",
      "Epoch 1 | Step 580600 | Avg Loss: 0.0146 | Grad Norm: 0.00869111\n",
      "Epoch 1 | Step 580700 | Avg Loss: 0.0146 | Grad Norm: 0.00747255\n",
      "Epoch 1 | Step 580800 | Avg Loss: 0.0144 | Grad Norm: 0.00868948\n",
      "Epoch 1 | Step 580900 | Avg Loss: 0.0149 | Grad Norm: 0.00910696\n",
      "Epoch 1 | Step 581000 | Avg Loss: 0.0146 | Grad Norm: 0.00876030\n",
      "Epoch 1 | Step 581100 | Avg Loss: 0.0143 | Grad Norm: 0.00883981\n",
      "Epoch 1 | Step 581200 | Avg Loss: 0.0147 | Grad Norm: 0.00769859\n",
      "Epoch 1 | Step 581300 | Avg Loss: 0.0148 | Grad Norm: 0.00873858\n",
      "Epoch 1 | Step 581400 | Avg Loss: 0.0151 | Grad Norm: 0.00916368\n",
      "Epoch 1 | Step 581500 | Avg Loss: 0.0145 | Grad Norm: 0.00927703\n",
      "Epoch 1 | Step 581600 | Avg Loss: 0.0146 | Grad Norm: 0.00847651\n",
      "Epoch 1 | Step 581700 | Avg Loss: 0.0148 | Grad Norm: 0.00860691\n",
      "Epoch 1 | Step 581800 | Avg Loss: 0.0145 | Grad Norm: 0.00868878\n",
      "Epoch 1 | Step 581900 | Avg Loss: 0.0143 | Grad Norm: 0.00881178\n",
      "Epoch 1 | Step 582000 | Avg Loss: 0.0144 | Grad Norm: 0.00878080\n",
      "Epoch 1 | Step 582100 | Avg Loss: 0.0146 | Grad Norm: 0.00871745\n",
      "Epoch 1 | Step 582200 | Avg Loss: 0.0146 | Grad Norm: 0.00756745\n",
      "Epoch 1 | Step 582300 | Avg Loss: 0.0148 | Grad Norm: 0.00840640\n",
      "Epoch 1 | Step 582400 | Avg Loss: 0.0146 | Grad Norm: 0.00850097\n",
      "Epoch 1 | Step 582500 | Avg Loss: 0.0147 | Grad Norm: 0.00887758\n",
      "Epoch 1 | Step 582600 | Avg Loss: 0.0147 | Grad Norm: 0.00845742\n",
      "Epoch 1 | Step 582700 | Avg Loss: 0.0145 | Grad Norm: 0.00795312\n",
      "Epoch 1 | Step 582800 | Avg Loss: 0.0146 | Grad Norm: 0.00848913\n",
      "Epoch 1 | Step 582900 | Avg Loss: 0.0144 | Grad Norm: 0.00885371\n",
      "Epoch 1 | Step 583000 | Avg Loss: 0.0142 | Grad Norm: 0.00858860\n",
      "Epoch 1 | Step 583100 | Avg Loss: 0.0143 | Grad Norm: 0.01073042\n",
      "Epoch 1 | Step 583200 | Avg Loss: 0.0146 | Grad Norm: 0.00942467\n",
      "Epoch 1 | Step 583300 | Avg Loss: 0.0146 | Grad Norm: 0.00850971\n",
      "Epoch 1 | Step 583400 | Avg Loss: 0.0148 | Grad Norm: 0.00882220\n",
      "Epoch 1 | Step 583500 | Avg Loss: 0.0151 | Grad Norm: 0.00860327\n",
      "Epoch 1 | Step 583600 | Avg Loss: 0.0149 | Grad Norm: 0.00904853\n",
      "Epoch 1 | Step 583700 | Avg Loss: 0.0150 | Grad Norm: 0.00911463\n",
      "Epoch 1 | Step 583800 | Avg Loss: 0.0151 | Grad Norm: 0.00886579\n",
      "Epoch 1 | Step 583900 | Avg Loss: 0.0149 | Grad Norm: 0.00939410\n",
      "Epoch 1 | Step 584000 | Avg Loss: 0.0148 | Grad Norm: 0.00996954\n",
      "Epoch 1 | Step 584100 | Avg Loss: 0.0143 | Grad Norm: 0.01149806\n",
      "Epoch 1 | Step 584200 | Avg Loss: 0.0148 | Grad Norm: 0.00806483\n",
      "Epoch 1 | Step 584300 | Avg Loss: 0.0148 | Grad Norm: 0.00873596\n",
      "Epoch 1 | Step 584400 | Avg Loss: 0.0150 | Grad Norm: 0.00934364\n",
      "Epoch 1 | Step 584500 | Avg Loss: 0.0150 | Grad Norm: 0.01008102\n",
      "Epoch 1 | Step 584600 | Avg Loss: 0.0153 | Grad Norm: 0.01071548\n",
      "Epoch 1 | Step 584700 | Avg Loss: 0.0152 | Grad Norm: 0.00966932\n",
      "Epoch 1 | Step 584800 | Avg Loss: 0.0155 | Grad Norm: 0.00945553\n",
      "Epoch 1 | Step 584900 | Avg Loss: 0.0155 | Grad Norm: 0.00754811\n",
      "Epoch 1 | Step 585000 | Avg Loss: 0.0156 | Grad Norm: 0.00934709\n",
      "Epoch 1 | Step 585100 | Avg Loss: 0.0157 | Grad Norm: 0.00810210\n",
      "Epoch 1 | Step 585200 | Avg Loss: 0.0160 | Grad Norm: 0.00979137\n",
      "Epoch 1 | Step 585300 | Avg Loss: 0.0156 | Grad Norm: 0.00803083\n",
      "Epoch 1 | Step 585400 | Avg Loss: 0.0153 | Grad Norm: 0.00994892\n",
      "Epoch 1 | Step 585500 | Avg Loss: 0.0154 | Grad Norm: 0.00901430\n",
      "Epoch 1 | Step 585600 | Avg Loss: 0.0153 | Grad Norm: 0.01035094\n",
      "Epoch 1 | Step 585700 | Avg Loss: 0.0155 | Grad Norm: 0.00964762\n",
      "Epoch 1 | Step 585800 | Avg Loss: 0.0153 | Grad Norm: 0.01086827\n",
      "Epoch 1 | Step 585900 | Avg Loss: 0.0154 | Grad Norm: 0.01014687\n",
      "Epoch 1 | Step 586000 | Avg Loss: 0.0149 | Grad Norm: 0.00937623\n",
      "Epoch 1 | Step 586100 | Avg Loss: 0.0145 | Grad Norm: 0.00891757\n",
      "Epoch 1 | Step 586200 | Avg Loss: 0.0144 | Grad Norm: 0.00910415\n",
      "Epoch 1 | Step 586300 | Avg Loss: 0.0143 | Grad Norm: 0.00968761\n",
      "Epoch 1 | Step 586400 | Avg Loss: 0.0147 | Grad Norm: 0.00925239\n",
      "Epoch 1 | Step 586500 | Avg Loss: 0.0146 | Grad Norm: 0.00931504\n",
      "Epoch 1 | Step 586600 | Avg Loss: 0.0151 | Grad Norm: 0.00800513\n",
      "Epoch 1 | Step 586700 | Avg Loss: 0.0153 | Grad Norm: 0.00965486\n",
      "Epoch 1 | Step 586800 | Avg Loss: 0.0152 | Grad Norm: 0.00870081\n",
      "Epoch 1 | Step 586900 | Avg Loss: 0.0151 | Grad Norm: 0.00811633\n",
      "Epoch 1 | Step 587000 | Avg Loss: 0.0148 | Grad Norm: 0.00879647\n",
      "Epoch 1 | Step 587100 | Avg Loss: 0.0149 | Grad Norm: 0.00714719\n",
      "Epoch 1 | Step 587200 | Avg Loss: 0.0152 | Grad Norm: 0.01098428\n",
      "Epoch 1 | Step 587300 | Avg Loss: 0.0150 | Grad Norm: 0.00846615\n",
      "Epoch 1 | Step 587400 | Avg Loss: 0.0151 | Grad Norm: 0.00712160\n",
      "Epoch 1 | Step 587500 | Avg Loss: 0.0153 | Grad Norm: 0.01040718\n",
      "Epoch 1 | Step 587600 | Avg Loss: 0.0157 | Grad Norm: 0.00939494\n",
      "Epoch 1 | Step 587700 | Avg Loss: 0.0153 | Grad Norm: 0.01058876\n",
      "Epoch 1 | Step 587800 | Avg Loss: 0.0151 | Grad Norm: 0.00976828\n",
      "Epoch 1 | Step 587900 | Avg Loss: 0.0150 | Grad Norm: 0.00870009\n",
      "Epoch 1 | Step 588000 | Avg Loss: 0.0150 | Grad Norm: 0.00884968\n",
      "Epoch 1 | Step 588100 | Avg Loss: 0.0153 | Grad Norm: 0.00891726\n",
      "Epoch 1 | Step 588200 | Avg Loss: 0.0148 | Grad Norm: 0.00868477\n",
      "Epoch 1 | Step 588300 | Avg Loss: 0.0147 | Grad Norm: 0.00861761\n",
      "Epoch 1 | Step 588400 | Avg Loss: 0.0146 | Grad Norm: 0.00904957\n",
      "Epoch 1 | Step 588500 | Avg Loss: 0.0148 | Grad Norm: 0.01019176\n",
      "Epoch 1 | Step 588600 | Avg Loss: 0.0148 | Grad Norm: 0.00711095\n",
      "Epoch 1 | Step 588700 | Avg Loss: 0.0146 | Grad Norm: 0.00840581\n",
      "Epoch 1 | Step 588800 | Avg Loss: 0.0146 | Grad Norm: 0.00704223\n",
      "Epoch 1 | Step 588900 | Avg Loss: 0.0147 | Grad Norm: 0.00918097\n",
      "Epoch 1 | Step 589000 | Avg Loss: 0.0149 | Grad Norm: 0.00909818\n",
      "Epoch 1 | Step 589100 | Avg Loss: 0.0148 | Grad Norm: 0.01032571\n",
      "Epoch 1 | Step 589200 | Avg Loss: 0.0146 | Grad Norm: 0.00894410\n",
      "Epoch 1 | Step 589300 | Avg Loss: 0.0148 | Grad Norm: 0.00910263\n",
      "Epoch 1 | Step 589400 | Avg Loss: 0.0146 | Grad Norm: 0.00917712\n",
      "Epoch 1 | Step 589500 | Avg Loss: 0.0146 | Grad Norm: 0.00878331\n",
      "Epoch 1 | Step 589600 | Avg Loss: 0.0144 | Grad Norm: 0.00907616\n",
      "Epoch 1 | Step 589700 | Avg Loss: 0.0146 | Grad Norm: 0.00910390\n",
      "Epoch 1 | Step 589800 | Avg Loss: 0.0146 | Grad Norm: 0.00896779\n",
      "Epoch 1 | Step 589900 | Avg Loss: 0.0147 | Grad Norm: 0.00969216\n",
      "Epoch 1 | Step 590000 | Avg Loss: 0.0147 | Grad Norm: 0.00837221\n",
      "Epoch 1 | Step 590100 | Avg Loss: 0.0146 | Grad Norm: 0.01023600\n",
      "Epoch 1 | Step 590200 | Avg Loss: 0.0144 | Grad Norm: 0.00832161\n",
      "Epoch 1 | Step 590300 | Avg Loss: 0.0144 | Grad Norm: 0.00866715\n",
      "Epoch 1 | Step 590400 | Avg Loss: 0.0146 | Grad Norm: 0.00921822\n",
      "Epoch 1 | Step 590500 | Avg Loss: 0.0146 | Grad Norm: 0.01081467\n",
      "Epoch 1 | Step 590600 | Avg Loss: 0.0146 | Grad Norm: 0.00879302\n",
      "Epoch 1 | Step 590700 | Avg Loss: 0.0145 | Grad Norm: 0.00851244\n",
      "Epoch 1 | Step 590800 | Avg Loss: 0.0145 | Grad Norm: 0.00866163\n",
      "Epoch 1 | Step 590900 | Avg Loss: 0.0145 | Grad Norm: 0.00789243\n",
      "Epoch 1 | Step 591000 | Avg Loss: 0.0143 | Grad Norm: 0.00963729\n",
      "Epoch 1 | Step 591100 | Avg Loss: 0.0145 | Grad Norm: 0.00902161\n",
      "Epoch 1 | Step 591200 | Avg Loss: 0.0149 | Grad Norm: 0.00988326\n",
      "Epoch 1 | Step 591300 | Avg Loss: 0.0147 | Grad Norm: 0.00889797\n",
      "Epoch 1 | Step 591400 | Avg Loss: 0.0148 | Grad Norm: 0.00790735\n",
      "Epoch 1 | Step 591500 | Avg Loss: 0.0151 | Grad Norm: 0.00781461\n",
      "Epoch 1 | Step 591600 | Avg Loss: 0.0152 | Grad Norm: 0.00851419\n",
      "Epoch 1 | Step 591700 | Avg Loss: 0.0151 | Grad Norm: 0.00845890\n",
      "Epoch 1 | Step 591800 | Avg Loss: 0.0152 | Grad Norm: 0.00830270\n",
      "Epoch 1 | Step 591900 | Avg Loss: 0.0154 | Grad Norm: 0.00918295\n",
      "Epoch 1 | Step 592000 | Avg Loss: 0.0154 | Grad Norm: 0.00990334\n",
      "Epoch 1 | Step 592100 | Avg Loss: 0.0154 | Grad Norm: 0.00969724\n",
      "Epoch 1 | Step 592200 | Avg Loss: 0.0150 | Grad Norm: 0.00871306\n",
      "Epoch 1 | Step 592300 | Avg Loss: 0.0148 | Grad Norm: 0.01078632\n",
      "Epoch 1 | Step 592400 | Avg Loss: 0.0148 | Grad Norm: 0.01248791\n",
      "Epoch 1 | Step 592500 | Avg Loss: 0.0149 | Grad Norm: 0.00901936\n",
      "Epoch 1 | Step 592600 | Avg Loss: 0.0147 | Grad Norm: 0.00935836\n",
      "Epoch 1 | Step 592700 | Avg Loss: 0.0146 | Grad Norm: 0.00893296\n",
      "Epoch 1 | Step 592800 | Avg Loss: 0.0145 | Grad Norm: 0.00894895\n",
      "Epoch 1 | Step 592900 | Avg Loss: 0.0143 | Grad Norm: 0.00923938\n",
      "Epoch 1 | Step 593000 | Avg Loss: 0.0144 | Grad Norm: 0.00731988\n",
      "Epoch 1 | Step 593100 | Avg Loss: 0.0146 | Grad Norm: 0.00938122\n",
      "Epoch 1 | Step 593200 | Avg Loss: 0.0146 | Grad Norm: 0.00948920\n",
      "Epoch 1 | Step 593300 | Avg Loss: 0.0149 | Grad Norm: 0.01016536\n",
      "Epoch 1 | Step 593400 | Avg Loss: 0.0150 | Grad Norm: 0.00978044\n",
      "Epoch 1 | Step 593500 | Avg Loss: 0.0147 | Grad Norm: 0.01037724\n",
      "Epoch 1 | Step 593600 | Avg Loss: 0.0148 | Grad Norm: 0.00744194\n",
      "Epoch 1 | Step 593700 | Avg Loss: 0.0152 | Grad Norm: 0.00849165\n",
      "Epoch 1 | Step 593800 | Avg Loss: 0.0152 | Grad Norm: 0.01040937\n",
      "Epoch 1 | Step 593900 | Avg Loss: 0.0147 | Grad Norm: 0.00845695\n",
      "Epoch 1 | Step 594000 | Avg Loss: 0.0147 | Grad Norm: 0.01073341\n",
      "Epoch 1 | Step 594100 | Avg Loss: 0.0148 | Grad Norm: 0.00852479\n",
      "Epoch 1 | Step 594200 | Avg Loss: 0.0145 | Grad Norm: 0.00966616\n",
      "Epoch 1 | Step 594300 | Avg Loss: 0.0147 | Grad Norm: 0.00910827\n",
      "Epoch 1 | Step 594400 | Avg Loss: 0.0147 | Grad Norm: 0.00899117\n",
      "Epoch 1 | Step 594500 | Avg Loss: 0.0146 | Grad Norm: 0.00785878\n",
      "Epoch 1 | Step 594600 | Avg Loss: 0.0147 | Grad Norm: 0.01047447\n",
      "Epoch 1 | Step 594700 | Avg Loss: 0.0143 | Grad Norm: 0.01011430\n",
      "Epoch 1 | Step 594800 | Avg Loss: 0.0143 | Grad Norm: 0.00888267\n",
      "Epoch 1 | Step 594900 | Avg Loss: 0.0145 | Grad Norm: 0.00959832\n",
      "Epoch 1 | Step 595000 | Avg Loss: 0.0146 | Grad Norm: 0.00948876\n",
      "Epoch 1 | Step 595100 | Avg Loss: 0.0149 | Grad Norm: 0.00913751\n",
      "Epoch 1 | Step 595200 | Avg Loss: 0.0145 | Grad Norm: 0.00880636\n",
      "Epoch 1 | Step 595300 | Avg Loss: 0.0147 | Grad Norm: 0.00787783\n",
      "Epoch 1 | Step 595400 | Avg Loss: 0.0149 | Grad Norm: 0.00939919\n",
      "Epoch 1 | Step 595500 | Avg Loss: 0.0149 | Grad Norm: 0.00868685\n",
      "Epoch 1 | Step 595600 | Avg Loss: 0.0150 | Grad Norm: 0.00902888\n",
      "Epoch 1 | Step 595700 | Avg Loss: 0.0150 | Grad Norm: 0.00916412\n",
      "Epoch 1 | Step 595800 | Avg Loss: 0.0147 | Grad Norm: 0.01001288\n",
      "Epoch 1 | Step 595900 | Avg Loss: 0.0149 | Grad Norm: 0.00924678\n",
      "Epoch 1 | Step 596000 | Avg Loss: 0.0148 | Grad Norm: 0.00983674\n",
      "Epoch 1 | Step 596100 | Avg Loss: 0.0147 | Grad Norm: 0.00870293\n",
      "Epoch 1 | Step 596200 | Avg Loss: 0.0148 | Grad Norm: 0.00785711\n",
      "Epoch 1 | Step 596300 | Avg Loss: 0.0148 | Grad Norm: 0.00793231\n",
      "Epoch 1 | Step 596400 | Avg Loss: 0.0149 | Grad Norm: 0.00955357\n",
      "Epoch 1 | Step 596500 | Avg Loss: 0.0149 | Grad Norm: 0.00815592\n",
      "Epoch 1 | Step 596600 | Avg Loss: 0.0147 | Grad Norm: 0.00927962\n",
      "Epoch 1 | Step 596700 | Avg Loss: 0.0147 | Grad Norm: 0.00827352\n",
      "Epoch 1 | Step 596800 | Avg Loss: 0.0148 | Grad Norm: 0.00746726\n",
      "Epoch 1 | Step 596900 | Avg Loss: 0.0148 | Grad Norm: 0.00891542\n",
      "Epoch 1 | Step 597000 | Avg Loss: 0.0148 | Grad Norm: 0.00871179\n",
      "Epoch 1 | Step 597100 | Avg Loss: 0.0148 | Grad Norm: 0.00812920\n",
      "Epoch 1 | Step 597200 | Avg Loss: 0.0149 | Grad Norm: 0.00821289\n",
      "Epoch 1 | Step 597300 | Avg Loss: 0.0148 | Grad Norm: 0.00926142\n",
      "Epoch 1 | Step 597400 | Avg Loss: 0.0144 | Grad Norm: 0.00755989\n",
      "Epoch 1 | Step 597500 | Avg Loss: 0.0145 | Grad Norm: 0.00851302\n",
      "Epoch 1 | Step 597600 | Avg Loss: 0.0147 | Grad Norm: 0.00941567\n",
      "Epoch 1 | Step 597700 | Avg Loss: 0.0144 | Grad Norm: 0.00784205\n",
      "Epoch 1 | Step 597800 | Avg Loss: 0.0146 | Grad Norm: 0.00883719\n",
      "Epoch 1 | Step 597900 | Avg Loss: 0.0144 | Grad Norm: 0.00792498\n",
      "Epoch 1 | Step 598000 | Avg Loss: 0.0145 | Grad Norm: 0.00827126\n",
      "Epoch 1 | Step 598100 | Avg Loss: 0.0147 | Grad Norm: 0.00778102\n",
      "Epoch 1 | Step 598200 | Avg Loss: 0.0152 | Grad Norm: 0.00800416\n",
      "Epoch 1 | Step 598300 | Avg Loss: 0.0150 | Grad Norm: 0.00919501\n",
      "Epoch 1 | Step 598400 | Avg Loss: 0.0147 | Grad Norm: 0.00918521\n",
      "Epoch 1 | Step 598500 | Avg Loss: 0.0149 | Grad Norm: 0.00930361\n",
      "Epoch 1 | Step 598600 | Avg Loss: 0.0140 | Grad Norm: 0.00970170\n",
      "Epoch 1 | Step 598700 | Avg Loss: 0.0142 | Grad Norm: 0.00858262\n",
      "Epoch 1 | Step 598800 | Avg Loss: 0.0143 | Grad Norm: 0.00868223\n",
      "Epoch 1 | Step 598900 | Avg Loss: 0.0145 | Grad Norm: 0.01024344\n",
      "Epoch 1 | Step 599000 | Avg Loss: 0.0147 | Grad Norm: 0.00827658\n",
      "Epoch 1 | Step 599100 | Avg Loss: 0.0148 | Grad Norm: 0.00945142\n",
      "Epoch 1 | Step 599200 | Avg Loss: 0.0148 | Grad Norm: 0.00885258\n",
      "Epoch 1 | Step 599300 | Avg Loss: 0.0150 | Grad Norm: 0.00747790\n",
      "Epoch 1 | Step 599400 | Avg Loss: 0.0151 | Grad Norm: 0.00967395\n",
      "Epoch 1 | Step 599500 | Avg Loss: 0.0147 | Grad Norm: 0.01024293\n",
      "Epoch 1 | Step 599600 | Avg Loss: 0.0144 | Grad Norm: 0.00788974\n",
      "Epoch 1 | Step 599700 | Avg Loss: 0.0146 | Grad Norm: 0.00951180\n",
      "Epoch 1 | Step 599800 | Avg Loss: 0.0144 | Grad Norm: 0.00932499\n",
      "Epoch 1 | Step 599900 | Avg Loss: 0.0146 | Grad Norm: 0.00795686\n",
      "Epoch 1 | Step 600000 | Avg Loss: 0.0146 | Grad Norm: 0.00792613\n",
      "Saving model at step600000\n",
      "Epoch 1 | Step 600100 | Avg Loss: 0.0145 | Grad Norm: 0.00793579\n",
      "Epoch 1 | Step 600200 | Avg Loss: 0.0145 | Grad Norm: 0.00802975\n",
      "Epoch 1 | Step 600300 | Avg Loss: 0.0145 | Grad Norm: 0.00979992\n",
      "Epoch 1 | Step 600400 | Avg Loss: 0.0144 | Grad Norm: 0.00800287\n",
      "Epoch 1 | Step 600500 | Avg Loss: 0.0146 | Grad Norm: 0.00953172\n",
      "Epoch 1 | Step 600600 | Avg Loss: 0.0148 | Grad Norm: 0.00771835\n",
      "Epoch 1 | Step 600700 | Avg Loss: 0.0149 | Grad Norm: 0.00980011\n",
      "Epoch 1 | Step 600800 | Avg Loss: 0.0150 | Grad Norm: 0.00869595\n",
      "Epoch 1 | Step 600900 | Avg Loss: 0.0148 | Grad Norm: 0.00884743\n",
      "Epoch 1 | Step 601000 | Avg Loss: 0.0152 | Grad Norm: 0.00928305\n",
      "Epoch 1 | Step 601100 | Avg Loss: 0.0149 | Grad Norm: 0.01189469\n",
      "Epoch 1 | Step 601200 | Avg Loss: 0.0150 | Grad Norm: 0.01012094\n",
      "Epoch 1 | Step 601300 | Avg Loss: 0.0150 | Grad Norm: 0.00802165\n",
      "Epoch 1 | Step 601400 | Avg Loss: 0.0147 | Grad Norm: 0.00825740\n",
      "Epoch 1 | Step 601500 | Avg Loss: 0.0146 | Grad Norm: 0.00851204\n",
      "Epoch 1 | Step 601600 | Avg Loss: 0.0148 | Grad Norm: 0.00837380\n",
      "Epoch 1 | Step 601700 | Avg Loss: 0.0149 | Grad Norm: 0.01011803\n",
      "Epoch 1 | Step 601800 | Avg Loss: 0.0148 | Grad Norm: 0.00770482\n",
      "Epoch 1 | Step 601900 | Avg Loss: 0.0146 | Grad Norm: 0.00950914\n",
      "Epoch 1 | Step 602000 | Avg Loss: 0.0144 | Grad Norm: 0.00843733\n",
      "Epoch 1 | Step 602100 | Avg Loss: 0.0145 | Grad Norm: 0.00988962\n",
      "Epoch 1 | Step 602200 | Avg Loss: 0.0143 | Grad Norm: 0.00937387\n",
      "Epoch 1 | Step 602300 | Avg Loss: 0.0141 | Grad Norm: 0.01029685\n",
      "Epoch 1 | Step 602400 | Avg Loss: 0.0147 | Grad Norm: 0.00837827\n",
      "Epoch 1 | Step 602500 | Avg Loss: 0.0148 | Grad Norm: 0.00889674\n",
      "Epoch 1 | Step 602600 | Avg Loss: 0.0148 | Grad Norm: 0.00995147\n",
      "Epoch 1 | Step 602700 | Avg Loss: 0.0146 | Grad Norm: 0.00908211\n",
      "Epoch 1 | Step 602800 | Avg Loss: 0.0147 | Grad Norm: 0.00803525\n",
      "Epoch 1 | Step 602900 | Avg Loss: 0.0145 | Grad Norm: 0.00870666\n",
      "Epoch 1 | Step 603000 | Avg Loss: 0.0147 | Grad Norm: 0.00892862\n",
      "Epoch 1 | Step 603100 | Avg Loss: 0.0146 | Grad Norm: 0.00889554\n",
      "Epoch 1 | Step 603200 | Avg Loss: 0.0152 | Grad Norm: 0.00841917\n",
      "Epoch 1 | Step 603300 | Avg Loss: 0.0147 | Grad Norm: 0.00892781\n",
      "Epoch 1 | Step 603400 | Avg Loss: 0.0147 | Grad Norm: 0.00956488\n",
      "Epoch 1 | Step 603500 | Avg Loss: 0.0146 | Grad Norm: 0.01024026\n",
      "Epoch 1 | Step 603600 | Avg Loss: 0.0148 | Grad Norm: 0.01005697\n",
      "Epoch 1 | Step 603700 | Avg Loss: 0.0151 | Grad Norm: 0.00936551\n",
      "Epoch 1 | Step 603800 | Avg Loss: 0.0149 | Grad Norm: 0.00941809\n",
      "Epoch 1 | Step 603900 | Avg Loss: 0.0150 | Grad Norm: 0.00949088\n",
      "Epoch 1 | Step 604000 | Avg Loss: 0.0150 | Grad Norm: 0.01088569\n",
      "Epoch 1 | Step 604100 | Avg Loss: 0.0151 | Grad Norm: 0.00898577\n",
      "Epoch 1 | Step 604200 | Avg Loss: 0.0146 | Grad Norm: 0.00864860\n",
      "Epoch 1 | Step 604300 | Avg Loss: 0.0148 | Grad Norm: 0.00878831\n",
      "Epoch 1 | Step 604400 | Avg Loss: 0.0147 | Grad Norm: 0.00868468\n",
      "Epoch 1 | Step 604500 | Avg Loss: 0.0149 | Grad Norm: 0.00915891\n",
      "Epoch 1 | Step 604600 | Avg Loss: 0.0155 | Grad Norm: 0.00940878\n",
      "Epoch 1 | Step 604700 | Avg Loss: 0.0153 | Grad Norm: 0.00794153\n",
      "Epoch 1 | Step 604800 | Avg Loss: 0.0148 | Grad Norm: 0.00854197\n",
      "Epoch 1 | Step 604900 | Avg Loss: 0.0150 | Grad Norm: 0.00930336\n",
      "Epoch 1 | Step 605000 | Avg Loss: 0.0151 | Grad Norm: 0.01037061\n",
      "Epoch 1 | Step 605100 | Avg Loss: 0.0158 | Grad Norm: 0.00965952\n",
      "Epoch 1 | Step 605200 | Avg Loss: 0.0151 | Grad Norm: 0.00833136\n",
      "Epoch 1 | Step 605300 | Avg Loss: 0.0146 | Grad Norm: 0.00979819\n",
      "Epoch 1 | Step 605400 | Avg Loss: 0.0148 | Grad Norm: 0.01205527\n",
      "Epoch 1 | Step 605500 | Avg Loss: 0.0151 | Grad Norm: 0.01007239\n",
      "Epoch 1 | Step 605600 | Avg Loss: 0.0150 | Grad Norm: 0.01009298\n",
      "Epoch 1 | Step 605700 | Avg Loss: 0.0147 | Grad Norm: 0.00955075\n",
      "Epoch 1 | Step 605800 | Avg Loss: 0.0144 | Grad Norm: 0.00817177\n",
      "Epoch 1 | Step 605900 | Avg Loss: 0.0139 | Grad Norm: 0.00696403\n",
      "Epoch 1 | Step 606000 | Avg Loss: 0.0139 | Grad Norm: 0.00800338\n",
      "Epoch 1 | Step 606100 | Avg Loss: 0.0141 | Grad Norm: 0.00935275\n",
      "Epoch 1 | Step 606200 | Avg Loss: 0.0146 | Grad Norm: 0.00900782\n",
      "Epoch 1 | Step 606300 | Avg Loss: 0.0150 | Grad Norm: 0.00972079\n",
      "Epoch 1 | Step 606400 | Avg Loss: 0.0149 | Grad Norm: 0.01135301\n",
      "Epoch 1 | Step 606500 | Avg Loss: 0.0148 | Grad Norm: 0.00818579\n",
      "Epoch 1 | Step 606600 | Avg Loss: 0.0151 | Grad Norm: 0.01193178\n",
      "Epoch 1 | Step 606700 | Avg Loss: 0.0146 | Grad Norm: 0.00901008\n",
      "Epoch 1 | Step 606800 | Avg Loss: 0.0144 | Grad Norm: 0.00858959\n",
      "Epoch 1 | Step 606900 | Avg Loss: 0.0148 | Grad Norm: 0.00914264\n",
      "Epoch 1 | Step 607000 | Avg Loss: 0.0146 | Grad Norm: 0.00916429\n",
      "Epoch 1 | Step 607100 | Avg Loss: 0.0147 | Grad Norm: 0.00955274\n",
      "Epoch 1 | Step 607200 | Avg Loss: 0.0147 | Grad Norm: 0.00752011\n",
      "Epoch 1 | Step 607300 | Avg Loss: 0.0144 | Grad Norm: 0.00866536\n",
      "Epoch 1 | Step 607400 | Avg Loss: 0.0149 | Grad Norm: 0.00828126\n",
      "Epoch 1 | Step 607500 | Avg Loss: 0.0151 | Grad Norm: 0.00810437\n",
      "Epoch 1 | Step 607600 | Avg Loss: 0.0153 | Grad Norm: 0.00813043\n",
      "Epoch 1 | Step 607700 | Avg Loss: 0.0150 | Grad Norm: 0.00812775\n",
      "Epoch 1 | Step 607800 | Avg Loss: 0.0150 | Grad Norm: 0.00986911\n",
      "Epoch 1 | Step 607900 | Avg Loss: 0.0150 | Grad Norm: 0.01041712\n",
      "Epoch 1 | Step 608000 | Avg Loss: 0.0152 | Grad Norm: 0.00843544\n",
      "Epoch 1 | Step 608100 | Avg Loss: 0.0151 | Grad Norm: 0.00875410\n",
      "Epoch 1 | Step 608200 | Avg Loss: 0.0146 | Grad Norm: 0.00879216\n",
      "Epoch 1 | Step 608300 | Avg Loss: 0.0147 | Grad Norm: 0.01076128\n",
      "Epoch 1 | Step 608400 | Avg Loss: 0.0148 | Grad Norm: 0.00854264\n",
      "Epoch 1 | Step 608500 | Avg Loss: 0.0148 | Grad Norm: 0.00964275\n",
      "Epoch 1 | Step 608600 | Avg Loss: 0.0150 | Grad Norm: 0.00907536\n",
      "Epoch 1 | Step 608700 | Avg Loss: 0.0151 | Grad Norm: 0.01051995\n",
      "Epoch 1 | Step 608800 | Avg Loss: 0.0151 | Grad Norm: 0.00811270\n",
      "Epoch 1 | Step 608900 | Avg Loss: 0.0149 | Grad Norm: 0.00737537\n",
      "Epoch 1 | Step 609000 | Avg Loss: 0.0147 | Grad Norm: 0.00796426\n",
      "Epoch 1 | Step 609100 | Avg Loss: 0.0145 | Grad Norm: 0.00933540\n",
      "Epoch 1 | Step 609200 | Avg Loss: 0.0146 | Grad Norm: 0.00957821\n",
      "Epoch 1 | Step 609300 | Avg Loss: 0.0145 | Grad Norm: 0.00880092\n",
      "Epoch 1 | Step 609400 | Avg Loss: 0.0147 | Grad Norm: 0.00907413\n",
      "Epoch 1 | Step 609500 | Avg Loss: 0.0144 | Grad Norm: 0.00829032\n",
      "Epoch 1 | Step 609600 | Avg Loss: 0.0143 | Grad Norm: 0.00843321\n",
      "Epoch 1 | Step 609700 | Avg Loss: 0.0147 | Grad Norm: 0.00859781\n",
      "Epoch 1 | Step 609800 | Avg Loss: 0.0145 | Grad Norm: 0.00812443\n",
      "Epoch 1 | Step 609900 | Avg Loss: 0.0148 | Grad Norm: 0.00933754\n",
      "Epoch 1 | Step 610000 | Avg Loss: 0.0150 | Grad Norm: 0.01000170\n",
      "Epoch 1 | Step 610100 | Avg Loss: 0.0153 | Grad Norm: 0.00945484\n",
      "Epoch 1 | Step 610200 | Avg Loss: 0.0152 | Grad Norm: 0.00911119\n",
      "Epoch 1 | Step 610300 | Avg Loss: 0.0151 | Grad Norm: 0.00864686\n",
      "Epoch 1 | Step 610400 | Avg Loss: 0.0147 | Grad Norm: 0.00931283\n",
      "Epoch 1 | Step 610500 | Avg Loss: 0.0147 | Grad Norm: 0.00898523\n",
      "Epoch 1 | Step 610600 | Avg Loss: 0.0149 | Grad Norm: 0.00934443\n",
      "Epoch 1 | Step 610700 | Avg Loss: 0.0148 | Grad Norm: 0.00877003\n",
      "Epoch 1 | Step 610800 | Avg Loss: 0.0148 | Grad Norm: 0.00949388\n",
      "Epoch 1 | Step 610900 | Avg Loss: 0.0147 | Grad Norm: 0.00913633\n",
      "Epoch 1 | Step 611000 | Avg Loss: 0.0149 | Grad Norm: 0.01011757\n",
      "Epoch 1 | Step 611100 | Avg Loss: 0.0146 | Grad Norm: 0.00821160\n",
      "Epoch 1 | Step 611200 | Avg Loss: 0.0146 | Grad Norm: 0.01015119\n",
      "Epoch 1 | Step 611300 | Avg Loss: 0.0145 | Grad Norm: 0.00929399\n",
      "Epoch 1 | Step 611400 | Avg Loss: 0.0143 | Grad Norm: 0.00982975\n",
      "Epoch 1 | Step 611500 | Avg Loss: 0.0149 | Grad Norm: 0.00817222\n",
      "Epoch 1 | Step 611600 | Avg Loss: 0.0150 | Grad Norm: 0.00960950\n",
      "Epoch 1 | Step 611700 | Avg Loss: 0.0151 | Grad Norm: 0.00868359\n",
      "Epoch 1 | Step 611800 | Avg Loss: 0.0147 | Grad Norm: 0.00916087\n",
      "Epoch 1 | Step 611900 | Avg Loss: 0.0151 | Grad Norm: 0.00971870\n",
      "Epoch 1 | Step 612000 | Avg Loss: 0.0147 | Grad Norm: 0.00874351\n",
      "Epoch 1 | Step 612100 | Avg Loss: 0.0150 | Grad Norm: 0.00811202\n",
      "Epoch 1 | Step 612200 | Avg Loss: 0.0148 | Grad Norm: 0.00943232\n",
      "Epoch 1 | Step 612300 | Avg Loss: 0.0144 | Grad Norm: 0.00834523\n",
      "Epoch 1 | Step 612400 | Avg Loss: 0.0146 | Grad Norm: 0.00857467\n",
      "Epoch 1 | Step 612500 | Avg Loss: 0.0145 | Grad Norm: 0.00802099\n",
      "Epoch 1 | Step 612600 | Avg Loss: 0.0148 | Grad Norm: 0.00876276\n",
      "Epoch 1 | Step 612700 | Avg Loss: 0.0147 | Grad Norm: 0.00732739\n",
      "Epoch 1 | Step 612800 | Avg Loss: 0.0146 | Grad Norm: 0.00899895\n",
      "Epoch 1 | Step 612900 | Avg Loss: 0.0149 | Grad Norm: 0.00746713\n",
      "Epoch 1 | Step 613000 | Avg Loss: 0.0148 | Grad Norm: 0.00758786\n",
      "Epoch 1 | Step 613100 | Avg Loss: 0.0151 | Grad Norm: 0.00904737\n",
      "Epoch 1 | Step 613200 | Avg Loss: 0.0149 | Grad Norm: 0.00877467\n",
      "Epoch 1 | Step 613300 | Avg Loss: 0.0151 | Grad Norm: 0.01010228\n",
      "Epoch 1 | Step 613400 | Avg Loss: 0.0147 | Grad Norm: 0.00990679\n",
      "Epoch 1 | Step 613500 | Avg Loss: 0.0147 | Grad Norm: 0.01010963\n",
      "Epoch 1 | Step 613600 | Avg Loss: 0.0151 | Grad Norm: 0.00928491\n",
      "Epoch 1 | Step 613700 | Avg Loss: 0.0152 | Grad Norm: 0.00803227\n",
      "Epoch 1 | Step 613800 | Avg Loss: 0.0150 | Grad Norm: 0.00845329\n",
      "Epoch 1 | Step 613900 | Avg Loss: 0.0147 | Grad Norm: 0.00914977\n",
      "Epoch 1 | Step 614000 | Avg Loss: 0.0149 | Grad Norm: 0.00813880\n",
      "Epoch 1 | Step 614100 | Avg Loss: 0.0148 | Grad Norm: 0.01049581\n",
      "Epoch 1 | Step 614200 | Avg Loss: 0.0150 | Grad Norm: 0.00872969\n",
      "Epoch 1 | Step 614300 | Avg Loss: 0.0150 | Grad Norm: 0.00864292\n",
      "Epoch 1 | Step 614400 | Avg Loss: 0.0148 | Grad Norm: 0.00939869\n",
      "Epoch 1 | Step 614500 | Avg Loss: 0.0148 | Grad Norm: 0.00864575\n",
      "Epoch 1 | Step 614600 | Avg Loss: 0.0150 | Grad Norm: 0.01135682\n",
      "Epoch 1 | Step 614700 | Avg Loss: 0.0148 | Grad Norm: 0.00895717\n",
      "Epoch 1 | Step 614800 | Avg Loss: 0.0148 | Grad Norm: 0.00795159\n",
      "Epoch 1 | Step 614900 | Avg Loss: 0.0149 | Grad Norm: 0.01077885\n",
      "Epoch 1 | Step 615000 | Avg Loss: 0.0151 | Grad Norm: 0.00860554\n",
      "Epoch 1 | Step 615100 | Avg Loss: 0.0149 | Grad Norm: 0.00946232\n",
      "Epoch 1 | Step 615200 | Avg Loss: 0.0148 | Grad Norm: 0.00939516\n",
      "Epoch 1 | Step 615300 | Avg Loss: 0.0149 | Grad Norm: 0.00966596\n",
      "Epoch 1 | Step 615400 | Avg Loss: 0.0146 | Grad Norm: 0.00785799\n",
      "Epoch 1 | Step 615500 | Avg Loss: 0.0147 | Grad Norm: 0.00878054\n",
      "Epoch 1 | Step 615600 | Avg Loss: 0.0149 | Grad Norm: 0.00888307\n",
      "Epoch 1 | Step 615700 | Avg Loss: 0.0148 | Grad Norm: 0.00743983\n",
      "Epoch 1 | Step 615800 | Avg Loss: 0.0150 | Grad Norm: 0.01091658\n",
      "Epoch 1 | Step 615900 | Avg Loss: 0.0152 | Grad Norm: 0.00886344\n",
      "Epoch 1 | Step 616000 | Avg Loss: 0.0153 | Grad Norm: 0.00877604\n",
      "Epoch 1 | Step 616100 | Avg Loss: 0.0152 | Grad Norm: 0.01004249\n",
      "Epoch 1 | Step 616200 | Avg Loss: 0.0148 | Grad Norm: 0.00758507\n",
      "Epoch 1 | Step 616300 | Avg Loss: 0.0147 | Grad Norm: 0.00987136\n",
      "Epoch 1 | Step 616400 | Avg Loss: 0.0148 | Grad Norm: 0.00821920\n",
      "Epoch 1 | Step 616500 | Avg Loss: 0.0151 | Grad Norm: 0.00732971\n",
      "Epoch 1 | Step 616600 | Avg Loss: 0.0151 | Grad Norm: 0.01042878\n",
      "Epoch 1 | Step 616700 | Avg Loss: 0.0150 | Grad Norm: 0.00821892\n",
      "Epoch 1 | Step 616800 | Avg Loss: 0.0150 | Grad Norm: 0.00931993\n",
      "Epoch 1 | Step 616900 | Avg Loss: 0.0152 | Grad Norm: 0.01005328\n",
      "Epoch 1 | Step 617000 | Avg Loss: 0.0154 | Grad Norm: 0.00847295\n",
      "Epoch 1 | Step 617100 | Avg Loss: 0.0148 | Grad Norm: 0.01028043\n",
      "Epoch 1 | Step 617200 | Avg Loss: 0.0147 | Grad Norm: 0.00888187\n",
      "Epoch 1 | Step 617300 | Avg Loss: 0.0146 | Grad Norm: 0.00826847\n",
      "Epoch 1 | Step 617400 | Avg Loss: 0.0149 | Grad Norm: 0.00962740\n",
      "Epoch 1 | Step 617500 | Avg Loss: 0.0148 | Grad Norm: 0.01033828\n",
      "Epoch 1 | Step 617600 | Avg Loss: 0.0150 | Grad Norm: 0.00772586\n",
      "Epoch 1 | Step 617700 | Avg Loss: 0.0149 | Grad Norm: 0.00858141\n",
      "Epoch 1 | Step 617800 | Avg Loss: 0.0148 | Grad Norm: 0.00853124\n",
      "Epoch 1 | Step 617900 | Avg Loss: 0.0154 | Grad Norm: 0.00951351\n",
      "Epoch 1 | Step 618000 | Avg Loss: 0.0148 | Grad Norm: 0.00885778\n",
      "Epoch 1 | Step 618100 | Avg Loss: 0.0151 | Grad Norm: 0.00919436\n",
      "Epoch 1 | Step 618200 | Avg Loss: 0.0149 | Grad Norm: 0.00884853\n",
      "Epoch 1 | Step 618300 | Avg Loss: 0.0146 | Grad Norm: 0.00962701\n",
      "Epoch 1 | Step 618400 | Avg Loss: 0.0146 | Grad Norm: 0.00744040\n",
      "Epoch 1 | Step 618500 | Avg Loss: 0.0145 | Grad Norm: 0.00853008\n",
      "Epoch 1 | Step 618600 | Avg Loss: 0.0146 | Grad Norm: 0.00809453\n",
      "Epoch 1 | Step 618700 | Avg Loss: 0.0146 | Grad Norm: 0.00846812\n",
      "Epoch 1 | Step 618800 | Avg Loss: 0.0148 | Grad Norm: 0.00955922\n",
      "Epoch 1 | Step 618900 | Avg Loss: 0.0147 | Grad Norm: 0.00815583\n",
      "Epoch 1 | Step 619000 | Avg Loss: 0.0146 | Grad Norm: 0.00847826\n",
      "Epoch 1 | Step 619100 | Avg Loss: 0.0145 | Grad Norm: 0.00910692\n",
      "Epoch 1 | Step 619200 | Avg Loss: 0.0146 | Grad Norm: 0.00846436\n",
      "Epoch 1 | Step 619300 | Avg Loss: 0.0149 | Grad Norm: 0.00919203\n",
      "Epoch 1 | Step 619400 | Avg Loss: 0.0150 | Grad Norm: 0.00780335\n",
      "Epoch 1 | Step 619500 | Avg Loss: 0.0150 | Grad Norm: 0.00858686\n",
      "Epoch 1 | Step 619600 | Avg Loss: 0.0154 | Grad Norm: 0.00839622\n",
      "Epoch 1 | Step 619700 | Avg Loss: 0.0154 | Grad Norm: 0.00958015\n",
      "Epoch 1 | Step 619800 | Avg Loss: 0.0153 | Grad Norm: 0.00792834\n",
      "Epoch 1 | Step 619900 | Avg Loss: 0.0151 | Grad Norm: 0.00914756\n",
      "Epoch 1 | Step 620000 | Avg Loss: 0.0151 | Grad Norm: 0.01045701\n",
      "Epoch 1 | Step 620100 | Avg Loss: 0.0150 | Grad Norm: 0.01037351\n",
      "Epoch 1 | Step 620200 | Avg Loss: 0.0149 | Grad Norm: 0.00938121\n",
      "Epoch 1 | Step 620300 | Avg Loss: 0.0149 | Grad Norm: 0.00962921\n",
      "Epoch 1 | Step 620400 | Avg Loss: 0.0152 | Grad Norm: 0.01067230\n",
      "Epoch 1 | Step 620500 | Avg Loss: 0.0151 | Grad Norm: 0.00862612\n",
      "Epoch 1 | Step 620600 | Avg Loss: 0.0148 | Grad Norm: 0.00909236\n",
      "Epoch 1 | Step 620700 | Avg Loss: 0.0148 | Grad Norm: 0.00732971\n",
      "Epoch 1 | Step 620800 | Avg Loss: 0.0151 | Grad Norm: 0.00999199\n",
      "Epoch 1 | Step 620900 | Avg Loss: 0.0151 | Grad Norm: 0.00811719\n",
      "Epoch 1 | Step 621000 | Avg Loss: 0.0152 | Grad Norm: 0.00978828\n",
      "Epoch 1 | Step 621100 | Avg Loss: 0.0150 | Grad Norm: 0.00789940\n",
      "Epoch 1 | Step 621200 | Avg Loss: 0.0146 | Grad Norm: 0.00856879\n",
      "Epoch 1 | Step 621300 | Avg Loss: 0.0145 | Grad Norm: 0.00899379\n",
      "Epoch 1 | Step 621400 | Avg Loss: 0.0145 | Grad Norm: 0.00981739\n",
      "Epoch 1 | Step 621500 | Avg Loss: 0.0144 | Grad Norm: 0.00763712\n",
      "Epoch 1 | Step 621600 | Avg Loss: 0.0146 | Grad Norm: 0.01019355\n",
      "Epoch 1 | Step 621700 | Avg Loss: 0.0149 | Grad Norm: 0.01063110\n",
      "Epoch 1 | Step 621800 | Avg Loss: 0.0150 | Grad Norm: 0.00919932\n",
      "Epoch 1 | Step 621900 | Avg Loss: 0.0149 | Grad Norm: 0.00829062\n",
      "Epoch 1 | Step 622000 | Avg Loss: 0.0148 | Grad Norm: 0.00923702\n",
      "Epoch 1 | Step 622100 | Avg Loss: 0.0146 | Grad Norm: 0.00850401\n",
      "Epoch 1 | Step 622200 | Avg Loss: 0.0149 | Grad Norm: 0.00823357\n",
      "Epoch 1 | Step 622300 | Avg Loss: 0.0145 | Grad Norm: 0.01039822\n",
      "Epoch 1 | Step 622400 | Avg Loss: 0.0146 | Grad Norm: 0.00833592\n",
      "Epoch 1 | Step 622500 | Avg Loss: 0.0146 | Grad Norm: 0.00827231\n",
      "Epoch 1 | Step 622600 | Avg Loss: 0.0147 | Grad Norm: 0.01000616\n",
      "Epoch 1 | Step 622700 | Avg Loss: 0.0146 | Grad Norm: 0.00789861\n",
      "Epoch 1 | Step 622800 | Avg Loss: 0.0144 | Grad Norm: 0.01018427\n",
      "Epoch 1 | Step 622900 | Avg Loss: 0.0146 | Grad Norm: 0.00852054\n",
      "Epoch 1 | Step 623000 | Avg Loss: 0.0145 | Grad Norm: 0.01103751\n",
      "Epoch 1 | Step 623100 | Avg Loss: 0.0144 | Grad Norm: 0.00860604\n",
      "Epoch 1 | Step 623200 | Avg Loss: 0.0141 | Grad Norm: 0.00945551\n",
      "Epoch 1 | Step 623300 | Avg Loss: 0.0144 | Grad Norm: 0.00740865\n",
      "Epoch 1 | Step 623400 | Avg Loss: 0.0142 | Grad Norm: 0.00891928\n",
      "Epoch 1 | Step 623500 | Avg Loss: 0.0141 | Grad Norm: 0.00778221\n",
      "Epoch 1 | Step 623600 | Avg Loss: 0.0139 | Grad Norm: 0.00831073\n",
      "Epoch 1 | Step 623700 | Avg Loss: 0.0143 | Grad Norm: 0.00851594\n",
      "Epoch 1 | Step 623800 | Avg Loss: 0.0148 | Grad Norm: 0.00892762\n",
      "Epoch 1 | Step 623900 | Avg Loss: 0.0149 | Grad Norm: 0.00877708\n",
      "Epoch 1 | Step 624000 | Avg Loss: 0.0144 | Grad Norm: 0.00938425\n",
      "Epoch 1 | Step 624100 | Avg Loss: 0.0143 | Grad Norm: 0.00868780\n",
      "Epoch 1 | Step 624200 | Avg Loss: 0.0144 | Grad Norm: 0.00857173\n",
      "Epoch 1 | Step 624300 | Avg Loss: 0.0147 | Grad Norm: 0.01049582\n",
      "Epoch 1 | Step 624400 | Avg Loss: 0.0145 | Grad Norm: 0.00895120\n",
      "Epoch 1 | Step 624500 | Avg Loss: 0.0150 | Grad Norm: 0.00918841\n",
      "Epoch 1 | Step 624600 | Avg Loss: 0.0146 | Grad Norm: 0.00977431\n",
      "Epoch 1 | Step 624700 | Avg Loss: 0.0146 | Grad Norm: 0.00909798\n",
      "Epoch 1 | Step 624800 | Avg Loss: 0.0147 | Grad Norm: 0.00895871\n",
      "Epoch 1 | Step 624900 | Avg Loss: 0.0147 | Grad Norm: 0.00946741\n",
      "Epoch 1 | Step 625000 | Avg Loss: 0.0145 | Grad Norm: 0.00832217\n",
      "Epoch 1 | Step 625100 | Avg Loss: 0.0141 | Grad Norm: 0.00872327\n",
      "Epoch 1 | Step 625200 | Avg Loss: 0.0142 | Grad Norm: 0.00824654\n",
      "Epoch 1 | Step 625300 | Avg Loss: 0.0140 | Grad Norm: 0.00702296\n",
      "Epoch 1 | Step 625400 | Avg Loss: 0.0140 | Grad Norm: 0.00825293\n",
      "Epoch 1 | Step 625500 | Avg Loss: 0.0147 | Grad Norm: 0.00873067\n",
      "Epoch 1 | Step 625600 | Avg Loss: 0.0146 | Grad Norm: 0.00908530\n",
      "Epoch 1 | Step 625700 | Avg Loss: 0.0145 | Grad Norm: 0.00988647\n",
      "Epoch 1 | Step 625800 | Avg Loss: 0.0143 | Grad Norm: 0.00889724\n",
      "Epoch 1 | Step 625900 | Avg Loss: 0.0141 | Grad Norm: 0.01010948\n",
      "Epoch 1 | Step 626000 | Avg Loss: 0.0145 | Grad Norm: 0.00855196\n",
      "Epoch 1 | Step 626100 | Avg Loss: 0.0147 | Grad Norm: 0.00989619\n",
      "Epoch 1 | Step 626200 | Avg Loss: 0.0147 | Grad Norm: 0.01032040\n",
      "Epoch 1 | Step 626300 | Avg Loss: 0.0143 | Grad Norm: 0.00790294\n",
      "Epoch 1 | Step 626400 | Avg Loss: 0.0147 | Grad Norm: 0.00948909\n",
      "Epoch 1 | Step 626500 | Avg Loss: 0.0150 | Grad Norm: 0.00839712\n",
      "Epoch 1 | Step 626600 | Avg Loss: 0.0150 | Grad Norm: 0.00898937\n",
      "Epoch 1 | Step 626700 | Avg Loss: 0.0147 | Grad Norm: 0.00973912\n",
      "Epoch 1 | Step 626800 | Avg Loss: 0.0147 | Grad Norm: 0.00905360\n",
      "Epoch 1 | Step 626900 | Avg Loss: 0.0149 | Grad Norm: 0.01001868\n",
      "Epoch 1 | Step 627000 | Avg Loss: 0.0149 | Grad Norm: 0.00960486\n",
      "Epoch 1 | Step 627100 | Avg Loss: 0.0146 | Grad Norm: 0.00880894\n",
      "Epoch 1 | Step 627200 | Avg Loss: 0.0144 | Grad Norm: 0.00944719\n",
      "Epoch 1 | Step 627300 | Avg Loss: 0.0147 | Grad Norm: 0.00943962\n",
      "Epoch 1 | Step 627400 | Avg Loss: 0.0148 | Grad Norm: 0.00895325\n",
      "Epoch 1 | Step 627500 | Avg Loss: 0.0146 | Grad Norm: 0.00804259\n",
      "Epoch 1 | Step 627600 | Avg Loss: 0.0143 | Grad Norm: 0.01009221\n",
      "Epoch 1 | Step 627700 | Avg Loss: 0.0148 | Grad Norm: 0.01011425\n",
      "Epoch 1 | Step 627800 | Avg Loss: 0.0145 | Grad Norm: 0.00864000\n",
      "Epoch 1 | Step 627900 | Avg Loss: 0.0149 | Grad Norm: 0.00768123\n",
      "Epoch 1 | Step 628000 | Avg Loss: 0.0147 | Grad Norm: 0.00893511\n",
      "Epoch 1 | Step 628100 | Avg Loss: 0.0144 | Grad Norm: 0.00818245\n",
      "Epoch 1 | Step 628200 | Avg Loss: 0.0144 | Grad Norm: 0.00992319\n",
      "Epoch 1 | Step 628300 | Avg Loss: 0.0142 | Grad Norm: 0.00752636\n",
      "Epoch 1 | Step 628400 | Avg Loss: 0.0141 | Grad Norm: 0.01001470\n",
      "Epoch 1 | Step 628500 | Avg Loss: 0.0147 | Grad Norm: 0.00920451\n",
      "Epoch 1 | Step 628600 | Avg Loss: 0.0150 | Grad Norm: 0.00837693\n",
      "Epoch 1 | Step 628700 | Avg Loss: 0.0150 | Grad Norm: 0.00857829\n",
      "Epoch 1 | Step 628800 | Avg Loss: 0.0148 | Grad Norm: 0.00964664\n",
      "Epoch 1 | Step 628900 | Avg Loss: 0.0144 | Grad Norm: 0.00953759\n",
      "Epoch 1 | Step 629000 | Avg Loss: 0.0144 | Grad Norm: 0.00867039\n",
      "Epoch 1 | Step 629100 | Avg Loss: 0.0146 | Grad Norm: 0.00909435\n",
      "Epoch 1 | Step 629200 | Avg Loss: 0.0148 | Grad Norm: 0.00809579\n",
      "Epoch 1 | Step 629300 | Avg Loss: 0.0148 | Grad Norm: 0.00881774\n",
      "Epoch 1 | Step 629400 | Avg Loss: 0.0148 | Grad Norm: 0.00783263\n",
      "Epoch 1 | Step 629500 | Avg Loss: 0.0148 | Grad Norm: 0.00876965\n",
      "Epoch 1 | Step 629600 | Avg Loss: 0.0147 | Grad Norm: 0.00877469\n",
      "Epoch 1 | Step 629700 | Avg Loss: 0.0151 | Grad Norm: 0.00800883\n",
      "Epoch 1 | Step 629800 | Avg Loss: 0.0153 | Grad Norm: 0.00840128\n",
      "Epoch 1 | Step 629900 | Avg Loss: 0.0151 | Grad Norm: 0.00925229\n",
      "Epoch 1 | Step 630000 | Avg Loss: 0.0149 | Grad Norm: 0.00961427\n",
      "Epoch 1 | Step 630100 | Avg Loss: 0.0150 | Grad Norm: 0.01028107\n",
      "Epoch 1 | Step 630200 | Avg Loss: 0.0146 | Grad Norm: 0.00891270\n",
      "Epoch 1 | Step 630300 | Avg Loss: 0.0149 | Grad Norm: 0.00835296\n",
      "Epoch 1 | Step 630400 | Avg Loss: 0.0150 | Grad Norm: 0.01000499\n",
      "Epoch 1 | Step 630500 | Avg Loss: 0.0150 | Grad Norm: 0.00926348\n",
      "Epoch 1 | Step 630600 | Avg Loss: 0.0149 | Grad Norm: 0.00865046\n",
      "Epoch 1 | Step 630700 | Avg Loss: 0.0149 | Grad Norm: 0.01021641\n",
      "Epoch 1 | Step 630800 | Avg Loss: 0.0149 | Grad Norm: 0.00930734\n",
      "Epoch 1 | Step 630900 | Avg Loss: 0.0151 | Grad Norm: 0.01008716\n",
      "Epoch 1 | Step 631000 | Avg Loss: 0.0155 | Grad Norm: 0.00911006\n",
      "Epoch 1 | Step 631100 | Avg Loss: 0.0152 | Grad Norm: 0.00868016\n",
      "Epoch 1 | Step 631200 | Avg Loss: 0.0153 | Grad Norm: 0.00942098\n",
      "Epoch 1 | Step 631300 | Avg Loss: 0.0152 | Grad Norm: 0.00973535\n",
      "Epoch 1 | Step 631400 | Avg Loss: 0.0157 | Grad Norm: 0.00996675\n",
      "Epoch 1 | Step 631500 | Avg Loss: 0.0155 | Grad Norm: 0.00892456\n",
      "Epoch 1 | Step 631600 | Avg Loss: 0.0150 | Grad Norm: 0.00880194\n",
      "Epoch 1 | Step 631700 | Avg Loss: 0.0152 | Grad Norm: 0.01119083\n",
      "Epoch 1 | Step 631800 | Avg Loss: 0.0147 | Grad Norm: 0.00894606\n",
      "Epoch 1 | Step 631900 | Avg Loss: 0.0150 | Grad Norm: 0.00919561\n",
      "Epoch 1 | Step 632000 | Avg Loss: 0.0145 | Grad Norm: 0.00911720\n",
      "Epoch 1 | Step 632100 | Avg Loss: 0.0145 | Grad Norm: 0.00793058\n",
      "Epoch 1 | Step 632200 | Avg Loss: 0.0145 | Grad Norm: 0.00851467\n",
      "Epoch 1 | Step 632300 | Avg Loss: 0.0145 | Grad Norm: 0.00843103\n",
      "Epoch 1 | Step 632400 | Avg Loss: 0.0146 | Grad Norm: 0.01045834\n",
      "Epoch 1 | Step 632500 | Avg Loss: 0.0143 | Grad Norm: 0.00941525\n",
      "Epoch 1 | Step 632600 | Avg Loss: 0.0146 | Grad Norm: 0.00916786\n",
      "Epoch 1 | Step 632700 | Avg Loss: 0.0148 | Grad Norm: 0.00892553\n",
      "Epoch 1 | Step 632800 | Avg Loss: 0.0153 | Grad Norm: 0.00829562\n",
      "Epoch 1 | Step 632900 | Avg Loss: 0.0151 | Grad Norm: 0.00833700\n",
      "Epoch 1 | Step 633000 | Avg Loss: 0.0149 | Grad Norm: 0.00891949\n",
      "Epoch 1 | Step 633100 | Avg Loss: 0.0145 | Grad Norm: 0.00895196\n",
      "Epoch 1 | Step 633200 | Avg Loss: 0.0144 | Grad Norm: 0.01003497\n",
      "Epoch 1 | Step 633300 | Avg Loss: 0.0145 | Grad Norm: 0.00765412\n",
      "Epoch 1 | Step 633400 | Avg Loss: 0.0148 | Grad Norm: 0.00996843\n",
      "Epoch 1 | Step 633500 | Avg Loss: 0.0151 | Grad Norm: 0.00977538\n",
      "Epoch 1 | Step 633600 | Avg Loss: 0.0150 | Grad Norm: 0.00806608\n",
      "Epoch 1 | Step 633700 | Avg Loss: 0.0145 | Grad Norm: 0.00846261\n",
      "Epoch 1 | Step 633800 | Avg Loss: 0.0144 | Grad Norm: 0.00797848\n",
      "Epoch 1 | Step 633900 | Avg Loss: 0.0144 | Grad Norm: 0.00790481\n",
      "Epoch 1 | Step 634000 | Avg Loss: 0.0145 | Grad Norm: 0.00935886\n",
      "Epoch 1 | Step 634100 | Avg Loss: 0.0145 | Grad Norm: 0.00931120\n",
      "Epoch 1 | Step 634200 | Avg Loss: 0.0146 | Grad Norm: 0.00846590\n",
      "Epoch 1 | Step 634300 | Avg Loss: 0.0146 | Grad Norm: 0.00899437\n",
      "Epoch 1 | Step 634400 | Avg Loss: 0.0148 | Grad Norm: 0.00851175\n",
      "Epoch 1 | Step 634500 | Avg Loss: 0.0149 | Grad Norm: 0.01090003\n",
      "Epoch 1 | Step 634600 | Avg Loss: 0.0147 | Grad Norm: 0.00874258\n",
      "Epoch 1 | Step 634700 | Avg Loss: 0.0148 | Grad Norm: 0.00886451\n",
      "Epoch 1 | Step 634800 | Avg Loss: 0.0151 | Grad Norm: 0.00980764\n",
      "Epoch 1 | Step 634900 | Avg Loss: 0.0154 | Grad Norm: 0.00900438\n",
      "Epoch 1 | Step 635000 | Avg Loss: 0.0151 | Grad Norm: 0.00844999\n",
      "Epoch 1 | Step 635100 | Avg Loss: 0.0148 | Grad Norm: 0.00884556\n",
      "Epoch 1 | Step 635200 | Avg Loss: 0.0150 | Grad Norm: 0.00857080\n",
      "Epoch 1 | Step 635300 | Avg Loss: 0.0150 | Grad Norm: 0.00857880\n",
      "Epoch 1 | Step 635400 | Avg Loss: 0.0147 | Grad Norm: 0.00781189\n",
      "Epoch 1 | Step 635500 | Avg Loss: 0.0146 | Grad Norm: 0.00900910\n",
      "Epoch 1 | Step 635600 | Avg Loss: 0.0151 | Grad Norm: 0.00994949\n",
      "Epoch 1 | Step 635700 | Avg Loss: 0.0150 | Grad Norm: 0.00904903\n",
      "Epoch 1 | Step 635800 | Avg Loss: 0.0148 | Grad Norm: 0.00955732\n",
      "Epoch 1 | Step 635900 | Avg Loss: 0.0148 | Grad Norm: 0.00860537\n",
      "Epoch 1 | Step 636000 | Avg Loss: 0.0151 | Grad Norm: 0.00949045\n",
      "Epoch 1 | Step 636100 | Avg Loss: 0.0147 | Grad Norm: 0.00971943\n",
      "Epoch 1 | Step 636200 | Avg Loss: 0.0148 | Grad Norm: 0.00918321\n",
      "Epoch 1 | Step 636300 | Avg Loss: 0.0151 | Grad Norm: 0.00909599\n",
      "Epoch 1 | Step 636400 | Avg Loss: 0.0148 | Grad Norm: 0.00956611\n",
      "Epoch 1 | Step 636500 | Avg Loss: 0.0148 | Grad Norm: 0.00874334\n",
      "Epoch 1 | Step 636600 | Avg Loss: 0.0146 | Grad Norm: 0.00981541\n",
      "Epoch 1 | Step 636700 | Avg Loss: 0.0146 | Grad Norm: 0.00828130\n",
      "Epoch 1 | Step 636800 | Avg Loss: 0.0147 | Grad Norm: 0.00894241\n",
      "Epoch 1 | Step 636900 | Avg Loss: 0.0145 | Grad Norm: 0.00899616\n",
      "Epoch 1 | Step 637000 | Avg Loss: 0.0146 | Grad Norm: 0.00786217\n",
      "Epoch 1 | Step 637100 | Avg Loss: 0.0145 | Grad Norm: 0.00802011\n",
      "Epoch 1 | Step 637200 | Avg Loss: 0.0144 | Grad Norm: 0.00778910\n",
      "Epoch 1 | Step 637300 | Avg Loss: 0.0144 | Grad Norm: 0.01127207\n",
      "Epoch 1 | Step 637400 | Avg Loss: 0.0148 | Grad Norm: 0.00988672\n",
      "Epoch 1 | Step 637500 | Avg Loss: 0.0147 | Grad Norm: 0.00755116\n",
      "Epoch 1 | Step 637600 | Avg Loss: 0.0148 | Grad Norm: 0.01024061\n",
      "Epoch 1 | Step 637700 | Avg Loss: 0.0152 | Grad Norm: 0.01032658\n",
      "Epoch 1 | Step 637800 | Avg Loss: 0.0149 | Grad Norm: 0.01037630\n",
      "Epoch 1 | Step 637900 | Avg Loss: 0.0151 | Grad Norm: 0.00857887\n",
      "Epoch 1 | Step 638000 | Avg Loss: 0.0150 | Grad Norm: 0.00739822\n",
      "Epoch 1 | Step 638100 | Avg Loss: 0.0152 | Grad Norm: 0.00999457\n",
      "Epoch 1 | Step 638200 | Avg Loss: 0.0149 | Grad Norm: 0.00835179\n",
      "Epoch 1 | Step 638300 | Avg Loss: 0.0146 | Grad Norm: 0.00862098\n",
      "Epoch 1 | Step 638400 | Avg Loss: 0.0148 | Grad Norm: 0.00737019\n",
      "Epoch 1 | Step 638500 | Avg Loss: 0.0150 | Grad Norm: 0.00855565\n",
      "Epoch 1 | Step 638600 | Avg Loss: 0.0149 | Grad Norm: 0.00884625\n",
      "Epoch 1 | Step 638700 | Avg Loss: 0.0146 | Grad Norm: 0.01031205\n",
      "Epoch 1 | Step 638800 | Avg Loss: 0.0145 | Grad Norm: 0.00924330\n",
      "Epoch 1 | Step 638900 | Avg Loss: 0.0148 | Grad Norm: 0.00836622\n",
      "Epoch 1 | Step 639000 | Avg Loss: 0.0147 | Grad Norm: 0.00841375\n",
      "Epoch 1 | Step 639100 | Avg Loss: 0.0148 | Grad Norm: 0.00899133\n",
      "Epoch 1 | Step 639200 | Avg Loss: 0.0148 | Grad Norm: 0.00848949\n",
      "Epoch 1 | Step 639300 | Avg Loss: 0.0147 | Grad Norm: 0.00823248\n",
      "Epoch 1 | Step 639400 | Avg Loss: 0.0152 | Grad Norm: 0.00877905\n",
      "Epoch 1 | Step 639500 | Avg Loss: 0.0152 | Grad Norm: 0.00777404\n",
      "Epoch 1 | Step 639600 | Avg Loss: 0.0152 | Grad Norm: 0.00972673\n",
      "Epoch 1 | Step 639700 | Avg Loss: 0.0150 | Grad Norm: 0.00806105\n",
      "Epoch 1 | Step 639800 | Avg Loss: 0.0151 | Grad Norm: 0.00841946\n",
      "Epoch 1 | Step 639900 | Avg Loss: 0.0150 | Grad Norm: 0.00836267\n",
      "Epoch 1 | Step 640000 | Avg Loss: 0.0152 | Grad Norm: 0.00945896\n",
      "Epoch 1 | Step 640100 | Avg Loss: 0.0149 | Grad Norm: 0.00723495\n",
      "Epoch 1 | Step 640200 | Avg Loss: 0.0147 | Grad Norm: 0.01044313\n",
      "Epoch 1 | Step 640300 | Avg Loss: 0.0146 | Grad Norm: 0.00798527\n",
      "Epoch 1 | Step 640400 | Avg Loss: 0.0143 | Grad Norm: 0.00854504\n",
      "Epoch 1 | Step 640500 | Avg Loss: 0.0146 | Grad Norm: 0.00906716\n",
      "Epoch 1 | Step 640600 | Avg Loss: 0.0146 | Grad Norm: 0.00811197\n",
      "Epoch 1 | Step 640700 | Avg Loss: 0.0143 | Grad Norm: 0.00816083\n",
      "Epoch 1 | Step 640800 | Avg Loss: 0.0144 | Grad Norm: 0.00860556\n",
      "Epoch 1 | Step 640900 | Avg Loss: 0.0143 | Grad Norm: 0.00999670\n",
      "Epoch 1 | Step 641000 | Avg Loss: 0.0142 | Grad Norm: 0.00889432\n",
      "Epoch 1 | Step 641100 | Avg Loss: 0.0146 | Grad Norm: 0.00940216\n",
      "Epoch 1 | Step 641200 | Avg Loss: 0.0145 | Grad Norm: 0.01017487\n",
      "Epoch 1 | Step 641300 | Avg Loss: 0.0147 | Grad Norm: 0.00949044\n",
      "Epoch 1 | Step 641400 | Avg Loss: 0.0149 | Grad Norm: 0.00806556\n",
      "Epoch 1 | Step 641500 | Avg Loss: 0.0148 | Grad Norm: 0.01035582\n",
      "Epoch 1 | Step 641600 | Avg Loss: 0.0146 | Grad Norm: 0.00883379\n",
      "Epoch 1 | Step 641700 | Avg Loss: 0.0144 | Grad Norm: 0.00852114\n",
      "Epoch 1 | Step 641800 | Avg Loss: 0.0141 | Grad Norm: 0.00909034\n",
      "Epoch 1 | Step 641900 | Avg Loss: 0.0146 | Grad Norm: 0.00876685\n",
      "Epoch 1 | Step 642000 | Avg Loss: 0.0148 | Grad Norm: 0.00868148\n",
      "Epoch 1 | Step 642100 | Avg Loss: 0.0150 | Grad Norm: 0.00845951\n",
      "Epoch 1 | Step 642200 | Avg Loss: 0.0149 | Grad Norm: 0.00856083\n",
      "Epoch 1 | Step 642300 | Avg Loss: 0.0151 | Grad Norm: 0.00834352\n",
      "Epoch 1 | Step 642400 | Avg Loss: 0.0147 | Grad Norm: 0.00991691\n",
      "Epoch 1 | Step 642500 | Avg Loss: 0.0150 | Grad Norm: 0.00803840\n",
      "Epoch 1 | Step 642600 | Avg Loss: 0.0149 | Grad Norm: 0.00783621\n",
      "Epoch 1 | Step 642700 | Avg Loss: 0.0148 | Grad Norm: 0.00900487\n",
      "Epoch 1 | Step 642800 | Avg Loss: 0.0151 | Grad Norm: 0.00978754\n",
      "Epoch 1 | Step 642900 | Avg Loss: 0.0146 | Grad Norm: 0.01253613\n",
      "Epoch 1 | Step 643000 | Avg Loss: 0.0147 | Grad Norm: 0.00893974\n",
      "Epoch 1 | Step 643100 | Avg Loss: 0.0147 | Grad Norm: 0.00914378\n",
      "Epoch 1 | Step 643200 | Avg Loss: 0.0145 | Grad Norm: 0.00871309\n",
      "Epoch 1 | Step 643300 | Avg Loss: 0.0147 | Grad Norm: 0.00932537\n",
      "Epoch 1 | Step 643400 | Avg Loss: 0.0147 | Grad Norm: 0.00814529\n",
      "Epoch 1 | Step 643500 | Avg Loss: 0.0149 | Grad Norm: 0.00891614\n",
      "Epoch 1 | Step 643600 | Avg Loss: 0.0149 | Grad Norm: 0.00954544\n",
      "Epoch 1 | Step 643700 | Avg Loss: 0.0150 | Grad Norm: 0.01010055\n",
      "Epoch 1 | Step 643800 | Avg Loss: 0.0149 | Grad Norm: 0.00870907\n",
      "Epoch 1 | Step 643900 | Avg Loss: 0.0150 | Grad Norm: 0.00853755\n",
      "Epoch 1 | Step 644000 | Avg Loss: 0.0149 | Grad Norm: 0.01030930\n",
      "Epoch 1 | Step 644100 | Avg Loss: 0.0150 | Grad Norm: 0.01034448\n",
      "Epoch 1 | Step 644200 | Avg Loss: 0.0146 | Grad Norm: 0.00840352\n",
      "Epoch 1 | Step 644300 | Avg Loss: 0.0146 | Grad Norm: 0.00937074\n",
      "Epoch 1 | Step 644400 | Avg Loss: 0.0148 | Grad Norm: 0.00831815\n",
      "Epoch 1 | Step 644500 | Avg Loss: 0.0147 | Grad Norm: 0.01081661\n",
      "Epoch 1 | Step 644600 | Avg Loss: 0.0147 | Grad Norm: 0.00860923\n",
      "Epoch 1 | Step 644700 | Avg Loss: 0.0149 | Grad Norm: 0.00847836\n",
      "Epoch 1 | Step 644800 | Avg Loss: 0.0147 | Grad Norm: 0.00938564\n",
      "Epoch 1 | Step 644900 | Avg Loss: 0.0148 | Grad Norm: 0.00973479\n",
      "Epoch 1 | Step 645000 | Avg Loss: 0.0148 | Grad Norm: 0.00918529\n",
      "Epoch 1 | Step 645100 | Avg Loss: 0.0146 | Grad Norm: 0.00802392\n",
      "Epoch 1 | Step 645200 | Avg Loss: 0.0144 | Grad Norm: 0.00854984\n",
      "Epoch 1 | Step 645300 | Avg Loss: 0.0143 | Grad Norm: 0.00857994\n",
      "Epoch 1 | Step 645400 | Avg Loss: 0.0144 | Grad Norm: 0.00908690\n",
      "Epoch 1 | Step 645500 | Avg Loss: 0.0142 | Grad Norm: 0.00916344\n",
      "Epoch 1 | Step 645600 | Avg Loss: 0.0143 | Grad Norm: 0.01038391\n",
      "Epoch 1 | Step 645700 | Avg Loss: 0.0144 | Grad Norm: 0.00953470\n",
      "Epoch 1 | Step 645800 | Avg Loss: 0.0143 | Grad Norm: 0.00959171\n",
      "Epoch 1 | Step 645900 | Avg Loss: 0.0142 | Grad Norm: 0.00784921\n",
      "Epoch 1 | Step 646000 | Avg Loss: 0.0145 | Grad Norm: 0.00802991\n",
      "Epoch 1 | Step 646100 | Avg Loss: 0.0143 | Grad Norm: 0.00889750\n",
      "Epoch 1 | Step 646200 | Avg Loss: 0.0145 | Grad Norm: 0.00882147\n",
      "Epoch 1 | Step 646300 | Avg Loss: 0.0144 | Grad Norm: 0.01023783\n",
      "Epoch 1 | Step 646400 | Avg Loss: 0.0144 | Grad Norm: 0.00919583\n",
      "Epoch 1 | Step 646500 | Avg Loss: 0.0144 | Grad Norm: 0.00886327\n",
      "Epoch 1 | Step 646600 | Avg Loss: 0.0147 | Grad Norm: 0.00912070\n",
      "Epoch 1 | Step 646700 | Avg Loss: 0.0145 | Grad Norm: 0.00866391\n",
      "Epoch 1 | Step 646800 | Avg Loss: 0.0144 | Grad Norm: 0.01007172\n",
      "Epoch 1 | Step 646900 | Avg Loss: 0.0144 | Grad Norm: 0.00792195\n",
      "Epoch 1 | Step 647000 | Avg Loss: 0.0147 | Grad Norm: 0.00942575\n",
      "Epoch 1 | Step 647100 | Avg Loss: 0.0148 | Grad Norm: 0.00811640\n",
      "Epoch 1 | Step 647200 | Avg Loss: 0.0148 | Grad Norm: 0.00908062\n",
      "Epoch 1 | Step 647300 | Avg Loss: 0.0147 | Grad Norm: 0.00923434\n",
      "Epoch 1 | Step 647400 | Avg Loss: 0.0150 | Grad Norm: 0.00947894\n",
      "Epoch 1 | Step 647500 | Avg Loss: 0.0151 | Grad Norm: 0.00921769\n",
      "Epoch 1 | Step 647600 | Avg Loss: 0.0145 | Grad Norm: 0.00996123\n",
      "Epoch 1 | Step 647700 | Avg Loss: 0.0145 | Grad Norm: 0.00941894\n",
      "Epoch 1 | Step 647800 | Avg Loss: 0.0147 | Grad Norm: 0.00854075\n",
      "Epoch 1 | Step 647900 | Avg Loss: 0.0147 | Grad Norm: 0.00712832\n",
      "Epoch 1 | Step 648000 | Avg Loss: 0.0147 | Grad Norm: 0.00861127\n",
      "Epoch 1 | Step 648100 | Avg Loss: 0.0149 | Grad Norm: 0.00905233\n",
      "Epoch 1 | Step 648200 | Avg Loss: 0.0149 | Grad Norm: 0.00947232\n",
      "Epoch 1 | Step 648300 | Avg Loss: 0.0152 | Grad Norm: 0.00936887\n",
      "Epoch 1 | Step 648400 | Avg Loss: 0.0146 | Grad Norm: 0.00956148\n",
      "Epoch 1 | Step 648500 | Avg Loss: 0.0149 | Grad Norm: 0.00982349\n",
      "Epoch 1 | Step 648600 | Avg Loss: 0.0147 | Grad Norm: 0.01000707\n",
      "Epoch 1 | Step 648700 | Avg Loss: 0.0145 | Grad Norm: 0.00905846\n",
      "Epoch 1 | Step 648800 | Avg Loss: 0.0148 | Grad Norm: 0.00972547\n",
      "Epoch 1 | Step 648900 | Avg Loss: 0.0149 | Grad Norm: 0.00930484\n",
      "Epoch 1 | Step 649000 | Avg Loss: 0.0151 | Grad Norm: 0.00942467\n",
      "Epoch 1 | Step 649100 | Avg Loss: 0.0152 | Grad Norm: 0.01211937\n",
      "Epoch 1 | Step 649200 | Avg Loss: 0.0154 | Grad Norm: 0.00776553\n",
      "Epoch 1 | Step 649300 | Avg Loss: 0.0150 | Grad Norm: 0.00897943\n",
      "Epoch 1 | Step 649400 | Avg Loss: 0.0154 | Grad Norm: 0.00914557\n",
      "Epoch 1 | Step 649500 | Avg Loss: 0.0155 | Grad Norm: 0.01149177\n",
      "Epoch 1 | Step 649600 | Avg Loss: 0.0154 | Grad Norm: 0.01144595\n",
      "Epoch 1 | Step 649700 | Avg Loss: 0.0151 | Grad Norm: 0.00984471\n",
      "Epoch 1 | Step 649800 | Avg Loss: 0.0151 | Grad Norm: 0.00969088\n",
      "Epoch 1 | Step 649900 | Avg Loss: 0.0147 | Grad Norm: 0.01007505\n",
      "Epoch 1 | Step 650000 | Avg Loss: 0.0149 | Grad Norm: 0.00858469\n",
      "Epoch 1 | Step 650100 | Avg Loss: 0.0147 | Grad Norm: 0.00832442\n",
      "Epoch 1 | Step 650200 | Avg Loss: 0.0147 | Grad Norm: 0.00926755\n",
      "Epoch 1 | Step 650300 | Avg Loss: 0.0145 | Grad Norm: 0.01042709\n",
      "Epoch 1 | Step 650400 | Avg Loss: 0.0148 | Grad Norm: 0.01003503\n",
      "Epoch 1 | Step 650500 | Avg Loss: 0.0151 | Grad Norm: 0.00751793\n",
      "Epoch 1 | Step 650600 | Avg Loss: 0.0152 | Grad Norm: 0.01092010\n",
      "Epoch 1 | Step 650700 | Avg Loss: 0.0154 | Grad Norm: 0.00817965\n",
      "Epoch 1 | Step 650800 | Avg Loss: 0.0152 | Grad Norm: 0.00986977\n",
      "Epoch 1 | Step 650900 | Avg Loss: 0.0150 | Grad Norm: 0.00953490\n",
      "Epoch 1 | Step 651000 | Avg Loss: 0.0152 | Grad Norm: 0.00812651\n",
      "Epoch 1 | Step 651100 | Avg Loss: 0.0154 | Grad Norm: 0.00828677\n",
      "Epoch 1 | Step 651200 | Avg Loss: 0.0154 | Grad Norm: 0.00995387\n",
      "Epoch 1 | Step 651300 | Avg Loss: 0.0152 | Grad Norm: 0.00899518\n",
      "Epoch 1 | Step 651400 | Avg Loss: 0.0146 | Grad Norm: 0.01113675\n",
      "Epoch 1 | Step 651500 | Avg Loss: 0.0148 | Grad Norm: 0.00848781\n",
      "Epoch 1 | Step 651600 | Avg Loss: 0.0149 | Grad Norm: 0.00913559\n",
      "Epoch 1 | Step 651700 | Avg Loss: 0.0149 | Grad Norm: 0.00883515\n",
      "Epoch 1 | Step 651800 | Avg Loss: 0.0147 | Grad Norm: 0.00853901\n",
      "Epoch 1 | Step 651900 | Avg Loss: 0.0146 | Grad Norm: 0.00870240\n",
      "Epoch 1 | Step 652000 | Avg Loss: 0.0143 | Grad Norm: 0.00872985\n",
      "Epoch 1 | Step 652100 | Avg Loss: 0.0144 | Grad Norm: 0.00983141\n",
      "Epoch 1 | Step 652200 | Avg Loss: 0.0146 | Grad Norm: 0.00784757\n",
      "Epoch 1 | Step 652300 | Avg Loss: 0.0143 | Grad Norm: 0.00811626\n",
      "Epoch 1 | Step 652400 | Avg Loss: 0.0149 | Grad Norm: 0.00839855\n",
      "Epoch 1 | Step 652500 | Avg Loss: 0.0150 | Grad Norm: 0.01003745\n",
      "Epoch 1 | Step 652600 | Avg Loss: 0.0148 | Grad Norm: 0.00845336\n",
      "Epoch 1 | Step 652700 | Avg Loss: 0.0146 | Grad Norm: 0.00832087\n",
      "Epoch 1 | Step 652800 | Avg Loss: 0.0147 | Grad Norm: 0.00785754\n",
      "Epoch 1 | Step 652900 | Avg Loss: 0.0144 | Grad Norm: 0.01273568\n",
      "Epoch 1 | Step 653000 | Avg Loss: 0.0146 | Grad Norm: 0.01056715\n",
      "Epoch 1 | Step 653100 | Avg Loss: 0.0146 | Grad Norm: 0.00964940\n",
      "Epoch 1 | Step 653200 | Avg Loss: 0.0152 | Grad Norm: 0.00923390\n",
      "Epoch 1 | Step 653300 | Avg Loss: 0.0149 | Grad Norm: 0.01087444\n",
      "Epoch 1 | Step 653400 | Avg Loss: 0.0153 | Grad Norm: 0.00765804\n",
      "Epoch 1 | Step 653500 | Avg Loss: 0.0149 | Grad Norm: 0.00812707\n",
      "Epoch 1 | Step 653600 | Avg Loss: 0.0145 | Grad Norm: 0.00774714\n",
      "Epoch 1 | Step 653700 | Avg Loss: 0.0150 | Grad Norm: 0.00902815\n",
      "Epoch 1 | Step 653800 | Avg Loss: 0.0150 | Grad Norm: 0.00802453\n",
      "Epoch 1 | Step 653900 | Avg Loss: 0.0149 | Grad Norm: 0.00872531\n",
      "Epoch 1 | Step 654000 | Avg Loss: 0.0144 | Grad Norm: 0.00886323\n",
      "Epoch 1 | Step 654100 | Avg Loss: 0.0145 | Grad Norm: 0.01009460\n",
      "Epoch 1 | Step 654200 | Avg Loss: 0.0143 | Grad Norm: 0.00821148\n",
      "Epoch 1 | Step 654300 | Avg Loss: 0.0142 | Grad Norm: 0.00823337\n",
      "Epoch 1 | Step 654400 | Avg Loss: 0.0147 | Grad Norm: 0.00880021\n",
      "Epoch 1 | Step 654500 | Avg Loss: 0.0148 | Grad Norm: 0.00894368\n",
      "Epoch 1 | Step 654600 | Avg Loss: 0.0149 | Grad Norm: 0.01009645\n",
      "Epoch 1 | Step 654700 | Avg Loss: 0.0150 | Grad Norm: 0.00976139\n",
      "Epoch 1 | Step 654800 | Avg Loss: 0.0155 | Grad Norm: 0.00902939\n",
      "Epoch 1 | Step 654900 | Avg Loss: 0.0151 | Grad Norm: 0.00813687\n",
      "Epoch 1 | Step 655000 | Avg Loss: 0.0148 | Grad Norm: 0.00862406\n",
      "Epoch 1 | Step 655100 | Avg Loss: 0.0147 | Grad Norm: 0.00960209\n",
      "Epoch 1 | Step 655200 | Avg Loss: 0.0148 | Grad Norm: 0.01057154\n",
      "Epoch 1 | Step 655300 | Avg Loss: 0.0151 | Grad Norm: 0.01004249\n",
      "Epoch 1 | Step 655400 | Avg Loss: 0.0152 | Grad Norm: 0.00925933\n",
      "Epoch 1 | Step 655500 | Avg Loss: 0.0150 | Grad Norm: 0.00895527\n",
      "Epoch 1 | Step 655600 | Avg Loss: 0.0150 | Grad Norm: 0.01217761\n",
      "Epoch 1 | Step 655700 | Avg Loss: 0.0146 | Grad Norm: 0.00755456\n",
      "Epoch 1 | Step 655800 | Avg Loss: 0.0149 | Grad Norm: 0.00828358\n",
      "Epoch 1 | Step 655900 | Avg Loss: 0.0147 | Grad Norm: 0.00814589\n",
      "Epoch 1 | Step 656000 | Avg Loss: 0.0150 | Grad Norm: 0.01027059\n",
      "Epoch 1 | Step 656100 | Avg Loss: 0.0151 | Grad Norm: 0.00912181\n",
      "Epoch 1 | Step 656200 | Avg Loss: 0.0149 | Grad Norm: 0.00874857\n",
      "Epoch 1 | Step 656300 | Avg Loss: 0.0148 | Grad Norm: 0.00898261\n",
      "Epoch 1 | Step 656400 | Avg Loss: 0.0145 | Grad Norm: 0.00762688\n",
      "Epoch 1 | Step 656500 | Avg Loss: 0.0145 | Grad Norm: 0.00841347\n",
      "Epoch 1 | Step 656600 | Avg Loss: 0.0148 | Grad Norm: 0.00923853\n",
      "Epoch 1 | Step 656700 | Avg Loss: 0.0151 | Grad Norm: 0.00932558\n",
      "Epoch 1 | Step 656800 | Avg Loss: 0.0151 | Grad Norm: 0.00952849\n",
      "Epoch 1 | Step 656900 | Avg Loss: 0.0153 | Grad Norm: 0.00953631\n",
      "Epoch 1 | Step 657000 | Avg Loss: 0.0148 | Grad Norm: 0.00923698\n",
      "Epoch 1 | Step 657100 | Avg Loss: 0.0149 | Grad Norm: 0.00954570\n",
      "Epoch 1 | Step 657200 | Avg Loss: 0.0149 | Grad Norm: 0.00891283\n",
      "Epoch 1 | Step 657300 | Avg Loss: 0.0150 | Grad Norm: 0.00983423\n",
      "Epoch 1 | Step 657400 | Avg Loss: 0.0148 | Grad Norm: 0.01065105\n",
      "Epoch 1 | Step 657500 | Avg Loss: 0.0148 | Grad Norm: 0.00881394\n",
      "Epoch 1 | Step 657600 | Avg Loss: 0.0146 | Grad Norm: 0.00817781\n",
      "Epoch 1 | Step 657700 | Avg Loss: 0.0150 | Grad Norm: 0.00994057\n",
      "Epoch 1 | Step 657800 | Avg Loss: 0.0151 | Grad Norm: 0.00847926\n",
      "Epoch 1 | Step 657900 | Avg Loss: 0.0147 | Grad Norm: 0.00791594\n",
      "Epoch 1 | Step 658000 | Avg Loss: 0.0147 | Grad Norm: 0.00909478\n",
      "Epoch 1 | Step 658100 | Avg Loss: 0.0145 | Grad Norm: 0.00992879\n",
      "Epoch 1 | Step 658200 | Avg Loss: 0.0145 | Grad Norm: 0.00890273\n",
      "Epoch 1 | Step 658300 | Avg Loss: 0.0143 | Grad Norm: 0.00809365\n",
      "Epoch 1 | Step 658400 | Avg Loss: 0.0141 | Grad Norm: 0.00942105\n",
      "Epoch 1 | Step 658500 | Avg Loss: 0.0140 | Grad Norm: 0.00848197\n",
      "Epoch 1 | Step 658600 | Avg Loss: 0.0143 | Grad Norm: 0.00888751\n",
      "Epoch 1 | Step 658700 | Avg Loss: 0.0144 | Grad Norm: 0.00912763\n",
      "Epoch 1 | Step 658800 | Avg Loss: 0.0142 | Grad Norm: 0.00918661\n",
      "Epoch 1 | Step 658900 | Avg Loss: 0.0142 | Grad Norm: 0.00877664\n",
      "Epoch 1 | Step 659000 | Avg Loss: 0.0146 | Grad Norm: 0.00862192\n",
      "Epoch 1 | Step 659100 | Avg Loss: 0.0147 | Grad Norm: 0.00929564\n",
      "Epoch 1 | Step 659200 | Avg Loss: 0.0153 | Grad Norm: 0.00877110\n",
      "Epoch 1 | Step 659300 | Avg Loss: 0.0157 | Grad Norm: 0.00899427\n",
      "Epoch 1 | Step 659400 | Avg Loss: 0.0156 | Grad Norm: 0.01011407\n",
      "Epoch 1 | Step 659500 | Avg Loss: 0.0151 | Grad Norm: 0.00889058\n",
      "Epoch 1 | Step 659600 | Avg Loss: 0.0148 | Grad Norm: 0.00875858\n",
      "Epoch 1 | Step 659700 | Avg Loss: 0.0148 | Grad Norm: 0.00805511\n",
      "Epoch 1 | Step 659800 | Avg Loss: 0.0150 | Grad Norm: 0.01151923\n",
      "Epoch 1 | Step 659900 | Avg Loss: 0.0152 | Grad Norm: 0.00851250\n",
      "Epoch 1 | Step 660000 | Avg Loss: 0.0149 | Grad Norm: 0.00818290\n",
      "Epoch 1 | Step 660100 | Avg Loss: 0.0152 | Grad Norm: 0.00893530\n",
      "Epoch 1 | Step 660200 | Avg Loss: 0.0151 | Grad Norm: 0.00891625\n",
      "Epoch 1 | Step 660300 | Avg Loss: 0.0148 | Grad Norm: 0.00777702\n",
      "Epoch 1 | Step 660400 | Avg Loss: 0.0149 | Grad Norm: 0.00933276\n",
      "Epoch 1 | Step 660500 | Avg Loss: 0.0150 | Grad Norm: 0.00899525\n",
      "Epoch 1 | Step 660600 | Avg Loss: 0.0152 | Grad Norm: 0.01020962\n",
      "Epoch 1 | Step 660700 | Avg Loss: 0.0149 | Grad Norm: 0.01229670\n",
      "Epoch 1 | Step 660800 | Avg Loss: 0.0148 | Grad Norm: 0.00863218\n",
      "Epoch 1 | Step 660900 | Avg Loss: 0.0148 | Grad Norm: 0.00810505\n",
      "Epoch 1 | Step 661000 | Avg Loss: 0.0151 | Grad Norm: 0.00788628\n",
      "Epoch 1 | Step 661100 | Avg Loss: 0.0151 | Grad Norm: 0.00897165\n",
      "Epoch 1 | Step 661200 | Avg Loss: 0.0152 | Grad Norm: 0.00993732\n",
      "Epoch 1 | Step 661300 | Avg Loss: 0.0148 | Grad Norm: 0.00843926\n",
      "Epoch 1 | Step 661400 | Avg Loss: 0.0150 | Grad Norm: 0.00777334\n",
      "Epoch 1 | Step 661500 | Avg Loss: 0.0150 | Grad Norm: 0.00847573\n",
      "Epoch 1 | Step 661600 | Avg Loss: 0.0147 | Grad Norm: 0.00977041\n",
      "Epoch 1 | Step 661700 | Avg Loss: 0.0147 | Grad Norm: 0.00971202\n",
      "Epoch 1 | Step 661800 | Avg Loss: 0.0144 | Grad Norm: 0.00736969\n",
      "Epoch 1 | Step 661900 | Avg Loss: 0.0140 | Grad Norm: 0.00883647\n",
      "Epoch 1 | Step 662000 | Avg Loss: 0.0147 | Grad Norm: 0.00813527\n",
      "Epoch 1 | Step 662100 | Avg Loss: 0.0145 | Grad Norm: 0.00880640\n",
      "Epoch 1 | Step 662200 | Avg Loss: 0.0146 | Grad Norm: 0.01006973\n",
      "Epoch 1 | Step 662300 | Avg Loss: 0.0145 | Grad Norm: 0.00965091\n",
      "Epoch 1 | Step 662400 | Avg Loss: 0.0146 | Grad Norm: 0.00774942\n",
      "Epoch 1 | Step 662500 | Avg Loss: 0.0141 | Grad Norm: 0.00795060\n",
      "Epoch 1 | Step 662600 | Avg Loss: 0.0144 | Grad Norm: 0.00813762\n",
      "Epoch 1 | Step 662700 | Avg Loss: 0.0144 | Grad Norm: 0.00835559\n",
      "Epoch 1 | Step 662800 | Avg Loss: 0.0146 | Grad Norm: 0.00844776\n",
      "Epoch 1 | Step 662900 | Avg Loss: 0.0144 | Grad Norm: 0.00912708\n",
      "Epoch 1 | Step 663000 | Avg Loss: 0.0142 | Grad Norm: 0.00780660\n",
      "Epoch 1 | Step 663100 | Avg Loss: 0.0145 | Grad Norm: 0.00898773\n",
      "Epoch 1 | Step 663200 | Avg Loss: 0.0147 | Grad Norm: 0.00934138\n",
      "Epoch 1 | Step 663300 | Avg Loss: 0.0153 | Grad Norm: 0.00755820\n",
      "Epoch 1 | Step 663400 | Avg Loss: 0.0152 | Grad Norm: 0.00916171\n",
      "Epoch 1 | Step 663500 | Avg Loss: 0.0151 | Grad Norm: 0.00946317\n",
      "Epoch 1 | Step 663600 | Avg Loss: 0.0153 | Grad Norm: 0.00855928\n",
      "Epoch 1 | Step 663700 | Avg Loss: 0.0155 | Grad Norm: 0.00915619\n",
      "Epoch 1 | Step 663800 | Avg Loss: 0.0155 | Grad Norm: 0.00874452\n",
      "Epoch 1 | Step 663900 | Avg Loss: 0.0152 | Grad Norm: 0.00953587\n",
      "Epoch 1 | Step 664000 | Avg Loss: 0.0156 | Grad Norm: 0.00985318\n",
      "Epoch 1 | Step 664100 | Avg Loss: 0.0154 | Grad Norm: 0.00902013\n",
      "Epoch 1 | Step 664200 | Avg Loss: 0.0144 | Grad Norm: 0.00839794\n",
      "Epoch 1 | Step 664300 | Avg Loss: 0.0144 | Grad Norm: 0.00883282\n",
      "Epoch 1 | Step 664400 | Avg Loss: 0.0141 | Grad Norm: 0.00878625\n",
      "Epoch 1 | Step 664500 | Avg Loss: 0.0140 | Grad Norm: 0.00796157\n",
      "Epoch 1 | Step 664600 | Avg Loss: 0.0141 | Grad Norm: 0.00765962\n",
      "Epoch 1 | Step 664700 | Avg Loss: 0.0141 | Grad Norm: 0.00883158\n",
      "Epoch 1 | Step 664800 | Avg Loss: 0.0142 | Grad Norm: 0.00821632\n",
      "Epoch 1 | Step 664900 | Avg Loss: 0.0143 | Grad Norm: 0.00842765\n",
      "Epoch 1 | Step 665000 | Avg Loss: 0.0142 | Grad Norm: 0.00801852\n",
      "Epoch 1 | Step 665100 | Avg Loss: 0.0142 | Grad Norm: 0.00770226\n",
      "Epoch 1 | Step 665200 | Avg Loss: 0.0145 | Grad Norm: 0.00954679\n",
      "Epoch 1 | Step 665300 | Avg Loss: 0.0149 | Grad Norm: 0.01014286\n",
      "Epoch 1 | Step 665400 | Avg Loss: 0.0147 | Grad Norm: 0.00840323\n",
      "Epoch 1 | Step 665500 | Avg Loss: 0.0146 | Grad Norm: 0.00866106\n",
      "Epoch 1 | Step 665600 | Avg Loss: 0.0147 | Grad Norm: 0.00741036\n",
      "Epoch 1 | Step 665700 | Avg Loss: 0.0148 | Grad Norm: 0.00929276\n",
      "Epoch 1 | Step 665800 | Avg Loss: 0.0153 | Grad Norm: 0.00823687\n",
      "Epoch 1 | Step 665900 | Avg Loss: 0.0148 | Grad Norm: 0.00918459\n",
      "Epoch 1 | Step 666000 | Avg Loss: 0.0150 | Grad Norm: 0.00793943\n",
      "Epoch 1 | Step 666100 | Avg Loss: 0.0146 | Grad Norm: 0.00787751\n",
      "Epoch 1 | Step 666200 | Avg Loss: 0.0145 | Grad Norm: 0.01437303\n",
      "Epoch 1 | Step 666300 | Avg Loss: 0.0149 | Grad Norm: 0.00814799\n",
      "Epoch 1 | Step 666400 | Avg Loss: 0.0150 | Grad Norm: 0.00759125\n",
      "Epoch 1 | Step 666500 | Avg Loss: 0.0150 | Grad Norm: 0.00775537\n",
      "Epoch 1 | Step 666600 | Avg Loss: 0.0145 | Grad Norm: 0.00797648\n",
      "Epoch 1 | Step 666700 | Avg Loss: 0.0148 | Grad Norm: 0.00868586\n",
      "Epoch 1 | Step 666800 | Avg Loss: 0.0151 | Grad Norm: 0.01023148\n",
      "Epoch 1 | Step 666900 | Avg Loss: 0.0152 | Grad Norm: 0.00859664\n",
      "Epoch 1 | Step 667000 | Avg Loss: 0.0149 | Grad Norm: 0.00873215\n",
      "Epoch 1 | Step 667100 | Avg Loss: 0.0150 | Grad Norm: 0.00855058\n",
      "Epoch 1 | Step 667200 | Avg Loss: 0.0149 | Grad Norm: 0.00770964\n",
      "Epoch 1 | Step 667300 | Avg Loss: 0.0149 | Grad Norm: 0.00889511\n",
      "Epoch 1 | Step 667400 | Avg Loss: 0.0147 | Grad Norm: 0.00819255\n",
      "Epoch 1 | Step 667500 | Avg Loss: 0.0146 | Grad Norm: 0.00896551\n",
      "Epoch 1 | Step 667600 | Avg Loss: 0.0149 | Grad Norm: 0.00825027\n",
      "Epoch 1 | Step 667700 | Avg Loss: 0.0149 | Grad Norm: 0.00933997\n",
      "Epoch 1 | Step 667800 | Avg Loss: 0.0151 | Grad Norm: 0.00999885\n",
      "Epoch 1 | Step 667900 | Avg Loss: 0.0149 | Grad Norm: 0.00833688\n",
      "Epoch 1 | Step 668000 | Avg Loss: 0.0144 | Grad Norm: 0.00745437\n",
      "Epoch 1 | Step 668100 | Avg Loss: 0.0143 | Grad Norm: 0.00809475\n",
      "Epoch 1 | Step 668200 | Avg Loss: 0.0143 | Grad Norm: 0.00861270\n",
      "Epoch 1 | Step 668300 | Avg Loss: 0.0146 | Grad Norm: 0.00889755\n",
      "Epoch 1 | Step 668400 | Avg Loss: 0.0150 | Grad Norm: 0.00834160\n",
      "Epoch 1 | Step 668500 | Avg Loss: 0.0149 | Grad Norm: 0.00963766\n",
      "Epoch 1 | Step 668600 | Avg Loss: 0.0150 | Grad Norm: 0.00826498\n",
      "Epoch 1 | Step 668700 | Avg Loss: 0.0152 | Grad Norm: 0.01181643\n",
      "Epoch 1 | Step 668800 | Avg Loss: 0.0150 | Grad Norm: 0.00967167\n",
      "Epoch 1 | Step 668900 | Avg Loss: 0.0149 | Grad Norm: 0.00818738\n",
      "Epoch 1 | Step 669000 | Avg Loss: 0.0152 | Grad Norm: 0.00859792\n",
      "Epoch 1 | Step 669100 | Avg Loss: 0.0148 | Grad Norm: 0.00895490\n",
      "Epoch 1 | Step 669200 | Avg Loss: 0.0148 | Grad Norm: 0.00908208\n",
      "Epoch 1 | Step 669300 | Avg Loss: 0.0153 | Grad Norm: 0.00869991\n",
      "Epoch 1 | Step 669400 | Avg Loss: 0.0150 | Grad Norm: 0.00855632\n",
      "Epoch 1 | Step 669500 | Avg Loss: 0.0150 | Grad Norm: 0.00839510\n",
      "Epoch 1 | Step 669600 | Avg Loss: 0.0154 | Grad Norm: 0.00805458\n",
      "Epoch 1 | Step 669700 | Avg Loss: 0.0155 | Grad Norm: 0.00889909\n",
      "Epoch 1 | Step 669800 | Avg Loss: 0.0153 | Grad Norm: 0.01047251\n",
      "Epoch 1 | Step 669900 | Avg Loss: 0.0150 | Grad Norm: 0.00836249\n",
      "Epoch 1 | Step 670000 | Avg Loss: 0.0152 | Grad Norm: 0.00996036\n",
      "Epoch 1 | Step 670100 | Avg Loss: 0.0152 | Grad Norm: 0.00884147\n",
      "Epoch 1 | Step 670200 | Avg Loss: 0.0151 | Grad Norm: 0.01021331\n",
      "Epoch 1 | Step 670300 | Avg Loss: 0.0149 | Grad Norm: 0.00960507\n",
      "Epoch 1 | Step 670400 | Avg Loss: 0.0149 | Grad Norm: 0.00918045\n",
      "Epoch 1 | Step 670500 | Avg Loss: 0.0150 | Grad Norm: 0.00823091\n",
      "Epoch 1 | Step 670600 | Avg Loss: 0.0150 | Grad Norm: 0.00890712\n",
      "Epoch 1 | Step 670700 | Avg Loss: 0.0150 | Grad Norm: 0.00902516\n",
      "Epoch 1 | Step 670800 | Avg Loss: 0.0147 | Grad Norm: 0.01004216\n",
      "Epoch 1 | Step 670900 | Avg Loss: 0.0145 | Grad Norm: 0.00893903\n",
      "Epoch 1 | Step 671000 | Avg Loss: 0.0145 | Grad Norm: 0.00756408\n",
      "Epoch 1 | Step 671100 | Avg Loss: 0.0145 | Grad Norm: 0.00817962\n",
      "Epoch 1 | Step 671200 | Avg Loss: 0.0143 | Grad Norm: 0.00714368\n",
      "Epoch 1 | Step 671300 | Avg Loss: 0.0142 | Grad Norm: 0.00742909\n",
      "Epoch 1 | Step 671400 | Avg Loss: 0.0142 | Grad Norm: 0.00799309\n",
      "Epoch 1 | Step 671500 | Avg Loss: 0.0142 | Grad Norm: 0.00810101\n",
      "Epoch 1 | Step 671600 | Avg Loss: 0.0144 | Grad Norm: 0.00658909\n",
      "Epoch 1 | Step 671700 | Avg Loss: 0.0144 | Grad Norm: 0.00867680\n",
      "Epoch 1 | Step 671800 | Avg Loss: 0.0146 | Grad Norm: 0.01039930\n",
      "Epoch 1 | Step 671900 | Avg Loss: 0.0149 | Grad Norm: 0.00775458\n",
      "Epoch 1 | Step 672000 | Avg Loss: 0.0147 | Grad Norm: 0.00876289\n",
      "Epoch 1 | Step 672100 | Avg Loss: 0.0148 | Grad Norm: 0.00831704\n",
      "Epoch 1 | Step 672200 | Avg Loss: 0.0151 | Grad Norm: 0.00891948\n",
      "Epoch 1 | Step 672300 | Avg Loss: 0.0150 | Grad Norm: 0.00878295\n",
      "Epoch 1 | Step 672400 | Avg Loss: 0.0146 | Grad Norm: 0.00958369\n",
      "Epoch 1 | Step 672500 | Avg Loss: 0.0148 | Grad Norm: 0.00793020\n",
      "Epoch 1 | Step 672600 | Avg Loss: 0.0149 | Grad Norm: 0.00755812\n",
      "Epoch 1 | Step 672700 | Avg Loss: 0.0150 | Grad Norm: 0.00943911\n",
      "Epoch 1 | Step 672800 | Avg Loss: 0.0150 | Grad Norm: 0.00835069\n",
      "Epoch 1 | Step 672900 | Avg Loss: 0.0155 | Grad Norm: 0.00916970\n",
      "Epoch 1 | Step 673000 | Avg Loss: 0.0150 | Grad Norm: 0.00859351\n",
      "Epoch 1 | Step 673100 | Avg Loss: 0.0150 | Grad Norm: 0.00918489\n",
      "Epoch 1 | Step 673200 | Avg Loss: 0.0150 | Grad Norm: 0.00819793\n",
      "Epoch 1 | Step 673300 | Avg Loss: 0.0148 | Grad Norm: 0.00781561\n",
      "Epoch 1 | Step 673400 | Avg Loss: 0.0145 | Grad Norm: 0.00804566\n",
      "Epoch 1 | Step 673500 | Avg Loss: 0.0147 | Grad Norm: 0.01238161\n",
      "Epoch 1 | Step 673600 | Avg Loss: 0.0148 | Grad Norm: 0.00813753\n",
      "Epoch 1 | Step 673700 | Avg Loss: 0.0148 | Grad Norm: 0.00950151\n",
      "Epoch 1 | Step 673800 | Avg Loss: 0.0152 | Grad Norm: 0.00918124\n",
      "Epoch 1 | Step 673900 | Avg Loss: 0.0149 | Grad Norm: 0.00799690\n",
      "Epoch 1 | Step 674000 | Avg Loss: 0.0147 | Grad Norm: 0.00887233\n",
      "Epoch 1 | Step 674100 | Avg Loss: 0.0149 | Grad Norm: 0.00790198\n",
      "Epoch 1 | Step 674200 | Avg Loss: 0.0151 | Grad Norm: 0.01030602\n",
      "Epoch 1 | Step 674300 | Avg Loss: 0.0152 | Grad Norm: 0.00733885\n",
      "Epoch 1 | Step 674400 | Avg Loss: 0.0150 | Grad Norm: 0.00902792\n",
      "Epoch 1 | Step 674500 | Avg Loss: 0.0148 | Grad Norm: 0.00988083\n",
      "Epoch 1 | Step 674600 | Avg Loss: 0.0149 | Grad Norm: 0.00891708\n",
      "Epoch 1 | Step 674700 | Avg Loss: 0.0149 | Grad Norm: 0.00834701\n",
      "Epoch 1 | Step 674800 | Avg Loss: 0.0153 | Grad Norm: 0.01006743\n",
      "Epoch 1 | Step 674900 | Avg Loss: 0.0148 | Grad Norm: 0.01006069\n",
      "Epoch 1 | Step 675000 | Avg Loss: 0.0145 | Grad Norm: 0.00924187\n",
      "Epoch 1 | Step 675100 | Avg Loss: 0.0140 | Grad Norm: 0.00832034\n",
      "Epoch 1 | Step 675200 | Avg Loss: 0.0142 | Grad Norm: 0.00874888\n",
      "Epoch 1 | Step 675300 | Avg Loss: 0.0140 | Grad Norm: 0.01023911\n",
      "Epoch 1 | Step 675400 | Avg Loss: 0.0139 | Grad Norm: 0.00849301\n",
      "Epoch 1 | Step 675500 | Avg Loss: 0.0140 | Grad Norm: 0.00717728\n",
      "Epoch 1 | Step 675600 | Avg Loss: 0.0142 | Grad Norm: 0.01011460\n",
      "Epoch 1 | Step 675700 | Avg Loss: 0.0143 | Grad Norm: 0.00804073\n",
      "Epoch 1 | Step 675800 | Avg Loss: 0.0142 | Grad Norm: 0.00862216\n",
      "Epoch 1 | Step 675900 | Avg Loss: 0.0142 | Grad Norm: 0.00973448\n",
      "Epoch 1 | Step 676000 | Avg Loss: 0.0145 | Grad Norm: 0.00835967\n",
      "Epoch 1 | Step 676100 | Avg Loss: 0.0144 | Grad Norm: 0.00851461\n",
      "Epoch 1 | Step 676200 | Avg Loss: 0.0145 | Grad Norm: 0.00821226\n",
      "Epoch 1 | Step 676300 | Avg Loss: 0.0148 | Grad Norm: 0.00901037\n",
      "Epoch 1 | Step 676400 | Avg Loss: 0.0145 | Grad Norm: 0.00905459\n",
      "Epoch 1 | Step 676500 | Avg Loss: 0.0142 | Grad Norm: 0.00893928\n",
      "Epoch 1 | Step 676600 | Avg Loss: 0.0142 | Grad Norm: 0.00868298\n",
      "Epoch 1 | Step 676700 | Avg Loss: 0.0142 | Grad Norm: 0.00838468\n",
      "Epoch 1 | Step 676800 | Avg Loss: 0.0144 | Grad Norm: 0.00875190\n",
      "Epoch 1 | Step 676900 | Avg Loss: 0.0146 | Grad Norm: 0.00844846\n",
      "Epoch 1 | Step 677000 | Avg Loss: 0.0147 | Grad Norm: 0.00818249\n",
      "Epoch 1 | Step 677100 | Avg Loss: 0.0149 | Grad Norm: 0.00833859\n",
      "Epoch 1 | Step 677200 | Avg Loss: 0.0148 | Grad Norm: 0.00916525\n",
      "Epoch 1 | Step 677300 | Avg Loss: 0.0151 | Grad Norm: 0.00949441\n",
      "Epoch 1 | Step 677400 | Avg Loss: 0.0153 | Grad Norm: 0.00911558\n",
      "Epoch 1 | Step 677500 | Avg Loss: 0.0148 | Grad Norm: 0.00842659\n",
      "Epoch 1 | Step 677600 | Avg Loss: 0.0149 | Grad Norm: 0.00910596\n",
      "Epoch 1 | Step 677700 | Avg Loss: 0.0146 | Grad Norm: 0.00783060\n",
      "Epoch 1 | Step 677800 | Avg Loss: 0.0143 | Grad Norm: 0.00870242\n",
      "Epoch 1 | Step 677900 | Avg Loss: 0.0143 | Grad Norm: 0.00929348\n",
      "Epoch 1 | Step 678000 | Avg Loss: 0.0145 | Grad Norm: 0.00837713\n",
      "Epoch 1 | Step 678100 | Avg Loss: 0.0145 | Grad Norm: 0.00803861\n",
      "Epoch 1 | Step 678200 | Avg Loss: 0.0150 | Grad Norm: 0.00850467\n",
      "Epoch 1 | Step 678300 | Avg Loss: 0.0147 | Grad Norm: 0.01037989\n",
      "Epoch 1 | Step 678400 | Avg Loss: 0.0145 | Grad Norm: 0.00939787\n",
      "Epoch 1 | Step 678500 | Avg Loss: 0.0146 | Grad Norm: 0.00898643\n",
      "Epoch 1 | Step 678600 | Avg Loss: 0.0154 | Grad Norm: 0.00813017\n",
      "Epoch 1 | Step 678700 | Avg Loss: 0.0153 | Grad Norm: 0.00962299\n",
      "Epoch 1 | Step 678800 | Avg Loss: 0.0151 | Grad Norm: 0.00826901\n",
      "Epoch 1 | Step 678900 | Avg Loss: 0.0148 | Grad Norm: 0.00761109\n",
      "Epoch 1 | Step 679000 | Avg Loss: 0.0149 | Grad Norm: 0.00762093\n",
      "Epoch 1 | Step 679100 | Avg Loss: 0.0150 | Grad Norm: 0.00803354\n",
      "Epoch 1 | Step 679200 | Avg Loss: 0.0153 | Grad Norm: 0.00945046\n",
      "Epoch 1 | Step 679300 | Avg Loss: 0.0155 | Grad Norm: 0.00909923\n",
      "Epoch 1 | Step 679400 | Avg Loss: 0.0150 | Grad Norm: 0.00710140\n",
      "Epoch 1 | Step 679500 | Avg Loss: 0.0153 | Grad Norm: 0.00918168\n",
      "Epoch 1 | Step 679600 | Avg Loss: 0.0152 | Grad Norm: 0.00734829\n",
      "Epoch 1 | Step 679700 | Avg Loss: 0.0150 | Grad Norm: 0.00806165\n",
      "Epoch 1 | Step 679800 | Avg Loss: 0.0152 | Grad Norm: 0.00875888\n",
      "Epoch 1 | Step 679900 | Avg Loss: 0.0151 | Grad Norm: 0.00859974\n",
      "Epoch 1 | Step 680000 | Avg Loss: 0.0152 | Grad Norm: 0.00948921\n",
      "Epoch 1 | Step 680100 | Avg Loss: 0.0154 | Grad Norm: 0.00875311\n",
      "Epoch 1 | Step 680200 | Avg Loss: 0.0154 | Grad Norm: 0.00903986\n",
      "Epoch 1 | Step 680300 | Avg Loss: 0.0153 | Grad Norm: 0.00934306\n",
      "Epoch 1 | Step 680400 | Avg Loss: 0.0152 | Grad Norm: 0.00897618\n",
      "Epoch 1 | Step 680500 | Avg Loss: 0.0154 | Grad Norm: 0.00961471\n",
      "Epoch 1 | Step 680600 | Avg Loss: 0.0152 | Grad Norm: 0.00886600\n",
      "Epoch 1 | Step 680700 | Avg Loss: 0.0154 | Grad Norm: 0.00866864\n",
      "Epoch 1 | Step 680800 | Avg Loss: 0.0150 | Grad Norm: 0.00931088\n",
      "Epoch 1 | Step 680900 | Avg Loss: 0.0149 | Grad Norm: 0.00893276\n",
      "Epoch 1 | Step 681000 | Avg Loss: 0.0148 | Grad Norm: 0.00792113\n",
      "Epoch 1 | Step 681100 | Avg Loss: 0.0145 | Grad Norm: 0.00847664\n",
      "Epoch 1 | Step 681200 | Avg Loss: 0.0148 | Grad Norm: 0.00982295\n",
      "Epoch 1 | Step 681300 | Avg Loss: 0.0148 | Grad Norm: 0.00836851\n",
      "Epoch 1 | Step 681400 | Avg Loss: 0.0151 | Grad Norm: 0.00843663\n",
      "Epoch 1 | Step 681500 | Avg Loss: 0.0150 | Grad Norm: 0.00806858\n",
      "Epoch 1 | Step 681600 | Avg Loss: 0.0147 | Grad Norm: 0.00928408\n",
      "Epoch 1 | Step 681700 | Avg Loss: 0.0147 | Grad Norm: 0.00974667\n",
      "Epoch 1 | Step 681800 | Avg Loss: 0.0146 | Grad Norm: 0.00979361\n",
      "Epoch 1 | Step 681900 | Avg Loss: 0.0145 | Grad Norm: 0.00882966\n",
      "Epoch 1 | Step 682000 | Avg Loss: 0.0147 | Grad Norm: 0.01022930\n",
      "Epoch 1 | Step 682100 | Avg Loss: 0.0149 | Grad Norm: 0.00827887\n",
      "Epoch 1 | Step 682200 | Avg Loss: 0.0149 | Grad Norm: 0.00860445\n",
      "Epoch 1 | Step 682300 | Avg Loss: 0.0149 | Grad Norm: 0.00982236\n",
      "Epoch 1 | Step 682400 | Avg Loss: 0.0145 | Grad Norm: 0.00914434\n",
      "Epoch 1 | Step 682500 | Avg Loss: 0.0145 | Grad Norm: 0.00860277\n",
      "Epoch 1 | Step 682600 | Avg Loss: 0.0145 | Grad Norm: 0.00882509\n",
      "Epoch 1 | Step 682700 | Avg Loss: 0.0143 | Grad Norm: 0.00962881\n",
      "Epoch 1 | Step 682800 | Avg Loss: 0.0146 | Grad Norm: 0.01382068\n",
      "Epoch 1 | Step 682900 | Avg Loss: 0.0149 | Grad Norm: 0.00843061\n",
      "Epoch 1 | Step 683000 | Avg Loss: 0.0151 | Grad Norm: 0.00879209\n",
      "Epoch 1 | Step 683100 | Avg Loss: 0.0150 | Grad Norm: 0.00863205\n",
      "Epoch 1 | Step 683200 | Avg Loss: 0.0145 | Grad Norm: 0.00727568\n",
      "Epoch 1 | Step 683300 | Avg Loss: 0.0145 | Grad Norm: 0.00877216\n",
      "Epoch 1 | Step 683400 | Avg Loss: 0.0142 | Grad Norm: 0.00917159\n",
      "Epoch 1 | Step 683500 | Avg Loss: 0.0139 | Grad Norm: 0.00944778\n",
      "Epoch 1 | Step 683600 | Avg Loss: 0.0142 | Grad Norm: 0.00887641\n",
      "Epoch 1 | Step 683700 | Avg Loss: 0.0142 | Grad Norm: 0.00828590\n",
      "Epoch 1 | Step 683800 | Avg Loss: 0.0143 | Grad Norm: 0.00888918\n",
      "Epoch 1 | Step 683900 | Avg Loss: 0.0143 | Grad Norm: 0.00814567\n",
      "Epoch 1 | Step 684000 | Avg Loss: 0.0146 | Grad Norm: 0.00738046\n",
      "Epoch 1 | Step 684100 | Avg Loss: 0.0143 | Grad Norm: 0.00713850\n",
      "Epoch 1 | Step 684200 | Avg Loss: 0.0145 | Grad Norm: 0.00910770\n",
      "Epoch 1 | Step 684300 | Avg Loss: 0.0146 | Grad Norm: 0.00831445\n",
      "Epoch 1 | Step 684400 | Avg Loss: 0.0147 | Grad Norm: 0.00820858\n",
      "Epoch 1 | Step 684500 | Avg Loss: 0.0148 | Grad Norm: 0.00924951\n",
      "Epoch 1 | Step 684600 | Avg Loss: 0.0152 | Grad Norm: 0.00772317\n",
      "Epoch 1 | Step 684700 | Avg Loss: 0.0150 | Grad Norm: 0.00931021\n",
      "Epoch 1 | Step 684800 | Avg Loss: 0.0152 | Grad Norm: 0.00828162\n",
      "Epoch 1 | Step 684900 | Avg Loss: 0.0151 | Grad Norm: 0.00933644\n",
      "Epoch 1 | Step 685000 | Avg Loss: 0.0150 | Grad Norm: 0.00798328\n",
      "Epoch 1 | Step 685100 | Avg Loss: 0.0149 | Grad Norm: 0.00865705\n",
      "Epoch 1 | Step 685200 | Avg Loss: 0.0156 | Grad Norm: 0.00853150\n",
      "Epoch 1 | Step 685300 | Avg Loss: 0.0152 | Grad Norm: 0.00866576\n",
      "Epoch 1 | Step 685400 | Avg Loss: 0.0152 | Grad Norm: 0.01023576\n",
      "Epoch 1 | Step 685500 | Avg Loss: 0.0153 | Grad Norm: 0.00863182\n",
      "Epoch 1 | Step 685600 | Avg Loss: 0.0153 | Grad Norm: 0.00941326\n",
      "Epoch 1 | Step 685700 | Avg Loss: 0.0154 | Grad Norm: 0.00852028\n",
      "Epoch 1 | Step 685800 | Avg Loss: 0.0152 | Grad Norm: 0.00851413\n",
      "Epoch 1 | Step 685900 | Avg Loss: 0.0146 | Grad Norm: 0.00760793\n",
      "Epoch 1 | Step 686000 | Avg Loss: 0.0145 | Grad Norm: 0.00972147\n",
      "Epoch 1 | Step 686100 | Avg Loss: 0.0147 | Grad Norm: 0.00839023\n",
      "Epoch 1 | Step 686200 | Avg Loss: 0.0143 | Grad Norm: 0.00881155\n",
      "Epoch 1 | Step 686300 | Avg Loss: 0.0146 | Grad Norm: 0.00830124\n",
      "Epoch 1 | Step 686400 | Avg Loss: 0.0149 | Grad Norm: 0.00986470\n",
      "Epoch 1 | Step 686500 | Avg Loss: 0.0149 | Grad Norm: 0.00834290\n",
      "Epoch 1 | Step 686600 | Avg Loss: 0.0148 | Grad Norm: 0.00841038\n",
      "Epoch 1 | Step 686700 | Avg Loss: 0.0147 | Grad Norm: 0.00867195\n",
      "Epoch 1 | Step 686800 | Avg Loss: 0.0152 | Grad Norm: 0.00822350\n",
      "Epoch 1 | Step 686900 | Avg Loss: 0.0152 | Grad Norm: 0.00784231\n",
      "Epoch 1 | Step 687000 | Avg Loss: 0.0154 | Grad Norm: 0.00881700\n",
      "Epoch 1 | Step 687100 | Avg Loss: 0.0154 | Grad Norm: 0.00917030\n",
      "Epoch 1 | Step 687200 | Avg Loss: 0.0149 | Grad Norm: 0.00854270\n",
      "Epoch 1 | Step 687300 | Avg Loss: 0.0151 | Grad Norm: 0.00694177\n",
      "Epoch 1 | Step 687400 | Avg Loss: 0.0147 | Grad Norm: 0.00836453\n",
      "Epoch 1 | Step 687500 | Avg Loss: 0.0155 | Grad Norm: 0.00807813\n",
      "Epoch 1 | Step 687600 | Avg Loss: 0.0153 | Grad Norm: 0.00873670\n",
      "Epoch 1 | Step 687700 | Avg Loss: 0.0154 | Grad Norm: 0.00769917\n",
      "Epoch 1 | Step 687800 | Avg Loss: 0.0155 | Grad Norm: 0.00988915\n",
      "Epoch 1 | Step 687900 | Avg Loss: 0.0155 | Grad Norm: 0.00859267\n",
      "Epoch 1 | Step 688000 | Avg Loss: 0.0149 | Grad Norm: 0.00840261\n",
      "Epoch 1 | Step 688100 | Avg Loss: 0.0147 | Grad Norm: 0.00723103\n",
      "Epoch 1 | Step 688200 | Avg Loss: 0.0152 | Grad Norm: 0.00837662\n",
      "Epoch 1 | Step 688300 | Avg Loss: 0.0154 | Grad Norm: 0.00763182\n",
      "Epoch 1 | Step 688400 | Avg Loss: 0.0149 | Grad Norm: 0.00897585\n",
      "Epoch 1 | Step 688500 | Avg Loss: 0.0149 | Grad Norm: 0.00807247\n",
      "Epoch 1 | Step 688600 | Avg Loss: 0.0149 | Grad Norm: 0.00865588\n",
      "Epoch 1 | Step 688700 | Avg Loss: 0.0148 | Grad Norm: 0.00857825\n",
      "Epoch 1 | Step 688800 | Avg Loss: 0.0147 | Grad Norm: 0.00833247\n",
      "Epoch 1 | Step 688900 | Avg Loss: 0.0148 | Grad Norm: 0.00897487\n",
      "Epoch 1 | Step 689000 | Avg Loss: 0.0144 | Grad Norm: 0.00769727\n",
      "Epoch 1 | Step 689100 | Avg Loss: 0.0144 | Grad Norm: 0.00814372\n",
      "Epoch 1 | Step 689200 | Avg Loss: 0.0144 | Grad Norm: 0.00745719\n",
      "Epoch 1 | Step 689300 | Avg Loss: 0.0148 | Grad Norm: 0.00881665\n",
      "Epoch 1 | Step 689400 | Avg Loss: 0.0147 | Grad Norm: 0.00763369\n",
      "Epoch 1 | Step 689500 | Avg Loss: 0.0149 | Grad Norm: 0.00864394\n",
      "Epoch 1 | Step 689600 | Avg Loss: 0.0147 | Grad Norm: 0.00900326\n",
      "Epoch 1 | Step 689700 | Avg Loss: 0.0147 | Grad Norm: 0.00923390\n",
      "Epoch 1 | Step 689800 | Avg Loss: 0.0144 | Grad Norm: 0.00889764\n",
      "Epoch 1 | Step 689900 | Avg Loss: 0.0142 | Grad Norm: 0.00879263\n",
      "Epoch 1 | Step 690000 | Avg Loss: 0.0142 | Grad Norm: 0.00929482\n",
      "Epoch 1 | Step 690100 | Avg Loss: 0.0144 | Grad Norm: 0.00855256\n",
      "Epoch 1 | Step 690200 | Avg Loss: 0.0143 | Grad Norm: 0.00837075\n",
      "Epoch 1 | Step 690300 | Avg Loss: 0.0141 | Grad Norm: 0.00878725\n",
      "Epoch 1 | Step 690400 | Avg Loss: 0.0148 | Grad Norm: 0.00831692\n",
      "Epoch 1 | Step 690500 | Avg Loss: 0.0147 | Grad Norm: 0.00858319\n",
      "Epoch 1 | Step 690600 | Avg Loss: 0.0148 | Grad Norm: 0.00842787\n",
      "Epoch 1 | Step 690700 | Avg Loss: 0.0148 | Grad Norm: 0.00947958\n",
      "Epoch 1 | Step 690800 | Avg Loss: 0.0147 | Grad Norm: 0.00786139\n",
      "Epoch 1 | Step 690900 | Avg Loss: 0.0152 | Grad Norm: 0.00860113\n",
      "Epoch 1 | Step 691000 | Avg Loss: 0.0149 | Grad Norm: 0.00834607\n",
      "Epoch 1 | Step 691100 | Avg Loss: 0.0145 | Grad Norm: 0.00726098\n",
      "Epoch 1 | Step 691200 | Avg Loss: 0.0146 | Grad Norm: 0.00755768\n",
      "Epoch 1 | Step 691300 | Avg Loss: 0.0148 | Grad Norm: 0.00951290\n",
      "Epoch 1 | Step 691400 | Avg Loss: 0.0147 | Grad Norm: 0.00780675\n",
      "Epoch 1 | Step 691500 | Avg Loss: 0.0147 | Grad Norm: 0.00927190\n",
      "Epoch 1 | Step 691600 | Avg Loss: 0.0145 | Grad Norm: 0.00937139\n",
      "Epoch 1 | Step 691700 | Avg Loss: 0.0151 | Grad Norm: 0.00852558\n",
      "Epoch 1 | Step 691800 | Avg Loss: 0.0146 | Grad Norm: 0.00741342\n",
      "Epoch 1 | Step 691900 | Avg Loss: 0.0148 | Grad Norm: 0.00758525\n",
      "Epoch 1 | Step 692000 | Avg Loss: 0.0147 | Grad Norm: 0.00861342\n",
      "Epoch 1 | Step 692100 | Avg Loss: 0.0148 | Grad Norm: 0.00822770\n",
      "Epoch 1 | Step 692200 | Avg Loss: 0.0150 | Grad Norm: 0.00757475\n",
      "Epoch 1 | Step 692300 | Avg Loss: 0.0150 | Grad Norm: 0.00876149\n",
      "Epoch 1 | Step 692400 | Avg Loss: 0.0152 | Grad Norm: 0.00751919\n",
      "Epoch 1 | Step 692500 | Avg Loss: 0.0153 | Grad Norm: 0.00806279\n",
      "Epoch 1 | Step 692600 | Avg Loss: 0.0149 | Grad Norm: 0.00808364\n",
      "Epoch 1 | Step 692700 | Avg Loss: 0.0149 | Grad Norm: 0.00897042\n",
      "Epoch 1 | Step 692800 | Avg Loss: 0.0152 | Grad Norm: 0.00859223\n",
      "Epoch 1 | Step 692900 | Avg Loss: 0.0151 | Grad Norm: 0.00908945\n",
      "Epoch 1 | Step 693000 | Avg Loss: 0.0151 | Grad Norm: 0.00831141\n",
      "Epoch 1 | Step 693100 | Avg Loss: 0.0151 | Grad Norm: 0.00942364\n",
      "Epoch 1 | Step 693200 | Avg Loss: 0.0151 | Grad Norm: 0.00872998\n",
      "Epoch 1 | Step 693300 | Avg Loss: 0.0152 | Grad Norm: 0.00840524\n",
      "Epoch 1 | Step 693400 | Avg Loss: 0.0148 | Grad Norm: 0.00919788\n",
      "Epoch 1 | Step 693500 | Avg Loss: 0.0149 | Grad Norm: 0.01032082\n",
      "Epoch 1 | Step 693600 | Avg Loss: 0.0147 | Grad Norm: 0.00808835\n",
      "Epoch 1 | Step 693700 | Avg Loss: 0.0150 | Grad Norm: 0.00900855\n",
      "Epoch 1 | Step 693800 | Avg Loss: 0.0148 | Grad Norm: 0.00796634\n",
      "Epoch 1 | Step 693900 | Avg Loss: 0.0150 | Grad Norm: 0.01045702\n",
      "Epoch 1 | Step 694000 | Avg Loss: 0.0152 | Grad Norm: 0.00897510\n",
      "Epoch 1 | Step 694100 | Avg Loss: 0.0150 | Grad Norm: 0.00885671\n",
      "Epoch 1 | Step 694200 | Avg Loss: 0.0154 | Grad Norm: 0.00839413\n",
      "Epoch 1 | Step 694300 | Avg Loss: 0.0159 | Grad Norm: 0.00864312\n",
      "Epoch 1 | Step 694400 | Avg Loss: 0.0155 | Grad Norm: 0.00947840\n",
      "Epoch 1 | Step 694500 | Avg Loss: 0.0150 | Grad Norm: 0.00766578\n",
      "Epoch 1 | Step 694600 | Avg Loss: 0.0152 | Grad Norm: 0.00954043\n",
      "Epoch 1 | Step 694700 | Avg Loss: 0.0151 | Grad Norm: 0.00952253\n",
      "Epoch 1 | Step 694800 | Avg Loss: 0.0152 | Grad Norm: 0.00812708\n",
      "Epoch 1 | Step 694900 | Avg Loss: 0.0152 | Grad Norm: 0.00925434\n",
      "Epoch 1 | Step 695000 | Avg Loss: 0.0151 | Grad Norm: 0.00877430\n",
      "Epoch 1 | Step 695100 | Avg Loss: 0.0153 | Grad Norm: 0.00797378\n",
      "Epoch 1 | Step 695200 | Avg Loss: 0.0150 | Grad Norm: 0.00826918\n",
      "Epoch 1 | Step 695300 | Avg Loss: 0.0149 | Grad Norm: 0.00851924\n",
      "Epoch 1 | Step 695400 | Avg Loss: 0.0148 | Grad Norm: 0.00887983\n",
      "Epoch 1 | Step 695500 | Avg Loss: 0.0148 | Grad Norm: 0.01058821\n",
      "Epoch 1 | Step 695600 | Avg Loss: 0.0152 | Grad Norm: 0.00945632\n",
      "Epoch 1 | Step 695700 | Avg Loss: 0.0149 | Grad Norm: 0.00983758\n",
      "Epoch 1 | Step 695800 | Avg Loss: 0.0151 | Grad Norm: 0.00942156\n",
      "Epoch 1 | Step 695900 | Avg Loss: 0.0149 | Grad Norm: 0.00814147\n",
      "Epoch 1 | Step 696000 | Avg Loss: 0.0151 | Grad Norm: 0.00867280\n",
      "Epoch 1 | Step 696100 | Avg Loss: 0.0153 | Grad Norm: 0.00879524\n",
      "Epoch 1 | Step 696200 | Avg Loss: 0.0149 | Grad Norm: 0.00973636\n",
      "Epoch 1 | Step 696300 | Avg Loss: 0.0148 | Grad Norm: 0.00828925\n",
      "Epoch 1 | Step 696400 | Avg Loss: 0.0148 | Grad Norm: 0.00889966\n",
      "Epoch 1 | Step 696500 | Avg Loss: 0.0149 | Grad Norm: 0.00755238\n",
      "Epoch 1 | Step 696600 | Avg Loss: 0.0149 | Grad Norm: 0.01000803\n",
      "Epoch 1 | Step 696700 | Avg Loss: 0.0150 | Grad Norm: 0.00749879\n",
      "Epoch 1 | Step 696800 | Avg Loss: 0.0147 | Grad Norm: 0.00861045\n",
      "Epoch 1 | Step 696900 | Avg Loss: 0.0150 | Grad Norm: 0.00789724\n",
      "Epoch 1 | Step 697000 | Avg Loss: 0.0150 | Grad Norm: 0.00787027\n",
      "Epoch 1 | Step 697100 | Avg Loss: 0.0147 | Grad Norm: 0.00857142\n",
      "Epoch 1 | Step 697200 | Avg Loss: 0.0151 | Grad Norm: 0.01004353\n",
      "Epoch 1 | Step 697300 | Avg Loss: 0.0153 | Grad Norm: 0.00887700\n",
      "Epoch 1 | Step 697400 | Avg Loss: 0.0155 | Grad Norm: 0.01014188\n",
      "Epoch 1 | Step 697500 | Avg Loss: 0.0153 | Grad Norm: 0.00864044\n",
      "Epoch 1 | Step 697600 | Avg Loss: 0.0152 | Grad Norm: 0.00832935\n",
      "Epoch 1 | Step 697700 | Avg Loss: 0.0154 | Grad Norm: 0.00798764\n",
      "Epoch 1 | Step 697800 | Avg Loss: 0.0156 | Grad Norm: 0.00980032\n",
      "Epoch 1 | Step 697900 | Avg Loss: 0.0154 | Grad Norm: 0.01100779\n",
      "Epoch 1 | Step 698000 | Avg Loss: 0.0151 | Grad Norm: 0.01106933\n",
      "Epoch 1 | Step 698100 | Avg Loss: 0.0150 | Grad Norm: 0.00925914\n",
      "Epoch 1 | Step 698200 | Avg Loss: 0.0152 | Grad Norm: 0.00892275\n",
      "Epoch 1 | Step 698300 | Avg Loss: 0.0154 | Grad Norm: 0.00889094\n",
      "Epoch 1 | Step 698400 | Avg Loss: 0.0156 | Grad Norm: 0.01015891\n",
      "Epoch 1 | Step 698500 | Avg Loss: 0.0153 | Grad Norm: 0.00869215\n",
      "Epoch 1 | Step 698600 | Avg Loss: 0.0153 | Grad Norm: 0.00851528\n",
      "Epoch 1 | Step 698700 | Avg Loss: 0.0153 | Grad Norm: 0.01054130\n",
      "Epoch 1 | Step 698800 | Avg Loss: 0.0152 | Grad Norm: 0.00861190\n",
      "Epoch 1 | Step 698900 | Avg Loss: 0.0149 | Grad Norm: 0.00838924\n",
      "Epoch 1 | Step 699000 | Avg Loss: 0.0149 | Grad Norm: 0.00886159\n",
      "Epoch 1 | Step 699100 | Avg Loss: 0.0147 | Grad Norm: 0.00828217\n",
      "Epoch 1 | Step 699200 | Avg Loss: 0.0151 | Grad Norm: 0.00918512\n",
      "Epoch 1 | Step 699300 | Avg Loss: 0.0153 | Grad Norm: 0.01178726\n",
      "Epoch 1 | Step 699400 | Avg Loss: 0.0150 | Grad Norm: 0.00811951\n",
      "Epoch 1 | Step 699500 | Avg Loss: 0.0149 | Grad Norm: 0.00739922\n",
      "Epoch 1 | Step 699600 | Avg Loss: 0.0152 | Grad Norm: 0.00902717\n",
      "Epoch 1 | Step 699700 | Avg Loss: 0.0148 | Grad Norm: 0.00870314\n",
      "Epoch 1 | Step 699800 | Avg Loss: 0.0149 | Grad Norm: 0.00846953\n",
      "Epoch 1 | Step 699900 | Avg Loss: 0.0147 | Grad Norm: 0.00854124\n",
      "Epoch 1 | Step 700000 | Avg Loss: 0.0145 | Grad Norm: 0.00749076\n",
      "Saving model at step700000\n",
      "Epoch 1 | Step 700100 | Avg Loss: 0.0146 | Grad Norm: 0.00855785\n",
      "Epoch 1 | Step 700200 | Avg Loss: 0.0150 | Grad Norm: 0.00899824\n",
      "Epoch 1 | Step 700300 | Avg Loss: 0.0150 | Grad Norm: 0.00893033\n",
      "Epoch 1 | Step 700400 | Avg Loss: 0.0153 | Grad Norm: 0.00946397\n",
      "Epoch 1 | Step 700500 | Avg Loss: 0.0152 | Grad Norm: 0.00905744\n",
      "Epoch 1 | Step 700600 | Avg Loss: 0.0151 | Grad Norm: 0.00793917\n",
      "Epoch 1 | Step 700700 | Avg Loss: 0.0147 | Grad Norm: 0.00805777\n",
      "Epoch 1 | Step 700800 | Avg Loss: 0.0147 | Grad Norm: 0.00723889\n",
      "Epoch 1 | Step 700900 | Avg Loss: 0.0148 | Grad Norm: 0.00802898\n",
      "Epoch 1 | Step 701000 | Avg Loss: 0.0145 | Grad Norm: 0.00847226\n",
      "Epoch 1 | Step 701100 | Avg Loss: 0.0151 | Grad Norm: 0.01019829\n",
      "Epoch 1 | Step 701200 | Avg Loss: 0.0150 | Grad Norm: 0.00894190\n",
      "Epoch 1 | Step 701300 | Avg Loss: 0.0146 | Grad Norm: 0.00862474\n",
      "Epoch 1 | Step 701400 | Avg Loss: 0.0147 | Grad Norm: 0.00852800\n",
      "Epoch 1 | Step 701500 | Avg Loss: 0.0149 | Grad Norm: 0.00863546\n",
      "Epoch 1 | Step 701600 | Avg Loss: 0.0149 | Grad Norm: 0.00881613\n",
      "Epoch 1 | Step 701700 | Avg Loss: 0.0151 | Grad Norm: 0.00836483\n",
      "Epoch 1 | Step 701800 | Avg Loss: 0.0150 | Grad Norm: 0.00896175\n",
      "Epoch 1 | Step 701900 | Avg Loss: 0.0149 | Grad Norm: 0.00781545\n",
      "Epoch 1 | Step 702000 | Avg Loss: 0.0148 | Grad Norm: 0.00807489\n",
      "Epoch 1 | Step 702100 | Avg Loss: 0.0150 | Grad Norm: 0.00968764\n",
      "Epoch 1 | Step 702200 | Avg Loss: 0.0151 | Grad Norm: 0.00845809\n",
      "Epoch 1 | Step 702300 | Avg Loss: 0.0147 | Grad Norm: 0.00877442\n",
      "Epoch 1 | Step 702400 | Avg Loss: 0.0146 | Grad Norm: 0.00796345\n",
      "Epoch 1 | Step 702500 | Avg Loss: 0.0147 | Grad Norm: 0.00778614\n",
      "Epoch 1 | Step 702600 | Avg Loss: 0.0149 | Grad Norm: 0.01141335\n",
      "Epoch 1 | Step 702700 | Avg Loss: 0.0149 | Grad Norm: 0.00960343\n",
      "Epoch 1 | Step 702800 | Avg Loss: 0.0147 | Grad Norm: 0.00953011\n",
      "Epoch 1 | Step 702900 | Avg Loss: 0.0144 | Grad Norm: 0.00865673\n",
      "Epoch 1 | Step 703000 | Avg Loss: 0.0145 | Grad Norm: 0.00841389\n",
      "Epoch 1 | Step 703100 | Avg Loss: 0.0149 | Grad Norm: 0.00997569\n",
      "Epoch 1 | Step 703200 | Avg Loss: 0.0146 | Grad Norm: 0.00924455\n",
      "Epoch 1 | Step 703300 | Avg Loss: 0.0147 | Grad Norm: 0.00917188\n",
      "Epoch 1 | Step 703400 | Avg Loss: 0.0151 | Grad Norm: 0.00921733\n",
      "Epoch 1 | Step 703500 | Avg Loss: 0.0147 | Grad Norm: 0.00959141\n",
      "Epoch 1 | Step 703600 | Avg Loss: 0.0146 | Grad Norm: 0.00903477\n",
      "Epoch 1 | Step 703700 | Avg Loss: 0.0148 | Grad Norm: 0.00851831\n",
      "Epoch 1 | Step 703800 | Avg Loss: 0.0149 | Grad Norm: 0.00908514\n",
      "Epoch 1 | Step 703900 | Avg Loss: 0.0150 | Grad Norm: 0.00879346\n",
      "Epoch 1 | Step 704000 | Avg Loss: 0.0150 | Grad Norm: 0.00838515\n",
      "Epoch 1 | Step 704100 | Avg Loss: 0.0146 | Grad Norm: 0.01041429\n",
      "Epoch 1 | Step 704200 | Avg Loss: 0.0150 | Grad Norm: 0.00869440\n",
      "Epoch 1 | Step 704300 | Avg Loss: 0.0148 | Grad Norm: 0.00885376\n",
      "Epoch 1 | Step 704400 | Avg Loss: 0.0146 | Grad Norm: 0.00919879\n",
      "Epoch 1 | Step 704500 | Avg Loss: 0.0145 | Grad Norm: 0.00880140\n",
      "Epoch 1 | Step 704600 | Avg Loss: 0.0146 | Grad Norm: 0.00807027\n",
      "Epoch 1 | Step 704700 | Avg Loss: 0.0149 | Grad Norm: 0.00784204\n",
      "Epoch 1 | Step 704800 | Avg Loss: 0.0147 | Grad Norm: 0.00878715\n",
      "Epoch 1 | Step 704900 | Avg Loss: 0.0147 | Grad Norm: 0.00880139\n",
      "Epoch 1 | Step 705000 | Avg Loss: 0.0146 | Grad Norm: 0.00865274\n",
      "Epoch 1 | Step 705100 | Avg Loss: 0.0149 | Grad Norm: 0.00973629\n",
      "Epoch 1 | Step 705200 | Avg Loss: 0.0147 | Grad Norm: 0.00913828\n",
      "Epoch 1 | Step 705300 | Avg Loss: 0.0145 | Grad Norm: 0.00928369\n",
      "Epoch 1 | Step 705400 | Avg Loss: 0.0147 | Grad Norm: 0.00783929\n",
      "Epoch 1 | Step 705500 | Avg Loss: 0.0147 | Grad Norm: 0.00806728\n",
      "Epoch 1 | Step 705600 | Avg Loss: 0.0148 | Grad Norm: 0.00920936\n",
      "Epoch 1 | Step 705700 | Avg Loss: 0.0150 | Grad Norm: 0.00898255\n",
      "Epoch 1 | Step 705800 | Avg Loss: 0.0148 | Grad Norm: 0.00960910\n",
      "Epoch 1 | Step 705900 | Avg Loss: 0.0152 | Grad Norm: 0.00863843\n",
      "Epoch 1 | Step 706000 | Avg Loss: 0.0145 | Grad Norm: 0.00821882\n",
      "Epoch 1 | Step 706100 | Avg Loss: 0.0145 | Grad Norm: 0.00865967\n",
      "Epoch 1 | Step 706200 | Avg Loss: 0.0147 | Grad Norm: 0.00788333\n",
      "Epoch 1 | Step 706300 | Avg Loss: 0.0144 | Grad Norm: 0.00983680\n",
      "Epoch 1 | Step 706400 | Avg Loss: 0.0148 | Grad Norm: 0.00952905\n",
      "Epoch 1 | Step 706500 | Avg Loss: 0.0153 | Grad Norm: 0.00815461\n",
      "Epoch 1 | Step 706600 | Avg Loss: 0.0153 | Grad Norm: 0.00836680\n",
      "Epoch 1 | Step 706700 | Avg Loss: 0.0153 | Grad Norm: 0.00849482\n",
      "Epoch 1 | Step 706800 | Avg Loss: 0.0152 | Grad Norm: 0.00880493\n",
      "Epoch 1 | Step 706900 | Avg Loss: 0.0151 | Grad Norm: 0.01112153\n",
      "Epoch 1 | Step 707000 | Avg Loss: 0.0151 | Grad Norm: 0.00909029\n",
      "Epoch 1 | Step 707100 | Avg Loss: 0.0151 | Grad Norm: 0.00884900\n",
      "Epoch 1 | Step 707200 | Avg Loss: 0.0150 | Grad Norm: 0.01078555\n",
      "Epoch 1 | Step 707300 | Avg Loss: 0.0149 | Grad Norm: 0.00790728\n",
      "Epoch 1 | Step 707400 | Avg Loss: 0.0149 | Grad Norm: 0.00906439\n",
      "Epoch 1 | Step 707500 | Avg Loss: 0.0149 | Grad Norm: 0.01171354\n",
      "Epoch 1 | Step 707600 | Avg Loss: 0.0151 | Grad Norm: 0.00969959\n",
      "Epoch 1 | Step 707700 | Avg Loss: 0.0150 | Grad Norm: 0.00977749\n",
      "Epoch 1 | Step 707800 | Avg Loss: 0.0153 | Grad Norm: 0.00808276\n",
      "Epoch 1 | Step 707900 | Avg Loss: 0.0151 | Grad Norm: 0.01178332\n",
      "Epoch 1 | Step 708000 | Avg Loss: 0.0147 | Grad Norm: 0.00868577\n",
      "Epoch 1 | Step 708100 | Avg Loss: 0.0147 | Grad Norm: 0.01201499\n",
      "Epoch 1 | Step 708200 | Avg Loss: 0.0147 | Grad Norm: 0.00857516\n",
      "Epoch 1 | Step 708300 | Avg Loss: 0.0150 | Grad Norm: 0.00826880\n",
      "Epoch 1 | Step 708400 | Avg Loss: 0.0155 | Grad Norm: 0.00943057\n",
      "Epoch 1 | Step 708500 | Avg Loss: 0.0155 | Grad Norm: 0.01028829\n",
      "Epoch 1 | Step 708600 | Avg Loss: 0.0154 | Grad Norm: 0.00868390\n",
      "Epoch 1 | Step 708700 | Avg Loss: 0.0149 | Grad Norm: 0.00962662\n",
      "Epoch 1 | Step 708800 | Avg Loss: 0.0150 | Grad Norm: 0.01065363\n",
      "Epoch 1 | Step 708900 | Avg Loss: 0.0146 | Grad Norm: 0.00744529\n",
      "Epoch 1 | Step 709000 | Avg Loss: 0.0150 | Grad Norm: 0.00987940\n",
      "Epoch 1 | Step 709100 | Avg Loss: 0.0150 | Grad Norm: 0.00949485\n",
      "Epoch 1 | Step 709200 | Avg Loss: 0.0148 | Grad Norm: 0.01028362\n",
      "Epoch 1 | Step 709300 | Avg Loss: 0.0146 | Grad Norm: 0.00864860\n",
      "Epoch 1 | Step 709400 | Avg Loss: 0.0148 | Grad Norm: 0.00833295\n",
      "Epoch 1 | Step 709500 | Avg Loss: 0.0149 | Grad Norm: 0.00925000\n",
      "Epoch 1 | Step 709600 | Avg Loss: 0.0145 | Grad Norm: 0.00939111\n",
      "Epoch 1 | Step 709700 | Avg Loss: 0.0145 | Grad Norm: 0.00817537\n",
      "Epoch 1 | Step 709800 | Avg Loss: 0.0145 | Grad Norm: 0.00877630\n",
      "Epoch 1 | Step 709900 | Avg Loss: 0.0147 | Grad Norm: 0.00874939\n",
      "Epoch 1 | Step 710000 | Avg Loss: 0.0150 | Grad Norm: 0.00810770\n",
      "Epoch 1 | Step 710100 | Avg Loss: 0.0153 | Grad Norm: 0.00910677\n",
      "Epoch 1 | Step 710200 | Avg Loss: 0.0152 | Grad Norm: 0.00863696\n",
      "Epoch 1 | Step 710300 | Avg Loss: 0.0147 | Grad Norm: 0.00991565\n",
      "Epoch 1 | Step 710400 | Avg Loss: 0.0148 | Grad Norm: 0.00775407\n",
      "Epoch 1 | Step 710500 | Avg Loss: 0.0148 | Grad Norm: 0.00948016\n",
      "Epoch 1 | Step 710600 | Avg Loss: 0.0151 | Grad Norm: 0.00868885\n",
      "Epoch 1 | Step 710700 | Avg Loss: 0.0149 | Grad Norm: 0.00851416\n",
      "Epoch 1 | Step 710800 | Avg Loss: 0.0153 | Grad Norm: 0.00922537\n",
      "Epoch 1 | Step 710900 | Avg Loss: 0.0150 | Grad Norm: 0.00872504\n",
      "Epoch 1 | Step 711000 | Avg Loss: 0.0148 | Grad Norm: 0.00833854\n",
      "Epoch 1 | Step 711100 | Avg Loss: 0.0147 | Grad Norm: 0.00785415\n",
      "Epoch 1 | Step 711200 | Avg Loss: 0.0144 | Grad Norm: 0.00880424\n",
      "Epoch 1 | Step 711300 | Avg Loss: 0.0145 | Grad Norm: 0.00767459\n",
      "Epoch 1 | Step 711400 | Avg Loss: 0.0144 | Grad Norm: 0.00775163\n",
      "Epoch 1 | Step 711500 | Avg Loss: 0.0143 | Grad Norm: 0.00956745\n",
      "Epoch 1 | Step 711600 | Avg Loss: 0.0146 | Grad Norm: 0.00991889\n",
      "Epoch 1 | Step 711700 | Avg Loss: 0.0144 | Grad Norm: 0.00808992\n",
      "Epoch 1 | Step 711800 | Avg Loss: 0.0144 | Grad Norm: 0.00771303\n",
      "Epoch 1 | Step 711900 | Avg Loss: 0.0147 | Grad Norm: 0.00961692\n",
      "Epoch 1 | Step 712000 | Avg Loss: 0.0151 | Grad Norm: 0.00924587\n",
      "Epoch 1 | Step 712100 | Avg Loss: 0.0153 | Grad Norm: 0.00759866\n",
      "Epoch 1 | Step 712200 | Avg Loss: 0.0149 | Grad Norm: 0.00933162\n",
      "Epoch 1 | Step 712300 | Avg Loss: 0.0150 | Grad Norm: 0.00998668\n",
      "Epoch 1 | Step 712400 | Avg Loss: 0.0150 | Grad Norm: 0.00837906\n",
      "Epoch 1 | Step 712500 | Avg Loss: 0.0149 | Grad Norm: 0.00855269\n",
      "Epoch 1 | Step 712600 | Avg Loss: 0.0147 | Grad Norm: 0.00877519\n",
      "Epoch 1 | Step 712700 | Avg Loss: 0.0147 | Grad Norm: 0.00884972\n",
      "Epoch 1 | Step 712800 | Avg Loss: 0.0149 | Grad Norm: 0.00870870\n",
      "Epoch 1 | Step 712900 | Avg Loss: 0.0148 | Grad Norm: 0.00888119\n",
      "Epoch 1 | Step 713000 | Avg Loss: 0.0146 | Grad Norm: 0.00938167\n",
      "Epoch 1 | Step 713100 | Avg Loss: 0.0147 | Grad Norm: 0.00824461\n",
      "Epoch 1 | Step 713200 | Avg Loss: 0.0148 | Grad Norm: 0.00808987\n",
      "Epoch 1 | Step 713300 | Avg Loss: 0.0148 | Grad Norm: 0.00780696\n",
      "Epoch 1 | Step 713400 | Avg Loss: 0.0149 | Grad Norm: 0.00826164\n",
      "Epoch 1 | Step 713500 | Avg Loss: 0.0152 | Grad Norm: 0.01159748\n",
      "Epoch 1 | Step 713600 | Avg Loss: 0.0153 | Grad Norm: 0.00815835\n",
      "Epoch 1 | Step 713700 | Avg Loss: 0.0150 | Grad Norm: 0.00879899\n",
      "Epoch 1 | Step 713800 | Avg Loss: 0.0151 | Grad Norm: 0.00956143\n",
      "Epoch 1 | Step 713900 | Avg Loss: 0.0153 | Grad Norm: 0.01007738\n",
      "Epoch 1 | Step 714000 | Avg Loss: 0.0148 | Grad Norm: 0.00869032\n",
      "Epoch 1 | Step 714100 | Avg Loss: 0.0150 | Grad Norm: 0.00926504\n",
      "Epoch 1 | Step 714200 | Avg Loss: 0.0146 | Grad Norm: 0.00915339\n",
      "Epoch 1 | Step 714300 | Avg Loss: 0.0148 | Grad Norm: 0.00953272\n",
      "Epoch 1 | Step 714400 | Avg Loss: 0.0143 | Grad Norm: 0.00968731\n",
      "Epoch 1 | Step 714500 | Avg Loss: 0.0144 | Grad Norm: 0.00819382\n",
      "Epoch 1 | Step 714600 | Avg Loss: 0.0146 | Grad Norm: 0.00834173\n",
      "Epoch 1 | Step 714700 | Avg Loss: 0.0148 | Grad Norm: 0.00939854\n",
      "Epoch 1 | Step 714800 | Avg Loss: 0.0147 | Grad Norm: 0.00909118\n",
      "Epoch 1 | Step 714900 | Avg Loss: 0.0141 | Grad Norm: 0.00832882\n",
      "Epoch 1 | Step 715000 | Avg Loss: 0.0141 | Grad Norm: 0.00727978\n",
      "Epoch 1 | Step 715100 | Avg Loss: 0.0149 | Grad Norm: 0.00871418\n",
      "Epoch 1 | Step 715200 | Avg Loss: 0.0147 | Grad Norm: 0.00965497\n",
      "Epoch 1 | Step 715300 | Avg Loss: 0.0144 | Grad Norm: 0.00906767\n",
      "Epoch 1 | Step 715400 | Avg Loss: 0.0147 | Grad Norm: 0.00920175\n",
      "Epoch 1 | Step 715500 | Avg Loss: 0.0147 | Grad Norm: 0.01176722\n",
      "Epoch 1 | Step 715600 | Avg Loss: 0.0149 | Grad Norm: 0.01163534\n",
      "Epoch 1 | Step 715700 | Avg Loss: 0.0151 | Grad Norm: 0.00984066\n",
      "Epoch 1 | Step 715800 | Avg Loss: 0.0153 | Grad Norm: 0.00915790\n",
      "Epoch 1 | Step 715900 | Avg Loss: 0.0150 | Grad Norm: 0.00901402\n",
      "Epoch 1 | Step 716000 | Avg Loss: 0.0151 | Grad Norm: 0.00985811\n",
      "Epoch 1 | Step 716100 | Avg Loss: 0.0152 | Grad Norm: 0.00960175\n",
      "Epoch 1 | Step 716200 | Avg Loss: 0.0154 | Grad Norm: 0.00765652\n",
      "Epoch 1 | Step 716300 | Avg Loss: 0.0153 | Grad Norm: 0.00782460\n",
      "Epoch 1 | Step 716400 | Avg Loss: 0.0153 | Grad Norm: 0.00780169\n",
      "Epoch 1 | Step 716500 | Avg Loss: 0.0152 | Grad Norm: 0.01005162\n",
      "Epoch 1 | Step 716600 | Avg Loss: 0.0148 | Grad Norm: 0.00825078\n",
      "Epoch 1 | Step 716700 | Avg Loss: 0.0148 | Grad Norm: 0.00831297\n",
      "Epoch 1 | Step 716800 | Avg Loss: 0.0151 | Grad Norm: 0.00882300\n",
      "Epoch 1 | Step 716900 | Avg Loss: 0.0154 | Grad Norm: 0.00842702\n",
      "Epoch 1 | Step 717000 | Avg Loss: 0.0152 | Grad Norm: 0.00872365\n",
      "Epoch 1 | Step 717100 | Avg Loss: 0.0152 | Grad Norm: 0.00852457\n",
      "Epoch 1 | Step 717200 | Avg Loss: 0.0150 | Grad Norm: 0.00782975\n",
      "Epoch 1 | Step 717300 | Avg Loss: 0.0149 | Grad Norm: 0.00928727\n",
      "Epoch 1 | Step 717400 | Avg Loss: 0.0149 | Grad Norm: 0.00793421\n",
      "Epoch 1 | Step 717500 | Avg Loss: 0.0151 | Grad Norm: 0.01002658\n",
      "Epoch 1 | Step 717600 | Avg Loss: 0.0156 | Grad Norm: 0.00828589\n",
      "Epoch 1 | Step 717700 | Avg Loss: 0.0155 | Grad Norm: 0.01100538\n",
      "Epoch 1 | Step 717800 | Avg Loss: 0.0155 | Grad Norm: 0.00824028\n",
      "Epoch 1 | Step 717900 | Avg Loss: 0.0153 | Grad Norm: 0.00827463\n",
      "Epoch 1 | Step 718000 | Avg Loss: 0.0153 | Grad Norm: 0.00915083\n",
      "Epoch 1 | Step 718100 | Avg Loss: 0.0152 | Grad Norm: 0.00812269\n",
      "Epoch 1 | Step 718200 | Avg Loss: 0.0152 | Grad Norm: 0.00836377\n",
      "Epoch 1 | Step 718300 | Avg Loss: 0.0155 | Grad Norm: 0.01062839\n",
      "Epoch 1 | Step 718400 | Avg Loss: 0.0155 | Grad Norm: 0.00908325\n",
      "Epoch 1 | Step 718500 | Avg Loss: 0.0154 | Grad Norm: 0.00830108\n",
      "Epoch 1 | Step 718600 | Avg Loss: 0.0151 | Grad Norm: 0.00879396\n",
      "Epoch 1 | Step 718700 | Avg Loss: 0.0151 | Grad Norm: 0.00831277\n",
      "Epoch 1 | Step 718800 | Avg Loss: 0.0148 | Grad Norm: 0.00906487\n",
      "Epoch 1 | Step 718900 | Avg Loss: 0.0152 | Grad Norm: 0.00902259\n",
      "Epoch 1 | Step 719000 | Avg Loss: 0.0149 | Grad Norm: 0.00790770\n",
      "Epoch 1 | Step 719100 | Avg Loss: 0.0146 | Grad Norm: 0.00804353\n",
      "Epoch 1 | Step 719200 | Avg Loss: 0.0148 | Grad Norm: 0.00815642\n",
      "Epoch 1 | Step 719300 | Avg Loss: 0.0146 | Grad Norm: 0.00919054\n",
      "Epoch 1 | Step 719400 | Avg Loss: 0.0148 | Grad Norm: 0.01050963\n",
      "Epoch 1 | Step 719500 | Avg Loss: 0.0146 | Grad Norm: 0.00798692\n",
      "Epoch 1 | Step 719600 | Avg Loss: 0.0147 | Grad Norm: 0.00805976\n",
      "Epoch 1 | Step 719700 | Avg Loss: 0.0145 | Grad Norm: 0.00814986\n",
      "Epoch 1 | Step 719800 | Avg Loss: 0.0146 | Grad Norm: 0.00918201\n",
      "Epoch 1 | Step 719900 | Avg Loss: 0.0147 | Grad Norm: 0.00827317\n",
      "Epoch 1 | Step 720000 | Avg Loss: 0.0145 | Grad Norm: 0.00790425\n",
      "Epoch 1 | Step 720100 | Avg Loss: 0.0147 | Grad Norm: 0.00921105\n",
      "Epoch 1 | Step 720200 | Avg Loss: 0.0147 | Grad Norm: 0.00860515\n",
      "Epoch 1 | Step 720300 | Avg Loss: 0.0145 | Grad Norm: 0.00800019\n",
      "Epoch 1 | Step 720400 | Avg Loss: 0.0144 | Grad Norm: 0.00844265\n",
      "Epoch 1 | Step 720500 | Avg Loss: 0.0145 | Grad Norm: 0.00945303\n",
      "Epoch 1 | Step 720600 | Avg Loss: 0.0140 | Grad Norm: 0.00848186\n",
      "Epoch 1 | Step 720700 | Avg Loss: 0.0141 | Grad Norm: 0.00924131\n",
      "Epoch 1 | Step 720800 | Avg Loss: 0.0142 | Grad Norm: 0.01067380\n",
      "Epoch 1 | Step 720900 | Avg Loss: 0.0142 | Grad Norm: 0.00899132\n",
      "Epoch 1 | Step 721000 | Avg Loss: 0.0140 | Grad Norm: 0.00908160\n",
      "Epoch 1 | Step 721100 | Avg Loss: 0.0143 | Grad Norm: 0.00903924\n",
      "Epoch 1 | Step 721200 | Avg Loss: 0.0146 | Grad Norm: 0.00812785\n",
      "Epoch 1 | Step 721300 | Avg Loss: 0.0146 | Grad Norm: 0.00990278\n",
      "Epoch 1 | Step 721400 | Avg Loss: 0.0146 | Grad Norm: 0.00896469\n",
      "Epoch 1 | Step 721500 | Avg Loss: 0.0148 | Grad Norm: 0.00877652\n",
      "Epoch 1 | Step 721600 | Avg Loss: 0.0149 | Grad Norm: 0.01043248\n",
      "Epoch 1 | Step 721700 | Avg Loss: 0.0148 | Grad Norm: 0.01108858\n",
      "Epoch 1 | Step 721800 | Avg Loss: 0.0148 | Grad Norm: 0.00905626\n",
      "Epoch 1 | Step 721900 | Avg Loss: 0.0147 | Grad Norm: 0.01090913\n",
      "Epoch 1 | Step 722000 | Avg Loss: 0.0144 | Grad Norm: 0.00904774\n",
      "Epoch 1 | Step 722100 | Avg Loss: 0.0147 | Grad Norm: 0.00984634\n",
      "Epoch 1 | Step 722200 | Avg Loss: 0.0146 | Grad Norm: 0.00808455\n",
      "Epoch 1 | Step 722300 | Avg Loss: 0.0147 | Grad Norm: 0.01029676\n",
      "Epoch 1 | Step 722400 | Avg Loss: 0.0148 | Grad Norm: 0.00871191\n",
      "Epoch 1 | Step 722500 | Avg Loss: 0.0149 | Grad Norm: 0.00878445\n",
      "Epoch 1 | Step 722600 | Avg Loss: 0.0151 | Grad Norm: 0.00849443\n",
      "Epoch 1 | Step 722700 | Avg Loss: 0.0149 | Grad Norm: 0.00852608\n",
      "Epoch 1 | Step 722800 | Avg Loss: 0.0147 | Grad Norm: 0.00749404\n",
      "Epoch 1 | Step 722900 | Avg Loss: 0.0147 | Grad Norm: 0.00892515\n",
      "Epoch 1 | Step 723000 | Avg Loss: 0.0148 | Grad Norm: 0.00986149\n",
      "Epoch 1 | Step 723100 | Avg Loss: 0.0148 | Grad Norm: 0.00993114\n",
      "Epoch 1 | Step 723200 | Avg Loss: 0.0147 | Grad Norm: 0.00851316\n",
      "Epoch 1 | Step 723300 | Avg Loss: 0.0149 | Grad Norm: 0.00830663\n",
      "Epoch 1 | Step 723400 | Avg Loss: 0.0145 | Grad Norm: 0.00822263\n",
      "Epoch 1 | Step 723500 | Avg Loss: 0.0146 | Grad Norm: 0.00848618\n",
      "Epoch 1 | Step 723600 | Avg Loss: 0.0143 | Grad Norm: 0.00864923\n",
      "Epoch 1 | Step 723700 | Avg Loss: 0.0145 | Grad Norm: 0.00788073\n",
      "Epoch 1 | Step 723800 | Avg Loss: 0.0147 | Grad Norm: 0.00894092\n",
      "Epoch 1 | Step 723900 | Avg Loss: 0.0148 | Grad Norm: 0.00812826\n",
      "Epoch 1 | Step 724000 | Avg Loss: 0.0148 | Grad Norm: 0.00889275\n",
      "Epoch 1 | Step 724100 | Avg Loss: 0.0146 | Grad Norm: 0.00994026\n",
      "Epoch 1 | Step 724200 | Avg Loss: 0.0147 | Grad Norm: 0.00911909\n",
      "Epoch 1 | Step 724300 | Avg Loss: 0.0148 | Grad Norm: 0.00915398\n",
      "Epoch 1 | Step 724400 | Avg Loss: 0.0150 | Grad Norm: 0.00781471\n",
      "Epoch 1 | Step 724500 | Avg Loss: 0.0149 | Grad Norm: 0.01021623\n",
      "Epoch 1 | Step 724600 | Avg Loss: 0.0149 | Grad Norm: 0.00731333\n",
      "Epoch 1 | Step 724700 | Avg Loss: 0.0151 | Grad Norm: 0.00998593\n",
      "Epoch 1 | Step 724800 | Avg Loss: 0.0148 | Grad Norm: 0.00857367\n",
      "Epoch 1 | Step 724900 | Avg Loss: 0.0146 | Grad Norm: 0.00889147\n",
      "Epoch 1 | Step 725000 | Avg Loss: 0.0144 | Grad Norm: 0.00745659\n",
      "Epoch 1 | Step 725100 | Avg Loss: 0.0148 | Grad Norm: 0.00867009\n",
      "Epoch 1 | Step 725200 | Avg Loss: 0.0145 | Grad Norm: 0.00928192\n",
      "Epoch 1 | Step 725300 | Avg Loss: 0.0146 | Grad Norm: 0.00909793\n",
      "Epoch 1 | Step 725400 | Avg Loss: 0.0148 | Grad Norm: 0.00739730\n",
      "Epoch 1 | Step 725500 | Avg Loss: 0.0144 | Grad Norm: 0.00926628\n",
      "Epoch 1 | Step 725600 | Avg Loss: 0.0141 | Grad Norm: 0.00926446\n",
      "Epoch 1 | Step 725700 | Avg Loss: 0.0143 | Grad Norm: 0.00770538\n",
      "Epoch 1 | Step 725800 | Avg Loss: 0.0137 | Grad Norm: 0.00794050\n",
      "Epoch 1 | Step 725900 | Avg Loss: 0.0140 | Grad Norm: 0.00879258\n",
      "Epoch 1 | Step 726000 | Avg Loss: 0.0143 | Grad Norm: 0.00911615\n",
      "Epoch 1 | Step 726100 | Avg Loss: 0.0146 | Grad Norm: 0.00801156\n",
      "Epoch 1 | Step 726200 | Avg Loss: 0.0146 | Grad Norm: 0.00823945\n",
      "Epoch 1 | Step 726300 | Avg Loss: 0.0146 | Grad Norm: 0.00886739\n",
      "Epoch 1 | Step 726400 | Avg Loss: 0.0141 | Grad Norm: 0.00734545\n",
      "Epoch 1 | Step 726500 | Avg Loss: 0.0144 | Grad Norm: 0.00878586\n",
      "Epoch 1 | Step 726600 | Avg Loss: 0.0146 | Grad Norm: 0.00792788\n",
      "Epoch 1 | Step 726700 | Avg Loss: 0.0148 | Grad Norm: 0.01032410\n",
      "Epoch 1 | Step 726800 | Avg Loss: 0.0146 | Grad Norm: 0.00794505\n",
      "Epoch 1 | Step 726900 | Avg Loss: 0.0148 | Grad Norm: 0.01138884\n",
      "Epoch 1 | Step 727000 | Avg Loss: 0.0148 | Grad Norm: 0.00837868\n",
      "Epoch 1 | Step 727100 | Avg Loss: 0.0149 | Grad Norm: 0.00868629\n",
      "Epoch 1 | Step 727200 | Avg Loss: 0.0148 | Grad Norm: 0.00946670\n",
      "Epoch 1 | Step 727300 | Avg Loss: 0.0147 | Grad Norm: 0.01170259\n",
      "Epoch 1 | Step 727400 | Avg Loss: 0.0147 | Grad Norm: 0.00827464\n",
      "Epoch 1 | Step 727500 | Avg Loss: 0.0143 | Grad Norm: 0.00882875\n",
      "Epoch 1 | Step 727600 | Avg Loss: 0.0147 | Grad Norm: 0.01083290\n",
      "Epoch 1 | Step 727700 | Avg Loss: 0.0147 | Grad Norm: 0.01148849\n",
      "Epoch 1 | Step 727800 | Avg Loss: 0.0148 | Grad Norm: 0.00923418\n",
      "Epoch 1 | Step 727900 | Avg Loss: 0.0149 | Grad Norm: 0.00884787\n",
      "Epoch 1 | Step 728000 | Avg Loss: 0.0153 | Grad Norm: 0.00896154\n",
      "Epoch 1 | Step 728100 | Avg Loss: 0.0150 | Grad Norm: 0.01013513\n",
      "Epoch 1 | Step 728200 | Avg Loss: 0.0144 | Grad Norm: 0.00990523\n",
      "Epoch 1 | Step 728300 | Avg Loss: 0.0142 | Grad Norm: 0.00721737\n",
      "Epoch 1 | Step 728400 | Avg Loss: 0.0146 | Grad Norm: 0.00870814\n",
      "Epoch 1 | Step 728500 | Avg Loss: 0.0146 | Grad Norm: 0.00853017\n",
      "Epoch 1 | Step 728600 | Avg Loss: 0.0146 | Grad Norm: 0.00831057\n",
      "Epoch 1 | Step 728700 | Avg Loss: 0.0147 | Grad Norm: 0.00923288\n",
      "Epoch 1 | Step 728800 | Avg Loss: 0.0150 | Grad Norm: 0.00870050\n",
      "Epoch 1 | Step 728900 | Avg Loss: 0.0153 | Grad Norm: 0.00829338\n",
      "Epoch 1 | Step 729000 | Avg Loss: 0.0149 | Grad Norm: 0.00870088\n",
      "Epoch 1 | Step 729100 | Avg Loss: 0.0149 | Grad Norm: 0.01395525\n",
      "Epoch 1 | Step 729200 | Avg Loss: 0.0148 | Grad Norm: 0.00986233\n",
      "Epoch 1 | Step 729300 | Avg Loss: 0.0148 | Grad Norm: 0.00947829\n",
      "Epoch 1 | Step 729400 | Avg Loss: 0.0149 | Grad Norm: 0.00910534\n",
      "Epoch 1 | Step 729500 | Avg Loss: 0.0149 | Grad Norm: 0.00788731\n",
      "Epoch 1 | Step 729600 | Avg Loss: 0.0151 | Grad Norm: 0.00759977\n",
      "Epoch 1 | Step 729700 | Avg Loss: 0.0150 | Grad Norm: 0.00809397\n",
      "Epoch 1 | Step 729800 | Avg Loss: 0.0154 | Grad Norm: 0.00863564\n",
      "Epoch 1 | Step 729900 | Avg Loss: 0.0154 | Grad Norm: 0.00874562\n",
      "Epoch 1 | Step 730000 | Avg Loss: 0.0147 | Grad Norm: 0.00922499\n",
      "Epoch 1 | Step 730100 | Avg Loss: 0.0149 | Grad Norm: 0.00895873\n",
      "Epoch 1 | Step 730200 | Avg Loss: 0.0149 | Grad Norm: 0.00931776\n",
      "Epoch 1 | Step 730300 | Avg Loss: 0.0150 | Grad Norm: 0.00786056\n",
      "Epoch 1 | Step 730400 | Avg Loss: 0.0152 | Grad Norm: 0.00873688\n",
      "Epoch 1 | Step 730500 | Avg Loss: 0.0154 | Grad Norm: 0.01075640\n",
      "Epoch 1 | Step 730600 | Avg Loss: 0.0151 | Grad Norm: 0.00924159\n",
      "Epoch 1 | Step 730700 | Avg Loss: 0.0150 | Grad Norm: 0.01000898\n",
      "Epoch 1 | Step 730800 | Avg Loss: 0.0152 | Grad Norm: 0.00853408\n",
      "Epoch 1 | Step 730900 | Avg Loss: 0.0152 | Grad Norm: 0.01007958\n",
      "Epoch 1 | Step 731000 | Avg Loss: 0.0151 | Grad Norm: 0.00845606\n",
      "Epoch 1 | Step 731100 | Avg Loss: 0.0145 | Grad Norm: 0.00818151\n",
      "Epoch 1 | Step 731200 | Avg Loss: 0.0144 | Grad Norm: 0.00865040\n",
      "Epoch 1 | Step 731300 | Avg Loss: 0.0146 | Grad Norm: 0.00840668\n",
      "Epoch 1 | Step 731400 | Avg Loss: 0.0148 | Grad Norm: 0.00736686\n",
      "Epoch 1 | Step 731500 | Avg Loss: 0.0150 | Grad Norm: 0.00986094\n",
      "Epoch 1 | Step 731600 | Avg Loss: 0.0152 | Grad Norm: 0.00931715\n",
      "Epoch 1 | Step 731700 | Avg Loss: 0.0151 | Grad Norm: 0.00768600\n",
      "Epoch 1 | Step 731800 | Avg Loss: 0.0149 | Grad Norm: 0.00798102\n",
      "Epoch 1 | Step 731900 | Avg Loss: 0.0149 | Grad Norm: 0.00986059\n",
      "Epoch 1 | Step 732000 | Avg Loss: 0.0148 | Grad Norm: 0.00795580\n",
      "Epoch 1 | Step 732100 | Avg Loss: 0.0145 | Grad Norm: 0.00908865\n",
      "Epoch 1 | Step 732200 | Avg Loss: 0.0146 | Grad Norm: 0.00831590\n",
      "Epoch 1 | Step 732300 | Avg Loss: 0.0148 | Grad Norm: 0.00885653\n",
      "Epoch 1 | Step 732400 | Avg Loss: 0.0147 | Grad Norm: 0.00853652\n",
      "Epoch 1 | Step 732500 | Avg Loss: 0.0142 | Grad Norm: 0.00861888\n",
      "Epoch 1 | Step 732600 | Avg Loss: 0.0139 | Grad Norm: 0.00793647\n",
      "Epoch 1 | Step 732700 | Avg Loss: 0.0141 | Grad Norm: 0.00909875\n",
      "Epoch 1 | Step 732800 | Avg Loss: 0.0144 | Grad Norm: 0.00782371\n",
      "Epoch 1 | Step 732900 | Avg Loss: 0.0144 | Grad Norm: 0.00854544\n",
      "Epoch 1 | Step 733000 | Avg Loss: 0.0146 | Grad Norm: 0.00938243\n",
      "Epoch 1 | Step 733100 | Avg Loss: 0.0147 | Grad Norm: 0.00823544\n",
      "Epoch 1 | Step 733200 | Avg Loss: 0.0148 | Grad Norm: 0.00903146\n",
      "Epoch 1 | Step 733300 | Avg Loss: 0.0142 | Grad Norm: 0.00767239\n",
      "Epoch 1 | Step 733400 | Avg Loss: 0.0144 | Grad Norm: 0.00904757\n",
      "Epoch 1 | Step 733500 | Avg Loss: 0.0144 | Grad Norm: 0.00901772\n",
      "Epoch 1 | Step 733600 | Avg Loss: 0.0148 | Grad Norm: 0.00886278\n",
      "Epoch 1 | Step 733700 | Avg Loss: 0.0147 | Grad Norm: 0.00880832\n",
      "Epoch 1 | Step 733800 | Avg Loss: 0.0151 | Grad Norm: 0.00889531\n",
      "Epoch 1 | Step 733900 | Avg Loss: 0.0152 | Grad Norm: 0.00936725\n",
      "Epoch 1 | Step 734000 | Avg Loss: 0.0151 | Grad Norm: 0.01012394\n",
      "Epoch 1 | Step 734100 | Avg Loss: 0.0151 | Grad Norm: 0.00884720\n",
      "Epoch 1 | Step 734200 | Avg Loss: 0.0148 | Grad Norm: 0.00892225\n",
      "Epoch 1 | Step 734300 | Avg Loss: 0.0150 | Grad Norm: 0.00801841\n",
      "Epoch 1 | Step 734400 | Avg Loss: 0.0145 | Grad Norm: 0.00809528\n",
      "Epoch 1 | Step 734500 | Avg Loss: 0.0149 | Grad Norm: 0.00848084\n",
      "Epoch 1 | Step 734600 | Avg Loss: 0.0148 | Grad Norm: 0.01048613\n",
      "Epoch 1 | Step 734700 | Avg Loss: 0.0147 | Grad Norm: 0.00940344\n",
      "Epoch 1 | Step 734800 | Avg Loss: 0.0150 | Grad Norm: 0.00927395\n",
      "Epoch 1 | Step 734900 | Avg Loss: 0.0148 | Grad Norm: 0.00963106\n",
      "Epoch 1 | Step 735000 | Avg Loss: 0.0149 | Grad Norm: 0.00799745\n",
      "Epoch 1 | Step 735100 | Avg Loss: 0.0150 | Grad Norm: 0.00924234\n",
      "Epoch 1 | Step 735200 | Avg Loss: 0.0149 | Grad Norm: 0.00925072\n",
      "Epoch 1 | Step 735300 | Avg Loss: 0.0147 | Grad Norm: 0.00848065\n",
      "Epoch 1 | Step 735400 | Avg Loss: 0.0146 | Grad Norm: 0.00841241\n",
      "Epoch 1 | Step 735500 | Avg Loss: 0.0142 | Grad Norm: 0.00779276\n",
      "Epoch 1 | Step 735600 | Avg Loss: 0.0144 | Grad Norm: 0.00892815\n",
      "Epoch 1 | Step 735700 | Avg Loss: 0.0145 | Grad Norm: 0.00747049\n",
      "Epoch 1 | Step 735800 | Avg Loss: 0.0145 | Grad Norm: 0.00860709\n",
      "Epoch 1 | Step 735900 | Avg Loss: 0.0145 | Grad Norm: 0.00842315\n",
      "Epoch 1 | Step 736000 | Avg Loss: 0.0148 | Grad Norm: 0.00879357\n",
      "Epoch 1 | Step 736100 | Avg Loss: 0.0147 | Grad Norm: 0.00934989\n",
      "Epoch 1 | Step 736200 | Avg Loss: 0.0145 | Grad Norm: 0.00877251\n",
      "Epoch 1 | Step 736300 | Avg Loss: 0.0147 | Grad Norm: 0.00974066\n",
      "Epoch 1 | Step 736400 | Avg Loss: 0.0147 | Grad Norm: 0.00852899\n",
      "Epoch 1 | Step 736500 | Avg Loss: 0.0147 | Grad Norm: 0.00865962\n",
      "Epoch 1 | Step 736600 | Avg Loss: 0.0147 | Grad Norm: 0.00962720\n",
      "Epoch 1 | Step 736700 | Avg Loss: 0.0148 | Grad Norm: 0.00994419\n",
      "Epoch 1 | Step 736800 | Avg Loss: 0.0149 | Grad Norm: 0.00892988\n",
      "Epoch 1 | Step 736900 | Avg Loss: 0.0151 | Grad Norm: 0.00832131\n",
      "Epoch 1 | Step 737000 | Avg Loss: 0.0150 | Grad Norm: 0.00784353\n",
      "Epoch 1 | Step 737100 | Avg Loss: 0.0151 | Grad Norm: 0.01008585\n",
      "Epoch 1 | Step 737200 | Avg Loss: 0.0151 | Grad Norm: 0.01052663\n",
      "Epoch 1 | Step 737300 | Avg Loss: 0.0153 | Grad Norm: 0.00946421\n",
      "Epoch 1 | Step 737400 | Avg Loss: 0.0154 | Grad Norm: 0.01086234\n",
      "Epoch 1 | Step 737500 | Avg Loss: 0.0156 | Grad Norm: 0.00851198\n",
      "Epoch 1 | Step 737600 | Avg Loss: 0.0154 | Grad Norm: 0.00917669\n",
      "Epoch 1 | Step 737700 | Avg Loss: 0.0155 | Grad Norm: 0.00911557\n",
      "Epoch 1 | Step 737800 | Avg Loss: 0.0153 | Grad Norm: 0.00947335\n",
      "Epoch 1 | Step 737900 | Avg Loss: 0.0153 | Grad Norm: 0.00730085\n",
      "Epoch 1 | Step 738000 | Avg Loss: 0.0144 | Grad Norm: 0.00960685\n",
      "Epoch 1 | Step 738100 | Avg Loss: 0.0148 | Grad Norm: 0.00862899\n",
      "Epoch 1 | Step 738200 | Avg Loss: 0.0149 | Grad Norm: 0.01002131\n",
      "Epoch 1 | Step 738300 | Avg Loss: 0.0152 | Grad Norm: 0.00832055\n",
      "Epoch 1 | Step 738400 | Avg Loss: 0.0148 | Grad Norm: 0.00773998\n",
      "Epoch 1 | Step 738500 | Avg Loss: 0.0145 | Grad Norm: 0.00913898\n",
      "Epoch 1 | Step 738600 | Avg Loss: 0.0148 | Grad Norm: 0.00824395\n",
      "Epoch 1 | Step 738700 | Avg Loss: 0.0144 | Grad Norm: 0.00950300\n",
      "Epoch 1 | Step 738800 | Avg Loss: 0.0146 | Grad Norm: 0.00867515\n",
      "Epoch 1 | Step 738900 | Avg Loss: 0.0153 | Grad Norm: 0.00917636\n",
      "Epoch 1 | Step 739000 | Avg Loss: 0.0151 | Grad Norm: 0.00842544\n",
      "Epoch 1 | Step 739100 | Avg Loss: 0.0157 | Grad Norm: 0.00928932\n",
      "Epoch 1 | Step 739200 | Avg Loss: 0.0158 | Grad Norm: 0.00737534\n",
      "Epoch 1 | Step 739300 | Avg Loss: 0.0157 | Grad Norm: 0.00825349\n",
      "Epoch 1 | Step 739400 | Avg Loss: 0.0156 | Grad Norm: 0.00844186\n",
      "Epoch 1 | Step 739500 | Avg Loss: 0.0156 | Grad Norm: 0.00935859\n",
      "Epoch 1 | Step 739600 | Avg Loss: 0.0154 | Grad Norm: 0.00878734\n",
      "Epoch 1 | Step 739700 | Avg Loss: 0.0150 | Grad Norm: 0.00741014\n",
      "Epoch 1 | Step 739800 | Avg Loss: 0.0150 | Grad Norm: 0.00863963\n",
      "Epoch 1 | Step 739900 | Avg Loss: 0.0148 | Grad Norm: 0.00978313\n",
      "Epoch 1 | Step 740000 | Avg Loss: 0.0150 | Grad Norm: 0.00820240\n",
      "Epoch 1 | Step 740100 | Avg Loss: 0.0150 | Grad Norm: 0.00959583\n",
      "Epoch 1 | Step 740200 | Avg Loss: 0.0152 | Grad Norm: 0.00990866\n",
      "Epoch 1 | Step 740300 | Avg Loss: 0.0156 | Grad Norm: 0.00975901\n",
      "Epoch 1 | Step 740400 | Avg Loss: 0.0153 | Grad Norm: 0.01082840\n",
      "Epoch 1 | Step 740500 | Avg Loss: 0.0155 | Grad Norm: 0.00904568\n",
      "Epoch 1 | Step 740600 | Avg Loss: 0.0152 | Grad Norm: 0.01054788\n",
      "Epoch 1 | Step 740700 | Avg Loss: 0.0153 | Grad Norm: 0.00847146\n",
      "Epoch 1 | Step 740800 | Avg Loss: 0.0149 | Grad Norm: 0.00832396\n",
      "Epoch 1 | Step 740900 | Avg Loss: 0.0148 | Grad Norm: 0.00995262\n",
      "Epoch 1 | Step 741000 | Avg Loss: 0.0153 | Grad Norm: 0.00973334\n",
      "Epoch 1 | Step 741100 | Avg Loss: 0.0154 | Grad Norm: 0.00872266\n",
      "Epoch 1 | Step 741200 | Avg Loss: 0.0153 | Grad Norm: 0.00785033\n",
      "Epoch 1 | Step 741300 | Avg Loss: 0.0151 | Grad Norm: 0.00788365\n",
      "Epoch 1 | Step 741400 | Avg Loss: 0.0153 | Grad Norm: 0.00820507\n",
      "Epoch 1 | Step 741500 | Avg Loss: 0.0152 | Grad Norm: 0.00805212\n",
      "Epoch 1 | Step 741600 | Avg Loss: 0.0157 | Grad Norm: 0.01159700\n",
      "Epoch 1 | Step 741700 | Avg Loss: 0.0152 | Grad Norm: 0.00958191\n",
      "Epoch 1 | Step 741800 | Avg Loss: 0.0151 | Grad Norm: 0.00853814\n",
      "Epoch 1 | Step 741900 | Avg Loss: 0.0149 | Grad Norm: 0.00839561\n",
      "Epoch 1 | Step 742000 | Avg Loss: 0.0149 | Grad Norm: 0.00905500\n",
      "Epoch 1 | Step 742100 | Avg Loss: 0.0148 | Grad Norm: 0.00874369\n",
      "Epoch 1 | Step 742200 | Avg Loss: 0.0147 | Grad Norm: 0.00908835\n",
      "Epoch 1 | Step 742300 | Avg Loss: 0.0146 | Grad Norm: 0.00794810\n",
      "Epoch 1 | Step 742400 | Avg Loss: 0.0147 | Grad Norm: 0.00809052\n",
      "Epoch 1 | Step 742500 | Avg Loss: 0.0148 | Grad Norm: 0.00770379\n",
      "Epoch 1 | Step 742600 | Avg Loss: 0.0147 | Grad Norm: 0.00894585\n",
      "Epoch 1 | Step 742700 | Avg Loss: 0.0148 | Grad Norm: 0.01029386\n",
      "Epoch 1 | Step 742800 | Avg Loss: 0.0149 | Grad Norm: 0.00999875\n",
      "Epoch 1 | Step 742900 | Avg Loss: 0.0149 | Grad Norm: 0.00871157\n",
      "Epoch 1 | Step 743000 | Avg Loss: 0.0146 | Grad Norm: 0.01064268\n",
      "Epoch 1 | Step 743100 | Avg Loss: 0.0153 | Grad Norm: 0.00831909\n",
      "Epoch 1 | Step 743200 | Avg Loss: 0.0154 | Grad Norm: 0.00860694\n",
      "Epoch 1 | Step 743300 | Avg Loss: 0.0149 | Grad Norm: 0.00797620\n",
      "Epoch 1 | Step 743400 | Avg Loss: 0.0148 | Grad Norm: 0.00875567\n",
      "Epoch 1 | Step 743500 | Avg Loss: 0.0150 | Grad Norm: 0.01074224\n",
      "Epoch 1 | Step 743600 | Avg Loss: 0.0150 | Grad Norm: 0.00833579\n",
      "Epoch 1 | Step 743700 | Avg Loss: 0.0149 | Grad Norm: 0.00910470\n",
      "Epoch 1 | Step 743800 | Avg Loss: 0.0148 | Grad Norm: 0.00917755\n",
      "Epoch 1 | Step 743900 | Avg Loss: 0.0145 | Grad Norm: 0.00912769\n",
      "Epoch 1 | Step 744000 | Avg Loss: 0.0151 | Grad Norm: 0.00754041\n",
      "Epoch 1 | Step 744100 | Avg Loss: 0.0149 | Grad Norm: 0.00912621\n",
      "Epoch 1 | Step 744200 | Avg Loss: 0.0149 | Grad Norm: 0.00833482\n",
      "Epoch 1 | Step 744300 | Avg Loss: 0.0157 | Grad Norm: 0.00889487\n",
      "Epoch 1 | Step 744400 | Avg Loss: 0.0155 | Grad Norm: 0.00864943\n",
      "Epoch 1 | Step 744500 | Avg Loss: 0.0153 | Grad Norm: 0.00888335\n",
      "Epoch 1 | Step 744600 | Avg Loss: 0.0154 | Grad Norm: 0.00897322\n",
      "Epoch 1 | Step 744700 | Avg Loss: 0.0152 | Grad Norm: 0.01033026\n",
      "Epoch 1 | Step 744800 | Avg Loss: 0.0154 | Grad Norm: 0.00943687\n",
      "Epoch 1 | Step 744900 | Avg Loss: 0.0150 | Grad Norm: 0.00787762\n",
      "Epoch 1 | Step 745000 | Avg Loss: 0.0150 | Grad Norm: 0.00759406\n",
      "Epoch 1 | Step 745100 | Avg Loss: 0.0149 | Grad Norm: 0.00771098\n",
      "Epoch 1 | Step 745200 | Avg Loss: 0.0149 | Grad Norm: 0.00795468\n",
      "Epoch 1 | Step 745300 | Avg Loss: 0.0148 | Grad Norm: 0.00804332\n",
      "Epoch 1 | Step 745400 | Avg Loss: 0.0144 | Grad Norm: 0.00885879\n",
      "Epoch 1 | Step 745500 | Avg Loss: 0.0145 | Grad Norm: 0.00933179\n",
      "Epoch 1 | Step 745600 | Avg Loss: 0.0141 | Grad Norm: 0.00976797\n",
      "Epoch 1 | Step 745700 | Avg Loss: 0.0141 | Grad Norm: 0.00917010\n",
      "Epoch 1 | Step 745800 | Avg Loss: 0.0139 | Grad Norm: 0.00878839\n",
      "Epoch 1 | Step 745900 | Avg Loss: 0.0142 | Grad Norm: 0.00969455\n",
      "Epoch 1 | Step 746000 | Avg Loss: 0.0145 | Grad Norm: 0.00749250\n",
      "Epoch 1 | Step 746100 | Avg Loss: 0.0149 | Grad Norm: 0.00870868\n",
      "Epoch 1 | Step 746200 | Avg Loss: 0.0150 | Grad Norm: 0.00931239\n",
      "Epoch 1 | Step 746300 | Avg Loss: 0.0152 | Grad Norm: 0.00856200\n",
      "Epoch 1 | Step 746400 | Avg Loss: 0.0153 | Grad Norm: 0.00881953\n",
      "Epoch 1 | Step 746500 | Avg Loss: 0.0151 | Grad Norm: 0.00937053\n",
      "Epoch 1 | Step 746600 | Avg Loss: 0.0151 | Grad Norm: 0.00948322\n",
      "Epoch 1 | Step 746700 | Avg Loss: 0.0150 | Grad Norm: 0.00797294\n",
      "Epoch 1 | Step 746800 | Avg Loss: 0.0150 | Grad Norm: 0.00805385\n",
      "Epoch 1 | Step 746900 | Avg Loss: 0.0148 | Grad Norm: 0.00850598\n",
      "Epoch 1 | Step 747000 | Avg Loss: 0.0148 | Grad Norm: 0.00914939\n",
      "Epoch 1 | Step 747100 | Avg Loss: 0.0151 | Grad Norm: 0.00926997\n",
      "Epoch 1 | Step 747200 | Avg Loss: 0.0149 | Grad Norm: 0.00851860\n",
      "Epoch 1 | Step 747300 | Avg Loss: 0.0151 | Grad Norm: 0.00807223\n",
      "Epoch 1 | Step 747400 | Avg Loss: 0.0150 | Grad Norm: 0.00936986\n",
      "Epoch 1 | Step 747500 | Avg Loss: 0.0148 | Grad Norm: 0.00834242\n",
      "Epoch 1 | Step 747600 | Avg Loss: 0.0153 | Grad Norm: 0.01050605\n",
      "Epoch 1 | Step 747700 | Avg Loss: 0.0152 | Grad Norm: 0.00779132\n",
      "Epoch 1 | Step 747800 | Avg Loss: 0.0150 | Grad Norm: 0.00963437\n",
      "Epoch 1 | Step 747900 | Avg Loss: 0.0149 | Grad Norm: 0.00779387\n",
      "Epoch 1 | Step 748000 | Avg Loss: 0.0151 | Grad Norm: 0.00903924\n",
      "Epoch 1 | Step 748100 | Avg Loss: 0.0155 | Grad Norm: 0.00935105\n",
      "Epoch 1 | Step 748200 | Avg Loss: 0.0152 | Grad Norm: 0.00816818\n",
      "Epoch 1 | Step 748300 | Avg Loss: 0.0149 | Grad Norm: 0.00994162\n",
      "Epoch 1 | Step 748400 | Avg Loss: 0.0150 | Grad Norm: 0.00956103\n",
      "Epoch 1 | Step 748500 | Avg Loss: 0.0146 | Grad Norm: 0.00837849\n",
      "Epoch 1 | Step 748600 | Avg Loss: 0.0149 | Grad Norm: 0.00747957\n",
      "Epoch 1 | Step 748700 | Avg Loss: 0.0147 | Grad Norm: 0.00872819\n",
      "Epoch 1 | Step 748800 | Avg Loss: 0.0150 | Grad Norm: 0.00836712\n",
      "Epoch 1 | Step 748900 | Avg Loss: 0.0150 | Grad Norm: 0.00890039\n",
      "Epoch 1 | Step 749000 | Avg Loss: 0.0148 | Grad Norm: 0.00848741\n",
      "Epoch 1 | Step 749100 | Avg Loss: 0.0146 | Grad Norm: 0.00880405\n",
      "Epoch 1 | Step 749200 | Avg Loss: 0.0149 | Grad Norm: 0.00980732\n",
      "Epoch 1 | Step 749300 | Avg Loss: 0.0148 | Grad Norm: 0.00914069\n",
      "Epoch 1 | Step 749400 | Avg Loss: 0.0149 | Grad Norm: 0.00981072\n",
      "Epoch 1 | Step 749500 | Avg Loss: 0.0153 | Grad Norm: 0.00984625\n",
      "Epoch 1 | Step 749600 | Avg Loss: 0.0155 | Grad Norm: 0.01110841\n",
      "Epoch 1 | Step 749700 | Avg Loss: 0.0153 | Grad Norm: 0.00965385\n",
      "Epoch 1 | Step 749800 | Avg Loss: 0.0150 | Grad Norm: 0.00857856\n",
      "Epoch 1 | Step 749900 | Avg Loss: 0.0148 | Grad Norm: 0.00773081\n",
      "Epoch 1 | Step 750000 | Avg Loss: 0.0144 | Grad Norm: 0.00846179\n",
      "Epoch 1 | Step 750100 | Avg Loss: 0.0148 | Grad Norm: 0.01028374\n",
      "Epoch 1 | Step 750200 | Avg Loss: 0.0148 | Grad Norm: 0.00902171\n",
      "Epoch 1 | Step 750300 | Avg Loss: 0.0147 | Grad Norm: 0.00745020\n",
      "Epoch 1 | Step 750400 | Avg Loss: 0.0148 | Grad Norm: 0.01078310\n",
      "Epoch 1 | Step 750500 | Avg Loss: 0.0152 | Grad Norm: 0.00958382\n",
      "Epoch 1 | Step 750600 | Avg Loss: 0.0155 | Grad Norm: 0.00913421\n",
      "Epoch 1 | Step 750700 | Avg Loss: 0.0152 | Grad Norm: 0.00973113\n",
      "Epoch 1 | Step 750800 | Avg Loss: 0.0149 | Grad Norm: 0.00846868\n",
      "Epoch 1 | Step 750900 | Avg Loss: 0.0151 | Grad Norm: 0.00786089\n",
      "Epoch 1 | Step 751000 | Avg Loss: 0.0154 | Grad Norm: 0.01018810\n",
      "Epoch 1 | Step 751100 | Avg Loss: 0.0149 | Grad Norm: 0.00937545\n",
      "Epoch 1 | Step 751200 | Avg Loss: 0.0149 | Grad Norm: 0.00806950\n",
      "Epoch 1 | Step 751300 | Avg Loss: 0.0144 | Grad Norm: 0.00854043\n",
      "Epoch 1 | Step 751400 | Avg Loss: 0.0148 | Grad Norm: 0.00786327\n",
      "Epoch 1 | Step 751500 | Avg Loss: 0.0148 | Grad Norm: 0.00975116\n",
      "Epoch 1 | Step 751600 | Avg Loss: 0.0149 | Grad Norm: 0.00789892\n",
      "Epoch 1 | Step 751700 | Avg Loss: 0.0148 | Grad Norm: 0.00959360\n",
      "Epoch 1 | Step 751800 | Avg Loss: 0.0155 | Grad Norm: 0.00910105\n",
      "Epoch 1 | Step 751900 | Avg Loss: 0.0159 | Grad Norm: 0.00904922\n",
      "Epoch 1 | Step 752000 | Avg Loss: 0.0156 | Grad Norm: 0.00880662\n",
      "Epoch 1 | Step 752100 | Avg Loss: 0.0159 | Grad Norm: 0.00772169\n",
      "Epoch 1 | Step 752200 | Avg Loss: 0.0159 | Grad Norm: 0.00949799\n",
      "Epoch 1 | Step 752300 | Avg Loss: 0.0157 | Grad Norm: 0.00892746\n",
      "Epoch 1 | Step 752400 | Avg Loss: 0.0153 | Grad Norm: 0.00969401\n",
      "Epoch 1 | Step 752500 | Avg Loss: 0.0154 | Grad Norm: 0.00730507\n",
      "Epoch 1 | Step 752600 | Avg Loss: 0.0157 | Grad Norm: 0.00892879\n",
      "Epoch 1 | Step 752700 | Avg Loss: 0.0158 | Grad Norm: 0.00812934\n",
      "Epoch 1 | Step 752800 | Avg Loss: 0.0151 | Grad Norm: 0.00879735\n",
      "Epoch 1 | Step 752900 | Avg Loss: 0.0149 | Grad Norm: 0.00988005\n",
      "Epoch 1 | Step 753000 | Avg Loss: 0.0148 | Grad Norm: 0.00921010\n",
      "Epoch 1 | Step 753100 | Avg Loss: 0.0152 | Grad Norm: 0.00986381\n",
      "Epoch 1 | Step 753200 | Avg Loss: 0.0152 | Grad Norm: 0.00919001\n",
      "Epoch 1 | Step 753300 | Avg Loss: 0.0151 | Grad Norm: 0.00802622\n",
      "Epoch 1 | Step 753400 | Avg Loss: 0.0151 | Grad Norm: 0.00773741\n",
      "Epoch 1 | Step 753500 | Avg Loss: 0.0152 | Grad Norm: 0.00865397\n",
      "Epoch 1 | Step 753600 | Avg Loss: 0.0152 | Grad Norm: 0.00830290\n",
      "Epoch 1 | Step 753700 | Avg Loss: 0.0150 | Grad Norm: 0.00897147\n",
      "Epoch 1 | Step 753800 | Avg Loss: 0.0146 | Grad Norm: 0.00874078\n",
      "Epoch 1 | Step 753900 | Avg Loss: 0.0145 | Grad Norm: 0.00866705\n",
      "Epoch 1 | Step 754000 | Avg Loss: 0.0147 | Grad Norm: 0.00818356\n",
      "Epoch 1 | Step 754100 | Avg Loss: 0.0150 | Grad Norm: 0.00893157\n",
      "Epoch 1 | Step 754200 | Avg Loss: 0.0150 | Grad Norm: 0.01015837\n",
      "Epoch 1 | Step 754300 | Avg Loss: 0.0153 | Grad Norm: 0.00865119\n",
      "Epoch 1 | Step 754400 | Avg Loss: 0.0151 | Grad Norm: 0.00890855\n",
      "Epoch 1 | Step 754500 | Avg Loss: 0.0152 | Grad Norm: 0.00950035\n",
      "Epoch 1 | Step 754600 | Avg Loss: 0.0151 | Grad Norm: 0.00896799\n",
      "Epoch 1 | Step 754700 | Avg Loss: 0.0150 | Grad Norm: 0.00763023\n",
      "Epoch 1 | Step 754800 | Avg Loss: 0.0151 | Grad Norm: 0.00814468\n",
      "Epoch 1 | Step 754900 | Avg Loss: 0.0151 | Grad Norm: 0.00826584\n",
      "Epoch 1 | Step 755000 | Avg Loss: 0.0154 | Grad Norm: 0.00784591\n",
      "Epoch 1 | Step 755100 | Avg Loss: 0.0153 | Grad Norm: 0.00807042\n",
      "Epoch 1 | Step 755200 | Avg Loss: 0.0152 | Grad Norm: 0.00808113\n",
      "Epoch 1 | Step 755300 | Avg Loss: 0.0150 | Grad Norm: 0.00854748\n",
      "Epoch 1 | Step 755400 | Avg Loss: 0.0151 | Grad Norm: 0.00887504\n",
      "Epoch 1 | Step 755500 | Avg Loss: 0.0144 | Grad Norm: 0.00713739\n",
      "Epoch 1 | Step 755600 | Avg Loss: 0.0144 | Grad Norm: 0.00840005\n",
      "Epoch 1 | Step 755700 | Avg Loss: 0.0151 | Grad Norm: 0.00861291\n",
      "Epoch 1 | Step 755800 | Avg Loss: 0.0155 | Grad Norm: 0.00928756\n",
      "Epoch 1 | Step 755900 | Avg Loss: 0.0152 | Grad Norm: 0.00718967\n",
      "Epoch 1 | Step 756000 | Avg Loss: 0.0147 | Grad Norm: 0.01024132\n",
      "Epoch 1 | Step 756100 | Avg Loss: 0.0150 | Grad Norm: 0.00979784\n",
      "Epoch 1 | Step 756200 | Avg Loss: 0.0153 | Grad Norm: 0.00836399\n",
      "Epoch 1 | Step 756300 | Avg Loss: 0.0154 | Grad Norm: 0.00852073\n",
      "Epoch 1 | Step 756400 | Avg Loss: 0.0150 | Grad Norm: 0.00817023\n",
      "Epoch 1 | Step 756500 | Avg Loss: 0.0151 | Grad Norm: 0.00946928\n",
      "Epoch 1 | Step 756600 | Avg Loss: 0.0153 | Grad Norm: 0.00778954\n",
      "Epoch 1 | Step 756700 | Avg Loss: 0.0152 | Grad Norm: 0.00895767\n",
      "Epoch 1 | Step 756800 | Avg Loss: 0.0151 | Grad Norm: 0.00981969\n",
      "Epoch 1 | Step 756900 | Avg Loss: 0.0149 | Grad Norm: 0.01005755\n",
      "Epoch 1 | Step 757000 | Avg Loss: 0.0151 | Grad Norm: 0.01001511\n",
      "Epoch 1 | Step 757100 | Avg Loss: 0.0152 | Grad Norm: 0.00784038\n",
      "Epoch 1 | Step 757200 | Avg Loss: 0.0149 | Grad Norm: 0.01013126\n",
      "Epoch 1 | Step 757300 | Avg Loss: 0.0148 | Grad Norm: 0.00899984\n",
      "Epoch 1 | Step 757400 | Avg Loss: 0.0146 | Grad Norm: 0.00799900\n",
      "Epoch 1 | Step 757500 | Avg Loss: 0.0147 | Grad Norm: 0.01057680\n",
      "Epoch 1 | Step 757600 | Avg Loss: 0.0149 | Grad Norm: 0.00936703\n",
      "Epoch 1 | Step 757700 | Avg Loss: 0.0149 | Grad Norm: 0.00977786\n",
      "Epoch 1 | Step 757800 | Avg Loss: 0.0148 | Grad Norm: 0.00894504\n",
      "Epoch 1 | Step 757900 | Avg Loss: 0.0145 | Grad Norm: 0.00849484\n",
      "Epoch 1 | Step 758000 | Avg Loss: 0.0147 | Grad Norm: 0.00920953\n",
      "Epoch 1 | Step 758100 | Avg Loss: 0.0145 | Grad Norm: 0.00974852\n",
      "Epoch 1 | Step 758200 | Avg Loss: 0.0143 | Grad Norm: 0.00981718\n",
      "Epoch 1 | Step 758300 | Avg Loss: 0.0145 | Grad Norm: 0.00884450\n",
      "Epoch 1 | Step 758400 | Avg Loss: 0.0149 | Grad Norm: 0.00854649\n",
      "Epoch 1 | Step 758500 | Avg Loss: 0.0150 | Grad Norm: 0.00773957\n",
      "Epoch 1 | Step 758600 | Avg Loss: 0.0149 | Grad Norm: 0.00945732\n",
      "Epoch 1 | Step 758700 | Avg Loss: 0.0152 | Grad Norm: 0.00890562\n",
      "Epoch 1 | Step 758800 | Avg Loss: 0.0155 | Grad Norm: 0.00895744\n",
      "Epoch 1 | Step 758900 | Avg Loss: 0.0149 | Grad Norm: 0.00932958\n",
      "Epoch 1 | Step 759000 | Avg Loss: 0.0149 | Grad Norm: 0.00904000\n",
      "Epoch 1 | Step 759100 | Avg Loss: 0.0150 | Grad Norm: 0.00830602\n",
      "Epoch 1 | Step 759200 | Avg Loss: 0.0148 | Grad Norm: 0.00853759\n",
      "Epoch 1 | Step 759300 | Avg Loss: 0.0148 | Grad Norm: 0.00810722\n",
      "Epoch 1 | Step 759400 | Avg Loss: 0.0150 | Grad Norm: 0.00990165\n",
      "Epoch 1 | Step 759500 | Avg Loss: 0.0152 | Grad Norm: 0.00888080\n",
      "Epoch 1 | Step 759600 | Avg Loss: 0.0150 | Grad Norm: 0.00927478\n",
      "Epoch 1 | Step 759700 | Avg Loss: 0.0147 | Grad Norm: 0.00919159\n",
      "Epoch 1 | Step 759800 | Avg Loss: 0.0147 | Grad Norm: 0.00800668\n",
      "Epoch 1 | Step 759900 | Avg Loss: 0.0147 | Grad Norm: 0.00924145\n",
      "Epoch 1 | Step 760000 | Avg Loss: 0.0149 | Grad Norm: 0.00942241\n",
      "Epoch 1 | Step 760100 | Avg Loss: 0.0145 | Grad Norm: 0.00981052\n",
      "Epoch 1 | Step 760200 | Avg Loss: 0.0146 | Grad Norm: 0.00949816\n",
      "Epoch 1 | Step 760300 | Avg Loss: 0.0144 | Grad Norm: 0.01064884\n",
      "Epoch 1 | Step 760400 | Avg Loss: 0.0148 | Grad Norm: 0.01122095\n",
      "Epoch 1 | Step 760500 | Avg Loss: 0.0147 | Grad Norm: 0.00988246\n",
      "Epoch 1 | Step 760600 | Avg Loss: 0.0149 | Grad Norm: 0.00840597\n",
      "Epoch 1 | Step 760700 | Avg Loss: 0.0150 | Grad Norm: 0.01016948\n",
      "Epoch 1 | Step 760800 | Avg Loss: 0.0150 | Grad Norm: 0.01017723\n",
      "Epoch 1 | Step 760900 | Avg Loss: 0.0145 | Grad Norm: 0.01142133\n",
      "Epoch 1 | Step 761000 | Avg Loss: 0.0143 | Grad Norm: 0.01154761\n",
      "Epoch 1 | Step 761100 | Avg Loss: 0.0146 | Grad Norm: 0.00923023\n",
      "Epoch 1 | Step 761200 | Avg Loss: 0.0150 | Grad Norm: 0.00968911\n",
      "Epoch 1 | Step 761300 | Avg Loss: 0.0149 | Grad Norm: 0.00904239\n",
      "Epoch 1 | Step 761400 | Avg Loss: 0.0154 | Grad Norm: 0.00979848\n",
      "Epoch 1 | Step 761500 | Avg Loss: 0.0152 | Grad Norm: 0.00778579\n",
      "Epoch 1 | Step 761600 | Avg Loss: 0.0152 | Grad Norm: 0.00879862\n",
      "Epoch 1 | Step 761700 | Avg Loss: 0.0152 | Grad Norm: 0.00965500\n",
      "Epoch 1 | Step 761800 | Avg Loss: 0.0151 | Grad Norm: 0.00858183\n",
      "Epoch 1 | Step 761900 | Avg Loss: 0.0153 | Grad Norm: 0.00998508\n",
      "Epoch 1 | Step 762000 | Avg Loss: 0.0150 | Grad Norm: 0.00928314\n",
      "Epoch 1 | Step 762100 | Avg Loss: 0.0149 | Grad Norm: 0.01028118\n",
      "Epoch 1 | Step 762200 | Avg Loss: 0.0147 | Grad Norm: 0.00881034\n",
      "Epoch 1 | Step 762300 | Avg Loss: 0.0147 | Grad Norm: 0.00821244\n",
      "Epoch 1 | Step 762400 | Avg Loss: 0.0151 | Grad Norm: 0.01014282\n",
      "Epoch 1 | Step 762500 | Avg Loss: 0.0151 | Grad Norm: 0.00959414\n",
      "Epoch 1 | Step 762600 | Avg Loss: 0.0148 | Grad Norm: 0.01047503\n",
      "Epoch 1 | Step 762700 | Avg Loss: 0.0155 | Grad Norm: 0.00844562\n",
      "Epoch 1 | Step 762800 | Avg Loss: 0.0154 | Grad Norm: 0.00903368\n",
      "Epoch 1 | Step 762900 | Avg Loss: 0.0154 | Grad Norm: 0.00853846\n",
      "Epoch 1 | Step 763000 | Avg Loss: 0.0154 | Grad Norm: 0.00890395\n",
      "Epoch 1 | Step 763100 | Avg Loss: 0.0152 | Grad Norm: 0.00873038\n",
      "Epoch 1 | Step 763200 | Avg Loss: 0.0155 | Grad Norm: 0.00875226\n",
      "Epoch 1 | Step 763300 | Avg Loss: 0.0154 | Grad Norm: 0.00865418\n",
      "Epoch 1 | Step 763400 | Avg Loss: 0.0154 | Grad Norm: 0.00987324\n",
      "Epoch 1 | Step 763500 | Avg Loss: 0.0152 | Grad Norm: 0.00776572\n",
      "Epoch 1 | Step 763600 | Avg Loss: 0.0152 | Grad Norm: 0.00712579\n",
      "Epoch 1 | Step 763700 | Avg Loss: 0.0150 | Grad Norm: 0.00932623\n",
      "Epoch 1 | Step 763800 | Avg Loss: 0.0150 | Grad Norm: 0.01040510\n",
      "Epoch 1 | Step 763900 | Avg Loss: 0.0149 | Grad Norm: 0.00912333\n",
      "Epoch 1 | Step 764000 | Avg Loss: 0.0148 | Grad Norm: 0.00792306\n",
      "Epoch 1 | Step 764100 | Avg Loss: 0.0144 | Grad Norm: 0.00723981\n",
      "Epoch 1 | Step 764200 | Avg Loss: 0.0143 | Grad Norm: 0.00850348\n",
      "Epoch 1 | Step 764300 | Avg Loss: 0.0141 | Grad Norm: 0.01231106\n",
      "Epoch 1 | Step 764400 | Avg Loss: 0.0144 | Grad Norm: 0.00997368\n",
      "Epoch 1 | Step 764500 | Avg Loss: 0.0147 | Grad Norm: 0.00951446\n",
      "Epoch 1 | Step 764600 | Avg Loss: 0.0145 | Grad Norm: 0.00805613\n",
      "Epoch 1 | Step 764700 | Avg Loss: 0.0146 | Grad Norm: 0.00998027\n",
      "Epoch 1 | Step 764800 | Avg Loss: 0.0148 | Grad Norm: 0.01016131\n",
      "Epoch 1 | Step 764900 | Avg Loss: 0.0143 | Grad Norm: 0.00831672\n",
      "Epoch 1 | Step 765000 | Avg Loss: 0.0145 | Grad Norm: 0.00933043\n",
      "Epoch 1 | Step 765100 | Avg Loss: 0.0146 | Grad Norm: 0.00868563\n",
      "Epoch 1 | Step 765200 | Avg Loss: 0.0145 | Grad Norm: 0.00904411\n",
      "Epoch 1 | Step 765300 | Avg Loss: 0.0147 | Grad Norm: 0.00839553\n",
      "Epoch 1 | Step 765400 | Avg Loss: 0.0144 | Grad Norm: 0.01013532\n",
      "Epoch 1 | Step 765500 | Avg Loss: 0.0150 | Grad Norm: 0.00885211\n",
      "Epoch 1 | Step 765600 | Avg Loss: 0.0147 | Grad Norm: 0.00837393\n",
      "Epoch 1 | Step 765700 | Avg Loss: 0.0144 | Grad Norm: 0.00809346\n",
      "Epoch 1 | Step 765800 | Avg Loss: 0.0143 | Grad Norm: 0.00908074\n",
      "Epoch 1 | Step 765900 | Avg Loss: 0.0142 | Grad Norm: 0.00773304\n",
      "Epoch 1 | Step 766000 | Avg Loss: 0.0142 | Grad Norm: 0.00784550\n",
      "Epoch 1 | Step 766100 | Avg Loss: 0.0147 | Grad Norm: 0.00813142\n",
      "Epoch 1 | Step 766200 | Avg Loss: 0.0150 | Grad Norm: 0.00854886\n",
      "Epoch 1 | Step 766300 | Avg Loss: 0.0152 | Grad Norm: 0.00800813\n",
      "Epoch 1 | Step 766400 | Avg Loss: 0.0151 | Grad Norm: 0.00956080\n",
      "Epoch 1 | Step 766500 | Avg Loss: 0.0149 | Grad Norm: 0.00748211\n",
      "Epoch 1 | Step 766600 | Avg Loss: 0.0149 | Grad Norm: 0.00819860\n",
      "Epoch 1 | Step 766700 | Avg Loss: 0.0147 | Grad Norm: 0.00784814\n",
      "Epoch 1 | Step 766800 | Avg Loss: 0.0146 | Grad Norm: 0.00788264\n",
      "Epoch 1 | Step 766900 | Avg Loss: 0.0148 | Grad Norm: 0.00862742\n",
      "Epoch 1 | Step 767000 | Avg Loss: 0.0145 | Grad Norm: 0.00968736\n",
      "Epoch 1 | Step 767100 | Avg Loss: 0.0145 | Grad Norm: 0.00808672\n",
      "Epoch 1 | Step 767200 | Avg Loss: 0.0146 | Grad Norm: 0.00882162\n",
      "Epoch 1 | Step 767300 | Avg Loss: 0.0147 | Grad Norm: 0.00864151\n",
      "Epoch 1 | Step 767400 | Avg Loss: 0.0149 | Grad Norm: 0.00802277\n",
      "Epoch 1 | Step 767500 | Avg Loss: 0.0147 | Grad Norm: 0.00809897\n",
      "Epoch 1 | Step 767600 | Avg Loss: 0.0151 | Grad Norm: 0.01029832\n",
      "Epoch 1 | Step 767700 | Avg Loss: 0.0147 | Grad Norm: 0.00851707\n",
      "Epoch 1 | Step 767800 | Avg Loss: 0.0144 | Grad Norm: 0.00887408\n",
      "Epoch 1 | Step 767900 | Avg Loss: 0.0143 | Grad Norm: 0.00830004\n",
      "Epoch 1 | Step 768000 | Avg Loss: 0.0144 | Grad Norm: 0.00841697\n",
      "Epoch 1 | Step 768100 | Avg Loss: 0.0147 | Grad Norm: 0.00801750\n",
      "Epoch 1 | Step 768200 | Avg Loss: 0.0144 | Grad Norm: 0.00906030\n",
      "Epoch 1 | Step 768300 | Avg Loss: 0.0148 | Grad Norm: 0.00986502\n",
      "Epoch 1 | Step 768400 | Avg Loss: 0.0147 | Grad Norm: 0.00878252\n",
      "Epoch 1 | Step 768500 | Avg Loss: 0.0149 | Grad Norm: 0.00931476\n",
      "Epoch 1 | Step 768600 | Avg Loss: 0.0151 | Grad Norm: 0.00774528\n",
      "Epoch 1 | Step 768700 | Avg Loss: 0.0153 | Grad Norm: 0.00890131\n",
      "Epoch 1 | Step 768800 | Avg Loss: 0.0155 | Grad Norm: 0.00835297\n",
      "Epoch 1 | Step 768900 | Avg Loss: 0.0151 | Grad Norm: 0.00914391\n",
      "Epoch 1 | Step 769000 | Avg Loss: 0.0150 | Grad Norm: 0.00902810\n",
      "Epoch 1 | Step 769100 | Avg Loss: 0.0150 | Grad Norm: 0.00990771\n",
      "Epoch 1 | Step 769200 | Avg Loss: 0.0145 | Grad Norm: 0.00827811\n",
      "Epoch 1 | Step 769300 | Avg Loss: 0.0145 | Grad Norm: 0.00950937\n",
      "Epoch 1 | Step 769400 | Avg Loss: 0.0148 | Grad Norm: 0.00730297\n",
      "Epoch 1 | Step 769500 | Avg Loss: 0.0147 | Grad Norm: 0.01030587\n",
      "Epoch 1 | Step 769600 | Avg Loss: 0.0146 | Grad Norm: 0.00998009\n",
      "Epoch 1 | Step 769700 | Avg Loss: 0.0148 | Grad Norm: 0.00965019\n",
      "Epoch 1 | Step 769800 | Avg Loss: 0.0149 | Grad Norm: 0.00801731\n",
      "Epoch 1 | Step 769900 | Avg Loss: 0.0148 | Grad Norm: 0.00983955\n",
      "Epoch 1 | Step 770000 | Avg Loss: 0.0151 | Grad Norm: 0.00804961\n",
      "Epoch 1 | Step 770100 | Avg Loss: 0.0150 | Grad Norm: 0.01031159\n",
      "Epoch 1 | Step 770200 | Avg Loss: 0.0148 | Grad Norm: 0.00939265\n",
      "Epoch 1 | Step 770300 | Avg Loss: 0.0150 | Grad Norm: 0.00923845\n",
      "Epoch 1 | Step 770400 | Avg Loss: 0.0153 | Grad Norm: 0.00916903\n",
      "Epoch 1 | Step 770500 | Avg Loss: 0.0153 | Grad Norm: 0.00878838\n",
      "Epoch 1 | Step 770600 | Avg Loss: 0.0151 | Grad Norm: 0.00880439\n",
      "Epoch 1 | Step 770700 | Avg Loss: 0.0151 | Grad Norm: 0.00826620\n",
      "Epoch 1 | Step 770800 | Avg Loss: 0.0151 | Grad Norm: 0.00938077\n",
      "Epoch 1 | Step 770900 | Avg Loss: 0.0150 | Grad Norm: 0.00889545\n",
      "Epoch 1 | Step 771000 | Avg Loss: 0.0147 | Grad Norm: 0.00870194\n",
      "Epoch 1 | Step 771100 | Avg Loss: 0.0149 | Grad Norm: 0.01059271\n",
      "Epoch 1 | Step 771200 | Avg Loss: 0.0145 | Grad Norm: 0.00831011\n",
      "Epoch 1 | Step 771300 | Avg Loss: 0.0150 | Grad Norm: 0.00852929\n",
      "Epoch 1 | Step 771400 | Avg Loss: 0.0150 | Grad Norm: 0.00765174\n",
      "Epoch 1 | Step 771500 | Avg Loss: 0.0148 | Grad Norm: 0.00824564\n",
      "Epoch 1 | Step 771600 | Avg Loss: 0.0144 | Grad Norm: 0.00844228\n",
      "Epoch 1 | Step 771700 | Avg Loss: 0.0143 | Grad Norm: 0.00809820\n",
      "Epoch 1 | Step 771800 | Avg Loss: 0.0144 | Grad Norm: 0.00918175\n",
      "Epoch 1 | Step 771900 | Avg Loss: 0.0143 | Grad Norm: 0.00906616\n",
      "Epoch 1 | Step 772000 | Avg Loss: 0.0142 | Grad Norm: 0.00756769\n",
      "Epoch 1 | Step 772100 | Avg Loss: 0.0146 | Grad Norm: 0.00792965\n",
      "Epoch 1 | Step 772200 | Avg Loss: 0.0146 | Grad Norm: 0.00939629\n",
      "Epoch 1 | Step 772300 | Avg Loss: 0.0147 | Grad Norm: 0.00796926\n",
      "Epoch 1 | Step 772400 | Avg Loss: 0.0144 | Grad Norm: 0.00755013\n",
      "Epoch 1 | Step 772500 | Avg Loss: 0.0147 | Grad Norm: 0.00827600\n",
      "Epoch 1 | Step 772600 | Avg Loss: 0.0149 | Grad Norm: 0.00870481\n",
      "Epoch 1 | Step 772700 | Avg Loss: 0.0149 | Grad Norm: 0.00820423\n",
      "Epoch 1 | Step 772800 | Avg Loss: 0.0150 | Grad Norm: 0.00855938\n",
      "Epoch 1 | Step 772900 | Avg Loss: 0.0151 | Grad Norm: 0.00766494\n",
      "Epoch 1 | Step 773000 | Avg Loss: 0.0153 | Grad Norm: 0.00800064\n",
      "Epoch 1 | Step 773100 | Avg Loss: 0.0150 | Grad Norm: 0.00832197\n",
      "Epoch 1 | Step 773200 | Avg Loss: 0.0151 | Grad Norm: 0.00850869\n",
      "Epoch 1 | Step 773300 | Avg Loss: 0.0149 | Grad Norm: 0.00865778\n",
      "Epoch 1 | Step 773400 | Avg Loss: 0.0150 | Grad Norm: 0.00819244\n",
      "Epoch 1 | Step 773500 | Avg Loss: 0.0152 | Grad Norm: 0.00884845\n",
      "Epoch 1 | Step 773600 | Avg Loss: 0.0152 | Grad Norm: 0.00888150\n",
      "Epoch 1 | Step 773700 | Avg Loss: 0.0154 | Grad Norm: 0.00973105\n",
      "Epoch 1 | Step 773800 | Avg Loss: 0.0151 | Grad Norm: 0.00991809\n",
      "Epoch 1 | Step 773900 | Avg Loss: 0.0148 | Grad Norm: 0.00972445\n",
      "Epoch 1 | Step 774000 | Avg Loss: 0.0150 | Grad Norm: 0.00972894\n",
      "Epoch 1 | Step 774100 | Avg Loss: 0.0151 | Grad Norm: 0.00926301\n",
      "Epoch 1 | Step 774200 | Avg Loss: 0.0151 | Grad Norm: 0.00976065\n",
      "Epoch 1 | Step 774300 | Avg Loss: 0.0148 | Grad Norm: 0.00902014\n",
      "Epoch 1 | Step 774400 | Avg Loss: 0.0151 | Grad Norm: 0.00967970\n",
      "Epoch 1 | Step 774500 | Avg Loss: 0.0152 | Grad Norm: 0.00858281\n",
      "Epoch 1 | Step 774600 | Avg Loss: 0.0145 | Grad Norm: 0.00802255\n",
      "Epoch 1 | Step 774700 | Avg Loss: 0.0146 | Grad Norm: 0.00913767\n",
      "Epoch 1 | Step 774800 | Avg Loss: 0.0150 | Grad Norm: 0.00924448\n",
      "Epoch 1 | Step 774900 | Avg Loss: 0.0153 | Grad Norm: 0.01151174\n",
      "Epoch 1 | Step 775000 | Avg Loss: 0.0153 | Grad Norm: 0.00935183\n",
      "Epoch 1 | Step 775100 | Avg Loss: 0.0153 | Grad Norm: 0.00786587\n",
      "Epoch 1 | Step 775200 | Avg Loss: 0.0154 | Grad Norm: 0.00781458\n",
      "Epoch 1 | Step 775300 | Avg Loss: 0.0150 | Grad Norm: 0.00919911\n",
      "Epoch 1 | Step 775400 | Avg Loss: 0.0148 | Grad Norm: 0.00933889\n",
      "Epoch 1 | Step 775500 | Avg Loss: 0.0147 | Grad Norm: 0.01005004\n",
      "Epoch 1 | Step 775600 | Avg Loss: 0.0152 | Grad Norm: 0.00897703\n",
      "Epoch 1 | Step 775700 | Avg Loss: 0.0153 | Grad Norm: 0.00829714\n",
      "Epoch 1 | Step 775800 | Avg Loss: 0.0152 | Grad Norm: 0.00968131\n",
      "Epoch 1 | Step 775900 | Avg Loss: 0.0149 | Grad Norm: 0.01024892\n",
      "Epoch 1 | Step 776000 | Avg Loss: 0.0152 | Grad Norm: 0.00894337\n",
      "Epoch 1 | Step 776100 | Avg Loss: 0.0150 | Grad Norm: 0.00892890\n",
      "Epoch 1 | Step 776200 | Avg Loss: 0.0150 | Grad Norm: 0.00969014\n",
      "Epoch 1 | Step 776300 | Avg Loss: 0.0144 | Grad Norm: 0.00814409\n",
      "Epoch 1 | Step 776400 | Avg Loss: 0.0150 | Grad Norm: 0.00857884\n",
      "Epoch 1 | Step 776500 | Avg Loss: 0.0154 | Grad Norm: 0.00956052\n",
      "Epoch 1 | Step 776600 | Avg Loss: 0.0152 | Grad Norm: 0.00861399\n",
      "Epoch 1 | Step 776700 | Avg Loss: 0.0150 | Grad Norm: 0.00933688\n",
      "Epoch 1 | Step 776800 | Avg Loss: 0.0152 | Grad Norm: 0.00801307\n",
      "Epoch 1 | Step 776900 | Avg Loss: 0.0149 | Grad Norm: 0.00835862\n",
      "Epoch 1 | Step 777000 | Avg Loss: 0.0147 | Grad Norm: 0.00744992\n",
      "Epoch 1 | Step 777100 | Avg Loss: 0.0152 | Grad Norm: 0.01042098\n",
      "Epoch 1 | Step 777200 | Avg Loss: 0.0149 | Grad Norm: 0.00870894\n",
      "Epoch 1 | Step 777300 | Avg Loss: 0.0150 | Grad Norm: 0.00801512\n",
      "Epoch 1 | Step 777400 | Avg Loss: 0.0148 | Grad Norm: 0.00921456\n",
      "Epoch 1 | Step 777500 | Avg Loss: 0.0151 | Grad Norm: 0.00857444\n",
      "Epoch 1 | Step 777600 | Avg Loss: 0.0150 | Grad Norm: 0.00764626\n",
      "Epoch 1 | Step 777700 | Avg Loss: 0.0147 | Grad Norm: 0.00847186\n",
      "Epoch 1 | Step 777800 | Avg Loss: 0.0147 | Grad Norm: 0.00812624\n",
      "Epoch 1 | Step 777900 | Avg Loss: 0.0145 | Grad Norm: 0.00842170\n",
      "Epoch 1 | Step 778000 | Avg Loss: 0.0142 | Grad Norm: 0.00913495\n",
      "Epoch 1 | Step 778100 | Avg Loss: 0.0143 | Grad Norm: 0.00944126\n",
      "Epoch 1 | Step 778200 | Avg Loss: 0.0145 | Grad Norm: 0.00947677\n",
      "Epoch 1 | Step 778300 | Avg Loss: 0.0144 | Grad Norm: 0.00731009\n",
      "Epoch 1 | Step 778400 | Avg Loss: 0.0146 | Grad Norm: 0.00862282\n",
      "Epoch 1 | Step 778500 | Avg Loss: 0.0147 | Grad Norm: 0.00891776\n",
      "Epoch 1 | Step 778600 | Avg Loss: 0.0147 | Grad Norm: 0.00846131\n",
      "Epoch 1 | Step 778700 | Avg Loss: 0.0141 | Grad Norm: 0.00934626\n",
      "Epoch 1 | Step 778800 | Avg Loss: 0.0144 | Grad Norm: 0.00852775\n",
      "Epoch 1 | Step 778900 | Avg Loss: 0.0146 | Grad Norm: 0.00777659\n",
      "Epoch 1 | Step 779000 | Avg Loss: 0.0143 | Grad Norm: 0.00890454\n",
      "Epoch 1 | Step 779100 | Avg Loss: 0.0147 | Grad Norm: 0.00812601\n",
      "Epoch 1 | Step 779200 | Avg Loss: 0.0148 | Grad Norm: 0.00771881\n",
      "Epoch 1 | Step 779300 | Avg Loss: 0.0145 | Grad Norm: 0.00939425\n",
      "Epoch 1 | Step 779400 | Avg Loss: 0.0148 | Grad Norm: 0.00833887\n",
      "Epoch 1 | Step 779500 | Avg Loss: 0.0150 | Grad Norm: 0.00823994\n",
      "Epoch 1 | Step 779600 | Avg Loss: 0.0145 | Grad Norm: 0.00826531\n",
      "Epoch 1 | Step 779700 | Avg Loss: 0.0148 | Grad Norm: 0.00973773\n",
      "Epoch 1 | Step 779800 | Avg Loss: 0.0148 | Grad Norm: 0.00840455\n",
      "Epoch 1 | Step 779900 | Avg Loss: 0.0144 | Grad Norm: 0.01009167\n",
      "Epoch 1 | Step 780000 | Avg Loss: 0.0146 | Grad Norm: 0.00989563\n",
      "Epoch 1 | Step 780100 | Avg Loss: 0.0146 | Grad Norm: 0.00889137\n",
      "Epoch 1 | Step 780200 | Avg Loss: 0.0146 | Grad Norm: 0.00883963\n",
      "Epoch 1 | Step 780300 | Avg Loss: 0.0146 | Grad Norm: 0.00859073\n",
      "Epoch 1 | Step 780400 | Avg Loss: 0.0147 | Grad Norm: 0.00897173\n",
      "Epoch 1 | Step 780500 | Avg Loss: 0.0149 | Grad Norm: 0.01211412\n",
      "Epoch 1 | Step 780600 | Avg Loss: 0.0147 | Grad Norm: 0.01059594\n",
      "Epoch 1 | Step 780700 | Avg Loss: 0.0145 | Grad Norm: 0.00820060\n",
      "Epoch 1 | Step 780800 | Avg Loss: 0.0144 | Grad Norm: 0.01041869\n",
      "Epoch 1 | Step 780900 | Avg Loss: 0.0145 | Grad Norm: 0.00844878\n",
      "Epoch 1 | Step 781000 | Avg Loss: 0.0140 | Grad Norm: 0.00713607\n",
      "Epoch 1 | Step 781100 | Avg Loss: 0.0132 | Grad Norm: 0.00773512\n",
      "Epoch 1 | Step 781200 | Avg Loss: 0.0129 | Grad Norm: 0.00753876\n",
      "Epoch 1, Loss: 0.0132\n",
      "Epoch 2 | Step 781300 | Avg Loss: 0.0149 | Grad Norm: 0.00921757\n",
      "Epoch 2 | Step 781400 | Avg Loss: 0.0160 | Grad Norm: 0.00833333\n",
      "Epoch 2 | Step 781500 | Avg Loss: 0.0151 | Grad Norm: 0.00936358\n",
      "Epoch 2 | Step 781600 | Avg Loss: 0.0149 | Grad Norm: 0.00826099\n",
      "Epoch 2 | Step 781700 | Avg Loss: 0.0146 | Grad Norm: 0.01019923\n",
      "Epoch 2 | Step 781800 | Avg Loss: 0.0147 | Grad Norm: 0.00897229\n",
      "Epoch 2 | Step 781900 | Avg Loss: 0.0153 | Grad Norm: 0.00916436\n",
      "Epoch 2 | Step 782000 | Avg Loss: 0.0155 | Grad Norm: 0.01068353\n",
      "Epoch 2 | Step 782100 | Avg Loss: 0.0156 | Grad Norm: 0.01011962\n",
      "Epoch 2 | Step 782200 | Avg Loss: 0.0156 | Grad Norm: 0.00925575\n",
      "Epoch 2 | Step 782300 | Avg Loss: 0.0152 | Grad Norm: 0.00859200\n",
      "Epoch 2 | Step 782400 | Avg Loss: 0.0150 | Grad Norm: 0.00890020\n",
      "Epoch 2 | Step 782500 | Avg Loss: 0.0147 | Grad Norm: 0.00810435\n",
      "Epoch 2 | Step 782600 | Avg Loss: 0.0146 | Grad Norm: 0.00958755\n",
      "Epoch 2 | Step 782700 | Avg Loss: 0.0146 | Grad Norm: 0.00734666\n",
      "Epoch 2 | Step 782800 | Avg Loss: 0.0144 | Grad Norm: 0.00915590\n",
      "Epoch 2 | Step 782900 | Avg Loss: 0.0143 | Grad Norm: 0.01175898\n",
      "Epoch 2 | Step 783000 | Avg Loss: 0.0144 | Grad Norm: 0.00872723\n",
      "Epoch 2 | Step 783100 | Avg Loss: 0.0145 | Grad Norm: 0.00768148\n",
      "Epoch 2 | Step 783200 | Avg Loss: 0.0147 | Grad Norm: 0.00856133\n",
      "Epoch 2 | Step 783300 | Avg Loss: 0.0147 | Grad Norm: 0.00854641\n",
      "Epoch 2 | Step 783400 | Avg Loss: 0.0146 | Grad Norm: 0.00799633\n",
      "Epoch 2 | Step 783500 | Avg Loss: 0.0148 | Grad Norm: 0.00873174\n",
      "Epoch 2 | Step 783600 | Avg Loss: 0.0148 | Grad Norm: 0.00949029\n",
      "Epoch 2 | Step 783700 | Avg Loss: 0.0151 | Grad Norm: 0.00813513\n",
      "Epoch 2 | Step 783800 | Avg Loss: 0.0150 | Grad Norm: 0.00964755\n",
      "Epoch 2 | Step 783900 | Avg Loss: 0.0152 | Grad Norm: 0.00789355\n",
      "Epoch 2 | Step 784000 | Avg Loss: 0.0151 | Grad Norm: 0.00779548\n",
      "Epoch 2 | Step 784100 | Avg Loss: 0.0150 | Grad Norm: 0.00779776\n",
      "Epoch 2 | Step 784200 | Avg Loss: 0.0147 | Grad Norm: 0.00807621\n",
      "Epoch 2 | Step 784300 | Avg Loss: 0.0149 | Grad Norm: 0.00818265\n",
      "Epoch 2 | Step 784400 | Avg Loss: 0.0145 | Grad Norm: 0.00856057\n",
      "Epoch 2 | Step 784500 | Avg Loss: 0.0145 | Grad Norm: 0.00817099\n",
      "Epoch 2 | Step 784600 | Avg Loss: 0.0145 | Grad Norm: 0.00867329\n",
      "Epoch 2 | Step 784700 | Avg Loss: 0.0144 | Grad Norm: 0.00784590\n",
      "Epoch 2 | Step 784800 | Avg Loss: 0.0143 | Grad Norm: 0.00835473\n",
      "Epoch 2 | Step 784900 | Avg Loss: 0.0146 | Grad Norm: 0.00796298\n",
      "Epoch 2 | Step 785000 | Avg Loss: 0.0147 | Grad Norm: 0.00841949\n",
      "Epoch 2 | Step 785100 | Avg Loss: 0.0150 | Grad Norm: 0.00819612\n",
      "Epoch 2 | Step 785200 | Avg Loss: 0.0148 | Grad Norm: 0.00909275\n",
      "Epoch 2 | Step 785300 | Avg Loss: 0.0146 | Grad Norm: 0.00745631\n",
      "Epoch 2 | Step 785400 | Avg Loss: 0.0143 | Grad Norm: 0.00880528\n",
      "Epoch 2 | Step 785500 | Avg Loss: 0.0143 | Grad Norm: 0.00801578\n",
      "Epoch 2 | Step 785600 | Avg Loss: 0.0147 | Grad Norm: 0.00795787\n",
      "Epoch 2 | Step 785700 | Avg Loss: 0.0147 | Grad Norm: 0.00870071\n",
      "Epoch 2 | Step 785800 | Avg Loss: 0.0143 | Grad Norm: 0.00811978\n",
      "Epoch 2 | Step 785900 | Avg Loss: 0.0143 | Grad Norm: 0.01076413\n",
      "Epoch 2 | Step 786000 | Avg Loss: 0.0143 | Grad Norm: 0.00777670\n",
      "Epoch 2 | Step 786100 | Avg Loss: 0.0148 | Grad Norm: 0.00905074\n",
      "Epoch 2 | Step 786200 | Avg Loss: 0.0146 | Grad Norm: 0.00923400\n",
      "Epoch 2 | Step 786300 | Avg Loss: 0.0148 | Grad Norm: 0.00835175\n",
      "Epoch 2 | Step 786400 | Avg Loss: 0.0147 | Grad Norm: 0.00814501\n",
      "Epoch 2 | Step 786500 | Avg Loss: 0.0147 | Grad Norm: 0.00833321\n",
      "Epoch 2 | Step 786600 | Avg Loss: 0.0150 | Grad Norm: 0.00729126\n",
      "Epoch 2 | Step 786700 | Avg Loss: 0.0153 | Grad Norm: 0.01196305\n",
      "Epoch 2 | Step 786800 | Avg Loss: 0.0149 | Grad Norm: 0.00815839\n",
      "Epoch 2 | Step 786900 | Avg Loss: 0.0149 | Grad Norm: 0.01017290\n",
      "Epoch 2 | Step 787000 | Avg Loss: 0.0150 | Grad Norm: 0.00888781\n",
      "Epoch 2 | Step 787100 | Avg Loss: 0.0148 | Grad Norm: 0.00841618\n",
      "Epoch 2 | Step 787200 | Avg Loss: 0.0150 | Grad Norm: 0.00802738\n",
      "Epoch 2 | Step 787300 | Avg Loss: 0.0146 | Grad Norm: 0.00752033\n",
      "Epoch 2 | Step 787400 | Avg Loss: 0.0146 | Grad Norm: 0.00803226\n",
      "Epoch 2 | Step 787500 | Avg Loss: 0.0150 | Grad Norm: 0.00863659\n",
      "Epoch 2 | Step 787600 | Avg Loss: 0.0151 | Grad Norm: 0.00790703\n",
      "Epoch 2 | Step 787700 | Avg Loss: 0.0149 | Grad Norm: 0.00935245\n",
      "Epoch 2 | Step 787800 | Avg Loss: 0.0149 | Grad Norm: 0.00858062\n",
      "Epoch 2 | Step 787900 | Avg Loss: 0.0151 | Grad Norm: 0.00939238\n",
      "Epoch 2 | Step 788000 | Avg Loss: 0.0149 | Grad Norm: 0.00813850\n",
      "Epoch 2 | Step 788100 | Avg Loss: 0.0149 | Grad Norm: 0.00847184\n",
      "Epoch 2 | Step 788200 | Avg Loss: 0.0150 | Grad Norm: 0.00884848\n",
      "Epoch 2 | Step 788300 | Avg Loss: 0.0148 | Grad Norm: 0.00898313\n",
      "Epoch 2 | Step 788400 | Avg Loss: 0.0145 | Grad Norm: 0.00753601\n",
      "Epoch 2 | Step 788500 | Avg Loss: 0.0141 | Grad Norm: 0.00769728\n",
      "Epoch 2 | Step 788600 | Avg Loss: 0.0145 | Grad Norm: 0.00885030\n",
      "Epoch 2 | Step 788700 | Avg Loss: 0.0143 | Grad Norm: 0.00837873\n",
      "Epoch 2 | Step 788800 | Avg Loss: 0.0144 | Grad Norm: 0.00699848\n",
      "Epoch 2 | Step 788900 | Avg Loss: 0.0149 | Grad Norm: 0.00768893\n",
      "Epoch 2 | Step 789000 | Avg Loss: 0.0146 | Grad Norm: 0.00921624\n",
      "Epoch 2 | Step 789100 | Avg Loss: 0.0146 | Grad Norm: 0.00843655\n",
      "Epoch 2 | Step 789200 | Avg Loss: 0.0151 | Grad Norm: 0.00794848\n",
      "Epoch 2 | Step 789300 | Avg Loss: 0.0149 | Grad Norm: 0.00945493\n",
      "Epoch 2 | Step 789400 | Avg Loss: 0.0148 | Grad Norm: 0.00846374\n",
      "Epoch 2 | Step 789500 | Avg Loss: 0.0150 | Grad Norm: 0.00813829\n",
      "Epoch 2 | Step 789600 | Avg Loss: 0.0146 | Grad Norm: 0.00897814\n",
      "Epoch 2 | Step 789700 | Avg Loss: 0.0148 | Grad Norm: 0.00729142\n",
      "Epoch 2 | Step 789800 | Avg Loss: 0.0148 | Grad Norm: 0.00838710\n",
      "Epoch 2 | Step 789900 | Avg Loss: 0.0153 | Grad Norm: 0.00886550\n",
      "Epoch 2 | Step 790000 | Avg Loss: 0.0151 | Grad Norm: 0.00876007\n",
      "Epoch 2 | Step 790100 | Avg Loss: 0.0150 | Grad Norm: 0.00790025\n",
      "Epoch 2 | Step 790200 | Avg Loss: 0.0147 | Grad Norm: 0.00805919\n",
      "Epoch 2 | Step 790300 | Avg Loss: 0.0148 | Grad Norm: 0.00887344\n",
      "Epoch 2 | Step 790400 | Avg Loss: 0.0149 | Grad Norm: 0.00962909\n",
      "Epoch 2 | Step 790500 | Avg Loss: 0.0147 | Grad Norm: 0.00947292\n",
      "Epoch 2 | Step 790600 | Avg Loss: 0.0147 | Grad Norm: 0.00879190\n",
      "Epoch 2 | Step 790700 | Avg Loss: 0.0147 | Grad Norm: 0.00854771\n",
      "Epoch 2 | Step 790800 | Avg Loss: 0.0149 | Grad Norm: 0.00880591\n",
      "Epoch 2 | Step 790900 | Avg Loss: 0.0149 | Grad Norm: 0.00723231\n",
      "Epoch 2 | Step 791000 | Avg Loss: 0.0150 | Grad Norm: 0.00897114\n",
      "Epoch 2 | Step 791100 | Avg Loss: 0.0150 | Grad Norm: 0.00979329\n",
      "Epoch 2 | Step 791200 | Avg Loss: 0.0150 | Grad Norm: 0.00987390\n",
      "Epoch 2 | Step 791300 | Avg Loss: 0.0150 | Grad Norm: 0.00938158\n",
      "Epoch 2 | Step 791400 | Avg Loss: 0.0151 | Grad Norm: 0.01062644\n",
      "Epoch 2 | Step 791500 | Avg Loss: 0.0151 | Grad Norm: 0.00912431\n",
      "Epoch 2 | Step 791600 | Avg Loss: 0.0150 | Grad Norm: 0.01097691\n",
      "Epoch 2 | Step 791700 | Avg Loss: 0.0153 | Grad Norm: 0.00806601\n",
      "Epoch 2 | Step 791800 | Avg Loss: 0.0157 | Grad Norm: 0.01122844\n",
      "Epoch 2 | Step 791900 | Avg Loss: 0.0152 | Grad Norm: 0.00854556\n",
      "Epoch 2 | Step 792000 | Avg Loss: 0.0152 | Grad Norm: 0.00913467\n",
      "Epoch 2 | Step 792100 | Avg Loss: 0.0153 | Grad Norm: 0.01002323\n",
      "Epoch 2 | Step 792200 | Avg Loss: 0.0148 | Grad Norm: 0.00897867\n",
      "Epoch 2 | Step 792300 | Avg Loss: 0.0152 | Grad Norm: 0.00867882\n",
      "Epoch 2 | Step 792400 | Avg Loss: 0.0151 | Grad Norm: 0.00920266\n",
      "Epoch 2 | Step 792500 | Avg Loss: 0.0150 | Grad Norm: 0.00815450\n",
      "Epoch 2 | Step 792600 | Avg Loss: 0.0148 | Grad Norm: 0.00846666\n",
      "Epoch 2 | Step 792700 | Avg Loss: 0.0148 | Grad Norm: 0.00853085\n",
      "Epoch 2 | Step 792800 | Avg Loss: 0.0152 | Grad Norm: 0.00993472\n",
      "Epoch 2 | Step 792900 | Avg Loss: 0.0146 | Grad Norm: 0.00870250\n",
      "Epoch 2 | Step 793000 | Avg Loss: 0.0149 | Grad Norm: 0.00943136\n",
      "Epoch 2 | Step 793100 | Avg Loss: 0.0145 | Grad Norm: 0.00792629\n",
      "Epoch 2 | Step 793200 | Avg Loss: 0.0146 | Grad Norm: 0.00827170\n",
      "Epoch 2 | Step 793300 | Avg Loss: 0.0146 | Grad Norm: 0.00801114\n",
      "Epoch 2 | Step 793400 | Avg Loss: 0.0145 | Grad Norm: 0.00745052\n",
      "Epoch 2 | Step 793500 | Avg Loss: 0.0143 | Grad Norm: 0.00794495\n",
      "Epoch 2 | Step 793600 | Avg Loss: 0.0147 | Grad Norm: 0.00949818\n",
      "Epoch 2 | Step 793700 | Avg Loss: 0.0152 | Grad Norm: 0.01026681\n",
      "Epoch 2 | Step 793800 | Avg Loss: 0.0152 | Grad Norm: 0.00843329\n",
      "Epoch 2 | Step 793900 | Avg Loss: 0.0149 | Grad Norm: 0.00970141\n",
      "Epoch 2 | Step 794000 | Avg Loss: 0.0144 | Grad Norm: 0.00816817\n",
      "Epoch 2 | Step 794100 | Avg Loss: 0.0150 | Grad Norm: 0.00828119\n",
      "Epoch 2 | Step 794200 | Avg Loss: 0.0149 | Grad Norm: 0.00765712\n",
      "Epoch 2 | Step 794300 | Avg Loss: 0.0149 | Grad Norm: 0.00944779\n",
      "Epoch 2 | Step 794400 | Avg Loss: 0.0148 | Grad Norm: 0.00800107\n",
      "Epoch 2 | Step 794500 | Avg Loss: 0.0149 | Grad Norm: 0.00863798\n",
      "Epoch 2 | Step 794600 | Avg Loss: 0.0146 | Grad Norm: 0.00962517\n",
      "Epoch 2 | Step 794700 | Avg Loss: 0.0150 | Grad Norm: 0.00882700\n",
      "Epoch 2 | Step 794800 | Avg Loss: 0.0151 | Grad Norm: 0.00819596\n",
      "Epoch 2 | Step 794900 | Avg Loss: 0.0147 | Grad Norm: 0.00919281\n",
      "Epoch 2 | Step 795000 | Avg Loss: 0.0145 | Grad Norm: 0.00820119\n",
      "Epoch 2 | Step 795100 | Avg Loss: 0.0148 | Grad Norm: 0.00803012\n",
      "Epoch 2 | Step 795200 | Avg Loss: 0.0151 | Grad Norm: 0.01027355\n",
      "Epoch 2 | Step 795300 | Avg Loss: 0.0148 | Grad Norm: 0.00861219\n",
      "Epoch 2 | Step 795400 | Avg Loss: 0.0148 | Grad Norm: 0.00878664\n",
      "Epoch 2 | Step 795500 | Avg Loss: 0.0146 | Grad Norm: 0.00768335\n",
      "Epoch 2 | Step 795600 | Avg Loss: 0.0146 | Grad Norm: 0.00870952\n",
      "Epoch 2 | Step 795700 | Avg Loss: 0.0150 | Grad Norm: 0.00914454\n",
      "Epoch 2 | Step 795800 | Avg Loss: 0.0149 | Grad Norm: 0.00953299\n",
      "Epoch 2 | Step 795900 | Avg Loss: 0.0151 | Grad Norm: 0.00872366\n",
      "Epoch 2 | Step 796000 | Avg Loss: 0.0147 | Grad Norm: 0.00781731\n",
      "Epoch 2 | Step 796100 | Avg Loss: 0.0146 | Grad Norm: 0.00823258\n",
      "Epoch 2 | Step 796200 | Avg Loss: 0.0148 | Grad Norm: 0.00935530\n",
      "Epoch 2 | Step 796300 | Avg Loss: 0.0150 | Grad Norm: 0.00911738\n",
      "Epoch 2 | Step 796400 | Avg Loss: 0.0146 | Grad Norm: 0.00760161\n",
      "Epoch 2 | Step 796500 | Avg Loss: 0.0148 | Grad Norm: 0.00928900\n",
      "Epoch 2 | Step 796600 | Avg Loss: 0.0148 | Grad Norm: 0.00903586\n",
      "Epoch 2 | Step 796700 | Avg Loss: 0.0145 | Grad Norm: 0.00831593\n",
      "Epoch 2 | Step 796800 | Avg Loss: 0.0151 | Grad Norm: 0.00763700\n",
      "Epoch 2 | Step 796900 | Avg Loss: 0.0152 | Grad Norm: 0.00994504\n",
      "Epoch 2 | Step 797000 | Avg Loss: 0.0157 | Grad Norm: 0.00832548\n",
      "Epoch 2 | Step 797100 | Avg Loss: 0.0159 | Grad Norm: 0.00821815\n",
      "Epoch 2 | Step 797200 | Avg Loss: 0.0157 | Grad Norm: 0.01099110\n",
      "Epoch 2 | Step 797300 | Avg Loss: 0.0156 | Grad Norm: 0.01013813\n",
      "Epoch 2 | Step 797400 | Avg Loss: 0.0158 | Grad Norm: 0.00914026\n",
      "Epoch 2 | Step 797500 | Avg Loss: 0.0157 | Grad Norm: 0.00830881\n",
      "Epoch 2 | Step 797600 | Avg Loss: 0.0154 | Grad Norm: 0.01031152\n",
      "Epoch 2 | Step 797700 | Avg Loss: 0.0156 | Grad Norm: 0.01010137\n",
      "Epoch 2 | Step 797800 | Avg Loss: 0.0152 | Grad Norm: 0.00820412\n",
      "Epoch 2 | Step 797900 | Avg Loss: 0.0146 | Grad Norm: 0.00949943\n",
      "Epoch 2 | Step 798000 | Avg Loss: 0.0148 | Grad Norm: 0.00897150\n",
      "Epoch 2 | Step 798100 | Avg Loss: 0.0151 | Grad Norm: 0.00940814\n",
      "Epoch 2 | Step 798200 | Avg Loss: 0.0153 | Grad Norm: 0.00835310\n",
      "Epoch 2 | Step 798300 | Avg Loss: 0.0151 | Grad Norm: 0.00963464\n",
      "Epoch 2 | Step 798400 | Avg Loss: 0.0151 | Grad Norm: 0.00854540\n",
      "Epoch 2 | Step 798500 | Avg Loss: 0.0148 | Grad Norm: 0.00730317\n",
      "Epoch 2 | Step 798600 | Avg Loss: 0.0148 | Grad Norm: 0.00828446\n",
      "Epoch 2 | Step 798700 | Avg Loss: 0.0148 | Grad Norm: 0.00816506\n",
      "Epoch 2 | Step 798800 | Avg Loss: 0.0146 | Grad Norm: 0.00699359\n",
      "Epoch 2 | Step 798900 | Avg Loss: 0.0145 | Grad Norm: 0.00850226\n",
      "Epoch 2 | Step 799000 | Avg Loss: 0.0147 | Grad Norm: 0.01051727\n",
      "Epoch 2 | Step 799100 | Avg Loss: 0.0145 | Grad Norm: 0.00816514\n",
      "Epoch 2 | Step 799200 | Avg Loss: 0.0148 | Grad Norm: 0.00827111\n",
      "Epoch 2 | Step 799300 | Avg Loss: 0.0148 | Grad Norm: 0.00894386\n",
      "Epoch 2 | Step 799400 | Avg Loss: 0.0144 | Grad Norm: 0.00791707\n",
      "Epoch 2 | Step 799500 | Avg Loss: 0.0145 | Grad Norm: 0.00722083\n",
      "Epoch 2 | Step 799600 | Avg Loss: 0.0146 | Grad Norm: 0.00780029\n",
      "Epoch 2 | Step 799700 | Avg Loss: 0.0148 | Grad Norm: 0.00818366\n",
      "Epoch 2 | Step 799800 | Avg Loss: 0.0148 | Grad Norm: 0.00839883\n",
      "Epoch 2 | Step 799900 | Avg Loss: 0.0151 | Grad Norm: 0.00805472\n",
      "Epoch 2 | Step 800000 | Avg Loss: 0.0151 | Grad Norm: 0.00951146\n",
      "Saving model at step800000\n",
      "Epoch 2 | Step 800100 | Avg Loss: 0.0152 | Grad Norm: 0.00814367\n",
      "Epoch 2 | Step 800200 | Avg Loss: 0.0153 | Grad Norm: 0.00898852\n",
      "Epoch 2 | Step 800300 | Avg Loss: 0.0153 | Grad Norm: 0.00888269\n",
      "Epoch 2 | Step 800400 | Avg Loss: 0.0154 | Grad Norm: 0.00885493\n",
      "Epoch 2 | Step 800500 | Avg Loss: 0.0153 | Grad Norm: 0.00844485\n",
      "Epoch 2 | Step 800600 | Avg Loss: 0.0148 | Grad Norm: 0.00917393\n",
      "Epoch 2 | Step 800700 | Avg Loss: 0.0146 | Grad Norm: 0.00831754\n",
      "Epoch 2 | Step 800800 | Avg Loss: 0.0141 | Grad Norm: 0.00944378\n",
      "Epoch 2 | Step 800900 | Avg Loss: 0.0145 | Grad Norm: 0.00902876\n",
      "Epoch 2 | Step 801000 | Avg Loss: 0.0144 | Grad Norm: 0.01038573\n",
      "Epoch 2 | Step 801100 | Avg Loss: 0.0144 | Grad Norm: 0.00744653\n",
      "Epoch 2 | Step 801200 | Avg Loss: 0.0145 | Grad Norm: 0.00747882\n",
      "Epoch 2 | Step 801300 | Avg Loss: 0.0146 | Grad Norm: 0.00822907\n",
      "Epoch 2 | Step 801400 | Avg Loss: 0.0144 | Grad Norm: 0.00786569\n",
      "Epoch 2 | Step 801500 | Avg Loss: 0.0146 | Grad Norm: 0.00910104\n",
      "Epoch 2 | Step 801600 | Avg Loss: 0.0149 | Grad Norm: 0.00729899\n",
      "Epoch 2 | Step 801700 | Avg Loss: 0.0149 | Grad Norm: 0.00798511\n",
      "Epoch 2 | Step 801800 | Avg Loss: 0.0148 | Grad Norm: 0.00906577\n",
      "Epoch 2 | Step 801900 | Avg Loss: 0.0151 | Grad Norm: 0.00801647\n",
      "Epoch 2 | Step 802000 | Avg Loss: 0.0148 | Grad Norm: 0.00812293\n",
      "Epoch 2 | Step 802100 | Avg Loss: 0.0145 | Grad Norm: 0.00794852\n",
      "Epoch 2 | Step 802200 | Avg Loss: 0.0146 | Grad Norm: 0.00835890\n",
      "Epoch 2 | Step 802300 | Avg Loss: 0.0143 | Grad Norm: 0.00738187\n",
      "Epoch 2 | Step 802400 | Avg Loss: 0.0147 | Grad Norm: 0.00802023\n",
      "Epoch 2 | Step 802500 | Avg Loss: 0.0146 | Grad Norm: 0.00825366\n",
      "Epoch 2 | Step 802600 | Avg Loss: 0.0147 | Grad Norm: 0.00870346\n",
      "Epoch 2 | Step 802700 | Avg Loss: 0.0151 | Grad Norm: 0.00942315\n",
      "Epoch 2 | Step 802800 | Avg Loss: 0.0150 | Grad Norm: 0.00964007\n",
      "Epoch 2 | Step 802900 | Avg Loss: 0.0151 | Grad Norm: 0.00923116\n",
      "Epoch 2 | Step 803000 | Avg Loss: 0.0156 | Grad Norm: 0.00918119\n",
      "Epoch 2 | Step 803100 | Avg Loss: 0.0154 | Grad Norm: 0.00938407\n",
      "Epoch 2 | Step 803200 | Avg Loss: 0.0152 | Grad Norm: 0.00958940\n",
      "Epoch 2 | Step 803300 | Avg Loss: 0.0149 | Grad Norm: 0.00866365\n",
      "Epoch 2 | Step 803400 | Avg Loss: 0.0147 | Grad Norm: 0.00830484\n",
      "Epoch 2 | Step 803500 | Avg Loss: 0.0146 | Grad Norm: 0.01036364\n",
      "Epoch 2 | Step 803600 | Avg Loss: 0.0149 | Grad Norm: 0.00889723\n",
      "Epoch 2 | Step 803700 | Avg Loss: 0.0147 | Grad Norm: 0.00874497\n",
      "Epoch 2 | Step 803800 | Avg Loss: 0.0148 | Grad Norm: 0.00948753\n",
      "Epoch 2 | Step 803900 | Avg Loss: 0.0147 | Grad Norm: 0.00914050\n",
      "Epoch 2 | Step 804000 | Avg Loss: 0.0146 | Grad Norm: 0.00857284\n",
      "Epoch 2 | Step 804100 | Avg Loss: 0.0153 | Grad Norm: 0.00963633\n",
      "Epoch 2 | Step 804200 | Avg Loss: 0.0151 | Grad Norm: 0.00879847\n",
      "Epoch 2 | Step 804300 | Avg Loss: 0.0147 | Grad Norm: 0.00920420\n",
      "Epoch 2 | Step 804400 | Avg Loss: 0.0148 | Grad Norm: 0.00901751\n",
      "Epoch 2 | Step 804500 | Avg Loss: 0.0144 | Grad Norm: 0.00807391\n",
      "Epoch 2 | Step 804600 | Avg Loss: 0.0145 | Grad Norm: 0.00896773\n",
      "Epoch 2 | Step 804700 | Avg Loss: 0.0148 | Grad Norm: 0.00912732\n",
      "Epoch 2 | Step 804800 | Avg Loss: 0.0153 | Grad Norm: 0.00824887\n",
      "Epoch 2 | Step 804900 | Avg Loss: 0.0152 | Grad Norm: 0.00802502\n",
      "Epoch 2 | Step 805000 | Avg Loss: 0.0152 | Grad Norm: 0.00754583\n",
      "Epoch 2 | Step 805100 | Avg Loss: 0.0150 | Grad Norm: 0.00898246\n",
      "Epoch 2 | Step 805200 | Avg Loss: 0.0151 | Grad Norm: 0.00901892\n",
      "Epoch 2 | Step 805300 | Avg Loss: 0.0150 | Grad Norm: 0.00892261\n",
      "Epoch 2 | Step 805400 | Avg Loss: 0.0154 | Grad Norm: 0.00873769\n",
      "Epoch 2 | Step 805500 | Avg Loss: 0.0151 | Grad Norm: 0.01000116\n",
      "Epoch 2 | Step 805600 | Avg Loss: 0.0151 | Grad Norm: 0.00818377\n",
      "Epoch 2 | Step 805700 | Avg Loss: 0.0149 | Grad Norm: 0.00824060\n",
      "Epoch 2 | Step 805800 | Avg Loss: 0.0150 | Grad Norm: 0.00838764\n",
      "Epoch 2 | Step 805900 | Avg Loss: 0.0154 | Grad Norm: 0.00878285\n",
      "Epoch 2 | Step 806000 | Avg Loss: 0.0156 | Grad Norm: 0.00939500\n",
      "Epoch 2 | Step 806100 | Avg Loss: 0.0150 | Grad Norm: 0.00924490\n",
      "Epoch 2 | Step 806200 | Avg Loss: 0.0152 | Grad Norm: 0.00780434\n",
      "Epoch 2 | Step 806300 | Avg Loss: 0.0152 | Grad Norm: 0.00779020\n",
      "Epoch 2 | Step 806400 | Avg Loss: 0.0152 | Grad Norm: 0.00811165\n",
      "Epoch 2 | Step 806500 | Avg Loss: 0.0156 | Grad Norm: 0.00993654\n",
      "Epoch 2 | Step 806600 | Avg Loss: 0.0155 | Grad Norm: 0.00839498\n",
      "Epoch 2 | Step 806700 | Avg Loss: 0.0156 | Grad Norm: 0.00936154\n",
      "Epoch 2 | Step 806800 | Avg Loss: 0.0155 | Grad Norm: 0.00842979\n",
      "Epoch 2 | Step 806900 | Avg Loss: 0.0154 | Grad Norm: 0.00980277\n",
      "Epoch 2 | Step 807000 | Avg Loss: 0.0153 | Grad Norm: 0.00861471\n",
      "Epoch 2 | Step 807100 | Avg Loss: 0.0155 | Grad Norm: 0.00977866\n",
      "Epoch 2 | Step 807200 | Avg Loss: 0.0154 | Grad Norm: 0.00838439\n",
      "Epoch 2 | Step 807300 | Avg Loss: 0.0147 | Grad Norm: 0.00871737\n",
      "Epoch 2 | Step 807400 | Avg Loss: 0.0146 | Grad Norm: 0.00788921\n",
      "Epoch 2 | Step 807500 | Avg Loss: 0.0152 | Grad Norm: 0.00911357\n",
      "Epoch 2 | Step 807600 | Avg Loss: 0.0152 | Grad Norm: 0.00832584\n",
      "Epoch 2 | Step 807700 | Avg Loss: 0.0148 | Grad Norm: 0.00929623\n",
      "Epoch 2 | Step 807800 | Avg Loss: 0.0148 | Grad Norm: 0.00967437\n",
      "Epoch 2 | Step 807900 | Avg Loss: 0.0147 | Grad Norm: 0.00817322\n",
      "Epoch 2 | Step 808000 | Avg Loss: 0.0151 | Grad Norm: 0.00820081\n",
      "Epoch 2 | Step 808100 | Avg Loss: 0.0151 | Grad Norm: 0.00958129\n",
      "Epoch 2 | Step 808200 | Avg Loss: 0.0151 | Grad Norm: 0.00813949\n",
      "Epoch 2 | Step 808300 | Avg Loss: 0.0150 | Grad Norm: 0.01002173\n",
      "Epoch 2 | Step 808400 | Avg Loss: 0.0150 | Grad Norm: 0.00788379\n",
      "Epoch 2 | Step 808500 | Avg Loss: 0.0150 | Grad Norm: 0.00964944\n",
      "Epoch 2 | Step 808600 | Avg Loss: 0.0149 | Grad Norm: 0.00748584\n",
      "Epoch 2 | Step 808700 | Avg Loss: 0.0155 | Grad Norm: 0.00969961\n",
      "Epoch 2 | Step 808800 | Avg Loss: 0.0154 | Grad Norm: 0.00811795\n",
      "Epoch 2 | Step 808900 | Avg Loss: 0.0153 | Grad Norm: 0.00874888\n",
      "Epoch 2 | Step 809000 | Avg Loss: 0.0153 | Grad Norm: 0.00824910\n",
      "Epoch 2 | Step 809100 | Avg Loss: 0.0154 | Grad Norm: 0.00758596\n",
      "Epoch 2 | Step 809200 | Avg Loss: 0.0153 | Grad Norm: 0.00806658\n",
      "Epoch 2 | Step 809300 | Avg Loss: 0.0151 | Grad Norm: 0.00811483\n",
      "Epoch 2 | Step 809400 | Avg Loss: 0.0152 | Grad Norm: 0.00970378\n",
      "Epoch 2 | Step 809500 | Avg Loss: 0.0147 | Grad Norm: 0.00851922\n",
      "Epoch 2 | Step 809600 | Avg Loss: 0.0147 | Grad Norm: 0.00860011\n",
      "Epoch 2 | Step 809700 | Avg Loss: 0.0150 | Grad Norm: 0.00800182\n",
      "Epoch 2 | Step 809800 | Avg Loss: 0.0152 | Grad Norm: 0.00914865\n",
      "Epoch 2 | Step 809900 | Avg Loss: 0.0150 | Grad Norm: 0.00847103\n",
      "Epoch 2 | Step 810000 | Avg Loss: 0.0146 | Grad Norm: 0.01012477\n",
      "Epoch 2 | Step 810100 | Avg Loss: 0.0144 | Grad Norm: 0.00830810\n",
      "Epoch 2 | Step 810200 | Avg Loss: 0.0146 | Grad Norm: 0.00835638\n",
      "Epoch 2 | Step 810300 | Avg Loss: 0.0145 | Grad Norm: 0.00895653\n",
      "Epoch 2 | Step 810400 | Avg Loss: 0.0149 | Grad Norm: 0.00859421\n",
      "Epoch 2 | Step 810500 | Avg Loss: 0.0147 | Grad Norm: 0.01290042\n",
      "Epoch 2 | Step 810600 | Avg Loss: 0.0145 | Grad Norm: 0.00893765\n",
      "Epoch 2 | Step 810700 | Avg Loss: 0.0147 | Grad Norm: 0.00783926\n",
      "Epoch 2 | Step 810800 | Avg Loss: 0.0148 | Grad Norm: 0.00870938\n",
      "Epoch 2 | Step 810900 | Avg Loss: 0.0149 | Grad Norm: 0.00916022\n",
      "Epoch 2 | Step 811000 | Avg Loss: 0.0149 | Grad Norm: 0.01243661\n",
      "Epoch 2 | Step 811100 | Avg Loss: 0.0152 | Grad Norm: 0.00873382\n",
      "Epoch 2 | Step 811200 | Avg Loss: 0.0149 | Grad Norm: 0.00823073\n",
      "Epoch 2 | Step 811300 | Avg Loss: 0.0147 | Grad Norm: 0.00900035\n",
      "Epoch 2 | Step 811400 | Avg Loss: 0.0150 | Grad Norm: 0.00980449\n",
      "Epoch 2 | Step 811500 | Avg Loss: 0.0151 | Grad Norm: 0.00838441\n",
      "Epoch 2 | Step 811600 | Avg Loss: 0.0151 | Grad Norm: 0.00961717\n",
      "Epoch 2 | Step 811700 | Avg Loss: 0.0149 | Grad Norm: 0.00747824\n",
      "Epoch 2 | Step 811800 | Avg Loss: 0.0148 | Grad Norm: 0.00795503\n",
      "Epoch 2 | Step 811900 | Avg Loss: 0.0146 | Grad Norm: 0.00891684\n",
      "Epoch 2 | Step 812000 | Avg Loss: 0.0150 | Grad Norm: 0.00878554\n",
      "Epoch 2 | Step 812100 | Avg Loss: 0.0149 | Grad Norm: 0.00893913\n",
      "Epoch 2 | Step 812200 | Avg Loss: 0.0150 | Grad Norm: 0.00818731\n",
      "Epoch 2 | Step 812300 | Avg Loss: 0.0150 | Grad Norm: 0.00919696\n",
      "Epoch 2 | Step 812400 | Avg Loss: 0.0148 | Grad Norm: 0.00951686\n",
      "Epoch 2 | Step 812500 | Avg Loss: 0.0146 | Grad Norm: 0.00851069\n",
      "Epoch 2 | Step 812600 | Avg Loss: 0.0147 | Grad Norm: 0.00840311\n",
      "Epoch 2 | Step 812700 | Avg Loss: 0.0150 | Grad Norm: 0.00995783\n",
      "Epoch 2 | Step 812800 | Avg Loss: 0.0146 | Grad Norm: 0.00881897\n",
      "Epoch 2 | Step 812900 | Avg Loss: 0.0147 | Grad Norm: 0.00767522\n",
      "Epoch 2 | Step 813000 | Avg Loss: 0.0147 | Grad Norm: 0.00798050\n",
      "Epoch 2 | Step 813100 | Avg Loss: 0.0152 | Grad Norm: 0.00930098\n",
      "Epoch 2 | Step 813200 | Avg Loss: 0.0153 | Grad Norm: 0.00837792\n",
      "Epoch 2 | Step 813300 | Avg Loss: 0.0151 | Grad Norm: 0.00804236\n",
      "Epoch 2 | Step 813400 | Avg Loss: 0.0152 | Grad Norm: 0.00952642\n",
      "Epoch 2 | Step 813500 | Avg Loss: 0.0151 | Grad Norm: 0.00952422\n",
      "Epoch 2 | Step 813600 | Avg Loss: 0.0153 | Grad Norm: 0.00893730\n",
      "Epoch 2 | Step 813700 | Avg Loss: 0.0155 | Grad Norm: 0.00766978\n",
      "Epoch 2 | Step 813800 | Avg Loss: 0.0154 | Grad Norm: 0.00873669\n",
      "Epoch 2 | Step 813900 | Avg Loss: 0.0151 | Grad Norm: 0.00727134\n",
      "Epoch 2 | Step 814000 | Avg Loss: 0.0152 | Grad Norm: 0.00803137\n",
      "Epoch 2 | Step 814100 | Avg Loss: 0.0150 | Grad Norm: 0.00704160\n",
      "Epoch 2 | Step 814200 | Avg Loss: 0.0151 | Grad Norm: 0.00823698\n",
      "Epoch 2 | Step 814300 | Avg Loss: 0.0149 | Grad Norm: 0.00698969\n",
      "Epoch 2 | Step 814400 | Avg Loss: 0.0147 | Grad Norm: 0.00863158\n",
      "Epoch 2 | Step 814500 | Avg Loss: 0.0146 | Grad Norm: 0.00792587\n",
      "Epoch 2 | Step 814600 | Avg Loss: 0.0150 | Grad Norm: 0.00877783\n",
      "Epoch 2 | Step 814700 | Avg Loss: 0.0151 | Grad Norm: 0.00898227\n",
      "Epoch 2 | Step 814800 | Avg Loss: 0.0149 | Grad Norm: 0.00847309\n",
      "Epoch 2 | Step 814900 | Avg Loss: 0.0151 | Grad Norm: 0.00846804\n",
      "Epoch 2 | Step 815000 | Avg Loss: 0.0151 | Grad Norm: 0.00953600\n",
      "Epoch 2 | Step 815100 | Avg Loss: 0.0151 | Grad Norm: 0.00863003\n",
      "Epoch 2 | Step 815200 | Avg Loss: 0.0147 | Grad Norm: 0.00765154\n",
      "Epoch 2 | Step 815300 | Avg Loss: 0.0144 | Grad Norm: 0.00868034\n",
      "Epoch 2 | Step 815400 | Avg Loss: 0.0142 | Grad Norm: 0.00745493\n",
      "Epoch 2 | Step 815500 | Avg Loss: 0.0139 | Grad Norm: 0.00857083\n",
      "Epoch 2 | Step 815600 | Avg Loss: 0.0145 | Grad Norm: 0.00721990\n",
      "Epoch 2 | Step 815700 | Avg Loss: 0.0148 | Grad Norm: 0.00841233\n",
      "Epoch 2 | Step 815800 | Avg Loss: 0.0148 | Grad Norm: 0.00771630\n",
      "Epoch 2 | Step 815900 | Avg Loss: 0.0146 | Grad Norm: 0.00999846\n",
      "Epoch 2 | Step 816000 | Avg Loss: 0.0145 | Grad Norm: 0.00882977\n",
      "Epoch 2 | Step 816100 | Avg Loss: 0.0146 | Grad Norm: 0.00846259\n",
      "Epoch 2 | Step 816200 | Avg Loss: 0.0147 | Grad Norm: 0.00860319\n",
      "Epoch 2 | Step 816300 | Avg Loss: 0.0147 | Grad Norm: 0.00808828\n",
      "Epoch 2 | Step 816400 | Avg Loss: 0.0148 | Grad Norm: 0.00900472\n",
      "Epoch 2 | Step 816500 | Avg Loss: 0.0151 | Grad Norm: 0.00791218\n",
      "Epoch 2 | Step 816600 | Avg Loss: 0.0149 | Grad Norm: 0.00913846\n",
      "Epoch 2 | Step 816700 | Avg Loss: 0.0153 | Grad Norm: 0.00958403\n",
      "Epoch 2 | Step 816800 | Avg Loss: 0.0153 | Grad Norm: 0.00873741\n",
      "Epoch 2 | Step 816900 | Avg Loss: 0.0154 | Grad Norm: 0.00792947\n",
      "Epoch 2 | Step 817000 | Avg Loss: 0.0147 | Grad Norm: 0.00938839\n",
      "Epoch 2 | Step 817100 | Avg Loss: 0.0150 | Grad Norm: 0.00839052\n",
      "Epoch 2 | Step 817200 | Avg Loss: 0.0148 | Grad Norm: 0.00877616\n",
      "Epoch 2 | Step 817300 | Avg Loss: 0.0144 | Grad Norm: 0.00777090\n",
      "Epoch 2 | Step 817400 | Avg Loss: 0.0148 | Grad Norm: 0.01007283\n",
      "Epoch 2 | Step 817500 | Avg Loss: 0.0153 | Grad Norm: 0.00859720\n",
      "Epoch 2 | Step 817600 | Avg Loss: 0.0155 | Grad Norm: 0.00864212\n",
      "Epoch 2 | Step 817700 | Avg Loss: 0.0153 | Grad Norm: 0.00856768\n",
      "Epoch 2 | Step 817800 | Avg Loss: 0.0152 | Grad Norm: 0.00847557\n",
      "Epoch 2 | Step 817900 | Avg Loss: 0.0153 | Grad Norm: 0.01047173\n",
      "Epoch 2 | Step 818000 | Avg Loss: 0.0152 | Grad Norm: 0.00953180\n",
      "Epoch 2 | Step 818100 | Avg Loss: 0.0151 | Grad Norm: 0.00919855\n",
      "Epoch 2 | Step 818200 | Avg Loss: 0.0156 | Grad Norm: 0.00899335\n",
      "Epoch 2 | Step 818300 | Avg Loss: 0.0157 | Grad Norm: 0.00920865\n",
      "Epoch 2 | Step 818400 | Avg Loss: 0.0152 | Grad Norm: 0.00912535\n",
      "Epoch 2 | Step 818500 | Avg Loss: 0.0151 | Grad Norm: 0.00812966\n",
      "Epoch 2 | Step 818600 | Avg Loss: 0.0151 | Grad Norm: 0.00941065\n",
      "Epoch 2 | Step 818700 | Avg Loss: 0.0150 | Grad Norm: 0.00810982\n",
      "Epoch 2 | Step 818800 | Avg Loss: 0.0151 | Grad Norm: 0.00904942\n",
      "Epoch 2 | Step 818900 | Avg Loss: 0.0150 | Grad Norm: 0.00826510\n",
      "Epoch 2 | Step 819000 | Avg Loss: 0.0155 | Grad Norm: 0.00843768\n",
      "Epoch 2 | Step 819100 | Avg Loss: 0.0152 | Grad Norm: 0.00924494\n",
      "Epoch 2 | Step 819200 | Avg Loss: 0.0150 | Grad Norm: 0.00755840\n",
      "Epoch 2 | Step 819300 | Avg Loss: 0.0156 | Grad Norm: 0.00850701\n",
      "Epoch 2 | Step 819400 | Avg Loss: 0.0156 | Grad Norm: 0.01034584\n",
      "Epoch 2 | Step 819500 | Avg Loss: 0.0155 | Grad Norm: 0.00956783\n",
      "Epoch 2 | Step 819600 | Avg Loss: 0.0156 | Grad Norm: 0.00751049\n",
      "Epoch 2 | Step 819700 | Avg Loss: 0.0154 | Grad Norm: 0.00912471\n",
      "Epoch 2 | Step 819800 | Avg Loss: 0.0153 | Grad Norm: 0.00935410\n",
      "Epoch 2 | Step 819900 | Avg Loss: 0.0151 | Grad Norm: 0.00743397\n",
      "Epoch 2 | Step 820000 | Avg Loss: 0.0153 | Grad Norm: 0.00759332\n",
      "Epoch 2 | Step 820100 | Avg Loss: 0.0152 | Grad Norm: 0.00994800\n",
      "Epoch 2 | Step 820200 | Avg Loss: 0.0148 | Grad Norm: 0.01054287\n",
      "Epoch 2 | Step 820300 | Avg Loss: 0.0151 | Grad Norm: 0.00873049\n",
      "Epoch 2 | Step 820400 | Avg Loss: 0.0149 | Grad Norm: 0.00956583\n",
      "Epoch 2 | Step 820500 | Avg Loss: 0.0150 | Grad Norm: 0.00941625\n",
      "Epoch 2 | Step 820600 | Avg Loss: 0.0147 | Grad Norm: 0.00836361\n",
      "Epoch 2 | Step 820700 | Avg Loss: 0.0151 | Grad Norm: 0.00944146\n",
      "Epoch 2 | Step 820800 | Avg Loss: 0.0147 | Grad Norm: 0.00990283\n",
      "Epoch 2 | Step 820900 | Avg Loss: 0.0149 | Grad Norm: 0.00969804\n",
      "Epoch 2 | Step 821000 | Avg Loss: 0.0151 | Grad Norm: 0.00948903\n",
      "Epoch 2 | Step 821100 | Avg Loss: 0.0151 | Grad Norm: 0.00941709\n",
      "Epoch 2 | Step 821200 | Avg Loss: 0.0147 | Grad Norm: 0.00930121\n",
      "Epoch 2 | Step 821300 | Avg Loss: 0.0149 | Grad Norm: 0.00802616\n",
      "Epoch 2 | Step 821400 | Avg Loss: 0.0149 | Grad Norm: 0.00864899\n",
      "Epoch 2 | Step 821500 | Avg Loss: 0.0150 | Grad Norm: 0.00823511\n",
      "Epoch 2 | Step 821600 | Avg Loss: 0.0145 | Grad Norm: 0.00890316\n",
      "Epoch 2 | Step 821700 | Avg Loss: 0.0146 | Grad Norm: 0.00979536\n",
      "Epoch 2 | Step 821800 | Avg Loss: 0.0148 | Grad Norm: 0.00922118\n",
      "Epoch 2 | Step 821900 | Avg Loss: 0.0143 | Grad Norm: 0.00964069\n",
      "Epoch 2 | Step 822000 | Avg Loss: 0.0146 | Grad Norm: 0.00751685\n",
      "Epoch 2 | Step 822100 | Avg Loss: 0.0145 | Grad Norm: 0.00767441\n",
      "Epoch 2 | Step 822200 | Avg Loss: 0.0147 | Grad Norm: 0.00873331\n",
      "Epoch 2 | Step 822300 | Avg Loss: 0.0147 | Grad Norm: 0.00781067\n",
      "Epoch 2 | Step 822400 | Avg Loss: 0.0150 | Grad Norm: 0.00860143\n",
      "Epoch 2 | Step 822500 | Avg Loss: 0.0152 | Grad Norm: 0.00901420\n",
      "Epoch 2 | Step 822600 | Avg Loss: 0.0151 | Grad Norm: 0.00996894\n",
      "Epoch 2 | Step 822700 | Avg Loss: 0.0147 | Grad Norm: 0.00927118\n",
      "Epoch 2 | Step 822800 | Avg Loss: 0.0147 | Grad Norm: 0.01027303\n",
      "Epoch 2 | Step 822900 | Avg Loss: 0.0151 | Grad Norm: 0.00789142\n",
      "Epoch 2 | Step 823000 | Avg Loss: 0.0150 | Grad Norm: 0.00910037\n",
      "Epoch 2 | Step 823100 | Avg Loss: 0.0150 | Grad Norm: 0.00895755\n",
      "Epoch 2 | Step 823200 | Avg Loss: 0.0155 | Grad Norm: 0.00741967\n",
      "Epoch 2 | Step 823300 | Avg Loss: 0.0154 | Grad Norm: 0.00845151\n",
      "Epoch 2 | Step 823400 | Avg Loss: 0.0152 | Grad Norm: 0.00846575\n",
      "Epoch 2 | Step 823500 | Avg Loss: 0.0157 | Grad Norm: 0.00955085\n",
      "Epoch 2 | Step 823600 | Avg Loss: 0.0156 | Grad Norm: 0.00844258\n",
      "Epoch 2 | Step 823700 | Avg Loss: 0.0156 | Grad Norm: 0.01010975\n",
      "Epoch 2 | Step 823800 | Avg Loss: 0.0155 | Grad Norm: 0.00814245\n",
      "Epoch 2 | Step 823900 | Avg Loss: 0.0150 | Grad Norm: 0.00839011\n",
      "Epoch 2 | Step 824000 | Avg Loss: 0.0150 | Grad Norm: 0.00827836\n",
      "Epoch 2 | Step 824100 | Avg Loss: 0.0148 | Grad Norm: 0.00782657\n",
      "Epoch 2 | Step 824200 | Avg Loss: 0.0145 | Grad Norm: 0.01058105\n",
      "Epoch 2 | Step 824300 | Avg Loss: 0.0146 | Grad Norm: 0.00947776\n",
      "Epoch 2 | Step 824400 | Avg Loss: 0.0151 | Grad Norm: 0.00955235\n",
      "Epoch 2 | Step 824500 | Avg Loss: 0.0149 | Grad Norm: 0.00842882\n",
      "Epoch 2 | Step 824600 | Avg Loss: 0.0149 | Grad Norm: 0.00838759\n",
      "Epoch 2 | Step 824700 | Avg Loss: 0.0151 | Grad Norm: 0.00783281\n",
      "Epoch 2 | Step 824800 | Avg Loss: 0.0152 | Grad Norm: 0.00803121\n",
      "Epoch 2 | Step 824900 | Avg Loss: 0.0148 | Grad Norm: 0.01029868\n",
      "Epoch 2 | Step 825000 | Avg Loss: 0.0147 | Grad Norm: 0.00828745\n",
      "Epoch 2 | Step 825100 | Avg Loss: 0.0150 | Grad Norm: 0.01095205\n",
      "Epoch 2 | Step 825200 | Avg Loss: 0.0147 | Grad Norm: 0.00931678\n",
      "Epoch 2 | Step 825300 | Avg Loss: 0.0149 | Grad Norm: 0.00856253\n",
      "Epoch 2 | Step 825400 | Avg Loss: 0.0150 | Grad Norm: 0.00924805\n",
      "Epoch 2 | Step 825500 | Avg Loss: 0.0151 | Grad Norm: 0.00955820\n",
      "Epoch 2 | Step 825600 | Avg Loss: 0.0157 | Grad Norm: 0.00824122\n",
      "Epoch 2 | Step 825700 | Avg Loss: 0.0154 | Grad Norm: 0.00760169\n",
      "Epoch 2 | Step 825800 | Avg Loss: 0.0154 | Grad Norm: 0.00790614\n",
      "Epoch 2 | Step 825900 | Avg Loss: 0.0153 | Grad Norm: 0.00818214\n",
      "Epoch 2 | Step 826000 | Avg Loss: 0.0157 | Grad Norm: 0.00840840\n",
      "Epoch 2 | Step 826100 | Avg Loss: 0.0155 | Grad Norm: 0.00913412\n",
      "Epoch 2 | Step 826200 | Avg Loss: 0.0150 | Grad Norm: 0.00862023\n",
      "Epoch 2 | Step 826300 | Avg Loss: 0.0149 | Grad Norm: 0.00868530\n",
      "Epoch 2 | Step 826400 | Avg Loss: 0.0146 | Grad Norm: 0.00779180\n",
      "Epoch 2 | Step 826500 | Avg Loss: 0.0147 | Grad Norm: 0.00924619\n",
      "Epoch 2 | Step 826600 | Avg Loss: 0.0145 | Grad Norm: 0.00978068\n",
      "Epoch 2 | Step 826700 | Avg Loss: 0.0148 | Grad Norm: 0.00809866\n",
      "Epoch 2 | Step 826800 | Avg Loss: 0.0145 | Grad Norm: 0.00858647\n",
      "Epoch 2 | Step 826900 | Avg Loss: 0.0149 | Grad Norm: 0.00712955\n",
      "Epoch 2 | Step 827000 | Avg Loss: 0.0150 | Grad Norm: 0.01019489\n",
      "Epoch 2 | Step 827100 | Avg Loss: 0.0144 | Grad Norm: 0.00912685\n",
      "Epoch 2 | Step 827200 | Avg Loss: 0.0144 | Grad Norm: 0.00891140\n",
      "Epoch 2 | Step 827300 | Avg Loss: 0.0144 | Grad Norm: 0.00857517\n",
      "Epoch 2 | Step 827400 | Avg Loss: 0.0145 | Grad Norm: 0.00835956\n",
      "Epoch 2 | Step 827500 | Avg Loss: 0.0145 | Grad Norm: 0.01031113\n",
      "Epoch 2 | Step 827600 | Avg Loss: 0.0148 | Grad Norm: 0.00925592\n",
      "Epoch 2 | Step 827700 | Avg Loss: 0.0146 | Grad Norm: 0.00875060\n",
      "Epoch 2 | Step 827800 | Avg Loss: 0.0143 | Grad Norm: 0.00834287\n",
      "Epoch 2 | Step 827900 | Avg Loss: 0.0145 | Grad Norm: 0.00905452\n",
      "Epoch 2 | Step 828000 | Avg Loss: 0.0148 | Grad Norm: 0.00958849\n",
      "Epoch 2 | Step 828100 | Avg Loss: 0.0145 | Grad Norm: 0.00995364\n",
      "Epoch 2 | Step 828200 | Avg Loss: 0.0145 | Grad Norm: 0.00742653\n",
      "Epoch 2 | Step 828300 | Avg Loss: 0.0151 | Grad Norm: 0.00930026\n",
      "Epoch 2 | Step 828400 | Avg Loss: 0.0149 | Grad Norm: 0.00796995\n",
      "Epoch 2 | Step 828500 | Avg Loss: 0.0149 | Grad Norm: 0.00912852\n",
      "Epoch 2 | Step 828600 | Avg Loss: 0.0150 | Grad Norm: 0.01012947\n",
      "Epoch 2 | Step 828700 | Avg Loss: 0.0153 | Grad Norm: 0.00871164\n",
      "Epoch 2 | Step 828800 | Avg Loss: 0.0152 | Grad Norm: 0.00924526\n",
      "Epoch 2 | Step 828900 | Avg Loss: 0.0154 | Grad Norm: 0.00977200\n",
      "Epoch 2 | Step 829000 | Avg Loss: 0.0158 | Grad Norm: 0.00839769\n",
      "Epoch 2 | Step 829100 | Avg Loss: 0.0153 | Grad Norm: 0.00914212\n",
      "Epoch 2 | Step 829200 | Avg Loss: 0.0152 | Grad Norm: 0.00845297\n",
      "Epoch 2 | Step 829300 | Avg Loss: 0.0149 | Grad Norm: 0.00860028\n",
      "Epoch 2 | Step 829400 | Avg Loss: 0.0149 | Grad Norm: 0.00990901\n",
      "Epoch 2 | Step 829500 | Avg Loss: 0.0151 | Grad Norm: 0.00842158\n",
      "Epoch 2 | Step 829600 | Avg Loss: 0.0151 | Grad Norm: 0.01228187\n",
      "Epoch 2 | Step 829700 | Avg Loss: 0.0153 | Grad Norm: 0.00754165\n",
      "Epoch 2 | Step 829800 | Avg Loss: 0.0149 | Grad Norm: 0.00915086\n",
      "Epoch 2 | Step 829900 | Avg Loss: 0.0151 | Grad Norm: 0.00730190\n",
      "Epoch 2 | Step 830000 | Avg Loss: 0.0150 | Grad Norm: 0.00792991\n",
      "Epoch 2 | Step 830100 | Avg Loss: 0.0147 | Grad Norm: 0.00883276\n",
      "Epoch 2 | Step 830200 | Avg Loss: 0.0150 | Grad Norm: 0.00743867\n",
      "Epoch 2 | Step 830300 | Avg Loss: 0.0148 | Grad Norm: 0.00797876\n",
      "Epoch 2 | Step 830400 | Avg Loss: 0.0147 | Grad Norm: 0.00761805\n",
      "Epoch 2 | Step 830500 | Avg Loss: 0.0150 | Grad Norm: 0.00831890\n",
      "Epoch 2 | Step 830600 | Avg Loss: 0.0157 | Grad Norm: 0.01017182\n",
      "Epoch 2 | Step 830700 | Avg Loss: 0.0156 | Grad Norm: 0.00801986\n",
      "Epoch 2 | Step 830800 | Avg Loss: 0.0150 | Grad Norm: 0.00818360\n",
      "Epoch 2 | Step 830900 | Avg Loss: 0.0149 | Grad Norm: 0.00720789\n",
      "Epoch 2 | Step 831000 | Avg Loss: 0.0147 | Grad Norm: 0.00888020\n",
      "Epoch 2 | Step 831100 | Avg Loss: 0.0147 | Grad Norm: 0.00823632\n",
      "Epoch 2 | Step 831200 | Avg Loss: 0.0150 | Grad Norm: 0.00860598\n",
      "Epoch 2 | Step 831300 | Avg Loss: 0.0148 | Grad Norm: 0.00879708\n",
      "Epoch 2 | Step 831400 | Avg Loss: 0.0151 | Grad Norm: 0.00843161\n",
      "Epoch 2 | Step 831500 | Avg Loss: 0.0147 | Grad Norm: 0.00828312\n",
      "Epoch 2 | Step 831600 | Avg Loss: 0.0149 | Grad Norm: 0.01045817\n",
      "Epoch 2 | Step 831700 | Avg Loss: 0.0150 | Grad Norm: 0.00853285\n",
      "Epoch 2 | Step 831800 | Avg Loss: 0.0151 | Grad Norm: 0.00916302\n",
      "Epoch 2 | Step 831900 | Avg Loss: 0.0149 | Grad Norm: 0.00900072\n",
      "Epoch 2 | Step 832000 | Avg Loss: 0.0146 | Grad Norm: 0.01042734\n",
      "Epoch 2 | Step 832100 | Avg Loss: 0.0145 | Grad Norm: 0.00809319\n",
      "Epoch 2 | Step 832200 | Avg Loss: 0.0150 | Grad Norm: 0.00903936\n",
      "Epoch 2 | Step 832300 | Avg Loss: 0.0147 | Grad Norm: 0.00824980\n",
      "Epoch 2 | Step 832400 | Avg Loss: 0.0152 | Grad Norm: 0.00906909\n",
      "Epoch 2 | Step 832500 | Avg Loss: 0.0148 | Grad Norm: 0.01036171\n",
      "Epoch 2 | Step 832600 | Avg Loss: 0.0149 | Grad Norm: 0.00802843\n",
      "Epoch 2 | Step 832700 | Avg Loss: 0.0151 | Grad Norm: 0.00837509\n",
      "Epoch 2 | Step 832800 | Avg Loss: 0.0151 | Grad Norm: 0.01103135\n",
      "Epoch 2 | Step 832900 | Avg Loss: 0.0149 | Grad Norm: 0.00895679\n",
      "Epoch 2 | Step 833000 | Avg Loss: 0.0149 | Grad Norm: 0.00882523\n",
      "Epoch 2 | Step 833100 | Avg Loss: 0.0154 | Grad Norm: 0.00921236\n",
      "Epoch 2 | Step 833200 | Avg Loss: 0.0153 | Grad Norm: 0.00932407\n",
      "Epoch 2 | Step 833300 | Avg Loss: 0.0152 | Grad Norm: 0.00800555\n",
      "Epoch 2 | Step 833400 | Avg Loss: 0.0154 | Grad Norm: 0.00910066\n",
      "Epoch 2 | Step 833500 | Avg Loss: 0.0154 | Grad Norm: 0.00839876\n",
      "Epoch 2 | Step 833600 | Avg Loss: 0.0149 | Grad Norm: 0.00829795\n",
      "Epoch 2 | Step 833700 | Avg Loss: 0.0149 | Grad Norm: 0.00897547\n",
      "Epoch 2 | Step 833800 | Avg Loss: 0.0148 | Grad Norm: 0.00740656\n",
      "Epoch 2 | Step 833900 | Avg Loss: 0.0149 | Grad Norm: 0.00735215\n",
      "Epoch 2 | Step 834000 | Avg Loss: 0.0146 | Grad Norm: 0.00804047\n",
      "Epoch 2 | Step 834100 | Avg Loss: 0.0147 | Grad Norm: 0.00800015\n",
      "Epoch 2 | Step 834200 | Avg Loss: 0.0147 | Grad Norm: 0.00857332\n",
      "Epoch 2 | Step 834300 | Avg Loss: 0.0146 | Grad Norm: 0.00911029\n",
      "Epoch 2 | Step 834400 | Avg Loss: 0.0144 | Grad Norm: 0.00965714\n",
      "Epoch 2 | Step 834500 | Avg Loss: 0.0148 | Grad Norm: 0.00733664\n",
      "Epoch 2 | Step 834600 | Avg Loss: 0.0149 | Grad Norm: 0.00872901\n",
      "Epoch 2 | Step 834700 | Avg Loss: 0.0149 | Grad Norm: 0.00946949\n",
      "Epoch 2 | Step 834800 | Avg Loss: 0.0148 | Grad Norm: 0.00891541\n",
      "Epoch 2 | Step 834900 | Avg Loss: 0.0148 | Grad Norm: 0.00861203\n",
      "Epoch 2 | Step 835000 | Avg Loss: 0.0149 | Grad Norm: 0.00883709\n",
      "Epoch 2 | Step 835100 | Avg Loss: 0.0150 | Grad Norm: 0.01074172\n",
      "Epoch 2 | Step 835200 | Avg Loss: 0.0152 | Grad Norm: 0.00937504\n",
      "Epoch 2 | Step 835300 | Avg Loss: 0.0156 | Grad Norm: 0.00846899\n",
      "Epoch 2 | Step 835400 | Avg Loss: 0.0154 | Grad Norm: 0.00831752\n",
      "Epoch 2 | Step 835500 | Avg Loss: 0.0154 | Grad Norm: 0.01000715\n",
      "Epoch 2 | Step 835600 | Avg Loss: 0.0156 | Grad Norm: 0.00783173\n",
      "Epoch 2 | Step 835700 | Avg Loss: 0.0152 | Grad Norm: 0.00824430\n",
      "Epoch 2 | Step 835800 | Avg Loss: 0.0155 | Grad Norm: 0.00885004\n",
      "Epoch 2 | Step 835900 | Avg Loss: 0.0152 | Grad Norm: 0.00883533\n",
      "Epoch 2 | Step 836000 | Avg Loss: 0.0154 | Grad Norm: 0.00815035\n",
      "Epoch 2 | Step 836100 | Avg Loss: 0.0153 | Grad Norm: 0.00966818\n",
      "Epoch 2 | Step 836200 | Avg Loss: 0.0153 | Grad Norm: 0.00694555\n",
      "Epoch 2 | Step 836300 | Avg Loss: 0.0154 | Grad Norm: 0.00853879\n",
      "Epoch 2 | Step 836400 | Avg Loss: 0.0154 | Grad Norm: 0.00838823\n",
      "Epoch 2 | Step 836500 | Avg Loss: 0.0152 | Grad Norm: 0.00946384\n",
      "Epoch 2 | Step 836600 | Avg Loss: 0.0152 | Grad Norm: 0.00956391\n",
      "Epoch 2 | Step 836700 | Avg Loss: 0.0153 | Grad Norm: 0.00908894\n",
      "Epoch 2 | Step 836800 | Avg Loss: 0.0148 | Grad Norm: 0.00789522\n",
      "Epoch 2 | Step 836900 | Avg Loss: 0.0151 | Grad Norm: 0.01082618\n",
      "Epoch 2 | Step 837000 | Avg Loss: 0.0151 | Grad Norm: 0.00829781\n",
      "Epoch 2 | Step 837100 | Avg Loss: 0.0149 | Grad Norm: 0.00809689\n",
      "Epoch 2 | Step 837200 | Avg Loss: 0.0151 | Grad Norm: 0.00802260\n",
      "Epoch 2 | Step 837300 | Avg Loss: 0.0153 | Grad Norm: 0.00998104\n",
      "Epoch 2 | Step 837400 | Avg Loss: 0.0156 | Grad Norm: 0.01098036\n",
      "Epoch 2 | Step 837500 | Avg Loss: 0.0153 | Grad Norm: 0.00780418\n",
      "Epoch 2 | Step 837600 | Avg Loss: 0.0152 | Grad Norm: 0.00867493\n",
      "Epoch 2 | Step 837700 | Avg Loss: 0.0153 | Grad Norm: 0.01001509\n",
      "Epoch 2 | Step 837800 | Avg Loss: 0.0151 | Grad Norm: 0.00842966\n",
      "Epoch 2 | Step 837900 | Avg Loss: 0.0152 | Grad Norm: 0.00949951\n",
      "Epoch 2 | Step 838000 | Avg Loss: 0.0157 | Grad Norm: 0.00945722\n",
      "Epoch 2 | Step 838100 | Avg Loss: 0.0161 | Grad Norm: 0.00999622\n",
      "Epoch 2 | Step 838200 | Avg Loss: 0.0159 | Grad Norm: 0.00793169\n",
      "Epoch 2 | Step 838300 | Avg Loss: 0.0155 | Grad Norm: 0.00926072\n",
      "Epoch 2 | Step 838400 | Avg Loss: 0.0151 | Grad Norm: 0.00885841\n",
      "Epoch 2 | Step 838500 | Avg Loss: 0.0150 | Grad Norm: 0.00954357\n",
      "Epoch 2 | Step 838600 | Avg Loss: 0.0150 | Grad Norm: 0.00844935\n",
      "Epoch 2 | Step 838700 | Avg Loss: 0.0152 | Grad Norm: 0.00884633\n",
      "Epoch 2 | Step 838800 | Avg Loss: 0.0147 | Grad Norm: 0.00845310\n",
      "Epoch 2 | Step 838900 | Avg Loss: 0.0147 | Grad Norm: 0.01095802\n",
      "Epoch 2 | Step 839000 | Avg Loss: 0.0148 | Grad Norm: 0.00873146\n",
      "Epoch 2 | Step 839100 | Avg Loss: 0.0144 | Grad Norm: 0.00964189\n",
      "Epoch 2 | Step 839200 | Avg Loss: 0.0144 | Grad Norm: 0.00856209\n",
      "Epoch 2 | Step 839300 | Avg Loss: 0.0146 | Grad Norm: 0.00918624\n",
      "Epoch 2 | Step 839400 | Avg Loss: 0.0146 | Grad Norm: 0.00945600\n",
      "Epoch 2 | Step 839500 | Avg Loss: 0.0145 | Grad Norm: 0.00797510\n",
      "Epoch 2 | Step 839600 | Avg Loss: 0.0148 | Grad Norm: 0.01102252\n",
      "Epoch 2 | Step 839700 | Avg Loss: 0.0152 | Grad Norm: 0.00870582\n",
      "Epoch 2 | Step 839800 | Avg Loss: 0.0148 | Grad Norm: 0.00861789\n",
      "Epoch 2 | Step 839900 | Avg Loss: 0.0146 | Grad Norm: 0.00939882\n",
      "Epoch 2 | Step 840000 | Avg Loss: 0.0147 | Grad Norm: 0.00888221\n",
      "Epoch 2 | Step 840100 | Avg Loss: 0.0149 | Grad Norm: 0.00900992\n",
      "Epoch 2 | Step 840200 | Avg Loss: 0.0149 | Grad Norm: 0.00928506\n",
      "Epoch 2 | Step 840300 | Avg Loss: 0.0148 | Grad Norm: 0.00963512\n",
      "Epoch 2 | Step 840400 | Avg Loss: 0.0151 | Grad Norm: 0.00778572\n",
      "Epoch 2 | Step 840500 | Avg Loss: 0.0148 | Grad Norm: 0.00956989\n",
      "Epoch 2 | Step 840600 | Avg Loss: 0.0147 | Grad Norm: 0.00845823\n",
      "Epoch 2 | Step 840700 | Avg Loss: 0.0146 | Grad Norm: 0.00987519\n",
      "Epoch 2 | Step 840800 | Avg Loss: 0.0145 | Grad Norm: 0.00927098\n",
      "Epoch 2 | Step 840900 | Avg Loss: 0.0149 | Grad Norm: 0.00926161\n",
      "Epoch 2 | Step 841000 | Avg Loss: 0.0145 | Grad Norm: 0.00848900\n",
      "Epoch 2 | Step 841100 | Avg Loss: 0.0145 | Grad Norm: 0.00769996\n",
      "Epoch 2 | Step 841200 | Avg Loss: 0.0147 | Grad Norm: 0.01031573\n",
      "Epoch 2 | Step 841300 | Avg Loss: 0.0150 | Grad Norm: 0.00951866\n",
      "Epoch 2 | Step 841400 | Avg Loss: 0.0154 | Grad Norm: 0.01203128\n",
      "Epoch 2 | Step 841500 | Avg Loss: 0.0147 | Grad Norm: 0.00760502\n",
      "Epoch 2 | Step 841600 | Avg Loss: 0.0149 | Grad Norm: 0.00838691\n",
      "Epoch 2 | Step 841700 | Avg Loss: 0.0149 | Grad Norm: 0.00915949\n",
      "Epoch 2 | Step 841800 | Avg Loss: 0.0146 | Grad Norm: 0.00751161\n",
      "Epoch 2 | Step 841900 | Avg Loss: 0.0146 | Grad Norm: 0.00826141\n",
      "Epoch 2 | Step 842000 | Avg Loss: 0.0146 | Grad Norm: 0.00897897\n",
      "Epoch 2 | Step 842100 | Avg Loss: 0.0149 | Grad Norm: 0.00918223\n",
      "Epoch 2 | Step 842200 | Avg Loss: 0.0151 | Grad Norm: 0.00767621\n",
      "Epoch 2 | Step 842300 | Avg Loss: 0.0148 | Grad Norm: 0.00800787\n",
      "Epoch 2 | Step 842400 | Avg Loss: 0.0150 | Grad Norm: 0.01162299\n",
      "Epoch 2 | Step 842500 | Avg Loss: 0.0148 | Grad Norm: 0.00917403\n",
      "Epoch 2 | Step 842600 | Avg Loss: 0.0150 | Grad Norm: 0.00813240\n",
      "Epoch 2 | Step 842700 | Avg Loss: 0.0150 | Grad Norm: 0.00801049\n",
      "Epoch 2 | Step 842800 | Avg Loss: 0.0151 | Grad Norm: 0.00790547\n",
      "Epoch 2 | Step 842900 | Avg Loss: 0.0149 | Grad Norm: 0.00897004\n",
      "Epoch 2 | Step 843000 | Avg Loss: 0.0146 | Grad Norm: 0.00820829\n",
      "Epoch 2 | Step 843100 | Avg Loss: 0.0150 | Grad Norm: 0.00909205\n",
      "Epoch 2 | Step 843200 | Avg Loss: 0.0150 | Grad Norm: 0.00953944\n",
      "Epoch 2 | Step 843300 | Avg Loss: 0.0154 | Grad Norm: 0.00812500\n",
      "Epoch 2 | Step 843400 | Avg Loss: 0.0155 | Grad Norm: 0.01127298\n",
      "Epoch 2 | Step 843500 | Avg Loss: 0.0151 | Grad Norm: 0.00882018\n",
      "Epoch 2 | Step 843600 | Avg Loss: 0.0146 | Grad Norm: 0.00758086\n",
      "Epoch 2 | Step 843700 | Avg Loss: 0.0142 | Grad Norm: 0.00862543\n",
      "Epoch 2 | Step 843800 | Avg Loss: 0.0145 | Grad Norm: 0.00801364\n",
      "Epoch 2 | Step 843900 | Avg Loss: 0.0148 | Grad Norm: 0.00852256\n",
      "Epoch 2 | Step 844000 | Avg Loss: 0.0149 | Grad Norm: 0.00824763\n",
      "Epoch 2 | Step 844100 | Avg Loss: 0.0148 | Grad Norm: 0.00868307\n",
      "Epoch 2 | Step 844200 | Avg Loss: 0.0150 | Grad Norm: 0.00808694\n",
      "Epoch 2 | Step 844300 | Avg Loss: 0.0154 | Grad Norm: 0.00780268\n",
      "Epoch 2 | Step 844400 | Avg Loss: 0.0151 | Grad Norm: 0.00794884\n",
      "Epoch 2 | Step 844500 | Avg Loss: 0.0152 | Grad Norm: 0.00856098\n",
      "Epoch 2 | Step 844600 | Avg Loss: 0.0151 | Grad Norm: 0.00819275\n",
      "Epoch 2 | Step 844700 | Avg Loss: 0.0148 | Grad Norm: 0.00913206\n",
      "Epoch 2 | Step 844800 | Avg Loss: 0.0145 | Grad Norm: 0.00936033\n",
      "Epoch 2 | Step 844900 | Avg Loss: 0.0143 | Grad Norm: 0.00725125\n",
      "Epoch 2 | Step 845000 | Avg Loss: 0.0145 | Grad Norm: 0.00870109\n",
      "Epoch 2 | Step 845100 | Avg Loss: 0.0142 | Grad Norm: 0.00859640\n",
      "Epoch 2 | Step 845200 | Avg Loss: 0.0143 | Grad Norm: 0.00881766\n",
      "Epoch 2 | Step 845300 | Avg Loss: 0.0142 | Grad Norm: 0.00756129\n",
      "Epoch 2 | Step 845400 | Avg Loss: 0.0145 | Grad Norm: 0.00822186\n",
      "Epoch 2 | Step 845500 | Avg Loss: 0.0144 | Grad Norm: 0.00925777\n",
      "Epoch 2 | Step 845600 | Avg Loss: 0.0149 | Grad Norm: 0.00936599\n",
      "Epoch 2 | Step 845700 | Avg Loss: 0.0153 | Grad Norm: 0.00921517\n",
      "Epoch 2 | Step 845800 | Avg Loss: 0.0150 | Grad Norm: 0.01018852\n",
      "Epoch 2 | Step 845900 | Avg Loss: 0.0151 | Grad Norm: 0.00777484\n",
      "Epoch 2 | Step 846000 | Avg Loss: 0.0150 | Grad Norm: 0.00952807\n",
      "Epoch 2 | Step 846100 | Avg Loss: 0.0150 | Grad Norm: 0.00789859\n",
      "Epoch 2 | Step 846200 | Avg Loss: 0.0148 | Grad Norm: 0.00997320\n",
      "Epoch 2 | Step 846300 | Avg Loss: 0.0147 | Grad Norm: 0.00783708\n",
      "Epoch 2 | Step 846400 | Avg Loss: 0.0150 | Grad Norm: 0.00814167\n",
      "Epoch 2 | Step 846500 | Avg Loss: 0.0145 | Grad Norm: 0.00791550\n",
      "Epoch 2 | Step 846600 | Avg Loss: 0.0145 | Grad Norm: 0.00753193\n",
      "Epoch 2 | Step 846700 | Avg Loss: 0.0142 | Grad Norm: 0.00829955\n",
      "Epoch 2 | Step 846800 | Avg Loss: 0.0145 | Grad Norm: 0.00739595\n",
      "Epoch 2 | Step 846900 | Avg Loss: 0.0144 | Grad Norm: 0.00814190\n",
      "Epoch 2 | Step 847000 | Avg Loss: 0.0142 | Grad Norm: 0.00934215\n",
      "Epoch 2 | Step 847100 | Avg Loss: 0.0146 | Grad Norm: 0.00942338\n",
      "Epoch 2 | Step 847200 | Avg Loss: 0.0146 | Grad Norm: 0.00783604\n",
      "Epoch 2 | Step 847300 | Avg Loss: 0.0148 | Grad Norm: 0.00856310\n",
      "Epoch 2 | Step 847400 | Avg Loss: 0.0147 | Grad Norm: 0.00938653\n",
      "Epoch 2 | Step 847500 | Avg Loss: 0.0146 | Grad Norm: 0.00779861\n",
      "Epoch 2 | Step 847600 | Avg Loss: 0.0147 | Grad Norm: 0.00797258\n",
      "Epoch 2 | Step 847700 | Avg Loss: 0.0154 | Grad Norm: 0.00944160\n",
      "Epoch 2 | Step 847800 | Avg Loss: 0.0152 | Grad Norm: 0.00847770\n",
      "Epoch 2 | Step 847900 | Avg Loss: 0.0151 | Grad Norm: 0.00968833\n",
      "Epoch 2 | Step 848000 | Avg Loss: 0.0147 | Grad Norm: 0.00815343\n",
      "Epoch 2 | Step 848100 | Avg Loss: 0.0144 | Grad Norm: 0.00989461\n",
      "Epoch 2 | Step 848200 | Avg Loss: 0.0146 | Grad Norm: 0.00831219\n",
      "Epoch 2 | Step 848300 | Avg Loss: 0.0146 | Grad Norm: 0.00845426\n",
      "Epoch 2 | Step 848400 | Avg Loss: 0.0146 | Grad Norm: 0.00963831\n",
      "Epoch 2 | Step 848500 | Avg Loss: 0.0147 | Grad Norm: 0.00943170\n",
      "Epoch 2 | Step 848600 | Avg Loss: 0.0147 | Grad Norm: 0.00834916\n",
      "Epoch 2 | Step 848700 | Avg Loss: 0.0147 | Grad Norm: 0.00834546\n",
      "Epoch 2 | Step 848800 | Avg Loss: 0.0154 | Grad Norm: 0.00965780\n",
      "Epoch 2 | Step 848900 | Avg Loss: 0.0155 | Grad Norm: 0.00723362\n",
      "Epoch 2 | Step 849000 | Avg Loss: 0.0151 | Grad Norm: 0.00797581\n",
      "Epoch 2 | Step 849100 | Avg Loss: 0.0150 | Grad Norm: 0.00853600\n",
      "Epoch 2 | Step 849200 | Avg Loss: 0.0146 | Grad Norm: 0.00857814\n",
      "Epoch 2 | Step 849300 | Avg Loss: 0.0145 | Grad Norm: 0.00859008\n",
      "Epoch 2 | Step 849400 | Avg Loss: 0.0145 | Grad Norm: 0.00925009\n",
      "Epoch 2 | Step 849500 | Avg Loss: 0.0145 | Grad Norm: 0.00803527\n",
      "Epoch 2 | Step 849600 | Avg Loss: 0.0145 | Grad Norm: 0.00884879\n",
      "Epoch 2 | Step 849700 | Avg Loss: 0.0146 | Grad Norm: 0.00731742\n",
      "Epoch 2 | Step 849800 | Avg Loss: 0.0145 | Grad Norm: 0.00969944\n",
      "Epoch 2 | Step 849900 | Avg Loss: 0.0146 | Grad Norm: 0.01116274\n",
      "Epoch 2 | Step 850000 | Avg Loss: 0.0147 | Grad Norm: 0.01152481\n",
      "Epoch 2 | Step 850100 | Avg Loss: 0.0150 | Grad Norm: 0.00913603\n",
      "Epoch 2 | Step 850200 | Avg Loss: 0.0155 | Grad Norm: 0.01025930\n",
      "Epoch 2 | Step 850300 | Avg Loss: 0.0153 | Grad Norm: 0.01210264\n",
      "Epoch 2 | Step 850400 | Avg Loss: 0.0151 | Grad Norm: 0.00904976\n",
      "Epoch 2 | Step 850500 | Avg Loss: 0.0147 | Grad Norm: 0.00900411\n",
      "Epoch 2 | Step 850600 | Avg Loss: 0.0148 | Grad Norm: 0.00888910\n",
      "Epoch 2 | Step 850700 | Avg Loss: 0.0150 | Grad Norm: 0.00837611\n",
      "Epoch 2 | Step 850800 | Avg Loss: 0.0151 | Grad Norm: 0.00799007\n",
      "Epoch 2 | Step 850900 | Avg Loss: 0.0146 | Grad Norm: 0.00826607\n",
      "Epoch 2 | Step 851000 | Avg Loss: 0.0147 | Grad Norm: 0.00841024\n",
      "Epoch 2 | Step 851100 | Avg Loss: 0.0148 | Grad Norm: 0.00826239\n",
      "Epoch 2 | Step 851200 | Avg Loss: 0.0149 | Grad Norm: 0.00953443\n",
      "Epoch 2 | Step 851300 | Avg Loss: 0.0151 | Grad Norm: 0.00833755\n",
      "Epoch 2 | Step 851400 | Avg Loss: 0.0153 | Grad Norm: 0.00886174\n",
      "Epoch 2 | Step 851500 | Avg Loss: 0.0148 | Grad Norm: 0.00766416\n",
      "Epoch 2 | Step 851600 | Avg Loss: 0.0148 | Grad Norm: 0.00906483\n",
      "Epoch 2 | Step 851700 | Avg Loss: 0.0148 | Grad Norm: 0.00826668\n",
      "Epoch 2 | Step 851800 | Avg Loss: 0.0150 | Grad Norm: 0.00791586\n",
      "Epoch 2 | Step 851900 | Avg Loss: 0.0152 | Grad Norm: 0.00924524\n",
      "Epoch 2 | Step 852000 | Avg Loss: 0.0145 | Grad Norm: 0.00696570\n",
      "Epoch 2 | Step 852100 | Avg Loss: 0.0146 | Grad Norm: 0.00807794\n",
      "Epoch 2 | Step 852200 | Avg Loss: 0.0148 | Grad Norm: 0.01038518\n",
      "Epoch 2 | Step 852300 | Avg Loss: 0.0149 | Grad Norm: 0.00734990\n",
      "Epoch 2 | Step 852400 | Avg Loss: 0.0155 | Grad Norm: 0.00837581\n",
      "Epoch 2 | Step 852500 | Avg Loss: 0.0153 | Grad Norm: 0.00895337\n",
      "Epoch 2 | Step 852600 | Avg Loss: 0.0149 | Grad Norm: 0.00756929\n",
      "Epoch 2 | Step 852700 | Avg Loss: 0.0147 | Grad Norm: 0.00792767\n",
      "Epoch 2 | Step 852800 | Avg Loss: 0.0149 | Grad Norm: 0.00898552\n",
      "Epoch 2 | Step 852900 | Avg Loss: 0.0152 | Grad Norm: 0.00802720\n",
      "Epoch 2 | Step 853000 | Avg Loss: 0.0149 | Grad Norm: 0.00939140\n",
      "Epoch 2 | Step 853100 | Avg Loss: 0.0146 | Grad Norm: 0.00858689\n",
      "Epoch 2 | Step 853200 | Avg Loss: 0.0146 | Grad Norm: 0.00896247\n",
      "Epoch 2 | Step 853300 | Avg Loss: 0.0150 | Grad Norm: 0.00930797\n",
      "Epoch 2 | Step 853400 | Avg Loss: 0.0148 | Grad Norm: 0.00775498\n",
      "Epoch 2 | Step 853500 | Avg Loss: 0.0148 | Grad Norm: 0.00943402\n",
      "Epoch 2 | Step 853600 | Avg Loss: 0.0147 | Grad Norm: 0.00757532\n",
      "Epoch 2 | Step 853700 | Avg Loss: 0.0149 | Grad Norm: 0.00885176\n",
      "Epoch 2 | Step 853800 | Avg Loss: 0.0150 | Grad Norm: 0.00788816\n",
      "Epoch 2 | Step 853900 | Avg Loss: 0.0151 | Grad Norm: 0.00733529\n",
      "Epoch 2 | Step 854000 | Avg Loss: 0.0148 | Grad Norm: 0.00842049\n",
      "Epoch 2 | Step 854100 | Avg Loss: 0.0148 | Grad Norm: 0.01010620\n",
      "Epoch 2 | Step 854200 | Avg Loss: 0.0155 | Grad Norm: 0.01014246\n",
      "Epoch 2 | Step 854300 | Avg Loss: 0.0155 | Grad Norm: 0.00920973\n",
      "Epoch 2 | Step 854400 | Avg Loss: 0.0154 | Grad Norm: 0.00695435\n",
      "Epoch 2 | Step 854500 | Avg Loss: 0.0154 | Grad Norm: 0.00890067\n",
      "Epoch 2 | Step 854600 | Avg Loss: 0.0150 | Grad Norm: 0.00857143\n",
      "Epoch 2 | Step 854700 | Avg Loss: 0.0146 | Grad Norm: 0.00924497\n",
      "Epoch 2 | Step 854800 | Avg Loss: 0.0147 | Grad Norm: 0.00909606\n",
      "Epoch 2 | Step 854900 | Avg Loss: 0.0148 | Grad Norm: 0.00807080\n",
      "Epoch 2 | Step 855000 | Avg Loss: 0.0149 | Grad Norm: 0.00907107\n",
      "Epoch 2 | Step 855100 | Avg Loss: 0.0146 | Grad Norm: 0.00861073\n",
      "Epoch 2 | Step 855200 | Avg Loss: 0.0144 | Grad Norm: 0.00962233\n",
      "Epoch 2 | Step 855300 | Avg Loss: 0.0147 | Grad Norm: 0.00853187\n",
      "Epoch 2 | Step 855400 | Avg Loss: 0.0150 | Grad Norm: 0.00875868\n",
      "Epoch 2 | Step 855500 | Avg Loss: 0.0147 | Grad Norm: 0.00744967\n",
      "Epoch 2 | Step 855600 | Avg Loss: 0.0148 | Grad Norm: 0.00829759\n",
      "Epoch 2 | Step 855700 | Avg Loss: 0.0149 | Grad Norm: 0.00844622\n",
      "Epoch 2 | Step 855800 | Avg Loss: 0.0152 | Grad Norm: 0.00779804\n",
      "Epoch 2 | Step 855900 | Avg Loss: 0.0153 | Grad Norm: 0.01013905\n",
      "Epoch 2 | Step 856000 | Avg Loss: 0.0150 | Grad Norm: 0.00871412\n",
      "Epoch 2 | Step 856100 | Avg Loss: 0.0149 | Grad Norm: 0.00856085\n",
      "Epoch 2 | Step 856200 | Avg Loss: 0.0152 | Grad Norm: 0.00892933\n",
      "Epoch 2 | Step 856300 | Avg Loss: 0.0154 | Grad Norm: 0.00988441\n",
      "Epoch 2 | Step 856400 | Avg Loss: 0.0152 | Grad Norm: 0.00893113\n",
      "Epoch 2 | Step 856500 | Avg Loss: 0.0145 | Grad Norm: 0.00728821\n",
      "Epoch 2 | Step 856600 | Avg Loss: 0.0147 | Grad Norm: 0.00883940\n",
      "Epoch 2 | Step 856700 | Avg Loss: 0.0147 | Grad Norm: 0.00877915\n",
      "Epoch 2 | Step 856800 | Avg Loss: 0.0147 | Grad Norm: 0.00841034\n",
      "Epoch 2 | Step 856900 | Avg Loss: 0.0149 | Grad Norm: 0.00754210\n",
      "Epoch 2 | Step 857000 | Avg Loss: 0.0149 | Grad Norm: 0.00977317\n",
      "Epoch 2 | Step 857100 | Avg Loss: 0.0150 | Grad Norm: 0.00887690\n",
      "Epoch 2 | Step 857200 | Avg Loss: 0.0150 | Grad Norm: 0.00916856\n",
      "Epoch 2 | Step 857300 | Avg Loss: 0.0151 | Grad Norm: 0.00964318\n",
      "Epoch 2 | Step 857400 | Avg Loss: 0.0149 | Grad Norm: 0.00857539\n",
      "Epoch 2 | Step 857500 | Avg Loss: 0.0151 | Grad Norm: 0.00742531\n",
      "Epoch 2 | Step 857600 | Avg Loss: 0.0150 | Grad Norm: 0.00933524\n",
      "Epoch 2 | Step 857700 | Avg Loss: 0.0147 | Grad Norm: 0.00888242\n",
      "Epoch 2 | Step 857800 | Avg Loss: 0.0146 | Grad Norm: 0.00896088\n",
      "Epoch 2 | Step 857900 | Avg Loss: 0.0146 | Grad Norm: 0.00964139\n",
      "Epoch 2 | Step 858000 | Avg Loss: 0.0146 | Grad Norm: 0.00992597\n",
      "Epoch 2 | Step 858100 | Avg Loss: 0.0148 | Grad Norm: 0.00839014\n",
      "Epoch 2 | Step 858200 | Avg Loss: 0.0150 | Grad Norm: 0.00899570\n",
      "Epoch 2 | Step 858300 | Avg Loss: 0.0149 | Grad Norm: 0.00903630\n",
      "Epoch 2 | Step 858400 | Avg Loss: 0.0149 | Grad Norm: 0.00870185\n",
      "Epoch 2 | Step 858500 | Avg Loss: 0.0150 | Grad Norm: 0.00930121\n",
      "Epoch 2 | Step 858600 | Avg Loss: 0.0151 | Grad Norm: 0.00902094\n",
      "Epoch 2 | Step 858700 | Avg Loss: 0.0150 | Grad Norm: 0.00874508\n",
      "Epoch 2 | Step 858800 | Avg Loss: 0.0155 | Grad Norm: 0.00905798\n",
      "Epoch 2 | Step 858900 | Avg Loss: 0.0156 | Grad Norm: 0.00804492\n",
      "Epoch 2 | Step 859000 | Avg Loss: 0.0156 | Grad Norm: 0.00834688\n",
      "Epoch 2 | Step 859100 | Avg Loss: 0.0155 | Grad Norm: 0.00904653\n",
      "Epoch 2 | Step 859200 | Avg Loss: 0.0154 | Grad Norm: 0.00911637\n",
      "Epoch 2 | Step 859300 | Avg Loss: 0.0153 | Grad Norm: 0.00966726\n",
      "Epoch 2 | Step 859400 | Avg Loss: 0.0153 | Grad Norm: 0.00862891\n",
      "Epoch 2 | Step 859500 | Avg Loss: 0.0153 | Grad Norm: 0.00891087\n",
      "Epoch 2 | Step 859600 | Avg Loss: 0.0155 | Grad Norm: 0.00819678\n",
      "Epoch 2 | Step 859700 | Avg Loss: 0.0154 | Grad Norm: 0.00894265\n",
      "Epoch 2 | Step 859800 | Avg Loss: 0.0154 | Grad Norm: 0.00856177\n",
      "Epoch 2 | Step 859900 | Avg Loss: 0.0154 | Grad Norm: 0.00812736\n",
      "Epoch 2 | Step 860000 | Avg Loss: 0.0152 | Grad Norm: 0.00907988\n",
      "Epoch 2 | Step 860100 | Avg Loss: 0.0157 | Grad Norm: 0.00926715\n",
      "Epoch 2 | Step 860200 | Avg Loss: 0.0156 | Grad Norm: 0.00945111\n",
      "Epoch 2 | Step 860300 | Avg Loss: 0.0155 | Grad Norm: 0.00817342\n",
      "Epoch 2 | Step 860400 | Avg Loss: 0.0149 | Grad Norm: 0.00919084\n",
      "Epoch 2 | Step 860500 | Avg Loss: 0.0149 | Grad Norm: 0.00797971\n",
      "Epoch 2 | Step 860600 | Avg Loss: 0.0149 | Grad Norm: 0.00911465\n",
      "Epoch 2 | Step 860700 | Avg Loss: 0.0150 | Grad Norm: 0.00991554\n",
      "Epoch 2 | Step 860800 | Avg Loss: 0.0150 | Grad Norm: 0.00879080\n",
      "Epoch 2 | Step 860900 | Avg Loss: 0.0153 | Grad Norm: 0.00907570\n",
      "Epoch 2 | Step 861000 | Avg Loss: 0.0154 | Grad Norm: 0.00860597\n",
      "Epoch 2 | Step 861100 | Avg Loss: 0.0156 | Grad Norm: 0.00865756\n",
      "Epoch 2 | Step 861200 | Avg Loss: 0.0156 | Grad Norm: 0.01019126\n",
      "Epoch 2 | Step 861300 | Avg Loss: 0.0151 | Grad Norm: 0.00882832\n",
      "Epoch 2 | Step 861400 | Avg Loss: 0.0151 | Grad Norm: 0.00742332\n",
      "Epoch 2 | Step 861500 | Avg Loss: 0.0150 | Grad Norm: 0.01018652\n",
      "Epoch 2 | Step 861600 | Avg Loss: 0.0150 | Grad Norm: 0.00798977\n",
      "Epoch 2 | Step 861700 | Avg Loss: 0.0149 | Grad Norm: 0.00804457\n",
      "Epoch 2 | Step 861800 | Avg Loss: 0.0151 | Grad Norm: 0.00873440\n",
      "Epoch 2 | Step 861900 | Avg Loss: 0.0151 | Grad Norm: 0.00913415\n",
      "Epoch 2 | Step 862000 | Avg Loss: 0.0149 | Grad Norm: 0.00881062\n",
      "Epoch 2 | Step 862100 | Avg Loss: 0.0144 | Grad Norm: 0.00830994\n",
      "Epoch 2 | Step 862200 | Avg Loss: 0.0146 | Grad Norm: 0.00864747\n",
      "Epoch 2 | Step 862300 | Avg Loss: 0.0147 | Grad Norm: 0.00799043\n",
      "Epoch 2 | Step 862400 | Avg Loss: 0.0147 | Grad Norm: 0.00881669\n",
      "Epoch 2 | Step 862500 | Avg Loss: 0.0145 | Grad Norm: 0.00894921\n",
      "Epoch 2 | Step 862600 | Avg Loss: 0.0145 | Grad Norm: 0.00874949\n",
      "Epoch 2 | Step 862700 | Avg Loss: 0.0148 | Grad Norm: 0.00771799\n",
      "Epoch 2 | Step 862800 | Avg Loss: 0.0148 | Grad Norm: 0.00799228\n",
      "Epoch 2 | Step 862900 | Avg Loss: 0.0148 | Grad Norm: 0.00725431\n",
      "Epoch 2 | Step 863000 | Avg Loss: 0.0145 | Grad Norm: 0.00916137\n",
      "Epoch 2 | Step 863100 | Avg Loss: 0.0146 | Grad Norm: 0.00987321\n",
      "Epoch 2 | Step 863200 | Avg Loss: 0.0150 | Grad Norm: 0.00893831\n",
      "Epoch 2 | Step 863300 | Avg Loss: 0.0147 | Grad Norm: 0.00955097\n",
      "Epoch 2 | Step 863400 | Avg Loss: 0.0146 | Grad Norm: 0.00794730\n",
      "Epoch 2 | Step 863500 | Avg Loss: 0.0143 | Grad Norm: 0.00863045\n",
      "Epoch 2 | Step 863600 | Avg Loss: 0.0147 | Grad Norm: 0.00836212\n",
      "Epoch 2 | Step 863700 | Avg Loss: 0.0146 | Grad Norm: 0.00818930\n",
      "Epoch 2 | Step 863800 | Avg Loss: 0.0146 | Grad Norm: 0.00781180\n",
      "Epoch 2 | Step 863900 | Avg Loss: 0.0145 | Grad Norm: 0.00723967\n",
      "Epoch 2 | Step 864000 | Avg Loss: 0.0146 | Grad Norm: 0.00964907\n",
      "Epoch 2 | Step 864100 | Avg Loss: 0.0145 | Grad Norm: 0.00882437\n",
      "Epoch 2 | Step 864200 | Avg Loss: 0.0151 | Grad Norm: 0.00925930\n",
      "Epoch 2 | Step 864300 | Avg Loss: 0.0152 | Grad Norm: 0.00805671\n",
      "Epoch 2 | Step 864400 | Avg Loss: 0.0147 | Grad Norm: 0.00813707\n",
      "Epoch 2 | Step 864500 | Avg Loss: 0.0150 | Grad Norm: 0.00854087\n",
      "Epoch 2 | Step 864600 | Avg Loss: 0.0147 | Grad Norm: 0.00872641\n",
      "Epoch 2 | Step 864700 | Avg Loss: 0.0152 | Grad Norm: 0.00841320\n",
      "Epoch 2 | Step 864800 | Avg Loss: 0.0153 | Grad Norm: 0.00780216\n",
      "Epoch 2 | Step 864900 | Avg Loss: 0.0147 | Grad Norm: 0.00837637\n",
      "Epoch 2 | Step 865000 | Avg Loss: 0.0149 | Grad Norm: 0.00889753\n",
      "Epoch 2 | Step 865100 | Avg Loss: 0.0153 | Grad Norm: 0.00960816\n",
      "Epoch 2 | Step 865200 | Avg Loss: 0.0151 | Grad Norm: 0.00839574\n",
      "Epoch 2 | Step 865300 | Avg Loss: 0.0148 | Grad Norm: 0.00866478\n",
      "Epoch 2 | Step 865400 | Avg Loss: 0.0144 | Grad Norm: 0.00864828\n",
      "Epoch 2 | Step 865500 | Avg Loss: 0.0147 | Grad Norm: 0.00941052\n",
      "Epoch 2 | Step 865600 | Avg Loss: 0.0151 | Grad Norm: 0.00786313\n",
      "Epoch 2 | Step 865700 | Avg Loss: 0.0152 | Grad Norm: 0.00834748\n",
      "Epoch 2 | Step 865800 | Avg Loss: 0.0149 | Grad Norm: 0.00848790\n",
      "Epoch 2 | Step 865900 | Avg Loss: 0.0147 | Grad Norm: 0.00782471\n",
      "Epoch 2 | Step 866000 | Avg Loss: 0.0149 | Grad Norm: 0.00949345\n",
      "Epoch 2 | Step 866100 | Avg Loss: 0.0151 | Grad Norm: 0.01081560\n",
      "Epoch 2 | Step 866200 | Avg Loss: 0.0146 | Grad Norm: 0.00980289\n",
      "Epoch 2 | Step 866300 | Avg Loss: 0.0144 | Grad Norm: 0.00822025\n",
      "Epoch 2 | Step 866400 | Avg Loss: 0.0147 | Grad Norm: 0.00825749\n",
      "Epoch 2 | Step 866500 | Avg Loss: 0.0148 | Grad Norm: 0.00983058\n",
      "Epoch 2 | Step 866600 | Avg Loss: 0.0144 | Grad Norm: 0.00858149\n",
      "Epoch 2 | Step 866700 | Avg Loss: 0.0143 | Grad Norm: 0.00848470\n",
      "Epoch 2 | Step 866800 | Avg Loss: 0.0144 | Grad Norm: 0.00807937\n",
      "Epoch 2 | Step 866900 | Avg Loss: 0.0142 | Grad Norm: 0.00733571\n",
      "Epoch 2 | Step 867000 | Avg Loss: 0.0144 | Grad Norm: 0.00886730\n",
      "Epoch 2 | Step 867100 | Avg Loss: 0.0147 | Grad Norm: 0.00856013\n",
      "Epoch 2 | Step 867200 | Avg Loss: 0.0149 | Grad Norm: 0.00869125\n",
      "Epoch 2 | Step 867300 | Avg Loss: 0.0149 | Grad Norm: 0.01030534\n",
      "Epoch 2 | Step 867400 | Avg Loss: 0.0150 | Grad Norm: 0.00819312\n",
      "Epoch 2 | Step 867500 | Avg Loss: 0.0149 | Grad Norm: 0.00850331\n",
      "Epoch 2 | Step 867600 | Avg Loss: 0.0149 | Grad Norm: 0.00969667\n",
      "Epoch 2 | Step 867700 | Avg Loss: 0.0148 | Grad Norm: 0.01110627\n",
      "Epoch 2 | Step 867800 | Avg Loss: 0.0151 | Grad Norm: 0.00841441\n",
      "Epoch 2 | Step 867900 | Avg Loss: 0.0147 | Grad Norm: 0.00781844\n",
      "Epoch 2 | Step 868000 | Avg Loss: 0.0149 | Grad Norm: 0.01126326\n",
      "Epoch 2 | Step 868100 | Avg Loss: 0.0148 | Grad Norm: 0.00826053\n",
      "Epoch 2 | Step 868200 | Avg Loss: 0.0150 | Grad Norm: 0.00853587\n",
      "Epoch 2 | Step 868300 | Avg Loss: 0.0150 | Grad Norm: 0.00800304\n",
      "Epoch 2 | Step 868400 | Avg Loss: 0.0149 | Grad Norm: 0.00834939\n",
      "Epoch 2 | Step 868500 | Avg Loss: 0.0153 | Grad Norm: 0.00669969\n",
      "Epoch 2 | Step 868600 | Avg Loss: 0.0150 | Grad Norm: 0.01006055\n",
      "Epoch 2 | Step 868700 | Avg Loss: 0.0154 | Grad Norm: 0.00855944\n",
      "Epoch 2 | Step 868800 | Avg Loss: 0.0154 | Grad Norm: 0.00890760\n",
      "Epoch 2 | Step 868900 | Avg Loss: 0.0156 | Grad Norm: 0.00829704\n",
      "Epoch 2 | Step 869000 | Avg Loss: 0.0155 | Grad Norm: 0.01219514\n",
      "Epoch 2 | Step 869100 | Avg Loss: 0.0152 | Grad Norm: 0.00879458\n",
      "Epoch 2 | Step 869200 | Avg Loss: 0.0151 | Grad Norm: 0.00841834\n",
      "Epoch 2 | Step 869300 | Avg Loss: 0.0153 | Grad Norm: 0.00858369\n",
      "Epoch 2 | Step 869400 | Avg Loss: 0.0150 | Grad Norm: 0.00744061\n",
      "Epoch 2 | Step 869500 | Avg Loss: 0.0150 | Grad Norm: 0.00960924\n",
      "Epoch 2 | Step 869600 | Avg Loss: 0.0150 | Grad Norm: 0.00766146\n",
      "Epoch 2 | Step 869700 | Avg Loss: 0.0150 | Grad Norm: 0.00897366\n",
      "Epoch 2 | Step 869800 | Avg Loss: 0.0147 | Grad Norm: 0.00756065\n",
      "Epoch 2 | Step 869900 | Avg Loss: 0.0145 | Grad Norm: 0.00790788\n",
      "Epoch 2 | Step 870000 | Avg Loss: 0.0150 | Grad Norm: 0.00764247\n",
      "Epoch 2 | Step 870100 | Avg Loss: 0.0148 | Grad Norm: 0.00813009\n",
      "Epoch 2 | Step 870200 | Avg Loss: 0.0150 | Grad Norm: 0.00769034\n",
      "Epoch 2 | Step 870300 | Avg Loss: 0.0148 | Grad Norm: 0.00904312\n",
      "Epoch 2 | Step 870400 | Avg Loss: 0.0150 | Grad Norm: 0.00718120\n",
      "Epoch 2 | Step 870500 | Avg Loss: 0.0153 | Grad Norm: 0.00774998\n",
      "Epoch 2 | Step 870600 | Avg Loss: 0.0152 | Grad Norm: 0.00820694\n",
      "Epoch 2 | Step 870700 | Avg Loss: 0.0154 | Grad Norm: 0.00777701\n",
      "Epoch 2 | Step 870800 | Avg Loss: 0.0154 | Grad Norm: 0.00775810\n",
      "Epoch 2 | Step 870900 | Avg Loss: 0.0145 | Grad Norm: 0.00714037\n",
      "Epoch 2 | Step 871000 | Avg Loss: 0.0142 | Grad Norm: 0.00755094\n",
      "Epoch 2 | Step 871100 | Avg Loss: 0.0146 | Grad Norm: 0.00817400\n",
      "Epoch 2 | Step 871200 | Avg Loss: 0.0147 | Grad Norm: 0.00799409\n",
      "Epoch 2 | Step 871300 | Avg Loss: 0.0151 | Grad Norm: 0.00822310\n",
      "Epoch 2 | Step 871400 | Avg Loss: 0.0152 | Grad Norm: 0.00809762\n",
      "Epoch 2 | Step 871500 | Avg Loss: 0.0154 | Grad Norm: 0.00702149\n",
      "Epoch 2 | Step 871600 | Avg Loss: 0.0151 | Grad Norm: 0.01018325\n",
      "Epoch 2 | Step 871700 | Avg Loss: 0.0154 | Grad Norm: 0.00802230\n",
      "Epoch 2 | Step 871800 | Avg Loss: 0.0151 | Grad Norm: 0.00901984\n",
      "Epoch 2 | Step 871900 | Avg Loss: 0.0152 | Grad Norm: 0.01030574\n",
      "Epoch 2 | Step 872000 | Avg Loss: 0.0150 | Grad Norm: 0.00914564\n",
      "Epoch 2 | Step 872100 | Avg Loss: 0.0145 | Grad Norm: 0.00901300\n",
      "Epoch 2 | Step 872200 | Avg Loss: 0.0144 | Grad Norm: 0.00779874\n",
      "Epoch 2 | Step 872300 | Avg Loss: 0.0143 | Grad Norm: 0.00859902\n",
      "Epoch 2 | Step 872400 | Avg Loss: 0.0146 | Grad Norm: 0.00958818\n",
      "Epoch 2 | Step 872500 | Avg Loss: 0.0151 | Grad Norm: 0.00930826\n",
      "Epoch 2 | Step 872600 | Avg Loss: 0.0152 | Grad Norm: 0.00997576\n",
      "Epoch 2 | Step 872700 | Avg Loss: 0.0148 | Grad Norm: 0.00995719\n",
      "Epoch 2 | Step 872800 | Avg Loss: 0.0146 | Grad Norm: 0.00832622\n",
      "Epoch 2 | Step 872900 | Avg Loss: 0.0149 | Grad Norm: 0.00813202\n",
      "Epoch 2 | Step 873000 | Avg Loss: 0.0148 | Grad Norm: 0.00992049\n",
      "Epoch 2 | Step 873100 | Avg Loss: 0.0147 | Grad Norm: 0.00981409\n",
      "Epoch 2 | Step 873200 | Avg Loss: 0.0150 | Grad Norm: 0.00867853\n",
      "Epoch 2 | Step 873300 | Avg Loss: 0.0148 | Grad Norm: 0.01159853\n",
      "Epoch 2 | Step 873400 | Avg Loss: 0.0150 | Grad Norm: 0.00998680\n",
      "Epoch 2 | Step 873500 | Avg Loss: 0.0149 | Grad Norm: 0.00933177\n",
      "Epoch 2 | Step 873600 | Avg Loss: 0.0151 | Grad Norm: 0.00782395\n",
      "Epoch 2 | Step 873700 | Avg Loss: 0.0149 | Grad Norm: 0.00820348\n",
      "Epoch 2 | Step 873800 | Avg Loss: 0.0149 | Grad Norm: 0.00981115\n",
      "Epoch 2 | Step 873900 | Avg Loss: 0.0148 | Grad Norm: 0.00821334\n",
      "Epoch 2 | Step 874000 | Avg Loss: 0.0150 | Grad Norm: 0.00812654\n",
      "Epoch 2 | Step 874100 | Avg Loss: 0.0146 | Grad Norm: 0.00753152\n",
      "Epoch 2 | Step 874200 | Avg Loss: 0.0145 | Grad Norm: 0.00703450\n",
      "Epoch 2 | Step 874300 | Avg Loss: 0.0148 | Grad Norm: 0.00918479\n",
      "Epoch 2 | Step 874400 | Avg Loss: 0.0145 | Grad Norm: 0.00934848\n",
      "Epoch 2 | Step 874500 | Avg Loss: 0.0143 | Grad Norm: 0.00740303\n",
      "Epoch 2 | Step 874600 | Avg Loss: 0.0144 | Grad Norm: 0.00861354\n",
      "Epoch 2 | Step 874700 | Avg Loss: 0.0145 | Grad Norm: 0.00986032\n",
      "Epoch 2 | Step 874800 | Avg Loss: 0.0148 | Grad Norm: 0.00849926\n",
      "Epoch 2 | Step 874900 | Avg Loss: 0.0146 | Grad Norm: 0.00902038\n",
      "Epoch 2 | Step 875000 | Avg Loss: 0.0146 | Grad Norm: 0.00843085\n",
      "Epoch 2 | Step 875100 | Avg Loss: 0.0151 | Grad Norm: 0.00917447\n",
      "Epoch 2 | Step 875200 | Avg Loss: 0.0151 | Grad Norm: 0.00888476\n",
      "Epoch 2 | Step 875300 | Avg Loss: 0.0149 | Grad Norm: 0.00883049\n",
      "Epoch 2 | Step 875400 | Avg Loss: 0.0151 | Grad Norm: 0.00913596\n",
      "Epoch 2 | Step 875500 | Avg Loss: 0.0148 | Grad Norm: 0.00855620\n",
      "Epoch 2 | Step 875600 | Avg Loss: 0.0147 | Grad Norm: 0.00912612\n",
      "Epoch 2 | Step 875700 | Avg Loss: 0.0148 | Grad Norm: 0.00883575\n",
      "Epoch 2 | Step 875800 | Avg Loss: 0.0148 | Grad Norm: 0.00898680\n",
      "Epoch 2 | Step 875900 | Avg Loss: 0.0147 | Grad Norm: 0.00918604\n",
      "Epoch 2 | Step 876000 | Avg Loss: 0.0148 | Grad Norm: 0.00998429\n",
      "Epoch 2 | Step 876100 | Avg Loss: 0.0147 | Grad Norm: 0.00806104\n",
      "Epoch 2 | Step 876200 | Avg Loss: 0.0149 | Grad Norm: 0.00908002\n",
      "Epoch 2 | Step 876300 | Avg Loss: 0.0149 | Grad Norm: 0.00781713\n",
      "Epoch 2 | Step 876400 | Avg Loss: 0.0150 | Grad Norm: 0.00858974\n",
      "Epoch 2 | Step 876500 | Avg Loss: 0.0151 | Grad Norm: 0.00845634\n",
      "Epoch 2 | Step 876600 | Avg Loss: 0.0149 | Grad Norm: 0.00933875\n",
      "Epoch 2 | Step 876700 | Avg Loss: 0.0151 | Grad Norm: 0.00938604\n",
      "Epoch 2 | Step 876800 | Avg Loss: 0.0147 | Grad Norm: 0.00848179\n",
      "Epoch 2 | Step 876900 | Avg Loss: 0.0146 | Grad Norm: 0.00883711\n",
      "Epoch 2 | Step 877000 | Avg Loss: 0.0145 | Grad Norm: 0.00761174\n",
      "Epoch 2 | Step 877100 | Avg Loss: 0.0145 | Grad Norm: 0.00801870\n",
      "Epoch 2 | Step 877200 | Avg Loss: 0.0142 | Grad Norm: 0.00793721\n",
      "Epoch 2 | Step 877300 | Avg Loss: 0.0141 | Grad Norm: 0.00874858\n",
      "Epoch 2 | Step 877400 | Avg Loss: 0.0139 | Grad Norm: 0.00814119\n",
      "Epoch 2 | Step 877500 | Avg Loss: 0.0141 | Grad Norm: 0.00825523\n",
      "Epoch 2 | Step 877600 | Avg Loss: 0.0143 | Grad Norm: 0.00888189\n",
      "Epoch 2 | Step 877700 | Avg Loss: 0.0146 | Grad Norm: 0.00856257\n",
      "Epoch 2 | Step 877800 | Avg Loss: 0.0146 | Grad Norm: 0.00782573\n",
      "Epoch 2 | Step 877900 | Avg Loss: 0.0150 | Grad Norm: 0.00837787\n",
      "Epoch 2 | Step 878000 | Avg Loss: 0.0150 | Grad Norm: 0.00983138\n",
      "Epoch 2 | Step 878100 | Avg Loss: 0.0151 | Grad Norm: 0.00865604\n",
      "Epoch 2 | Step 878200 | Avg Loss: 0.0150 | Grad Norm: 0.00841198\n",
      "Epoch 2 | Step 878300 | Avg Loss: 0.0150 | Grad Norm: 0.00814884\n",
      "Epoch 2 | Step 878400 | Avg Loss: 0.0148 | Grad Norm: 0.01068572\n",
      "Epoch 2 | Step 878500 | Avg Loss: 0.0154 | Grad Norm: 0.00744602\n",
      "Epoch 2 | Step 878600 | Avg Loss: 0.0153 | Grad Norm: 0.00842468\n",
      "Epoch 2 | Step 878700 | Avg Loss: 0.0153 | Grad Norm: 0.00911264\n",
      "Epoch 2 | Step 878800 | Avg Loss: 0.0153 | Grad Norm: 0.01124250\n",
      "Epoch 2 | Step 878900 | Avg Loss: 0.0153 | Grad Norm: 0.00954018\n",
      "Epoch 2 | Step 879000 | Avg Loss: 0.0154 | Grad Norm: 0.00851092\n",
      "Epoch 2 | Step 879100 | Avg Loss: 0.0157 | Grad Norm: 0.01057681\n",
      "Epoch 2 | Step 879200 | Avg Loss: 0.0161 | Grad Norm: 0.00879392\n",
      "Epoch 2 | Step 879300 | Avg Loss: 0.0153 | Grad Norm: 0.01041250\n",
      "Epoch 2 | Step 879400 | Avg Loss: 0.0153 | Grad Norm: 0.00794474\n",
      "Epoch 2 | Step 879500 | Avg Loss: 0.0152 | Grad Norm: 0.00922602\n",
      "Epoch 2 | Step 879600 | Avg Loss: 0.0151 | Grad Norm: 0.00860359\n",
      "Epoch 2 | Step 879700 | Avg Loss: 0.0150 | Grad Norm: 0.00850467\n",
      "Epoch 2 | Step 879800 | Avg Loss: 0.0148 | Grad Norm: 0.00820307\n",
      "Epoch 2 | Step 879900 | Avg Loss: 0.0150 | Grad Norm: 0.00861609\n",
      "Epoch 2 | Step 880000 | Avg Loss: 0.0151 | Grad Norm: 0.00768261\n",
      "Epoch 2 | Step 880100 | Avg Loss: 0.0151 | Grad Norm: 0.00877180\n",
      "Epoch 2 | Step 880200 | Avg Loss: 0.0152 | Grad Norm: 0.00838355\n",
      "Epoch 2 | Step 880300 | Avg Loss: 0.0152 | Grad Norm: 0.00925776\n",
      "Epoch 2 | Step 880400 | Avg Loss: 0.0148 | Grad Norm: 0.00848372\n",
      "Epoch 2 | Step 880500 | Avg Loss: 0.0149 | Grad Norm: 0.00713477\n",
      "Epoch 2 | Step 880600 | Avg Loss: 0.0145 | Grad Norm: 0.00838982\n",
      "Epoch 2 | Step 880700 | Avg Loss: 0.0142 | Grad Norm: 0.00694242\n",
      "Epoch 2 | Step 880800 | Avg Loss: 0.0143 | Grad Norm: 0.00740770\n",
      "Epoch 2 | Step 880900 | Avg Loss: 0.0145 | Grad Norm: 0.00761347\n",
      "Epoch 2 | Step 881000 | Avg Loss: 0.0144 | Grad Norm: 0.00835387\n",
      "Epoch 2 | Step 881100 | Avg Loss: 0.0146 | Grad Norm: 0.00823112\n",
      "Epoch 2 | Step 881200 | Avg Loss: 0.0147 | Grad Norm: 0.00762023\n",
      "Epoch 2 | Step 881300 | Avg Loss: 0.0146 | Grad Norm: 0.00902971\n",
      "Epoch 2 | Step 881400 | Avg Loss: 0.0144 | Grad Norm: 0.00875698\n",
      "Epoch 2 | Step 881500 | Avg Loss: 0.0144 | Grad Norm: 0.00852989\n",
      "Epoch 2 | Step 881600 | Avg Loss: 0.0147 | Grad Norm: 0.00910498\n",
      "Epoch 2 | Step 881700 | Avg Loss: 0.0149 | Grad Norm: 0.00744491\n",
      "Epoch 2 | Step 881800 | Avg Loss: 0.0150 | Grad Norm: 0.00870317\n",
      "Epoch 2 | Step 881900 | Avg Loss: 0.0155 | Grad Norm: 0.00900564\n",
      "Epoch 2 | Step 882000 | Avg Loss: 0.0155 | Grad Norm: 0.00959133\n",
      "Epoch 2 | Step 882100 | Avg Loss: 0.0156 | Grad Norm: 0.00869941\n",
      "Epoch 2 | Step 882200 | Avg Loss: 0.0157 | Grad Norm: 0.01023047\n",
      "Epoch 2 | Step 882300 | Avg Loss: 0.0153 | Grad Norm: 0.00817762\n",
      "Epoch 2 | Step 882400 | Avg Loss: 0.0151 | Grad Norm: 0.00856644\n",
      "Epoch 2 | Step 882500 | Avg Loss: 0.0150 | Grad Norm: 0.00838545\n",
      "Epoch 2 | Step 882600 | Avg Loss: 0.0151 | Grad Norm: 0.00868023\n",
      "Epoch 2 | Step 882700 | Avg Loss: 0.0148 | Grad Norm: 0.00849115\n",
      "Epoch 2 | Step 882800 | Avg Loss: 0.0143 | Grad Norm: 0.00711706\n",
      "Epoch 2 | Step 882900 | Avg Loss: 0.0143 | Grad Norm: 0.01078942\n",
      "Epoch 2 | Step 883000 | Avg Loss: 0.0141 | Grad Norm: 0.00739140\n",
      "Epoch 2 | Step 883100 | Avg Loss: 0.0141 | Grad Norm: 0.00852168\n",
      "Epoch 2 | Step 883200 | Avg Loss: 0.0144 | Grad Norm: 0.00898428\n",
      "Epoch 2 | Step 883300 | Avg Loss: 0.0144 | Grad Norm: 0.00894228\n",
      "Epoch 2 | Step 883400 | Avg Loss: 0.0143 | Grad Norm: 0.00662840\n",
      "Epoch 2 | Step 883500 | Avg Loss: 0.0141 | Grad Norm: 0.00863842\n",
      "Epoch 2 | Step 883600 | Avg Loss: 0.0144 | Grad Norm: 0.00853690\n",
      "Epoch 2 | Step 883700 | Avg Loss: 0.0145 | Grad Norm: 0.00936724\n",
      "Epoch 2 | Step 883800 | Avg Loss: 0.0151 | Grad Norm: 0.00903105\n",
      "Epoch 2 | Step 883900 | Avg Loss: 0.0151 | Grad Norm: 0.00857436\n",
      "Epoch 2 | Step 884000 | Avg Loss: 0.0149 | Grad Norm: 0.00941013\n",
      "Epoch 2 | Step 884100 | Avg Loss: 0.0147 | Grad Norm: 0.00850896\n",
      "Epoch 2 | Step 884200 | Avg Loss: 0.0149 | Grad Norm: 0.00843073\n",
      "Epoch 2 | Step 884300 | Avg Loss: 0.0147 | Grad Norm: 0.00856040\n",
      "Epoch 2 | Step 884400 | Avg Loss: 0.0147 | Grad Norm: 0.00821827\n",
      "Epoch 2 | Step 884500 | Avg Loss: 0.0141 | Grad Norm: 0.00749360\n",
      "Epoch 2 | Step 884600 | Avg Loss: 0.0142 | Grad Norm: 0.00886391\n",
      "Epoch 2 | Step 884700 | Avg Loss: 0.0147 | Grad Norm: 0.00826532\n",
      "Epoch 2 | Step 884800 | Avg Loss: 0.0151 | Grad Norm: 0.00884745\n",
      "Epoch 2 | Step 884900 | Avg Loss: 0.0148 | Grad Norm: 0.00896407\n",
      "Epoch 2 | Step 885000 | Avg Loss: 0.0147 | Grad Norm: 0.00979691\n",
      "Epoch 2 | Step 885100 | Avg Loss: 0.0144 | Grad Norm: 0.00844729\n",
      "Epoch 2 | Step 885200 | Avg Loss: 0.0147 | Grad Norm: 0.00949978\n",
      "Epoch 2 | Step 885300 | Avg Loss: 0.0147 | Grad Norm: 0.00862451\n",
      "Epoch 2 | Step 885400 | Avg Loss: 0.0148 | Grad Norm: 0.00779422\n",
      "Epoch 2 | Step 885500 | Avg Loss: 0.0146 | Grad Norm: 0.00807865\n",
      "Epoch 2 | Step 885600 | Avg Loss: 0.0148 | Grad Norm: 0.00825096\n",
      "Epoch 2 | Step 885700 | Avg Loss: 0.0153 | Grad Norm: 0.00986455\n",
      "Epoch 2 | Step 885800 | Avg Loss: 0.0149 | Grad Norm: 0.00986104\n",
      "Epoch 2 | Step 885900 | Avg Loss: 0.0149 | Grad Norm: 0.00864512\n",
      "Epoch 2 | Step 886000 | Avg Loss: 0.0147 | Grad Norm: 0.00947230\n",
      "Epoch 2 | Step 886100 | Avg Loss: 0.0149 | Grad Norm: 0.00962464\n",
      "Epoch 2 | Step 886200 | Avg Loss: 0.0144 | Grad Norm: 0.00884634\n",
      "Epoch 2 | Step 886300 | Avg Loss: 0.0149 | Grad Norm: 0.00838490\n",
      "Epoch 2 | Step 886400 | Avg Loss: 0.0149 | Grad Norm: 0.00908455\n",
      "Epoch 2 | Step 886500 | Avg Loss: 0.0152 | Grad Norm: 0.01036578\n",
      "Epoch 2 | Step 886600 | Avg Loss: 0.0150 | Grad Norm: 0.00884148\n",
      "Epoch 2 | Step 886700 | Avg Loss: 0.0147 | Grad Norm: 0.01009852\n",
      "Epoch 2 | Step 886800 | Avg Loss: 0.0140 | Grad Norm: 0.00872864\n",
      "Epoch 2 | Step 886900 | Avg Loss: 0.0146 | Grad Norm: 0.00831194\n",
      "Epoch 2 | Step 887000 | Avg Loss: 0.0147 | Grad Norm: 0.00897227\n",
      "Epoch 2 | Step 887100 | Avg Loss: 0.0147 | Grad Norm: 0.00982679\n",
      "Epoch 2 | Step 887200 | Avg Loss: 0.0148 | Grad Norm: 0.00756957\n",
      "Epoch 2 | Step 887300 | Avg Loss: 0.0151 | Grad Norm: 0.01019937\n",
      "Epoch 2 | Step 887400 | Avg Loss: 0.0152 | Grad Norm: 0.00909107\n",
      "Epoch 2 | Step 887500 | Avg Loss: 0.0149 | Grad Norm: 0.00823422\n",
      "Epoch 2 | Step 887600 | Avg Loss: 0.0148 | Grad Norm: 0.00786132\n",
      "Epoch 2 | Step 887700 | Avg Loss: 0.0147 | Grad Norm: 0.00965383\n",
      "Epoch 2 | Step 887800 | Avg Loss: 0.0145 | Grad Norm: 0.00948473\n",
      "Epoch 2 | Step 887900 | Avg Loss: 0.0141 | Grad Norm: 0.00778275\n",
      "Epoch 2 | Step 888000 | Avg Loss: 0.0143 | Grad Norm: 0.00823562\n",
      "Epoch 2 | Step 888100 | Avg Loss: 0.0146 | Grad Norm: 0.00855577\n",
      "Epoch 2 | Step 888200 | Avg Loss: 0.0146 | Grad Norm: 0.00815521\n",
      "Epoch 2 | Step 888300 | Avg Loss: 0.0148 | Grad Norm: 0.00845812\n",
      "Epoch 2 | Step 888400 | Avg Loss: 0.0150 | Grad Norm: 0.01028055\n",
      "Epoch 2 | Step 888500 | Avg Loss: 0.0159 | Grad Norm: 0.00880832\n",
      "Epoch 2 | Step 888600 | Avg Loss: 0.0158 | Grad Norm: 0.00680906\n",
      "Epoch 2 | Step 888700 | Avg Loss: 0.0156 | Grad Norm: 0.00726528\n",
      "Epoch 2 | Step 888800 | Avg Loss: 0.0157 | Grad Norm: 0.00942833\n",
      "Epoch 2 | Step 888900 | Avg Loss: 0.0155 | Grad Norm: 0.00879065\n",
      "Epoch 2 | Step 889000 | Avg Loss: 0.0155 | Grad Norm: 0.00854174\n",
      "Epoch 2 | Step 889100 | Avg Loss: 0.0154 | Grad Norm: 0.00760317\n",
      "Epoch 2 | Step 889200 | Avg Loss: 0.0153 | Grad Norm: 0.00826870\n",
      "Epoch 2 | Step 889300 | Avg Loss: 0.0155 | Grad Norm: 0.00952302\n",
      "Epoch 2 | Step 889400 | Avg Loss: 0.0155 | Grad Norm: 0.01040739\n",
      "Epoch 2 | Step 889500 | Avg Loss: 0.0152 | Grad Norm: 0.00812732\n",
      "Epoch 2 | Step 889600 | Avg Loss: 0.0152 | Grad Norm: 0.00815688\n",
      "Epoch 2 | Step 889700 | Avg Loss: 0.0152 | Grad Norm: 0.00838860\n",
      "Epoch 2 | Step 889800 | Avg Loss: 0.0148 | Grad Norm: 0.00762318\n",
      "Epoch 2 | Step 889900 | Avg Loss: 0.0149 | Grad Norm: 0.00901762\n",
      "Epoch 2 | Step 890000 | Avg Loss: 0.0149 | Grad Norm: 0.00930146\n",
      "Epoch 2 | Step 890100 | Avg Loss: 0.0153 | Grad Norm: 0.00859198\n",
      "Epoch 2 | Step 890200 | Avg Loss: 0.0153 | Grad Norm: 0.00882112\n",
      "Epoch 2 | Step 890300 | Avg Loss: 0.0147 | Grad Norm: 0.00828648\n",
      "Epoch 2 | Step 890400 | Avg Loss: 0.0147 | Grad Norm: 0.00773214\n",
      "Epoch 2 | Step 890500 | Avg Loss: 0.0149 | Grad Norm: 0.00853436\n",
      "Epoch 2 | Step 890600 | Avg Loss: 0.0152 | Grad Norm: 0.00731715\n",
      "Epoch 2 | Step 890700 | Avg Loss: 0.0151 | Grad Norm: 0.00809296\n",
      "Epoch 2 | Step 890800 | Avg Loss: 0.0152 | Grad Norm: 0.00786083\n",
      "Epoch 2 | Step 890900 | Avg Loss: 0.0149 | Grad Norm: 0.00730059\n",
      "Epoch 2 | Step 891000 | Avg Loss: 0.0148 | Grad Norm: 0.00801312\n",
      "Epoch 2 | Step 891100 | Avg Loss: 0.0147 | Grad Norm: 0.01003458\n",
      "Epoch 2 | Step 891200 | Avg Loss: 0.0147 | Grad Norm: 0.00848516\n",
      "Epoch 2 | Step 891300 | Avg Loss: 0.0147 | Grad Norm: 0.00805847\n",
      "Epoch 2 | Step 891400 | Avg Loss: 0.0145 | Grad Norm: 0.00841009\n",
      "Epoch 2 | Step 891500 | Avg Loss: 0.0142 | Grad Norm: 0.00779338\n",
      "Epoch 2 | Step 891600 | Avg Loss: 0.0144 | Grad Norm: 0.00871869\n",
      "Epoch 2 | Step 891700 | Avg Loss: 0.0144 | Grad Norm: 0.00861382\n",
      "Epoch 2 | Step 891800 | Avg Loss: 0.0146 | Grad Norm: 0.00803595\n",
      "Epoch 2 | Step 891900 | Avg Loss: 0.0150 | Grad Norm: 0.00845661\n",
      "Epoch 2 | Step 892000 | Avg Loss: 0.0151 | Grad Norm: 0.00809302\n",
      "Epoch 2 | Step 892100 | Avg Loss: 0.0149 | Grad Norm: 0.00999612\n",
      "Epoch 2 | Step 892200 | Avg Loss: 0.0150 | Grad Norm: 0.00945970\n",
      "Epoch 2 | Step 892300 | Avg Loss: 0.0150 | Grad Norm: 0.00858333\n",
      "Epoch 2 | Step 892400 | Avg Loss: 0.0149 | Grad Norm: 0.00814635\n",
      "Epoch 2 | Step 892500 | Avg Loss: 0.0149 | Grad Norm: 0.00718851\n",
      "Epoch 2 | Step 892600 | Avg Loss: 0.0150 | Grad Norm: 0.01106921\n",
      "Epoch 2 | Step 892700 | Avg Loss: 0.0151 | Grad Norm: 0.00840052\n",
      "Epoch 2 | Step 892800 | Avg Loss: 0.0148 | Grad Norm: 0.00746432\n",
      "Epoch 2 | Step 892900 | Avg Loss: 0.0151 | Grad Norm: 0.00965864\n",
      "Epoch 2 | Step 893000 | Avg Loss: 0.0147 | Grad Norm: 0.00772094\n",
      "Epoch 2 | Step 893100 | Avg Loss: 0.0148 | Grad Norm: 0.00812552\n",
      "Epoch 2 | Step 893200 | Avg Loss: 0.0148 | Grad Norm: 0.00765684\n",
      "Epoch 2 | Step 893300 | Avg Loss: 0.0148 | Grad Norm: 0.00851583\n",
      "Epoch 2 | Step 893400 | Avg Loss: 0.0150 | Grad Norm: 0.00866952\n",
      "Epoch 2 | Step 893500 | Avg Loss: 0.0149 | Grad Norm: 0.00944107\n",
      "Epoch 2 | Step 893600 | Avg Loss: 0.0149 | Grad Norm: 0.00937832\n",
      "Epoch 2 | Step 893700 | Avg Loss: 0.0146 | Grad Norm: 0.00834017\n",
      "Epoch 2 | Step 893800 | Avg Loss: 0.0149 | Grad Norm: 0.00909193\n",
      "Epoch 2 | Step 893900 | Avg Loss: 0.0149 | Grad Norm: 0.00923471\n",
      "Epoch 2 | Step 894000 | Avg Loss: 0.0150 | Grad Norm: 0.00894424\n",
      "Epoch 2 | Step 894100 | Avg Loss: 0.0150 | Grad Norm: 0.01001566\n",
      "Epoch 2 | Step 894200 | Avg Loss: 0.0152 | Grad Norm: 0.00842201\n",
      "Epoch 2 | Step 894300 | Avg Loss: 0.0155 | Grad Norm: 0.01201513\n",
      "Epoch 2 | Step 894400 | Avg Loss: 0.0154 | Grad Norm: 0.00846660\n",
      "Epoch 2 | Step 894500 | Avg Loss: 0.0154 | Grad Norm: 0.00895944\n",
      "Epoch 2 | Step 894600 | Avg Loss: 0.0151 | Grad Norm: 0.00926210\n",
      "Epoch 2 | Step 894700 | Avg Loss: 0.0149 | Grad Norm: 0.00789066\n",
      "Epoch 2 | Step 894800 | Avg Loss: 0.0146 | Grad Norm: 0.00979187\n",
      "Epoch 2 | Step 894900 | Avg Loss: 0.0145 | Grad Norm: 0.00936196\n",
      "Epoch 2 | Step 895000 | Avg Loss: 0.0147 | Grad Norm: 0.01028319\n",
      "Epoch 2 | Step 895100 | Avg Loss: 0.0151 | Grad Norm: 0.00868669\n",
      "Epoch 2 | Step 895200 | Avg Loss: 0.0151 | Grad Norm: 0.00849297\n",
      "Epoch 2 | Step 895300 | Avg Loss: 0.0149 | Grad Norm: 0.00941029\n",
      "Epoch 2 | Step 895400 | Avg Loss: 0.0151 | Grad Norm: 0.00864696\n",
      "Epoch 2 | Step 895500 | Avg Loss: 0.0150 | Grad Norm: 0.00851715\n",
      "Epoch 2 | Step 895600 | Avg Loss: 0.0151 | Grad Norm: 0.00804846\n",
      "Epoch 2 | Step 895700 | Avg Loss: 0.0153 | Grad Norm: 0.00893661\n",
      "Epoch 2 | Step 895800 | Avg Loss: 0.0152 | Grad Norm: 0.00971418\n",
      "Epoch 2 | Step 895900 | Avg Loss: 0.0149 | Grad Norm: 0.00878416\n",
      "Epoch 2 | Step 896000 | Avg Loss: 0.0144 | Grad Norm: 0.00791999\n",
      "Epoch 2 | Step 896100 | Avg Loss: 0.0145 | Grad Norm: 0.00814391\n",
      "Epoch 2 | Step 896200 | Avg Loss: 0.0151 | Grad Norm: 0.00764917\n",
      "Epoch 2 | Step 896300 | Avg Loss: 0.0151 | Grad Norm: 0.00800898\n",
      "Epoch 2 | Step 896400 | Avg Loss: 0.0149 | Grad Norm: 0.00791660\n",
      "Epoch 2 | Step 896500 | Avg Loss: 0.0149 | Grad Norm: 0.00832071\n",
      "Epoch 2 | Step 896600 | Avg Loss: 0.0147 | Grad Norm: 0.00930418\n",
      "Epoch 2 | Step 896700 | Avg Loss: 0.0145 | Grad Norm: 0.00926414\n",
      "Epoch 2 | Step 896800 | Avg Loss: 0.0145 | Grad Norm: 0.00966334\n",
      "Epoch 2 | Step 896900 | Avg Loss: 0.0148 | Grad Norm: 0.00847870\n",
      "Epoch 2 | Step 897000 | Avg Loss: 0.0147 | Grad Norm: 0.00829793\n",
      "Epoch 2 | Step 897100 | Avg Loss: 0.0149 | Grad Norm: 0.00848419\n",
      "Epoch 2 | Step 897200 | Avg Loss: 0.0146 | Grad Norm: 0.00892495\n",
      "Epoch 2 | Step 897300 | Avg Loss: 0.0148 | Grad Norm: 0.00875252\n",
      "Epoch 2 | Step 897400 | Avg Loss: 0.0150 | Grad Norm: 0.00903385\n",
      "Epoch 2 | Step 897500 | Avg Loss: 0.0150 | Grad Norm: 0.00842741\n",
      "Epoch 2 | Step 897600 | Avg Loss: 0.0154 | Grad Norm: 0.00775791\n",
      "Epoch 2 | Step 897700 | Avg Loss: 0.0154 | Grad Norm: 0.00933084\n",
      "Epoch 2 | Step 897800 | Avg Loss: 0.0151 | Grad Norm: 0.00745824\n",
      "Epoch 2 | Step 897900 | Avg Loss: 0.0151 | Grad Norm: 0.00808967\n",
      "Epoch 2 | Step 898000 | Avg Loss: 0.0150 | Grad Norm: 0.00864499\n",
      "Epoch 2 | Step 898100 | Avg Loss: 0.0151 | Grad Norm: 0.00826443\n",
      "Epoch 2 | Step 898200 | Avg Loss: 0.0149 | Grad Norm: 0.00815494\n",
      "Epoch 2 | Step 898300 | Avg Loss: 0.0148 | Grad Norm: 0.00856233\n",
      "Epoch 2 | Step 898400 | Avg Loss: 0.0148 | Grad Norm: 0.00851543\n",
      "Epoch 2 | Step 898500 | Avg Loss: 0.0147 | Grad Norm: 0.00782890\n",
      "Epoch 2 | Step 898600 | Avg Loss: 0.0150 | Grad Norm: 0.00811009\n",
      "Epoch 2 | Step 898700 | Avg Loss: 0.0149 | Grad Norm: 0.01090739\n",
      "Epoch 2 | Step 898800 | Avg Loss: 0.0149 | Grad Norm: 0.00934195\n",
      "Epoch 2 | Step 898900 | Avg Loss: 0.0156 | Grad Norm: 0.01000205\n",
      "Epoch 2 | Step 899000 | Avg Loss: 0.0155 | Grad Norm: 0.00925303\n",
      "Epoch 2 | Step 899100 | Avg Loss: 0.0152 | Grad Norm: 0.00966601\n",
      "Epoch 2 | Step 899200 | Avg Loss: 0.0152 | Grad Norm: 0.00824059\n",
      "Epoch 2 | Step 899300 | Avg Loss: 0.0155 | Grad Norm: 0.00961188\n",
      "Epoch 2 | Step 899400 | Avg Loss: 0.0153 | Grad Norm: 0.00881686\n",
      "Epoch 2 | Step 899500 | Avg Loss: 0.0152 | Grad Norm: 0.00822812\n",
      "Epoch 2 | Step 899600 | Avg Loss: 0.0153 | Grad Norm: 0.00962094\n",
      "Epoch 2 | Step 899700 | Avg Loss: 0.0150 | Grad Norm: 0.01040708\n",
      "Epoch 2 | Step 899800 | Avg Loss: 0.0150 | Grad Norm: 0.00942919\n",
      "Epoch 2 | Step 899900 | Avg Loss: 0.0149 | Grad Norm: 0.00950426\n",
      "Epoch 2 | Step 900000 | Avg Loss: 0.0147 | Grad Norm: 0.00813908\n",
      "Saving model at step900000\n",
      "Epoch 2 | Step 900100 | Avg Loss: 0.0146 | Grad Norm: 0.01133183\n",
      "Epoch 2 | Step 900200 | Avg Loss: 0.0145 | Grad Norm: 0.00794321\n",
      "Epoch 2 | Step 900300 | Avg Loss: 0.0143 | Grad Norm: 0.00902511\n",
      "Epoch 2 | Step 900400 | Avg Loss: 0.0145 | Grad Norm: 0.00755055\n",
      "Epoch 2 | Step 900500 | Avg Loss: 0.0148 | Grad Norm: 0.00885559\n",
      "Epoch 2 | Step 900600 | Avg Loss: 0.0148 | Grad Norm: 0.00755233\n",
      "Epoch 2 | Step 900700 | Avg Loss: 0.0145 | Grad Norm: 0.00809920\n",
      "Epoch 2 | Step 900800 | Avg Loss: 0.0143 | Grad Norm: 0.00747297\n",
      "Epoch 2 | Step 900900 | Avg Loss: 0.0142 | Grad Norm: 0.00864479\n",
      "Epoch 2 | Step 901000 | Avg Loss: 0.0145 | Grad Norm: 0.00779268\n",
      "Epoch 2 | Step 901100 | Avg Loss: 0.0149 | Grad Norm: 0.00809791\n",
      "Epoch 2 | Step 901200 | Avg Loss: 0.0152 | Grad Norm: 0.00927956\n",
      "Epoch 2 | Step 901300 | Avg Loss: 0.0150 | Grad Norm: 0.00775070\n",
      "Epoch 2 | Step 901400 | Avg Loss: 0.0147 | Grad Norm: 0.00807790\n",
      "Epoch 2 | Step 901500 | Avg Loss: 0.0149 | Grad Norm: 0.00822110\n",
      "Epoch 2 | Step 901600 | Avg Loss: 0.0147 | Grad Norm: 0.00830767\n",
      "Epoch 2 | Step 901700 | Avg Loss: 0.0147 | Grad Norm: 0.01058668\n",
      "Epoch 2 | Step 901800 | Avg Loss: 0.0148 | Grad Norm: 0.00768718\n",
      "Epoch 2 | Step 901900 | Avg Loss: 0.0147 | Grad Norm: 0.00731896\n",
      "Epoch 2 | Step 902000 | Avg Loss: 0.0143 | Grad Norm: 0.00952968\n",
      "Epoch 2 | Step 902100 | Avg Loss: 0.0145 | Grad Norm: 0.00716996\n",
      "Epoch 2 | Step 902200 | Avg Loss: 0.0145 | Grad Norm: 0.00776334\n",
      "Epoch 2 | Step 902300 | Avg Loss: 0.0144 | Grad Norm: 0.00887797\n",
      "Epoch 2 | Step 902400 | Avg Loss: 0.0144 | Grad Norm: 0.01071688\n",
      "Epoch 2 | Step 902500 | Avg Loss: 0.0145 | Grad Norm: 0.00711846\n",
      "Epoch 2 | Step 902600 | Avg Loss: 0.0146 | Grad Norm: 0.00775993\n",
      "Epoch 2 | Step 902700 | Avg Loss: 0.0149 | Grad Norm: 0.00763162\n",
      "Epoch 2 | Step 902800 | Avg Loss: 0.0149 | Grad Norm: 0.00854201\n",
      "Epoch 2 | Step 902900 | Avg Loss: 0.0150 | Grad Norm: 0.00905675\n",
      "Epoch 2 | Step 903000 | Avg Loss: 0.0149 | Grad Norm: 0.00800995\n",
      "Epoch 2 | Step 903100 | Avg Loss: 0.0146 | Grad Norm: 0.00921582\n",
      "Epoch 2 | Step 903200 | Avg Loss: 0.0147 | Grad Norm: 0.00817174\n",
      "Epoch 2 | Step 903300 | Avg Loss: 0.0148 | Grad Norm: 0.00834340\n",
      "Epoch 2 | Step 903400 | Avg Loss: 0.0149 | Grad Norm: 0.01029753\n",
      "Epoch 2 | Step 903500 | Avg Loss: 0.0147 | Grad Norm: 0.00835897\n",
      "Epoch 2 | Step 903600 | Avg Loss: 0.0146 | Grad Norm: 0.00816932\n",
      "Epoch 2 | Step 903700 | Avg Loss: 0.0145 | Grad Norm: 0.00824174\n",
      "Epoch 2 | Step 903800 | Avg Loss: 0.0151 | Grad Norm: 0.00871901\n",
      "Epoch 2 | Step 903900 | Avg Loss: 0.0142 | Grad Norm: 0.00889944\n",
      "Epoch 2 | Step 904000 | Avg Loss: 0.0147 | Grad Norm: 0.00869043\n",
      "Epoch 2 | Step 904100 | Avg Loss: 0.0145 | Grad Norm: 0.00821779\n",
      "Epoch 2 | Step 904200 | Avg Loss: 0.0141 | Grad Norm: 0.00890314\n",
      "Epoch 2 | Step 904300 | Avg Loss: 0.0145 | Grad Norm: 0.00880467\n",
      "Epoch 2 | Step 904400 | Avg Loss: 0.0145 | Grad Norm: 0.00916601\n",
      "Epoch 2 | Step 904500 | Avg Loss: 0.0145 | Grad Norm: 0.00780127\n",
      "Epoch 2 | Step 904600 | Avg Loss: 0.0144 | Grad Norm: 0.00788699\n",
      "Epoch 2 | Step 904700 | Avg Loss: 0.0145 | Grad Norm: 0.00876976\n",
      "Epoch 2 | Step 904800 | Avg Loss: 0.0145 | Grad Norm: 0.00947735\n",
      "Epoch 2 | Step 904900 | Avg Loss: 0.0145 | Grad Norm: 0.00761552\n",
      "Epoch 2 | Step 905000 | Avg Loss: 0.0144 | Grad Norm: 0.00848839\n",
      "Epoch 2 | Step 905100 | Avg Loss: 0.0144 | Grad Norm: 0.00848821\n",
      "Epoch 2 | Step 905200 | Avg Loss: 0.0149 | Grad Norm: 0.01002382\n",
      "Epoch 2 | Step 905300 | Avg Loss: 0.0150 | Grad Norm: 0.00802171\n",
      "Epoch 2 | Step 905400 | Avg Loss: 0.0151 | Grad Norm: 0.00932387\n",
      "Epoch 2 | Step 905500 | Avg Loss: 0.0149 | Grad Norm: 0.00795648\n",
      "Epoch 2 | Step 905600 | Avg Loss: 0.0149 | Grad Norm: 0.00890864\n",
      "Epoch 2 | Step 905700 | Avg Loss: 0.0147 | Grad Norm: 0.01155221\n",
      "Epoch 2 | Step 905800 | Avg Loss: 0.0146 | Grad Norm: 0.00850217\n",
      "Epoch 2 | Step 905900 | Avg Loss: 0.0145 | Grad Norm: 0.00847002\n",
      "Epoch 2 | Step 906000 | Avg Loss: 0.0145 | Grad Norm: 0.00827317\n",
      "Epoch 2 | Step 906100 | Avg Loss: 0.0148 | Grad Norm: 0.00966005\n",
      "Epoch 2 | Step 906200 | Avg Loss: 0.0149 | Grad Norm: 0.00916016\n",
      "Epoch 2 | Step 906300 | Avg Loss: 0.0149 | Grad Norm: 0.00748008\n",
      "Epoch 2 | Step 906400 | Avg Loss: 0.0149 | Grad Norm: 0.00949356\n",
      "Epoch 2 | Step 906500 | Avg Loss: 0.0149 | Grad Norm: 0.00775388\n",
      "Epoch 2 | Step 906600 | Avg Loss: 0.0147 | Grad Norm: 0.00800994\n",
      "Epoch 2 | Step 906700 | Avg Loss: 0.0150 | Grad Norm: 0.00929367\n",
      "Epoch 2 | Step 906800 | Avg Loss: 0.0149 | Grad Norm: 0.00838798\n",
      "Epoch 2 | Step 906900 | Avg Loss: 0.0148 | Grad Norm: 0.00820888\n",
      "Epoch 2 | Step 907000 | Avg Loss: 0.0146 | Grad Norm: 0.00860529\n",
      "Epoch 2 | Step 907100 | Avg Loss: 0.0148 | Grad Norm: 0.00938438\n",
      "Epoch 2 | Step 907200 | Avg Loss: 0.0149 | Grad Norm: 0.00914941\n",
      "Epoch 2 | Step 907300 | Avg Loss: 0.0151 | Grad Norm: 0.00864323\n",
      "Epoch 2 | Step 907400 | Avg Loss: 0.0152 | Grad Norm: 0.00846609\n",
      "Epoch 2 | Step 907500 | Avg Loss: 0.0153 | Grad Norm: 0.00992298\n",
      "Epoch 2 | Step 907600 | Avg Loss: 0.0152 | Grad Norm: 0.01411570\n",
      "Epoch 2 | Step 907700 | Avg Loss: 0.0151 | Grad Norm: 0.00901292\n",
      "Epoch 2 | Step 907800 | Avg Loss: 0.0151 | Grad Norm: 0.00804222\n",
      "Epoch 2 | Step 907900 | Avg Loss: 0.0150 | Grad Norm: 0.00964792\n",
      "Epoch 2 | Step 908000 | Avg Loss: 0.0149 | Grad Norm: 0.00832572\n",
      "Epoch 2 | Step 908100 | Avg Loss: 0.0145 | Grad Norm: 0.00856553\n",
      "Epoch 2 | Step 908200 | Avg Loss: 0.0147 | Grad Norm: 0.01084937\n",
      "Epoch 2 | Step 908300 | Avg Loss: 0.0148 | Grad Norm: 0.00825884\n",
      "Epoch 2 | Step 908400 | Avg Loss: 0.0150 | Grad Norm: 0.00867832\n",
      "Epoch 2 | Step 908500 | Avg Loss: 0.0150 | Grad Norm: 0.00939328\n",
      "Epoch 2 | Step 908600 | Avg Loss: 0.0146 | Grad Norm: 0.00823865\n",
      "Epoch 2 | Step 908700 | Avg Loss: 0.0142 | Grad Norm: 0.00988815\n",
      "Epoch 2 | Step 908800 | Avg Loss: 0.0143 | Grad Norm: 0.00870312\n",
      "Epoch 2 | Step 908900 | Avg Loss: 0.0146 | Grad Norm: 0.00823941\n",
      "Epoch 2 | Step 909000 | Avg Loss: 0.0153 | Grad Norm: 0.00870454\n",
      "Epoch 2 | Step 909100 | Avg Loss: 0.0151 | Grad Norm: 0.00774573\n",
      "Epoch 2 | Step 909200 | Avg Loss: 0.0152 | Grad Norm: 0.01057927\n",
      "Epoch 2 | Step 909300 | Avg Loss: 0.0149 | Grad Norm: 0.00994908\n",
      "Epoch 2 | Step 909400 | Avg Loss: 0.0145 | Grad Norm: 0.00872686\n",
      "Epoch 2 | Step 909500 | Avg Loss: 0.0147 | Grad Norm: 0.00844826\n",
      "Epoch 2 | Step 909600 | Avg Loss: 0.0147 | Grad Norm: 0.00909673\n",
      "Epoch 2 | Step 909700 | Avg Loss: 0.0147 | Grad Norm: 0.00835849\n",
      "Epoch 2 | Step 909800 | Avg Loss: 0.0145 | Grad Norm: 0.01014689\n",
      "Epoch 2 | Step 909900 | Avg Loss: 0.0148 | Grad Norm: 0.01249271\n",
      "Epoch 2 | Step 910000 | Avg Loss: 0.0149 | Grad Norm: 0.00945989\n",
      "Epoch 2 | Step 910100 | Avg Loss: 0.0151 | Grad Norm: 0.00859398\n",
      "Epoch 2 | Step 910200 | Avg Loss: 0.0150 | Grad Norm: 0.00844373\n",
      "Epoch 2 | Step 910300 | Avg Loss: 0.0152 | Grad Norm: 0.00801031\n",
      "Epoch 2 | Step 910400 | Avg Loss: 0.0155 | Grad Norm: 0.00964727\n",
      "Epoch 2 | Step 910500 | Avg Loss: 0.0154 | Grad Norm: 0.00865141\n",
      "Epoch 2 | Step 910600 | Avg Loss: 0.0153 | Grad Norm: 0.00942088\n",
      "Epoch 2 | Step 910700 | Avg Loss: 0.0152 | Grad Norm: 0.00957059\n",
      "Epoch 2 | Step 910800 | Avg Loss: 0.0150 | Grad Norm: 0.00800154\n",
      "Epoch 2 | Step 910900 | Avg Loss: 0.0154 | Grad Norm: 0.01076277\n",
      "Epoch 2 | Step 911000 | Avg Loss: 0.0153 | Grad Norm: 0.00851927\n",
      "Epoch 2 | Step 911100 | Avg Loss: 0.0156 | Grad Norm: 0.00887075\n",
      "Epoch 2 | Step 911200 | Avg Loss: 0.0154 | Grad Norm: 0.00898477\n",
      "Epoch 2 | Step 911300 | Avg Loss: 0.0148 | Grad Norm: 0.00789744\n",
      "Epoch 2 | Step 911400 | Avg Loss: 0.0147 | Grad Norm: 0.00952677\n",
      "Epoch 2 | Step 911500 | Avg Loss: 0.0149 | Grad Norm: 0.00865957\n",
      "Epoch 2 | Step 911600 | Avg Loss: 0.0148 | Grad Norm: 0.00899097\n",
      "Epoch 2 | Step 911700 | Avg Loss: 0.0148 | Grad Norm: 0.00923976\n",
      "Epoch 2 | Step 911800 | Avg Loss: 0.0153 | Grad Norm: 0.00917584\n",
      "Epoch 2 | Step 911900 | Avg Loss: 0.0154 | Grad Norm: 0.00797155\n",
      "Epoch 2 | Step 912000 | Avg Loss: 0.0153 | Grad Norm: 0.00700267\n",
      "Epoch 2 | Step 912100 | Avg Loss: 0.0151 | Grad Norm: 0.01092816\n",
      "Epoch 2 | Step 912200 | Avg Loss: 0.0152 | Grad Norm: 0.00801953\n",
      "Epoch 2 | Step 912300 | Avg Loss: 0.0152 | Grad Norm: 0.00674610\n",
      "Epoch 2 | Step 912400 | Avg Loss: 0.0151 | Grad Norm: 0.00862962\n",
      "Epoch 2 | Step 912500 | Avg Loss: 0.0155 | Grad Norm: 0.00868364\n",
      "Epoch 2 | Step 912600 | Avg Loss: 0.0155 | Grad Norm: 0.00963486\n",
      "Epoch 2 | Step 912700 | Avg Loss: 0.0151 | Grad Norm: 0.00976106\n",
      "Epoch 2 | Step 912800 | Avg Loss: 0.0149 | Grad Norm: 0.00807924\n",
      "Epoch 2 | Step 912900 | Avg Loss: 0.0151 | Grad Norm: 0.00792746\n",
      "Epoch 2 | Step 913000 | Avg Loss: 0.0154 | Grad Norm: 0.00880461\n",
      "Epoch 2 | Step 913100 | Avg Loss: 0.0151 | Grad Norm: 0.00784633\n",
      "Epoch 2 | Step 913200 | Avg Loss: 0.0147 | Grad Norm: 0.00830204\n",
      "Epoch 2 | Step 913300 | Avg Loss: 0.0146 | Grad Norm: 0.00905486\n",
      "Epoch 2 | Step 913400 | Avg Loss: 0.0153 | Grad Norm: 0.00963649\n",
      "Epoch 2 | Step 913500 | Avg Loss: 0.0151 | Grad Norm: 0.00832302\n",
      "Epoch 2 | Step 913600 | Avg Loss: 0.0153 | Grad Norm: 0.00867976\n",
      "Epoch 2 | Step 913700 | Avg Loss: 0.0155 | Grad Norm: 0.00870433\n",
      "Epoch 2 | Step 913800 | Avg Loss: 0.0154 | Grad Norm: 0.00816402\n",
      "Epoch 2 | Step 913900 | Avg Loss: 0.0153 | Grad Norm: 0.00814119\n",
      "Epoch 2 | Step 914000 | Avg Loss: 0.0154 | Grad Norm: 0.00902754\n",
      "Epoch 2 | Step 914100 | Avg Loss: 0.0152 | Grad Norm: 0.00871576\n",
      "Epoch 2 | Step 914200 | Avg Loss: 0.0154 | Grad Norm: 0.00990570\n",
      "Epoch 2 | Step 914300 | Avg Loss: 0.0151 | Grad Norm: 0.01028164\n",
      "Epoch 2 | Step 914400 | Avg Loss: 0.0148 | Grad Norm: 0.01029042\n",
      "Epoch 2 | Step 914500 | Avg Loss: 0.0153 | Grad Norm: 0.00900946\n",
      "Epoch 2 | Step 914600 | Avg Loss: 0.0150 | Grad Norm: 0.00900343\n",
      "Epoch 2 | Step 914700 | Avg Loss: 0.0150 | Grad Norm: 0.00810795\n",
      "Epoch 2 | Step 914800 | Avg Loss: 0.0149 | Grad Norm: 0.00858486\n",
      "Epoch 2 | Step 914900 | Avg Loss: 0.0149 | Grad Norm: 0.00793797\n",
      "Epoch 2 | Step 915000 | Avg Loss: 0.0144 | Grad Norm: 0.00840967\n",
      "Epoch 2 | Step 915100 | Avg Loss: 0.0142 | Grad Norm: 0.00919496\n",
      "Epoch 2 | Step 915200 | Avg Loss: 0.0145 | Grad Norm: 0.00755580\n",
      "Epoch 2 | Step 915300 | Avg Loss: 0.0146 | Grad Norm: 0.00892363\n",
      "Epoch 2 | Step 915400 | Avg Loss: 0.0145 | Grad Norm: 0.00940259\n",
      "Epoch 2 | Step 915500 | Avg Loss: 0.0144 | Grad Norm: 0.00791485\n",
      "Epoch 2 | Step 915600 | Avg Loss: 0.0150 | Grad Norm: 0.00811355\n",
      "Epoch 2 | Step 915700 | Avg Loss: 0.0150 | Grad Norm: 0.00768769\n",
      "Epoch 2 | Step 915800 | Avg Loss: 0.0150 | Grad Norm: 0.00893884\n",
      "Epoch 2 | Step 915900 | Avg Loss: 0.0148 | Grad Norm: 0.01003719\n",
      "Epoch 2 | Step 916000 | Avg Loss: 0.0145 | Grad Norm: 0.00961992\n",
      "Epoch 2 | Step 916100 | Avg Loss: 0.0148 | Grad Norm: 0.00790211\n",
      "Epoch 2 | Step 916200 | Avg Loss: 0.0149 | Grad Norm: 0.00867841\n",
      "Epoch 2 | Step 916300 | Avg Loss: 0.0149 | Grad Norm: 0.00899598\n",
      "Epoch 2 | Step 916400 | Avg Loss: 0.0154 | Grad Norm: 0.00960980\n",
      "Epoch 2 | Step 916500 | Avg Loss: 0.0154 | Grad Norm: 0.00842282\n",
      "Epoch 2 | Step 916600 | Avg Loss: 0.0152 | Grad Norm: 0.00741854\n",
      "Epoch 2 | Step 916700 | Avg Loss: 0.0154 | Grad Norm: 0.00914828\n",
      "Epoch 2 | Step 916800 | Avg Loss: 0.0154 | Grad Norm: 0.00846903\n",
      "Epoch 2 | Step 916900 | Avg Loss: 0.0150 | Grad Norm: 0.00809868\n",
      "Epoch 2 | Step 917000 | Avg Loss: 0.0151 | Grad Norm: 0.00912924\n",
      "Epoch 2 | Step 917100 | Avg Loss: 0.0150 | Grad Norm: 0.00929231\n",
      "Epoch 2 | Step 917200 | Avg Loss: 0.0152 | Grad Norm: 0.00792454\n",
      "Epoch 2 | Step 917300 | Avg Loss: 0.0150 | Grad Norm: 0.00780171\n",
      "Epoch 2 | Step 917400 | Avg Loss: 0.0150 | Grad Norm: 0.00813377\n",
      "Epoch 2 | Step 917500 | Avg Loss: 0.0147 | Grad Norm: 0.01082448\n",
      "Epoch 2 | Step 917600 | Avg Loss: 0.0144 | Grad Norm: 0.01033885\n",
      "Epoch 2 | Step 917700 | Avg Loss: 0.0143 | Grad Norm: 0.00727142\n",
      "Epoch 2 | Step 917800 | Avg Loss: 0.0144 | Grad Norm: 0.00816591\n",
      "Epoch 2 | Step 917900 | Avg Loss: 0.0140 | Grad Norm: 0.00939841\n",
      "Epoch 2 | Step 918000 | Avg Loss: 0.0143 | Grad Norm: 0.00852458\n",
      "Epoch 2 | Step 918100 | Avg Loss: 0.0146 | Grad Norm: 0.00737205\n",
      "Epoch 2 | Step 918200 | Avg Loss: 0.0148 | Grad Norm: 0.01215557\n",
      "Epoch 2 | Step 918300 | Avg Loss: 0.0145 | Grad Norm: 0.00827191\n",
      "Epoch 2 | Step 918400 | Avg Loss: 0.0144 | Grad Norm: 0.00864073\n",
      "Epoch 2 | Step 918500 | Avg Loss: 0.0148 | Grad Norm: 0.01088648\n",
      "Epoch 2 | Step 918600 | Avg Loss: 0.0147 | Grad Norm: 0.00767436\n",
      "Epoch 2 | Step 918700 | Avg Loss: 0.0148 | Grad Norm: 0.00720113\n",
      "Epoch 2 | Step 918800 | Avg Loss: 0.0147 | Grad Norm: 0.00968185\n",
      "Epoch 2 | Step 918900 | Avg Loss: 0.0146 | Grad Norm: 0.00964264\n",
      "Epoch 2 | Step 919000 | Avg Loss: 0.0149 | Grad Norm: 0.00870084\n",
      "Epoch 2 | Step 919100 | Avg Loss: 0.0148 | Grad Norm: 0.00786383\n",
      "Epoch 2 | Step 919200 | Avg Loss: 0.0147 | Grad Norm: 0.00885929\n",
      "Epoch 2 | Step 919300 | Avg Loss: 0.0149 | Grad Norm: 0.00868850\n",
      "Epoch 2 | Step 919400 | Avg Loss: 0.0151 | Grad Norm: 0.00862338\n",
      "Epoch 2 | Step 919500 | Avg Loss: 0.0152 | Grad Norm: 0.00986479\n",
      "Epoch 2 | Step 919600 | Avg Loss: 0.0151 | Grad Norm: 0.00925448\n",
      "Epoch 2 | Step 919700 | Avg Loss: 0.0145 | Grad Norm: 0.00770497\n",
      "Epoch 2 | Step 919800 | Avg Loss: 0.0145 | Grad Norm: 0.00818812\n",
      "Epoch 2 | Step 919900 | Avg Loss: 0.0149 | Grad Norm: 0.00772626\n",
      "Epoch 2 | Step 920000 | Avg Loss: 0.0144 | Grad Norm: 0.00811826\n",
      "Epoch 2 | Step 920100 | Avg Loss: 0.0147 | Grad Norm: 0.00937302\n",
      "Epoch 2 | Step 920200 | Avg Loss: 0.0146 | Grad Norm: 0.00856440\n",
      "Epoch 2 | Step 920300 | Avg Loss: 0.0144 | Grad Norm: 0.00704173\n",
      "Epoch 2 | Step 920400 | Avg Loss: 0.0150 | Grad Norm: 0.01092770\n",
      "Epoch 2 | Step 920500 | Avg Loss: 0.0151 | Grad Norm: 0.01119392\n",
      "Epoch 2 | Step 920600 | Avg Loss: 0.0152 | Grad Norm: 0.00886807\n",
      "Epoch 2 | Step 920700 | Avg Loss: 0.0152 | Grad Norm: 0.00771734\n",
      "Epoch 2 | Step 920800 | Avg Loss: 0.0152 | Grad Norm: 0.00724075\n",
      "Epoch 2 | Step 920900 | Avg Loss: 0.0154 | Grad Norm: 0.01017178\n",
      "Epoch 2 | Step 921000 | Avg Loss: 0.0154 | Grad Norm: 0.00875464\n",
      "Epoch 2 | Step 921100 | Avg Loss: 0.0151 | Grad Norm: 0.00793248\n",
      "Epoch 2 | Step 921200 | Avg Loss: 0.0147 | Grad Norm: 0.00933249\n",
      "Epoch 2 | Step 921300 | Avg Loss: 0.0146 | Grad Norm: 0.00748215\n",
      "Epoch 2 | Step 921400 | Avg Loss: 0.0145 | Grad Norm: 0.00773032\n",
      "Epoch 2 | Step 921500 | Avg Loss: 0.0142 | Grad Norm: 0.00820923\n",
      "Epoch 2 | Step 921600 | Avg Loss: 0.0144 | Grad Norm: 0.00950362\n",
      "Epoch 2 | Step 921700 | Avg Loss: 0.0145 | Grad Norm: 0.01118619\n",
      "Epoch 2 | Step 921800 | Avg Loss: 0.0149 | Grad Norm: 0.01010185\n",
      "Epoch 2 | Step 921900 | Avg Loss: 0.0150 | Grad Norm: 0.00924476\n",
      "Epoch 2 | Step 922000 | Avg Loss: 0.0151 | Grad Norm: 0.00854067\n",
      "Epoch 2 | Step 922100 | Avg Loss: 0.0151 | Grad Norm: 0.00873522\n",
      "Epoch 2 | Step 922200 | Avg Loss: 0.0149 | Grad Norm: 0.00838290\n",
      "Epoch 2 | Step 922300 | Avg Loss: 0.0150 | Grad Norm: 0.00965159\n",
      "Epoch 2 | Step 922400 | Avg Loss: 0.0151 | Grad Norm: 0.01098513\n",
      "Epoch 2 | Step 922500 | Avg Loss: 0.0151 | Grad Norm: 0.01009315\n",
      "Epoch 2 | Step 922600 | Avg Loss: 0.0150 | Grad Norm: 0.01139993\n",
      "Epoch 2 | Step 922700 | Avg Loss: 0.0148 | Grad Norm: 0.00990329\n",
      "Epoch 2 | Step 922800 | Avg Loss: 0.0148 | Grad Norm: 0.00839381\n",
      "Epoch 2 | Step 922900 | Avg Loss: 0.0149 | Grad Norm: 0.00962392\n",
      "Epoch 2 | Step 923000 | Avg Loss: 0.0146 | Grad Norm: 0.00724156\n",
      "Epoch 2 | Step 923100 | Avg Loss: 0.0144 | Grad Norm: 0.00951164\n",
      "Epoch 2 | Step 923200 | Avg Loss: 0.0147 | Grad Norm: 0.00875657\n",
      "Epoch 2 | Step 923300 | Avg Loss: 0.0146 | Grad Norm: 0.00737673\n",
      "Epoch 2 | Step 923400 | Avg Loss: 0.0151 | Grad Norm: 0.00919016\n",
      "Epoch 2 | Step 923500 | Avg Loss: 0.0150 | Grad Norm: 0.00811324\n",
      "Epoch 2 | Step 923600 | Avg Loss: 0.0152 | Grad Norm: 0.00894251\n",
      "Epoch 2 | Step 923700 | Avg Loss: 0.0149 | Grad Norm: 0.00849219\n",
      "Epoch 2 | Step 923800 | Avg Loss: 0.0147 | Grad Norm: 0.00854102\n",
      "Epoch 2 | Step 923900 | Avg Loss: 0.0146 | Grad Norm: 0.00913342\n",
      "Epoch 2 | Step 924000 | Avg Loss: 0.0145 | Grad Norm: 0.00816353\n",
      "Epoch 2 | Step 924100 | Avg Loss: 0.0141 | Grad Norm: 0.00882990\n",
      "Epoch 2 | Step 924200 | Avg Loss: 0.0143 | Grad Norm: 0.01004658\n",
      "Epoch 2 | Step 924300 | Avg Loss: 0.0146 | Grad Norm: 0.00924184\n",
      "Epoch 2 | Step 924400 | Avg Loss: 0.0149 | Grad Norm: 0.00849030\n",
      "Epoch 2 | Step 924500 | Avg Loss: 0.0148 | Grad Norm: 0.00711714\n",
      "Epoch 2 | Step 924600 | Avg Loss: 0.0146 | Grad Norm: 0.00903623\n",
      "Epoch 2 | Step 924700 | Avg Loss: 0.0149 | Grad Norm: 0.00831897\n",
      "Epoch 2 | Step 924800 | Avg Loss: 0.0146 | Grad Norm: 0.00808863\n",
      "Epoch 2 | Step 924900 | Avg Loss: 0.0145 | Grad Norm: 0.00754495\n",
      "Epoch 2 | Step 925000 | Avg Loss: 0.0145 | Grad Norm: 0.00994483\n",
      "Epoch 2 | Step 925100 | Avg Loss: 0.0146 | Grad Norm: 0.00874478\n",
      "Epoch 2 | Step 925200 | Avg Loss: 0.0148 | Grad Norm: 0.00885800\n",
      "Epoch 2 | Step 925300 | Avg Loss: 0.0146 | Grad Norm: 0.00766319\n",
      "Epoch 2 | Step 925400 | Avg Loss: 0.0146 | Grad Norm: 0.00936953\n",
      "Epoch 2 | Step 925500 | Avg Loss: 0.0148 | Grad Norm: 0.00770649\n",
      "Epoch 2 | Step 925600 | Avg Loss: 0.0150 | Grad Norm: 0.00804413\n",
      "Epoch 2 | Step 925700 | Avg Loss: 0.0150 | Grad Norm: 0.00767405\n",
      "Epoch 2 | Step 925800 | Avg Loss: 0.0150 | Grad Norm: 0.00838934\n",
      "Epoch 2 | Step 925900 | Avg Loss: 0.0149 | Grad Norm: 0.00887037\n",
      "Epoch 2 | Step 926000 | Avg Loss: 0.0152 | Grad Norm: 0.00887217\n",
      "Epoch 2 | Step 926100 | Avg Loss: 0.0154 | Grad Norm: 0.00947025\n",
      "Epoch 2 | Step 926200 | Avg Loss: 0.0153 | Grad Norm: 0.00799253\n",
      "Epoch 2 | Step 926300 | Avg Loss: 0.0150 | Grad Norm: 0.00798967\n",
      "Epoch 2 | Step 926400 | Avg Loss: 0.0146 | Grad Norm: 0.00865851\n",
      "Epoch 2 | Step 926500 | Avg Loss: 0.0150 | Grad Norm: 0.00900000\n",
      "Epoch 2 | Step 926600 | Avg Loss: 0.0149 | Grad Norm: 0.00767795\n",
      "Epoch 2 | Step 926700 | Avg Loss: 0.0150 | Grad Norm: 0.00801079\n",
      "Epoch 2 | Step 926800 | Avg Loss: 0.0151 | Grad Norm: 0.00768882\n",
      "Epoch 2 | Step 926900 | Avg Loss: 0.0149 | Grad Norm: 0.00764182\n",
      "Epoch 2 | Step 927000 | Avg Loss: 0.0149 | Grad Norm: 0.00762698\n",
      "Epoch 2 | Step 927100 | Avg Loss: 0.0149 | Grad Norm: 0.00924064\n",
      "Epoch 2 | Step 927200 | Avg Loss: 0.0151 | Grad Norm: 0.00946401\n",
      "Epoch 2 | Step 927300 | Avg Loss: 0.0147 | Grad Norm: 0.00886015\n",
      "Epoch 2 | Step 927400 | Avg Loss: 0.0147 | Grad Norm: 0.01037597\n",
      "Epoch 2 | Step 927500 | Avg Loss: 0.0145 | Grad Norm: 0.00825850\n",
      "Epoch 2 | Step 927600 | Avg Loss: 0.0150 | Grad Norm: 0.00956290\n",
      "Epoch 2 | Step 927700 | Avg Loss: 0.0148 | Grad Norm: 0.01016521\n",
      "Epoch 2 | Step 927800 | Avg Loss: 0.0145 | Grad Norm: 0.00882337\n",
      "Epoch 2 | Step 927900 | Avg Loss: 0.0144 | Grad Norm: 0.00795116\n",
      "Epoch 2 | Step 928000 | Avg Loss: 0.0145 | Grad Norm: 0.00757791\n",
      "Epoch 2 | Step 928100 | Avg Loss: 0.0146 | Grad Norm: 0.00772298\n",
      "Epoch 2 | Step 928200 | Avg Loss: 0.0150 | Grad Norm: 0.00876468\n",
      "Epoch 2 | Step 928300 | Avg Loss: 0.0156 | Grad Norm: 0.00856142\n",
      "Epoch 2 | Step 928400 | Avg Loss: 0.0157 | Grad Norm: 0.00812830\n",
      "Epoch 2 | Step 928500 | Avg Loss: 0.0153 | Grad Norm: 0.00844263\n",
      "Epoch 2 | Step 928600 | Avg Loss: 0.0152 | Grad Norm: 0.00831764\n",
      "Epoch 2 | Step 928700 | Avg Loss: 0.0148 | Grad Norm: 0.00903234\n",
      "Epoch 2 | Step 928800 | Avg Loss: 0.0148 | Grad Norm: 0.00942822\n",
      "Epoch 2 | Step 928900 | Avg Loss: 0.0148 | Grad Norm: 0.00923286\n",
      "Epoch 2 | Step 929000 | Avg Loss: 0.0146 | Grad Norm: 0.00866248\n",
      "Epoch 2 | Step 929100 | Avg Loss: 0.0146 | Grad Norm: 0.00873326\n",
      "Epoch 2 | Step 929200 | Avg Loss: 0.0149 | Grad Norm: 0.00987781\n",
      "Epoch 2 | Step 929300 | Avg Loss: 0.0151 | Grad Norm: 0.00690588\n",
      "Epoch 2 | Step 929400 | Avg Loss: 0.0146 | Grad Norm: 0.00898321\n",
      "Epoch 2 | Step 929500 | Avg Loss: 0.0149 | Grad Norm: 0.00868372\n",
      "Epoch 2 | Step 929600 | Avg Loss: 0.0147 | Grad Norm: 0.00640344\n",
      "Epoch 2 | Step 929700 | Avg Loss: 0.0145 | Grad Norm: 0.00792968\n",
      "Epoch 2 | Step 929800 | Avg Loss: 0.0150 | Grad Norm: 0.00805580\n",
      "Epoch 2 | Step 929900 | Avg Loss: 0.0152 | Grad Norm: 0.00925595\n",
      "Epoch 2 | Step 930000 | Avg Loss: 0.0153 | Grad Norm: 0.00868597\n",
      "Epoch 2 | Step 930100 | Avg Loss: 0.0151 | Grad Norm: 0.00950446\n",
      "Epoch 2 | Step 930200 | Avg Loss: 0.0150 | Grad Norm: 0.00772852\n",
      "Epoch 2 | Step 930300 | Avg Loss: 0.0147 | Grad Norm: 0.00762109\n",
      "Epoch 2 | Step 930400 | Avg Loss: 0.0150 | Grad Norm: 0.00777805\n",
      "Epoch 2 | Step 930500 | Avg Loss: 0.0150 | Grad Norm: 0.00802918\n",
      "Epoch 2 | Step 930600 | Avg Loss: 0.0148 | Grad Norm: 0.00958190\n",
      "Epoch 2 | Step 930700 | Avg Loss: 0.0148 | Grad Norm: 0.00862593\n",
      "Epoch 2 | Step 930800 | Avg Loss: 0.0147 | Grad Norm: 0.00785315\n",
      "Epoch 2 | Step 930900 | Avg Loss: 0.0146 | Grad Norm: 0.00829085\n",
      "Epoch 2 | Step 931000 | Avg Loss: 0.0149 | Grad Norm: 0.00795815\n",
      "Epoch 2 | Step 931100 | Avg Loss: 0.0144 | Grad Norm: 0.00855950\n",
      "Epoch 2 | Step 931200 | Avg Loss: 0.0144 | Grad Norm: 0.00890819\n",
      "Epoch 2 | Step 931300 | Avg Loss: 0.0142 | Grad Norm: 0.00844864\n",
      "Epoch 2 | Step 931400 | Avg Loss: 0.0146 | Grad Norm: 0.00709131\n",
      "Epoch 2 | Step 931500 | Avg Loss: 0.0147 | Grad Norm: 0.00879325\n",
      "Epoch 2 | Step 931600 | Avg Loss: 0.0147 | Grad Norm: 0.00859282\n",
      "Epoch 2 | Step 931700 | Avg Loss: 0.0149 | Grad Norm: 0.00852834\n",
      "Epoch 2 | Step 931800 | Avg Loss: 0.0149 | Grad Norm: 0.00826216\n",
      "Epoch 2 | Step 931900 | Avg Loss: 0.0147 | Grad Norm: 0.00879509\n",
      "Epoch 2 | Step 932000 | Avg Loss: 0.0147 | Grad Norm: 0.00908774\n",
      "Epoch 2 | Step 932100 | Avg Loss: 0.0146 | Grad Norm: 0.00813614\n",
      "Epoch 2 | Step 932200 | Avg Loss: 0.0143 | Grad Norm: 0.00858479\n",
      "Epoch 2 | Step 932300 | Avg Loss: 0.0144 | Grad Norm: 0.00874795\n",
      "Epoch 2 | Step 932400 | Avg Loss: 0.0142 | Grad Norm: 0.01007108\n",
      "Epoch 2 | Step 932500 | Avg Loss: 0.0142 | Grad Norm: 0.00933166\n",
      "Epoch 2 | Step 932600 | Avg Loss: 0.0144 | Grad Norm: 0.00768943\n",
      "Epoch 2 | Step 932700 | Avg Loss: 0.0145 | Grad Norm: 0.00777841\n",
      "Epoch 2 | Step 932800 | Avg Loss: 0.0144 | Grad Norm: 0.00771061\n",
      "Epoch 2 | Step 932900 | Avg Loss: 0.0144 | Grad Norm: 0.00974097\n",
      "Epoch 2 | Step 933000 | Avg Loss: 0.0144 | Grad Norm: 0.00841693\n",
      "Epoch 2 | Step 933100 | Avg Loss: 0.0148 | Grad Norm: 0.00892250\n",
      "Epoch 2 | Step 933200 | Avg Loss: 0.0148 | Grad Norm: 0.00857199\n",
      "Epoch 2 | Step 933300 | Avg Loss: 0.0145 | Grad Norm: 0.00820110\n",
      "Epoch 2 | Step 933400 | Avg Loss: 0.0143 | Grad Norm: 0.01076878\n",
      "Epoch 2 | Step 933500 | Avg Loss: 0.0143 | Grad Norm: 0.00777300\n",
      "Epoch 2 | Step 933600 | Avg Loss: 0.0149 | Grad Norm: 0.00925426\n",
      "Epoch 2 | Step 933700 | Avg Loss: 0.0149 | Grad Norm: 0.00808264\n",
      "Epoch 2 | Step 933800 | Avg Loss: 0.0147 | Grad Norm: 0.00827220\n",
      "Epoch 2 | Step 933900 | Avg Loss: 0.0148 | Grad Norm: 0.00912519\n",
      "Epoch 2 | Step 934000 | Avg Loss: 0.0149 | Grad Norm: 0.00934763\n",
      "Epoch 2 | Step 934100 | Avg Loss: 0.0151 | Grad Norm: 0.00884384\n",
      "Epoch 2 | Step 934200 | Avg Loss: 0.0147 | Grad Norm: 0.00870157\n",
      "Epoch 2 | Step 934300 | Avg Loss: 0.0144 | Grad Norm: 0.00767777\n",
      "Epoch 2 | Step 934400 | Avg Loss: 0.0146 | Grad Norm: 0.00905460\n",
      "Epoch 2 | Step 934500 | Avg Loss: 0.0144 | Grad Norm: 0.00865833\n",
      "Epoch 2 | Step 934600 | Avg Loss: 0.0146 | Grad Norm: 0.00875478\n",
      "Epoch 2 | Step 934700 | Avg Loss: 0.0150 | Grad Norm: 0.00772454\n",
      "Epoch 2 | Step 934800 | Avg Loss: 0.0147 | Grad Norm: 0.00805142\n",
      "Epoch 2 | Step 934900 | Avg Loss: 0.0144 | Grad Norm: 0.00844235\n",
      "Epoch 2 | Step 935000 | Avg Loss: 0.0145 | Grad Norm: 0.00833917\n",
      "Epoch 2 | Step 935100 | Avg Loss: 0.0147 | Grad Norm: 0.00861377\n",
      "Epoch 2 | Step 935200 | Avg Loss: 0.0152 | Grad Norm: 0.00920536\n",
      "Epoch 2 | Step 935300 | Avg Loss: 0.0149 | Grad Norm: 0.00934749\n",
      "Epoch 2 | Step 935400 | Avg Loss: 0.0151 | Grad Norm: 0.00771040\n",
      "Epoch 2 | Step 935500 | Avg Loss: 0.0148 | Grad Norm: 0.00955067\n",
      "Epoch 2 | Step 935600 | Avg Loss: 0.0147 | Grad Norm: 0.00806544\n",
      "Epoch 2 | Step 935700 | Avg Loss: 0.0142 | Grad Norm: 0.00741598\n",
      "Epoch 2 | Step 935800 | Avg Loss: 0.0141 | Grad Norm: 0.00813724\n",
      "Epoch 2 | Step 935900 | Avg Loss: 0.0142 | Grad Norm: 0.00908314\n",
      "Epoch 2 | Step 936000 | Avg Loss: 0.0146 | Grad Norm: 0.00895556\n",
      "Epoch 2 | Step 936100 | Avg Loss: 0.0151 | Grad Norm: 0.00865093\n",
      "Epoch 2 | Step 936200 | Avg Loss: 0.0152 | Grad Norm: 0.00869685\n",
      "Epoch 2 | Step 936300 | Avg Loss: 0.0147 | Grad Norm: 0.00943984\n",
      "Epoch 2 | Step 936400 | Avg Loss: 0.0148 | Grad Norm: 0.00974517\n",
      "Epoch 2 | Step 936500 | Avg Loss: 0.0146 | Grad Norm: 0.00782651\n",
      "Epoch 2 | Step 936600 | Avg Loss: 0.0147 | Grad Norm: 0.00689905\n",
      "Epoch 2 | Step 936700 | Avg Loss: 0.0144 | Grad Norm: 0.00914755\n",
      "Epoch 2 | Step 936800 | Avg Loss: 0.0144 | Grad Norm: 0.00840770\n",
      "Epoch 2 | Step 936900 | Avg Loss: 0.0143 | Grad Norm: 0.00906793\n",
      "Epoch 2 | Step 937000 | Avg Loss: 0.0142 | Grad Norm: 0.00743045\n",
      "Epoch 2 | Step 937100 | Avg Loss: 0.0140 | Grad Norm: 0.00841249\n",
      "Epoch 2 | Step 937200 | Avg Loss: 0.0142 | Grad Norm: 0.00773715\n",
      "Epoch 2 | Step 937300 | Avg Loss: 0.0143 | Grad Norm: 0.00721000\n",
      "Epoch 2 | Step 937400 | Avg Loss: 0.0149 | Grad Norm: 0.00890117\n",
      "Epoch 2 | Step 937500 | Avg Loss: 0.0150 | Grad Norm: 0.00889763\n",
      "Epoch 2 | Step 937600 | Avg Loss: 0.0153 | Grad Norm: 0.00848531\n",
      "Epoch 2 | Step 937700 | Avg Loss: 0.0152 | Grad Norm: 0.00752770\n",
      "Epoch 2 | Step 937800 | Avg Loss: 0.0150 | Grad Norm: 0.00866101\n",
      "Epoch 2 | Step 937900 | Avg Loss: 0.0148 | Grad Norm: 0.00778891\n",
      "Epoch 2 | Step 938000 | Avg Loss: 0.0147 | Grad Norm: 0.00728193\n",
      "Epoch 2 | Step 938100 | Avg Loss: 0.0145 | Grad Norm: 0.00734529\n",
      "Epoch 2 | Step 938200 | Avg Loss: 0.0146 | Grad Norm: 0.00751736\n",
      "Epoch 2 | Step 938300 | Avg Loss: 0.0146 | Grad Norm: 0.00888868\n",
      "Epoch 2 | Step 938400 | Avg Loss: 0.0146 | Grad Norm: 0.00828054\n",
      "Epoch 2 | Step 938500 | Avg Loss: 0.0146 | Grad Norm: 0.00850247\n",
      "Epoch 2 | Step 938600 | Avg Loss: 0.0147 | Grad Norm: 0.00960682\n",
      "Epoch 2 | Step 938700 | Avg Loss: 0.0151 | Grad Norm: 0.00787671\n",
      "Epoch 2 | Step 938800 | Avg Loss: 0.0149 | Grad Norm: 0.00974748\n",
      "Epoch 2 | Step 938900 | Avg Loss: 0.0145 | Grad Norm: 0.00828613\n",
      "Epoch 2 | Step 939000 | Avg Loss: 0.0148 | Grad Norm: 0.00832463\n",
      "Epoch 2 | Step 939100 | Avg Loss: 0.0146 | Grad Norm: 0.00790477\n",
      "Epoch 2 | Step 939200 | Avg Loss: 0.0145 | Grad Norm: 0.00775904\n",
      "Epoch 2 | Step 939300 | Avg Loss: 0.0147 | Grad Norm: 0.00788797\n",
      "Epoch 2 | Step 939400 | Avg Loss: 0.0147 | Grad Norm: 0.00859322\n",
      "Epoch 2 | Step 939500 | Avg Loss: 0.0148 | Grad Norm: 0.00839645\n",
      "Epoch 2 | Step 939600 | Avg Loss: 0.0148 | Grad Norm: 0.00819341\n",
      "Epoch 2 | Step 939700 | Avg Loss: 0.0151 | Grad Norm: 0.00769215\n",
      "Epoch 2 | Step 939800 | Avg Loss: 0.0149 | Grad Norm: 0.00784475\n",
      "Epoch 2 | Step 939900 | Avg Loss: 0.0149 | Grad Norm: 0.00965652\n",
      "Epoch 2 | Step 940000 | Avg Loss: 0.0150 | Grad Norm: 0.00869982\n",
      "Epoch 2 | Step 940100 | Avg Loss: 0.0151 | Grad Norm: 0.00784172\n",
      "Epoch 2 | Step 940200 | Avg Loss: 0.0150 | Grad Norm: 0.00847465\n",
      "Epoch 2 | Step 940300 | Avg Loss: 0.0151 | Grad Norm: 0.00721227\n",
      "Epoch 2 | Step 940400 | Avg Loss: 0.0149 | Grad Norm: 0.00870117\n",
      "Epoch 2 | Step 940500 | Avg Loss: 0.0152 | Grad Norm: 0.00931740\n",
      "Epoch 2 | Step 940600 | Avg Loss: 0.0151 | Grad Norm: 0.00773802\n",
      "Epoch 2 | Step 940700 | Avg Loss: 0.0153 | Grad Norm: 0.00988182\n",
      "Epoch 2 | Step 940800 | Avg Loss: 0.0150 | Grad Norm: 0.00865538\n",
      "Epoch 2 | Step 940900 | Avg Loss: 0.0152 | Grad Norm: 0.00756486\n",
      "Epoch 2 | Step 941000 | Avg Loss: 0.0152 | Grad Norm: 0.00850567\n",
      "Epoch 2 | Step 941100 | Avg Loss: 0.0153 | Grad Norm: 0.00816846\n",
      "Epoch 2 | Step 941200 | Avg Loss: 0.0150 | Grad Norm: 0.00840153\n",
      "Epoch 2 | Step 941300 | Avg Loss: 0.0149 | Grad Norm: 0.00736859\n",
      "Epoch 2 | Step 941400 | Avg Loss: 0.0150 | Grad Norm: 0.00873005\n",
      "Epoch 2 | Step 941500 | Avg Loss: 0.0149 | Grad Norm: 0.00846562\n",
      "Epoch 2 | Step 941600 | Avg Loss: 0.0148 | Grad Norm: 0.00791464\n",
      "Epoch 2 | Step 941700 | Avg Loss: 0.0149 | Grad Norm: 0.00887523\n",
      "Epoch 2 | Step 941800 | Avg Loss: 0.0149 | Grad Norm: 0.00842721\n",
      "Epoch 2 | Step 941900 | Avg Loss: 0.0149 | Grad Norm: 0.00801952\n",
      "Epoch 2 | Step 942000 | Avg Loss: 0.0148 | Grad Norm: 0.00813469\n",
      "Epoch 2 | Step 942100 | Avg Loss: 0.0148 | Grad Norm: 0.00847458\n",
      "Epoch 2 | Step 942200 | Avg Loss: 0.0147 | Grad Norm: 0.00812173\n",
      "Epoch 2 | Step 942300 | Avg Loss: 0.0146 | Grad Norm: 0.00770783\n",
      "Epoch 2 | Step 942400 | Avg Loss: 0.0147 | Grad Norm: 0.00905559\n",
      "Epoch 2 | Step 942500 | Avg Loss: 0.0146 | Grad Norm: 0.00751271\n",
      "Epoch 2 | Step 942600 | Avg Loss: 0.0147 | Grad Norm: 0.00964562\n",
      "Epoch 2 | Step 942700 | Avg Loss: 0.0146 | Grad Norm: 0.00820586\n",
      "Epoch 2 | Step 942800 | Avg Loss: 0.0144 | Grad Norm: 0.00836891\n",
      "Epoch 2 | Step 942900 | Avg Loss: 0.0146 | Grad Norm: 0.00754149\n",
      "Epoch 2 | Step 943000 | Avg Loss: 0.0150 | Grad Norm: 0.00971201\n",
      "Epoch 2 | Step 943100 | Avg Loss: 0.0146 | Grad Norm: 0.00808549\n",
      "Epoch 2 | Step 943200 | Avg Loss: 0.0142 | Grad Norm: 0.00864742\n",
      "Epoch 2 | Step 943300 | Avg Loss: 0.0144 | Grad Norm: 0.00744166\n",
      "Epoch 2 | Step 943400 | Avg Loss: 0.0149 | Grad Norm: 0.00801054\n",
      "Epoch 2 | Step 943500 | Avg Loss: 0.0150 | Grad Norm: 0.00751264\n",
      "Epoch 2 | Step 943600 | Avg Loss: 0.0146 | Grad Norm: 0.00804118\n",
      "Epoch 2 | Step 943700 | Avg Loss: 0.0142 | Grad Norm: 0.00711199\n",
      "Epoch 2 | Step 943800 | Avg Loss: 0.0143 | Grad Norm: 0.00699137\n",
      "Epoch 2 | Step 943900 | Avg Loss: 0.0146 | Grad Norm: 0.00954315\n",
      "Epoch 2 | Step 944000 | Avg Loss: 0.0147 | Grad Norm: 0.00878245\n",
      "Epoch 2 | Step 944100 | Avg Loss: 0.0151 | Grad Norm: 0.00863349\n",
      "Epoch 2 | Step 944200 | Avg Loss: 0.0151 | Grad Norm: 0.00853274\n",
      "Epoch 2 | Step 944300 | Avg Loss: 0.0151 | Grad Norm: 0.00752002\n",
      "Epoch 2 | Step 944400 | Avg Loss: 0.0152 | Grad Norm: 0.00755922\n",
      "Epoch 2 | Step 944500 | Avg Loss: 0.0147 | Grad Norm: 0.00798301\n",
      "Epoch 2 | Step 944600 | Avg Loss: 0.0149 | Grad Norm: 0.00927153\n",
      "Epoch 2 | Step 944700 | Avg Loss: 0.0153 | Grad Norm: 0.01036758\n",
      "Epoch 2 | Step 944800 | Avg Loss: 0.0151 | Grad Norm: 0.00970140\n",
      "Epoch 2 | Step 944900 | Avg Loss: 0.0152 | Grad Norm: 0.00960657\n",
      "Epoch 2 | Step 945000 | Avg Loss: 0.0150 | Grad Norm: 0.00884486\n",
      "Epoch 2 | Step 945100 | Avg Loss: 0.0152 | Grad Norm: 0.00844239\n",
      "Epoch 2 | Step 945200 | Avg Loss: 0.0152 | Grad Norm: 0.00888728\n",
      "Epoch 2 | Step 945300 | Avg Loss: 0.0149 | Grad Norm: 0.00987917\n",
      "Epoch 2 | Step 945400 | Avg Loss: 0.0155 | Grad Norm: 0.00924503\n",
      "Epoch 2 | Step 945500 | Avg Loss: 0.0155 | Grad Norm: 0.00842544\n",
      "Epoch 2 | Step 945600 | Avg Loss: 0.0152 | Grad Norm: 0.00824114\n",
      "Epoch 2 | Step 945700 | Avg Loss: 0.0149 | Grad Norm: 0.00745700\n",
      "Epoch 2 | Step 945800 | Avg Loss: 0.0146 | Grad Norm: 0.00819838\n",
      "Epoch 2 | Step 945900 | Avg Loss: 0.0147 | Grad Norm: 0.00841119\n",
      "Epoch 2 | Step 946000 | Avg Loss: 0.0144 | Grad Norm: 0.00904566\n",
      "Epoch 2 | Step 946100 | Avg Loss: 0.0148 | Grad Norm: 0.00753987\n",
      "Epoch 2 | Step 946200 | Avg Loss: 0.0147 | Grad Norm: 0.00849787\n",
      "Epoch 2 | Step 946300 | Avg Loss: 0.0146 | Grad Norm: 0.00822619\n",
      "Epoch 2 | Step 946400 | Avg Loss: 0.0145 | Grad Norm: 0.00990598\n",
      "Epoch 2 | Step 946500 | Avg Loss: 0.0145 | Grad Norm: 0.00880786\n",
      "Epoch 2 | Step 946600 | Avg Loss: 0.0154 | Grad Norm: 0.00815859\n",
      "Epoch 2 | Step 946700 | Avg Loss: 0.0153 | Grad Norm: 0.00773349\n",
      "Epoch 2 | Step 946800 | Avg Loss: 0.0150 | Grad Norm: 0.00810240\n",
      "Epoch 2 | Step 946900 | Avg Loss: 0.0148 | Grad Norm: 0.00741973\n",
      "Epoch 2 | Step 947000 | Avg Loss: 0.0147 | Grad Norm: 0.00917911\n",
      "Epoch 2 | Step 947100 | Avg Loss: 0.0146 | Grad Norm: 0.00770056\n",
      "Epoch 2 | Step 947200 | Avg Loss: 0.0146 | Grad Norm: 0.00868594\n",
      "Epoch 2 | Step 947300 | Avg Loss: 0.0146 | Grad Norm: 0.00825573\n",
      "Epoch 2 | Step 947400 | Avg Loss: 0.0146 | Grad Norm: 0.00858929\n",
      "Epoch 2 | Step 947500 | Avg Loss: 0.0151 | Grad Norm: 0.00975067\n",
      "Epoch 2 | Step 947600 | Avg Loss: 0.0152 | Grad Norm: 0.00964052\n",
      "Epoch 2 | Step 947700 | Avg Loss: 0.0153 | Grad Norm: 0.00777229\n",
      "Epoch 2 | Step 947800 | Avg Loss: 0.0153 | Grad Norm: 0.00894000\n",
      "Epoch 2 | Step 947900 | Avg Loss: 0.0152 | Grad Norm: 0.01035313\n",
      "Epoch 2 | Step 948000 | Avg Loss: 0.0152 | Grad Norm: 0.01001188\n",
      "Epoch 2 | Step 948100 | Avg Loss: 0.0151 | Grad Norm: 0.00906038\n",
      "Epoch 2 | Step 948200 | Avg Loss: 0.0151 | Grad Norm: 0.00726650\n",
      "Epoch 2 | Step 948300 | Avg Loss: 0.0151 | Grad Norm: 0.00741920\n",
      "Epoch 2 | Step 948400 | Avg Loss: 0.0154 | Grad Norm: 0.00980561\n",
      "Epoch 2 | Step 948500 | Avg Loss: 0.0152 | Grad Norm: 0.00951502\n",
      "Epoch 2 | Step 948600 | Avg Loss: 0.0152 | Grad Norm: 0.00830507\n",
      "Epoch 2 | Step 948700 | Avg Loss: 0.0151 | Grad Norm: 0.00968228\n",
      "Epoch 2 | Step 948800 | Avg Loss: 0.0155 | Grad Norm: 0.00821143\n",
      "Epoch 2 | Step 948900 | Avg Loss: 0.0156 | Grad Norm: 0.00977658\n",
      "Epoch 2 | Step 949000 | Avg Loss: 0.0158 | Grad Norm: 0.00869595\n",
      "Epoch 2 | Step 949100 | Avg Loss: 0.0154 | Grad Norm: 0.00744265\n",
      "Epoch 2 | Step 949200 | Avg Loss: 0.0150 | Grad Norm: 0.00798257\n",
      "Epoch 2 | Step 949300 | Avg Loss: 0.0153 | Grad Norm: 0.00875492\n",
      "Epoch 2 | Step 949400 | Avg Loss: 0.0155 | Grad Norm: 0.00897929\n",
      "Epoch 2 | Step 949500 | Avg Loss: 0.0155 | Grad Norm: 0.00850999\n",
      "Epoch 2 | Step 949600 | Avg Loss: 0.0149 | Grad Norm: 0.00831370\n",
      "Epoch 2 | Step 949700 | Avg Loss: 0.0149 | Grad Norm: 0.00849608\n",
      "Epoch 2 | Step 949800 | Avg Loss: 0.0153 | Grad Norm: 0.01035067\n",
      "Epoch 2 | Step 949900 | Avg Loss: 0.0154 | Grad Norm: 0.00793793\n",
      "Epoch 2 | Step 950000 | Avg Loss: 0.0148 | Grad Norm: 0.00719446\n",
      "Epoch 2 | Step 950100 | Avg Loss: 0.0150 | Grad Norm: 0.00911491\n",
      "Epoch 2 | Step 950200 | Avg Loss: 0.0148 | Grad Norm: 0.00917566\n",
      "Epoch 2 | Step 950300 | Avg Loss: 0.0145 | Grad Norm: 0.00859421\n",
      "Epoch 2 | Step 950400 | Avg Loss: 0.0145 | Grad Norm: 0.00898152\n",
      "Epoch 2 | Step 950500 | Avg Loss: 0.0147 | Grad Norm: 0.00743894\n",
      "Epoch 2 | Step 950600 | Avg Loss: 0.0143 | Grad Norm: 0.00698064\n",
      "Epoch 2 | Step 950700 | Avg Loss: 0.0145 | Grad Norm: 0.00863718\n",
      "Epoch 2 | Step 950800 | Avg Loss: 0.0145 | Grad Norm: 0.00740555\n",
      "Epoch 2 | Step 950900 | Avg Loss: 0.0146 | Grad Norm: 0.01021833\n",
      "Epoch 2 | Step 951000 | Avg Loss: 0.0150 | Grad Norm: 0.00814117\n",
      "Epoch 2 | Step 951100 | Avg Loss: 0.0146 | Grad Norm: 0.00969306\n",
      "Epoch 2 | Step 951200 | Avg Loss: 0.0146 | Grad Norm: 0.00876761\n",
      "Epoch 2 | Step 951300 | Avg Loss: 0.0145 | Grad Norm: 0.00772600\n",
      "Epoch 2 | Step 951400 | Avg Loss: 0.0145 | Grad Norm: 0.00823163\n",
      "Epoch 2 | Step 951500 | Avg Loss: 0.0147 | Grad Norm: 0.00894395\n",
      "Epoch 2 | Step 951600 | Avg Loss: 0.0151 | Grad Norm: 0.00740702\n",
      "Epoch 2 | Step 951700 | Avg Loss: 0.0150 | Grad Norm: 0.00931961\n",
      "Epoch 2 | Step 951800 | Avg Loss: 0.0150 | Grad Norm: 0.00721617\n",
      "Epoch 2 | Step 951900 | Avg Loss: 0.0150 | Grad Norm: 0.00785562\n",
      "Epoch 2 | Step 952000 | Avg Loss: 0.0150 | Grad Norm: 0.00849711\n",
      "Epoch 2 | Step 952100 | Avg Loss: 0.0151 | Grad Norm: 0.00801777\n",
      "Epoch 2 | Step 952200 | Avg Loss: 0.0150 | Grad Norm: 0.00841910\n",
      "Epoch 2 | Step 952300 | Avg Loss: 0.0146 | Grad Norm: 0.00840260\n",
      "Epoch 2 | Step 952400 | Avg Loss: 0.0145 | Grad Norm: 0.00889054\n",
      "Epoch 2 | Step 952500 | Avg Loss: 0.0148 | Grad Norm: 0.00770206\n",
      "Epoch 2 | Step 952600 | Avg Loss: 0.0150 | Grad Norm: 0.00859577\n",
      "Epoch 2 | Step 952700 | Avg Loss: 0.0151 | Grad Norm: 0.00774214\n",
      "Epoch 2 | Step 952800 | Avg Loss: 0.0151 | Grad Norm: 0.00745325\n",
      "Epoch 2 | Step 952900 | Avg Loss: 0.0147 | Grad Norm: 0.00908989\n",
      "Epoch 2 | Step 953000 | Avg Loss: 0.0150 | Grad Norm: 0.00898970\n",
      "Epoch 2 | Step 953100 | Avg Loss: 0.0151 | Grad Norm: 0.00829045\n",
      "Epoch 2 | Step 953200 | Avg Loss: 0.0153 | Grad Norm: 0.00979499\n",
      "Epoch 2 | Step 953300 | Avg Loss: 0.0152 | Grad Norm: 0.00984964\n",
      "Epoch 2 | Step 953400 | Avg Loss: 0.0150 | Grad Norm: 0.00850785\n",
      "Epoch 2 | Step 953500 | Avg Loss: 0.0146 | Grad Norm: 0.00762943\n",
      "Epoch 2 | Step 953600 | Avg Loss: 0.0146 | Grad Norm: 0.00857263\n",
      "Epoch 2 | Step 953700 | Avg Loss: 0.0145 | Grad Norm: 0.00812246\n",
      "Epoch 2 | Step 953800 | Avg Loss: 0.0146 | Grad Norm: 0.00841924\n",
      "Epoch 2 | Step 953900 | Avg Loss: 0.0142 | Grad Norm: 0.00795652\n",
      "Epoch 2 | Step 954000 | Avg Loss: 0.0143 | Grad Norm: 0.00925481\n",
      "Epoch 2 | Step 954100 | Avg Loss: 0.0142 | Grad Norm: 0.00920908\n",
      "Epoch 2 | Step 954200 | Avg Loss: 0.0144 | Grad Norm: 0.01045887\n",
      "Epoch 2 | Step 954300 | Avg Loss: 0.0144 | Grad Norm: 0.00926179\n",
      "Epoch 2 | Step 954400 | Avg Loss: 0.0149 | Grad Norm: 0.00918071\n",
      "Epoch 2 | Step 954500 | Avg Loss: 0.0153 | Grad Norm: 0.01011254\n",
      "Epoch 2 | Step 954600 | Avg Loss: 0.0151 | Grad Norm: 0.00918019\n",
      "Epoch 2 | Step 954700 | Avg Loss: 0.0150 | Grad Norm: 0.00771318\n",
      "Epoch 2 | Step 954800 | Avg Loss: 0.0151 | Grad Norm: 0.00840408\n",
      "Epoch 2 | Step 954900 | Avg Loss: 0.0150 | Grad Norm: 0.00930395\n",
      "Epoch 2 | Step 955000 | Avg Loss: 0.0150 | Grad Norm: 0.00826573\n",
      "Epoch 2 | Step 955100 | Avg Loss: 0.0151 | Grad Norm: 0.01008075\n",
      "Epoch 2 | Step 955200 | Avg Loss: 0.0155 | Grad Norm: 0.00703603\n",
      "Epoch 2 | Step 955300 | Avg Loss: 0.0153 | Grad Norm: 0.01032981\n",
      "Epoch 2 | Step 955400 | Avg Loss: 0.0153 | Grad Norm: 0.00799897\n",
      "Epoch 2 | Step 955500 | Avg Loss: 0.0149 | Grad Norm: 0.00810699\n",
      "Epoch 2 | Step 955600 | Avg Loss: 0.0153 | Grad Norm: 0.00940291\n",
      "Epoch 2 | Step 955700 | Avg Loss: 0.0153 | Grad Norm: 0.01020553\n",
      "Epoch 2 | Step 955800 | Avg Loss: 0.0150 | Grad Norm: 0.00867838\n",
      "Epoch 2 | Step 955900 | Avg Loss: 0.0150 | Grad Norm: 0.00862900\n",
      "Epoch 2 | Step 956000 | Avg Loss: 0.0152 | Grad Norm: 0.00984799\n",
      "Epoch 2 | Step 956100 | Avg Loss: 0.0153 | Grad Norm: 0.01043643\n",
      "Epoch 2 | Step 956200 | Avg Loss: 0.0152 | Grad Norm: 0.00841079\n",
      "Epoch 2 | Step 956300 | Avg Loss: 0.0152 | Grad Norm: 0.00948013\n",
      "Epoch 2 | Step 956400 | Avg Loss: 0.0150 | Grad Norm: 0.01109860\n",
      "Epoch 2 | Step 956500 | Avg Loss: 0.0149 | Grad Norm: 0.00830732\n",
      "Epoch 2 | Step 956600 | Avg Loss: 0.0149 | Grad Norm: 0.01065128\n",
      "Epoch 2 | Step 956700 | Avg Loss: 0.0148 | Grad Norm: 0.00870438\n",
      "Epoch 2 | Step 956800 | Avg Loss: 0.0147 | Grad Norm: 0.00844113\n",
      "Epoch 2 | Step 956900 | Avg Loss: 0.0151 | Grad Norm: 0.00839713\n",
      "Epoch 2 | Step 957000 | Avg Loss: 0.0153 | Grad Norm: 0.00834906\n",
      "Epoch 2 | Step 957100 | Avg Loss: 0.0153 | Grad Norm: 0.00853897\n",
      "Epoch 2 | Step 957200 | Avg Loss: 0.0152 | Grad Norm: 0.00852092\n",
      "Epoch 2 | Step 957300 | Avg Loss: 0.0151 | Grad Norm: 0.00727729\n",
      "Epoch 2 | Step 957400 | Avg Loss: 0.0146 | Grad Norm: 0.00916711\n",
      "Epoch 2 | Step 957500 | Avg Loss: 0.0148 | Grad Norm: 0.01007144\n",
      "Epoch 2 | Step 957600 | Avg Loss: 0.0149 | Grad Norm: 0.00877778\n",
      "Epoch 2 | Step 957700 | Avg Loss: 0.0151 | Grad Norm: 0.00825274\n",
      "Epoch 2 | Step 957800 | Avg Loss: 0.0150 | Grad Norm: 0.00903912\n",
      "Epoch 2 | Step 957900 | Avg Loss: 0.0148 | Grad Norm: 0.00758278\n",
      "Epoch 2 | Step 958000 | Avg Loss: 0.0151 | Grad Norm: 0.00871922\n",
      "Epoch 2 | Step 958100 | Avg Loss: 0.0156 | Grad Norm: 0.00927097\n",
      "Epoch 2 | Step 958200 | Avg Loss: 0.0154 | Grad Norm: 0.00977069\n",
      "Epoch 2 | Step 958300 | Avg Loss: 0.0151 | Grad Norm: 0.00814135\n",
      "Epoch 2 | Step 958400 | Avg Loss: 0.0147 | Grad Norm: 0.00810594\n",
      "Epoch 2 | Step 958500 | Avg Loss: 0.0152 | Grad Norm: 0.00731441\n",
      "Epoch 2 | Step 958600 | Avg Loss: 0.0153 | Grad Norm: 0.00811096\n",
      "Epoch 2 | Step 958700 | Avg Loss: 0.0154 | Grad Norm: 0.00819382\n",
      "Epoch 2 | Step 958800 | Avg Loss: 0.0149 | Grad Norm: 0.00747845\n",
      "Epoch 2 | Step 958900 | Avg Loss: 0.0147 | Grad Norm: 0.00697052\n",
      "Epoch 2 | Step 959000 | Avg Loss: 0.0143 | Grad Norm: 0.00731073\n",
      "Epoch 2 | Step 959100 | Avg Loss: 0.0147 | Grad Norm: 0.00744322\n",
      "Epoch 2 | Step 959200 | Avg Loss: 0.0152 | Grad Norm: 0.00802410\n",
      "Epoch 2 | Step 959300 | Avg Loss: 0.0152 | Grad Norm: 0.01068235\n",
      "Epoch 2 | Step 959400 | Avg Loss: 0.0152 | Grad Norm: 0.00771408\n",
      "Epoch 2 | Step 959500 | Avg Loss: 0.0151 | Grad Norm: 0.00932455\n",
      "Epoch 2 | Step 959600 | Avg Loss: 0.0149 | Grad Norm: 0.00861868\n",
      "Epoch 2 | Step 959700 | Avg Loss: 0.0149 | Grad Norm: 0.00806618\n",
      "Epoch 2 | Step 959800 | Avg Loss: 0.0148 | Grad Norm: 0.00812716\n",
      "Epoch 2 | Step 959900 | Avg Loss: 0.0147 | Grad Norm: 0.00939762\n",
      "Epoch 2 | Step 960000 | Avg Loss: 0.0153 | Grad Norm: 0.00840992\n",
      "Epoch 2 | Step 960100 | Avg Loss: 0.0151 | Grad Norm: 0.00950799\n",
      "Epoch 2 | Step 960200 | Avg Loss: 0.0154 | Grad Norm: 0.00891536\n",
      "Epoch 2 | Step 960300 | Avg Loss: 0.0154 | Grad Norm: 0.00905077\n",
      "Epoch 2 | Step 960400 | Avg Loss: 0.0153 | Grad Norm: 0.00858370\n",
      "Epoch 2 | Step 960500 | Avg Loss: 0.0156 | Grad Norm: 0.00880294\n",
      "Epoch 2 | Step 960600 | Avg Loss: 0.0151 | Grad Norm: 0.00763420\n",
      "Epoch 2 | Step 960700 | Avg Loss: 0.0151 | Grad Norm: 0.00945145\n",
      "Epoch 2 | Step 960800 | Avg Loss: 0.0156 | Grad Norm: 0.00914868\n",
      "Epoch 2 | Step 960900 | Avg Loss: 0.0154 | Grad Norm: 0.00893156\n",
      "Epoch 2 | Step 961000 | Avg Loss: 0.0153 | Grad Norm: 0.00794707\n",
      "Epoch 2 | Step 961100 | Avg Loss: 0.0155 | Grad Norm: 0.00863644\n",
      "Epoch 2 | Step 961200 | Avg Loss: 0.0155 | Grad Norm: 0.00859292\n",
      "Epoch 2 | Step 961300 | Avg Loss: 0.0153 | Grad Norm: 0.00810881\n",
      "Epoch 2 | Step 961400 | Avg Loss: 0.0151 | Grad Norm: 0.00861541\n",
      "Epoch 2 | Step 961500 | Avg Loss: 0.0153 | Grad Norm: 0.00703033\n",
      "Epoch 2 | Step 961600 | Avg Loss: 0.0156 | Grad Norm: 0.00800355\n",
      "Epoch 2 | Step 961700 | Avg Loss: 0.0153 | Grad Norm: 0.00777690\n",
      "Epoch 2 | Step 961800 | Avg Loss: 0.0152 | Grad Norm: 0.00815134\n",
      "Epoch 2 | Step 961900 | Avg Loss: 0.0153 | Grad Norm: 0.00828019\n",
      "Epoch 2 | Step 962000 | Avg Loss: 0.0151 | Grad Norm: 0.00709823\n",
      "Epoch 2 | Step 962100 | Avg Loss: 0.0149 | Grad Norm: 0.00849664\n",
      "Epoch 2 | Step 962200 | Avg Loss: 0.0153 | Grad Norm: 0.00738771\n",
      "Epoch 2 | Step 962300 | Avg Loss: 0.0149 | Grad Norm: 0.00858334\n",
      "Epoch 2 | Step 962400 | Avg Loss: 0.0151 | Grad Norm: 0.00861547\n",
      "Epoch 2 | Step 962500 | Avg Loss: 0.0153 | Grad Norm: 0.00829029\n",
      "Epoch 2 | Step 962600 | Avg Loss: 0.0153 | Grad Norm: 0.01003595\n",
      "Epoch 2 | Step 962700 | Avg Loss: 0.0152 | Grad Norm: 0.00963510\n",
      "Epoch 2 | Step 962800 | Avg Loss: 0.0147 | Grad Norm: 0.00844745\n",
      "Epoch 2 | Step 962900 | Avg Loss: 0.0152 | Grad Norm: 0.00989028\n",
      "Epoch 2 | Step 963000 | Avg Loss: 0.0148 | Grad Norm: 0.00813827\n",
      "Epoch 2 | Step 963100 | Avg Loss: 0.0147 | Grad Norm: 0.00969436\n",
      "Epoch 2 | Step 963200 | Avg Loss: 0.0147 | Grad Norm: 0.00823965\n",
      "Epoch 2 | Step 963300 | Avg Loss: 0.0144 | Grad Norm: 0.00761900\n",
      "Epoch 2 | Step 963400 | Avg Loss: 0.0145 | Grad Norm: 0.00734604\n",
      "Epoch 2 | Step 963500 | Avg Loss: 0.0151 | Grad Norm: 0.00821850\n",
      "Epoch 2 | Step 963600 | Avg Loss: 0.0151 | Grad Norm: 0.00942509\n",
      "Epoch 2 | Step 963700 | Avg Loss: 0.0151 | Grad Norm: 0.00893240\n",
      "Epoch 2 | Step 963800 | Avg Loss: 0.0148 | Grad Norm: 0.00739048\n",
      "Epoch 2 | Step 963900 | Avg Loss: 0.0145 | Grad Norm: 0.00752169\n",
      "Epoch 2 | Step 964000 | Avg Loss: 0.0142 | Grad Norm: 0.00809849\n",
      "Epoch 2 | Step 964100 | Avg Loss: 0.0144 | Grad Norm: 0.00828644\n",
      "Epoch 2 | Step 964200 | Avg Loss: 0.0147 | Grad Norm: 0.00854518\n",
      "Epoch 2 | Step 964300 | Avg Loss: 0.0146 | Grad Norm: 0.00963957\n",
      "Epoch 2 | Step 964400 | Avg Loss: 0.0149 | Grad Norm: 0.00863095\n",
      "Epoch 2 | Step 964500 | Avg Loss: 0.0146 | Grad Norm: 0.00895318\n",
      "Epoch 2 | Step 964600 | Avg Loss: 0.0146 | Grad Norm: 0.00713837\n",
      "Epoch 2 | Step 964700 | Avg Loss: 0.0146 | Grad Norm: 0.00789470\n",
      "Epoch 2 | Step 964800 | Avg Loss: 0.0143 | Grad Norm: 0.00690726\n",
      "Epoch 2 | Step 964900 | Avg Loss: 0.0149 | Grad Norm: 0.01008012\n",
      "Epoch 2 | Step 965000 | Avg Loss: 0.0147 | Grad Norm: 0.00768850\n",
      "Epoch 2 | Step 965100 | Avg Loss: 0.0146 | Grad Norm: 0.00791311\n",
      "Epoch 2 | Step 965200 | Avg Loss: 0.0147 | Grad Norm: 0.00903858\n",
      "Epoch 2 | Step 965300 | Avg Loss: 0.0146 | Grad Norm: 0.00869951\n",
      "Epoch 2 | Step 965400 | Avg Loss: 0.0151 | Grad Norm: 0.00851247\n",
      "Epoch 2 | Step 965500 | Avg Loss: 0.0150 | Grad Norm: 0.00860229\n",
      "Epoch 2 | Step 965600 | Avg Loss: 0.0150 | Grad Norm: 0.00966055\n",
      "Epoch 2 | Step 965700 | Avg Loss: 0.0150 | Grad Norm: 0.00864283\n",
      "Epoch 2 | Step 965800 | Avg Loss: 0.0148 | Grad Norm: 0.00861059\n",
      "Epoch 2 | Step 965900 | Avg Loss: 0.0149 | Grad Norm: 0.00816401\n",
      "Epoch 2 | Step 966000 | Avg Loss: 0.0147 | Grad Norm: 0.00824597\n",
      "Epoch 2 | Step 966100 | Avg Loss: 0.0148 | Grad Norm: 0.00862506\n",
      "Epoch 2 | Step 966200 | Avg Loss: 0.0147 | Grad Norm: 0.00738744\n",
      "Epoch 2 | Step 966300 | Avg Loss: 0.0150 | Grad Norm: 0.00873651\n",
      "Epoch 2 | Step 966400 | Avg Loss: 0.0148 | Grad Norm: 0.00982156\n",
      "Epoch 2 | Step 966500 | Avg Loss: 0.0145 | Grad Norm: 0.00876774\n",
      "Epoch 2 | Step 966600 | Avg Loss: 0.0146 | Grad Norm: 0.00814993\n",
      "Epoch 2 | Step 966700 | Avg Loss: 0.0148 | Grad Norm: 0.00910154\n",
      "Epoch 2 | Step 966800 | Avg Loss: 0.0145 | Grad Norm: 0.00832820\n",
      "Epoch 2 | Step 966900 | Avg Loss: 0.0148 | Grad Norm: 0.00814656\n",
      "Epoch 2 | Step 967000 | Avg Loss: 0.0146 | Grad Norm: 0.00911089\n",
      "Epoch 2 | Step 967100 | Avg Loss: 0.0145 | Grad Norm: 0.00820744\n",
      "Epoch 2 | Step 967200 | Avg Loss: 0.0145 | Grad Norm: 0.00835550\n",
      "Epoch 2 | Step 967300 | Avg Loss: 0.0144 | Grad Norm: 0.00908838\n",
      "Epoch 2 | Step 967400 | Avg Loss: 0.0144 | Grad Norm: 0.00963324\n",
      "Epoch 2 | Step 967500 | Avg Loss: 0.0147 | Grad Norm: 0.00909697\n",
      "Epoch 2 | Step 967600 | Avg Loss: 0.0143 | Grad Norm: 0.00725820\n",
      "Epoch 2 | Step 967700 | Avg Loss: 0.0141 | Grad Norm: 0.00750773\n",
      "Epoch 2 | Step 967800 | Avg Loss: 0.0144 | Grad Norm: 0.00761854\n",
      "Epoch 2 | Step 967900 | Avg Loss: 0.0145 | Grad Norm: 0.00996112\n",
      "Epoch 2 | Step 968000 | Avg Loss: 0.0149 | Grad Norm: 0.00799415\n",
      "Epoch 2 | Step 968100 | Avg Loss: 0.0150 | Grad Norm: 0.00868868\n",
      "Epoch 2 | Step 968200 | Avg Loss: 0.0153 | Grad Norm: 0.00803312\n",
      "Epoch 2 | Step 968300 | Avg Loss: 0.0150 | Grad Norm: 0.00837467\n",
      "Epoch 2 | Step 968400 | Avg Loss: 0.0150 | Grad Norm: 0.00930538\n",
      "Epoch 2 | Step 968500 | Avg Loss: 0.0154 | Grad Norm: 0.00782431\n",
      "Epoch 2 | Step 968600 | Avg Loss: 0.0154 | Grad Norm: 0.00843522\n",
      "Epoch 2 | Step 968700 | Avg Loss: 0.0156 | Grad Norm: 0.00971019\n",
      "Epoch 2 | Step 968800 | Avg Loss: 0.0152 | Grad Norm: 0.00789631\n",
      "Epoch 2 | Step 968900 | Avg Loss: 0.0154 | Grad Norm: 0.00962861\n",
      "Epoch 2 | Step 969000 | Avg Loss: 0.0153 | Grad Norm: 0.00968841\n",
      "Epoch 2 | Step 969100 | Avg Loss: 0.0156 | Grad Norm: 0.00882529\n",
      "Epoch 2 | Step 969200 | Avg Loss: 0.0160 | Grad Norm: 0.00758350\n",
      "Epoch 2 | Step 969300 | Avg Loss: 0.0154 | Grad Norm: 0.00863439\n",
      "Epoch 2 | Step 969400 | Avg Loss: 0.0154 | Grad Norm: 0.01006711\n",
      "Epoch 2 | Step 969500 | Avg Loss: 0.0152 | Grad Norm: 0.00828957\n",
      "Epoch 2 | Step 969600 | Avg Loss: 0.0150 | Grad Norm: 0.00876613\n",
      "Epoch 2 | Step 969700 | Avg Loss: 0.0151 | Grad Norm: 0.00841250\n",
      "Epoch 2 | Step 969800 | Avg Loss: 0.0152 | Grad Norm: 0.00881925\n",
      "Epoch 2 | Step 969900 | Avg Loss: 0.0155 | Grad Norm: 0.01077710\n",
      "Epoch 2 | Step 970000 | Avg Loss: 0.0152 | Grad Norm: 0.00730915\n",
      "Epoch 2 | Step 970100 | Avg Loss: 0.0149 | Grad Norm: 0.01060202\n",
      "Epoch 2 | Step 970200 | Avg Loss: 0.0149 | Grad Norm: 0.00858929\n",
      "Epoch 2 | Step 970300 | Avg Loss: 0.0151 | Grad Norm: 0.00986217\n",
      "Epoch 2 | Step 970400 | Avg Loss: 0.0156 | Grad Norm: 0.00992235\n",
      "Epoch 2 | Step 970500 | Avg Loss: 0.0155 | Grad Norm: 0.00811568\n",
      "Epoch 2 | Step 970600 | Avg Loss: 0.0153 | Grad Norm: 0.01006641\n",
      "Epoch 2 | Step 970700 | Avg Loss: 0.0149 | Grad Norm: 0.01297080\n",
      "Epoch 2 | Step 970800 | Avg Loss: 0.0147 | Grad Norm: 0.00792567\n",
      "Epoch 2 | Step 970900 | Avg Loss: 0.0148 | Grad Norm: 0.00826948\n",
      "Epoch 2 | Step 971000 | Avg Loss: 0.0148 | Grad Norm: 0.00777735\n",
      "Epoch 2 | Step 971100 | Avg Loss: 0.0147 | Grad Norm: 0.00759676\n",
      "Epoch 2 | Step 971200 | Avg Loss: 0.0145 | Grad Norm: 0.00842489\n",
      "Epoch 2 | Step 971300 | Avg Loss: 0.0146 | Grad Norm: 0.00781530\n",
      "Epoch 2 | Step 971400 | Avg Loss: 0.0147 | Grad Norm: 0.00943018\n",
      "Epoch 2 | Step 971500 | Avg Loss: 0.0149 | Grad Norm: 0.00724838\n",
      "Epoch 2 | Step 971600 | Avg Loss: 0.0154 | Grad Norm: 0.00982209\n",
      "Epoch 2 | Step 971700 | Avg Loss: 0.0154 | Grad Norm: 0.00809908\n",
      "Epoch 2 | Step 971800 | Avg Loss: 0.0151 | Grad Norm: 0.00817397\n",
      "Epoch 2 | Step 971900 | Avg Loss: 0.0147 | Grad Norm: 0.00824115\n",
      "Epoch 2 | Step 972000 | Avg Loss: 0.0147 | Grad Norm: 0.00801314\n",
      "Epoch 2 | Step 972100 | Avg Loss: 0.0145 | Grad Norm: 0.00867345\n",
      "Epoch 2 | Step 972200 | Avg Loss: 0.0144 | Grad Norm: 0.00799841\n",
      "Epoch 2 | Step 972300 | Avg Loss: 0.0148 | Grad Norm: 0.01061759\n",
      "Epoch 2 | Step 972400 | Avg Loss: 0.0147 | Grad Norm: 0.00963927\n",
      "Epoch 2 | Step 972500 | Avg Loss: 0.0146 | Grad Norm: 0.00963450\n",
      "Epoch 2 | Step 972600 | Avg Loss: 0.0144 | Grad Norm: 0.00854746\n",
      "Epoch 2 | Step 972700 | Avg Loss: 0.0149 | Grad Norm: 0.00891976\n",
      "Epoch 2 | Step 972800 | Avg Loss: 0.0149 | Grad Norm: 0.00760196\n",
      "Epoch 2 | Step 972900 | Avg Loss: 0.0149 | Grad Norm: 0.00890311\n",
      "Epoch 2 | Step 973000 | Avg Loss: 0.0149 | Grad Norm: 0.00879045\n",
      "Epoch 2 | Step 973100 | Avg Loss: 0.0148 | Grad Norm: 0.00787015\n",
      "Epoch 2 | Step 973200 | Avg Loss: 0.0152 | Grad Norm: 0.00851859\n",
      "Epoch 2 | Step 973300 | Avg Loss: 0.0152 | Grad Norm: 0.00750146\n",
      "Epoch 2 | Step 973400 | Avg Loss: 0.0152 | Grad Norm: 0.00897840\n",
      "Epoch 2 | Step 973500 | Avg Loss: 0.0149 | Grad Norm: 0.00823541\n",
      "Epoch 2 | Step 973600 | Avg Loss: 0.0150 | Grad Norm: 0.00745710\n",
      "Epoch 2 | Step 973700 | Avg Loss: 0.0153 | Grad Norm: 0.00817307\n",
      "Epoch 2 | Step 973800 | Avg Loss: 0.0154 | Grad Norm: 0.00793189\n",
      "Epoch 2 | Step 973900 | Avg Loss: 0.0148 | Grad Norm: 0.00997144\n",
      "Epoch 2 | Step 974000 | Avg Loss: 0.0147 | Grad Norm: 0.00891035\n",
      "Epoch 2 | Step 974100 | Avg Loss: 0.0151 | Grad Norm: 0.00835771\n",
      "Epoch 2 | Step 974200 | Avg Loss: 0.0154 | Grad Norm: 0.00886216\n",
      "Epoch 2 | Step 974300 | Avg Loss: 0.0154 | Grad Norm: 0.00878868\n",
      "Epoch 2 | Step 974400 | Avg Loss: 0.0151 | Grad Norm: 0.00900539\n",
      "Epoch 2 | Step 974500 | Avg Loss: 0.0155 | Grad Norm: 0.00877060\n",
      "Epoch 2 | Step 974600 | Avg Loss: 0.0154 | Grad Norm: 0.00838800\n",
      "Epoch 2 | Step 974700 | Avg Loss: 0.0152 | Grad Norm: 0.00850535\n",
      "Epoch 2 | Step 974800 | Avg Loss: 0.0154 | Grad Norm: 0.00803802\n",
      "Epoch 2 | Step 974900 | Avg Loss: 0.0156 | Grad Norm: 0.00992577\n",
      "Epoch 2 | Step 975000 | Avg Loss: 0.0149 | Grad Norm: 0.00749807\n",
      "Epoch 2 | Step 975100 | Avg Loss: 0.0148 | Grad Norm: 0.00853182\n",
      "Epoch 2 | Step 975200 | Avg Loss: 0.0151 | Grad Norm: 0.00878037\n",
      "Epoch 2 | Step 975300 | Avg Loss: 0.0153 | Grad Norm: 0.01012370\n",
      "Epoch 2 | Step 975400 | Avg Loss: 0.0153 | Grad Norm: 0.00854480\n",
      "Epoch 2 | Step 975500 | Avg Loss: 0.0150 | Grad Norm: 0.00807793\n",
      "Epoch 2 | Step 975600 | Avg Loss: 0.0148 | Grad Norm: 0.00875065\n",
      "Epoch 2 | Step 975700 | Avg Loss: 0.0145 | Grad Norm: 0.00952495\n",
      "Epoch 2 | Step 975800 | Avg Loss: 0.0147 | Grad Norm: 0.00951530\n",
      "Epoch 2 | Step 975900 | Avg Loss: 0.0149 | Grad Norm: 0.00825565\n",
      "Epoch 2 | Step 976000 | Avg Loss: 0.0147 | Grad Norm: 0.00839839\n",
      "Epoch 2 | Step 976100 | Avg Loss: 0.0151 | Grad Norm: 0.00841587\n",
      "Epoch 2 | Step 976200 | Avg Loss: 0.0151 | Grad Norm: 0.00897779\n",
      "Epoch 2 | Step 976300 | Avg Loss: 0.0150 | Grad Norm: 0.01074281\n",
      "Epoch 2 | Step 976400 | Avg Loss: 0.0152 | Grad Norm: 0.00854798\n",
      "Epoch 2 | Step 976500 | Avg Loss: 0.0152 | Grad Norm: 0.00797876\n",
      "Epoch 2 | Step 976600 | Avg Loss: 0.0155 | Grad Norm: 0.00923399\n",
      "Epoch 2 | Step 976700 | Avg Loss: 0.0156 | Grad Norm: 0.00810906\n",
      "Epoch 2 | Step 976800 | Avg Loss: 0.0154 | Grad Norm: 0.00713281\n",
      "Epoch 2 | Step 976900 | Avg Loss: 0.0151 | Grad Norm: 0.00794099\n",
      "Epoch 2 | Step 977000 | Avg Loss: 0.0153 | Grad Norm: 0.00805907\n",
      "Epoch 2 | Step 977100 | Avg Loss: 0.0151 | Grad Norm: 0.00711319\n",
      "Epoch 2 | Step 977200 | Avg Loss: 0.0152 | Grad Norm: 0.00897167\n",
      "Epoch 2 | Step 977300 | Avg Loss: 0.0150 | Grad Norm: 0.00836693\n",
      "Epoch 2 | Step 977400 | Avg Loss: 0.0152 | Grad Norm: 0.00832901\n",
      "Epoch 2 | Step 977500 | Avg Loss: 0.0150 | Grad Norm: 0.00828342\n",
      "Epoch 2 | Step 977600 | Avg Loss: 0.0150 | Grad Norm: 0.00989259\n",
      "Epoch 2 | Step 977700 | Avg Loss: 0.0148 | Grad Norm: 0.00910097\n",
      "Epoch 2 | Step 977800 | Avg Loss: 0.0146 | Grad Norm: 0.00889611\n",
      "Epoch 2 | Step 977900 | Avg Loss: 0.0146 | Grad Norm: 0.00795202\n",
      "Epoch 2 | Step 978000 | Avg Loss: 0.0147 | Grad Norm: 0.00714048\n",
      "Epoch 2 | Step 978100 | Avg Loss: 0.0148 | Grad Norm: 0.00942954\n",
      "Epoch 2 | Step 978200 | Avg Loss: 0.0150 | Grad Norm: 0.00868123\n",
      "Epoch 2 | Step 978300 | Avg Loss: 0.0150 | Grad Norm: 0.00868477\n",
      "Epoch 2 | Step 978400 | Avg Loss: 0.0150 | Grad Norm: 0.00694893\n",
      "Epoch 2 | Step 978500 | Avg Loss: 0.0149 | Grad Norm: 0.00862161\n",
      "Epoch 2 | Step 978600 | Avg Loss: 0.0146 | Grad Norm: 0.00769453\n",
      "Epoch 2 | Step 978700 | Avg Loss: 0.0145 | Grad Norm: 0.00844882\n",
      "Epoch 2 | Step 978800 | Avg Loss: 0.0147 | Grad Norm: 0.00795930\n",
      "Epoch 2 | Step 978900 | Avg Loss: 0.0152 | Grad Norm: 0.00817685\n",
      "Epoch 2 | Step 979000 | Avg Loss: 0.0151 | Grad Norm: 0.00885345\n",
      "Epoch 2 | Step 979100 | Avg Loss: 0.0153 | Grad Norm: 0.00809767\n",
      "Epoch 2 | Step 979200 | Avg Loss: 0.0157 | Grad Norm: 0.00947640\n",
      "Epoch 2 | Step 979300 | Avg Loss: 0.0153 | Grad Norm: 0.00923458\n",
      "Epoch 2 | Step 979400 | Avg Loss: 0.0149 | Grad Norm: 0.00767414\n",
      "Epoch 2 | Step 979500 | Avg Loss: 0.0149 | Grad Norm: 0.00830572\n",
      "Epoch 2 | Step 979600 | Avg Loss: 0.0148 | Grad Norm: 0.00824252\n",
      "Epoch 2 | Step 979700 | Avg Loss: 0.0150 | Grad Norm: 0.00871004\n",
      "Epoch 2 | Step 979800 | Avg Loss: 0.0148 | Grad Norm: 0.00697603\n",
      "Epoch 2 | Step 979900 | Avg Loss: 0.0150 | Grad Norm: 0.00799423\n",
      "Epoch 2 | Step 980000 | Avg Loss: 0.0147 | Grad Norm: 0.00929017\n",
      "Epoch 2 | Step 980100 | Avg Loss: 0.0151 | Grad Norm: 0.00894736\n",
      "Epoch 2 | Step 980200 | Avg Loss: 0.0151 | Grad Norm: 0.00858825\n",
      "Epoch 2 | Step 980300 | Avg Loss: 0.0150 | Grad Norm: 0.00842072\n",
      "Epoch 2 | Step 980400 | Avg Loss: 0.0150 | Grad Norm: 0.00790238\n",
      "Epoch 2 | Step 980500 | Avg Loss: 0.0152 | Grad Norm: 0.01004027\n",
      "Epoch 2 | Step 980600 | Avg Loss: 0.0153 | Grad Norm: 0.00831060\n",
      "Epoch 2 | Step 980700 | Avg Loss: 0.0154 | Grad Norm: 0.00815830\n",
      "Epoch 2 | Step 980800 | Avg Loss: 0.0154 | Grad Norm: 0.00766376\n",
      "Epoch 2 | Step 980900 | Avg Loss: 0.0156 | Grad Norm: 0.00886277\n",
      "Epoch 2 | Step 981000 | Avg Loss: 0.0152 | Grad Norm: 0.00900874\n",
      "Epoch 2 | Step 981100 | Avg Loss: 0.0147 | Grad Norm: 0.00961937\n",
      "Epoch 2 | Step 981200 | Avg Loss: 0.0149 | Grad Norm: 0.00988452\n",
      "Epoch 2 | Step 981300 | Avg Loss: 0.0147 | Grad Norm: 0.00885617\n",
      "Epoch 2 | Step 981400 | Avg Loss: 0.0146 | Grad Norm: 0.00889375\n",
      "Epoch 2 | Step 981500 | Avg Loss: 0.0144 | Grad Norm: 0.00866598\n",
      "Epoch 2 | Step 981600 | Avg Loss: 0.0147 | Grad Norm: 0.00790843\n",
      "Epoch 2 | Step 981700 | Avg Loss: 0.0147 | Grad Norm: 0.00846613\n",
      "Epoch 2 | Step 981800 | Avg Loss: 0.0149 | Grad Norm: 0.00947023\n",
      "Epoch 2 | Step 981900 | Avg Loss: 0.0149 | Grad Norm: 0.00902984\n",
      "Epoch 2 | Step 982000 | Avg Loss: 0.0150 | Grad Norm: 0.00862765\n",
      "Epoch 2 | Step 982100 | Avg Loss: 0.0152 | Grad Norm: 0.00856263\n",
      "Epoch 2 | Step 982200 | Avg Loss: 0.0151 | Grad Norm: 0.00869334\n",
      "Epoch 2 | Step 982300 | Avg Loss: 0.0151 | Grad Norm: 0.00897213\n",
      "Epoch 2 | Step 982400 | Avg Loss: 0.0153 | Grad Norm: 0.00921993\n",
      "Epoch 2 | Step 982500 | Avg Loss: 0.0153 | Grad Norm: 0.00776901\n",
      "Epoch 2 | Step 982600 | Avg Loss: 0.0152 | Grad Norm: 0.00801839\n",
      "Epoch 2 | Step 982700 | Avg Loss: 0.0148 | Grad Norm: 0.00781636\n",
      "Epoch 2 | Step 982800 | Avg Loss: 0.0148 | Grad Norm: 0.00945955\n",
      "Epoch 2 | Step 982900 | Avg Loss: 0.0150 | Grad Norm: 0.00821340\n",
      "Epoch 2 | Step 983000 | Avg Loss: 0.0146 | Grad Norm: 0.00854037\n",
      "Epoch 2 | Step 983100 | Avg Loss: 0.0147 | Grad Norm: 0.00942022\n",
      "Epoch 2 | Step 983200 | Avg Loss: 0.0146 | Grad Norm: 0.00962146\n",
      "Epoch 2 | Step 983300 | Avg Loss: 0.0147 | Grad Norm: 0.00892890\n",
      "Epoch 2 | Step 983400 | Avg Loss: 0.0146 | Grad Norm: 0.00835797\n",
      "Epoch 2 | Step 983500 | Avg Loss: 0.0149 | Grad Norm: 0.00778715\n",
      "Epoch 2 | Step 983600 | Avg Loss: 0.0147 | Grad Norm: 0.00783700\n",
      "Epoch 2 | Step 983700 | Avg Loss: 0.0148 | Grad Norm: 0.00868367\n",
      "Epoch 2 | Step 983800 | Avg Loss: 0.0151 | Grad Norm: 0.00999850\n",
      "Epoch 2 | Step 983900 | Avg Loss: 0.0148 | Grad Norm: 0.00845391\n",
      "Epoch 2 | Step 984000 | Avg Loss: 0.0155 | Grad Norm: 0.00797234\n",
      "Epoch 2 | Step 984100 | Avg Loss: 0.0158 | Grad Norm: 0.00894127\n",
      "Epoch 2 | Step 984200 | Avg Loss: 0.0155 | Grad Norm: 0.00827778\n",
      "Epoch 2 | Step 984300 | Avg Loss: 0.0153 | Grad Norm: 0.00984317\n",
      "Epoch 2 | Step 984400 | Avg Loss: 0.0153 | Grad Norm: 0.01020794\n",
      "Epoch 2 | Step 984500 | Avg Loss: 0.0152 | Grad Norm: 0.00798554\n",
      "Epoch 2 | Step 984600 | Avg Loss: 0.0149 | Grad Norm: 0.00774395\n",
      "Epoch 2 | Step 984700 | Avg Loss: 0.0152 | Grad Norm: 0.00719049\n",
      "Epoch 2 | Step 984800 | Avg Loss: 0.0150 | Grad Norm: 0.00797560\n",
      "Epoch 2 | Step 984900 | Avg Loss: 0.0152 | Grad Norm: 0.01007041\n",
      "Epoch 2 | Step 985000 | Avg Loss: 0.0156 | Grad Norm: 0.00798638\n",
      "Epoch 2 | Step 985100 | Avg Loss: 0.0151 | Grad Norm: 0.00790375\n",
      "Epoch 2 | Step 985200 | Avg Loss: 0.0146 | Grad Norm: 0.00855921\n",
      "Epoch 2 | Step 985300 | Avg Loss: 0.0150 | Grad Norm: 0.01085790\n",
      "Epoch 2 | Step 985400 | Avg Loss: 0.0152 | Grad Norm: 0.00999627\n",
      "Epoch 2 | Step 985500 | Avg Loss: 0.0152 | Grad Norm: 0.00871199\n",
      "Epoch 2 | Step 985600 | Avg Loss: 0.0150 | Grad Norm: 0.01146454\n",
      "Epoch 2 | Step 985700 | Avg Loss: 0.0150 | Grad Norm: 0.00896867\n",
      "Epoch 2 | Step 985800 | Avg Loss: 0.0150 | Grad Norm: 0.00950703\n",
      "Epoch 2 | Step 985900 | Avg Loss: 0.0152 | Grad Norm: 0.00916854\n",
      "Epoch 2 | Step 986000 | Avg Loss: 0.0153 | Grad Norm: 0.00812907\n",
      "Epoch 2 | Step 986100 | Avg Loss: 0.0146 | Grad Norm: 0.00750217\n",
      "Epoch 2 | Step 986200 | Avg Loss: 0.0149 | Grad Norm: 0.00824984\n",
      "Epoch 2 | Step 986300 | Avg Loss: 0.0150 | Grad Norm: 0.00892874\n",
      "Epoch 2 | Step 986400 | Avg Loss: 0.0146 | Grad Norm: 0.00743520\n",
      "Epoch 2 | Step 986500 | Avg Loss: 0.0148 | Grad Norm: 0.00785871\n",
      "Epoch 2 | Step 986600 | Avg Loss: 0.0149 | Grad Norm: 0.00985158\n",
      "Epoch 2 | Step 986700 | Avg Loss: 0.0149 | Grad Norm: 0.00871202\n",
      "Epoch 2 | Step 986800 | Avg Loss: 0.0147 | Grad Norm: 0.00758388\n",
      "Epoch 2 | Step 986900 | Avg Loss: 0.0146 | Grad Norm: 0.00826224\n",
      "Epoch 2 | Step 987000 | Avg Loss: 0.0152 | Grad Norm: 0.00969179\n",
      "Epoch 2 | Step 987100 | Avg Loss: 0.0151 | Grad Norm: 0.01034278\n",
      "Epoch 2 | Step 987200 | Avg Loss: 0.0151 | Grad Norm: 0.00803360\n",
      "Epoch 2 | Step 987300 | Avg Loss: 0.0152 | Grad Norm: 0.00931184\n",
      "Epoch 2 | Step 987400 | Avg Loss: 0.0154 | Grad Norm: 0.00919320\n",
      "Epoch 2 | Step 987500 | Avg Loss: 0.0151 | Grad Norm: 0.00880403\n",
      "Epoch 2 | Step 987600 | Avg Loss: 0.0151 | Grad Norm: 0.00876882\n",
      "Epoch 2 | Step 987700 | Avg Loss: 0.0151 | Grad Norm: 0.00918939\n",
      "Epoch 2 | Step 987800 | Avg Loss: 0.0150 | Grad Norm: 0.00687372\n",
      "Epoch 2 | Step 987900 | Avg Loss: 0.0153 | Grad Norm: 0.00860796\n",
      "Epoch 2 | Step 988000 | Avg Loss: 0.0155 | Grad Norm: 0.01009014\n",
      "Epoch 2 | Step 988100 | Avg Loss: 0.0154 | Grad Norm: 0.00777488\n",
      "Epoch 2 | Step 988200 | Avg Loss: 0.0153 | Grad Norm: 0.00883567\n",
      "Epoch 2 | Step 988300 | Avg Loss: 0.0155 | Grad Norm: 0.01010732\n",
      "Epoch 2 | Step 988400 | Avg Loss: 0.0153 | Grad Norm: 0.00827582\n",
      "Epoch 2 | Step 988500 | Avg Loss: 0.0153 | Grad Norm: 0.00745614\n",
      "Epoch 2 | Step 988600 | Avg Loss: 0.0152 | Grad Norm: 0.00903804\n",
      "Epoch 2 | Step 988700 | Avg Loss: 0.0151 | Grad Norm: 0.01051062\n",
      "Epoch 2 | Step 988800 | Avg Loss: 0.0147 | Grad Norm: 0.00843768\n",
      "Epoch 2 | Step 988900 | Avg Loss: 0.0147 | Grad Norm: 0.00935961\n",
      "Epoch 2 | Step 989000 | Avg Loss: 0.0148 | Grad Norm: 0.00966746\n",
      "Epoch 2 | Step 989100 | Avg Loss: 0.0152 | Grad Norm: 0.00929463\n",
      "Epoch 2 | Step 989200 | Avg Loss: 0.0148 | Grad Norm: 0.00804376\n",
      "Epoch 2 | Step 989300 | Avg Loss: 0.0151 | Grad Norm: 0.01012151\n",
      "Epoch 2 | Step 989400 | Avg Loss: 0.0153 | Grad Norm: 0.00868831\n",
      "Epoch 2 | Step 989500 | Avg Loss: 0.0155 | Grad Norm: 0.00890750\n",
      "Epoch 2 | Step 989600 | Avg Loss: 0.0153 | Grad Norm: 0.00919178\n",
      "Epoch 2 | Step 989700 | Avg Loss: 0.0153 | Grad Norm: 0.00771549\n",
      "Epoch 2 | Step 989800 | Avg Loss: 0.0155 | Grad Norm: 0.00861223\n",
      "Epoch 2 | Step 989900 | Avg Loss: 0.0156 | Grad Norm: 0.00979893\n",
      "Epoch 2 | Step 990000 | Avg Loss: 0.0151 | Grad Norm: 0.00955223\n",
      "Epoch 2 | Step 990100 | Avg Loss: 0.0152 | Grad Norm: 0.01020776\n",
      "Epoch 2 | Step 990200 | Avg Loss: 0.0153 | Grad Norm: 0.00858951\n",
      "Epoch 2 | Step 990300 | Avg Loss: 0.0152 | Grad Norm: 0.00906964\n",
      "Epoch 2 | Step 990400 | Avg Loss: 0.0154 | Grad Norm: 0.00959879\n",
      "Epoch 2 | Step 990500 | Avg Loss: 0.0151 | Grad Norm: 0.01012035\n",
      "Epoch 2 | Step 990600 | Avg Loss: 0.0153 | Grad Norm: 0.00859218\n",
      "Epoch 2 | Step 990700 | Avg Loss: 0.0154 | Grad Norm: 0.00826892\n",
      "Epoch 2 | Step 990800 | Avg Loss: 0.0148 | Grad Norm: 0.00855947\n",
      "Epoch 2 | Step 990900 | Avg Loss: 0.0148 | Grad Norm: 0.00796965\n",
      "Epoch 2 | Step 991000 | Avg Loss: 0.0149 | Grad Norm: 0.00986016\n",
      "Epoch 2 | Step 991100 | Avg Loss: 0.0150 | Grad Norm: 0.00833264\n",
      "Epoch 2 | Step 991200 | Avg Loss: 0.0152 | Grad Norm: 0.00741309\n",
      "Epoch 2 | Step 991300 | Avg Loss: 0.0150 | Grad Norm: 0.00976820\n",
      "Epoch 2 | Step 991400 | Avg Loss: 0.0150 | Grad Norm: 0.00797458\n",
      "Epoch 2 | Step 991500 | Avg Loss: 0.0154 | Grad Norm: 0.01039173\n",
      "Epoch 2 | Step 991600 | Avg Loss: 0.0152 | Grad Norm: 0.00890799\n",
      "Epoch 2 | Step 991700 | Avg Loss: 0.0151 | Grad Norm: 0.00914311\n",
      "Epoch 2 | Step 991800 | Avg Loss: 0.0150 | Grad Norm: 0.00987320\n",
      "Epoch 2 | Step 991900 | Avg Loss: 0.0152 | Grad Norm: 0.00852269\n",
      "Epoch 2 | Step 992000 | Avg Loss: 0.0151 | Grad Norm: 0.00798666\n",
      "Epoch 2 | Step 992100 | Avg Loss: 0.0149 | Grad Norm: 0.00857308\n",
      "Epoch 2 | Step 992200 | Avg Loss: 0.0145 | Grad Norm: 0.00805304\n",
      "Epoch 2 | Step 992300 | Avg Loss: 0.0142 | Grad Norm: 0.00901895\n",
      "Epoch 2 | Step 992400 | Avg Loss: 0.0144 | Grad Norm: 0.00899851\n",
      "Epoch 2 | Step 992500 | Avg Loss: 0.0145 | Grad Norm: 0.00874430\n",
      "Epoch 2 | Step 992600 | Avg Loss: 0.0145 | Grad Norm: 0.00832083\n",
      "Epoch 2 | Step 992700 | Avg Loss: 0.0147 | Grad Norm: 0.00889111\n",
      "Epoch 2 | Step 992800 | Avg Loss: 0.0144 | Grad Norm: 0.00928534\n",
      "Epoch 2 | Step 992900 | Avg Loss: 0.0147 | Grad Norm: 0.00844093\n",
      "Epoch 2 | Step 993000 | Avg Loss: 0.0146 | Grad Norm: 0.00821836\n",
      "Epoch 2 | Step 993100 | Avg Loss: 0.0146 | Grad Norm: 0.00791679\n",
      "Epoch 2 | Step 993200 | Avg Loss: 0.0154 | Grad Norm: 0.00757694\n",
      "Epoch 2 | Step 993300 | Avg Loss: 0.0159 | Grad Norm: 0.01005503\n",
      "Epoch 2 | Step 993400 | Avg Loss: 0.0154 | Grad Norm: 0.00909626\n",
      "Epoch 2 | Step 993500 | Avg Loss: 0.0153 | Grad Norm: 0.01044621\n",
      "Epoch 2 | Step 993600 | Avg Loss: 0.0153 | Grad Norm: 0.00766961\n",
      "Epoch 2 | Step 993700 | Avg Loss: 0.0151 | Grad Norm: 0.00853092\n",
      "Epoch 2 | Step 993800 | Avg Loss: 0.0154 | Grad Norm: 0.00844479\n",
      "Epoch 2 | Step 993900 | Avg Loss: 0.0151 | Grad Norm: 0.00877021\n",
      "Epoch 2 | Step 994000 | Avg Loss: 0.0152 | Grad Norm: 0.00874950\n",
      "Epoch 2 | Step 994100 | Avg Loss: 0.0149 | Grad Norm: 0.00777977\n",
      "Epoch 2 | Step 994200 | Avg Loss: 0.0150 | Grad Norm: 0.00873528\n",
      "Epoch 2 | Step 994300 | Avg Loss: 0.0150 | Grad Norm: 0.00823950\n",
      "Epoch 2 | Step 994400 | Avg Loss: 0.0145 | Grad Norm: 0.00885213\n",
      "Epoch 2 | Step 994500 | Avg Loss: 0.0146 | Grad Norm: 0.00822993\n",
      "Epoch 2 | Step 994600 | Avg Loss: 0.0145 | Grad Norm: 0.00770002\n",
      "Epoch 2 | Step 994700 | Avg Loss: 0.0143 | Grad Norm: 0.00852012\n",
      "Epoch 2 | Step 994800 | Avg Loss: 0.0148 | Grad Norm: 0.00776502\n",
      "Epoch 2 | Step 994900 | Avg Loss: 0.0148 | Grad Norm: 0.00800000\n",
      "Epoch 2 | Step 995000 | Avg Loss: 0.0150 | Grad Norm: 0.00836493\n",
      "Epoch 2 | Step 995100 | Avg Loss: 0.0150 | Grad Norm: 0.00854729\n",
      "Epoch 2 | Step 995200 | Avg Loss: 0.0148 | Grad Norm: 0.00807267\n",
      "Epoch 2 | Step 995300 | Avg Loss: 0.0150 | Grad Norm: 0.00874644\n",
      "Epoch 2 | Step 995400 | Avg Loss: 0.0150 | Grad Norm: 0.00830414\n",
      "Epoch 2 | Step 995500 | Avg Loss: 0.0154 | Grad Norm: 0.00839419\n",
      "Epoch 2 | Step 995600 | Avg Loss: 0.0153 | Grad Norm: 0.00998140\n",
      "Epoch 2 | Step 995700 | Avg Loss: 0.0151 | Grad Norm: 0.00793966\n",
      "Epoch 2 | Step 995800 | Avg Loss: 0.0150 | Grad Norm: 0.00913639\n",
      "Epoch 2 | Step 995900 | Avg Loss: 0.0143 | Grad Norm: 0.00973724\n",
      "Epoch 2 | Step 996000 | Avg Loss: 0.0143 | Grad Norm: 0.00807019\n",
      "Epoch 2 | Step 996100 | Avg Loss: 0.0146 | Grad Norm: 0.00663362\n",
      "Epoch 2 | Step 996200 | Avg Loss: 0.0149 | Grad Norm: 0.01003264\n",
      "Epoch 2 | Step 996300 | Avg Loss: 0.0150 | Grad Norm: 0.00824749\n",
      "Epoch 2 | Step 996400 | Avg Loss: 0.0149 | Grad Norm: 0.00780835\n",
      "Epoch 2 | Step 996500 | Avg Loss: 0.0147 | Grad Norm: 0.00880530\n",
      "Epoch 2 | Step 996600 | Avg Loss: 0.0148 | Grad Norm: 0.00844158\n",
      "Epoch 2 | Step 996700 | Avg Loss: 0.0148 | Grad Norm: 0.01032286\n",
      "Epoch 2 | Step 996800 | Avg Loss: 0.0149 | Grad Norm: 0.00825720\n",
      "Epoch 2 | Step 996900 | Avg Loss: 0.0150 | Grad Norm: 0.01085759\n",
      "Epoch 2 | Step 997000 | Avg Loss: 0.0148 | Grad Norm: 0.00892402\n",
      "Epoch 2 | Step 997100 | Avg Loss: 0.0144 | Grad Norm: 0.00800294\n",
      "Epoch 2 | Step 997200 | Avg Loss: 0.0145 | Grad Norm: 0.00813939\n",
      "Epoch 2 | Step 997300 | Avg Loss: 0.0146 | Grad Norm: 0.00858942\n",
      "Epoch 2 | Step 997400 | Avg Loss: 0.0147 | Grad Norm: 0.00642774\n",
      "Epoch 2 | Step 997500 | Avg Loss: 0.0148 | Grad Norm: 0.00868124\n",
      "Epoch 2 | Step 997600 | Avg Loss: 0.0149 | Grad Norm: 0.00754970\n",
      "Epoch 2 | Step 997700 | Avg Loss: 0.0146 | Grad Norm: 0.00832441\n",
      "Epoch 2 | Step 997800 | Avg Loss: 0.0149 | Grad Norm: 0.00948865\n",
      "Epoch 2 | Step 997900 | Avg Loss: 0.0147 | Grad Norm: 0.00830795\n",
      "Epoch 2 | Step 998000 | Avg Loss: 0.0148 | Grad Norm: 0.00881753\n",
      "Epoch 2 | Step 998100 | Avg Loss: 0.0147 | Grad Norm: 0.00845448\n",
      "Epoch 2 | Step 998200 | Avg Loss: 0.0149 | Grad Norm: 0.00778038\n",
      "Epoch 2 | Step 998300 | Avg Loss: 0.0150 | Grad Norm: 0.00809277\n",
      "Epoch 2 | Step 998400 | Avg Loss: 0.0152 | Grad Norm: 0.00884695\n",
      "Epoch 2 | Step 998500 | Avg Loss: 0.0154 | Grad Norm: 0.00922230\n",
      "Epoch 2 | Step 998600 | Avg Loss: 0.0154 | Grad Norm: 0.00950764\n",
      "Epoch 2 | Step 998700 | Avg Loss: 0.0154 | Grad Norm: 0.00861128\n",
      "Epoch 2 | Step 998800 | Avg Loss: 0.0152 | Grad Norm: 0.00828923\n",
      "Epoch 2 | Step 998900 | Avg Loss: 0.0155 | Grad Norm: 0.00809136\n",
      "Epoch 2 | Step 999000 | Avg Loss: 0.0155 | Grad Norm: 0.00960681\n",
      "Epoch 2 | Step 999100 | Avg Loss: 0.0158 | Grad Norm: 0.00921266\n",
      "Epoch 2 | Step 999200 | Avg Loss: 0.0154 | Grad Norm: 0.00846299\n",
      "Epoch 2 | Step 999300 | Avg Loss: 0.0152 | Grad Norm: 0.00873642\n",
      "Epoch 2 | Step 999400 | Avg Loss: 0.0150 | Grad Norm: 0.00806600\n",
      "Epoch 2 | Step 999500 | Avg Loss: 0.0150 | Grad Norm: 0.00871406\n",
      "Epoch 2 | Step 999600 | Avg Loss: 0.0150 | Grad Norm: 0.00768600\n",
      "Epoch 2 | Step 999700 | Avg Loss: 0.0153 | Grad Norm: 0.00830783\n",
      "Epoch 2 | Step 999800 | Avg Loss: 0.0147 | Grad Norm: 0.01044775\n",
      "Epoch 2 | Step 999900 | Avg Loss: 0.0148 | Grad Norm: 0.00885914\n",
      "Epoch 2 | Step 1000000 | Avg Loss: 0.0147 | Grad Norm: 0.00773103\n",
      "Saving model at step1000000\n",
      "Epoch 2 | Step 1000100 | Avg Loss: 0.0151 | Grad Norm: 0.00883505\n",
      "Epoch 2 | Step 1000200 | Avg Loss: 0.0152 | Grad Norm: 0.00876078\n",
      "Epoch 2 | Step 1000300 | Avg Loss: 0.0154 | Grad Norm: 0.00904287\n",
      "Epoch 2 | Step 1000400 | Avg Loss: 0.0154 | Grad Norm: 0.00868016\n",
      "Epoch 2 | Step 1000500 | Avg Loss: 0.0155 | Grad Norm: 0.00815344\n",
      "Epoch 2 | Step 1000600 | Avg Loss: 0.0150 | Grad Norm: 0.00861657\n",
      "Epoch 2 | Step 1000700 | Avg Loss: 0.0153 | Grad Norm: 0.01055896\n",
      "Epoch 2 | Step 1000800 | Avg Loss: 0.0149 | Grad Norm: 0.00903863\n",
      "Epoch 2 | Step 1000900 | Avg Loss: 0.0148 | Grad Norm: 0.00830440\n",
      "Epoch 2 | Step 1001000 | Avg Loss: 0.0150 | Grad Norm: 0.00864429\n",
      "Epoch 2 | Step 1001100 | Avg Loss: 0.0153 | Grad Norm: 0.00812500\n",
      "Epoch 2 | Step 1001200 | Avg Loss: 0.0151 | Grad Norm: 0.00822168\n",
      "Epoch 2 | Step 1001300 | Avg Loss: 0.0150 | Grad Norm: 0.00796338\n",
      "Epoch 2 | Step 1001400 | Avg Loss: 0.0147 | Grad Norm: 0.00776814\n",
      "Epoch 2 | Step 1001500 | Avg Loss: 0.0145 | Grad Norm: 0.01083006\n",
      "Epoch 2 | Step 1001600 | Avg Loss: 0.0150 | Grad Norm: 0.01445658\n",
      "Epoch 2 | Step 1001700 | Avg Loss: 0.0148 | Grad Norm: 0.00851719\n",
      "Epoch 2 | Step 1001800 | Avg Loss: 0.0147 | Grad Norm: 0.00831015\n",
      "Epoch 2 | Step 1001900 | Avg Loss: 0.0145 | Grad Norm: 0.00890323\n",
      "Epoch 2 | Step 1002000 | Avg Loss: 0.0146 | Grad Norm: 0.00741529\n",
      "Epoch 2 | Step 1002100 | Avg Loss: 0.0145 | Grad Norm: 0.00814409\n",
      "Epoch 2 | Step 1002200 | Avg Loss: 0.0147 | Grad Norm: 0.00839450\n",
      "Epoch 2 | Step 1002300 | Avg Loss: 0.0147 | Grad Norm: 0.00732401\n",
      "Epoch 2 | Step 1002400 | Avg Loss: 0.0147 | Grad Norm: 0.00785696\n",
      "Epoch 2 | Step 1002500 | Avg Loss: 0.0147 | Grad Norm: 0.00949590\n",
      "Epoch 2 | Step 1002600 | Avg Loss: 0.0151 | Grad Norm: 0.00811107\n",
      "Epoch 2 | Step 1002700 | Avg Loss: 0.0152 | Grad Norm: 0.00910808\n",
      "Epoch 2 | Step 1002800 | Avg Loss: 0.0150 | Grad Norm: 0.00921968\n",
      "Epoch 2 | Step 1002900 | Avg Loss: 0.0152 | Grad Norm: 0.00773439\n",
      "Epoch 2 | Step 1003000 | Avg Loss: 0.0156 | Grad Norm: 0.00752352\n",
      "Epoch 2 | Step 1003100 | Avg Loss: 0.0153 | Grad Norm: 0.01089188\n",
      "Epoch 2 | Step 1003200 | Avg Loss: 0.0152 | Grad Norm: 0.00882559\n",
      "Epoch 2 | Step 1003300 | Avg Loss: 0.0151 | Grad Norm: 0.00830292\n",
      "Epoch 2 | Step 1003400 | Avg Loss: 0.0148 | Grad Norm: 0.00852690\n",
      "Epoch 2 | Step 1003500 | Avg Loss: 0.0147 | Grad Norm: 0.00913032\n",
      "Epoch 2 | Step 1003600 | Avg Loss: 0.0148 | Grad Norm: 0.00901279\n",
      "Epoch 2 | Step 1003700 | Avg Loss: 0.0149 | Grad Norm: 0.00927832\n",
      "Epoch 2 | Step 1003800 | Avg Loss: 0.0145 | Grad Norm: 0.00697317\n",
      "Epoch 2 | Step 1003900 | Avg Loss: 0.0145 | Grad Norm: 0.00796113\n",
      "Epoch 2 | Step 1004000 | Avg Loss: 0.0145 | Grad Norm: 0.00794579\n",
      "Epoch 2 | Step 1004100 | Avg Loss: 0.0151 | Grad Norm: 0.00785366\n",
      "Epoch 2 | Step 1004200 | Avg Loss: 0.0152 | Grad Norm: 0.00755793\n",
      "Epoch 2 | Step 1004300 | Avg Loss: 0.0152 | Grad Norm: 0.00769464\n",
      "Epoch 2 | Step 1004400 | Avg Loss: 0.0152 | Grad Norm: 0.00792192\n",
      "Epoch 2 | Step 1004500 | Avg Loss: 0.0148 | Grad Norm: 0.00988436\n",
      "Epoch 2 | Step 1004600 | Avg Loss: 0.0145 | Grad Norm: 0.00886904\n",
      "Epoch 2 | Step 1004700 | Avg Loss: 0.0148 | Grad Norm: 0.00842886\n",
      "Epoch 2 | Step 1004800 | Avg Loss: 0.0148 | Grad Norm: 0.00775235\n",
      "Epoch 2 | Step 1004900 | Avg Loss: 0.0151 | Grad Norm: 0.00809949\n",
      "Epoch 2 | Step 1005000 | Avg Loss: 0.0148 | Grad Norm: 0.00951199\n",
      "Epoch 2 | Step 1005100 | Avg Loss: 0.0144 | Grad Norm: 0.00862689\n",
      "Epoch 2 | Step 1005200 | Avg Loss: 0.0145 | Grad Norm: 0.01029117\n",
      "Epoch 2 | Step 1005300 | Avg Loss: 0.0146 | Grad Norm: 0.00816121\n",
      "Epoch 2 | Step 1005400 | Avg Loss: 0.0147 | Grad Norm: 0.00746291\n",
      "Epoch 2 | Step 1005500 | Avg Loss: 0.0151 | Grad Norm: 0.00923427\n",
      "Epoch 2 | Step 1005600 | Avg Loss: 0.0151 | Grad Norm: 0.00797332\n",
      "Epoch 2 | Step 1005700 | Avg Loss: 0.0150 | Grad Norm: 0.00855189\n",
      "Epoch 2 | Step 1005800 | Avg Loss: 0.0150 | Grad Norm: 0.00823737\n",
      "Epoch 2 | Step 1005900 | Avg Loss: 0.0152 | Grad Norm: 0.00912988\n",
      "Epoch 2 | Step 1006000 | Avg Loss: 0.0153 | Grad Norm: 0.00863673\n",
      "Epoch 2 | Step 1006100 | Avg Loss: 0.0153 | Grad Norm: 0.00829406\n",
      "Epoch 2 | Step 1006200 | Avg Loss: 0.0154 | Grad Norm: 0.00855743\n",
      "Epoch 2 | Step 1006300 | Avg Loss: 0.0156 | Grad Norm: 0.00884330\n",
      "Epoch 2 | Step 1006400 | Avg Loss: 0.0148 | Grad Norm: 0.00858633\n",
      "Epoch 2 | Step 1006500 | Avg Loss: 0.0146 | Grad Norm: 0.00893159\n",
      "Epoch 2 | Step 1006600 | Avg Loss: 0.0147 | Grad Norm: 0.01029794\n",
      "Epoch 2 | Step 1006700 | Avg Loss: 0.0151 | Grad Norm: 0.00817708\n",
      "Epoch 2 | Step 1006800 | Avg Loss: 0.0151 | Grad Norm: 0.00941779\n",
      "Epoch 2 | Step 1006900 | Avg Loss: 0.0153 | Grad Norm: 0.00997643\n",
      "Epoch 2 | Step 1007000 | Avg Loss: 0.0153 | Grad Norm: 0.01304431\n",
      "Epoch 2 | Step 1007100 | Avg Loss: 0.0153 | Grad Norm: 0.00905403\n",
      "Epoch 2 | Step 1007200 | Avg Loss: 0.0152 | Grad Norm: 0.00840572\n",
      "Epoch 2 | Step 1007300 | Avg Loss: 0.0149 | Grad Norm: 0.00988484\n",
      "Epoch 2 | Step 1007400 | Avg Loss: 0.0151 | Grad Norm: 0.00771285\n",
      "Epoch 2 | Step 1007500 | Avg Loss: 0.0150 | Grad Norm: 0.00949171\n",
      "Epoch 2 | Step 1007600 | Avg Loss: 0.0148 | Grad Norm: 0.00758882\n",
      "Epoch 2 | Step 1007700 | Avg Loss: 0.0149 | Grad Norm: 0.00837711\n",
      "Epoch 2 | Step 1007800 | Avg Loss: 0.0152 | Grad Norm: 0.00810995\n",
      "Epoch 2 | Step 1007900 | Avg Loss: 0.0149 | Grad Norm: 0.00860305\n",
      "Epoch 2 | Step 1008000 | Avg Loss: 0.0148 | Grad Norm: 0.01140069\n",
      "Epoch 2 | Step 1008100 | Avg Loss: 0.0146 | Grad Norm: 0.00801172\n",
      "Epoch 2 | Step 1008200 | Avg Loss: 0.0143 | Grad Norm: 0.00879661\n",
      "Epoch 2 | Step 1008300 | Avg Loss: 0.0145 | Grad Norm: 0.01026266\n",
      "Epoch 2 | Step 1008400 | Avg Loss: 0.0145 | Grad Norm: 0.00901062\n",
      "Epoch 2 | Step 1008500 | Avg Loss: 0.0148 | Grad Norm: 0.00822956\n",
      "Epoch 2 | Step 1008600 | Avg Loss: 0.0148 | Grad Norm: 0.00897956\n",
      "Epoch 2 | Step 1008700 | Avg Loss: 0.0154 | Grad Norm: 0.00916553\n",
      "Epoch 2 | Step 1008800 | Avg Loss: 0.0153 | Grad Norm: 0.01016848\n",
      "Epoch 2 | Step 1008900 | Avg Loss: 0.0154 | Grad Norm: 0.00838960\n",
      "Epoch 2 | Step 1009000 | Avg Loss: 0.0153 | Grad Norm: 0.00788801\n",
      "Epoch 2 | Step 1009100 | Avg Loss: 0.0152 | Grad Norm: 0.00881186\n",
      "Epoch 2 | Step 1009200 | Avg Loss: 0.0151 | Grad Norm: 0.00859484\n",
      "Epoch 2 | Step 1009300 | Avg Loss: 0.0150 | Grad Norm: 0.00777960\n",
      "Epoch 2 | Step 1009400 | Avg Loss: 0.0152 | Grad Norm: 0.00747765\n",
      "Epoch 2 | Step 1009500 | Avg Loss: 0.0152 | Grad Norm: 0.00892263\n",
      "Epoch 2 | Step 1009600 | Avg Loss: 0.0151 | Grad Norm: 0.00831749\n",
      "Epoch 2 | Step 1009700 | Avg Loss: 0.0153 | Grad Norm: 0.00865134\n",
      "Epoch 2 | Step 1009800 | Avg Loss: 0.0152 | Grad Norm: 0.00863934\n",
      "Epoch 2 | Step 1009900 | Avg Loss: 0.0152 | Grad Norm: 0.00833472\n",
      "Epoch 2 | Step 1010000 | Avg Loss: 0.0151 | Grad Norm: 0.01013076\n",
      "Epoch 2 | Step 1010100 | Avg Loss: 0.0148 | Grad Norm: 0.00865486\n",
      "Epoch 2 | Step 1010200 | Avg Loss: 0.0147 | Grad Norm: 0.00746320\n",
      "Epoch 2 | Step 1010300 | Avg Loss: 0.0147 | Grad Norm: 0.00682101\n",
      "Epoch 2 | Step 1010400 | Avg Loss: 0.0152 | Grad Norm: 0.01054609\n",
      "Epoch 2 | Step 1010500 | Avg Loss: 0.0151 | Grad Norm: 0.00827247\n",
      "Epoch 2 | Step 1010600 | Avg Loss: 0.0148 | Grad Norm: 0.00800618\n",
      "Epoch 2 | Step 1010700 | Avg Loss: 0.0144 | Grad Norm: 0.00798533\n",
      "Epoch 2 | Step 1010800 | Avg Loss: 0.0149 | Grad Norm: 0.01035862\n",
      "Epoch 2 | Step 1010900 | Avg Loss: 0.0149 | Grad Norm: 0.00776542\n",
      "Epoch 2 | Step 1011000 | Avg Loss: 0.0148 | Grad Norm: 0.00751766\n",
      "Epoch 2 | Step 1011100 | Avg Loss: 0.0148 | Grad Norm: 0.00982549\n",
      "Epoch 2 | Step 1011200 | Avg Loss: 0.0145 | Grad Norm: 0.00988584\n",
      "Epoch 2 | Step 1011300 | Avg Loss: 0.0148 | Grad Norm: 0.00933616\n",
      "Epoch 2 | Step 1011400 | Avg Loss: 0.0144 | Grad Norm: 0.00849765\n",
      "Epoch 2 | Step 1011500 | Avg Loss: 0.0144 | Grad Norm: 0.00902020\n",
      "Epoch 2 | Step 1011600 | Avg Loss: 0.0144 | Grad Norm: 0.00839511\n",
      "Epoch 2 | Step 1011700 | Avg Loss: 0.0140 | Grad Norm: 0.00890146\n",
      "Epoch 2 | Step 1011800 | Avg Loss: 0.0142 | Grad Norm: 0.00862470\n",
      "Epoch 2 | Step 1011900 | Avg Loss: 0.0150 | Grad Norm: 0.00907889\n",
      "Epoch 2 | Step 1012000 | Avg Loss: 0.0148 | Grad Norm: 0.00779312\n",
      "Epoch 2 | Step 1012100 | Avg Loss: 0.0145 | Grad Norm: 0.00867324\n",
      "Epoch 2 | Step 1012200 | Avg Loss: 0.0144 | Grad Norm: 0.00823639\n",
      "Epoch 2 | Step 1012300 | Avg Loss: 0.0145 | Grad Norm: 0.00772436\n",
      "Epoch 2 | Step 1012400 | Avg Loss: 0.0146 | Grad Norm: 0.00719673\n",
      "Epoch 2 | Step 1012500 | Avg Loss: 0.0148 | Grad Norm: 0.00946764\n",
      "Epoch 2 | Step 1012600 | Avg Loss: 0.0150 | Grad Norm: 0.00849121\n",
      "Epoch 2 | Step 1012700 | Avg Loss: 0.0151 | Grad Norm: 0.00766827\n",
      "Epoch 2 | Step 1012800 | Avg Loss: 0.0153 | Grad Norm: 0.00957530\n",
      "Epoch 2 | Step 1012900 | Avg Loss: 0.0150 | Grad Norm: 0.00880777\n",
      "Epoch 2 | Step 1013000 | Avg Loss: 0.0154 | Grad Norm: 0.00735369\n",
      "Epoch 2 | Step 1013100 | Avg Loss: 0.0151 | Grad Norm: 0.00811062\n",
      "Epoch 2 | Step 1013200 | Avg Loss: 0.0152 | Grad Norm: 0.01022732\n",
      "Epoch 2 | Step 1013300 | Avg Loss: 0.0150 | Grad Norm: 0.00726960\n",
      "Epoch 2 | Step 1013400 | Avg Loss: 0.0150 | Grad Norm: 0.00708483\n",
      "Epoch 2 | Step 1013500 | Avg Loss: 0.0150 | Grad Norm: 0.00775477\n",
      "Epoch 2 | Step 1013600 | Avg Loss: 0.0148 | Grad Norm: 0.00818087\n",
      "Epoch 2 | Step 1013700 | Avg Loss: 0.0146 | Grad Norm: 0.00682094\n",
      "Epoch 2 | Step 1013800 | Avg Loss: 0.0147 | Grad Norm: 0.00862913\n",
      "Epoch 2 | Step 1013900 | Avg Loss: 0.0148 | Grad Norm: 0.00786036\n",
      "Epoch 2 | Step 1014000 | Avg Loss: 0.0150 | Grad Norm: 0.00848181\n",
      "Epoch 2 | Step 1014100 | Avg Loss: 0.0150 | Grad Norm: 0.00998421\n",
      "Epoch 2 | Step 1014200 | Avg Loss: 0.0152 | Grad Norm: 0.00860341\n",
      "Epoch 2 | Step 1014300 | Avg Loss: 0.0156 | Grad Norm: 0.00831314\n",
      "Epoch 2 | Step 1014400 | Avg Loss: 0.0155 | Grad Norm: 0.00915635\n",
      "Epoch 2 | Step 1014500 | Avg Loss: 0.0152 | Grad Norm: 0.00860353\n",
      "Epoch 2 | Step 1014600 | Avg Loss: 0.0147 | Grad Norm: 0.01008339\n",
      "Epoch 2 | Step 1014700 | Avg Loss: 0.0146 | Grad Norm: 0.00865871\n",
      "Epoch 2 | Step 1014800 | Avg Loss: 0.0154 | Grad Norm: 0.01000337\n",
      "Epoch 2 | Step 1014900 | Avg Loss: 0.0152 | Grad Norm: 0.00873098\n",
      "Epoch 2 | Step 1015000 | Avg Loss: 0.0147 | Grad Norm: 0.00933135\n",
      "Epoch 2 | Step 1015100 | Avg Loss: 0.0147 | Grad Norm: 0.00889258\n",
      "Epoch 2 | Step 1015200 | Avg Loss: 0.0150 | Grad Norm: 0.00878696\n",
      "Epoch 2 | Step 1015300 | Avg Loss: 0.0147 | Grad Norm: 0.00817263\n",
      "Epoch 2 | Step 1015400 | Avg Loss: 0.0151 | Grad Norm: 0.00824440\n",
      "Epoch 2 | Step 1015500 | Avg Loss: 0.0152 | Grad Norm: 0.00916614\n",
      "Epoch 2 | Step 1015600 | Avg Loss: 0.0149 | Grad Norm: 0.00767165\n",
      "Epoch 2 | Step 1015700 | Avg Loss: 0.0150 | Grad Norm: 0.00719130\n",
      "Epoch 2 | Step 1015800 | Avg Loss: 0.0147 | Grad Norm: 0.00905005\n",
      "Epoch 2 | Step 1015900 | Avg Loss: 0.0149 | Grad Norm: 0.00746533\n",
      "Epoch 2 | Step 1016000 | Avg Loss: 0.0150 | Grad Norm: 0.00922008\n",
      "Epoch 2 | Step 1016100 | Avg Loss: 0.0151 | Grad Norm: 0.00784595\n",
      "Epoch 2 | Step 1016200 | Avg Loss: 0.0151 | Grad Norm: 0.00942086\n",
      "Epoch 2 | Step 1016300 | Avg Loss: 0.0147 | Grad Norm: 0.00738018\n",
      "Epoch 2 | Step 1016400 | Avg Loss: 0.0150 | Grad Norm: 0.00848737\n",
      "Epoch 2 | Step 1016500 | Avg Loss: 0.0146 | Grad Norm: 0.00853059\n",
      "Epoch 2 | Step 1016600 | Avg Loss: 0.0149 | Grad Norm: 0.00754813\n",
      "Epoch 2 | Step 1016700 | Avg Loss: 0.0147 | Grad Norm: 0.00719967\n",
      "Epoch 2 | Step 1016800 | Avg Loss: 0.0148 | Grad Norm: 0.00879007\n",
      "Epoch 2 | Step 1016900 | Avg Loss: 0.0149 | Grad Norm: 0.00798878\n",
      "Epoch 2 | Step 1017000 | Avg Loss: 0.0146 | Grad Norm: 0.00885959\n",
      "Epoch 2 | Step 1017100 | Avg Loss: 0.0148 | Grad Norm: 0.00938820\n",
      "Epoch 2 | Step 1017200 | Avg Loss: 0.0150 | Grad Norm: 0.01013902\n",
      "Epoch 2 | Step 1017300 | Avg Loss: 0.0149 | Grad Norm: 0.00855816\n",
      "Epoch 2 | Step 1017400 | Avg Loss: 0.0149 | Grad Norm: 0.00788538\n",
      "Epoch 2 | Step 1017500 | Avg Loss: 0.0148 | Grad Norm: 0.00864775\n",
      "Epoch 2 | Step 1017600 | Avg Loss: 0.0150 | Grad Norm: 0.00783663\n",
      "Epoch 2 | Step 1017700 | Avg Loss: 0.0150 | Grad Norm: 0.00822651\n",
      "Epoch 2 | Step 1017800 | Avg Loss: 0.0148 | Grad Norm: 0.00845500\n",
      "Epoch 2 | Step 1017900 | Avg Loss: 0.0147 | Grad Norm: 0.00715516\n",
      "Epoch 2 | Step 1018000 | Avg Loss: 0.0145 | Grad Norm: 0.01089423\n",
      "Epoch 2 | Step 1018100 | Avg Loss: 0.0149 | Grad Norm: 0.00845761\n",
      "Epoch 2 | Step 1018200 | Avg Loss: 0.0149 | Grad Norm: 0.00794347\n",
      "Epoch 2 | Step 1018300 | Avg Loss: 0.0149 | Grad Norm: 0.00884604\n",
      "Epoch 2 | Step 1018400 | Avg Loss: 0.0150 | Grad Norm: 0.00843136\n",
      "Epoch 2 | Step 1018500 | Avg Loss: 0.0151 | Grad Norm: 0.00952017\n",
      "Epoch 2 | Step 1018600 | Avg Loss: 0.0152 | Grad Norm: 0.00866270\n",
      "Epoch 2 | Step 1018700 | Avg Loss: 0.0151 | Grad Norm: 0.01086414\n",
      "Epoch 2 | Step 1018800 | Avg Loss: 0.0151 | Grad Norm: 0.00984898\n",
      "Epoch 2 | Step 1018900 | Avg Loss: 0.0149 | Grad Norm: 0.00815827\n",
      "Epoch 2 | Step 1019000 | Avg Loss: 0.0149 | Grad Norm: 0.00789322\n",
      "Epoch 2 | Step 1019100 | Avg Loss: 0.0149 | Grad Norm: 0.00705065\n",
      "Epoch 2 | Step 1019200 | Avg Loss: 0.0147 | Grad Norm: 0.00828218\n",
      "Epoch 2 | Step 1019300 | Avg Loss: 0.0151 | Grad Norm: 0.00808909\n",
      "Epoch 2 | Step 1019400 | Avg Loss: 0.0154 | Grad Norm: 0.00769319\n",
      "Epoch 2 | Step 1019500 | Avg Loss: 0.0151 | Grad Norm: 0.00905804\n",
      "Epoch 2 | Step 1019600 | Avg Loss: 0.0146 | Grad Norm: 0.00870174\n",
      "Epoch 2 | Step 1019700 | Avg Loss: 0.0147 | Grad Norm: 0.00808768\n",
      "Epoch 2 | Step 1019800 | Avg Loss: 0.0150 | Grad Norm: 0.00800412\n",
      "Epoch 2 | Step 1019900 | Avg Loss: 0.0149 | Grad Norm: 0.00781899\n",
      "Epoch 2 | Step 1020000 | Avg Loss: 0.0151 | Grad Norm: 0.00836066\n",
      "Epoch 2 | Step 1020100 | Avg Loss: 0.0152 | Grad Norm: 0.00755067\n",
      "Epoch 2 | Step 1020200 | Avg Loss: 0.0151 | Grad Norm: 0.00842118\n",
      "Epoch 2 | Step 1020300 | Avg Loss: 0.0148 | Grad Norm: 0.00778707\n",
      "Epoch 2 | Step 1020400 | Avg Loss: 0.0147 | Grad Norm: 0.00722877\n",
      "Epoch 2 | Step 1020500 | Avg Loss: 0.0144 | Grad Norm: 0.00961063\n",
      "Epoch 2 | Step 1020600 | Avg Loss: 0.0146 | Grad Norm: 0.00931156\n",
      "Epoch 2 | Step 1020700 | Avg Loss: 0.0148 | Grad Norm: 0.01082478\n",
      "Epoch 2 | Step 1020800 | Avg Loss: 0.0150 | Grad Norm: 0.00835364\n",
      "Epoch 2 | Step 1020900 | Avg Loss: 0.0151 | Grad Norm: 0.00773325\n",
      "Epoch 2 | Step 1021000 | Avg Loss: 0.0148 | Grad Norm: 0.00846756\n",
      "Epoch 2 | Step 1021100 | Avg Loss: 0.0147 | Grad Norm: 0.00901340\n",
      "Epoch 2 | Step 1021200 | Avg Loss: 0.0149 | Grad Norm: 0.00837618\n",
      "Epoch 2 | Step 1021300 | Avg Loss: 0.0150 | Grad Norm: 0.00780879\n",
      "Epoch 2 | Step 1021400 | Avg Loss: 0.0146 | Grad Norm: 0.00820624\n",
      "Epoch 2 | Step 1021500 | Avg Loss: 0.0148 | Grad Norm: 0.00792211\n",
      "Epoch 2 | Step 1021600 | Avg Loss: 0.0153 | Grad Norm: 0.00794877\n",
      "Epoch 2 | Step 1021700 | Avg Loss: 0.0151 | Grad Norm: 0.00859921\n",
      "Epoch 2 | Step 1021800 | Avg Loss: 0.0152 | Grad Norm: 0.00738452\n",
      "Epoch 2 | Step 1021900 | Avg Loss: 0.0151 | Grad Norm: 0.00844061\n",
      "Epoch 2 | Step 1022000 | Avg Loss: 0.0152 | Grad Norm: 0.00879020\n",
      "Epoch 2 | Step 1022100 | Avg Loss: 0.0151 | Grad Norm: 0.00833389\n",
      "Epoch 2 | Step 1022200 | Avg Loss: 0.0152 | Grad Norm: 0.00796814\n",
      "Epoch 2 | Step 1022300 | Avg Loss: 0.0152 | Grad Norm: 0.00824531\n",
      "Epoch 2 | Step 1022400 | Avg Loss: 0.0152 | Grad Norm: 0.01007839\n",
      "Epoch 2 | Step 1022500 | Avg Loss: 0.0151 | Grad Norm: 0.00791102\n",
      "Epoch 2 | Step 1022600 | Avg Loss: 0.0152 | Grad Norm: 0.00711551\n",
      "Epoch 2 | Step 1022700 | Avg Loss: 0.0153 | Grad Norm: 0.00875579\n",
      "Epoch 2 | Step 1022800 | Avg Loss: 0.0150 | Grad Norm: 0.00786139\n",
      "Epoch 2 | Step 1022900 | Avg Loss: 0.0151 | Grad Norm: 0.00936378\n",
      "Epoch 2 | Step 1023000 | Avg Loss: 0.0152 | Grad Norm: 0.00843515\n",
      "Epoch 2 | Step 1023100 | Avg Loss: 0.0154 | Grad Norm: 0.00955678\n",
      "Epoch 2 | Step 1023200 | Avg Loss: 0.0156 | Grad Norm: 0.00806263\n",
      "Epoch 2 | Step 1023300 | Avg Loss: 0.0154 | Grad Norm: 0.00791377\n",
      "Epoch 2 | Step 1023400 | Avg Loss: 0.0151 | Grad Norm: 0.00768915\n",
      "Epoch 2 | Step 1023500 | Avg Loss: 0.0149 | Grad Norm: 0.00889790\n",
      "Epoch 2 | Step 1023600 | Avg Loss: 0.0147 | Grad Norm: 0.00787674\n",
      "Epoch 2 | Step 1023700 | Avg Loss: 0.0148 | Grad Norm: 0.00753542\n",
      "Epoch 2 | Step 1023800 | Avg Loss: 0.0147 | Grad Norm: 0.00877487\n",
      "Epoch 2 | Step 1023900 | Avg Loss: 0.0148 | Grad Norm: 0.00897830\n",
      "Epoch 2 | Step 1024000 | Avg Loss: 0.0151 | Grad Norm: 0.00995309\n",
      "Epoch 2 | Step 1024100 | Avg Loss: 0.0150 | Grad Norm: 0.01092211\n",
      "Epoch 2 | Step 1024200 | Avg Loss: 0.0145 | Grad Norm: 0.00908217\n",
      "Epoch 2 | Step 1024300 | Avg Loss: 0.0142 | Grad Norm: 0.01031665\n",
      "Epoch 2 | Step 1024400 | Avg Loss: 0.0145 | Grad Norm: 0.00785932\n",
      "Epoch 2 | Step 1024500 | Avg Loss: 0.0145 | Grad Norm: 0.01078494\n",
      "Epoch 2 | Step 1024600 | Avg Loss: 0.0148 | Grad Norm: 0.01063888\n",
      "Epoch 2 | Step 1024700 | Avg Loss: 0.0150 | Grad Norm: 0.00792549\n",
      "Epoch 2 | Step 1024800 | Avg Loss: 0.0151 | Grad Norm: 0.00816688\n",
      "Epoch 2 | Step 1024900 | Avg Loss: 0.0151 | Grad Norm: 0.00952499\n",
      "Epoch 2 | Step 1025000 | Avg Loss: 0.0145 | Grad Norm: 0.00899800\n",
      "Epoch 2 | Step 1025100 | Avg Loss: 0.0146 | Grad Norm: 0.00769888\n",
      "Epoch 2 | Step 1025200 | Avg Loss: 0.0151 | Grad Norm: 0.00874091\n",
      "Epoch 2 | Step 1025300 | Avg Loss: 0.0148 | Grad Norm: 0.00798599\n",
      "Epoch 2 | Step 1025400 | Avg Loss: 0.0150 | Grad Norm: 0.00737618\n",
      "Epoch 2 | Step 1025500 | Avg Loss: 0.0148 | Grad Norm: 0.00814346\n",
      "Epoch 2 | Step 1025600 | Avg Loss: 0.0150 | Grad Norm: 0.00776006\n",
      "Epoch 2 | Step 1025700 | Avg Loss: 0.0148 | Grad Norm: 0.00828329\n",
      "Epoch 2 | Step 1025800 | Avg Loss: 0.0146 | Grad Norm: 0.00933612\n",
      "Epoch 2 | Step 1025900 | Avg Loss: 0.0144 | Grad Norm: 0.00670119\n",
      "Epoch 2 | Step 1026000 | Avg Loss: 0.0145 | Grad Norm: 0.00701297\n",
      "Epoch 2 | Step 1026100 | Avg Loss: 0.0146 | Grad Norm: 0.01047951\n",
      "Epoch 2 | Step 1026200 | Avg Loss: 0.0146 | Grad Norm: 0.00857206\n",
      "Epoch 2 | Step 1026300 | Avg Loss: 0.0146 | Grad Norm: 0.00857220\n",
      "Epoch 2 | Step 1026400 | Avg Loss: 0.0152 | Grad Norm: 0.00850074\n",
      "Epoch 2 | Step 1026500 | Avg Loss: 0.0146 | Grad Norm: 0.00741371\n",
      "Epoch 2 | Step 1026600 | Avg Loss: 0.0148 | Grad Norm: 0.00742313\n",
      "Epoch 2 | Step 1026700 | Avg Loss: 0.0150 | Grad Norm: 0.00781028\n",
      "Epoch 2 | Step 1026800 | Avg Loss: 0.0150 | Grad Norm: 0.00692515\n",
      "Epoch 2 | Step 1026900 | Avg Loss: 0.0148 | Grad Norm: 0.00889735\n",
      "Epoch 2 | Step 1027000 | Avg Loss: 0.0150 | Grad Norm: 0.00852486\n",
      "Epoch 2 | Step 1027100 | Avg Loss: 0.0154 | Grad Norm: 0.00846451\n",
      "Epoch 2 | Step 1027200 | Avg Loss: 0.0152 | Grad Norm: 0.01071611\n",
      "Epoch 2 | Step 1027300 | Avg Loss: 0.0151 | Grad Norm: 0.00776885\n",
      "Epoch 2 | Step 1027400 | Avg Loss: 0.0150 | Grad Norm: 0.00746659\n",
      "Epoch 2 | Step 1027500 | Avg Loss: 0.0149 | Grad Norm: 0.00864694\n",
      "Epoch 2 | Step 1027600 | Avg Loss: 0.0149 | Grad Norm: 0.00872193\n",
      "Epoch 2 | Step 1027700 | Avg Loss: 0.0148 | Grad Norm: 0.00822874\n",
      "Epoch 2 | Step 1027800 | Avg Loss: 0.0149 | Grad Norm: 0.00958552\n",
      "Epoch 2 | Step 1027900 | Avg Loss: 0.0151 | Grad Norm: 0.00960606\n",
      "Epoch 2 | Step 1028000 | Avg Loss: 0.0151 | Grad Norm: 0.00847866\n",
      "Epoch 2 | Step 1028100 | Avg Loss: 0.0145 | Grad Norm: 0.00785063\n",
      "Epoch 2 | Step 1028200 | Avg Loss: 0.0146 | Grad Norm: 0.00765217\n",
      "Epoch 2 | Step 1028300 | Avg Loss: 0.0148 | Grad Norm: 0.00850704\n",
      "Epoch 2 | Step 1028400 | Avg Loss: 0.0151 | Grad Norm: 0.00854458\n",
      "Epoch 2 | Step 1028500 | Avg Loss: 0.0151 | Grad Norm: 0.00802687\n",
      "Epoch 2 | Step 1028600 | Avg Loss: 0.0147 | Grad Norm: 0.00849234\n",
      "Epoch 2 | Step 1028700 | Avg Loss: 0.0146 | Grad Norm: 0.00779290\n",
      "Epoch 2 | Step 1028800 | Avg Loss: 0.0148 | Grad Norm: 0.00639109\n",
      "Epoch 2 | Step 1028900 | Avg Loss: 0.0151 | Grad Norm: 0.00980917\n",
      "Epoch 2 | Step 1029000 | Avg Loss: 0.0153 | Grad Norm: 0.00900775\n",
      "Epoch 2 | Step 1029100 | Avg Loss: 0.0155 | Grad Norm: 0.00770951\n",
      "Epoch 2 | Step 1029200 | Avg Loss: 0.0153 | Grad Norm: 0.00883219\n",
      "Epoch 2 | Step 1029300 | Avg Loss: 0.0156 | Grad Norm: 0.00852638\n",
      "Epoch 2 | Step 1029400 | Avg Loss: 0.0153 | Grad Norm: 0.00817002\n",
      "Epoch 2 | Step 1029500 | Avg Loss: 0.0155 | Grad Norm: 0.00798268\n",
      "Epoch 2 | Step 1029600 | Avg Loss: 0.0149 | Grad Norm: 0.00730049\n",
      "Epoch 2 | Step 1029700 | Avg Loss: 0.0147 | Grad Norm: 0.00995990\n",
      "Epoch 2 | Step 1029800 | Avg Loss: 0.0147 | Grad Norm: 0.00760819\n",
      "Epoch 2 | Step 1029900 | Avg Loss: 0.0150 | Grad Norm: 0.00969423\n",
      "Epoch 2 | Step 1030000 | Avg Loss: 0.0153 | Grad Norm: 0.01173891\n",
      "Epoch 2 | Step 1030100 | Avg Loss: 0.0153 | Grad Norm: 0.00838862\n",
      "Epoch 2 | Step 1030200 | Avg Loss: 0.0152 | Grad Norm: 0.00773188\n",
      "Epoch 2 | Step 1030300 | Avg Loss: 0.0151 | Grad Norm: 0.00989564\n",
      "Epoch 2 | Step 1030400 | Avg Loss: 0.0149 | Grad Norm: 0.00854926\n",
      "Epoch 2 | Step 1030500 | Avg Loss: 0.0148 | Grad Norm: 0.00815892\n",
      "Epoch 2 | Step 1030600 | Avg Loss: 0.0145 | Grad Norm: 0.00953788\n",
      "Epoch 2 | Step 1030700 | Avg Loss: 0.0147 | Grad Norm: 0.00830326\n",
      "Epoch 2 | Step 1030800 | Avg Loss: 0.0149 | Grad Norm: 0.00862604\n",
      "Epoch 2 | Step 1030900 | Avg Loss: 0.0151 | Grad Norm: 0.00816450\n",
      "Epoch 2 | Step 1031000 | Avg Loss: 0.0147 | Grad Norm: 0.00886199\n",
      "Epoch 2 | Step 1031100 | Avg Loss: 0.0146 | Grad Norm: 0.00801565\n",
      "Epoch 2 | Step 1031200 | Avg Loss: 0.0147 | Grad Norm: 0.01122167\n",
      "Epoch 2 | Step 1031300 | Avg Loss: 0.0147 | Grad Norm: 0.00747786\n",
      "Epoch 2 | Step 1031400 | Avg Loss: 0.0146 | Grad Norm: 0.00932236\n",
      "Epoch 2 | Step 1031500 | Avg Loss: 0.0146 | Grad Norm: 0.01268619\n",
      "Epoch 2 | Step 1031600 | Avg Loss: 0.0141 | Grad Norm: 0.00839187\n",
      "Epoch 2 | Step 1031700 | Avg Loss: 0.0139 | Grad Norm: 0.00910543\n",
      "Epoch 2 | Step 1031800 | Avg Loss: 0.0143 | Grad Norm: 0.00818970\n",
      "Epoch 2 | Step 1031900 | Avg Loss: 0.0145 | Grad Norm: 0.00717365\n",
      "Epoch 2 | Step 1032000 | Avg Loss: 0.0145 | Grad Norm: 0.00979528\n",
      "Epoch 2 | Step 1032100 | Avg Loss: 0.0142 | Grad Norm: 0.00844519\n",
      "Epoch 2 | Step 1032200 | Avg Loss: 0.0141 | Grad Norm: 0.00825393\n",
      "Epoch 2 | Step 1032300 | Avg Loss: 0.0139 | Grad Norm: 0.00720964\n",
      "Epoch 2 | Step 1032400 | Avg Loss: 0.0134 | Grad Norm: 0.00741627\n",
      "Epoch 2 | Step 1032500 | Avg Loss: 0.0139 | Grad Norm: 0.00896057\n",
      "Epoch 2 | Step 1032600 | Avg Loss: 0.0141 | Grad Norm: 0.00865958\n",
      "Epoch 2 | Step 1032700 | Avg Loss: 0.0140 | Grad Norm: 0.00825062\n",
      "Epoch 2 | Step 1032800 | Avg Loss: 0.0144 | Grad Norm: 0.00715066\n",
      "Epoch 2 | Step 1032900 | Avg Loss: 0.0147 | Grad Norm: 0.00842922\n",
      "Epoch 2 | Step 1033000 | Avg Loss: 0.0147 | Grad Norm: 0.01120169\n",
      "Epoch 2 | Step 1033100 | Avg Loss: 0.0147 | Grad Norm: 0.00869976\n",
      "Epoch 2 | Step 1033200 | Avg Loss: 0.0148 | Grad Norm: 0.00782938\n",
      "Epoch 2 | Step 1033300 | Avg Loss: 0.0148 | Grad Norm: 0.00895360\n",
      "Epoch 2 | Step 1033400 | Avg Loss: 0.0147 | Grad Norm: 0.00844303\n",
      "Epoch 2 | Step 1033500 | Avg Loss: 0.0148 | Grad Norm: 0.00904043\n",
      "Epoch 2 | Step 1033600 | Avg Loss: 0.0150 | Grad Norm: 0.00842585\n",
      "Epoch 2 | Step 1033700 | Avg Loss: 0.0150 | Grad Norm: 0.00885931\n",
      "Epoch 2 | Step 1033800 | Avg Loss: 0.0150 | Grad Norm: 0.00967923\n",
      "Epoch 2 | Step 1033900 | Avg Loss: 0.0150 | Grad Norm: 0.00934729\n",
      "Epoch 2 | Step 1034000 | Avg Loss: 0.0152 | Grad Norm: 0.00796684\n",
      "Epoch 2 | Step 1034100 | Avg Loss: 0.0148 | Grad Norm: 0.00898795\n",
      "Epoch 2 | Step 1034200 | Avg Loss: 0.0149 | Grad Norm: 0.00873872\n",
      "Epoch 2 | Step 1034300 | Avg Loss: 0.0146 | Grad Norm: 0.00808382\n",
      "Epoch 2 | Step 1034400 | Avg Loss: 0.0147 | Grad Norm: 0.01143340\n",
      "Epoch 2 | Step 1034500 | Avg Loss: 0.0144 | Grad Norm: 0.00900108\n",
      "Epoch 2 | Step 1034600 | Avg Loss: 0.0150 | Grad Norm: 0.00931991\n",
      "Epoch 2 | Step 1034700 | Avg Loss: 0.0148 | Grad Norm: 0.01024097\n",
      "Epoch 2 | Step 1034800 | Avg Loss: 0.0145 | Grad Norm: 0.00886746\n",
      "Epoch 2 | Step 1034900 | Avg Loss: 0.0147 | Grad Norm: 0.00943668\n",
      "Epoch 2 | Step 1035000 | Avg Loss: 0.0144 | Grad Norm: 0.01067000\n",
      "Epoch 2 | Step 1035100 | Avg Loss: 0.0144 | Grad Norm: 0.00806311\n",
      "Epoch 2 | Step 1035200 | Avg Loss: 0.0142 | Grad Norm: 0.00823285\n",
      "Epoch 2 | Step 1035300 | Avg Loss: 0.0144 | Grad Norm: 0.00856877\n",
      "Epoch 2 | Step 1035400 | Avg Loss: 0.0145 | Grad Norm: 0.00703983\n",
      "Epoch 2 | Step 1035500 | Avg Loss: 0.0149 | Grad Norm: 0.00720605\n",
      "Epoch 2 | Step 1035600 | Avg Loss: 0.0151 | Grad Norm: 0.01213659\n",
      "Epoch 2 | Step 1035700 | Avg Loss: 0.0147 | Grad Norm: 0.00850988\n",
      "Epoch 2 | Step 1035800 | Avg Loss: 0.0149 | Grad Norm: 0.00913158\n",
      "Epoch 2 | Step 1035900 | Avg Loss: 0.0148 | Grad Norm: 0.00691543\n",
      "Epoch 2 | Step 1036000 | Avg Loss: 0.0146 | Grad Norm: 0.00849792\n",
      "Epoch 2 | Step 1036100 | Avg Loss: 0.0148 | Grad Norm: 0.00758885\n",
      "Epoch 2 | Step 1036200 | Avg Loss: 0.0148 | Grad Norm: 0.00735037\n",
      "Epoch 2 | Step 1036300 | Avg Loss: 0.0146 | Grad Norm: 0.00859671\n",
      "Epoch 2 | Step 1036400 | Avg Loss: 0.0146 | Grad Norm: 0.00796889\n",
      "Epoch 2 | Step 1036500 | Avg Loss: 0.0146 | Grad Norm: 0.00939185\n",
      "Epoch 2 | Step 1036600 | Avg Loss: 0.0149 | Grad Norm: 0.00829315\n",
      "Epoch 2 | Step 1036700 | Avg Loss: 0.0149 | Grad Norm: 0.00794119\n",
      "Epoch 2 | Step 1036800 | Avg Loss: 0.0149 | Grad Norm: 0.00963389\n",
      "Epoch 2 | Step 1036900 | Avg Loss: 0.0149 | Grad Norm: 0.00913995\n",
      "Epoch 2 | Step 1037000 | Avg Loss: 0.0153 | Grad Norm: 0.00860802\n",
      "Epoch 2 | Step 1037100 | Avg Loss: 0.0149 | Grad Norm: 0.00821716\n",
      "Epoch 2 | Step 1037200 | Avg Loss: 0.0150 | Grad Norm: 0.00869621\n",
      "Epoch 2 | Step 1037300 | Avg Loss: 0.0150 | Grad Norm: 0.00832344\n",
      "Epoch 2 | Step 1037400 | Avg Loss: 0.0148 | Grad Norm: 0.00878472\n",
      "Epoch 2 | Step 1037500 | Avg Loss: 0.0149 | Grad Norm: 0.01003418\n",
      "Epoch 2 | Step 1037600 | Avg Loss: 0.0148 | Grad Norm: 0.00847957\n",
      "Epoch 2 | Step 1037700 | Avg Loss: 0.0149 | Grad Norm: 0.00778392\n",
      "Epoch 2 | Step 1037800 | Avg Loss: 0.0149 | Grad Norm: 0.00824394\n",
      "Epoch 2 | Step 1037900 | Avg Loss: 0.0145 | Grad Norm: 0.00839888\n",
      "Epoch 2 | Step 1038000 | Avg Loss: 0.0145 | Grad Norm: 0.00739175\n",
      "Epoch 2 | Step 1038100 | Avg Loss: 0.0146 | Grad Norm: 0.00935628\n",
      "Epoch 2 | Step 1038200 | Avg Loss: 0.0146 | Grad Norm: 0.01087016\n",
      "Epoch 2 | Step 1038300 | Avg Loss: 0.0147 | Grad Norm: 0.00896618\n",
      "Epoch 2 | Step 1038400 | Avg Loss: 0.0150 | Grad Norm: 0.00993290\n",
      "Epoch 2 | Step 1038500 | Avg Loss: 0.0149 | Grad Norm: 0.00874990\n",
      "Epoch 2 | Step 1038600 | Avg Loss: 0.0145 | Grad Norm: 0.00977827\n",
      "Epoch 2 | Step 1038700 | Avg Loss: 0.0148 | Grad Norm: 0.00829318\n",
      "Epoch 2 | Step 1038800 | Avg Loss: 0.0151 | Grad Norm: 0.01049902\n",
      "Epoch 2 | Step 1038900 | Avg Loss: 0.0149 | Grad Norm: 0.00776432\n",
      "Epoch 2 | Step 1039000 | Avg Loss: 0.0150 | Grad Norm: 0.00710927\n",
      "Epoch 2 | Step 1039100 | Avg Loss: 0.0153 | Grad Norm: 0.00911397\n",
      "Epoch 2 | Step 1039200 | Avg Loss: 0.0151 | Grad Norm: 0.00802331\n",
      "Epoch 2 | Step 1039300 | Avg Loss: 0.0152 | Grad Norm: 0.00737947\n",
      "Epoch 2 | Step 1039400 | Avg Loss: 0.0147 | Grad Norm: 0.00869371\n",
      "Epoch 2 | Step 1039500 | Avg Loss: 0.0144 | Grad Norm: 0.00769289\n",
      "Epoch 2 | Step 1039600 | Avg Loss: 0.0145 | Grad Norm: 0.00790703\n",
      "Epoch 2 | Step 1039700 | Avg Loss: 0.0143 | Grad Norm: 0.00885614\n",
      "Epoch 2 | Step 1039800 | Avg Loss: 0.0146 | Grad Norm: 0.00706089\n",
      "Epoch 2 | Step 1039900 | Avg Loss: 0.0150 | Grad Norm: 0.00778096\n",
      "Epoch 2 | Step 1040000 | Avg Loss: 0.0152 | Grad Norm: 0.00912022\n",
      "Epoch 2 | Step 1040100 | Avg Loss: 0.0153 | Grad Norm: 0.00947869\n",
      "Epoch 2 | Step 1040200 | Avg Loss: 0.0149 | Grad Norm: 0.00914163\n",
      "Epoch 2 | Step 1040300 | Avg Loss: 0.0152 | Grad Norm: 0.00856372\n",
      "Epoch 2 | Step 1040400 | Avg Loss: 0.0152 | Grad Norm: 0.00800131\n",
      "Epoch 2 | Step 1040500 | Avg Loss: 0.0146 | Grad Norm: 0.00971268\n",
      "Epoch 2 | Step 1040600 | Avg Loss: 0.0146 | Grad Norm: 0.00898781\n",
      "Epoch 2 | Step 1040700 | Avg Loss: 0.0147 | Grad Norm: 0.00914237\n",
      "Epoch 2 | Step 1040800 | Avg Loss: 0.0145 | Grad Norm: 0.00852334\n",
      "Epoch 2 | Step 1040900 | Avg Loss: 0.0145 | Grad Norm: 0.00940046\n",
      "Epoch 2 | Step 1041000 | Avg Loss: 0.0147 | Grad Norm: 0.00824163\n",
      "Epoch 2 | Step 1041100 | Avg Loss: 0.0147 | Grad Norm: 0.00844017\n",
      "Epoch 2 | Step 1041200 | Avg Loss: 0.0147 | Grad Norm: 0.00898561\n",
      "Epoch 2 | Step 1041300 | Avg Loss: 0.0147 | Grad Norm: 0.00844705\n",
      "Epoch 2 | Step 1041400 | Avg Loss: 0.0146 | Grad Norm: 0.00827165\n",
      "Epoch 2 | Step 1041500 | Avg Loss: 0.0144 | Grad Norm: 0.00741110\n",
      "Epoch 2 | Step 1041600 | Avg Loss: 0.0147 | Grad Norm: 0.00827451\n",
      "Epoch 2 | Step 1041700 | Avg Loss: 0.0148 | Grad Norm: 0.01031278\n",
      "Epoch 2 | Step 1041800 | Avg Loss: 0.0150 | Grad Norm: 0.00776476\n",
      "Epoch 2 | Step 1041900 | Avg Loss: 0.0150 | Grad Norm: 0.01033353\n",
      "Epoch 2 | Step 1042000 | Avg Loss: 0.0155 | Grad Norm: 0.00886163\n",
      "Epoch 2 | Step 1042100 | Avg Loss: 0.0153 | Grad Norm: 0.00891156\n",
      "Epoch 2 | Step 1042200 | Avg Loss: 0.0151 | Grad Norm: 0.00878804\n",
      "Epoch 2 | Step 1042300 | Avg Loss: 0.0147 | Grad Norm: 0.00836769\n",
      "Epoch 2 | Step 1042400 | Avg Loss: 0.0147 | Grad Norm: 0.00977647\n",
      "Epoch 2 | Step 1042500 | Avg Loss: 0.0149 | Grad Norm: 0.00785062\n",
      "Epoch 2 | Step 1042600 | Avg Loss: 0.0149 | Grad Norm: 0.00733430\n",
      "Epoch 2 | Step 1042700 | Avg Loss: 0.0151 | Grad Norm: 0.00801954\n",
      "Epoch 2 | Step 1042800 | Avg Loss: 0.0151 | Grad Norm: 0.00723762\n",
      "Epoch 2 | Step 1042900 | Avg Loss: 0.0150 | Grad Norm: 0.00913141\n",
      "Epoch 2 | Step 1043000 | Avg Loss: 0.0150 | Grad Norm: 0.00885325\n",
      "Epoch 2 | Step 1043100 | Avg Loss: 0.0149 | Grad Norm: 0.00916021\n",
      "Epoch 2 | Step 1043200 | Avg Loss: 0.0152 | Grad Norm: 0.01048550\n",
      "Epoch 2 | Step 1043300 | Avg Loss: 0.0153 | Grad Norm: 0.00855205\n",
      "Epoch 2 | Step 1043400 | Avg Loss: 0.0155 | Grad Norm: 0.00715733\n",
      "Epoch 2 | Step 1043500 | Avg Loss: 0.0150 | Grad Norm: 0.00979037\n",
      "Epoch 2 | Step 1043600 | Avg Loss: 0.0148 | Grad Norm: 0.00844968\n",
      "Epoch 2 | Step 1043700 | Avg Loss: 0.0147 | Grad Norm: 0.00865004\n",
      "Epoch 2 | Step 1043800 | Avg Loss: 0.0153 | Grad Norm: 0.00838349\n",
      "Epoch 2 | Step 1043900 | Avg Loss: 0.0148 | Grad Norm: 0.00805887\n",
      "Epoch 2 | Step 1044000 | Avg Loss: 0.0149 | Grad Norm: 0.00829653\n",
      "Epoch 2 | Step 1044100 | Avg Loss: 0.0153 | Grad Norm: 0.00997474\n",
      "Epoch 2 | Step 1044200 | Avg Loss: 0.0152 | Grad Norm: 0.00850600\n",
      "Epoch 2 | Step 1044300 | Avg Loss: 0.0147 | Grad Norm: 0.00828369\n",
      "Epoch 2 | Step 1044400 | Avg Loss: 0.0150 | Grad Norm: 0.00805789\n",
      "Epoch 2 | Step 1044500 | Avg Loss: 0.0147 | Grad Norm: 0.00764963\n",
      "Epoch 2 | Step 1044600 | Avg Loss: 0.0149 | Grad Norm: 0.00900849\n",
      "Epoch 2 | Step 1044700 | Avg Loss: 0.0150 | Grad Norm: 0.00820357\n",
      "Epoch 2 | Step 1044800 | Avg Loss: 0.0149 | Grad Norm: 0.00816825\n",
      "Epoch 2 | Step 1044900 | Avg Loss: 0.0152 | Grad Norm: 0.00779184\n",
      "Epoch 2 | Step 1045000 | Avg Loss: 0.0154 | Grad Norm: 0.00702104\n",
      "Epoch 2 | Step 1045100 | Avg Loss: 0.0152 | Grad Norm: 0.00987440\n",
      "Epoch 2 | Step 1045200 | Avg Loss: 0.0154 | Grad Norm: 0.00809953\n",
      "Epoch 2 | Step 1045300 | Avg Loss: 0.0153 | Grad Norm: 0.00850549\n",
      "Epoch 2 | Step 1045400 | Avg Loss: 0.0156 | Grad Norm: 0.00806587\n",
      "Epoch 2 | Step 1045500 | Avg Loss: 0.0156 | Grad Norm: 0.00938149\n",
      "Epoch 2 | Step 1045600 | Avg Loss: 0.0152 | Grad Norm: 0.00871870\n",
      "Epoch 2 | Step 1045700 | Avg Loss: 0.0153 | Grad Norm: 0.00856049\n",
      "Epoch 2 | Step 1045800 | Avg Loss: 0.0150 | Grad Norm: 0.00908586\n",
      "Epoch 2 | Step 1045900 | Avg Loss: 0.0150 | Grad Norm: 0.00812430\n",
      "Epoch 2 | Step 1046000 | Avg Loss: 0.0152 | Grad Norm: 0.00731439\n",
      "Epoch 2 | Step 1046100 | Avg Loss: 0.0154 | Grad Norm: 0.00915496\n",
      "Epoch 2 | Step 1046200 | Avg Loss: 0.0155 | Grad Norm: 0.00919771\n",
      "Epoch 2 | Step 1046300 | Avg Loss: 0.0155 | Grad Norm: 0.00963489\n",
      "Epoch 2 | Step 1046400 | Avg Loss: 0.0155 | Grad Norm: 0.00826669\n",
      "Epoch 2 | Step 1046500 | Avg Loss: 0.0152 | Grad Norm: 0.00662653\n",
      "Epoch 2 | Step 1046600 | Avg Loss: 0.0146 | Grad Norm: 0.00926990\n",
      "Epoch 2 | Step 1046700 | Avg Loss: 0.0150 | Grad Norm: 0.00961164\n",
      "Epoch 2 | Step 1046800 | Avg Loss: 0.0158 | Grad Norm: 0.01108156\n",
      "Epoch 2 | Step 1046900 | Avg Loss: 0.0153 | Grad Norm: 0.00759963\n",
      "Epoch 2 | Step 1047000 | Avg Loss: 0.0150 | Grad Norm: 0.00842929\n",
      "Epoch 2 | Step 1047100 | Avg Loss: 0.0151 | Grad Norm: 0.00829824\n",
      "Epoch 2 | Step 1047200 | Avg Loss: 0.0150 | Grad Norm: 0.00770551\n",
      "Epoch 2 | Step 1047300 | Avg Loss: 0.0149 | Grad Norm: 0.00925069\n",
      "Epoch 2 | Step 1047400 | Avg Loss: 0.0152 | Grad Norm: 0.00834862\n",
      "Epoch 2 | Step 1047500 | Avg Loss: 0.0153 | Grad Norm: 0.00846141\n",
      "Epoch 2 | Step 1047600 | Avg Loss: 0.0152 | Grad Norm: 0.00744804\n",
      "Epoch 2 | Step 1047700 | Avg Loss: 0.0156 | Grad Norm: 0.00821989\n",
      "Epoch 2 | Step 1047800 | Avg Loss: 0.0156 | Grad Norm: 0.00723439\n",
      "Epoch 2 | Step 1047900 | Avg Loss: 0.0153 | Grad Norm: 0.00724051\n",
      "Epoch 2 | Step 1048000 | Avg Loss: 0.0153 | Grad Norm: 0.00797713\n",
      "Epoch 2 | Step 1048100 | Avg Loss: 0.0151 | Grad Norm: 0.00964705\n",
      "Epoch 2 | Step 1048200 | Avg Loss: 0.0149 | Grad Norm: 0.00848526\n",
      "Epoch 2 | Step 1048300 | Avg Loss: 0.0150 | Grad Norm: 0.00845872\n",
      "Epoch 2 | Step 1048400 | Avg Loss: 0.0149 | Grad Norm: 0.00885371\n",
      "Epoch 2 | Step 1048500 | Avg Loss: 0.0147 | Grad Norm: 0.00840760\n",
      "Epoch 2 | Step 1048600 | Avg Loss: 0.0144 | Grad Norm: 0.00805930\n",
      "Epoch 2 | Step 1048700 | Avg Loss: 0.0143 | Grad Norm: 0.00785265\n",
      "Epoch 2 | Step 1048800 | Avg Loss: 0.0147 | Grad Norm: 0.00756685\n",
      "Epoch 2 | Step 1048900 | Avg Loss: 0.0148 | Grad Norm: 0.00813868\n",
      "Epoch 2 | Step 1049000 | Avg Loss: 0.0148 | Grad Norm: 0.00734665\n",
      "Epoch 2 | Step 1049100 | Avg Loss: 0.0148 | Grad Norm: 0.00840240\n",
      "Epoch 2 | Step 1049200 | Avg Loss: 0.0150 | Grad Norm: 0.00910662\n",
      "Epoch 2 | Step 1049300 | Avg Loss: 0.0149 | Grad Norm: 0.01030069\n",
      "Epoch 2 | Step 1049400 | Avg Loss: 0.0147 | Grad Norm: 0.00672068\n",
      "Epoch 2 | Step 1049500 | Avg Loss: 0.0150 | Grad Norm: 0.00831405\n",
      "Epoch 2 | Step 1049600 | Avg Loss: 0.0149 | Grad Norm: 0.00873760\n",
      "Epoch 2 | Step 1049700 | Avg Loss: 0.0149 | Grad Norm: 0.00847186\n",
      "Epoch 2 | Step 1049800 | Avg Loss: 0.0151 | Grad Norm: 0.00910391\n",
      "Epoch 2 | Step 1049900 | Avg Loss: 0.0149 | Grad Norm: 0.00875943\n",
      "Epoch 2 | Step 1050000 | Avg Loss: 0.0150 | Grad Norm: 0.00985261\n",
      "Epoch 2 | Step 1050100 | Avg Loss: 0.0152 | Grad Norm: 0.00868360\n",
      "Epoch 2 | Step 1050200 | Avg Loss: 0.0150 | Grad Norm: 0.00869434\n",
      "Epoch 2 | Step 1050300 | Avg Loss: 0.0151 | Grad Norm: 0.00874230\n",
      "Epoch 2 | Step 1050400 | Avg Loss: 0.0151 | Grad Norm: 0.00696307\n",
      "Epoch 2 | Step 1050500 | Avg Loss: 0.0151 | Grad Norm: 0.00980711\n",
      "Epoch 2 | Step 1050600 | Avg Loss: 0.0153 | Grad Norm: 0.00895789\n",
      "Epoch 2 | Step 1050700 | Avg Loss: 0.0152 | Grad Norm: 0.00860827\n",
      "Epoch 2 | Step 1050800 | Avg Loss: 0.0151 | Grad Norm: 0.00858231\n",
      "Epoch 2 | Step 1050900 | Avg Loss: 0.0150 | Grad Norm: 0.00786708\n",
      "Epoch 2 | Step 1051000 | Avg Loss: 0.0150 | Grad Norm: 0.00713653\n",
      "Epoch 2 | Step 1051100 | Avg Loss: 0.0149 | Grad Norm: 0.00969936\n",
      "Epoch 2 | Step 1051200 | Avg Loss: 0.0149 | Grad Norm: 0.00820480\n",
      "Epoch 2 | Step 1051300 | Avg Loss: 0.0152 | Grad Norm: 0.00952016\n",
      "Epoch 2 | Step 1051400 | Avg Loss: 0.0152 | Grad Norm: 0.00972605\n",
      "Epoch 2 | Step 1051500 | Avg Loss: 0.0152 | Grad Norm: 0.00738076\n",
      "Epoch 2 | Step 1051600 | Avg Loss: 0.0152 | Grad Norm: 0.00828904\n",
      "Epoch 2 | Step 1051700 | Avg Loss: 0.0147 | Grad Norm: 0.00741123\n",
      "Epoch 2 | Step 1051800 | Avg Loss: 0.0150 | Grad Norm: 0.00823457\n",
      "Epoch 2 | Step 1051900 | Avg Loss: 0.0149 | Grad Norm: 0.00796063\n",
      "Epoch 2 | Step 1052000 | Avg Loss: 0.0149 | Grad Norm: 0.00840153\n",
      "Epoch 2 | Step 1052100 | Avg Loss: 0.0146 | Grad Norm: 0.00809047\n",
      "Epoch 2 | Step 1052200 | Avg Loss: 0.0143 | Grad Norm: 0.00930654\n",
      "Epoch 2 | Step 1052300 | Avg Loss: 0.0145 | Grad Norm: 0.00776940\n",
      "Epoch 2 | Step 1052400 | Avg Loss: 0.0146 | Grad Norm: 0.00734729\n",
      "Epoch 2 | Step 1052500 | Avg Loss: 0.0149 | Grad Norm: 0.00919108\n",
      "Epoch 2 | Step 1052600 | Avg Loss: 0.0146 | Grad Norm: 0.00919301\n",
      "Epoch 2 | Step 1052700 | Avg Loss: 0.0148 | Grad Norm: 0.00819395\n",
      "Epoch 2 | Step 1052800 | Avg Loss: 0.0150 | Grad Norm: 0.00903882\n",
      "Epoch 2 | Step 1052900 | Avg Loss: 0.0149 | Grad Norm: 0.01091941\n",
      "Epoch 2 | Step 1053000 | Avg Loss: 0.0147 | Grad Norm: 0.00854124\n",
      "Epoch 2 | Step 1053100 | Avg Loss: 0.0148 | Grad Norm: 0.00753443\n",
      "Epoch 2 | Step 1053200 | Avg Loss: 0.0149 | Grad Norm: 0.00842198\n",
      "Epoch 2 | Step 1053300 | Avg Loss: 0.0147 | Grad Norm: 0.00840991\n",
      "Epoch 2 | Step 1053400 | Avg Loss: 0.0146 | Grad Norm: 0.00953887\n",
      "Epoch 2 | Step 1053500 | Avg Loss: 0.0150 | Grad Norm: 0.00812577\n",
      "Epoch 2 | Step 1053600 | Avg Loss: 0.0150 | Grad Norm: 0.00807073\n",
      "Epoch 2 | Step 1053700 | Avg Loss: 0.0151 | Grad Norm: 0.00921597\n",
      "Epoch 2 | Step 1053800 | Avg Loss: 0.0150 | Grad Norm: 0.00867069\n",
      "Epoch 2 | Step 1053900 | Avg Loss: 0.0153 | Grad Norm: 0.00912528\n",
      "Epoch 2 | Step 1054000 | Avg Loss: 0.0154 | Grad Norm: 0.00745904\n",
      "Epoch 2 | Step 1054100 | Avg Loss: 0.0152 | Grad Norm: 0.00911230\n",
      "Epoch 2 | Step 1054200 | Avg Loss: 0.0145 | Grad Norm: 0.00718186\n",
      "Epoch 2 | Step 1054300 | Avg Loss: 0.0142 | Grad Norm: 0.00817648\n",
      "Epoch 2 | Step 1054400 | Avg Loss: 0.0145 | Grad Norm: 0.00850105\n",
      "Epoch 2 | Step 1054500 | Avg Loss: 0.0146 | Grad Norm: 0.00902009\n",
      "Epoch 2 | Step 1054600 | Avg Loss: 0.0145 | Grad Norm: 0.00953866\n",
      "Epoch 2 | Step 1054700 | Avg Loss: 0.0146 | Grad Norm: 0.00722925\n",
      "Epoch 2 | Step 1054800 | Avg Loss: 0.0145 | Grad Norm: 0.00820736\n",
      "Epoch 2 | Step 1054900 | Avg Loss: 0.0145 | Grad Norm: 0.00977769\n",
      "Epoch 2 | Step 1055000 | Avg Loss: 0.0146 | Grad Norm: 0.00856478\n",
      "Epoch 2 | Step 1055100 | Avg Loss: 0.0148 | Grad Norm: 0.00759270\n",
      "Epoch 2 | Step 1055200 | Avg Loss: 0.0146 | Grad Norm: 0.00863674\n",
      "Epoch 2 | Step 1055300 | Avg Loss: 0.0148 | Grad Norm: 0.00863968\n",
      "Epoch 2 | Step 1055400 | Avg Loss: 0.0146 | Grad Norm: 0.00814033\n",
      "Epoch 2 | Step 1055500 | Avg Loss: 0.0147 | Grad Norm: 0.00840749\n",
      "Epoch 2 | Step 1055600 | Avg Loss: 0.0146 | Grad Norm: 0.00895385\n",
      "Epoch 2 | Step 1055700 | Avg Loss: 0.0146 | Grad Norm: 0.00796588\n",
      "Epoch 2 | Step 1055800 | Avg Loss: 0.0146 | Grad Norm: 0.00759671\n",
      "Epoch 2 | Step 1055900 | Avg Loss: 0.0144 | Grad Norm: 0.00725795\n",
      "Epoch 2 | Step 1056000 | Avg Loss: 0.0144 | Grad Norm: 0.00975957\n",
      "Epoch 2 | Step 1056100 | Avg Loss: 0.0150 | Grad Norm: 0.00840679\n",
      "Epoch 2 | Step 1056200 | Avg Loss: 0.0151 | Grad Norm: 0.00823754\n",
      "Epoch 2 | Step 1056300 | Avg Loss: 0.0153 | Grad Norm: 0.00714598\n",
      "Epoch 2 | Step 1056400 | Avg Loss: 0.0153 | Grad Norm: 0.00906657\n",
      "Epoch 2 | Step 1056500 | Avg Loss: 0.0152 | Grad Norm: 0.00846662\n",
      "Epoch 2 | Step 1056600 | Avg Loss: 0.0151 | Grad Norm: 0.00836155\n",
      "Epoch 2 | Step 1056700 | Avg Loss: 0.0148 | Grad Norm: 0.01021140\n",
      "Epoch 2 | Step 1056800 | Avg Loss: 0.0149 | Grad Norm: 0.00836935\n",
      "Epoch 2 | Step 1056900 | Avg Loss: 0.0146 | Grad Norm: 0.00872860\n",
      "Epoch 2 | Step 1057000 | Avg Loss: 0.0148 | Grad Norm: 0.00762087\n",
      "Epoch 2 | Step 1057100 | Avg Loss: 0.0150 | Grad Norm: 0.00820176\n",
      "Epoch 2 | Step 1057200 | Avg Loss: 0.0152 | Grad Norm: 0.00830076\n",
      "Epoch 2 | Step 1057300 | Avg Loss: 0.0153 | Grad Norm: 0.00966240\n",
      "Epoch 2 | Step 1057400 | Avg Loss: 0.0152 | Grad Norm: 0.00799110\n",
      "Epoch 2 | Step 1057500 | Avg Loss: 0.0151 | Grad Norm: 0.00818695\n",
      "Epoch 2 | Step 1057600 | Avg Loss: 0.0155 | Grad Norm: 0.00958183\n",
      "Epoch 2 | Step 1057700 | Avg Loss: 0.0155 | Grad Norm: 0.00806678\n",
      "Epoch 2 | Step 1057800 | Avg Loss: 0.0150 | Grad Norm: 0.00734179\n",
      "Epoch 2 | Step 1057900 | Avg Loss: 0.0145 | Grad Norm: 0.00691771\n",
      "Epoch 2 | Step 1058000 | Avg Loss: 0.0146 | Grad Norm: 0.00928128\n",
      "Epoch 2 | Step 1058100 | Avg Loss: 0.0150 | Grad Norm: 0.00759403\n",
      "Epoch 2 | Step 1058200 | Avg Loss: 0.0152 | Grad Norm: 0.00741162\n",
      "Epoch 2 | Step 1058300 | Avg Loss: 0.0149 | Grad Norm: 0.00787053\n",
      "Epoch 2 | Step 1058400 | Avg Loss: 0.0146 | Grad Norm: 0.00726574\n",
      "Epoch 2 | Step 1058500 | Avg Loss: 0.0145 | Grad Norm: 0.00807445\n",
      "Epoch 2 | Step 1058600 | Avg Loss: 0.0148 | Grad Norm: 0.00782217\n",
      "Epoch 2 | Step 1058700 | Avg Loss: 0.0150 | Grad Norm: 0.00777402\n",
      "Epoch 2 | Step 1058800 | Avg Loss: 0.0147 | Grad Norm: 0.00801389\n",
      "Epoch 2 | Step 1058900 | Avg Loss: 0.0147 | Grad Norm: 0.00722575\n",
      "Epoch 2 | Step 1059000 | Avg Loss: 0.0144 | Grad Norm: 0.00884333\n",
      "Epoch 2 | Step 1059100 | Avg Loss: 0.0148 | Grad Norm: 0.00799350\n",
      "Epoch 2 | Step 1059200 | Avg Loss: 0.0148 | Grad Norm: 0.00823872\n",
      "Epoch 2 | Step 1059300 | Avg Loss: 0.0149 | Grad Norm: 0.00834760\n",
      "Epoch 2 | Step 1059400 | Avg Loss: 0.0148 | Grad Norm: 0.00769186\n",
      "Epoch 2 | Step 1059500 | Avg Loss: 0.0150 | Grad Norm: 0.00819100\n",
      "Epoch 2 | Step 1059600 | Avg Loss: 0.0152 | Grad Norm: 0.00785556\n",
      "Epoch 2 | Step 1059700 | Avg Loss: 0.0146 | Grad Norm: 0.00815448\n",
      "Epoch 2 | Step 1059800 | Avg Loss: 0.0146 | Grad Norm: 0.00926464\n",
      "Epoch 2 | Step 1059900 | Avg Loss: 0.0147 | Grad Norm: 0.00868655\n",
      "Epoch 2 | Step 1060000 | Avg Loss: 0.0146 | Grad Norm: 0.00759964\n",
      "Epoch 2 | Step 1060100 | Avg Loss: 0.0149 | Grad Norm: 0.01022394\n",
      "Epoch 2 | Step 1060200 | Avg Loss: 0.0151 | Grad Norm: 0.00735316\n",
      "Epoch 2 | Step 1060300 | Avg Loss: 0.0150 | Grad Norm: 0.00748787\n",
      "Epoch 2 | Step 1060400 | Avg Loss: 0.0151 | Grad Norm: 0.00816054\n",
      "Epoch 2 | Step 1060500 | Avg Loss: 0.0152 | Grad Norm: 0.00936981\n",
      "Epoch 2 | Step 1060600 | Avg Loss: 0.0154 | Grad Norm: 0.00939372\n",
      "Epoch 2 | Step 1060700 | Avg Loss: 0.0155 | Grad Norm: 0.00935450\n",
      "Epoch 2 | Step 1060800 | Avg Loss: 0.0159 | Grad Norm: 0.00773639\n",
      "Epoch 2 | Step 1060900 | Avg Loss: 0.0156 | Grad Norm: 0.00830302\n",
      "Epoch 2 | Step 1061000 | Avg Loss: 0.0153 | Grad Norm: 0.00783012\n",
      "Epoch 2 | Step 1061100 | Avg Loss: 0.0154 | Grad Norm: 0.00858232\n",
      "Epoch 2 | Step 1061200 | Avg Loss: 0.0159 | Grad Norm: 0.00917467\n",
      "Epoch 2 | Step 1061300 | Avg Loss: 0.0155 | Grad Norm: 0.00865703\n",
      "Epoch 2 | Step 1061400 | Avg Loss: 0.0152 | Grad Norm: 0.00942020\n",
      "Epoch 2 | Step 1061500 | Avg Loss: 0.0149 | Grad Norm: 0.00847871\n",
      "Epoch 2 | Step 1061600 | Avg Loss: 0.0150 | Grad Norm: 0.00914790\n",
      "Epoch 2 | Step 1061700 | Avg Loss: 0.0149 | Grad Norm: 0.00747830\n",
      "Epoch 2 | Step 1061800 | Avg Loss: 0.0148 | Grad Norm: 0.00851581\n",
      "Epoch 2 | Step 1061900 | Avg Loss: 0.0150 | Grad Norm: 0.00935007\n",
      "Epoch 2 | Step 1062000 | Avg Loss: 0.0152 | Grad Norm: 0.00826611\n",
      "Epoch 2 | Step 1062100 | Avg Loss: 0.0147 | Grad Norm: 0.00777916\n",
      "Epoch 2 | Step 1062200 | Avg Loss: 0.0149 | Grad Norm: 0.00852679\n",
      "Epoch 2 | Step 1062300 | Avg Loss: 0.0150 | Grad Norm: 0.00853606\n",
      "Epoch 2 | Step 1062400 | Avg Loss: 0.0151 | Grad Norm: 0.00893046\n",
      "Epoch 2 | Step 1062500 | Avg Loss: 0.0151 | Grad Norm: 0.00797852\n",
      "Epoch 2 | Step 1062600 | Avg Loss: 0.0154 | Grad Norm: 0.00836835\n",
      "Epoch 2 | Step 1062700 | Avg Loss: 0.0156 | Grad Norm: 0.01058094\n",
      "Epoch 2 | Step 1062800 | Avg Loss: 0.0152 | Grad Norm: 0.00917132\n",
      "Epoch 2 | Step 1062900 | Avg Loss: 0.0151 | Grad Norm: 0.00851197\n",
      "Epoch 2 | Step 1063000 | Avg Loss: 0.0150 | Grad Norm: 0.00809103\n",
      "Epoch 2 | Step 1063100 | Avg Loss: 0.0150 | Grad Norm: 0.00759239\n",
      "Epoch 2 | Step 1063200 | Avg Loss: 0.0151 | Grad Norm: 0.00826298\n",
      "Epoch 2 | Step 1063300 | Avg Loss: 0.0152 | Grad Norm: 0.00857465\n",
      "Epoch 2 | Step 1063400 | Avg Loss: 0.0148 | Grad Norm: 0.00773428\n",
      "Epoch 2 | Step 1063500 | Avg Loss: 0.0149 | Grad Norm: 0.00893010\n",
      "Epoch 2 | Step 1063600 | Avg Loss: 0.0150 | Grad Norm: 0.00903990\n",
      "Epoch 2 | Step 1063700 | Avg Loss: 0.0150 | Grad Norm: 0.00976402\n",
      "Epoch 2 | Step 1063800 | Avg Loss: 0.0152 | Grad Norm: 0.00894133\n",
      "Epoch 2 | Step 1063900 | Avg Loss: 0.0152 | Grad Norm: 0.00893181\n",
      "Epoch 2 | Step 1064000 | Avg Loss: 0.0148 | Grad Norm: 0.00803101\n",
      "Epoch 2 | Step 1064100 | Avg Loss: 0.0149 | Grad Norm: 0.00808491\n",
      "Epoch 2 | Step 1064200 | Avg Loss: 0.0153 | Grad Norm: 0.00834816\n",
      "Epoch 2 | Step 1064300 | Avg Loss: 0.0151 | Grad Norm: 0.00894307\n",
      "Epoch 2 | Step 1064400 | Avg Loss: 0.0149 | Grad Norm: 0.00797215\n",
      "Epoch 2 | Step 1064500 | Avg Loss: 0.0151 | Grad Norm: 0.00839104\n",
      "Epoch 2 | Step 1064600 | Avg Loss: 0.0153 | Grad Norm: 0.00835759\n",
      "Epoch 2 | Step 1064700 | Avg Loss: 0.0152 | Grad Norm: 0.00824398\n",
      "Epoch 2 | Step 1064800 | Avg Loss: 0.0150 | Grad Norm: 0.00825911\n",
      "Epoch 2 | Step 1064900 | Avg Loss: 0.0150 | Grad Norm: 0.00983237\n",
      "Epoch 2 | Step 1065000 | Avg Loss: 0.0148 | Grad Norm: 0.00869178\n",
      "Epoch 2 | Step 1065100 | Avg Loss: 0.0147 | Grad Norm: 0.00726404\n",
      "Epoch 2 | Step 1065200 | Avg Loss: 0.0150 | Grad Norm: 0.00783979\n",
      "Epoch 2 | Step 1065300 | Avg Loss: 0.0148 | Grad Norm: 0.00902093\n",
      "Epoch 2 | Step 1065400 | Avg Loss: 0.0150 | Grad Norm: 0.00832783\n",
      "Epoch 2 | Step 1065500 | Avg Loss: 0.0149 | Grad Norm: 0.00757776\n",
      "Epoch 2 | Step 1065600 | Avg Loss: 0.0148 | Grad Norm: 0.00824028\n",
      "Epoch 2 | Step 1065700 | Avg Loss: 0.0146 | Grad Norm: 0.00961990\n",
      "Epoch 2 | Step 1065800 | Avg Loss: 0.0148 | Grad Norm: 0.00894097\n",
      "Epoch 2 | Step 1065900 | Avg Loss: 0.0150 | Grad Norm: 0.00866214\n",
      "Epoch 2 | Step 1066000 | Avg Loss: 0.0152 | Grad Norm: 0.00857584\n",
      "Epoch 2 | Step 1066100 | Avg Loss: 0.0151 | Grad Norm: 0.00816943\n",
      "Epoch 2 | Step 1066200 | Avg Loss: 0.0155 | Grad Norm: 0.00898211\n",
      "Epoch 2 | Step 1066300 | Avg Loss: 0.0156 | Grad Norm: 0.00781574\n",
      "Epoch 2 | Step 1066400 | Avg Loss: 0.0152 | Grad Norm: 0.00988517\n",
      "Epoch 2 | Step 1066500 | Avg Loss: 0.0151 | Grad Norm: 0.00887128\n",
      "Epoch 2 | Step 1066600 | Avg Loss: 0.0148 | Grad Norm: 0.01032944\n",
      "Epoch 2 | Step 1066700 | Avg Loss: 0.0150 | Grad Norm: 0.00741867\n",
      "Epoch 2 | Step 1066800 | Avg Loss: 0.0146 | Grad Norm: 0.00794206\n",
      "Epoch 2 | Step 1066900 | Avg Loss: 0.0146 | Grad Norm: 0.00899681\n",
      "Epoch 2 | Step 1067000 | Avg Loss: 0.0144 | Grad Norm: 0.00811404\n",
      "Epoch 2 | Step 1067100 | Avg Loss: 0.0145 | Grad Norm: 0.01067057\n",
      "Epoch 2 | Step 1067200 | Avg Loss: 0.0141 | Grad Norm: 0.00833669\n",
      "Epoch 2 | Step 1067300 | Avg Loss: 0.0143 | Grad Norm: 0.00675971\n",
      "Epoch 2 | Step 1067400 | Avg Loss: 0.0145 | Grad Norm: 0.00873789\n",
      "Epoch 2 | Step 1067500 | Avg Loss: 0.0145 | Grad Norm: 0.00769425\n",
      "Epoch 2 | Step 1067600 | Avg Loss: 0.0146 | Grad Norm: 0.00735390\n",
      "Epoch 2 | Step 1067700 | Avg Loss: 0.0145 | Grad Norm: 0.00975517\n",
      "Epoch 2 | Step 1067800 | Avg Loss: 0.0143 | Grad Norm: 0.00876476\n",
      "Epoch 2 | Step 1067900 | Avg Loss: 0.0147 | Grad Norm: 0.01241109\n",
      "Epoch 2 | Step 1068000 | Avg Loss: 0.0149 | Grad Norm: 0.00934297\n",
      "Epoch 2 | Step 1068100 | Avg Loss: 0.0154 | Grad Norm: 0.00892618\n",
      "Epoch 2 | Step 1068200 | Avg Loss: 0.0156 | Grad Norm: 0.00872510\n",
      "Epoch 2 | Step 1068300 | Avg Loss: 0.0155 | Grad Norm: 0.00784168\n",
      "Epoch 2 | Step 1068400 | Avg Loss: 0.0151 | Grad Norm: 0.00762731\n",
      "Epoch 2 | Step 1068500 | Avg Loss: 0.0157 | Grad Norm: 0.00866273\n",
      "Epoch 2 | Step 1068600 | Avg Loss: 0.0151 | Grad Norm: 0.00927260\n",
      "Epoch 2 | Step 1068700 | Avg Loss: 0.0148 | Grad Norm: 0.00790647\n",
      "Epoch 2 | Step 1068800 | Avg Loss: 0.0155 | Grad Norm: 0.00864565\n",
      "Epoch 2 | Step 1068900 | Avg Loss: 0.0154 | Grad Norm: 0.00897463\n",
      "Epoch 2 | Step 1069000 | Avg Loss: 0.0151 | Grad Norm: 0.01019292\n",
      "Epoch 2 | Step 1069100 | Avg Loss: 0.0150 | Grad Norm: 0.01093355\n",
      "Epoch 2 | Step 1069200 | Avg Loss: 0.0150 | Grad Norm: 0.00889425\n",
      "Epoch 2 | Step 1069300 | Avg Loss: 0.0148 | Grad Norm: 0.00887104\n",
      "Epoch 2 | Step 1069400 | Avg Loss: 0.0147 | Grad Norm: 0.00894159\n",
      "Epoch 2 | Step 1069500 | Avg Loss: 0.0147 | Grad Norm: 0.00999539\n",
      "Epoch 2 | Step 1069600 | Avg Loss: 0.0148 | Grad Norm: 0.00878974\n",
      "Epoch 2 | Step 1069700 | Avg Loss: 0.0148 | Grad Norm: 0.00894500\n",
      "Epoch 2 | Step 1069800 | Avg Loss: 0.0147 | Grad Norm: 0.01048624\n",
      "Epoch 2 | Step 1069900 | Avg Loss: 0.0151 | Grad Norm: 0.00811431\n",
      "Epoch 2 | Step 1070000 | Avg Loss: 0.0153 | Grad Norm: 0.00827321\n",
      "Epoch 2 | Step 1070100 | Avg Loss: 0.0150 | Grad Norm: 0.00742287\n",
      "Epoch 2 | Step 1070200 | Avg Loss: 0.0151 | Grad Norm: 0.00937581\n",
      "Epoch 2 | Step 1070300 | Avg Loss: 0.0154 | Grad Norm: 0.00844833\n",
      "Epoch 2 | Step 1070400 | Avg Loss: 0.0156 | Grad Norm: 0.00859729\n",
      "Epoch 2 | Step 1070500 | Avg Loss: 0.0152 | Grad Norm: 0.00795113\n",
      "Epoch 2 | Step 1070600 | Avg Loss: 0.0150 | Grad Norm: 0.00843273\n",
      "Epoch 2 | Step 1070700 | Avg Loss: 0.0147 | Grad Norm: 0.00886596\n",
      "Epoch 2 | Step 1070800 | Avg Loss: 0.0147 | Grad Norm: 0.00906001\n",
      "Epoch 2 | Step 1070900 | Avg Loss: 0.0149 | Grad Norm: 0.00830667\n",
      "Epoch 2 | Step 1071000 | Avg Loss: 0.0152 | Grad Norm: 0.00997194\n",
      "Epoch 2 | Step 1071100 | Avg Loss: 0.0151 | Grad Norm: 0.00824581\n",
      "Epoch 2 | Step 1071200 | Avg Loss: 0.0152 | Grad Norm: 0.00818151\n",
      "Epoch 2 | Step 1071300 | Avg Loss: 0.0155 | Grad Norm: 0.00870614\n",
      "Epoch 2 | Step 1071400 | Avg Loss: 0.0153 | Grad Norm: 0.00864228\n",
      "Epoch 2 | Step 1071500 | Avg Loss: 0.0152 | Grad Norm: 0.00976586\n",
      "Epoch 2 | Step 1071600 | Avg Loss: 0.0144 | Grad Norm: 0.00967920\n",
      "Epoch 2 | Step 1071700 | Avg Loss: 0.0145 | Grad Norm: 0.00991776\n",
      "Epoch 2 | Step 1071800 | Avg Loss: 0.0147 | Grad Norm: 0.00834475\n",
      "Epoch 2 | Step 1071900 | Avg Loss: 0.0147 | Grad Norm: 0.00831347\n",
      "Epoch 2 | Step 1072000 | Avg Loss: 0.0142 | Grad Norm: 0.00777464\n",
      "Epoch 2 | Step 1072100 | Avg Loss: 0.0144 | Grad Norm: 0.00898471\n",
      "Epoch 2 | Step 1072200 | Avg Loss: 0.0149 | Grad Norm: 0.00718279\n",
      "Epoch 2 | Step 1072300 | Avg Loss: 0.0149 | Grad Norm: 0.00838516\n",
      "Epoch 2 | Step 1072400 | Avg Loss: 0.0148 | Grad Norm: 0.00786274\n",
      "Epoch 2 | Step 1072500 | Avg Loss: 0.0150 | Grad Norm: 0.00784001\n",
      "Epoch 2 | Step 1072600 | Avg Loss: 0.0153 | Grad Norm: 0.00841784\n",
      "Epoch 2 | Step 1072700 | Avg Loss: 0.0154 | Grad Norm: 0.00712824\n",
      "Epoch 2 | Step 1072800 | Avg Loss: 0.0150 | Grad Norm: 0.00896248\n",
      "Epoch 2 | Step 1072900 | Avg Loss: 0.0150 | Grad Norm: 0.00835303\n",
      "Epoch 2 | Step 1073000 | Avg Loss: 0.0154 | Grad Norm: 0.01048024\n",
      "Epoch 2 | Step 1073100 | Avg Loss: 0.0153 | Grad Norm: 0.00934915\n",
      "Epoch 2 | Step 1073200 | Avg Loss: 0.0153 | Grad Norm: 0.01143412\n",
      "Epoch 2 | Step 1073300 | Avg Loss: 0.0155 | Grad Norm: 0.00915426\n",
      "Epoch 2 | Step 1073400 | Avg Loss: 0.0154 | Grad Norm: 0.01034205\n",
      "Epoch 2 | Step 1073500 | Avg Loss: 0.0150 | Grad Norm: 0.00777466\n",
      "Epoch 2 | Step 1073600 | Avg Loss: 0.0150 | Grad Norm: 0.00868941\n",
      "Epoch 2 | Step 1073700 | Avg Loss: 0.0147 | Grad Norm: 0.00794398\n",
      "Epoch 2 | Step 1073800 | Avg Loss: 0.0147 | Grad Norm: 0.00779510\n",
      "Epoch 2 | Step 1073900 | Avg Loss: 0.0146 | Grad Norm: 0.00761840\n",
      "Epoch 2 | Step 1074000 | Avg Loss: 0.0148 | Grad Norm: 0.00784019\n",
      "Epoch 2 | Step 1074100 | Avg Loss: 0.0152 | Grad Norm: 0.00844964\n",
      "Epoch 2 | Step 1074200 | Avg Loss: 0.0150 | Grad Norm: 0.00779373\n",
      "Epoch 2 | Step 1074300 | Avg Loss: 0.0154 | Grad Norm: 0.00766380\n",
      "Epoch 2 | Step 1074400 | Avg Loss: 0.0155 | Grad Norm: 0.01046759\n",
      "Epoch 2 | Step 1074500 | Avg Loss: 0.0153 | Grad Norm: 0.00875769\n",
      "Epoch 2 | Step 1074600 | Avg Loss: 0.0153 | Grad Norm: 0.00864243\n",
      "Epoch 2 | Step 1074700 | Avg Loss: 0.0149 | Grad Norm: 0.00959080\n",
      "Epoch 2 | Step 1074800 | Avg Loss: 0.0148 | Grad Norm: 0.00943654\n",
      "Epoch 2 | Step 1074900 | Avg Loss: 0.0148 | Grad Norm: 0.00687432\n",
      "Epoch 2 | Step 1075000 | Avg Loss: 0.0148 | Grad Norm: 0.00949538\n",
      "Epoch 2 | Step 1075100 | Avg Loss: 0.0146 | Grad Norm: 0.00844908\n",
      "Epoch 2 | Step 1075200 | Avg Loss: 0.0146 | Grad Norm: 0.00882896\n",
      "Epoch 2 | Step 1075300 | Avg Loss: 0.0143 | Grad Norm: 0.00882732\n",
      "Epoch 2 | Step 1075400 | Avg Loss: 0.0148 | Grad Norm: 0.00999195\n",
      "Epoch 2 | Step 1075500 | Avg Loss: 0.0150 | Grad Norm: 0.00849257\n",
      "Epoch 2 | Step 1075600 | Avg Loss: 0.0149 | Grad Norm: 0.00898674\n",
      "Epoch 2 | Step 1075700 | Avg Loss: 0.0150 | Grad Norm: 0.00876405\n",
      "Epoch 2 | Step 1075800 | Avg Loss: 0.0152 | Grad Norm: 0.00791565\n",
      "Epoch 2 | Step 1075900 | Avg Loss: 0.0153 | Grad Norm: 0.00778423\n",
      "Epoch 2 | Step 1076000 | Avg Loss: 0.0157 | Grad Norm: 0.00799066\n",
      "Epoch 2 | Step 1076100 | Avg Loss: 0.0155 | Grad Norm: 0.00896892\n",
      "Epoch 2 | Step 1076200 | Avg Loss: 0.0154 | Grad Norm: 0.00903120\n",
      "Epoch 2 | Step 1076300 | Avg Loss: 0.0151 | Grad Norm: 0.00822242\n",
      "Epoch 2 | Step 1076400 | Avg Loss: 0.0149 | Grad Norm: 0.00886061\n",
      "Epoch 2 | Step 1076500 | Avg Loss: 0.0150 | Grad Norm: 0.00835659\n",
      "Epoch 2 | Step 1076600 | Avg Loss: 0.0150 | Grad Norm: 0.00726782\n",
      "Epoch 2 | Step 1076700 | Avg Loss: 0.0149 | Grad Norm: 0.00791472\n",
      "Epoch 2 | Step 1076800 | Avg Loss: 0.0151 | Grad Norm: 0.00812519\n",
      "Epoch 2 | Step 1076900 | Avg Loss: 0.0149 | Grad Norm: 0.00929359\n",
      "Epoch 2 | Step 1077000 | Avg Loss: 0.0149 | Grad Norm: 0.00956695\n",
      "Epoch 2 | Step 1077100 | Avg Loss: 0.0149 | Grad Norm: 0.00809995\n",
      "Epoch 2 | Step 1077200 | Avg Loss: 0.0152 | Grad Norm: 0.01026982\n",
      "Epoch 2 | Step 1077300 | Avg Loss: 0.0154 | Grad Norm: 0.00954693\n",
      "Epoch 2 | Step 1077400 | Avg Loss: 0.0156 | Grad Norm: 0.00856603\n",
      "Epoch 2 | Step 1077500 | Avg Loss: 0.0156 | Grad Norm: 0.00909567\n",
      "Epoch 2 | Step 1077600 | Avg Loss: 0.0158 | Grad Norm: 0.00869902\n",
      "Epoch 2 | Step 1077700 | Avg Loss: 0.0151 | Grad Norm: 0.01083802\n",
      "Epoch 2 | Step 1077800 | Avg Loss: 0.0149 | Grad Norm: 0.00739090\n",
      "Epoch 2 | Step 1077900 | Avg Loss: 0.0152 | Grad Norm: 0.00809633\n",
      "Epoch 2 | Step 1078000 | Avg Loss: 0.0149 | Grad Norm: 0.00829164\n",
      "Epoch 2 | Step 1078100 | Avg Loss: 0.0150 | Grad Norm: 0.00789520\n",
      "Epoch 2 | Step 1078200 | Avg Loss: 0.0151 | Grad Norm: 0.00830043\n",
      "Epoch 2 | Step 1078300 | Avg Loss: 0.0153 | Grad Norm: 0.00742132\n",
      "Epoch 2 | Step 1078400 | Avg Loss: 0.0153 | Grad Norm: 0.00865172\n",
      "Epoch 2 | Step 1078500 | Avg Loss: 0.0151 | Grad Norm: 0.00804041\n",
      "Epoch 2 | Step 1078600 | Avg Loss: 0.0152 | Grad Norm: 0.00887098\n",
      "Epoch 2 | Step 1078700 | Avg Loss: 0.0156 | Grad Norm: 0.00942990\n",
      "Epoch 2 | Step 1078800 | Avg Loss: 0.0158 | Grad Norm: 0.00854019\n",
      "Epoch 2 | Step 1078900 | Avg Loss: 0.0156 | Grad Norm: 0.00787643\n",
      "Epoch 2 | Step 1079000 | Avg Loss: 0.0156 | Grad Norm: 0.00920925\n",
      "Epoch 2 | Step 1079100 | Avg Loss: 0.0159 | Grad Norm: 0.00814895\n",
      "Epoch 2 | Step 1079200 | Avg Loss: 0.0159 | Grad Norm: 0.00809494\n",
      "Epoch 2 | Step 1079300 | Avg Loss: 0.0155 | Grad Norm: 0.01124200\n",
      "Epoch 2 | Step 1079400 | Avg Loss: 0.0150 | Grad Norm: 0.00854860\n",
      "Epoch 2 | Step 1079500 | Avg Loss: 0.0149 | Grad Norm: 0.00877218\n",
      "Epoch 2 | Step 1079600 | Avg Loss: 0.0154 | Grad Norm: 0.00805282\n",
      "Epoch 2 | Step 1079700 | Avg Loss: 0.0148 | Grad Norm: 0.00847043\n",
      "Epoch 2 | Step 1079800 | Avg Loss: 0.0150 | Grad Norm: 0.00854941\n",
      "Epoch 2 | Step 1079900 | Avg Loss: 0.0153 | Grad Norm: 0.00808139\n",
      "Epoch 2 | Step 1080000 | Avg Loss: 0.0150 | Grad Norm: 0.00910785\n",
      "Epoch 2 | Step 1080100 | Avg Loss: 0.0152 | Grad Norm: 0.00773683\n",
      "Epoch 2 | Step 1080200 | Avg Loss: 0.0149 | Grad Norm: 0.00981938\n",
      "Epoch 2 | Step 1080300 | Avg Loss: 0.0146 | Grad Norm: 0.00805284\n",
      "Epoch 2 | Step 1080400 | Avg Loss: 0.0150 | Grad Norm: 0.00883359\n",
      "Epoch 2 | Step 1080500 | Avg Loss: 0.0147 | Grad Norm: 0.00919022\n",
      "Epoch 2 | Step 1080600 | Avg Loss: 0.0145 | Grad Norm: 0.00792198\n",
      "Epoch 2 | Step 1080700 | Avg Loss: 0.0149 | Grad Norm: 0.00894088\n",
      "Epoch 2 | Step 1080800 | Avg Loss: 0.0145 | Grad Norm: 0.00784331\n",
      "Epoch 2 | Step 1080900 | Avg Loss: 0.0147 | Grad Norm: 0.00982413\n",
      "Epoch 2 | Step 1081000 | Avg Loss: 0.0145 | Grad Norm: 0.00788872\n",
      "Epoch 2 | Step 1081100 | Avg Loss: 0.0146 | Grad Norm: 0.00932132\n",
      "Epoch 2 | Step 1081200 | Avg Loss: 0.0144 | Grad Norm: 0.00901605\n",
      "Epoch 2 | Step 1081300 | Avg Loss: 0.0145 | Grad Norm: 0.00739618\n",
      "Epoch 2 | Step 1081400 | Avg Loss: 0.0145 | Grad Norm: 0.00914156\n",
      "Epoch 2 | Step 1081500 | Avg Loss: 0.0146 | Grad Norm: 0.00702109\n",
      "Epoch 2 | Step 1081600 | Avg Loss: 0.0142 | Grad Norm: 0.00843202\n",
      "Epoch 2 | Step 1081700 | Avg Loss: 0.0141 | Grad Norm: 0.00697194\n",
      "Epoch 2 | Step 1081800 | Avg Loss: 0.0143 | Grad Norm: 0.00979150\n",
      "Epoch 2 | Step 1081900 | Avg Loss: 0.0144 | Grad Norm: 0.00742424\n",
      "Epoch 2 | Step 1082000 | Avg Loss: 0.0142 | Grad Norm: 0.00883661\n",
      "Epoch 2 | Step 1082100 | Avg Loss: 0.0142 | Grad Norm: 0.00789758\n",
      "Epoch 2 | Step 1082200 | Avg Loss: 0.0145 | Grad Norm: 0.00837812\n",
      "Epoch 2 | Step 1082300 | Avg Loss: 0.0147 | Grad Norm: 0.00725800\n",
      "Epoch 2 | Step 1082400 | Avg Loss: 0.0145 | Grad Norm: 0.00856549\n",
      "Epoch 2 | Step 1082500 | Avg Loss: 0.0146 | Grad Norm: 0.00874881\n",
      "Epoch 2 | Step 1082600 | Avg Loss: 0.0145 | Grad Norm: 0.00917878\n",
      "Epoch 2 | Step 1082700 | Avg Loss: 0.0150 | Grad Norm: 0.01043407\n",
      "Epoch 2 | Step 1082800 | Avg Loss: 0.0147 | Grad Norm: 0.00801953\n",
      "Epoch 2 | Step 1082900 | Avg Loss: 0.0147 | Grad Norm: 0.00811141\n",
      "Epoch 2 | Step 1083000 | Avg Loss: 0.0150 | Grad Norm: 0.00778886\n",
      "Epoch 2 | Step 1083100 | Avg Loss: 0.0148 | Grad Norm: 0.00852196\n",
      "Epoch 2 | Step 1083200 | Avg Loss: 0.0146 | Grad Norm: 0.00829012\n",
      "Epoch 2 | Step 1083300 | Avg Loss: 0.0144 | Grad Norm: 0.00742128\n",
      "Epoch 2 | Step 1083400 | Avg Loss: 0.0144 | Grad Norm: 0.00927050\n",
      "Epoch 2 | Step 1083500 | Avg Loss: 0.0144 | Grad Norm: 0.00734195\n",
      "Epoch 2 | Step 1083600 | Avg Loss: 0.0145 | Grad Norm: 0.00912268\n",
      "Epoch 2 | Step 1083700 | Avg Loss: 0.0144 | Grad Norm: 0.00815017\n",
      "Epoch 2 | Step 1083800 | Avg Loss: 0.0143 | Grad Norm: 0.00910979\n",
      "Epoch 2 | Step 1083900 | Avg Loss: 0.0145 | Grad Norm: 0.00720238\n",
      "Epoch 2 | Step 1084000 | Avg Loss: 0.0146 | Grad Norm: 0.00819490\n",
      "Epoch 2 | Step 1084100 | Avg Loss: 0.0147 | Grad Norm: 0.00874728\n",
      "Epoch 2 | Step 1084200 | Avg Loss: 0.0147 | Grad Norm: 0.00737261\n",
      "Epoch 2 | Step 1084300 | Avg Loss: 0.0150 | Grad Norm: 0.00880352\n",
      "Epoch 2 | Step 1084400 | Avg Loss: 0.0150 | Grad Norm: 0.00967328\n",
      "Epoch 2 | Step 1084500 | Avg Loss: 0.0149 | Grad Norm: 0.00956168\n",
      "Epoch 2 | Step 1084600 | Avg Loss: 0.0148 | Grad Norm: 0.00894272\n",
      "Epoch 2 | Step 1084700 | Avg Loss: 0.0150 | Grad Norm: 0.00823033\n",
      "Epoch 2 | Step 1084800 | Avg Loss: 0.0148 | Grad Norm: 0.00974419\n",
      "Epoch 2 | Step 1084900 | Avg Loss: 0.0146 | Grad Norm: 0.00793240\n",
      "Epoch 2 | Step 1085000 | Avg Loss: 0.0145 | Grad Norm: 0.00842298\n",
      "Epoch 2 | Step 1085100 | Avg Loss: 0.0145 | Grad Norm: 0.00775862\n",
      "Epoch 2 | Step 1085200 | Avg Loss: 0.0147 | Grad Norm: 0.00826438\n",
      "Epoch 2 | Step 1085300 | Avg Loss: 0.0149 | Grad Norm: 0.01214758\n",
      "Epoch 2 | Step 1085400 | Avg Loss: 0.0148 | Grad Norm: 0.00828144\n",
      "Epoch 2 | Step 1085500 | Avg Loss: 0.0148 | Grad Norm: 0.01029575\n",
      "Epoch 2 | Step 1085600 | Avg Loss: 0.0147 | Grad Norm: 0.00832736\n",
      "Epoch 2 | Step 1085700 | Avg Loss: 0.0144 | Grad Norm: 0.00845408\n",
      "Epoch 2 | Step 1085800 | Avg Loss: 0.0149 | Grad Norm: 0.00811466\n",
      "Epoch 2 | Step 1085900 | Avg Loss: 0.0145 | Grad Norm: 0.00742397\n",
      "Epoch 2 | Step 1086000 | Avg Loss: 0.0148 | Grad Norm: 0.00847653\n",
      "Epoch 2 | Step 1086100 | Avg Loss: 0.0145 | Grad Norm: 0.00783856\n",
      "Epoch 2 | Step 1086200 | Avg Loss: 0.0144 | Grad Norm: 0.00856547\n",
      "Epoch 2 | Step 1086300 | Avg Loss: 0.0147 | Grad Norm: 0.00940381\n",
      "Epoch 2 | Step 1086400 | Avg Loss: 0.0148 | Grad Norm: 0.00817552\n",
      "Epoch 2 | Step 1086500 | Avg Loss: 0.0146 | Grad Norm: 0.00800539\n",
      "Epoch 2 | Step 1086600 | Avg Loss: 0.0149 | Grad Norm: 0.00755041\n",
      "Epoch 2 | Step 1086700 | Avg Loss: 0.0147 | Grad Norm: 0.00804956\n",
      "Epoch 2 | Step 1086800 | Avg Loss: 0.0149 | Grad Norm: 0.00888201\n",
      "Epoch 2 | Step 1086900 | Avg Loss: 0.0149 | Grad Norm: 0.00807886\n",
      "Epoch 2 | Step 1087000 | Avg Loss: 0.0147 | Grad Norm: 0.00846805\n",
      "Epoch 2 | Step 1087100 | Avg Loss: 0.0149 | Grad Norm: 0.00886106\n",
      "Epoch 2 | Step 1087200 | Avg Loss: 0.0147 | Grad Norm: 0.00967296\n",
      "Epoch 2 | Step 1087300 | Avg Loss: 0.0150 | Grad Norm: 0.00746803\n",
      "Epoch 2 | Step 1087400 | Avg Loss: 0.0145 | Grad Norm: 0.00866883\n",
      "Epoch 2 | Step 1087500 | Avg Loss: 0.0143 | Grad Norm: 0.00835902\n",
      "Epoch 2 | Step 1087600 | Avg Loss: 0.0145 | Grad Norm: 0.00832706\n",
      "Epoch 2 | Step 1087700 | Avg Loss: 0.0149 | Grad Norm: 0.00919855\n",
      "Epoch 2 | Step 1087800 | Avg Loss: 0.0147 | Grad Norm: 0.00662915\n",
      "Epoch 2 | Step 1087900 | Avg Loss: 0.0148 | Grad Norm: 0.00983950\n",
      "Epoch 2 | Step 1088000 | Avg Loss: 0.0145 | Grad Norm: 0.00775597\n",
      "Epoch 2 | Step 1088100 | Avg Loss: 0.0146 | Grad Norm: 0.00865164\n",
      "Epoch 2 | Step 1088200 | Avg Loss: 0.0149 | Grad Norm: 0.00798074\n",
      "Epoch 2 | Step 1088300 | Avg Loss: 0.0148 | Grad Norm: 0.00901976\n",
      "Epoch 2 | Step 1088400 | Avg Loss: 0.0150 | Grad Norm: 0.00723596\n",
      "Epoch 2 | Step 1088500 | Avg Loss: 0.0148 | Grad Norm: 0.00702722\n",
      "Epoch 2 | Step 1088600 | Avg Loss: 0.0148 | Grad Norm: 0.00759059\n",
      "Epoch 2 | Step 1088700 | Avg Loss: 0.0153 | Grad Norm: 0.00893371\n",
      "Epoch 2 | Step 1088800 | Avg Loss: 0.0151 | Grad Norm: 0.00931606\n",
      "Epoch 2 | Step 1088900 | Avg Loss: 0.0148 | Grad Norm: 0.00872526\n",
      "Epoch 2 | Step 1089000 | Avg Loss: 0.0149 | Grad Norm: 0.00751635\n",
      "Epoch 2 | Step 1089100 | Avg Loss: 0.0147 | Grad Norm: 0.00851850\n",
      "Epoch 2 | Step 1089200 | Avg Loss: 0.0148 | Grad Norm: 0.00913872\n",
      "Epoch 2 | Step 1089300 | Avg Loss: 0.0152 | Grad Norm: 0.00776399\n",
      "Epoch 2 | Step 1089400 | Avg Loss: 0.0148 | Grad Norm: 0.00774573\n",
      "Epoch 2 | Step 1089500 | Avg Loss: 0.0145 | Grad Norm: 0.00931178\n",
      "Epoch 2 | Step 1089600 | Avg Loss: 0.0146 | Grad Norm: 0.01075585\n",
      "Epoch 2 | Step 1089700 | Avg Loss: 0.0149 | Grad Norm: 0.00963606\n",
      "Epoch 2 | Step 1089800 | Avg Loss: 0.0148 | Grad Norm: 0.00877528\n",
      "Epoch 2 | Step 1089900 | Avg Loss: 0.0146 | Grad Norm: 0.00917869\n",
      "Epoch 2 | Step 1090000 | Avg Loss: 0.0146 | Grad Norm: 0.00884271\n",
      "Epoch 2 | Step 1090100 | Avg Loss: 0.0149 | Grad Norm: 0.00788389\n",
      "Epoch 2 | Step 1090200 | Avg Loss: 0.0146 | Grad Norm: 0.00817328\n",
      "Epoch 2 | Step 1090300 | Avg Loss: 0.0146 | Grad Norm: 0.00913217\n",
      "Epoch 2 | Step 1090400 | Avg Loss: 0.0143 | Grad Norm: 0.00780582\n",
      "Epoch 2 | Step 1090500 | Avg Loss: 0.0149 | Grad Norm: 0.00753605\n",
      "Epoch 2 | Step 1090600 | Avg Loss: 0.0150 | Grad Norm: 0.00885813\n",
      "Epoch 2 | Step 1090700 | Avg Loss: 0.0147 | Grad Norm: 0.00973959\n",
      "Epoch 2 | Step 1090800 | Avg Loss: 0.0142 | Grad Norm: 0.00750572\n",
      "Epoch 2 | Step 1090900 | Avg Loss: 0.0147 | Grad Norm: 0.00774412\n",
      "Epoch 2 | Step 1091000 | Avg Loss: 0.0147 | Grad Norm: 0.00815271\n",
      "Epoch 2 | Step 1091100 | Avg Loss: 0.0145 | Grad Norm: 0.00873850\n",
      "Epoch 2 | Step 1091200 | Avg Loss: 0.0149 | Grad Norm: 0.00899649\n",
      "Epoch 2 | Step 1091300 | Avg Loss: 0.0148 | Grad Norm: 0.00957499\n",
      "Epoch 2 | Step 1091400 | Avg Loss: 0.0151 | Grad Norm: 0.00876708\n",
      "Epoch 2 | Step 1091500 | Avg Loss: 0.0151 | Grad Norm: 0.00924676\n",
      "Epoch 2 | Step 1091600 | Avg Loss: 0.0149 | Grad Norm: 0.00668138\n",
      "Epoch 2 | Step 1091700 | Avg Loss: 0.0145 | Grad Norm: 0.00956608\n",
      "Epoch 2 | Step 1091800 | Avg Loss: 0.0148 | Grad Norm: 0.00918779\n",
      "Epoch 2 | Step 1091900 | Avg Loss: 0.0148 | Grad Norm: 0.00830159\n",
      "Epoch 2 | Step 1092000 | Avg Loss: 0.0149 | Grad Norm: 0.00746642\n",
      "Epoch 2 | Step 1092100 | Avg Loss: 0.0150 | Grad Norm: 0.00800821\n",
      "Epoch 2 | Step 1092200 | Avg Loss: 0.0147 | Grad Norm: 0.00762700\n",
      "Epoch 2 | Step 1092300 | Avg Loss: 0.0149 | Grad Norm: 0.00809325\n",
      "Epoch 2 | Step 1092400 | Avg Loss: 0.0151 | Grad Norm: 0.00753293\n",
      "Epoch 2 | Step 1092500 | Avg Loss: 0.0154 | Grad Norm: 0.01031144\n",
      "Epoch 2 | Step 1092600 | Avg Loss: 0.0153 | Grad Norm: 0.00988379\n",
      "Epoch 2 | Step 1092700 | Avg Loss: 0.0151 | Grad Norm: 0.00854883\n",
      "Epoch 2 | Step 1092800 | Avg Loss: 0.0149 | Grad Norm: 0.00988275\n",
      "Epoch 2 | Step 1092900 | Avg Loss: 0.0154 | Grad Norm: 0.00890096\n",
      "Epoch 2 | Step 1093000 | Avg Loss: 0.0148 | Grad Norm: 0.00746418\n",
      "Epoch 2 | Step 1093100 | Avg Loss: 0.0149 | Grad Norm: 0.00823016\n",
      "Epoch 2 | Step 1093200 | Avg Loss: 0.0148 | Grad Norm: 0.00825266\n",
      "Epoch 2 | Step 1093300 | Avg Loss: 0.0150 | Grad Norm: 0.00825829\n",
      "Epoch 2 | Step 1093400 | Avg Loss: 0.0153 | Grad Norm: 0.00743651\n",
      "Epoch 2 | Step 1093500 | Avg Loss: 0.0151 | Grad Norm: 0.00818141\n",
      "Epoch 2 | Step 1093600 | Avg Loss: 0.0152 | Grad Norm: 0.00927336\n",
      "Epoch 2 | Step 1093700 | Avg Loss: 0.0152 | Grad Norm: 0.00720365\n",
      "Epoch 2 | Step 1093800 | Avg Loss: 0.0154 | Grad Norm: 0.00804152\n",
      "Epoch 2 | Step 1093900 | Avg Loss: 0.0154 | Grad Norm: 0.00882854\n",
      "Epoch 2 | Step 1094000 | Avg Loss: 0.0153 | Grad Norm: 0.01048102\n",
      "Epoch 2 | Step 1094100 | Avg Loss: 0.0149 | Grad Norm: 0.00816819\n",
      "Epoch 2 | Step 1094200 | Avg Loss: 0.0149 | Grad Norm: 0.00762768\n",
      "Epoch 2 | Step 1094300 | Avg Loss: 0.0149 | Grad Norm: 0.00767381\n",
      "Epoch 2 | Step 1094400 | Avg Loss: 0.0148 | Grad Norm: 0.00810189\n",
      "Epoch 2 | Step 1094500 | Avg Loss: 0.0150 | Grad Norm: 0.00730087\n",
      "Epoch 2 | Step 1094600 | Avg Loss: 0.0146 | Grad Norm: 0.00868694\n",
      "Epoch 2 | Step 1094700 | Avg Loss: 0.0146 | Grad Norm: 0.00939380\n",
      "Epoch 2 | Step 1094800 | Avg Loss: 0.0147 | Grad Norm: 0.00854751\n",
      "Epoch 2 | Step 1094900 | Avg Loss: 0.0142 | Grad Norm: 0.00804763\n",
      "Epoch 2 | Step 1095000 | Avg Loss: 0.0148 | Grad Norm: 0.00857980\n",
      "Epoch 2 | Step 1095100 | Avg Loss: 0.0148 | Grad Norm: 0.00960752\n",
      "Epoch 2 | Step 1095200 | Avg Loss: 0.0143 | Grad Norm: 0.00813087\n",
      "Epoch 2 | Step 1095300 | Avg Loss: 0.0143 | Grad Norm: 0.00745348\n",
      "Epoch 2 | Step 1095400 | Avg Loss: 0.0145 | Grad Norm: 0.00759102\n",
      "Epoch 2 | Step 1095500 | Avg Loss: 0.0141 | Grad Norm: 0.00941836\n",
      "Epoch 2 | Step 1095600 | Avg Loss: 0.0142 | Grad Norm: 0.00852748\n",
      "Epoch 2 | Step 1095700 | Avg Loss: 0.0143 | Grad Norm: 0.00762482\n",
      "Epoch 2 | Step 1095800 | Avg Loss: 0.0149 | Grad Norm: 0.00764026\n",
      "Epoch 2 | Step 1095900 | Avg Loss: 0.0150 | Grad Norm: 0.00845307\n",
      "Epoch 2 | Step 1096000 | Avg Loss: 0.0152 | Grad Norm: 0.00754419\n",
      "Epoch 2 | Step 1096100 | Avg Loss: 0.0155 | Grad Norm: 0.00874977\n",
      "Epoch 2 | Step 1096200 | Avg Loss: 0.0156 | Grad Norm: 0.00883590\n",
      "Epoch 2 | Step 1096300 | Avg Loss: 0.0156 | Grad Norm: 0.00905656\n",
      "Epoch 2 | Step 1096400 | Avg Loss: 0.0153 | Grad Norm: 0.00886194\n",
      "Epoch 2 | Step 1096500 | Avg Loss: 0.0152 | Grad Norm: 0.00827027\n",
      "Epoch 2 | Step 1096600 | Avg Loss: 0.0151 | Grad Norm: 0.00806106\n",
      "Epoch 2 | Step 1096700 | Avg Loss: 0.0150 | Grad Norm: 0.00831018\n",
      "Epoch 2 | Step 1096800 | Avg Loss: 0.0151 | Grad Norm: 0.01019407\n",
      "Epoch 2 | Step 1096900 | Avg Loss: 0.0151 | Grad Norm: 0.00795802\n",
      "Epoch 2 | Step 1097000 | Avg Loss: 0.0152 | Grad Norm: 0.00856589\n",
      "Epoch 2 | Step 1097100 | Avg Loss: 0.0153 | Grad Norm: 0.01072376\n",
      "Epoch 2 | Step 1097200 | Avg Loss: 0.0152 | Grad Norm: 0.00932837\n",
      "Epoch 2 | Step 1097300 | Avg Loss: 0.0151 | Grad Norm: 0.00868488\n",
      "Epoch 2 | Step 1097400 | Avg Loss: 0.0150 | Grad Norm: 0.00938030\n",
      "Epoch 2 | Step 1097500 | Avg Loss: 0.0149 | Grad Norm: 0.00808110\n",
      "Epoch 2 | Step 1097600 | Avg Loss: 0.0151 | Grad Norm: 0.00992501\n",
      "Epoch 2 | Step 1097700 | Avg Loss: 0.0148 | Grad Norm: 0.00991342\n",
      "Epoch 2 | Step 1097800 | Avg Loss: 0.0150 | Grad Norm: 0.00928268\n",
      "Epoch 2 | Step 1097900 | Avg Loss: 0.0152 | Grad Norm: 0.00958182\n",
      "Epoch 2 | Step 1098000 | Avg Loss: 0.0151 | Grad Norm: 0.00911948\n",
      "Epoch 2 | Step 1098100 | Avg Loss: 0.0151 | Grad Norm: 0.00843484\n",
      "Epoch 2 | Step 1098200 | Avg Loss: 0.0148 | Grad Norm: 0.00903181\n",
      "Epoch 2 | Step 1098300 | Avg Loss: 0.0156 | Grad Norm: 0.00757097\n",
      "Epoch 2 | Step 1098400 | Avg Loss: 0.0155 | Grad Norm: 0.00848027\n",
      "Epoch 2 | Step 1098500 | Avg Loss: 0.0156 | Grad Norm: 0.01023313\n",
      "Epoch 2 | Step 1098600 | Avg Loss: 0.0154 | Grad Norm: 0.00917742\n",
      "Epoch 2 | Step 1098700 | Avg Loss: 0.0152 | Grad Norm: 0.00657871\n",
      "Epoch 2 | Step 1098800 | Avg Loss: 0.0151 | Grad Norm: 0.00997306\n",
      "Epoch 2 | Step 1098900 | Avg Loss: 0.0152 | Grad Norm: 0.01024857\n",
      "Epoch 2 | Step 1099000 | Avg Loss: 0.0149 | Grad Norm: 0.00844967\n",
      "Epoch 2 | Step 1099100 | Avg Loss: 0.0148 | Grad Norm: 0.00819285\n",
      "Epoch 2 | Step 1099200 | Avg Loss: 0.0147 | Grad Norm: 0.01074510\n",
      "Epoch 2 | Step 1099300 | Avg Loss: 0.0150 | Grad Norm: 0.00766745\n",
      "Epoch 2 | Step 1099400 | Avg Loss: 0.0150 | Grad Norm: 0.00891821\n",
      "Epoch 2 | Step 1099500 | Avg Loss: 0.0148 | Grad Norm: 0.00795132\n",
      "Epoch 2 | Step 1099600 | Avg Loss: 0.0148 | Grad Norm: 0.00942546\n",
      "Epoch 2 | Step 1099700 | Avg Loss: 0.0145 | Grad Norm: 0.00895072\n",
      "Epoch 2 | Step 1099800 | Avg Loss: 0.0146 | Grad Norm: 0.00963765\n",
      "Epoch 2 | Step 1099900 | Avg Loss: 0.0153 | Grad Norm: 0.00836299\n",
      "Epoch 2 | Step 1100000 | Avg Loss: 0.0151 | Grad Norm: 0.00855795\n",
      "Saving model at step1100000\n",
      "Epoch 2 | Step 1100100 | Avg Loss: 0.0152 | Grad Norm: 0.00775684\n",
      "Epoch 2 | Step 1100200 | Avg Loss: 0.0153 | Grad Norm: 0.00881125\n",
      "Epoch 2 | Step 1100300 | Avg Loss: 0.0152 | Grad Norm: 0.00918111\n",
      "Epoch 2 | Step 1100400 | Avg Loss: 0.0155 | Grad Norm: 0.00946951\n",
      "Epoch 2 | Step 1100500 | Avg Loss: 0.0156 | Grad Norm: 0.00873416\n",
      "Epoch 2 | Step 1100600 | Avg Loss: 0.0153 | Grad Norm: 0.00939995\n",
      "Epoch 2 | Step 1100700 | Avg Loss: 0.0152 | Grad Norm: 0.00769510\n",
      "Epoch 2 | Step 1100800 | Avg Loss: 0.0153 | Grad Norm: 0.00975760\n",
      "Epoch 2 | Step 1100900 | Avg Loss: 0.0157 | Grad Norm: 0.00731991\n",
      "Epoch 2 | Step 1101000 | Avg Loss: 0.0159 | Grad Norm: 0.00861592\n",
      "Epoch 2 | Step 1101100 | Avg Loss: 0.0160 | Grad Norm: 0.00827131\n",
      "Epoch 2 | Step 1101200 | Avg Loss: 0.0160 | Grad Norm: 0.00941017\n",
      "Epoch 2 | Step 1101300 | Avg Loss: 0.0158 | Grad Norm: 0.01028099\n",
      "Epoch 2 | Step 1101400 | Avg Loss: 0.0153 | Grad Norm: 0.01104491\n",
      "Epoch 2 | Step 1101500 | Avg Loss: 0.0152 | Grad Norm: 0.00851756\n",
      "Epoch 2 | Step 1101600 | Avg Loss: 0.0155 | Grad Norm: 0.00808219\n",
      "Epoch 2 | Step 1101700 | Avg Loss: 0.0154 | Grad Norm: 0.00748498\n",
      "Epoch 2 | Step 1101800 | Avg Loss: 0.0159 | Grad Norm: 0.00998775\n",
      "Epoch 2 | Step 1101900 | Avg Loss: 0.0158 | Grad Norm: 0.00812490\n",
      "Epoch 2 | Step 1102000 | Avg Loss: 0.0153 | Grad Norm: 0.00883484\n",
      "Epoch 2 | Step 1102100 | Avg Loss: 0.0152 | Grad Norm: 0.00782684\n",
      "Epoch 2 | Step 1102200 | Avg Loss: 0.0152 | Grad Norm: 0.00892692\n",
      "Epoch 2 | Step 1102300 | Avg Loss: 0.0152 | Grad Norm: 0.00854156\n",
      "Epoch 2 | Step 1102400 | Avg Loss: 0.0154 | Grad Norm: 0.00945514\n",
      "Epoch 2 | Step 1102500 | Avg Loss: 0.0154 | Grad Norm: 0.00852731\n",
      "Epoch 2 | Step 1102600 | Avg Loss: 0.0151 | Grad Norm: 0.00977869\n",
      "Epoch 2 | Step 1102700 | Avg Loss: 0.0151 | Grad Norm: 0.00865504\n",
      "Epoch 2 | Step 1102800 | Avg Loss: 0.0151 | Grad Norm: 0.00905039\n",
      "Epoch 2 | Step 1102900 | Avg Loss: 0.0150 | Grad Norm: 0.00797522\n",
      "Epoch 2 | Step 1103000 | Avg Loss: 0.0153 | Grad Norm: 0.00813739\n",
      "Epoch 2 | Step 1103100 | Avg Loss: 0.0152 | Grad Norm: 0.00866913\n",
      "Epoch 2 | Step 1103200 | Avg Loss: 0.0156 | Grad Norm: 0.00767541\n",
      "Epoch 2 | Step 1103300 | Avg Loss: 0.0151 | Grad Norm: 0.00992139\n",
      "Epoch 2 | Step 1103400 | Avg Loss: 0.0148 | Grad Norm: 0.00776229\n",
      "Epoch 2 | Step 1103500 | Avg Loss: 0.0146 | Grad Norm: 0.00813823\n",
      "Epoch 2 | Step 1103600 | Avg Loss: 0.0149 | Grad Norm: 0.01105773\n",
      "Epoch 2 | Step 1103700 | Avg Loss: 0.0149 | Grad Norm: 0.00883995\n",
      "Epoch 2 | Step 1103800 | Avg Loss: 0.0149 | Grad Norm: 0.00841913\n",
      "Epoch 2 | Step 1103900 | Avg Loss: 0.0148 | Grad Norm: 0.00830026\n",
      "Epoch 2 | Step 1104000 | Avg Loss: 0.0146 | Grad Norm: 0.00944355\n",
      "Epoch 2 | Step 1104100 | Avg Loss: 0.0150 | Grad Norm: 0.00790372\n",
      "Epoch 2 | Step 1104200 | Avg Loss: 0.0150 | Grad Norm: 0.00848619\n",
      "Epoch 2 | Step 1104300 | Avg Loss: 0.0150 | Grad Norm: 0.00787201\n",
      "Epoch 2 | Step 1104400 | Avg Loss: 0.0147 | Grad Norm: 0.00792293\n",
      "Epoch 2 | Step 1104500 | Avg Loss: 0.0148 | Grad Norm: 0.00841903\n",
      "Epoch 2 | Step 1104600 | Avg Loss: 0.0149 | Grad Norm: 0.00844421\n",
      "Epoch 2 | Step 1104700 | Avg Loss: 0.0149 | Grad Norm: 0.00834550\n",
      "Epoch 2 | Step 1104800 | Avg Loss: 0.0153 | Grad Norm: 0.00938539\n",
      "Epoch 2 | Step 1104900 | Avg Loss: 0.0154 | Grad Norm: 0.00820350\n",
      "Epoch 2 | Step 1105000 | Avg Loss: 0.0153 | Grad Norm: 0.00787685\n",
      "Epoch 2 | Step 1105100 | Avg Loss: 0.0147 | Grad Norm: 0.00828939\n",
      "Epoch 2 | Step 1105200 | Avg Loss: 0.0147 | Grad Norm: 0.00884775\n",
      "Epoch 2 | Step 1105300 | Avg Loss: 0.0148 | Grad Norm: 0.00758830\n",
      "Epoch 2 | Step 1105400 | Avg Loss: 0.0151 | Grad Norm: 0.00880294\n",
      "Epoch 2 | Step 1105500 | Avg Loss: 0.0148 | Grad Norm: 0.00784607\n",
      "Epoch 2 | Step 1105600 | Avg Loss: 0.0150 | Grad Norm: 0.00780827\n",
      "Epoch 2 | Step 1105700 | Avg Loss: 0.0148 | Grad Norm: 0.00860833\n",
      "Epoch 2 | Step 1105800 | Avg Loss: 0.0149 | Grad Norm: 0.00879855\n",
      "Epoch 2 | Step 1105900 | Avg Loss: 0.0149 | Grad Norm: 0.00899844\n",
      "Epoch 2 | Step 1106000 | Avg Loss: 0.0151 | Grad Norm: 0.00818017\n",
      "Epoch 2 | Step 1106100 | Avg Loss: 0.0154 | Grad Norm: 0.01054132\n",
      "Epoch 2 | Step 1106200 | Avg Loss: 0.0157 | Grad Norm: 0.00761467\n",
      "Epoch 2 | Step 1106300 | Avg Loss: 0.0153 | Grad Norm: 0.00868958\n",
      "Epoch 2 | Step 1106400 | Avg Loss: 0.0151 | Grad Norm: 0.00771170\n",
      "Epoch 2 | Step 1106500 | Avg Loss: 0.0146 | Grad Norm: 0.00859458\n",
      "Epoch 2 | Step 1106600 | Avg Loss: 0.0147 | Grad Norm: 0.00747638\n",
      "Epoch 2 | Step 1106700 | Avg Loss: 0.0148 | Grad Norm: 0.00949300\n",
      "Epoch 2 | Step 1106800 | Avg Loss: 0.0149 | Grad Norm: 0.00826942\n",
      "Epoch 2 | Step 1106900 | Avg Loss: 0.0148 | Grad Norm: 0.00789698\n",
      "Epoch 2 | Step 1107000 | Avg Loss: 0.0148 | Grad Norm: 0.00916096\n",
      "Epoch 2 | Step 1107100 | Avg Loss: 0.0147 | Grad Norm: 0.00788906\n",
      "Epoch 2 | Step 1107200 | Avg Loss: 0.0147 | Grad Norm: 0.00695087\n",
      "Epoch 2 | Step 1107300 | Avg Loss: 0.0148 | Grad Norm: 0.00724774\n",
      "Epoch 2 | Step 1107400 | Avg Loss: 0.0147 | Grad Norm: 0.00634339\n",
      "Epoch 2 | Step 1107500 | Avg Loss: 0.0141 | Grad Norm: 0.00841258\n",
      "Epoch 2 | Step 1107600 | Avg Loss: 0.0141 | Grad Norm: 0.00963921\n",
      "Epoch 2 | Step 1107700 | Avg Loss: 0.0141 | Grad Norm: 0.00882558\n",
      "Epoch 2 | Step 1107800 | Avg Loss: 0.0147 | Grad Norm: 0.00792032\n",
      "Epoch 2 | Step 1107900 | Avg Loss: 0.0147 | Grad Norm: 0.00967388\n",
      "Epoch 2 | Step 1108000 | Avg Loss: 0.0145 | Grad Norm: 0.00718764\n",
      "Epoch 2 | Step 1108100 | Avg Loss: 0.0151 | Grad Norm: 0.00757331\n",
      "Epoch 2 | Step 1108200 | Avg Loss: 0.0151 | Grad Norm: 0.00786178\n",
      "Epoch 2 | Step 1108300 | Avg Loss: 0.0149 | Grad Norm: 0.00929407\n",
      "Epoch 2 | Step 1108400 | Avg Loss: 0.0152 | Grad Norm: 0.00701085\n",
      "Epoch 2 | Step 1108500 | Avg Loss: 0.0153 | Grad Norm: 0.00757635\n",
      "Epoch 2 | Step 1108600 | Avg Loss: 0.0148 | Grad Norm: 0.01009989\n",
      "Epoch 2 | Step 1108700 | Avg Loss: 0.0152 | Grad Norm: 0.00763883\n",
      "Epoch 2 | Step 1108800 | Avg Loss: 0.0152 | Grad Norm: 0.00857163\n",
      "Epoch 2 | Step 1108900 | Avg Loss: 0.0150 | Grad Norm: 0.00796002\n",
      "Epoch 2 | Step 1109000 | Avg Loss: 0.0145 | Grad Norm: 0.00789638\n",
      "Epoch 2 | Step 1109100 | Avg Loss: 0.0141 | Grad Norm: 0.00872124\n",
      "Epoch 2 | Step 1109200 | Avg Loss: 0.0142 | Grad Norm: 0.00908703\n",
      "Epoch 2 | Step 1109300 | Avg Loss: 0.0142 | Grad Norm: 0.00751056\n",
      "Epoch 2 | Step 1109400 | Avg Loss: 0.0144 | Grad Norm: 0.00903139\n",
      "Epoch 2 | Step 1109500 | Avg Loss: 0.0147 | Grad Norm: 0.00803737\n",
      "Epoch 2 | Step 1109600 | Avg Loss: 0.0147 | Grad Norm: 0.00897871\n",
      "Epoch 2 | Step 1109700 | Avg Loss: 0.0146 | Grad Norm: 0.00783550\n",
      "Epoch 2 | Step 1109800 | Avg Loss: 0.0149 | Grad Norm: 0.00889285\n",
      "Epoch 2 | Step 1109900 | Avg Loss: 0.0147 | Grad Norm: 0.00760710\n",
      "Epoch 2 | Step 1110000 | Avg Loss: 0.0149 | Grad Norm: 0.00869938\n",
      "Epoch 2 | Step 1110100 | Avg Loss: 0.0153 | Grad Norm: 0.00835026\n",
      "Epoch 2 | Step 1110200 | Avg Loss: 0.0152 | Grad Norm: 0.00815125\n",
      "Epoch 2 | Step 1110300 | Avg Loss: 0.0151 | Grad Norm: 0.00896740\n",
      "Epoch 2 | Step 1110400 | Avg Loss: 0.0147 | Grad Norm: 0.00805471\n",
      "Epoch 2 | Step 1110500 | Avg Loss: 0.0149 | Grad Norm: 0.00951540\n",
      "Epoch 2 | Step 1110600 | Avg Loss: 0.0149 | Grad Norm: 0.01062924\n",
      "Epoch 2 | Step 1110700 | Avg Loss: 0.0146 | Grad Norm: 0.00764719\n",
      "Epoch 2 | Step 1110800 | Avg Loss: 0.0144 | Grad Norm: 0.00810973\n",
      "Epoch 2 | Step 1110900 | Avg Loss: 0.0147 | Grad Norm: 0.01039426\n",
      "Epoch 2 | Step 1111000 | Avg Loss: 0.0145 | Grad Norm: 0.00730104\n",
      "Epoch 2 | Step 1111100 | Avg Loss: 0.0150 | Grad Norm: 0.01016471\n",
      "Epoch 2 | Step 1111200 | Avg Loss: 0.0149 | Grad Norm: 0.00767678\n",
      "Epoch 2 | Step 1111300 | Avg Loss: 0.0145 | Grad Norm: 0.00756987\n",
      "Epoch 2 | Step 1111400 | Avg Loss: 0.0143 | Grad Norm: 0.01135985\n",
      "Epoch 2 | Step 1111500 | Avg Loss: 0.0148 | Grad Norm: 0.00799778\n",
      "Epoch 2 | Step 1111600 | Avg Loss: 0.0148 | Grad Norm: 0.00836477\n",
      "Epoch 2 | Step 1111700 | Avg Loss: 0.0153 | Grad Norm: 0.00778515\n",
      "Epoch 2 | Step 1111800 | Avg Loss: 0.0151 | Grad Norm: 0.00926638\n",
      "Epoch 2 | Step 1111900 | Avg Loss: 0.0154 | Grad Norm: 0.00960314\n",
      "Epoch 2 | Step 1112000 | Avg Loss: 0.0155 | Grad Norm: 0.00937242\n",
      "Epoch 2 | Step 1112100 | Avg Loss: 0.0156 | Grad Norm: 0.00861096\n",
      "Epoch 2 | Step 1112200 | Avg Loss: 0.0155 | Grad Norm: 0.00751900\n",
      "Epoch 2 | Step 1112300 | Avg Loss: 0.0156 | Grad Norm: 0.00770379\n",
      "Epoch 2 | Step 1112400 | Avg Loss: 0.0157 | Grad Norm: 0.00772490\n",
      "Epoch 2 | Step 1112500 | Avg Loss: 0.0154 | Grad Norm: 0.00710214\n",
      "Epoch 2 | Step 1112600 | Avg Loss: 0.0154 | Grad Norm: 0.00910899\n",
      "Epoch 2 | Step 1112700 | Avg Loss: 0.0153 | Grad Norm: 0.00990696\n",
      "Epoch 2 | Step 1112800 | Avg Loss: 0.0160 | Grad Norm: 0.00959781\n",
      "Epoch 2 | Step 1112900 | Avg Loss: 0.0158 | Grad Norm: 0.00808884\n",
      "Epoch 2 | Step 1113000 | Avg Loss: 0.0156 | Grad Norm: 0.00822163\n",
      "Epoch 2 | Step 1113100 | Avg Loss: 0.0153 | Grad Norm: 0.00833187\n",
      "Epoch 2 | Step 1113200 | Avg Loss: 0.0151 | Grad Norm: 0.00806577\n",
      "Epoch 2 | Step 1113300 | Avg Loss: 0.0152 | Grad Norm: 0.00920487\n",
      "Epoch 2 | Step 1113400 | Avg Loss: 0.0152 | Grad Norm: 0.00629829\n",
      "Epoch 2 | Step 1113500 | Avg Loss: 0.0151 | Grad Norm: 0.00726483\n",
      "Epoch 2 | Step 1113600 | Avg Loss: 0.0149 | Grad Norm: 0.00901177\n",
      "Epoch 2 | Step 1113700 | Avg Loss: 0.0150 | Grad Norm: 0.00771820\n",
      "Epoch 2 | Step 1113800 | Avg Loss: 0.0151 | Grad Norm: 0.00857109\n",
      "Epoch 2 | Step 1113900 | Avg Loss: 0.0150 | Grad Norm: 0.00816831\n",
      "Epoch 2 | Step 1114000 | Avg Loss: 0.0149 | Grad Norm: 0.00841266\n",
      "Epoch 2 | Step 1114100 | Avg Loss: 0.0151 | Grad Norm: 0.00797315\n",
      "Epoch 2 | Step 1114200 | Avg Loss: 0.0153 | Grad Norm: 0.00828236\n",
      "Epoch 2 | Step 1114300 | Avg Loss: 0.0151 | Grad Norm: 0.00793719\n",
      "Epoch 2 | Step 1114400 | Avg Loss: 0.0152 | Grad Norm: 0.00969583\n",
      "Epoch 2 | Step 1114500 | Avg Loss: 0.0150 | Grad Norm: 0.00801697\n",
      "Epoch 2 | Step 1114600 | Avg Loss: 0.0156 | Grad Norm: 0.00798506\n",
      "Epoch 2 | Step 1114700 | Avg Loss: 0.0153 | Grad Norm: 0.00822470\n",
      "Epoch 2 | Step 1114800 | Avg Loss: 0.0154 | Grad Norm: 0.00804537\n",
      "Epoch 2 | Step 1114900 | Avg Loss: 0.0151 | Grad Norm: 0.00821681\n",
      "Epoch 2 | Step 1115000 | Avg Loss: 0.0150 | Grad Norm: 0.00882443\n",
      "Epoch 2 | Step 1115100 | Avg Loss: 0.0152 | Grad Norm: 0.00778143\n",
      "Epoch 2 | Step 1115200 | Avg Loss: 0.0151 | Grad Norm: 0.00818158\n",
      "Epoch 2 | Step 1115300 | Avg Loss: 0.0151 | Grad Norm: 0.00869465\n",
      "Epoch 2 | Step 1115400 | Avg Loss: 0.0156 | Grad Norm: 0.00846905\n",
      "Epoch 2 | Step 1115500 | Avg Loss: 0.0158 | Grad Norm: 0.00872272\n",
      "Epoch 2 | Step 1115600 | Avg Loss: 0.0152 | Grad Norm: 0.00848101\n",
      "Epoch 2 | Step 1115700 | Avg Loss: 0.0152 | Grad Norm: 0.01043592\n",
      "Epoch 2 | Step 1115800 | Avg Loss: 0.0147 | Grad Norm: 0.01009383\n",
      "Epoch 2 | Step 1115900 | Avg Loss: 0.0150 | Grad Norm: 0.00831288\n",
      "Epoch 2 | Step 1116000 | Avg Loss: 0.0151 | Grad Norm: 0.00788841\n",
      "Epoch 2 | Step 1116100 | Avg Loss: 0.0150 | Grad Norm: 0.00913516\n",
      "Epoch 2 | Step 1116200 | Avg Loss: 0.0151 | Grad Norm: 0.00899175\n",
      "Epoch 2 | Step 1116300 | Avg Loss: 0.0155 | Grad Norm: 0.00801873\n",
      "Epoch 2 | Step 1116400 | Avg Loss: 0.0154 | Grad Norm: 0.00804893\n",
      "Epoch 2 | Step 1116500 | Avg Loss: 0.0151 | Grad Norm: 0.00801755\n",
      "Epoch 2 | Step 1116600 | Avg Loss: 0.0150 | Grad Norm: 0.00947924\n",
      "Epoch 2 | Step 1116700 | Avg Loss: 0.0147 | Grad Norm: 0.00916464\n",
      "Epoch 2 | Step 1116800 | Avg Loss: 0.0150 | Grad Norm: 0.00846052\n",
      "Epoch 2 | Step 1116900 | Avg Loss: 0.0154 | Grad Norm: 0.00835225\n",
      "Epoch 2 | Step 1117000 | Avg Loss: 0.0150 | Grad Norm: 0.00804801\n",
      "Epoch 2 | Step 1117100 | Avg Loss: 0.0150 | Grad Norm: 0.00793501\n",
      "Epoch 2 | Step 1117200 | Avg Loss: 0.0150 | Grad Norm: 0.00881810\n",
      "Epoch 2 | Step 1117300 | Avg Loss: 0.0149 | Grad Norm: 0.00672929\n",
      "Epoch 2 | Step 1117400 | Avg Loss: 0.0151 | Grad Norm: 0.01031271\n",
      "Epoch 2 | Step 1117500 | Avg Loss: 0.0148 | Grad Norm: 0.00825396\n",
      "Epoch 2 | Step 1117600 | Avg Loss: 0.0148 | Grad Norm: 0.00988248\n",
      "Epoch 2 | Step 1117700 | Avg Loss: 0.0147 | Grad Norm: 0.00864997\n",
      "Epoch 2 | Step 1117800 | Avg Loss: 0.0146 | Grad Norm: 0.00847277\n",
      "Epoch 2 | Step 1117900 | Avg Loss: 0.0145 | Grad Norm: 0.00838533\n",
      "Epoch 2 | Step 1118000 | Avg Loss: 0.0150 | Grad Norm: 0.00838054\n",
      "Epoch 2 | Step 1118100 | Avg Loss: 0.0146 | Grad Norm: 0.00741792\n",
      "Epoch 2 | Step 1118200 | Avg Loss: 0.0147 | Grad Norm: 0.00912416\n",
      "Epoch 2 | Step 1118300 | Avg Loss: 0.0147 | Grad Norm: 0.00819971\n",
      "Epoch 2 | Step 1118400 | Avg Loss: 0.0151 | Grad Norm: 0.00823522\n",
      "Epoch 2 | Step 1118500 | Avg Loss: 0.0151 | Grad Norm: 0.00762999\n",
      "Epoch 2 | Step 1118600 | Avg Loss: 0.0150 | Grad Norm: 0.01059119\n",
      "Epoch 2 | Step 1118700 | Avg Loss: 0.0150 | Grad Norm: 0.00919404\n",
      "Epoch 2 | Step 1118800 | Avg Loss: 0.0147 | Grad Norm: 0.00809185\n",
      "Epoch 2 | Step 1118900 | Avg Loss: 0.0148 | Grad Norm: 0.00952846\n",
      "Epoch 2 | Step 1119000 | Avg Loss: 0.0149 | Grad Norm: 0.00852996\n",
      "Epoch 2 | Step 1119100 | Avg Loss: 0.0149 | Grad Norm: 0.00767231\n",
      "Epoch 2 | Step 1119200 | Avg Loss: 0.0149 | Grad Norm: 0.00886822\n",
      "Epoch 2 | Step 1119300 | Avg Loss: 0.0149 | Grad Norm: 0.00722941\n",
      "Epoch 2 | Step 1119400 | Avg Loss: 0.0150 | Grad Norm: 0.00820712\n",
      "Epoch 2 | Step 1119500 | Avg Loss: 0.0148 | Grad Norm: 0.01055605\n",
      "Epoch 2 | Step 1119600 | Avg Loss: 0.0147 | Grad Norm: 0.00750458\n",
      "Epoch 2 | Step 1119700 | Avg Loss: 0.0148 | Grad Norm: 0.00968511\n",
      "Epoch 2 | Step 1119800 | Avg Loss: 0.0149 | Grad Norm: 0.00809049\n",
      "Epoch 2 | Step 1119900 | Avg Loss: 0.0149 | Grad Norm: 0.00848530\n",
      "Epoch 2 | Step 1120000 | Avg Loss: 0.0146 | Grad Norm: 0.00870758\n",
      "Epoch 2 | Step 1120100 | Avg Loss: 0.0146 | Grad Norm: 0.00825098\n",
      "Epoch 2 | Step 1120200 | Avg Loss: 0.0149 | Grad Norm: 0.00908779\n",
      "Epoch 2 | Step 1120300 | Avg Loss: 0.0148 | Grad Norm: 0.00803207\n",
      "Epoch 2 | Step 1120400 | Avg Loss: 0.0148 | Grad Norm: 0.00717396\n",
      "Epoch 2 | Step 1120500 | Avg Loss: 0.0144 | Grad Norm: 0.00810194\n",
      "Epoch 2 | Step 1120600 | Avg Loss: 0.0144 | Grad Norm: 0.00914849\n",
      "Epoch 2 | Step 1120700 | Avg Loss: 0.0143 | Grad Norm: 0.00927097\n",
      "Epoch 2 | Step 1120800 | Avg Loss: 0.0145 | Grad Norm: 0.00844829\n",
      "Epoch 2 | Step 1120900 | Avg Loss: 0.0143 | Grad Norm: 0.00870483\n",
      "Epoch 2 | Step 1121000 | Avg Loss: 0.0138 | Grad Norm: 0.00734474\n",
      "Epoch 2 | Step 1121100 | Avg Loss: 0.0144 | Grad Norm: 0.00796500\n",
      "Epoch 2 | Step 1121200 | Avg Loss: 0.0145 | Grad Norm: 0.00917365\n",
      "Epoch 2 | Step 1121300 | Avg Loss: 0.0148 | Grad Norm: 0.00806945\n",
      "Epoch 2 | Step 1121400 | Avg Loss: 0.0149 | Grad Norm: 0.00786383\n",
      "Epoch 2 | Step 1121500 | Avg Loss: 0.0144 | Grad Norm: 0.00834879\n",
      "Epoch 2 | Step 1121600 | Avg Loss: 0.0140 | Grad Norm: 0.01070692\n",
      "Epoch 2 | Step 1121700 | Avg Loss: 0.0146 | Grad Norm: 0.00847079\n",
      "Epoch 2 | Step 1121800 | Avg Loss: 0.0147 | Grad Norm: 0.00832650\n",
      "Epoch 2 | Step 1121900 | Avg Loss: 0.0149 | Grad Norm: 0.00697135\n",
      "Epoch 2 | Step 1122000 | Avg Loss: 0.0148 | Grad Norm: 0.00808383\n",
      "Epoch 2 | Step 1122100 | Avg Loss: 0.0150 | Grad Norm: 0.00951277\n",
      "Epoch 2 | Step 1122200 | Avg Loss: 0.0147 | Grad Norm: 0.00889052\n",
      "Epoch 2 | Step 1122300 | Avg Loss: 0.0148 | Grad Norm: 0.00824194\n",
      "Epoch 2 | Step 1122400 | Avg Loss: 0.0147 | Grad Norm: 0.01003674\n",
      "Epoch 2 | Step 1122500 | Avg Loss: 0.0144 | Grad Norm: 0.00903686\n",
      "Epoch 2 | Step 1122600 | Avg Loss: 0.0147 | Grad Norm: 0.00836584\n",
      "Epoch 2 | Step 1122700 | Avg Loss: 0.0148 | Grad Norm: 0.00765229\n",
      "Epoch 2 | Step 1122800 | Avg Loss: 0.0150 | Grad Norm: 0.00837587\n",
      "Epoch 2 | Step 1122900 | Avg Loss: 0.0146 | Grad Norm: 0.00708756\n",
      "Epoch 2 | Step 1123000 | Avg Loss: 0.0148 | Grad Norm: 0.00786365\n",
      "Epoch 2 | Step 1123100 | Avg Loss: 0.0149 | Grad Norm: 0.01008015\n",
      "Epoch 2 | Step 1123200 | Avg Loss: 0.0151 | Grad Norm: 0.00874248\n",
      "Epoch 2 | Step 1123300 | Avg Loss: 0.0150 | Grad Norm: 0.00862586\n",
      "Epoch 2 | Step 1123400 | Avg Loss: 0.0153 | Grad Norm: 0.00883783\n",
      "Epoch 2 | Step 1123500 | Avg Loss: 0.0150 | Grad Norm: 0.00952759\n",
      "Epoch 2 | Step 1123600 | Avg Loss: 0.0148 | Grad Norm: 0.00770639\n",
      "Epoch 2 | Step 1123700 | Avg Loss: 0.0145 | Grad Norm: 0.00771912\n",
      "Epoch 2 | Step 1123800 | Avg Loss: 0.0148 | Grad Norm: 0.00830888\n",
      "Epoch 2 | Step 1123900 | Avg Loss: 0.0152 | Grad Norm: 0.00903790\n",
      "Epoch 2 | Step 1124000 | Avg Loss: 0.0151 | Grad Norm: 0.00835689\n",
      "Epoch 2 | Step 1124100 | Avg Loss: 0.0149 | Grad Norm: 0.00749746\n",
      "Epoch 2 | Step 1124200 | Avg Loss: 0.0148 | Grad Norm: 0.00786469\n",
      "Epoch 2 | Step 1124300 | Avg Loss: 0.0149 | Grad Norm: 0.00776983\n",
      "Epoch 2 | Step 1124400 | Avg Loss: 0.0149 | Grad Norm: 0.00860448\n",
      "Epoch 2 | Step 1124500 | Avg Loss: 0.0148 | Grad Norm: 0.00763309\n",
      "Epoch 2 | Step 1124600 | Avg Loss: 0.0148 | Grad Norm: 0.00728565\n",
      "Epoch 2 | Step 1124700 | Avg Loss: 0.0151 | Grad Norm: 0.00932086\n",
      "Epoch 2 | Step 1124800 | Avg Loss: 0.0148 | Grad Norm: 0.00966064\n",
      "Epoch 2 | Step 1124900 | Avg Loss: 0.0152 | Grad Norm: 0.01095472\n",
      "Epoch 2 | Step 1125000 | Avg Loss: 0.0151 | Grad Norm: 0.00857502\n",
      "Epoch 2 | Step 1125100 | Avg Loss: 0.0152 | Grad Norm: 0.00949054\n",
      "Epoch 2 | Step 1125200 | Avg Loss: 0.0155 | Grad Norm: 0.00783581\n",
      "Epoch 2 | Step 1125300 | Avg Loss: 0.0150 | Grad Norm: 0.00919666\n",
      "Epoch 2 | Step 1125400 | Avg Loss: 0.0152 | Grad Norm: 0.00859780\n",
      "Epoch 2 | Step 1125500 | Avg Loss: 0.0155 | Grad Norm: 0.00985649\n",
      "Epoch 2 | Step 1125600 | Avg Loss: 0.0151 | Grad Norm: 0.00770322\n",
      "Epoch 2 | Step 1125700 | Avg Loss: 0.0149 | Grad Norm: 0.00971448\n",
      "Epoch 2 | Step 1125800 | Avg Loss: 0.0155 | Grad Norm: 0.00884763\n",
      "Epoch 2 | Step 1125900 | Avg Loss: 0.0152 | Grad Norm: 0.00911575\n",
      "Epoch 2 | Step 1126000 | Avg Loss: 0.0153 | Grad Norm: 0.00802965\n",
      "Epoch 2 | Step 1126100 | Avg Loss: 0.0150 | Grad Norm: 0.00957805\n",
      "Epoch 2 | Step 1126200 | Avg Loss: 0.0149 | Grad Norm: 0.00884442\n",
      "Epoch 2 | Step 1126300 | Avg Loss: 0.0146 | Grad Norm: 0.01127393\n",
      "Epoch 2 | Step 1126400 | Avg Loss: 0.0145 | Grad Norm: 0.00802305\n",
      "Epoch 2 | Step 1126500 | Avg Loss: 0.0147 | Grad Norm: 0.00779316\n",
      "Epoch 2 | Step 1126600 | Avg Loss: 0.0144 | Grad Norm: 0.01028357\n",
      "Epoch 2 | Step 1126700 | Avg Loss: 0.0145 | Grad Norm: 0.00895826\n",
      "Epoch 2 | Step 1126800 | Avg Loss: 0.0147 | Grad Norm: 0.00822419\n",
      "Epoch 2 | Step 1126900 | Avg Loss: 0.0147 | Grad Norm: 0.00858865\n",
      "Epoch 2 | Step 1127000 | Avg Loss: 0.0148 | Grad Norm: 0.00778624\n",
      "Epoch 2 | Step 1127100 | Avg Loss: 0.0146 | Grad Norm: 0.00808643\n",
      "Epoch 2 | Step 1127200 | Avg Loss: 0.0147 | Grad Norm: 0.00870385\n",
      "Epoch 2 | Step 1127300 | Avg Loss: 0.0151 | Grad Norm: 0.00887428\n",
      "Epoch 2 | Step 1127400 | Avg Loss: 0.0150 | Grad Norm: 0.00838394\n",
      "Epoch 2 | Step 1127500 | Avg Loss: 0.0148 | Grad Norm: 0.00855884\n",
      "Epoch 2 | Step 1127600 | Avg Loss: 0.0145 | Grad Norm: 0.00756721\n",
      "Epoch 2 | Step 1127700 | Avg Loss: 0.0146 | Grad Norm: 0.00898422\n",
      "Epoch 2 | Step 1127800 | Avg Loss: 0.0148 | Grad Norm: 0.00939920\n",
      "Epoch 2 | Step 1127900 | Avg Loss: 0.0146 | Grad Norm: 0.00778195\n",
      "Epoch 2 | Step 1128000 | Avg Loss: 0.0146 | Grad Norm: 0.00985973\n",
      "Epoch 2 | Step 1128100 | Avg Loss: 0.0144 | Grad Norm: 0.00759040\n",
      "Epoch 2 | Step 1128200 | Avg Loss: 0.0148 | Grad Norm: 0.00890122\n",
      "Epoch 2 | Step 1128300 | Avg Loss: 0.0148 | Grad Norm: 0.01043525\n",
      "Epoch 2 | Step 1128400 | Avg Loss: 0.0149 | Grad Norm: 0.00922201\n",
      "Epoch 2 | Step 1128500 | Avg Loss: 0.0145 | Grad Norm: 0.00850227\n",
      "Epoch 2 | Step 1128600 | Avg Loss: 0.0145 | Grad Norm: 0.00794574\n",
      "Epoch 2 | Step 1128700 | Avg Loss: 0.0146 | Grad Norm: 0.00754789\n",
      "Epoch 2 | Step 1128800 | Avg Loss: 0.0147 | Grad Norm: 0.00780689\n",
      "Epoch 2 | Step 1128900 | Avg Loss: 0.0148 | Grad Norm: 0.01004906\n",
      "Epoch 2 | Step 1129000 | Avg Loss: 0.0153 | Grad Norm: 0.00813968\n",
      "Epoch 2 | Step 1129100 | Avg Loss: 0.0148 | Grad Norm: 0.00825816\n",
      "Epoch 2 | Step 1129200 | Avg Loss: 0.0149 | Grad Norm: 0.00814569\n",
      "Epoch 2 | Step 1129300 | Avg Loss: 0.0152 | Grad Norm: 0.00903727\n",
      "Epoch 2 | Step 1129400 | Avg Loss: 0.0148 | Grad Norm: 0.01019219\n",
      "Epoch 2 | Step 1129500 | Avg Loss: 0.0153 | Grad Norm: 0.00856694\n",
      "Epoch 2 | Step 1129600 | Avg Loss: 0.0150 | Grad Norm: 0.00890663\n",
      "Epoch 2 | Step 1129700 | Avg Loss: 0.0147 | Grad Norm: 0.00886828\n",
      "Epoch 2 | Step 1129800 | Avg Loss: 0.0147 | Grad Norm: 0.00789966\n",
      "Epoch 2 | Step 1129900 | Avg Loss: 0.0151 | Grad Norm: 0.00756167\n",
      "Epoch 2 | Step 1130000 | Avg Loss: 0.0151 | Grad Norm: 0.00851024\n",
      "Epoch 2 | Step 1130100 | Avg Loss: 0.0153 | Grad Norm: 0.00845703\n",
      "Epoch 2 | Step 1130200 | Avg Loss: 0.0150 | Grad Norm: 0.00825285\n",
      "Epoch 2 | Step 1130300 | Avg Loss: 0.0148 | Grad Norm: 0.00885500\n",
      "Epoch 2 | Step 1130400 | Avg Loss: 0.0148 | Grad Norm: 0.00818419\n",
      "Epoch 2 | Step 1130500 | Avg Loss: 0.0151 | Grad Norm: 0.00709119\n",
      "Epoch 2 | Step 1130600 | Avg Loss: 0.0150 | Grad Norm: 0.00810540\n",
      "Epoch 2 | Step 1130700 | Avg Loss: 0.0152 | Grad Norm: 0.00808963\n",
      "Epoch 2 | Step 1130800 | Avg Loss: 0.0152 | Grad Norm: 0.00770003\n",
      "Epoch 2 | Step 1130900 | Avg Loss: 0.0151 | Grad Norm: 0.00903038\n",
      "Epoch 2 | Step 1131000 | Avg Loss: 0.0151 | Grad Norm: 0.00789815\n",
      "Epoch 2 | Step 1131100 | Avg Loss: 0.0153 | Grad Norm: 0.00804847\n",
      "Epoch 2 | Step 1131200 | Avg Loss: 0.0146 | Grad Norm: 0.00820098\n",
      "Epoch 2 | Step 1131300 | Avg Loss: 0.0146 | Grad Norm: 0.00783662\n",
      "Epoch 2 | Step 1131400 | Avg Loss: 0.0151 | Grad Norm: 0.01086443\n",
      "Epoch 2 | Step 1131500 | Avg Loss: 0.0150 | Grad Norm: 0.00848917\n",
      "Epoch 2 | Step 1131600 | Avg Loss: 0.0150 | Grad Norm: 0.00794308\n",
      "Epoch 2 | Step 1131700 | Avg Loss: 0.0150 | Grad Norm: 0.01117497\n",
      "Epoch 2 | Step 1131800 | Avg Loss: 0.0153 | Grad Norm: 0.00771922\n",
      "Epoch 2 | Step 1131900 | Avg Loss: 0.0151 | Grad Norm: 0.00909115\n",
      "Epoch 2 | Step 1132000 | Avg Loss: 0.0152 | Grad Norm: 0.00860958\n",
      "Epoch 2 | Step 1132100 | Avg Loss: 0.0152 | Grad Norm: 0.00824226\n",
      "Epoch 2 | Step 1132200 | Avg Loss: 0.0153 | Grad Norm: 0.00961505\n",
      "Epoch 2 | Step 1132300 | Avg Loss: 0.0155 | Grad Norm: 0.00865797\n",
      "Epoch 2 | Step 1132400 | Avg Loss: 0.0153 | Grad Norm: 0.00666857\n",
      "Epoch 2 | Step 1132500 | Avg Loss: 0.0147 | Grad Norm: 0.00989554\n",
      "Epoch 2 | Step 1132600 | Avg Loss: 0.0147 | Grad Norm: 0.00910009\n",
      "Epoch 2 | Step 1132700 | Avg Loss: 0.0148 | Grad Norm: 0.00854904\n",
      "Epoch 2 | Step 1132800 | Avg Loss: 0.0149 | Grad Norm: 0.00775579\n",
      "Epoch 2 | Step 1132900 | Avg Loss: 0.0152 | Grad Norm: 0.00862074\n",
      "Epoch 2 | Step 1133000 | Avg Loss: 0.0154 | Grad Norm: 0.00933182\n",
      "Epoch 2 | Step 1133100 | Avg Loss: 0.0152 | Grad Norm: 0.01037685\n",
      "Epoch 2 | Step 1133200 | Avg Loss: 0.0151 | Grad Norm: 0.00976668\n",
      "Epoch 2 | Step 1133300 | Avg Loss: 0.0154 | Grad Norm: 0.01196177\n",
      "Epoch 2 | Step 1133400 | Avg Loss: 0.0154 | Grad Norm: 0.00715359\n",
      "Epoch 2 | Step 1133500 | Avg Loss: 0.0153 | Grad Norm: 0.00726489\n",
      "Epoch 2 | Step 1133600 | Avg Loss: 0.0151 | Grad Norm: 0.00778749\n",
      "Epoch 2 | Step 1133700 | Avg Loss: 0.0152 | Grad Norm: 0.00911125\n",
      "Epoch 2 | Step 1133800 | Avg Loss: 0.0154 | Grad Norm: 0.00919106\n",
      "Epoch 2 | Step 1133900 | Avg Loss: 0.0156 | Grad Norm: 0.00787009\n",
      "Epoch 2 | Step 1134000 | Avg Loss: 0.0151 | Grad Norm: 0.00870591\n",
      "Epoch 2 | Step 1134100 | Avg Loss: 0.0148 | Grad Norm: 0.00864477\n",
      "Epoch 2 | Step 1134200 | Avg Loss: 0.0148 | Grad Norm: 0.00728353\n",
      "Epoch 2 | Step 1134300 | Avg Loss: 0.0148 | Grad Norm: 0.00886281\n",
      "Epoch 2 | Step 1134400 | Avg Loss: 0.0148 | Grad Norm: 0.00884023\n",
      "Epoch 2 | Step 1134500 | Avg Loss: 0.0150 | Grad Norm: 0.00892193\n",
      "Epoch 2 | Step 1134600 | Avg Loss: 0.0149 | Grad Norm: 0.00813473\n",
      "Epoch 2 | Step 1134700 | Avg Loss: 0.0146 | Grad Norm: 0.00853759\n",
      "Epoch 2 | Step 1134800 | Avg Loss: 0.0145 | Grad Norm: 0.00788847\n",
      "Epoch 2 | Step 1134900 | Avg Loss: 0.0144 | Grad Norm: 0.00840248\n",
      "Epoch 2 | Step 1135000 | Avg Loss: 0.0148 | Grad Norm: 0.00748999\n",
      "Epoch 2 | Step 1135100 | Avg Loss: 0.0151 | Grad Norm: 0.00734491\n",
      "Epoch 2 | Step 1135200 | Avg Loss: 0.0149 | Grad Norm: 0.00860320\n",
      "Epoch 2 | Step 1135300 | Avg Loss: 0.0149 | Grad Norm: 0.00741537\n",
      "Epoch 2 | Step 1135400 | Avg Loss: 0.0147 | Grad Norm: 0.00863109\n",
      "Epoch 2 | Step 1135500 | Avg Loss: 0.0151 | Grad Norm: 0.00902453\n",
      "Epoch 2 | Step 1135600 | Avg Loss: 0.0152 | Grad Norm: 0.00878880\n",
      "Epoch 2 | Step 1135700 | Avg Loss: 0.0151 | Grad Norm: 0.00868274\n",
      "Epoch 2 | Step 1135800 | Avg Loss: 0.0151 | Grad Norm: 0.00718862\n",
      "Epoch 2 | Step 1135900 | Avg Loss: 0.0147 | Grad Norm: 0.00799800\n",
      "Epoch 2 | Step 1136000 | Avg Loss: 0.0148 | Grad Norm: 0.00756254\n",
      "Epoch 2 | Step 1136100 | Avg Loss: 0.0149 | Grad Norm: 0.00885789\n",
      "Epoch 2 | Step 1136200 | Avg Loss: 0.0150 | Grad Norm: 0.00884560\n",
      "Epoch 2 | Step 1136300 | Avg Loss: 0.0147 | Grad Norm: 0.00766998\n",
      "Epoch 2 | Step 1136400 | Avg Loss: 0.0145 | Grad Norm: 0.00871648\n",
      "Epoch 2 | Step 1136500 | Avg Loss: 0.0146 | Grad Norm: 0.00782898\n",
      "Epoch 2 | Step 1136600 | Avg Loss: 0.0146 | Grad Norm: 0.00875072\n",
      "Epoch 2 | Step 1136700 | Avg Loss: 0.0148 | Grad Norm: 0.00788439\n",
      "Epoch 2 | Step 1136800 | Avg Loss: 0.0148 | Grad Norm: 0.00782158\n",
      "Epoch 2 | Step 1136900 | Avg Loss: 0.0146 | Grad Norm: 0.00707752\n",
      "Epoch 2 | Step 1137000 | Avg Loss: 0.0145 | Grad Norm: 0.00693155\n",
      "Epoch 2 | Step 1137100 | Avg Loss: 0.0145 | Grad Norm: 0.00807782\n",
      "Epoch 2 | Step 1137200 | Avg Loss: 0.0147 | Grad Norm: 0.00863725\n",
      "Epoch 2 | Step 1137300 | Avg Loss: 0.0150 | Grad Norm: 0.00800463\n",
      "Epoch 2 | Step 1137400 | Avg Loss: 0.0147 | Grad Norm: 0.00784578\n",
      "Epoch 2 | Step 1137500 | Avg Loss: 0.0148 | Grad Norm: 0.00759721\n",
      "Epoch 2 | Step 1137600 | Avg Loss: 0.0151 | Grad Norm: 0.00820198\n",
      "Epoch 2 | Step 1137700 | Avg Loss: 0.0153 | Grad Norm: 0.00885685\n",
      "Epoch 2 | Step 1137800 | Avg Loss: 0.0146 | Grad Norm: 0.00920165\n",
      "Epoch 2 | Step 1137900 | Avg Loss: 0.0147 | Grad Norm: 0.00750044\n",
      "Epoch 2 | Step 1138000 | Avg Loss: 0.0143 | Grad Norm: 0.00821230\n",
      "Epoch 2 | Step 1138100 | Avg Loss: 0.0144 | Grad Norm: 0.00980462\n",
      "Epoch 2 | Step 1138200 | Avg Loss: 0.0144 | Grad Norm: 0.00705167\n",
      "Epoch 2 | Step 1138300 | Avg Loss: 0.0142 | Grad Norm: 0.00806196\n",
      "Epoch 2 | Step 1138400 | Avg Loss: 0.0146 | Grad Norm: 0.00755312\n",
      "Epoch 2 | Step 1138500 | Avg Loss: 0.0144 | Grad Norm: 0.00755620\n",
      "Epoch 2 | Step 1138600 | Avg Loss: 0.0147 | Grad Norm: 0.00804658\n",
      "Epoch 2 | Step 1138700 | Avg Loss: 0.0142 | Grad Norm: 0.00722561\n",
      "Epoch 2 | Step 1138800 | Avg Loss: 0.0142 | Grad Norm: 0.00850141\n",
      "Epoch 2 | Step 1138900 | Avg Loss: 0.0147 | Grad Norm: 0.00740852\n",
      "Epoch 2 | Step 1139000 | Avg Loss: 0.0145 | Grad Norm: 0.00836696\n",
      "Epoch 2 | Step 1139100 | Avg Loss: 0.0144 | Grad Norm: 0.00785447\n",
      "Epoch 2 | Step 1139200 | Avg Loss: 0.0147 | Grad Norm: 0.00843481\n",
      "Epoch 2 | Step 1139300 | Avg Loss: 0.0147 | Grad Norm: 0.00923926\n",
      "Epoch 2 | Step 1139400 | Avg Loss: 0.0144 | Grad Norm: 0.00788452\n",
      "Epoch 2 | Step 1139500 | Avg Loss: 0.0142 | Grad Norm: 0.00776481\n",
      "Epoch 2 | Step 1139600 | Avg Loss: 0.0143 | Grad Norm: 0.00716301\n",
      "Epoch 2 | Step 1139700 | Avg Loss: 0.0146 | Grad Norm: 0.00862849\n",
      "Epoch 2 | Step 1139800 | Avg Loss: 0.0148 | Grad Norm: 0.00908707\n",
      "Epoch 2 | Step 1139900 | Avg Loss: 0.0153 | Grad Norm: 0.00746160\n",
      "Epoch 2 | Step 1140000 | Avg Loss: 0.0152 | Grad Norm: 0.00870771\n",
      "Epoch 2 | Step 1140100 | Avg Loss: 0.0150 | Grad Norm: 0.00850309\n",
      "Epoch 2 | Step 1140200 | Avg Loss: 0.0150 | Grad Norm: 0.00885444\n",
      "Epoch 2 | Step 1140300 | Avg Loss: 0.0148 | Grad Norm: 0.00772830\n",
      "Epoch 2 | Step 1140400 | Avg Loss: 0.0148 | Grad Norm: 0.00839015\n",
      "Epoch 2 | Step 1140500 | Avg Loss: 0.0148 | Grad Norm: 0.00649202\n",
      "Epoch 2 | Step 1140600 | Avg Loss: 0.0146 | Grad Norm: 0.00872466\n",
      "Epoch 2 | Step 1140700 | Avg Loss: 0.0145 | Grad Norm: 0.00780396\n",
      "Epoch 2 | Step 1140800 | Avg Loss: 0.0147 | Grad Norm: 0.00733986\n",
      "Epoch 2 | Step 1140900 | Avg Loss: 0.0148 | Grad Norm: 0.00886407\n",
      "Epoch 2 | Step 1141000 | Avg Loss: 0.0150 | Grad Norm: 0.00893942\n",
      "Epoch 2 | Step 1141100 | Avg Loss: 0.0152 | Grad Norm: 0.00852334\n",
      "Epoch 2 | Step 1141200 | Avg Loss: 0.0154 | Grad Norm: 0.00855872\n",
      "Epoch 2 | Step 1141300 | Avg Loss: 0.0151 | Grad Norm: 0.00916291\n",
      "Epoch 2 | Step 1141400 | Avg Loss: 0.0151 | Grad Norm: 0.00956360\n",
      "Epoch 2 | Step 1141500 | Avg Loss: 0.0147 | Grad Norm: 0.00765296\n",
      "Epoch 2 | Step 1141600 | Avg Loss: 0.0143 | Grad Norm: 0.00948644\n",
      "Epoch 2 | Step 1141700 | Avg Loss: 0.0147 | Grad Norm: 0.00917506\n",
      "Epoch 2 | Step 1141800 | Avg Loss: 0.0148 | Grad Norm: 0.00704788\n",
      "Epoch 2 | Step 1141900 | Avg Loss: 0.0149 | Grad Norm: 0.00788368\n",
      "Epoch 2 | Step 1142000 | Avg Loss: 0.0151 | Grad Norm: 0.00815436\n",
      "Epoch 2 | Step 1142100 | Avg Loss: 0.0150 | Grad Norm: 0.00874469\n",
      "Epoch 2 | Step 1142200 | Avg Loss: 0.0144 | Grad Norm: 0.00911908\n",
      "Epoch 2 | Step 1142300 | Avg Loss: 0.0145 | Grad Norm: 0.01022192\n",
      "Epoch 2 | Step 1142400 | Avg Loss: 0.0150 | Grad Norm: 0.00775781\n",
      "Epoch 2 | Step 1142500 | Avg Loss: 0.0147 | Grad Norm: 0.00898961\n",
      "Epoch 2 | Step 1142600 | Avg Loss: 0.0150 | Grad Norm: 0.01281824\n",
      "Epoch 2 | Step 1142700 | Avg Loss: 0.0150 | Grad Norm: 0.00960787\n",
      "Epoch 2 | Step 1142800 | Avg Loss: 0.0148 | Grad Norm: 0.00729409\n",
      "Epoch 2 | Step 1142900 | Avg Loss: 0.0143 | Grad Norm: 0.00739897\n",
      "Epoch 2 | Step 1143000 | Avg Loss: 0.0147 | Grad Norm: 0.00884315\n",
      "Epoch 2 | Step 1143100 | Avg Loss: 0.0150 | Grad Norm: 0.01066381\n",
      "Epoch 2 | Step 1143200 | Avg Loss: 0.0150 | Grad Norm: 0.00824952\n",
      "Epoch 2 | Step 1143300 | Avg Loss: 0.0152 | Grad Norm: 0.00796059\n",
      "Epoch 2 | Step 1143400 | Avg Loss: 0.0150 | Grad Norm: 0.00774743\n",
      "Epoch 2 | Step 1143500 | Avg Loss: 0.0145 | Grad Norm: 0.00758819\n",
      "Epoch 2 | Step 1143600 | Avg Loss: 0.0145 | Grad Norm: 0.00769056\n",
      "Epoch 2 | Step 1143700 | Avg Loss: 0.0148 | Grad Norm: 0.00739736\n",
      "Epoch 2 | Step 1143800 | Avg Loss: 0.0151 | Grad Norm: 0.00864424\n",
      "Epoch 2 | Step 1143900 | Avg Loss: 0.0150 | Grad Norm: 0.00976550\n",
      "Epoch 2 | Step 1144000 | Avg Loss: 0.0147 | Grad Norm: 0.00911885\n",
      "Epoch 2 | Step 1144100 | Avg Loss: 0.0146 | Grad Norm: 0.00940326\n",
      "Epoch 2 | Step 1144200 | Avg Loss: 0.0146 | Grad Norm: 0.00826174\n",
      "Epoch 2 | Step 1144300 | Avg Loss: 0.0149 | Grad Norm: 0.00958733\n",
      "Epoch 2 | Step 1144400 | Avg Loss: 0.0150 | Grad Norm: 0.00828404\n",
      "Epoch 2 | Step 1144500 | Avg Loss: 0.0153 | Grad Norm: 0.00800005\n",
      "Epoch 2 | Step 1144600 | Avg Loss: 0.0153 | Grad Norm: 0.00795874\n",
      "Epoch 2 | Step 1144700 | Avg Loss: 0.0150 | Grad Norm: 0.00864508\n",
      "Epoch 2 | Step 1144800 | Avg Loss: 0.0146 | Grad Norm: 0.00887358\n",
      "Epoch 2 | Step 1144900 | Avg Loss: 0.0150 | Grad Norm: 0.00894933\n",
      "Epoch 2 | Step 1145000 | Avg Loss: 0.0149 | Grad Norm: 0.00976261\n",
      "Epoch 2 | Step 1145100 | Avg Loss: 0.0148 | Grad Norm: 0.00762076\n",
      "Epoch 2 | Step 1145200 | Avg Loss: 0.0147 | Grad Norm: 0.00790836\n",
      "Epoch 2 | Step 1145300 | Avg Loss: 0.0153 | Grad Norm: 0.00824819\n",
      "Epoch 2 | Step 1145400 | Avg Loss: 0.0148 | Grad Norm: 0.00731427\n",
      "Epoch 2 | Step 1145500 | Avg Loss: 0.0151 | Grad Norm: 0.00912412\n",
      "Epoch 2 | Step 1145600 | Avg Loss: 0.0152 | Grad Norm: 0.00942728\n",
      "Epoch 2 | Step 1145700 | Avg Loss: 0.0152 | Grad Norm: 0.00856710\n",
      "Epoch 2 | Step 1145800 | Avg Loss: 0.0153 | Grad Norm: 0.00732689\n",
      "Epoch 2 | Step 1145900 | Avg Loss: 0.0153 | Grad Norm: 0.00885933\n",
      "Epoch 2 | Step 1146000 | Avg Loss: 0.0153 | Grad Norm: 0.00948454\n",
      "Epoch 2 | Step 1146100 | Avg Loss: 0.0152 | Grad Norm: 0.00800354\n",
      "Epoch 2 | Step 1146200 | Avg Loss: 0.0152 | Grad Norm: 0.00792543\n",
      "Epoch 2 | Step 1146300 | Avg Loss: 0.0149 | Grad Norm: 0.00838736\n",
      "Epoch 2 | Step 1146400 | Avg Loss: 0.0149 | Grad Norm: 0.00777545\n",
      "Epoch 2 | Step 1146500 | Avg Loss: 0.0147 | Grad Norm: 0.00797330\n",
      "Epoch 2 | Step 1146600 | Avg Loss: 0.0150 | Grad Norm: 0.00875268\n",
      "Epoch 2 | Step 1146700 | Avg Loss: 0.0149 | Grad Norm: 0.00892268\n",
      "Epoch 2 | Step 1146800 | Avg Loss: 0.0146 | Grad Norm: 0.00921832\n",
      "Epoch 2 | Step 1146900 | Avg Loss: 0.0147 | Grad Norm: 0.00766511\n",
      "Epoch 2 | Step 1147000 | Avg Loss: 0.0148 | Grad Norm: 0.00749545\n",
      "Epoch 2 | Step 1147100 | Avg Loss: 0.0150 | Grad Norm: 0.00929166\n",
      "Epoch 2 | Step 1147200 | Avg Loss: 0.0152 | Grad Norm: 0.00925793\n",
      "Epoch 2 | Step 1147300 | Avg Loss: 0.0149 | Grad Norm: 0.00957914\n",
      "Epoch 2 | Step 1147400 | Avg Loss: 0.0149 | Grad Norm: 0.00865318\n",
      "Epoch 2 | Step 1147500 | Avg Loss: 0.0151 | Grad Norm: 0.00897166\n",
      "Epoch 2 | Step 1147600 | Avg Loss: 0.0152 | Grad Norm: 0.00893519\n",
      "Epoch 2 | Step 1147700 | Avg Loss: 0.0150 | Grad Norm: 0.00821720\n",
      "Epoch 2 | Step 1147800 | Avg Loss: 0.0147 | Grad Norm: 0.00775781\n",
      "Epoch 2 | Step 1147900 | Avg Loss: 0.0146 | Grad Norm: 0.00816988\n",
      "Epoch 2 | Step 1148000 | Avg Loss: 0.0145 | Grad Norm: 0.00986001\n",
      "Epoch 2 | Step 1148100 | Avg Loss: 0.0147 | Grad Norm: 0.00807860\n",
      "Epoch 2 | Step 1148200 | Avg Loss: 0.0147 | Grad Norm: 0.00810681\n",
      "Epoch 2 | Step 1148300 | Avg Loss: 0.0149 | Grad Norm: 0.00864951\n",
      "Epoch 2 | Step 1148400 | Avg Loss: 0.0153 | Grad Norm: 0.01066036\n",
      "Epoch 2 | Step 1148500 | Avg Loss: 0.0152 | Grad Norm: 0.00941970\n",
      "Epoch 2 | Step 1148600 | Avg Loss: 0.0146 | Grad Norm: 0.00856159\n",
      "Epoch 2 | Step 1148700 | Avg Loss: 0.0148 | Grad Norm: 0.00809713\n",
      "Epoch 2 | Step 1148800 | Avg Loss: 0.0150 | Grad Norm: 0.00797114\n",
      "Epoch 2 | Step 1148900 | Avg Loss: 0.0153 | Grad Norm: 0.00776909\n",
      "Epoch 2 | Step 1149000 | Avg Loss: 0.0150 | Grad Norm: 0.00955557\n",
      "Epoch 2 | Step 1149100 | Avg Loss: 0.0151 | Grad Norm: 0.00822373\n",
      "Epoch 2 | Step 1149200 | Avg Loss: 0.0149 | Grad Norm: 0.00884929\n",
      "Epoch 2 | Step 1149300 | Avg Loss: 0.0152 | Grad Norm: 0.00934479\n",
      "Epoch 2 | Step 1149400 | Avg Loss: 0.0149 | Grad Norm: 0.00892568\n",
      "Epoch 2 | Step 1149500 | Avg Loss: 0.0150 | Grad Norm: 0.00822912\n",
      "Epoch 2 | Step 1149600 | Avg Loss: 0.0153 | Grad Norm: 0.00829581\n",
      "Epoch 2 | Step 1149700 | Avg Loss: 0.0153 | Grad Norm: 0.00870700\n",
      "Epoch 2 | Step 1149800 | Avg Loss: 0.0153 | Grad Norm: 0.00789690\n",
      "Epoch 2 | Step 1149900 | Avg Loss: 0.0151 | Grad Norm: 0.00830623\n",
      "Epoch 2 | Step 1150000 | Avg Loss: 0.0152 | Grad Norm: 0.00818245\n",
      "Epoch 2 | Step 1150100 | Avg Loss: 0.0148 | Grad Norm: 0.00858152\n",
      "Epoch 2 | Step 1150200 | Avg Loss: 0.0148 | Grad Norm: 0.00794459\n",
      "Epoch 2 | Step 1150300 | Avg Loss: 0.0146 | Grad Norm: 0.00852104\n",
      "Epoch 2 | Step 1150400 | Avg Loss: 0.0146 | Grad Norm: 0.00882030\n",
      "Epoch 2 | Step 1150500 | Avg Loss: 0.0149 | Grad Norm: 0.00827540\n",
      "Epoch 2 | Step 1150600 | Avg Loss: 0.0148 | Grad Norm: 0.00898127\n",
      "Epoch 2 | Step 1150700 | Avg Loss: 0.0152 | Grad Norm: 0.00901229\n",
      "Epoch 2 | Step 1150800 | Avg Loss: 0.0151 | Grad Norm: 0.00841923\n",
      "Epoch 2 | Step 1150900 | Avg Loss: 0.0150 | Grad Norm: 0.01007001\n",
      "Epoch 2 | Step 1151000 | Avg Loss: 0.0148 | Grad Norm: 0.00810546\n",
      "Epoch 2 | Step 1151100 | Avg Loss: 0.0150 | Grad Norm: 0.00816336\n",
      "Epoch 2 | Step 1151200 | Avg Loss: 0.0152 | Grad Norm: 0.00974312\n",
      "Epoch 2 | Step 1151300 | Avg Loss: 0.0145 | Grad Norm: 0.00870065\n",
      "Epoch 2 | Step 1151400 | Avg Loss: 0.0145 | Grad Norm: 0.00851534\n",
      "Epoch 2 | Step 1151500 | Avg Loss: 0.0144 | Grad Norm: 0.00730543\n",
      "Epoch 2 | Step 1151600 | Avg Loss: 0.0142 | Grad Norm: 0.01022513\n",
      "Epoch 2 | Step 1151700 | Avg Loss: 0.0142 | Grad Norm: 0.00754524\n",
      "Epoch 2 | Step 1151800 | Avg Loss: 0.0139 | Grad Norm: 0.00876624\n",
      "Epoch 2 | Step 1151900 | Avg Loss: 0.0138 | Grad Norm: 0.00898029\n",
      "Epoch 2 | Step 1152000 | Avg Loss: 0.0142 | Grad Norm: 0.00706162\n",
      "Epoch 2 | Step 1152100 | Avg Loss: 0.0144 | Grad Norm: 0.00857644\n",
      "Epoch 2 | Step 1152200 | Avg Loss: 0.0143 | Grad Norm: 0.00693775\n",
      "Epoch 2 | Step 1152300 | Avg Loss: 0.0146 | Grad Norm: 0.00871773\n",
      "Epoch 2 | Step 1152400 | Avg Loss: 0.0147 | Grad Norm: 0.00689897\n",
      "Epoch 2 | Step 1152500 | Avg Loss: 0.0146 | Grad Norm: 0.00808207\n",
      "Epoch 2 | Step 1152600 | Avg Loss: 0.0148 | Grad Norm: 0.00846619\n",
      "Epoch 2 | Step 1152700 | Avg Loss: 0.0149 | Grad Norm: 0.00789802\n",
      "Epoch 2 | Step 1152800 | Avg Loss: 0.0148 | Grad Norm: 0.00846856\n",
      "Epoch 2 | Step 1152900 | Avg Loss: 0.0146 | Grad Norm: 0.00823840\n",
      "Epoch 2 | Step 1153000 | Avg Loss: 0.0148 | Grad Norm: 0.00713531\n",
      "Epoch 2 | Step 1153100 | Avg Loss: 0.0148 | Grad Norm: 0.01020091\n",
      "Epoch 2 | Step 1153200 | Avg Loss: 0.0143 | Grad Norm: 0.00801523\n",
      "Epoch 2 | Step 1153300 | Avg Loss: 0.0142 | Grad Norm: 0.00770407\n",
      "Epoch 2 | Step 1153400 | Avg Loss: 0.0144 | Grad Norm: 0.00669609\n",
      "Epoch 2 | Step 1153500 | Avg Loss: 0.0149 | Grad Norm: 0.00818457\n",
      "Epoch 2 | Step 1153600 | Avg Loss: 0.0152 | Grad Norm: 0.00998941\n",
      "Epoch 2 | Step 1153700 | Avg Loss: 0.0154 | Grad Norm: 0.00963497\n",
      "Epoch 2 | Step 1153800 | Avg Loss: 0.0151 | Grad Norm: 0.00982423\n",
      "Epoch 2 | Step 1153900 | Avg Loss: 0.0150 | Grad Norm: 0.01229478\n",
      "Epoch 2 | Step 1154000 | Avg Loss: 0.0147 | Grad Norm: 0.00804990\n",
      "Epoch 2 | Step 1154100 | Avg Loss: 0.0147 | Grad Norm: 0.00865761\n",
      "Epoch 2 | Step 1154200 | Avg Loss: 0.0144 | Grad Norm: 0.00865248\n",
      "Epoch 2 | Step 1154300 | Avg Loss: 0.0144 | Grad Norm: 0.00841308\n",
      "Epoch 2 | Step 1154400 | Avg Loss: 0.0149 | Grad Norm: 0.01023775\n",
      "Epoch 2 | Step 1154500 | Avg Loss: 0.0148 | Grad Norm: 0.00831772\n",
      "Epoch 2 | Step 1154600 | Avg Loss: 0.0148 | Grad Norm: 0.00798888\n",
      "Epoch 2 | Step 1154700 | Avg Loss: 0.0149 | Grad Norm: 0.00767844\n",
      "Epoch 2 | Step 1154800 | Avg Loss: 0.0150 | Grad Norm: 0.00755494\n",
      "Epoch 2 | Step 1154900 | Avg Loss: 0.0147 | Grad Norm: 0.00762360\n",
      "Epoch 2 | Step 1155000 | Avg Loss: 0.0149 | Grad Norm: 0.00980237\n",
      "Epoch 2 | Step 1155100 | Avg Loss: 0.0150 | Grad Norm: 0.00822715\n",
      "Epoch 2 | Step 1155200 | Avg Loss: 0.0150 | Grad Norm: 0.00867544\n",
      "Epoch 2 | Step 1155300 | Avg Loss: 0.0149 | Grad Norm: 0.00846490\n",
      "Epoch 2 | Step 1155400 | Avg Loss: 0.0143 | Grad Norm: 0.00866947\n",
      "Epoch 2 | Step 1155500 | Avg Loss: 0.0146 | Grad Norm: 0.00901446\n",
      "Epoch 2 | Step 1155600 | Avg Loss: 0.0144 | Grad Norm: 0.00802239\n",
      "Epoch 2 | Step 1155700 | Avg Loss: 0.0144 | Grad Norm: 0.00785141\n",
      "Epoch 2 | Step 1155800 | Avg Loss: 0.0143 | Grad Norm: 0.00971348\n",
      "Epoch 2 | Step 1155900 | Avg Loss: 0.0146 | Grad Norm: 0.00936600\n",
      "Epoch 2 | Step 1156000 | Avg Loss: 0.0151 | Grad Norm: 0.00894578\n",
      "Epoch 2 | Step 1156100 | Avg Loss: 0.0147 | Grad Norm: 0.00815217\n",
      "Epoch 2 | Step 1156200 | Avg Loss: 0.0147 | Grad Norm: 0.00820068\n",
      "Epoch 2 | Step 1156300 | Avg Loss: 0.0147 | Grad Norm: 0.00782499\n",
      "Epoch 2 | Step 1156400 | Avg Loss: 0.0147 | Grad Norm: 0.00994260\n",
      "Epoch 2 | Step 1156500 | Avg Loss: 0.0149 | Grad Norm: 0.00731743\n",
      "Epoch 2 | Step 1156600 | Avg Loss: 0.0149 | Grad Norm: 0.00744486\n",
      "Epoch 2 | Step 1156700 | Avg Loss: 0.0145 | Grad Norm: 0.01149817\n",
      "Epoch 2 | Step 1156800 | Avg Loss: 0.0147 | Grad Norm: 0.00838773\n",
      "Epoch 2 | Step 1156900 | Avg Loss: 0.0148 | Grad Norm: 0.00879475\n",
      "Epoch 2 | Step 1157000 | Avg Loss: 0.0153 | Grad Norm: 0.00834522\n",
      "Epoch 2 | Step 1157100 | Avg Loss: 0.0153 | Grad Norm: 0.00853159\n",
      "Epoch 2 | Step 1157200 | Avg Loss: 0.0150 | Grad Norm: 0.01149210\n",
      "Epoch 2 | Step 1157300 | Avg Loss: 0.0151 | Grad Norm: 0.00754214\n",
      "Epoch 2 | Step 1157400 | Avg Loss: 0.0155 | Grad Norm: 0.00904462\n",
      "Epoch 2 | Step 1157500 | Avg Loss: 0.0160 | Grad Norm: 0.00920340\n",
      "Epoch 2 | Step 1157600 | Avg Loss: 0.0158 | Grad Norm: 0.00884313\n",
      "Epoch 2 | Step 1157700 | Avg Loss: 0.0160 | Grad Norm: 0.00793679\n",
      "Epoch 2 | Step 1157800 | Avg Loss: 0.0159 | Grad Norm: 0.00895837\n",
      "Epoch 2 | Step 1157900 | Avg Loss: 0.0156 | Grad Norm: 0.00897131\n",
      "Epoch 2 | Step 1158000 | Avg Loss: 0.0160 | Grad Norm: 0.00837304\n",
      "Epoch 2 | Step 1158100 | Avg Loss: 0.0158 | Grad Norm: 0.01055259\n",
      "Epoch 2 | Step 1158200 | Avg Loss: 0.0159 | Grad Norm: 0.00985811\n",
      "Epoch 2 | Step 1158300 | Avg Loss: 0.0158 | Grad Norm: 0.00830655\n",
      "Epoch 2 | Step 1158400 | Avg Loss: 0.0153 | Grad Norm: 0.00912198\n",
      "Epoch 2 | Step 1158500 | Avg Loss: 0.0149 | Grad Norm: 0.00830896\n",
      "Epoch 2 | Step 1158600 | Avg Loss: 0.0149 | Grad Norm: 0.00850878\n",
      "Epoch 2 | Step 1158700 | Avg Loss: 0.0149 | Grad Norm: 0.00811723\n",
      "Epoch 2 | Step 1158800 | Avg Loss: 0.0149 | Grad Norm: 0.00836759\n",
      "Epoch 2 | Step 1158900 | Avg Loss: 0.0147 | Grad Norm: 0.00858670\n",
      "Epoch 2 | Step 1159000 | Avg Loss: 0.0147 | Grad Norm: 0.00788865\n",
      "Epoch 2 | Step 1159100 | Avg Loss: 0.0149 | Grad Norm: 0.00883608\n",
      "Epoch 2 | Step 1159200 | Avg Loss: 0.0152 | Grad Norm: 0.00786342\n",
      "Epoch 2 | Step 1159300 | Avg Loss: 0.0153 | Grad Norm: 0.00794496\n",
      "Epoch 2 | Step 1159400 | Avg Loss: 0.0149 | Grad Norm: 0.01004455\n",
      "Epoch 2 | Step 1159500 | Avg Loss: 0.0148 | Grad Norm: 0.00867546\n",
      "Epoch 2 | Step 1159600 | Avg Loss: 0.0148 | Grad Norm: 0.00753037\n",
      "Epoch 2 | Step 1159700 | Avg Loss: 0.0147 | Grad Norm: 0.01054570\n",
      "Epoch 2 | Step 1159800 | Avg Loss: 0.0152 | Grad Norm: 0.00902973\n",
      "Epoch 2 | Step 1159900 | Avg Loss: 0.0154 | Grad Norm: 0.00793837\n",
      "Epoch 2 | Step 1160000 | Avg Loss: 0.0152 | Grad Norm: 0.00666594\n",
      "Epoch 2 | Step 1160100 | Avg Loss: 0.0151 | Grad Norm: 0.00987081\n",
      "Epoch 2 | Step 1160200 | Avg Loss: 0.0150 | Grad Norm: 0.00802316\n",
      "Epoch 2 | Step 1160300 | Avg Loss: 0.0151 | Grad Norm: 0.00991599\n",
      "Epoch 2 | Step 1160400 | Avg Loss: 0.0151 | Grad Norm: 0.00869427\n",
      "Epoch 2 | Step 1160500 | Avg Loss: 0.0153 | Grad Norm: 0.00799444\n",
      "Epoch 2 | Step 1160600 | Avg Loss: 0.0153 | Grad Norm: 0.00797615\n",
      "Epoch 2 | Step 1160700 | Avg Loss: 0.0153 | Grad Norm: 0.00869022\n",
      "Epoch 2 | Step 1160800 | Avg Loss: 0.0155 | Grad Norm: 0.00864628\n",
      "Epoch 2 | Step 1160900 | Avg Loss: 0.0150 | Grad Norm: 0.00743281\n",
      "Epoch 2 | Step 1161000 | Avg Loss: 0.0153 | Grad Norm: 0.00867423\n",
      "Epoch 2 | Step 1161100 | Avg Loss: 0.0149 | Grad Norm: 0.00909670\n",
      "Epoch 2 | Step 1161200 | Avg Loss: 0.0148 | Grad Norm: 0.00975401\n",
      "Epoch 2 | Step 1161300 | Avg Loss: 0.0147 | Grad Norm: 0.00801915\n",
      "Epoch 2 | Step 1161400 | Avg Loss: 0.0148 | Grad Norm: 0.00664834\n",
      "Epoch 2 | Step 1161500 | Avg Loss: 0.0152 | Grad Norm: 0.00758441\n",
      "Epoch 2 | Step 1161600 | Avg Loss: 0.0150 | Grad Norm: 0.00814616\n",
      "Epoch 2 | Step 1161700 | Avg Loss: 0.0150 | Grad Norm: 0.00872297\n",
      "Epoch 2 | Step 1161800 | Avg Loss: 0.0146 | Grad Norm: 0.00820818\n",
      "Epoch 2 | Step 1161900 | Avg Loss: 0.0149 | Grad Norm: 0.00761995\n",
      "Epoch 2 | Step 1162000 | Avg Loss: 0.0151 | Grad Norm: 0.00922506\n",
      "Epoch 2 | Step 1162100 | Avg Loss: 0.0157 | Grad Norm: 0.00852308\n",
      "Epoch 2 | Step 1162200 | Avg Loss: 0.0158 | Grad Norm: 0.00885869\n",
      "Epoch 2 | Step 1162300 | Avg Loss: 0.0154 | Grad Norm: 0.00813157\n",
      "Epoch 2 | Step 1162400 | Avg Loss: 0.0152 | Grad Norm: 0.00965302\n",
      "Epoch 2 | Step 1162500 | Avg Loss: 0.0148 | Grad Norm: 0.00744310\n",
      "Epoch 2 | Step 1162600 | Avg Loss: 0.0145 | Grad Norm: 0.00717471\n",
      "Epoch 2 | Step 1162700 | Avg Loss: 0.0148 | Grad Norm: 0.00773481\n",
      "Epoch 2 | Step 1162800 | Avg Loss: 0.0149 | Grad Norm: 0.00831755\n",
      "Epoch 2 | Step 1162900 | Avg Loss: 0.0151 | Grad Norm: 0.00732439\n",
      "Epoch 2 | Step 1163000 | Avg Loss: 0.0150 | Grad Norm: 0.00849187\n",
      "Epoch 2 | Step 1163100 | Avg Loss: 0.0149 | Grad Norm: 0.01170230\n",
      "Epoch 2 | Step 1163200 | Avg Loss: 0.0150 | Grad Norm: 0.00755838\n",
      "Epoch 2 | Step 1163300 | Avg Loss: 0.0146 | Grad Norm: 0.00805618\n",
      "Epoch 2 | Step 1163400 | Avg Loss: 0.0150 | Grad Norm: 0.00907894\n",
      "Epoch 2 | Step 1163500 | Avg Loss: 0.0155 | Grad Norm: 0.00922124\n",
      "Epoch 2 | Step 1163600 | Avg Loss: 0.0155 | Grad Norm: 0.00764268\n",
      "Epoch 2 | Step 1163700 | Avg Loss: 0.0153 | Grad Norm: 0.00906435\n",
      "Epoch 2 | Step 1163800 | Avg Loss: 0.0151 | Grad Norm: 0.00775953\n",
      "Epoch 2 | Step 1163900 | Avg Loss: 0.0150 | Grad Norm: 0.00830287\n",
      "Epoch 2 | Step 1164000 | Avg Loss: 0.0147 | Grad Norm: 0.00874695\n",
      "Epoch 2 | Step 1164100 | Avg Loss: 0.0146 | Grad Norm: 0.00785934\n",
      "Epoch 2 | Step 1164200 | Avg Loss: 0.0147 | Grad Norm: 0.00898014\n",
      "Epoch 2 | Step 1164300 | Avg Loss: 0.0149 | Grad Norm: 0.00856719\n",
      "Epoch 2 | Step 1164400 | Avg Loss: 0.0152 | Grad Norm: 0.00786427\n",
      "Epoch 2 | Step 1164500 | Avg Loss: 0.0149 | Grad Norm: 0.00736127\n",
      "Epoch 2 | Step 1164600 | Avg Loss: 0.0147 | Grad Norm: 0.00762365\n",
      "Epoch 2 | Step 1164700 | Avg Loss: 0.0146 | Grad Norm: 0.00937203\n",
      "Epoch 2 | Step 1164800 | Avg Loss: 0.0147 | Grad Norm: 0.00809136\n",
      "Epoch 2 | Step 1164900 | Avg Loss: 0.0150 | Grad Norm: 0.00806175\n",
      "Epoch 2 | Step 1165000 | Avg Loss: 0.0145 | Grad Norm: 0.00725993\n",
      "Epoch 2 | Step 1165100 | Avg Loss: 0.0143 | Grad Norm: 0.00985578\n",
      "Epoch 2 | Step 1165200 | Avg Loss: 0.0145 | Grad Norm: 0.00953242\n",
      "Epoch 2 | Step 1165300 | Avg Loss: 0.0145 | Grad Norm: 0.00863524\n",
      "Epoch 2 | Step 1165400 | Avg Loss: 0.0148 | Grad Norm: 0.00739957\n",
      "Epoch 2 | Step 1165500 | Avg Loss: 0.0147 | Grad Norm: 0.00925440\n",
      "Epoch 2 | Step 1165600 | Avg Loss: 0.0146 | Grad Norm: 0.00864678\n",
      "Epoch 2 | Step 1165700 | Avg Loss: 0.0151 | Grad Norm: 0.00720072\n",
      "Epoch 2 | Step 1165800 | Avg Loss: 0.0154 | Grad Norm: 0.00789904\n",
      "Epoch 2 | Step 1165900 | Avg Loss: 0.0153 | Grad Norm: 0.00824297\n",
      "Epoch 2 | Step 1166000 | Avg Loss: 0.0153 | Grad Norm: 0.00905323\n",
      "Epoch 2 | Step 1166100 | Avg Loss: 0.0152 | Grad Norm: 0.00911433\n",
      "Epoch 2 | Step 1166200 | Avg Loss: 0.0150 | Grad Norm: 0.00811857\n",
      "Epoch 2 | Step 1166300 | Avg Loss: 0.0146 | Grad Norm: 0.00872954\n",
      "Epoch 2 | Step 1166400 | Avg Loss: 0.0148 | Grad Norm: 0.00847661\n",
      "Epoch 2 | Step 1166500 | Avg Loss: 0.0146 | Grad Norm: 0.00891961\n",
      "Epoch 2 | Step 1166600 | Avg Loss: 0.0151 | Grad Norm: 0.00897839\n",
      "Epoch 2 | Step 1166700 | Avg Loss: 0.0147 | Grad Norm: 0.00809476\n",
      "Epoch 2 | Step 1166800 | Avg Loss: 0.0149 | Grad Norm: 0.00877147\n",
      "Epoch 2 | Step 1166900 | Avg Loss: 0.0150 | Grad Norm: 0.01009157\n",
      "Epoch 2 | Step 1167000 | Avg Loss: 0.0150 | Grad Norm: 0.00816761\n",
      "Epoch 2 | Step 1167100 | Avg Loss: 0.0147 | Grad Norm: 0.00751734\n",
      "Epoch 2 | Step 1167200 | Avg Loss: 0.0146 | Grad Norm: 0.00752333\n",
      "Epoch 2 | Step 1167300 | Avg Loss: 0.0145 | Grad Norm: 0.00936156\n",
      "Epoch 2 | Step 1167400 | Avg Loss: 0.0143 | Grad Norm: 0.00822370\n",
      "Epoch 2 | Step 1167500 | Avg Loss: 0.0143 | Grad Norm: 0.00781481\n",
      "Epoch 2 | Step 1167600 | Avg Loss: 0.0144 | Grad Norm: 0.00752827\n",
      "Epoch 2 | Step 1167700 | Avg Loss: 0.0144 | Grad Norm: 0.00749983\n",
      "Epoch 2 | Step 1167800 | Avg Loss: 0.0145 | Grad Norm: 0.00651090\n",
      "Epoch 2 | Step 1167900 | Avg Loss: 0.0140 | Grad Norm: 0.00746337\n",
      "Epoch 2 | Step 1168000 | Avg Loss: 0.0142 | Grad Norm: 0.00966954\n",
      "Epoch 2 | Step 1168100 | Avg Loss: 0.0142 | Grad Norm: 0.00765840\n",
      "Epoch 2 | Step 1168200 | Avg Loss: 0.0146 | Grad Norm: 0.01117403\n",
      "Epoch 2 | Step 1168300 | Avg Loss: 0.0141 | Grad Norm: 0.00737161\n",
      "Epoch 2 | Step 1168400 | Avg Loss: 0.0145 | Grad Norm: 0.00855548\n",
      "Epoch 2 | Step 1168500 | Avg Loss: 0.0143 | Grad Norm: 0.00782511\n",
      "Epoch 2 | Step 1168600 | Avg Loss: 0.0143 | Grad Norm: 0.00874986\n",
      "Epoch 2 | Step 1168700 | Avg Loss: 0.0144 | Grad Norm: 0.00695672\n",
      "Epoch 2 | Step 1168800 | Avg Loss: 0.0150 | Grad Norm: 0.00929237\n",
      "Epoch 2 | Step 1168900 | Avg Loss: 0.0150 | Grad Norm: 0.00763115\n",
      "Epoch 2 | Step 1169000 | Avg Loss: 0.0151 | Grad Norm: 0.00858624\n",
      "Epoch 2 | Step 1169100 | Avg Loss: 0.0153 | Grad Norm: 0.00818835\n",
      "Epoch 2 | Step 1169200 | Avg Loss: 0.0149 | Grad Norm: 0.00794601\n",
      "Epoch 2 | Step 1169300 | Avg Loss: 0.0152 | Grad Norm: 0.00862116\n",
      "Epoch 2 | Step 1169400 | Avg Loss: 0.0151 | Grad Norm: 0.00953387\n",
      "Epoch 2 | Step 1169500 | Avg Loss: 0.0151 | Grad Norm: 0.00893098\n",
      "Epoch 2 | Step 1169600 | Avg Loss: 0.0152 | Grad Norm: 0.00803190\n",
      "Epoch 2 | Step 1169700 | Avg Loss: 0.0152 | Grad Norm: 0.00818858\n",
      "Epoch 2 | Step 1169800 | Avg Loss: 0.0150 | Grad Norm: 0.00883713\n",
      "Epoch 2 | Step 1169900 | Avg Loss: 0.0154 | Grad Norm: 0.00766336\n",
      "Epoch 2 | Step 1170000 | Avg Loss: 0.0155 | Grad Norm: 0.01023903\n",
      "Epoch 2 | Step 1170100 | Avg Loss: 0.0154 | Grad Norm: 0.00799239\n",
      "Epoch 2 | Step 1170200 | Avg Loss: 0.0154 | Grad Norm: 0.00898871\n",
      "Epoch 2 | Step 1170300 | Avg Loss: 0.0152 | Grad Norm: 0.00829493\n",
      "Epoch 2 | Step 1170400 | Avg Loss: 0.0153 | Grad Norm: 0.00826233\n",
      "Epoch 2 | Step 1170500 | Avg Loss: 0.0152 | Grad Norm: 0.00788503\n",
      "Epoch 2 | Step 1170600 | Avg Loss: 0.0156 | Grad Norm: 0.00780488\n",
      "Epoch 2 | Step 1170700 | Avg Loss: 0.0154 | Grad Norm: 0.00772520\n",
      "Epoch 2 | Step 1170800 | Avg Loss: 0.0154 | Grad Norm: 0.00818123\n",
      "Epoch 2 | Step 1170900 | Avg Loss: 0.0151 | Grad Norm: 0.00799040\n",
      "Epoch 2 | Step 1171000 | Avg Loss: 0.0151 | Grad Norm: 0.00692417\n",
      "Epoch 2 | Step 1171100 | Avg Loss: 0.0149 | Grad Norm: 0.00785967\n",
      "Epoch 2 | Step 1171200 | Avg Loss: 0.0152 | Grad Norm: 0.00832020\n",
      "Epoch 2 | Step 1171300 | Avg Loss: 0.0152 | Grad Norm: 0.00822262\n",
      "Epoch 2 | Step 1171400 | Avg Loss: 0.0154 | Grad Norm: 0.00791529\n",
      "Epoch 2 | Step 1171500 | Avg Loss: 0.0155 | Grad Norm: 0.00978583\n",
      "Epoch 2 | Step 1171600 | Avg Loss: 0.0156 | Grad Norm: 0.00824522\n",
      "Epoch 2 | Step 1171700 | Avg Loss: 0.0157 | Grad Norm: 0.00914715\n",
      "Epoch 2 | Step 1171800 | Avg Loss: 0.0157 | Grad Norm: 0.00764432\n",
      "Epoch 2 | Step 1171900 | Avg Loss: 0.0153 | Grad Norm: 0.00749787\n",
      "Epoch 2 | Step 1172000 | Avg Loss: 0.0153 | Grad Norm: 0.00821727\n",
      "Epoch 2 | Step 1172100 | Avg Loss: 0.0153 | Grad Norm: 0.00782693\n",
      "Epoch 2 | Step 1172200 | Avg Loss: 0.0151 | Grad Norm: 0.00747886\n",
      "Epoch 2 | Step 1172300 | Avg Loss: 0.0151 | Grad Norm: 0.00890223\n",
      "Epoch 2 | Step 1172400 | Avg Loss: 0.0154 | Grad Norm: 0.00964016\n",
      "Epoch 2 | Step 1172500 | Avg Loss: 0.0155 | Grad Norm: 0.00855710\n",
      "Epoch 2 | Step 1172600 | Avg Loss: 0.0153 | Grad Norm: 0.00952940\n",
      "Epoch 2 | Step 1172700 | Avg Loss: 0.0153 | Grad Norm: 0.00629763\n",
      "Epoch 2 | Step 1172800 | Avg Loss: 0.0155 | Grad Norm: 0.00813734\n",
      "Epoch 2 | Step 1172900 | Avg Loss: 0.0156 | Grad Norm: 0.00866315\n",
      "Epoch 2 | Step 1173000 | Avg Loss: 0.0154 | Grad Norm: 0.00977836\n",
      "Epoch 2 | Step 1173100 | Avg Loss: 0.0153 | Grad Norm: 0.00804855\n",
      "Epoch 2 | Step 1173200 | Avg Loss: 0.0152 | Grad Norm: 0.00684425\n",
      "Epoch 2 | Step 1173300 | Avg Loss: 0.0149 | Grad Norm: 0.00886337\n",
      "Epoch 2 | Step 1173400 | Avg Loss: 0.0152 | Grad Norm: 0.00840850\n",
      "Epoch 2 | Step 1173500 | Avg Loss: 0.0150 | Grad Norm: 0.00832577\n",
      "Epoch 2 | Step 1173600 | Avg Loss: 0.0148 | Grad Norm: 0.00941044\n",
      "Epoch 2 | Step 1173700 | Avg Loss: 0.0149 | Grad Norm: 0.00786738\n",
      "Epoch 2 | Step 1173800 | Avg Loss: 0.0150 | Grad Norm: 0.00852193\n",
      "Epoch 2 | Step 1173900 | Avg Loss: 0.0153 | Grad Norm: 0.00821995\n",
      "Epoch 2 | Step 1174000 | Avg Loss: 0.0153 | Grad Norm: 0.00826909\n",
      "Epoch 2 | Step 1174100 | Avg Loss: 0.0151 | Grad Norm: 0.00948972\n",
      "Epoch 2 | Step 1174200 | Avg Loss: 0.0150 | Grad Norm: 0.00780410\n",
      "Epoch 2 | Step 1174300 | Avg Loss: 0.0156 | Grad Norm: 0.00774799\n",
      "Epoch 2 | Step 1174400 | Avg Loss: 0.0154 | Grad Norm: 0.00892394\n",
      "Epoch 2 | Step 1174500 | Avg Loss: 0.0149 | Grad Norm: 0.00823028\n",
      "Epoch 2 | Step 1174600 | Avg Loss: 0.0151 | Grad Norm: 0.00760463\n",
      "Epoch 2 | Step 1174700 | Avg Loss: 0.0155 | Grad Norm: 0.00901090\n",
      "Epoch 2 | Step 1174800 | Avg Loss: 0.0152 | Grad Norm: 0.00712848\n",
      "Epoch 2 | Step 1174900 | Avg Loss: 0.0152 | Grad Norm: 0.00778382\n",
      "Epoch 2 | Step 1175000 | Avg Loss: 0.0151 | Grad Norm: 0.00840395\n",
      "Epoch 2 | Step 1175100 | Avg Loss: 0.0156 | Grad Norm: 0.00916040\n",
      "Epoch 2 | Step 1175200 | Avg Loss: 0.0156 | Grad Norm: 0.00850713\n",
      "Epoch 2 | Step 1175300 | Avg Loss: 0.0157 | Grad Norm: 0.00850329\n",
      "Epoch 2 | Step 1175400 | Avg Loss: 0.0156 | Grad Norm: 0.00867150\n",
      "Epoch 2 | Step 1175500 | Avg Loss: 0.0156 | Grad Norm: 0.00795323\n",
      "Epoch 2 | Step 1175600 | Avg Loss: 0.0156 | Grad Norm: 0.00890181\n",
      "Epoch 2 | Step 1175700 | Avg Loss: 0.0153 | Grad Norm: 0.00749890\n",
      "Epoch 2 | Step 1175800 | Avg Loss: 0.0152 | Grad Norm: 0.00775717\n",
      "Epoch 2 | Step 1175900 | Avg Loss: 0.0150 | Grad Norm: 0.00745238\n",
      "Epoch 2 | Step 1176000 | Avg Loss: 0.0151 | Grad Norm: 0.00851830\n",
      "Epoch 2 | Step 1176100 | Avg Loss: 0.0148 | Grad Norm: 0.00752001\n",
      "Epoch 2 | Step 1176200 | Avg Loss: 0.0147 | Grad Norm: 0.00886961\n",
      "Epoch 2 | Step 1176300 | Avg Loss: 0.0146 | Grad Norm: 0.00796851\n",
      "Epoch 2 | Step 1176400 | Avg Loss: 0.0148 | Grad Norm: 0.00814705\n",
      "Epoch 2 | Step 1176500 | Avg Loss: 0.0148 | Grad Norm: 0.00836372\n",
      "Epoch 2 | Step 1176600 | Avg Loss: 0.0147 | Grad Norm: 0.00776540\n",
      "Epoch 2 | Step 1176700 | Avg Loss: 0.0148 | Grad Norm: 0.00850736\n",
      "Epoch 2 | Step 1176800 | Avg Loss: 0.0147 | Grad Norm: 0.00848456\n",
      "Epoch 2 | Step 1176900 | Avg Loss: 0.0150 | Grad Norm: 0.01042522\n",
      "Epoch 2 | Step 1177000 | Avg Loss: 0.0145 | Grad Norm: 0.01112485\n",
      "Epoch 2 | Step 1177100 | Avg Loss: 0.0148 | Grad Norm: 0.00771975\n",
      "Epoch 2 | Step 1177200 | Avg Loss: 0.0149 | Grad Norm: 0.00677096\n",
      "Epoch 2 | Step 1177300 | Avg Loss: 0.0145 | Grad Norm: 0.00819462\n",
      "Epoch 2 | Step 1177400 | Avg Loss: 0.0144 | Grad Norm: 0.00781761\n",
      "Epoch 2 | Step 1177500 | Avg Loss: 0.0150 | Grad Norm: 0.00898399\n",
      "Epoch 2 | Step 1177600 | Avg Loss: 0.0150 | Grad Norm: 0.00841948\n",
      "Epoch 2 | Step 1177700 | Avg Loss: 0.0147 | Grad Norm: 0.00754361\n",
      "Epoch 2 | Step 1177800 | Avg Loss: 0.0145 | Grad Norm: 0.00924871\n",
      "Epoch 2 | Step 1177900 | Avg Loss: 0.0146 | Grad Norm: 0.00758130\n",
      "Epoch 2 | Step 1178000 | Avg Loss: 0.0148 | Grad Norm: 0.00860863\n",
      "Epoch 2 | Step 1178100 | Avg Loss: 0.0149 | Grad Norm: 0.00928143\n",
      "Epoch 2 | Step 1178200 | Avg Loss: 0.0154 | Grad Norm: 0.00842251\n",
      "Epoch 2 | Step 1178300 | Avg Loss: 0.0152 | Grad Norm: 0.00765277\n",
      "Epoch 2 | Step 1178400 | Avg Loss: 0.0150 | Grad Norm: 0.00988982\n",
      "Epoch 2 | Step 1178500 | Avg Loss: 0.0153 | Grad Norm: 0.00919006\n",
      "Epoch 2 | Step 1178600 | Avg Loss: 0.0154 | Grad Norm: 0.00937572\n",
      "Epoch 2 | Step 1178700 | Avg Loss: 0.0154 | Grad Norm: 0.00785212\n",
      "Epoch 2 | Step 1178800 | Avg Loss: 0.0153 | Grad Norm: 0.01001791\n",
      "Epoch 2 | Step 1178900 | Avg Loss: 0.0149 | Grad Norm: 0.00910647\n",
      "Epoch 2 | Step 1179000 | Avg Loss: 0.0148 | Grad Norm: 0.01006699\n",
      "Epoch 2 | Step 1179100 | Avg Loss: 0.0148 | Grad Norm: 0.00876512\n",
      "Epoch 2 | Step 1179200 | Avg Loss: 0.0148 | Grad Norm: 0.00791851\n",
      "Epoch 2 | Step 1179300 | Avg Loss: 0.0148 | Grad Norm: 0.00800217\n",
      "Epoch 2 | Step 1179400 | Avg Loss: 0.0146 | Grad Norm: 0.00881819\n",
      "Epoch 2 | Step 1179500 | Avg Loss: 0.0147 | Grad Norm: 0.00774540\n",
      "Epoch 2 | Step 1179600 | Avg Loss: 0.0148 | Grad Norm: 0.00940284\n",
      "Epoch 2 | Step 1179700 | Avg Loss: 0.0151 | Grad Norm: 0.00687300\n",
      "Epoch 2 | Step 1179800 | Avg Loss: 0.0148 | Grad Norm: 0.00822186\n",
      "Epoch 2 | Step 1179900 | Avg Loss: 0.0150 | Grad Norm: 0.00833201\n",
      "Epoch 2 | Step 1180000 | Avg Loss: 0.0148 | Grad Norm: 0.00958743\n",
      "Epoch 2 | Step 1180100 | Avg Loss: 0.0150 | Grad Norm: 0.00863253\n",
      "Epoch 2 | Step 1180200 | Avg Loss: 0.0152 | Grad Norm: 0.00912784\n",
      "Epoch 2 | Step 1180300 | Avg Loss: 0.0152 | Grad Norm: 0.00690129\n",
      "Epoch 2 | Step 1180400 | Avg Loss: 0.0147 | Grad Norm: 0.00773866\n",
      "Epoch 2 | Step 1180500 | Avg Loss: 0.0147 | Grad Norm: 0.00802915\n",
      "Epoch 2 | Step 1180600 | Avg Loss: 0.0152 | Grad Norm: 0.00937732\n",
      "Epoch 2 | Step 1180700 | Avg Loss: 0.0153 | Grad Norm: 0.00828240\n",
      "Epoch 2 | Step 1180800 | Avg Loss: 0.0157 | Grad Norm: 0.00833560\n",
      "Epoch 2 | Step 1180900 | Avg Loss: 0.0160 | Grad Norm: 0.00851594\n",
      "Epoch 2 | Step 1181000 | Avg Loss: 0.0155 | Grad Norm: 0.00852334\n",
      "Epoch 2 | Step 1181100 | Avg Loss: 0.0155 | Grad Norm: 0.00696690\n",
      "Epoch 2 | Step 1181200 | Avg Loss: 0.0154 | Grad Norm: 0.01023226\n",
      "Epoch 2 | Step 1181300 | Avg Loss: 0.0151 | Grad Norm: 0.00888736\n",
      "Epoch 2 | Step 1181400 | Avg Loss: 0.0152 | Grad Norm: 0.00951250\n",
      "Epoch 2 | Step 1181500 | Avg Loss: 0.0151 | Grad Norm: 0.00911006\n",
      "Epoch 2 | Step 1181600 | Avg Loss: 0.0154 | Grad Norm: 0.00831381\n",
      "Epoch 2 | Step 1181700 | Avg Loss: 0.0149 | Grad Norm: 0.00940411\n",
      "Epoch 2 | Step 1181800 | Avg Loss: 0.0146 | Grad Norm: 0.00875901\n",
      "Epoch 2 | Step 1181900 | Avg Loss: 0.0145 | Grad Norm: 0.00978326\n",
      "Epoch 2 | Step 1182000 | Avg Loss: 0.0144 | Grad Norm: 0.00703174\n",
      "Epoch 2 | Step 1182100 | Avg Loss: 0.0145 | Grad Norm: 0.00805499\n",
      "Epoch 2 | Step 1182200 | Avg Loss: 0.0145 | Grad Norm: 0.00768225\n",
      "Epoch 2 | Step 1182300 | Avg Loss: 0.0145 | Grad Norm: 0.00907095\n",
      "Epoch 2 | Step 1182400 | Avg Loss: 0.0144 | Grad Norm: 0.00953796\n",
      "Epoch 2 | Step 1182500 | Avg Loss: 0.0145 | Grad Norm: 0.00817283\n",
      "Epoch 2 | Step 1182600 | Avg Loss: 0.0149 | Grad Norm: 0.00864460\n",
      "Epoch 2 | Step 1182700 | Avg Loss: 0.0148 | Grad Norm: 0.00823460\n",
      "Epoch 2 | Step 1182800 | Avg Loss: 0.0149 | Grad Norm: 0.00835813\n",
      "Epoch 2 | Step 1182900 | Avg Loss: 0.0147 | Grad Norm: 0.01032055\n",
      "Epoch 2 | Step 1183000 | Avg Loss: 0.0146 | Grad Norm: 0.00651520\n",
      "Epoch 2 | Step 1183100 | Avg Loss: 0.0145 | Grad Norm: 0.00730396\n",
      "Epoch 2 | Step 1183200 | Avg Loss: 0.0142 | Grad Norm: 0.00838446\n",
      "Epoch 2 | Step 1183300 | Avg Loss: 0.0145 | Grad Norm: 0.00784274\n",
      "Epoch 2 | Step 1183400 | Avg Loss: 0.0151 | Grad Norm: 0.01060387\n",
      "Epoch 2 | Step 1183500 | Avg Loss: 0.0149 | Grad Norm: 0.00798111\n",
      "Epoch 2 | Step 1183600 | Avg Loss: 0.0147 | Grad Norm: 0.00789169\n",
      "Epoch 2 | Step 1183700 | Avg Loss: 0.0145 | Grad Norm: 0.00915110\n",
      "Epoch 2 | Step 1183800 | Avg Loss: 0.0147 | Grad Norm: 0.01061173\n",
      "Epoch 2 | Step 1183900 | Avg Loss: 0.0145 | Grad Norm: 0.00833812\n",
      "Epoch 2 | Step 1184000 | Avg Loss: 0.0147 | Grad Norm: 0.00897604\n",
      "Epoch 2 | Step 1184100 | Avg Loss: 0.0145 | Grad Norm: 0.00774867\n",
      "Epoch 2 | Step 1184200 | Avg Loss: 0.0146 | Grad Norm: 0.00751268\n",
      "Epoch 2 | Step 1184300 | Avg Loss: 0.0144 | Grad Norm: 0.00770208\n",
      "Epoch 2 | Step 1184400 | Avg Loss: 0.0147 | Grad Norm: 0.00883634\n",
      "Epoch 2 | Step 1184500 | Avg Loss: 0.0149 | Grad Norm: 0.00780107\n",
      "Epoch 2 | Step 1184600 | Avg Loss: 0.0146 | Grad Norm: 0.00842232\n",
      "Epoch 2 | Step 1184700 | Avg Loss: 0.0149 | Grad Norm: 0.00853814\n",
      "Epoch 2 | Step 1184800 | Avg Loss: 0.0148 | Grad Norm: 0.00826176\n",
      "Epoch 2 | Step 1184900 | Avg Loss: 0.0148 | Grad Norm: 0.00872006\n",
      "Epoch 2 | Step 1185000 | Avg Loss: 0.0151 | Grad Norm: 0.00801629\n",
      "Epoch 2 | Step 1185100 | Avg Loss: 0.0149 | Grad Norm: 0.00807692\n",
      "Epoch 2 | Step 1185200 | Avg Loss: 0.0151 | Grad Norm: 0.00873415\n",
      "Epoch 2 | Step 1185300 | Avg Loss: 0.0151 | Grad Norm: 0.00874860\n",
      "Epoch 2 | Step 1185400 | Avg Loss: 0.0152 | Grad Norm: 0.00836098\n",
      "Epoch 2 | Step 1185500 | Avg Loss: 0.0150 | Grad Norm: 0.01123062\n",
      "Epoch 2 | Step 1185600 | Avg Loss: 0.0150 | Grad Norm: 0.00774465\n",
      "Epoch 2 | Step 1185700 | Avg Loss: 0.0151 | Grad Norm: 0.00861609\n",
      "Epoch 2 | Step 1185800 | Avg Loss: 0.0157 | Grad Norm: 0.00886618\n",
      "Epoch 2 | Step 1185900 | Avg Loss: 0.0157 | Grad Norm: 0.00962249\n",
      "Epoch 2 | Step 1186000 | Avg Loss: 0.0157 | Grad Norm: 0.00898183\n",
      "Epoch 2 | Step 1186100 | Avg Loss: 0.0155 | Grad Norm: 0.00805230\n",
      "Epoch 2 | Step 1186200 | Avg Loss: 0.0152 | Grad Norm: 0.00863103\n",
      "Epoch 2 | Step 1186300 | Avg Loss: 0.0150 | Grad Norm: 0.00866849\n",
      "Epoch 2 | Step 1186400 | Avg Loss: 0.0151 | Grad Norm: 0.00775014\n",
      "Epoch 2 | Step 1186500 | Avg Loss: 0.0153 | Grad Norm: 0.00936674\n",
      "Epoch 2 | Step 1186600 | Avg Loss: 0.0153 | Grad Norm: 0.00886993\n",
      "Epoch 2 | Step 1186700 | Avg Loss: 0.0153 | Grad Norm: 0.00829228\n",
      "Epoch 2 | Step 1186800 | Avg Loss: 0.0151 | Grad Norm: 0.00980345\n",
      "Epoch 2 | Step 1186900 | Avg Loss: 0.0152 | Grad Norm: 0.00858784\n",
      "Epoch 2 | Step 1187000 | Avg Loss: 0.0151 | Grad Norm: 0.00817291\n",
      "Epoch 2 | Step 1187100 | Avg Loss: 0.0153 | Grad Norm: 0.00714252\n",
      "Epoch 2 | Step 1187200 | Avg Loss: 0.0151 | Grad Norm: 0.00755976\n",
      "Epoch 2 | Step 1187300 | Avg Loss: 0.0147 | Grad Norm: 0.00916081\n",
      "Epoch 2 | Step 1187400 | Avg Loss: 0.0149 | Grad Norm: 0.00744870\n",
      "Epoch 2 | Step 1187500 | Avg Loss: 0.0148 | Grad Norm: 0.00922748\n",
      "Epoch 2 | Step 1187600 | Avg Loss: 0.0151 | Grad Norm: 0.00810855\n",
      "Epoch 2 | Step 1187700 | Avg Loss: 0.0152 | Grad Norm: 0.01247478\n",
      "Epoch 2 | Step 1187800 | Avg Loss: 0.0151 | Grad Norm: 0.01021698\n",
      "Epoch 2 | Step 1187900 | Avg Loss: 0.0152 | Grad Norm: 0.00790798\n",
      "Epoch 2 | Step 1188000 | Avg Loss: 0.0154 | Grad Norm: 0.00774616\n",
      "Epoch 2 | Step 1188100 | Avg Loss: 0.0154 | Grad Norm: 0.00954301\n",
      "Epoch 2 | Step 1188200 | Avg Loss: 0.0153 | Grad Norm: 0.00848157\n",
      "Epoch 2 | Step 1188300 | Avg Loss: 0.0156 | Grad Norm: 0.00714721\n",
      "Epoch 2 | Step 1188400 | Avg Loss: 0.0151 | Grad Norm: 0.00895919\n",
      "Epoch 2 | Step 1188500 | Avg Loss: 0.0151 | Grad Norm: 0.00830045\n",
      "Epoch 2 | Step 1188600 | Avg Loss: 0.0153 | Grad Norm: 0.00966500\n",
      "Epoch 2 | Step 1188700 | Avg Loss: 0.0152 | Grad Norm: 0.00843527\n",
      "Epoch 2 | Step 1188800 | Avg Loss: 0.0153 | Grad Norm: 0.00949218\n",
      "Epoch 2 | Step 1188900 | Avg Loss: 0.0153 | Grad Norm: 0.00787250\n",
      "Epoch 2 | Step 1189000 | Avg Loss: 0.0151 | Grad Norm: 0.01165310\n",
      "Epoch 2 | Step 1189100 | Avg Loss: 0.0150 | Grad Norm: 0.00796005\n",
      "Epoch 2 | Step 1189200 | Avg Loss: 0.0151 | Grad Norm: 0.00754450\n",
      "Epoch 2 | Step 1189300 | Avg Loss: 0.0151 | Grad Norm: 0.01133985\n",
      "Epoch 2 | Step 1189400 | Avg Loss: 0.0149 | Grad Norm: 0.00975429\n",
      "Epoch 2 | Step 1189500 | Avg Loss: 0.0152 | Grad Norm: 0.00951906\n",
      "Epoch 2 | Step 1189600 | Avg Loss: 0.0148 | Grad Norm: 0.00963045\n",
      "Epoch 2 | Step 1189700 | Avg Loss: 0.0149 | Grad Norm: 0.00824821\n",
      "Epoch 2 | Step 1189800 | Avg Loss: 0.0147 | Grad Norm: 0.00764356\n",
      "Epoch 2 | Step 1189900 | Avg Loss: 0.0146 | Grad Norm: 0.00797960\n",
      "Epoch 2 | Step 1190000 | Avg Loss: 0.0151 | Grad Norm: 0.00876571\n",
      "Epoch 2 | Step 1190100 | Avg Loss: 0.0154 | Grad Norm: 0.00896237\n",
      "Epoch 2 | Step 1190200 | Avg Loss: 0.0152 | Grad Norm: 0.00821112\n",
      "Epoch 2 | Step 1190300 | Avg Loss: 0.0149 | Grad Norm: 0.00751146\n",
      "Epoch 2 | Step 1190400 | Avg Loss: 0.0150 | Grad Norm: 0.00831098\n",
      "Epoch 2 | Step 1190500 | Avg Loss: 0.0153 | Grad Norm: 0.01073419\n",
      "Epoch 2 | Step 1190600 | Avg Loss: 0.0157 | Grad Norm: 0.00827061\n",
      "Epoch 2 | Step 1190700 | Avg Loss: 0.0155 | Grad Norm: 0.01134976\n",
      "Epoch 2 | Step 1190800 | Avg Loss: 0.0155 | Grad Norm: 0.00776308\n",
      "Epoch 2 | Step 1190900 | Avg Loss: 0.0154 | Grad Norm: 0.00894469\n",
      "Epoch 2 | Step 1191000 | Avg Loss: 0.0150 | Grad Norm: 0.01180658\n",
      "Epoch 2 | Step 1191100 | Avg Loss: 0.0145 | Grad Norm: 0.00760642\n",
      "Epoch 2 | Step 1191200 | Avg Loss: 0.0147 | Grad Norm: 0.00867645\n",
      "Epoch 2 | Step 1191300 | Avg Loss: 0.0144 | Grad Norm: 0.00767159\n",
      "Epoch 2 | Step 1191400 | Avg Loss: 0.0146 | Grad Norm: 0.00783286\n",
      "Epoch 2 | Step 1191500 | Avg Loss: 0.0145 | Grad Norm: 0.00855494\n",
      "Epoch 2 | Step 1191600 | Avg Loss: 0.0148 | Grad Norm: 0.00804575\n",
      "Epoch 2 | Step 1191700 | Avg Loss: 0.0147 | Grad Norm: 0.00890441\n",
      "Epoch 2 | Step 1191800 | Avg Loss: 0.0147 | Grad Norm: 0.00855371\n",
      "Epoch 2 | Step 1191900 | Avg Loss: 0.0146 | Grad Norm: 0.00787399\n",
      "Epoch 2 | Step 1192000 | Avg Loss: 0.0149 | Grad Norm: 0.00659798\n",
      "Epoch 2 | Step 1192100 | Avg Loss: 0.0146 | Grad Norm: 0.00941599\n",
      "Epoch 2 | Step 1192200 | Avg Loss: 0.0150 | Grad Norm: 0.00962246\n",
      "Epoch 2 | Step 1192300 | Avg Loss: 0.0148 | Grad Norm: 0.00796608\n",
      "Epoch 2 | Step 1192400 | Avg Loss: 0.0150 | Grad Norm: 0.00810874\n",
      "Epoch 2 | Step 1192500 | Avg Loss: 0.0155 | Grad Norm: 0.00850732\n",
      "Epoch 2 | Step 1192600 | Avg Loss: 0.0153 | Grad Norm: 0.00886159\n",
      "Epoch 2 | Step 1192700 | Avg Loss: 0.0154 | Grad Norm: 0.00885453\n",
      "Epoch 2 | Step 1192800 | Avg Loss: 0.0153 | Grad Norm: 0.00814676\n",
      "Epoch 2 | Step 1192900 | Avg Loss: 0.0150 | Grad Norm: 0.01071777\n",
      "Epoch 2 | Step 1193000 | Avg Loss: 0.0150 | Grad Norm: 0.00807798\n",
      "Epoch 2 | Step 1193100 | Avg Loss: 0.0152 | Grad Norm: 0.00994642\n",
      "Epoch 2 | Step 1193200 | Avg Loss: 0.0152 | Grad Norm: 0.00912128\n",
      "Epoch 2 | Step 1193300 | Avg Loss: 0.0152 | Grad Norm: 0.00776102\n",
      "Epoch 2 | Step 1193400 | Avg Loss: 0.0150 | Grad Norm: 0.00958887\n",
      "Epoch 2 | Step 1193500 | Avg Loss: 0.0147 | Grad Norm: 0.00813934\n",
      "Epoch 2 | Step 1193600 | Avg Loss: 0.0152 | Grad Norm: 0.00889226\n",
      "Epoch 2 | Step 1193700 | Avg Loss: 0.0152 | Grad Norm: 0.00855124\n",
      "Epoch 2 | Step 1193800 | Avg Loss: 0.0151 | Grad Norm: 0.00755194\n",
      "Epoch 2 | Step 1193900 | Avg Loss: 0.0150 | Grad Norm: 0.00779118\n",
      "Epoch 2 | Step 1194000 | Avg Loss: 0.0149 | Grad Norm: 0.01004935\n",
      "Epoch 2 | Step 1194100 | Avg Loss: 0.0150 | Grad Norm: 0.00816175\n",
      "Epoch 2 | Step 1194200 | Avg Loss: 0.0151 | Grad Norm: 0.00896699\n",
      "Epoch 2 | Step 1194300 | Avg Loss: 0.0150 | Grad Norm: 0.00887483\n",
      "Epoch 2 | Step 1194400 | Avg Loss: 0.0151 | Grad Norm: 0.00826732\n",
      "Epoch 2 | Step 1194500 | Avg Loss: 0.0149 | Grad Norm: 0.00836674\n",
      "Epoch 2 | Step 1194600 | Avg Loss: 0.0147 | Grad Norm: 0.00905547\n",
      "Epoch 2 | Step 1194700 | Avg Loss: 0.0147 | Grad Norm: 0.00771534\n",
      "Epoch 2 | Step 1194800 | Avg Loss: 0.0150 | Grad Norm: 0.00812819\n",
      "Epoch 2 | Step 1194900 | Avg Loss: 0.0150 | Grad Norm: 0.00860834\n",
      "Epoch 2 | Step 1195000 | Avg Loss: 0.0149 | Grad Norm: 0.00925242\n",
      "Epoch 2 | Step 1195100 | Avg Loss: 0.0149 | Grad Norm: 0.00938024\n",
      "Epoch 2 | Step 1195200 | Avg Loss: 0.0151 | Grad Norm: 0.01124736\n",
      "Epoch 2 | Step 1195300 | Avg Loss: 0.0152 | Grad Norm: 0.00872367\n",
      "Epoch 2 | Step 1195400 | Avg Loss: 0.0150 | Grad Norm: 0.00818558\n",
      "Epoch 2 | Step 1195500 | Avg Loss: 0.0154 | Grad Norm: 0.00856085\n",
      "Epoch 2 | Step 1195600 | Avg Loss: 0.0155 | Grad Norm: 0.00865793\n",
      "Epoch 2 | Step 1195700 | Avg Loss: 0.0157 | Grad Norm: 0.00792679\n",
      "Epoch 2 | Step 1195800 | Avg Loss: 0.0157 | Grad Norm: 0.01114815\n",
      "Epoch 2 | Step 1195900 | Avg Loss: 0.0154 | Grad Norm: 0.00836482\n",
      "Epoch 2 | Step 1196000 | Avg Loss: 0.0155 | Grad Norm: 0.01101726\n",
      "Epoch 2 | Step 1196100 | Avg Loss: 0.0152 | Grad Norm: 0.00753642\n",
      "Epoch 2 | Step 1196200 | Avg Loss: 0.0154 | Grad Norm: 0.00904667\n",
      "Epoch 2 | Step 1196300 | Avg Loss: 0.0151 | Grad Norm: 0.00838837\n",
      "Epoch 2 | Step 1196400 | Avg Loss: 0.0147 | Grad Norm: 0.00982745\n",
      "Epoch 2 | Step 1196500 | Avg Loss: 0.0147 | Grad Norm: 0.00857385\n",
      "Epoch 2 | Step 1196600 | Avg Loss: 0.0148 | Grad Norm: 0.00783482\n",
      "Epoch 2 | Step 1196700 | Avg Loss: 0.0150 | Grad Norm: 0.00803949\n",
      "Epoch 2 | Step 1196800 | Avg Loss: 0.0152 | Grad Norm: 0.00912509\n",
      "Epoch 2 | Step 1196900 | Avg Loss: 0.0151 | Grad Norm: 0.00804308\n",
      "Epoch 2 | Step 1197000 | Avg Loss: 0.0149 | Grad Norm: 0.00790417\n",
      "Epoch 2 | Step 1197100 | Avg Loss: 0.0152 | Grad Norm: 0.00999476\n",
      "Epoch 2 | Step 1197200 | Avg Loss: 0.0152 | Grad Norm: 0.00830405\n",
      "Epoch 2 | Step 1197300 | Avg Loss: 0.0155 | Grad Norm: 0.00981400\n",
      "Epoch 2 | Step 1197400 | Avg Loss: 0.0153 | Grad Norm: 0.00855728\n",
      "Epoch 2 | Step 1197500 | Avg Loss: 0.0152 | Grad Norm: 0.00770821\n",
      "Epoch 2 | Step 1197600 | Avg Loss: 0.0152 | Grad Norm: 0.00841718\n",
      "Epoch 2 | Step 1197700 | Avg Loss: 0.0151 | Grad Norm: 0.00850724\n",
      "Epoch 2 | Step 1197800 | Avg Loss: 0.0148 | Grad Norm: 0.01022776\n",
      "Epoch 2 | Step 1197900 | Avg Loss: 0.0147 | Grad Norm: 0.00842213\n",
      "Epoch 2 | Step 1198000 | Avg Loss: 0.0150 | Grad Norm: 0.00900948\n",
      "Epoch 2 | Step 1198100 | Avg Loss: 0.0153 | Grad Norm: 0.00742984\n",
      "Epoch 2 | Step 1198200 | Avg Loss: 0.0151 | Grad Norm: 0.00901124\n",
      "Epoch 2 | Step 1198300 | Avg Loss: 0.0151 | Grad Norm: 0.00837228\n",
      "Epoch 2 | Step 1198400 | Avg Loss: 0.0152 | Grad Norm: 0.01021838\n",
      "Epoch 2 | Step 1198500 | Avg Loss: 0.0155 | Grad Norm: 0.01084385\n",
      "Epoch 2 | Step 1198600 | Avg Loss: 0.0157 | Grad Norm: 0.00930843\n",
      "Epoch 2 | Step 1198700 | Avg Loss: 0.0156 | Grad Norm: 0.00903943\n",
      "Epoch 2 | Step 1198800 | Avg Loss: 0.0152 | Grad Norm: 0.00969744\n",
      "Epoch 2 | Step 1198900 | Avg Loss: 0.0154 | Grad Norm: 0.00905034\n",
      "Epoch 2 | Step 1199000 | Avg Loss: 0.0156 | Grad Norm: 0.00810477\n",
      "Epoch 2 | Step 1199100 | Avg Loss: 0.0152 | Grad Norm: 0.00753726\n",
      "Epoch 2 | Step 1199200 | Avg Loss: 0.0151 | Grad Norm: 0.00727070\n",
      "Epoch 2 | Step 1199300 | Avg Loss: 0.0153 | Grad Norm: 0.00966326\n",
      "Epoch 2 | Step 1199400 | Avg Loss: 0.0152 | Grad Norm: 0.00812961\n",
      "Epoch 2 | Step 1199500 | Avg Loss: 0.0152 | Grad Norm: 0.00890932\n",
      "Epoch 2 | Step 1199600 | Avg Loss: 0.0150 | Grad Norm: 0.00874146\n",
      "Epoch 2 | Step 1199700 | Avg Loss: 0.0153 | Grad Norm: 0.00907016\n",
      "Epoch 2 | Step 1199800 | Avg Loss: 0.0150 | Grad Norm: 0.00722588\n",
      "Epoch 2 | Step 1199900 | Avg Loss: 0.0154 | Grad Norm: 0.00820399\n",
      "Epoch 2 | Step 1200000 | Avg Loss: 0.0156 | Grad Norm: 0.00836635\n",
      "Saving model at step1200000\n",
      "Epoch 2 | Step 1200100 | Avg Loss: 0.0156 | Grad Norm: 0.00952566\n",
      "Epoch 2 | Step 1200200 | Avg Loss: 0.0153 | Grad Norm: 0.00864519\n",
      "Epoch 2 | Step 1200300 | Avg Loss: 0.0152 | Grad Norm: 0.00918291\n",
      "Epoch 2 | Step 1200400 | Avg Loss: 0.0152 | Grad Norm: 0.00785549\n",
      "Epoch 2 | Step 1200500 | Avg Loss: 0.0152 | Grad Norm: 0.00892715\n",
      "Epoch 2 | Step 1200600 | Avg Loss: 0.0154 | Grad Norm: 0.00903556\n",
      "Epoch 2 | Step 1200700 | Avg Loss: 0.0156 | Grad Norm: 0.00785318\n",
      "Epoch 2 | Step 1200800 | Avg Loss: 0.0153 | Grad Norm: 0.00975343\n",
      "Epoch 2 | Step 1200900 | Avg Loss: 0.0153 | Grad Norm: 0.00790764\n",
      "Epoch 2 | Step 1201000 | Avg Loss: 0.0148 | Grad Norm: 0.00899504\n",
      "Epoch 2 | Step 1201100 | Avg Loss: 0.0144 | Grad Norm: 0.00770217\n",
      "Epoch 2 | Step 1201200 | Avg Loss: 0.0143 | Grad Norm: 0.00879768\n",
      "Epoch 2 | Step 1201300 | Avg Loss: 0.0142 | Grad Norm: 0.00886073\n",
      "Epoch 2 | Step 1201400 | Avg Loss: 0.0146 | Grad Norm: 0.00869374\n",
      "Epoch 2 | Step 1201500 | Avg Loss: 0.0146 | Grad Norm: 0.00929795\n",
      "Epoch 2 | Step 1201600 | Avg Loss: 0.0147 | Grad Norm: 0.00747474\n",
      "Epoch 2 | Step 1201700 | Avg Loss: 0.0152 | Grad Norm: 0.00859562\n",
      "Epoch 2 | Step 1201800 | Avg Loss: 0.0149 | Grad Norm: 0.00882504\n",
      "Epoch 2 | Step 1201900 | Avg Loss: 0.0150 | Grad Norm: 0.00911877\n",
      "Epoch 2 | Step 1202000 | Avg Loss: 0.0153 | Grad Norm: 0.00810001\n",
      "Epoch 2 | Step 1202100 | Avg Loss: 0.0154 | Grad Norm: 0.00734265\n",
      "Epoch 2 | Step 1202200 | Avg Loss: 0.0151 | Grad Norm: 0.00778783\n",
      "Epoch 2 | Step 1202300 | Avg Loss: 0.0153 | Grad Norm: 0.00801128\n",
      "Epoch 2 | Step 1202400 | Avg Loss: 0.0152 | Grad Norm: 0.00821522\n",
      "Epoch 2 | Step 1202500 | Avg Loss: 0.0151 | Grad Norm: 0.00873383\n",
      "Epoch 2 | Step 1202600 | Avg Loss: 0.0147 | Grad Norm: 0.00903122\n",
      "Epoch 2 | Step 1202700 | Avg Loss: 0.0147 | Grad Norm: 0.00885374\n",
      "Epoch 2 | Step 1202800 | Avg Loss: 0.0149 | Grad Norm: 0.00912026\n",
      "Epoch 2 | Step 1202900 | Avg Loss: 0.0148 | Grad Norm: 0.00726941\n",
      "Epoch 2 | Step 1203000 | Avg Loss: 0.0150 | Grad Norm: 0.00843277\n",
      "Epoch 2 | Step 1203100 | Avg Loss: 0.0153 | Grad Norm: 0.00827912\n",
      "Epoch 2 | Step 1203200 | Avg Loss: 0.0154 | Grad Norm: 0.00709585\n",
      "Epoch 2 | Step 1203300 | Avg Loss: 0.0151 | Grad Norm: 0.00761368\n",
      "Epoch 2 | Step 1203400 | Avg Loss: 0.0149 | Grad Norm: 0.00881878\n",
      "Epoch 2 | Step 1203500 | Avg Loss: 0.0147 | Grad Norm: 0.00805837\n",
      "Epoch 2 | Step 1203600 | Avg Loss: 0.0146 | Grad Norm: 0.00897058\n",
      "Epoch 2 | Step 1203700 | Avg Loss: 0.0148 | Grad Norm: 0.00752849\n",
      "Epoch 2 | Step 1203800 | Avg Loss: 0.0149 | Grad Norm: 0.01000499\n",
      "Epoch 2 | Step 1203900 | Avg Loss: 0.0149 | Grad Norm: 0.00900512\n",
      "Epoch 2 | Step 1204000 | Avg Loss: 0.0151 | Grad Norm: 0.00897246\n",
      "Epoch 2 | Step 1204100 | Avg Loss: 0.0151 | Grad Norm: 0.00931047\n",
      "Epoch 2 | Step 1204200 | Avg Loss: 0.0153 | Grad Norm: 0.00984459\n",
      "Epoch 2 | Step 1204300 | Avg Loss: 0.0150 | Grad Norm: 0.00874674\n",
      "Epoch 2 | Step 1204400 | Avg Loss: 0.0148 | Grad Norm: 0.00814737\n",
      "Epoch 2 | Step 1204500 | Avg Loss: 0.0152 | Grad Norm: 0.00901438\n",
      "Epoch 2 | Step 1204600 | Avg Loss: 0.0151 | Grad Norm: 0.00964516\n",
      "Epoch 2 | Step 1204700 | Avg Loss: 0.0149 | Grad Norm: 0.00819545\n",
      "Epoch 2 | Step 1204800 | Avg Loss: 0.0149 | Grad Norm: 0.00849893\n",
      "Epoch 2 | Step 1204900 | Avg Loss: 0.0149 | Grad Norm: 0.00914173\n",
      "Epoch 2 | Step 1205000 | Avg Loss: 0.0149 | Grad Norm: 0.00841730\n",
      "Epoch 2 | Step 1205100 | Avg Loss: 0.0147 | Grad Norm: 0.00927541\n",
      "Epoch 2 | Step 1205200 | Avg Loss: 0.0145 | Grad Norm: 0.00791505\n",
      "Epoch 2 | Step 1205300 | Avg Loss: 0.0150 | Grad Norm: 0.00892126\n",
      "Epoch 2 | Step 1205400 | Avg Loss: 0.0149 | Grad Norm: 0.00858085\n",
      "Epoch 2 | Step 1205500 | Avg Loss: 0.0151 | Grad Norm: 0.00927889\n",
      "Epoch 2 | Step 1205600 | Avg Loss: 0.0150 | Grad Norm: 0.00835056\n",
      "Epoch 2 | Step 1205700 | Avg Loss: 0.0150 | Grad Norm: 0.00772616\n",
      "Epoch 2 | Step 1205800 | Avg Loss: 0.0147 | Grad Norm: 0.00738050\n",
      "Epoch 2 | Step 1205900 | Avg Loss: 0.0150 | Grad Norm: 0.00761941\n",
      "Epoch 2 | Step 1206000 | Avg Loss: 0.0151 | Grad Norm: 0.00738920\n",
      "Epoch 2 | Step 1206100 | Avg Loss: 0.0154 | Grad Norm: 0.00759179\n",
      "Epoch 2 | Step 1206200 | Avg Loss: 0.0152 | Grad Norm: 0.00846834\n",
      "Epoch 2 | Step 1206300 | Avg Loss: 0.0154 | Grad Norm: 0.00934185\n",
      "Epoch 2 | Step 1206400 | Avg Loss: 0.0154 | Grad Norm: 0.00742938\n",
      "Epoch 2 | Step 1206500 | Avg Loss: 0.0152 | Grad Norm: 0.00758368\n",
      "Epoch 2 | Step 1206600 | Avg Loss: 0.0151 | Grad Norm: 0.00824165\n",
      "Epoch 2 | Step 1206700 | Avg Loss: 0.0152 | Grad Norm: 0.00838951\n",
      "Epoch 2 | Step 1206800 | Avg Loss: 0.0152 | Grad Norm: 0.00755785\n",
      "Epoch 2 | Step 1206900 | Avg Loss: 0.0149 | Grad Norm: 0.00819516\n",
      "Epoch 2 | Step 1207000 | Avg Loss: 0.0151 | Grad Norm: 0.00852035\n",
      "Epoch 2 | Step 1207100 | Avg Loss: 0.0149 | Grad Norm: 0.00797592\n",
      "Epoch 2 | Step 1207200 | Avg Loss: 0.0147 | Grad Norm: 0.00692022\n",
      "Epoch 2 | Step 1207300 | Avg Loss: 0.0147 | Grad Norm: 0.00826347\n",
      "Epoch 2 | Step 1207400 | Avg Loss: 0.0148 | Grad Norm: 0.00958409\n",
      "Epoch 2 | Step 1207500 | Avg Loss: 0.0146 | Grad Norm: 0.00748538\n",
      "Epoch 2 | Step 1207600 | Avg Loss: 0.0148 | Grad Norm: 0.00742993\n",
      "Epoch 2 | Step 1207700 | Avg Loss: 0.0145 | Grad Norm: 0.00939115\n",
      "Epoch 2 | Step 1207800 | Avg Loss: 0.0144 | Grad Norm: 0.00859625\n",
      "Epoch 2 | Step 1207900 | Avg Loss: 0.0142 | Grad Norm: 0.00847281\n",
      "Epoch 2 | Step 1208000 | Avg Loss: 0.0148 | Grad Norm: 0.00775943\n",
      "Epoch 2 | Step 1208100 | Avg Loss: 0.0150 | Grad Norm: 0.00829078\n",
      "Epoch 2 | Step 1208200 | Avg Loss: 0.0149 | Grad Norm: 0.00730294\n",
      "Epoch 2 | Step 1208300 | Avg Loss: 0.0151 | Grad Norm: 0.01018723\n",
      "Epoch 2 | Step 1208400 | Avg Loss: 0.0150 | Grad Norm: 0.00779227\n",
      "Epoch 2 | Step 1208500 | Avg Loss: 0.0151 | Grad Norm: 0.00717624\n",
      "Epoch 2 | Step 1208600 | Avg Loss: 0.0153 | Grad Norm: 0.00901739\n",
      "Epoch 2 | Step 1208700 | Avg Loss: 0.0155 | Grad Norm: 0.00841344\n",
      "Epoch 2 | Step 1208800 | Avg Loss: 0.0153 | Grad Norm: 0.00931396\n",
      "Epoch 2 | Step 1208900 | Avg Loss: 0.0156 | Grad Norm: 0.00792946\n",
      "Epoch 2 | Step 1209000 | Avg Loss: 0.0153 | Grad Norm: 0.01084284\n",
      "Epoch 2 | Step 1209100 | Avg Loss: 0.0151 | Grad Norm: 0.00861204\n",
      "Epoch 2 | Step 1209200 | Avg Loss: 0.0153 | Grad Norm: 0.00811007\n",
      "Epoch 2 | Step 1209300 | Avg Loss: 0.0151 | Grad Norm: 0.00820139\n",
      "Epoch 2 | Step 1209400 | Avg Loss: 0.0153 | Grad Norm: 0.00984884\n",
      "Epoch 2 | Step 1209500 | Avg Loss: 0.0153 | Grad Norm: 0.00898429\n",
      "Epoch 2 | Step 1209600 | Avg Loss: 0.0151 | Grad Norm: 0.00765483\n",
      "Epoch 2 | Step 1209700 | Avg Loss: 0.0150 | Grad Norm: 0.00960344\n",
      "Epoch 2 | Step 1209800 | Avg Loss: 0.0149 | Grad Norm: 0.00891166\n",
      "Epoch 2 | Step 1209900 | Avg Loss: 0.0149 | Grad Norm: 0.01055208\n",
      "Epoch 2 | Step 1210000 | Avg Loss: 0.0152 | Grad Norm: 0.00923537\n",
      "Epoch 2 | Step 1210100 | Avg Loss: 0.0148 | Grad Norm: 0.00734867\n",
      "Epoch 2 | Step 1210200 | Avg Loss: 0.0151 | Grad Norm: 0.00892196\n",
      "Epoch 2 | Step 1210300 | Avg Loss: 0.0151 | Grad Norm: 0.01016145\n",
      "Epoch 2 | Step 1210400 | Avg Loss: 0.0150 | Grad Norm: 0.00846359\n",
      "Epoch 2 | Step 1210500 | Avg Loss: 0.0150 | Grad Norm: 0.00791376\n",
      "Epoch 2 | Step 1210600 | Avg Loss: 0.0149 | Grad Norm: 0.00842255\n",
      "Epoch 2 | Step 1210700 | Avg Loss: 0.0150 | Grad Norm: 0.00909137\n",
      "Epoch 2 | Step 1210800 | Avg Loss: 0.0154 | Grad Norm: 0.00878474\n",
      "Epoch 2 | Step 1210900 | Avg Loss: 0.0153 | Grad Norm: 0.00791588\n",
      "Epoch 2 | Step 1211000 | Avg Loss: 0.0154 | Grad Norm: 0.00933338\n",
      "Epoch 2 | Step 1211100 | Avg Loss: 0.0152 | Grad Norm: 0.00706842\n",
      "Epoch 2 | Step 1211200 | Avg Loss: 0.0153 | Grad Norm: 0.00749090\n",
      "Epoch 2 | Step 1211300 | Avg Loss: 0.0153 | Grad Norm: 0.00877632\n",
      "Epoch 2 | Step 1211400 | Avg Loss: 0.0152 | Grad Norm: 0.00928253\n",
      "Epoch 2 | Step 1211500 | Avg Loss: 0.0149 | Grad Norm: 0.00878182\n",
      "Epoch 2 | Step 1211600 | Avg Loss: 0.0150 | Grad Norm: 0.00876083\n",
      "Epoch 2 | Step 1211700 | Avg Loss: 0.0148 | Grad Norm: 0.00776961\n",
      "Epoch 2 | Step 1211800 | Avg Loss: 0.0148 | Grad Norm: 0.00762648\n",
      "Epoch 2 | Step 1211900 | Avg Loss: 0.0148 | Grad Norm: 0.00853331\n",
      "Epoch 2 | Step 1212000 | Avg Loss: 0.0147 | Grad Norm: 0.00760929\n",
      "Epoch 2 | Step 1212100 | Avg Loss: 0.0151 | Grad Norm: 0.00782481\n",
      "Epoch 2 | Step 1212200 | Avg Loss: 0.0149 | Grad Norm: 0.00914988\n",
      "Epoch 2 | Step 1212300 | Avg Loss: 0.0150 | Grad Norm: 0.00745575\n",
      "Epoch 2 | Step 1212400 | Avg Loss: 0.0149 | Grad Norm: 0.00821804\n",
      "Epoch 2 | Step 1212500 | Avg Loss: 0.0154 | Grad Norm: 0.00898579\n",
      "Epoch 2 | Step 1212600 | Avg Loss: 0.0151 | Grad Norm: 0.00842682\n",
      "Epoch 2 | Step 1212700 | Avg Loss: 0.0148 | Grad Norm: 0.00915803\n",
      "Epoch 2 | Step 1212800 | Avg Loss: 0.0149 | Grad Norm: 0.00807050\n",
      "Epoch 2 | Step 1212900 | Avg Loss: 0.0147 | Grad Norm: 0.00713902\n",
      "Epoch 2 | Step 1213000 | Avg Loss: 0.0146 | Grad Norm: 0.00800816\n",
      "Epoch 2 | Step 1213100 | Avg Loss: 0.0150 | Grad Norm: 0.00849870\n",
      "Epoch 2 | Step 1213200 | Avg Loss: 0.0153 | Grad Norm: 0.00865550\n",
      "Epoch 2 | Step 1213300 | Avg Loss: 0.0151 | Grad Norm: 0.00822599\n",
      "Epoch 2 | Step 1213400 | Avg Loss: 0.0150 | Grad Norm: 0.00759398\n",
      "Epoch 2 | Step 1213500 | Avg Loss: 0.0151 | Grad Norm: 0.00745926\n",
      "Epoch 2 | Step 1213600 | Avg Loss: 0.0154 | Grad Norm: 0.00834182\n",
      "Epoch 2 | Step 1213700 | Avg Loss: 0.0151 | Grad Norm: 0.00906607\n",
      "Epoch 2 | Step 1213800 | Avg Loss: 0.0155 | Grad Norm: 0.00808705\n",
      "Epoch 2 | Step 1213900 | Avg Loss: 0.0154 | Grad Norm: 0.00806803\n",
      "Epoch 2 | Step 1214000 | Avg Loss: 0.0155 | Grad Norm: 0.00864043\n",
      "Epoch 2 | Step 1214100 | Avg Loss: 0.0156 | Grad Norm: 0.00866353\n",
      "Epoch 2 | Step 1214200 | Avg Loss: 0.0155 | Grad Norm: 0.00916441\n",
      "Epoch 2 | Step 1214300 | Avg Loss: 0.0154 | Grad Norm: 0.00754340\n",
      "Epoch 2 | Step 1214400 | Avg Loss: 0.0150 | Grad Norm: 0.00981570\n",
      "Epoch 2 | Step 1214500 | Avg Loss: 0.0151 | Grad Norm: 0.00879412\n",
      "Epoch 2 | Step 1214600 | Avg Loss: 0.0151 | Grad Norm: 0.00970609\n",
      "Epoch 2 | Step 1214700 | Avg Loss: 0.0151 | Grad Norm: 0.00779981\n",
      "Epoch 2 | Step 1214800 | Avg Loss: 0.0150 | Grad Norm: 0.00858128\n",
      "Epoch 2 | Step 1214900 | Avg Loss: 0.0152 | Grad Norm: 0.01036209\n",
      "Epoch 2 | Step 1215000 | Avg Loss: 0.0150 | Grad Norm: 0.00721836\n",
      "Epoch 2 | Step 1215100 | Avg Loss: 0.0149 | Grad Norm: 0.00875738\n",
      "Epoch 2 | Step 1215200 | Avg Loss: 0.0153 | Grad Norm: 0.00785680\n",
      "Epoch 2 | Step 1215300 | Avg Loss: 0.0153 | Grad Norm: 0.00847365\n",
      "Epoch 2 | Step 1215400 | Avg Loss: 0.0149 | Grad Norm: 0.00862017\n",
      "Epoch 2 | Step 1215500 | Avg Loss: 0.0149 | Grad Norm: 0.00823523\n",
      "Epoch 2 | Step 1215600 | Avg Loss: 0.0150 | Grad Norm: 0.00855852\n",
      "Epoch 2 | Step 1215700 | Avg Loss: 0.0152 | Grad Norm: 0.00920907\n",
      "Epoch 2 | Step 1215800 | Avg Loss: 0.0150 | Grad Norm: 0.00878248\n",
      "Epoch 2 | Step 1215900 | Avg Loss: 0.0147 | Grad Norm: 0.00861094\n",
      "Epoch 2 | Step 1216000 | Avg Loss: 0.0148 | Grad Norm: 0.00731712\n",
      "Epoch 2 | Step 1216100 | Avg Loss: 0.0150 | Grad Norm: 0.00943764\n",
      "Epoch 2 | Step 1216200 | Avg Loss: 0.0150 | Grad Norm: 0.00762909\n",
      "Epoch 2 | Step 1216300 | Avg Loss: 0.0149 | Grad Norm: 0.00709283\n",
      "Epoch 2 | Step 1216400 | Avg Loss: 0.0147 | Grad Norm: 0.00746359\n",
      "Epoch 2 | Step 1216500 | Avg Loss: 0.0148 | Grad Norm: 0.00803024\n",
      "Epoch 2 | Step 1216600 | Avg Loss: 0.0149 | Grad Norm: 0.00763436\n",
      "Epoch 2 | Step 1216700 | Avg Loss: 0.0151 | Grad Norm: 0.00708632\n",
      "Epoch 2 | Step 1216800 | Avg Loss: 0.0147 | Grad Norm: 0.00902842\n",
      "Epoch 2 | Step 1216900 | Avg Loss: 0.0149 | Grad Norm: 0.00912885\n",
      "Epoch 2 | Step 1217000 | Avg Loss: 0.0150 | Grad Norm: 0.00792117\n",
      "Epoch 2 | Step 1217100 | Avg Loss: 0.0151 | Grad Norm: 0.00912676\n",
      "Epoch 2 | Step 1217200 | Avg Loss: 0.0150 | Grad Norm: 0.00775683\n",
      "Epoch 2 | Step 1217300 | Avg Loss: 0.0150 | Grad Norm: 0.00785221\n",
      "Epoch 2 | Step 1217400 | Avg Loss: 0.0149 | Grad Norm: 0.00895055\n",
      "Epoch 2 | Step 1217500 | Avg Loss: 0.0151 | Grad Norm: 0.00800178\n",
      "Epoch 2 | Step 1217600 | Avg Loss: 0.0154 | Grad Norm: 0.00808444\n",
      "Epoch 2 | Step 1217700 | Avg Loss: 0.0155 | Grad Norm: 0.00746212\n",
      "Epoch 2 | Step 1217800 | Avg Loss: 0.0154 | Grad Norm: 0.00866841\n",
      "Epoch 2 | Step 1217900 | Avg Loss: 0.0151 | Grad Norm: 0.01086880\n",
      "Epoch 2 | Step 1218000 | Avg Loss: 0.0152 | Grad Norm: 0.00883973\n",
      "Epoch 2 | Step 1218100 | Avg Loss: 0.0154 | Grad Norm: 0.00891918\n",
      "Epoch 2 | Step 1218200 | Avg Loss: 0.0149 | Grad Norm: 0.00887914\n",
      "Epoch 2 | Step 1218300 | Avg Loss: 0.0147 | Grad Norm: 0.00857331\n",
      "Epoch 2 | Step 1218400 | Avg Loss: 0.0146 | Grad Norm: 0.00867796\n",
      "Epoch 2 | Step 1218500 | Avg Loss: 0.0150 | Grad Norm: 0.00887171\n",
      "Epoch 2 | Step 1218600 | Avg Loss: 0.0153 | Grad Norm: 0.00861793\n",
      "Epoch 2 | Step 1218700 | Avg Loss: 0.0154 | Grad Norm: 0.00776993\n",
      "Epoch 2 | Step 1218800 | Avg Loss: 0.0151 | Grad Norm: 0.00796541\n",
      "Epoch 2 | Step 1218900 | Avg Loss: 0.0153 | Grad Norm: 0.00877742\n",
      "Epoch 2 | Step 1219000 | Avg Loss: 0.0150 | Grad Norm: 0.00764448\n",
      "Epoch 2 | Step 1219100 | Avg Loss: 0.0150 | Grad Norm: 0.01072158\n",
      "Epoch 2 | Step 1219200 | Avg Loss: 0.0148 | Grad Norm: 0.01036648\n",
      "Epoch 2 | Step 1219300 | Avg Loss: 0.0147 | Grad Norm: 0.00826914\n",
      "Epoch 2 | Step 1219400 | Avg Loss: 0.0147 | Grad Norm: 0.01010711\n",
      "Epoch 2 | Step 1219500 | Avg Loss: 0.0151 | Grad Norm: 0.00888804\n",
      "Epoch 2 | Step 1219600 | Avg Loss: 0.0151 | Grad Norm: 0.00945904\n",
      "Epoch 2 | Step 1219700 | Avg Loss: 0.0149 | Grad Norm: 0.00789629\n",
      "Epoch 2 | Step 1219800 | Avg Loss: 0.0147 | Grad Norm: 0.00790434\n",
      "Epoch 2 | Step 1219900 | Avg Loss: 0.0143 | Grad Norm: 0.00717667\n",
      "Epoch 2 | Step 1220000 | Avg Loss: 0.0142 | Grad Norm: 0.00764282\n",
      "Epoch 2 | Step 1220100 | Avg Loss: 0.0142 | Grad Norm: 0.00776444\n",
      "Epoch 2 | Step 1220200 | Avg Loss: 0.0140 | Grad Norm: 0.00752352\n",
      "Epoch 2 | Step 1220300 | Avg Loss: 0.0144 | Grad Norm: 0.00757089\n",
      "Epoch 2 | Step 1220400 | Avg Loss: 0.0145 | Grad Norm: 0.00690909\n",
      "Epoch 2 | Step 1220500 | Avg Loss: 0.0145 | Grad Norm: 0.00919733\n",
      "Epoch 2 | Step 1220600 | Avg Loss: 0.0148 | Grad Norm: 0.00965425\n",
      "Epoch 2 | Step 1220700 | Avg Loss: 0.0149 | Grad Norm: 0.00822965\n",
      "Epoch 2 | Step 1220800 | Avg Loss: 0.0149 | Grad Norm: 0.00889063\n",
      "Epoch 2 | Step 1220900 | Avg Loss: 0.0154 | Grad Norm: 0.00862679\n",
      "Epoch 2 | Step 1221000 | Avg Loss: 0.0153 | Grad Norm: 0.00910644\n",
      "Epoch 2 | Step 1221100 | Avg Loss: 0.0159 | Grad Norm: 0.00796303\n",
      "Epoch 2 | Step 1221200 | Avg Loss: 0.0157 | Grad Norm: 0.00999877\n",
      "Epoch 2 | Step 1221300 | Avg Loss: 0.0155 | Grad Norm: 0.00766070\n",
      "Epoch 2 | Step 1221400 | Avg Loss: 0.0152 | Grad Norm: 0.00832685\n",
      "Epoch 2 | Step 1221500 | Avg Loss: 0.0148 | Grad Norm: 0.00935438\n",
      "Epoch 2 | Step 1221600 | Avg Loss: 0.0144 | Grad Norm: 0.00798406\n",
      "Epoch 2 | Step 1221700 | Avg Loss: 0.0143 | Grad Norm: 0.00795556\n",
      "Epoch 2 | Step 1221800 | Avg Loss: 0.0142 | Grad Norm: 0.00719658\n",
      "Epoch 2 | Step 1221900 | Avg Loss: 0.0146 | Grad Norm: 0.00776771\n",
      "Epoch 2 | Step 1222000 | Avg Loss: 0.0144 | Grad Norm: 0.00727677\n",
      "Epoch 2 | Step 1222100 | Avg Loss: 0.0138 | Grad Norm: 0.00726974\n",
      "Epoch 2 | Step 1222200 | Avg Loss: 0.0140 | Grad Norm: 0.00791435\n",
      "Epoch 2 | Step 1222300 | Avg Loss: 0.0145 | Grad Norm: 0.00761206\n",
      "Epoch 2 | Step 1222400 | Avg Loss: 0.0148 | Grad Norm: 0.00818659\n",
      "Epoch 2 | Step 1222500 | Avg Loss: 0.0151 | Grad Norm: 0.00796066\n",
      "Epoch 2 | Step 1222600 | Avg Loss: 0.0151 | Grad Norm: 0.01003526\n",
      "Epoch 2 | Step 1222700 | Avg Loss: 0.0154 | Grad Norm: 0.00787669\n",
      "Epoch 2 | Step 1222800 | Avg Loss: 0.0156 | Grad Norm: 0.00809563\n",
      "Epoch 2 | Step 1222900 | Avg Loss: 0.0152 | Grad Norm: 0.00823021\n",
      "Epoch 2 | Step 1223000 | Avg Loss: 0.0150 | Grad Norm: 0.00867548\n",
      "Epoch 2 | Step 1223100 | Avg Loss: 0.0152 | Grad Norm: 0.00783075\n",
      "Epoch 2 | Step 1223200 | Avg Loss: 0.0149 | Grad Norm: 0.01014463\n",
      "Epoch 2 | Step 1223300 | Avg Loss: 0.0147 | Grad Norm: 0.00727613\n",
      "Epoch 2 | Step 1223400 | Avg Loss: 0.0143 | Grad Norm: 0.00795374\n",
      "Epoch 2 | Step 1223500 | Avg Loss: 0.0144 | Grad Norm: 0.00813528\n",
      "Epoch 2 | Step 1223600 | Avg Loss: 0.0148 | Grad Norm: 0.00753417\n",
      "Epoch 2 | Step 1223700 | Avg Loss: 0.0147 | Grad Norm: 0.00932423\n",
      "Epoch 2 | Step 1223800 | Avg Loss: 0.0149 | Grad Norm: 0.00923782\n",
      "Epoch 2 | Step 1223900 | Avg Loss: 0.0149 | Grad Norm: 0.00795198\n",
      "Epoch 2 | Step 1224000 | Avg Loss: 0.0150 | Grad Norm: 0.00821859\n",
      "Epoch 2 | Step 1224100 | Avg Loss: 0.0147 | Grad Norm: 0.00809904\n",
      "Epoch 2 | Step 1224200 | Avg Loss: 0.0146 | Grad Norm: 0.00828965\n",
      "Epoch 2 | Step 1224300 | Avg Loss: 0.0142 | Grad Norm: 0.00770054\n",
      "Epoch 2 | Step 1224400 | Avg Loss: 0.0148 | Grad Norm: 0.00846566\n",
      "Epoch 2 | Step 1224500 | Avg Loss: 0.0149 | Grad Norm: 0.00935823\n",
      "Epoch 2 | Step 1224600 | Avg Loss: 0.0154 | Grad Norm: 0.01002565\n",
      "Epoch 2 | Step 1224700 | Avg Loss: 0.0152 | Grad Norm: 0.00963347\n",
      "Epoch 2 | Step 1224800 | Avg Loss: 0.0153 | Grad Norm: 0.00868078\n",
      "Epoch 2 | Step 1224900 | Avg Loss: 0.0153 | Grad Norm: 0.00689888\n",
      "Epoch 2 | Step 1225000 | Avg Loss: 0.0153 | Grad Norm: 0.00748183\n",
      "Epoch 2 | Step 1225100 | Avg Loss: 0.0153 | Grad Norm: 0.00956057\n",
      "Epoch 2 | Step 1225200 | Avg Loss: 0.0156 | Grad Norm: 0.00881889\n",
      "Epoch 2 | Step 1225300 | Avg Loss: 0.0155 | Grad Norm: 0.00878178\n",
      "Epoch 2 | Step 1225400 | Avg Loss: 0.0156 | Grad Norm: 0.00852820\n",
      "Epoch 2 | Step 1225500 | Avg Loss: 0.0153 | Grad Norm: 0.00785682\n",
      "Epoch 2 | Step 1225600 | Avg Loss: 0.0155 | Grad Norm: 0.00879125\n",
      "Epoch 2 | Step 1225700 | Avg Loss: 0.0153 | Grad Norm: 0.00847508\n",
      "Epoch 2 | Step 1225800 | Avg Loss: 0.0151 | Grad Norm: 0.00939561\n",
      "Epoch 2 | Step 1225900 | Avg Loss: 0.0152 | Grad Norm: 0.00834443\n",
      "Epoch 2 | Step 1226000 | Avg Loss: 0.0153 | Grad Norm: 0.00798661\n",
      "Epoch 2 | Step 1226100 | Avg Loss: 0.0158 | Grad Norm: 0.00881108\n",
      "Epoch 2 | Step 1226200 | Avg Loss: 0.0157 | Grad Norm: 0.00833578\n",
      "Epoch 2 | Step 1226300 | Avg Loss: 0.0151 | Grad Norm: 0.00809323\n",
      "Epoch 2 | Step 1226400 | Avg Loss: 0.0151 | Grad Norm: 0.00959769\n",
      "Epoch 2 | Step 1226500 | Avg Loss: 0.0151 | Grad Norm: 0.00729387\n",
      "Epoch 2 | Step 1226600 | Avg Loss: 0.0151 | Grad Norm: 0.00950585\n",
      "Epoch 2 | Step 1226700 | Avg Loss: 0.0154 | Grad Norm: 0.00802827\n",
      "Epoch 2 | Step 1226800 | Avg Loss: 0.0156 | Grad Norm: 0.00783426\n",
      "Epoch 2 | Step 1226900 | Avg Loss: 0.0156 | Grad Norm: 0.00861059\n",
      "Epoch 2 | Step 1227000 | Avg Loss: 0.0153 | Grad Norm: 0.00813589\n",
      "Epoch 2 | Step 1227100 | Avg Loss: 0.0149 | Grad Norm: 0.00739728\n",
      "Epoch 2 | Step 1227200 | Avg Loss: 0.0149 | Grad Norm: 0.00802586\n",
      "Epoch 2 | Step 1227300 | Avg Loss: 0.0152 | Grad Norm: 0.00835183\n",
      "Epoch 2 | Step 1227400 | Avg Loss: 0.0147 | Grad Norm: 0.00892077\n",
      "Epoch 2 | Step 1227500 | Avg Loss: 0.0148 | Grad Norm: 0.00776225\n",
      "Epoch 2 | Step 1227600 | Avg Loss: 0.0147 | Grad Norm: 0.00775858\n",
      "Epoch 2 | Step 1227700 | Avg Loss: 0.0149 | Grad Norm: 0.00843067\n",
      "Epoch 2 | Step 1227800 | Avg Loss: 0.0149 | Grad Norm: 0.00905878\n",
      "Epoch 2 | Step 1227900 | Avg Loss: 0.0153 | Grad Norm: 0.00856272\n",
      "Epoch 2 | Step 1228000 | Avg Loss: 0.0153 | Grad Norm: 0.00798355\n",
      "Epoch 2 | Step 1228100 | Avg Loss: 0.0150 | Grad Norm: 0.00799954\n",
      "Epoch 2 | Step 1228200 | Avg Loss: 0.0154 | Grad Norm: 0.00928058\n",
      "Epoch 2 | Step 1228300 | Avg Loss: 0.0159 | Grad Norm: 0.00881674\n",
      "Epoch 2 | Step 1228400 | Avg Loss: 0.0155 | Grad Norm: 0.00752997\n",
      "Epoch 2 | Step 1228500 | Avg Loss: 0.0152 | Grad Norm: 0.00813134\n",
      "Epoch 2 | Step 1228600 | Avg Loss: 0.0157 | Grad Norm: 0.00927875\n",
      "Epoch 2 | Step 1228700 | Avg Loss: 0.0152 | Grad Norm: 0.00843639\n",
      "Epoch 2 | Step 1228800 | Avg Loss: 0.0149 | Grad Norm: 0.00752594\n",
      "Epoch 2 | Step 1228900 | Avg Loss: 0.0149 | Grad Norm: 0.00918822\n",
      "Epoch 2 | Step 1229000 | Avg Loss: 0.0152 | Grad Norm: 0.00763289\n",
      "Epoch 2 | Step 1229100 | Avg Loss: 0.0157 | Grad Norm: 0.00848278\n",
      "Epoch 2 | Step 1229200 | Avg Loss: 0.0158 | Grad Norm: 0.00818902\n",
      "Epoch 2 | Step 1229300 | Avg Loss: 0.0160 | Grad Norm: 0.00772920\n",
      "Epoch 2 | Step 1229400 | Avg Loss: 0.0159 | Grad Norm: 0.00903699\n",
      "Epoch 2 | Step 1229500 | Avg Loss: 0.0157 | Grad Norm: 0.00841623\n",
      "Epoch 2 | Step 1229600 | Avg Loss: 0.0152 | Grad Norm: 0.00851850\n",
      "Epoch 2 | Step 1229700 | Avg Loss: 0.0153 | Grad Norm: 0.00746733\n",
      "Epoch 2 | Step 1229800 | Avg Loss: 0.0151 | Grad Norm: 0.00843638\n",
      "Epoch 2 | Step 1229900 | Avg Loss: 0.0147 | Grad Norm: 0.00863210\n",
      "Epoch 2 | Step 1230000 | Avg Loss: 0.0150 | Grad Norm: 0.00837090\n",
      "Epoch 2 | Step 1230100 | Avg Loss: 0.0153 | Grad Norm: 0.00843493\n",
      "Epoch 2 | Step 1230200 | Avg Loss: 0.0151 | Grad Norm: 0.00794811\n",
      "Epoch 2 | Step 1230300 | Avg Loss: 0.0149 | Grad Norm: 0.00884060\n",
      "Epoch 2 | Step 1230400 | Avg Loss: 0.0153 | Grad Norm: 0.00899888\n",
      "Epoch 2 | Step 1230500 | Avg Loss: 0.0155 | Grad Norm: 0.00845036\n",
      "Epoch 2 | Step 1230600 | Avg Loss: 0.0151 | Grad Norm: 0.00839897\n",
      "Epoch 2 | Step 1230700 | Avg Loss: 0.0149 | Grad Norm: 0.00818217\n",
      "Epoch 2 | Step 1230800 | Avg Loss: 0.0152 | Grad Norm: 0.00942679\n",
      "Epoch 2 | Step 1230900 | Avg Loss: 0.0154 | Grad Norm: 0.00753140\n",
      "Epoch 2 | Step 1231000 | Avg Loss: 0.0153 | Grad Norm: 0.00858704\n",
      "Epoch 2 | Step 1231100 | Avg Loss: 0.0150 | Grad Norm: 0.00831585\n",
      "Epoch 2 | Step 1231200 | Avg Loss: 0.0150 | Grad Norm: 0.00911211\n",
      "Epoch 2 | Step 1231300 | Avg Loss: 0.0146 | Grad Norm: 0.00822294\n",
      "Epoch 2 | Step 1231400 | Avg Loss: 0.0146 | Grad Norm: 0.00809109\n",
      "Epoch 2 | Step 1231500 | Avg Loss: 0.0147 | Grad Norm: 0.00802987\n",
      "Epoch 2 | Step 1231600 | Avg Loss: 0.0147 | Grad Norm: 0.00892517\n",
      "Epoch 2 | Step 1231700 | Avg Loss: 0.0146 | Grad Norm: 0.00733756\n",
      "Epoch 2 | Step 1231800 | Avg Loss: 0.0142 | Grad Norm: 0.00732154\n",
      "Epoch 2 | Step 1231900 | Avg Loss: 0.0146 | Grad Norm: 0.00777216\n",
      "Epoch 2 | Step 1232000 | Avg Loss: 0.0146 | Grad Norm: 0.00798369\n",
      "Epoch 2 | Step 1232100 | Avg Loss: 0.0145 | Grad Norm: 0.00748484\n",
      "Epoch 2 | Step 1232200 | Avg Loss: 0.0149 | Grad Norm: 0.00822921\n",
      "Epoch 2 | Step 1232300 | Avg Loss: 0.0148 | Grad Norm: 0.00943457\n",
      "Epoch 2 | Step 1232400 | Avg Loss: 0.0151 | Grad Norm: 0.00803966\n",
      "Epoch 2 | Step 1232500 | Avg Loss: 0.0154 | Grad Norm: 0.00724747\n",
      "Epoch 2 | Step 1232600 | Avg Loss: 0.0154 | Grad Norm: 0.00872451\n",
      "Epoch 2 | Step 1232700 | Avg Loss: 0.0154 | Grad Norm: 0.00733915\n",
      "Epoch 2 | Step 1232800 | Avg Loss: 0.0151 | Grad Norm: 0.00752129\n",
      "Epoch 2 | Step 1232900 | Avg Loss: 0.0149 | Grad Norm: 0.00918259\n",
      "Epoch 2 | Step 1233000 | Avg Loss: 0.0154 | Grad Norm: 0.00953718\n",
      "Epoch 2 | Step 1233100 | Avg Loss: 0.0153 | Grad Norm: 0.00739213\n",
      "Epoch 2 | Step 1233200 | Avg Loss: 0.0153 | Grad Norm: 0.00791445\n",
      "Epoch 2 | Step 1233300 | Avg Loss: 0.0155 | Grad Norm: 0.00882909\n",
      "Epoch 2 | Step 1233400 | Avg Loss: 0.0156 | Grad Norm: 0.00986860\n",
      "Epoch 2 | Step 1233500 | Avg Loss: 0.0157 | Grad Norm: 0.00837762\n",
      "Epoch 2 | Step 1233600 | Avg Loss: 0.0157 | Grad Norm: 0.00783473\n",
      "Epoch 2 | Step 1233700 | Avg Loss: 0.0152 | Grad Norm: 0.00788800\n",
      "Epoch 2 | Step 1233800 | Avg Loss: 0.0153 | Grad Norm: 0.00881576\n",
      "Epoch 2 | Step 1233900 | Avg Loss: 0.0151 | Grad Norm: 0.00792678\n",
      "Epoch 2 | Step 1234000 | Avg Loss: 0.0151 | Grad Norm: 0.00790723\n",
      "Epoch 2 | Step 1234100 | Avg Loss: 0.0149 | Grad Norm: 0.00952531\n",
      "Epoch 2 | Step 1234200 | Avg Loss: 0.0148 | Grad Norm: 0.00824461\n",
      "Epoch 2 | Step 1234300 | Avg Loss: 0.0145 | Grad Norm: 0.00812832\n",
      "Epoch 2 | Step 1234400 | Avg Loss: 0.0151 | Grad Norm: 0.00845628\n",
      "Epoch 2 | Step 1234500 | Avg Loss: 0.0151 | Grad Norm: 0.00804918\n",
      "Epoch 2 | Step 1234600 | Avg Loss: 0.0147 | Grad Norm: 0.00882601\n",
      "Epoch 2 | Step 1234700 | Avg Loss: 0.0148 | Grad Norm: 0.00763492\n",
      "Epoch 2 | Step 1234800 | Avg Loss: 0.0148 | Grad Norm: 0.00966205\n",
      "Epoch 2 | Step 1234900 | Avg Loss: 0.0148 | Grad Norm: 0.00939080\n",
      "Epoch 2 | Step 1235000 | Avg Loss: 0.0150 | Grad Norm: 0.00891068\n",
      "Epoch 2 | Step 1235100 | Avg Loss: 0.0148 | Grad Norm: 0.00930599\n",
      "Epoch 2 | Step 1235200 | Avg Loss: 0.0149 | Grad Norm: 0.00725764\n",
      "Epoch 2 | Step 1235300 | Avg Loss: 0.0151 | Grad Norm: 0.00761722\n",
      "Epoch 2 | Step 1235400 | Avg Loss: 0.0150 | Grad Norm: 0.00792910\n",
      "Epoch 2 | Step 1235500 | Avg Loss: 0.0155 | Grad Norm: 0.00724153\n",
      "Epoch 2 | Step 1235600 | Avg Loss: 0.0156 | Grad Norm: 0.00873231\n",
      "Epoch 2 | Step 1235700 | Avg Loss: 0.0157 | Grad Norm: 0.00995969\n",
      "Epoch 2 | Step 1235800 | Avg Loss: 0.0153 | Grad Norm: 0.00842772\n",
      "Epoch 2 | Step 1235900 | Avg Loss: 0.0156 | Grad Norm: 0.00874746\n",
      "Epoch 2 | Step 1236000 | Avg Loss: 0.0157 | Grad Norm: 0.00949437\n",
      "Epoch 2 | Step 1236100 | Avg Loss: 0.0158 | Grad Norm: 0.00893242\n",
      "Epoch 2 | Step 1236200 | Avg Loss: 0.0155 | Grad Norm: 0.00888035\n",
      "Epoch 2 | Step 1236300 | Avg Loss: 0.0159 | Grad Norm: 0.00789966\n",
      "Epoch 2 | Step 1236400 | Avg Loss: 0.0157 | Grad Norm: 0.00788854\n",
      "Epoch 2 | Step 1236500 | Avg Loss: 0.0153 | Grad Norm: 0.00845192\n",
      "Epoch 2 | Step 1236600 | Avg Loss: 0.0152 | Grad Norm: 0.00831952\n",
      "Epoch 2 | Step 1236700 | Avg Loss: 0.0155 | Grad Norm: 0.00926849\n",
      "Epoch 2 | Step 1236800 | Avg Loss: 0.0153 | Grad Norm: 0.00808375\n",
      "Epoch 2 | Step 1236900 | Avg Loss: 0.0150 | Grad Norm: 0.00906308\n",
      "Epoch 2 | Step 1237000 | Avg Loss: 0.0147 | Grad Norm: 0.01058119\n",
      "Epoch 2 | Step 1237100 | Avg Loss: 0.0147 | Grad Norm: 0.00728891\n",
      "Epoch 2 | Step 1237200 | Avg Loss: 0.0142 | Grad Norm: 0.00763352\n",
      "Epoch 2 | Step 1237300 | Avg Loss: 0.0141 | Grad Norm: 0.00766023\n",
      "Epoch 2 | Step 1237400 | Avg Loss: 0.0145 | Grad Norm: 0.00851920\n",
      "Epoch 2 | Step 1237500 | Avg Loss: 0.0148 | Grad Norm: 0.00845926\n",
      "Epoch 2 | Step 1237600 | Avg Loss: 0.0145 | Grad Norm: 0.00746978\n",
      "Epoch 2 | Step 1237700 | Avg Loss: 0.0147 | Grad Norm: 0.00844158\n",
      "Epoch 2 | Step 1237800 | Avg Loss: 0.0146 | Grad Norm: 0.00886008\n",
      "Epoch 2 | Step 1237900 | Avg Loss: 0.0146 | Grad Norm: 0.00912960\n",
      "Epoch 2 | Step 1238000 | Avg Loss: 0.0148 | Grad Norm: 0.00858509\n",
      "Epoch 2 | Step 1238100 | Avg Loss: 0.0150 | Grad Norm: 0.00862810\n",
      "Epoch 2 | Step 1238200 | Avg Loss: 0.0149 | Grad Norm: 0.00810108\n",
      "Epoch 2 | Step 1238300 | Avg Loss: 0.0147 | Grad Norm: 0.00776152\n",
      "Epoch 2 | Step 1238400 | Avg Loss: 0.0148 | Grad Norm: 0.00904291\n",
      "Epoch 2 | Step 1238500 | Avg Loss: 0.0142 | Grad Norm: 0.00735405\n",
      "Epoch 2 | Step 1238600 | Avg Loss: 0.0147 | Grad Norm: 0.01047424\n",
      "Epoch 2 | Step 1238700 | Avg Loss: 0.0149 | Grad Norm: 0.00693878\n",
      "Epoch 2 | Step 1238800 | Avg Loss: 0.0154 | Grad Norm: 0.00761077\n",
      "Epoch 2 | Step 1238900 | Avg Loss: 0.0151 | Grad Norm: 0.00696421\n",
      "Epoch 2 | Step 1239000 | Avg Loss: 0.0149 | Grad Norm: 0.00707963\n",
      "Epoch 2 | Step 1239100 | Avg Loss: 0.0148 | Grad Norm: 0.00858594\n",
      "Epoch 2 | Step 1239200 | Avg Loss: 0.0146 | Grad Norm: 0.00785694\n",
      "Epoch 2 | Step 1239300 | Avg Loss: 0.0147 | Grad Norm: 0.00692074\n",
      "Epoch 2 | Step 1239400 | Avg Loss: 0.0146 | Grad Norm: 0.00736491\n",
      "Epoch 2 | Step 1239500 | Avg Loss: 0.0150 | Grad Norm: 0.01227867\n",
      "Epoch 2 | Step 1239600 | Avg Loss: 0.0148 | Grad Norm: 0.00898978\n",
      "Epoch 2 | Step 1239700 | Avg Loss: 0.0150 | Grad Norm: 0.00858213\n",
      "Epoch 2 | Step 1239800 | Avg Loss: 0.0150 | Grad Norm: 0.00775048\n",
      "Epoch 2 | Step 1239900 | Avg Loss: 0.0152 | Grad Norm: 0.00790729\n",
      "Epoch 2 | Step 1240000 | Avg Loss: 0.0152 | Grad Norm: 0.00993824\n",
      "Epoch 2 | Step 1240100 | Avg Loss: 0.0154 | Grad Norm: 0.00771821\n",
      "Epoch 2 | Step 1240200 | Avg Loss: 0.0151 | Grad Norm: 0.00805284\n",
      "Epoch 2 | Step 1240300 | Avg Loss: 0.0152 | Grad Norm: 0.00718041\n",
      "Epoch 2 | Step 1240400 | Avg Loss: 0.0153 | Grad Norm: 0.00869527\n",
      "Epoch 2 | Step 1240500 | Avg Loss: 0.0149 | Grad Norm: 0.00728747\n",
      "Epoch 2 | Step 1240600 | Avg Loss: 0.0147 | Grad Norm: 0.00896863\n",
      "Epoch 2 | Step 1240700 | Avg Loss: 0.0146 | Grad Norm: 0.00816332\n",
      "Epoch 2 | Step 1240800 | Avg Loss: 0.0148 | Grad Norm: 0.00802643\n",
      "Epoch 2 | Step 1240900 | Avg Loss: 0.0149 | Grad Norm: 0.00867378\n",
      "Epoch 2 | Step 1241000 | Avg Loss: 0.0149 | Grad Norm: 0.00798189\n",
      "Epoch 2 | Step 1241100 | Avg Loss: 0.0147 | Grad Norm: 0.00659238\n",
      "Epoch 2 | Step 1241200 | Avg Loss: 0.0147 | Grad Norm: 0.00771573\n",
      "Epoch 2 | Step 1241300 | Avg Loss: 0.0147 | Grad Norm: 0.00782044\n",
      "Epoch 2 | Step 1241400 | Avg Loss: 0.0145 | Grad Norm: 0.00818470\n",
      "Epoch 2 | Step 1241500 | Avg Loss: 0.0146 | Grad Norm: 0.00848260\n",
      "Epoch 2 | Step 1241600 | Avg Loss: 0.0149 | Grad Norm: 0.00898064\n",
      "Epoch 2 | Step 1241700 | Avg Loss: 0.0148 | Grad Norm: 0.00898198\n",
      "Epoch 2 | Step 1241800 | Avg Loss: 0.0147 | Grad Norm: 0.00947955\n",
      "Epoch 2 | Step 1241900 | Avg Loss: 0.0147 | Grad Norm: 0.00956423\n",
      "Epoch 2 | Step 1242000 | Avg Loss: 0.0144 | Grad Norm: 0.00970434\n",
      "Epoch 2 | Step 1242100 | Avg Loss: 0.0142 | Grad Norm: 0.00879116\n",
      "Epoch 2 | Step 1242200 | Avg Loss: 0.0148 | Grad Norm: 0.01112451\n",
      "Epoch 2 | Step 1242300 | Avg Loss: 0.0147 | Grad Norm: 0.00948412\n",
      "Epoch 2 | Step 1242400 | Avg Loss: 0.0147 | Grad Norm: 0.00821923\n",
      "Epoch 2 | Step 1242500 | Avg Loss: 0.0144 | Grad Norm: 0.00831321\n",
      "Epoch 2 | Step 1242600 | Avg Loss: 0.0148 | Grad Norm: 0.00688855\n",
      "Epoch 2 | Step 1242700 | Avg Loss: 0.0145 | Grad Norm: 0.00972679\n",
      "Epoch 2 | Step 1242800 | Avg Loss: 0.0143 | Grad Norm: 0.00801631\n",
      "Epoch 2 | Step 1242900 | Avg Loss: 0.0145 | Grad Norm: 0.00727646\n",
      "Epoch 2 | Step 1243000 | Avg Loss: 0.0147 | Grad Norm: 0.00879118\n",
      "Epoch 2 | Step 1243100 | Avg Loss: 0.0144 | Grad Norm: 0.00758251\n",
      "Epoch 2 | Step 1243200 | Avg Loss: 0.0143 | Grad Norm: 0.00826421\n",
      "Epoch 2 | Step 1243300 | Avg Loss: 0.0142 | Grad Norm: 0.00725570\n",
      "Epoch 2 | Step 1243400 | Avg Loss: 0.0146 | Grad Norm: 0.00760079\n",
      "Epoch 2 | Step 1243500 | Avg Loss: 0.0145 | Grad Norm: 0.00722838\n",
      "Epoch 2 | Step 1243600 | Avg Loss: 0.0144 | Grad Norm: 0.00932926\n",
      "Epoch 2 | Step 1243700 | Avg Loss: 0.0145 | Grad Norm: 0.00967653\n",
      "Epoch 2 | Step 1243800 | Avg Loss: 0.0146 | Grad Norm: 0.00977716\n",
      "Epoch 2 | Step 1243900 | Avg Loss: 0.0148 | Grad Norm: 0.00830150\n",
      "Epoch 2 | Step 1244000 | Avg Loss: 0.0147 | Grad Norm: 0.00823097\n",
      "Epoch 2 | Step 1244100 | Avg Loss: 0.0145 | Grad Norm: 0.01048793\n",
      "Epoch 2 | Step 1244200 | Avg Loss: 0.0151 | Grad Norm: 0.00756469\n",
      "Epoch 2 | Step 1244300 | Avg Loss: 0.0151 | Grad Norm: 0.00881000\n",
      "Epoch 2 | Step 1244400 | Avg Loss: 0.0151 | Grad Norm: 0.00784085\n",
      "Epoch 2 | Step 1244500 | Avg Loss: 0.0152 | Grad Norm: 0.00858085\n",
      "Epoch 2 | Step 1244600 | Avg Loss: 0.0151 | Grad Norm: 0.01001462\n",
      "Epoch 2 | Step 1244700 | Avg Loss: 0.0151 | Grad Norm: 0.00853010\n",
      "Epoch 2 | Step 1244800 | Avg Loss: 0.0153 | Grad Norm: 0.00789268\n",
      "Epoch 2 | Step 1244900 | Avg Loss: 0.0149 | Grad Norm: 0.00836354\n",
      "Epoch 2 | Step 1245000 | Avg Loss: 0.0149 | Grad Norm: 0.00833273\n",
      "Epoch 2 | Step 1245100 | Avg Loss: 0.0147 | Grad Norm: 0.00791170\n",
      "Epoch 2 | Step 1245200 | Avg Loss: 0.0145 | Grad Norm: 0.00758853\n",
      "Epoch 2 | Step 1245300 | Avg Loss: 0.0147 | Grad Norm: 0.00823898\n",
      "Epoch 2 | Step 1245400 | Avg Loss: 0.0150 | Grad Norm: 0.00872763\n",
      "Epoch 2 | Step 1245500 | Avg Loss: 0.0149 | Grad Norm: 0.01155644\n",
      "Epoch 2 | Step 1245600 | Avg Loss: 0.0152 | Grad Norm: 0.00826090\n",
      "Epoch 2 | Step 1245700 | Avg Loss: 0.0149 | Grad Norm: 0.00888791\n",
      "Epoch 2 | Step 1245800 | Avg Loss: 0.0148 | Grad Norm: 0.00884186\n",
      "Epoch 2 | Step 1245900 | Avg Loss: 0.0149 | Grad Norm: 0.00867687\n",
      "Epoch 2 | Step 1246000 | Avg Loss: 0.0150 | Grad Norm: 0.00786282\n",
      "Epoch 2 | Step 1246100 | Avg Loss: 0.0149 | Grad Norm: 0.00859512\n",
      "Epoch 2 | Step 1246200 | Avg Loss: 0.0152 | Grad Norm: 0.00869160\n",
      "Epoch 2 | Step 1246300 | Avg Loss: 0.0152 | Grad Norm: 0.00837258\n",
      "Epoch 2 | Step 1246400 | Avg Loss: 0.0150 | Grad Norm: 0.00830513\n",
      "Epoch 2 | Step 1246500 | Avg Loss: 0.0151 | Grad Norm: 0.00914796\n",
      "Epoch 2 | Step 1246600 | Avg Loss: 0.0149 | Grad Norm: 0.00942333\n",
      "Epoch 2 | Step 1246700 | Avg Loss: 0.0148 | Grad Norm: 0.00783258\n",
      "Epoch 2 | Step 1246800 | Avg Loss: 0.0153 | Grad Norm: 0.00759074\n",
      "Epoch 2 | Step 1246900 | Avg Loss: 0.0150 | Grad Norm: 0.00868589\n",
      "Epoch 2 | Step 1247000 | Avg Loss: 0.0149 | Grad Norm: 0.00939581\n",
      "Epoch 2 | Step 1247100 | Avg Loss: 0.0153 | Grad Norm: 0.00767988\n",
      "Epoch 2 | Step 1247200 | Avg Loss: 0.0154 | Grad Norm: 0.00847569\n",
      "Epoch 2 | Step 1247300 | Avg Loss: 0.0155 | Grad Norm: 0.00692116\n",
      "Epoch 2 | Step 1247400 | Avg Loss: 0.0155 | Grad Norm: 0.00837820\n",
      "Epoch 2 | Step 1247500 | Avg Loss: 0.0155 | Grad Norm: 0.00853433\n",
      "Epoch 2 | Step 1247600 | Avg Loss: 0.0153 | Grad Norm: 0.00840376\n",
      "Epoch 2 | Step 1247700 | Avg Loss: 0.0153 | Grad Norm: 0.00734638\n",
      "Epoch 2 | Step 1247800 | Avg Loss: 0.0150 | Grad Norm: 0.00946113\n",
      "Epoch 2 | Step 1247900 | Avg Loss: 0.0149 | Grad Norm: 0.00738125\n",
      "Epoch 2 | Step 1248000 | Avg Loss: 0.0148 | Grad Norm: 0.00944290\n",
      "Epoch 2 | Step 1248100 | Avg Loss: 0.0150 | Grad Norm: 0.00813299\n",
      "Epoch 2 | Step 1248200 | Avg Loss: 0.0152 | Grad Norm: 0.00873855\n",
      "Epoch 2 | Step 1248300 | Avg Loss: 0.0149 | Grad Norm: 0.01030324\n",
      "Epoch 2 | Step 1248400 | Avg Loss: 0.0149 | Grad Norm: 0.01053207\n",
      "Epoch 2 | Step 1248500 | Avg Loss: 0.0154 | Grad Norm: 0.00645433\n",
      "Epoch 2 | Step 1248600 | Avg Loss: 0.0150 | Grad Norm: 0.00871688\n",
      "Epoch 2 | Step 1248700 | Avg Loss: 0.0151 | Grad Norm: 0.00845662\n",
      "Epoch 2 | Step 1248800 | Avg Loss: 0.0147 | Grad Norm: 0.00772181\n",
      "Epoch 2 | Step 1248900 | Avg Loss: 0.0149 | Grad Norm: 0.00876746\n",
      "Epoch 2 | Step 1249000 | Avg Loss: 0.0152 | Grad Norm: 0.00826938\n",
      "Epoch 2 | Step 1249100 | Avg Loss: 0.0153 | Grad Norm: 0.00922319\n",
      "Epoch 2 | Step 1249200 | Avg Loss: 0.0150 | Grad Norm: 0.00963581\n",
      "Epoch 2 | Step 1249300 | Avg Loss: 0.0148 | Grad Norm: 0.00836597\n",
      "Epoch 2 | Step 1249400 | Avg Loss: 0.0149 | Grad Norm: 0.00820730\n",
      "Epoch 2 | Step 1249500 | Avg Loss: 0.0148 | Grad Norm: 0.00772453\n",
      "Epoch 2 | Step 1249600 | Avg Loss: 0.0153 | Grad Norm: 0.00770990\n",
      "Epoch 2 | Step 1249700 | Avg Loss: 0.0149 | Grad Norm: 0.00849371\n",
      "Epoch 2 | Step 1249800 | Avg Loss: 0.0153 | Grad Norm: 0.00790050\n",
      "Epoch 2 | Step 1249900 | Avg Loss: 0.0151 | Grad Norm: 0.00796273\n",
      "Epoch 2 | Step 1250000 | Avg Loss: 0.0150 | Grad Norm: 0.00736072\n",
      "Epoch 2 | Step 1250100 | Avg Loss: 0.0148 | Grad Norm: 0.00770368\n",
      "Epoch 2 | Step 1250200 | Avg Loss: 0.0150 | Grad Norm: 0.00853320\n",
      "Epoch 2 | Step 1250300 | Avg Loss: 0.0147 | Grad Norm: 0.00747510\n",
      "Epoch 2 | Step 1250400 | Avg Loss: 0.0151 | Grad Norm: 0.00926792\n",
      "Epoch 2 | Step 1250500 | Avg Loss: 0.0148 | Grad Norm: 0.00702615\n",
      "Epoch 2 | Step 1250600 | Avg Loss: 0.0152 | Grad Norm: 0.00813987\n",
      "Epoch 2 | Step 1250700 | Avg Loss: 0.0148 | Grad Norm: 0.00899970\n",
      "Epoch 2 | Step 1250800 | Avg Loss: 0.0150 | Grad Norm: 0.00797753\n",
      "Epoch 2 | Step 1250900 | Avg Loss: 0.0151 | Grad Norm: 0.00906365\n",
      "Epoch 2 | Step 1251000 | Avg Loss: 0.0151 | Grad Norm: 0.01128121\n",
      "Epoch 2 | Step 1251100 | Avg Loss: 0.0151 | Grad Norm: 0.00857826\n",
      "Epoch 2 | Step 1251200 | Avg Loss: 0.0150 | Grad Norm: 0.00939191\n",
      "Epoch 2 | Step 1251300 | Avg Loss: 0.0152 | Grad Norm: 0.00934848\n",
      "Epoch 2 | Step 1251400 | Avg Loss: 0.0154 | Grad Norm: 0.00823536\n",
      "Epoch 2 | Step 1251500 | Avg Loss: 0.0154 | Grad Norm: 0.01147728\n",
      "Epoch 2 | Step 1251600 | Avg Loss: 0.0150 | Grad Norm: 0.00863282\n",
      "Epoch 2 | Step 1251700 | Avg Loss: 0.0149 | Grad Norm: 0.00717414\n",
      "Epoch 2 | Step 1251800 | Avg Loss: 0.0151 | Grad Norm: 0.00817961\n",
      "Epoch 2 | Step 1251900 | Avg Loss: 0.0153 | Grad Norm: 0.00973465\n",
      "Epoch 2 | Step 1252000 | Avg Loss: 0.0150 | Grad Norm: 0.00734165\n",
      "Epoch 2 | Step 1252100 | Avg Loss: 0.0148 | Grad Norm: 0.00783459\n",
      "Epoch 2 | Step 1252200 | Avg Loss: 0.0147 | Grad Norm: 0.00918712\n",
      "Epoch 2 | Step 1252300 | Avg Loss: 0.0148 | Grad Norm: 0.00886286\n",
      "Epoch 2 | Step 1252400 | Avg Loss: 0.0149 | Grad Norm: 0.00918373\n",
      "Epoch 2 | Step 1252500 | Avg Loss: 0.0148 | Grad Norm: 0.00818347\n",
      "Epoch 2 | Step 1252600 | Avg Loss: 0.0150 | Grad Norm: 0.00854442\n",
      "Epoch 2 | Step 1252700 | Avg Loss: 0.0148 | Grad Norm: 0.00828406\n",
      "Epoch 2 | Step 1252800 | Avg Loss: 0.0147 | Grad Norm: 0.00702103\n",
      "Epoch 2 | Step 1252900 | Avg Loss: 0.0147 | Grad Norm: 0.00877160\n",
      "Epoch 2 | Step 1253000 | Avg Loss: 0.0150 | Grad Norm: 0.00774776\n",
      "Epoch 2 | Step 1253100 | Avg Loss: 0.0153 | Grad Norm: 0.00938388\n",
      "Epoch 2 | Step 1253200 | Avg Loss: 0.0151 | Grad Norm: 0.00862011\n",
      "Epoch 2 | Step 1253300 | Avg Loss: 0.0148 | Grad Norm: 0.00819823\n",
      "Epoch 2 | Step 1253400 | Avg Loss: 0.0147 | Grad Norm: 0.00796977\n",
      "Epoch 2 | Step 1253500 | Avg Loss: 0.0145 | Grad Norm: 0.00722917\n",
      "Epoch 2 | Step 1253600 | Avg Loss: 0.0146 | Grad Norm: 0.00755042\n",
      "Epoch 2 | Step 1253700 | Avg Loss: 0.0147 | Grad Norm: 0.00858805\n",
      "Epoch 2 | Step 1253800 | Avg Loss: 0.0149 | Grad Norm: 0.00730725\n",
      "Epoch 2 | Step 1253900 | Avg Loss: 0.0149 | Grad Norm: 0.00903262\n",
      "Epoch 2 | Step 1254000 | Avg Loss: 0.0147 | Grad Norm: 0.00890464\n",
      "Epoch 2 | Step 1254100 | Avg Loss: 0.0149 | Grad Norm: 0.00922350\n",
      "Epoch 2 | Step 1254200 | Avg Loss: 0.0151 | Grad Norm: 0.00827930\n",
      "Epoch 2 | Step 1254300 | Avg Loss: 0.0148 | Grad Norm: 0.00794230\n",
      "Epoch 2 | Step 1254400 | Avg Loss: 0.0147 | Grad Norm: 0.00778910\n",
      "Epoch 2 | Step 1254500 | Avg Loss: 0.0147 | Grad Norm: 0.00824496\n",
      "Epoch 2 | Step 1254600 | Avg Loss: 0.0146 | Grad Norm: 0.00812609\n",
      "Epoch 2 | Step 1254700 | Avg Loss: 0.0149 | Grad Norm: 0.00813434\n",
      "Epoch 2 | Step 1254800 | Avg Loss: 0.0144 | Grad Norm: 0.00760749\n",
      "Epoch 2 | Step 1254900 | Avg Loss: 0.0148 | Grad Norm: 0.00751581\n",
      "Epoch 2 | Step 1255000 | Avg Loss: 0.0149 | Grad Norm: 0.00849469\n",
      "Epoch 2 | Step 1255100 | Avg Loss: 0.0153 | Grad Norm: 0.00925718\n",
      "Epoch 2 | Step 1255200 | Avg Loss: 0.0154 | Grad Norm: 0.00969033\n",
      "Epoch 2 | Step 1255300 | Avg Loss: 0.0152 | Grad Norm: 0.00882828\n",
      "Epoch 2 | Step 1255400 | Avg Loss: 0.0150 | Grad Norm: 0.00747640\n",
      "Epoch 2 | Step 1255500 | Avg Loss: 0.0150 | Grad Norm: 0.00877547\n",
      "Epoch 2 | Step 1255600 | Avg Loss: 0.0152 | Grad Norm: 0.00891548\n",
      "Epoch 2 | Step 1255700 | Avg Loss: 0.0155 | Grad Norm: 0.00861171\n",
      "Epoch 2 | Step 1255800 | Avg Loss: 0.0157 | Grad Norm: 0.00858333\n",
      "Epoch 2 | Step 1255900 | Avg Loss: 0.0152 | Grad Norm: 0.00777065\n",
      "Epoch 2 | Step 1256000 | Avg Loss: 0.0154 | Grad Norm: 0.00930742\n",
      "Epoch 2 | Step 1256100 | Avg Loss: 0.0152 | Grad Norm: 0.00970081\n",
      "Epoch 2 | Step 1256200 | Avg Loss: 0.0154 | Grad Norm: 0.00965782\n",
      "Epoch 2 | Step 1256300 | Avg Loss: 0.0152 | Grad Norm: 0.00731721\n",
      "Epoch 2 | Step 1256400 | Avg Loss: 0.0152 | Grad Norm: 0.01065757\n",
      "Epoch 2 | Step 1256500 | Avg Loss: 0.0152 | Grad Norm: 0.00937570\n",
      "Epoch 2 | Step 1256600 | Avg Loss: 0.0146 | Grad Norm: 0.00878156\n",
      "Epoch 2 | Step 1256700 | Avg Loss: 0.0148 | Grad Norm: 0.00748654\n",
      "Epoch 2 | Step 1256800 | Avg Loss: 0.0143 | Grad Norm: 0.00732825\n",
      "Epoch 2 | Step 1256900 | Avg Loss: 0.0144 | Grad Norm: 0.00891121\n",
      "Epoch 2 | Step 1257000 | Avg Loss: 0.0144 | Grad Norm: 0.00852294\n",
      "Epoch 2 | Step 1257100 | Avg Loss: 0.0145 | Grad Norm: 0.00739992\n",
      "Epoch 2 | Step 1257200 | Avg Loss: 0.0146 | Grad Norm: 0.00883570\n",
      "Epoch 2 | Step 1257300 | Avg Loss: 0.0144 | Grad Norm: 0.00849368\n",
      "Epoch 2 | Step 1257400 | Avg Loss: 0.0147 | Grad Norm: 0.00767512\n",
      "Epoch 2 | Step 1257500 | Avg Loss: 0.0146 | Grad Norm: 0.00810754\n",
      "Epoch 2 | Step 1257600 | Avg Loss: 0.0151 | Grad Norm: 0.00788337\n",
      "Epoch 2 | Step 1257700 | Avg Loss: 0.0151 | Grad Norm: 0.00816735\n",
      "Epoch 2 | Step 1257800 | Avg Loss: 0.0155 | Grad Norm: 0.00729361\n",
      "Epoch 2 | Step 1257900 | Avg Loss: 0.0157 | Grad Norm: 0.00745337\n",
      "Epoch 2 | Step 1258000 | Avg Loss: 0.0154 | Grad Norm: 0.00744947\n",
      "Epoch 2 | Step 1258100 | Avg Loss: 0.0154 | Grad Norm: 0.00925491\n",
      "Epoch 2 | Step 1258200 | Avg Loss: 0.0152 | Grad Norm: 0.00806029\n",
      "Epoch 2 | Step 1258300 | Avg Loss: 0.0150 | Grad Norm: 0.00792797\n",
      "Epoch 2 | Step 1258400 | Avg Loss: 0.0147 | Grad Norm: 0.00799120\n",
      "Epoch 2 | Step 1258500 | Avg Loss: 0.0149 | Grad Norm: 0.00758862\n",
      "Epoch 2 | Step 1258600 | Avg Loss: 0.0150 | Grad Norm: 0.00797192\n",
      "Epoch 2 | Step 1258700 | Avg Loss: 0.0150 | Grad Norm: 0.00810929\n",
      "Epoch 2 | Step 1258800 | Avg Loss: 0.0153 | Grad Norm: 0.00911961\n",
      "Epoch 2 | Step 1258900 | Avg Loss: 0.0150 | Grad Norm: 0.00803410\n",
      "Epoch 2 | Step 1259000 | Avg Loss: 0.0153 | Grad Norm: 0.00957828\n",
      "Epoch 2 | Step 1259100 | Avg Loss: 0.0150 | Grad Norm: 0.00815137\n",
      "Epoch 2 | Step 1259200 | Avg Loss: 0.0153 | Grad Norm: 0.00756635\n",
      "Epoch 2 | Step 1259300 | Avg Loss: 0.0157 | Grad Norm: 0.00752671\n",
      "Epoch 2 | Step 1259400 | Avg Loss: 0.0155 | Grad Norm: 0.00815342\n",
      "Epoch 2 | Step 1259500 | Avg Loss: 0.0155 | Grad Norm: 0.00718956\n",
      "Epoch 2 | Step 1259600 | Avg Loss: 0.0151 | Grad Norm: 0.00752861\n",
      "Epoch 2 | Step 1259700 | Avg Loss: 0.0154 | Grad Norm: 0.00925256\n",
      "Epoch 2 | Step 1259800 | Avg Loss: 0.0155 | Grad Norm: 0.00842344\n",
      "Epoch 2 | Step 1259900 | Avg Loss: 0.0156 | Grad Norm: 0.00899165\n",
      "Epoch 2 | Step 1260000 | Avg Loss: 0.0152 | Grad Norm: 0.00799883\n",
      "Epoch 2 | Step 1260100 | Avg Loss: 0.0148 | Grad Norm: 0.00694811\n",
      "Epoch 2 | Step 1260200 | Avg Loss: 0.0151 | Grad Norm: 0.00766547\n",
      "Epoch 2 | Step 1260300 | Avg Loss: 0.0151 | Grad Norm: 0.00877098\n",
      "Epoch 2 | Step 1260400 | Avg Loss: 0.0150 | Grad Norm: 0.00821323\n",
      "Epoch 2 | Step 1260500 | Avg Loss: 0.0145 | Grad Norm: 0.00742555\n",
      "Epoch 2 | Step 1260600 | Avg Loss: 0.0148 | Grad Norm: 0.00998662\n",
      "Epoch 2 | Step 1260700 | Avg Loss: 0.0150 | Grad Norm: 0.00872180\n",
      "Epoch 2 | Step 1260800 | Avg Loss: 0.0153 | Grad Norm: 0.00842681\n",
      "Epoch 2 | Step 1260900 | Avg Loss: 0.0154 | Grad Norm: 0.00883724\n",
      "Epoch 2 | Step 1261000 | Avg Loss: 0.0155 | Grad Norm: 0.00896463\n",
      "Epoch 2 | Step 1261100 | Avg Loss: 0.0153 | Grad Norm: 0.00772319\n",
      "Epoch 2 | Step 1261200 | Avg Loss: 0.0155 | Grad Norm: 0.00886260\n",
      "Epoch 2 | Step 1261300 | Avg Loss: 0.0157 | Grad Norm: 0.00847485\n",
      "Epoch 2 | Step 1261400 | Avg Loss: 0.0156 | Grad Norm: 0.00826786\n",
      "Epoch 2 | Step 1261500 | Avg Loss: 0.0155 | Grad Norm: 0.00942476\n",
      "Epoch 2 | Step 1261600 | Avg Loss: 0.0154 | Grad Norm: 0.00797612\n",
      "Epoch 2 | Step 1261700 | Avg Loss: 0.0148 | Grad Norm: 0.00868052\n",
      "Epoch 2 | Step 1261800 | Avg Loss: 0.0147 | Grad Norm: 0.00926676\n",
      "Epoch 2 | Step 1261900 | Avg Loss: 0.0144 | Grad Norm: 0.00854071\n",
      "Epoch 2 | Step 1262000 | Avg Loss: 0.0146 | Grad Norm: 0.00721527\n",
      "Epoch 2 | Step 1262100 | Avg Loss: 0.0147 | Grad Norm: 0.00784035\n",
      "Epoch 2 | Step 1262200 | Avg Loss: 0.0150 | Grad Norm: 0.00949829\n",
      "Epoch 2 | Step 1262300 | Avg Loss: 0.0152 | Grad Norm: 0.00764939\n",
      "Epoch 2 | Step 1262400 | Avg Loss: 0.0153 | Grad Norm: 0.00700679\n",
      "Epoch 2 | Step 1262500 | Avg Loss: 0.0154 | Grad Norm: 0.00796113\n",
      "Epoch 2 | Step 1262600 | Avg Loss: 0.0155 | Grad Norm: 0.00912762\n",
      "Epoch 2 | Step 1262700 | Avg Loss: 0.0153 | Grad Norm: 0.00986703\n",
      "Epoch 2 | Step 1262800 | Avg Loss: 0.0152 | Grad Norm: 0.00676655\n",
      "Epoch 2 | Step 1262900 | Avg Loss: 0.0153 | Grad Norm: 0.00922826\n",
      "Epoch 2 | Step 1263000 | Avg Loss: 0.0153 | Grad Norm: 0.00917794\n",
      "Epoch 2 | Step 1263100 | Avg Loss: 0.0151 | Grad Norm: 0.00794449\n",
      "Epoch 2 | Step 1263200 | Avg Loss: 0.0146 | Grad Norm: 0.00821373\n",
      "Epoch 2 | Step 1263300 | Avg Loss: 0.0149 | Grad Norm: 0.00883274\n",
      "Epoch 2 | Step 1263400 | Avg Loss: 0.0150 | Grad Norm: 0.00845039\n",
      "Epoch 2 | Step 1263500 | Avg Loss: 0.0148 | Grad Norm: 0.00976149\n",
      "Epoch 2 | Step 1263600 | Avg Loss: 0.0147 | Grad Norm: 0.01008603\n",
      "Epoch 2 | Step 1263700 | Avg Loss: 0.0153 | Grad Norm: 0.00926945\n",
      "Epoch 2 | Step 1263800 | Avg Loss: 0.0154 | Grad Norm: 0.00867646\n",
      "Epoch 2 | Step 1263900 | Avg Loss: 0.0156 | Grad Norm: 0.00821793\n",
      "Epoch 2 | Step 1264000 | Avg Loss: 0.0154 | Grad Norm: 0.00842458\n",
      "Epoch 2 | Step 1264100 | Avg Loss: 0.0154 | Grad Norm: 0.00694208\n",
      "Epoch 2 | Step 1264200 | Avg Loss: 0.0152 | Grad Norm: 0.00844108\n",
      "Epoch 2 | Step 1264300 | Avg Loss: 0.0151 | Grad Norm: 0.00803641\n",
      "Epoch 2 | Step 1264400 | Avg Loss: 0.0151 | Grad Norm: 0.00883852\n",
      "Epoch 2 | Step 1264500 | Avg Loss: 0.0152 | Grad Norm: 0.00995569\n",
      "Epoch 2 | Step 1264600 | Avg Loss: 0.0150 | Grad Norm: 0.00764505\n",
      "Epoch 2 | Step 1264700 | Avg Loss: 0.0152 | Grad Norm: 0.00908620\n",
      "Epoch 2 | Step 1264800 | Avg Loss: 0.0150 | Grad Norm: 0.00729915\n",
      "Epoch 2 | Step 1264900 | Avg Loss: 0.0148 | Grad Norm: 0.00830645\n",
      "Epoch 2 | Step 1265000 | Avg Loss: 0.0150 | Grad Norm: 0.00858797\n",
      "Epoch 2 | Step 1265100 | Avg Loss: 0.0150 | Grad Norm: 0.00863868\n",
      "Epoch 2 | Step 1265200 | Avg Loss: 0.0148 | Grad Norm: 0.00729677\n",
      "Epoch 2 | Step 1265300 | Avg Loss: 0.0152 | Grad Norm: 0.00831863\n",
      "Epoch 2 | Step 1265400 | Avg Loss: 0.0150 | Grad Norm: 0.00725464\n",
      "Epoch 2 | Step 1265500 | Avg Loss: 0.0149 | Grad Norm: 0.00857077\n",
      "Epoch 2 | Step 1265600 | Avg Loss: 0.0148 | Grad Norm: 0.00928108\n",
      "Epoch 2 | Step 1265700 | Avg Loss: 0.0151 | Grad Norm: 0.00839588\n",
      "Epoch 2 | Step 1265800 | Avg Loss: 0.0149 | Grad Norm: 0.00815044\n",
      "Epoch 2 | Step 1265900 | Avg Loss: 0.0150 | Grad Norm: 0.00900659\n",
      "Epoch 2 | Step 1266000 | Avg Loss: 0.0149 | Grad Norm: 0.00765466\n",
      "Epoch 2 | Step 1266100 | Avg Loss: 0.0148 | Grad Norm: 0.00816443\n",
      "Epoch 2 | Step 1266200 | Avg Loss: 0.0149 | Grad Norm: 0.00782490\n",
      "Epoch 2 | Step 1266300 | Avg Loss: 0.0149 | Grad Norm: 0.00705765\n",
      "Epoch 2 | Step 1266400 | Avg Loss: 0.0148 | Grad Norm: 0.00825573\n",
      "Epoch 2 | Step 1266500 | Avg Loss: 0.0143 | Grad Norm: 0.00656299\n",
      "Epoch 2 | Step 1266600 | Avg Loss: 0.0145 | Grad Norm: 0.00723853\n",
      "Epoch 2 | Step 1266700 | Avg Loss: 0.0148 | Grad Norm: 0.00754088\n",
      "Epoch 2 | Step 1266800 | Avg Loss: 0.0147 | Grad Norm: 0.00860834\n",
      "Epoch 2 | Step 1266900 | Avg Loss: 0.0147 | Grad Norm: 0.00641857\n",
      "Epoch 2 | Step 1267000 | Avg Loss: 0.0151 | Grad Norm: 0.00852498\n",
      "Epoch 2 | Step 1267100 | Avg Loss: 0.0147 | Grad Norm: 0.00785274\n",
      "Epoch 2 | Step 1267200 | Avg Loss: 0.0146 | Grad Norm: 0.00817960\n",
      "Epoch 2 | Step 1267300 | Avg Loss: 0.0152 | Grad Norm: 0.00967931\n",
      "Epoch 2 | Step 1267400 | Avg Loss: 0.0152 | Grad Norm: 0.00845640\n",
      "Epoch 2 | Step 1267500 | Avg Loss: 0.0148 | Grad Norm: 0.00848369\n",
      "Epoch 2 | Step 1267600 | Avg Loss: 0.0156 | Grad Norm: 0.00802665\n",
      "Epoch 2 | Step 1267700 | Avg Loss: 0.0155 | Grad Norm: 0.00879742\n",
      "Epoch 2 | Step 1267800 | Avg Loss: 0.0150 | Grad Norm: 0.00760757\n",
      "Epoch 2 | Step 1267900 | Avg Loss: 0.0151 | Grad Norm: 0.00804845\n",
      "Epoch 2 | Step 1268000 | Avg Loss: 0.0149 | Grad Norm: 0.00799992\n",
      "Epoch 2 | Step 1268100 | Avg Loss: 0.0149 | Grad Norm: 0.00818710\n",
      "Epoch 2 | Step 1268200 | Avg Loss: 0.0151 | Grad Norm: 0.00856190\n",
      "Epoch 2 | Step 1268300 | Avg Loss: 0.0153 | Grad Norm: 0.00878039\n",
      "Epoch 2 | Step 1268400 | Avg Loss: 0.0154 | Grad Norm: 0.00783282\n",
      "Epoch 2 | Step 1268500 | Avg Loss: 0.0152 | Grad Norm: 0.01005006\n",
      "Epoch 2 | Step 1268600 | Avg Loss: 0.0151 | Grad Norm: 0.00795780\n",
      "Epoch 2 | Step 1268700 | Avg Loss: 0.0148 | Grad Norm: 0.00736865\n",
      "Epoch 2 | Step 1268800 | Avg Loss: 0.0144 | Grad Norm: 0.00774217\n",
      "Epoch 2 | Step 1268900 | Avg Loss: 0.0144 | Grad Norm: 0.00843703\n",
      "Epoch 2 | Step 1269000 | Avg Loss: 0.0144 | Grad Norm: 0.00737280\n",
      "Epoch 2 | Step 1269100 | Avg Loss: 0.0146 | Grad Norm: 0.00730940\n",
      "Epoch 2 | Step 1269200 | Avg Loss: 0.0147 | Grad Norm: 0.00929318\n",
      "Epoch 2 | Step 1269300 | Avg Loss: 0.0147 | Grad Norm: 0.00791442\n",
      "Epoch 2 | Step 1269400 | Avg Loss: 0.0149 | Grad Norm: 0.00763022\n",
      "Epoch 2 | Step 1269500 | Avg Loss: 0.0145 | Grad Norm: 0.00792952\n",
      "Epoch 2 | Step 1269600 | Avg Loss: 0.0149 | Grad Norm: 0.00857512\n",
      "Epoch 2 | Step 1269700 | Avg Loss: 0.0151 | Grad Norm: 0.00807421\n",
      "Epoch 2 | Step 1269800 | Avg Loss: 0.0148 | Grad Norm: 0.00820165\n",
      "Epoch 2 | Step 1269900 | Avg Loss: 0.0152 | Grad Norm: 0.00782778\n",
      "Epoch 2 | Step 1270000 | Avg Loss: 0.0153 | Grad Norm: 0.00769735\n",
      "Epoch 2 | Step 1270100 | Avg Loss: 0.0154 | Grad Norm: 0.00749680\n",
      "Epoch 2 | Step 1270200 | Avg Loss: 0.0156 | Grad Norm: 0.00941485\n",
      "Epoch 2 | Step 1270300 | Avg Loss: 0.0154 | Grad Norm: 0.00881021\n",
      "Epoch 2 | Step 1270400 | Avg Loss: 0.0150 | Grad Norm: 0.00783164\n",
      "Epoch 2 | Step 1270500 | Avg Loss: 0.0150 | Grad Norm: 0.00734850\n",
      "Epoch 2 | Step 1270600 | Avg Loss: 0.0153 | Grad Norm: 0.00792805\n",
      "Epoch 2 | Step 1270700 | Avg Loss: 0.0150 | Grad Norm: 0.00890345\n",
      "Epoch 2 | Step 1270800 | Avg Loss: 0.0151 | Grad Norm: 0.00794232\n",
      "Epoch 2 | Step 1270900 | Avg Loss: 0.0149 | Grad Norm: 0.00862407\n",
      "Epoch 2 | Step 1271000 | Avg Loss: 0.0151 | Grad Norm: 0.00752029\n",
      "Epoch 2 | Step 1271100 | Avg Loss: 0.0154 | Grad Norm: 0.01009359\n",
      "Epoch 2 | Step 1271200 | Avg Loss: 0.0156 | Grad Norm: 0.00873378\n",
      "Epoch 2 | Step 1271300 | Avg Loss: 0.0157 | Grad Norm: 0.01083273\n",
      "Epoch 2 | Step 1271400 | Avg Loss: 0.0158 | Grad Norm: 0.00852430\n",
      "Epoch 2 | Step 1271500 | Avg Loss: 0.0158 | Grad Norm: 0.00917503\n",
      "Epoch 2 | Step 1271600 | Avg Loss: 0.0159 | Grad Norm: 0.01039687\n",
      "Epoch 2 | Step 1271700 | Avg Loss: 0.0155 | Grad Norm: 0.00920057\n",
      "Epoch 2 | Step 1271800 | Avg Loss: 0.0159 | Grad Norm: 0.00943739\n",
      "Epoch 2 | Step 1271900 | Avg Loss: 0.0154 | Grad Norm: 0.00940744\n",
      "Epoch 2 | Step 1272000 | Avg Loss: 0.0151 | Grad Norm: 0.00751156\n",
      "Epoch 2 | Step 1272100 | Avg Loss: 0.0147 | Grad Norm: 0.00825479\n",
      "Epoch 2 | Step 1272200 | Avg Loss: 0.0156 | Grad Norm: 0.00916654\n",
      "Epoch 2 | Step 1272300 | Avg Loss: 0.0152 | Grad Norm: 0.00776155\n",
      "Epoch 2 | Step 1272400 | Avg Loss: 0.0153 | Grad Norm: 0.00953802\n",
      "Epoch 2 | Step 1272500 | Avg Loss: 0.0151 | Grad Norm: 0.00891280\n",
      "Epoch 2 | Step 1272600 | Avg Loss: 0.0147 | Grad Norm: 0.00918420\n",
      "Epoch 2 | Step 1272700 | Avg Loss: 0.0148 | Grad Norm: 0.00800657\n",
      "Epoch 2 | Step 1272800 | Avg Loss: 0.0149 | Grad Norm: 0.00720696\n",
      "Epoch 2 | Step 1272900 | Avg Loss: 0.0151 | Grad Norm: 0.00886068\n",
      "Epoch 2 | Step 1273000 | Avg Loss: 0.0151 | Grad Norm: 0.00863548\n",
      "Epoch 2 | Step 1273100 | Avg Loss: 0.0147 | Grad Norm: 0.00750077\n",
      "Epoch 2 | Step 1273200 | Avg Loss: 0.0146 | Grad Norm: 0.00842359\n",
      "Epoch 2 | Step 1273300 | Avg Loss: 0.0148 | Grad Norm: 0.00882477\n",
      "Epoch 2 | Step 1273400 | Avg Loss: 0.0147 | Grad Norm: 0.01044723\n",
      "Epoch 2 | Step 1273500 | Avg Loss: 0.0146 | Grad Norm: 0.00725342\n",
      "Epoch 2 | Step 1273600 | Avg Loss: 0.0149 | Grad Norm: 0.00770298\n",
      "Epoch 2 | Step 1273700 | Avg Loss: 0.0151 | Grad Norm: 0.00909366\n",
      "Epoch 2 | Step 1273800 | Avg Loss: 0.0153 | Grad Norm: 0.00885975\n",
      "Epoch 2 | Step 1273900 | Avg Loss: 0.0154 | Grad Norm: 0.00767997\n",
      "Epoch 2 | Step 1274000 | Avg Loss: 0.0148 | Grad Norm: 0.00885482\n",
      "Epoch 2 | Step 1274100 | Avg Loss: 0.0148 | Grad Norm: 0.00814615\n",
      "Epoch 2 | Step 1274200 | Avg Loss: 0.0150 | Grad Norm: 0.00908590\n",
      "Epoch 2 | Step 1274300 | Avg Loss: 0.0154 | Grad Norm: 0.00863328\n",
      "Epoch 2 | Step 1274400 | Avg Loss: 0.0154 | Grad Norm: 0.00922435\n",
      "Epoch 2 | Step 1274500 | Avg Loss: 0.0154 | Grad Norm: 0.00766450\n",
      "Epoch 2 | Step 1274600 | Avg Loss: 0.0152 | Grad Norm: 0.00830832\n",
      "Epoch 2 | Step 1274700 | Avg Loss: 0.0154 | Grad Norm: 0.00845630\n",
      "Epoch 2 | Step 1274800 | Avg Loss: 0.0152 | Grad Norm: 0.00785449\n",
      "Epoch 2 | Step 1274900 | Avg Loss: 0.0152 | Grad Norm: 0.00862184\n",
      "Epoch 2 | Step 1275000 | Avg Loss: 0.0156 | Grad Norm: 0.00893324\n",
      "Epoch 2 | Step 1275100 | Avg Loss: 0.0153 | Grad Norm: 0.00926992\n",
      "Epoch 2 | Step 1275200 | Avg Loss: 0.0151 | Grad Norm: 0.00804239\n",
      "Epoch 2 | Step 1275300 | Avg Loss: 0.0151 | Grad Norm: 0.00763233\n",
      "Epoch 2 | Step 1275400 | Avg Loss: 0.0152 | Grad Norm: 0.00733006\n",
      "Epoch 2 | Step 1275500 | Avg Loss: 0.0152 | Grad Norm: 0.00913207\n",
      "Epoch 2 | Step 1275600 | Avg Loss: 0.0146 | Grad Norm: 0.00910504\n",
      "Epoch 2 | Step 1275700 | Avg Loss: 0.0146 | Grad Norm: 0.00781802\n",
      "Epoch 2 | Step 1275800 | Avg Loss: 0.0149 | Grad Norm: 0.00735181\n",
      "Epoch 2 | Step 1275900 | Avg Loss: 0.0149 | Grad Norm: 0.00871449\n",
      "Epoch 2 | Step 1276000 | Avg Loss: 0.0149 | Grad Norm: 0.00807388\n",
      "Epoch 2 | Step 1276100 | Avg Loss: 0.0151 | Grad Norm: 0.00898097\n",
      "Epoch 2 | Step 1276200 | Avg Loss: 0.0155 | Grad Norm: 0.00955895\n",
      "Epoch 2 | Step 1276300 | Avg Loss: 0.0153 | Grad Norm: 0.00871490\n",
      "Epoch 2 | Step 1276400 | Avg Loss: 0.0154 | Grad Norm: 0.00926935\n",
      "Epoch 2 | Step 1276500 | Avg Loss: 0.0154 | Grad Norm: 0.00847294\n",
      "Epoch 2 | Step 1276600 | Avg Loss: 0.0154 | Grad Norm: 0.00826361\n",
      "Epoch 2 | Step 1276700 | Avg Loss: 0.0154 | Grad Norm: 0.00819891\n",
      "Epoch 2 | Step 1276800 | Avg Loss: 0.0153 | Grad Norm: 0.00826719\n",
      "Epoch 2 | Step 1276900 | Avg Loss: 0.0153 | Grad Norm: 0.00972270\n",
      "Epoch 2 | Step 1277000 | Avg Loss: 0.0149 | Grad Norm: 0.00769820\n",
      "Epoch 2 | Step 1277100 | Avg Loss: 0.0146 | Grad Norm: 0.00939566\n",
      "Epoch 2 | Step 1277200 | Avg Loss: 0.0148 | Grad Norm: 0.00705578\n",
      "Epoch 2 | Step 1277300 | Avg Loss: 0.0149 | Grad Norm: 0.00848171\n",
      "Epoch 2 | Step 1277400 | Avg Loss: 0.0149 | Grad Norm: 0.00845926\n",
      "Epoch 2 | Step 1277500 | Avg Loss: 0.0147 | Grad Norm: 0.00755120\n",
      "Epoch 2 | Step 1277600 | Avg Loss: 0.0149 | Grad Norm: 0.00911566\n",
      "Epoch 2 | Step 1277700 | Avg Loss: 0.0145 | Grad Norm: 0.00792734\n",
      "Epoch 2 | Step 1277800 | Avg Loss: 0.0147 | Grad Norm: 0.00811353\n",
      "Epoch 2 | Step 1277900 | Avg Loss: 0.0152 | Grad Norm: 0.00812272\n",
      "Epoch 2 | Step 1278000 | Avg Loss: 0.0153 | Grad Norm: 0.00673193\n",
      "Epoch 2 | Step 1278100 | Avg Loss: 0.0154 | Grad Norm: 0.00849232\n",
      "Epoch 2 | Step 1278200 | Avg Loss: 0.0151 | Grad Norm: 0.00888503\n",
      "Epoch 2 | Step 1278300 | Avg Loss: 0.0145 | Grad Norm: 0.00681587\n",
      "Epoch 2 | Step 1278400 | Avg Loss: 0.0150 | Grad Norm: 0.00824236\n",
      "Epoch 2 | Step 1278500 | Avg Loss: 0.0149 | Grad Norm: 0.00858794\n",
      "Epoch 2 | Step 1278600 | Avg Loss: 0.0146 | Grad Norm: 0.00995128\n",
      "Epoch 2 | Step 1278700 | Avg Loss: 0.0148 | Grad Norm: 0.00905693\n",
      "Epoch 2 | Step 1278800 | Avg Loss: 0.0146 | Grad Norm: 0.00810667\n",
      "Epoch 2 | Step 1278900 | Avg Loss: 0.0148 | Grad Norm: 0.00876585\n",
      "Epoch 2 | Step 1279000 | Avg Loss: 0.0143 | Grad Norm: 0.00776959\n",
      "Epoch 2 | Step 1279100 | Avg Loss: 0.0143 | Grad Norm: 0.00855103\n",
      "Epoch 2 | Step 1279200 | Avg Loss: 0.0144 | Grad Norm: 0.00823053\n",
      "Epoch 2 | Step 1279300 | Avg Loss: 0.0146 | Grad Norm: 0.00753389\n",
      "Epoch 2 | Step 1279400 | Avg Loss: 0.0146 | Grad Norm: 0.00816257\n",
      "Epoch 2 | Step 1279500 | Avg Loss: 0.0146 | Grad Norm: 0.00766235\n",
      "Epoch 2 | Step 1279600 | Avg Loss: 0.0145 | Grad Norm: 0.00864108\n",
      "Epoch 2 | Step 1279700 | Avg Loss: 0.0143 | Grad Norm: 0.00772469\n",
      "Epoch 2 | Step 1279800 | Avg Loss: 0.0147 | Grad Norm: 0.00815437\n",
      "Epoch 2 | Step 1279900 | Avg Loss: 0.0150 | Grad Norm: 0.00858423\n",
      "Epoch 2 | Step 1280000 | Avg Loss: 0.0156 | Grad Norm: 0.00886423\n",
      "Epoch 2 | Step 1280100 | Avg Loss: 0.0153 | Grad Norm: 0.00803658\n",
      "Epoch 2 | Step 1280200 | Avg Loss: 0.0150 | Grad Norm: 0.00791200\n",
      "Epoch 2 | Step 1280300 | Avg Loss: 0.0152 | Grad Norm: 0.00860842\n",
      "Epoch 2 | Step 1280400 | Avg Loss: 0.0156 | Grad Norm: 0.01041663\n",
      "Epoch 2 | Step 1280500 | Avg Loss: 0.0153 | Grad Norm: 0.00794366\n",
      "Epoch 2 | Step 1280600 | Avg Loss: 0.0152 | Grad Norm: 0.00769010\n",
      "Epoch 2 | Step 1280700 | Avg Loss: 0.0151 | Grad Norm: 0.00892355\n",
      "Epoch 2 | Step 1280800 | Avg Loss: 0.0153 | Grad Norm: 0.00841349\n",
      "Epoch 2 | Step 1280900 | Avg Loss: 0.0155 | Grad Norm: 0.00853521\n",
      "Epoch 2 | Step 1281000 | Avg Loss: 0.0155 | Grad Norm: 0.00702711\n",
      "Epoch 2 | Step 1281100 | Avg Loss: 0.0153 | Grad Norm: 0.00947433\n",
      "Epoch 2 | Step 1281200 | Avg Loss: 0.0156 | Grad Norm: 0.00883875\n",
      "Epoch 2 | Step 1281300 | Avg Loss: 0.0156 | Grad Norm: 0.00793288\n",
      "Epoch 2 | Step 1281400 | Avg Loss: 0.0152 | Grad Norm: 0.00879449\n",
      "Epoch 2 | Step 1281500 | Avg Loss: 0.0154 | Grad Norm: 0.01034297\n",
      "Epoch 2 | Step 1281600 | Avg Loss: 0.0154 | Grad Norm: 0.00941030\n",
      "Epoch 2 | Step 1281700 | Avg Loss: 0.0151 | Grad Norm: 0.00867112\n",
      "Epoch 2 | Step 1281800 | Avg Loss: 0.0150 | Grad Norm: 0.00925234\n",
      "Epoch 2 | Step 1281900 | Avg Loss: 0.0150 | Grad Norm: 0.00923797\n",
      "Epoch 2 | Step 1282000 | Avg Loss: 0.0151 | Grad Norm: 0.00955655\n",
      "Epoch 2 | Step 1282100 | Avg Loss: 0.0149 | Grad Norm: 0.00904143\n",
      "Epoch 2 | Step 1282200 | Avg Loss: 0.0150 | Grad Norm: 0.00792434\n",
      "Epoch 2 | Step 1282300 | Avg Loss: 0.0150 | Grad Norm: 0.00999282\n",
      "Epoch 2 | Step 1282400 | Avg Loss: 0.0148 | Grad Norm: 0.00856979\n",
      "Epoch 2 | Step 1282500 | Avg Loss: 0.0150 | Grad Norm: 0.00853445\n",
      "Epoch 2 | Step 1282600 | Avg Loss: 0.0146 | Grad Norm: 0.00753596\n",
      "Epoch 2 | Step 1282700 | Avg Loss: 0.0148 | Grad Norm: 0.01013260\n",
      "Epoch 2 | Step 1282800 | Avg Loss: 0.0145 | Grad Norm: 0.00843977\n",
      "Epoch 2 | Step 1282900 | Avg Loss: 0.0143 | Grad Norm: 0.00897180\n",
      "Epoch 2 | Step 1283000 | Avg Loss: 0.0143 | Grad Norm: 0.00780971\n",
      "Epoch 2 | Step 1283100 | Avg Loss: 0.0138 | Grad Norm: 0.00856406\n",
      "Epoch 2 | Step 1283200 | Avg Loss: 0.0140 | Grad Norm: 0.00822384\n",
      "Epoch 2 | Step 1283300 | Avg Loss: 0.0141 | Grad Norm: 0.00809724\n",
      "Epoch 2 | Step 1283400 | Avg Loss: 0.0142 | Grad Norm: 0.00940903\n",
      "Epoch 2 | Step 1283500 | Avg Loss: 0.0141 | Grad Norm: 0.00779110\n",
      "Epoch 2 | Step 1283600 | Avg Loss: 0.0146 | Grad Norm: 0.00819403\n",
      "Epoch 2 | Step 1283700 | Avg Loss: 0.0147 | Grad Norm: 0.00911504\n",
      "Epoch 2 | Step 1283800 | Avg Loss: 0.0142 | Grad Norm: 0.00705460\n",
      "Epoch 2 | Step 1283900 | Avg Loss: 0.0139 | Grad Norm: 0.00790840\n",
      "Epoch 2 | Step 1284000 | Avg Loss: 0.0142 | Grad Norm: 0.00841969\n",
      "Epoch 2 | Step 1284100 | Avg Loss: 0.0140 | Grad Norm: 0.00772855\n",
      "Epoch 2 | Step 1284200 | Avg Loss: 0.0144 | Grad Norm: 0.00964263\n",
      "Epoch 2 | Step 1284300 | Avg Loss: 0.0144 | Grad Norm: 0.00756320\n",
      "Epoch 2 | Step 1284400 | Avg Loss: 0.0146 | Grad Norm: 0.00768465\n",
      "Epoch 2 | Step 1284500 | Avg Loss: 0.0149 | Grad Norm: 0.00870949\n",
      "Epoch 2 | Step 1284600 | Avg Loss: 0.0145 | Grad Norm: 0.00794967\n",
      "Epoch 2 | Step 1284700 | Avg Loss: 0.0147 | Grad Norm: 0.00799449\n",
      "Epoch 2 | Step 1284800 | Avg Loss: 0.0151 | Grad Norm: 0.00843611\n",
      "Epoch 2 | Step 1284900 | Avg Loss: 0.0153 | Grad Norm: 0.00926501\n",
      "Epoch 2 | Step 1285000 | Avg Loss: 0.0152 | Grad Norm: 0.00883123\n",
      "Epoch 2 | Step 1285100 | Avg Loss: 0.0149 | Grad Norm: 0.01050540\n",
      "Epoch 2 | Step 1285200 | Avg Loss: 0.0150 | Grad Norm: 0.01106033\n",
      "Epoch 2 | Step 1285300 | Avg Loss: 0.0151 | Grad Norm: 0.01128958\n",
      "Epoch 2 | Step 1285400 | Avg Loss: 0.0150 | Grad Norm: 0.00799120\n",
      "Epoch 2 | Step 1285500 | Avg Loss: 0.0154 | Grad Norm: 0.00780705\n",
      "Epoch 2 | Step 1285600 | Avg Loss: 0.0151 | Grad Norm: 0.00867604\n",
      "Epoch 2 | Step 1285700 | Avg Loss: 0.0152 | Grad Norm: 0.01118446\n",
      "Epoch 2 | Step 1285800 | Avg Loss: 0.0154 | Grad Norm: 0.00831131\n",
      "Epoch 2 | Step 1285900 | Avg Loss: 0.0153 | Grad Norm: 0.00835891\n",
      "Epoch 2 | Step 1286000 | Avg Loss: 0.0156 | Grad Norm: 0.00868468\n",
      "Epoch 2 | Step 1286100 | Avg Loss: 0.0160 | Grad Norm: 0.00939947\n",
      "Epoch 2 | Step 1286200 | Avg Loss: 0.0153 | Grad Norm: 0.00741205\n",
      "Epoch 2 | Step 1286300 | Avg Loss: 0.0150 | Grad Norm: 0.00901012\n",
      "Epoch 2 | Step 1286400 | Avg Loss: 0.0149 | Grad Norm: 0.00863689\n",
      "Epoch 2 | Step 1286500 | Avg Loss: 0.0150 | Grad Norm: 0.00859042\n",
      "Epoch 2 | Step 1286600 | Avg Loss: 0.0152 | Grad Norm: 0.00770432\n",
      "Epoch 2 | Step 1286700 | Avg Loss: 0.0153 | Grad Norm: 0.00852848\n",
      "Epoch 2 | Step 1286800 | Avg Loss: 0.0153 | Grad Norm: 0.00769304\n",
      "Epoch 2 | Step 1286900 | Avg Loss: 0.0156 | Grad Norm: 0.00865264\n",
      "Epoch 2 | Step 1287000 | Avg Loss: 0.0160 | Grad Norm: 0.00809597\n",
      "Epoch 2 | Step 1287100 | Avg Loss: 0.0158 | Grad Norm: 0.00989983\n",
      "Epoch 2 | Step 1287200 | Avg Loss: 0.0158 | Grad Norm: 0.00835053\n",
      "Epoch 2 | Step 1287300 | Avg Loss: 0.0159 | Grad Norm: 0.01035457\n",
      "Epoch 2 | Step 1287400 | Avg Loss: 0.0157 | Grad Norm: 0.00836854\n",
      "Epoch 2 | Step 1287500 | Avg Loss: 0.0153 | Grad Norm: 0.01241336\n",
      "Epoch 2 | Step 1287600 | Avg Loss: 0.0150 | Grad Norm: 0.00981113\n",
      "Epoch 2 | Step 1287700 | Avg Loss: 0.0153 | Grad Norm: 0.00948800\n",
      "Epoch 2 | Step 1287800 | Avg Loss: 0.0150 | Grad Norm: 0.00850861\n",
      "Epoch 2 | Step 1287900 | Avg Loss: 0.0149 | Grad Norm: 0.00957601\n",
      "Epoch 2 | Step 1288000 | Avg Loss: 0.0150 | Grad Norm: 0.01217953\n",
      "Epoch 2 | Step 1288100 | Avg Loss: 0.0153 | Grad Norm: 0.00791112\n",
      "Epoch 2 | Step 1288200 | Avg Loss: 0.0154 | Grad Norm: 0.00739677\n",
      "Epoch 2 | Step 1288300 | Avg Loss: 0.0155 | Grad Norm: 0.00819314\n",
      "Epoch 2 | Step 1288400 | Avg Loss: 0.0149 | Grad Norm: 0.00792116\n",
      "Epoch 2 | Step 1288500 | Avg Loss: 0.0149 | Grad Norm: 0.00779663\n",
      "Epoch 2 | Step 1288600 | Avg Loss: 0.0148 | Grad Norm: 0.00922829\n",
      "Epoch 2 | Step 1288700 | Avg Loss: 0.0150 | Grad Norm: 0.00719294\n",
      "Epoch 2 | Step 1288800 | Avg Loss: 0.0153 | Grad Norm: 0.00776521\n",
      "Epoch 2 | Step 1288900 | Avg Loss: 0.0152 | Grad Norm: 0.00737326\n",
      "Epoch 2 | Step 1289000 | Avg Loss: 0.0155 | Grad Norm: 0.00943249\n",
      "Epoch 2 | Step 1289100 | Avg Loss: 0.0154 | Grad Norm: 0.00979326\n",
      "Epoch 2 | Step 1289200 | Avg Loss: 0.0155 | Grad Norm: 0.00893102\n",
      "Epoch 2 | Step 1289300 | Avg Loss: 0.0155 | Grad Norm: 0.00847629\n",
      "Epoch 2 | Step 1289400 | Avg Loss: 0.0152 | Grad Norm: 0.00846465\n",
      "Epoch 2 | Step 1289500 | Avg Loss: 0.0150 | Grad Norm: 0.00775798\n",
      "Epoch 2 | Step 1289600 | Avg Loss: 0.0147 | Grad Norm: 0.00688107\n",
      "Epoch 2 | Step 1289700 | Avg Loss: 0.0151 | Grad Norm: 0.00844798\n",
      "Epoch 2 | Step 1289800 | Avg Loss: 0.0152 | Grad Norm: 0.00883283\n",
      "Epoch 2 | Step 1289900 | Avg Loss: 0.0150 | Grad Norm: 0.00858763\n",
      "Epoch 2 | Step 1290000 | Avg Loss: 0.0149 | Grad Norm: 0.00829596\n",
      "Epoch 2 | Step 1290100 | Avg Loss: 0.0149 | Grad Norm: 0.00819662\n",
      "Epoch 2 | Step 1290200 | Avg Loss: 0.0145 | Grad Norm: 0.00733903\n",
      "Epoch 2 | Step 1290300 | Avg Loss: 0.0144 | Grad Norm: 0.01074599\n",
      "Epoch 2 | Step 1290400 | Avg Loss: 0.0145 | Grad Norm: 0.00893400\n",
      "Epoch 2 | Step 1290500 | Avg Loss: 0.0146 | Grad Norm: 0.00869283\n",
      "Epoch 2 | Step 1290600 | Avg Loss: 0.0145 | Grad Norm: 0.00806080\n",
      "Epoch 2 | Step 1290700 | Avg Loss: 0.0144 | Grad Norm: 0.00837348\n",
      "Epoch 2 | Step 1290800 | Avg Loss: 0.0144 | Grad Norm: 0.00698328\n",
      "Epoch 2 | Step 1290900 | Avg Loss: 0.0142 | Grad Norm: 0.00720004\n",
      "Epoch 2 | Step 1291000 | Avg Loss: 0.0145 | Grad Norm: 0.00852191\n",
      "Epoch 2 | Step 1291100 | Avg Loss: 0.0145 | Grad Norm: 0.00817025\n",
      "Epoch 2 | Step 1291200 | Avg Loss: 0.0141 | Grad Norm: 0.00829644\n",
      "Epoch 2 | Step 1291300 | Avg Loss: 0.0141 | Grad Norm: 0.00771559\n",
      "Epoch 2 | Step 1291400 | Avg Loss: 0.0144 | Grad Norm: 0.00785948\n",
      "Epoch 2 | Step 1291500 | Avg Loss: 0.0145 | Grad Norm: 0.00738522\n",
      "Epoch 2 | Step 1291600 | Avg Loss: 0.0147 | Grad Norm: 0.00892151\n",
      "Epoch 2 | Step 1291700 | Avg Loss: 0.0147 | Grad Norm: 0.01193665\n",
      "Epoch 2 | Step 1291800 | Avg Loss: 0.0149 | Grad Norm: 0.00867380\n",
      "Epoch 2 | Step 1291900 | Avg Loss: 0.0157 | Grad Norm: 0.00734379\n",
      "Epoch 2 | Step 1292000 | Avg Loss: 0.0153 | Grad Norm: 0.00947403\n",
      "Epoch 2 | Step 1292100 | Avg Loss: 0.0152 | Grad Norm: 0.00765713\n",
      "Epoch 2 | Step 1292200 | Avg Loss: 0.0154 | Grad Norm: 0.00824076\n",
      "Epoch 2 | Step 1292300 | Avg Loss: 0.0154 | Grad Norm: 0.00816847\n",
      "Epoch 2 | Step 1292400 | Avg Loss: 0.0153 | Grad Norm: 0.00827101\n",
      "Epoch 2 | Step 1292500 | Avg Loss: 0.0154 | Grad Norm: 0.00726271\n",
      "Epoch 2 | Step 1292600 | Avg Loss: 0.0152 | Grad Norm: 0.00801433\n",
      "Epoch 2 | Step 1292700 | Avg Loss: 0.0154 | Grad Norm: 0.00863573\n",
      "Epoch 2 | Step 1292800 | Avg Loss: 0.0154 | Grad Norm: 0.00908159\n",
      "Epoch 2 | Step 1292900 | Avg Loss: 0.0151 | Grad Norm: 0.00803340\n",
      "Epoch 2 | Step 1293000 | Avg Loss: 0.0149 | Grad Norm: 0.00816298\n",
      "Epoch 2 | Step 1293100 | Avg Loss: 0.0150 | Grad Norm: 0.00959309\n",
      "Epoch 2 | Step 1293200 | Avg Loss: 0.0152 | Grad Norm: 0.00996481\n",
      "Epoch 2 | Step 1293300 | Avg Loss: 0.0148 | Grad Norm: 0.00768316\n",
      "Epoch 2 | Step 1293400 | Avg Loss: 0.0149 | Grad Norm: 0.00830889\n",
      "Epoch 2 | Step 1293500 | Avg Loss: 0.0150 | Grad Norm: 0.00822223\n",
      "Epoch 2 | Step 1293600 | Avg Loss: 0.0153 | Grad Norm: 0.01086767\n",
      "Epoch 2 | Step 1293700 | Avg Loss: 0.0156 | Grad Norm: 0.00746314\n",
      "Epoch 2 | Step 1293800 | Avg Loss: 0.0152 | Grad Norm: 0.00829199\n",
      "Epoch 2 | Step 1293900 | Avg Loss: 0.0149 | Grad Norm: 0.00832613\n",
      "Epoch 2 | Step 1294000 | Avg Loss: 0.0149 | Grad Norm: 0.00845350\n",
      "Epoch 2 | Step 1294100 | Avg Loss: 0.0147 | Grad Norm: 0.00918665\n",
      "Epoch 2 | Step 1294200 | Avg Loss: 0.0151 | Grad Norm: 0.00896988\n",
      "Epoch 2 | Step 1294300 | Avg Loss: 0.0153 | Grad Norm: 0.00863224\n",
      "Epoch 2 | Step 1294400 | Avg Loss: 0.0150 | Grad Norm: 0.00841404\n",
      "Epoch 2 | Step 1294500 | Avg Loss: 0.0150 | Grad Norm: 0.00882640\n",
      "Epoch 2 | Step 1294600 | Avg Loss: 0.0146 | Grad Norm: 0.00837609\n",
      "Epoch 2 | Step 1294700 | Avg Loss: 0.0148 | Grad Norm: 0.00785665\n",
      "Epoch 2 | Step 1294800 | Avg Loss: 0.0145 | Grad Norm: 0.00854012\n",
      "Epoch 2 | Step 1294900 | Avg Loss: 0.0143 | Grad Norm: 0.00764072\n",
      "Epoch 2 | Step 1295000 | Avg Loss: 0.0146 | Grad Norm: 0.00912168\n",
      "Epoch 2 | Step 1295100 | Avg Loss: 0.0150 | Grad Norm: 0.00903976\n",
      "Epoch 2 | Step 1295200 | Avg Loss: 0.0150 | Grad Norm: 0.00833510\n",
      "Epoch 2 | Step 1295300 | Avg Loss: 0.0151 | Grad Norm: 0.00802163\n",
      "Epoch 2 | Step 1295400 | Avg Loss: 0.0148 | Grad Norm: 0.00758044\n",
      "Epoch 2 | Step 1295500 | Avg Loss: 0.0152 | Grad Norm: 0.00827481\n",
      "Epoch 2 | Step 1295600 | Avg Loss: 0.0154 | Grad Norm: 0.00959436\n",
      "Epoch 2 | Step 1295700 | Avg Loss: 0.0155 | Grad Norm: 0.00723717\n",
      "Epoch 2 | Step 1295800 | Avg Loss: 0.0155 | Grad Norm: 0.00747421\n",
      "Epoch 2 | Step 1295900 | Avg Loss: 0.0155 | Grad Norm: 0.00843297\n",
      "Epoch 2 | Step 1296000 | Avg Loss: 0.0157 | Grad Norm: 0.00861343\n",
      "Epoch 2 | Step 1296100 | Avg Loss: 0.0156 | Grad Norm: 0.00828129\n",
      "Epoch 2 | Step 1296200 | Avg Loss: 0.0160 | Grad Norm: 0.00769473\n",
      "Epoch 2 | Step 1296300 | Avg Loss: 0.0158 | Grad Norm: 0.00864374\n",
      "Epoch 2 | Step 1296400 | Avg Loss: 0.0155 | Grad Norm: 0.00793658\n",
      "Epoch 2 | Step 1296500 | Avg Loss: 0.0156 | Grad Norm: 0.00994103\n",
      "Epoch 2 | Step 1296600 | Avg Loss: 0.0151 | Grad Norm: 0.00835015\n",
      "Epoch 2 | Step 1296700 | Avg Loss: 0.0152 | Grad Norm: 0.00774651\n",
      "Epoch 2 | Step 1296800 | Avg Loss: 0.0155 | Grad Norm: 0.00934617\n",
      "Epoch 2 | Step 1296900 | Avg Loss: 0.0155 | Grad Norm: 0.00794393\n",
      "Epoch 2 | Step 1297000 | Avg Loss: 0.0155 | Grad Norm: 0.01124687\n",
      "Epoch 2 | Step 1297100 | Avg Loss: 0.0154 | Grad Norm: 0.00833034\n",
      "Epoch 2 | Step 1297200 | Avg Loss: 0.0155 | Grad Norm: 0.00777524\n",
      "Epoch 2 | Step 1297300 | Avg Loss: 0.0150 | Grad Norm: 0.00874968\n",
      "Epoch 2 | Step 1297400 | Avg Loss: 0.0151 | Grad Norm: 0.00822021\n",
      "Epoch 2 | Step 1297500 | Avg Loss: 0.0153 | Grad Norm: 0.00932388\n",
      "Epoch 2 | Step 1297600 | Avg Loss: 0.0155 | Grad Norm: 0.00872127\n",
      "Epoch 2 | Step 1297700 | Avg Loss: 0.0155 | Grad Norm: 0.00816938\n",
      "Epoch 2 | Step 1297800 | Avg Loss: 0.0152 | Grad Norm: 0.00882368\n",
      "Epoch 2 | Step 1297900 | Avg Loss: 0.0156 | Grad Norm: 0.00880093\n",
      "Epoch 2 | Step 1298000 | Avg Loss: 0.0155 | Grad Norm: 0.00886814\n",
      "Epoch 2 | Step 1298100 | Avg Loss: 0.0154 | Grad Norm: 0.00846251\n",
      "Epoch 2 | Step 1298200 | Avg Loss: 0.0157 | Grad Norm: 0.01000820\n",
      "Epoch 2 | Step 1298300 | Avg Loss: 0.0152 | Grad Norm: 0.00902602\n",
      "Epoch 2 | Step 1298400 | Avg Loss: 0.0149 | Grad Norm: 0.00701608\n",
      "Epoch 2 | Step 1298500 | Avg Loss: 0.0151 | Grad Norm: 0.00886177\n",
      "Epoch 2 | Step 1298600 | Avg Loss: 0.0147 | Grad Norm: 0.00944497\n",
      "Epoch 2 | Step 1298700 | Avg Loss: 0.0147 | Grad Norm: 0.00741351\n",
      "Epoch 2 | Step 1298800 | Avg Loss: 0.0149 | Grad Norm: 0.00810630\n",
      "Epoch 2 | Step 1298900 | Avg Loss: 0.0150 | Grad Norm: 0.00745864\n",
      "Epoch 2 | Step 1299000 | Avg Loss: 0.0150 | Grad Norm: 0.01035287\n",
      "Epoch 2 | Step 1299100 | Avg Loss: 0.0145 | Grad Norm: 0.00784066\n",
      "Epoch 2 | Step 1299200 | Avg Loss: 0.0145 | Grad Norm: 0.00851763\n",
      "Epoch 2 | Step 1299300 | Avg Loss: 0.0148 | Grad Norm: 0.00736037\n",
      "Epoch 2 | Step 1299400 | Avg Loss: 0.0148 | Grad Norm: 0.01068676\n",
      "Epoch 2 | Step 1299500 | Avg Loss: 0.0152 | Grad Norm: 0.00758607\n",
      "Epoch 2 | Step 1299600 | Avg Loss: 0.0152 | Grad Norm: 0.00890585\n",
      "Epoch 2 | Step 1299700 | Avg Loss: 0.0153 | Grad Norm: 0.00711579\n",
      "Epoch 2 | Step 1299800 | Avg Loss: 0.0152 | Grad Norm: 0.00845091\n",
      "Epoch 2 | Step 1299900 | Avg Loss: 0.0149 | Grad Norm: 0.00828555\n",
      "Epoch 2 | Step 1300000 | Avg Loss: 0.0148 | Grad Norm: 0.00802673\n",
      "Saving model at step1300000\n",
      "Epoch 2 | Step 1300100 | Avg Loss: 0.0147 | Grad Norm: 0.00796791\n",
      "Epoch 2 | Step 1300200 | Avg Loss: 0.0150 | Grad Norm: 0.00860605\n",
      "Epoch 2 | Step 1300300 | Avg Loss: 0.0150 | Grad Norm: 0.00727688\n",
      "Epoch 2 | Step 1300400 | Avg Loss: 0.0147 | Grad Norm: 0.00866930\n",
      "Epoch 2 | Step 1300500 | Avg Loss: 0.0146 | Grad Norm: 0.00707256\n",
      "Epoch 2 | Step 1300600 | Avg Loss: 0.0144 | Grad Norm: 0.00714852\n",
      "Epoch 2 | Step 1300700 | Avg Loss: 0.0147 | Grad Norm: 0.00752797\n",
      "Epoch 2 | Step 1300800 | Avg Loss: 0.0146 | Grad Norm: 0.00882382\n",
      "Epoch 2 | Step 1300900 | Avg Loss: 0.0141 | Grad Norm: 0.00751010\n",
      "Epoch 2 | Step 1301000 | Avg Loss: 0.0142 | Grad Norm: 0.00745540\n",
      "Epoch 2 | Step 1301100 | Avg Loss: 0.0148 | Grad Norm: 0.00776264\n",
      "Epoch 2 | Step 1301200 | Avg Loss: 0.0149 | Grad Norm: 0.00785958\n",
      "Epoch 2 | Step 1301300 | Avg Loss: 0.0150 | Grad Norm: 0.00863668\n",
      "Epoch 2 | Step 1301400 | Avg Loss: 0.0150 | Grad Norm: 0.00811453\n",
      "Epoch 2 | Step 1301500 | Avg Loss: 0.0150 | Grad Norm: 0.00839726\n",
      "Epoch 2 | Step 1301600 | Avg Loss: 0.0151 | Grad Norm: 0.00678067\n",
      "Epoch 2 | Step 1301700 | Avg Loss: 0.0152 | Grad Norm: 0.00746201\n",
      "Epoch 2 | Step 1301800 | Avg Loss: 0.0152 | Grad Norm: 0.00902626\n",
      "Epoch 2 | Step 1301900 | Avg Loss: 0.0150 | Grad Norm: 0.00816882\n",
      "Epoch 2 | Step 1302000 | Avg Loss: 0.0154 | Grad Norm: 0.00884888\n",
      "Epoch 2 | Step 1302100 | Avg Loss: 0.0156 | Grad Norm: 0.00804707\n",
      "Epoch 2 | Step 1302200 | Avg Loss: 0.0148 | Grad Norm: 0.00777376\n",
      "Epoch 2 | Step 1302300 | Avg Loss: 0.0147 | Grad Norm: 0.00830924\n",
      "Epoch 2 | Step 1302400 | Avg Loss: 0.0146 | Grad Norm: 0.00805059\n",
      "Epoch 2 | Step 1302500 | Avg Loss: 0.0148 | Grad Norm: 0.00755463\n",
      "Epoch 2 | Step 1302600 | Avg Loss: 0.0146 | Grad Norm: 0.00770211\n",
      "Epoch 2 | Step 1302700 | Avg Loss: 0.0146 | Grad Norm: 0.00727400\n",
      "Epoch 2 | Step 1302800 | Avg Loss: 0.0149 | Grad Norm: 0.00791510\n",
      "Epoch 2 | Step 1302900 | Avg Loss: 0.0151 | Grad Norm: 0.00895412\n",
      "Epoch 2 | Step 1303000 | Avg Loss: 0.0151 | Grad Norm: 0.00912225\n",
      "Epoch 2 | Step 1303100 | Avg Loss: 0.0149 | Grad Norm: 0.00812231\n",
      "Epoch 2 | Step 1303200 | Avg Loss: 0.0152 | Grad Norm: 0.00805652\n",
      "Epoch 2 | Step 1303300 | Avg Loss: 0.0151 | Grad Norm: 0.00964924\n",
      "Epoch 2 | Step 1303400 | Avg Loss: 0.0148 | Grad Norm: 0.00746699\n",
      "Epoch 2 | Step 1303500 | Avg Loss: 0.0145 | Grad Norm: 0.00810466\n",
      "Epoch 2 | Step 1303600 | Avg Loss: 0.0144 | Grad Norm: 0.00885612\n",
      "Epoch 2 | Step 1303700 | Avg Loss: 0.0149 | Grad Norm: 0.00820520\n",
      "Epoch 2 | Step 1303800 | Avg Loss: 0.0148 | Grad Norm: 0.00682901\n",
      "Epoch 2 | Step 1303900 | Avg Loss: 0.0151 | Grad Norm: 0.00745718\n",
      "Epoch 2 | Step 1304000 | Avg Loss: 0.0147 | Grad Norm: 0.00900410\n",
      "Epoch 2 | Step 1304100 | Avg Loss: 0.0149 | Grad Norm: 0.00713007\n",
      "Epoch 2 | Step 1304200 | Avg Loss: 0.0151 | Grad Norm: 0.00993895\n",
      "Epoch 2 | Step 1304300 | Avg Loss: 0.0151 | Grad Norm: 0.00852131\n",
      "Epoch 2 | Step 1304400 | Avg Loss: 0.0147 | Grad Norm: 0.00785343\n",
      "Epoch 2 | Step 1304500 | Avg Loss: 0.0151 | Grad Norm: 0.00748972\n",
      "Epoch 2 | Step 1304600 | Avg Loss: 0.0151 | Grad Norm: 0.00801996\n",
      "Epoch 2 | Step 1304700 | Avg Loss: 0.0152 | Grad Norm: 0.00807414\n",
      "Epoch 2 | Step 1304800 | Avg Loss: 0.0153 | Grad Norm: 0.00921635\n",
      "Epoch 2 | Step 1304900 | Avg Loss: 0.0151 | Grad Norm: 0.00743198\n",
      "Epoch 2 | Step 1305000 | Avg Loss: 0.0149 | Grad Norm: 0.00864454\n",
      "Epoch 2 | Step 1305100 | Avg Loss: 0.0147 | Grad Norm: 0.00796234\n",
      "Epoch 2 | Step 1305200 | Avg Loss: 0.0148 | Grad Norm: 0.00800443\n",
      "Epoch 2 | Step 1305300 | Avg Loss: 0.0147 | Grad Norm: 0.00903464\n",
      "Epoch 2 | Step 1305400 | Avg Loss: 0.0146 | Grad Norm: 0.00777957\n",
      "Epoch 2 | Step 1305500 | Avg Loss: 0.0148 | Grad Norm: 0.00815209\n",
      "Epoch 2 | Step 1305600 | Avg Loss: 0.0149 | Grad Norm: 0.00742481\n",
      "Epoch 2 | Step 1305700 | Avg Loss: 0.0148 | Grad Norm: 0.00775866\n",
      "Epoch 2 | Step 1305800 | Avg Loss: 0.0145 | Grad Norm: 0.00759577\n",
      "Epoch 2 | Step 1305900 | Avg Loss: 0.0146 | Grad Norm: 0.00793899\n",
      "Epoch 2 | Step 1306000 | Avg Loss: 0.0148 | Grad Norm: 0.00873309\n",
      "Epoch 2 | Step 1306100 | Avg Loss: 0.0147 | Grad Norm: 0.00890036\n",
      "Epoch 2 | Step 1306200 | Avg Loss: 0.0149 | Grad Norm: 0.00919061\n",
      "Epoch 2 | Step 1306300 | Avg Loss: 0.0153 | Grad Norm: 0.00973848\n",
      "Epoch 2 | Step 1306400 | Avg Loss: 0.0152 | Grad Norm: 0.00839078\n",
      "Epoch 2 | Step 1306500 | Avg Loss: 0.0155 | Grad Norm: 0.00698349\n",
      "Epoch 2 | Step 1306600 | Avg Loss: 0.0156 | Grad Norm: 0.00946938\n",
      "Epoch 2 | Step 1306700 | Avg Loss: 0.0153 | Grad Norm: 0.00885706\n",
      "Epoch 2 | Step 1306800 | Avg Loss: 0.0155 | Grad Norm: 0.00766492\n",
      "Epoch 2 | Step 1306900 | Avg Loss: 0.0156 | Grad Norm: 0.00838466\n",
      "Epoch 2 | Step 1307000 | Avg Loss: 0.0154 | Grad Norm: 0.00868978\n",
      "Epoch 2 | Step 1307100 | Avg Loss: 0.0151 | Grad Norm: 0.01083772\n",
      "Epoch 2 | Step 1307200 | Avg Loss: 0.0147 | Grad Norm: 0.00773764\n",
      "Epoch 2 | Step 1307300 | Avg Loss: 0.0149 | Grad Norm: 0.00877688\n",
      "Epoch 2 | Step 1307400 | Avg Loss: 0.0148 | Grad Norm: 0.00936381\n",
      "Epoch 2 | Step 1307500 | Avg Loss: 0.0148 | Grad Norm: 0.00767824\n",
      "Epoch 2 | Step 1307600 | Avg Loss: 0.0146 | Grad Norm: 0.00708078\n",
      "Epoch 2 | Step 1307700 | Avg Loss: 0.0147 | Grad Norm: 0.00758237\n",
      "Epoch 2 | Step 1307800 | Avg Loss: 0.0149 | Grad Norm: 0.00771954\n",
      "Epoch 2 | Step 1307900 | Avg Loss: 0.0149 | Grad Norm: 0.00797224\n",
      "Epoch 2 | Step 1308000 | Avg Loss: 0.0149 | Grad Norm: 0.00769267\n",
      "Epoch 2 | Step 1308100 | Avg Loss: 0.0149 | Grad Norm: 0.00782653\n",
      "Epoch 2 | Step 1308200 | Avg Loss: 0.0150 | Grad Norm: 0.01145969\n",
      "Epoch 2 | Step 1308300 | Avg Loss: 0.0153 | Grad Norm: 0.00748165\n",
      "Epoch 2 | Step 1308400 | Avg Loss: 0.0151 | Grad Norm: 0.00727206\n",
      "Epoch 2 | Step 1308500 | Avg Loss: 0.0156 | Grad Norm: 0.00817720\n",
      "Epoch 2 | Step 1308600 | Avg Loss: 0.0156 | Grad Norm: 0.00815060\n",
      "Epoch 2 | Step 1308700 | Avg Loss: 0.0156 | Grad Norm: 0.01311086\n",
      "Epoch 2 | Step 1308800 | Avg Loss: 0.0157 | Grad Norm: 0.00815296\n",
      "Epoch 2 | Step 1308900 | Avg Loss: 0.0157 | Grad Norm: 0.00735995\n",
      "Epoch 2 | Step 1309000 | Avg Loss: 0.0154 | Grad Norm: 0.00863143\n",
      "Epoch 2 | Step 1309100 | Avg Loss: 0.0153 | Grad Norm: 0.00839087\n",
      "Epoch 2 | Step 1309200 | Avg Loss: 0.0153 | Grad Norm: 0.00995331\n",
      "Epoch 2 | Step 1309300 | Avg Loss: 0.0149 | Grad Norm: 0.00959160\n",
      "Epoch 2 | Step 1309400 | Avg Loss: 0.0148 | Grad Norm: 0.00877528\n",
      "Epoch 2 | Step 1309500 | Avg Loss: 0.0145 | Grad Norm: 0.00676644\n",
      "Epoch 2 | Step 1309600 | Avg Loss: 0.0143 | Grad Norm: 0.00747334\n",
      "Epoch 2 | Step 1309700 | Avg Loss: 0.0145 | Grad Norm: 0.00813957\n",
      "Epoch 2 | Step 1309800 | Avg Loss: 0.0146 | Grad Norm: 0.00875169\n",
      "Epoch 2 | Step 1309900 | Avg Loss: 0.0147 | Grad Norm: 0.00843221\n",
      "Epoch 2 | Step 1310000 | Avg Loss: 0.0145 | Grad Norm: 0.00796175\n",
      "Epoch 2 | Step 1310100 | Avg Loss: 0.0151 | Grad Norm: 0.00840021\n",
      "Epoch 2 | Step 1310200 | Avg Loss: 0.0150 | Grad Norm: 0.00844704\n",
      "Epoch 2 | Step 1310300 | Avg Loss: 0.0152 | Grad Norm: 0.01000442\n",
      "Epoch 2 | Step 1310400 | Avg Loss: 0.0152 | Grad Norm: 0.00750029\n",
      "Epoch 2 | Step 1310500 | Avg Loss: 0.0153 | Grad Norm: 0.00941030\n",
      "Epoch 2 | Step 1310600 | Avg Loss: 0.0153 | Grad Norm: 0.00776054\n",
      "Epoch 2 | Step 1310700 | Avg Loss: 0.0148 | Grad Norm: 0.01031890\n",
      "Epoch 2 | Step 1310800 | Avg Loss: 0.0149 | Grad Norm: 0.01006728\n",
      "Epoch 2 | Step 1310900 | Avg Loss: 0.0150 | Grad Norm: 0.00858887\n",
      "Epoch 2 | Step 1311000 | Avg Loss: 0.0152 | Grad Norm: 0.00788209\n",
      "Epoch 2 | Step 1311100 | Avg Loss: 0.0147 | Grad Norm: 0.00806592\n",
      "Epoch 2 | Step 1311200 | Avg Loss: 0.0146 | Grad Norm: 0.00838718\n",
      "Epoch 2 | Step 1311300 | Avg Loss: 0.0149 | Grad Norm: 0.00644988\n",
      "Epoch 2 | Step 1311400 | Avg Loss: 0.0151 | Grad Norm: 0.00778556\n",
      "Epoch 2 | Step 1311500 | Avg Loss: 0.0150 | Grad Norm: 0.00834550\n",
      "Epoch 2 | Step 1311600 | Avg Loss: 0.0149 | Grad Norm: 0.00698843\n",
      "Epoch 2 | Step 1311700 | Avg Loss: 0.0146 | Grad Norm: 0.00642424\n",
      "Epoch 2 | Step 1311800 | Avg Loss: 0.0148 | Grad Norm: 0.00957585\n",
      "Epoch 2 | Step 1311900 | Avg Loss: 0.0148 | Grad Norm: 0.00928583\n",
      "Epoch 2 | Step 1312000 | Avg Loss: 0.0149 | Grad Norm: 0.00796380\n",
      "Epoch 2 | Step 1312100 | Avg Loss: 0.0149 | Grad Norm: 0.00773672\n",
      "Epoch 2 | Step 1312200 | Avg Loss: 0.0145 | Grad Norm: 0.00807339\n",
      "Epoch 2 | Step 1312300 | Avg Loss: 0.0146 | Grad Norm: 0.00795582\n",
      "Epoch 2 | Step 1312400 | Avg Loss: 0.0144 | Grad Norm: 0.00808522\n",
      "Epoch 2 | Step 1312500 | Avg Loss: 0.0146 | Grad Norm: 0.00835403\n",
      "Epoch 2 | Step 1312600 | Avg Loss: 0.0149 | Grad Norm: 0.00845186\n",
      "Epoch 2 | Step 1312700 | Avg Loss: 0.0145 | Grad Norm: 0.00791602\n",
      "Epoch 2 | Step 1312800 | Avg Loss: 0.0148 | Grad Norm: 0.00759373\n",
      "Epoch 2 | Step 1312900 | Avg Loss: 0.0146 | Grad Norm: 0.00862755\n",
      "Epoch 2 | Step 1313000 | Avg Loss: 0.0150 | Grad Norm: 0.00754662\n",
      "Epoch 2 | Step 1313100 | Avg Loss: 0.0149 | Grad Norm: 0.00874393\n",
      "Epoch 2 | Step 1313200 | Avg Loss: 0.0151 | Grad Norm: 0.00861468\n",
      "Epoch 2 | Step 1313300 | Avg Loss: 0.0153 | Grad Norm: 0.01065392\n",
      "Epoch 2 | Step 1313400 | Avg Loss: 0.0152 | Grad Norm: 0.01016049\n",
      "Epoch 2 | Step 1313500 | Avg Loss: 0.0152 | Grad Norm: 0.00784781\n",
      "Epoch 2 | Step 1313600 | Avg Loss: 0.0151 | Grad Norm: 0.00820438\n",
      "Epoch 2 | Step 1313700 | Avg Loss: 0.0151 | Grad Norm: 0.00838259\n",
      "Epoch 2 | Step 1313800 | Avg Loss: 0.0148 | Grad Norm: 0.00881964\n",
      "Epoch 2 | Step 1313900 | Avg Loss: 0.0147 | Grad Norm: 0.00831895\n",
      "Epoch 2 | Step 1314000 | Avg Loss: 0.0147 | Grad Norm: 0.00926572\n",
      "Epoch 2 | Step 1314100 | Avg Loss: 0.0142 | Grad Norm: 0.00871386\n",
      "Epoch 2 | Step 1314200 | Avg Loss: 0.0143 | Grad Norm: 0.00751010\n",
      "Epoch 2 | Step 1314300 | Avg Loss: 0.0142 | Grad Norm: 0.00701749\n",
      "Epoch 2 | Step 1314400 | Avg Loss: 0.0145 | Grad Norm: 0.00942217\n",
      "Epoch 2 | Step 1314500 | Avg Loss: 0.0147 | Grad Norm: 0.00836689\n",
      "Epoch 2 | Step 1314600 | Avg Loss: 0.0152 | Grad Norm: 0.00885431\n",
      "Epoch 2 | Step 1314700 | Avg Loss: 0.0151 | Grad Norm: 0.00728780\n",
      "Epoch 2 | Step 1314800 | Avg Loss: 0.0148 | Grad Norm: 0.00873744\n",
      "Epoch 2 | Step 1314900 | Avg Loss: 0.0147 | Grad Norm: 0.00831790\n",
      "Epoch 2 | Step 1315000 | Avg Loss: 0.0147 | Grad Norm: 0.00767247\n",
      "Epoch 2 | Step 1315100 | Avg Loss: 0.0150 | Grad Norm: 0.00881299\n",
      "Epoch 2 | Step 1315200 | Avg Loss: 0.0147 | Grad Norm: 0.00925264\n",
      "Epoch 2 | Step 1315300 | Avg Loss: 0.0147 | Grad Norm: 0.01188324\n",
      "Epoch 2 | Step 1315400 | Avg Loss: 0.0147 | Grad Norm: 0.00940715\n",
      "Epoch 2 | Step 1315500 | Avg Loss: 0.0148 | Grad Norm: 0.00781628\n",
      "Epoch 2 | Step 1315600 | Avg Loss: 0.0148 | Grad Norm: 0.00881854\n",
      "Epoch 2 | Step 1315700 | Avg Loss: 0.0147 | Grad Norm: 0.00824971\n",
      "Epoch 2 | Step 1315800 | Avg Loss: 0.0146 | Grad Norm: 0.00783932\n",
      "Epoch 2 | Step 1315900 | Avg Loss: 0.0146 | Grad Norm: 0.00813382\n",
      "Epoch 2 | Step 1316000 | Avg Loss: 0.0146 | Grad Norm: 0.00737706\n",
      "Epoch 2 | Step 1316100 | Avg Loss: 0.0145 | Grad Norm: 0.00803324\n",
      "Epoch 2 | Step 1316200 | Avg Loss: 0.0148 | Grad Norm: 0.00918539\n",
      "Epoch 2 | Step 1316300 | Avg Loss: 0.0148 | Grad Norm: 0.01107930\n",
      "Epoch 2 | Step 1316400 | Avg Loss: 0.0148 | Grad Norm: 0.01066389\n",
      "Epoch 2 | Step 1316500 | Avg Loss: 0.0148 | Grad Norm: 0.00959384\n",
      "Epoch 2 | Step 1316600 | Avg Loss: 0.0144 | Grad Norm: 0.00914811\n",
      "Epoch 2 | Step 1316700 | Avg Loss: 0.0144 | Grad Norm: 0.00794782\n",
      "Epoch 2 | Step 1316800 | Avg Loss: 0.0144 | Grad Norm: 0.00747599\n",
      "Epoch 2 | Step 1316900 | Avg Loss: 0.0142 | Grad Norm: 0.00764549\n",
      "Epoch 2 | Step 1317000 | Avg Loss: 0.0146 | Grad Norm: 0.00764637\n",
      "Epoch 2 | Step 1317100 | Avg Loss: 0.0148 | Grad Norm: 0.00759341\n",
      "Epoch 2 | Step 1317200 | Avg Loss: 0.0146 | Grad Norm: 0.00876911\n",
      "Epoch 2 | Step 1317300 | Avg Loss: 0.0146 | Grad Norm: 0.00785979\n",
      "Epoch 2 | Step 1317400 | Avg Loss: 0.0143 | Grad Norm: 0.01083033\n",
      "Epoch 2 | Step 1317500 | Avg Loss: 0.0148 | Grad Norm: 0.00853830\n",
      "Epoch 2 | Step 1317600 | Avg Loss: 0.0145 | Grad Norm: 0.00773624\n",
      "Epoch 2 | Step 1317700 | Avg Loss: 0.0150 | Grad Norm: 0.00846791\n",
      "Epoch 2 | Step 1317800 | Avg Loss: 0.0150 | Grad Norm: 0.01063063\n",
      "Epoch 2 | Step 1317900 | Avg Loss: 0.0146 | Grad Norm: 0.00793797\n",
      "Epoch 2 | Step 1318000 | Avg Loss: 0.0143 | Grad Norm: 0.00910612\n",
      "Epoch 2 | Step 1318100 | Avg Loss: 0.0143 | Grad Norm: 0.00808576\n",
      "Epoch 2 | Step 1318200 | Avg Loss: 0.0142 | Grad Norm: 0.00826637\n",
      "Epoch 2 | Step 1318300 | Avg Loss: 0.0144 | Grad Norm: 0.00766628\n",
      "Epoch 2 | Step 1318400 | Avg Loss: 0.0143 | Grad Norm: 0.00738533\n",
      "Epoch 2 | Step 1318500 | Avg Loss: 0.0143 | Grad Norm: 0.00882552\n",
      "Epoch 2 | Step 1318600 | Avg Loss: 0.0148 | Grad Norm: 0.00735250\n",
      "Epoch 2 | Step 1318700 | Avg Loss: 0.0147 | Grad Norm: 0.00734651\n",
      "Epoch 2 | Step 1318800 | Avg Loss: 0.0146 | Grad Norm: 0.00705810\n",
      "Epoch 2 | Step 1318900 | Avg Loss: 0.0144 | Grad Norm: 0.00790398\n",
      "Epoch 2 | Step 1319000 | Avg Loss: 0.0147 | Grad Norm: 0.00703969\n",
      "Epoch 2 | Step 1319100 | Avg Loss: 0.0149 | Grad Norm: 0.00938546\n",
      "Epoch 2 | Step 1319200 | Avg Loss: 0.0149 | Grad Norm: 0.00950183\n",
      "Epoch 2 | Step 1319300 | Avg Loss: 0.0152 | Grad Norm: 0.01085971\n",
      "Epoch 2 | Step 1319400 | Avg Loss: 0.0151 | Grad Norm: 0.00895169\n",
      "Epoch 2 | Step 1319500 | Avg Loss: 0.0150 | Grad Norm: 0.00782161\n",
      "Epoch 2 | Step 1319600 | Avg Loss: 0.0151 | Grad Norm: 0.00791876\n",
      "Epoch 2 | Step 1319700 | Avg Loss: 0.0154 | Grad Norm: 0.00785634\n",
      "Epoch 2 | Step 1319800 | Avg Loss: 0.0154 | Grad Norm: 0.00804360\n",
      "Epoch 2 | Step 1319900 | Avg Loss: 0.0153 | Grad Norm: 0.00854518\n",
      "Epoch 2 | Step 1320000 | Avg Loss: 0.0157 | Grad Norm: 0.00896809\n",
      "Epoch 2 | Step 1320100 | Avg Loss: 0.0154 | Grad Norm: 0.00726053\n",
      "Epoch 2 | Step 1320200 | Avg Loss: 0.0156 | Grad Norm: 0.00776809\n",
      "Epoch 2 | Step 1320300 | Avg Loss: 0.0155 | Grad Norm: 0.00846581\n",
      "Epoch 2 | Step 1320400 | Avg Loss: 0.0156 | Grad Norm: 0.00761568\n",
      "Epoch 2 | Step 1320500 | Avg Loss: 0.0154 | Grad Norm: 0.00919625\n",
      "Epoch 2 | Step 1320600 | Avg Loss: 0.0154 | Grad Norm: 0.00864647\n",
      "Epoch 2 | Step 1320700 | Avg Loss: 0.0154 | Grad Norm: 0.00817266\n",
      "Epoch 2 | Step 1320800 | Avg Loss: 0.0149 | Grad Norm: 0.00737373\n",
      "Epoch 2 | Step 1320900 | Avg Loss: 0.0151 | Grad Norm: 0.00872504\n",
      "Epoch 2 | Step 1321000 | Avg Loss: 0.0150 | Grad Norm: 0.00894496\n",
      "Epoch 2 | Step 1321100 | Avg Loss: 0.0145 | Grad Norm: 0.00744528\n",
      "Epoch 2 | Step 1321200 | Avg Loss: 0.0149 | Grad Norm: 0.00883976\n",
      "Epoch 2 | Step 1321300 | Avg Loss: 0.0151 | Grad Norm: 0.00803938\n",
      "Epoch 2 | Step 1321400 | Avg Loss: 0.0153 | Grad Norm: 0.00928417\n",
      "Epoch 2 | Step 1321500 | Avg Loss: 0.0156 | Grad Norm: 0.00858845\n",
      "Epoch 2 | Step 1321600 | Avg Loss: 0.0157 | Grad Norm: 0.00781822\n",
      "Epoch 2 | Step 1321700 | Avg Loss: 0.0158 | Grad Norm: 0.00882649\n",
      "Epoch 2 | Step 1321800 | Avg Loss: 0.0154 | Grad Norm: 0.00840279\n",
      "Epoch 2 | Step 1321900 | Avg Loss: 0.0154 | Grad Norm: 0.00843936\n",
      "Epoch 2 | Step 1322000 | Avg Loss: 0.0161 | Grad Norm: 0.00882772\n",
      "Epoch 2 | Step 1322100 | Avg Loss: 0.0160 | Grad Norm: 0.00989171\n",
      "Epoch 2 | Step 1322200 | Avg Loss: 0.0156 | Grad Norm: 0.00821437\n",
      "Epoch 2 | Step 1322300 | Avg Loss: 0.0155 | Grad Norm: 0.01040692\n",
      "Epoch 2 | Step 1322400 | Avg Loss: 0.0152 | Grad Norm: 0.00845433\n",
      "Epoch 2 | Step 1322500 | Avg Loss: 0.0158 | Grad Norm: 0.00787062\n",
      "Epoch 2 | Step 1322600 | Avg Loss: 0.0157 | Grad Norm: 0.00802058\n",
      "Epoch 2 | Step 1322700 | Avg Loss: 0.0152 | Grad Norm: 0.00779523\n",
      "Epoch 2 | Step 1322800 | Avg Loss: 0.0151 | Grad Norm: 0.00839598\n",
      "Epoch 2 | Step 1322900 | Avg Loss: 0.0153 | Grad Norm: 0.00911324\n",
      "Epoch 2 | Step 1323000 | Avg Loss: 0.0149 | Grad Norm: 0.00807246\n",
      "Epoch 2 | Step 1323100 | Avg Loss: 0.0152 | Grad Norm: 0.00861533\n",
      "Epoch 2 | Step 1323200 | Avg Loss: 0.0148 | Grad Norm: 0.00868197\n",
      "Epoch 2 | Step 1323300 | Avg Loss: 0.0150 | Grad Norm: 0.00885535\n",
      "Epoch 2 | Step 1323400 | Avg Loss: 0.0149 | Grad Norm: 0.00722159\n",
      "Epoch 2 | Step 1323500 | Avg Loss: 0.0152 | Grad Norm: 0.00776537\n",
      "Epoch 2 | Step 1323600 | Avg Loss: 0.0152 | Grad Norm: 0.00889004\n",
      "Epoch 2 | Step 1323700 | Avg Loss: 0.0151 | Grad Norm: 0.00849994\n",
      "Epoch 2 | Step 1323800 | Avg Loss: 0.0150 | Grad Norm: 0.00941881\n",
      "Epoch 2 | Step 1323900 | Avg Loss: 0.0154 | Grad Norm: 0.00848942\n",
      "Epoch 2 | Step 1324000 | Avg Loss: 0.0150 | Grad Norm: 0.00817041\n",
      "Epoch 2 | Step 1324100 | Avg Loss: 0.0152 | Grad Norm: 0.00931026\n",
      "Epoch 2 | Step 1324200 | Avg Loss: 0.0153 | Grad Norm: 0.00817609\n",
      "Epoch 2 | Step 1324300 | Avg Loss: 0.0152 | Grad Norm: 0.00829440\n",
      "Epoch 2 | Step 1324400 | Avg Loss: 0.0152 | Grad Norm: 0.00745659\n",
      "Epoch 2 | Step 1324500 | Avg Loss: 0.0155 | Grad Norm: 0.00887156\n",
      "Epoch 2 | Step 1324600 | Avg Loss: 0.0155 | Grad Norm: 0.01138888\n",
      "Epoch 2 | Step 1324700 | Avg Loss: 0.0151 | Grad Norm: 0.00799617\n",
      "Epoch 2 | Step 1324800 | Avg Loss: 0.0152 | Grad Norm: 0.00698829\n",
      "Epoch 2 | Step 1324900 | Avg Loss: 0.0149 | Grad Norm: 0.00866243\n",
      "Epoch 2 | Step 1325000 | Avg Loss: 0.0147 | Grad Norm: 0.00744068\n",
      "Epoch 2 | Step 1325100 | Avg Loss: 0.0149 | Grad Norm: 0.00926956\n",
      "Epoch 2 | Step 1325200 | Avg Loss: 0.0149 | Grad Norm: 0.00954373\n",
      "Epoch 2 | Step 1325300 | Avg Loss: 0.0151 | Grad Norm: 0.00735431\n",
      "Epoch 2 | Step 1325400 | Avg Loss: 0.0149 | Grad Norm: 0.00717673\n",
      "Epoch 2 | Step 1325500 | Avg Loss: 0.0151 | Grad Norm: 0.00850138\n",
      "Epoch 2 | Step 1325600 | Avg Loss: 0.0154 | Grad Norm: 0.00940473\n",
      "Epoch 2 | Step 1325700 | Avg Loss: 0.0151 | Grad Norm: 0.00794174\n",
      "Epoch 2 | Step 1325800 | Avg Loss: 0.0152 | Grad Norm: 0.00778431\n",
      "Epoch 2 | Step 1325900 | Avg Loss: 0.0149 | Grad Norm: 0.00786258\n",
      "Epoch 2 | Step 1326000 | Avg Loss: 0.0152 | Grad Norm: 0.00687619\n",
      "Epoch 2 | Step 1326100 | Avg Loss: 0.0149 | Grad Norm: 0.00837521\n",
      "Epoch 2 | Step 1326200 | Avg Loss: 0.0150 | Grad Norm: 0.00865624\n",
      "Epoch 2 | Step 1326300 | Avg Loss: 0.0150 | Grad Norm: 0.00846987\n",
      "Epoch 2 | Step 1326400 | Avg Loss: 0.0153 | Grad Norm: 0.00876821\n",
      "Epoch 2 | Step 1326500 | Avg Loss: 0.0153 | Grad Norm: 0.00835869\n",
      "Epoch 2 | Step 1326600 | Avg Loss: 0.0152 | Grad Norm: 0.00840861\n",
      "Epoch 2 | Step 1326700 | Avg Loss: 0.0152 | Grad Norm: 0.01097080\n",
      "Epoch 2 | Step 1326800 | Avg Loss: 0.0148 | Grad Norm: 0.00922232\n",
      "Epoch 2 | Step 1326900 | Avg Loss: 0.0145 | Grad Norm: 0.00891245\n",
      "Epoch 2 | Step 1327000 | Avg Loss: 0.0143 | Grad Norm: 0.00729764\n",
      "Epoch 2 | Step 1327100 | Avg Loss: 0.0145 | Grad Norm: 0.00837453\n",
      "Epoch 2 | Step 1327200 | Avg Loss: 0.0146 | Grad Norm: 0.00903903\n",
      "Epoch 2 | Step 1327300 | Avg Loss: 0.0150 | Grad Norm: 0.00909612\n",
      "Epoch 2 | Step 1327400 | Avg Loss: 0.0149 | Grad Norm: 0.00872127\n",
      "Epoch 2 | Step 1327500 | Avg Loss: 0.0151 | Grad Norm: 0.00799038\n",
      "Epoch 2 | Step 1327600 | Avg Loss: 0.0147 | Grad Norm: 0.00926572\n",
      "Epoch 2 | Step 1327700 | Avg Loss: 0.0145 | Grad Norm: 0.00770463\n",
      "Epoch 2 | Step 1327800 | Avg Loss: 0.0142 | Grad Norm: 0.00686106\n",
      "Epoch 2 | Step 1327900 | Avg Loss: 0.0146 | Grad Norm: 0.00807638\n",
      "Epoch 2 | Step 1328000 | Avg Loss: 0.0149 | Grad Norm: 0.00798489\n",
      "Epoch 2 | Step 1328100 | Avg Loss: 0.0146 | Grad Norm: 0.00857474\n",
      "Epoch 2 | Step 1328200 | Avg Loss: 0.0146 | Grad Norm: 0.00977295\n",
      "Epoch 2 | Step 1328300 | Avg Loss: 0.0147 | Grad Norm: 0.00778716\n",
      "Epoch 2 | Step 1328400 | Avg Loss: 0.0147 | Grad Norm: 0.00971057\n",
      "Epoch 2 | Step 1328500 | Avg Loss: 0.0151 | Grad Norm: 0.00780513\n",
      "Epoch 2 | Step 1328600 | Avg Loss: 0.0148 | Grad Norm: 0.00876929\n",
      "Epoch 2 | Step 1328700 | Avg Loss: 0.0147 | Grad Norm: 0.00843695\n",
      "Epoch 2 | Step 1328800 | Avg Loss: 0.0148 | Grad Norm: 0.01025312\n",
      "Epoch 2 | Step 1328900 | Avg Loss: 0.0146 | Grad Norm: 0.00974122\n",
      "Epoch 2 | Step 1329000 | Avg Loss: 0.0144 | Grad Norm: 0.00706386\n",
      "Epoch 2 | Step 1329100 | Avg Loss: 0.0148 | Grad Norm: 0.00943305\n",
      "Epoch 2 | Step 1329200 | Avg Loss: 0.0154 | Grad Norm: 0.01216359\n",
      "Epoch 2 | Step 1329300 | Avg Loss: 0.0150 | Grad Norm: 0.00732993\n",
      "Epoch 2 | Step 1329400 | Avg Loss: 0.0149 | Grad Norm: 0.00868591\n",
      "Epoch 2 | Step 1329500 | Avg Loss: 0.0149 | Grad Norm: 0.00788306\n",
      "Epoch 2 | Step 1329600 | Avg Loss: 0.0149 | Grad Norm: 0.00839607\n",
      "Epoch 2 | Step 1329700 | Avg Loss: 0.0149 | Grad Norm: 0.00834989\n",
      "Epoch 2 | Step 1329800 | Avg Loss: 0.0148 | Grad Norm: 0.00922744\n",
      "Epoch 2 | Step 1329900 | Avg Loss: 0.0147 | Grad Norm: 0.00885391\n",
      "Epoch 2 | Step 1330000 | Avg Loss: 0.0153 | Grad Norm: 0.00854800\n",
      "Epoch 2 | Step 1330100 | Avg Loss: 0.0154 | Grad Norm: 0.00895753\n",
      "Epoch 2 | Step 1330200 | Avg Loss: 0.0152 | Grad Norm: 0.00904192\n",
      "Epoch 2 | Step 1330300 | Avg Loss: 0.0156 | Grad Norm: 0.00885695\n",
      "Epoch 2 | Step 1330400 | Avg Loss: 0.0149 | Grad Norm: 0.00857520\n",
      "Epoch 2 | Step 1330500 | Avg Loss: 0.0152 | Grad Norm: 0.00978710\n",
      "Epoch 2 | Step 1330600 | Avg Loss: 0.0151 | Grad Norm: 0.00914006\n",
      "Epoch 2 | Step 1330700 | Avg Loss: 0.0152 | Grad Norm: 0.00997838\n",
      "Epoch 2 | Step 1330800 | Avg Loss: 0.0149 | Grad Norm: 0.00805289\n",
      "Epoch 2 | Step 1330900 | Avg Loss: 0.0149 | Grad Norm: 0.00886733\n",
      "Epoch 2 | Step 1331000 | Avg Loss: 0.0149 | Grad Norm: 0.01156550\n",
      "Epoch 2 | Step 1331100 | Avg Loss: 0.0148 | Grad Norm: 0.00795517\n",
      "Epoch 2 | Step 1331200 | Avg Loss: 0.0149 | Grad Norm: 0.00817785\n",
      "Epoch 2 | Step 1331300 | Avg Loss: 0.0147 | Grad Norm: 0.00818918\n",
      "Epoch 2 | Step 1331400 | Avg Loss: 0.0148 | Grad Norm: 0.00793033\n",
      "Epoch 2 | Step 1331500 | Avg Loss: 0.0147 | Grad Norm: 0.00947615\n",
      "Epoch 2 | Step 1331600 | Avg Loss: 0.0146 | Grad Norm: 0.00813714\n",
      "Epoch 2 | Step 1331700 | Avg Loss: 0.0148 | Grad Norm: 0.00932363\n",
      "Epoch 2 | Step 1331800 | Avg Loss: 0.0150 | Grad Norm: 0.00795498\n",
      "Epoch 2 | Step 1331900 | Avg Loss: 0.0152 | Grad Norm: 0.00912880\n",
      "Epoch 2 | Step 1332000 | Avg Loss: 0.0151 | Grad Norm: 0.01018986\n",
      "Epoch 2 | Step 1332100 | Avg Loss: 0.0152 | Grad Norm: 0.00837947\n",
      "Epoch 2 | Step 1332200 | Avg Loss: 0.0149 | Grad Norm: 0.00813371\n",
      "Epoch 2 | Step 1332300 | Avg Loss: 0.0149 | Grad Norm: 0.00862944\n",
      "Epoch 2 | Step 1332400 | Avg Loss: 0.0146 | Grad Norm: 0.00781754\n",
      "Epoch 2 | Step 1332500 | Avg Loss: 0.0146 | Grad Norm: 0.00862790\n",
      "Epoch 2 | Step 1332600 | Avg Loss: 0.0148 | Grad Norm: 0.00851526\n",
      "Epoch 2 | Step 1332700 | Avg Loss: 0.0148 | Grad Norm: 0.00819443\n",
      "Epoch 2 | Step 1332800 | Avg Loss: 0.0146 | Grad Norm: 0.00913672\n",
      "Epoch 2 | Step 1332900 | Avg Loss: 0.0154 | Grad Norm: 0.00772893\n",
      "Epoch 2 | Step 1333000 | Avg Loss: 0.0150 | Grad Norm: 0.00906804\n",
      "Epoch 2 | Step 1333100 | Avg Loss: 0.0151 | Grad Norm: 0.00941624\n",
      "Epoch 2 | Step 1333200 | Avg Loss: 0.0148 | Grad Norm: 0.00831762\n",
      "Epoch 2 | Step 1333300 | Avg Loss: 0.0149 | Grad Norm: 0.00793240\n",
      "Epoch 2 | Step 1333400 | Avg Loss: 0.0149 | Grad Norm: 0.00951610\n",
      "Epoch 2 | Step 1333500 | Avg Loss: 0.0145 | Grad Norm: 0.00953492\n",
      "Epoch 2 | Step 1333600 | Avg Loss: 0.0148 | Grad Norm: 0.00787035\n",
      "Epoch 2 | Step 1333700 | Avg Loss: 0.0145 | Grad Norm: 0.00912489\n",
      "Epoch 2 | Step 1333800 | Avg Loss: 0.0147 | Grad Norm: 0.00771891\n",
      "Epoch 2 | Step 1333900 | Avg Loss: 0.0152 | Grad Norm: 0.01000884\n",
      "Epoch 2 | Step 1334000 | Avg Loss: 0.0156 | Grad Norm: 0.00818739\n",
      "Epoch 2 | Step 1334100 | Avg Loss: 0.0154 | Grad Norm: 0.01005888\n",
      "Epoch 2 | Step 1334200 | Avg Loss: 0.0150 | Grad Norm: 0.00961723\n",
      "Epoch 2 | Step 1334300 | Avg Loss: 0.0148 | Grad Norm: 0.00976901\n",
      "Epoch 2 | Step 1334400 | Avg Loss: 0.0147 | Grad Norm: 0.00823223\n",
      "Epoch 2 | Step 1334500 | Avg Loss: 0.0150 | Grad Norm: 0.00920614\n",
      "Epoch 2 | Step 1334600 | Avg Loss: 0.0154 | Grad Norm: 0.00740851\n",
      "Epoch 2 | Step 1334700 | Avg Loss: 0.0156 | Grad Norm: 0.01027981\n",
      "Epoch 2 | Step 1334800 | Avg Loss: 0.0153 | Grad Norm: 0.00863471\n",
      "Epoch 2 | Step 1334900 | Avg Loss: 0.0150 | Grad Norm: 0.00884464\n",
      "Epoch 2 | Step 1335000 | Avg Loss: 0.0154 | Grad Norm: 0.00841910\n",
      "Epoch 2 | Step 1335100 | Avg Loss: 0.0149 | Grad Norm: 0.00795596\n",
      "Epoch 2 | Step 1335200 | Avg Loss: 0.0147 | Grad Norm: 0.00804730\n",
      "Epoch 2 | Step 1335300 | Avg Loss: 0.0148 | Grad Norm: 0.00880430\n",
      "Epoch 2 | Step 1335400 | Avg Loss: 0.0149 | Grad Norm: 0.00956998\n",
      "Epoch 2 | Step 1335500 | Avg Loss: 0.0149 | Grad Norm: 0.00904908\n",
      "Epoch 2 | Step 1335600 | Avg Loss: 0.0148 | Grad Norm: 0.00883902\n",
      "Epoch 2 | Step 1335700 | Avg Loss: 0.0151 | Grad Norm: 0.00848595\n",
      "Epoch 2 | Step 1335800 | Avg Loss: 0.0154 | Grad Norm: 0.00831430\n",
      "Epoch 2 | Step 1335900 | Avg Loss: 0.0150 | Grad Norm: 0.00727252\n",
      "Epoch 2 | Step 1336000 | Avg Loss: 0.0149 | Grad Norm: 0.00870970\n",
      "Epoch 2 | Step 1336100 | Avg Loss: 0.0145 | Grad Norm: 0.00832639\n",
      "Epoch 2 | Step 1336200 | Avg Loss: 0.0150 | Grad Norm: 0.00813956\n",
      "Epoch 2 | Step 1336300 | Avg Loss: 0.0152 | Grad Norm: 0.00935855\n",
      "Epoch 2 | Step 1336400 | Avg Loss: 0.0149 | Grad Norm: 0.00727724\n",
      "Epoch 2 | Step 1336500 | Avg Loss: 0.0151 | Grad Norm: 0.00876025\n",
      "Epoch 2 | Step 1336600 | Avg Loss: 0.0149 | Grad Norm: 0.00719131\n",
      "Epoch 2 | Step 1336700 | Avg Loss: 0.0150 | Grad Norm: 0.00747056\n",
      "Epoch 2 | Step 1336800 | Avg Loss: 0.0154 | Grad Norm: 0.00953697\n",
      "Epoch 2 | Step 1336900 | Avg Loss: 0.0151 | Grad Norm: 0.00820856\n",
      "Epoch 2 | Step 1337000 | Avg Loss: 0.0151 | Grad Norm: 0.00938900\n",
      "Epoch 2 | Step 1337100 | Avg Loss: 0.0151 | Grad Norm: 0.00992925\n",
      "Epoch 2 | Step 1337200 | Avg Loss: 0.0157 | Grad Norm: 0.00806644\n",
      "Epoch 2 | Step 1337300 | Avg Loss: 0.0155 | Grad Norm: 0.00817795\n",
      "Epoch 2 | Step 1337400 | Avg Loss: 0.0156 | Grad Norm: 0.01011391\n",
      "Epoch 2 | Step 1337500 | Avg Loss: 0.0152 | Grad Norm: 0.00800897\n",
      "Epoch 2 | Step 1337600 | Avg Loss: 0.0150 | Grad Norm: 0.00859555\n",
      "Epoch 2 | Step 1337700 | Avg Loss: 0.0153 | Grad Norm: 0.00745515\n",
      "Epoch 2 | Step 1337800 | Avg Loss: 0.0153 | Grad Norm: 0.00984083\n",
      "Epoch 2 | Step 1337900 | Avg Loss: 0.0151 | Grad Norm: 0.00794990\n",
      "Epoch 2 | Step 1338000 | Avg Loss: 0.0152 | Grad Norm: 0.00901747\n",
      "Epoch 2 | Step 1338100 | Avg Loss: 0.0153 | Grad Norm: 0.00844696\n",
      "Epoch 2 | Step 1338200 | Avg Loss: 0.0153 | Grad Norm: 0.00834492\n",
      "Epoch 2 | Step 1338300 | Avg Loss: 0.0152 | Grad Norm: 0.00909734\n",
      "Epoch 2 | Step 1338400 | Avg Loss: 0.0153 | Grad Norm: 0.00810996\n",
      "Epoch 2 | Step 1338500 | Avg Loss: 0.0152 | Grad Norm: 0.00878583\n",
      "Epoch 2 | Step 1338600 | Avg Loss: 0.0156 | Grad Norm: 0.00757278\n",
      "Epoch 2 | Step 1338700 | Avg Loss: 0.0157 | Grad Norm: 0.00979007\n",
      "Epoch 2 | Step 1338800 | Avg Loss: 0.0158 | Grad Norm: 0.00934822\n",
      "Epoch 2 | Step 1338900 | Avg Loss: 0.0159 | Grad Norm: 0.00873991\n",
      "Epoch 2 | Step 1339000 | Avg Loss: 0.0151 | Grad Norm: 0.00800715\n",
      "Epoch 2 | Step 1339100 | Avg Loss: 0.0153 | Grad Norm: 0.00883780\n",
      "Epoch 2 | Step 1339200 | Avg Loss: 0.0151 | Grad Norm: 0.00826554\n",
      "Epoch 2 | Step 1339300 | Avg Loss: 0.0150 | Grad Norm: 0.00669064\n",
      "Epoch 2 | Step 1339400 | Avg Loss: 0.0150 | Grad Norm: 0.00799562\n",
      "Epoch 2 | Step 1339500 | Avg Loss: 0.0151 | Grad Norm: 0.01025297\n",
      "Epoch 2 | Step 1339600 | Avg Loss: 0.0154 | Grad Norm: 0.01023960\n",
      "Epoch 2 | Step 1339700 | Avg Loss: 0.0152 | Grad Norm: 0.00819268\n",
      "Epoch 2 | Step 1339800 | Avg Loss: 0.0150 | Grad Norm: 0.01004171\n",
      "Epoch 2 | Step 1339900 | Avg Loss: 0.0151 | Grad Norm: 0.00813406\n",
      "Epoch 2 | Step 1340000 | Avg Loss: 0.0152 | Grad Norm: 0.00895387\n",
      "Epoch 2 | Step 1340100 | Avg Loss: 0.0150 | Grad Norm: 0.00796644\n",
      "Epoch 2 | Step 1340200 | Avg Loss: 0.0152 | Grad Norm: 0.00902222\n",
      "Epoch 2 | Step 1340300 | Avg Loss: 0.0153 | Grad Norm: 0.00888287\n",
      "Epoch 2 | Step 1340400 | Avg Loss: 0.0153 | Grad Norm: 0.00839460\n",
      "Epoch 2 | Step 1340500 | Avg Loss: 0.0151 | Grad Norm: 0.00911324\n",
      "Epoch 2 | Step 1340600 | Avg Loss: 0.0153 | Grad Norm: 0.00802655\n",
      "Epoch 2 | Step 1340700 | Avg Loss: 0.0152 | Grad Norm: 0.00928487\n",
      "Epoch 2 | Step 1340800 | Avg Loss: 0.0153 | Grad Norm: 0.00900289\n",
      "Epoch 2 | Step 1340900 | Avg Loss: 0.0153 | Grad Norm: 0.00885577\n",
      "Epoch 2 | Step 1341000 | Avg Loss: 0.0152 | Grad Norm: 0.00889446\n",
      "Epoch 2 | Step 1341100 | Avg Loss: 0.0153 | Grad Norm: 0.00799268\n",
      "Epoch 2 | Step 1341200 | Avg Loss: 0.0150 | Grad Norm: 0.00831420\n",
      "Epoch 2 | Step 1341300 | Avg Loss: 0.0151 | Grad Norm: 0.00854181\n",
      "Epoch 2 | Step 1341400 | Avg Loss: 0.0151 | Grad Norm: 0.00763585\n",
      "Epoch 2 | Step 1341500 | Avg Loss: 0.0151 | Grad Norm: 0.00782834\n",
      "Epoch 2 | Step 1341600 | Avg Loss: 0.0149 | Grad Norm: 0.00777844\n",
      "Epoch 2 | Step 1341700 | Avg Loss: 0.0150 | Grad Norm: 0.00896680\n",
      "Epoch 2 | Step 1341800 | Avg Loss: 0.0148 | Grad Norm: 0.00809920\n",
      "Epoch 2 | Step 1341900 | Avg Loss: 0.0148 | Grad Norm: 0.00772001\n",
      "Epoch 2 | Step 1342000 | Avg Loss: 0.0146 | Grad Norm: 0.00713603\n",
      "Epoch 2 | Step 1342100 | Avg Loss: 0.0144 | Grad Norm: 0.00815620\n",
      "Epoch 2 | Step 1342200 | Avg Loss: 0.0145 | Grad Norm: 0.00648990\n",
      "Epoch 2 | Step 1342300 | Avg Loss: 0.0148 | Grad Norm: 0.00799023\n",
      "Epoch 2 | Step 1342400 | Avg Loss: 0.0153 | Grad Norm: 0.00912956\n",
      "Epoch 2 | Step 1342500 | Avg Loss: 0.0150 | Grad Norm: 0.00985127\n",
      "Epoch 2 | Step 1342600 | Avg Loss: 0.0150 | Grad Norm: 0.00801548\n",
      "Epoch 2 | Step 1342700 | Avg Loss: 0.0149 | Grad Norm: 0.00763813\n",
      "Epoch 2 | Step 1342800 | Avg Loss: 0.0150 | Grad Norm: 0.00990740\n",
      "Epoch 2 | Step 1342900 | Avg Loss: 0.0153 | Grad Norm: 0.00751544\n",
      "Epoch 2 | Step 1343000 | Avg Loss: 0.0150 | Grad Norm: 0.00744840\n",
      "Epoch 2 | Step 1343100 | Avg Loss: 0.0150 | Grad Norm: 0.00799245\n",
      "Epoch 2 | Step 1343200 | Avg Loss: 0.0151 | Grad Norm: 0.00686819\n",
      "Epoch 2 | Step 1343300 | Avg Loss: 0.0151 | Grad Norm: 0.00768437\n",
      "Epoch 2 | Step 1343400 | Avg Loss: 0.0152 | Grad Norm: 0.00715718\n",
      "Epoch 2 | Step 1343500 | Avg Loss: 0.0152 | Grad Norm: 0.00767493\n",
      "Epoch 2 | Step 1343600 | Avg Loss: 0.0151 | Grad Norm: 0.00831344\n",
      "Epoch 2 | Step 1343700 | Avg Loss: 0.0151 | Grad Norm: 0.00888055\n",
      "Epoch 2 | Step 1343800 | Avg Loss: 0.0150 | Grad Norm: 0.00755162\n",
      "Epoch 2 | Step 1343900 | Avg Loss: 0.0151 | Grad Norm: 0.00992336\n",
      "Epoch 2 | Step 1344000 | Avg Loss: 0.0154 | Grad Norm: 0.00857764\n",
      "Epoch 2 | Step 1344100 | Avg Loss: 0.0156 | Grad Norm: 0.00802764\n",
      "Epoch 2 | Step 1344200 | Avg Loss: 0.0154 | Grad Norm: 0.00962609\n",
      "Epoch 2 | Step 1344300 | Avg Loss: 0.0154 | Grad Norm: 0.00875824\n",
      "Epoch 2 | Step 1344400 | Avg Loss: 0.0153 | Grad Norm: 0.00813423\n",
      "Epoch 2 | Step 1344500 | Avg Loss: 0.0151 | Grad Norm: 0.00938756\n",
      "Epoch 2 | Step 1344600 | Avg Loss: 0.0146 | Grad Norm: 0.00771308\n",
      "Epoch 2 | Step 1344700 | Avg Loss: 0.0146 | Grad Norm: 0.00877557\n",
      "Epoch 2 | Step 1344800 | Avg Loss: 0.0142 | Grad Norm: 0.00930184\n",
      "Epoch 2 | Step 1344900 | Avg Loss: 0.0148 | Grad Norm: 0.00863531\n",
      "Epoch 2 | Step 1345000 | Avg Loss: 0.0146 | Grad Norm: 0.00876532\n",
      "Epoch 2 | Step 1345100 | Avg Loss: 0.0149 | Grad Norm: 0.00694809\n",
      "Epoch 2 | Step 1345200 | Avg Loss: 0.0146 | Grad Norm: 0.00741413\n",
      "Epoch 2 | Step 1345300 | Avg Loss: 0.0146 | Grad Norm: 0.00755411\n",
      "Epoch 2 | Step 1345400 | Avg Loss: 0.0149 | Grad Norm: 0.00798808\n",
      "Epoch 2 | Step 1345500 | Avg Loss: 0.0147 | Grad Norm: 0.00770546\n",
      "Epoch 2 | Step 1345600 | Avg Loss: 0.0147 | Grad Norm: 0.00997989\n",
      "Epoch 2 | Step 1345700 | Avg Loss: 0.0148 | Grad Norm: 0.00914286\n",
      "Epoch 2 | Step 1345800 | Avg Loss: 0.0146 | Grad Norm: 0.00743258\n",
      "Epoch 2 | Step 1345900 | Avg Loss: 0.0148 | Grad Norm: 0.00984784\n",
      "Epoch 2 | Step 1346000 | Avg Loss: 0.0148 | Grad Norm: 0.00747828\n",
      "Epoch 2 | Step 1346100 | Avg Loss: 0.0146 | Grad Norm: 0.00816764\n",
      "Epoch 2 | Step 1346200 | Avg Loss: 0.0145 | Grad Norm: 0.00916379\n",
      "Epoch 2 | Step 1346300 | Avg Loss: 0.0151 | Grad Norm: 0.00911107\n",
      "Epoch 2 | Step 1346400 | Avg Loss: 0.0155 | Grad Norm: 0.01007326\n",
      "Epoch 2 | Step 1346500 | Avg Loss: 0.0158 | Grad Norm: 0.00907523\n",
      "Epoch 2 | Step 1346600 | Avg Loss: 0.0150 | Grad Norm: 0.00807643\n",
      "Epoch 2 | Step 1346700 | Avg Loss: 0.0149 | Grad Norm: 0.01226193\n",
      "Epoch 2 | Step 1346800 | Avg Loss: 0.0148 | Grad Norm: 0.00882950\n",
      "Epoch 2 | Step 1346900 | Avg Loss: 0.0150 | Grad Norm: 0.00861411\n",
      "Epoch 2 | Step 1347000 | Avg Loss: 0.0150 | Grad Norm: 0.00830400\n",
      "Epoch 2 | Step 1347100 | Avg Loss: 0.0148 | Grad Norm: 0.00979648\n",
      "Epoch 2 | Step 1347200 | Avg Loss: 0.0148 | Grad Norm: 0.00811943\n",
      "Epoch 2 | Step 1347300 | Avg Loss: 0.0147 | Grad Norm: 0.00679346\n",
      "Epoch 2 | Step 1347400 | Avg Loss: 0.0149 | Grad Norm: 0.00755312\n",
      "Epoch 2 | Step 1347500 | Avg Loss: 0.0151 | Grad Norm: 0.00851252\n",
      "Epoch 2 | Step 1347600 | Avg Loss: 0.0150 | Grad Norm: 0.00849604\n",
      "Epoch 2 | Step 1347700 | Avg Loss: 0.0150 | Grad Norm: 0.01105499\n",
      "Epoch 2 | Step 1347800 | Avg Loss: 0.0153 | Grad Norm: 0.00984998\n",
      "Epoch 2 | Step 1347900 | Avg Loss: 0.0150 | Grad Norm: 0.00798754\n",
      "Epoch 2 | Step 1348000 | Avg Loss: 0.0148 | Grad Norm: 0.00758828\n",
      "Epoch 2 | Step 1348100 | Avg Loss: 0.0148 | Grad Norm: 0.00887152\n",
      "Epoch 2 | Step 1348200 | Avg Loss: 0.0145 | Grad Norm: 0.00707399\n",
      "Epoch 2 | Step 1348300 | Avg Loss: 0.0146 | Grad Norm: 0.00735956\n",
      "Epoch 2 | Step 1348400 | Avg Loss: 0.0146 | Grad Norm: 0.00913657\n",
      "Epoch 2 | Step 1348500 | Avg Loss: 0.0151 | Grad Norm: 0.00883922\n",
      "Epoch 2 | Step 1348600 | Avg Loss: 0.0150 | Grad Norm: 0.00869717\n",
      "Epoch 2 | Step 1348700 | Avg Loss: 0.0151 | Grad Norm: 0.00854583\n",
      "Epoch 2 | Step 1348800 | Avg Loss: 0.0154 | Grad Norm: 0.00919939\n",
      "Epoch 2 | Step 1348900 | Avg Loss: 0.0154 | Grad Norm: 0.00811247\n",
      "Epoch 2 | Step 1349000 | Avg Loss: 0.0149 | Grad Norm: 0.00801040\n",
      "Epoch 2 | Step 1349100 | Avg Loss: 0.0150 | Grad Norm: 0.00792495\n",
      "Epoch 2 | Step 1349200 | Avg Loss: 0.0151 | Grad Norm: 0.00833214\n",
      "Epoch 2 | Step 1349300 | Avg Loss: 0.0150 | Grad Norm: 0.00812067\n",
      "Epoch 2 | Step 1349400 | Avg Loss: 0.0151 | Grad Norm: 0.00879819\n",
      "Epoch 2 | Step 1349500 | Avg Loss: 0.0152 | Grad Norm: 0.01019728\n",
      "Epoch 2 | Step 1349600 | Avg Loss: 0.0153 | Grad Norm: 0.00789359\n",
      "Epoch 2 | Step 1349700 | Avg Loss: 0.0155 | Grad Norm: 0.00835528\n",
      "Epoch 2 | Step 1349800 | Avg Loss: 0.0156 | Grad Norm: 0.00798019\n",
      "Epoch 2 | Step 1349900 | Avg Loss: 0.0154 | Grad Norm: 0.00823378\n",
      "Epoch 2 | Step 1350000 | Avg Loss: 0.0147 | Grad Norm: 0.00743438\n",
      "Epoch 2 | Step 1350100 | Avg Loss: 0.0148 | Grad Norm: 0.00901913\n",
      "Epoch 2 | Step 1350200 | Avg Loss: 0.0149 | Grad Norm: 0.00859097\n",
      "Epoch 2 | Step 1350300 | Avg Loss: 0.0148 | Grad Norm: 0.00751022\n",
      "Epoch 2 | Step 1350400 | Avg Loss: 0.0148 | Grad Norm: 0.00743321\n",
      "Epoch 2 | Step 1350500 | Avg Loss: 0.0146 | Grad Norm: 0.00870107\n",
      "Epoch 2 | Step 1350600 | Avg Loss: 0.0152 | Grad Norm: 0.00838201\n",
      "Epoch 2 | Step 1350700 | Avg Loss: 0.0147 | Grad Norm: 0.00884394\n",
      "Epoch 2 | Step 1350800 | Avg Loss: 0.0151 | Grad Norm: 0.00804776\n",
      "Epoch 2 | Step 1350900 | Avg Loss: 0.0152 | Grad Norm: 0.00971729\n",
      "Epoch 2 | Step 1351000 | Avg Loss: 0.0152 | Grad Norm: 0.00879792\n",
      "Epoch 2 | Step 1351100 | Avg Loss: 0.0149 | Grad Norm: 0.00711647\n",
      "Epoch 2 | Step 1351200 | Avg Loss: 0.0148 | Grad Norm: 0.00964963\n",
      "Epoch 2 | Step 1351300 | Avg Loss: 0.0148 | Grad Norm: 0.00977699\n",
      "Epoch 2 | Step 1351400 | Avg Loss: 0.0149 | Grad Norm: 0.00874667\n",
      "Epoch 2 | Step 1351500 | Avg Loss: 0.0152 | Grad Norm: 0.00824897\n",
      "Epoch 2 | Step 1351600 | Avg Loss: 0.0154 | Grad Norm: 0.00956383\n",
      "Epoch 2 | Step 1351700 | Avg Loss: 0.0155 | Grad Norm: 0.00831405\n",
      "Epoch 2 | Step 1351800 | Avg Loss: 0.0154 | Grad Norm: 0.01137570\n",
      "Epoch 2 | Step 1351900 | Avg Loss: 0.0152 | Grad Norm: 0.01083848\n",
      "Epoch 2 | Step 1352000 | Avg Loss: 0.0150 | Grad Norm: 0.00883532\n",
      "Epoch 2 | Step 1352100 | Avg Loss: 0.0151 | Grad Norm: 0.00855182\n",
      "Epoch 2 | Step 1352200 | Avg Loss: 0.0152 | Grad Norm: 0.00848987\n",
      "Epoch 2 | Step 1352300 | Avg Loss: 0.0153 | Grad Norm: 0.00890175\n",
      "Epoch 2 | Step 1352400 | Avg Loss: 0.0150 | Grad Norm: 0.01027535\n",
      "Epoch 2 | Step 1352500 | Avg Loss: 0.0148 | Grad Norm: 0.00935252\n",
      "Epoch 2 | Step 1352600 | Avg Loss: 0.0155 | Grad Norm: 0.00865728\n",
      "Epoch 2 | Step 1352700 | Avg Loss: 0.0152 | Grad Norm: 0.00843234\n",
      "Epoch 2 | Step 1352800 | Avg Loss: 0.0151 | Grad Norm: 0.00929867\n",
      "Epoch 2 | Step 1352900 | Avg Loss: 0.0152 | Grad Norm: 0.00828565\n",
      "Epoch 2 | Step 1353000 | Avg Loss: 0.0147 | Grad Norm: 0.00695812\n",
      "Epoch 2 | Step 1353100 | Avg Loss: 0.0146 | Grad Norm: 0.00881877\n",
      "Epoch 2 | Step 1353200 | Avg Loss: 0.0149 | Grad Norm: 0.00773294\n",
      "Epoch 2 | Step 1353300 | Avg Loss: 0.0151 | Grad Norm: 0.00776059\n",
      "Epoch 2 | Step 1353400 | Avg Loss: 0.0151 | Grad Norm: 0.00907878\n",
      "Epoch 2 | Step 1353500 | Avg Loss: 0.0149 | Grad Norm: 0.00767061\n",
      "Epoch 2 | Step 1353600 | Avg Loss: 0.0151 | Grad Norm: 0.00909700\n",
      "Epoch 2 | Step 1353700 | Avg Loss: 0.0149 | Grad Norm: 0.00856004\n",
      "Epoch 2 | Step 1353800 | Avg Loss: 0.0151 | Grad Norm: 0.00880325\n",
      "Epoch 2 | Step 1353900 | Avg Loss: 0.0151 | Grad Norm: 0.00773679\n",
      "Epoch 2 | Step 1354000 | Avg Loss: 0.0150 | Grad Norm: 0.00843837\n",
      "Epoch 2 | Step 1354100 | Avg Loss: 0.0151 | Grad Norm: 0.00792845\n",
      "Epoch 2 | Step 1354200 | Avg Loss: 0.0147 | Grad Norm: 0.00794057\n",
      "Epoch 2 | Step 1354300 | Avg Loss: 0.0148 | Grad Norm: 0.00916822\n",
      "Epoch 2 | Step 1354400 | Avg Loss: 0.0145 | Grad Norm: 0.00807827\n",
      "Epoch 2 | Step 1354500 | Avg Loss: 0.0145 | Grad Norm: 0.00932343\n",
      "Epoch 2 | Step 1354600 | Avg Loss: 0.0145 | Grad Norm: 0.00864462\n",
      "Epoch 2 | Step 1354700 | Avg Loss: 0.0145 | Grad Norm: 0.00750142\n",
      "Epoch 2 | Step 1354800 | Avg Loss: 0.0148 | Grad Norm: 0.00858576\n",
      "Epoch 2 | Step 1354900 | Avg Loss: 0.0148 | Grad Norm: 0.00965058\n",
      "Epoch 2 | Step 1355000 | Avg Loss: 0.0148 | Grad Norm: 0.00910498\n",
      "Epoch 2 | Step 1355100 | Avg Loss: 0.0148 | Grad Norm: 0.00785839\n",
      "Epoch 2 | Step 1355200 | Avg Loss: 0.0151 | Grad Norm: 0.00821402\n",
      "Epoch 2 | Step 1355300 | Avg Loss: 0.0150 | Grad Norm: 0.00842016\n",
      "Epoch 2 | Step 1355400 | Avg Loss: 0.0149 | Grad Norm: 0.00862351\n",
      "Epoch 2 | Step 1355500 | Avg Loss: 0.0148 | Grad Norm: 0.00854004\n",
      "Epoch 2 | Step 1355600 | Avg Loss: 0.0149 | Grad Norm: 0.00820761\n",
      "Epoch 2 | Step 1355700 | Avg Loss: 0.0149 | Grad Norm: 0.00760946\n",
      "Epoch 2 | Step 1355800 | Avg Loss: 0.0149 | Grad Norm: 0.00797230\n",
      "Epoch 2 | Step 1355900 | Avg Loss: 0.0148 | Grad Norm: 0.00872783\n",
      "Epoch 2 | Step 1356000 | Avg Loss: 0.0150 | Grad Norm: 0.00864237\n",
      "Epoch 2 | Step 1356100 | Avg Loss: 0.0152 | Grad Norm: 0.00744513\n",
      "Epoch 2 | Step 1356200 | Avg Loss: 0.0150 | Grad Norm: 0.00883661\n",
      "Epoch 2 | Step 1356300 | Avg Loss: 0.0148 | Grad Norm: 0.00861865\n",
      "Epoch 2 | Step 1356400 | Avg Loss: 0.0148 | Grad Norm: 0.00715031\n",
      "Epoch 2 | Step 1356500 | Avg Loss: 0.0149 | Grad Norm: 0.00890166\n",
      "Epoch 2 | Step 1356600 | Avg Loss: 0.0151 | Grad Norm: 0.00853670\n",
      "Epoch 2 | Step 1356700 | Avg Loss: 0.0150 | Grad Norm: 0.00780563\n",
      "Epoch 2 | Step 1356800 | Avg Loss: 0.0150 | Grad Norm: 0.00786428\n",
      "Epoch 2 | Step 1356900 | Avg Loss: 0.0152 | Grad Norm: 0.00788118\n",
      "Epoch 2 | Step 1357000 | Avg Loss: 0.0148 | Grad Norm: 0.00850243\n",
      "Epoch 2 | Step 1357100 | Avg Loss: 0.0147 | Grad Norm: 0.00909046\n",
      "Epoch 2 | Step 1357200 | Avg Loss: 0.0149 | Grad Norm: 0.00822161\n",
      "Epoch 2 | Step 1357300 | Avg Loss: 0.0149 | Grad Norm: 0.01037330\n",
      "Epoch 2 | Step 1357400 | Avg Loss: 0.0149 | Grad Norm: 0.00943020\n",
      "Epoch 2 | Step 1357500 | Avg Loss: 0.0148 | Grad Norm: 0.00968484\n",
      "Epoch 2 | Step 1357600 | Avg Loss: 0.0145 | Grad Norm: 0.00897580\n",
      "Epoch 2 | Step 1357700 | Avg Loss: 0.0145 | Grad Norm: 0.00816376\n",
      "Epoch 2 | Step 1357800 | Avg Loss: 0.0146 | Grad Norm: 0.00753786\n",
      "Epoch 2 | Step 1357900 | Avg Loss: 0.0149 | Grad Norm: 0.00830061\n",
      "Epoch 2 | Step 1358000 | Avg Loss: 0.0148 | Grad Norm: 0.00862668\n",
      "Epoch 2 | Step 1358100 | Avg Loss: 0.0148 | Grad Norm: 0.00818467\n",
      "Epoch 2 | Step 1358200 | Avg Loss: 0.0146 | Grad Norm: 0.00778117\n",
      "Epoch 2 | Step 1358300 | Avg Loss: 0.0144 | Grad Norm: 0.00891549\n",
      "Epoch 2 | Step 1358400 | Avg Loss: 0.0145 | Grad Norm: 0.00869467\n",
      "Epoch 2 | Step 1358500 | Avg Loss: 0.0143 | Grad Norm: 0.00801490\n",
      "Epoch 2 | Step 1358600 | Avg Loss: 0.0146 | Grad Norm: 0.00760245\n",
      "Epoch 2 | Step 1358700 | Avg Loss: 0.0151 | Grad Norm: 0.00725000\n",
      "Epoch 2 | Step 1358800 | Avg Loss: 0.0147 | Grad Norm: 0.00682079\n",
      "Epoch 2 | Step 1358900 | Avg Loss: 0.0148 | Grad Norm: 0.00715383\n",
      "Epoch 2 | Step 1359000 | Avg Loss: 0.0148 | Grad Norm: 0.00854104\n",
      "Epoch 2 | Step 1359100 | Avg Loss: 0.0149 | Grad Norm: 0.01020459\n",
      "Epoch 2 | Step 1359200 | Avg Loss: 0.0148 | Grad Norm: 0.00931215\n",
      "Epoch 2 | Step 1359300 | Avg Loss: 0.0150 | Grad Norm: 0.00742984\n",
      "Epoch 2 | Step 1359400 | Avg Loss: 0.0149 | Grad Norm: 0.00794194\n",
      "Epoch 2 | Step 1359500 | Avg Loss: 0.0147 | Grad Norm: 0.00968980\n",
      "Epoch 2 | Step 1359600 | Avg Loss: 0.0145 | Grad Norm: 0.00701895\n",
      "Epoch 2 | Step 1359700 | Avg Loss: 0.0143 | Grad Norm: 0.00819544\n",
      "Epoch 2 | Step 1359800 | Avg Loss: 0.0145 | Grad Norm: 0.00884631\n",
      "Epoch 2 | Step 1359900 | Avg Loss: 0.0147 | Grad Norm: 0.00855874\n",
      "Epoch 2 | Step 1360000 | Avg Loss: 0.0148 | Grad Norm: 0.00763745\n",
      "Epoch 2 | Step 1360100 | Avg Loss: 0.0150 | Grad Norm: 0.00842142\n",
      "Epoch 2 | Step 1360200 | Avg Loss: 0.0147 | Grad Norm: 0.00873230\n",
      "Epoch 2 | Step 1360300 | Avg Loss: 0.0149 | Grad Norm: 0.00803180\n",
      "Epoch 2 | Step 1360400 | Avg Loss: 0.0146 | Grad Norm: 0.00894126\n",
      "Epoch 2 | Step 1360500 | Avg Loss: 0.0148 | Grad Norm: 0.00894706\n",
      "Epoch 2 | Step 1360600 | Avg Loss: 0.0148 | Grad Norm: 0.00842012\n",
      "Epoch 2 | Step 1360700 | Avg Loss: 0.0148 | Grad Norm: 0.00746905\n",
      "Epoch 2 | Step 1360800 | Avg Loss: 0.0152 | Grad Norm: 0.00744946\n",
      "Epoch 2 | Step 1360900 | Avg Loss: 0.0152 | Grad Norm: 0.00933969\n",
      "Epoch 2 | Step 1361000 | Avg Loss: 0.0148 | Grad Norm: 0.00789309\n",
      "Epoch 2 | Step 1361100 | Avg Loss: 0.0147 | Grad Norm: 0.00877362\n",
      "Epoch 2 | Step 1361200 | Avg Loss: 0.0146 | Grad Norm: 0.00990118\n",
      "Epoch 2 | Step 1361300 | Avg Loss: 0.0148 | Grad Norm: 0.00732131\n",
      "Epoch 2 | Step 1361400 | Avg Loss: 0.0147 | Grad Norm: 0.00852138\n",
      "Epoch 2 | Step 1361500 | Avg Loss: 0.0145 | Grad Norm: 0.00822652\n",
      "Epoch 2 | Step 1361600 | Avg Loss: 0.0145 | Grad Norm: 0.00800447\n",
      "Epoch 2 | Step 1361700 | Avg Loss: 0.0148 | Grad Norm: 0.00798864\n",
      "Epoch 2 | Step 1361800 | Avg Loss: 0.0150 | Grad Norm: 0.00742338\n",
      "Epoch 2 | Step 1361900 | Avg Loss: 0.0149 | Grad Norm: 0.00730340\n",
      "Epoch 2 | Step 1362000 | Avg Loss: 0.0146 | Grad Norm: 0.00855716\n",
      "Epoch 2 | Step 1362100 | Avg Loss: 0.0148 | Grad Norm: 0.00710931\n",
      "Epoch 2 | Step 1362200 | Avg Loss: 0.0152 | Grad Norm: 0.00639963\n",
      "Epoch 2 | Step 1362300 | Avg Loss: 0.0145 | Grad Norm: 0.00932693\n",
      "Epoch 2 | Step 1362400 | Avg Loss: 0.0147 | Grad Norm: 0.00793899\n",
      "Epoch 2 | Step 1362500 | Avg Loss: 0.0148 | Grad Norm: 0.00890755\n",
      "Epoch 2 | Step 1362600 | Avg Loss: 0.0151 | Grad Norm: 0.00804332\n",
      "Epoch 2 | Step 1362700 | Avg Loss: 0.0150 | Grad Norm: 0.00795052\n",
      "Epoch 2 | Step 1362800 | Avg Loss: 0.0150 | Grad Norm: 0.00857394\n",
      "Epoch 2 | Step 1362900 | Avg Loss: 0.0148 | Grad Norm: 0.00814060\n",
      "Epoch 2 | Step 1363000 | Avg Loss: 0.0146 | Grad Norm: 0.00885681\n",
      "Epoch 2 | Step 1363100 | Avg Loss: 0.0145 | Grad Norm: 0.00857126\n",
      "Epoch 2 | Step 1363200 | Avg Loss: 0.0145 | Grad Norm: 0.00764529\n",
      "Epoch 2 | Step 1363300 | Avg Loss: 0.0148 | Grad Norm: 0.00872174\n",
      "Epoch 2 | Step 1363400 | Avg Loss: 0.0146 | Grad Norm: 0.00833289\n",
      "Epoch 2 | Step 1363500 | Avg Loss: 0.0150 | Grad Norm: 0.00748403\n",
      "Epoch 2 | Step 1363600 | Avg Loss: 0.0148 | Grad Norm: 0.00851632\n",
      "Epoch 2 | Step 1363700 | Avg Loss: 0.0148 | Grad Norm: 0.00723199\n",
      "Epoch 2 | Step 1363800 | Avg Loss: 0.0147 | Grad Norm: 0.00841159\n",
      "Epoch 2 | Step 1363900 | Avg Loss: 0.0146 | Grad Norm: 0.00843610\n",
      "Epoch 2 | Step 1364000 | Avg Loss: 0.0145 | Grad Norm: 0.01062729\n",
      "Epoch 2 | Step 1364100 | Avg Loss: 0.0147 | Grad Norm: 0.00719308\n",
      "Epoch 2 | Step 1364200 | Avg Loss: 0.0146 | Grad Norm: 0.00780004\n",
      "Epoch 2 | Step 1364300 | Avg Loss: 0.0144 | Grad Norm: 0.00814606\n",
      "Epoch 2 | Step 1364400 | Avg Loss: 0.0145 | Grad Norm: 0.00789933\n",
      "Epoch 2 | Step 1364500 | Avg Loss: 0.0148 | Grad Norm: 0.00850003\n",
      "Epoch 2 | Step 1364600 | Avg Loss: 0.0148 | Grad Norm: 0.01042205\n",
      "Epoch 2 | Step 1364700 | Avg Loss: 0.0149 | Grad Norm: 0.00819543\n",
      "Epoch 2 | Step 1364800 | Avg Loss: 0.0154 | Grad Norm: 0.00838966\n",
      "Epoch 2 | Step 1364900 | Avg Loss: 0.0154 | Grad Norm: 0.00855592\n",
      "Epoch 2 | Step 1365000 | Avg Loss: 0.0154 | Grad Norm: 0.00984186\n",
      "Epoch 2 | Step 1365100 | Avg Loss: 0.0154 | Grad Norm: 0.00817187\n",
      "Epoch 2 | Step 1365200 | Avg Loss: 0.0151 | Grad Norm: 0.00870809\n",
      "Epoch 2 | Step 1365300 | Avg Loss: 0.0149 | Grad Norm: 0.00972676\n",
      "Epoch 2 | Step 1365400 | Avg Loss: 0.0148 | Grad Norm: 0.01074673\n",
      "Epoch 2 | Step 1365500 | Avg Loss: 0.0149 | Grad Norm: 0.00805938\n",
      "Epoch 2 | Step 1365600 | Avg Loss: 0.0151 | Grad Norm: 0.00845762\n",
      "Epoch 2 | Step 1365700 | Avg Loss: 0.0151 | Grad Norm: 0.00857059\n",
      "Epoch 2 | Step 1365800 | Avg Loss: 0.0151 | Grad Norm: 0.00907325\n",
      "Epoch 2 | Step 1365900 | Avg Loss: 0.0153 | Grad Norm: 0.00928891\n",
      "Epoch 2 | Step 1366000 | Avg Loss: 0.0155 | Grad Norm: 0.00940001\n",
      "Epoch 2 | Step 1366100 | Avg Loss: 0.0157 | Grad Norm: 0.00869036\n",
      "Epoch 2 | Step 1366200 | Avg Loss: 0.0154 | Grad Norm: 0.00820614\n",
      "Epoch 2 | Step 1366300 | Avg Loss: 0.0155 | Grad Norm: 0.00826840\n",
      "Epoch 2 | Step 1366400 | Avg Loss: 0.0158 | Grad Norm: 0.00943256\n",
      "Epoch 2 | Step 1366500 | Avg Loss: 0.0159 | Grad Norm: 0.00858676\n",
      "Epoch 2 | Step 1366600 | Avg Loss: 0.0156 | Grad Norm: 0.00898138\n",
      "Epoch 2 | Step 1366700 | Avg Loss: 0.0155 | Grad Norm: 0.00854698\n",
      "Epoch 2 | Step 1366800 | Avg Loss: 0.0154 | Grad Norm: 0.00898765\n",
      "Epoch 2 | Step 1366900 | Avg Loss: 0.0154 | Grad Norm: 0.00991198\n",
      "Epoch 2 | Step 1367000 | Avg Loss: 0.0154 | Grad Norm: 0.00917853\n",
      "Epoch 2 | Step 1367100 | Avg Loss: 0.0154 | Grad Norm: 0.00729501\n",
      "Epoch 2 | Step 1367200 | Avg Loss: 0.0156 | Grad Norm: 0.00948985\n",
      "Epoch 2 | Step 1367300 | Avg Loss: 0.0150 | Grad Norm: 0.00766349\n",
      "Epoch 2 | Step 1367400 | Avg Loss: 0.0149 | Grad Norm: 0.00897875\n",
      "Epoch 2 | Step 1367500 | Avg Loss: 0.0150 | Grad Norm: 0.00843994\n",
      "Epoch 2 | Step 1367600 | Avg Loss: 0.0147 | Grad Norm: 0.00844379\n",
      "Epoch 2 | Step 1367700 | Avg Loss: 0.0147 | Grad Norm: 0.00863400\n",
      "Epoch 2 | Step 1367800 | Avg Loss: 0.0151 | Grad Norm: 0.00921604\n",
      "Epoch 2 | Step 1367900 | Avg Loss: 0.0153 | Grad Norm: 0.00986093\n",
      "Epoch 2 | Step 1368000 | Avg Loss: 0.0153 | Grad Norm: 0.01007022\n",
      "Epoch 2 | Step 1368100 | Avg Loss: 0.0154 | Grad Norm: 0.00943285\n",
      "Epoch 2 | Step 1368200 | Avg Loss: 0.0153 | Grad Norm: 0.00818788\n",
      "Epoch 2 | Step 1368300 | Avg Loss: 0.0152 | Grad Norm: 0.00886651\n",
      "Epoch 2 | Step 1368400 | Avg Loss: 0.0152 | Grad Norm: 0.00963899\n",
      "Epoch 2 | Step 1368500 | Avg Loss: 0.0152 | Grad Norm: 0.00997697\n",
      "Epoch 2 | Step 1368600 | Avg Loss: 0.0154 | Grad Norm: 0.00843189\n",
      "Epoch 2 | Step 1368700 | Avg Loss: 0.0153 | Grad Norm: 0.00964128\n",
      "Epoch 2 | Step 1368800 | Avg Loss: 0.0158 | Grad Norm: 0.01017587\n",
      "Epoch 2 | Step 1368900 | Avg Loss: 0.0155 | Grad Norm: 0.00979064\n",
      "Epoch 2 | Step 1369000 | Avg Loss: 0.0156 | Grad Norm: 0.00842783\n",
      "Epoch 2 | Step 1369100 | Avg Loss: 0.0153 | Grad Norm: 0.00735221\n",
      "Epoch 2 | Step 1369200 | Avg Loss: 0.0153 | Grad Norm: 0.00921249\n",
      "Epoch 2 | Step 1369300 | Avg Loss: 0.0155 | Grad Norm: 0.00919240\n",
      "Epoch 2 | Step 1369400 | Avg Loss: 0.0150 | Grad Norm: 0.00796486\n",
      "Epoch 2 | Step 1369500 | Avg Loss: 0.0151 | Grad Norm: 0.00806384\n",
      "Epoch 2 | Step 1369600 | Avg Loss: 0.0148 | Grad Norm: 0.00730736\n",
      "Epoch 2 | Step 1369700 | Avg Loss: 0.0146 | Grad Norm: 0.00768947\n",
      "Epoch 2 | Step 1369800 | Avg Loss: 0.0152 | Grad Norm: 0.00819600\n",
      "Epoch 2 | Step 1369900 | Avg Loss: 0.0150 | Grad Norm: 0.00866238\n",
      "Epoch 2 | Step 1370000 | Avg Loss: 0.0148 | Grad Norm: 0.00728145\n",
      "Epoch 2 | Step 1370100 | Avg Loss: 0.0147 | Grad Norm: 0.00836270\n",
      "Epoch 2 | Step 1370200 | Avg Loss: 0.0150 | Grad Norm: 0.00882455\n",
      "Epoch 2 | Step 1370300 | Avg Loss: 0.0148 | Grad Norm: 0.00782320\n",
      "Epoch 2 | Step 1370400 | Avg Loss: 0.0148 | Grad Norm: 0.00821126\n",
      "Epoch 2 | Step 1370500 | Avg Loss: 0.0147 | Grad Norm: 0.00733686\n",
      "Epoch 2 | Step 1370600 | Avg Loss: 0.0149 | Grad Norm: 0.00859877\n",
      "Epoch 2 | Step 1370700 | Avg Loss: 0.0146 | Grad Norm: 0.00797955\n",
      "Epoch 2 | Step 1370800 | Avg Loss: 0.0147 | Grad Norm: 0.00756242\n",
      "Epoch 2 | Step 1370900 | Avg Loss: 0.0147 | Grad Norm: 0.00790139\n",
      "Epoch 2 | Step 1371000 | Avg Loss: 0.0148 | Grad Norm: 0.00838490\n",
      "Epoch 2 | Step 1371100 | Avg Loss: 0.0148 | Grad Norm: 0.00829640\n",
      "Epoch 2 | Step 1371200 | Avg Loss: 0.0147 | Grad Norm: 0.00960713\n",
      "Epoch 2 | Step 1371300 | Avg Loss: 0.0148 | Grad Norm: 0.00776408\n",
      "Epoch 2 | Step 1371400 | Avg Loss: 0.0148 | Grad Norm: 0.00906401\n",
      "Epoch 2 | Step 1371500 | Avg Loss: 0.0146 | Grad Norm: 0.00830871\n",
      "Epoch 2 | Step 1371600 | Avg Loss: 0.0149 | Grad Norm: 0.00740603\n",
      "Epoch 2 | Step 1371700 | Avg Loss: 0.0150 | Grad Norm: 0.00828956\n",
      "Epoch 2 | Step 1371800 | Avg Loss: 0.0148 | Grad Norm: 0.00827104\n",
      "Epoch 2 | Step 1371900 | Avg Loss: 0.0145 | Grad Norm: 0.00755659\n",
      "Epoch 2 | Step 1372000 | Avg Loss: 0.0144 | Grad Norm: 0.00713160\n",
      "Epoch 2 | Step 1372100 | Avg Loss: 0.0148 | Grad Norm: 0.00869624\n",
      "Epoch 2 | Step 1372200 | Avg Loss: 0.0146 | Grad Norm: 0.00809786\n",
      "Epoch 2 | Step 1372300 | Avg Loss: 0.0146 | Grad Norm: 0.00708659\n",
      "Epoch 2 | Step 1372400 | Avg Loss: 0.0148 | Grad Norm: 0.00844708\n",
      "Epoch 2 | Step 1372500 | Avg Loss: 0.0152 | Grad Norm: 0.00729414\n",
      "Epoch 2 | Step 1372600 | Avg Loss: 0.0146 | Grad Norm: 0.00705917\n",
      "Epoch 2 | Step 1372700 | Avg Loss: 0.0152 | Grad Norm: 0.00875672\n",
      "Epoch 2 | Step 1372800 | Avg Loss: 0.0155 | Grad Norm: 0.00846771\n",
      "Epoch 2 | Step 1372900 | Avg Loss: 0.0152 | Grad Norm: 0.00777884\n",
      "Epoch 2 | Step 1373000 | Avg Loss: 0.0154 | Grad Norm: 0.00804906\n",
      "Epoch 2 | Step 1373100 | Avg Loss: 0.0154 | Grad Norm: 0.00742526\n",
      "Epoch 2 | Step 1373200 | Avg Loss: 0.0158 | Grad Norm: 0.00852561\n",
      "Epoch 2 | Step 1373300 | Avg Loss: 0.0155 | Grad Norm: 0.01056803\n",
      "Epoch 2 | Step 1373400 | Avg Loss: 0.0155 | Grad Norm: 0.00898050\n",
      "Epoch 2 | Step 1373500 | Avg Loss: 0.0150 | Grad Norm: 0.00836739\n",
      "Epoch 2 | Step 1373600 | Avg Loss: 0.0152 | Grad Norm: 0.00693451\n",
      "Epoch 2 | Step 1373700 | Avg Loss: 0.0147 | Grad Norm: 0.00955395\n",
      "Epoch 2 | Step 1373800 | Avg Loss: 0.0150 | Grad Norm: 0.00826145\n",
      "Epoch 2 | Step 1373900 | Avg Loss: 0.0149 | Grad Norm: 0.00794309\n",
      "Epoch 2 | Step 1374000 | Avg Loss: 0.0145 | Grad Norm: 0.00690322\n",
      "Epoch 2 | Step 1374100 | Avg Loss: 0.0144 | Grad Norm: 0.00677245\n",
      "Epoch 2 | Step 1374200 | Avg Loss: 0.0147 | Grad Norm: 0.00749386\n",
      "Epoch 2 | Step 1374300 | Avg Loss: 0.0148 | Grad Norm: 0.00726757\n",
      "Epoch 2 | Step 1374400 | Avg Loss: 0.0146 | Grad Norm: 0.00805917\n",
      "Epoch 2 | Step 1374500 | Avg Loss: 0.0150 | Grad Norm: 0.00864030\n",
      "Epoch 2 | Step 1374600 | Avg Loss: 0.0151 | Grad Norm: 0.00829622\n",
      "Epoch 2 | Step 1374700 | Avg Loss: 0.0151 | Grad Norm: 0.00736244\n",
      "Epoch 2 | Step 1374800 | Avg Loss: 0.0149 | Grad Norm: 0.00757040\n",
      "Epoch 2 | Step 1374900 | Avg Loss: 0.0152 | Grad Norm: 0.00844586\n",
      "Epoch 2 | Step 1375000 | Avg Loss: 0.0153 | Grad Norm: 0.00835773\n",
      "Epoch 2 | Step 1375100 | Avg Loss: 0.0151 | Grad Norm: 0.00832599\n",
      "Epoch 2 | Step 1375200 | Avg Loss: 0.0150 | Grad Norm: 0.00792223\n",
      "Epoch 2 | Step 1375300 | Avg Loss: 0.0149 | Grad Norm: 0.00879406\n",
      "Epoch 2 | Step 1375400 | Avg Loss: 0.0151 | Grad Norm: 0.00732440\n",
      "Epoch 2 | Step 1375500 | Avg Loss: 0.0149 | Grad Norm: 0.00766684\n",
      "Epoch 2 | Step 1375600 | Avg Loss: 0.0149 | Grad Norm: 0.00771530\n",
      "Epoch 2 | Step 1375700 | Avg Loss: 0.0147 | Grad Norm: 0.00788438\n",
      "Epoch 2 | Step 1375800 | Avg Loss: 0.0150 | Grad Norm: 0.00820314\n",
      "Epoch 2 | Step 1375900 | Avg Loss: 0.0149 | Grad Norm: 0.00812694\n",
      "Epoch 2 | Step 1376000 | Avg Loss: 0.0148 | Grad Norm: 0.00760503\n",
      "Epoch 2 | Step 1376100 | Avg Loss: 0.0145 | Grad Norm: 0.00713923\n",
      "Epoch 2 | Step 1376200 | Avg Loss: 0.0147 | Grad Norm: 0.00826229\n",
      "Epoch 2 | Step 1376300 | Avg Loss: 0.0150 | Grad Norm: 0.00760270\n",
      "Epoch 2 | Step 1376400 | Avg Loss: 0.0151 | Grad Norm: 0.00762040\n",
      "Epoch 2 | Step 1376500 | Avg Loss: 0.0150 | Grad Norm: 0.00875645\n",
      "Epoch 2 | Step 1376600 | Avg Loss: 0.0151 | Grad Norm: 0.00918746\n",
      "Epoch 2 | Step 1376700 | Avg Loss: 0.0152 | Grad Norm: 0.00766393\n",
      "Epoch 2 | Step 1376800 | Avg Loss: 0.0152 | Grad Norm: 0.00826923\n",
      "Epoch 2 | Step 1376900 | Avg Loss: 0.0153 | Grad Norm: 0.00907785\n",
      "Epoch 2 | Step 1377000 | Avg Loss: 0.0151 | Grad Norm: 0.00703599\n",
      "Epoch 2 | Step 1377100 | Avg Loss: 0.0150 | Grad Norm: 0.00768418\n",
      "Epoch 2 | Step 1377200 | Avg Loss: 0.0150 | Grad Norm: 0.00810464\n",
      "Epoch 2 | Step 1377300 | Avg Loss: 0.0152 | Grad Norm: 0.00962952\n",
      "Epoch 2 | Step 1377400 | Avg Loss: 0.0155 | Grad Norm: 0.00867507\n",
      "Epoch 2 | Step 1377500 | Avg Loss: 0.0153 | Grad Norm: 0.00825196\n",
      "Epoch 2 | Step 1377600 | Avg Loss: 0.0153 | Grad Norm: 0.00800446\n",
      "Epoch 2 | Step 1377700 | Avg Loss: 0.0152 | Grad Norm: 0.00788383\n",
      "Epoch 2 | Step 1377800 | Avg Loss: 0.0149 | Grad Norm: 0.00799669\n",
      "Epoch 2 | Step 1377900 | Avg Loss: 0.0149 | Grad Norm: 0.00806310\n",
      "Epoch 2 | Step 1378000 | Avg Loss: 0.0151 | Grad Norm: 0.00852204\n",
      "Epoch 2 | Step 1378100 | Avg Loss: 0.0152 | Grad Norm: 0.00902416\n",
      "Epoch 2 | Step 1378200 | Avg Loss: 0.0153 | Grad Norm: 0.00809904\n",
      "Epoch 2 | Step 1378300 | Avg Loss: 0.0157 | Grad Norm: 0.00822587\n",
      "Epoch 2 | Step 1378400 | Avg Loss: 0.0151 | Grad Norm: 0.00711777\n",
      "Epoch 2 | Step 1378500 | Avg Loss: 0.0152 | Grad Norm: 0.00890845\n",
      "Epoch 2 | Step 1378600 | Avg Loss: 0.0150 | Grad Norm: 0.00717291\n",
      "Epoch 2 | Step 1378700 | Avg Loss: 0.0147 | Grad Norm: 0.00766896\n",
      "Epoch 2 | Step 1378800 | Avg Loss: 0.0151 | Grad Norm: 0.00741864\n",
      "Epoch 2 | Step 1378900 | Avg Loss: 0.0152 | Grad Norm: 0.00632585\n",
      "Epoch 2 | Step 1379000 | Avg Loss: 0.0149 | Grad Norm: 0.00758066\n",
      "Epoch 2 | Step 1379100 | Avg Loss: 0.0148 | Grad Norm: 0.00676698\n",
      "Epoch 2 | Step 1379200 | Avg Loss: 0.0151 | Grad Norm: 0.00762500\n",
      "Epoch 2 | Step 1379300 | Avg Loss: 0.0149 | Grad Norm: 0.00907114\n",
      "Epoch 2 | Step 1379400 | Avg Loss: 0.0153 | Grad Norm: 0.00744523\n",
      "Epoch 2 | Step 1379500 | Avg Loss: 0.0154 | Grad Norm: 0.00810028\n",
      "Epoch 2 | Step 1379600 | Avg Loss: 0.0151 | Grad Norm: 0.00706079\n",
      "Epoch 2 | Step 1379700 | Avg Loss: 0.0150 | Grad Norm: 0.00814508\n",
      "Epoch 2 | Step 1379800 | Avg Loss: 0.0147 | Grad Norm: 0.00819195\n",
      "Epoch 2 | Step 1379900 | Avg Loss: 0.0145 | Grad Norm: 0.00790330\n",
      "Epoch 2 | Step 1380000 | Avg Loss: 0.0145 | Grad Norm: 0.00783000\n",
      "Epoch 2 | Step 1380100 | Avg Loss: 0.0145 | Grad Norm: 0.00869324\n",
      "Epoch 2 | Step 1380200 | Avg Loss: 0.0150 | Grad Norm: 0.00828699\n",
      "Epoch 2 | Step 1380300 | Avg Loss: 0.0149 | Grad Norm: 0.00752425\n",
      "Epoch 2 | Step 1380400 | Avg Loss: 0.0148 | Grad Norm: 0.00895261\n",
      "Epoch 2 | Step 1380500 | Avg Loss: 0.0151 | Grad Norm: 0.00674474\n",
      "Epoch 2 | Step 1380600 | Avg Loss: 0.0152 | Grad Norm: 0.00915477\n",
      "Epoch 2 | Step 1380700 | Avg Loss: 0.0149 | Grad Norm: 0.00830573\n",
      "Epoch 2 | Step 1380800 | Avg Loss: 0.0147 | Grad Norm: 0.00833862\n",
      "Epoch 2 | Step 1380900 | Avg Loss: 0.0146 | Grad Norm: 0.00906246\n",
      "Epoch 2 | Step 1381000 | Avg Loss: 0.0148 | Grad Norm: 0.00799800\n",
      "Epoch 2 | Step 1381100 | Avg Loss: 0.0148 | Grad Norm: 0.00777224\n",
      "Epoch 2 | Step 1381200 | Avg Loss: 0.0148 | Grad Norm: 0.00947936\n",
      "Epoch 2 | Step 1381300 | Avg Loss: 0.0147 | Grad Norm: 0.00879458\n",
      "Epoch 2 | Step 1381400 | Avg Loss: 0.0146 | Grad Norm: 0.00872289\n",
      "Epoch 2 | Step 1381500 | Avg Loss: 0.0148 | Grad Norm: 0.00835760\n",
      "Epoch 2 | Step 1381600 | Avg Loss: 0.0147 | Grad Norm: 0.00839327\n",
      "Epoch 2 | Step 1381700 | Avg Loss: 0.0146 | Grad Norm: 0.00666570\n",
      "Epoch 2 | Step 1381800 | Avg Loss: 0.0149 | Grad Norm: 0.00842391\n",
      "Epoch 2 | Step 1381900 | Avg Loss: 0.0151 | Grad Norm: 0.00942998\n",
      "Epoch 2 | Step 1382000 | Avg Loss: 0.0154 | Grad Norm: 0.00798826\n",
      "Epoch 2 | Step 1382100 | Avg Loss: 0.0153 | Grad Norm: 0.01021850\n",
      "Epoch 2 | Step 1382200 | Avg Loss: 0.0153 | Grad Norm: 0.00837741\n",
      "Epoch 2 | Step 1382300 | Avg Loss: 0.0152 | Grad Norm: 0.00844594\n",
      "Epoch 2 | Step 1382400 | Avg Loss: 0.0153 | Grad Norm: 0.00745154\n",
      "Epoch 2 | Step 1382500 | Avg Loss: 0.0150 | Grad Norm: 0.00819777\n",
      "Epoch 2 | Step 1382600 | Avg Loss: 0.0149 | Grad Norm: 0.00737914\n",
      "Epoch 2 | Step 1382700 | Avg Loss: 0.0147 | Grad Norm: 0.00801142\n",
      "Epoch 2 | Step 1382800 | Avg Loss: 0.0148 | Grad Norm: 0.00973730\n",
      "Epoch 2 | Step 1382900 | Avg Loss: 0.0149 | Grad Norm: 0.00773841\n",
      "Epoch 2 | Step 1383000 | Avg Loss: 0.0149 | Grad Norm: 0.00844505\n",
      "Epoch 2 | Step 1383100 | Avg Loss: 0.0149 | Grad Norm: 0.00880601\n",
      "Epoch 2 | Step 1383200 | Avg Loss: 0.0148 | Grad Norm: 0.01051885\n",
      "Epoch 2 | Step 1383300 | Avg Loss: 0.0148 | Grad Norm: 0.00683541\n",
      "Epoch 2 | Step 1383400 | Avg Loss: 0.0149 | Grad Norm: 0.00707799\n",
      "Epoch 2 | Step 1383500 | Avg Loss: 0.0147 | Grad Norm: 0.00846190\n",
      "Epoch 2 | Step 1383600 | Avg Loss: 0.0146 | Grad Norm: 0.00785271\n",
      "Epoch 2 | Step 1383700 | Avg Loss: 0.0149 | Grad Norm: 0.00900548\n",
      "Epoch 2 | Step 1383800 | Avg Loss: 0.0150 | Grad Norm: 0.00864187\n",
      "Epoch 2 | Step 1383900 | Avg Loss: 0.0151 | Grad Norm: 0.00780962\n",
      "Epoch 2 | Step 1384000 | Avg Loss: 0.0149 | Grad Norm: 0.00946602\n",
      "Epoch 2 | Step 1384100 | Avg Loss: 0.0149 | Grad Norm: 0.00809601\n",
      "Epoch 2 | Step 1384200 | Avg Loss: 0.0148 | Grad Norm: 0.00751256\n",
      "Epoch 2 | Step 1384300 | Avg Loss: 0.0149 | Grad Norm: 0.00780996\n",
      "Epoch 2 | Step 1384400 | Avg Loss: 0.0148 | Grad Norm: 0.00874300\n",
      "Epoch 2 | Step 1384500 | Avg Loss: 0.0151 | Grad Norm: 0.00914653\n",
      "Epoch 2 | Step 1384600 | Avg Loss: 0.0147 | Grad Norm: 0.00874737\n",
      "Epoch 2 | Step 1384700 | Avg Loss: 0.0148 | Grad Norm: 0.00858221\n",
      "Epoch 2 | Step 1384800 | Avg Loss: 0.0148 | Grad Norm: 0.00813030\n",
      "Epoch 2 | Step 1384900 | Avg Loss: 0.0149 | Grad Norm: 0.00793927\n",
      "Epoch 2 | Step 1385000 | Avg Loss: 0.0153 | Grad Norm: 0.00794169\n",
      "Epoch 2 | Step 1385100 | Avg Loss: 0.0150 | Grad Norm: 0.00768222\n",
      "Epoch 2 | Step 1385200 | Avg Loss: 0.0152 | Grad Norm: 0.00831496\n",
      "Epoch 2 | Step 1385300 | Avg Loss: 0.0154 | Grad Norm: 0.00850075\n",
      "Epoch 2 | Step 1385400 | Avg Loss: 0.0150 | Grad Norm: 0.00829567\n",
      "Epoch 2 | Step 1385500 | Avg Loss: 0.0150 | Grad Norm: 0.00746378\n",
      "Epoch 2 | Step 1385600 | Avg Loss: 0.0150 | Grad Norm: 0.00901809\n",
      "Epoch 2 | Step 1385700 | Avg Loss: 0.0149 | Grad Norm: 0.00845061\n",
      "Epoch 2 | Step 1385800 | Avg Loss: 0.0153 | Grad Norm: 0.01038431\n",
      "Epoch 2 | Step 1385900 | Avg Loss: 0.0157 | Grad Norm: 0.00818253\n",
      "Epoch 2 | Step 1386000 | Avg Loss: 0.0151 | Grad Norm: 0.00748447\n",
      "Epoch 2 | Step 1386100 | Avg Loss: 0.0149 | Grad Norm: 0.00799490\n",
      "Epoch 2 | Step 1386200 | Avg Loss: 0.0152 | Grad Norm: 0.00856027\n",
      "Epoch 2 | Step 1386300 | Avg Loss: 0.0156 | Grad Norm: 0.01030303\n",
      "Epoch 2 | Step 1386400 | Avg Loss: 0.0155 | Grad Norm: 0.00861775\n",
      "Epoch 2 | Step 1386500 | Avg Loss: 0.0150 | Grad Norm: 0.00886625\n",
      "Epoch 2 | Step 1386600 | Avg Loss: 0.0149 | Grad Norm: 0.00911632\n",
      "Epoch 2 | Step 1386700 | Avg Loss: 0.0151 | Grad Norm: 0.00785106\n",
      "Epoch 2 | Step 1386800 | Avg Loss: 0.0150 | Grad Norm: 0.00716891\n",
      "Epoch 2 | Step 1386900 | Avg Loss: 0.0153 | Grad Norm: 0.00790983\n",
      "Epoch 2 | Step 1387000 | Avg Loss: 0.0146 | Grad Norm: 0.00747662\n",
      "Epoch 2 | Step 1387100 | Avg Loss: 0.0143 | Grad Norm: 0.00750766\n",
      "Epoch 2 | Step 1387200 | Avg Loss: 0.0141 | Grad Norm: 0.00799588\n",
      "Epoch 2 | Step 1387300 | Avg Loss: 0.0143 | Grad Norm: 0.00820183\n",
      "Epoch 2 | Step 1387400 | Avg Loss: 0.0144 | Grad Norm: 0.00889229\n",
      "Epoch 2 | Step 1387500 | Avg Loss: 0.0150 | Grad Norm: 0.00974333\n",
      "Epoch 2 | Step 1387600 | Avg Loss: 0.0153 | Grad Norm: 0.00989636\n",
      "Epoch 2 | Step 1387700 | Avg Loss: 0.0150 | Grad Norm: 0.00749382\n",
      "Epoch 2 | Step 1387800 | Avg Loss: 0.0150 | Grad Norm: 0.00722902\n",
      "Epoch 2 | Step 1387900 | Avg Loss: 0.0149 | Grad Norm: 0.00858806\n",
      "Epoch 2 | Step 1388000 | Avg Loss: 0.0150 | Grad Norm: 0.00874774\n",
      "Epoch 2 | Step 1388100 | Avg Loss: 0.0150 | Grad Norm: 0.00837887\n",
      "Epoch 2 | Step 1388200 | Avg Loss: 0.0147 | Grad Norm: 0.00825127\n",
      "Epoch 2 | Step 1388300 | Avg Loss: 0.0151 | Grad Norm: 0.00785952\n",
      "Epoch 2 | Step 1388400 | Avg Loss: 0.0149 | Grad Norm: 0.00901151\n",
      "Epoch 2 | Step 1388500 | Avg Loss: 0.0148 | Grad Norm: 0.00695986\n",
      "Epoch 2 | Step 1388600 | Avg Loss: 0.0145 | Grad Norm: 0.00689994\n",
      "Epoch 2 | Step 1388700 | Avg Loss: 0.0150 | Grad Norm: 0.00850366\n",
      "Epoch 2 | Step 1388800 | Avg Loss: 0.0148 | Grad Norm: 0.00938384\n",
      "Epoch 2 | Step 1388900 | Avg Loss: 0.0150 | Grad Norm: 0.00726671\n",
      "Epoch 2 | Step 1389000 | Avg Loss: 0.0150 | Grad Norm: 0.00875884\n",
      "Epoch 2 | Step 1389100 | Avg Loss: 0.0150 | Grad Norm: 0.01006539\n",
      "Epoch 2 | Step 1389200 | Avg Loss: 0.0149 | Grad Norm: 0.00876154\n",
      "Epoch 2 | Step 1389300 | Avg Loss: 0.0150 | Grad Norm: 0.00971376\n",
      "Epoch 2 | Step 1389400 | Avg Loss: 0.0149 | Grad Norm: 0.00713892\n",
      "Epoch 2 | Step 1389500 | Avg Loss: 0.0148 | Grad Norm: 0.00787499\n",
      "Epoch 2 | Step 1389600 | Avg Loss: 0.0148 | Grad Norm: 0.01100984\n",
      "Epoch 2 | Step 1389700 | Avg Loss: 0.0150 | Grad Norm: 0.00852597\n",
      "Epoch 2 | Step 1389800 | Avg Loss: 0.0149 | Grad Norm: 0.00809869\n",
      "Epoch 2 | Step 1389900 | Avg Loss: 0.0151 | Grad Norm: 0.00940102\n",
      "Epoch 2 | Step 1390000 | Avg Loss: 0.0151 | Grad Norm: 0.00894273\n",
      "Epoch 2 | Step 1390100 | Avg Loss: 0.0151 | Grad Norm: 0.00925098\n",
      "Epoch 2 | Step 1390200 | Avg Loss: 0.0152 | Grad Norm: 0.00945914\n",
      "Epoch 2 | Step 1390300 | Avg Loss: 0.0150 | Grad Norm: 0.00764378\n",
      "Epoch 2 | Step 1390400 | Avg Loss: 0.0147 | Grad Norm: 0.00806160\n",
      "Epoch 2 | Step 1390500 | Avg Loss: 0.0147 | Grad Norm: 0.00808473\n",
      "Epoch 2 | Step 1390600 | Avg Loss: 0.0149 | Grad Norm: 0.00840812\n",
      "Epoch 2 | Step 1390700 | Avg Loss: 0.0152 | Grad Norm: 0.00718105\n",
      "Epoch 2 | Step 1390800 | Avg Loss: 0.0146 | Grad Norm: 0.00881865\n",
      "Epoch 2 | Step 1390900 | Avg Loss: 0.0147 | Grad Norm: 0.00848042\n",
      "Epoch 2 | Step 1391000 | Avg Loss: 0.0150 | Grad Norm: 0.00907555\n",
      "Epoch 2 | Step 1391100 | Avg Loss: 0.0149 | Grad Norm: 0.00795242\n",
      "Epoch 2 | Step 1391200 | Avg Loss: 0.0151 | Grad Norm: 0.00824659\n",
      "Epoch 2 | Step 1391300 | Avg Loss: 0.0153 | Grad Norm: 0.00858338\n",
      "Epoch 2 | Step 1391400 | Avg Loss: 0.0154 | Grad Norm: 0.00867683\n",
      "Epoch 2 | Step 1391500 | Avg Loss: 0.0154 | Grad Norm: 0.00828500\n",
      "Epoch 2 | Step 1391600 | Avg Loss: 0.0151 | Grad Norm: 0.00832607\n",
      "Epoch 2 | Step 1391700 | Avg Loss: 0.0149 | Grad Norm: 0.00705174\n",
      "Epoch 2 | Step 1391800 | Avg Loss: 0.0150 | Grad Norm: 0.01107670\n",
      "Epoch 2 | Step 1391900 | Avg Loss: 0.0151 | Grad Norm: 0.00805948\n",
      "Epoch 2 | Step 1392000 | Avg Loss: 0.0150 | Grad Norm: 0.00764781\n",
      "Epoch 2 | Step 1392100 | Avg Loss: 0.0151 | Grad Norm: 0.00911496\n",
      "Epoch 2 | Step 1392200 | Avg Loss: 0.0154 | Grad Norm: 0.00788844\n",
      "Epoch 2 | Step 1392300 | Avg Loss: 0.0150 | Grad Norm: 0.00713474\n",
      "Epoch 2 | Step 1392400 | Avg Loss: 0.0150 | Grad Norm: 0.00830203\n",
      "Epoch 2 | Step 1392500 | Avg Loss: 0.0148 | Grad Norm: 0.00911857\n",
      "Epoch 2 | Step 1392600 | Avg Loss: 0.0147 | Grad Norm: 0.00985405\n",
      "Epoch 2 | Step 1392700 | Avg Loss: 0.0148 | Grad Norm: 0.00821239\n",
      "Epoch 2 | Step 1392800 | Avg Loss: 0.0151 | Grad Norm: 0.00787137\n",
      "Epoch 2 | Step 1392900 | Avg Loss: 0.0154 | Grad Norm: 0.00762092\n",
      "Epoch 2 | Step 1393000 | Avg Loss: 0.0149 | Grad Norm: 0.00791911\n",
      "Epoch 2 | Step 1393100 | Avg Loss: 0.0150 | Grad Norm: 0.00844392\n",
      "Epoch 2 | Step 1393200 | Avg Loss: 0.0152 | Grad Norm: 0.00932601\n",
      "Epoch 2 | Step 1393300 | Avg Loss: 0.0150 | Grad Norm: 0.00872003\n",
      "Epoch 2 | Step 1393400 | Avg Loss: 0.0154 | Grad Norm: 0.00976062\n",
      "Epoch 2 | Step 1393500 | Avg Loss: 0.0153 | Grad Norm: 0.00812246\n",
      "Epoch 2 | Step 1393600 | Avg Loss: 0.0150 | Grad Norm: 0.00896465\n",
      "Epoch 2 | Step 1393700 | Avg Loss: 0.0150 | Grad Norm: 0.00694053\n",
      "Epoch 2 | Step 1393800 | Avg Loss: 0.0150 | Grad Norm: 0.00755008\n",
      "Epoch 2 | Step 1393900 | Avg Loss: 0.0149 | Grad Norm: 0.00746016\n",
      "Epoch 2 | Step 1394000 | Avg Loss: 0.0149 | Grad Norm: 0.01065877\n",
      "Epoch 2 | Step 1394100 | Avg Loss: 0.0147 | Grad Norm: 0.00827862\n",
      "Epoch 2 | Step 1394200 | Avg Loss: 0.0148 | Grad Norm: 0.00722224\n",
      "Epoch 2 | Step 1394300 | Avg Loss: 0.0152 | Grad Norm: 0.00818115\n",
      "Epoch 2 | Step 1394400 | Avg Loss: 0.0151 | Grad Norm: 0.00818302\n",
      "Epoch 2 | Step 1394500 | Avg Loss: 0.0152 | Grad Norm: 0.00841634\n",
      "Epoch 2 | Step 1394600 | Avg Loss: 0.0154 | Grad Norm: 0.00789668\n",
      "Epoch 2 | Step 1394700 | Avg Loss: 0.0151 | Grad Norm: 0.00958252\n",
      "Epoch 2 | Step 1394800 | Avg Loss: 0.0152 | Grad Norm: 0.00804259\n",
      "Epoch 2 | Step 1394900 | Avg Loss: 0.0157 | Grad Norm: 0.00920016\n",
      "Epoch 2 | Step 1395000 | Avg Loss: 0.0154 | Grad Norm: 0.00850553\n",
      "Epoch 2 | Step 1395100 | Avg Loss: 0.0151 | Grad Norm: 0.00697595\n",
      "Epoch 2 | Step 1395200 | Avg Loss: 0.0150 | Grad Norm: 0.00985021\n",
      "Epoch 2 | Step 1395300 | Avg Loss: 0.0152 | Grad Norm: 0.00795106\n",
      "Epoch 2 | Step 1395400 | Avg Loss: 0.0152 | Grad Norm: 0.00855466\n",
      "Epoch 2 | Step 1395500 | Avg Loss: 0.0155 | Grad Norm: 0.00803800\n",
      "Epoch 2 | Step 1395600 | Avg Loss: 0.0152 | Grad Norm: 0.00859056\n",
      "Epoch 2 | Step 1395700 | Avg Loss: 0.0151 | Grad Norm: 0.00783436\n",
      "Epoch 2 | Step 1395800 | Avg Loss: 0.0152 | Grad Norm: 0.00838436\n",
      "Epoch 2 | Step 1395900 | Avg Loss: 0.0153 | Grad Norm: 0.00793117\n",
      "Epoch 2 | Step 1396000 | Avg Loss: 0.0150 | Grad Norm: 0.00825933\n",
      "Epoch 2 | Step 1396100 | Avg Loss: 0.0150 | Grad Norm: 0.00793166\n",
      "Epoch 2 | Step 1396200 | Avg Loss: 0.0153 | Grad Norm: 0.01126162\n",
      "Epoch 2 | Step 1396300 | Avg Loss: 0.0151 | Grad Norm: 0.01015344\n",
      "Epoch 2 | Step 1396400 | Avg Loss: 0.0149 | Grad Norm: 0.00845466\n",
      "Epoch 2 | Step 1396500 | Avg Loss: 0.0151 | Grad Norm: 0.00929239\n",
      "Epoch 2 | Step 1396600 | Avg Loss: 0.0151 | Grad Norm: 0.01029868\n",
      "Epoch 2 | Step 1396700 | Avg Loss: 0.0147 | Grad Norm: 0.00834020\n",
      "Epoch 2 | Step 1396800 | Avg Loss: 0.0147 | Grad Norm: 0.00794931\n",
      "Epoch 2 | Step 1396900 | Avg Loss: 0.0150 | Grad Norm: 0.00911594\n",
      "Epoch 2 | Step 1397000 | Avg Loss: 0.0149 | Grad Norm: 0.00894376\n",
      "Epoch 2 | Step 1397100 | Avg Loss: 0.0151 | Grad Norm: 0.00728980\n",
      "Epoch 2 | Step 1397200 | Avg Loss: 0.0153 | Grad Norm: 0.00825961\n",
      "Epoch 2 | Step 1397300 | Avg Loss: 0.0153 | Grad Norm: 0.00806992\n",
      "Epoch 2 | Step 1397400 | Avg Loss: 0.0151 | Grad Norm: 0.00832974\n",
      "Epoch 2 | Step 1397500 | Avg Loss: 0.0151 | Grad Norm: 0.00863126\n",
      "Epoch 2 | Step 1397600 | Avg Loss: 0.0151 | Grad Norm: 0.00874042\n",
      "Epoch 2 | Step 1397700 | Avg Loss: 0.0150 | Grad Norm: 0.00732930\n",
      "Epoch 2 | Step 1397800 | Avg Loss: 0.0153 | Grad Norm: 0.00871508\n",
      "Epoch 2 | Step 1397900 | Avg Loss: 0.0154 | Grad Norm: 0.00824597\n",
      "Epoch 2 | Step 1398000 | Avg Loss: 0.0153 | Grad Norm: 0.01081093\n",
      "Epoch 2 | Step 1398100 | Avg Loss: 0.0152 | Grad Norm: 0.01034123\n",
      "Epoch 2 | Step 1398200 | Avg Loss: 0.0154 | Grad Norm: 0.00757200\n",
      "Epoch 2 | Step 1398300 | Avg Loss: 0.0153 | Grad Norm: 0.00837444\n",
      "Epoch 2 | Step 1398400 | Avg Loss: 0.0148 | Grad Norm: 0.00731415\n",
      "Epoch 2 | Step 1398500 | Avg Loss: 0.0149 | Grad Norm: 0.00832239\n",
      "Epoch 2 | Step 1398600 | Avg Loss: 0.0148 | Grad Norm: 0.00895655\n",
      "Epoch 2 | Step 1398700 | Avg Loss: 0.0149 | Grad Norm: 0.00882330\n",
      "Epoch 2 | Step 1398800 | Avg Loss: 0.0151 | Grad Norm: 0.00873749\n",
      "Epoch 2 | Step 1398900 | Avg Loss: 0.0152 | Grad Norm: 0.00760937\n",
      "Epoch 2 | Step 1399000 | Avg Loss: 0.0154 | Grad Norm: 0.00812759\n",
      "Epoch 2 | Step 1399100 | Avg Loss: 0.0153 | Grad Norm: 0.00824663\n",
      "Epoch 2 | Step 1399200 | Avg Loss: 0.0154 | Grad Norm: 0.00905998\n",
      "Epoch 2 | Step 1399300 | Avg Loss: 0.0149 | Grad Norm: 0.00861609\n",
      "Epoch 2 | Step 1399400 | Avg Loss: 0.0153 | Grad Norm: 0.00870862\n",
      "Epoch 2 | Step 1399500 | Avg Loss: 0.0148 | Grad Norm: 0.00777522\n",
      "Epoch 2 | Step 1399600 | Avg Loss: 0.0148 | Grad Norm: 0.00918070\n",
      "Epoch 2 | Step 1399700 | Avg Loss: 0.0147 | Grad Norm: 0.00802810\n",
      "Epoch 2 | Step 1399800 | Avg Loss: 0.0145 | Grad Norm: 0.00860591\n",
      "Epoch 2 | Step 1399900 | Avg Loss: 0.0144 | Grad Norm: 0.00708306\n",
      "Epoch 2 | Step 1400000 | Avg Loss: 0.0147 | Grad Norm: 0.00669957\n",
      "Saving model at step1400000\n",
      "Epoch 2 | Step 1400100 | Avg Loss: 0.0149 | Grad Norm: 0.00713086\n",
      "Epoch 2 | Step 1400200 | Avg Loss: 0.0149 | Grad Norm: 0.00826651\n",
      "Epoch 2 | Step 1400300 | Avg Loss: 0.0147 | Grad Norm: 0.00777366\n",
      "Epoch 2 | Step 1400400 | Avg Loss: 0.0147 | Grad Norm: 0.00731988\n",
      "Epoch 2 | Step 1400500 | Avg Loss: 0.0149 | Grad Norm: 0.00848506\n",
      "Epoch 2 | Step 1400600 | Avg Loss: 0.0150 | Grad Norm: 0.00696370\n",
      "Epoch 2 | Step 1400700 | Avg Loss: 0.0152 | Grad Norm: 0.00688679\n",
      "Epoch 2 | Step 1400800 | Avg Loss: 0.0152 | Grad Norm: 0.00788959\n",
      "Epoch 2 | Step 1400900 | Avg Loss: 0.0150 | Grad Norm: 0.00811584\n",
      "Epoch 2 | Step 1401000 | Avg Loss: 0.0154 | Grad Norm: 0.00832033\n",
      "Epoch 2 | Step 1401100 | Avg Loss: 0.0154 | Grad Norm: 0.00847783\n",
      "Epoch 2 | Step 1401200 | Avg Loss: 0.0152 | Grad Norm: 0.00848705\n",
      "Epoch 2 | Step 1401300 | Avg Loss: 0.0154 | Grad Norm: 0.00852089\n",
      "Epoch 2 | Step 1401400 | Avg Loss: 0.0154 | Grad Norm: 0.00790824\n",
      "Epoch 2 | Step 1401500 | Avg Loss: 0.0152 | Grad Norm: 0.00953972\n",
      "Epoch 2 | Step 1401600 | Avg Loss: 0.0154 | Grad Norm: 0.00919596\n",
      "Epoch 2 | Step 1401700 | Avg Loss: 0.0155 | Grad Norm: 0.00726324\n",
      "Epoch 2 | Step 1401800 | Avg Loss: 0.0152 | Grad Norm: 0.01002737\n",
      "Epoch 2 | Step 1401900 | Avg Loss: 0.0149 | Grad Norm: 0.00743420\n",
      "Epoch 2 | Step 1402000 | Avg Loss: 0.0153 | Grad Norm: 0.00742221\n",
      "Epoch 2 | Step 1402100 | Avg Loss: 0.0151 | Grad Norm: 0.00756409\n",
      "Epoch 2 | Step 1402200 | Avg Loss: 0.0152 | Grad Norm: 0.00838367\n",
      "Epoch 2 | Step 1402300 | Avg Loss: 0.0153 | Grad Norm: 0.00945581\n",
      "Epoch 2 | Step 1402400 | Avg Loss: 0.0149 | Grad Norm: 0.00831174\n",
      "Epoch 2 | Step 1402500 | Avg Loss: 0.0147 | Grad Norm: 0.00833978\n",
      "Epoch 2 | Step 1402600 | Avg Loss: 0.0145 | Grad Norm: 0.00804042\n",
      "Epoch 2 | Step 1402700 | Avg Loss: 0.0147 | Grad Norm: 0.00784548\n",
      "Epoch 2 | Step 1402800 | Avg Loss: 0.0145 | Grad Norm: 0.00790673\n",
      "Epoch 2 | Step 1402900 | Avg Loss: 0.0150 | Grad Norm: 0.00593926\n",
      "Epoch 2 | Step 1403000 | Avg Loss: 0.0152 | Grad Norm: 0.00887281\n",
      "Epoch 2 | Step 1403100 | Avg Loss: 0.0151 | Grad Norm: 0.01069023\n",
      "Epoch 2 | Step 1403200 | Avg Loss: 0.0151 | Grad Norm: 0.00824256\n",
      "Epoch 2 | Step 1403300 | Avg Loss: 0.0148 | Grad Norm: 0.00978366\n",
      "Epoch 2 | Step 1403400 | Avg Loss: 0.0148 | Grad Norm: 0.00863953\n",
      "Epoch 2 | Step 1403500 | Avg Loss: 0.0147 | Grad Norm: 0.01298555\n",
      "Epoch 2 | Step 1403600 | Avg Loss: 0.0145 | Grad Norm: 0.00862004\n",
      "Epoch 2 | Step 1403700 | Avg Loss: 0.0149 | Grad Norm: 0.00824569\n",
      "Epoch 2 | Step 1403800 | Avg Loss: 0.0151 | Grad Norm: 0.00877988\n",
      "Epoch 2 | Step 1403900 | Avg Loss: 0.0151 | Grad Norm: 0.00833343\n",
      "Epoch 2 | Step 1404000 | Avg Loss: 0.0145 | Grad Norm: 0.00897794\n",
      "Epoch 2 | Step 1404100 | Avg Loss: 0.0147 | Grad Norm: 0.00856414\n",
      "Epoch 2 | Step 1404200 | Avg Loss: 0.0146 | Grad Norm: 0.00803285\n",
      "Epoch 2 | Step 1404300 | Avg Loss: 0.0148 | Grad Norm: 0.00826722\n",
      "Epoch 2 | Step 1404400 | Avg Loss: 0.0144 | Grad Norm: 0.00749873\n",
      "Epoch 2 | Step 1404500 | Avg Loss: 0.0147 | Grad Norm: 0.00810714\n",
      "Epoch 2 | Step 1404600 | Avg Loss: 0.0146 | Grad Norm: 0.00742455\n",
      "Epoch 2 | Step 1404700 | Avg Loss: 0.0143 | Grad Norm: 0.00710105\n",
      "Epoch 2 | Step 1404800 | Avg Loss: 0.0142 | Grad Norm: 0.00723291\n",
      "Epoch 2 | Step 1404900 | Avg Loss: 0.0141 | Grad Norm: 0.00770661\n",
      "Epoch 2 | Step 1405000 | Avg Loss: 0.0146 | Grad Norm: 0.00708020\n",
      "Epoch 2 | Step 1405100 | Avg Loss: 0.0152 | Grad Norm: 0.00794371\n",
      "Epoch 2 | Step 1405200 | Avg Loss: 0.0150 | Grad Norm: 0.00761596\n",
      "Epoch 2 | Step 1405300 | Avg Loss: 0.0142 | Grad Norm: 0.00844173\n",
      "Epoch 2 | Step 1405400 | Avg Loss: 0.0146 | Grad Norm: 0.00739807\n",
      "Epoch 2 | Step 1405500 | Avg Loss: 0.0149 | Grad Norm: 0.00989579\n",
      "Epoch 2 | Step 1405600 | Avg Loss: 0.0149 | Grad Norm: 0.00764158\n",
      "Epoch 2 | Step 1405700 | Avg Loss: 0.0150 | Grad Norm: 0.00746444\n",
      "Epoch 2 | Step 1405800 | Avg Loss: 0.0150 | Grad Norm: 0.00845223\n",
      "Epoch 2 | Step 1405900 | Avg Loss: 0.0149 | Grad Norm: 0.00862704\n",
      "Epoch 2 | Step 1406000 | Avg Loss: 0.0149 | Grad Norm: 0.00834171\n",
      "Epoch 2 | Step 1406100 | Avg Loss: 0.0149 | Grad Norm: 0.00850896\n",
      "Epoch 2 | Step 1406200 | Avg Loss: 0.0148 | Grad Norm: 0.00890935\n",
      "Epoch 2 | Step 1406300 | Avg Loss: 0.0144 | Grad Norm: 0.00792199\n",
      "Epoch 2 | Step 1406400 | Avg Loss: 0.0143 | Grad Norm: 0.00902549\n",
      "Epoch 2 | Step 1406500 | Avg Loss: 0.0141 | Grad Norm: 0.00843286\n",
      "Epoch 2 | Step 1406600 | Avg Loss: 0.0141 | Grad Norm: 0.00795063\n",
      "Epoch 2 | Step 1406700 | Avg Loss: 0.0144 | Grad Norm: 0.00967729\n",
      "Epoch 2 | Step 1406800 | Avg Loss: 0.0148 | Grad Norm: 0.00839257\n",
      "Epoch 2 | Step 1406900 | Avg Loss: 0.0147 | Grad Norm: 0.00843403\n",
      "Epoch 2 | Step 1407000 | Avg Loss: 0.0147 | Grad Norm: 0.00929025\n",
      "Epoch 2 | Step 1407100 | Avg Loss: 0.0146 | Grad Norm: 0.00872833\n",
      "Epoch 2 | Step 1407200 | Avg Loss: 0.0145 | Grad Norm: 0.00908636\n",
      "Epoch 2 | Step 1407300 | Avg Loss: 0.0146 | Grad Norm: 0.01024771\n",
      "Epoch 2 | Step 1407400 | Avg Loss: 0.0151 | Grad Norm: 0.00726586\n",
      "Epoch 2 | Step 1407500 | Avg Loss: 0.0147 | Grad Norm: 0.00825101\n",
      "Epoch 2 | Step 1407600 | Avg Loss: 0.0150 | Grad Norm: 0.00771590\n",
      "Epoch 2 | Step 1407700 | Avg Loss: 0.0152 | Grad Norm: 0.00795113\n",
      "Epoch 2 | Step 1407800 | Avg Loss: 0.0150 | Grad Norm: 0.00768525\n",
      "Epoch 2 | Step 1407900 | Avg Loss: 0.0152 | Grad Norm: 0.00863932\n",
      "Epoch 2 | Step 1408000 | Avg Loss: 0.0150 | Grad Norm: 0.01086765\n",
      "Epoch 2 | Step 1408100 | Avg Loss: 0.0151 | Grad Norm: 0.00955053\n",
      "Epoch 2 | Step 1408200 | Avg Loss: 0.0152 | Grad Norm: 0.00944814\n",
      "Epoch 2 | Step 1408300 | Avg Loss: 0.0152 | Grad Norm: 0.00745758\n",
      "Epoch 2 | Step 1408400 | Avg Loss: 0.0147 | Grad Norm: 0.00905313\n",
      "Epoch 2 | Step 1408500 | Avg Loss: 0.0149 | Grad Norm: 0.00856803\n",
      "Epoch 2 | Step 1408600 | Avg Loss: 0.0150 | Grad Norm: 0.00916764\n",
      "Epoch 2 | Step 1408700 | Avg Loss: 0.0151 | Grad Norm: 0.00890017\n",
      "Epoch 2 | Step 1408800 | Avg Loss: 0.0147 | Grad Norm: 0.00915418\n",
      "Epoch 2 | Step 1408900 | Avg Loss: 0.0150 | Grad Norm: 0.00934062\n",
      "Epoch 2 | Step 1409000 | Avg Loss: 0.0150 | Grad Norm: 0.00839714\n",
      "Epoch 2 | Step 1409100 | Avg Loss: 0.0151 | Grad Norm: 0.00774222\n",
      "Epoch 2 | Step 1409200 | Avg Loss: 0.0154 | Grad Norm: 0.00987269\n",
      "Epoch 2 | Step 1409300 | Avg Loss: 0.0148 | Grad Norm: 0.00778390\n",
      "Epoch 2 | Step 1409400 | Avg Loss: 0.0147 | Grad Norm: 0.00832297\n",
      "Epoch 2 | Step 1409500 | Avg Loss: 0.0147 | Grad Norm: 0.00890588\n",
      "Epoch 2 | Step 1409600 | Avg Loss: 0.0142 | Grad Norm: 0.00806643\n",
      "Epoch 2 | Step 1409700 | Avg Loss: 0.0145 | Grad Norm: 0.00886732\n",
      "Epoch 2 | Step 1409800 | Avg Loss: 0.0152 | Grad Norm: 0.00772652\n",
      "Epoch 2 | Step 1409900 | Avg Loss: 0.0151 | Grad Norm: 0.00801008\n",
      "Epoch 2 | Step 1410000 | Avg Loss: 0.0151 | Grad Norm: 0.00727592\n",
      "Epoch 2 | Step 1410100 | Avg Loss: 0.0149 | Grad Norm: 0.00920401\n",
      "Epoch 2 | Step 1410200 | Avg Loss: 0.0147 | Grad Norm: 0.00848396\n",
      "Epoch 2 | Step 1410300 | Avg Loss: 0.0148 | Grad Norm: 0.00872181\n",
      "Epoch 2 | Step 1410400 | Avg Loss: 0.0147 | Grad Norm: 0.00918197\n",
      "Epoch 2 | Step 1410500 | Avg Loss: 0.0147 | Grad Norm: 0.00847966\n",
      "Epoch 2 | Step 1410600 | Avg Loss: 0.0149 | Grad Norm: 0.00806113\n",
      "Epoch 2 | Step 1410700 | Avg Loss: 0.0148 | Grad Norm: 0.00811196\n",
      "Epoch 2 | Step 1410800 | Avg Loss: 0.0151 | Grad Norm: 0.00729012\n",
      "Epoch 2 | Step 1410900 | Avg Loss: 0.0152 | Grad Norm: 0.00865589\n",
      "Epoch 2 | Step 1411000 | Avg Loss: 0.0156 | Grad Norm: 0.00861591\n",
      "Epoch 2 | Step 1411100 | Avg Loss: 0.0153 | Grad Norm: 0.00791042\n",
      "Epoch 2 | Step 1411200 | Avg Loss: 0.0151 | Grad Norm: 0.00819870\n",
      "Epoch 2 | Step 1411300 | Avg Loss: 0.0150 | Grad Norm: 0.00823531\n",
      "Epoch 2 | Step 1411400 | Avg Loss: 0.0150 | Grad Norm: 0.00751391\n",
      "Epoch 2 | Step 1411500 | Avg Loss: 0.0151 | Grad Norm: 0.00918171\n",
      "Epoch 2 | Step 1411600 | Avg Loss: 0.0150 | Grad Norm: 0.00762654\n",
      "Epoch 2 | Step 1411700 | Avg Loss: 0.0153 | Grad Norm: 0.00860464\n",
      "Epoch 2 | Step 1411800 | Avg Loss: 0.0152 | Grad Norm: 0.00837565\n",
      "Epoch 2 | Step 1411900 | Avg Loss: 0.0149 | Grad Norm: 0.00740879\n",
      "Epoch 2 | Step 1412000 | Avg Loss: 0.0151 | Grad Norm: 0.00842179\n",
      "Epoch 2 | Step 1412100 | Avg Loss: 0.0152 | Grad Norm: 0.00985330\n",
      "Epoch 2 | Step 1412200 | Avg Loss: 0.0153 | Grad Norm: 0.00936485\n",
      "Epoch 2 | Step 1412300 | Avg Loss: 0.0156 | Grad Norm: 0.01152403\n",
      "Epoch 2 | Step 1412400 | Avg Loss: 0.0153 | Grad Norm: 0.00814451\n",
      "Epoch 2 | Step 1412500 | Avg Loss: 0.0152 | Grad Norm: 0.00766967\n",
      "Epoch 2 | Step 1412600 | Avg Loss: 0.0153 | Grad Norm: 0.00720484\n",
      "Epoch 2 | Step 1412700 | Avg Loss: 0.0156 | Grad Norm: 0.00846402\n",
      "Epoch 2 | Step 1412800 | Avg Loss: 0.0155 | Grad Norm: 0.00800573\n",
      "Epoch 2 | Step 1412900 | Avg Loss: 0.0155 | Grad Norm: 0.00845443\n",
      "Epoch 2 | Step 1413000 | Avg Loss: 0.0151 | Grad Norm: 0.00887250\n",
      "Epoch 2 | Step 1413100 | Avg Loss: 0.0153 | Grad Norm: 0.00848055\n",
      "Epoch 2 | Step 1413200 | Avg Loss: 0.0152 | Grad Norm: 0.00830383\n",
      "Epoch 2 | Step 1413300 | Avg Loss: 0.0151 | Grad Norm: 0.00895454\n",
      "Epoch 2 | Step 1413400 | Avg Loss: 0.0149 | Grad Norm: 0.00760769\n",
      "Epoch 2 | Step 1413500 | Avg Loss: 0.0149 | Grad Norm: 0.00832101\n",
      "Epoch 2 | Step 1413600 | Avg Loss: 0.0150 | Grad Norm: 0.00919978\n",
      "Epoch 2 | Step 1413700 | Avg Loss: 0.0145 | Grad Norm: 0.00677299\n",
      "Epoch 2 | Step 1413800 | Avg Loss: 0.0149 | Grad Norm: 0.00783299\n",
      "Epoch 2 | Step 1413900 | Avg Loss: 0.0150 | Grad Norm: 0.00915292\n",
      "Epoch 2 | Step 1414000 | Avg Loss: 0.0153 | Grad Norm: 0.00869166\n",
      "Epoch 2 | Step 1414100 | Avg Loss: 0.0153 | Grad Norm: 0.00765518\n",
      "Epoch 2 | Step 1414200 | Avg Loss: 0.0149 | Grad Norm: 0.00768732\n",
      "Epoch 2 | Step 1414300 | Avg Loss: 0.0150 | Grad Norm: 0.00815345\n",
      "Epoch 2 | Step 1414400 | Avg Loss: 0.0145 | Grad Norm: 0.00985883\n",
      "Epoch 2 | Step 1414500 | Avg Loss: 0.0147 | Grad Norm: 0.00712609\n",
      "Epoch 2 | Step 1414600 | Avg Loss: 0.0148 | Grad Norm: 0.00940937\n",
      "Epoch 2 | Step 1414700 | Avg Loss: 0.0150 | Grad Norm: 0.00692236\n",
      "Epoch 2 | Step 1414800 | Avg Loss: 0.0149 | Grad Norm: 0.00825596\n",
      "Epoch 2 | Step 1414900 | Avg Loss: 0.0148 | Grad Norm: 0.00706204\n",
      "Epoch 2 | Step 1415000 | Avg Loss: 0.0147 | Grad Norm: 0.00812301\n",
      "Epoch 2 | Step 1415100 | Avg Loss: 0.0147 | Grad Norm: 0.00855523\n",
      "Epoch 2 | Step 1415200 | Avg Loss: 0.0146 | Grad Norm: 0.00764794\n",
      "Epoch 2 | Step 1415300 | Avg Loss: 0.0146 | Grad Norm: 0.00730319\n",
      "Epoch 2 | Step 1415400 | Avg Loss: 0.0148 | Grad Norm: 0.00833662\n",
      "Epoch 2 | Step 1415500 | Avg Loss: 0.0149 | Grad Norm: 0.00810882\n",
      "Epoch 2 | Step 1415600 | Avg Loss: 0.0149 | Grad Norm: 0.00874239\n",
      "Epoch 2 | Step 1415700 | Avg Loss: 0.0150 | Grad Norm: 0.00845625\n",
      "Epoch 2 | Step 1415800 | Avg Loss: 0.0151 | Grad Norm: 0.00815059\n",
      "Epoch 2 | Step 1415900 | Avg Loss: 0.0148 | Grad Norm: 0.00875838\n",
      "Epoch 2 | Step 1416000 | Avg Loss: 0.0150 | Grad Norm: 0.00864121\n",
      "Epoch 2 | Step 1416100 | Avg Loss: 0.0154 | Grad Norm: 0.00889186\n",
      "Epoch 2 | Step 1416200 | Avg Loss: 0.0152 | Grad Norm: 0.00759166\n",
      "Epoch 2 | Step 1416300 | Avg Loss: 0.0151 | Grad Norm: 0.00939289\n",
      "Epoch 2 | Step 1416400 | Avg Loss: 0.0151 | Grad Norm: 0.00747543\n",
      "Epoch 2 | Step 1416500 | Avg Loss: 0.0151 | Grad Norm: 0.00893841\n",
      "Epoch 2 | Step 1416600 | Avg Loss: 0.0153 | Grad Norm: 0.00999756\n",
      "Epoch 2 | Step 1416700 | Avg Loss: 0.0146 | Grad Norm: 0.00810566\n",
      "Epoch 2 | Step 1416800 | Avg Loss: 0.0151 | Grad Norm: 0.00829043\n",
      "Epoch 2 | Step 1416900 | Avg Loss: 0.0157 | Grad Norm: 0.00813280\n",
      "Epoch 2 | Step 1417000 | Avg Loss: 0.0152 | Grad Norm: 0.00801449\n",
      "Epoch 2 | Step 1417100 | Avg Loss: 0.0150 | Grad Norm: 0.01175656\n",
      "Epoch 2 | Step 1417200 | Avg Loss: 0.0153 | Grad Norm: 0.00841003\n",
      "Epoch 2 | Step 1417300 | Avg Loss: 0.0155 | Grad Norm: 0.00955403\n",
      "Epoch 2 | Step 1417400 | Avg Loss: 0.0154 | Grad Norm: 0.00805639\n",
      "Epoch 2 | Step 1417500 | Avg Loss: 0.0153 | Grad Norm: 0.00952836\n",
      "Epoch 2 | Step 1417600 | Avg Loss: 0.0154 | Grad Norm: 0.00795607\n",
      "Epoch 2 | Step 1417700 | Avg Loss: 0.0152 | Grad Norm: 0.00689325\n",
      "Epoch 2 | Step 1417800 | Avg Loss: 0.0152 | Grad Norm: 0.00998098\n",
      "Epoch 2 | Step 1417900 | Avg Loss: 0.0149 | Grad Norm: 0.00725990\n",
      "Epoch 2 | Step 1418000 | Avg Loss: 0.0149 | Grad Norm: 0.00707601\n",
      "Epoch 2 | Step 1418100 | Avg Loss: 0.0148 | Grad Norm: 0.00739389\n",
      "Epoch 2 | Step 1418200 | Avg Loss: 0.0149 | Grad Norm: 0.00987190\n",
      "Epoch 2 | Step 1418300 | Avg Loss: 0.0149 | Grad Norm: 0.00731315\n",
      "Epoch 2 | Step 1418400 | Avg Loss: 0.0148 | Grad Norm: 0.00727753\n",
      "Epoch 2 | Step 1418500 | Avg Loss: 0.0146 | Grad Norm: 0.00758757\n",
      "Epoch 2 | Step 1418600 | Avg Loss: 0.0146 | Grad Norm: 0.00853051\n",
      "Epoch 2 | Step 1418700 | Avg Loss: 0.0149 | Grad Norm: 0.00810978\n",
      "Epoch 2 | Step 1418800 | Avg Loss: 0.0147 | Grad Norm: 0.00778236\n",
      "Epoch 2 | Step 1418900 | Avg Loss: 0.0152 | Grad Norm: 0.00805005\n",
      "Epoch 2 | Step 1419000 | Avg Loss: 0.0152 | Grad Norm: 0.00789516\n",
      "Epoch 2 | Step 1419100 | Avg Loss: 0.0150 | Grad Norm: 0.00919239\n",
      "Epoch 2 | Step 1419200 | Avg Loss: 0.0152 | Grad Norm: 0.00886422\n",
      "Epoch 2 | Step 1419300 | Avg Loss: 0.0152 | Grad Norm: 0.00905268\n",
      "Epoch 2 | Step 1419400 | Avg Loss: 0.0149 | Grad Norm: 0.00861110\n",
      "Epoch 2 | Step 1419500 | Avg Loss: 0.0148 | Grad Norm: 0.00845421\n",
      "Epoch 2 | Step 1419600 | Avg Loss: 0.0151 | Grad Norm: 0.00883136\n",
      "Epoch 2 | Step 1419700 | Avg Loss: 0.0151 | Grad Norm: 0.00850215\n",
      "Epoch 2 | Step 1419800 | Avg Loss: 0.0152 | Grad Norm: 0.00846614\n",
      "Epoch 2 | Step 1419900 | Avg Loss: 0.0147 | Grad Norm: 0.00816955\n",
      "Epoch 2 | Step 1420000 | Avg Loss: 0.0148 | Grad Norm: 0.00833510\n",
      "Epoch 2 | Step 1420100 | Avg Loss: 0.0150 | Grad Norm: 0.00762793\n",
      "Epoch 2 | Step 1420200 | Avg Loss: 0.0152 | Grad Norm: 0.01006356\n",
      "Epoch 2 | Step 1420300 | Avg Loss: 0.0150 | Grad Norm: 0.00912072\n",
      "Epoch 2 | Step 1420400 | Avg Loss: 0.0151 | Grad Norm: 0.00775944\n",
      "Epoch 2 | Step 1420500 | Avg Loss: 0.0152 | Grad Norm: 0.00809970\n",
      "Epoch 2 | Step 1420600 | Avg Loss: 0.0149 | Grad Norm: 0.00801485\n",
      "Epoch 2 | Step 1420700 | Avg Loss: 0.0155 | Grad Norm: 0.00790791\n",
      "Epoch 2 | Step 1420800 | Avg Loss: 0.0154 | Grad Norm: 0.00775938\n",
      "Epoch 2 | Step 1420900 | Avg Loss: 0.0152 | Grad Norm: 0.00984633\n",
      "Epoch 2 | Step 1421000 | Avg Loss: 0.0153 | Grad Norm: 0.01019105\n",
      "Epoch 2 | Step 1421100 | Avg Loss: 0.0154 | Grad Norm: 0.01053752\n",
      "Epoch 2 | Step 1421200 | Avg Loss: 0.0153 | Grad Norm: 0.00857563\n",
      "Epoch 2 | Step 1421300 | Avg Loss: 0.0155 | Grad Norm: 0.00763771\n",
      "Epoch 2 | Step 1421400 | Avg Loss: 0.0149 | Grad Norm: 0.00851242\n",
      "Epoch 2 | Step 1421500 | Avg Loss: 0.0150 | Grad Norm: 0.00810732\n",
      "Epoch 2 | Step 1421600 | Avg Loss: 0.0147 | Grad Norm: 0.00711893\n",
      "Epoch 2 | Step 1421700 | Avg Loss: 0.0147 | Grad Norm: 0.00818059\n",
      "Epoch 2 | Step 1421800 | Avg Loss: 0.0149 | Grad Norm: 0.00831849\n",
      "Epoch 2 | Step 1421900 | Avg Loss: 0.0147 | Grad Norm: 0.00917414\n",
      "Epoch 2 | Step 1422000 | Avg Loss: 0.0149 | Grad Norm: 0.00732574\n",
      "Epoch 2 | Step 1422100 | Avg Loss: 0.0144 | Grad Norm: 0.00702489\n",
      "Epoch 2 | Step 1422200 | Avg Loss: 0.0146 | Grad Norm: 0.00864748\n",
      "Epoch 2 | Step 1422300 | Avg Loss: 0.0148 | Grad Norm: 0.00746121\n",
      "Epoch 2 | Step 1422400 | Avg Loss: 0.0148 | Grad Norm: 0.00860150\n",
      "Epoch 2 | Step 1422500 | Avg Loss: 0.0148 | Grad Norm: 0.00969793\n",
      "Epoch 2 | Step 1422600 | Avg Loss: 0.0150 | Grad Norm: 0.00887121\n",
      "Epoch 2 | Step 1422700 | Avg Loss: 0.0153 | Grad Norm: 0.00858188\n",
      "Epoch 2 | Step 1422800 | Avg Loss: 0.0150 | Grad Norm: 0.00736562\n",
      "Epoch 2 | Step 1422900 | Avg Loss: 0.0147 | Grad Norm: 0.00819167\n",
      "Epoch 2 | Step 1423000 | Avg Loss: 0.0144 | Grad Norm: 0.00987857\n",
      "Epoch 2 | Step 1423100 | Avg Loss: 0.0146 | Grad Norm: 0.00809948\n",
      "Epoch 2 | Step 1423200 | Avg Loss: 0.0148 | Grad Norm: 0.00762171\n",
      "Epoch 2 | Step 1423300 | Avg Loss: 0.0152 | Grad Norm: 0.00713412\n",
      "Epoch 2 | Step 1423400 | Avg Loss: 0.0151 | Grad Norm: 0.01066397\n",
      "Epoch 2 | Step 1423500 | Avg Loss: 0.0154 | Grad Norm: 0.00677655\n",
      "Epoch 2 | Step 1423600 | Avg Loss: 0.0152 | Grad Norm: 0.00778967\n",
      "Epoch 2 | Step 1423700 | Avg Loss: 0.0151 | Grad Norm: 0.00844818\n",
      "Epoch 2 | Step 1423800 | Avg Loss: 0.0151 | Grad Norm: 0.00788649\n",
      "Epoch 2 | Step 1423900 | Avg Loss: 0.0148 | Grad Norm: 0.00835355\n",
      "Epoch 2 | Step 1424000 | Avg Loss: 0.0150 | Grad Norm: 0.00775068\n",
      "Epoch 2 | Step 1424100 | Avg Loss: 0.0149 | Grad Norm: 0.00806710\n",
      "Epoch 2 | Step 1424200 | Avg Loss: 0.0148 | Grad Norm: 0.00770613\n",
      "Epoch 2 | Step 1424300 | Avg Loss: 0.0151 | Grad Norm: 0.01067604\n",
      "Epoch 2 | Step 1424400 | Avg Loss: 0.0149 | Grad Norm: 0.00770445\n",
      "Epoch 2 | Step 1424500 | Avg Loss: 0.0150 | Grad Norm: 0.00849164\n",
      "Epoch 2 | Step 1424600 | Avg Loss: 0.0149 | Grad Norm: 0.00906028\n",
      "Epoch 2 | Step 1424700 | Avg Loss: 0.0147 | Grad Norm: 0.00783918\n",
      "Epoch 2 | Step 1424800 | Avg Loss: 0.0151 | Grad Norm: 0.00750863\n",
      "Epoch 2 | Step 1424900 | Avg Loss: 0.0150 | Grad Norm: 0.00742899\n",
      "Epoch 2 | Step 1425000 | Avg Loss: 0.0153 | Grad Norm: 0.00778112\n",
      "Epoch 2 | Step 1425100 | Avg Loss: 0.0149 | Grad Norm: 0.00728773\n",
      "Epoch 2 | Step 1425200 | Avg Loss: 0.0151 | Grad Norm: 0.00834982\n",
      "Epoch 2 | Step 1425300 | Avg Loss: 0.0153 | Grad Norm: 0.00909911\n",
      "Epoch 2 | Step 1425400 | Avg Loss: 0.0150 | Grad Norm: 0.00758071\n",
      "Epoch 2 | Step 1425500 | Avg Loss: 0.0148 | Grad Norm: 0.00882088\n",
      "Epoch 2 | Step 1425600 | Avg Loss: 0.0149 | Grad Norm: 0.00747742\n",
      "Epoch 2 | Step 1425700 | Avg Loss: 0.0149 | Grad Norm: 0.00879218\n",
      "Epoch 2 | Step 1425800 | Avg Loss: 0.0149 | Grad Norm: 0.00880177\n",
      "Epoch 2 | Step 1425900 | Avg Loss: 0.0152 | Grad Norm: 0.00851328\n",
      "Epoch 2 | Step 1426000 | Avg Loss: 0.0150 | Grad Norm: 0.00760821\n",
      "Epoch 2 | Step 1426100 | Avg Loss: 0.0149 | Grad Norm: 0.00766828\n",
      "Epoch 2 | Step 1426200 | Avg Loss: 0.0151 | Grad Norm: 0.00924489\n",
      "Epoch 2 | Step 1426300 | Avg Loss: 0.0148 | Grad Norm: 0.00657039\n",
      "Epoch 2 | Step 1426400 | Avg Loss: 0.0149 | Grad Norm: 0.00890027\n",
      "Epoch 2 | Step 1426500 | Avg Loss: 0.0145 | Grad Norm: 0.00785376\n",
      "Epoch 2 | Step 1426600 | Avg Loss: 0.0145 | Grad Norm: 0.00815226\n",
      "Epoch 2 | Step 1426700 | Avg Loss: 0.0144 | Grad Norm: 0.00781639\n",
      "Epoch 2 | Step 1426800 | Avg Loss: 0.0145 | Grad Norm: 0.00935931\n",
      "Epoch 2 | Step 1426900 | Avg Loss: 0.0145 | Grad Norm: 0.00959763\n",
      "Epoch 2 | Step 1427000 | Avg Loss: 0.0147 | Grad Norm: 0.00835179\n",
      "Epoch 2 | Step 1427100 | Avg Loss: 0.0145 | Grad Norm: 0.00814556\n",
      "Epoch 2 | Step 1427200 | Avg Loss: 0.0146 | Grad Norm: 0.00886590\n",
      "Epoch 2 | Step 1427300 | Avg Loss: 0.0147 | Grad Norm: 0.00888161\n",
      "Epoch 2 | Step 1427400 | Avg Loss: 0.0146 | Grad Norm: 0.01035942\n",
      "Epoch 2 | Step 1427500 | Avg Loss: 0.0149 | Grad Norm: 0.00805616\n",
      "Epoch 2 | Step 1427600 | Avg Loss: 0.0146 | Grad Norm: 0.00842306\n",
      "Epoch 2 | Step 1427700 | Avg Loss: 0.0147 | Grad Norm: 0.00838848\n",
      "Epoch 2 | Step 1427800 | Avg Loss: 0.0146 | Grad Norm: 0.00735950\n",
      "Epoch 2 | Step 1427900 | Avg Loss: 0.0151 | Grad Norm: 0.00680921\n",
      "Epoch 2 | Step 1428000 | Avg Loss: 0.0146 | Grad Norm: 0.00710288\n",
      "Epoch 2 | Step 1428100 | Avg Loss: 0.0145 | Grad Norm: 0.00879396\n",
      "Epoch 2 | Step 1428200 | Avg Loss: 0.0145 | Grad Norm: 0.01468168\n",
      "Epoch 2 | Step 1428300 | Avg Loss: 0.0150 | Grad Norm: 0.00910657\n",
      "Epoch 2 | Step 1428400 | Avg Loss: 0.0151 | Grad Norm: 0.00833581\n",
      "Epoch 2 | Step 1428500 | Avg Loss: 0.0149 | Grad Norm: 0.00807444\n",
      "Epoch 2 | Step 1428600 | Avg Loss: 0.0151 | Grad Norm: 0.00847398\n",
      "Epoch 2 | Step 1428700 | Avg Loss: 0.0154 | Grad Norm: 0.00858180\n",
      "Epoch 2 | Step 1428800 | Avg Loss: 0.0150 | Grad Norm: 0.01098342\n",
      "Epoch 2 | Step 1428900 | Avg Loss: 0.0148 | Grad Norm: 0.00866426\n",
      "Epoch 2 | Step 1429000 | Avg Loss: 0.0146 | Grad Norm: 0.00824167\n",
      "Epoch 2 | Step 1429100 | Avg Loss: 0.0147 | Grad Norm: 0.00870899\n",
      "Epoch 2 | Step 1429200 | Avg Loss: 0.0150 | Grad Norm: 0.00840340\n",
      "Epoch 2 | Step 1429300 | Avg Loss: 0.0147 | Grad Norm: 0.00714203\n",
      "Epoch 2 | Step 1429400 | Avg Loss: 0.0150 | Grad Norm: 0.00840118\n",
      "Epoch 2 | Step 1429500 | Avg Loss: 0.0150 | Grad Norm: 0.00786687\n",
      "Epoch 2 | Step 1429600 | Avg Loss: 0.0151 | Grad Norm: 0.00932595\n",
      "Epoch 2 | Step 1429700 | Avg Loss: 0.0149 | Grad Norm: 0.00785152\n",
      "Epoch 2 | Step 1429800 | Avg Loss: 0.0149 | Grad Norm: 0.00785919\n",
      "Epoch 2 | Step 1429900 | Avg Loss: 0.0147 | Grad Norm: 0.00815512\n",
      "Epoch 2 | Step 1430000 | Avg Loss: 0.0149 | Grad Norm: 0.00816978\n",
      "Epoch 2 | Step 1430100 | Avg Loss: 0.0149 | Grad Norm: 0.00798643\n",
      "Epoch 2 | Step 1430200 | Avg Loss: 0.0150 | Grad Norm: 0.00919790\n",
      "Epoch 2 | Step 1430300 | Avg Loss: 0.0154 | Grad Norm: 0.00741994\n",
      "Epoch 2 | Step 1430400 | Avg Loss: 0.0153 | Grad Norm: 0.00892218\n",
      "Epoch 2 | Step 1430500 | Avg Loss: 0.0153 | Grad Norm: 0.00930614\n",
      "Epoch 2 | Step 1430600 | Avg Loss: 0.0154 | Grad Norm: 0.00980873\n",
      "Epoch 2 | Step 1430700 | Avg Loss: 0.0157 | Grad Norm: 0.00756822\n",
      "Epoch 2 | Step 1430800 | Avg Loss: 0.0157 | Grad Norm: 0.00944087\n",
      "Epoch 2 | Step 1430900 | Avg Loss: 0.0153 | Grad Norm: 0.00720907\n",
      "Epoch 2 | Step 1431000 | Avg Loss: 0.0153 | Grad Norm: 0.00784916\n",
      "Epoch 2 | Step 1431100 | Avg Loss: 0.0149 | Grad Norm: 0.00825734\n",
      "Epoch 2 | Step 1431200 | Avg Loss: 0.0151 | Grad Norm: 0.00784434\n",
      "Epoch 2 | Step 1431300 | Avg Loss: 0.0148 | Grad Norm: 0.00694509\n",
      "Epoch 2 | Step 1431400 | Avg Loss: 0.0147 | Grad Norm: 0.00836762\n",
      "Epoch 2 | Step 1431500 | Avg Loss: 0.0146 | Grad Norm: 0.00883243\n",
      "Epoch 2 | Step 1431600 | Avg Loss: 0.0149 | Grad Norm: 0.01141738\n",
      "Epoch 2 | Step 1431700 | Avg Loss: 0.0148 | Grad Norm: 0.00816648\n",
      "Epoch 2 | Step 1431800 | Avg Loss: 0.0154 | Grad Norm: 0.00835882\n",
      "Epoch 2 | Step 1431900 | Avg Loss: 0.0156 | Grad Norm: 0.00820704\n",
      "Epoch 2 | Step 1432000 | Avg Loss: 0.0154 | Grad Norm: 0.00766804\n",
      "Epoch 2 | Step 1432100 | Avg Loss: 0.0154 | Grad Norm: 0.00941640\n",
      "Epoch 2 | Step 1432200 | Avg Loss: 0.0152 | Grad Norm: 0.00845718\n",
      "Epoch 2 | Step 1432300 | Avg Loss: 0.0155 | Grad Norm: 0.01038790\n",
      "Epoch 2 | Step 1432400 | Avg Loss: 0.0157 | Grad Norm: 0.00779860\n",
      "Epoch 2 | Step 1432500 | Avg Loss: 0.0156 | Grad Norm: 0.00761554\n",
      "Epoch 2 | Step 1432600 | Avg Loss: 0.0151 | Grad Norm: 0.00841101\n",
      "Epoch 2 | Step 1432700 | Avg Loss: 0.0150 | Grad Norm: 0.00752464\n",
      "Epoch 2 | Step 1432800 | Avg Loss: 0.0150 | Grad Norm: 0.00721730\n",
      "Epoch 2 | Step 1432900 | Avg Loss: 0.0148 | Grad Norm: 0.00867598\n",
      "Epoch 2 | Step 1433000 | Avg Loss: 0.0151 | Grad Norm: 0.00724476\n",
      "Epoch 2 | Step 1433100 | Avg Loss: 0.0151 | Grad Norm: 0.00759583\n",
      "Epoch 2 | Step 1433200 | Avg Loss: 0.0147 | Grad Norm: 0.00825174\n",
      "Epoch 2 | Step 1433300 | Avg Loss: 0.0144 | Grad Norm: 0.00761356\n",
      "Epoch 2 | Step 1433400 | Avg Loss: 0.0147 | Grad Norm: 0.00800227\n",
      "Epoch 2 | Step 1433500 | Avg Loss: 0.0146 | Grad Norm: 0.00788637\n",
      "Epoch 2 | Step 1433600 | Avg Loss: 0.0149 | Grad Norm: 0.00968785\n",
      "Epoch 2 | Step 1433700 | Avg Loss: 0.0152 | Grad Norm: 0.00782481\n",
      "Epoch 2 | Step 1433800 | Avg Loss: 0.0151 | Grad Norm: 0.00794162\n",
      "Epoch 2 | Step 1433900 | Avg Loss: 0.0149 | Grad Norm: 0.00775327\n",
      "Epoch 2 | Step 1434000 | Avg Loss: 0.0151 | Grad Norm: 0.00707459\n",
      "Epoch 2 | Step 1434100 | Avg Loss: 0.0148 | Grad Norm: 0.00704799\n",
      "Epoch 2 | Step 1434200 | Avg Loss: 0.0148 | Grad Norm: 0.00879328\n",
      "Epoch 2 | Step 1434300 | Avg Loss: 0.0148 | Grad Norm: 0.00731619\n",
      "Epoch 2 | Step 1434400 | Avg Loss: 0.0151 | Grad Norm: 0.00702413\n",
      "Epoch 2 | Step 1434500 | Avg Loss: 0.0152 | Grad Norm: 0.00741433\n",
      "Epoch 2 | Step 1434600 | Avg Loss: 0.0150 | Grad Norm: 0.00799467\n",
      "Epoch 2 | Step 1434700 | Avg Loss: 0.0154 | Grad Norm: 0.00825539\n",
      "Epoch 2 | Step 1434800 | Avg Loss: 0.0151 | Grad Norm: 0.00771218\n",
      "Epoch 2 | Step 1434900 | Avg Loss: 0.0149 | Grad Norm: 0.00779628\n",
      "Epoch 2 | Step 1435000 | Avg Loss: 0.0151 | Grad Norm: 0.00942236\n",
      "Epoch 2 | Step 1435100 | Avg Loss: 0.0153 | Grad Norm: 0.00794030\n",
      "Epoch 2 | Step 1435200 | Avg Loss: 0.0150 | Grad Norm: 0.00704692\n",
      "Epoch 2 | Step 1435300 | Avg Loss: 0.0146 | Grad Norm: 0.00854254\n",
      "Epoch 2 | Step 1435400 | Avg Loss: 0.0148 | Grad Norm: 0.00881752\n",
      "Epoch 2 | Step 1435500 | Avg Loss: 0.0144 | Grad Norm: 0.00762678\n",
      "Epoch 2 | Step 1435600 | Avg Loss: 0.0149 | Grad Norm: 0.00853760\n",
      "Epoch 2 | Step 1435700 | Avg Loss: 0.0153 | Grad Norm: 0.00832694\n",
      "Epoch 2 | Step 1435800 | Avg Loss: 0.0149 | Grad Norm: 0.00968234\n",
      "Epoch 2 | Step 1435900 | Avg Loss: 0.0151 | Grad Norm: 0.00878686\n",
      "Epoch 2 | Step 1436000 | Avg Loss: 0.0154 | Grad Norm: 0.00749875\n",
      "Epoch 2 | Step 1436100 | Avg Loss: 0.0155 | Grad Norm: 0.00756548\n",
      "Epoch 2 | Step 1436200 | Avg Loss: 0.0150 | Grad Norm: 0.00818169\n",
      "Epoch 2 | Step 1436300 | Avg Loss: 0.0147 | Grad Norm: 0.00965715\n",
      "Epoch 2 | Step 1436400 | Avg Loss: 0.0149 | Grad Norm: 0.00911130\n",
      "Epoch 2 | Step 1436500 | Avg Loss: 0.0152 | Grad Norm: 0.00947991\n",
      "Epoch 2 | Step 1436600 | Avg Loss: 0.0153 | Grad Norm: 0.00764237\n",
      "Epoch 2 | Step 1436700 | Avg Loss: 0.0153 | Grad Norm: 0.00815134\n",
      "Epoch 2 | Step 1436800 | Avg Loss: 0.0152 | Grad Norm: 0.00947690\n",
      "Epoch 2 | Step 1436900 | Avg Loss: 0.0152 | Grad Norm: 0.00726126\n",
      "Epoch 2 | Step 1437000 | Avg Loss: 0.0149 | Grad Norm: 0.00855765\n",
      "Epoch 2 | Step 1437100 | Avg Loss: 0.0151 | Grad Norm: 0.00891315\n",
      "Epoch 2 | Step 1437200 | Avg Loss: 0.0153 | Grad Norm: 0.00777835\n",
      "Epoch 2 | Step 1437300 | Avg Loss: 0.0154 | Grad Norm: 0.00858580\n",
      "Epoch 2 | Step 1437400 | Avg Loss: 0.0153 | Grad Norm: 0.00799644\n",
      "Epoch 2 | Step 1437500 | Avg Loss: 0.0151 | Grad Norm: 0.00771099\n",
      "Epoch 2 | Step 1437600 | Avg Loss: 0.0151 | Grad Norm: 0.00855613\n",
      "Epoch 2 | Step 1437700 | Avg Loss: 0.0144 | Grad Norm: 0.00811352\n",
      "Epoch 2 | Step 1437800 | Avg Loss: 0.0147 | Grad Norm: 0.00870648\n",
      "Epoch 2 | Step 1437900 | Avg Loss: 0.0152 | Grad Norm: 0.00771742\n",
      "Epoch 2 | Step 1438000 | Avg Loss: 0.0158 | Grad Norm: 0.00971945\n",
      "Epoch 2 | Step 1438100 | Avg Loss: 0.0155 | Grad Norm: 0.00819392\n",
      "Epoch 2 | Step 1438200 | Avg Loss: 0.0152 | Grad Norm: 0.00789479\n",
      "Epoch 2 | Step 1438300 | Avg Loss: 0.0149 | Grad Norm: 0.00880393\n",
      "Epoch 2 | Step 1438400 | Avg Loss: 0.0149 | Grad Norm: 0.00783552\n",
      "Epoch 2 | Step 1438500 | Avg Loss: 0.0149 | Grad Norm: 0.00893110\n",
      "Epoch 2 | Step 1438600 | Avg Loss: 0.0150 | Grad Norm: 0.00873752\n",
      "Epoch 2 | Step 1438700 | Avg Loss: 0.0150 | Grad Norm: 0.00826555\n",
      "Epoch 2 | Step 1438800 | Avg Loss: 0.0150 | Grad Norm: 0.00837805\n",
      "Epoch 2 | Step 1438900 | Avg Loss: 0.0149 | Grad Norm: 0.00773375\n",
      "Epoch 2 | Step 1439000 | Avg Loss: 0.0152 | Grad Norm: 0.00798184\n",
      "Epoch 2 | Step 1439100 | Avg Loss: 0.0151 | Grad Norm: 0.00830047\n",
      "Epoch 2 | Step 1439200 | Avg Loss: 0.0147 | Grad Norm: 0.00904947\n",
      "Epoch 2 | Step 1439300 | Avg Loss: 0.0148 | Grad Norm: 0.00776791\n",
      "Epoch 2 | Step 1439400 | Avg Loss: 0.0148 | Grad Norm: 0.00795016\n",
      "Epoch 2 | Step 1439500 | Avg Loss: 0.0148 | Grad Norm: 0.00837271\n",
      "Epoch 2 | Step 1439600 | Avg Loss: 0.0144 | Grad Norm: 0.01089223\n",
      "Epoch 2 | Step 1439700 | Avg Loss: 0.0142 | Grad Norm: 0.00707226\n",
      "Epoch 2 | Step 1439800 | Avg Loss: 0.0143 | Grad Norm: 0.00796434\n",
      "Epoch 2 | Step 1439900 | Avg Loss: 0.0143 | Grad Norm: 0.00734724\n",
      "Epoch 2 | Step 1440000 | Avg Loss: 0.0145 | Grad Norm: 0.00809597\n",
      "Epoch 2 | Step 1440100 | Avg Loss: 0.0146 | Grad Norm: 0.00877759\n",
      "Epoch 2 | Step 1440200 | Avg Loss: 0.0145 | Grad Norm: 0.00805468\n",
      "Epoch 2 | Step 1440300 | Avg Loss: 0.0147 | Grad Norm: 0.00843974\n",
      "Epoch 2 | Step 1440400 | Avg Loss: 0.0153 | Grad Norm: 0.00810068\n",
      "Epoch 2 | Step 1440500 | Avg Loss: 0.0155 | Grad Norm: 0.00768718\n",
      "Epoch 2 | Step 1440600 | Avg Loss: 0.0160 | Grad Norm: 0.00775996\n",
      "Epoch 2 | Step 1440700 | Avg Loss: 0.0154 | Grad Norm: 0.00789294\n",
      "Epoch 2 | Step 1440800 | Avg Loss: 0.0149 | Grad Norm: 0.00774470\n",
      "Epoch 2 | Step 1440900 | Avg Loss: 0.0150 | Grad Norm: 0.00853953\n",
      "Epoch 2 | Step 1441000 | Avg Loss: 0.0149 | Grad Norm: 0.00892286\n",
      "Epoch 2 | Step 1441100 | Avg Loss: 0.0155 | Grad Norm: 0.00780405\n",
      "Epoch 2 | Step 1441200 | Avg Loss: 0.0155 | Grad Norm: 0.00854476\n",
      "Epoch 2 | Step 1441300 | Avg Loss: 0.0156 | Grad Norm: 0.00815732\n",
      "Epoch 2 | Step 1441400 | Avg Loss: 0.0158 | Grad Norm: 0.00724608\n",
      "Epoch 2 | Step 1441500 | Avg Loss: 0.0151 | Grad Norm: 0.00818632\n",
      "Epoch 2 | Step 1441600 | Avg Loss: 0.0152 | Grad Norm: 0.01259639\n",
      "Epoch 2 | Step 1441700 | Avg Loss: 0.0152 | Grad Norm: 0.00731046\n",
      "Epoch 2 | Step 1441800 | Avg Loss: 0.0153 | Grad Norm: 0.00857953\n",
      "Epoch 2 | Step 1441900 | Avg Loss: 0.0151 | Grad Norm: 0.00904262\n",
      "Epoch 2 | Step 1442000 | Avg Loss: 0.0152 | Grad Norm: 0.00921662\n",
      "Epoch 2 | Step 1442100 | Avg Loss: 0.0150 | Grad Norm: 0.00828351\n",
      "Epoch 2 | Step 1442200 | Avg Loss: 0.0154 | Grad Norm: 0.00901993\n",
      "Epoch 2 | Step 1442300 | Avg Loss: 0.0152 | Grad Norm: 0.00895334\n",
      "Epoch 2 | Step 1442400 | Avg Loss: 0.0155 | Grad Norm: 0.00815477\n",
      "Epoch 2 | Step 1442500 | Avg Loss: 0.0154 | Grad Norm: 0.00987713\n",
      "Epoch 2 | Step 1442600 | Avg Loss: 0.0149 | Grad Norm: 0.00962595\n",
      "Epoch 2 | Step 1442700 | Avg Loss: 0.0152 | Grad Norm: 0.00888347\n",
      "Epoch 2 | Step 1442800 | Avg Loss: 0.0150 | Grad Norm: 0.00844525\n",
      "Epoch 2 | Step 1442900 | Avg Loss: 0.0148 | Grad Norm: 0.00807136\n",
      "Epoch 2 | Step 1443000 | Avg Loss: 0.0146 | Grad Norm: 0.00970036\n",
      "Epoch 2 | Step 1443100 | Avg Loss: 0.0143 | Grad Norm: 0.01128555\n",
      "Epoch 2 | Step 1443200 | Avg Loss: 0.0142 | Grad Norm: 0.00771743\n",
      "Epoch 2 | Step 1443300 | Avg Loss: 0.0147 | Grad Norm: 0.00790404\n",
      "Epoch 2 | Step 1443400 | Avg Loss: 0.0150 | Grad Norm: 0.00747957\n",
      "Epoch 2 | Step 1443500 | Avg Loss: 0.0148 | Grad Norm: 0.00702844\n",
      "Epoch 2 | Step 1443600 | Avg Loss: 0.0146 | Grad Norm: 0.00758961\n",
      "Epoch 2 | Step 1443700 | Avg Loss: 0.0147 | Grad Norm: 0.00868274\n",
      "Epoch 2 | Step 1443800 | Avg Loss: 0.0148 | Grad Norm: 0.00799919\n",
      "Epoch 2 | Step 1443900 | Avg Loss: 0.0144 | Grad Norm: 0.00941196\n",
      "Epoch 2 | Step 1444000 | Avg Loss: 0.0145 | Grad Norm: 0.00729770\n",
      "Epoch 2 | Step 1444100 | Avg Loss: 0.0147 | Grad Norm: 0.00832217\n",
      "Epoch 2 | Step 1444200 | Avg Loss: 0.0149 | Grad Norm: 0.00697484\n",
      "Epoch 2 | Step 1444300 | Avg Loss: 0.0146 | Grad Norm: 0.00863600\n",
      "Epoch 2 | Step 1444400 | Avg Loss: 0.0147 | Grad Norm: 0.00890385\n",
      "Epoch 2 | Step 1444500 | Avg Loss: 0.0150 | Grad Norm: 0.00839346\n",
      "Epoch 2 | Step 1444600 | Avg Loss: 0.0152 | Grad Norm: 0.00845124\n",
      "Epoch 2 | Step 1444700 | Avg Loss: 0.0153 | Grad Norm: 0.00889119\n",
      "Epoch 2 | Step 1444800 | Avg Loss: 0.0155 | Grad Norm: 0.00893814\n",
      "Epoch 2 | Step 1444900 | Avg Loss: 0.0154 | Grad Norm: 0.00860939\n",
      "Epoch 2 | Step 1445000 | Avg Loss: 0.0159 | Grad Norm: 0.00818835\n",
      "Epoch 2 | Step 1445100 | Avg Loss: 0.0154 | Grad Norm: 0.00888661\n",
      "Epoch 2 | Step 1445200 | Avg Loss: 0.0156 | Grad Norm: 0.00715901\n",
      "Epoch 2 | Step 1445300 | Avg Loss: 0.0155 | Grad Norm: 0.00987312\n",
      "Epoch 2 | Step 1445400 | Avg Loss: 0.0151 | Grad Norm: 0.00973917\n",
      "Epoch 2 | Step 1445500 | Avg Loss: 0.0144 | Grad Norm: 0.00806887\n",
      "Epoch 2 | Step 1445600 | Avg Loss: 0.0146 | Grad Norm: 0.00867797\n",
      "Epoch 2 | Step 1445700 | Avg Loss: 0.0141 | Grad Norm: 0.00777648\n",
      "Epoch 2 | Step 1445800 | Avg Loss: 0.0140 | Grad Norm: 0.00906830\n",
      "Epoch 2 | Step 1445900 | Avg Loss: 0.0143 | Grad Norm: 0.00748143\n",
      "Epoch 2 | Step 1446000 | Avg Loss: 0.0143 | Grad Norm: 0.00781428\n",
      "Epoch 2 | Step 1446100 | Avg Loss: 0.0143 | Grad Norm: 0.00800915\n",
      "Epoch 2 | Step 1446200 | Avg Loss: 0.0142 | Grad Norm: 0.00913756\n",
      "Epoch 2 | Step 1446300 | Avg Loss: 0.0142 | Grad Norm: 0.00849594\n",
      "Epoch 2 | Step 1446400 | Avg Loss: 0.0143 | Grad Norm: 0.00802716\n",
      "Epoch 2 | Step 1446500 | Avg Loss: 0.0150 | Grad Norm: 0.00854494\n",
      "Epoch 2 | Step 1446600 | Avg Loss: 0.0150 | Grad Norm: 0.00719969\n",
      "Epoch 2 | Step 1446700 | Avg Loss: 0.0147 | Grad Norm: 0.00956013\n",
      "Epoch 2 | Step 1446800 | Avg Loss: 0.0148 | Grad Norm: 0.00821528\n",
      "Epoch 2 | Step 1446900 | Avg Loss: 0.0150 | Grad Norm: 0.00805756\n",
      "Epoch 2 | Step 1447000 | Avg Loss: 0.0151 | Grad Norm: 0.00883963\n",
      "Epoch 2 | Step 1447100 | Avg Loss: 0.0154 | Grad Norm: 0.00771662\n",
      "Epoch 2 | Step 1447200 | Avg Loss: 0.0152 | Grad Norm: 0.00899885\n",
      "Epoch 2 | Step 1447300 | Avg Loss: 0.0150 | Grad Norm: 0.01055076\n",
      "Epoch 2 | Step 1447400 | Avg Loss: 0.0149 | Grad Norm: 0.00781865\n",
      "Epoch 2 | Step 1447500 | Avg Loss: 0.0150 | Grad Norm: 0.00682988\n",
      "Epoch 2 | Step 1447600 | Avg Loss: 0.0150 | Grad Norm: 0.00827255\n",
      "Epoch 2 | Step 1447700 | Avg Loss: 0.0153 | Grad Norm: 0.00771282\n",
      "Epoch 2 | Step 1447800 | Avg Loss: 0.0152 | Grad Norm: 0.01148726\n",
      "Epoch 2 | Step 1447900 | Avg Loss: 0.0149 | Grad Norm: 0.00797476\n",
      "Epoch 2 | Step 1448000 | Avg Loss: 0.0152 | Grad Norm: 0.00878352\n",
      "Epoch 2 | Step 1448100 | Avg Loss: 0.0154 | Grad Norm: 0.00928827\n",
      "Epoch 2 | Step 1448200 | Avg Loss: 0.0154 | Grad Norm: 0.00761660\n",
      "Epoch 2 | Step 1448300 | Avg Loss: 0.0151 | Grad Norm: 0.00819796\n",
      "Epoch 2 | Step 1448400 | Avg Loss: 0.0154 | Grad Norm: 0.00869192\n",
      "Epoch 2 | Step 1448500 | Avg Loss: 0.0153 | Grad Norm: 0.00798707\n",
      "Epoch 2 | Step 1448600 | Avg Loss: 0.0149 | Grad Norm: 0.00801613\n",
      "Epoch 2 | Step 1448700 | Avg Loss: 0.0149 | Grad Norm: 0.00900928\n",
      "Epoch 2 | Step 1448800 | Avg Loss: 0.0151 | Grad Norm: 0.00798162\n",
      "Epoch 2 | Step 1448900 | Avg Loss: 0.0151 | Grad Norm: 0.00856348\n",
      "Epoch 2 | Step 1449000 | Avg Loss: 0.0154 | Grad Norm: 0.00721894\n",
      "Epoch 2 | Step 1449100 | Avg Loss: 0.0152 | Grad Norm: 0.00762473\n",
      "Epoch 2 | Step 1449200 | Avg Loss: 0.0148 | Grad Norm: 0.00871640\n",
      "Epoch 2 | Step 1449300 | Avg Loss: 0.0145 | Grad Norm: 0.00803737\n",
      "Epoch 2 | Step 1449400 | Avg Loss: 0.0147 | Grad Norm: 0.00792921\n",
      "Epoch 2 | Step 1449500 | Avg Loss: 0.0146 | Grad Norm: 0.00789155\n",
      "Epoch 2 | Step 1449600 | Avg Loss: 0.0149 | Grad Norm: 0.00755091\n",
      "Epoch 2 | Step 1449700 | Avg Loss: 0.0153 | Grad Norm: 0.00789367\n",
      "Epoch 2 | Step 1449800 | Avg Loss: 0.0151 | Grad Norm: 0.01156345\n",
      "Epoch 2 | Step 1449900 | Avg Loss: 0.0153 | Grad Norm: 0.00751295\n",
      "Epoch 2 | Step 1450000 | Avg Loss: 0.0153 | Grad Norm: 0.00771685\n",
      "Epoch 2 | Step 1450100 | Avg Loss: 0.0151 | Grad Norm: 0.00723718\n",
      "Epoch 2 | Step 1450200 | Avg Loss: 0.0153 | Grad Norm: 0.00843501\n",
      "Epoch 2 | Step 1450300 | Avg Loss: 0.0152 | Grad Norm: 0.00859323\n",
      "Epoch 2 | Step 1450400 | Avg Loss: 0.0152 | Grad Norm: 0.00754288\n",
      "Epoch 2 | Step 1450500 | Avg Loss: 0.0153 | Grad Norm: 0.00793466\n",
      "Epoch 2 | Step 1450600 | Avg Loss: 0.0154 | Grad Norm: 0.00954879\n",
      "Epoch 2 | Step 1450700 | Avg Loss: 0.0151 | Grad Norm: 0.00864065\n",
      "Epoch 2 | Step 1450800 | Avg Loss: 0.0154 | Grad Norm: 0.00784515\n",
      "Epoch 2 | Step 1450900 | Avg Loss: 0.0156 | Grad Norm: 0.00872436\n",
      "Epoch 2 | Step 1451000 | Avg Loss: 0.0156 | Grad Norm: 0.00982345\n",
      "Epoch 2 | Step 1451100 | Avg Loss: 0.0154 | Grad Norm: 0.01052395\n",
      "Epoch 2 | Step 1451200 | Avg Loss: 0.0154 | Grad Norm: 0.00988872\n",
      "Epoch 2 | Step 1451300 | Avg Loss: 0.0153 | Grad Norm: 0.01129989\n",
      "Epoch 2 | Step 1451400 | Avg Loss: 0.0153 | Grad Norm: 0.00883462\n",
      "Epoch 2 | Step 1451500 | Avg Loss: 0.0152 | Grad Norm: 0.01040986\n",
      "Epoch 2 | Step 1451600 | Avg Loss: 0.0154 | Grad Norm: 0.00859196\n",
      "Epoch 2 | Step 1451700 | Avg Loss: 0.0153 | Grad Norm: 0.00729501\n",
      "Epoch 2 | Step 1451800 | Avg Loss: 0.0154 | Grad Norm: 0.00827448\n",
      "Epoch 2 | Step 1451900 | Avg Loss: 0.0151 | Grad Norm: 0.00773137\n",
      "Epoch 2 | Step 1452000 | Avg Loss: 0.0150 | Grad Norm: 0.00773970\n",
      "Epoch 2 | Step 1452100 | Avg Loss: 0.0150 | Grad Norm: 0.00735084\n",
      "Epoch 2 | Step 1452200 | Avg Loss: 0.0148 | Grad Norm: 0.00688991\n",
      "Epoch 2 | Step 1452300 | Avg Loss: 0.0148 | Grad Norm: 0.00947929\n",
      "Epoch 2 | Step 1452400 | Avg Loss: 0.0148 | Grad Norm: 0.01098295\n",
      "Epoch 2 | Step 1452500 | Avg Loss: 0.0146 | Grad Norm: 0.00744204\n",
      "Epoch 2 | Step 1452600 | Avg Loss: 0.0144 | Grad Norm: 0.00795294\n",
      "Epoch 2 | Step 1452700 | Avg Loss: 0.0143 | Grad Norm: 0.00640735\n",
      "Epoch 2 | Step 1452800 | Avg Loss: 0.0144 | Grad Norm: 0.00766165\n",
      "Epoch 2 | Step 1452900 | Avg Loss: 0.0149 | Grad Norm: 0.00740535\n",
      "Epoch 2 | Step 1453000 | Avg Loss: 0.0147 | Grad Norm: 0.00861706\n",
      "Epoch 2 | Step 1453100 | Avg Loss: 0.0149 | Grad Norm: 0.00918652\n",
      "Epoch 2 | Step 1453200 | Avg Loss: 0.0153 | Grad Norm: 0.00705620\n",
      "Epoch 2 | Step 1453300 | Avg Loss: 0.0149 | Grad Norm: 0.00747941\n",
      "Epoch 2 | Step 1453400 | Avg Loss: 0.0154 | Grad Norm: 0.00763312\n",
      "Epoch 2 | Step 1453500 | Avg Loss: 0.0154 | Grad Norm: 0.00862943\n",
      "Epoch 2 | Step 1453600 | Avg Loss: 0.0151 | Grad Norm: 0.01111736\n",
      "Epoch 2 | Step 1453700 | Avg Loss: 0.0148 | Grad Norm: 0.00837748\n",
      "Epoch 2 | Step 1453800 | Avg Loss: 0.0150 | Grad Norm: 0.01067539\n",
      "Epoch 2 | Step 1453900 | Avg Loss: 0.0150 | Grad Norm: 0.00724381\n",
      "Epoch 2 | Step 1454000 | Avg Loss: 0.0151 | Grad Norm: 0.00737755\n",
      "Epoch 2 | Step 1454100 | Avg Loss: 0.0155 | Grad Norm: 0.01075637\n",
      "Epoch 2 | Step 1454200 | Avg Loss: 0.0155 | Grad Norm: 0.00719557\n",
      "Epoch 2 | Step 1454300 | Avg Loss: 0.0152 | Grad Norm: 0.00879019\n",
      "Epoch 2 | Step 1454400 | Avg Loss: 0.0150 | Grad Norm: 0.00824684\n",
      "Epoch 2 | Step 1454500 | Avg Loss: 0.0148 | Grad Norm: 0.00827111\n",
      "Epoch 2 | Step 1454600 | Avg Loss: 0.0148 | Grad Norm: 0.00692407\n",
      "Epoch 2 | Step 1454700 | Avg Loss: 0.0151 | Grad Norm: 0.00868494\n",
      "Epoch 2 | Step 1454800 | Avg Loss: 0.0148 | Grad Norm: 0.00926413\n",
      "Epoch 2 | Step 1454900 | Avg Loss: 0.0150 | Grad Norm: 0.00804683\n",
      "Epoch 2 | Step 1455000 | Avg Loss: 0.0153 | Grad Norm: 0.00700104\n",
      "Epoch 2 | Step 1455100 | Avg Loss: 0.0154 | Grad Norm: 0.00811679\n",
      "Epoch 2 | Step 1455200 | Avg Loss: 0.0146 | Grad Norm: 0.00688071\n",
      "Epoch 2 | Step 1455300 | Avg Loss: 0.0151 | Grad Norm: 0.00797540\n",
      "Epoch 2 | Step 1455400 | Avg Loss: 0.0150 | Grad Norm: 0.00835089\n",
      "Epoch 2 | Step 1455500 | Avg Loss: 0.0152 | Grad Norm: 0.00771189\n",
      "Epoch 2 | Step 1455600 | Avg Loss: 0.0152 | Grad Norm: 0.00764650\n",
      "Epoch 2 | Step 1455700 | Avg Loss: 0.0150 | Grad Norm: 0.00873470\n",
      "Epoch 2 | Step 1455800 | Avg Loss: 0.0149 | Grad Norm: 0.00946056\n",
      "Epoch 2 | Step 1455900 | Avg Loss: 0.0148 | Grad Norm: 0.00867419\n",
      "Epoch 2 | Step 1456000 | Avg Loss: 0.0151 | Grad Norm: 0.00922670\n",
      "Epoch 2 | Step 1456100 | Avg Loss: 0.0151 | Grad Norm: 0.00778075\n",
      "Epoch 2 | Step 1456200 | Avg Loss: 0.0150 | Grad Norm: 0.00942048\n",
      "Epoch 2 | Step 1456300 | Avg Loss: 0.0147 | Grad Norm: 0.00868458\n",
      "Epoch 2 | Step 1456400 | Avg Loss: 0.0143 | Grad Norm: 0.00745735\n",
      "Epoch 2 | Step 1456500 | Avg Loss: 0.0143 | Grad Norm: 0.00868747\n",
      "Epoch 2 | Step 1456600 | Avg Loss: 0.0142 | Grad Norm: 0.00765986\n",
      "Epoch 2 | Step 1456700 | Avg Loss: 0.0143 | Grad Norm: 0.00737401\n",
      "Epoch 2 | Step 1456800 | Avg Loss: 0.0144 | Grad Norm: 0.00865523\n",
      "Epoch 2 | Step 1456900 | Avg Loss: 0.0144 | Grad Norm: 0.00777778\n",
      "Epoch 2 | Step 1457000 | Avg Loss: 0.0144 | Grad Norm: 0.00779009\n",
      "Epoch 2 | Step 1457100 | Avg Loss: 0.0147 | Grad Norm: 0.00899888\n",
      "Epoch 2 | Step 1457200 | Avg Loss: 0.0147 | Grad Norm: 0.00833330\n",
      "Epoch 2 | Step 1457300 | Avg Loss: 0.0149 | Grad Norm: 0.00898511\n",
      "Epoch 2 | Step 1457400 | Avg Loss: 0.0148 | Grad Norm: 0.00813734\n",
      "Epoch 2 | Step 1457500 | Avg Loss: 0.0152 | Grad Norm: 0.00955909\n",
      "Epoch 2 | Step 1457600 | Avg Loss: 0.0148 | Grad Norm: 0.00713775\n",
      "Epoch 2 | Step 1457700 | Avg Loss: 0.0143 | Grad Norm: 0.00703555\n",
      "Epoch 2 | Step 1457800 | Avg Loss: 0.0145 | Grad Norm: 0.00726596\n",
      "Epoch 2 | Step 1457900 | Avg Loss: 0.0148 | Grad Norm: 0.00862958\n",
      "Epoch 2 | Step 1458000 | Avg Loss: 0.0142 | Grad Norm: 0.00747588\n",
      "Epoch 2 | Step 1458100 | Avg Loss: 0.0147 | Grad Norm: 0.00782030\n",
      "Epoch 2 | Step 1458200 | Avg Loss: 0.0149 | Grad Norm: 0.00943561\n",
      "Epoch 2 | Step 1458300 | Avg Loss: 0.0150 | Grad Norm: 0.00878804\n",
      "Epoch 2 | Step 1458400 | Avg Loss: 0.0152 | Grad Norm: 0.01060931\n",
      "Epoch 2 | Step 1458500 | Avg Loss: 0.0152 | Grad Norm: 0.00782387\n",
      "Epoch 2 | Step 1458600 | Avg Loss: 0.0154 | Grad Norm: 0.00998721\n",
      "Epoch 2 | Step 1458700 | Avg Loss: 0.0155 | Grad Norm: 0.00884326\n",
      "Epoch 2 | Step 1458800 | Avg Loss: 0.0149 | Grad Norm: 0.00883304\n",
      "Epoch 2 | Step 1458900 | Avg Loss: 0.0149 | Grad Norm: 0.00721663\n",
      "Epoch 2 | Step 1459000 | Avg Loss: 0.0148 | Grad Norm: 0.00690944\n",
      "Epoch 2 | Step 1459100 | Avg Loss: 0.0146 | Grad Norm: 0.00745499\n",
      "Epoch 2 | Step 1459200 | Avg Loss: 0.0146 | Grad Norm: 0.00880335\n",
      "Epoch 2 | Step 1459300 | Avg Loss: 0.0151 | Grad Norm: 0.00797770\n",
      "Epoch 2 | Step 1459400 | Avg Loss: 0.0149 | Grad Norm: 0.00815917\n",
      "Epoch 2 | Step 1459500 | Avg Loss: 0.0150 | Grad Norm: 0.00764164\n",
      "Epoch 2 | Step 1459600 | Avg Loss: 0.0148 | Grad Norm: 0.00829231\n",
      "Epoch 2 | Step 1459700 | Avg Loss: 0.0147 | Grad Norm: 0.00855885\n",
      "Epoch 2 | Step 1459800 | Avg Loss: 0.0152 | Grad Norm: 0.00758258\n",
      "Epoch 2 | Step 1459900 | Avg Loss: 0.0158 | Grad Norm: 0.01017645\n",
      "Epoch 2 | Step 1460000 | Avg Loss: 0.0155 | Grad Norm: 0.00828955\n",
      "Epoch 2 | Step 1460100 | Avg Loss: 0.0156 | Grad Norm: 0.00741180\n",
      "Epoch 2 | Step 1460200 | Avg Loss: 0.0150 | Grad Norm: 0.00846382\n",
      "Epoch 2 | Step 1460300 | Avg Loss: 0.0150 | Grad Norm: 0.00876887\n",
      "Epoch 2 | Step 1460400 | Avg Loss: 0.0155 | Grad Norm: 0.00803831\n",
      "Epoch 2 | Step 1460500 | Avg Loss: 0.0157 | Grad Norm: 0.00702467\n",
      "Epoch 2 | Step 1460600 | Avg Loss: 0.0157 | Grad Norm: 0.00769006\n",
      "Epoch 2 | Step 1460700 | Avg Loss: 0.0152 | Grad Norm: 0.00761809\n",
      "Epoch 2 | Step 1460800 | Avg Loss: 0.0156 | Grad Norm: 0.01057181\n",
      "Epoch 2 | Step 1460900 | Avg Loss: 0.0153 | Grad Norm: 0.00962372\n",
      "Epoch 2 | Step 1461000 | Avg Loss: 0.0152 | Grad Norm: 0.00769110\n",
      "Epoch 2 | Step 1461100 | Avg Loss: 0.0154 | Grad Norm: 0.00894217\n",
      "Epoch 2 | Step 1461200 | Avg Loss: 0.0154 | Grad Norm: 0.00811263\n",
      "Epoch 2 | Step 1461300 | Avg Loss: 0.0155 | Grad Norm: 0.00838298\n",
      "Epoch 2 | Step 1461400 | Avg Loss: 0.0153 | Grad Norm: 0.00838955\n",
      "Epoch 2 | Step 1461500 | Avg Loss: 0.0156 | Grad Norm: 0.00858079\n",
      "Epoch 2 | Step 1461600 | Avg Loss: 0.0156 | Grad Norm: 0.00864050\n",
      "Epoch 2 | Step 1461700 | Avg Loss: 0.0157 | Grad Norm: 0.00883707\n",
      "Epoch 2 | Step 1461800 | Avg Loss: 0.0153 | Grad Norm: 0.00949403\n",
      "Epoch 2 | Step 1461900 | Avg Loss: 0.0153 | Grad Norm: 0.00840135\n",
      "Epoch 2 | Step 1462000 | Avg Loss: 0.0153 | Grad Norm: 0.00884773\n",
      "Epoch 2 | Step 1462100 | Avg Loss: 0.0152 | Grad Norm: 0.00814023\n",
      "Epoch 2 | Step 1462200 | Avg Loss: 0.0150 | Grad Norm: 0.00661685\n",
      "Epoch 2 | Step 1462300 | Avg Loss: 0.0149 | Grad Norm: 0.00807314\n",
      "Epoch 2 | Step 1462400 | Avg Loss: 0.0148 | Grad Norm: 0.00878362\n",
      "Epoch 2 | Step 1462500 | Avg Loss: 0.0149 | Grad Norm: 0.00760947\n",
      "Epoch 2 | Step 1462600 | Avg Loss: 0.0151 | Grad Norm: 0.00837773\n",
      "Epoch 2 | Step 1462700 | Avg Loss: 0.0152 | Grad Norm: 0.00720124\n",
      "Epoch 2 | Step 1462800 | Avg Loss: 0.0150 | Grad Norm: 0.00790961\n",
      "Epoch 2 | Step 1462900 | Avg Loss: 0.0149 | Grad Norm: 0.00738943\n",
      "Epoch 2 | Step 1463000 | Avg Loss: 0.0150 | Grad Norm: 0.00927658\n",
      "Epoch 2 | Step 1463100 | Avg Loss: 0.0147 | Grad Norm: 0.00803169\n",
      "Epoch 2 | Step 1463200 | Avg Loss: 0.0146 | Grad Norm: 0.00893579\n",
      "Epoch 2 | Step 1463300 | Avg Loss: 0.0148 | Grad Norm: 0.00827131\n",
      "Epoch 2 | Step 1463400 | Avg Loss: 0.0152 | Grad Norm: 0.00959667\n",
      "Epoch 2 | Step 1463500 | Avg Loss: 0.0152 | Grad Norm: 0.00746334\n",
      "Epoch 2 | Step 1463600 | Avg Loss: 0.0146 | Grad Norm: 0.00788645\n",
      "Epoch 2 | Step 1463700 | Avg Loss: 0.0144 | Grad Norm: 0.00724663\n",
      "Epoch 2 | Step 1463800 | Avg Loss: 0.0145 | Grad Norm: 0.00751281\n",
      "Epoch 2 | Step 1463900 | Avg Loss: 0.0146 | Grad Norm: 0.00796696\n",
      "Epoch 2 | Step 1464000 | Avg Loss: 0.0145 | Grad Norm: 0.00796051\n",
      "Epoch 2 | Step 1464100 | Avg Loss: 0.0147 | Grad Norm: 0.00790111\n",
      "Epoch 2 | Step 1464200 | Avg Loss: 0.0149 | Grad Norm: 0.01023865\n",
      "Epoch 2 | Step 1464300 | Avg Loss: 0.0152 | Grad Norm: 0.00971970\n",
      "Epoch 2 | Step 1464400 | Avg Loss: 0.0149 | Grad Norm: 0.00727284\n",
      "Epoch 2 | Step 1464500 | Avg Loss: 0.0147 | Grad Norm: 0.00847447\n",
      "Epoch 2 | Step 1464600 | Avg Loss: 0.0146 | Grad Norm: 0.00872352\n",
      "Epoch 2 | Step 1464700 | Avg Loss: 0.0139 | Grad Norm: 0.00789426\n",
      "Epoch 2 | Step 1464800 | Avg Loss: 0.0143 | Grad Norm: 0.00811369\n",
      "Epoch 2 | Step 1464900 | Avg Loss: 0.0141 | Grad Norm: 0.00867743\n",
      "Epoch 2 | Step 1465000 | Avg Loss: 0.0143 | Grad Norm: 0.00798246\n",
      "Epoch 2 | Step 1465100 | Avg Loss: 0.0144 | Grad Norm: 0.00733806\n",
      "Epoch 2 | Step 1465200 | Avg Loss: 0.0147 | Grad Norm: 0.00790740\n",
      "Epoch 2 | Step 1465300 | Avg Loss: 0.0147 | Grad Norm: 0.00874149\n",
      "Epoch 2 | Step 1465400 | Avg Loss: 0.0145 | Grad Norm: 0.01068637\n",
      "Epoch 2 | Step 1465500 | Avg Loss: 0.0148 | Grad Norm: 0.00914258\n",
      "Epoch 2 | Step 1465600 | Avg Loss: 0.0150 | Grad Norm: 0.00879097\n",
      "Epoch 2 | Step 1465700 | Avg Loss: 0.0150 | Grad Norm: 0.00720203\n",
      "Epoch 2 | Step 1465800 | Avg Loss: 0.0154 | Grad Norm: 0.00850375\n",
      "Epoch 2 | Step 1465900 | Avg Loss: 0.0154 | Grad Norm: 0.00976674\n",
      "Epoch 2 | Step 1466000 | Avg Loss: 0.0153 | Grad Norm: 0.00842316\n",
      "Epoch 2 | Step 1466100 | Avg Loss: 0.0156 | Grad Norm: 0.00777149\n",
      "Epoch 2 | Step 1466200 | Avg Loss: 0.0154 | Grad Norm: 0.00831389\n",
      "Epoch 2 | Step 1466300 | Avg Loss: 0.0151 | Grad Norm: 0.00794125\n",
      "Epoch 2 | Step 1466400 | Avg Loss: 0.0153 | Grad Norm: 0.00684030\n",
      "Epoch 2 | Step 1466500 | Avg Loss: 0.0155 | Grad Norm: 0.00908228\n",
      "Epoch 2 | Step 1466600 | Avg Loss: 0.0154 | Grad Norm: 0.00839880\n",
      "Epoch 2 | Step 1466700 | Avg Loss: 0.0154 | Grad Norm: 0.00829434\n",
      "Epoch 2 | Step 1466800 | Avg Loss: 0.0153 | Grad Norm: 0.00837151\n",
      "Epoch 2 | Step 1466900 | Avg Loss: 0.0155 | Grad Norm: 0.00911159\n",
      "Epoch 2 | Step 1467000 | Avg Loss: 0.0155 | Grad Norm: 0.00958521\n",
      "Epoch 2 | Step 1467100 | Avg Loss: 0.0151 | Grad Norm: 0.00718441\n",
      "Epoch 2 | Step 1467200 | Avg Loss: 0.0145 | Grad Norm: 0.00857624\n",
      "Epoch 2 | Step 1467300 | Avg Loss: 0.0147 | Grad Norm: 0.00841602\n",
      "Epoch 2 | Step 1467400 | Avg Loss: 0.0146 | Grad Norm: 0.00776260\n",
      "Epoch 2 | Step 1467500 | Avg Loss: 0.0146 | Grad Norm: 0.00846272\n",
      "Epoch 2 | Step 1467600 | Avg Loss: 0.0149 | Grad Norm: 0.00980913\n",
      "Epoch 2 | Step 1467700 | Avg Loss: 0.0151 | Grad Norm: 0.00929999\n",
      "Epoch 2 | Step 1467800 | Avg Loss: 0.0152 | Grad Norm: 0.00746600\n",
      "Epoch 2 | Step 1467900 | Avg Loss: 0.0149 | Grad Norm: 0.00832967\n",
      "Epoch 2 | Step 1468000 | Avg Loss: 0.0154 | Grad Norm: 0.00780906\n",
      "Epoch 2 | Step 1468100 | Avg Loss: 0.0154 | Grad Norm: 0.00978062\n",
      "Epoch 2 | Step 1468200 | Avg Loss: 0.0155 | Grad Norm: 0.00850852\n",
      "Epoch 2 | Step 1468300 | Avg Loss: 0.0156 | Grad Norm: 0.01028694\n",
      "Epoch 2 | Step 1468400 | Avg Loss: 0.0153 | Grad Norm: 0.00733159\n",
      "Epoch 2 | Step 1468500 | Avg Loss: 0.0154 | Grad Norm: 0.01018168\n",
      "Epoch 2 | Step 1468600 | Avg Loss: 0.0153 | Grad Norm: 0.00771999\n",
      "Epoch 2 | Step 1468700 | Avg Loss: 0.0155 | Grad Norm: 0.00769610\n",
      "Epoch 2 | Step 1468800 | Avg Loss: 0.0157 | Grad Norm: 0.00813568\n",
      "Epoch 2 | Step 1468900 | Avg Loss: 0.0156 | Grad Norm: 0.00905427\n",
      "Epoch 2 | Step 1469000 | Avg Loss: 0.0158 | Grad Norm: 0.00978948\n",
      "Epoch 2 | Step 1469100 | Avg Loss: 0.0158 | Grad Norm: 0.00933568\n",
      "Epoch 2 | Step 1469200 | Avg Loss: 0.0157 | Grad Norm: 0.00863083\n",
      "Epoch 2 | Step 1469300 | Avg Loss: 0.0153 | Grad Norm: 0.00764802\n",
      "Epoch 2 | Step 1469400 | Avg Loss: 0.0150 | Grad Norm: 0.00959981\n",
      "Epoch 2 | Step 1469500 | Avg Loss: 0.0155 | Grad Norm: 0.00810908\n",
      "Epoch 2 | Step 1469600 | Avg Loss: 0.0153 | Grad Norm: 0.00848629\n",
      "Epoch 2 | Step 1469700 | Avg Loss: 0.0153 | Grad Norm: 0.00793771\n",
      "Epoch 2 | Step 1469800 | Avg Loss: 0.0153 | Grad Norm: 0.00862905\n",
      "Epoch 2 | Step 1469900 | Avg Loss: 0.0151 | Grad Norm: 0.00863067\n",
      "Epoch 2 | Step 1470000 | Avg Loss: 0.0151 | Grad Norm: 0.01054348\n",
      "Epoch 2 | Step 1470100 | Avg Loss: 0.0150 | Grad Norm: 0.00805779\n",
      "Epoch 2 | Step 1470200 | Avg Loss: 0.0148 | Grad Norm: 0.00811751\n",
      "Epoch 2 | Step 1470300 | Avg Loss: 0.0146 | Grad Norm: 0.00735758\n",
      "Epoch 2 | Step 1470400 | Avg Loss: 0.0150 | Grad Norm: 0.00766358\n",
      "Epoch 2 | Step 1470500 | Avg Loss: 0.0147 | Grad Norm: 0.00804144\n",
      "Epoch 2 | Step 1470600 | Avg Loss: 0.0151 | Grad Norm: 0.00734765\n",
      "Epoch 2 | Step 1470700 | Avg Loss: 0.0150 | Grad Norm: 0.00855984\n",
      "Epoch 2 | Step 1470800 | Avg Loss: 0.0148 | Grad Norm: 0.01050663\n",
      "Epoch 2 | Step 1470900 | Avg Loss: 0.0152 | Grad Norm: 0.00708929\n",
      "Epoch 2 | Step 1471000 | Avg Loss: 0.0148 | Grad Norm: 0.00800784\n",
      "Epoch 2 | Step 1471100 | Avg Loss: 0.0144 | Grad Norm: 0.00699395\n",
      "Epoch 2 | Step 1471200 | Avg Loss: 0.0141 | Grad Norm: 0.00773176\n",
      "Epoch 2 | Step 1471300 | Avg Loss: 0.0142 | Grad Norm: 0.00778739\n",
      "Epoch 2 | Step 1471400 | Avg Loss: 0.0143 | Grad Norm: 0.00869159\n",
      "Epoch 2 | Step 1471500 | Avg Loss: 0.0144 | Grad Norm: 0.00755336\n",
      "Epoch 2 | Step 1471600 | Avg Loss: 0.0145 | Grad Norm: 0.00927144\n",
      "Epoch 2 | Step 1471700 | Avg Loss: 0.0148 | Grad Norm: 0.00882512\n",
      "Epoch 2 | Step 1471800 | Avg Loss: 0.0147 | Grad Norm: 0.00875564\n",
      "Epoch 2 | Step 1471900 | Avg Loss: 0.0151 | Grad Norm: 0.00837409\n",
      "Epoch 2 | Step 1472000 | Avg Loss: 0.0151 | Grad Norm: 0.00780935\n",
      "Epoch 2 | Step 1472100 | Avg Loss: 0.0150 | Grad Norm: 0.00794777\n",
      "Epoch 2 | Step 1472200 | Avg Loss: 0.0152 | Grad Norm: 0.00821464\n",
      "Epoch 2 | Step 1472300 | Avg Loss: 0.0149 | Grad Norm: 0.00745117\n",
      "Epoch 2 | Step 1472400 | Avg Loss: 0.0144 | Grad Norm: 0.00837662\n",
      "Epoch 2 | Step 1472500 | Avg Loss: 0.0149 | Grad Norm: 0.00714962\n",
      "Epoch 2 | Step 1472600 | Avg Loss: 0.0149 | Grad Norm: 0.00819089\n",
      "Epoch 2 | Step 1472700 | Avg Loss: 0.0147 | Grad Norm: 0.00783673\n",
      "Epoch 2 | Step 1472800 | Avg Loss: 0.0148 | Grad Norm: 0.00772390\n",
      "Epoch 2 | Step 1472900 | Avg Loss: 0.0150 | Grad Norm: 0.00877881\n",
      "Epoch 2 | Step 1473000 | Avg Loss: 0.0150 | Grad Norm: 0.00737412\n",
      "Epoch 2 | Step 1473100 | Avg Loss: 0.0151 | Grad Norm: 0.00782453\n",
      "Epoch 2 | Step 1473200 | Avg Loss: 0.0150 | Grad Norm: 0.00812434\n",
      "Epoch 2 | Step 1473300 | Avg Loss: 0.0150 | Grad Norm: 0.00809432\n",
      "Epoch 2 | Step 1473400 | Avg Loss: 0.0152 | Grad Norm: 0.00795442\n",
      "Epoch 2 | Step 1473500 | Avg Loss: 0.0152 | Grad Norm: 0.00714008\n",
      "Epoch 2 | Step 1473600 | Avg Loss: 0.0155 | Grad Norm: 0.00857570\n",
      "Epoch 2 | Step 1473700 | Avg Loss: 0.0154 | Grad Norm: 0.00935133\n",
      "Epoch 2 | Step 1473800 | Avg Loss: 0.0153 | Grad Norm: 0.00785857\n",
      "Epoch 2 | Step 1473900 | Avg Loss: 0.0152 | Grad Norm: 0.00978339\n",
      "Epoch 2 | Step 1474000 | Avg Loss: 0.0154 | Grad Norm: 0.00855611\n",
      "Epoch 2 | Step 1474100 | Avg Loss: 0.0155 | Grad Norm: 0.01017135\n",
      "Epoch 2 | Step 1474200 | Avg Loss: 0.0153 | Grad Norm: 0.00709445\n",
      "Epoch 2 | Step 1474300 | Avg Loss: 0.0155 | Grad Norm: 0.01038178\n",
      "Epoch 2 | Step 1474400 | Avg Loss: 0.0154 | Grad Norm: 0.00808885\n",
      "Epoch 2 | Step 1474500 | Avg Loss: 0.0156 | Grad Norm: 0.00849357\n",
      "Epoch 2 | Step 1474600 | Avg Loss: 0.0151 | Grad Norm: 0.01069311\n",
      "Epoch 2 | Step 1474700 | Avg Loss: 0.0153 | Grad Norm: 0.00809798\n",
      "Epoch 2 | Step 1474800 | Avg Loss: 0.0150 | Grad Norm: 0.00864681\n",
      "Epoch 2 | Step 1474900 | Avg Loss: 0.0150 | Grad Norm: 0.00812061\n",
      "Epoch 2 | Step 1475000 | Avg Loss: 0.0150 | Grad Norm: 0.00885627\n",
      "Epoch 2 | Step 1475100 | Avg Loss: 0.0152 | Grad Norm: 0.01185342\n",
      "Epoch 2 | Step 1475200 | Avg Loss: 0.0151 | Grad Norm: 0.00772879\n",
      "Epoch 2 | Step 1475300 | Avg Loss: 0.0154 | Grad Norm: 0.00930886\n",
      "Epoch 2 | Step 1475400 | Avg Loss: 0.0153 | Grad Norm: 0.00836919\n",
      "Epoch 2 | Step 1475500 | Avg Loss: 0.0157 | Grad Norm: 0.01016083\n",
      "Epoch 2 | Step 1475600 | Avg Loss: 0.0160 | Grad Norm: 0.00841109\n",
      "Epoch 2 | Step 1475700 | Avg Loss: 0.0154 | Grad Norm: 0.00914763\n",
      "Epoch 2 | Step 1475800 | Avg Loss: 0.0153 | Grad Norm: 0.00842069\n",
      "Epoch 2 | Step 1475900 | Avg Loss: 0.0153 | Grad Norm: 0.00799294\n",
      "Epoch 2 | Step 1476000 | Avg Loss: 0.0153 | Grad Norm: 0.00929750\n",
      "Epoch 2 | Step 1476100 | Avg Loss: 0.0155 | Grad Norm: 0.00837817\n",
      "Epoch 2 | Step 1476200 | Avg Loss: 0.0154 | Grad Norm: 0.00900674\n",
      "Epoch 2 | Step 1476300 | Avg Loss: 0.0153 | Grad Norm: 0.00863656\n",
      "Epoch 2 | Step 1476400 | Avg Loss: 0.0156 | Grad Norm: 0.00772988\n",
      "Epoch 2 | Step 1476500 | Avg Loss: 0.0151 | Grad Norm: 0.00923706\n",
      "Epoch 2 | Step 1476600 | Avg Loss: 0.0149 | Grad Norm: 0.00863720\n",
      "Epoch 2 | Step 1476700 | Avg Loss: 0.0149 | Grad Norm: 0.00879284\n",
      "Epoch 2 | Step 1476800 | Avg Loss: 0.0151 | Grad Norm: 0.00848808\n",
      "Epoch 2 | Step 1476900 | Avg Loss: 0.0151 | Grad Norm: 0.00945107\n",
      "Epoch 2 | Step 1477000 | Avg Loss: 0.0149 | Grad Norm: 0.00867751\n",
      "Epoch 2 | Step 1477100 | Avg Loss: 0.0149 | Grad Norm: 0.00790473\n",
      "Epoch 2 | Step 1477200 | Avg Loss: 0.0152 | Grad Norm: 0.00789984\n",
      "Epoch 2 | Step 1477300 | Avg Loss: 0.0152 | Grad Norm: 0.00868171\n",
      "Epoch 2 | Step 1477400 | Avg Loss: 0.0150 | Grad Norm: 0.00980269\n",
      "Epoch 2 | Step 1477500 | Avg Loss: 0.0151 | Grad Norm: 0.00806955\n",
      "Epoch 2 | Step 1477600 | Avg Loss: 0.0152 | Grad Norm: 0.00866682\n",
      "Epoch 2 | Step 1477700 | Avg Loss: 0.0150 | Grad Norm: 0.00763454\n",
      "Epoch 2 | Step 1477800 | Avg Loss: 0.0149 | Grad Norm: 0.00798107\n",
      "Epoch 2 | Step 1477900 | Avg Loss: 0.0152 | Grad Norm: 0.00850319\n",
      "Epoch 2 | Step 1478000 | Avg Loss: 0.0150 | Grad Norm: 0.00879670\n",
      "Epoch 2 | Step 1478100 | Avg Loss: 0.0151 | Grad Norm: 0.00795929\n",
      "Epoch 2 | Step 1478200 | Avg Loss: 0.0150 | Grad Norm: 0.00908487\n",
      "Epoch 2 | Step 1478300 | Avg Loss: 0.0148 | Grad Norm: 0.01050799\n",
      "Epoch 2 | Step 1478400 | Avg Loss: 0.0152 | Grad Norm: 0.00872349\n",
      "Epoch 2 | Step 1478500 | Avg Loss: 0.0152 | Grad Norm: 0.00765954\n",
      "Epoch 2 | Step 1478600 | Avg Loss: 0.0155 | Grad Norm: 0.00864900\n",
      "Epoch 2 | Step 1478700 | Avg Loss: 0.0155 | Grad Norm: 0.00979077\n",
      "Epoch 2 | Step 1478800 | Avg Loss: 0.0157 | Grad Norm: 0.00838766\n",
      "Epoch 2 | Step 1478900 | Avg Loss: 0.0155 | Grad Norm: 0.00780798\n",
      "Epoch 2 | Step 1479000 | Avg Loss: 0.0154 | Grad Norm: 0.00703036\n",
      "Epoch 2 | Step 1479100 | Avg Loss: 0.0159 | Grad Norm: 0.00837820\n",
      "Epoch 2 | Step 1479200 | Avg Loss: 0.0154 | Grad Norm: 0.00830469\n",
      "Epoch 2 | Step 1479300 | Avg Loss: 0.0151 | Grad Norm: 0.00969641\n",
      "Epoch 2 | Step 1479400 | Avg Loss: 0.0155 | Grad Norm: 0.01016024\n",
      "Epoch 2 | Step 1479500 | Avg Loss: 0.0154 | Grad Norm: 0.00809516\n",
      "Epoch 2 | Step 1479600 | Avg Loss: 0.0155 | Grad Norm: 0.00882200\n",
      "Epoch 2 | Step 1479700 | Avg Loss: 0.0158 | Grad Norm: 0.00751710\n",
      "Epoch 2 | Step 1479800 | Avg Loss: 0.0154 | Grad Norm: 0.00903980\n",
      "Epoch 2 | Step 1479900 | Avg Loss: 0.0156 | Grad Norm: 0.00831734\n",
      "Epoch 2 | Step 1480000 | Avg Loss: 0.0157 | Grad Norm: 0.00833514\n",
      "Epoch 2 | Step 1480100 | Avg Loss: 0.0152 | Grad Norm: 0.00926949\n",
      "Epoch 2 | Step 1480200 | Avg Loss: 0.0149 | Grad Norm: 0.00780501\n",
      "Epoch 2 | Step 1480300 | Avg Loss: 0.0147 | Grad Norm: 0.00982250\n",
      "Epoch 2 | Step 1480400 | Avg Loss: 0.0150 | Grad Norm: 0.00700709\n",
      "Epoch 2 | Step 1480500 | Avg Loss: 0.0153 | Grad Norm: 0.00803156\n",
      "Epoch 2 | Step 1480600 | Avg Loss: 0.0153 | Grad Norm: 0.00924643\n",
      "Epoch 2 | Step 1480700 | Avg Loss: 0.0152 | Grad Norm: 0.00786015\n",
      "Epoch 2 | Step 1480800 | Avg Loss: 0.0153 | Grad Norm: 0.00785954\n",
      "Epoch 2 | Step 1480900 | Avg Loss: 0.0152 | Grad Norm: 0.00848623\n",
      "Epoch 2 | Step 1481000 | Avg Loss: 0.0150 | Grad Norm: 0.00806193\n",
      "Epoch 2 | Step 1481100 | Avg Loss: 0.0150 | Grad Norm: 0.00918603\n",
      "Epoch 2 | Step 1481200 | Avg Loss: 0.0150 | Grad Norm: 0.00889771\n",
      "Epoch 2 | Step 1481300 | Avg Loss: 0.0150 | Grad Norm: 0.00730138\n",
      "Epoch 2 | Step 1481400 | Avg Loss: 0.0152 | Grad Norm: 0.00762016\n",
      "Epoch 2 | Step 1481500 | Avg Loss: 0.0152 | Grad Norm: 0.00815427\n",
      "Epoch 2 | Step 1481600 | Avg Loss: 0.0153 | Grad Norm: 0.00833784\n",
      "Epoch 2 | Step 1481700 | Avg Loss: 0.0153 | Grad Norm: 0.00947017\n",
      "Epoch 2 | Step 1481800 | Avg Loss: 0.0154 | Grad Norm: 0.00706349\n",
      "Epoch 2 | Step 1481900 | Avg Loss: 0.0150 | Grad Norm: 0.00918573\n",
      "Epoch 2 | Step 1482000 | Avg Loss: 0.0147 | Grad Norm: 0.00955676\n",
      "Epoch 2 | Step 1482100 | Avg Loss: 0.0148 | Grad Norm: 0.00756966\n",
      "Epoch 2 | Step 1482200 | Avg Loss: 0.0149 | Grad Norm: 0.00829237\n",
      "Epoch 2 | Step 1482300 | Avg Loss: 0.0150 | Grad Norm: 0.00826020\n",
      "Epoch 2 | Step 1482400 | Avg Loss: 0.0153 | Grad Norm: 0.00811236\n",
      "Epoch 2 | Step 1482500 | Avg Loss: 0.0151 | Grad Norm: 0.00869299\n",
      "Epoch 2 | Step 1482600 | Avg Loss: 0.0146 | Grad Norm: 0.00722910\n",
      "Epoch 2 | Step 1482700 | Avg Loss: 0.0150 | Grad Norm: 0.00832957\n",
      "Epoch 2 | Step 1482800 | Avg Loss: 0.0149 | Grad Norm: 0.00723445\n",
      "Epoch 2 | Step 1482900 | Avg Loss: 0.0152 | Grad Norm: 0.00709251\n",
      "Epoch 2 | Step 1483000 | Avg Loss: 0.0152 | Grad Norm: 0.00789674\n",
      "Epoch 2 | Step 1483100 | Avg Loss: 0.0151 | Grad Norm: 0.00866121\n",
      "Epoch 2 | Step 1483200 | Avg Loss: 0.0150 | Grad Norm: 0.00750461\n",
      "Epoch 2 | Step 1483300 | Avg Loss: 0.0149 | Grad Norm: 0.00847566\n",
      "Epoch 2 | Step 1483400 | Avg Loss: 0.0151 | Grad Norm: 0.00798027\n",
      "Epoch 2 | Step 1483500 | Avg Loss: 0.0150 | Grad Norm: 0.00852717\n",
      "Epoch 2 | Step 1483600 | Avg Loss: 0.0149 | Grad Norm: 0.00846485\n",
      "Epoch 2 | Step 1483700 | Avg Loss: 0.0148 | Grad Norm: 0.00833470\n",
      "Epoch 2 | Step 1483800 | Avg Loss: 0.0149 | Grad Norm: 0.00850581\n",
      "Epoch 2 | Step 1483900 | Avg Loss: 0.0149 | Grad Norm: 0.00708563\n",
      "Epoch 2 | Step 1484000 | Avg Loss: 0.0150 | Grad Norm: 0.00790582\n",
      "Epoch 2 | Step 1484100 | Avg Loss: 0.0150 | Grad Norm: 0.00752262\n",
      "Epoch 2 | Step 1484200 | Avg Loss: 0.0146 | Grad Norm: 0.00783569\n",
      "Epoch 2 | Step 1484300 | Avg Loss: 0.0150 | Grad Norm: 0.00761641\n",
      "Epoch 2 | Step 1484400 | Avg Loss: 0.0150 | Grad Norm: 0.00969407\n",
      "Epoch 2 | Step 1484500 | Avg Loss: 0.0149 | Grad Norm: 0.00818571\n",
      "Epoch 2 | Step 1484600 | Avg Loss: 0.0151 | Grad Norm: 0.00810471\n",
      "Epoch 2 | Step 1484700 | Avg Loss: 0.0151 | Grad Norm: 0.00802007\n",
      "Epoch 2 | Step 1484800 | Avg Loss: 0.0150 | Grad Norm: 0.00778056\n",
      "Epoch 2 | Step 1484900 | Avg Loss: 0.0148 | Grad Norm: 0.00764415\n",
      "Epoch 2 | Step 1485000 | Avg Loss: 0.0150 | Grad Norm: 0.00834871\n",
      "Epoch 2 | Step 1485100 | Avg Loss: 0.0150 | Grad Norm: 0.00773578\n",
      "Epoch 2 | Step 1485200 | Avg Loss: 0.0151 | Grad Norm: 0.00761534\n",
      "Epoch 2 | Step 1485300 | Avg Loss: 0.0148 | Grad Norm: 0.00927100\n",
      "Epoch 2 | Step 1485400 | Avg Loss: 0.0149 | Grad Norm: 0.00816255\n",
      "Epoch 2 | Step 1485500 | Avg Loss: 0.0150 | Grad Norm: 0.00873054\n",
      "Epoch 2 | Step 1485600 | Avg Loss: 0.0147 | Grad Norm: 0.00856002\n",
      "Epoch 2 | Step 1485700 | Avg Loss: 0.0145 | Grad Norm: 0.00836211\n",
      "Epoch 2 | Step 1485800 | Avg Loss: 0.0146 | Grad Norm: 0.00818904\n",
      "Epoch 2 | Step 1485900 | Avg Loss: 0.0148 | Grad Norm: 0.00796466\n",
      "Epoch 2 | Step 1486000 | Avg Loss: 0.0148 | Grad Norm: 0.00961848\n",
      "Epoch 2 | Step 1486100 | Avg Loss: 0.0148 | Grad Norm: 0.00774033\n",
      "Epoch 2 | Step 1486200 | Avg Loss: 0.0148 | Grad Norm: 0.00913646\n",
      "Epoch 2 | Step 1486300 | Avg Loss: 0.0147 | Grad Norm: 0.01013212\n",
      "Epoch 2 | Step 1486400 | Avg Loss: 0.0147 | Grad Norm: 0.00845593\n",
      "Epoch 2 | Step 1486500 | Avg Loss: 0.0148 | Grad Norm: 0.00837108\n",
      "Epoch 2 | Step 1486600 | Avg Loss: 0.0150 | Grad Norm: 0.00913734\n",
      "Epoch 2 | Step 1486700 | Avg Loss: 0.0146 | Grad Norm: 0.00772929\n",
      "Epoch 2 | Step 1486800 | Avg Loss: 0.0149 | Grad Norm: 0.00946376\n",
      "Epoch 2 | Step 1486900 | Avg Loss: 0.0150 | Grad Norm: 0.00905422\n",
      "Epoch 2 | Step 1487000 | Avg Loss: 0.0153 | Grad Norm: 0.00984989\n",
      "Epoch 2 | Step 1487100 | Avg Loss: 0.0152 | Grad Norm: 0.00823081\n",
      "Epoch 2 | Step 1487200 | Avg Loss: 0.0150 | Grad Norm: 0.00822464\n",
      "Epoch 2 | Step 1487300 | Avg Loss: 0.0148 | Grad Norm: 0.00800872\n",
      "Epoch 2 | Step 1487400 | Avg Loss: 0.0146 | Grad Norm: 0.00874009\n",
      "Epoch 2 | Step 1487500 | Avg Loss: 0.0150 | Grad Norm: 0.00737029\n",
      "Epoch 2 | Step 1487600 | Avg Loss: 0.0149 | Grad Norm: 0.00904709\n",
      "Epoch 2 | Step 1487700 | Avg Loss: 0.0150 | Grad Norm: 0.01011962\n",
      "Epoch 2 | Step 1487800 | Avg Loss: 0.0154 | Grad Norm: 0.00910532\n",
      "Epoch 2 | Step 1487900 | Avg Loss: 0.0152 | Grad Norm: 0.00860034\n",
      "Epoch 2 | Step 1488000 | Avg Loss: 0.0151 | Grad Norm: 0.00982660\n",
      "Epoch 2 | Step 1488100 | Avg Loss: 0.0154 | Grad Norm: 0.00895504\n",
      "Epoch 2 | Step 1488200 | Avg Loss: 0.0151 | Grad Norm: 0.00820861\n",
      "Epoch 2 | Step 1488300 | Avg Loss: 0.0154 | Grad Norm: 0.01005428\n",
      "Epoch 2 | Step 1488400 | Avg Loss: 0.0153 | Grad Norm: 0.00873074\n",
      "Epoch 2 | Step 1488500 | Avg Loss: 0.0151 | Grad Norm: 0.00905928\n",
      "Epoch 2 | Step 1488600 | Avg Loss: 0.0150 | Grad Norm: 0.00875535\n",
      "Epoch 2 | Step 1488700 | Avg Loss: 0.0150 | Grad Norm: 0.00858663\n",
      "Epoch 2 | Step 1488800 | Avg Loss: 0.0150 | Grad Norm: 0.00854540\n",
      "Epoch 2 | Step 1488900 | Avg Loss: 0.0150 | Grad Norm: 0.00742075\n",
      "Epoch 2 | Step 1489000 | Avg Loss: 0.0154 | Grad Norm: 0.00736959\n",
      "Epoch 2 | Step 1489100 | Avg Loss: 0.0152 | Grad Norm: 0.00801838\n",
      "Epoch 2 | Step 1489200 | Avg Loss: 0.0150 | Grad Norm: 0.00902507\n",
      "Epoch 2 | Step 1489300 | Avg Loss: 0.0150 | Grad Norm: 0.00868442\n",
      "Epoch 2 | Step 1489400 | Avg Loss: 0.0150 | Grad Norm: 0.00786473\n",
      "Epoch 2 | Step 1489500 | Avg Loss: 0.0155 | Grad Norm: 0.00795207\n",
      "Epoch 2 | Step 1489600 | Avg Loss: 0.0154 | Grad Norm: 0.00778021\n",
      "Epoch 2 | Step 1489700 | Avg Loss: 0.0156 | Grad Norm: 0.00854029\n",
      "Epoch 2 | Step 1489800 | Avg Loss: 0.0157 | Grad Norm: 0.00807759\n",
      "Epoch 2 | Step 1489900 | Avg Loss: 0.0154 | Grad Norm: 0.00888189\n",
      "Epoch 2 | Step 1490000 | Avg Loss: 0.0153 | Grad Norm: 0.00876408\n",
      "Epoch 2 | Step 1490100 | Avg Loss: 0.0150 | Grad Norm: 0.00935186\n",
      "Epoch 2 | Step 1490200 | Avg Loss: 0.0150 | Grad Norm: 0.00799222\n",
      "Epoch 2 | Step 1490300 | Avg Loss: 0.0151 | Grad Norm: 0.00944215\n",
      "Epoch 2 | Step 1490400 | Avg Loss: 0.0150 | Grad Norm: 0.00766477\n",
      "Epoch 2 | Step 1490500 | Avg Loss: 0.0149 | Grad Norm: 0.00796413\n",
      "Epoch 2 | Step 1490600 | Avg Loss: 0.0150 | Grad Norm: 0.00956962\n",
      "Epoch 2 | Step 1490700 | Avg Loss: 0.0151 | Grad Norm: 0.00971451\n",
      "Epoch 2 | Step 1490800 | Avg Loss: 0.0149 | Grad Norm: 0.00909843\n",
      "Epoch 2 | Step 1490900 | Avg Loss: 0.0150 | Grad Norm: 0.00806742\n",
      "Epoch 2 | Step 1491000 | Avg Loss: 0.0147 | Grad Norm: 0.00872836\n",
      "Epoch 2 | Step 1491100 | Avg Loss: 0.0147 | Grad Norm: 0.00879902\n",
      "Epoch 2 | Step 1491200 | Avg Loss: 0.0150 | Grad Norm: 0.00780835\n",
      "Epoch 2 | Step 1491300 | Avg Loss: 0.0155 | Grad Norm: 0.00831736\n",
      "Epoch 2 | Step 1491400 | Avg Loss: 0.0154 | Grad Norm: 0.00861011\n",
      "Epoch 2 | Step 1491500 | Avg Loss: 0.0150 | Grad Norm: 0.00957737\n",
      "Epoch 2 | Step 1491600 | Avg Loss: 0.0148 | Grad Norm: 0.00863755\n",
      "Epoch 2 | Step 1491700 | Avg Loss: 0.0151 | Grad Norm: 0.00849724\n",
      "Epoch 2 | Step 1491800 | Avg Loss: 0.0150 | Grad Norm: 0.00852360\n",
      "Epoch 2 | Step 1491900 | Avg Loss: 0.0152 | Grad Norm: 0.00861801\n",
      "Epoch 2 | Step 1492000 | Avg Loss: 0.0157 | Grad Norm: 0.00785782\n",
      "Epoch 2 | Step 1492100 | Avg Loss: 0.0155 | Grad Norm: 0.00763203\n",
      "Epoch 2 | Step 1492200 | Avg Loss: 0.0153 | Grad Norm: 0.00852612\n",
      "Epoch 2 | Step 1492300 | Avg Loss: 0.0152 | Grad Norm: 0.00723836\n",
      "Epoch 2 | Step 1492400 | Avg Loss: 0.0146 | Grad Norm: 0.00940424\n",
      "Epoch 2 | Step 1492500 | Avg Loss: 0.0145 | Grad Norm: 0.00679240\n",
      "Epoch 2 | Step 1492600 | Avg Loss: 0.0146 | Grad Norm: 0.00823549\n",
      "Epoch 2 | Step 1492700 | Avg Loss: 0.0145 | Grad Norm: 0.00793827\n",
      "Epoch 2 | Step 1492800 | Avg Loss: 0.0148 | Grad Norm: 0.00838931\n",
      "Epoch 2 | Step 1492900 | Avg Loss: 0.0147 | Grad Norm: 0.00822027\n",
      "Epoch 2 | Step 1493000 | Avg Loss: 0.0144 | Grad Norm: 0.00730725\n",
      "Epoch 2 | Step 1493100 | Avg Loss: 0.0149 | Grad Norm: 0.00714491\n",
      "Epoch 2 | Step 1493200 | Avg Loss: 0.0152 | Grad Norm: 0.00818839\n",
      "Epoch 2 | Step 1493300 | Avg Loss: 0.0156 | Grad Norm: 0.00787733\n",
      "Epoch 2 | Step 1493400 | Avg Loss: 0.0156 | Grad Norm: 0.00885179\n",
      "Epoch 2 | Step 1493500 | Avg Loss: 0.0150 | Grad Norm: 0.00928180\n",
      "Epoch 2 | Step 1493600 | Avg Loss: 0.0151 | Grad Norm: 0.01088408\n",
      "Epoch 2 | Step 1493700 | Avg Loss: 0.0152 | Grad Norm: 0.00982980\n",
      "Epoch 2 | Step 1493800 | Avg Loss: 0.0152 | Grad Norm: 0.00762271\n",
      "Epoch 2 | Step 1493900 | Avg Loss: 0.0149 | Grad Norm: 0.00773149\n",
      "Epoch 2 | Step 1494000 | Avg Loss: 0.0150 | Grad Norm: 0.00825648\n",
      "Epoch 2 | Step 1494100 | Avg Loss: 0.0148 | Grad Norm: 0.00874360\n",
      "Epoch 2 | Step 1494200 | Avg Loss: 0.0150 | Grad Norm: 0.00724623\n",
      "Epoch 2 | Step 1494300 | Avg Loss: 0.0147 | Grad Norm: 0.00881795\n",
      "Epoch 2 | Step 1494400 | Avg Loss: 0.0150 | Grad Norm: 0.00857957\n",
      "Epoch 2 | Step 1494500 | Avg Loss: 0.0149 | Grad Norm: 0.00792088\n",
      "Epoch 2 | Step 1494600 | Avg Loss: 0.0149 | Grad Norm: 0.00860357\n",
      "Epoch 2 | Step 1494700 | Avg Loss: 0.0153 | Grad Norm: 0.00798513\n",
      "Epoch 2 | Step 1494800 | Avg Loss: 0.0155 | Grad Norm: 0.00829371\n",
      "Epoch 2 | Step 1494900 | Avg Loss: 0.0153 | Grad Norm: 0.00760533\n",
      "Epoch 2 | Step 1495000 | Avg Loss: 0.0153 | Grad Norm: 0.00768637\n",
      "Epoch 2 | Step 1495100 | Avg Loss: 0.0154 | Grad Norm: 0.00766232\n",
      "Epoch 2 | Step 1495200 | Avg Loss: 0.0153 | Grad Norm: 0.00775773\n",
      "Epoch 2 | Step 1495300 | Avg Loss: 0.0147 | Grad Norm: 0.00801328\n",
      "Epoch 2 | Step 1495400 | Avg Loss: 0.0150 | Grad Norm: 0.00841432\n",
      "Epoch 2 | Step 1495500 | Avg Loss: 0.0150 | Grad Norm: 0.00844205\n",
      "Epoch 2 | Step 1495600 | Avg Loss: 0.0149 | Grad Norm: 0.00767776\n",
      "Epoch 2 | Step 1495700 | Avg Loss: 0.0148 | Grad Norm: 0.00782704\n",
      "Epoch 2 | Step 1495800 | Avg Loss: 0.0149 | Grad Norm: 0.00845532\n",
      "Epoch 2 | Step 1495900 | Avg Loss: 0.0149 | Grad Norm: 0.00775573\n",
      "Epoch 2 | Step 1496000 | Avg Loss: 0.0150 | Grad Norm: 0.00754829\n",
      "Epoch 2 | Step 1496100 | Avg Loss: 0.0148 | Grad Norm: 0.00861000\n",
      "Epoch 2 | Step 1496200 | Avg Loss: 0.0146 | Grad Norm: 0.00762644\n",
      "Epoch 2 | Step 1496300 | Avg Loss: 0.0148 | Grad Norm: 0.00857190\n",
      "Epoch 2 | Step 1496400 | Avg Loss: 0.0151 | Grad Norm: 0.00930944\n",
      "Epoch 2 | Step 1496500 | Avg Loss: 0.0152 | Grad Norm: 0.00766563\n",
      "Epoch 2 | Step 1496600 | Avg Loss: 0.0149 | Grad Norm: 0.01015605\n",
      "Epoch 2 | Step 1496700 | Avg Loss: 0.0151 | Grad Norm: 0.00764334\n",
      "Epoch 2 | Step 1496800 | Avg Loss: 0.0153 | Grad Norm: 0.00814145\n",
      "Epoch 2 | Step 1496900 | Avg Loss: 0.0152 | Grad Norm: 0.00893505\n",
      "Epoch 2 | Step 1497000 | Avg Loss: 0.0154 | Grad Norm: 0.00905573\n",
      "Epoch 2 | Step 1497100 | Avg Loss: 0.0155 | Grad Norm: 0.00779776\n",
      "Epoch 2 | Step 1497200 | Avg Loss: 0.0152 | Grad Norm: 0.00915294\n",
      "Epoch 2 | Step 1497300 | Avg Loss: 0.0156 | Grad Norm: 0.00835164\n",
      "Epoch 2 | Step 1497400 | Avg Loss: 0.0157 | Grad Norm: 0.00834974\n",
      "Epoch 2 | Step 1497500 | Avg Loss: 0.0157 | Grad Norm: 0.00962042\n",
      "Epoch 2 | Step 1497600 | Avg Loss: 0.0156 | Grad Norm: 0.00957249\n",
      "Epoch 2 | Step 1497700 | Avg Loss: 0.0156 | Grad Norm: 0.00739214\n",
      "Epoch 2 | Step 1497800 | Avg Loss: 0.0151 | Grad Norm: 0.00772647\n",
      "Epoch 2 | Step 1497900 | Avg Loss: 0.0150 | Grad Norm: 0.00805791\n",
      "Epoch 2 | Step 1498000 | Avg Loss: 0.0150 | Grad Norm: 0.00789935\n",
      "Epoch 2 | Step 1498100 | Avg Loss: 0.0156 | Grad Norm: 0.00906540\n",
      "Epoch 2 | Step 1498200 | Avg Loss: 0.0153 | Grad Norm: 0.00768494\n",
      "Epoch 2 | Step 1498300 | Avg Loss: 0.0154 | Grad Norm: 0.00936253\n",
      "Epoch 2 | Step 1498400 | Avg Loss: 0.0153 | Grad Norm: 0.00780325\n",
      "Epoch 2 | Step 1498500 | Avg Loss: 0.0151 | Grad Norm: 0.00951337\n",
      "Epoch 2 | Step 1498600 | Avg Loss: 0.0150 | Grad Norm: 0.00849003\n",
      "Epoch 2 | Step 1498700 | Avg Loss: 0.0155 | Grad Norm: 0.00826838\n",
      "Epoch 2 | Step 1498800 | Avg Loss: 0.0156 | Grad Norm: 0.00907209\n",
      "Epoch 2 | Step 1498900 | Avg Loss: 0.0155 | Grad Norm: 0.00854288\n",
      "Epoch 2 | Step 1499000 | Avg Loss: 0.0155 | Grad Norm: 0.00817215\n",
      "Epoch 2 | Step 1499100 | Avg Loss: 0.0158 | Grad Norm: 0.00715483\n",
      "Epoch 2 | Step 1499200 | Avg Loss: 0.0156 | Grad Norm: 0.00729290\n",
      "Epoch 2 | Step 1499300 | Avg Loss: 0.0157 | Grad Norm: 0.00835244\n",
      "Epoch 2 | Step 1499400 | Avg Loss: 0.0155 | Grad Norm: 0.00918890\n",
      "Epoch 2 | Step 1499500 | Avg Loss: 0.0155 | Grad Norm: 0.00791132\n",
      "Epoch 2 | Step 1499600 | Avg Loss: 0.0157 | Grad Norm: 0.00923787\n",
      "Epoch 2 | Step 1499700 | Avg Loss: 0.0157 | Grad Norm: 0.00760794\n",
      "Epoch 2 | Step 1499800 | Avg Loss: 0.0156 | Grad Norm: 0.01056094\n",
      "Epoch 2 | Step 1499900 | Avg Loss: 0.0153 | Grad Norm: 0.00819331\n",
      "Epoch 2 | Step 1500000 | Avg Loss: 0.0153 | Grad Norm: 0.00920942\n",
      "Saving model at step1500000\n",
      "Epoch 2 | Step 1500100 | Avg Loss: 0.0153 | Grad Norm: 0.00813027\n",
      "Epoch 2 | Step 1500200 | Avg Loss: 0.0151 | Grad Norm: 0.00782936\n",
      "Epoch 2 | Step 1500300 | Avg Loss: 0.0150 | Grad Norm: 0.00991484\n",
      "Epoch 2 | Step 1500400 | Avg Loss: 0.0148 | Grad Norm: 0.00768029\n",
      "Epoch 2 | Step 1500500 | Avg Loss: 0.0149 | Grad Norm: 0.00831581\n",
      "Epoch 2 | Step 1500600 | Avg Loss: 0.0149 | Grad Norm: 0.00910875\n",
      "Epoch 2 | Step 1500700 | Avg Loss: 0.0147 | Grad Norm: 0.00839034\n",
      "Epoch 2 | Step 1500800 | Avg Loss: 0.0150 | Grad Norm: 0.00735226\n",
      "Epoch 2 | Step 1500900 | Avg Loss: 0.0148 | Grad Norm: 0.00755276\n",
      "Epoch 2 | Step 1501000 | Avg Loss: 0.0147 | Grad Norm: 0.00837877\n",
      "Epoch 2 | Step 1501100 | Avg Loss: 0.0147 | Grad Norm: 0.00840905\n",
      "Epoch 2 | Step 1501200 | Avg Loss: 0.0149 | Grad Norm: 0.00872942\n",
      "Epoch 2 | Step 1501300 | Avg Loss: 0.0147 | Grad Norm: 0.00704091\n",
      "Epoch 2 | Step 1501400 | Avg Loss: 0.0147 | Grad Norm: 0.00871493\n",
      "Epoch 2 | Step 1501500 | Avg Loss: 0.0149 | Grad Norm: 0.00740550\n",
      "Epoch 2 | Step 1501600 | Avg Loss: 0.0144 | Grad Norm: 0.00757130\n",
      "Epoch 2 | Step 1501700 | Avg Loss: 0.0150 | Grad Norm: 0.01035148\n",
      "Epoch 2 | Step 1501800 | Avg Loss: 0.0147 | Grad Norm: 0.00905406\n",
      "Epoch 2 | Step 1501900 | Avg Loss: 0.0145 | Grad Norm: 0.00712801\n",
      "Epoch 2 | Step 1502000 | Avg Loss: 0.0141 | Grad Norm: 0.00771191\n",
      "Epoch 2 | Step 1502100 | Avg Loss: 0.0144 | Grad Norm: 0.00868167\n",
      "Epoch 2 | Step 1502200 | Avg Loss: 0.0145 | Grad Norm: 0.00845385\n",
      "Epoch 2 | Step 1502300 | Avg Loss: 0.0143 | Grad Norm: 0.00865851\n",
      "Epoch 2 | Step 1502400 | Avg Loss: 0.0146 | Grad Norm: 0.00832405\n",
      "Epoch 2 | Step 1502500 | Avg Loss: 0.0148 | Grad Norm: 0.00792184\n",
      "Epoch 2 | Step 1502600 | Avg Loss: 0.0148 | Grad Norm: 0.00848909\n",
      "Epoch 2 | Step 1502700 | Avg Loss: 0.0148 | Grad Norm: 0.01046661\n",
      "Epoch 2 | Step 1502800 | Avg Loss: 0.0152 | Grad Norm: 0.00828637\n",
      "Epoch 2 | Step 1502900 | Avg Loss: 0.0152 | Grad Norm: 0.00870555\n",
      "Epoch 2 | Step 1503000 | Avg Loss: 0.0151 | Grad Norm: 0.00834723\n",
      "Epoch 2 | Step 1503100 | Avg Loss: 0.0151 | Grad Norm: 0.00904553\n",
      "Epoch 2 | Step 1503200 | Avg Loss: 0.0147 | Grad Norm: 0.00787606\n",
      "Epoch 2 | Step 1503300 | Avg Loss: 0.0146 | Grad Norm: 0.00891057\n",
      "Epoch 2 | Step 1503400 | Avg Loss: 0.0146 | Grad Norm: 0.00710796\n",
      "Epoch 2 | Step 1503500 | Avg Loss: 0.0149 | Grad Norm: 0.00832836\n",
      "Epoch 2 | Step 1503600 | Avg Loss: 0.0148 | Grad Norm: 0.00790710\n",
      "Epoch 2 | Step 1503700 | Avg Loss: 0.0153 | Grad Norm: 0.00798450\n",
      "Epoch 2 | Step 1503800 | Avg Loss: 0.0153 | Grad Norm: 0.00898860\n",
      "Epoch 2 | Step 1503900 | Avg Loss: 0.0152 | Grad Norm: 0.00820384\n",
      "Epoch 2 | Step 1504000 | Avg Loss: 0.0152 | Grad Norm: 0.00900574\n",
      "Epoch 2 | Step 1504100 | Avg Loss: 0.0150 | Grad Norm: 0.00763007\n",
      "Epoch 2 | Step 1504200 | Avg Loss: 0.0149 | Grad Norm: 0.00828718\n",
      "Epoch 2 | Step 1504300 | Avg Loss: 0.0150 | Grad Norm: 0.00859525\n",
      "Epoch 2 | Step 1504400 | Avg Loss: 0.0149 | Grad Norm: 0.00832949\n",
      "Epoch 2 | Step 1504500 | Avg Loss: 0.0150 | Grad Norm: 0.00995717\n",
      "Epoch 2 | Step 1504600 | Avg Loss: 0.0149 | Grad Norm: 0.00737470\n",
      "Epoch 2 | Step 1504700 | Avg Loss: 0.0149 | Grad Norm: 0.00835243\n",
      "Epoch 2 | Step 1504800 | Avg Loss: 0.0147 | Grad Norm: 0.00779813\n",
      "Epoch 2 | Step 1504900 | Avg Loss: 0.0146 | Grad Norm: 0.00839719\n",
      "Epoch 2 | Step 1505000 | Avg Loss: 0.0147 | Grad Norm: 0.00840293\n",
      "Epoch 2 | Step 1505100 | Avg Loss: 0.0148 | Grad Norm: 0.00773383\n",
      "Epoch 2 | Step 1505200 | Avg Loss: 0.0150 | Grad Norm: 0.00813089\n",
      "Epoch 2 | Step 1505300 | Avg Loss: 0.0152 | Grad Norm: 0.00937066\n",
      "Epoch 2 | Step 1505400 | Avg Loss: 0.0151 | Grad Norm: 0.01086337\n",
      "Epoch 2 | Step 1505500 | Avg Loss: 0.0149 | Grad Norm: 0.00978023\n",
      "Epoch 2 | Step 1505600 | Avg Loss: 0.0152 | Grad Norm: 0.00796561\n",
      "Epoch 2 | Step 1505700 | Avg Loss: 0.0156 | Grad Norm: 0.00803368\n",
      "Epoch 2 | Step 1505800 | Avg Loss: 0.0149 | Grad Norm: 0.00925248\n",
      "Epoch 2 | Step 1505900 | Avg Loss: 0.0152 | Grad Norm: 0.00877532\n",
      "Epoch 2 | Step 1506000 | Avg Loss: 0.0153 | Grad Norm: 0.00766690\n",
      "Epoch 2 | Step 1506100 | Avg Loss: 0.0149 | Grad Norm: 0.00795989\n",
      "Epoch 2 | Step 1506200 | Avg Loss: 0.0148 | Grad Norm: 0.00869285\n",
      "Epoch 2 | Step 1506300 | Avg Loss: 0.0148 | Grad Norm: 0.00801123\n",
      "Epoch 2 | Step 1506400 | Avg Loss: 0.0149 | Grad Norm: 0.00817349\n",
      "Epoch 2 | Step 1506500 | Avg Loss: 0.0146 | Grad Norm: 0.00847391\n",
      "Epoch 2 | Step 1506600 | Avg Loss: 0.0148 | Grad Norm: 0.00835006\n",
      "Epoch 2 | Step 1506700 | Avg Loss: 0.0148 | Grad Norm: 0.00804515\n",
      "Epoch 2 | Step 1506800 | Avg Loss: 0.0143 | Grad Norm: 0.00912035\n",
      "Epoch 2 | Step 1506900 | Avg Loss: 0.0144 | Grad Norm: 0.00771318\n",
      "Epoch 2 | Step 1507000 | Avg Loss: 0.0142 | Grad Norm: 0.00930416\n",
      "Epoch 2 | Step 1507100 | Avg Loss: 0.0143 | Grad Norm: 0.00829921\n",
      "Epoch 2 | Step 1507200 | Avg Loss: 0.0143 | Grad Norm: 0.00856601\n",
      "Epoch 2 | Step 1507300 | Avg Loss: 0.0148 | Grad Norm: 0.00763746\n",
      "Epoch 2 | Step 1507400 | Avg Loss: 0.0148 | Grad Norm: 0.00841050\n",
      "Epoch 2 | Step 1507500 | Avg Loss: 0.0149 | Grad Norm: 0.00771924\n",
      "Epoch 2 | Step 1507600 | Avg Loss: 0.0144 | Grad Norm: 0.00745962\n",
      "Epoch 2 | Step 1507700 | Avg Loss: 0.0145 | Grad Norm: 0.00904527\n",
      "Epoch 2 | Step 1507800 | Avg Loss: 0.0148 | Grad Norm: 0.01194032\n",
      "Epoch 2 | Step 1507900 | Avg Loss: 0.0147 | Grad Norm: 0.00987895\n",
      "Epoch 2 | Step 1508000 | Avg Loss: 0.0146 | Grad Norm: 0.00669163\n",
      "Epoch 2 | Step 1508100 | Avg Loss: 0.0150 | Grad Norm: 0.00833349\n",
      "Epoch 2 | Step 1508200 | Avg Loss: 0.0150 | Grad Norm: 0.00870770\n",
      "Epoch 2 | Step 1508300 | Avg Loss: 0.0150 | Grad Norm: 0.00745477\n",
      "Epoch 2 | Step 1508400 | Avg Loss: 0.0151 | Grad Norm: 0.00904443\n",
      "Epoch 2 | Step 1508500 | Avg Loss: 0.0149 | Grad Norm: 0.00695264\n",
      "Epoch 2 | Step 1508600 | Avg Loss: 0.0151 | Grad Norm: 0.00891512\n",
      "Epoch 2 | Step 1508700 | Avg Loss: 0.0147 | Grad Norm: 0.00814130\n",
      "Epoch 2 | Step 1508800 | Avg Loss: 0.0146 | Grad Norm: 0.00747280\n",
      "Epoch 2 | Step 1508900 | Avg Loss: 0.0150 | Grad Norm: 0.00807924\n",
      "Epoch 2 | Step 1509000 | Avg Loss: 0.0148 | Grad Norm: 0.00809224\n",
      "Epoch 2 | Step 1509100 | Avg Loss: 0.0150 | Grad Norm: 0.00818476\n",
      "Epoch 2 | Step 1509200 | Avg Loss: 0.0153 | Grad Norm: 0.00683765\n",
      "Epoch 2 | Step 1509300 | Avg Loss: 0.0157 | Grad Norm: 0.00847611\n",
      "Epoch 2 | Step 1509400 | Avg Loss: 0.0150 | Grad Norm: 0.00697147\n",
      "Epoch 2 | Step 1509500 | Avg Loss: 0.0144 | Grad Norm: 0.00681059\n",
      "Epoch 2 | Step 1509600 | Avg Loss: 0.0146 | Grad Norm: 0.00882877\n",
      "Epoch 2 | Step 1509700 | Avg Loss: 0.0147 | Grad Norm: 0.00693626\n",
      "Epoch 2 | Step 1509800 | Avg Loss: 0.0150 | Grad Norm: 0.00791030\n",
      "Epoch 2 | Step 1509900 | Avg Loss: 0.0151 | Grad Norm: 0.00968232\n",
      "Epoch 2 | Step 1510000 | Avg Loss: 0.0151 | Grad Norm: 0.00842083\n",
      "Epoch 2 | Step 1510100 | Avg Loss: 0.0153 | Grad Norm: 0.00726604\n",
      "Epoch 2 | Step 1510200 | Avg Loss: 0.0151 | Grad Norm: 0.00762989\n",
      "Epoch 2 | Step 1510300 | Avg Loss: 0.0150 | Grad Norm: 0.00730375\n",
      "Epoch 2 | Step 1510400 | Avg Loss: 0.0152 | Grad Norm: 0.00791418\n",
      "Epoch 2 | Step 1510500 | Avg Loss: 0.0152 | Grad Norm: 0.00944840\n",
      "Epoch 2 | Step 1510600 | Avg Loss: 0.0152 | Grad Norm: 0.00862515\n",
      "Epoch 2 | Step 1510700 | Avg Loss: 0.0151 | Grad Norm: 0.00762201\n",
      "Epoch 2 | Step 1510800 | Avg Loss: 0.0152 | Grad Norm: 0.00836665\n",
      "Epoch 2 | Step 1510900 | Avg Loss: 0.0150 | Grad Norm: 0.00902836\n",
      "Epoch 2 | Step 1511000 | Avg Loss: 0.0154 | Grad Norm: 0.00860900\n",
      "Epoch 2 | Step 1511100 | Avg Loss: 0.0157 | Grad Norm: 0.00743421\n",
      "Epoch 2 | Step 1511200 | Avg Loss: 0.0151 | Grad Norm: 0.01032692\n",
      "Epoch 2 | Step 1511300 | Avg Loss: 0.0149 | Grad Norm: 0.00830686\n",
      "Epoch 2 | Step 1511400 | Avg Loss: 0.0150 | Grad Norm: 0.00844492\n",
      "Epoch 2 | Step 1511500 | Avg Loss: 0.0152 | Grad Norm: 0.00927047\n",
      "Epoch 2 | Step 1511600 | Avg Loss: 0.0152 | Grad Norm: 0.00856966\n",
      "Epoch 2 | Step 1511700 | Avg Loss: 0.0156 | Grad Norm: 0.00724034\n",
      "Epoch 2 | Step 1511800 | Avg Loss: 0.0154 | Grad Norm: 0.00847510\n",
      "Epoch 2 | Step 1511900 | Avg Loss: 0.0152 | Grad Norm: 0.00787923\n",
      "Epoch 2 | Step 1512000 | Avg Loss: 0.0151 | Grad Norm: 0.00879051\n",
      "Epoch 2 | Step 1512100 | Avg Loss: 0.0151 | Grad Norm: 0.00848202\n",
      "Epoch 2 | Step 1512200 | Avg Loss: 0.0153 | Grad Norm: 0.00867510\n",
      "Epoch 2 | Step 1512300 | Avg Loss: 0.0151 | Grad Norm: 0.00677566\n",
      "Epoch 2 | Step 1512400 | Avg Loss: 0.0146 | Grad Norm: 0.00740333\n",
      "Epoch 2 | Step 1512500 | Avg Loss: 0.0145 | Grad Norm: 0.00931609\n",
      "Epoch 2 | Step 1512600 | Avg Loss: 0.0148 | Grad Norm: 0.01067381\n",
      "Epoch 2 | Step 1512700 | Avg Loss: 0.0151 | Grad Norm: 0.00841137\n",
      "Epoch 2 | Step 1512800 | Avg Loss: 0.0155 | Grad Norm: 0.00891629\n",
      "Epoch 2 | Step 1512900 | Avg Loss: 0.0152 | Grad Norm: 0.00714679\n",
      "Epoch 2 | Step 1513000 | Avg Loss: 0.0151 | Grad Norm: 0.00794711\n",
      "Epoch 2 | Step 1513100 | Avg Loss: 0.0153 | Grad Norm: 0.00789457\n",
      "Epoch 2 | Step 1513200 | Avg Loss: 0.0150 | Grad Norm: 0.00996010\n",
      "Epoch 2 | Step 1513300 | Avg Loss: 0.0149 | Grad Norm: 0.00827953\n",
      "Epoch 2 | Step 1513400 | Avg Loss: 0.0147 | Grad Norm: 0.00976246\n",
      "Epoch 2 | Step 1513500 | Avg Loss: 0.0150 | Grad Norm: 0.00805575\n",
      "Epoch 2 | Step 1513600 | Avg Loss: 0.0152 | Grad Norm: 0.00856558\n",
      "Epoch 2 | Step 1513700 | Avg Loss: 0.0147 | Grad Norm: 0.00786337\n",
      "Epoch 2 | Step 1513800 | Avg Loss: 0.0143 | Grad Norm: 0.00887016\n",
      "Epoch 2 | Step 1513900 | Avg Loss: 0.0146 | Grad Norm: 0.00819263\n",
      "Epoch 2 | Step 1514000 | Avg Loss: 0.0146 | Grad Norm: 0.00766919\n",
      "Epoch 2 | Step 1514100 | Avg Loss: 0.0145 | Grad Norm: 0.00809067\n",
      "Epoch 2 | Step 1514200 | Avg Loss: 0.0152 | Grad Norm: 0.00783160\n",
      "Epoch 2 | Step 1514300 | Avg Loss: 0.0147 | Grad Norm: 0.00816401\n",
      "Epoch 2 | Step 1514400 | Avg Loss: 0.0149 | Grad Norm: 0.00864295\n",
      "Epoch 2 | Step 1514500 | Avg Loss: 0.0144 | Grad Norm: 0.00867894\n",
      "Epoch 2 | Step 1514600 | Avg Loss: 0.0147 | Grad Norm: 0.00724002\n",
      "Epoch 2 | Step 1514700 | Avg Loss: 0.0148 | Grad Norm: 0.00793108\n",
      "Epoch 2 | Step 1514800 | Avg Loss: 0.0149 | Grad Norm: 0.00910611\n",
      "Epoch 2 | Step 1514900 | Avg Loss: 0.0149 | Grad Norm: 0.00768240\n",
      "Epoch 2 | Step 1515000 | Avg Loss: 0.0149 | Grad Norm: 0.00864836\n",
      "Epoch 2 | Step 1515100 | Avg Loss: 0.0151 | Grad Norm: 0.00946000\n",
      "Epoch 2 | Step 1515200 | Avg Loss: 0.0153 | Grad Norm: 0.00980392\n",
      "Epoch 2 | Step 1515300 | Avg Loss: 0.0154 | Grad Norm: 0.00799278\n",
      "Epoch 2 | Step 1515400 | Avg Loss: 0.0151 | Grad Norm: 0.00839970\n",
      "Epoch 2 | Step 1515500 | Avg Loss: 0.0153 | Grad Norm: 0.00905110\n",
      "Epoch 2 | Step 1515600 | Avg Loss: 0.0147 | Grad Norm: 0.00752556\n",
      "Epoch 2 | Step 1515700 | Avg Loss: 0.0150 | Grad Norm: 0.00810526\n",
      "Epoch 2 | Step 1515800 | Avg Loss: 0.0152 | Grad Norm: 0.00877268\n",
      "Epoch 2 | Step 1515900 | Avg Loss: 0.0148 | Grad Norm: 0.00905288\n",
      "Epoch 2 | Step 1516000 | Avg Loss: 0.0151 | Grad Norm: 0.01011223\n",
      "Epoch 2 | Step 1516100 | Avg Loss: 0.0153 | Grad Norm: 0.00833874\n",
      "Epoch 2 | Step 1516200 | Avg Loss: 0.0154 | Grad Norm: 0.00941201\n",
      "Epoch 2 | Step 1516300 | Avg Loss: 0.0152 | Grad Norm: 0.00736385\n",
      "Epoch 2 | Step 1516400 | Avg Loss: 0.0153 | Grad Norm: 0.01071392\n",
      "Epoch 2 | Step 1516500 | Avg Loss: 0.0151 | Grad Norm: 0.00855470\n",
      "Epoch 2 | Step 1516600 | Avg Loss: 0.0149 | Grad Norm: 0.00780902\n",
      "Epoch 2 | Step 1516700 | Avg Loss: 0.0151 | Grad Norm: 0.00893208\n",
      "Epoch 2 | Step 1516800 | Avg Loss: 0.0142 | Grad Norm: 0.00871208\n",
      "Epoch 2 | Step 1516900 | Avg Loss: 0.0146 | Grad Norm: 0.00889357\n",
      "Epoch 2 | Step 1517000 | Avg Loss: 0.0148 | Grad Norm: 0.00702045\n",
      "Epoch 2 | Step 1517100 | Avg Loss: 0.0147 | Grad Norm: 0.00864935\n",
      "Epoch 2 | Step 1517200 | Avg Loss: 0.0147 | Grad Norm: 0.00786488\n",
      "Epoch 2 | Step 1517300 | Avg Loss: 0.0150 | Grad Norm: 0.00702759\n",
      "Epoch 2 | Step 1517400 | Avg Loss: 0.0149 | Grad Norm: 0.00856007\n",
      "Epoch 2 | Step 1517500 | Avg Loss: 0.0147 | Grad Norm: 0.00646011\n",
      "Epoch 2 | Step 1517600 | Avg Loss: 0.0150 | Grad Norm: 0.00873861\n",
      "Epoch 2 | Step 1517700 | Avg Loss: 0.0148 | Grad Norm: 0.00969642\n",
      "Epoch 2 | Step 1517800 | Avg Loss: 0.0147 | Grad Norm: 0.00840346\n",
      "Epoch 2 | Step 1517900 | Avg Loss: 0.0148 | Grad Norm: 0.00796168\n",
      "Epoch 2 | Step 1518000 | Avg Loss: 0.0148 | Grad Norm: 0.00998324\n",
      "Epoch 2 | Step 1518100 | Avg Loss: 0.0152 | Grad Norm: 0.00865547\n",
      "Epoch 2 | Step 1518200 | Avg Loss: 0.0152 | Grad Norm: 0.00809970\n",
      "Epoch 2 | Step 1518300 | Avg Loss: 0.0153 | Grad Norm: 0.00876565\n",
      "Epoch 2 | Step 1518400 | Avg Loss: 0.0151 | Grad Norm: 0.00982703\n",
      "Epoch 2 | Step 1518500 | Avg Loss: 0.0154 | Grad Norm: 0.00804518\n",
      "Epoch 2 | Step 1518600 | Avg Loss: 0.0155 | Grad Norm: 0.00759023\n",
      "Epoch 2 | Step 1518700 | Avg Loss: 0.0158 | Grad Norm: 0.00929476\n",
      "Epoch 2 | Step 1518800 | Avg Loss: 0.0158 | Grad Norm: 0.00744093\n",
      "Epoch 2 | Step 1518900 | Avg Loss: 0.0156 | Grad Norm: 0.00955233\n",
      "Epoch 2 | Step 1519000 | Avg Loss: 0.0156 | Grad Norm: 0.00687326\n",
      "Epoch 2 | Step 1519100 | Avg Loss: 0.0155 | Grad Norm: 0.00836455\n",
      "Epoch 2 | Step 1519200 | Avg Loss: 0.0149 | Grad Norm: 0.00938075\n",
      "Epoch 2 | Step 1519300 | Avg Loss: 0.0147 | Grad Norm: 0.00819989\n",
      "Epoch 2 | Step 1519400 | Avg Loss: 0.0150 | Grad Norm: 0.00865743\n",
      "Epoch 2 | Step 1519500 | Avg Loss: 0.0153 | Grad Norm: 0.00870681\n",
      "Epoch 2 | Step 1519600 | Avg Loss: 0.0151 | Grad Norm: 0.00981631\n",
      "Epoch 2 | Step 1519700 | Avg Loss: 0.0148 | Grad Norm: 0.00781944\n",
      "Epoch 2 | Step 1519800 | Avg Loss: 0.0148 | Grad Norm: 0.01027003\n",
      "Epoch 2 | Step 1519900 | Avg Loss: 0.0150 | Grad Norm: 0.00781279\n",
      "Epoch 2 | Step 1520000 | Avg Loss: 0.0148 | Grad Norm: 0.00870117\n",
      "Epoch 2 | Step 1520100 | Avg Loss: 0.0154 | Grad Norm: 0.00740542\n",
      "Epoch 2 | Step 1520200 | Avg Loss: 0.0154 | Grad Norm: 0.00832426\n",
      "Epoch 2 | Step 1520300 | Avg Loss: 0.0155 | Grad Norm: 0.00738073\n",
      "Epoch 2 | Step 1520400 | Avg Loss: 0.0157 | Grad Norm: 0.00796124\n",
      "Epoch 2 | Step 1520500 | Avg Loss: 0.0159 | Grad Norm: 0.01039125\n",
      "Epoch 2 | Step 1520600 | Avg Loss: 0.0160 | Grad Norm: 0.00846984\n",
      "Epoch 2 | Step 1520700 | Avg Loss: 0.0155 | Grad Norm: 0.00839243\n",
      "Epoch 2 | Step 1520800 | Avg Loss: 0.0157 | Grad Norm: 0.00767912\n",
      "Epoch 2 | Step 1520900 | Avg Loss: 0.0152 | Grad Norm: 0.00757572\n",
      "Epoch 2 | Step 1521000 | Avg Loss: 0.0155 | Grad Norm: 0.00734947\n",
      "Epoch 2 | Step 1521100 | Avg Loss: 0.0151 | Grad Norm: 0.00927917\n",
      "Epoch 2 | Step 1521200 | Avg Loss: 0.0148 | Grad Norm: 0.00889048\n",
      "Epoch 2 | Step 1521300 | Avg Loss: 0.0150 | Grad Norm: 0.00827886\n",
      "Epoch 2 | Step 1521400 | Avg Loss: 0.0153 | Grad Norm: 0.00968599\n",
      "Epoch 2 | Step 1521500 | Avg Loss: 0.0156 | Grad Norm: 0.01082755\n",
      "Epoch 2 | Step 1521600 | Avg Loss: 0.0156 | Grad Norm: 0.00758692\n",
      "Epoch 2 | Step 1521700 | Avg Loss: 0.0156 | Grad Norm: 0.00761816\n",
      "Epoch 2 | Step 1521800 | Avg Loss: 0.0156 | Grad Norm: 0.00802944\n",
      "Epoch 2 | Step 1521900 | Avg Loss: 0.0156 | Grad Norm: 0.01191293\n",
      "Epoch 2 | Step 1522000 | Avg Loss: 0.0153 | Grad Norm: 0.00757195\n",
      "Epoch 2 | Step 1522100 | Avg Loss: 0.0151 | Grad Norm: 0.01003479\n",
      "Epoch 2 | Step 1522200 | Avg Loss: 0.0152 | Grad Norm: 0.00755946\n",
      "Epoch 2 | Step 1522300 | Avg Loss: 0.0153 | Grad Norm: 0.00809297\n",
      "Epoch 2 | Step 1522400 | Avg Loss: 0.0154 | Grad Norm: 0.00832581\n",
      "Epoch 2 | Step 1522500 | Avg Loss: 0.0153 | Grad Norm: 0.00843104\n",
      "Epoch 2 | Step 1522600 | Avg Loss: 0.0153 | Grad Norm: 0.01103122\n",
      "Epoch 2 | Step 1522700 | Avg Loss: 0.0154 | Grad Norm: 0.01055525\n",
      "Epoch 2 | Step 1522800 | Avg Loss: 0.0154 | Grad Norm: 0.00712788\n",
      "Epoch 2 | Step 1522900 | Avg Loss: 0.0154 | Grad Norm: 0.00781081\n",
      "Epoch 2 | Step 1523000 | Avg Loss: 0.0152 | Grad Norm: 0.00942084\n",
      "Epoch 2 | Step 1523100 | Avg Loss: 0.0153 | Grad Norm: 0.00813674\n",
      "Epoch 2 | Step 1523200 | Avg Loss: 0.0150 | Grad Norm: 0.00919629\n",
      "Epoch 2 | Step 1523300 | Avg Loss: 0.0150 | Grad Norm: 0.00731305\n",
      "Epoch 2 | Step 1523400 | Avg Loss: 0.0149 | Grad Norm: 0.00879228\n",
      "Epoch 2 | Step 1523500 | Avg Loss: 0.0150 | Grad Norm: 0.00874569\n",
      "Epoch 2 | Step 1523600 | Avg Loss: 0.0151 | Grad Norm: 0.00785086\n",
      "Epoch 2 | Step 1523700 | Avg Loss: 0.0148 | Grad Norm: 0.00789424\n",
      "Epoch 2 | Step 1523800 | Avg Loss: 0.0151 | Grad Norm: 0.00840265\n",
      "Epoch 2 | Step 1523900 | Avg Loss: 0.0150 | Grad Norm: 0.00854255\n",
      "Epoch 2 | Step 1524000 | Avg Loss: 0.0151 | Grad Norm: 0.00778348\n",
      "Epoch 2 | Step 1524100 | Avg Loss: 0.0154 | Grad Norm: 0.00900280\n",
      "Epoch 2 | Step 1524200 | Avg Loss: 0.0152 | Grad Norm: 0.00952513\n",
      "Epoch 2 | Step 1524300 | Avg Loss: 0.0153 | Grad Norm: 0.00779491\n",
      "Epoch 2 | Step 1524400 | Avg Loss: 0.0157 | Grad Norm: 0.00870708\n",
      "Epoch 2 | Step 1524500 | Avg Loss: 0.0153 | Grad Norm: 0.00926893\n",
      "Epoch 2 | Step 1524600 | Avg Loss: 0.0153 | Grad Norm: 0.00961061\n",
      "Epoch 2 | Step 1524700 | Avg Loss: 0.0153 | Grad Norm: 0.00849274\n",
      "Epoch 2 | Step 1524800 | Avg Loss: 0.0152 | Grad Norm: 0.00833405\n",
      "Epoch 2 | Step 1524900 | Avg Loss: 0.0152 | Grad Norm: 0.00801272\n",
      "Epoch 2 | Step 1525000 | Avg Loss: 0.0151 | Grad Norm: 0.00847686\n",
      "Epoch 2 | Step 1525100 | Avg Loss: 0.0148 | Grad Norm: 0.00964562\n",
      "Epoch 2 | Step 1525200 | Avg Loss: 0.0149 | Grad Norm: 0.00795867\n",
      "Epoch 2 | Step 1525300 | Avg Loss: 0.0150 | Grad Norm: 0.00968046\n",
      "Epoch 2 | Step 1525400 | Avg Loss: 0.0152 | Grad Norm: 0.00782669\n",
      "Epoch 2 | Step 1525500 | Avg Loss: 0.0154 | Grad Norm: 0.01074428\n",
      "Epoch 2 | Step 1525600 | Avg Loss: 0.0157 | Grad Norm: 0.00850895\n",
      "Epoch 2 | Step 1525700 | Avg Loss: 0.0156 | Grad Norm: 0.00871060\n",
      "Epoch 2 | Step 1525800 | Avg Loss: 0.0154 | Grad Norm: 0.00838626\n",
      "Epoch 2 | Step 1525900 | Avg Loss: 0.0153 | Grad Norm: 0.00699847\n",
      "Epoch 2 | Step 1526000 | Avg Loss: 0.0153 | Grad Norm: 0.00705434\n",
      "Epoch 2 | Step 1526100 | Avg Loss: 0.0154 | Grad Norm: 0.00841631\n",
      "Epoch 2 | Step 1526200 | Avg Loss: 0.0155 | Grad Norm: 0.00834569\n",
      "Epoch 2 | Step 1526300 | Avg Loss: 0.0151 | Grad Norm: 0.00782762\n",
      "Epoch 2 | Step 1526400 | Avg Loss: 0.0151 | Grad Norm: 0.00766334\n",
      "Epoch 2 | Step 1526500 | Avg Loss: 0.0151 | Grad Norm: 0.00946722\n",
      "Epoch 2 | Step 1526600 | Avg Loss: 0.0146 | Grad Norm: 0.00864239\n",
      "Epoch 2 | Step 1526700 | Avg Loss: 0.0145 | Grad Norm: 0.00795854\n",
      "Epoch 2 | Step 1526800 | Avg Loss: 0.0144 | Grad Norm: 0.00745353\n",
      "Epoch 2 | Step 1526900 | Avg Loss: 0.0144 | Grad Norm: 0.00625991\n",
      "Epoch 2 | Step 1527000 | Avg Loss: 0.0143 | Grad Norm: 0.00681603\n",
      "Epoch 2 | Step 1527100 | Avg Loss: 0.0143 | Grad Norm: 0.00885566\n",
      "Epoch 2 | Step 1527200 | Avg Loss: 0.0145 | Grad Norm: 0.00764535\n",
      "Epoch 2 | Step 1527300 | Avg Loss: 0.0151 | Grad Norm: 0.00844460\n",
      "Epoch 2 | Step 1527400 | Avg Loss: 0.0152 | Grad Norm: 0.00791949\n",
      "Epoch 2 | Step 1527500 | Avg Loss: 0.0154 | Grad Norm: 0.00968524\n",
      "Epoch 2 | Step 1527600 | Avg Loss: 0.0157 | Grad Norm: 0.00773269\n",
      "Epoch 2 | Step 1527700 | Avg Loss: 0.0155 | Grad Norm: 0.00970492\n",
      "Epoch 2 | Step 1527800 | Avg Loss: 0.0154 | Grad Norm: 0.01053807\n",
      "Epoch 2 | Step 1527900 | Avg Loss: 0.0150 | Grad Norm: 0.00786710\n",
      "Epoch 2 | Step 1528000 | Avg Loss: 0.0153 | Grad Norm: 0.00819963\n",
      "Epoch 2 | Step 1528100 | Avg Loss: 0.0153 | Grad Norm: 0.00855009\n",
      "Epoch 2 | Step 1528200 | Avg Loss: 0.0151 | Grad Norm: 0.01106139\n",
      "Epoch 2 | Step 1528300 | Avg Loss: 0.0151 | Grad Norm: 0.00889829\n",
      "Epoch 2 | Step 1528400 | Avg Loss: 0.0153 | Grad Norm: 0.00701094\n",
      "Epoch 2 | Step 1528500 | Avg Loss: 0.0154 | Grad Norm: 0.00898797\n",
      "Epoch 2 | Step 1528600 | Avg Loss: 0.0152 | Grad Norm: 0.00725249\n",
      "Epoch 2 | Step 1528700 | Avg Loss: 0.0151 | Grad Norm: 0.00727216\n",
      "Epoch 2 | Step 1528800 | Avg Loss: 0.0151 | Grad Norm: 0.00778318\n",
      "Epoch 2 | Step 1528900 | Avg Loss: 0.0154 | Grad Norm: 0.00812301\n",
      "Epoch 2 | Step 1529000 | Avg Loss: 0.0150 | Grad Norm: 0.00871782\n",
      "Epoch 2 | Step 1529100 | Avg Loss: 0.0151 | Grad Norm: 0.00890615\n",
      "Epoch 2 | Step 1529200 | Avg Loss: 0.0151 | Grad Norm: 0.00782026\n",
      "Epoch 2 | Step 1529300 | Avg Loss: 0.0155 | Grad Norm: 0.00779460\n",
      "Epoch 2 | Step 1529400 | Avg Loss: 0.0155 | Grad Norm: 0.00885075\n",
      "Epoch 2 | Step 1529500 | Avg Loss: 0.0153 | Grad Norm: 0.00976202\n",
      "Epoch 2 | Step 1529600 | Avg Loss: 0.0151 | Grad Norm: 0.00894738\n",
      "Epoch 2 | Step 1529700 | Avg Loss: 0.0149 | Grad Norm: 0.00917272\n",
      "Epoch 2 | Step 1529800 | Avg Loss: 0.0150 | Grad Norm: 0.00802871\n",
      "Epoch 2 | Step 1529900 | Avg Loss: 0.0152 | Grad Norm: 0.00759292\n",
      "Epoch 2 | Step 1530000 | Avg Loss: 0.0152 | Grad Norm: 0.00776053\n",
      "Epoch 2 | Step 1530100 | Avg Loss: 0.0156 | Grad Norm: 0.00831604\n",
      "Epoch 2 | Step 1530200 | Avg Loss: 0.0153 | Grad Norm: 0.00726241\n",
      "Epoch 2 | Step 1530300 | Avg Loss: 0.0150 | Grad Norm: 0.00978514\n",
      "Epoch 2 | Step 1530400 | Avg Loss: 0.0151 | Grad Norm: 0.00821442\n",
      "Epoch 2 | Step 1530500 | Avg Loss: 0.0151 | Grad Norm: 0.00861913\n",
      "Epoch 2 | Step 1530600 | Avg Loss: 0.0151 | Grad Norm: 0.00710449\n",
      "Epoch 2 | Step 1530700 | Avg Loss: 0.0151 | Grad Norm: 0.00799356\n",
      "Epoch 2 | Step 1530800 | Avg Loss: 0.0156 | Grad Norm: 0.00765255\n",
      "Epoch 2 | Step 1530900 | Avg Loss: 0.0157 | Grad Norm: 0.00927084\n",
      "Epoch 2 | Step 1531000 | Avg Loss: 0.0154 | Grad Norm: 0.00788660\n",
      "Epoch 2 | Step 1531100 | Avg Loss: 0.0150 | Grad Norm: 0.00920143\n",
      "Epoch 2 | Step 1531200 | Avg Loss: 0.0150 | Grad Norm: 0.00702336\n",
      "Epoch 2 | Step 1531300 | Avg Loss: 0.0150 | Grad Norm: 0.00915636\n",
      "Epoch 2 | Step 1531400 | Avg Loss: 0.0151 | Grad Norm: 0.00772532\n",
      "Epoch 2 | Step 1531500 | Avg Loss: 0.0148 | Grad Norm: 0.00962373\n",
      "Epoch 2 | Step 1531600 | Avg Loss: 0.0148 | Grad Norm: 0.00859735\n",
      "Epoch 2 | Step 1531700 | Avg Loss: 0.0151 | Grad Norm: 0.00694581\n",
      "Epoch 2 | Step 1531800 | Avg Loss: 0.0157 | Grad Norm: 0.00813962\n",
      "Epoch 2 | Step 1531900 | Avg Loss: 0.0156 | Grad Norm: 0.00907757\n",
      "Epoch 2 | Step 1532000 | Avg Loss: 0.0152 | Grad Norm: 0.00761585\n",
      "Epoch 2 | Step 1532100 | Avg Loss: 0.0150 | Grad Norm: 0.01235765\n",
      "Epoch 2 | Step 1532200 | Avg Loss: 0.0155 | Grad Norm: 0.00848376\n",
      "Epoch 2 | Step 1532300 | Avg Loss: 0.0152 | Grad Norm: 0.00833696\n",
      "Epoch 2 | Step 1532400 | Avg Loss: 0.0151 | Grad Norm: 0.00903503\n",
      "Epoch 2 | Step 1532500 | Avg Loss: 0.0150 | Grad Norm: 0.00898629\n",
      "Epoch 2 | Step 1532600 | Avg Loss: 0.0148 | Grad Norm: 0.00825909\n",
      "Epoch 2 | Step 1532700 | Avg Loss: 0.0150 | Grad Norm: 0.00806455\n",
      "Epoch 2 | Step 1532800 | Avg Loss: 0.0150 | Grad Norm: 0.00839392\n",
      "Epoch 2 | Step 1532900 | Avg Loss: 0.0151 | Grad Norm: 0.00691358\n",
      "Epoch 2 | Step 1533000 | Avg Loss: 0.0153 | Grad Norm: 0.00977903\n",
      "Epoch 2 | Step 1533100 | Avg Loss: 0.0158 | Grad Norm: 0.00799801\n",
      "Epoch 2 | Step 1533200 | Avg Loss: 0.0160 | Grad Norm: 0.00922640\n",
      "Epoch 2 | Step 1533300 | Avg Loss: 0.0161 | Grad Norm: 0.00752462\n",
      "Epoch 2 | Step 1533400 | Avg Loss: 0.0165 | Grad Norm: 0.00835599\n",
      "Epoch 2 | Step 1533500 | Avg Loss: 0.0161 | Grad Norm: 0.00792330\n",
      "Epoch 2 | Step 1533600 | Avg Loss: 0.0158 | Grad Norm: 0.00946140\n",
      "Epoch 2 | Step 1533700 | Avg Loss: 0.0158 | Grad Norm: 0.00848396\n",
      "Epoch 2 | Step 1533800 | Avg Loss: 0.0160 | Grad Norm: 0.00874005\n",
      "Epoch 2 | Step 1533900 | Avg Loss: 0.0159 | Grad Norm: 0.00756191\n",
      "Epoch 2 | Step 1534000 | Avg Loss: 0.0154 | Grad Norm: 0.00760466\n",
      "Epoch 2 | Step 1534100 | Avg Loss: 0.0153 | Grad Norm: 0.00746581\n",
      "Epoch 2 | Step 1534200 | Avg Loss: 0.0152 | Grad Norm: 0.00869945\n",
      "Epoch 2 | Step 1534300 | Avg Loss: 0.0151 | Grad Norm: 0.00919257\n",
      "Epoch 2 | Step 1534400 | Avg Loss: 0.0153 | Grad Norm: 0.00846639\n",
      "Epoch 2 | Step 1534500 | Avg Loss: 0.0155 | Grad Norm: 0.00884032\n",
      "Epoch 2 | Step 1534600 | Avg Loss: 0.0153 | Grad Norm: 0.00770801\n",
      "Epoch 2 | Step 1534700 | Avg Loss: 0.0155 | Grad Norm: 0.00817141\n",
      "Epoch 2 | Step 1534800 | Avg Loss: 0.0153 | Grad Norm: 0.00931149\n",
      "Epoch 2 | Step 1534900 | Avg Loss: 0.0152 | Grad Norm: 0.00831365\n",
      "Epoch 2 | Step 1535000 | Avg Loss: 0.0148 | Grad Norm: 0.00800004\n",
      "Epoch 2 | Step 1535100 | Avg Loss: 0.0146 | Grad Norm: 0.00848775\n",
      "Epoch 2 | Step 1535200 | Avg Loss: 0.0151 | Grad Norm: 0.00726272\n",
      "Epoch 2 | Step 1535300 | Avg Loss: 0.0153 | Grad Norm: 0.00797908\n",
      "Epoch 2 | Step 1535400 | Avg Loss: 0.0152 | Grad Norm: 0.01103067\n",
      "Epoch 2 | Step 1535500 | Avg Loss: 0.0158 | Grad Norm: 0.01009609\n",
      "Epoch 2 | Step 1535600 | Avg Loss: 0.0156 | Grad Norm: 0.00967724\n",
      "Epoch 2 | Step 1535700 | Avg Loss: 0.0154 | Grad Norm: 0.00709480\n",
      "Epoch 2 | Step 1535800 | Avg Loss: 0.0155 | Grad Norm: 0.00733485\n",
      "Epoch 2 | Step 1535900 | Avg Loss: 0.0155 | Grad Norm: 0.00925370\n",
      "Epoch 2 | Step 1536000 | Avg Loss: 0.0154 | Grad Norm: 0.00891483\n",
      "Epoch 2 | Step 1536100 | Avg Loss: 0.0153 | Grad Norm: 0.00894408\n",
      "Epoch 2 | Step 1536200 | Avg Loss: 0.0154 | Grad Norm: 0.00999353\n",
      "Epoch 2 | Step 1536300 | Avg Loss: 0.0155 | Grad Norm: 0.00889485\n",
      "Epoch 2 | Step 1536400 | Avg Loss: 0.0156 | Grad Norm: 0.00773913\n",
      "Epoch 2 | Step 1536500 | Avg Loss: 0.0152 | Grad Norm: 0.00799877\n",
      "Epoch 2 | Step 1536600 | Avg Loss: 0.0153 | Grad Norm: 0.00783996\n",
      "Epoch 2 | Step 1536700 | Avg Loss: 0.0151 | Grad Norm: 0.00724770\n",
      "Epoch 2 | Step 1536800 | Avg Loss: 0.0148 | Grad Norm: 0.00759987\n",
      "Epoch 2 | Step 1536900 | Avg Loss: 0.0149 | Grad Norm: 0.00787941\n",
      "Epoch 2 | Step 1537000 | Avg Loss: 0.0154 | Grad Norm: 0.00844844\n",
      "Epoch 2 | Step 1537100 | Avg Loss: 0.0157 | Grad Norm: 0.01007739\n",
      "Epoch 2 | Step 1537200 | Avg Loss: 0.0151 | Grad Norm: 0.00876084\n",
      "Epoch 2 | Step 1537300 | Avg Loss: 0.0150 | Grad Norm: 0.00798709\n",
      "Epoch 2 | Step 1537400 | Avg Loss: 0.0154 | Grad Norm: 0.00772248\n",
      "Epoch 2 | Step 1537500 | Avg Loss: 0.0157 | Grad Norm: 0.00857995\n",
      "Epoch 2 | Step 1537600 | Avg Loss: 0.0156 | Grad Norm: 0.00884288\n",
      "Epoch 2 | Step 1537700 | Avg Loss: 0.0157 | Grad Norm: 0.00886445\n",
      "Epoch 2 | Step 1537800 | Avg Loss: 0.0153 | Grad Norm: 0.00839098\n",
      "Epoch 2 | Step 1537900 | Avg Loss: 0.0155 | Grad Norm: 0.00824093\n",
      "Epoch 2 | Step 1538000 | Avg Loss: 0.0155 | Grad Norm: 0.01009426\n",
      "Epoch 2 | Step 1538100 | Avg Loss: 0.0154 | Grad Norm: 0.00811964\n",
      "Epoch 2 | Step 1538200 | Avg Loss: 0.0152 | Grad Norm: 0.00815330\n",
      "Epoch 2 | Step 1538300 | Avg Loss: 0.0154 | Grad Norm: 0.00868560\n",
      "Epoch 2 | Step 1538400 | Avg Loss: 0.0153 | Grad Norm: 0.00812358\n",
      "Epoch 2 | Step 1538500 | Avg Loss: 0.0153 | Grad Norm: 0.00817427\n",
      "Epoch 2 | Step 1538600 | Avg Loss: 0.0151 | Grad Norm: 0.00925162\n",
      "Epoch 2 | Step 1538700 | Avg Loss: 0.0148 | Grad Norm: 0.00762576\n",
      "Epoch 2 | Step 1538800 | Avg Loss: 0.0149 | Grad Norm: 0.00724554\n",
      "Epoch 2 | Step 1538900 | Avg Loss: 0.0150 | Grad Norm: 0.00808508\n",
      "Epoch 2 | Step 1539000 | Avg Loss: 0.0147 | Grad Norm: 0.00905686\n",
      "Epoch 2 | Step 1539100 | Avg Loss: 0.0148 | Grad Norm: 0.00843439\n",
      "Epoch 2 | Step 1539200 | Avg Loss: 0.0149 | Grad Norm: 0.01016044\n",
      "Epoch 2 | Step 1539300 | Avg Loss: 0.0148 | Grad Norm: 0.00796774\n",
      "Epoch 2 | Step 1539400 | Avg Loss: 0.0147 | Grad Norm: 0.00834543\n",
      "Epoch 2 | Step 1539500 | Avg Loss: 0.0145 | Grad Norm: 0.00694397\n",
      "Epoch 2 | Step 1539600 | Avg Loss: 0.0147 | Grad Norm: 0.00814721\n",
      "Epoch 2 | Step 1539700 | Avg Loss: 0.0152 | Grad Norm: 0.00803685\n",
      "Epoch 2 | Step 1539800 | Avg Loss: 0.0151 | Grad Norm: 0.00838976\n",
      "Epoch 2 | Step 1539900 | Avg Loss: 0.0151 | Grad Norm: 0.00870313\n",
      "Epoch 2 | Step 1540000 | Avg Loss: 0.0154 | Grad Norm: 0.00799383\n",
      "Epoch 2 | Step 1540100 | Avg Loss: 0.0155 | Grad Norm: 0.00838606\n",
      "Epoch 2 | Step 1540200 | Avg Loss: 0.0153 | Grad Norm: 0.00918474\n",
      "Epoch 2 | Step 1540300 | Avg Loss: 0.0153 | Grad Norm: 0.00933080\n",
      "Epoch 2 | Step 1540400 | Avg Loss: 0.0152 | Grad Norm: 0.00788984\n",
      "Epoch 2 | Step 1540500 | Avg Loss: 0.0153 | Grad Norm: 0.00815688\n",
      "Epoch 2 | Step 1540600 | Avg Loss: 0.0151 | Grad Norm: 0.00781229\n",
      "Epoch 2 | Step 1540700 | Avg Loss: 0.0155 | Grad Norm: 0.00999992\n",
      "Epoch 2 | Step 1540800 | Avg Loss: 0.0152 | Grad Norm: 0.00921419\n",
      "Epoch 2 | Step 1540900 | Avg Loss: 0.0150 | Grad Norm: 0.00816737\n",
      "Epoch 2 | Step 1541000 | Avg Loss: 0.0147 | Grad Norm: 0.00856359\n",
      "Epoch 2 | Step 1541100 | Avg Loss: 0.0150 | Grad Norm: 0.01218151\n",
      "Epoch 2 | Step 1541200 | Avg Loss: 0.0150 | Grad Norm: 0.00760811\n",
      "Epoch 2 | Step 1541300 | Avg Loss: 0.0148 | Grad Norm: 0.00890431\n",
      "Epoch 2 | Step 1541400 | Avg Loss: 0.0147 | Grad Norm: 0.00924024\n",
      "Epoch 2 | Step 1541500 | Avg Loss: 0.0147 | Grad Norm: 0.00835486\n",
      "Epoch 2 | Step 1541600 | Avg Loss: 0.0149 | Grad Norm: 0.00845975\n",
      "Epoch 2 | Step 1541700 | Avg Loss: 0.0148 | Grad Norm: 0.01014525\n",
      "Epoch 2 | Step 1541800 | Avg Loss: 0.0151 | Grad Norm: 0.00875691\n",
      "Epoch 2 | Step 1541900 | Avg Loss: 0.0154 | Grad Norm: 0.00906119\n",
      "Epoch 2 | Step 1542000 | Avg Loss: 0.0152 | Grad Norm: 0.00890501\n",
      "Epoch 2 | Step 1542100 | Avg Loss: 0.0148 | Grad Norm: 0.00754210\n",
      "Epoch 2 | Step 1542200 | Avg Loss: 0.0147 | Grad Norm: 0.00772153\n",
      "Epoch 2 | Step 1542300 | Avg Loss: 0.0149 | Grad Norm: 0.00902294\n",
      "Epoch 2 | Step 1542400 | Avg Loss: 0.0151 | Grad Norm: 0.00834390\n",
      "Epoch 2 | Step 1542500 | Avg Loss: 0.0151 | Grad Norm: 0.00713846\n",
      "Epoch 2 | Step 1542600 | Avg Loss: 0.0154 | Grad Norm: 0.00740186\n",
      "Epoch 2 | Step 1542700 | Avg Loss: 0.0157 | Grad Norm: 0.00953379\n",
      "Epoch 2 | Step 1542800 | Avg Loss: 0.0152 | Grad Norm: 0.00946594\n",
      "Epoch 2 | Step 1542900 | Avg Loss: 0.0154 | Grad Norm: 0.00810101\n",
      "Epoch 2 | Step 1543000 | Avg Loss: 0.0152 | Grad Norm: 0.00792165\n",
      "Epoch 2 | Step 1543100 | Avg Loss: 0.0152 | Grad Norm: 0.00905166\n",
      "Epoch 2 | Step 1543200 | Avg Loss: 0.0153 | Grad Norm: 0.00775128\n",
      "Epoch 2 | Step 1543300 | Avg Loss: 0.0151 | Grad Norm: 0.00837310\n",
      "Epoch 2 | Step 1543400 | Avg Loss: 0.0150 | Grad Norm: 0.00823974\n",
      "Epoch 2 | Step 1543500 | Avg Loss: 0.0151 | Grad Norm: 0.00861131\n",
      "Epoch 2 | Step 1543600 | Avg Loss: 0.0151 | Grad Norm: 0.00751595\n",
      "Epoch 2 | Step 1543700 | Avg Loss: 0.0152 | Grad Norm: 0.00909551\n",
      "Epoch 2 | Step 1543800 | Avg Loss: 0.0152 | Grad Norm: 0.00985889\n",
      "Epoch 2 | Step 1543900 | Avg Loss: 0.0154 | Grad Norm: 0.00810453\n",
      "Epoch 2 | Step 1544000 | Avg Loss: 0.0155 | Grad Norm: 0.00842402\n",
      "Epoch 2 | Step 1544100 | Avg Loss: 0.0155 | Grad Norm: 0.00731254\n",
      "Epoch 2 | Step 1544200 | Avg Loss: 0.0158 | Grad Norm: 0.00814417\n",
      "Epoch 2 | Step 1544300 | Avg Loss: 0.0155 | Grad Norm: 0.00810054\n",
      "Epoch 2 | Step 1544400 | Avg Loss: 0.0155 | Grad Norm: 0.00754213\n",
      "Epoch 2 | Step 1544500 | Avg Loss: 0.0156 | Grad Norm: 0.00668184\n",
      "Epoch 2 | Step 1544600 | Avg Loss: 0.0158 | Grad Norm: 0.00845495\n",
      "Epoch 2 | Step 1544700 | Avg Loss: 0.0155 | Grad Norm: 0.00864100\n",
      "Epoch 2 | Step 1544800 | Avg Loss: 0.0155 | Grad Norm: 0.00933839\n",
      "Epoch 2 | Step 1544900 | Avg Loss: 0.0152 | Grad Norm: 0.00900701\n",
      "Epoch 2 | Step 1545000 | Avg Loss: 0.0151 | Grad Norm: 0.00950074\n",
      "Epoch 2 | Step 1545100 | Avg Loss: 0.0151 | Grad Norm: 0.00881963\n",
      "Epoch 2 | Step 1545200 | Avg Loss: 0.0148 | Grad Norm: 0.01074254\n",
      "Epoch 2 | Step 1545300 | Avg Loss: 0.0148 | Grad Norm: 0.00916755\n",
      "Epoch 2 | Step 1545400 | Avg Loss: 0.0146 | Grad Norm: 0.00813510\n",
      "Epoch 2 | Step 1545500 | Avg Loss: 0.0146 | Grad Norm: 0.00864135\n",
      "Epoch 2 | Step 1545600 | Avg Loss: 0.0146 | Grad Norm: 0.00696270\n",
      "Epoch 2 | Step 1545700 | Avg Loss: 0.0147 | Grad Norm: 0.00867422\n",
      "Epoch 2 | Step 1545800 | Avg Loss: 0.0148 | Grad Norm: 0.00761563\n",
      "Epoch 2 | Step 1545900 | Avg Loss: 0.0146 | Grad Norm: 0.00771530\n",
      "Epoch 2 | Step 1546000 | Avg Loss: 0.0151 | Grad Norm: 0.00721121\n",
      "Epoch 2 | Step 1546100 | Avg Loss: 0.0147 | Grad Norm: 0.00782070\n",
      "Epoch 2 | Step 1546200 | Avg Loss: 0.0146 | Grad Norm: 0.00728426\n",
      "Epoch 2 | Step 1546300 | Avg Loss: 0.0147 | Grad Norm: 0.00764369\n",
      "Epoch 2 | Step 1546400 | Avg Loss: 0.0149 | Grad Norm: 0.00754926\n",
      "Epoch 2 | Step 1546500 | Avg Loss: 0.0147 | Grad Norm: 0.00739760\n",
      "Epoch 2 | Step 1546600 | Avg Loss: 0.0147 | Grad Norm: 0.01015784\n",
      "Epoch 2 | Step 1546700 | Avg Loss: 0.0149 | Grad Norm: 0.01135945\n",
      "Epoch 2 | Step 1546800 | Avg Loss: 0.0150 | Grad Norm: 0.00838560\n",
      "Epoch 2 | Step 1546900 | Avg Loss: 0.0149 | Grad Norm: 0.00764768\n",
      "Epoch 2 | Step 1547000 | Avg Loss: 0.0145 | Grad Norm: 0.00966165\n",
      "Epoch 2 | Step 1547100 | Avg Loss: 0.0145 | Grad Norm: 0.00885639\n",
      "Epoch 2 | Step 1547200 | Avg Loss: 0.0143 | Grad Norm: 0.00725245\n",
      "Epoch 2 | Step 1547300 | Avg Loss: 0.0146 | Grad Norm: 0.00859111\n",
      "Epoch 2 | Step 1547400 | Avg Loss: 0.0149 | Grad Norm: 0.00778248\n",
      "Epoch 2 | Step 1547500 | Avg Loss: 0.0155 | Grad Norm: 0.00829634\n",
      "Epoch 2 | Step 1547600 | Avg Loss: 0.0151 | Grad Norm: 0.00725508\n",
      "Epoch 2 | Step 1547700 | Avg Loss: 0.0151 | Grad Norm: 0.00734708\n",
      "Epoch 2 | Step 1547800 | Avg Loss: 0.0152 | Grad Norm: 0.00840976\n",
      "Epoch 2 | Step 1547900 | Avg Loss: 0.0153 | Grad Norm: 0.00967281\n",
      "Epoch 2 | Step 1548000 | Avg Loss: 0.0147 | Grad Norm: 0.00880335\n",
      "Epoch 2 | Step 1548100 | Avg Loss: 0.0150 | Grad Norm: 0.00919114\n",
      "Epoch 2 | Step 1548200 | Avg Loss: 0.0151 | Grad Norm: 0.00880435\n",
      "Epoch 2 | Step 1548300 | Avg Loss: 0.0148 | Grad Norm: 0.00835513\n",
      "Epoch 2 | Step 1548400 | Avg Loss: 0.0148 | Grad Norm: 0.00678846\n",
      "Epoch 2 | Step 1548500 | Avg Loss: 0.0150 | Grad Norm: 0.00824142\n",
      "Epoch 2 | Step 1548600 | Avg Loss: 0.0148 | Grad Norm: 0.01017959\n",
      "Epoch 2 | Step 1548700 | Avg Loss: 0.0149 | Grad Norm: 0.00832260\n",
      "Epoch 2 | Step 1548800 | Avg Loss: 0.0151 | Grad Norm: 0.00725472\n",
      "Epoch 2 | Step 1548900 | Avg Loss: 0.0152 | Grad Norm: 0.00805936\n",
      "Epoch 2 | Step 1549000 | Avg Loss: 0.0150 | Grad Norm: 0.00818297\n",
      "Epoch 2 | Step 1549100 | Avg Loss: 0.0146 | Grad Norm: 0.00798897\n",
      "Epoch 2 | Step 1549200 | Avg Loss: 0.0148 | Grad Norm: 0.00771847\n",
      "Epoch 2 | Step 1549300 | Avg Loss: 0.0148 | Grad Norm: 0.00704875\n",
      "Epoch 2 | Step 1549400 | Avg Loss: 0.0148 | Grad Norm: 0.00783616\n",
      "Epoch 2 | Step 1549500 | Avg Loss: 0.0149 | Grad Norm: 0.00791782\n",
      "Epoch 2 | Step 1549600 | Avg Loss: 0.0151 | Grad Norm: 0.00768577\n",
      "Epoch 2 | Step 1549700 | Avg Loss: 0.0153 | Grad Norm: 0.00865828\n",
      "Epoch 2 | Step 1549800 | Avg Loss: 0.0154 | Grad Norm: 0.00908814\n",
      "Epoch 2 | Step 1549900 | Avg Loss: 0.0155 | Grad Norm: 0.00825411\n",
      "Epoch 2 | Step 1550000 | Avg Loss: 0.0159 | Grad Norm: 0.00821363\n",
      "Epoch 2 | Step 1550100 | Avg Loss: 0.0155 | Grad Norm: 0.00955103\n",
      "Epoch 2 | Step 1550200 | Avg Loss: 0.0154 | Grad Norm: 0.00990384\n",
      "Epoch 2 | Step 1550300 | Avg Loss: 0.0150 | Grad Norm: 0.00882692\n",
      "Epoch 2 | Step 1550400 | Avg Loss: 0.0149 | Grad Norm: 0.00860826\n",
      "Epoch 2 | Step 1550500 | Avg Loss: 0.0149 | Grad Norm: 0.00860080\n",
      "Epoch 2 | Step 1550600 | Avg Loss: 0.0149 | Grad Norm: 0.00750035\n",
      "Epoch 2 | Step 1550700 | Avg Loss: 0.0148 | Grad Norm: 0.00762246\n",
      "Epoch 2 | Step 1550800 | Avg Loss: 0.0148 | Grad Norm: 0.00751172\n",
      "Epoch 2 | Step 1550900 | Avg Loss: 0.0151 | Grad Norm: 0.00823988\n",
      "Epoch 2 | Step 1551000 | Avg Loss: 0.0150 | Grad Norm: 0.00699965\n",
      "Epoch 2 | Step 1551100 | Avg Loss: 0.0149 | Grad Norm: 0.00858342\n",
      "Epoch 2 | Step 1551200 | Avg Loss: 0.0151 | Grad Norm: 0.00823315\n",
      "Epoch 2 | Step 1551300 | Avg Loss: 0.0150 | Grad Norm: 0.01008767\n",
      "Epoch 2 | Step 1551400 | Avg Loss: 0.0152 | Grad Norm: 0.00831013\n",
      "Epoch 2 | Step 1551500 | Avg Loss: 0.0150 | Grad Norm: 0.00850924\n",
      "Epoch 2 | Step 1551600 | Avg Loss: 0.0154 | Grad Norm: 0.00796350\n",
      "Epoch 2 | Step 1551700 | Avg Loss: 0.0155 | Grad Norm: 0.00773306\n",
      "Epoch 2 | Step 1551800 | Avg Loss: 0.0153 | Grad Norm: 0.00984792\n",
      "Epoch 2 | Step 1551900 | Avg Loss: 0.0153 | Grad Norm: 0.00697684\n",
      "Epoch 2 | Step 1552000 | Avg Loss: 0.0154 | Grad Norm: 0.00974652\n",
      "Epoch 2 | Step 1552100 | Avg Loss: 0.0152 | Grad Norm: 0.01035834\n",
      "Epoch 2 | Step 1552200 | Avg Loss: 0.0150 | Grad Norm: 0.00967270\n",
      "Epoch 2 | Step 1552300 | Avg Loss: 0.0150 | Grad Norm: 0.00648940\n",
      "Epoch 2 | Step 1552400 | Avg Loss: 0.0150 | Grad Norm: 0.00769345\n",
      "Epoch 2 | Step 1552500 | Avg Loss: 0.0148 | Grad Norm: 0.00955187\n",
      "Epoch 2 | Step 1552600 | Avg Loss: 0.0152 | Grad Norm: 0.01196841\n",
      "Epoch 2 | Step 1552700 | Avg Loss: 0.0151 | Grad Norm: 0.00822164\n",
      "Epoch 2 | Step 1552800 | Avg Loss: 0.0150 | Grad Norm: 0.00882498\n",
      "Epoch 2 | Step 1552900 | Avg Loss: 0.0146 | Grad Norm: 0.00725538\n",
      "Epoch 2 | Step 1553000 | Avg Loss: 0.0145 | Grad Norm: 0.00832356\n",
      "Epoch 2 | Step 1553100 | Avg Loss: 0.0147 | Grad Norm: 0.00864966\n",
      "Epoch 2 | Step 1553200 | Avg Loss: 0.0146 | Grad Norm: 0.00796504\n",
      "Epoch 2 | Step 1553300 | Avg Loss: 0.0146 | Grad Norm: 0.00897789\n",
      "Epoch 2 | Step 1553400 | Avg Loss: 0.0148 | Grad Norm: 0.00840155\n",
      "Epoch 2 | Step 1553500 | Avg Loss: 0.0149 | Grad Norm: 0.00843501\n",
      "Epoch 2 | Step 1553600 | Avg Loss: 0.0150 | Grad Norm: 0.00745329\n",
      "Epoch 2 | Step 1553700 | Avg Loss: 0.0150 | Grad Norm: 0.00747098\n",
      "Epoch 2 | Step 1553800 | Avg Loss: 0.0148 | Grad Norm: 0.00762050\n",
      "Epoch 2 | Step 1553900 | Avg Loss: 0.0151 | Grad Norm: 0.00732058\n",
      "Epoch 2 | Step 1554000 | Avg Loss: 0.0150 | Grad Norm: 0.00890161\n",
      "Epoch 2 | Step 1554100 | Avg Loss: 0.0152 | Grad Norm: 0.00829633\n",
      "Epoch 2 | Step 1554200 | Avg Loss: 0.0155 | Grad Norm: 0.00855103\n",
      "Epoch 2 | Step 1554300 | Avg Loss: 0.0151 | Grad Norm: 0.00854535\n",
      "Epoch 2 | Step 1554400 | Avg Loss: 0.0152 | Grad Norm: 0.00765231\n",
      "Epoch 2 | Step 1554500 | Avg Loss: 0.0149 | Grad Norm: 0.01007026\n",
      "Epoch 2 | Step 1554600 | Avg Loss: 0.0153 | Grad Norm: 0.00947857\n",
      "Epoch 2 | Step 1554700 | Avg Loss: 0.0153 | Grad Norm: 0.00809320\n",
      "Epoch 2 | Step 1554800 | Avg Loss: 0.0155 | Grad Norm: 0.00754725\n",
      "Epoch 2 | Step 1554900 | Avg Loss: 0.0155 | Grad Norm: 0.00885233\n",
      "Epoch 2 | Step 1555000 | Avg Loss: 0.0155 | Grad Norm: 0.00812081\n",
      "Epoch 2 | Step 1555100 | Avg Loss: 0.0152 | Grad Norm: 0.00759767\n",
      "Epoch 2 | Step 1555200 | Avg Loss: 0.0150 | Grad Norm: 0.00843787\n",
      "Epoch 2 | Step 1555300 | Avg Loss: 0.0152 | Grad Norm: 0.00829185\n",
      "Epoch 2 | Step 1555400 | Avg Loss: 0.0153 | Grad Norm: 0.00821531\n",
      "Epoch 2 | Step 1555500 | Avg Loss: 0.0152 | Grad Norm: 0.00817241\n",
      "Epoch 2 | Step 1555600 | Avg Loss: 0.0150 | Grad Norm: 0.00806173\n",
      "Epoch 2 | Step 1555700 | Avg Loss: 0.0153 | Grad Norm: 0.00924986\n",
      "Epoch 2 | Step 1555800 | Avg Loss: 0.0150 | Grad Norm: 0.00679947\n",
      "Epoch 2 | Step 1555900 | Avg Loss: 0.0150 | Grad Norm: 0.00923188\n",
      "Epoch 2 | Step 1556000 | Avg Loss: 0.0152 | Grad Norm: 0.00872690\n",
      "Epoch 2 | Step 1556100 | Avg Loss: 0.0153 | Grad Norm: 0.00738428\n",
      "Epoch 2 | Step 1556200 | Avg Loss: 0.0155 | Grad Norm: 0.00846205\n",
      "Epoch 2 | Step 1556300 | Avg Loss: 0.0152 | Grad Norm: 0.00931296\n",
      "Epoch 2 | Step 1556400 | Avg Loss: 0.0157 | Grad Norm: 0.00922055\n",
      "Epoch 2 | Step 1556500 | Avg Loss: 0.0154 | Grad Norm: 0.00917978\n",
      "Epoch 2 | Step 1556600 | Avg Loss: 0.0152 | Grad Norm: 0.00962907\n",
      "Epoch 2 | Step 1556700 | Avg Loss: 0.0153 | Grad Norm: 0.00906401\n",
      "Epoch 2 | Step 1556800 | Avg Loss: 0.0154 | Grad Norm: 0.00880001\n",
      "Epoch 2 | Step 1556900 | Avg Loss: 0.0156 | Grad Norm: 0.00999576\n",
      "Epoch 2 | Step 1557000 | Avg Loss: 0.0156 | Grad Norm: 0.00896960\n",
      "Epoch 2 | Step 1557100 | Avg Loss: 0.0153 | Grad Norm: 0.00999441\n",
      "Epoch 2 | Step 1557200 | Avg Loss: 0.0154 | Grad Norm: 0.00855096\n",
      "Epoch 2 | Step 1557300 | Avg Loss: 0.0154 | Grad Norm: 0.00930066\n",
      "Epoch 2 | Step 1557400 | Avg Loss: 0.0152 | Grad Norm: 0.00960456\n",
      "Epoch 2 | Step 1557500 | Avg Loss: 0.0148 | Grad Norm: 0.00790740\n",
      "Epoch 2 | Step 1557600 | Avg Loss: 0.0150 | Grad Norm: 0.00903471\n",
      "Epoch 2 | Step 1557700 | Avg Loss: 0.0155 | Grad Norm: 0.00782588\n",
      "Epoch 2 | Step 1557800 | Avg Loss: 0.0156 | Grad Norm: 0.00783638\n",
      "Epoch 2 | Step 1557900 | Avg Loss: 0.0151 | Grad Norm: 0.00841155\n",
      "Epoch 2 | Step 1558000 | Avg Loss: 0.0152 | Grad Norm: 0.00927764\n",
      "Epoch 2 | Step 1558100 | Avg Loss: 0.0153 | Grad Norm: 0.00795293\n",
      "Epoch 2 | Step 1558200 | Avg Loss: 0.0151 | Grad Norm: 0.00841162\n",
      "Epoch 2 | Step 1558300 | Avg Loss: 0.0152 | Grad Norm: 0.00842739\n",
      "Epoch 2 | Step 1558400 | Avg Loss: 0.0150 | Grad Norm: 0.00905306\n",
      "Epoch 2 | Step 1558500 | Avg Loss: 0.0153 | Grad Norm: 0.00890584\n",
      "Epoch 2 | Step 1558600 | Avg Loss: 0.0155 | Grad Norm: 0.00921905\n",
      "Epoch 2 | Step 1558700 | Avg Loss: 0.0151 | Grad Norm: 0.00858613\n",
      "Epoch 2 | Step 1558800 | Avg Loss: 0.0153 | Grad Norm: 0.00891153\n",
      "Epoch 2 | Step 1558900 | Avg Loss: 0.0151 | Grad Norm: 0.00867875\n",
      "Epoch 2 | Step 1559000 | Avg Loss: 0.0150 | Grad Norm: 0.00759432\n",
      "Epoch 2 | Step 1559100 | Avg Loss: 0.0148 | Grad Norm: 0.01154288\n",
      "Epoch 2 | Step 1559200 | Avg Loss: 0.0147 | Grad Norm: 0.00823969\n",
      "Epoch 2 | Step 1559300 | Avg Loss: 0.0146 | Grad Norm: 0.00832880\n",
      "Epoch 2 | Step 1559400 | Avg Loss: 0.0144 | Grad Norm: 0.00850148\n",
      "Epoch 2 | Step 1559500 | Avg Loss: 0.0145 | Grad Norm: 0.00911088\n",
      "Epoch 2 | Step 1559600 | Avg Loss: 0.0146 | Grad Norm: 0.00829890\n",
      "Epoch 2 | Step 1559700 | Avg Loss: 0.0147 | Grad Norm: 0.00757477\n",
      "Epoch 2 | Step 1559800 | Avg Loss: 0.0148 | Grad Norm: 0.00766939\n",
      "Epoch 2 | Step 1559900 | Avg Loss: 0.0146 | Grad Norm: 0.00822008\n",
      "Epoch 2 | Step 1560000 | Avg Loss: 0.0145 | Grad Norm: 0.00844611\n",
      "Epoch 2 | Step 1560100 | Avg Loss: 0.0147 | Grad Norm: 0.01134725\n",
      "Epoch 2 | Step 1560200 | Avg Loss: 0.0146 | Grad Norm: 0.01018679\n",
      "Epoch 2 | Step 1560300 | Avg Loss: 0.0149 | Grad Norm: 0.00890809\n",
      "Epoch 2 | Step 1560400 | Avg Loss: 0.0150 | Grad Norm: 0.00930454\n",
      "Epoch 2 | Step 1560500 | Avg Loss: 0.0150 | Grad Norm: 0.00820263\n",
      "Epoch 2 | Step 1560600 | Avg Loss: 0.0148 | Grad Norm: 0.00945416\n",
      "Epoch 2 | Step 1560700 | Avg Loss: 0.0152 | Grad Norm: 0.00683250\n",
      "Epoch 2 | Step 1560800 | Avg Loss: 0.0151 | Grad Norm: 0.00807516\n",
      "Epoch 2 | Step 1560900 | Avg Loss: 0.0151 | Grad Norm: 0.00717357\n",
      "Epoch 2 | Step 1561000 | Avg Loss: 0.0150 | Grad Norm: 0.00750811\n",
      "Epoch 2 | Step 1561100 | Avg Loss: 0.0149 | Grad Norm: 0.00746278\n",
      "Epoch 2 | Step 1561200 | Avg Loss: 0.0149 | Grad Norm: 0.00865777\n",
      "Epoch 2 | Step 1561300 | Avg Loss: 0.0151 | Grad Norm: 0.00973159\n",
      "Epoch 2 | Step 1561400 | Avg Loss: 0.0146 | Grad Norm: 0.00655485\n",
      "Epoch 2 | Step 1561500 | Avg Loss: 0.0147 | Grad Norm: 0.00750819\n",
      "Epoch 2 | Step 1561600 | Avg Loss: 0.0146 | Grad Norm: 0.00914495\n",
      "Epoch 2 | Step 1561700 | Avg Loss: 0.0147 | Grad Norm: 0.00859148\n",
      "Epoch 2 | Step 1561800 | Avg Loss: 0.0150 | Grad Norm: 0.01036093\n",
      "Epoch 2 | Step 1561900 | Avg Loss: 0.0148 | Grad Norm: 0.00867053\n",
      "Epoch 2 | Step 1562000 | Avg Loss: 0.0141 | Grad Norm: 0.00823945\n",
      "Epoch 2 | Step 1562100 | Avg Loss: 0.0144 | Grad Norm: 0.00888789\n",
      "Epoch 2 | Step 1562200 | Avg Loss: 0.0144 | Grad Norm: 0.00688923\n",
      "Epoch 2 | Step 1562300 | Avg Loss: 0.0139 | Grad Norm: 0.00725896\n",
      "Epoch 2 | Step 1562400 | Avg Loss: 0.0134 | Grad Norm: 0.01040798\n",
      "Epoch 2 | Step 1562500 | Avg Loss: 0.0129 | Grad Norm: 0.00720358\n",
      "Epoch 2, Loss: 0.0126\n",
      "Epoch 3 | Step 1562600 | Avg Loss: 0.0167 | Grad Norm: 0.00899184\n",
      "Epoch 3 | Step 1562700 | Avg Loss: 0.0154 | Grad Norm: 0.00765470\n",
      "Epoch 3 | Step 1562800 | Avg Loss: 0.0150 | Grad Norm: 0.00799654\n",
      "Epoch 3 | Step 1562900 | Avg Loss: 0.0147 | Grad Norm: 0.00752705\n",
      "Epoch 3 | Step 1563000 | Avg Loss: 0.0150 | Grad Norm: 0.00897508\n",
      "Epoch 3 | Step 1563100 | Avg Loss: 0.0153 | Grad Norm: 0.01015632\n",
      "Epoch 3 | Step 1563200 | Avg Loss: 0.0157 | Grad Norm: 0.00925598\n",
      "Epoch 3 | Step 1563300 | Avg Loss: 0.0158 | Grad Norm: 0.00755182\n",
      "Epoch 3 | Step 1563400 | Avg Loss: 0.0160 | Grad Norm: 0.01111675\n",
      "Epoch 3 | Step 1563500 | Avg Loss: 0.0157 | Grad Norm: 0.00880689\n",
      "Epoch 3 | Step 1563600 | Avg Loss: 0.0154 | Grad Norm: 0.00961482\n",
      "Epoch 3 | Step 1563700 | Avg Loss: 0.0149 | Grad Norm: 0.00848996\n",
      "Epoch 3 | Step 1563800 | Avg Loss: 0.0153 | Grad Norm: 0.00857410\n",
      "Epoch 3 | Step 1563900 | Avg Loss: 0.0149 | Grad Norm: 0.01117079\n",
      "Epoch 3 | Step 1564000 | Avg Loss: 0.0146 | Grad Norm: 0.00847316\n",
      "Epoch 3 | Step 1564100 | Avg Loss: 0.0146 | Grad Norm: 0.00818796\n",
      "Epoch 3 | Step 1564200 | Avg Loss: 0.0145 | Grad Norm: 0.00859658\n",
      "Epoch 3 | Step 1564300 | Avg Loss: 0.0147 | Grad Norm: 0.00763145\n",
      "Epoch 3 | Step 1564400 | Avg Loss: 0.0149 | Grad Norm: 0.00857727\n",
      "Epoch 3 | Step 1564500 | Avg Loss: 0.0150 | Grad Norm: 0.00948227\n",
      "Epoch 3 | Step 1564600 | Avg Loss: 0.0150 | Grad Norm: 0.00862355\n",
      "Epoch 3 | Step 1564700 | Avg Loss: 0.0148 | Grad Norm: 0.00825804\n",
      "Epoch 3 | Step 1564800 | Avg Loss: 0.0151 | Grad Norm: 0.00794812\n",
      "Epoch 3 | Step 1564900 | Avg Loss: 0.0149 | Grad Norm: 0.00911178\n",
      "Epoch 3 | Step 1565000 | Avg Loss: 0.0152 | Grad Norm: 0.00937968\n",
      "Epoch 3 | Step 1565100 | Avg Loss: 0.0153 | Grad Norm: 0.00830128\n",
      "Epoch 3 | Step 1565200 | Avg Loss: 0.0155 | Grad Norm: 0.00964362\n",
      "Epoch 3 | Step 1565300 | Avg Loss: 0.0153 | Grad Norm: 0.00968834\n",
      "Epoch 3 | Step 1565400 | Avg Loss: 0.0151 | Grad Norm: 0.00951187\n",
      "Epoch 3 | Step 1565500 | Avg Loss: 0.0153 | Grad Norm: 0.00842278\n",
      "Epoch 3 | Step 1565600 | Avg Loss: 0.0150 | Grad Norm: 0.01210500\n",
      "Epoch 3 | Step 1565700 | Avg Loss: 0.0148 | Grad Norm: 0.00940718\n",
      "Epoch 3 | Step 1565800 | Avg Loss: 0.0148 | Grad Norm: 0.00762090\n",
      "Epoch 3 | Step 1565900 | Avg Loss: 0.0148 | Grad Norm: 0.00760143\n",
      "Epoch 3 | Step 1566000 | Avg Loss: 0.0145 | Grad Norm: 0.00844872\n",
      "Epoch 3 | Step 1566100 | Avg Loss: 0.0147 | Grad Norm: 0.00826688\n",
      "Epoch 3 | Step 1566200 | Avg Loss: 0.0149 | Grad Norm: 0.01075551\n",
      "Epoch 3 | Step 1566300 | Avg Loss: 0.0153 | Grad Norm: 0.00830251\n",
      "Epoch 3 | Step 1566400 | Avg Loss: 0.0152 | Grad Norm: 0.00962130\n",
      "Epoch 3 | Step 1566500 | Avg Loss: 0.0151 | Grad Norm: 0.00814990\n",
      "Epoch 3 | Step 1566600 | Avg Loss: 0.0147 | Grad Norm: 0.00845844\n",
      "Epoch 3 | Step 1566700 | Avg Loss: 0.0145 | Grad Norm: 0.00922826\n",
      "Epoch 3 | Step 1566800 | Avg Loss: 0.0147 | Grad Norm: 0.00843131\n",
      "Epoch 3 | Step 1566900 | Avg Loss: 0.0151 | Grad Norm: 0.00722332\n",
      "Epoch 3 | Step 1567000 | Avg Loss: 0.0147 | Grad Norm: 0.00817247\n",
      "Epoch 3 | Step 1567100 | Avg Loss: 0.0143 | Grad Norm: 0.00954182\n",
      "Epoch 3 | Step 1567200 | Avg Loss: 0.0144 | Grad Norm: 0.00846079\n",
      "Epoch 3 | Step 1567300 | Avg Loss: 0.0147 | Grad Norm: 0.00840585\n",
      "Epoch 3 | Step 1567400 | Avg Loss: 0.0147 | Grad Norm: 0.00783503\n",
      "Epoch 3 | Step 1567500 | Avg Loss: 0.0148 | Grad Norm: 0.00731164\n",
      "Epoch 3 | Step 1567600 | Avg Loss: 0.0147 | Grad Norm: 0.00769189\n",
      "Epoch 3 | Step 1567700 | Avg Loss: 0.0148 | Grad Norm: 0.00967469\n",
      "Epoch 3 | Step 1567800 | Avg Loss: 0.0149 | Grad Norm: 0.00825087\n",
      "Epoch 3 | Step 1567900 | Avg Loss: 0.0152 | Grad Norm: 0.00820884\n",
      "Epoch 3 | Step 1568000 | Avg Loss: 0.0153 | Grad Norm: 0.00905716\n",
      "Epoch 3 | Step 1568100 | Avg Loss: 0.0150 | Grad Norm: 0.00892520\n",
      "Epoch 3 | Step 1568200 | Avg Loss: 0.0150 | Grad Norm: 0.00837493\n",
      "Epoch 3 | Step 1568300 | Avg Loss: 0.0152 | Grad Norm: 0.00674234\n",
      "Epoch 3 | Step 1568400 | Avg Loss: 0.0149 | Grad Norm: 0.00766917\n",
      "Epoch 3 | Step 1568500 | Avg Loss: 0.0151 | Grad Norm: 0.00803327\n",
      "Epoch 3 | Step 1568600 | Avg Loss: 0.0146 | Grad Norm: 0.00737525\n",
      "Epoch 3 | Step 1568700 | Avg Loss: 0.0149 | Grad Norm: 0.00841825\n",
      "Epoch 3 | Step 1568800 | Avg Loss: 0.0152 | Grad Norm: 0.00845151\n",
      "Epoch 3 | Step 1568900 | Avg Loss: 0.0153 | Grad Norm: 0.00814320\n",
      "Epoch 3 | Step 1569000 | Avg Loss: 0.0153 | Grad Norm: 0.00890376\n",
      "Epoch 3 | Step 1569100 | Avg Loss: 0.0152 | Grad Norm: 0.00804579\n",
      "Epoch 3 | Step 1569200 | Avg Loss: 0.0151 | Grad Norm: 0.00831614\n",
      "Epoch 3 | Step 1569300 | Avg Loss: 0.0149 | Grad Norm: 0.00773368\n",
      "Epoch 3 | Step 1569400 | Avg Loss: 0.0154 | Grad Norm: 0.00717672\n",
      "Epoch 3 | Step 1569500 | Avg Loss: 0.0151 | Grad Norm: 0.00872531\n",
      "Epoch 3 | Step 1569600 | Avg Loss: 0.0145 | Grad Norm: 0.00844751\n",
      "Epoch 3 | Step 1569700 | Avg Loss: 0.0143 | Grad Norm: 0.00731442\n",
      "Epoch 3 | Step 1569800 | Avg Loss: 0.0147 | Grad Norm: 0.00821803\n",
      "Epoch 3 | Step 1569900 | Avg Loss: 0.0147 | Grad Norm: 0.00862854\n",
      "Epoch 3 | Step 1570000 | Avg Loss: 0.0147 | Grad Norm: 0.00800073\n",
      "Epoch 3 | Step 1570100 | Avg Loss: 0.0150 | Grad Norm: 0.00826689\n",
      "Epoch 3 | Step 1570200 | Avg Loss: 0.0148 | Grad Norm: 0.00816963\n",
      "Epoch 3 | Step 1570300 | Avg Loss: 0.0150 | Grad Norm: 0.00792130\n",
      "Epoch 3 | Step 1570400 | Avg Loss: 0.0148 | Grad Norm: 0.01120683\n",
      "Epoch 3 | Step 1570500 | Avg Loss: 0.0151 | Grad Norm: 0.00701047\n",
      "Epoch 3 | Step 1570600 | Avg Loss: 0.0150 | Grad Norm: 0.00798449\n",
      "Epoch 3 | Step 1570700 | Avg Loss: 0.0149 | Grad Norm: 0.01000184\n",
      "Epoch 3 | Step 1570800 | Avg Loss: 0.0151 | Grad Norm: 0.00724934\n",
      "Epoch 3 | Step 1570900 | Avg Loss: 0.0151 | Grad Norm: 0.00754563\n",
      "Epoch 3 | Step 1571000 | Avg Loss: 0.0150 | Grad Norm: 0.00927709\n",
      "Epoch 3 | Step 1571100 | Avg Loss: 0.0153 | Grad Norm: 0.00806404\n",
      "Epoch 3 | Step 1571200 | Avg Loss: 0.0154 | Grad Norm: 0.00839746\n",
      "Epoch 3 | Step 1571300 | Avg Loss: 0.0154 | Grad Norm: 0.00951430\n",
      "Epoch 3 | Step 1571400 | Avg Loss: 0.0152 | Grad Norm: 0.00766333\n",
      "Epoch 3 | Step 1571500 | Avg Loss: 0.0153 | Grad Norm: 0.00988686\n",
      "Epoch 3 | Step 1571600 | Avg Loss: 0.0150 | Grad Norm: 0.00803345\n",
      "Epoch 3 | Step 1571700 | Avg Loss: 0.0152 | Grad Norm: 0.00851316\n",
      "Epoch 3 | Step 1571800 | Avg Loss: 0.0149 | Grad Norm: 0.00844072\n",
      "Epoch 3 | Step 1571900 | Avg Loss: 0.0146 | Grad Norm: 0.00810254\n",
      "Epoch 3 | Step 1572000 | Avg Loss: 0.0152 | Grad Norm: 0.00999768\n",
      "Epoch 3 | Step 1572100 | Avg Loss: 0.0151 | Grad Norm: 0.00732132\n",
      "Epoch 3 | Step 1572200 | Avg Loss: 0.0150 | Grad Norm: 0.00753653\n",
      "Epoch 3 | Step 1572300 | Avg Loss: 0.0149 | Grad Norm: 0.00771991\n",
      "Epoch 3 | Step 1572400 | Avg Loss: 0.0153 | Grad Norm: 0.00908246\n",
      "Epoch 3 | Step 1572500 | Avg Loss: 0.0151 | Grad Norm: 0.00811283\n",
      "Epoch 3 | Step 1572600 | Avg Loss: 0.0154 | Grad Norm: 0.00949633\n",
      "Epoch 3 | Step 1572700 | Avg Loss: 0.0153 | Grad Norm: 0.00851479\n",
      "Epoch 3 | Step 1572800 | Avg Loss: 0.0148 | Grad Norm: 0.00744859\n",
      "Epoch 3 | Step 1572900 | Avg Loss: 0.0151 | Grad Norm: 0.01013721\n",
      "Epoch 3 | Step 1573000 | Avg Loss: 0.0156 | Grad Norm: 0.00755979\n",
      "Epoch 3 | Step 1573100 | Avg Loss: 0.0159 | Grad Norm: 0.00979960\n",
      "Epoch 3 | Step 1573200 | Avg Loss: 0.0156 | Grad Norm: 0.00966920\n",
      "Epoch 3 | Step 1573300 | Avg Loss: 0.0156 | Grad Norm: 0.00870898\n",
      "Epoch 3 | Step 1573400 | Avg Loss: 0.0152 | Grad Norm: 0.00894357\n",
      "Epoch 3 | Step 1573500 | Avg Loss: 0.0151 | Grad Norm: 0.00927393\n",
      "Epoch 3 | Step 1573600 | Avg Loss: 0.0155 | Grad Norm: 0.00855709\n",
      "Epoch 3 | Step 1573700 | Avg Loss: 0.0154 | Grad Norm: 0.00731960\n",
      "Epoch 3 | Step 1573800 | Avg Loss: 0.0154 | Grad Norm: 0.00923665\n",
      "Epoch 3 | Step 1573900 | Avg Loss: 0.0151 | Grad Norm: 0.00763020\n",
      "Epoch 3 | Step 1574000 | Avg Loss: 0.0153 | Grad Norm: 0.00790145\n",
      "Epoch 3 | Step 1574100 | Avg Loss: 0.0153 | Grad Norm: 0.00764786\n",
      "Epoch 3 | Step 1574200 | Avg Loss: 0.0150 | Grad Norm: 0.00798908\n",
      "Epoch 3 | Step 1574300 | Avg Loss: 0.0148 | Grad Norm: 0.00885529\n",
      "Epoch 3 | Step 1574400 | Avg Loss: 0.0147 | Grad Norm: 0.00736553\n",
      "Epoch 3 | Step 1574500 | Avg Loss: 0.0146 | Grad Norm: 0.01152574\n",
      "Epoch 3 | Step 1574600 | Avg Loss: 0.0149 | Grad Norm: 0.01089200\n",
      "Epoch 3 | Step 1574700 | Avg Loss: 0.0146 | Grad Norm: 0.00748722\n",
      "Epoch 3 | Step 1574800 | Avg Loss: 0.0148 | Grad Norm: 0.00954525\n",
      "Epoch 3 | Step 1574900 | Avg Loss: 0.0154 | Grad Norm: 0.00866458\n",
      "Epoch 3 | Step 1575000 | Avg Loss: 0.0156 | Grad Norm: 0.00803575\n",
      "Epoch 3 | Step 1575100 | Avg Loss: 0.0153 | Grad Norm: 0.00883314\n",
      "Epoch 3 | Step 1575200 | Avg Loss: 0.0151 | Grad Norm: 0.00814219\n",
      "Epoch 3 | Step 1575300 | Avg Loss: 0.0149 | Grad Norm: 0.00736406\n",
      "Epoch 3 | Step 1575400 | Avg Loss: 0.0153 | Grad Norm: 0.00914083\n",
      "Epoch 3 | Step 1575500 | Avg Loss: 0.0150 | Grad Norm: 0.00977406\n",
      "Epoch 3 | Step 1575600 | Avg Loss: 0.0152 | Grad Norm: 0.00835577\n",
      "Epoch 3 | Step 1575700 | Avg Loss: 0.0148 | Grad Norm: 0.00898553\n",
      "Epoch 3 | Step 1575800 | Avg Loss: 0.0148 | Grad Norm: 0.00763475\n",
      "Epoch 3 | Step 1575900 | Avg Loss: 0.0149 | Grad Norm: 0.00769379\n",
      "Epoch 3 | Step 1576000 | Avg Loss: 0.0150 | Grad Norm: 0.01026883\n",
      "Epoch 3 | Step 1576100 | Avg Loss: 0.0149 | Grad Norm: 0.00880495\n",
      "Epoch 3 | Step 1576200 | Avg Loss: 0.0146 | Grad Norm: 0.00731410\n",
      "Epoch 3 | Step 1576300 | Avg Loss: 0.0148 | Grad Norm: 0.00771817\n",
      "Epoch 3 | Step 1576400 | Avg Loss: 0.0154 | Grad Norm: 0.00904750\n",
      "Epoch 3 | Step 1576500 | Avg Loss: 0.0152 | Grad Norm: 0.00905572\n",
      "Epoch 3 | Step 1576600 | Avg Loss: 0.0147 | Grad Norm: 0.00909090\n",
      "Epoch 3 | Step 1576700 | Avg Loss: 0.0150 | Grad Norm: 0.00932551\n",
      "Epoch 3 | Step 1576800 | Avg Loss: 0.0150 | Grad Norm: 0.00693033\n",
      "Epoch 3 | Step 1576900 | Avg Loss: 0.0148 | Grad Norm: 0.00935455\n",
      "Epoch 3 | Step 1577000 | Avg Loss: 0.0152 | Grad Norm: 0.00856159\n",
      "Epoch 3 | Step 1577100 | Avg Loss: 0.0152 | Grad Norm: 0.00911056\n",
      "Epoch 3 | Step 1577200 | Avg Loss: 0.0152 | Grad Norm: 0.00799265\n",
      "Epoch 3 | Step 1577300 | Avg Loss: 0.0147 | Grad Norm: 0.00907374\n",
      "Epoch 3 | Step 1577400 | Avg Loss: 0.0151 | Grad Norm: 0.00752888\n",
      "Epoch 3 | Step 1577500 | Avg Loss: 0.0153 | Grad Norm: 0.00923713\n",
      "Epoch 3 | Step 1577600 | Avg Loss: 0.0150 | Grad Norm: 0.00827003\n",
      "Epoch 3 | Step 1577700 | Avg Loss: 0.0151 | Grad Norm: 0.00760194\n",
      "Epoch 3 | Step 1577800 | Avg Loss: 0.0151 | Grad Norm: 0.01043521\n",
      "Epoch 3 | Step 1577900 | Avg Loss: 0.0150 | Grad Norm: 0.00823542\n",
      "Epoch 3 | Step 1578000 | Avg Loss: 0.0153 | Grad Norm: 0.00884355\n",
      "Epoch 3 | Step 1578100 | Avg Loss: 0.0154 | Grad Norm: 0.01047905\n",
      "Epoch 3 | Step 1578200 | Avg Loss: 0.0155 | Grad Norm: 0.00939719\n",
      "Epoch 3 | Step 1578300 | Avg Loss: 0.0161 | Grad Norm: 0.00841929\n",
      "Epoch 3 | Step 1578400 | Avg Loss: 0.0161 | Grad Norm: 0.00840429\n",
      "Epoch 3 | Step 1578500 | Avg Loss: 0.0157 | Grad Norm: 0.00729919\n",
      "Epoch 3 | Step 1578600 | Avg Loss: 0.0159 | Grad Norm: 0.00776821\n",
      "Epoch 3 | Step 1578700 | Avg Loss: 0.0158 | Grad Norm: 0.00898754\n",
      "Epoch 3 | Step 1578800 | Avg Loss: 0.0156 | Grad Norm: 0.00957229\n",
      "Epoch 3 | Step 1578900 | Avg Loss: 0.0154 | Grad Norm: 0.00776754\n",
      "Epoch 3 | Step 1579000 | Avg Loss: 0.0157 | Grad Norm: 0.00706204\n",
      "Epoch 3 | Step 1579100 | Avg Loss: 0.0151 | Grad Norm: 0.00881153\n",
      "Epoch 3 | Step 1579200 | Avg Loss: 0.0146 | Grad Norm: 0.01110053\n",
      "Epoch 3 | Step 1579300 | Avg Loss: 0.0150 | Grad Norm: 0.00782619\n",
      "Epoch 3 | Step 1579400 | Avg Loss: 0.0155 | Grad Norm: 0.00868903\n",
      "Epoch 3 | Step 1579500 | Avg Loss: 0.0153 | Grad Norm: 0.00913953\n",
      "Epoch 3 | Step 1579600 | Avg Loss: 0.0152 | Grad Norm: 0.00784625\n",
      "Epoch 3 | Step 1579700 | Avg Loss: 0.0150 | Grad Norm: 0.01073658\n",
      "Epoch 3 | Step 1579800 | Avg Loss: 0.0150 | Grad Norm: 0.00870842\n",
      "Epoch 3 | Step 1579900 | Avg Loss: 0.0152 | Grad Norm: 0.00857326\n",
      "Epoch 3 | Step 1580000 | Avg Loss: 0.0152 | Grad Norm: 0.00876199\n",
      "Epoch 3 | Step 1580100 | Avg Loss: 0.0149 | Grad Norm: 0.00884405\n",
      "Epoch 3 | Step 1580200 | Avg Loss: 0.0147 | Grad Norm: 0.00849645\n",
      "Epoch 3 | Step 1580300 | Avg Loss: 0.0150 | Grad Norm: 0.01423245\n",
      "Epoch 3 | Step 1580400 | Avg Loss: 0.0148 | Grad Norm: 0.00861389\n",
      "Epoch 3 | Step 1580500 | Avg Loss: 0.0149 | Grad Norm: 0.00995353\n",
      "Epoch 3 | Step 1580600 | Avg Loss: 0.0146 | Grad Norm: 0.00938124\n",
      "Epoch 3 | Step 1580700 | Avg Loss: 0.0147 | Grad Norm: 0.00841492\n",
      "Epoch 3 | Step 1580800 | Avg Loss: 0.0149 | Grad Norm: 0.00768829\n",
      "Epoch 3 | Step 1580900 | Avg Loss: 0.0147 | Grad Norm: 0.00765562\n",
      "Epoch 3 | Step 1581000 | Avg Loss: 0.0152 | Grad Norm: 0.00917310\n",
      "Epoch 3 | Step 1581100 | Avg Loss: 0.0152 | Grad Norm: 0.00801277\n",
      "Epoch 3 | Step 1581200 | Avg Loss: 0.0153 | Grad Norm: 0.01080874\n",
      "Epoch 3 | Step 1581300 | Avg Loss: 0.0155 | Grad Norm: 0.00890713\n",
      "Epoch 3 | Step 1581400 | Avg Loss: 0.0156 | Grad Norm: 0.00955961\n",
      "Epoch 3 | Step 1581500 | Avg Loss: 0.0156 | Grad Norm: 0.00761998\n",
      "Epoch 3 | Step 1581600 | Avg Loss: 0.0156 | Grad Norm: 0.00831033\n",
      "Epoch 3 | Step 1581700 | Avg Loss: 0.0156 | Grad Norm: 0.00763484\n",
      "Epoch 3 | Step 1581800 | Avg Loss: 0.0154 | Grad Norm: 0.00847570\n",
      "Epoch 3 | Step 1581900 | Avg Loss: 0.0151 | Grad Norm: 0.00856784\n",
      "Epoch 3 | Step 1582000 | Avg Loss: 0.0147 | Grad Norm: 0.00633581\n",
      "Epoch 3 | Step 1582100 | Avg Loss: 0.0145 | Grad Norm: 0.00850331\n",
      "Epoch 3 | Step 1582200 | Avg Loss: 0.0148 | Grad Norm: 0.00763098\n",
      "Epoch 3 | Step 1582300 | Avg Loss: 0.0147 | Grad Norm: 0.00959242\n",
      "Epoch 3 | Step 1582400 | Avg Loss: 0.0148 | Grad Norm: 0.00752349\n",
      "Epoch 3 | Step 1582500 | Avg Loss: 0.0148 | Grad Norm: 0.00917699\n",
      "Epoch 3 | Step 1582600 | Avg Loss: 0.0146 | Grad Norm: 0.01035761\n",
      "Epoch 3 | Step 1582700 | Avg Loss: 0.0145 | Grad Norm: 0.00921815\n",
      "Epoch 3 | Step 1582800 | Avg Loss: 0.0150 | Grad Norm: 0.00769755\n",
      "Epoch 3 | Step 1582900 | Avg Loss: 0.0150 | Grad Norm: 0.00826783\n",
      "Epoch 3 | Step 1583000 | Avg Loss: 0.0153 | Grad Norm: 0.00968081\n",
      "Epoch 3 | Step 1583100 | Avg Loss: 0.0153 | Grad Norm: 0.00918422\n",
      "Epoch 3 | Step 1583200 | Avg Loss: 0.0151 | Grad Norm: 0.00826694\n",
      "Epoch 3 | Step 1583300 | Avg Loss: 0.0150 | Grad Norm: 0.00709954\n",
      "Epoch 3 | Step 1583400 | Avg Loss: 0.0148 | Grad Norm: 0.00750933\n",
      "Epoch 3 | Step 1583500 | Avg Loss: 0.0149 | Grad Norm: 0.00751944\n",
      "Epoch 3 | Step 1583600 | Avg Loss: 0.0149 | Grad Norm: 0.00961594\n",
      "Epoch 3 | Step 1583700 | Avg Loss: 0.0151 | Grad Norm: 0.00686953\n",
      "Epoch 3 | Step 1583800 | Avg Loss: 0.0149 | Grad Norm: 0.01041501\n",
      "Epoch 3 | Step 1583900 | Avg Loss: 0.0149 | Grad Norm: 0.00776911\n",
      "Epoch 3 | Step 1584000 | Avg Loss: 0.0153 | Grad Norm: 0.00774936\n",
      "Epoch 3 | Step 1584100 | Avg Loss: 0.0154 | Grad Norm: 0.00869814\n",
      "Epoch 3 | Step 1584200 | Avg Loss: 0.0153 | Grad Norm: 0.00922160\n",
      "Epoch 3 | Step 1584300 | Avg Loss: 0.0157 | Grad Norm: 0.00702030\n",
      "Epoch 3 | Step 1584400 | Avg Loss: 0.0153 | Grad Norm: 0.01081190\n",
      "Epoch 3 | Step 1584500 | Avg Loss: 0.0152 | Grad Norm: 0.00812380\n",
      "Epoch 3 | Step 1584600 | Avg Loss: 0.0149 | Grad Norm: 0.00927860\n",
      "Epoch 3 | Step 1584700 | Avg Loss: 0.0150 | Grad Norm: 0.00792499\n",
      "Epoch 3 | Step 1584800 | Avg Loss: 0.0151 | Grad Norm: 0.00961564\n",
      "Epoch 3 | Step 1584900 | Avg Loss: 0.0151 | Grad Norm: 0.00849056\n",
      "Epoch 3 | Step 1585000 | Avg Loss: 0.0151 | Grad Norm: 0.00718073\n",
      "Epoch 3 | Step 1585100 | Avg Loss: 0.0148 | Grad Norm: 0.00888575\n",
      "Epoch 3 | Step 1585200 | Avg Loss: 0.0151 | Grad Norm: 0.01086341\n",
      "Epoch 3 | Step 1585300 | Avg Loss: 0.0154 | Grad Norm: 0.00913769\n",
      "Epoch 3 | Step 1585400 | Avg Loss: 0.0155 | Grad Norm: 0.00880687\n",
      "Epoch 3 | Step 1585500 | Avg Loss: 0.0150 | Grad Norm: 0.00889943\n",
      "Epoch 3 | Step 1585600 | Avg Loss: 0.0148 | Grad Norm: 0.00927436\n",
      "Epoch 3 | Step 1585700 | Avg Loss: 0.0147 | Grad Norm: 0.00851534\n",
      "Epoch 3 | Step 1585800 | Avg Loss: 0.0148 | Grad Norm: 0.00787034\n",
      "Epoch 3 | Step 1585900 | Avg Loss: 0.0149 | Grad Norm: 0.00884136\n",
      "Epoch 3 | Step 1586000 | Avg Loss: 0.0150 | Grad Norm: 0.00807125\n",
      "Epoch 3 | Step 1586100 | Avg Loss: 0.0159 | Grad Norm: 0.00918000\n",
      "Epoch 3 | Step 1586200 | Avg Loss: 0.0157 | Grad Norm: 0.00907319\n",
      "Epoch 3 | Step 1586300 | Avg Loss: 0.0153 | Grad Norm: 0.00803206\n",
      "Epoch 3 | Step 1586400 | Avg Loss: 0.0152 | Grad Norm: 0.00831584\n",
      "Epoch 3 | Step 1586500 | Avg Loss: 0.0152 | Grad Norm: 0.00960306\n",
      "Epoch 3 | Step 1586600 | Avg Loss: 0.0153 | Grad Norm: 0.00865882\n",
      "Epoch 3 | Step 1586700 | Avg Loss: 0.0155 | Grad Norm: 0.00898701\n",
      "Epoch 3 | Step 1586800 | Avg Loss: 0.0154 | Grad Norm: 0.00875799\n",
      "Epoch 3 | Step 1586900 | Avg Loss: 0.0152 | Grad Norm: 0.00771229\n",
      "Epoch 3 | Step 1587000 | Avg Loss: 0.0150 | Grad Norm: 0.00915560\n",
      "Epoch 3 | Step 1587100 | Avg Loss: 0.0153 | Grad Norm: 0.00899578\n",
      "Epoch 3 | Step 1587200 | Avg Loss: 0.0156 | Grad Norm: 0.00755590\n",
      "Epoch 3 | Step 1587300 | Avg Loss: 0.0156 | Grad Norm: 0.00917636\n",
      "Epoch 3 | Step 1587400 | Avg Loss: 0.0151 | Grad Norm: 0.00841990\n",
      "Epoch 3 | Step 1587500 | Avg Loss: 0.0152 | Grad Norm: 0.00808043\n",
      "Epoch 3 | Step 1587600 | Avg Loss: 0.0155 | Grad Norm: 0.00895381\n",
      "Epoch 3 | Step 1587700 | Avg Loss: 0.0156 | Grad Norm: 0.00845364\n",
      "Epoch 3 | Step 1587800 | Avg Loss: 0.0160 | Grad Norm: 0.00784468\n",
      "Epoch 3 | Step 1587900 | Avg Loss: 0.0157 | Grad Norm: 0.00968628\n",
      "Epoch 3 | Step 1588000 | Avg Loss: 0.0159 | Grad Norm: 0.00838563\n",
      "Epoch 3 | Step 1588100 | Avg Loss: 0.0157 | Grad Norm: 0.01124909\n",
      "Epoch 3 | Step 1588200 | Avg Loss: 0.0156 | Grad Norm: 0.01051545\n",
      "Epoch 3 | Step 1588300 | Avg Loss: 0.0157 | Grad Norm: 0.01133869\n",
      "Epoch 3 | Step 1588400 | Avg Loss: 0.0157 | Grad Norm: 0.00914243\n",
      "Epoch 3 | Step 1588500 | Avg Loss: 0.0151 | Grad Norm: 0.00843515\n",
      "Epoch 3 | Step 1588600 | Avg Loss: 0.0149 | Grad Norm: 0.00857236\n",
      "Epoch 3 | Step 1588700 | Avg Loss: 0.0151 | Grad Norm: 0.00925115\n",
      "Epoch 3 | Step 1588800 | Avg Loss: 0.0156 | Grad Norm: 0.00867787\n",
      "Epoch 3 | Step 1588900 | Avg Loss: 0.0152 | Grad Norm: 0.00699922\n",
      "Epoch 3 | Step 1589000 | Avg Loss: 0.0149 | Grad Norm: 0.00825302\n",
      "Epoch 3 | Step 1589100 | Avg Loss: 0.0149 | Grad Norm: 0.00747283\n",
      "Epoch 3 | Step 1589200 | Avg Loss: 0.0152 | Grad Norm: 0.00742362\n",
      "Epoch 3 | Step 1589300 | Avg Loss: 0.0153 | Grad Norm: 0.00820810\n",
      "Epoch 3 | Step 1589400 | Avg Loss: 0.0154 | Grad Norm: 0.00884316\n",
      "Epoch 3 | Step 1589500 | Avg Loss: 0.0153 | Grad Norm: 0.00887585\n",
      "Epoch 3 | Step 1589600 | Avg Loss: 0.0154 | Grad Norm: 0.00711604\n",
      "Epoch 3 | Step 1589700 | Avg Loss: 0.0150 | Grad Norm: 0.00825368\n",
      "Epoch 3 | Step 1589800 | Avg Loss: 0.0148 | Grad Norm: 0.00777412\n",
      "Epoch 3 | Step 1589900 | Avg Loss: 0.0152 | Grad Norm: 0.00902494\n",
      "Epoch 3 | Step 1590000 | Avg Loss: 0.0156 | Grad Norm: 0.01033307\n",
      "Epoch 3 | Step 1590100 | Avg Loss: 0.0153 | Grad Norm: 0.00844832\n",
      "Epoch 3 | Step 1590200 | Avg Loss: 0.0154 | Grad Norm: 0.00784167\n",
      "Epoch 3 | Step 1590300 | Avg Loss: 0.0155 | Grad Norm: 0.00998117\n",
      "Epoch 3 | Step 1590400 | Avg Loss: 0.0154 | Grad Norm: 0.00790724\n",
      "Epoch 3 | Step 1590500 | Avg Loss: 0.0153 | Grad Norm: 0.00772134\n",
      "Epoch 3 | Step 1590600 | Avg Loss: 0.0153 | Grad Norm: 0.00922695\n",
      "Epoch 3 | Step 1590700 | Avg Loss: 0.0152 | Grad Norm: 0.00800760\n",
      "Epoch 3 | Step 1590800 | Avg Loss: 0.0148 | Grad Norm: 0.00952440\n",
      "Epoch 3 | Step 1590900 | Avg Loss: 0.0147 | Grad Norm: 0.00871586\n",
      "Epoch 3 | Step 1591000 | Avg Loss: 0.0153 | Grad Norm: 0.00777779\n",
      "Epoch 3 | Step 1591100 | Avg Loss: 0.0153 | Grad Norm: 0.00669707\n",
      "Epoch 3 | Step 1591200 | Avg Loss: 0.0150 | Grad Norm: 0.00818427\n",
      "Epoch 3 | Step 1591300 | Avg Loss: 0.0150 | Grad Norm: 0.00789726\n",
      "Epoch 3 | Step 1591400 | Avg Loss: 0.0150 | Grad Norm: 0.00756385\n",
      "Epoch 3 | Step 1591500 | Avg Loss: 0.0149 | Grad Norm: 0.00781090\n",
      "Epoch 3 | Step 1591600 | Avg Loss: 0.0149 | Grad Norm: 0.00805525\n",
      "Epoch 3 | Step 1591700 | Avg Loss: 0.0153 | Grad Norm: 0.00810445\n",
      "Epoch 3 | Step 1591800 | Avg Loss: 0.0147 | Grad Norm: 0.00743229\n",
      "Epoch 3 | Step 1591900 | Avg Loss: 0.0147 | Grad Norm: 0.00831047\n",
      "Epoch 3 | Step 1592000 | Avg Loss: 0.0153 | Grad Norm: 0.00871218\n",
      "Epoch 3 | Step 1592100 | Avg Loss: 0.0150 | Grad Norm: 0.01029953\n",
      "Epoch 3 | Step 1592200 | Avg Loss: 0.0151 | Grad Norm: 0.00780228\n",
      "Epoch 3 | Step 1592300 | Avg Loss: 0.0155 | Grad Norm: 0.00660740\n",
      "Epoch 3 | Step 1592400 | Avg Loss: 0.0154 | Grad Norm: 0.00898372\n",
      "Epoch 3 | Step 1592500 | Avg Loss: 0.0153 | Grad Norm: 0.00743294\n",
      "Epoch 3 | Step 1592600 | Avg Loss: 0.0151 | Grad Norm: 0.00901000\n",
      "Epoch 3 | Step 1592700 | Avg Loss: 0.0152 | Grad Norm: 0.00772259\n",
      "Epoch 3 | Step 1592800 | Avg Loss: 0.0153 | Grad Norm: 0.00786558\n",
      "Epoch 3 | Step 1592900 | Avg Loss: 0.0156 | Grad Norm: 0.00829766\n",
      "Epoch 3 | Step 1593000 | Avg Loss: 0.0151 | Grad Norm: 0.00736472\n",
      "Epoch 3 | Step 1593100 | Avg Loss: 0.0150 | Grad Norm: 0.00816895\n",
      "Epoch 3 | Step 1593200 | Avg Loss: 0.0151 | Grad Norm: 0.00773754\n",
      "Epoch 3 | Step 1593300 | Avg Loss: 0.0151 | Grad Norm: 0.00781346\n",
      "Epoch 3 | Step 1593400 | Avg Loss: 0.0152 | Grad Norm: 0.00947013\n",
      "Epoch 3 | Step 1593500 | Avg Loss: 0.0150 | Grad Norm: 0.01031814\n",
      "Epoch 3 | Step 1593600 | Avg Loss: 0.0152 | Grad Norm: 0.00728410\n",
      "Epoch 3 | Step 1593700 | Avg Loss: 0.0152 | Grad Norm: 0.00756723\n",
      "Epoch 3 | Step 1593800 | Avg Loss: 0.0149 | Grad Norm: 0.00779415\n",
      "Epoch 3 | Step 1593900 | Avg Loss: 0.0151 | Grad Norm: 0.00975373\n",
      "Epoch 3 | Step 1594000 | Avg Loss: 0.0152 | Grad Norm: 0.00836465\n",
      "Epoch 3 | Step 1594100 | Avg Loss: 0.0149 | Grad Norm: 0.00734964\n",
      "Epoch 3 | Step 1594200 | Avg Loss: 0.0148 | Grad Norm: 0.00846036\n",
      "Epoch 3 | Step 1594300 | Avg Loss: 0.0152 | Grad Norm: 0.00796800\n",
      "Epoch 3 | Step 1594400 | Avg Loss: 0.0153 | Grad Norm: 0.00907091\n",
      "Epoch 3 | Step 1594500 | Avg Loss: 0.0152 | Grad Norm: 0.00744737\n",
      "Epoch 3 | Step 1594600 | Avg Loss: 0.0152 | Grad Norm: 0.00812726\n",
      "Epoch 3 | Step 1594700 | Avg Loss: 0.0155 | Grad Norm: 0.00844075\n",
      "Epoch 3 | Step 1594800 | Avg Loss: 0.0155 | Grad Norm: 0.00717837\n",
      "Epoch 3 | Step 1594900 | Avg Loss: 0.0158 | Grad Norm: 0.00827800\n",
      "Epoch 3 | Step 1595000 | Avg Loss: 0.0155 | Grad Norm: 0.00866134\n",
      "Epoch 3 | Step 1595100 | Avg Loss: 0.0154 | Grad Norm: 0.00817958\n",
      "Epoch 3 | Step 1595200 | Avg Loss: 0.0152 | Grad Norm: 0.00907956\n",
      "Epoch 3 | Step 1595300 | Avg Loss: 0.0153 | Grad Norm: 0.00835931\n",
      "Epoch 3 | Step 1595400 | Avg Loss: 0.0152 | Grad Norm: 0.00948855\n",
      "Epoch 3 | Step 1595500 | Avg Loss: 0.0151 | Grad Norm: 0.00733417\n",
      "Epoch 3 | Step 1595600 | Avg Loss: 0.0150 | Grad Norm: 0.00831711\n",
      "Epoch 3 | Step 1595700 | Avg Loss: 0.0148 | Grad Norm: 0.00849806\n",
      "Epoch 3 | Step 1595800 | Avg Loss: 0.0153 | Grad Norm: 0.00955271\n",
      "Epoch 3 | Step 1595900 | Avg Loss: 0.0151 | Grad Norm: 0.01200669\n",
      "Epoch 3 | Step 1596000 | Avg Loss: 0.0149 | Grad Norm: 0.01128924\n",
      "Epoch 3 | Step 1596100 | Avg Loss: 0.0150 | Grad Norm: 0.00853202\n",
      "Epoch 3 | Step 1596200 | Avg Loss: 0.0152 | Grad Norm: 0.00791916\n",
      "Epoch 3 | Step 1596300 | Avg Loss: 0.0150 | Grad Norm: 0.00921175\n",
      "Epoch 3 | Step 1596400 | Avg Loss: 0.0150 | Grad Norm: 0.00664814\n",
      "Epoch 3 | Step 1596500 | Avg Loss: 0.0149 | Grad Norm: 0.00826405\n",
      "Epoch 3 | Step 1596600 | Avg Loss: 0.0143 | Grad Norm: 0.00883085\n",
      "Epoch 3 | Step 1596700 | Avg Loss: 0.0142 | Grad Norm: 0.00804054\n",
      "Epoch 3 | Step 1596800 | Avg Loss: 0.0142 | Grad Norm: 0.00883370\n",
      "Epoch 3 | Step 1596900 | Avg Loss: 0.0149 | Grad Norm: 0.00906859\n",
      "Epoch 3 | Step 1597000 | Avg Loss: 0.0150 | Grad Norm: 0.00870929\n",
      "Epoch 3 | Step 1597100 | Avg Loss: 0.0153 | Grad Norm: 0.00722061\n",
      "Epoch 3 | Step 1597200 | Avg Loss: 0.0147 | Grad Norm: 0.00869073\n",
      "Epoch 3 | Step 1597300 | Avg Loss: 0.0151 | Grad Norm: 0.00878016\n",
      "Epoch 3 | Step 1597400 | Avg Loss: 0.0151 | Grad Norm: 0.00772300\n",
      "Epoch 3 | Step 1597500 | Avg Loss: 0.0149 | Grad Norm: 0.00777634\n",
      "Epoch 3 | Step 1597600 | Avg Loss: 0.0147 | Grad Norm: 0.00818264\n",
      "Epoch 3 | Step 1597700 | Avg Loss: 0.0150 | Grad Norm: 0.00752424\n",
      "Epoch 3 | Step 1597800 | Avg Loss: 0.0151 | Grad Norm: 0.00948997\n",
      "Epoch 3 | Step 1597900 | Avg Loss: 0.0151 | Grad Norm: 0.00717973\n",
      "Epoch 3 | Step 1598000 | Avg Loss: 0.0156 | Grad Norm: 0.00754222\n",
      "Epoch 3 | Step 1598100 | Avg Loss: 0.0156 | Grad Norm: 0.01020985\n",
      "Epoch 3 | Step 1598200 | Avg Loss: 0.0153 | Grad Norm: 0.00804951\n",
      "Epoch 3 | Step 1598300 | Avg Loss: 0.0150 | Grad Norm: 0.00845102\n",
      "Epoch 3 | Step 1598400 | Avg Loss: 0.0150 | Grad Norm: 0.00751928\n",
      "Epoch 3 | Step 1598500 | Avg Loss: 0.0146 | Grad Norm: 0.00772822\n",
      "Epoch 3 | Step 1598600 | Avg Loss: 0.0145 | Grad Norm: 0.00856498\n",
      "Epoch 3 | Step 1598700 | Avg Loss: 0.0150 | Grad Norm: 0.00859458\n",
      "Epoch 3 | Step 1598800 | Avg Loss: 0.0154 | Grad Norm: 0.00803459\n",
      "Epoch 3 | Step 1598900 | Avg Loss: 0.0154 | Grad Norm: 0.00778167\n",
      "Epoch 3 | Step 1599000 | Avg Loss: 0.0154 | Grad Norm: 0.00850514\n",
      "Epoch 3 | Step 1599100 | Avg Loss: 0.0153 | Grad Norm: 0.00821956\n",
      "Epoch 3 | Step 1599200 | Avg Loss: 0.0152 | Grad Norm: 0.00919273\n",
      "Epoch 3 | Step 1599300 | Avg Loss: 0.0155 | Grad Norm: 0.00930252\n",
      "Epoch 3 | Step 1599400 | Avg Loss: 0.0155 | Grad Norm: 0.00873620\n",
      "Epoch 3 | Step 1599500 | Avg Loss: 0.0155 | Grad Norm: 0.00797852\n",
      "Epoch 3 | Step 1599600 | Avg Loss: 0.0155 | Grad Norm: 0.00960964\n",
      "Epoch 3 | Step 1599700 | Avg Loss: 0.0152 | Grad Norm: 0.00861731\n",
      "Epoch 3 | Step 1599800 | Avg Loss: 0.0151 | Grad Norm: 0.00898536\n",
      "Epoch 3 | Step 1599900 | Avg Loss: 0.0153 | Grad Norm: 0.00828888\n",
      "Epoch 3 | Step 1600000 | Avg Loss: 0.0152 | Grad Norm: 0.00826580\n",
      "Saving model at step1600000\n",
      "Epoch 3 | Step 1600100 | Avg Loss: 0.0152 | Grad Norm: 0.00780183\n",
      "Epoch 3 | Step 1600200 | Avg Loss: 0.0152 | Grad Norm: 0.00788560\n",
      "Epoch 3 | Step 1600300 | Avg Loss: 0.0154 | Grad Norm: 0.00875934\n",
      "Epoch 3 | Step 1600400 | Avg Loss: 0.0153 | Grad Norm: 0.00883926\n",
      "Epoch 3 | Step 1600500 | Avg Loss: 0.0157 | Grad Norm: 0.00740769\n",
      "Epoch 3 | Step 1600600 | Avg Loss: 0.0156 | Grad Norm: 0.00796189\n",
      "Epoch 3 | Step 1600700 | Avg Loss: 0.0156 | Grad Norm: 0.00935375\n",
      "Epoch 3 | Step 1600800 | Avg Loss: 0.0156 | Grad Norm: 0.00879608\n",
      "Epoch 3 | Step 1600900 | Avg Loss: 0.0157 | Grad Norm: 0.00696434\n",
      "Epoch 3 | Step 1601000 | Avg Loss: 0.0154 | Grad Norm: 0.00764990\n",
      "Epoch 3 | Step 1601100 | Avg Loss: 0.0154 | Grad Norm: 0.00808504\n",
      "Epoch 3 | Step 1601200 | Avg Loss: 0.0157 | Grad Norm: 0.01040886\n",
      "Epoch 3 | Step 1601300 | Avg Loss: 0.0155 | Grad Norm: 0.01097158\n",
      "Epoch 3 | Step 1601400 | Avg Loss: 0.0153 | Grad Norm: 0.00758298\n",
      "Epoch 3 | Step 1601500 | Avg Loss: 0.0150 | Grad Norm: 0.00790062\n",
      "Epoch 3 | Step 1601600 | Avg Loss: 0.0155 | Grad Norm: 0.00821889\n",
      "Epoch 3 | Step 1601700 | Avg Loss: 0.0154 | Grad Norm: 0.00925178\n",
      "Epoch 3 | Step 1601800 | Avg Loss: 0.0149 | Grad Norm: 0.00741475\n",
      "Epoch 3 | Step 1601900 | Avg Loss: 0.0154 | Grad Norm: 0.01066742\n",
      "Epoch 3 | Step 1602000 | Avg Loss: 0.0152 | Grad Norm: 0.00869078\n",
      "Epoch 3 | Step 1602100 | Avg Loss: 0.0149 | Grad Norm: 0.00856688\n",
      "Epoch 3 | Step 1602200 | Avg Loss: 0.0152 | Grad Norm: 0.00774824\n",
      "Epoch 3 | Step 1602300 | Avg Loss: 0.0153 | Grad Norm: 0.00817265\n",
      "Epoch 3 | Step 1602400 | Avg Loss: 0.0151 | Grad Norm: 0.00850948\n",
      "Epoch 3 | Step 1602500 | Avg Loss: 0.0152 | Grad Norm: 0.00781779\n",
      "Epoch 3 | Step 1602600 | Avg Loss: 0.0150 | Grad Norm: 0.00721929\n",
      "Epoch 3 | Step 1602700 | Avg Loss: 0.0149 | Grad Norm: 0.00945726\n",
      "Epoch 3 | Step 1602800 | Avg Loss: 0.0147 | Grad Norm: 0.00840939\n",
      "Epoch 3 | Step 1602900 | Avg Loss: 0.0147 | Grad Norm: 0.00682060\n",
      "Epoch 3 | Step 1603000 | Avg Loss: 0.0148 | Grad Norm: 0.00665532\n",
      "Epoch 3 | Step 1603100 | Avg Loss: 0.0148 | Grad Norm: 0.00743781\n",
      "Epoch 3 | Step 1603200 | Avg Loss: 0.0143 | Grad Norm: 0.00846728\n",
      "Epoch 3 | Step 1603300 | Avg Loss: 0.0147 | Grad Norm: 0.00852790\n",
      "Epoch 3 | Step 1603400 | Avg Loss: 0.0147 | Grad Norm: 0.00750659\n",
      "Epoch 3 | Step 1603500 | Avg Loss: 0.0148 | Grad Norm: 0.00779002\n",
      "Epoch 3 | Step 1603600 | Avg Loss: 0.0149 | Grad Norm: 0.00815057\n",
      "Epoch 3 | Step 1603700 | Avg Loss: 0.0152 | Grad Norm: 0.00760824\n",
      "Epoch 3 | Step 1603800 | Avg Loss: 0.0152 | Grad Norm: 0.00952908\n",
      "Epoch 3 | Step 1603900 | Avg Loss: 0.0150 | Grad Norm: 0.00825990\n",
      "Epoch 3 | Step 1604000 | Avg Loss: 0.0145 | Grad Norm: 0.00823148\n",
      "Epoch 3 | Step 1604100 | Avg Loss: 0.0150 | Grad Norm: 0.00967029\n",
      "Epoch 3 | Step 1604200 | Avg Loss: 0.0152 | Grad Norm: 0.00868273\n",
      "Epoch 3 | Step 1604300 | Avg Loss: 0.0152 | Grad Norm: 0.00897669\n",
      "Epoch 3 | Step 1604400 | Avg Loss: 0.0153 | Grad Norm: 0.00907367\n",
      "Epoch 3 | Step 1604500 | Avg Loss: 0.0156 | Grad Norm: 0.00922907\n",
      "Epoch 3 | Step 1604600 | Avg Loss: 0.0154 | Grad Norm: 0.00823914\n",
      "Epoch 3 | Step 1604700 | Avg Loss: 0.0156 | Grad Norm: 0.00780765\n",
      "Epoch 3 | Step 1604800 | Avg Loss: 0.0158 | Grad Norm: 0.00792259\n",
      "Epoch 3 | Step 1604900 | Avg Loss: 0.0161 | Grad Norm: 0.00908033\n",
      "Epoch 3 | Step 1605000 | Avg Loss: 0.0159 | Grad Norm: 0.00941663\n",
      "Epoch 3 | Step 1605100 | Avg Loss: 0.0156 | Grad Norm: 0.00872375\n",
      "Epoch 3 | Step 1605200 | Avg Loss: 0.0151 | Grad Norm: 0.00712120\n",
      "Epoch 3 | Step 1605300 | Avg Loss: 0.0151 | Grad Norm: 0.00929636\n",
      "Epoch 3 | Step 1605400 | Avg Loss: 0.0146 | Grad Norm: 0.00748800\n",
      "Epoch 3 | Step 1605500 | Avg Loss: 0.0147 | Grad Norm: 0.00732138\n",
      "Epoch 3 | Step 1605600 | Avg Loss: 0.0152 | Grad Norm: 0.00786431\n",
      "Epoch 3 | Step 1605700 | Avg Loss: 0.0153 | Grad Norm: 0.00980540\n",
      "Epoch 3 | Step 1605800 | Avg Loss: 0.0150 | Grad Norm: 0.01170530\n",
      "Epoch 3 | Step 1605900 | Avg Loss: 0.0154 | Grad Norm: 0.00994089\n",
      "Epoch 3 | Step 1606000 | Avg Loss: 0.0156 | Grad Norm: 0.00762997\n",
      "Epoch 3 | Step 1606100 | Avg Loss: 0.0152 | Grad Norm: 0.00831486\n",
      "Epoch 3 | Step 1606200 | Avg Loss: 0.0151 | Grad Norm: 0.00806736\n",
      "Epoch 3 | Step 1606300 | Avg Loss: 0.0151 | Grad Norm: 0.00932421\n",
      "Epoch 3 | Step 1606400 | Avg Loss: 0.0150 | Grad Norm: 0.00779793\n",
      "Epoch 3 | Step 1606500 | Avg Loss: 0.0149 | Grad Norm: 0.00999274\n",
      "Epoch 3 | Step 1606600 | Avg Loss: 0.0150 | Grad Norm: 0.00820249\n",
      "Epoch 3 | Step 1606700 | Avg Loss: 0.0151 | Grad Norm: 0.00860901\n",
      "Epoch 3 | Step 1606800 | Avg Loss: 0.0155 | Grad Norm: 0.00828677\n",
      "Epoch 3 | Step 1606900 | Avg Loss: 0.0155 | Grad Norm: 0.00840733\n",
      "Epoch 3 | Step 1607000 | Avg Loss: 0.0155 | Grad Norm: 0.00769427\n",
      "Epoch 3 | Step 1607100 | Avg Loss: 0.0152 | Grad Norm: 0.00898641\n",
      "Epoch 3 | Step 1607200 | Avg Loss: 0.0155 | Grad Norm: 0.01198686\n",
      "Epoch 3 | Step 1607300 | Avg Loss: 0.0156 | Grad Norm: 0.01029054\n",
      "Epoch 3 | Step 1607400 | Avg Loss: 0.0154 | Grad Norm: 0.00815135\n",
      "Epoch 3 | Step 1607500 | Avg Loss: 0.0151 | Grad Norm: 0.00896924\n",
      "Epoch 3 | Step 1607600 | Avg Loss: 0.0150 | Grad Norm: 0.00779193\n",
      "Epoch 3 | Step 1607700 | Avg Loss: 0.0146 | Grad Norm: 0.00760691\n",
      "Epoch 3 | Step 1607800 | Avg Loss: 0.0148 | Grad Norm: 0.00728962\n",
      "Epoch 3 | Step 1607900 | Avg Loss: 0.0148 | Grad Norm: 0.00792872\n",
      "Epoch 3 | Step 1608000 | Avg Loss: 0.0148 | Grad Norm: 0.00808836\n",
      "Epoch 3 | Step 1608100 | Avg Loss: 0.0151 | Grad Norm: 0.00756690\n",
      "Epoch 3 | Step 1608200 | Avg Loss: 0.0150 | Grad Norm: 0.00724228\n",
      "Epoch 3 | Step 1608300 | Avg Loss: 0.0150 | Grad Norm: 0.00780091\n",
      "Epoch 3 | Step 1608400 | Avg Loss: 0.0146 | Grad Norm: 0.00952347\n",
      "Epoch 3 | Step 1608500 | Avg Loss: 0.0146 | Grad Norm: 0.00778828\n",
      "Epoch 3 | Step 1608600 | Avg Loss: 0.0146 | Grad Norm: 0.00879817\n",
      "Epoch 3 | Step 1608700 | Avg Loss: 0.0145 | Grad Norm: 0.00869711\n",
      "Epoch 3 | Step 1608800 | Avg Loss: 0.0149 | Grad Norm: 0.00655621\n",
      "Epoch 3 | Step 1608900 | Avg Loss: 0.0150 | Grad Norm: 0.00822131\n",
      "Epoch 3 | Step 1609000 | Avg Loss: 0.0146 | Grad Norm: 0.00755572\n",
      "Epoch 3 | Step 1609100 | Avg Loss: 0.0144 | Grad Norm: 0.00873789\n",
      "Epoch 3 | Step 1609200 | Avg Loss: 0.0148 | Grad Norm: 0.00795336\n",
      "Epoch 3 | Step 1609300 | Avg Loss: 0.0148 | Grad Norm: 0.00768308\n",
      "Epoch 3 | Step 1609400 | Avg Loss: 0.0147 | Grad Norm: 0.00748620\n",
      "Epoch 3 | Step 1609500 | Avg Loss: 0.0148 | Grad Norm: 0.00917965\n",
      "Epoch 3 | Step 1609600 | Avg Loss: 0.0151 | Grad Norm: 0.01024651\n",
      "Epoch 3 | Step 1609700 | Avg Loss: 0.0149 | Grad Norm: 0.01133255\n",
      "Epoch 3 | Step 1609800 | Avg Loss: 0.0150 | Grad Norm: 0.00810245\n",
      "Epoch 3 | Step 1609900 | Avg Loss: 0.0156 | Grad Norm: 0.00819499\n",
      "Epoch 3 | Step 1610000 | Avg Loss: 0.0153 | Grad Norm: 0.00695895\n",
      "Epoch 3 | Step 1610100 | Avg Loss: 0.0154 | Grad Norm: 0.00773713\n",
      "Epoch 3 | Step 1610200 | Avg Loss: 0.0156 | Grad Norm: 0.00869583\n",
      "Epoch 3 | Step 1610300 | Avg Loss: 0.0156 | Grad Norm: 0.00830421\n",
      "Epoch 3 | Step 1610400 | Avg Loss: 0.0154 | Grad Norm: 0.00971837\n",
      "Epoch 3 | Step 1610500 | Avg Loss: 0.0150 | Grad Norm: 0.00842151\n",
      "Epoch 3 | Step 1610600 | Avg Loss: 0.0152 | Grad Norm: 0.01002948\n",
      "Epoch 3 | Step 1610700 | Avg Loss: 0.0150 | Grad Norm: 0.00884410\n",
      "Epoch 3 | Step 1610800 | Avg Loss: 0.0149 | Grad Norm: 0.00836106\n",
      "Epoch 3 | Step 1610900 | Avg Loss: 0.0152 | Grad Norm: 0.00950348\n",
      "Epoch 3 | Step 1611000 | Avg Loss: 0.0153 | Grad Norm: 0.00751460\n",
      "Epoch 3 | Step 1611100 | Avg Loss: 0.0150 | Grad Norm: 0.00731378\n",
      "Epoch 3 | Step 1611200 | Avg Loss: 0.0153 | Grad Norm: 0.00895732\n",
      "Epoch 3 | Step 1611300 | Avg Loss: 0.0147 | Grad Norm: 0.00810834\n",
      "Epoch 3 | Step 1611400 | Avg Loss: 0.0149 | Grad Norm: 0.00783237\n",
      "Epoch 3 | Step 1611500 | Avg Loss: 0.0152 | Grad Norm: 0.01091583\n",
      "Epoch 3 | Step 1611600 | Avg Loss: 0.0150 | Grad Norm: 0.00742892\n",
      "Epoch 3 | Step 1611700 | Avg Loss: 0.0150 | Grad Norm: 0.00775371\n",
      "Epoch 3 | Step 1611800 | Avg Loss: 0.0154 | Grad Norm: 0.00789655\n",
      "Epoch 3 | Step 1611900 | Avg Loss: 0.0156 | Grad Norm: 0.00850665\n",
      "Epoch 3 | Step 1612000 | Avg Loss: 0.0153 | Grad Norm: 0.00829261\n",
      "Epoch 3 | Step 1612100 | Avg Loss: 0.0151 | Grad Norm: 0.00779935\n",
      "Epoch 3 | Step 1612200 | Avg Loss: 0.0150 | Grad Norm: 0.00707634\n",
      "Epoch 3 | Step 1612300 | Avg Loss: 0.0150 | Grad Norm: 0.00739140\n",
      "Epoch 3 | Step 1612400 | Avg Loss: 0.0149 | Grad Norm: 0.00734918\n",
      "Epoch 3 | Step 1612500 | Avg Loss: 0.0149 | Grad Norm: 0.00770390\n",
      "Epoch 3 | Step 1612600 | Avg Loss: 0.0151 | Grad Norm: 0.00863802\n",
      "Epoch 3 | Step 1612700 | Avg Loss: 0.0149 | Grad Norm: 0.00836039\n",
      "Epoch 3 | Step 1612800 | Avg Loss: 0.0150 | Grad Norm: 0.00786246\n",
      "Epoch 3 | Step 1612900 | Avg Loss: 0.0150 | Grad Norm: 0.00798551\n",
      "Epoch 3 | Step 1613000 | Avg Loss: 0.0152 | Grad Norm: 0.00838252\n",
      "Epoch 3 | Step 1613100 | Avg Loss: 0.0151 | Grad Norm: 0.00769174\n",
      "Epoch 3 | Step 1613200 | Avg Loss: 0.0149 | Grad Norm: 0.00940528\n",
      "Epoch 3 | Step 1613300 | Avg Loss: 0.0148 | Grad Norm: 0.00880292\n",
      "Epoch 3 | Step 1613400 | Avg Loss: 0.0148 | Grad Norm: 0.00794216\n",
      "Epoch 3 | Step 1613500 | Avg Loss: 0.0152 | Grad Norm: 0.00802810\n",
      "Epoch 3 | Step 1613600 | Avg Loss: 0.0151 | Grad Norm: 0.00911696\n",
      "Epoch 3 | Step 1613700 | Avg Loss: 0.0149 | Grad Norm: 0.00791314\n",
      "Epoch 3 | Step 1613800 | Avg Loss: 0.0148 | Grad Norm: 0.01178155\n",
      "Epoch 3 | Step 1613900 | Avg Loss: 0.0150 | Grad Norm: 0.00812715\n",
      "Epoch 3 | Step 1614000 | Avg Loss: 0.0150 | Grad Norm: 0.00922141\n",
      "Epoch 3 | Step 1614100 | Avg Loss: 0.0153 | Grad Norm: 0.01022964\n",
      "Epoch 3 | Step 1614200 | Avg Loss: 0.0152 | Grad Norm: 0.00709331\n",
      "Epoch 3 | Step 1614300 | Avg Loss: 0.0152 | Grad Norm: 0.00782975\n",
      "Epoch 3 | Step 1614400 | Avg Loss: 0.0155 | Grad Norm: 0.00775901\n",
      "Epoch 3 | Step 1614500 | Avg Loss: 0.0151 | Grad Norm: 0.00901647\n",
      "Epoch 3 | Step 1614600 | Avg Loss: 0.0154 | Grad Norm: 0.00812776\n",
      "Epoch 3 | Step 1614700 | Avg Loss: 0.0154 | Grad Norm: 0.00920699\n",
      "Epoch 3 | Step 1614800 | Avg Loss: 0.0153 | Grad Norm: 0.00833501\n",
      "Epoch 3 | Step 1614900 | Avg Loss: 0.0149 | Grad Norm: 0.00958878\n",
      "Epoch 3 | Step 1615000 | Avg Loss: 0.0150 | Grad Norm: 0.00734920\n",
      "Epoch 3 | Step 1615100 | Avg Loss: 0.0150 | Grad Norm: 0.00822305\n",
      "Epoch 3 | Step 1615200 | Avg Loss: 0.0151 | Grad Norm: 0.00736733\n",
      "Epoch 3 | Step 1615300 | Avg Loss: 0.0148 | Grad Norm: 0.00878680\n",
      "Epoch 3 | Step 1615400 | Avg Loss: 0.0148 | Grad Norm: 0.00807217\n",
      "Epoch 3 | Step 1615500 | Avg Loss: 0.0146 | Grad Norm: 0.00732171\n",
      "Epoch 3 | Step 1615600 | Avg Loss: 0.0146 | Grad Norm: 0.00681475\n",
      "Epoch 3 | Step 1615700 | Avg Loss: 0.0147 | Grad Norm: 0.00846885\n",
      "Epoch 3 | Step 1615800 | Avg Loss: 0.0149 | Grad Norm: 0.00758587\n",
      "Epoch 3 | Step 1615900 | Avg Loss: 0.0150 | Grad Norm: 0.00628855\n",
      "Epoch 3 | Step 1616000 | Avg Loss: 0.0152 | Grad Norm: 0.00870824\n",
      "Epoch 3 | Step 1616100 | Avg Loss: 0.0149 | Grad Norm: 0.00895747\n",
      "Epoch 3 | Step 1616200 | Avg Loss: 0.0149 | Grad Norm: 0.00793759\n",
      "Epoch 3 | Step 1616300 | Avg Loss: 0.0150 | Grad Norm: 0.01027249\n",
      "Epoch 3 | Step 1616400 | Avg Loss: 0.0152 | Grad Norm: 0.00944141\n",
      "Epoch 3 | Step 1616500 | Avg Loss: 0.0156 | Grad Norm: 0.00846543\n",
      "Epoch 3 | Step 1616600 | Avg Loss: 0.0155 | Grad Norm: 0.00919311\n",
      "Epoch 3 | Step 1616700 | Avg Loss: 0.0156 | Grad Norm: 0.00850459\n",
      "Epoch 3 | Step 1616800 | Avg Loss: 0.0156 | Grad Norm: 0.00810996\n",
      "Epoch 3 | Step 1616900 | Avg Loss: 0.0156 | Grad Norm: 0.01143442\n",
      "Epoch 3 | Step 1617000 | Avg Loss: 0.0156 | Grad Norm: 0.00934237\n",
      "Epoch 3 | Step 1617100 | Avg Loss: 0.0155 | Grad Norm: 0.00911855\n",
      "Epoch 3 | Step 1617200 | Avg Loss: 0.0157 | Grad Norm: 0.00825717\n",
      "Epoch 3 | Step 1617300 | Avg Loss: 0.0156 | Grad Norm: 0.00923837\n",
      "Epoch 3 | Step 1617400 | Avg Loss: 0.0154 | Grad Norm: 0.00748541\n",
      "Epoch 3 | Step 1617500 | Avg Loss: 0.0157 | Grad Norm: 0.00840111\n",
      "Epoch 3 | Step 1617600 | Avg Loss: 0.0158 | Grad Norm: 0.00863847\n",
      "Epoch 3 | Step 1617700 | Avg Loss: 0.0155 | Grad Norm: 0.00739714\n",
      "Epoch 3 | Step 1617800 | Avg Loss: 0.0154 | Grad Norm: 0.00759839\n",
      "Epoch 3 | Step 1617900 | Avg Loss: 0.0151 | Grad Norm: 0.00886536\n",
      "Epoch 3 | Step 1618000 | Avg Loss: 0.0155 | Grad Norm: 0.00873354\n",
      "Epoch 3 | Step 1618100 | Avg Loss: 0.0151 | Grad Norm: 0.00745799\n",
      "Epoch 3 | Step 1618200 | Avg Loss: 0.0152 | Grad Norm: 0.00802690\n",
      "Epoch 3 | Step 1618300 | Avg Loss: 0.0150 | Grad Norm: 0.00904546\n",
      "Epoch 3 | Step 1618400 | Avg Loss: 0.0152 | Grad Norm: 0.00831985\n",
      "Epoch 3 | Step 1618500 | Avg Loss: 0.0151 | Grad Norm: 0.00810761\n",
      "Epoch 3 | Step 1618600 | Avg Loss: 0.0156 | Grad Norm: 0.00824706\n",
      "Epoch 3 | Step 1618700 | Avg Loss: 0.0158 | Grad Norm: 0.00847740\n",
      "Epoch 3 | Step 1618800 | Avg Loss: 0.0154 | Grad Norm: 0.00949915\n",
      "Epoch 3 | Step 1618900 | Avg Loss: 0.0154 | Grad Norm: 0.00864376\n",
      "Epoch 3 | Step 1619000 | Avg Loss: 0.0153 | Grad Norm: 0.00974600\n",
      "Epoch 3 | Step 1619100 | Avg Loss: 0.0154 | Grad Norm: 0.00756774\n",
      "Epoch 3 | Step 1619200 | Avg Loss: 0.0155 | Grad Norm: 0.00816156\n",
      "Epoch 3 | Step 1619300 | Avg Loss: 0.0160 | Grad Norm: 0.00954702\n",
      "Epoch 3 | Step 1619400 | Avg Loss: 0.0163 | Grad Norm: 0.01206601\n",
      "Epoch 3 | Step 1619500 | Avg Loss: 0.0158 | Grad Norm: 0.00816178\n",
      "Epoch 3 | Step 1619600 | Avg Loss: 0.0152 | Grad Norm: 0.00883587\n",
      "Epoch 3 | Step 1619700 | Avg Loss: 0.0152 | Grad Norm: 0.00857350\n",
      "Epoch 3 | Step 1619800 | Avg Loss: 0.0150 | Grad Norm: 0.01001723\n",
      "Epoch 3 | Step 1619900 | Avg Loss: 0.0149 | Grad Norm: 0.00843304\n",
      "Epoch 3 | Step 1620000 | Avg Loss: 0.0149 | Grad Norm: 0.00798963\n",
      "Epoch 3 | Step 1620100 | Avg Loss: 0.0147 | Grad Norm: 0.01190903\n",
      "Epoch 3 | Step 1620200 | Avg Loss: 0.0148 | Grad Norm: 0.00730886\n",
      "Epoch 3 | Step 1620300 | Avg Loss: 0.0148 | Grad Norm: 0.00729719\n",
      "Epoch 3 | Step 1620400 | Avg Loss: 0.0147 | Grad Norm: 0.00860373\n",
      "Epoch 3 | Step 1620500 | Avg Loss: 0.0147 | Grad Norm: 0.00879078\n",
      "Epoch 3 | Step 1620600 | Avg Loss: 0.0149 | Grad Norm: 0.00789545\n",
      "Epoch 3 | Step 1620700 | Avg Loss: 0.0150 | Grad Norm: 0.00880916\n",
      "Epoch 3 | Step 1620800 | Avg Loss: 0.0151 | Grad Norm: 0.00965085\n",
      "Epoch 3 | Step 1620900 | Avg Loss: 0.0153 | Grad Norm: 0.00827854\n",
      "Epoch 3 | Step 1621000 | Avg Loss: 0.0155 | Grad Norm: 0.00951971\n",
      "Epoch 3 | Step 1621100 | Avg Loss: 0.0150 | Grad Norm: 0.00778616\n",
      "Epoch 3 | Step 1621200 | Avg Loss: 0.0149 | Grad Norm: 0.00795360\n",
      "Epoch 3 | Step 1621300 | Avg Loss: 0.0148 | Grad Norm: 0.00844922\n",
      "Epoch 3 | Step 1621400 | Avg Loss: 0.0151 | Grad Norm: 0.00827774\n",
      "Epoch 3 | Step 1621500 | Avg Loss: 0.0151 | Grad Norm: 0.00791769\n",
      "Epoch 3 | Step 1621600 | Avg Loss: 0.0151 | Grad Norm: 0.00913921\n",
      "Epoch 3 | Step 1621700 | Avg Loss: 0.0152 | Grad Norm: 0.00795528\n",
      "Epoch 3 | Step 1621800 | Avg Loss: 0.0149 | Grad Norm: 0.00776943\n",
      "Epoch 3 | Step 1621900 | Avg Loss: 0.0149 | Grad Norm: 0.00910201\n",
      "Epoch 3 | Step 1622000 | Avg Loss: 0.0148 | Grad Norm: 0.00912022\n",
      "Epoch 3 | Step 1622100 | Avg Loss: 0.0149 | Grad Norm: 0.00793981\n",
      "Epoch 3 | Step 1622200 | Avg Loss: 0.0147 | Grad Norm: 0.00923625\n",
      "Epoch 3 | Step 1622300 | Avg Loss: 0.0146 | Grad Norm: 0.00669928\n",
      "Epoch 3 | Step 1622400 | Avg Loss: 0.0148 | Grad Norm: 0.00666600\n",
      "Epoch 3 | Step 1622500 | Avg Loss: 0.0148 | Grad Norm: 0.01060011\n",
      "Epoch 3 | Step 1622600 | Avg Loss: 0.0152 | Grad Norm: 0.00747311\n",
      "Epoch 3 | Step 1622700 | Avg Loss: 0.0153 | Grad Norm: 0.00980584\n",
      "Epoch 3 | Step 1622800 | Avg Loss: 0.0152 | Grad Norm: 0.00999460\n",
      "Epoch 3 | Step 1622900 | Avg Loss: 0.0150 | Grad Norm: 0.00756301\n",
      "Epoch 3 | Step 1623000 | Avg Loss: 0.0150 | Grad Norm: 0.00755159\n",
      "Epoch 3 | Step 1623100 | Avg Loss: 0.0147 | Grad Norm: 0.00795063\n",
      "Epoch 3 | Step 1623200 | Avg Loss: 0.0146 | Grad Norm: 0.00808968\n",
      "Epoch 3 | Step 1623300 | Avg Loss: 0.0149 | Grad Norm: 0.00881036\n",
      "Epoch 3 | Step 1623400 | Avg Loss: 0.0151 | Grad Norm: 0.00842805\n",
      "Epoch 3 | Step 1623500 | Avg Loss: 0.0151 | Grad Norm: 0.00882407\n",
      "Epoch 3 | Step 1623600 | Avg Loss: 0.0154 | Grad Norm: 0.00879247\n",
      "Epoch 3 | Step 1623700 | Avg Loss: 0.0151 | Grad Norm: 0.00944784\n",
      "Epoch 3 | Step 1623800 | Avg Loss: 0.0150 | Grad Norm: 0.00823187\n",
      "Epoch 3 | Step 1623900 | Avg Loss: 0.0149 | Grad Norm: 0.00925491\n",
      "Epoch 3 | Step 1624000 | Avg Loss: 0.0150 | Grad Norm: 0.00702493\n",
      "Epoch 3 | Step 1624100 | Avg Loss: 0.0153 | Grad Norm: 0.01109188\n",
      "Epoch 3 | Step 1624200 | Avg Loss: 0.0149 | Grad Norm: 0.00984117\n",
      "Epoch 3 | Step 1624300 | Avg Loss: 0.0150 | Grad Norm: 0.00773881\n",
      "Epoch 3 | Step 1624400 | Avg Loss: 0.0152 | Grad Norm: 0.00956199\n",
      "Epoch 3 | Step 1624500 | Avg Loss: 0.0155 | Grad Norm: 0.00889995\n",
      "Epoch 3 | Step 1624600 | Avg Loss: 0.0158 | Grad Norm: 0.00735084\n",
      "Epoch 3 | Step 1624700 | Avg Loss: 0.0154 | Grad Norm: 0.00726288\n",
      "Epoch 3 | Step 1624800 | Avg Loss: 0.0150 | Grad Norm: 0.00755377\n",
      "Epoch 3 | Step 1624900 | Avg Loss: 0.0145 | Grad Norm: 0.00932323\n",
      "Epoch 3 | Step 1625000 | Avg Loss: 0.0147 | Grad Norm: 0.00787249\n",
      "Epoch 3 | Step 1625100 | Avg Loss: 0.0149 | Grad Norm: 0.00871966\n",
      "Epoch 3 | Step 1625200 | Avg Loss: 0.0148 | Grad Norm: 0.00880925\n",
      "Epoch 3 | Step 1625300 | Avg Loss: 0.0151 | Grad Norm: 0.00925243\n",
      "Epoch 3 | Step 1625400 | Avg Loss: 0.0149 | Grad Norm: 0.00717516\n",
      "Epoch 3 | Step 1625500 | Avg Loss: 0.0155 | Grad Norm: 0.00742356\n",
      "Epoch 3 | Step 1625600 | Avg Loss: 0.0155 | Grad Norm: 0.00796748\n",
      "Epoch 3 | Step 1625700 | Avg Loss: 0.0151 | Grad Norm: 0.00739892\n",
      "Epoch 3 | Step 1625800 | Avg Loss: 0.0153 | Grad Norm: 0.01029836\n",
      "Epoch 3 | Step 1625900 | Avg Loss: 0.0151 | Grad Norm: 0.00773864\n",
      "Epoch 3 | Step 1626000 | Avg Loss: 0.0147 | Grad Norm: 0.00676379\n",
      "Epoch 3 | Step 1626100 | Avg Loss: 0.0144 | Grad Norm: 0.00780681\n",
      "Epoch 3 | Step 1626200 | Avg Loss: 0.0145 | Grad Norm: 0.00806529\n",
      "Epoch 3 | Step 1626300 | Avg Loss: 0.0144 | Grad Norm: 0.01106423\n",
      "Epoch 3 | Step 1626400 | Avg Loss: 0.0145 | Grad Norm: 0.00849793\n",
      "Epoch 3 | Step 1626500 | Avg Loss: 0.0143 | Grad Norm: 0.00789573\n",
      "Epoch 3 | Step 1626600 | Avg Loss: 0.0143 | Grad Norm: 0.00686368\n",
      "Epoch 3 | Step 1626700 | Avg Loss: 0.0144 | Grad Norm: 0.00870409\n",
      "Epoch 3 | Step 1626800 | Avg Loss: 0.0146 | Grad Norm: 0.01026164\n",
      "Epoch 3 | Step 1626900 | Avg Loss: 0.0151 | Grad Norm: 0.01097199\n",
      "Epoch 3 | Step 1627000 | Avg Loss: 0.0154 | Grad Norm: 0.00914929\n",
      "Epoch 3 | Step 1627100 | Avg Loss: 0.0153 | Grad Norm: 0.00770734\n",
      "Epoch 3 | Step 1627200 | Avg Loss: 0.0151 | Grad Norm: 0.00807408\n",
      "Epoch 3 | Step 1627300 | Avg Loss: 0.0151 | Grad Norm: 0.00899972\n",
      "Epoch 3 | Step 1627400 | Avg Loss: 0.0148 | Grad Norm: 0.00744273\n",
      "Epoch 3 | Step 1627500 | Avg Loss: 0.0149 | Grad Norm: 0.00773385\n",
      "Epoch 3 | Step 1627600 | Avg Loss: 0.0148 | Grad Norm: 0.00802665\n",
      "Epoch 3 | Step 1627700 | Avg Loss: 0.0149 | Grad Norm: 0.00757007\n",
      "Epoch 3 | Step 1627800 | Avg Loss: 0.0147 | Grad Norm: 0.00721789\n",
      "Epoch 3 | Step 1627900 | Avg Loss: 0.0147 | Grad Norm: 0.00809222\n",
      "Epoch 3 | Step 1628000 | Avg Loss: 0.0144 | Grad Norm: 0.00784451\n",
      "Epoch 3 | Step 1628100 | Avg Loss: 0.0145 | Grad Norm: 0.00792458\n",
      "Epoch 3 | Step 1628200 | Avg Loss: 0.0143 | Grad Norm: 0.00844148\n",
      "Epoch 3 | Step 1628300 | Avg Loss: 0.0147 | Grad Norm: 0.00901962\n",
      "Epoch 3 | Step 1628400 | Avg Loss: 0.0146 | Grad Norm: 0.00845317\n",
      "Epoch 3 | Step 1628500 | Avg Loss: 0.0148 | Grad Norm: 0.00925886\n",
      "Epoch 3 | Step 1628600 | Avg Loss: 0.0149 | Grad Norm: 0.00851532\n",
      "Epoch 3 | Step 1628700 | Avg Loss: 0.0147 | Grad Norm: 0.00791225\n",
      "Epoch 3 | Step 1628800 | Avg Loss: 0.0145 | Grad Norm: 0.00864408\n",
      "Epoch 3 | Step 1628900 | Avg Loss: 0.0153 | Grad Norm: 0.00951348\n",
      "Epoch 3 | Step 1629000 | Avg Loss: 0.0151 | Grad Norm: 0.00773249\n",
      "Epoch 3 | Step 1629100 | Avg Loss: 0.0156 | Grad Norm: 0.00829496\n",
      "Epoch 3 | Step 1629200 | Avg Loss: 0.0150 | Grad Norm: 0.00760787\n",
      "Epoch 3 | Step 1629300 | Avg Loss: 0.0144 | Grad Norm: 0.00787247\n",
      "Epoch 3 | Step 1629400 | Avg Loss: 0.0145 | Grad Norm: 0.00778018\n",
      "Epoch 3 | Step 1629500 | Avg Loss: 0.0149 | Grad Norm: 0.00778963\n",
      "Epoch 3 | Step 1629600 | Avg Loss: 0.0147 | Grad Norm: 0.00892627\n",
      "Epoch 3 | Step 1629700 | Avg Loss: 0.0148 | Grad Norm: 0.01054840\n",
      "Epoch 3 | Step 1629800 | Avg Loss: 0.0146 | Grad Norm: 0.00701970\n",
      "Epoch 3 | Step 1629900 | Avg Loss: 0.0148 | Grad Norm: 0.00774000\n",
      "Epoch 3 | Step 1630000 | Avg Loss: 0.0151 | Grad Norm: 0.00801704\n",
      "Epoch 3 | Step 1630100 | Avg Loss: 0.0156 | Grad Norm: 0.00920101\n",
      "Epoch 3 | Step 1630200 | Avg Loss: 0.0152 | Grad Norm: 0.00822193\n",
      "Epoch 3 | Step 1630300 | Avg Loss: 0.0152 | Grad Norm: 0.01023165\n",
      "Epoch 3 | Step 1630400 | Avg Loss: 0.0150 | Grad Norm: 0.00695164\n",
      "Epoch 3 | Step 1630500 | Avg Loss: 0.0146 | Grad Norm: 0.00777790\n",
      "Epoch 3 | Step 1630600 | Avg Loss: 0.0149 | Grad Norm: 0.00851152\n",
      "Epoch 3 | Step 1630700 | Avg Loss: 0.0149 | Grad Norm: 0.01206839\n",
      "Epoch 3 | Step 1630800 | Avg Loss: 0.0148 | Grad Norm: 0.00984339\n",
      "Epoch 3 | Step 1630900 | Avg Loss: 0.0150 | Grad Norm: 0.00815172\n",
      "Epoch 3 | Step 1631000 | Avg Loss: 0.0147 | Grad Norm: 0.00790451\n",
      "Epoch 3 | Step 1631100 | Avg Loss: 0.0150 | Grad Norm: 0.00778780\n",
      "Epoch 3 | Step 1631200 | Avg Loss: 0.0149 | Grad Norm: 0.00868661\n",
      "Epoch 3 | Step 1631300 | Avg Loss: 0.0151 | Grad Norm: 0.00713331\n",
      "Epoch 3 | Step 1631400 | Avg Loss: 0.0153 | Grad Norm: 0.00763837\n",
      "Epoch 3 | Step 1631500 | Avg Loss: 0.0156 | Grad Norm: 0.01022962\n",
      "Epoch 3 | Step 1631600 | Avg Loss: 0.0152 | Grad Norm: 0.00989588\n",
      "Epoch 3 | Step 1631700 | Avg Loss: 0.0150 | Grad Norm: 0.00769035\n",
      "Epoch 3 | Step 1631800 | Avg Loss: 0.0149 | Grad Norm: 0.01000333\n",
      "Epoch 3 | Step 1631900 | Avg Loss: 0.0148 | Grad Norm: 0.00808426\n",
      "Epoch 3 | Step 1632000 | Avg Loss: 0.0152 | Grad Norm: 0.00956738\n",
      "Epoch 3 | Step 1632100 | Avg Loss: 0.0152 | Grad Norm: 0.00793211\n",
      "Epoch 3 | Step 1632200 | Avg Loss: 0.0148 | Grad Norm: 0.00748005\n",
      "Epoch 3 | Step 1632300 | Avg Loss: 0.0149 | Grad Norm: 0.00672999\n",
      "Epoch 3 | Step 1632400 | Avg Loss: 0.0151 | Grad Norm: 0.00954550\n",
      "Epoch 3 | Step 1632500 | Avg Loss: 0.0153 | Grad Norm: 0.00728585\n",
      "Epoch 3 | Step 1632600 | Avg Loss: 0.0153 | Grad Norm: 0.00861574\n",
      "Epoch 3 | Step 1632700 | Avg Loss: 0.0151 | Grad Norm: 0.00893507\n",
      "Epoch 3 | Step 1632800 | Avg Loss: 0.0151 | Grad Norm: 0.00760108\n",
      "Epoch 3 | Step 1632900 | Avg Loss: 0.0149 | Grad Norm: 0.00707201\n",
      "Epoch 3 | Step 1633000 | Avg Loss: 0.0149 | Grad Norm: 0.00819927\n",
      "Epoch 3 | Step 1633100 | Avg Loss: 0.0152 | Grad Norm: 0.00719242\n",
      "Epoch 3 | Step 1633200 | Avg Loss: 0.0152 | Grad Norm: 0.00753084\n",
      "Epoch 3 | Step 1633300 | Avg Loss: 0.0146 | Grad Norm: 0.00885318\n",
      "Epoch 3 | Step 1633400 | Avg Loss: 0.0148 | Grad Norm: 0.00898759\n",
      "Epoch 3 | Step 1633500 | Avg Loss: 0.0153 | Grad Norm: 0.00754200\n",
      "Epoch 3 | Step 1633600 | Avg Loss: 0.0153 | Grad Norm: 0.00749945\n",
      "Epoch 3 | Step 1633700 | Avg Loss: 0.0155 | Grad Norm: 0.00685271\n",
      "Epoch 3 | Step 1633800 | Avg Loss: 0.0151 | Grad Norm: 0.00719697\n",
      "Epoch 3 | Step 1633900 | Avg Loss: 0.0151 | Grad Norm: 0.00861186\n",
      "Epoch 3 | Step 1634000 | Avg Loss: 0.0149 | Grad Norm: 0.00727665\n",
      "Epoch 3 | Step 1634100 | Avg Loss: 0.0153 | Grad Norm: 0.00777950\n",
      "Epoch 3 | Step 1634200 | Avg Loss: 0.0151 | Grad Norm: 0.00748018\n",
      "Epoch 3 | Step 1634300 | Avg Loss: 0.0151 | Grad Norm: 0.00784921\n",
      "Epoch 3 | Step 1634400 | Avg Loss: 0.0148 | Grad Norm: 0.00877103\n",
      "Epoch 3 | Step 1634500 | Avg Loss: 0.0152 | Grad Norm: 0.00769748\n",
      "Epoch 3 | Step 1634600 | Avg Loss: 0.0150 | Grad Norm: 0.00883362\n",
      "Epoch 3 | Step 1634700 | Avg Loss: 0.0149 | Grad Norm: 0.00728069\n",
      "Epoch 3 | Step 1634800 | Avg Loss: 0.0149 | Grad Norm: 0.00813146\n",
      "Epoch 3 | Step 1634900 | Avg Loss: 0.0148 | Grad Norm: 0.00910657\n",
      "Epoch 3 | Step 1635000 | Avg Loss: 0.0152 | Grad Norm: 0.00852737\n",
      "Epoch 3 | Step 1635100 | Avg Loss: 0.0151 | Grad Norm: 0.00807242\n",
      "Epoch 3 | Step 1635200 | Avg Loss: 0.0150 | Grad Norm: 0.00659979\n",
      "Epoch 3 | Step 1635300 | Avg Loss: 0.0151 | Grad Norm: 0.00790569\n",
      "Epoch 3 | Step 1635400 | Avg Loss: 0.0154 | Grad Norm: 0.01098034\n",
      "Epoch 3 | Step 1635500 | Avg Loss: 0.0157 | Grad Norm: 0.01119303\n",
      "Epoch 3 | Step 1635600 | Avg Loss: 0.0156 | Grad Norm: 0.00838218\n",
      "Epoch 3 | Step 1635700 | Avg Loss: 0.0154 | Grad Norm: 0.00902387\n",
      "Epoch 3 | Step 1635800 | Avg Loss: 0.0154 | Grad Norm: 0.00829863\n",
      "Epoch 3 | Step 1635900 | Avg Loss: 0.0151 | Grad Norm: 0.00800047\n",
      "Epoch 3 | Step 1636000 | Avg Loss: 0.0148 | Grad Norm: 0.00925087\n",
      "Epoch 3 | Step 1636100 | Avg Loss: 0.0148 | Grad Norm: 0.00930373\n",
      "Epoch 3 | Step 1636200 | Avg Loss: 0.0148 | Grad Norm: 0.00863348\n",
      "Epoch 3 | Step 1636300 | Avg Loss: 0.0150 | Grad Norm: 0.00793936\n",
      "Epoch 3 | Step 1636400 | Avg Loss: 0.0143 | Grad Norm: 0.00753466\n",
      "Epoch 3 | Step 1636500 | Avg Loss: 0.0146 | Grad Norm: 0.00804697\n",
      "Epoch 3 | Step 1636600 | Avg Loss: 0.0150 | Grad Norm: 0.01065237\n",
      "Epoch 3 | Step 1636700 | Avg Loss: 0.0148 | Grad Norm: 0.00863139\n",
      "Epoch 3 | Step 1636800 | Avg Loss: 0.0146 | Grad Norm: 0.00761305\n",
      "Epoch 3 | Step 1636900 | Avg Loss: 0.0149 | Grad Norm: 0.00976054\n",
      "Epoch 3 | Step 1637000 | Avg Loss: 0.0152 | Grad Norm: 0.00780526\n",
      "Epoch 3 | Step 1637100 | Avg Loss: 0.0154 | Grad Norm: 0.00752854\n",
      "Epoch 3 | Step 1637200 | Avg Loss: 0.0153 | Grad Norm: 0.00819820\n",
      "Epoch 3 | Step 1637300 | Avg Loss: 0.0152 | Grad Norm: 0.00767408\n",
      "Epoch 3 | Step 1637400 | Avg Loss: 0.0150 | Grad Norm: 0.00988186\n",
      "Epoch 3 | Step 1637500 | Avg Loss: 0.0155 | Grad Norm: 0.00829940\n",
      "Epoch 3 | Step 1637600 | Avg Loss: 0.0154 | Grad Norm: 0.00702563\n",
      "Epoch 3 | Step 1637700 | Avg Loss: 0.0150 | Grad Norm: 0.01055836\n",
      "Epoch 3 | Step 1637800 | Avg Loss: 0.0151 | Grad Norm: 0.00863269\n",
      "Epoch 3 | Step 1637900 | Avg Loss: 0.0150 | Grad Norm: 0.00814740\n",
      "Epoch 3 | Step 1638000 | Avg Loss: 0.0149 | Grad Norm: 0.00984013\n",
      "Epoch 3 | Step 1638100 | Avg Loss: 0.0149 | Grad Norm: 0.00815159\n",
      "Epoch 3 | Step 1638200 | Avg Loss: 0.0152 | Grad Norm: 0.00745682\n",
      "Epoch 3 | Step 1638300 | Avg Loss: 0.0153 | Grad Norm: 0.00745779\n",
      "Epoch 3 | Step 1638400 | Avg Loss: 0.0151 | Grad Norm: 0.00744259\n",
      "Epoch 3 | Step 1638500 | Avg Loss: 0.0154 | Grad Norm: 0.00715576\n",
      "Epoch 3 | Step 1638600 | Avg Loss: 0.0153 | Grad Norm: 0.00727630\n",
      "Epoch 3 | Step 1638700 | Avg Loss: 0.0153 | Grad Norm: 0.00849160\n",
      "Epoch 3 | Step 1638800 | Avg Loss: 0.0156 | Grad Norm: 0.00892664\n",
      "Epoch 3 | Step 1638900 | Avg Loss: 0.0151 | Grad Norm: 0.00807170\n",
      "Epoch 3 | Step 1639000 | Avg Loss: 0.0151 | Grad Norm: 0.00720666\n",
      "Epoch 3 | Step 1639100 | Avg Loss: 0.0150 | Grad Norm: 0.00860739\n",
      "Epoch 3 | Step 1639200 | Avg Loss: 0.0149 | Grad Norm: 0.00780609\n",
      "Epoch 3 | Step 1639300 | Avg Loss: 0.0150 | Grad Norm: 0.00878225\n",
      "Epoch 3 | Step 1639400 | Avg Loss: 0.0150 | Grad Norm: 0.00877178\n",
      "Epoch 3 | Step 1639500 | Avg Loss: 0.0151 | Grad Norm: 0.00653790\n",
      "Epoch 3 | Step 1639600 | Avg Loss: 0.0152 | Grad Norm: 0.00750921\n",
      "Epoch 3 | Step 1639700 | Avg Loss: 0.0152 | Grad Norm: 0.00778077\n",
      "Epoch 3 | Step 1639800 | Avg Loss: 0.0154 | Grad Norm: 0.00796673\n",
      "Epoch 3 | Step 1639900 | Avg Loss: 0.0151 | Grad Norm: 0.00781157\n",
      "Epoch 3 | Step 1640000 | Avg Loss: 0.0156 | Grad Norm: 0.00865106\n",
      "Epoch 3 | Step 1640100 | Avg Loss: 0.0157 | Grad Norm: 0.00770091\n",
      "Epoch 3 | Step 1640200 | Avg Loss: 0.0157 | Grad Norm: 0.00925685\n",
      "Epoch 3 | Step 1640300 | Avg Loss: 0.0157 | Grad Norm: 0.01099853\n",
      "Epoch 3 | Step 1640400 | Avg Loss: 0.0158 | Grad Norm: 0.00904683\n",
      "Epoch 3 | Step 1640500 | Avg Loss: 0.0156 | Grad Norm: 0.01150900\n",
      "Epoch 3 | Step 1640600 | Avg Loss: 0.0154 | Grad Norm: 0.00795980\n",
      "Epoch 3 | Step 1640700 | Avg Loss: 0.0155 | Grad Norm: 0.00826903\n",
      "Epoch 3 | Step 1640800 | Avg Loss: 0.0158 | Grad Norm: 0.00734881\n",
      "Epoch 3 | Step 1640900 | Avg Loss: 0.0158 | Grad Norm: 0.00743847\n",
      "Epoch 3 | Step 1641000 | Avg Loss: 0.0157 | Grad Norm: 0.00847904\n",
      "Epoch 3 | Step 1641100 | Avg Loss: 0.0155 | Grad Norm: 0.00861757\n",
      "Epoch 3 | Step 1641200 | Avg Loss: 0.0155 | Grad Norm: 0.00816733\n",
      "Epoch 3 | Step 1641300 | Avg Loss: 0.0157 | Grad Norm: 0.00901204\n",
      "Epoch 3 | Step 1641400 | Avg Loss: 0.0155 | Grad Norm: 0.01029034\n",
      "Epoch 3 | Step 1641500 | Avg Loss: 0.0156 | Grad Norm: 0.00758027\n",
      "Epoch 3 | Step 1641600 | Avg Loss: 0.0154 | Grad Norm: 0.00692759\n",
      "Epoch 3 | Step 1641700 | Avg Loss: 0.0150 | Grad Norm: 0.00736135\n",
      "Epoch 3 | Step 1641800 | Avg Loss: 0.0149 | Grad Norm: 0.00826536\n",
      "Epoch 3 | Step 1641900 | Avg Loss: 0.0149 | Grad Norm: 0.00773957\n",
      "Epoch 3 | Step 1642000 | Avg Loss: 0.0151 | Grad Norm: 0.00781946\n",
      "Epoch 3 | Step 1642100 | Avg Loss: 0.0154 | Grad Norm: 0.00742599\n",
      "Epoch 3 | Step 1642200 | Avg Loss: 0.0157 | Grad Norm: 0.00831960\n",
      "Epoch 3 | Step 1642300 | Avg Loss: 0.0154 | Grad Norm: 0.00819517\n",
      "Epoch 3 | Step 1642400 | Avg Loss: 0.0157 | Grad Norm: 0.00821769\n",
      "Epoch 3 | Step 1642500 | Avg Loss: 0.0155 | Grad Norm: 0.00736654\n",
      "Epoch 3 | Step 1642600 | Avg Loss: 0.0154 | Grad Norm: 0.00903334\n",
      "Epoch 3 | Step 1642700 | Avg Loss: 0.0151 | Grad Norm: 0.00906500\n",
      "Epoch 3 | Step 1642800 | Avg Loss: 0.0152 | Grad Norm: 0.00730480\n",
      "Epoch 3 | Step 1642900 | Avg Loss: 0.0155 | Grad Norm: 0.00868133\n",
      "Epoch 3 | Step 1643000 | Avg Loss: 0.0150 | Grad Norm: 0.00762840\n",
      "Epoch 3 | Step 1643100 | Avg Loss: 0.0152 | Grad Norm: 0.00778550\n",
      "Epoch 3 | Step 1643200 | Avg Loss: 0.0153 | Grad Norm: 0.00711490\n",
      "Epoch 3 | Step 1643300 | Avg Loss: 0.0149 | Grad Norm: 0.00846422\n",
      "Epoch 3 | Step 1643400 | Avg Loss: 0.0146 | Grad Norm: 0.00826251\n",
      "Epoch 3 | Step 1643500 | Avg Loss: 0.0149 | Grad Norm: 0.00765438\n",
      "Epoch 3 | Step 1643600 | Avg Loss: 0.0150 | Grad Norm: 0.00906308\n",
      "Epoch 3 | Step 1643700 | Avg Loss: 0.0147 | Grad Norm: 0.00909800\n",
      "Epoch 3 | Step 1643800 | Avg Loss: 0.0144 | Grad Norm: 0.00686791\n",
      "Epoch 3 | Step 1643900 | Avg Loss: 0.0147 | Grad Norm: 0.00803550\n",
      "Epoch 3 | Step 1644000 | Avg Loss: 0.0152 | Grad Norm: 0.00879748\n",
      "Epoch 3 | Step 1644100 | Avg Loss: 0.0147 | Grad Norm: 0.00831392\n",
      "Epoch 3 | Step 1644200 | Avg Loss: 0.0147 | Grad Norm: 0.00775319\n",
      "Epoch 3 | Step 1644300 | Avg Loss: 0.0148 | Grad Norm: 0.00804655\n",
      "Epoch 3 | Step 1644400 | Avg Loss: 0.0151 | Grad Norm: 0.00794233\n",
      "Epoch 3 | Step 1644500 | Avg Loss: 0.0148 | Grad Norm: 0.00928173\n",
      "Epoch 3 | Step 1644600 | Avg Loss: 0.0147 | Grad Norm: 0.01037677\n",
      "Epoch 3 | Step 1644700 | Avg Loss: 0.0147 | Grad Norm: 0.00782530\n",
      "Epoch 3 | Step 1644800 | Avg Loss: 0.0146 | Grad Norm: 0.00836598\n",
      "Epoch 3 | Step 1644900 | Avg Loss: 0.0148 | Grad Norm: 0.00803651\n",
      "Epoch 3 | Step 1645000 | Avg Loss: 0.0147 | Grad Norm: 0.00825824\n",
      "Epoch 3 | Step 1645100 | Avg Loss: 0.0149 | Grad Norm: 0.00731288\n",
      "Epoch 3 | Step 1645200 | Avg Loss: 0.0148 | Grad Norm: 0.00780173\n",
      "Epoch 3 | Step 1645300 | Avg Loss: 0.0149 | Grad Norm: 0.00795207\n",
      "Epoch 3 | Step 1645400 | Avg Loss: 0.0150 | Grad Norm: 0.00881602\n",
      "Epoch 3 | Step 1645500 | Avg Loss: 0.0152 | Grad Norm: 0.00781705\n",
      "Epoch 3 | Step 1645600 | Avg Loss: 0.0152 | Grad Norm: 0.00679879\n",
      "Epoch 3 | Step 1645700 | Avg Loss: 0.0148 | Grad Norm: 0.00726091\n",
      "Epoch 3 | Step 1645800 | Avg Loss: 0.0150 | Grad Norm: 0.00933898\n",
      "Epoch 3 | Step 1645900 | Avg Loss: 0.0151 | Grad Norm: 0.00793645\n",
      "Epoch 3 | Step 1646000 | Avg Loss: 0.0156 | Grad Norm: 0.00700844\n",
      "Epoch 3 | Step 1646100 | Avg Loss: 0.0153 | Grad Norm: 0.00903049\n",
      "Epoch 3 | Step 1646200 | Avg Loss: 0.0148 | Grad Norm: 0.00749349\n",
      "Epoch 3 | Step 1646300 | Avg Loss: 0.0155 | Grad Norm: 0.00792413\n",
      "Epoch 3 | Step 1646400 | Avg Loss: 0.0153 | Grad Norm: 0.00934882\n",
      "Epoch 3 | Step 1646500 | Avg Loss: 0.0152 | Grad Norm: 0.00732563\n",
      "Epoch 3 | Step 1646600 | Avg Loss: 0.0149 | Grad Norm: 0.00751350\n",
      "Epoch 3 | Step 1646700 | Avg Loss: 0.0148 | Grad Norm: 0.00853280\n",
      "Epoch 3 | Step 1646800 | Avg Loss: 0.0149 | Grad Norm: 0.00743527\n",
      "Epoch 3 | Step 1646900 | Avg Loss: 0.0156 | Grad Norm: 0.00984066\n",
      "Epoch 3 | Step 1647000 | Avg Loss: 0.0150 | Grad Norm: 0.00855008\n",
      "Epoch 3 | Step 1647100 | Avg Loss: 0.0150 | Grad Norm: 0.00900777\n",
      "Epoch 3 | Step 1647200 | Avg Loss: 0.0149 | Grad Norm: 0.00824583\n",
      "Epoch 3 | Step 1647300 | Avg Loss: 0.0150 | Grad Norm: 0.00968826\n",
      "Epoch 3 | Step 1647400 | Avg Loss: 0.0153 | Grad Norm: 0.00836317\n",
      "Epoch 3 | Step 1647500 | Avg Loss: 0.0147 | Grad Norm: 0.00726631\n",
      "Epoch 3 | Step 1647600 | Avg Loss: 0.0149 | Grad Norm: 0.01013824\n",
      "Epoch 3 | Step 1647700 | Avg Loss: 0.0147 | Grad Norm: 0.00799064\n",
      "Epoch 3 | Step 1647800 | Avg Loss: 0.0149 | Grad Norm: 0.00968501\n",
      "Epoch 3 | Step 1647900 | Avg Loss: 0.0146 | Grad Norm: 0.00802878\n",
      "Epoch 3 | Step 1648000 | Avg Loss: 0.0145 | Grad Norm: 0.00787394\n",
      "Epoch 3 | Step 1648100 | Avg Loss: 0.0142 | Grad Norm: 0.00794207\n",
      "Epoch 3 | Step 1648200 | Avg Loss: 0.0145 | Grad Norm: 0.00715057\n",
      "Epoch 3 | Step 1648300 | Avg Loss: 0.0146 | Grad Norm: 0.00754986\n",
      "Epoch 3 | Step 1648400 | Avg Loss: 0.0151 | Grad Norm: 0.00806094\n",
      "Epoch 3 | Step 1648500 | Avg Loss: 0.0152 | Grad Norm: 0.00773283\n",
      "Epoch 3 | Step 1648600 | Avg Loss: 0.0150 | Grad Norm: 0.00941802\n",
      "Epoch 3 | Step 1648700 | Avg Loss: 0.0152 | Grad Norm: 0.00794760\n",
      "Epoch 3 | Step 1648800 | Avg Loss: 0.0154 | Grad Norm: 0.00797965\n",
      "Epoch 3 | Step 1648900 | Avg Loss: 0.0151 | Grad Norm: 0.00749636\n",
      "Epoch 3 | Step 1649000 | Avg Loss: 0.0149 | Grad Norm: 0.00802039\n",
      "Epoch 3 | Step 1649100 | Avg Loss: 0.0150 | Grad Norm: 0.00869127\n",
      "Epoch 3 | Step 1649200 | Avg Loss: 0.0147 | Grad Norm: 0.00916769\n",
      "Epoch 3 | Step 1649300 | Avg Loss: 0.0152 | Grad Norm: 0.00831097\n",
      "Epoch 3 | Step 1649400 | Avg Loss: 0.0150 | Grad Norm: 0.00792916\n",
      "Epoch 3 | Step 1649500 | Avg Loss: 0.0153 | Grad Norm: 0.00758567\n",
      "Epoch 3 | Step 1649600 | Avg Loss: 0.0152 | Grad Norm: 0.00798749\n",
      "Epoch 3 | Step 1649700 | Avg Loss: 0.0154 | Grad Norm: 0.00710680\n",
      "Epoch 3 | Step 1649800 | Avg Loss: 0.0154 | Grad Norm: 0.00953903\n",
      "Epoch 3 | Step 1649900 | Avg Loss: 0.0155 | Grad Norm: 0.00779545\n",
      "Epoch 3 | Step 1650000 | Avg Loss: 0.0154 | Grad Norm: 0.00857915\n",
      "Epoch 3 | Step 1650100 | Avg Loss: 0.0155 | Grad Norm: 0.00856641\n",
      "Epoch 3 | Step 1650200 | Avg Loss: 0.0158 | Grad Norm: 0.00814009\n",
      "Epoch 3 | Step 1650300 | Avg Loss: 0.0154 | Grad Norm: 0.00784475\n",
      "Epoch 3 | Step 1650400 | Avg Loss: 0.0154 | Grad Norm: 0.00863330\n",
      "Epoch 3 | Step 1650500 | Avg Loss: 0.0154 | Grad Norm: 0.00820011\n",
      "Epoch 3 | Step 1650600 | Avg Loss: 0.0153 | Grad Norm: 0.00783984\n",
      "Epoch 3 | Step 1650700 | Avg Loss: 0.0154 | Grad Norm: 0.00847664\n",
      "Epoch 3 | Step 1650800 | Avg Loss: 0.0151 | Grad Norm: 0.00881439\n",
      "Epoch 3 | Step 1650900 | Avg Loss: 0.0148 | Grad Norm: 0.00820691\n",
      "Epoch 3 | Step 1651000 | Avg Loss: 0.0148 | Grad Norm: 0.00875327\n",
      "Epoch 3 | Step 1651100 | Avg Loss: 0.0145 | Grad Norm: 0.00790766\n",
      "Epoch 3 | Step 1651200 | Avg Loss: 0.0147 | Grad Norm: 0.00789429\n",
      "Epoch 3 | Step 1651300 | Avg Loss: 0.0148 | Grad Norm: 0.00793439\n",
      "Epoch 3 | Step 1651400 | Avg Loss: 0.0149 | Grad Norm: 0.00892556\n",
      "Epoch 3 | Step 1651500 | Avg Loss: 0.0150 | Grad Norm: 0.00933566\n",
      "Epoch 3 | Step 1651600 | Avg Loss: 0.0150 | Grad Norm: 0.00796265\n",
      "Epoch 3 | Step 1651700 | Avg Loss: 0.0152 | Grad Norm: 0.00763358\n",
      "Epoch 3 | Step 1651800 | Avg Loss: 0.0155 | Grad Norm: 0.00814389\n",
      "Epoch 3 | Step 1651900 | Avg Loss: 0.0155 | Grad Norm: 0.00841620\n",
      "Epoch 3 | Step 1652000 | Avg Loss: 0.0154 | Grad Norm: 0.00832655\n",
      "Epoch 3 | Step 1652100 | Avg Loss: 0.0153 | Grad Norm: 0.00861604\n",
      "Epoch 3 | Step 1652200 | Avg Loss: 0.0145 | Grad Norm: 0.00858773\n",
      "Epoch 3 | Step 1652300 | Avg Loss: 0.0141 | Grad Norm: 0.00791014\n",
      "Epoch 3 | Step 1652400 | Avg Loss: 0.0149 | Grad Norm: 0.00822129\n",
      "Epoch 3 | Step 1652500 | Avg Loss: 0.0153 | Grad Norm: 0.00975321\n",
      "Epoch 3 | Step 1652600 | Avg Loss: 0.0151 | Grad Norm: 0.00733019\n",
      "Epoch 3 | Step 1652700 | Avg Loss: 0.0154 | Grad Norm: 0.00798540\n",
      "Epoch 3 | Step 1652800 | Avg Loss: 0.0151 | Grad Norm: 0.00849615\n",
      "Epoch 3 | Step 1652900 | Avg Loss: 0.0154 | Grad Norm: 0.00918153\n",
      "Epoch 3 | Step 1653000 | Avg Loss: 0.0155 | Grad Norm: 0.00824025\n",
      "Epoch 3 | Step 1653100 | Avg Loss: 0.0156 | Grad Norm: 0.00849373\n",
      "Epoch 3 | Step 1653200 | Avg Loss: 0.0154 | Grad Norm: 0.00871615\n",
      "Epoch 3 | Step 1653300 | Avg Loss: 0.0151 | Grad Norm: 0.00826368\n",
      "Epoch 3 | Step 1653400 | Avg Loss: 0.0147 | Grad Norm: 0.00807131\n",
      "Epoch 3 | Step 1653500 | Avg Loss: 0.0146 | Grad Norm: 0.00730811\n",
      "Epoch 3 | Step 1653600 | Avg Loss: 0.0145 | Grad Norm: 0.00910467\n",
      "Epoch 3 | Step 1653700 | Avg Loss: 0.0149 | Grad Norm: 0.00896579\n",
      "Epoch 3 | Step 1653800 | Avg Loss: 0.0153 | Grad Norm: 0.00899578\n",
      "Epoch 3 | Step 1653900 | Avg Loss: 0.0152 | Grad Norm: 0.00812722\n",
      "Epoch 3 | Step 1654000 | Avg Loss: 0.0147 | Grad Norm: 0.00790153\n",
      "Epoch 3 | Step 1654100 | Avg Loss: 0.0148 | Grad Norm: 0.00740435\n",
      "Epoch 3 | Step 1654200 | Avg Loss: 0.0151 | Grad Norm: 0.00848769\n",
      "Epoch 3 | Step 1654300 | Avg Loss: 0.0148 | Grad Norm: 0.00940556\n",
      "Epoch 3 | Step 1654400 | Avg Loss: 0.0150 | Grad Norm: 0.01026662\n",
      "Epoch 3 | Step 1654500 | Avg Loss: 0.0152 | Grad Norm: 0.01006970\n",
      "Epoch 3 | Step 1654600 | Avg Loss: 0.0153 | Grad Norm: 0.00779167\n",
      "Epoch 3 | Step 1654700 | Avg Loss: 0.0151 | Grad Norm: 0.00836759\n",
      "Epoch 3 | Step 1654800 | Avg Loss: 0.0154 | Grad Norm: 0.00970410\n",
      "Epoch 3 | Step 1654900 | Avg Loss: 0.0152 | Grad Norm: 0.00930408\n",
      "Epoch 3 | Step 1655000 | Avg Loss: 0.0153 | Grad Norm: 0.00866715\n",
      "Epoch 3 | Step 1655100 | Avg Loss: 0.0151 | Grad Norm: 0.00735809\n",
      "Epoch 3 | Step 1655200 | Avg Loss: 0.0151 | Grad Norm: 0.00745639\n",
      "Epoch 3 | Step 1655300 | Avg Loss: 0.0149 | Grad Norm: 0.00699969\n",
      "Epoch 3 | Step 1655400 | Avg Loss: 0.0147 | Grad Norm: 0.00803779\n",
      "Epoch 3 | Step 1655500 | Avg Loss: 0.0147 | Grad Norm: 0.00750977\n",
      "Epoch 3 | Step 1655600 | Avg Loss: 0.0150 | Grad Norm: 0.00930531\n",
      "Epoch 3 | Step 1655700 | Avg Loss: 0.0148 | Grad Norm: 0.00996036\n",
      "Epoch 3 | Step 1655800 | Avg Loss: 0.0146 | Grad Norm: 0.00798203\n",
      "Epoch 3 | Step 1655900 | Avg Loss: 0.0146 | Grad Norm: 0.00844438\n",
      "Epoch 3 | Step 1656000 | Avg Loss: 0.0148 | Grad Norm: 0.00751861\n",
      "Epoch 3 | Step 1656100 | Avg Loss: 0.0147 | Grad Norm: 0.00799302\n",
      "Epoch 3 | Step 1656200 | Avg Loss: 0.0146 | Grad Norm: 0.00772030\n",
      "Epoch 3 | Step 1656300 | Avg Loss: 0.0148 | Grad Norm: 0.01182777\n",
      "Epoch 3 | Step 1656400 | Avg Loss: 0.0152 | Grad Norm: 0.00837023\n",
      "Epoch 3 | Step 1656500 | Avg Loss: 0.0152 | Grad Norm: 0.00767825\n",
      "Epoch 3 | Step 1656600 | Avg Loss: 0.0151 | Grad Norm: 0.00801794\n",
      "Epoch 3 | Step 1656700 | Avg Loss: 0.0151 | Grad Norm: 0.00823092\n",
      "Epoch 3 | Step 1656800 | Avg Loss: 0.0151 | Grad Norm: 0.00741132\n",
      "Epoch 3 | Step 1656900 | Avg Loss: 0.0152 | Grad Norm: 0.00787316\n",
      "Epoch 3 | Step 1657000 | Avg Loss: 0.0149 | Grad Norm: 0.00931508\n",
      "Epoch 3 | Step 1657100 | Avg Loss: 0.0150 | Grad Norm: 0.00799359\n",
      "Epoch 3 | Step 1657200 | Avg Loss: 0.0147 | Grad Norm: 0.00823276\n",
      "Epoch 3 | Step 1657300 | Avg Loss: 0.0148 | Grad Norm: 0.00829045\n",
      "Epoch 3 | Step 1657400 | Avg Loss: 0.0147 | Grad Norm: 0.00746981\n",
      "Epoch 3 | Step 1657500 | Avg Loss: 0.0150 | Grad Norm: 0.00850862\n",
      "Epoch 3 | Step 1657600 | Avg Loss: 0.0150 | Grad Norm: 0.00712033\n",
      "Epoch 3 | Step 1657700 | Avg Loss: 0.0154 | Grad Norm: 0.00843862\n",
      "Epoch 3 | Step 1657800 | Avg Loss: 0.0150 | Grad Norm: 0.00786026\n",
      "Epoch 3 | Step 1657900 | Avg Loss: 0.0150 | Grad Norm: 0.00930051\n",
      "Epoch 3 | Step 1658000 | Avg Loss: 0.0151 | Grad Norm: 0.00651991\n",
      "Epoch 3 | Step 1658100 | Avg Loss: 0.0146 | Grad Norm: 0.00707428\n",
      "Epoch 3 | Step 1658200 | Avg Loss: 0.0147 | Grad Norm: 0.00765127\n",
      "Epoch 3 | Step 1658300 | Avg Loss: 0.0147 | Grad Norm: 0.00812108\n",
      "Epoch 3 | Step 1658400 | Avg Loss: 0.0145 | Grad Norm: 0.00903095\n",
      "Epoch 3 | Step 1658500 | Avg Loss: 0.0143 | Grad Norm: 0.00814734\n",
      "Epoch 3 | Step 1658600 | Avg Loss: 0.0143 | Grad Norm: 0.00874102\n",
      "Epoch 3 | Step 1658700 | Avg Loss: 0.0140 | Grad Norm: 0.00760245\n",
      "Epoch 3 | Step 1658800 | Avg Loss: 0.0144 | Grad Norm: 0.00835278\n",
      "Epoch 3 | Step 1658900 | Avg Loss: 0.0145 | Grad Norm: 0.00869183\n",
      "Epoch 3 | Step 1659000 | Avg Loss: 0.0148 | Grad Norm: 0.00869600\n",
      "Epoch 3 | Step 1659100 | Avg Loss: 0.0149 | Grad Norm: 0.00735356\n",
      "Epoch 3 | Step 1659200 | Avg Loss: 0.0150 | Grad Norm: 0.00815757\n",
      "Epoch 3 | Step 1659300 | Avg Loss: 0.0153 | Grad Norm: 0.00848900\n",
      "Epoch 3 | Step 1659400 | Avg Loss: 0.0153 | Grad Norm: 0.00812714\n",
      "Epoch 3 | Step 1659500 | Avg Loss: 0.0150 | Grad Norm: 0.01133033\n",
      "Epoch 3 | Step 1659600 | Avg Loss: 0.0151 | Grad Norm: 0.00751483\n",
      "Epoch 3 | Step 1659700 | Avg Loss: 0.0153 | Grad Norm: 0.00699441\n",
      "Epoch 3 | Step 1659800 | Avg Loss: 0.0155 | Grad Norm: 0.00891867\n",
      "Epoch 3 | Step 1659900 | Avg Loss: 0.0154 | Grad Norm: 0.00809731\n",
      "Epoch 3 | Step 1660000 | Avg Loss: 0.0155 | Grad Norm: 0.00818356\n",
      "Epoch 3 | Step 1660100 | Avg Loss: 0.0152 | Grad Norm: 0.00809214\n",
      "Epoch 3 | Step 1660200 | Avg Loss: 0.0153 | Grad Norm: 0.00937257\n",
      "Epoch 3 | Step 1660300 | Avg Loss: 0.0155 | Grad Norm: 0.00869318\n",
      "Epoch 3 | Step 1660400 | Avg Loss: 0.0163 | Grad Norm: 0.00779676\n",
      "Epoch 3 | Step 1660500 | Avg Loss: 0.0156 | Grad Norm: 0.00777531\n",
      "Epoch 3 | Step 1660600 | Avg Loss: 0.0155 | Grad Norm: 0.00828251\n",
      "Epoch 3 | Step 1660700 | Avg Loss: 0.0155 | Grad Norm: 0.00907906\n",
      "Epoch 3 | Step 1660800 | Avg Loss: 0.0153 | Grad Norm: 0.00980053\n",
      "Epoch 3 | Step 1660900 | Avg Loss: 0.0152 | Grad Norm: 0.00695967\n",
      "Epoch 3 | Step 1661000 | Avg Loss: 0.0152 | Grad Norm: 0.00836027\n",
      "Epoch 3 | Step 1661100 | Avg Loss: 0.0148 | Grad Norm: 0.00990230\n",
      "Epoch 3 | Step 1661200 | Avg Loss: 0.0154 | Grad Norm: 0.00864529\n",
      "Epoch 3 | Step 1661300 | Avg Loss: 0.0151 | Grad Norm: 0.00790097\n",
      "Epoch 3 | Step 1661400 | Avg Loss: 0.0151 | Grad Norm: 0.01016559\n",
      "Epoch 3 | Step 1661500 | Avg Loss: 0.0154 | Grad Norm: 0.00822405\n",
      "Epoch 3 | Step 1661600 | Avg Loss: 0.0153 | Grad Norm: 0.00749194\n",
      "Epoch 3 | Step 1661700 | Avg Loss: 0.0151 | Grad Norm: 0.00693076\n",
      "Epoch 3 | Step 1661800 | Avg Loss: 0.0147 | Grad Norm: 0.00729751\n",
      "Epoch 3 | Step 1661900 | Avg Loss: 0.0145 | Grad Norm: 0.00761780\n",
      "Epoch 3 | Step 1662000 | Avg Loss: 0.0142 | Grad Norm: 0.00848017\n",
      "Epoch 3 | Step 1662100 | Avg Loss: 0.0146 | Grad Norm: 0.00786041\n",
      "Epoch 3 | Step 1662200 | Avg Loss: 0.0145 | Grad Norm: 0.00826991\n",
      "Epoch 3 | Step 1662300 | Avg Loss: 0.0143 | Grad Norm: 0.00852127\n",
      "Epoch 3 | Step 1662400 | Avg Loss: 0.0147 | Grad Norm: 0.00781969\n",
      "Epoch 3 | Step 1662500 | Avg Loss: 0.0148 | Grad Norm: 0.00988000\n",
      "Epoch 3 | Step 1662600 | Avg Loss: 0.0147 | Grad Norm: 0.00705162\n",
      "Epoch 3 | Step 1662700 | Avg Loss: 0.0145 | Grad Norm: 0.00737424\n",
      "Epoch 3 | Step 1662800 | Avg Loss: 0.0150 | Grad Norm: 0.00701839\n",
      "Epoch 3 | Step 1662900 | Avg Loss: 0.0149 | Grad Norm: 0.00758692\n",
      "Epoch 3 | Step 1663000 | Avg Loss: 0.0150 | Grad Norm: 0.00927492\n",
      "Epoch 3 | Step 1663100 | Avg Loss: 0.0153 | Grad Norm: 0.01034858\n",
      "Epoch 3 | Step 1663200 | Avg Loss: 0.0156 | Grad Norm: 0.00864796\n",
      "Epoch 3 | Step 1663300 | Avg Loss: 0.0158 | Grad Norm: 0.00860328\n",
      "Epoch 3 | Step 1663400 | Avg Loss: 0.0157 | Grad Norm: 0.00870416\n",
      "Epoch 3 | Step 1663500 | Avg Loss: 0.0154 | Grad Norm: 0.00788860\n",
      "Epoch 3 | Step 1663600 | Avg Loss: 0.0154 | Grad Norm: 0.00830337\n",
      "Epoch 3 | Step 1663700 | Avg Loss: 0.0150 | Grad Norm: 0.01003963\n",
      "Epoch 3 | Step 1663800 | Avg Loss: 0.0151 | Grad Norm: 0.00979429\n",
      "Epoch 3 | Step 1663900 | Avg Loss: 0.0151 | Grad Norm: 0.00742149\n",
      "Epoch 3 | Step 1664000 | Avg Loss: 0.0147 | Grad Norm: 0.00671437\n",
      "Epoch 3 | Step 1664100 | Avg Loss: 0.0144 | Grad Norm: 0.00698725\n",
      "Epoch 3 | Step 1664200 | Avg Loss: 0.0146 | Grad Norm: 0.00836870\n",
      "Epoch 3 | Step 1664300 | Avg Loss: 0.0142 | Grad Norm: 0.01119782\n",
      "Epoch 3 | Step 1664400 | Avg Loss: 0.0144 | Grad Norm: 0.00756704\n",
      "Epoch 3 | Step 1664500 | Avg Loss: 0.0148 | Grad Norm: 0.00883186\n",
      "Epoch 3 | Step 1664600 | Avg Loss: 0.0145 | Grad Norm: 0.00756250\n",
      "Epoch 3 | Step 1664700 | Avg Loss: 0.0145 | Grad Norm: 0.00747411\n",
      "Epoch 3 | Step 1664800 | Avg Loss: 0.0143 | Grad Norm: 0.00820317\n",
      "Epoch 3 | Step 1664900 | Avg Loss: 0.0147 | Grad Norm: 0.00812316\n",
      "Epoch 3 | Step 1665000 | Avg Loss: 0.0149 | Grad Norm: 0.00957808\n",
      "Epoch 3 | Step 1665100 | Avg Loss: 0.0154 | Grad Norm: 0.00896359\n",
      "Epoch 3 | Step 1665200 | Avg Loss: 0.0152 | Grad Norm: 0.00877570\n",
      "Epoch 3 | Step 1665300 | Avg Loss: 0.0150 | Grad Norm: 0.00730064\n",
      "Epoch 3 | Step 1665400 | Avg Loss: 0.0151 | Grad Norm: 0.00819289\n",
      "Epoch 3 | Step 1665500 | Avg Loss: 0.0149 | Grad Norm: 0.00992900\n",
      "Epoch 3 | Step 1665600 | Avg Loss: 0.0151 | Grad Norm: 0.00740784\n",
      "Epoch 3 | Step 1665700 | Avg Loss: 0.0147 | Grad Norm: 0.00829038\n",
      "Epoch 3 | Step 1665800 | Avg Loss: 0.0144 | Grad Norm: 0.00800280\n",
      "Epoch 3 | Step 1665900 | Avg Loss: 0.0148 | Grad Norm: 0.00820112\n",
      "Epoch 3 | Step 1666000 | Avg Loss: 0.0153 | Grad Norm: 0.00873128\n",
      "Epoch 3 | Step 1666100 | Avg Loss: 0.0150 | Grad Norm: 0.00741946\n",
      "Epoch 3 | Step 1666200 | Avg Loss: 0.0150 | Grad Norm: 0.00857499\n",
      "Epoch 3 | Step 1666300 | Avg Loss: 0.0150 | Grad Norm: 0.00796210\n",
      "Epoch 3 | Step 1666400 | Avg Loss: 0.0149 | Grad Norm: 0.00918663\n",
      "Epoch 3 | Step 1666500 | Avg Loss: 0.0150 | Grad Norm: 0.00857396\n",
      "Epoch 3 | Step 1666600 | Avg Loss: 0.0149 | Grad Norm: 0.00931664\n",
      "Epoch 3 | Step 1666700 | Avg Loss: 0.0147 | Grad Norm: 0.00776895\n",
      "Epoch 3 | Step 1666800 | Avg Loss: 0.0147 | Grad Norm: 0.00767975\n",
      "Epoch 3 | Step 1666900 | Avg Loss: 0.0150 | Grad Norm: 0.00836300\n",
      "Epoch 3 | Step 1667000 | Avg Loss: 0.0155 | Grad Norm: 0.00818604\n",
      "Epoch 3 | Step 1667100 | Avg Loss: 0.0153 | Grad Norm: 0.00891286\n",
      "Epoch 3 | Step 1667200 | Avg Loss: 0.0149 | Grad Norm: 0.00810618\n",
      "Epoch 3 | Step 1667300 | Avg Loss: 0.0148 | Grad Norm: 0.00852203\n",
      "Epoch 3 | Step 1667400 | Avg Loss: 0.0149 | Grad Norm: 0.01182945\n",
      "Epoch 3 | Step 1667500 | Avg Loss: 0.0150 | Grad Norm: 0.00839728\n",
      "Epoch 3 | Step 1667600 | Avg Loss: 0.0150 | Grad Norm: 0.00801645\n",
      "Epoch 3 | Step 1667700 | Avg Loss: 0.0152 | Grad Norm: 0.00886821\n",
      "Epoch 3 | Step 1667800 | Avg Loss: 0.0152 | Grad Norm: 0.00815625\n",
      "Epoch 3 | Step 1667900 | Avg Loss: 0.0154 | Grad Norm: 0.00823267\n",
      "Epoch 3 | Step 1668000 | Avg Loss: 0.0145 | Grad Norm: 0.00760944\n",
      "Epoch 3 | Step 1668100 | Avg Loss: 0.0145 | Grad Norm: 0.00876052\n",
      "Epoch 3 | Step 1668200 | Avg Loss: 0.0148 | Grad Norm: 0.00734260\n",
      "Epoch 3 | Step 1668300 | Avg Loss: 0.0151 | Grad Norm: 0.00882728\n",
      "Epoch 3 | Step 1668400 | Avg Loss: 0.0150 | Grad Norm: 0.00769147\n",
      "Epoch 3 | Step 1668500 | Avg Loss: 0.0153 | Grad Norm: 0.00926780\n",
      "Epoch 3 | Step 1668600 | Avg Loss: 0.0154 | Grad Norm: 0.00723790\n",
      "Epoch 3 | Step 1668700 | Avg Loss: 0.0151 | Grad Norm: 0.00786008\n",
      "Epoch 3 | Step 1668800 | Avg Loss: 0.0152 | Grad Norm: 0.00674776\n",
      "Epoch 3 | Step 1668900 | Avg Loss: 0.0148 | Grad Norm: 0.00767832\n",
      "Epoch 3 | Step 1669000 | Avg Loss: 0.0146 | Grad Norm: 0.00839318\n",
      "Epoch 3 | Step 1669100 | Avg Loss: 0.0143 | Grad Norm: 0.00808948\n",
      "Epoch 3 | Step 1669200 | Avg Loss: 0.0145 | Grad Norm: 0.00741672\n",
      "Epoch 3 | Step 1669300 | Avg Loss: 0.0144 | Grad Norm: 0.00836306\n",
      "Epoch 3 | Step 1669400 | Avg Loss: 0.0146 | Grad Norm: 0.00841407\n",
      "Epoch 3 | Step 1669500 | Avg Loss: 0.0146 | Grad Norm: 0.00828871\n",
      "Epoch 3 | Step 1669600 | Avg Loss: 0.0149 | Grad Norm: 0.00765883\n",
      "Epoch 3 | Step 1669700 | Avg Loss: 0.0155 | Grad Norm: 0.00702993\n",
      "Epoch 3 | Step 1669800 | Avg Loss: 0.0161 | Grad Norm: 0.00847645\n",
      "Epoch 3 | Step 1669900 | Avg Loss: 0.0157 | Grad Norm: 0.01176793\n",
      "Epoch 3 | Step 1670000 | Avg Loss: 0.0159 | Grad Norm: 0.00794681\n",
      "Epoch 3 | Step 1670100 | Avg Loss: 0.0156 | Grad Norm: 0.00925433\n",
      "Epoch 3 | Step 1670200 | Avg Loss: 0.0155 | Grad Norm: 0.00854774\n",
      "Epoch 3 | Step 1670300 | Avg Loss: 0.0157 | Grad Norm: 0.00775012\n",
      "Epoch 3 | Step 1670400 | Avg Loss: 0.0158 | Grad Norm: 0.00754959\n",
      "Epoch 3 | Step 1670500 | Avg Loss: 0.0155 | Grad Norm: 0.00854484\n",
      "Epoch 3 | Step 1670600 | Avg Loss: 0.0156 | Grad Norm: 0.00792704\n",
      "Epoch 3 | Step 1670700 | Avg Loss: 0.0155 | Grad Norm: 0.00816931\n",
      "Epoch 3 | Step 1670800 | Avg Loss: 0.0151 | Grad Norm: 0.01097710\n",
      "Epoch 3 | Step 1670900 | Avg Loss: 0.0156 | Grad Norm: 0.00815143\n",
      "Epoch 3 | Step 1671000 | Avg Loss: 0.0149 | Grad Norm: 0.00786423\n",
      "Epoch 3 | Step 1671100 | Avg Loss: 0.0148 | Grad Norm: 0.00791999\n",
      "Epoch 3 | Step 1671200 | Avg Loss: 0.0148 | Grad Norm: 0.00775739\n",
      "Epoch 3 | Step 1671300 | Avg Loss: 0.0150 | Grad Norm: 0.00744390\n",
      "Epoch 3 | Step 1671400 | Avg Loss: 0.0152 | Grad Norm: 0.00841165\n",
      "Epoch 3 | Step 1671500 | Avg Loss: 0.0153 | Grad Norm: 0.00753587\n",
      "Epoch 3 | Step 1671600 | Avg Loss: 0.0149 | Grad Norm: 0.01232342\n",
      "Epoch 3 | Step 1671700 | Avg Loss: 0.0149 | Grad Norm: 0.00930245\n",
      "Epoch 3 | Step 1671800 | Avg Loss: 0.0151 | Grad Norm: 0.00937806\n",
      "Epoch 3 | Step 1671900 | Avg Loss: 0.0150 | Grad Norm: 0.00920300\n",
      "Epoch 3 | Step 1672000 | Avg Loss: 0.0151 | Grad Norm: 0.00828467\n",
      "Epoch 3 | Step 1672100 | Avg Loss: 0.0152 | Grad Norm: 0.00711237\n",
      "Epoch 3 | Step 1672200 | Avg Loss: 0.0149 | Grad Norm: 0.00751191\n",
      "Epoch 3 | Step 1672300 | Avg Loss: 0.0151 | Grad Norm: 0.00790016\n",
      "Epoch 3 | Step 1672400 | Avg Loss: 0.0148 | Grad Norm: 0.00683499\n",
      "Epoch 3 | Step 1672500 | Avg Loss: 0.0152 | Grad Norm: 0.01033530\n",
      "Epoch 3 | Step 1672600 | Avg Loss: 0.0148 | Grad Norm: 0.00969023\n",
      "Epoch 3 | Step 1672700 | Avg Loss: 0.0145 | Grad Norm: 0.00844326\n",
      "Epoch 3 | Step 1672800 | Avg Loss: 0.0145 | Grad Norm: 0.00859336\n",
      "Epoch 3 | Step 1672900 | Avg Loss: 0.0150 | Grad Norm: 0.00782064\n",
      "Epoch 3 | Step 1673000 | Avg Loss: 0.0145 | Grad Norm: 0.00729565\n",
      "Epoch 3 | Step 1673100 | Avg Loss: 0.0148 | Grad Norm: 0.00883979\n",
      "Epoch 3 | Step 1673200 | Avg Loss: 0.0153 | Grad Norm: 0.00836177\n",
      "Epoch 3 | Step 1673300 | Avg Loss: 0.0151 | Grad Norm: 0.01125669\n",
      "Epoch 3 | Step 1673400 | Avg Loss: 0.0154 | Grad Norm: 0.01003898\n",
      "Epoch 3 | Step 1673500 | Avg Loss: 0.0152 | Grad Norm: 0.00647689\n",
      "Epoch 3 | Step 1673600 | Avg Loss: 0.0152 | Grad Norm: 0.00766620\n",
      "Epoch 3 | Step 1673700 | Avg Loss: 0.0151 | Grad Norm: 0.00852602\n",
      "Epoch 3 | Step 1673800 | Avg Loss: 0.0150 | Grad Norm: 0.00764529\n",
      "Epoch 3 | Step 1673900 | Avg Loss: 0.0153 | Grad Norm: 0.00754106\n",
      "Epoch 3 | Step 1674000 | Avg Loss: 0.0149 | Grad Norm: 0.00788399\n",
      "Epoch 3 | Step 1674100 | Avg Loss: 0.0150 | Grad Norm: 0.00742829\n",
      "Epoch 3 | Step 1674200 | Avg Loss: 0.0151 | Grad Norm: 0.00800774\n",
      "Epoch 3 | Step 1674300 | Avg Loss: 0.0148 | Grad Norm: 0.00890747\n",
      "Epoch 3 | Step 1674400 | Avg Loss: 0.0150 | Grad Norm: 0.00789611\n",
      "Epoch 3 | Step 1674500 | Avg Loss: 0.0149 | Grad Norm: 0.00703156\n",
      "Epoch 3 | Step 1674600 | Avg Loss: 0.0149 | Grad Norm: 0.00849161\n",
      "Epoch 3 | Step 1674700 | Avg Loss: 0.0151 | Grad Norm: 0.00721653\n",
      "Epoch 3 | Step 1674800 | Avg Loss: 0.0150 | Grad Norm: 0.00817128\n",
      "Epoch 3 | Step 1674900 | Avg Loss: 0.0148 | Grad Norm: 0.00642150\n",
      "Epoch 3 | Step 1675000 | Avg Loss: 0.0148 | Grad Norm: 0.00847776\n",
      "Epoch 3 | Step 1675100 | Avg Loss: 0.0150 | Grad Norm: 0.00767182\n",
      "Epoch 3 | Step 1675200 | Avg Loss: 0.0149 | Grad Norm: 0.00913455\n",
      "Epoch 3 | Step 1675300 | Avg Loss: 0.0150 | Grad Norm: 0.00774535\n",
      "Epoch 3 | Step 1675400 | Avg Loss: 0.0153 | Grad Norm: 0.00711488\n",
      "Epoch 3 | Step 1675500 | Avg Loss: 0.0154 | Grad Norm: 0.00903592\n",
      "Epoch 3 | Step 1675600 | Avg Loss: 0.0157 | Grad Norm: 0.00887101\n",
      "Epoch 3 | Step 1675700 | Avg Loss: 0.0156 | Grad Norm: 0.00849399\n",
      "Epoch 3 | Step 1675800 | Avg Loss: 0.0156 | Grad Norm: 0.00872852\n",
      "Epoch 3 | Step 1675900 | Avg Loss: 0.0151 | Grad Norm: 0.00833024\n",
      "Epoch 3 | Step 1676000 | Avg Loss: 0.0150 | Grad Norm: 0.00723504\n",
      "Epoch 3 | Step 1676100 | Avg Loss: 0.0147 | Grad Norm: 0.00770446\n",
      "Epoch 3 | Step 1676200 | Avg Loss: 0.0148 | Grad Norm: 0.00761409\n",
      "Epoch 3 | Step 1676300 | Avg Loss: 0.0147 | Grad Norm: 0.00792749\n",
      "Epoch 3 | Step 1676400 | Avg Loss: 0.0153 | Grad Norm: 0.01086753\n",
      "Epoch 3 | Step 1676500 | Avg Loss: 0.0151 | Grad Norm: 0.00931803\n",
      "Epoch 3 | Step 1676600 | Avg Loss: 0.0147 | Grad Norm: 0.00873312\n",
      "Epoch 3 | Step 1676700 | Avg Loss: 0.0153 | Grad Norm: 0.00929127\n",
      "Epoch 3 | Step 1676800 | Avg Loss: 0.0152 | Grad Norm: 0.00947373\n",
      "Epoch 3 | Step 1676900 | Avg Loss: 0.0151 | Grad Norm: 0.00900507\n",
      "Epoch 3 | Step 1677000 | Avg Loss: 0.0151 | Grad Norm: 0.00718430\n",
      "Epoch 3 | Step 1677100 | Avg Loss: 0.0149 | Grad Norm: 0.00847445\n",
      "Epoch 3 | Step 1677200 | Avg Loss: 0.0147 | Grad Norm: 0.00861826\n",
      "Epoch 3 | Step 1677300 | Avg Loss: 0.0145 | Grad Norm: 0.00758520\n",
      "Epoch 3 | Step 1677400 | Avg Loss: 0.0148 | Grad Norm: 0.00820071\n",
      "Epoch 3 | Step 1677500 | Avg Loss: 0.0153 | Grad Norm: 0.00773185\n",
      "Epoch 3 | Step 1677600 | Avg Loss: 0.0154 | Grad Norm: 0.00830195\n",
      "Epoch 3 | Step 1677700 | Avg Loss: 0.0151 | Grad Norm: 0.00735439\n",
      "Epoch 3 | Step 1677800 | Avg Loss: 0.0150 | Grad Norm: 0.00810489\n",
      "Epoch 3 | Step 1677900 | Avg Loss: 0.0147 | Grad Norm: 0.00812585\n",
      "Epoch 3 | Step 1678000 | Avg Loss: 0.0145 | Grad Norm: 0.00698371\n",
      "Epoch 3 | Step 1678100 | Avg Loss: 0.0149 | Grad Norm: 0.00805064\n",
      "Epoch 3 | Step 1678200 | Avg Loss: 0.0147 | Grad Norm: 0.00882962\n",
      "Epoch 3 | Step 1678300 | Avg Loss: 0.0148 | Grad Norm: 0.00719851\n",
      "Epoch 3 | Step 1678400 | Avg Loss: 0.0148 | Grad Norm: 0.00849388\n",
      "Epoch 3 | Step 1678500 | Avg Loss: 0.0147 | Grad Norm: 0.00829093\n",
      "Epoch 3 | Step 1678600 | Avg Loss: 0.0149 | Grad Norm: 0.00791217\n",
      "Epoch 3 | Step 1678700 | Avg Loss: 0.0151 | Grad Norm: 0.00748071\n",
      "Epoch 3 | Step 1678800 | Avg Loss: 0.0154 | Grad Norm: 0.00823926\n",
      "Epoch 3 | Step 1678900 | Avg Loss: 0.0156 | Grad Norm: 0.00895607\n",
      "Epoch 3 | Step 1679000 | Avg Loss: 0.0151 | Grad Norm: 0.00827963\n",
      "Epoch 3 | Step 1679100 | Avg Loss: 0.0152 | Grad Norm: 0.00858530\n",
      "Epoch 3 | Step 1679200 | Avg Loss: 0.0153 | Grad Norm: 0.00792994\n",
      "Epoch 3 | Step 1679300 | Avg Loss: 0.0152 | Grad Norm: 0.00750476\n",
      "Epoch 3 | Step 1679400 | Avg Loss: 0.0152 | Grad Norm: 0.00861261\n",
      "Epoch 3 | Step 1679500 | Avg Loss: 0.0151 | Grad Norm: 0.01118643\n",
      "Epoch 3 | Step 1679600 | Avg Loss: 0.0149 | Grad Norm: 0.00774005\n",
      "Epoch 3 | Step 1679700 | Avg Loss: 0.0148 | Grad Norm: 0.00857968\n",
      "Epoch 3 | Step 1679800 | Avg Loss: 0.0152 | Grad Norm: 0.00748591\n",
      "Epoch 3 | Step 1679900 | Avg Loss: 0.0149 | Grad Norm: 0.00969249\n",
      "Epoch 3 | Step 1680000 | Avg Loss: 0.0150 | Grad Norm: 0.00811089\n",
      "Epoch 3 | Step 1680100 | Avg Loss: 0.0154 | Grad Norm: 0.00954821\n",
      "Epoch 3 | Step 1680200 | Avg Loss: 0.0156 | Grad Norm: 0.00744911\n",
      "Epoch 3 | Step 1680300 | Avg Loss: 0.0155 | Grad Norm: 0.00783801\n",
      "Epoch 3 | Step 1680400 | Avg Loss: 0.0152 | Grad Norm: 0.00669229\n",
      "Epoch 3 | Step 1680500 | Avg Loss: 0.0152 | Grad Norm: 0.00773795\n",
      "Epoch 3 | Step 1680600 | Avg Loss: 0.0157 | Grad Norm: 0.00895016\n",
      "Epoch 3 | Step 1680700 | Avg Loss: 0.0155 | Grad Norm: 0.00756916\n",
      "Epoch 3 | Step 1680800 | Avg Loss: 0.0155 | Grad Norm: 0.00735552\n",
      "Epoch 3 | Step 1680900 | Avg Loss: 0.0154 | Grad Norm: 0.00794003\n",
      "Epoch 3 | Step 1681000 | Avg Loss: 0.0152 | Grad Norm: 0.00888285\n",
      "Epoch 3 | Step 1681100 | Avg Loss: 0.0153 | Grad Norm: 0.00837810\n",
      "Epoch 3 | Step 1681200 | Avg Loss: 0.0151 | Grad Norm: 0.00796822\n",
      "Epoch 3 | Step 1681300 | Avg Loss: 0.0149 | Grad Norm: 0.00767927\n",
      "Epoch 3 | Step 1681400 | Avg Loss: 0.0147 | Grad Norm: 0.00810155\n",
      "Epoch 3 | Step 1681500 | Avg Loss: 0.0146 | Grad Norm: 0.00850287\n",
      "Epoch 3 | Step 1681600 | Avg Loss: 0.0145 | Grad Norm: 0.00672523\n",
      "Epoch 3 | Step 1681700 | Avg Loss: 0.0146 | Grad Norm: 0.00752202\n",
      "Epoch 3 | Step 1681800 | Avg Loss: 0.0149 | Grad Norm: 0.00746160\n",
      "Epoch 3 | Step 1681900 | Avg Loss: 0.0146 | Grad Norm: 0.00806569\n",
      "Epoch 3 | Step 1682000 | Avg Loss: 0.0144 | Grad Norm: 0.00757302\n",
      "Epoch 3 | Step 1682100 | Avg Loss: 0.0143 | Grad Norm: 0.00706155\n",
      "Epoch 3 | Step 1682200 | Avg Loss: 0.0145 | Grad Norm: 0.00828507\n",
      "Epoch 3 | Step 1682300 | Avg Loss: 0.0147 | Grad Norm: 0.00806209\n",
      "Epoch 3 | Step 1682400 | Avg Loss: 0.0150 | Grad Norm: 0.00700217\n",
      "Epoch 3 | Step 1682500 | Avg Loss: 0.0156 | Grad Norm: 0.00778446\n",
      "Epoch 3 | Step 1682600 | Avg Loss: 0.0149 | Grad Norm: 0.00863981\n",
      "Epoch 3 | Step 1682700 | Avg Loss: 0.0150 | Grad Norm: 0.00855056\n",
      "Epoch 3 | Step 1682800 | Avg Loss: 0.0150 | Grad Norm: 0.00715939\n",
      "Epoch 3 | Step 1682900 | Avg Loss: 0.0148 | Grad Norm: 0.00831607\n",
      "Epoch 3 | Step 1683000 | Avg Loss: 0.0149 | Grad Norm: 0.00763649\n",
      "Epoch 3 | Step 1683100 | Avg Loss: 0.0148 | Grad Norm: 0.00704839\n",
      "Epoch 3 | Step 1683200 | Avg Loss: 0.0150 | Grad Norm: 0.00796975\n",
      "Epoch 3 | Step 1683300 | Avg Loss: 0.0145 | Grad Norm: 0.00816784\n",
      "Epoch 3 | Step 1683400 | Avg Loss: 0.0147 | Grad Norm: 0.00853061\n",
      "Epoch 3 | Step 1683500 | Avg Loss: 0.0145 | Grad Norm: 0.00836660\n",
      "Epoch 3 | Step 1683600 | Avg Loss: 0.0143 | Grad Norm: 0.00773482\n",
      "Epoch 3 | Step 1683700 | Avg Loss: 0.0144 | Grad Norm: 0.00813216\n",
      "Epoch 3 | Step 1683800 | Avg Loss: 0.0148 | Grad Norm: 0.01009941\n",
      "Epoch 3 | Step 1683900 | Avg Loss: 0.0147 | Grad Norm: 0.00832579\n",
      "Epoch 3 | Step 1684000 | Avg Loss: 0.0148 | Grad Norm: 0.00922780\n",
      "Epoch 3 | Step 1684100 | Avg Loss: 0.0150 | Grad Norm: 0.00821050\n",
      "Epoch 3 | Step 1684200 | Avg Loss: 0.0150 | Grad Norm: 0.00820887\n",
      "Epoch 3 | Step 1684300 | Avg Loss: 0.0148 | Grad Norm: 0.00629725\n",
      "Epoch 3 | Step 1684400 | Avg Loss: 0.0147 | Grad Norm: 0.00844706\n",
      "Epoch 3 | Step 1684500 | Avg Loss: 0.0149 | Grad Norm: 0.00999850\n",
      "Epoch 3 | Step 1684600 | Avg Loss: 0.0151 | Grad Norm: 0.00830905\n",
      "Epoch 3 | Step 1684700 | Avg Loss: 0.0146 | Grad Norm: 0.00786748\n",
      "Epoch 3 | Step 1684800 | Avg Loss: 0.0146 | Grad Norm: 0.00814075\n",
      "Epoch 3 | Step 1684900 | Avg Loss: 0.0150 | Grad Norm: 0.00718383\n",
      "Epoch 3 | Step 1685000 | Avg Loss: 0.0146 | Grad Norm: 0.00782427\n",
      "Epoch 3 | Step 1685100 | Avg Loss: 0.0148 | Grad Norm: 0.00786568\n",
      "Epoch 3 | Step 1685200 | Avg Loss: 0.0146 | Grad Norm: 0.00939129\n",
      "Epoch 3 | Step 1685300 | Avg Loss: 0.0148 | Grad Norm: 0.00746200\n",
      "Epoch 3 | Step 1685400 | Avg Loss: 0.0148 | Grad Norm: 0.00772252\n",
      "Epoch 3 | Step 1685500 | Avg Loss: 0.0146 | Grad Norm: 0.00713303\n",
      "Epoch 3 | Step 1685600 | Avg Loss: 0.0147 | Grad Norm: 0.00804680\n",
      "Epoch 3 | Step 1685700 | Avg Loss: 0.0148 | Grad Norm: 0.00885139\n",
      "Epoch 3 | Step 1685800 | Avg Loss: 0.0146 | Grad Norm: 0.00676658\n",
      "Epoch 3 | Step 1685900 | Avg Loss: 0.0146 | Grad Norm: 0.00854284\n",
      "Epoch 3 | Step 1686000 | Avg Loss: 0.0146 | Grad Norm: 0.00927335\n",
      "Epoch 3 | Step 1686100 | Avg Loss: 0.0144 | Grad Norm: 0.00822688\n",
      "Epoch 3 | Step 1686200 | Avg Loss: 0.0147 | Grad Norm: 0.00841879\n",
      "Epoch 3 | Step 1686300 | Avg Loss: 0.0144 | Grad Norm: 0.00767843\n",
      "Epoch 3 | Step 1686400 | Avg Loss: 0.0148 | Grad Norm: 0.00862331\n",
      "Epoch 3 | Step 1686500 | Avg Loss: 0.0151 | Grad Norm: 0.00985606\n",
      "Epoch 3 | Step 1686600 | Avg Loss: 0.0151 | Grad Norm: 0.00688466\n",
      "Epoch 3 | Step 1686700 | Avg Loss: 0.0151 | Grad Norm: 0.00696730\n",
      "Epoch 3 | Step 1686800 | Avg Loss: 0.0149 | Grad Norm: 0.00719968\n",
      "Epoch 3 | Step 1686900 | Avg Loss: 0.0148 | Grad Norm: 0.00909839\n",
      "Epoch 3 | Step 1687000 | Avg Loss: 0.0147 | Grad Norm: 0.00726673\n",
      "Epoch 3 | Step 1687100 | Avg Loss: 0.0145 | Grad Norm: 0.00785890\n",
      "Epoch 3 | Step 1687200 | Avg Loss: 0.0146 | Grad Norm: 0.00704011\n",
      "Epoch 3 | Step 1687300 | Avg Loss: 0.0146 | Grad Norm: 0.00815707\n",
      "Epoch 3 | Step 1687400 | Avg Loss: 0.0150 | Grad Norm: 0.01105461\n",
      "Epoch 3 | Step 1687500 | Avg Loss: 0.0153 | Grad Norm: 0.01082049\n",
      "Epoch 3 | Step 1687600 | Avg Loss: 0.0152 | Grad Norm: 0.00711334\n",
      "Epoch 3 | Step 1687700 | Avg Loss: 0.0153 | Grad Norm: 0.00838235\n",
      "Epoch 3 | Step 1687800 | Avg Loss: 0.0148 | Grad Norm: 0.00666415\n",
      "Epoch 3 | Step 1687900 | Avg Loss: 0.0151 | Grad Norm: 0.00714944\n",
      "Epoch 3 | Step 1688000 | Avg Loss: 0.0152 | Grad Norm: 0.00736282\n",
      "Epoch 3 | Step 1688100 | Avg Loss: 0.0152 | Grad Norm: 0.00867922\n",
      "Epoch 3 | Step 1688200 | Avg Loss: 0.0147 | Grad Norm: 0.00813357\n",
      "Epoch 3 | Step 1688300 | Avg Loss: 0.0149 | Grad Norm: 0.00937406\n",
      "Epoch 3 | Step 1688400 | Avg Loss: 0.0149 | Grad Norm: 0.00758557\n",
      "Epoch 3 | Step 1688500 | Avg Loss: 0.0151 | Grad Norm: 0.00850789\n",
      "Epoch 3 | Step 1688600 | Avg Loss: 0.0154 | Grad Norm: 0.00816260\n",
      "Epoch 3 | Step 1688700 | Avg Loss: 0.0154 | Grad Norm: 0.00941347\n",
      "Epoch 3 | Step 1688800 | Avg Loss: 0.0152 | Grad Norm: 0.00731656\n",
      "Epoch 3 | Step 1688900 | Avg Loss: 0.0153 | Grad Norm: 0.00843200\n",
      "Epoch 3 | Step 1689000 | Avg Loss: 0.0152 | Grad Norm: 0.00654546\n",
      "Epoch 3 | Step 1689100 | Avg Loss: 0.0149 | Grad Norm: 0.00815299\n",
      "Epoch 3 | Step 1689200 | Avg Loss: 0.0150 | Grad Norm: 0.00971335\n",
      "Epoch 3 | Step 1689300 | Avg Loss: 0.0148 | Grad Norm: 0.00768475\n",
      "Epoch 3 | Step 1689400 | Avg Loss: 0.0145 | Grad Norm: 0.00779203\n",
      "Epoch 3 | Step 1689500 | Avg Loss: 0.0148 | Grad Norm: 0.00696426\n",
      "Epoch 3 | Step 1689600 | Avg Loss: 0.0152 | Grad Norm: 0.01142192\n",
      "Epoch 3 | Step 1689700 | Avg Loss: 0.0152 | Grad Norm: 0.00971154\n",
      "Epoch 3 | Step 1689800 | Avg Loss: 0.0149 | Grad Norm: 0.00816604\n",
      "Epoch 3 | Step 1689900 | Avg Loss: 0.0147 | Grad Norm: 0.00765084\n",
      "Epoch 3 | Step 1690000 | Avg Loss: 0.0145 | Grad Norm: 0.00729290\n",
      "Epoch 3 | Step 1690100 | Avg Loss: 0.0147 | Grad Norm: 0.00690271\n",
      "Epoch 3 | Step 1690200 | Avg Loss: 0.0148 | Grad Norm: 0.01066108\n",
      "Epoch 3 | Step 1690300 | Avg Loss: 0.0152 | Grad Norm: 0.00792336\n",
      "Epoch 3 | Step 1690400 | Avg Loss: 0.0154 | Grad Norm: 0.00753042\n",
      "Epoch 3 | Step 1690500 | Avg Loss: 0.0152 | Grad Norm: 0.00927562\n",
      "Epoch 3 | Step 1690600 | Avg Loss: 0.0148 | Grad Norm: 0.00766462\n",
      "Epoch 3 | Step 1690700 | Avg Loss: 0.0148 | Grad Norm: 0.00849957\n",
      "Epoch 3 | Step 1690800 | Avg Loss: 0.0148 | Grad Norm: 0.00688789\n",
      "Epoch 3 | Step 1690900 | Avg Loss: 0.0147 | Grad Norm: 0.00766433\n",
      "Epoch 3 | Step 1691000 | Avg Loss: 0.0148 | Grad Norm: 0.00921705\n",
      "Epoch 3 | Step 1691100 | Avg Loss: 0.0147 | Grad Norm: 0.01047044\n",
      "Epoch 3 | Step 1691200 | Avg Loss: 0.0147 | Grad Norm: 0.00906298\n",
      "Epoch 3 | Step 1691300 | Avg Loss: 0.0152 | Grad Norm: 0.00884721\n",
      "Epoch 3 | Step 1691400 | Avg Loss: 0.0152 | Grad Norm: 0.00644082\n",
      "Epoch 3 | Step 1691500 | Avg Loss: 0.0152 | Grad Norm: 0.00914223\n",
      "Epoch 3 | Step 1691600 | Avg Loss: 0.0153 | Grad Norm: 0.00942161\n",
      "Epoch 3 | Step 1691700 | Avg Loss: 0.0154 | Grad Norm: 0.00780305\n",
      "Epoch 3 | Step 1691800 | Avg Loss: 0.0153 | Grad Norm: 0.00799097\n",
      "Epoch 3 | Step 1691900 | Avg Loss: 0.0150 | Grad Norm: 0.00823325\n",
      "Epoch 3 | Step 1692000 | Avg Loss: 0.0151 | Grad Norm: 0.00836775\n",
      "Epoch 3 | Step 1692100 | Avg Loss: 0.0154 | Grad Norm: 0.00768292\n",
      "Epoch 3 | Step 1692200 | Avg Loss: 0.0152 | Grad Norm: 0.01051105\n",
      "Epoch 3 | Step 1692300 | Avg Loss: 0.0155 | Grad Norm: 0.00829812\n",
      "Epoch 3 | Step 1692400 | Avg Loss: 0.0155 | Grad Norm: 0.00873984\n",
      "Epoch 3 | Step 1692500 | Avg Loss: 0.0152 | Grad Norm: 0.00940057\n",
      "Epoch 3 | Step 1692600 | Avg Loss: 0.0150 | Grad Norm: 0.00978039\n",
      "Epoch 3 | Step 1692700 | Avg Loss: 0.0151 | Grad Norm: 0.00804823\n",
      "Epoch 3 | Step 1692800 | Avg Loss: 0.0151 | Grad Norm: 0.00896454\n",
      "Epoch 3 | Step 1692900 | Avg Loss: 0.0149 | Grad Norm: 0.00789462\n",
      "Epoch 3 | Step 1693000 | Avg Loss: 0.0150 | Grad Norm: 0.00722484\n",
      "Epoch 3 | Step 1693100 | Avg Loss: 0.0154 | Grad Norm: 0.00794071\n",
      "Epoch 3 | Step 1693200 | Avg Loss: 0.0156 | Grad Norm: 0.00935423\n",
      "Epoch 3 | Step 1693300 | Avg Loss: 0.0153 | Grad Norm: 0.00828536\n",
      "Epoch 3 | Step 1693400 | Avg Loss: 0.0152 | Grad Norm: 0.00781778\n",
      "Epoch 3 | Step 1693500 | Avg Loss: 0.0153 | Grad Norm: 0.00789192\n",
      "Epoch 3 | Step 1693600 | Avg Loss: 0.0151 | Grad Norm: 0.00754742\n",
      "Epoch 3 | Step 1693700 | Avg Loss: 0.0152 | Grad Norm: 0.00715972\n",
      "Epoch 3 | Step 1693800 | Avg Loss: 0.0157 | Grad Norm: 0.00863976\n",
      "Epoch 3 | Step 1693900 | Avg Loss: 0.0155 | Grad Norm: 0.00866054\n",
      "Epoch 3 | Step 1694000 | Avg Loss: 0.0152 | Grad Norm: 0.00862308\n",
      "Epoch 3 | Step 1694100 | Avg Loss: 0.0153 | Grad Norm: 0.00791262\n",
      "Epoch 3 | Step 1694200 | Avg Loss: 0.0156 | Grad Norm: 0.00938056\n",
      "Epoch 3 | Step 1694300 | Avg Loss: 0.0154 | Grad Norm: 0.00883937\n",
      "Epoch 3 | Step 1694400 | Avg Loss: 0.0149 | Grad Norm: 0.00953993\n",
      "Epoch 3 | Step 1694500 | Avg Loss: 0.0147 | Grad Norm: 0.00795963\n",
      "Epoch 3 | Step 1694600 | Avg Loss: 0.0150 | Grad Norm: 0.00832179\n",
      "Epoch 3 | Step 1694700 | Avg Loss: 0.0155 | Grad Norm: 0.00699094\n",
      "Epoch 3 | Step 1694800 | Avg Loss: 0.0156 | Grad Norm: 0.00854059\n",
      "Epoch 3 | Step 1694900 | Avg Loss: 0.0158 | Grad Norm: 0.00858598\n",
      "Epoch 3 | Step 1695000 | Avg Loss: 0.0153 | Grad Norm: 0.00848168\n",
      "Epoch 3 | Step 1695100 | Avg Loss: 0.0154 | Grad Norm: 0.00739144\n",
      "Epoch 3 | Step 1695200 | Avg Loss: 0.0156 | Grad Norm: 0.00766966\n",
      "Epoch 3 | Step 1695300 | Avg Loss: 0.0155 | Grad Norm: 0.00760679\n",
      "Epoch 3 | Step 1695400 | Avg Loss: 0.0154 | Grad Norm: 0.00850087\n",
      "Epoch 3 | Step 1695500 | Avg Loss: 0.0155 | Grad Norm: 0.00884937\n",
      "Epoch 3 | Step 1695600 | Avg Loss: 0.0153 | Grad Norm: 0.00776156\n",
      "Epoch 3 | Step 1695700 | Avg Loss: 0.0153 | Grad Norm: 0.00814500\n",
      "Epoch 3 | Step 1695800 | Avg Loss: 0.0149 | Grad Norm: 0.00874742\n",
      "Epoch 3 | Step 1695900 | Avg Loss: 0.0153 | Grad Norm: 0.00802502\n",
      "Epoch 3 | Step 1696000 | Avg Loss: 0.0150 | Grad Norm: 0.00991837\n",
      "Epoch 3 | Step 1696100 | Avg Loss: 0.0153 | Grad Norm: 0.00748054\n",
      "Epoch 3 | Step 1696200 | Avg Loss: 0.0149 | Grad Norm: 0.00838756\n",
      "Epoch 3 | Step 1696300 | Avg Loss: 0.0146 | Grad Norm: 0.00819558\n",
      "Epoch 3 | Step 1696400 | Avg Loss: 0.0146 | Grad Norm: 0.01144469\n",
      "Epoch 3 | Step 1696500 | Avg Loss: 0.0145 | Grad Norm: 0.00831271\n",
      "Epoch 3 | Step 1696600 | Avg Loss: 0.0145 | Grad Norm: 0.00942412\n",
      "Epoch 3 | Step 1696700 | Avg Loss: 0.0146 | Grad Norm: 0.00818524\n",
      "Epoch 3 | Step 1696800 | Avg Loss: 0.0147 | Grad Norm: 0.00797349\n",
      "Epoch 3 | Step 1696900 | Avg Loss: 0.0153 | Grad Norm: 0.00844962\n",
      "Epoch 3 | Step 1697000 | Avg Loss: 0.0150 | Grad Norm: 0.00759780\n",
      "Epoch 3 | Step 1697100 | Avg Loss: 0.0150 | Grad Norm: 0.00897301\n",
      "Epoch 3 | Step 1697200 | Avg Loss: 0.0147 | Grad Norm: 0.00962637\n",
      "Epoch 3 | Step 1697300 | Avg Loss: 0.0148 | Grad Norm: 0.00868331\n",
      "Epoch 3 | Step 1697400 | Avg Loss: 0.0151 | Grad Norm: 0.00827144\n",
      "Epoch 3 | Step 1697500 | Avg Loss: 0.0150 | Grad Norm: 0.00693548\n",
      "Epoch 3 | Step 1697600 | Avg Loss: 0.0151 | Grad Norm: 0.00950647\n",
      "Epoch 3 | Step 1697700 | Avg Loss: 0.0155 | Grad Norm: 0.00779535\n",
      "Epoch 3 | Step 1697800 | Avg Loss: 0.0154 | Grad Norm: 0.01040596\n",
      "Epoch 3 | Step 1697900 | Avg Loss: 0.0154 | Grad Norm: 0.00729764\n",
      "Epoch 3 | Step 1698000 | Avg Loss: 0.0156 | Grad Norm: 0.00798837\n",
      "Epoch 3 | Step 1698100 | Avg Loss: 0.0154 | Grad Norm: 0.00770571\n",
      "Epoch 3 | Step 1698200 | Avg Loss: 0.0152 | Grad Norm: 0.00826831\n",
      "Epoch 3 | Step 1698300 | Avg Loss: 0.0152 | Grad Norm: 0.00798996\n",
      "Epoch 3 | Step 1698400 | Avg Loss: 0.0153 | Grad Norm: 0.00781115\n",
      "Epoch 3 | Step 1698500 | Avg Loss: 0.0150 | Grad Norm: 0.00815575\n",
      "Epoch 3 | Step 1698600 | Avg Loss: 0.0154 | Grad Norm: 0.00789198\n",
      "Epoch 3 | Step 1698700 | Avg Loss: 0.0148 | Grad Norm: 0.00831885\n",
      "Epoch 3 | Step 1698800 | Avg Loss: 0.0145 | Grad Norm: 0.00832777\n",
      "Epoch 3 | Step 1698900 | Avg Loss: 0.0146 | Grad Norm: 0.00748483\n",
      "Epoch 3 | Step 1699000 | Avg Loss: 0.0143 | Grad Norm: 0.00760069\n",
      "Epoch 3 | Step 1699100 | Avg Loss: 0.0144 | Grad Norm: 0.00957013\n",
      "Epoch 3 | Step 1699200 | Avg Loss: 0.0144 | Grad Norm: 0.00767407\n",
      "Epoch 3 | Step 1699300 | Avg Loss: 0.0147 | Grad Norm: 0.00719823\n",
      "Epoch 3 | Step 1699400 | Avg Loss: 0.0148 | Grad Norm: 0.00776671\n",
      "Epoch 3 | Step 1699500 | Avg Loss: 0.0150 | Grad Norm: 0.00895923\n",
      "Epoch 3 | Step 1699600 | Avg Loss: 0.0148 | Grad Norm: 0.00868046\n",
      "Epoch 3 | Step 1699700 | Avg Loss: 0.0147 | Grad Norm: 0.00875844\n",
      "Epoch 3 | Step 1699800 | Avg Loss: 0.0149 | Grad Norm: 0.00808681\n",
      "Epoch 3 | Step 1699900 | Avg Loss: 0.0152 | Grad Norm: 0.00847257\n",
      "Epoch 3 | Step 1700000 | Avg Loss: 0.0151 | Grad Norm: 0.00985310\n",
      "Saving model at step1700000\n",
      "Epoch 3 | Step 1700100 | Avg Loss: 0.0149 | Grad Norm: 0.00816749\n",
      "Epoch 3 | Step 1700200 | Avg Loss: 0.0150 | Grad Norm: 0.00732586\n",
      "Epoch 3 | Step 1700300 | Avg Loss: 0.0151 | Grad Norm: 0.00850687\n",
      "Epoch 3 | Step 1700400 | Avg Loss: 0.0150 | Grad Norm: 0.00798133\n",
      "Epoch 3 | Step 1700500 | Avg Loss: 0.0151 | Grad Norm: 0.00847968\n",
      "Epoch 3 | Step 1700600 | Avg Loss: 0.0151 | Grad Norm: 0.00865570\n",
      "Epoch 3 | Step 1700700 | Avg Loss: 0.0154 | Grad Norm: 0.00698176\n",
      "Epoch 3 | Step 1700800 | Avg Loss: 0.0155 | Grad Norm: 0.00883377\n",
      "Epoch 3 | Step 1700900 | Avg Loss: 0.0153 | Grad Norm: 0.00854883\n",
      "Epoch 3 | Step 1701000 | Avg Loss: 0.0147 | Grad Norm: 0.00769280\n",
      "Epoch 3 | Step 1701100 | Avg Loss: 0.0152 | Grad Norm: 0.00915056\n",
      "Epoch 3 | Step 1701200 | Avg Loss: 0.0150 | Grad Norm: 0.01048698\n",
      "Epoch 3 | Step 1701300 | Avg Loss: 0.0149 | Grad Norm: 0.00971510\n",
      "Epoch 3 | Step 1701400 | Avg Loss: 0.0150 | Grad Norm: 0.00612472\n",
      "Epoch 3 | Step 1701500 | Avg Loss: 0.0146 | Grad Norm: 0.01007783\n",
      "Epoch 3 | Step 1701600 | Avg Loss: 0.0148 | Grad Norm: 0.00893272\n",
      "Epoch 3 | Step 1701700 | Avg Loss: 0.0154 | Grad Norm: 0.00741498\n",
      "Epoch 3 | Step 1701800 | Avg Loss: 0.0154 | Grad Norm: 0.01026659\n",
      "Epoch 3 | Step 1701900 | Avg Loss: 0.0155 | Grad Norm: 0.00856896\n",
      "Epoch 3 | Step 1702000 | Avg Loss: 0.0153 | Grad Norm: 0.00841858\n",
      "Epoch 3 | Step 1702100 | Avg Loss: 0.0154 | Grad Norm: 0.00830916\n",
      "Epoch 3 | Step 1702200 | Avg Loss: 0.0156 | Grad Norm: 0.00910876\n",
      "Epoch 3 | Step 1702300 | Avg Loss: 0.0157 | Grad Norm: 0.00950523\n",
      "Epoch 3 | Step 1702400 | Avg Loss: 0.0151 | Grad Norm: 0.00759638\n",
      "Epoch 3 | Step 1702500 | Avg Loss: 0.0148 | Grad Norm: 0.00784694\n",
      "Epoch 3 | Step 1702600 | Avg Loss: 0.0147 | Grad Norm: 0.00905297\n",
      "Epoch 3 | Step 1702700 | Avg Loss: 0.0146 | Grad Norm: 0.00826840\n",
      "Epoch 3 | Step 1702800 | Avg Loss: 0.0147 | Grad Norm: 0.01027920\n",
      "Epoch 3 | Step 1702900 | Avg Loss: 0.0146 | Grad Norm: 0.00853881\n",
      "Epoch 3 | Step 1703000 | Avg Loss: 0.0150 | Grad Norm: 0.00904243\n",
      "Epoch 3 | Step 1703100 | Avg Loss: 0.0153 | Grad Norm: 0.01124444\n",
      "Epoch 3 | Step 1703200 | Avg Loss: 0.0155 | Grad Norm: 0.00811847\n",
      "Epoch 3 | Step 1703300 | Avg Loss: 0.0154 | Grad Norm: 0.00775443\n",
      "Epoch 3 | Step 1703400 | Avg Loss: 0.0154 | Grad Norm: 0.00748881\n",
      "Epoch 3 | Step 1703500 | Avg Loss: 0.0155 | Grad Norm: 0.00916263\n",
      "Epoch 3 | Step 1703600 | Avg Loss: 0.0155 | Grad Norm: 0.00858850\n",
      "Epoch 3 | Step 1703700 | Avg Loss: 0.0151 | Grad Norm: 0.00815272\n",
      "Epoch 3 | Step 1703800 | Avg Loss: 0.0153 | Grad Norm: 0.01409647\n",
      "Epoch 3 | Step 1703900 | Avg Loss: 0.0151 | Grad Norm: 0.00713419\n",
      "Epoch 3 | Step 1704000 | Avg Loss: 0.0149 | Grad Norm: 0.00709107\n",
      "Epoch 3 | Step 1704100 | Avg Loss: 0.0150 | Grad Norm: 0.00748811\n",
      "Epoch 3 | Step 1704200 | Avg Loss: 0.0148 | Grad Norm: 0.00849033\n",
      "Epoch 3 | Step 1704300 | Avg Loss: 0.0147 | Grad Norm: 0.00763570\n",
      "Epoch 3 | Step 1704400 | Avg Loss: 0.0146 | Grad Norm: 0.00925758\n",
      "Epoch 3 | Step 1704500 | Avg Loss: 0.0148 | Grad Norm: 0.00979312\n",
      "Epoch 3 | Step 1704600 | Avg Loss: 0.0149 | Grad Norm: 0.00825639\n",
      "Epoch 3 | Step 1704700 | Avg Loss: 0.0151 | Grad Norm: 0.00814571\n",
      "Epoch 3 | Step 1704800 | Avg Loss: 0.0152 | Grad Norm: 0.00860471\n",
      "Epoch 3 | Step 1704900 | Avg Loss: 0.0155 | Grad Norm: 0.00813674\n",
      "Epoch 3 | Step 1705000 | Avg Loss: 0.0151 | Grad Norm: 0.00739490\n",
      "Epoch 3 | Step 1705100 | Avg Loss: 0.0146 | Grad Norm: 0.00789067\n",
      "Epoch 3 | Step 1705200 | Avg Loss: 0.0148 | Grad Norm: 0.00939683\n",
      "Epoch 3 | Step 1705300 | Avg Loss: 0.0146 | Grad Norm: 0.00782061\n",
      "Epoch 3 | Step 1705400 | Avg Loss: 0.0143 | Grad Norm: 0.00868033\n",
      "Epoch 3 | Step 1705500 | Avg Loss: 0.0146 | Grad Norm: 0.00811426\n",
      "Epoch 3 | Step 1705600 | Avg Loss: 0.0147 | Grad Norm: 0.00823164\n",
      "Epoch 3 | Step 1705700 | Avg Loss: 0.0149 | Grad Norm: 0.00779013\n",
      "Epoch 3 | Step 1705800 | Avg Loss: 0.0148 | Grad Norm: 0.00771079\n",
      "Epoch 3 | Step 1705900 | Avg Loss: 0.0149 | Grad Norm: 0.00948833\n",
      "Epoch 3 | Step 1706000 | Avg Loss: 0.0149 | Grad Norm: 0.00757510\n",
      "Epoch 3 | Step 1706100 | Avg Loss: 0.0147 | Grad Norm: 0.00837080\n",
      "Epoch 3 | Step 1706200 | Avg Loss: 0.0145 | Grad Norm: 0.00947465\n",
      "Epoch 3 | Step 1706300 | Avg Loss: 0.0146 | Grad Norm: 0.00758057\n",
      "Epoch 3 | Step 1706400 | Avg Loss: 0.0149 | Grad Norm: 0.00805365\n",
      "Epoch 3 | Step 1706500 | Avg Loss: 0.0145 | Grad Norm: 0.00661500\n",
      "Epoch 3 | Step 1706600 | Avg Loss: 0.0147 | Grad Norm: 0.00695562\n",
      "Epoch 3 | Step 1706700 | Avg Loss: 0.0150 | Grad Norm: 0.00755215\n",
      "Epoch 3 | Step 1706800 | Avg Loss: 0.0151 | Grad Norm: 0.00860681\n",
      "Epoch 3 | Step 1706900 | Avg Loss: 0.0153 | Grad Norm: 0.00751201\n",
      "Epoch 3 | Step 1707000 | Avg Loss: 0.0153 | Grad Norm: 0.00831718\n",
      "Epoch 3 | Step 1707100 | Avg Loss: 0.0153 | Grad Norm: 0.00881174\n",
      "Epoch 3 | Step 1707200 | Avg Loss: 0.0154 | Grad Norm: 0.00841516\n",
      "Epoch 3 | Step 1707300 | Avg Loss: 0.0156 | Grad Norm: 0.00765815\n",
      "Epoch 3 | Step 1707400 | Avg Loss: 0.0155 | Grad Norm: 0.01031035\n",
      "Epoch 3 | Step 1707500 | Avg Loss: 0.0151 | Grad Norm: 0.00871201\n",
      "Epoch 3 | Step 1707600 | Avg Loss: 0.0147 | Grad Norm: 0.00774824\n",
      "Epoch 3 | Step 1707700 | Avg Loss: 0.0149 | Grad Norm: 0.00805565\n",
      "Epoch 3 | Step 1707800 | Avg Loss: 0.0152 | Grad Norm: 0.00800031\n",
      "Epoch 3 | Step 1707900 | Avg Loss: 0.0152 | Grad Norm: 0.00829362\n",
      "Epoch 3 | Step 1708000 | Avg Loss: 0.0155 | Grad Norm: 0.00747983\n",
      "Epoch 3 | Step 1708100 | Avg Loss: 0.0153 | Grad Norm: 0.00794470\n",
      "Epoch 3 | Step 1708200 | Avg Loss: 0.0152 | Grad Norm: 0.00789102\n",
      "Epoch 3 | Step 1708300 | Avg Loss: 0.0149 | Grad Norm: 0.00793733\n",
      "Epoch 3 | Step 1708400 | Avg Loss: 0.0149 | Grad Norm: 0.00907908\n",
      "Epoch 3 | Step 1708500 | Avg Loss: 0.0151 | Grad Norm: 0.00802066\n",
      "Epoch 3 | Step 1708600 | Avg Loss: 0.0148 | Grad Norm: 0.00686302\n",
      "Epoch 3 | Step 1708700 | Avg Loss: 0.0147 | Grad Norm: 0.01032212\n",
      "Epoch 3 | Step 1708800 | Avg Loss: 0.0149 | Grad Norm: 0.00780041\n",
      "Epoch 3 | Step 1708900 | Avg Loss: 0.0151 | Grad Norm: 0.00817028\n",
      "Epoch 3 | Step 1709000 | Avg Loss: 0.0147 | Grad Norm: 0.00848443\n",
      "Epoch 3 | Step 1709100 | Avg Loss: 0.0145 | Grad Norm: 0.00653878\n",
      "Epoch 3 | Step 1709200 | Avg Loss: 0.0145 | Grad Norm: 0.00695156\n",
      "Epoch 3 | Step 1709300 | Avg Loss: 0.0145 | Grad Norm: 0.00831934\n",
      "Epoch 3 | Step 1709400 | Avg Loss: 0.0150 | Grad Norm: 0.00796228\n",
      "Epoch 3 | Step 1709500 | Avg Loss: 0.0154 | Grad Norm: 0.00857731\n",
      "Epoch 3 | Step 1709600 | Avg Loss: 0.0156 | Grad Norm: 0.00874371\n",
      "Epoch 3 | Step 1709700 | Avg Loss: 0.0158 | Grad Norm: 0.00810524\n",
      "Epoch 3 | Step 1709800 | Avg Loss: 0.0153 | Grad Norm: 0.00896988\n",
      "Epoch 3 | Step 1709900 | Avg Loss: 0.0152 | Grad Norm: 0.00740344\n",
      "Epoch 3 | Step 1710000 | Avg Loss: 0.0149 | Grad Norm: 0.00740555\n",
      "Epoch 3 | Step 1710100 | Avg Loss: 0.0147 | Grad Norm: 0.00851690\n",
      "Epoch 3 | Step 1710200 | Avg Loss: 0.0149 | Grad Norm: 0.00978399\n",
      "Epoch 3 | Step 1710300 | Avg Loss: 0.0149 | Grad Norm: 0.00948393\n",
      "Epoch 3 | Step 1710400 | Avg Loss: 0.0147 | Grad Norm: 0.00763103\n",
      "Epoch 3 | Step 1710500 | Avg Loss: 0.0150 | Grad Norm: 0.00741460\n",
      "Epoch 3 | Step 1710600 | Avg Loss: 0.0151 | Grad Norm: 0.00875319\n",
      "Epoch 3 | Step 1710700 | Avg Loss: 0.0148 | Grad Norm: 0.00772039\n",
      "Epoch 3 | Step 1710800 | Avg Loss: 0.0149 | Grad Norm: 0.00961590\n",
      "Epoch 3 | Step 1710900 | Avg Loss: 0.0147 | Grad Norm: 0.00748113\n",
      "Epoch 3 | Step 1711000 | Avg Loss: 0.0150 | Grad Norm: 0.00763793\n",
      "Epoch 3 | Step 1711100 | Avg Loss: 0.0155 | Grad Norm: 0.00830472\n",
      "Epoch 3 | Step 1711200 | Avg Loss: 0.0154 | Grad Norm: 0.00803950\n",
      "Epoch 3 | Step 1711300 | Avg Loss: 0.0153 | Grad Norm: 0.00872328\n",
      "Epoch 3 | Step 1711400 | Avg Loss: 0.0151 | Grad Norm: 0.00788284\n",
      "Epoch 3 | Step 1711500 | Avg Loss: 0.0149 | Grad Norm: 0.00742254\n",
      "Epoch 3 | Step 1711600 | Avg Loss: 0.0150 | Grad Norm: 0.00820372\n",
      "Epoch 3 | Step 1711700 | Avg Loss: 0.0152 | Grad Norm: 0.00722834\n",
      "Epoch 3 | Step 1711800 | Avg Loss: 0.0151 | Grad Norm: 0.00677560\n",
      "Epoch 3 | Step 1711900 | Avg Loss: 0.0151 | Grad Norm: 0.00768630\n",
      "Epoch 3 | Step 1712000 | Avg Loss: 0.0151 | Grad Norm: 0.00900933\n",
      "Epoch 3 | Step 1712100 | Avg Loss: 0.0149 | Grad Norm: 0.00940957\n",
      "Epoch 3 | Step 1712200 | Avg Loss: 0.0147 | Grad Norm: 0.00779009\n",
      "Epoch 3 | Step 1712300 | Avg Loss: 0.0150 | Grad Norm: 0.00855615\n",
      "Epoch 3 | Step 1712400 | Avg Loss: 0.0145 | Grad Norm: 0.00845051\n",
      "Epoch 3 | Step 1712500 | Avg Loss: 0.0145 | Grad Norm: 0.00717395\n",
      "Epoch 3 | Step 1712600 | Avg Loss: 0.0145 | Grad Norm: 0.00844006\n",
      "Epoch 3 | Step 1712700 | Avg Loss: 0.0149 | Grad Norm: 0.00901381\n",
      "Epoch 3 | Step 1712800 | Avg Loss: 0.0149 | Grad Norm: 0.00708778\n",
      "Epoch 3 | Step 1712900 | Avg Loss: 0.0148 | Grad Norm: 0.00987565\n",
      "Epoch 3 | Step 1713000 | Avg Loss: 0.0149 | Grad Norm: 0.00831098\n",
      "Epoch 3 | Step 1713100 | Avg Loss: 0.0148 | Grad Norm: 0.00944809\n",
      "Epoch 3 | Step 1713200 | Avg Loss: 0.0149 | Grad Norm: 0.00800539\n",
      "Epoch 3 | Step 1713300 | Avg Loss: 0.0147 | Grad Norm: 0.00868175\n",
      "Epoch 3 | Step 1713400 | Avg Loss: 0.0146 | Grad Norm: 0.00897181\n",
      "Epoch 3 | Step 1713500 | Avg Loss: 0.0143 | Grad Norm: 0.00828548\n",
      "Epoch 3 | Step 1713600 | Avg Loss: 0.0145 | Grad Norm: 0.00853181\n",
      "Epoch 3 | Step 1713700 | Avg Loss: 0.0144 | Grad Norm: 0.00816936\n",
      "Epoch 3 | Step 1713800 | Avg Loss: 0.0143 | Grad Norm: 0.00818172\n",
      "Epoch 3 | Step 1713900 | Avg Loss: 0.0145 | Grad Norm: 0.00797967\n",
      "Epoch 3 | Step 1714000 | Avg Loss: 0.0146 | Grad Norm: 0.00775446\n",
      "Epoch 3 | Step 1714100 | Avg Loss: 0.0144 | Grad Norm: 0.00886768\n",
      "Epoch 3 | Step 1714200 | Avg Loss: 0.0146 | Grad Norm: 0.00777776\n",
      "Epoch 3 | Step 1714300 | Avg Loss: 0.0145 | Grad Norm: 0.00886627\n",
      "Epoch 3 | Step 1714400 | Avg Loss: 0.0150 | Grad Norm: 0.00704451\n",
      "Epoch 3 | Step 1714500 | Avg Loss: 0.0145 | Grad Norm: 0.00844352\n",
      "Epoch 3 | Step 1714600 | Avg Loss: 0.0144 | Grad Norm: 0.00727393\n",
      "Epoch 3 | Step 1714700 | Avg Loss: 0.0147 | Grad Norm: 0.00740861\n",
      "Epoch 3 | Step 1714800 | Avg Loss: 0.0149 | Grad Norm: 0.00771210\n",
      "Epoch 3 | Step 1714900 | Avg Loss: 0.0151 | Grad Norm: 0.00687780\n",
      "Epoch 3 | Step 1715000 | Avg Loss: 0.0150 | Grad Norm: 0.00777077\n",
      "Epoch 3 | Step 1715100 | Avg Loss: 0.0151 | Grad Norm: 0.00725299\n",
      "Epoch 3 | Step 1715200 | Avg Loss: 0.0151 | Grad Norm: 0.00807735\n",
      "Epoch 3 | Step 1715300 | Avg Loss: 0.0152 | Grad Norm: 0.00851535\n",
      "Epoch 3 | Step 1715400 | Avg Loss: 0.0148 | Grad Norm: 0.00777591\n",
      "Epoch 3 | Step 1715500 | Avg Loss: 0.0147 | Grad Norm: 0.00993135\n",
      "Epoch 3 | Step 1715600 | Avg Loss: 0.0148 | Grad Norm: 0.00713044\n",
      "Epoch 3 | Step 1715700 | Avg Loss: 0.0144 | Grad Norm: 0.00916520\n",
      "Epoch 3 | Step 1715800 | Avg Loss: 0.0145 | Grad Norm: 0.00774648\n",
      "Epoch 3 | Step 1715900 | Avg Loss: 0.0151 | Grad Norm: 0.00753662\n",
      "Epoch 3 | Step 1716000 | Avg Loss: 0.0150 | Grad Norm: 0.00837312\n",
      "Epoch 3 | Step 1716100 | Avg Loss: 0.0147 | Grad Norm: 0.00761037\n",
      "Epoch 3 | Step 1716200 | Avg Loss: 0.0145 | Grad Norm: 0.00804115\n",
      "Epoch 3 | Step 1716300 | Avg Loss: 0.0147 | Grad Norm: 0.00755470\n",
      "Epoch 3 | Step 1716400 | Avg Loss: 0.0152 | Grad Norm: 0.00824259\n",
      "Epoch 3 | Step 1716500 | Avg Loss: 0.0151 | Grad Norm: 0.00757467\n",
      "Epoch 3 | Step 1716600 | Avg Loss: 0.0154 | Grad Norm: 0.00904138\n",
      "Epoch 3 | Step 1716700 | Avg Loss: 0.0154 | Grad Norm: 0.00735565\n",
      "Epoch 3 | Step 1716800 | Avg Loss: 0.0152 | Grad Norm: 0.00903697\n",
      "Epoch 3 | Step 1716900 | Avg Loss: 0.0144 | Grad Norm: 0.00757082\n",
      "Epoch 3 | Step 1717000 | Avg Loss: 0.0144 | Grad Norm: 0.00845309\n",
      "Epoch 3 | Step 1717100 | Avg Loss: 0.0145 | Grad Norm: 0.00732209\n",
      "Epoch 3 | Step 1717200 | Avg Loss: 0.0145 | Grad Norm: 0.00844094\n",
      "Epoch 3 | Step 1717300 | Avg Loss: 0.0151 | Grad Norm: 0.00710267\n",
      "Epoch 3 | Step 1717400 | Avg Loss: 0.0153 | Grad Norm: 0.00903499\n",
      "Epoch 3 | Step 1717500 | Avg Loss: 0.0154 | Grad Norm: 0.00748442\n",
      "Epoch 3 | Step 1717600 | Avg Loss: 0.0153 | Grad Norm: 0.00972532\n",
      "Epoch 3 | Step 1717700 | Avg Loss: 0.0152 | Grad Norm: 0.00842411\n",
      "Epoch 3 | Step 1717800 | Avg Loss: 0.0148 | Grad Norm: 0.00944261\n",
      "Epoch 3 | Step 1717900 | Avg Loss: 0.0148 | Grad Norm: 0.00829719\n",
      "Epoch 3 | Step 1718000 | Avg Loss: 0.0146 | Grad Norm: 0.00743038\n",
      "Epoch 3 | Step 1718100 | Avg Loss: 0.0145 | Grad Norm: 0.00769296\n",
      "Epoch 3 | Step 1718200 | Avg Loss: 0.0143 | Grad Norm: 0.00822383\n",
      "Epoch 3 | Step 1718300 | Avg Loss: 0.0139 | Grad Norm: 0.00760555\n",
      "Epoch 3 | Step 1718400 | Avg Loss: 0.0139 | Grad Norm: 0.00805393\n",
      "Epoch 3 | Step 1718500 | Avg Loss: 0.0142 | Grad Norm: 0.00865946\n",
      "Epoch 3 | Step 1718600 | Avg Loss: 0.0145 | Grad Norm: 0.00866535\n",
      "Epoch 3 | Step 1718700 | Avg Loss: 0.0151 | Grad Norm: 0.00771674\n",
      "Epoch 3 | Step 1718800 | Avg Loss: 0.0153 | Grad Norm: 0.00776694\n",
      "Epoch 3 | Step 1718900 | Avg Loss: 0.0152 | Grad Norm: 0.00721281\n",
      "Epoch 3 | Step 1719000 | Avg Loss: 0.0151 | Grad Norm: 0.00777009\n",
      "Epoch 3 | Step 1719100 | Avg Loss: 0.0149 | Grad Norm: 0.00807510\n",
      "Epoch 3 | Step 1719200 | Avg Loss: 0.0151 | Grad Norm: 0.00812281\n",
      "Epoch 3 | Step 1719300 | Avg Loss: 0.0146 | Grad Norm: 0.00794849\n",
      "Epoch 3 | Step 1719400 | Avg Loss: 0.0147 | Grad Norm: 0.00864426\n",
      "Epoch 3 | Step 1719500 | Avg Loss: 0.0150 | Grad Norm: 0.00948092\n",
      "Epoch 3 | Step 1719600 | Avg Loss: 0.0149 | Grad Norm: 0.00700239\n",
      "Epoch 3 | Step 1719700 | Avg Loss: 0.0146 | Grad Norm: 0.00613586\n",
      "Epoch 3 | Step 1719800 | Avg Loss: 0.0148 | Grad Norm: 0.00842623\n",
      "Epoch 3 | Step 1719900 | Avg Loss: 0.0152 | Grad Norm: 0.00843188\n",
      "Epoch 3 | Step 1720000 | Avg Loss: 0.0151 | Grad Norm: 0.00723727\n",
      "Epoch 3 | Step 1720100 | Avg Loss: 0.0150 | Grad Norm: 0.00745102\n",
      "Epoch 3 | Step 1720200 | Avg Loss: 0.0151 | Grad Norm: 0.00776867\n",
      "Epoch 3 | Step 1720300 | Avg Loss: 0.0148 | Grad Norm: 0.00644151\n",
      "Epoch 3 | Step 1720400 | Avg Loss: 0.0146 | Grad Norm: 0.01008970\n",
      "Epoch 3 | Step 1720500 | Avg Loss: 0.0150 | Grad Norm: 0.00721429\n",
      "Epoch 3 | Step 1720600 | Avg Loss: 0.0150 | Grad Norm: 0.00741074\n",
      "Epoch 3 | Step 1720700 | Avg Loss: 0.0150 | Grad Norm: 0.00827878\n",
      "Epoch 3 | Step 1720800 | Avg Loss: 0.0150 | Grad Norm: 0.00785823\n",
      "Epoch 3 | Step 1720900 | Avg Loss: 0.0152 | Grad Norm: 0.00904084\n",
      "Epoch 3 | Step 1721000 | Avg Loss: 0.0151 | Grad Norm: 0.00845233\n",
      "Epoch 3 | Step 1721100 | Avg Loss: 0.0152 | Grad Norm: 0.00714649\n",
      "Epoch 3 | Step 1721200 | Avg Loss: 0.0152 | Grad Norm: 0.01047738\n",
      "Epoch 3 | Step 1721300 | Avg Loss: 0.0150 | Grad Norm: 0.00763740\n",
      "Epoch 3 | Step 1721400 | Avg Loss: 0.0152 | Grad Norm: 0.00780343\n",
      "Epoch 3 | Step 1721500 | Avg Loss: 0.0151 | Grad Norm: 0.00680763\n",
      "Epoch 3 | Step 1721600 | Avg Loss: 0.0151 | Grad Norm: 0.00782991\n",
      "Epoch 3 | Step 1721700 | Avg Loss: 0.0150 | Grad Norm: 0.00975799\n",
      "Epoch 3 | Step 1721800 | Avg Loss: 0.0152 | Grad Norm: 0.00664154\n",
      "Epoch 3 | Step 1721900 | Avg Loss: 0.0152 | Grad Norm: 0.00705286\n",
      "Epoch 3 | Step 1722000 | Avg Loss: 0.0153 | Grad Norm: 0.00792657\n",
      "Epoch 3 | Step 1722100 | Avg Loss: 0.0151 | Grad Norm: 0.00724704\n",
      "Epoch 3 | Step 1722200 | Avg Loss: 0.0153 | Grad Norm: 0.00744867\n",
      "Epoch 3 | Step 1722300 | Avg Loss: 0.0157 | Grad Norm: 0.00736896\n",
      "Epoch 3 | Step 1722400 | Avg Loss: 0.0154 | Grad Norm: 0.00867736\n",
      "Epoch 3 | Step 1722500 | Avg Loss: 0.0149 | Grad Norm: 0.00789421\n",
      "Epoch 3 | Step 1722600 | Avg Loss: 0.0152 | Grad Norm: 0.00699893\n",
      "Epoch 3 | Step 1722700 | Avg Loss: 0.0151 | Grad Norm: 0.01014509\n",
      "Epoch 3 | Step 1722800 | Avg Loss: 0.0150 | Grad Norm: 0.00757061\n",
      "Epoch 3 | Step 1722900 | Avg Loss: 0.0151 | Grad Norm: 0.00797265\n",
      "Epoch 3 | Step 1723000 | Avg Loss: 0.0152 | Grad Norm: 0.00895050\n",
      "Epoch 3 | Step 1723100 | Avg Loss: 0.0150 | Grad Norm: 0.00835341\n",
      "Epoch 3 | Step 1723200 | Avg Loss: 0.0148 | Grad Norm: 0.00751693\n",
      "Epoch 3 | Step 1723300 | Avg Loss: 0.0149 | Grad Norm: 0.00775703\n",
      "Epoch 3 | Step 1723400 | Avg Loss: 0.0148 | Grad Norm: 0.00788289\n",
      "Epoch 3 | Step 1723500 | Avg Loss: 0.0145 | Grad Norm: 0.00846694\n",
      "Epoch 3 | Step 1723600 | Avg Loss: 0.0149 | Grad Norm: 0.00743731\n",
      "Epoch 3 | Step 1723700 | Avg Loss: 0.0147 | Grad Norm: 0.00809914\n",
      "Epoch 3 | Step 1723800 | Avg Loss: 0.0149 | Grad Norm: 0.00774206\n",
      "Epoch 3 | Step 1723900 | Avg Loss: 0.0148 | Grad Norm: 0.00801966\n",
      "Epoch 3 | Step 1724000 | Avg Loss: 0.0148 | Grad Norm: 0.00758743\n",
      "Epoch 3 | Step 1724100 | Avg Loss: 0.0149 | Grad Norm: 0.00832620\n",
      "Epoch 3 | Step 1724200 | Avg Loss: 0.0151 | Grad Norm: 0.00755085\n",
      "Epoch 3 | Step 1724300 | Avg Loss: 0.0149 | Grad Norm: 0.00774581\n",
      "Epoch 3 | Step 1724400 | Avg Loss: 0.0147 | Grad Norm: 0.00747281\n",
      "Epoch 3 | Step 1724500 | Avg Loss: 0.0145 | Grad Norm: 0.00824238\n",
      "Epoch 3 | Step 1724600 | Avg Loss: 0.0148 | Grad Norm: 0.00820757\n",
      "Epoch 3 | Step 1724700 | Avg Loss: 0.0154 | Grad Norm: 0.00811708\n",
      "Epoch 3 | Step 1724800 | Avg Loss: 0.0149 | Grad Norm: 0.00702554\n",
      "Epoch 3 | Step 1724900 | Avg Loss: 0.0147 | Grad Norm: 0.00803898\n",
      "Epoch 3 | Step 1725000 | Avg Loss: 0.0145 | Grad Norm: 0.00736741\n",
      "Epoch 3 | Step 1725100 | Avg Loss: 0.0146 | Grad Norm: 0.00835369\n",
      "Epoch 3 | Step 1725200 | Avg Loss: 0.0148 | Grad Norm: 0.00789780\n",
      "Epoch 3 | Step 1725300 | Avg Loss: 0.0152 | Grad Norm: 0.00827166\n",
      "Epoch 3 | Step 1725400 | Avg Loss: 0.0151 | Grad Norm: 0.00740582\n",
      "Epoch 3 | Step 1725500 | Avg Loss: 0.0150 | Grad Norm: 0.00881902\n",
      "Epoch 3 | Step 1725600 | Avg Loss: 0.0153 | Grad Norm: 0.00811430\n",
      "Epoch 3 | Step 1725700 | Avg Loss: 0.0151 | Grad Norm: 0.00859038\n",
      "Epoch 3 | Step 1725800 | Avg Loss: 0.0150 | Grad Norm: 0.00747048\n",
      "Epoch 3 | Step 1725900 | Avg Loss: 0.0153 | Grad Norm: 0.00755081\n",
      "Epoch 3 | Step 1726000 | Avg Loss: 0.0154 | Grad Norm: 0.00873381\n",
      "Epoch 3 | Step 1726100 | Avg Loss: 0.0153 | Grad Norm: 0.00695987\n",
      "Epoch 3 | Step 1726200 | Avg Loss: 0.0151 | Grad Norm: 0.00801867\n",
      "Epoch 3 | Step 1726300 | Avg Loss: 0.0150 | Grad Norm: 0.00842896\n",
      "Epoch 3 | Step 1726400 | Avg Loss: 0.0152 | Grad Norm: 0.01186541\n",
      "Epoch 3 | Step 1726500 | Avg Loss: 0.0151 | Grad Norm: 0.00794436\n",
      "Epoch 3 | Step 1726600 | Avg Loss: 0.0155 | Grad Norm: 0.00973119\n",
      "Epoch 3 | Step 1726700 | Avg Loss: 0.0156 | Grad Norm: 0.00831931\n",
      "Epoch 3 | Step 1726800 | Avg Loss: 0.0157 | Grad Norm: 0.00848291\n",
      "Epoch 3 | Step 1726900 | Avg Loss: 0.0152 | Grad Norm: 0.00829027\n",
      "Epoch 3 | Step 1727000 | Avg Loss: 0.0151 | Grad Norm: 0.00734016\n",
      "Epoch 3 | Step 1727100 | Avg Loss: 0.0150 | Grad Norm: 0.00909682\n",
      "Epoch 3 | Step 1727200 | Avg Loss: 0.0149 | Grad Norm: 0.00779269\n",
      "Epoch 3 | Step 1727300 | Avg Loss: 0.0149 | Grad Norm: 0.00692110\n",
      "Epoch 3 | Step 1727400 | Avg Loss: 0.0150 | Grad Norm: 0.00819978\n",
      "Epoch 3 | Step 1727500 | Avg Loss: 0.0147 | Grad Norm: 0.01062281\n",
      "Epoch 3 | Step 1727600 | Avg Loss: 0.0146 | Grad Norm: 0.00814001\n",
      "Epoch 3 | Step 1727700 | Avg Loss: 0.0147 | Grad Norm: 0.00842720\n",
      "Epoch 3 | Step 1727800 | Avg Loss: 0.0151 | Grad Norm: 0.00830764\n",
      "Epoch 3 | Step 1727900 | Avg Loss: 0.0153 | Grad Norm: 0.00793801\n",
      "Epoch 3 | Step 1728000 | Avg Loss: 0.0152 | Grad Norm: 0.00700852\n",
      "Epoch 3 | Step 1728100 | Avg Loss: 0.0152 | Grad Norm: 0.00815916\n",
      "Epoch 3 | Step 1728200 | Avg Loss: 0.0151 | Grad Norm: 0.00922755\n",
      "Epoch 3 | Step 1728300 | Avg Loss: 0.0148 | Grad Norm: 0.00668043\n",
      "Epoch 3 | Step 1728400 | Avg Loss: 0.0146 | Grad Norm: 0.00739751\n",
      "Epoch 3 | Step 1728500 | Avg Loss: 0.0149 | Grad Norm: 0.00854628\n",
      "Epoch 3 | Step 1728600 | Avg Loss: 0.0148 | Grad Norm: 0.00773832\n",
      "Epoch 3 | Step 1728700 | Avg Loss: 0.0148 | Grad Norm: 0.00786275\n",
      "Epoch 3 | Step 1728800 | Avg Loss: 0.0153 | Grad Norm: 0.00930503\n",
      "Epoch 3 | Step 1728900 | Avg Loss: 0.0155 | Grad Norm: 0.00790108\n",
      "Epoch 3 | Step 1729000 | Avg Loss: 0.0155 | Grad Norm: 0.00725385\n",
      "Epoch 3 | Step 1729100 | Avg Loss: 0.0153 | Grad Norm: 0.01007581\n",
      "Epoch 3 | Step 1729200 | Avg Loss: 0.0154 | Grad Norm: 0.00899880\n",
      "Epoch 3 | Step 1729300 | Avg Loss: 0.0152 | Grad Norm: 0.00781445\n",
      "Epoch 3 | Step 1729400 | Avg Loss: 0.0151 | Grad Norm: 0.00880013\n",
      "Epoch 3 | Step 1729500 | Avg Loss: 0.0150 | Grad Norm: 0.00841827\n",
      "Epoch 3 | Step 1729600 | Avg Loss: 0.0152 | Grad Norm: 0.00826359\n",
      "Epoch 3 | Step 1729700 | Avg Loss: 0.0156 | Grad Norm: 0.00756262\n",
      "Epoch 3 | Step 1729800 | Avg Loss: 0.0155 | Grad Norm: 0.00808720\n",
      "Epoch 3 | Step 1729900 | Avg Loss: 0.0153 | Grad Norm: 0.00702094\n",
      "Epoch 3 | Step 1730000 | Avg Loss: 0.0153 | Grad Norm: 0.00756329\n",
      "Epoch 3 | Step 1730100 | Avg Loss: 0.0156 | Grad Norm: 0.00926952\n",
      "Epoch 3 | Step 1730200 | Avg Loss: 0.0158 | Grad Norm: 0.00856199\n",
      "Epoch 3 | Step 1730300 | Avg Loss: 0.0157 | Grad Norm: 0.00787324\n",
      "Epoch 3 | Step 1730400 | Avg Loss: 0.0153 | Grad Norm: 0.00807043\n",
      "Epoch 3 | Step 1730500 | Avg Loss: 0.0150 | Grad Norm: 0.00905724\n",
      "Epoch 3 | Step 1730600 | Avg Loss: 0.0153 | Grad Norm: 0.00757986\n",
      "Epoch 3 | Step 1730700 | Avg Loss: 0.0157 | Grad Norm: 0.00845202\n",
      "Epoch 3 | Step 1730800 | Avg Loss: 0.0153 | Grad Norm: 0.00807896\n",
      "Epoch 3 | Step 1730900 | Avg Loss: 0.0151 | Grad Norm: 0.00926408\n",
      "Epoch 3 | Step 1731000 | Avg Loss: 0.0153 | Grad Norm: 0.00790074\n",
      "Epoch 3 | Step 1731100 | Avg Loss: 0.0156 | Grad Norm: 0.00722928\n",
      "Epoch 3 | Step 1731200 | Avg Loss: 0.0153 | Grad Norm: 0.00924655\n",
      "Epoch 3 | Step 1731300 | Avg Loss: 0.0150 | Grad Norm: 0.00781839\n",
      "Epoch 3 | Step 1731400 | Avg Loss: 0.0151 | Grad Norm: 0.00767752\n",
      "Epoch 3 | Step 1731500 | Avg Loss: 0.0147 | Grad Norm: 0.00781914\n",
      "Epoch 3 | Step 1731600 | Avg Loss: 0.0147 | Grad Norm: 0.00798227\n",
      "Epoch 3 | Step 1731700 | Avg Loss: 0.0146 | Grad Norm: 0.00784883\n",
      "Epoch 3 | Step 1731800 | Avg Loss: 0.0148 | Grad Norm: 0.00768245\n",
      "Epoch 3 | Step 1731900 | Avg Loss: 0.0145 | Grad Norm: 0.00714971\n",
      "Epoch 3 | Step 1732000 | Avg Loss: 0.0146 | Grad Norm: 0.00749583\n",
      "Epoch 3 | Step 1732100 | Avg Loss: 0.0148 | Grad Norm: 0.00764916\n",
      "Epoch 3 | Step 1732200 | Avg Loss: 0.0149 | Grad Norm: 0.00770994\n",
      "Epoch 3 | Step 1732300 | Avg Loss: 0.0147 | Grad Norm: 0.00773085\n",
      "Epoch 3 | Step 1732400 | Avg Loss: 0.0142 | Grad Norm: 0.00691397\n",
      "Epoch 3 | Step 1732500 | Avg Loss: 0.0143 | Grad Norm: 0.00730456\n",
      "Epoch 3 | Step 1732600 | Avg Loss: 0.0145 | Grad Norm: 0.00785528\n",
      "Epoch 3 | Step 1732700 | Avg Loss: 0.0143 | Grad Norm: 0.00777175\n",
      "Epoch 3 | Step 1732800 | Avg Loss: 0.0151 | Grad Norm: 0.00720497\n",
      "Epoch 3 | Step 1732900 | Avg Loss: 0.0149 | Grad Norm: 0.00698611\n",
      "Epoch 3 | Step 1733000 | Avg Loss: 0.0149 | Grad Norm: 0.00754838\n",
      "Epoch 3 | Step 1733100 | Avg Loss: 0.0152 | Grad Norm: 0.00777254\n",
      "Epoch 3 | Step 1733200 | Avg Loss: 0.0152 | Grad Norm: 0.00923991\n",
      "Epoch 3 | Step 1733300 | Avg Loss: 0.0151 | Grad Norm: 0.00865201\n",
      "Epoch 3 | Step 1733400 | Avg Loss: 0.0150 | Grad Norm: 0.00674963\n",
      "Epoch 3 | Step 1733500 | Avg Loss: 0.0148 | Grad Norm: 0.00933623\n",
      "Epoch 3 | Step 1733600 | Avg Loss: 0.0144 | Grad Norm: 0.00690701\n",
      "Epoch 3 | Step 1733700 | Avg Loss: 0.0149 | Grad Norm: 0.00705159\n",
      "Epoch 3 | Step 1733800 | Avg Loss: 0.0150 | Grad Norm: 0.00863898\n",
      "Epoch 3 | Step 1733900 | Avg Loss: 0.0150 | Grad Norm: 0.00752679\n",
      "Epoch 3 | Step 1734000 | Avg Loss: 0.0154 | Grad Norm: 0.00948584\n",
      "Epoch 3 | Step 1734100 | Avg Loss: 0.0151 | Grad Norm: 0.00647603\n",
      "Epoch 3 | Step 1734200 | Avg Loss: 0.0149 | Grad Norm: 0.00784752\n",
      "Epoch 3 | Step 1734300 | Avg Loss: 0.0152 | Grad Norm: 0.00906017\n",
      "Epoch 3 | Step 1734400 | Avg Loss: 0.0155 | Grad Norm: 0.00918975\n",
      "Epoch 3 | Step 1734500 | Avg Loss: 0.0152 | Grad Norm: 0.00778897\n",
      "Epoch 3 | Step 1734600 | Avg Loss: 0.0150 | Grad Norm: 0.00845576\n",
      "Epoch 3 | Step 1734700 | Avg Loss: 0.0150 | Grad Norm: 0.00683687\n",
      "Epoch 3 | Step 1734800 | Avg Loss: 0.0148 | Grad Norm: 0.00767407\n",
      "Epoch 3 | Step 1734900 | Avg Loss: 0.0146 | Grad Norm: 0.00821948\n",
      "Epoch 3 | Step 1735000 | Avg Loss: 0.0144 | Grad Norm: 0.00749352\n",
      "Epoch 3 | Step 1735100 | Avg Loss: 0.0143 | Grad Norm: 0.00790102\n",
      "Epoch 3 | Step 1735200 | Avg Loss: 0.0143 | Grad Norm: 0.00751958\n",
      "Epoch 3 | Step 1735300 | Avg Loss: 0.0145 | Grad Norm: 0.00835205\n",
      "Epoch 3 | Step 1735400 | Avg Loss: 0.0144 | Grad Norm: 0.00822239\n",
      "Epoch 3 | Step 1735500 | Avg Loss: 0.0145 | Grad Norm: 0.00752308\n",
      "Epoch 3 | Step 1735600 | Avg Loss: 0.0146 | Grad Norm: 0.01089870\n",
      "Epoch 3 | Step 1735700 | Avg Loss: 0.0151 | Grad Norm: 0.00911484\n",
      "Epoch 3 | Step 1735800 | Avg Loss: 0.0151 | Grad Norm: 0.00724058\n",
      "Epoch 3 | Step 1735900 | Avg Loss: 0.0153 | Grad Norm: 0.00995419\n",
      "Epoch 3 | Step 1736000 | Avg Loss: 0.0151 | Grad Norm: 0.00814272\n",
      "Epoch 3 | Step 1736100 | Avg Loss: 0.0151 | Grad Norm: 0.00869299\n",
      "Epoch 3 | Step 1736200 | Avg Loss: 0.0153 | Grad Norm: 0.00824926\n",
      "Epoch 3 | Step 1736300 | Avg Loss: 0.0152 | Grad Norm: 0.01013425\n",
      "Epoch 3 | Step 1736400 | Avg Loss: 0.0155 | Grad Norm: 0.00904635\n",
      "Epoch 3 | Step 1736500 | Avg Loss: 0.0156 | Grad Norm: 0.00775244\n",
      "Epoch 3 | Step 1736600 | Avg Loss: 0.0155 | Grad Norm: 0.00696270\n",
      "Epoch 3 | Step 1736700 | Avg Loss: 0.0153 | Grad Norm: 0.00826300\n",
      "Epoch 3 | Step 1736800 | Avg Loss: 0.0151 | Grad Norm: 0.00853398\n",
      "Epoch 3 | Step 1736900 | Avg Loss: 0.0154 | Grad Norm: 0.00853494\n",
      "Epoch 3 | Step 1737000 | Avg Loss: 0.0157 | Grad Norm: 0.00792992\n",
      "Epoch 3 | Step 1737100 | Avg Loss: 0.0151 | Grad Norm: 0.00845201\n",
      "Epoch 3 | Step 1737200 | Avg Loss: 0.0154 | Grad Norm: 0.00723393\n",
      "Epoch 3 | Step 1737300 | Avg Loss: 0.0153 | Grad Norm: 0.00787923\n",
      "Epoch 3 | Step 1737400 | Avg Loss: 0.0156 | Grad Norm: 0.00723899\n",
      "Epoch 3 | Step 1737500 | Avg Loss: 0.0153 | Grad Norm: 0.00952437\n",
      "Epoch 3 | Step 1737600 | Avg Loss: 0.0153 | Grad Norm: 0.00753331\n",
      "Epoch 3 | Step 1737700 | Avg Loss: 0.0150 | Grad Norm: 0.00964809\n",
      "Epoch 3 | Step 1737800 | Avg Loss: 0.0148 | Grad Norm: 0.00731255\n",
      "Epoch 3 | Step 1737900 | Avg Loss: 0.0149 | Grad Norm: 0.00885464\n",
      "Epoch 3 | Step 1738000 | Avg Loss: 0.0148 | Grad Norm: 0.00827370\n",
      "Epoch 3 | Step 1738100 | Avg Loss: 0.0148 | Grad Norm: 0.00758667\n",
      "Epoch 3 | Step 1738200 | Avg Loss: 0.0155 | Grad Norm: 0.00822650\n",
      "Epoch 3 | Step 1738300 | Avg Loss: 0.0157 | Grad Norm: 0.00775112\n",
      "Epoch 3 | Step 1738400 | Avg Loss: 0.0153 | Grad Norm: 0.00961226\n",
      "Epoch 3 | Step 1738500 | Avg Loss: 0.0149 | Grad Norm: 0.01013483\n",
      "Epoch 3 | Step 1738600 | Avg Loss: 0.0151 | Grad Norm: 0.00803089\n",
      "Epoch 3 | Step 1738700 | Avg Loss: 0.0150 | Grad Norm: 0.00922131\n",
      "Epoch 3 | Step 1738800 | Avg Loss: 0.0149 | Grad Norm: 0.00831883\n",
      "Epoch 3 | Step 1738900 | Avg Loss: 0.0148 | Grad Norm: 0.00708539\n",
      "Epoch 3 | Step 1739000 | Avg Loss: 0.0149 | Grad Norm: 0.00915620\n",
      "Epoch 3 | Step 1739100 | Avg Loss: 0.0151 | Grad Norm: 0.00924054\n",
      "Epoch 3 | Step 1739200 | Avg Loss: 0.0151 | Grad Norm: 0.01055688\n",
      "Epoch 3 | Step 1739300 | Avg Loss: 0.0155 | Grad Norm: 0.00770353\n",
      "Epoch 3 | Step 1739400 | Avg Loss: 0.0154 | Grad Norm: 0.01035300\n",
      "Epoch 3 | Step 1739500 | Avg Loss: 0.0156 | Grad Norm: 0.00770935\n",
      "Epoch 3 | Step 1739600 | Avg Loss: 0.0153 | Grad Norm: 0.00923784\n",
      "Epoch 3 | Step 1739700 | Avg Loss: 0.0154 | Grad Norm: 0.00825575\n",
      "Epoch 3 | Step 1739800 | Avg Loss: 0.0152 | Grad Norm: 0.01013972\n",
      "Epoch 3 | Step 1739900 | Avg Loss: 0.0154 | Grad Norm: 0.00879792\n",
      "Epoch 3 | Step 1740000 | Avg Loss: 0.0154 | Grad Norm: 0.00971162\n",
      "Epoch 3 | Step 1740100 | Avg Loss: 0.0151 | Grad Norm: 0.00795258\n",
      "Epoch 3 | Step 1740200 | Avg Loss: 0.0148 | Grad Norm: 0.00875956\n",
      "Epoch 3 | Step 1740300 | Avg Loss: 0.0146 | Grad Norm: 0.00762096\n",
      "Epoch 3 | Step 1740400 | Avg Loss: 0.0149 | Grad Norm: 0.00889250\n",
      "Epoch 3 | Step 1740500 | Avg Loss: 0.0155 | Grad Norm: 0.00723174\n",
      "Epoch 3 | Step 1740600 | Avg Loss: 0.0153 | Grad Norm: 0.01062947\n",
      "Epoch 3 | Step 1740700 | Avg Loss: 0.0151 | Grad Norm: 0.00712653\n",
      "Epoch 3 | Step 1740800 | Avg Loss: 0.0150 | Grad Norm: 0.00697661\n",
      "Epoch 3 | Step 1740900 | Avg Loss: 0.0151 | Grad Norm: 0.00757322\n",
      "Epoch 3 | Step 1741000 | Avg Loss: 0.0151 | Grad Norm: 0.00677875\n",
      "Epoch 3 | Step 1741100 | Avg Loss: 0.0147 | Grad Norm: 0.00763656\n",
      "Epoch 3 | Step 1741200 | Avg Loss: 0.0151 | Grad Norm: 0.00775817\n",
      "Epoch 3 | Step 1741300 | Avg Loss: 0.0152 | Grad Norm: 0.00790322\n",
      "Epoch 3 | Step 1741400 | Avg Loss: 0.0154 | Grad Norm: 0.00874346\n",
      "Epoch 3 | Step 1741500 | Avg Loss: 0.0155 | Grad Norm: 0.01225065\n",
      "Epoch 3 | Step 1741600 | Avg Loss: 0.0151 | Grad Norm: 0.00855767\n",
      "Epoch 3 | Step 1741700 | Avg Loss: 0.0155 | Grad Norm: 0.00753264\n",
      "Epoch 3 | Step 1741800 | Avg Loss: 0.0156 | Grad Norm: 0.00776523\n",
      "Epoch 3 | Step 1741900 | Avg Loss: 0.0153 | Grad Norm: 0.01058620\n",
      "Epoch 3 | Step 1742000 | Avg Loss: 0.0156 | Grad Norm: 0.00771348\n",
      "Epoch 3 | Step 1742100 | Avg Loss: 0.0157 | Grad Norm: 0.00741413\n",
      "Epoch 3 | Step 1742200 | Avg Loss: 0.0157 | Grad Norm: 0.00997520\n",
      "Epoch 3 | Step 1742300 | Avg Loss: 0.0157 | Grad Norm: 0.00915965\n",
      "Epoch 3 | Step 1742400 | Avg Loss: 0.0154 | Grad Norm: 0.00839792\n",
      "Epoch 3 | Step 1742500 | Avg Loss: 0.0154 | Grad Norm: 0.00899368\n",
      "Epoch 3 | Step 1742600 | Avg Loss: 0.0155 | Grad Norm: 0.00836480\n",
      "Epoch 3 | Step 1742700 | Avg Loss: 0.0152 | Grad Norm: 0.00757198\n",
      "Epoch 3 | Step 1742800 | Avg Loss: 0.0155 | Grad Norm: 0.01065376\n",
      "Epoch 3 | Step 1742900 | Avg Loss: 0.0155 | Grad Norm: 0.00863491\n",
      "Epoch 3 | Step 1743000 | Avg Loss: 0.0154 | Grad Norm: 0.00807376\n",
      "Epoch 3 | Step 1743100 | Avg Loss: 0.0152 | Grad Norm: 0.00782571\n",
      "Epoch 3 | Step 1743200 | Avg Loss: 0.0152 | Grad Norm: 0.00804687\n",
      "Epoch 3 | Step 1743300 | Avg Loss: 0.0150 | Grad Norm: 0.00723711\n",
      "Epoch 3 | Step 1743400 | Avg Loss: 0.0152 | Grad Norm: 0.00760986\n",
      "Epoch 3 | Step 1743500 | Avg Loss: 0.0150 | Grad Norm: 0.00849388\n",
      "Epoch 3 | Step 1743600 | Avg Loss: 0.0152 | Grad Norm: 0.00841707\n",
      "Epoch 3 | Step 1743700 | Avg Loss: 0.0156 | Grad Norm: 0.00920372\n",
      "Epoch 3 | Step 1743800 | Avg Loss: 0.0155 | Grad Norm: 0.00827153\n",
      "Epoch 3 | Step 1743900 | Avg Loss: 0.0157 | Grad Norm: 0.00851893\n",
      "Epoch 3 | Step 1744000 | Avg Loss: 0.0150 | Grad Norm: 0.00883176\n",
      "Epoch 3 | Step 1744100 | Avg Loss: 0.0152 | Grad Norm: 0.00755482\n",
      "Epoch 3 | Step 1744200 | Avg Loss: 0.0152 | Grad Norm: 0.00845306\n",
      "Epoch 3 | Step 1744300 | Avg Loss: 0.0151 | Grad Norm: 0.00898776\n",
      "Epoch 3 | Step 1744400 | Avg Loss: 0.0147 | Grad Norm: 0.00757392\n",
      "Epoch 3 | Step 1744500 | Avg Loss: 0.0149 | Grad Norm: 0.00709369\n",
      "Epoch 3 | Step 1744600 | Avg Loss: 0.0149 | Grad Norm: 0.00799280\n",
      "Epoch 3 | Step 1744700 | Avg Loss: 0.0151 | Grad Norm: 0.00772441\n",
      "Epoch 3 | Step 1744800 | Avg Loss: 0.0153 | Grad Norm: 0.00818072\n",
      "Epoch 3 | Step 1744900 | Avg Loss: 0.0151 | Grad Norm: 0.00761239\n",
      "Epoch 3 | Step 1745000 | Avg Loss: 0.0149 | Grad Norm: 0.00832385\n",
      "Epoch 3 | Step 1745100 | Avg Loss: 0.0148 | Grad Norm: 0.00774997\n",
      "Epoch 3 | Step 1745200 | Avg Loss: 0.0145 | Grad Norm: 0.00739111\n",
      "Epoch 3 | Step 1745300 | Avg Loss: 0.0146 | Grad Norm: 0.00757495\n",
      "Epoch 3 | Step 1745400 | Avg Loss: 0.0147 | Grad Norm: 0.00827334\n",
      "Epoch 3 | Step 1745500 | Avg Loss: 0.0148 | Grad Norm: 0.00826085\n",
      "Epoch 3 | Step 1745600 | Avg Loss: 0.0149 | Grad Norm: 0.00810708\n",
      "Epoch 3 | Step 1745700 | Avg Loss: 0.0148 | Grad Norm: 0.00737192\n",
      "Epoch 3 | Step 1745800 | Avg Loss: 0.0148 | Grad Norm: 0.00841863\n",
      "Epoch 3 | Step 1745900 | Avg Loss: 0.0147 | Grad Norm: 0.00683019\n",
      "Epoch 3 | Step 1746000 | Avg Loss: 0.0147 | Grad Norm: 0.00916047\n",
      "Epoch 3 | Step 1746100 | Avg Loss: 0.0148 | Grad Norm: 0.00872728\n",
      "Epoch 3 | Step 1746200 | Avg Loss: 0.0148 | Grad Norm: 0.00698988\n",
      "Epoch 3 | Step 1746300 | Avg Loss: 0.0151 | Grad Norm: 0.00946907\n",
      "Epoch 3 | Step 1746400 | Avg Loss: 0.0149 | Grad Norm: 0.00731421\n",
      "Epoch 3 | Step 1746500 | Avg Loss: 0.0147 | Grad Norm: 0.00944553\n",
      "Epoch 3 | Step 1746600 | Avg Loss: 0.0151 | Grad Norm: 0.00868178\n",
      "Epoch 3 | Step 1746700 | Avg Loss: 0.0152 | Grad Norm: 0.00803288\n",
      "Epoch 3 | Step 1746800 | Avg Loss: 0.0150 | Grad Norm: 0.00960091\n",
      "Epoch 3 | Step 1746900 | Avg Loss: 0.0152 | Grad Norm: 0.00887358\n",
      "Epoch 3 | Step 1747000 | Avg Loss: 0.0148 | Grad Norm: 0.00793715\n",
      "Epoch 3 | Step 1747100 | Avg Loss: 0.0149 | Grad Norm: 0.00745438\n",
      "Epoch 3 | Step 1747200 | Avg Loss: 0.0148 | Grad Norm: 0.00741003\n",
      "Epoch 3 | Step 1747300 | Avg Loss: 0.0150 | Grad Norm: 0.00851441\n",
      "Epoch 3 | Step 1747400 | Avg Loss: 0.0148 | Grad Norm: 0.00635924\n",
      "Epoch 3 | Step 1747500 | Avg Loss: 0.0149 | Grad Norm: 0.00816778\n",
      "Epoch 3 | Step 1747600 | Avg Loss: 0.0151 | Grad Norm: 0.00725138\n",
      "Epoch 3 | Step 1747700 | Avg Loss: 0.0145 | Grad Norm: 0.00821693\n",
      "Epoch 3 | Step 1747800 | Avg Loss: 0.0145 | Grad Norm: 0.00802088\n",
      "Epoch 3 | Step 1747900 | Avg Loss: 0.0145 | Grad Norm: 0.00756042\n",
      "Epoch 3 | Step 1748000 | Avg Loss: 0.0147 | Grad Norm: 0.00914293\n",
      "Epoch 3 | Step 1748100 | Avg Loss: 0.0147 | Grad Norm: 0.00650442\n",
      "Epoch 3 | Step 1748200 | Avg Loss: 0.0146 | Grad Norm: 0.00702654\n",
      "Epoch 3 | Step 1748300 | Avg Loss: 0.0145 | Grad Norm: 0.00645402\n",
      "Epoch 3 | Step 1748400 | Avg Loss: 0.0148 | Grad Norm: 0.00824555\n",
      "Epoch 3 | Step 1748500 | Avg Loss: 0.0148 | Grad Norm: 0.00861752\n",
      "Epoch 3 | Step 1748600 | Avg Loss: 0.0150 | Grad Norm: 0.00709392\n",
      "Epoch 3 | Step 1748700 | Avg Loss: 0.0145 | Grad Norm: 0.00859250\n",
      "Epoch 3 | Step 1748800 | Avg Loss: 0.0146 | Grad Norm: 0.00743913\n",
      "Epoch 3 | Step 1748900 | Avg Loss: 0.0142 | Grad Norm: 0.00786895\n",
      "Epoch 3 | Step 1749000 | Avg Loss: 0.0143 | Grad Norm: 0.00773126\n",
      "Epoch 3 | Step 1749100 | Avg Loss: 0.0145 | Grad Norm: 0.00772818\n",
      "Epoch 3 | Step 1749200 | Avg Loss: 0.0148 | Grad Norm: 0.00801339\n",
      "Epoch 3 | Step 1749300 | Avg Loss: 0.0148 | Grad Norm: 0.01001905\n",
      "Epoch 3 | Step 1749400 | Avg Loss: 0.0151 | Grad Norm: 0.00940717\n",
      "Epoch 3 | Step 1749500 | Avg Loss: 0.0151 | Grad Norm: 0.00765957\n",
      "Epoch 3 | Step 1749600 | Avg Loss: 0.0152 | Grad Norm: 0.00953343\n",
      "Epoch 3 | Step 1749700 | Avg Loss: 0.0151 | Grad Norm: 0.00878798\n",
      "Epoch 3 | Step 1749800 | Avg Loss: 0.0152 | Grad Norm: 0.01102709\n",
      "Epoch 3 | Step 1749900 | Avg Loss: 0.0156 | Grad Norm: 0.00795217\n",
      "Epoch 3 | Step 1750000 | Avg Loss: 0.0154 | Grad Norm: 0.00781592\n",
      "Epoch 3 | Step 1750100 | Avg Loss: 0.0154 | Grad Norm: 0.00814945\n",
      "Epoch 3 | Step 1750200 | Avg Loss: 0.0155 | Grad Norm: 0.00901876\n",
      "Epoch 3 | Step 1750300 | Avg Loss: 0.0156 | Grad Norm: 0.00848260\n",
      "Epoch 3 | Step 1750400 | Avg Loss: 0.0157 | Grad Norm: 0.00938261\n",
      "Epoch 3 | Step 1750500 | Avg Loss: 0.0159 | Grad Norm: 0.01015519\n",
      "Epoch 3 | Step 1750600 | Avg Loss: 0.0158 | Grad Norm: 0.00908329\n",
      "Epoch 3 | Step 1750700 | Avg Loss: 0.0155 | Grad Norm: 0.00819152\n",
      "Epoch 3 | Step 1750800 | Avg Loss: 0.0154 | Grad Norm: 0.00733172\n",
      "Epoch 3 | Step 1750900 | Avg Loss: 0.0149 | Grad Norm: 0.00903288\n",
      "Epoch 3 | Step 1751000 | Avg Loss: 0.0153 | Grad Norm: 0.00902509\n",
      "Epoch 3 | Step 1751100 | Avg Loss: 0.0152 | Grad Norm: 0.00936814\n",
      "Epoch 3 | Step 1751200 | Avg Loss: 0.0155 | Grad Norm: 0.00847824\n",
      "Epoch 3 | Step 1751300 | Avg Loss: 0.0152 | Grad Norm: 0.00745316\n",
      "Epoch 3 | Step 1751400 | Avg Loss: 0.0150 | Grad Norm: 0.00737142\n",
      "Epoch 3 | Step 1751500 | Avg Loss: 0.0150 | Grad Norm: 0.00875124\n",
      "Epoch 3 | Step 1751600 | Avg Loss: 0.0153 | Grad Norm: 0.00917290\n",
      "Epoch 3 | Step 1751700 | Avg Loss: 0.0154 | Grad Norm: 0.00950113\n",
      "Epoch 3 | Step 1751800 | Avg Loss: 0.0155 | Grad Norm: 0.00938349\n",
      "Epoch 3 | Step 1751900 | Avg Loss: 0.0151 | Grad Norm: 0.00755534\n",
      "Epoch 3 | Step 1752000 | Avg Loss: 0.0150 | Grad Norm: 0.00900588\n",
      "Epoch 3 | Step 1752100 | Avg Loss: 0.0149 | Grad Norm: 0.00870218\n",
      "Epoch 3 | Step 1752200 | Avg Loss: 0.0149 | Grad Norm: 0.00682152\n",
      "Epoch 3 | Step 1752300 | Avg Loss: 0.0148 | Grad Norm: 0.00786553\n",
      "Epoch 3 | Step 1752400 | Avg Loss: 0.0147 | Grad Norm: 0.00832622\n",
      "Epoch 3 | Step 1752500 | Avg Loss: 0.0148 | Grad Norm: 0.00665511\n",
      "Epoch 3 | Step 1752600 | Avg Loss: 0.0147 | Grad Norm: 0.00856810\n",
      "Epoch 3 | Step 1752700 | Avg Loss: 0.0148 | Grad Norm: 0.00822151\n",
      "Epoch 3 | Step 1752800 | Avg Loss: 0.0152 | Grad Norm: 0.00765675\n",
      "Epoch 3 | Step 1752900 | Avg Loss: 0.0156 | Grad Norm: 0.00779204\n",
      "Epoch 3 | Step 1753000 | Avg Loss: 0.0154 | Grad Norm: 0.01019851\n",
      "Epoch 3 | Step 1753100 | Avg Loss: 0.0153 | Grad Norm: 0.00962577\n",
      "Epoch 3 | Step 1753200 | Avg Loss: 0.0149 | Grad Norm: 0.00835829\n",
      "Epoch 3 | Step 1753300 | Avg Loss: 0.0148 | Grad Norm: 0.00725137\n",
      "Epoch 3 | Step 1753400 | Avg Loss: 0.0148 | Grad Norm: 0.00649223\n",
      "Epoch 3 | Step 1753500 | Avg Loss: 0.0149 | Grad Norm: 0.00854635\n",
      "Epoch 3 | Step 1753600 | Avg Loss: 0.0149 | Grad Norm: 0.00782581\n",
      "Epoch 3 | Step 1753700 | Avg Loss: 0.0150 | Grad Norm: 0.00913141\n",
      "Epoch 3 | Step 1753800 | Avg Loss: 0.0146 | Grad Norm: 0.00815026\n",
      "Epoch 3 | Step 1753900 | Avg Loss: 0.0150 | Grad Norm: 0.00730281\n",
      "Epoch 3 | Step 1754000 | Avg Loss: 0.0151 | Grad Norm: 0.00922700\n",
      "Epoch 3 | Step 1754100 | Avg Loss: 0.0147 | Grad Norm: 0.00840911\n",
      "Epoch 3 | Step 1754200 | Avg Loss: 0.0148 | Grad Norm: 0.00908027\n",
      "Epoch 3 | Step 1754300 | Avg Loss: 0.0149 | Grad Norm: 0.00855055\n",
      "Epoch 3 | Step 1754400 | Avg Loss: 0.0153 | Grad Norm: 0.00794056\n",
      "Epoch 3 | Step 1754500 | Avg Loss: 0.0152 | Grad Norm: 0.00858036\n",
      "Epoch 3 | Step 1754600 | Avg Loss: 0.0152 | Grad Norm: 0.00864261\n",
      "Epoch 3 | Step 1754700 | Avg Loss: 0.0152 | Grad Norm: 0.00816807\n",
      "Epoch 3 | Step 1754800 | Avg Loss: 0.0151 | Grad Norm: 0.00905348\n",
      "Epoch 3 | Step 1754900 | Avg Loss: 0.0150 | Grad Norm: 0.00847403\n",
      "Epoch 3 | Step 1755000 | Avg Loss: 0.0155 | Grad Norm: 0.00874153\n",
      "Epoch 3 | Step 1755100 | Avg Loss: 0.0152 | Grad Norm: 0.00770453\n",
      "Epoch 3 | Step 1755200 | Avg Loss: 0.0148 | Grad Norm: 0.00847129\n",
      "Epoch 3 | Step 1755300 | Avg Loss: 0.0150 | Grad Norm: 0.00771697\n",
      "Epoch 3 | Step 1755400 | Avg Loss: 0.0152 | Grad Norm: 0.00836730\n",
      "Epoch 3 | Step 1755500 | Avg Loss: 0.0155 | Grad Norm: 0.00851971\n",
      "Epoch 3 | Step 1755600 | Avg Loss: 0.0151 | Grad Norm: 0.00832184\n",
      "Epoch 3 | Step 1755700 | Avg Loss: 0.0154 | Grad Norm: 0.00900294\n",
      "Epoch 3 | Step 1755800 | Avg Loss: 0.0154 | Grad Norm: 0.00817522\n",
      "Epoch 3 | Step 1755900 | Avg Loss: 0.0154 | Grad Norm: 0.00945073\n",
      "Epoch 3 | Step 1756000 | Avg Loss: 0.0153 | Grad Norm: 0.00762791\n",
      "Epoch 3 | Step 1756100 | Avg Loss: 0.0155 | Grad Norm: 0.00774905\n",
      "Epoch 3 | Step 1756200 | Avg Loss: 0.0152 | Grad Norm: 0.00924417\n",
      "Epoch 3 | Step 1756300 | Avg Loss: 0.0150 | Grad Norm: 0.00817591\n",
      "Epoch 3 | Step 1756400 | Avg Loss: 0.0150 | Grad Norm: 0.00855444\n",
      "Epoch 3 | Step 1756500 | Avg Loss: 0.0155 | Grad Norm: 0.00786603\n",
      "Epoch 3 | Step 1756600 | Avg Loss: 0.0153 | Grad Norm: 0.00812553\n",
      "Epoch 3 | Step 1756700 | Avg Loss: 0.0153 | Grad Norm: 0.00728779\n",
      "Epoch 3 | Step 1756800 | Avg Loss: 0.0150 | Grad Norm: 0.00950406\n",
      "Epoch 3 | Step 1756900 | Avg Loss: 0.0147 | Grad Norm: 0.00729669\n",
      "Epoch 3 | Step 1757000 | Avg Loss: 0.0146 | Grad Norm: 0.00783170\n",
      "Epoch 3 | Step 1757100 | Avg Loss: 0.0150 | Grad Norm: 0.00742391\n",
      "Epoch 3 | Step 1757200 | Avg Loss: 0.0151 | Grad Norm: 0.00676849\n",
      "Epoch 3 | Step 1757300 | Avg Loss: 0.0148 | Grad Norm: 0.00848975\n",
      "Epoch 3 | Step 1757400 | Avg Loss: 0.0151 | Grad Norm: 0.00750525\n",
      "Epoch 3 | Step 1757500 | Avg Loss: 0.0148 | Grad Norm: 0.00875755\n",
      "Epoch 3 | Step 1757600 | Avg Loss: 0.0152 | Grad Norm: 0.00770710\n",
      "Epoch 3 | Step 1757700 | Avg Loss: 0.0152 | Grad Norm: 0.00819887\n",
      "Epoch 3 | Step 1757800 | Avg Loss: 0.0154 | Grad Norm: 0.00794131\n",
      "Epoch 3 | Step 1757900 | Avg Loss: 0.0159 | Grad Norm: 0.00884017\n",
      "Epoch 3 | Step 1758000 | Avg Loss: 0.0158 | Grad Norm: 0.00797369\n",
      "Epoch 3 | Step 1758100 | Avg Loss: 0.0158 | Grad Norm: 0.00869817\n",
      "Epoch 3 | Step 1758200 | Avg Loss: 0.0152 | Grad Norm: 0.00790869\n",
      "Epoch 3 | Step 1758300 | Avg Loss: 0.0152 | Grad Norm: 0.00906768\n",
      "Epoch 3 | Step 1758400 | Avg Loss: 0.0153 | Grad Norm: 0.00764141\n",
      "Epoch 3 | Step 1758500 | Avg Loss: 0.0149 | Grad Norm: 0.00701798\n",
      "Epoch 3 | Step 1758600 | Avg Loss: 0.0153 | Grad Norm: 0.00832306\n",
      "Epoch 3 | Step 1758700 | Avg Loss: 0.0152 | Grad Norm: 0.00861719\n",
      "Epoch 3 | Step 1758800 | Avg Loss: 0.0150 | Grad Norm: 0.00743436\n",
      "Epoch 3 | Step 1758900 | Avg Loss: 0.0148 | Grad Norm: 0.00758908\n",
      "Epoch 3 | Step 1759000 | Avg Loss: 0.0147 | Grad Norm: 0.00885647\n",
      "Epoch 3 | Step 1759100 | Avg Loss: 0.0149 | Grad Norm: 0.00751393\n",
      "Epoch 3 | Step 1759200 | Avg Loss: 0.0146 | Grad Norm: 0.00810303\n",
      "Epoch 3 | Step 1759300 | Avg Loss: 0.0150 | Grad Norm: 0.00812467\n",
      "Epoch 3 | Step 1759400 | Avg Loss: 0.0150 | Grad Norm: 0.00779866\n",
      "Epoch 3 | Step 1759500 | Avg Loss: 0.0150 | Grad Norm: 0.00708658\n",
      "Epoch 3 | Step 1759600 | Avg Loss: 0.0152 | Grad Norm: 0.00799725\n",
      "Epoch 3 | Step 1759700 | Avg Loss: 0.0150 | Grad Norm: 0.00834538\n",
      "Epoch 3 | Step 1759800 | Avg Loss: 0.0149 | Grad Norm: 0.00831019\n",
      "Epoch 3 | Step 1759900 | Avg Loss: 0.0147 | Grad Norm: 0.00801016\n",
      "Epoch 3 | Step 1760000 | Avg Loss: 0.0146 | Grad Norm: 0.00750306\n",
      "Epoch 3 | Step 1760100 | Avg Loss: 0.0150 | Grad Norm: 0.00793471\n",
      "Epoch 3 | Step 1760200 | Avg Loss: 0.0151 | Grad Norm: 0.00844681\n",
      "Epoch 3 | Step 1760300 | Avg Loss: 0.0152 | Grad Norm: 0.00764441\n",
      "Epoch 3 | Step 1760400 | Avg Loss: 0.0155 | Grad Norm: 0.00716130\n",
      "Epoch 3 | Step 1760500 | Avg Loss: 0.0156 | Grad Norm: 0.00973864\n",
      "Epoch 3 | Step 1760600 | Avg Loss: 0.0153 | Grad Norm: 0.00861359\n",
      "Epoch 3 | Step 1760700 | Avg Loss: 0.0152 | Grad Norm: 0.00813335\n",
      "Epoch 3 | Step 1760800 | Avg Loss: 0.0148 | Grad Norm: 0.00798768\n",
      "Epoch 3 | Step 1760900 | Avg Loss: 0.0152 | Grad Norm: 0.00755754\n",
      "Epoch 3 | Step 1761000 | Avg Loss: 0.0152 | Grad Norm: 0.00766043\n",
      "Epoch 3 | Step 1761100 | Avg Loss: 0.0152 | Grad Norm: 0.00792740\n",
      "Epoch 3 | Step 1761200 | Avg Loss: 0.0149 | Grad Norm: 0.00870440\n",
      "Epoch 3 | Step 1761300 | Avg Loss: 0.0149 | Grad Norm: 0.00769505\n",
      "Epoch 3 | Step 1761400 | Avg Loss: 0.0152 | Grad Norm: 0.00799390\n",
      "Epoch 3 | Step 1761500 | Avg Loss: 0.0152 | Grad Norm: 0.00876593\n",
      "Epoch 3 | Step 1761600 | Avg Loss: 0.0152 | Grad Norm: 0.00766849\n",
      "Epoch 3 | Step 1761700 | Avg Loss: 0.0152 | Grad Norm: 0.00862936\n",
      "Epoch 3 | Step 1761800 | Avg Loss: 0.0152 | Grad Norm: 0.00725071\n",
      "Epoch 3 | Step 1761900 | Avg Loss: 0.0155 | Grad Norm: 0.00948715\n",
      "Epoch 3 | Step 1762000 | Avg Loss: 0.0155 | Grad Norm: 0.01064654\n",
      "Epoch 3 | Step 1762100 | Avg Loss: 0.0156 | Grad Norm: 0.00848997\n",
      "Epoch 3 | Step 1762200 | Avg Loss: 0.0156 | Grad Norm: 0.00807681\n",
      "Epoch 3 | Step 1762300 | Avg Loss: 0.0153 | Grad Norm: 0.00822644\n",
      "Epoch 3 | Step 1762400 | Avg Loss: 0.0148 | Grad Norm: 0.00811672\n",
      "Epoch 3 | Step 1762500 | Avg Loss: 0.0150 | Grad Norm: 0.00768293\n",
      "Epoch 3 | Step 1762600 | Avg Loss: 0.0148 | Grad Norm: 0.00984200\n",
      "Epoch 3 | Step 1762700 | Avg Loss: 0.0146 | Grad Norm: 0.00774685\n",
      "Epoch 3 | Step 1762800 | Avg Loss: 0.0149 | Grad Norm: 0.00811345\n",
      "Epoch 3 | Step 1762900 | Avg Loss: 0.0149 | Grad Norm: 0.00830894\n",
      "Epoch 3 | Step 1763000 | Avg Loss: 0.0151 | Grad Norm: 0.00804681\n",
      "Epoch 3 | Step 1763100 | Avg Loss: 0.0151 | Grad Norm: 0.00905485\n",
      "Epoch 3 | Step 1763200 | Avg Loss: 0.0148 | Grad Norm: 0.00949711\n",
      "Epoch 3 | Step 1763300 | Avg Loss: 0.0153 | Grad Norm: 0.01062694\n",
      "Epoch 3 | Step 1763400 | Avg Loss: 0.0151 | Grad Norm: 0.00817509\n",
      "Epoch 3 | Step 1763500 | Avg Loss: 0.0150 | Grad Norm: 0.00998788\n",
      "Epoch 3 | Step 1763600 | Avg Loss: 0.0152 | Grad Norm: 0.00913403\n",
      "Epoch 3 | Step 1763700 | Avg Loss: 0.0152 | Grad Norm: 0.00742115\n",
      "Epoch 3 | Step 1763800 | Avg Loss: 0.0153 | Grad Norm: 0.00879630\n",
      "Epoch 3 | Step 1763900 | Avg Loss: 0.0148 | Grad Norm: 0.00816994\n",
      "Epoch 3 | Step 1764000 | Avg Loss: 0.0151 | Grad Norm: 0.00896396\n",
      "Epoch 3 | Step 1764100 | Avg Loss: 0.0154 | Grad Norm: 0.00958304\n",
      "Epoch 3 | Step 1764200 | Avg Loss: 0.0153 | Grad Norm: 0.00713144\n",
      "Epoch 3 | Step 1764300 | Avg Loss: 0.0148 | Grad Norm: 0.00701025\n",
      "Epoch 3 | Step 1764400 | Avg Loss: 0.0148 | Grad Norm: 0.00905022\n",
      "Epoch 3 | Step 1764500 | Avg Loss: 0.0150 | Grad Norm: 0.00757206\n",
      "Epoch 3 | Step 1764600 | Avg Loss: 0.0150 | Grad Norm: 0.00817000\n",
      "Epoch 3 | Step 1764700 | Avg Loss: 0.0152 | Grad Norm: 0.00728594\n",
      "Epoch 3 | Step 1764800 | Avg Loss: 0.0149 | Grad Norm: 0.00908276\n",
      "Epoch 3 | Step 1764900 | Avg Loss: 0.0149 | Grad Norm: 0.00806993\n",
      "Epoch 3 | Step 1765000 | Avg Loss: 0.0149 | Grad Norm: 0.00807535\n",
      "Epoch 3 | Step 1765100 | Avg Loss: 0.0152 | Grad Norm: 0.00952591\n",
      "Epoch 3 | Step 1765200 | Avg Loss: 0.0153 | Grad Norm: 0.00825388\n",
      "Epoch 3 | Step 1765300 | Avg Loss: 0.0162 | Grad Norm: 0.00918900\n",
      "Epoch 3 | Step 1765400 | Avg Loss: 0.0159 | Grad Norm: 0.00777327\n",
      "Epoch 3 | Step 1765500 | Avg Loss: 0.0159 | Grad Norm: 0.00746613\n",
      "Epoch 3 | Step 1765600 | Avg Loss: 0.0157 | Grad Norm: 0.00669277\n",
      "Epoch 3 | Step 1765700 | Avg Loss: 0.0157 | Grad Norm: 0.00828831\n",
      "Epoch 3 | Step 1765800 | Avg Loss: 0.0152 | Grad Norm: 0.00817060\n",
      "Epoch 3 | Step 1765900 | Avg Loss: 0.0151 | Grad Norm: 0.00808598\n",
      "Epoch 3 | Step 1766000 | Avg Loss: 0.0152 | Grad Norm: 0.00754568\n",
      "Epoch 3 | Step 1766100 | Avg Loss: 0.0152 | Grad Norm: 0.00778153\n",
      "Epoch 3 | Step 1766200 | Avg Loss: 0.0159 | Grad Norm: 0.00944405\n",
      "Epoch 3 | Step 1766300 | Avg Loss: 0.0155 | Grad Norm: 0.00873945\n",
      "Epoch 3 | Step 1766400 | Avg Loss: 0.0152 | Grad Norm: 0.00807305\n",
      "Epoch 3 | Step 1766500 | Avg Loss: 0.0150 | Grad Norm: 0.00713572\n",
      "Epoch 3 | Step 1766600 | Avg Loss: 0.0150 | Grad Norm: 0.00841641\n",
      "Epoch 3 | Step 1766700 | Avg Loss: 0.0154 | Grad Norm: 0.00851093\n",
      "Epoch 3 | Step 1766800 | Avg Loss: 0.0151 | Grad Norm: 0.00871673\n",
      "Epoch 3 | Step 1766900 | Avg Loss: 0.0151 | Grad Norm: 0.00749602\n",
      "Epoch 3 | Step 1767000 | Avg Loss: 0.0151 | Grad Norm: 0.00712669\n",
      "Epoch 3 | Step 1767100 | Avg Loss: 0.0153 | Grad Norm: 0.00910069\n",
      "Epoch 3 | Step 1767200 | Avg Loss: 0.0151 | Grad Norm: 0.00721740\n",
      "Epoch 3 | Step 1767300 | Avg Loss: 0.0150 | Grad Norm: 0.00821478\n",
      "Epoch 3 | Step 1767400 | Avg Loss: 0.0148 | Grad Norm: 0.00667174\n",
      "Epoch 3 | Step 1767500 | Avg Loss: 0.0148 | Grad Norm: 0.00785921\n",
      "Epoch 3 | Step 1767600 | Avg Loss: 0.0150 | Grad Norm: 0.00740403\n",
      "Epoch 3 | Step 1767700 | Avg Loss: 0.0147 | Grad Norm: 0.00677705\n",
      "Epoch 3 | Step 1767800 | Avg Loss: 0.0150 | Grad Norm: 0.00869997\n",
      "Epoch 3 | Step 1767900 | Avg Loss: 0.0152 | Grad Norm: 0.00717970\n",
      "Epoch 3 | Step 1768000 | Avg Loss: 0.0151 | Grad Norm: 0.00706394\n",
      "Epoch 3 | Step 1768100 | Avg Loss: 0.0149 | Grad Norm: 0.00735902\n",
      "Epoch 3 | Step 1768200 | Avg Loss: 0.0148 | Grad Norm: 0.00912357\n",
      "Epoch 3 | Step 1768300 | Avg Loss: 0.0153 | Grad Norm: 0.00790665\n",
      "Epoch 3 | Step 1768400 | Avg Loss: 0.0152 | Grad Norm: 0.00755320\n",
      "Epoch 3 | Step 1768500 | Avg Loss: 0.0152 | Grad Norm: 0.00864832\n",
      "Epoch 3 | Step 1768600 | Avg Loss: 0.0154 | Grad Norm: 0.00848767\n",
      "Epoch 3 | Step 1768700 | Avg Loss: 0.0151 | Grad Norm: 0.00744505\n",
      "Epoch 3 | Step 1768800 | Avg Loss: 0.0154 | Grad Norm: 0.00885096\n",
      "Epoch 3 | Step 1768900 | Avg Loss: 0.0152 | Grad Norm: 0.00765517\n",
      "Epoch 3 | Step 1769000 | Avg Loss: 0.0152 | Grad Norm: 0.00824606\n",
      "Epoch 3 | Step 1769100 | Avg Loss: 0.0151 | Grad Norm: 0.01379700\n",
      "Epoch 3 | Step 1769200 | Avg Loss: 0.0154 | Grad Norm: 0.00758310\n",
      "Epoch 3 | Step 1769300 | Avg Loss: 0.0153 | Grad Norm: 0.00709981\n",
      "Epoch 3 | Step 1769400 | Avg Loss: 0.0152 | Grad Norm: 0.00812850\n",
      "Epoch 3 | Step 1769500 | Avg Loss: 0.0154 | Grad Norm: 0.00940645\n",
      "Epoch 3 | Step 1769600 | Avg Loss: 0.0154 | Grad Norm: 0.00856157\n",
      "Epoch 3 | Step 1769700 | Avg Loss: 0.0155 | Grad Norm: 0.00889093\n",
      "Epoch 3 | Step 1769800 | Avg Loss: 0.0151 | Grad Norm: 0.00940605\n",
      "Epoch 3 | Step 1769900 | Avg Loss: 0.0153 | Grad Norm: 0.00835913\n",
      "Epoch 3 | Step 1770000 | Avg Loss: 0.0151 | Grad Norm: 0.00843457\n",
      "Epoch 3 | Step 1770100 | Avg Loss: 0.0149 | Grad Norm: 0.00694195\n",
      "Epoch 3 | Step 1770200 | Avg Loss: 0.0147 | Grad Norm: 0.00803848\n",
      "Epoch 3 | Step 1770300 | Avg Loss: 0.0151 | Grad Norm: 0.00972375\n",
      "Epoch 3 | Step 1770400 | Avg Loss: 0.0151 | Grad Norm: 0.00750918\n",
      "Epoch 3 | Step 1770500 | Avg Loss: 0.0152 | Grad Norm: 0.00893761\n",
      "Epoch 3 | Step 1770600 | Avg Loss: 0.0151 | Grad Norm: 0.00897315\n",
      "Epoch 3 | Step 1770700 | Avg Loss: 0.0155 | Grad Norm: 0.00742067\n",
      "Epoch 3 | Step 1770800 | Avg Loss: 0.0155 | Grad Norm: 0.00914872\n",
      "Epoch 3 | Step 1770900 | Avg Loss: 0.0153 | Grad Norm: 0.00841526\n",
      "Epoch 3 | Step 1771000 | Avg Loss: 0.0154 | Grad Norm: 0.00765279\n",
      "Epoch 3 | Step 1771100 | Avg Loss: 0.0155 | Grad Norm: 0.00873154\n",
      "Epoch 3 | Step 1771200 | Avg Loss: 0.0154 | Grad Norm: 0.00804411\n",
      "Epoch 3 | Step 1771300 | Avg Loss: 0.0150 | Grad Norm: 0.00696382\n",
      "Epoch 3 | Step 1771400 | Avg Loss: 0.0154 | Grad Norm: 0.00946119\n",
      "Epoch 3 | Step 1771500 | Avg Loss: 0.0153 | Grad Norm: 0.00724420\n",
      "Epoch 3 | Step 1771600 | Avg Loss: 0.0155 | Grad Norm: 0.00947470\n",
      "Epoch 3 | Step 1771700 | Avg Loss: 0.0154 | Grad Norm: 0.00905952\n",
      "Epoch 3 | Step 1771800 | Avg Loss: 0.0156 | Grad Norm: 0.00901073\n",
      "Epoch 3 | Step 1771900 | Avg Loss: 0.0158 | Grad Norm: 0.00893453\n",
      "Epoch 3 | Step 1772000 | Avg Loss: 0.0153 | Grad Norm: 0.00875692\n",
      "Epoch 3 | Step 1772100 | Avg Loss: 0.0151 | Grad Norm: 0.00769304\n",
      "Epoch 3 | Step 1772200 | Avg Loss: 0.0147 | Grad Norm: 0.00821340\n",
      "Epoch 3 | Step 1772300 | Avg Loss: 0.0149 | Grad Norm: 0.00851585\n",
      "Epoch 3 | Step 1772400 | Avg Loss: 0.0151 | Grad Norm: 0.00869803\n",
      "Epoch 3 | Step 1772500 | Avg Loss: 0.0152 | Grad Norm: 0.00848552\n",
      "Epoch 3 | Step 1772600 | Avg Loss: 0.0152 | Grad Norm: 0.00833546\n",
      "Epoch 3 | Step 1772700 | Avg Loss: 0.0152 | Grad Norm: 0.00860045\n",
      "Epoch 3 | Step 1772800 | Avg Loss: 0.0153 | Grad Norm: 0.00808901\n",
      "Epoch 3 | Step 1772900 | Avg Loss: 0.0154 | Grad Norm: 0.00890830\n",
      "Epoch 3 | Step 1773000 | Avg Loss: 0.0151 | Grad Norm: 0.00751136\n",
      "Epoch 3 | Step 1773100 | Avg Loss: 0.0151 | Grad Norm: 0.00723328\n",
      "Epoch 3 | Step 1773200 | Avg Loss: 0.0152 | Grad Norm: 0.00789774\n",
      "Epoch 3 | Step 1773300 | Avg Loss: 0.0154 | Grad Norm: 0.00788367\n",
      "Epoch 3 | Step 1773400 | Avg Loss: 0.0151 | Grad Norm: 0.01031417\n",
      "Epoch 3 | Step 1773500 | Avg Loss: 0.0145 | Grad Norm: 0.00746459\n",
      "Epoch 3 | Step 1773600 | Avg Loss: 0.0145 | Grad Norm: 0.00773337\n",
      "Epoch 3 | Step 1773700 | Avg Loss: 0.0147 | Grad Norm: 0.00826417\n",
      "Epoch 3 | Step 1773800 | Avg Loss: 0.0145 | Grad Norm: 0.00738244\n",
      "Epoch 3 | Step 1773900 | Avg Loss: 0.0150 | Grad Norm: 0.00733083\n",
      "Epoch 3 | Step 1774000 | Avg Loss: 0.0146 | Grad Norm: 0.00780958\n",
      "Epoch 3 | Step 1774100 | Avg Loss: 0.0148 | Grad Norm: 0.00858405\n",
      "Epoch 3 | Step 1774200 | Avg Loss: 0.0148 | Grad Norm: 0.00766408\n",
      "Epoch 3 | Step 1774300 | Avg Loss: 0.0148 | Grad Norm: 0.00739694\n",
      "Epoch 3 | Step 1774400 | Avg Loss: 0.0150 | Grad Norm: 0.00709018\n",
      "Epoch 3 | Step 1774500 | Avg Loss: 0.0156 | Grad Norm: 0.00828998\n",
      "Epoch 3 | Step 1774600 | Avg Loss: 0.0156 | Grad Norm: 0.00714972\n",
      "Epoch 3 | Step 1774700 | Avg Loss: 0.0154 | Grad Norm: 0.00755748\n",
      "Epoch 3 | Step 1774800 | Avg Loss: 0.0153 | Grad Norm: 0.00849542\n",
      "Epoch 3 | Step 1774900 | Avg Loss: 0.0154 | Grad Norm: 0.00802555\n",
      "Epoch 3 | Step 1775000 | Avg Loss: 0.0154 | Grad Norm: 0.00930186\n",
      "Epoch 3 | Step 1775100 | Avg Loss: 0.0154 | Grad Norm: 0.00768784\n",
      "Epoch 3 | Step 1775200 | Avg Loss: 0.0150 | Grad Norm: 0.00694134\n",
      "Epoch 3 | Step 1775300 | Avg Loss: 0.0152 | Grad Norm: 0.00848996\n",
      "Epoch 3 | Step 1775400 | Avg Loss: 0.0149 | Grad Norm: 0.00926503\n",
      "Epoch 3 | Step 1775500 | Avg Loss: 0.0151 | Grad Norm: 0.00888398\n",
      "Epoch 3 | Step 1775600 | Avg Loss: 0.0147 | Grad Norm: 0.00785964\n",
      "Epoch 3 | Step 1775700 | Avg Loss: 0.0148 | Grad Norm: 0.00803922\n",
      "Epoch 3 | Step 1775800 | Avg Loss: 0.0147 | Grad Norm: 0.00824451\n",
      "Epoch 3 | Step 1775900 | Avg Loss: 0.0144 | Grad Norm: 0.00913647\n",
      "Epoch 3 | Step 1776000 | Avg Loss: 0.0148 | Grad Norm: 0.01013223\n",
      "Epoch 3 | Step 1776100 | Avg Loss: 0.0148 | Grad Norm: 0.00713809\n",
      "Epoch 3 | Step 1776200 | Avg Loss: 0.0149 | Grad Norm: 0.00803269\n",
      "Epoch 3 | Step 1776300 | Avg Loss: 0.0153 | Grad Norm: 0.00856352\n",
      "Epoch 3 | Step 1776400 | Avg Loss: 0.0151 | Grad Norm: 0.01166888\n",
      "Epoch 3 | Step 1776500 | Avg Loss: 0.0150 | Grad Norm: 0.01066029\n",
      "Epoch 3 | Step 1776600 | Avg Loss: 0.0152 | Grad Norm: 0.00760522\n",
      "Epoch 3 | Step 1776700 | Avg Loss: 0.0153 | Grad Norm: 0.00789752\n",
      "Epoch 3 | Step 1776800 | Avg Loss: 0.0154 | Grad Norm: 0.00781606\n",
      "Epoch 3 | Step 1776900 | Avg Loss: 0.0153 | Grad Norm: 0.00692111\n",
      "Epoch 3 | Step 1777000 | Avg Loss: 0.0151 | Grad Norm: 0.00678052\n",
      "Epoch 3 | Step 1777100 | Avg Loss: 0.0146 | Grad Norm: 0.00908663\n",
      "Epoch 3 | Step 1777200 | Avg Loss: 0.0147 | Grad Norm: 0.00765984\n",
      "Epoch 3 | Step 1777300 | Avg Loss: 0.0146 | Grad Norm: 0.00856638\n",
      "Epoch 3 | Step 1777400 | Avg Loss: 0.0147 | Grad Norm: 0.01041764\n",
      "Epoch 3 | Step 1777500 | Avg Loss: 0.0151 | Grad Norm: 0.00824787\n",
      "Epoch 3 | Step 1777600 | Avg Loss: 0.0152 | Grad Norm: 0.00765509\n",
      "Epoch 3 | Step 1777700 | Avg Loss: 0.0150 | Grad Norm: 0.00701509\n",
      "Epoch 3 | Step 1777800 | Avg Loss: 0.0149 | Grad Norm: 0.00926889\n",
      "Epoch 3 | Step 1777900 | Avg Loss: 0.0146 | Grad Norm: 0.00768422\n",
      "Epoch 3 | Step 1778000 | Avg Loss: 0.0148 | Grad Norm: 0.00828826\n",
      "Epoch 3 | Step 1778100 | Avg Loss: 0.0151 | Grad Norm: 0.00672799\n",
      "Epoch 3 | Step 1778200 | Avg Loss: 0.0149 | Grad Norm: 0.00746498\n",
      "Epoch 3 | Step 1778300 | Avg Loss: 0.0145 | Grad Norm: 0.00840318\n",
      "Epoch 3 | Step 1778400 | Avg Loss: 0.0147 | Grad Norm: 0.00666441\n",
      "Epoch 3 | Step 1778500 | Avg Loss: 0.0143 | Grad Norm: 0.00781085\n",
      "Epoch 3 | Step 1778600 | Avg Loss: 0.0148 | Grad Norm: 0.01391817\n",
      "Epoch 3 | Step 1778700 | Avg Loss: 0.0150 | Grad Norm: 0.00727782\n",
      "Epoch 3 | Step 1778800 | Avg Loss: 0.0151 | Grad Norm: 0.00724827\n",
      "Epoch 3 | Step 1778900 | Avg Loss: 0.0151 | Grad Norm: 0.00843820\n",
      "Epoch 3 | Step 1779000 | Avg Loss: 0.0149 | Grad Norm: 0.00744266\n",
      "Epoch 3 | Step 1779100 | Avg Loss: 0.0151 | Grad Norm: 0.01225576\n",
      "Epoch 3 | Step 1779200 | Avg Loss: 0.0150 | Grad Norm: 0.00887346\n",
      "Epoch 3 | Step 1779300 | Avg Loss: 0.0150 | Grad Norm: 0.00901636\n",
      "Epoch 3 | Step 1779400 | Avg Loss: 0.0148 | Grad Norm: 0.00737802\n",
      "Epoch 3 | Step 1779500 | Avg Loss: 0.0149 | Grad Norm: 0.00747969\n",
      "Epoch 3 | Step 1779600 | Avg Loss: 0.0154 | Grad Norm: 0.00888383\n",
      "Epoch 3 | Step 1779700 | Avg Loss: 0.0154 | Grad Norm: 0.00854074\n",
      "Epoch 3 | Step 1779800 | Avg Loss: 0.0153 | Grad Norm: 0.00988116\n",
      "Epoch 3 | Step 1779900 | Avg Loss: 0.0154 | Grad Norm: 0.00795203\n",
      "Epoch 3 | Step 1780000 | Avg Loss: 0.0153 | Grad Norm: 0.00845498\n",
      "Epoch 3 | Step 1780100 | Avg Loss: 0.0155 | Grad Norm: 0.00845392\n",
      "Epoch 3 | Step 1780200 | Avg Loss: 0.0155 | Grad Norm: 0.00853692\n",
      "Epoch 3 | Step 1780300 | Avg Loss: 0.0156 | Grad Norm: 0.00898896\n",
      "Epoch 3 | Step 1780400 | Avg Loss: 0.0159 | Grad Norm: 0.00712118\n",
      "Epoch 3 | Step 1780500 | Avg Loss: 0.0157 | Grad Norm: 0.00783561\n",
      "Epoch 3 | Step 1780600 | Avg Loss: 0.0151 | Grad Norm: 0.00904540\n",
      "Epoch 3 | Step 1780700 | Avg Loss: 0.0152 | Grad Norm: 0.00791155\n",
      "Epoch 3 | Step 1780800 | Avg Loss: 0.0151 | Grad Norm: 0.00637502\n",
      "Epoch 3 | Step 1780900 | Avg Loss: 0.0154 | Grad Norm: 0.00804676\n",
      "Epoch 3 | Step 1781000 | Avg Loss: 0.0151 | Grad Norm: 0.00799575\n",
      "Epoch 3 | Step 1781100 | Avg Loss: 0.0147 | Grad Norm: 0.00713495\n",
      "Epoch 3 | Step 1781200 | Avg Loss: 0.0145 | Grad Norm: 0.00846691\n",
      "Epoch 3 | Step 1781300 | Avg Loss: 0.0148 | Grad Norm: 0.00695628\n",
      "Epoch 3 | Step 1781400 | Avg Loss: 0.0150 | Grad Norm: 0.00930793\n",
      "Epoch 3 | Step 1781500 | Avg Loss: 0.0152 | Grad Norm: 0.00816021\n",
      "Epoch 3 | Step 1781600 | Avg Loss: 0.0156 | Grad Norm: 0.00762890\n",
      "Epoch 3 | Step 1781700 | Avg Loss: 0.0154 | Grad Norm: 0.00831523\n",
      "Epoch 3 | Step 1781800 | Avg Loss: 0.0153 | Grad Norm: 0.00783573\n",
      "Epoch 3 | Step 1781900 | Avg Loss: 0.0150 | Grad Norm: 0.00906306\n",
      "Epoch 3 | Step 1782000 | Avg Loss: 0.0150 | Grad Norm: 0.00888174\n",
      "Epoch 3 | Step 1782100 | Avg Loss: 0.0150 | Grad Norm: 0.00778606\n",
      "Epoch 3 | Step 1782200 | Avg Loss: 0.0149 | Grad Norm: 0.00804118\n",
      "Epoch 3 | Step 1782300 | Avg Loss: 0.0153 | Grad Norm: 0.00918585\n",
      "Epoch 3 | Step 1782400 | Avg Loss: 0.0151 | Grad Norm: 0.00768467\n",
      "Epoch 3 | Step 1782500 | Avg Loss: 0.0151 | Grad Norm: 0.00856586\n",
      "Epoch 3 | Step 1782600 | Avg Loss: 0.0150 | Grad Norm: 0.00861377\n",
      "Epoch 3 | Step 1782700 | Avg Loss: 0.0147 | Grad Norm: 0.00842646\n",
      "Epoch 3 | Step 1782800 | Avg Loss: 0.0145 | Grad Norm: 0.00890646\n",
      "Epoch 3 | Step 1782900 | Avg Loss: 0.0152 | Grad Norm: 0.00920468\n",
      "Epoch 3 | Step 1783000 | Avg Loss: 0.0148 | Grad Norm: 0.00962256\n",
      "Epoch 3 | Step 1783100 | Avg Loss: 0.0146 | Grad Norm: 0.00814782\n",
      "Epoch 3 | Step 1783200 | Avg Loss: 0.0148 | Grad Norm: 0.00793102\n",
      "Epoch 3 | Step 1783300 | Avg Loss: 0.0145 | Grad Norm: 0.00658805\n",
      "Epoch 3 | Step 1783400 | Avg Loss: 0.0150 | Grad Norm: 0.00853526\n",
      "Epoch 3 | Step 1783500 | Avg Loss: 0.0150 | Grad Norm: 0.00738561\n",
      "Epoch 3 | Step 1783600 | Avg Loss: 0.0150 | Grad Norm: 0.00693305\n",
      "Epoch 3 | Step 1783700 | Avg Loss: 0.0149 | Grad Norm: 0.00776167\n",
      "Epoch 3 | Step 1783800 | Avg Loss: 0.0154 | Grad Norm: 0.00716957\n",
      "Epoch 3 | Step 1783900 | Avg Loss: 0.0154 | Grad Norm: 0.00857195\n",
      "Epoch 3 | Step 1784000 | Avg Loss: 0.0153 | Grad Norm: 0.00805014\n",
      "Epoch 3 | Step 1784100 | Avg Loss: 0.0153 | Grad Norm: 0.00795223\n",
      "Epoch 3 | Step 1784200 | Avg Loss: 0.0156 | Grad Norm: 0.00820905\n",
      "Epoch 3 | Step 1784300 | Avg Loss: 0.0159 | Grad Norm: 0.00932954\n",
      "Epoch 3 | Step 1784400 | Avg Loss: 0.0154 | Grad Norm: 0.00809040\n",
      "Epoch 3 | Step 1784500 | Avg Loss: 0.0158 | Grad Norm: 0.00818382\n",
      "Epoch 3 | Step 1784600 | Avg Loss: 0.0153 | Grad Norm: 0.00693219\n",
      "Epoch 3 | Step 1784700 | Avg Loss: 0.0150 | Grad Norm: 0.00786941\n",
      "Epoch 3 | Step 1784800 | Avg Loss: 0.0153 | Grad Norm: 0.00764992\n",
      "Epoch 3 | Step 1784900 | Avg Loss: 0.0152 | Grad Norm: 0.00710690\n",
      "Epoch 3 | Step 1785000 | Avg Loss: 0.0149 | Grad Norm: 0.00857755\n",
      "Epoch 3 | Step 1785100 | Avg Loss: 0.0149 | Grad Norm: 0.00743547\n",
      "Epoch 3 | Step 1785200 | Avg Loss: 0.0146 | Grad Norm: 0.00728021\n",
      "Epoch 3 | Step 1785300 | Avg Loss: 0.0149 | Grad Norm: 0.00810708\n",
      "Epoch 3 | Step 1785400 | Avg Loss: 0.0152 | Grad Norm: 0.00708784\n",
      "Epoch 3 | Step 1785500 | Avg Loss: 0.0152 | Grad Norm: 0.00994175\n",
      "Epoch 3 | Step 1785600 | Avg Loss: 0.0153 | Grad Norm: 0.00787688\n",
      "Epoch 3 | Step 1785700 | Avg Loss: 0.0154 | Grad Norm: 0.00846028\n",
      "Epoch 3 | Step 1785800 | Avg Loss: 0.0150 | Grad Norm: 0.00773792\n",
      "Epoch 3 | Step 1785900 | Avg Loss: 0.0149 | Grad Norm: 0.00790423\n",
      "Epoch 3 | Step 1786000 | Avg Loss: 0.0147 | Grad Norm: 0.00684441\n",
      "Epoch 3 | Step 1786100 | Avg Loss: 0.0149 | Grad Norm: 0.00935785\n",
      "Epoch 3 | Step 1786200 | Avg Loss: 0.0149 | Grad Norm: 0.00704502\n",
      "Epoch 3 | Step 1786300 | Avg Loss: 0.0147 | Grad Norm: 0.00716759\n",
      "Epoch 3 | Step 1786400 | Avg Loss: 0.0145 | Grad Norm: 0.00836307\n",
      "Epoch 3 | Step 1786500 | Avg Loss: 0.0147 | Grad Norm: 0.00782762\n",
      "Epoch 3 | Step 1786600 | Avg Loss: 0.0147 | Grad Norm: 0.00826751\n",
      "Epoch 3 | Step 1786700 | Avg Loss: 0.0148 | Grad Norm: 0.00745723\n",
      "Epoch 3 | Step 1786800 | Avg Loss: 0.0149 | Grad Norm: 0.00670762\n",
      "Epoch 3 | Step 1786900 | Avg Loss: 0.0151 | Grad Norm: 0.00733714\n",
      "Epoch 3 | Step 1787000 | Avg Loss: 0.0151 | Grad Norm: 0.00890300\n",
      "Epoch 3 | Step 1787100 | Avg Loss: 0.0153 | Grad Norm: 0.00804189\n",
      "Epoch 3 | Step 1787200 | Avg Loss: 0.0153 | Grad Norm: 0.00911228\n",
      "Epoch 3 | Step 1787300 | Avg Loss: 0.0155 | Grad Norm: 0.00918671\n",
      "Epoch 3 | Step 1787400 | Avg Loss: 0.0156 | Grad Norm: 0.00688984\n",
      "Epoch 3 | Step 1787500 | Avg Loss: 0.0157 | Grad Norm: 0.00919418\n",
      "Epoch 3 | Step 1787600 | Avg Loss: 0.0153 | Grad Norm: 0.00735882\n",
      "Epoch 3 | Step 1787700 | Avg Loss: 0.0148 | Grad Norm: 0.00906321\n",
      "Epoch 3 | Step 1787800 | Avg Loss: 0.0149 | Grad Norm: 0.00845903\n",
      "Epoch 3 | Step 1787900 | Avg Loss: 0.0150 | Grad Norm: 0.00798245\n",
      "Epoch 3 | Step 1788000 | Avg Loss: 0.0150 | Grad Norm: 0.00669516\n",
      "Epoch 3 | Step 1788100 | Avg Loss: 0.0151 | Grad Norm: 0.01024275\n",
      "Epoch 3 | Step 1788200 | Avg Loss: 0.0152 | Grad Norm: 0.00753363\n",
      "Epoch 3 | Step 1788300 | Avg Loss: 0.0154 | Grad Norm: 0.00902009\n",
      "Epoch 3 | Step 1788400 | Avg Loss: 0.0153 | Grad Norm: 0.00944224\n",
      "Epoch 3 | Step 1788500 | Avg Loss: 0.0153 | Grad Norm: 0.00782592\n",
      "Epoch 3 | Step 1788600 | Avg Loss: 0.0149 | Grad Norm: 0.01083368\n",
      "Epoch 3 | Step 1788700 | Avg Loss: 0.0153 | Grad Norm: 0.00950461\n",
      "Epoch 3 | Step 1788800 | Avg Loss: 0.0149 | Grad Norm: 0.00830144\n",
      "Epoch 3 | Step 1788900 | Avg Loss: 0.0151 | Grad Norm: 0.00805748\n",
      "Epoch 3 | Step 1789000 | Avg Loss: 0.0153 | Grad Norm: 0.00810194\n",
      "Epoch 3 | Step 1789100 | Avg Loss: 0.0154 | Grad Norm: 0.00842587\n",
      "Epoch 3 | Step 1789200 | Avg Loss: 0.0150 | Grad Norm: 0.00768036\n",
      "Epoch 3 | Step 1789300 | Avg Loss: 0.0149 | Grad Norm: 0.00744019\n",
      "Epoch 3 | Step 1789400 | Avg Loss: 0.0146 | Grad Norm: 0.00892598\n",
      "Epoch 3 | Step 1789500 | Avg Loss: 0.0145 | Grad Norm: 0.00878949\n",
      "Epoch 3 | Step 1789600 | Avg Loss: 0.0147 | Grad Norm: 0.00663200\n",
      "Epoch 3 | Step 1789700 | Avg Loss: 0.0147 | Grad Norm: 0.00789082\n",
      "Epoch 3 | Step 1789800 | Avg Loss: 0.0148 | Grad Norm: 0.00797631\n",
      "Epoch 3 | Step 1789900 | Avg Loss: 0.0151 | Grad Norm: 0.00785834\n",
      "Epoch 3 | Step 1790000 | Avg Loss: 0.0157 | Grad Norm: 0.00787642\n",
      "Epoch 3 | Step 1790100 | Avg Loss: 0.0158 | Grad Norm: 0.00741852\n",
      "Epoch 3 | Step 1790200 | Avg Loss: 0.0157 | Grad Norm: 0.00834287\n",
      "Epoch 3 | Step 1790300 | Avg Loss: 0.0155 | Grad Norm: 0.00854701\n",
      "Epoch 3 | Step 1790400 | Avg Loss: 0.0152 | Grad Norm: 0.00817970\n",
      "Epoch 3 | Step 1790500 | Avg Loss: 0.0150 | Grad Norm: 0.00816746\n",
      "Epoch 3 | Step 1790600 | Avg Loss: 0.0155 | Grad Norm: 0.01055620\n",
      "Epoch 3 | Step 1790700 | Avg Loss: 0.0151 | Grad Norm: 0.00744808\n",
      "Epoch 3 | Step 1790800 | Avg Loss: 0.0153 | Grad Norm: 0.00827106\n",
      "Epoch 3 | Step 1790900 | Avg Loss: 0.0156 | Grad Norm: 0.00889781\n",
      "Epoch 3 | Step 1791000 | Avg Loss: 0.0153 | Grad Norm: 0.00774452\n",
      "Epoch 3 | Step 1791100 | Avg Loss: 0.0151 | Grad Norm: 0.00790924\n",
      "Epoch 3 | Step 1791200 | Avg Loss: 0.0154 | Grad Norm: 0.00912658\n",
      "Epoch 3 | Step 1791300 | Avg Loss: 0.0154 | Grad Norm: 0.00771831\n",
      "Epoch 3 | Step 1791400 | Avg Loss: 0.0151 | Grad Norm: 0.00842686\n",
      "Epoch 3 | Step 1791500 | Avg Loss: 0.0149 | Grad Norm: 0.00697942\n",
      "Epoch 3 | Step 1791600 | Avg Loss: 0.0153 | Grad Norm: 0.00824181\n",
      "Epoch 3 | Step 1791700 | Avg Loss: 0.0154 | Grad Norm: 0.00804144\n",
      "Epoch 3 | Step 1791800 | Avg Loss: 0.0153 | Grad Norm: 0.00885545\n",
      "Epoch 3 | Step 1791900 | Avg Loss: 0.0150 | Grad Norm: 0.00828319\n",
      "Epoch 3 | Step 1792000 | Avg Loss: 0.0146 | Grad Norm: 0.00815186\n",
      "Epoch 3 | Step 1792100 | Avg Loss: 0.0151 | Grad Norm: 0.01001559\n",
      "Epoch 3 | Step 1792200 | Avg Loss: 0.0148 | Grad Norm: 0.00823667\n",
      "Epoch 3 | Step 1792300 | Avg Loss: 0.0149 | Grad Norm: 0.00739759\n",
      "Epoch 3 | Step 1792400 | Avg Loss: 0.0147 | Grad Norm: 0.00716544\n",
      "Epoch 3 | Step 1792500 | Avg Loss: 0.0146 | Grad Norm: 0.00767724\n",
      "Epoch 3 | Step 1792600 | Avg Loss: 0.0148 | Grad Norm: 0.00866878\n",
      "Epoch 3 | Step 1792700 | Avg Loss: 0.0146 | Grad Norm: 0.00810641\n",
      "Epoch 3 | Step 1792800 | Avg Loss: 0.0143 | Grad Norm: 0.00912983\n",
      "Epoch 3 | Step 1792900 | Avg Loss: 0.0142 | Grad Norm: 0.00715481\n",
      "Epoch 3 | Step 1793000 | Avg Loss: 0.0141 | Grad Norm: 0.00826074\n",
      "Epoch 3 | Step 1793100 | Avg Loss: 0.0149 | Grad Norm: 0.00699604\n",
      "Epoch 3 | Step 1793200 | Avg Loss: 0.0149 | Grad Norm: 0.01024687\n",
      "Epoch 3 | Step 1793300 | Avg Loss: 0.0150 | Grad Norm: 0.00797295\n",
      "Epoch 3 | Step 1793400 | Avg Loss: 0.0148 | Grad Norm: 0.00993819\n",
      "Epoch 3 | Step 1793500 | Avg Loss: 0.0149 | Grad Norm: 0.00801126\n",
      "Epoch 3 | Step 1793600 | Avg Loss: 0.0150 | Grad Norm: 0.00817433\n",
      "Epoch 3 | Step 1793700 | Avg Loss: 0.0149 | Grad Norm: 0.00844580\n",
      "Epoch 3 | Step 1793800 | Avg Loss: 0.0152 | Grad Norm: 0.00869143\n",
      "Epoch 3 | Step 1793900 | Avg Loss: 0.0151 | Grad Norm: 0.00808738\n",
      "Epoch 3 | Step 1794000 | Avg Loss: 0.0152 | Grad Norm: 0.00771259\n",
      "Epoch 3 | Step 1794100 | Avg Loss: 0.0150 | Grad Norm: 0.00793550\n",
      "Epoch 3 | Step 1794200 | Avg Loss: 0.0153 | Grad Norm: 0.00942125\n",
      "Epoch 3 | Step 1794300 | Avg Loss: 0.0153 | Grad Norm: 0.00854338\n",
      "Epoch 3 | Step 1794400 | Avg Loss: 0.0151 | Grad Norm: 0.00875248\n",
      "Epoch 3 | Step 1794500 | Avg Loss: 0.0154 | Grad Norm: 0.00741777\n",
      "Epoch 3 | Step 1794600 | Avg Loss: 0.0153 | Grad Norm: 0.00791534\n",
      "Epoch 3 | Step 1794700 | Avg Loss: 0.0156 | Grad Norm: 0.00869405\n",
      "Epoch 3 | Step 1794800 | Avg Loss: 0.0149 | Grad Norm: 0.00881661\n",
      "Epoch 3 | Step 1794900 | Avg Loss: 0.0148 | Grad Norm: 0.00852410\n",
      "Epoch 3 | Step 1795000 | Avg Loss: 0.0148 | Grad Norm: 0.00775663\n",
      "Epoch 3 | Step 1795100 | Avg Loss: 0.0147 | Grad Norm: 0.00819983\n",
      "Epoch 3 | Step 1795200 | Avg Loss: 0.0152 | Grad Norm: 0.00800369\n",
      "Epoch 3 | Step 1795300 | Avg Loss: 0.0151 | Grad Norm: 0.00804977\n",
      "Epoch 3 | Step 1795400 | Avg Loss: 0.0153 | Grad Norm: 0.00851464\n",
      "Epoch 3 | Step 1795500 | Avg Loss: 0.0154 | Grad Norm: 0.00807579\n",
      "Epoch 3 | Step 1795600 | Avg Loss: 0.0153 | Grad Norm: 0.00733757\n",
      "Epoch 3 | Step 1795700 | Avg Loss: 0.0156 | Grad Norm: 0.00974785\n",
      "Epoch 3 | Step 1795800 | Avg Loss: 0.0148 | Grad Norm: 0.00786958\n",
      "Epoch 3 | Step 1795900 | Avg Loss: 0.0146 | Grad Norm: 0.00824144\n",
      "Epoch 3 | Step 1796000 | Avg Loss: 0.0155 | Grad Norm: 0.00793629\n",
      "Epoch 3 | Step 1796100 | Avg Loss: 0.0154 | Grad Norm: 0.00734556\n",
      "Epoch 3 | Step 1796200 | Avg Loss: 0.0151 | Grad Norm: 0.00844050\n",
      "Epoch 3 | Step 1796300 | Avg Loss: 0.0147 | Grad Norm: 0.00823093\n",
      "Epoch 3 | Step 1796400 | Avg Loss: 0.0149 | Grad Norm: 0.00793282\n",
      "Epoch 3 | Step 1796500 | Avg Loss: 0.0150 | Grad Norm: 0.00720272\n",
      "Epoch 3 | Step 1796600 | Avg Loss: 0.0150 | Grad Norm: 0.00806155\n",
      "Epoch 3 | Step 1796700 | Avg Loss: 0.0154 | Grad Norm: 0.00710906\n",
      "Epoch 3 | Step 1796800 | Avg Loss: 0.0152 | Grad Norm: 0.00902287\n",
      "Epoch 3 | Step 1796900 | Avg Loss: 0.0150 | Grad Norm: 0.00744360\n",
      "Epoch 3 | Step 1797000 | Avg Loss: 0.0151 | Grad Norm: 0.00764333\n",
      "Epoch 3 | Step 1797100 | Avg Loss: 0.0150 | Grad Norm: 0.00817441\n",
      "Epoch 3 | Step 1797200 | Avg Loss: 0.0153 | Grad Norm: 0.00920921\n",
      "Epoch 3 | Step 1797300 | Avg Loss: 0.0153 | Grad Norm: 0.00862228\n",
      "Epoch 3 | Step 1797400 | Avg Loss: 0.0152 | Grad Norm: 0.00668594\n",
      "Epoch 3 | Step 1797500 | Avg Loss: 0.0150 | Grad Norm: 0.00770145\n",
      "Epoch 3 | Step 1797600 | Avg Loss: 0.0151 | Grad Norm: 0.00812368\n",
      "Epoch 3 | Step 1797700 | Avg Loss: 0.0150 | Grad Norm: 0.00661708\n",
      "Epoch 3 | Step 1797800 | Avg Loss: 0.0149 | Grad Norm: 0.00827950\n",
      "Epoch 3 | Step 1797900 | Avg Loss: 0.0150 | Grad Norm: 0.00876499\n",
      "Epoch 3 | Step 1798000 | Avg Loss: 0.0149 | Grad Norm: 0.00899787\n",
      "Epoch 3 | Step 1798100 | Avg Loss: 0.0148 | Grad Norm: 0.00793726\n",
      "Epoch 3 | Step 1798200 | Avg Loss: 0.0148 | Grad Norm: 0.00759900\n",
      "Epoch 3 | Step 1798300 | Avg Loss: 0.0147 | Grad Norm: 0.00753197\n",
      "Epoch 3 | Step 1798400 | Avg Loss: 0.0150 | Grad Norm: 0.00856227\n",
      "Epoch 3 | Step 1798500 | Avg Loss: 0.0151 | Grad Norm: 0.00884976\n",
      "Epoch 3 | Step 1798600 | Avg Loss: 0.0150 | Grad Norm: 0.00877579\n",
      "Epoch 3 | Step 1798700 | Avg Loss: 0.0148 | Grad Norm: 0.00805722\n",
      "Epoch 3 | Step 1798800 | Avg Loss: 0.0150 | Grad Norm: 0.00826062\n",
      "Epoch 3 | Step 1798900 | Avg Loss: 0.0152 | Grad Norm: 0.00778767\n",
      "Epoch 3 | Step 1799000 | Avg Loss: 0.0149 | Grad Norm: 0.00909256\n",
      "Epoch 3 | Step 1799100 | Avg Loss: 0.0147 | Grad Norm: 0.00864192\n",
      "Epoch 3 | Step 1799200 | Avg Loss: 0.0145 | Grad Norm: 0.00836396\n",
      "Epoch 3 | Step 1799300 | Avg Loss: 0.0150 | Grad Norm: 0.00895458\n",
      "Epoch 3 | Step 1799400 | Avg Loss: 0.0151 | Grad Norm: 0.00757544\n",
      "Epoch 3 | Step 1799500 | Avg Loss: 0.0153 | Grad Norm: 0.00915923\n",
      "Epoch 3 | Step 1799600 | Avg Loss: 0.0152 | Grad Norm: 0.00938844\n",
      "Epoch 3 | Step 1799700 | Avg Loss: 0.0153 | Grad Norm: 0.00810266\n",
      "Epoch 3 | Step 1799800 | Avg Loss: 0.0152 | Grad Norm: 0.00770996\n",
      "Epoch 3 | Step 1799900 | Avg Loss: 0.0152 | Grad Norm: 0.00744145\n",
      "Epoch 3 | Step 1800000 | Avg Loss: 0.0151 | Grad Norm: 0.01007668\n",
      "Saving model at step1800000\n",
      "Epoch 3 | Step 1800100 | Avg Loss: 0.0153 | Grad Norm: 0.00785068\n",
      "Epoch 3 | Step 1800200 | Avg Loss: 0.0152 | Grad Norm: 0.00829257\n",
      "Epoch 3 | Step 1800300 | Avg Loss: 0.0150 | Grad Norm: 0.00657359\n",
      "Epoch 3 | Step 1800400 | Avg Loss: 0.0149 | Grad Norm: 0.00825295\n",
      "Epoch 3 | Step 1800500 | Avg Loss: 0.0153 | Grad Norm: 0.00715611\n",
      "Epoch 3 | Step 1800600 | Avg Loss: 0.0149 | Grad Norm: 0.00628196\n",
      "Epoch 3 | Step 1800700 | Avg Loss: 0.0153 | Grad Norm: 0.00682380\n",
      "Epoch 3 | Step 1800800 | Avg Loss: 0.0151 | Grad Norm: 0.00692477\n",
      "Epoch 3 | Step 1800900 | Avg Loss: 0.0149 | Grad Norm: 0.00790543\n",
      "Epoch 3 | Step 1801000 | Avg Loss: 0.0148 | Grad Norm: 0.00873702\n",
      "Epoch 3 | Step 1801100 | Avg Loss: 0.0150 | Grad Norm: 0.00823707\n",
      "Epoch 3 | Step 1801200 | Avg Loss: 0.0150 | Grad Norm: 0.00801700\n",
      "Epoch 3 | Step 1801300 | Avg Loss: 0.0152 | Grad Norm: 0.00851008\n",
      "Epoch 3 | Step 1801400 | Avg Loss: 0.0149 | Grad Norm: 0.00758685\n",
      "Epoch 3 | Step 1801500 | Avg Loss: 0.0152 | Grad Norm: 0.00880620\n",
      "Epoch 3 | Step 1801600 | Avg Loss: 0.0150 | Grad Norm: 0.00772574\n",
      "Epoch 3 | Step 1801700 | Avg Loss: 0.0145 | Grad Norm: 0.00929404\n",
      "Epoch 3 | Step 1801800 | Avg Loss: 0.0145 | Grad Norm: 0.00829173\n",
      "Epoch 3 | Step 1801900 | Avg Loss: 0.0150 | Grad Norm: 0.00802005\n",
      "Epoch 3 | Step 1802000 | Avg Loss: 0.0148 | Grad Norm: 0.00860116\n",
      "Epoch 3 | Step 1802100 | Avg Loss: 0.0152 | Grad Norm: 0.00718999\n",
      "Epoch 3 | Step 1802200 | Avg Loss: 0.0152 | Grad Norm: 0.00770349\n",
      "Epoch 3 | Step 1802300 | Avg Loss: 0.0148 | Grad Norm: 0.00703428\n",
      "Epoch 3 | Step 1802400 | Avg Loss: 0.0150 | Grad Norm: 0.00871604\n",
      "Epoch 3 | Step 1802500 | Avg Loss: 0.0150 | Grad Norm: 0.00902364\n",
      "Epoch 3 | Step 1802600 | Avg Loss: 0.0149 | Grad Norm: 0.00770885\n",
      "Epoch 3 | Step 1802700 | Avg Loss: 0.0147 | Grad Norm: 0.00759334\n",
      "Epoch 3 | Step 1802800 | Avg Loss: 0.0152 | Grad Norm: 0.00755586\n",
      "Epoch 3 | Step 1802900 | Avg Loss: 0.0152 | Grad Norm: 0.00720500\n",
      "Epoch 3 | Step 1803000 | Avg Loss: 0.0151 | Grad Norm: 0.00853440\n",
      "Epoch 3 | Step 1803100 | Avg Loss: 0.0154 | Grad Norm: 0.00726648\n",
      "Epoch 3 | Step 1803200 | Avg Loss: 0.0150 | Grad Norm: 0.00821568\n",
      "Epoch 3 | Step 1803300 | Avg Loss: 0.0148 | Grad Norm: 0.00763728\n",
      "Epoch 3 | Step 1803400 | Avg Loss: 0.0152 | Grad Norm: 0.00843776\n",
      "Epoch 3 | Step 1803500 | Avg Loss: 0.0154 | Grad Norm: 0.00767801\n",
      "Epoch 3 | Step 1803600 | Avg Loss: 0.0151 | Grad Norm: 0.00923990\n",
      "Epoch 3 | Step 1803700 | Avg Loss: 0.0152 | Grad Norm: 0.00824878\n",
      "Epoch 3 | Step 1803800 | Avg Loss: 0.0152 | Grad Norm: 0.00846710\n",
      "Epoch 3 | Step 1803900 | Avg Loss: 0.0154 | Grad Norm: 0.00801592\n",
      "Epoch 3 | Step 1804000 | Avg Loss: 0.0155 | Grad Norm: 0.00880941\n",
      "Epoch 3 | Step 1804100 | Avg Loss: 0.0150 | Grad Norm: 0.00677715\n",
      "Epoch 3 | Step 1804200 | Avg Loss: 0.0153 | Grad Norm: 0.00947472\n",
      "Epoch 3 | Step 1804300 | Avg Loss: 0.0153 | Grad Norm: 0.00804959\n",
      "Epoch 3 | Step 1804400 | Avg Loss: 0.0154 | Grad Norm: 0.00766156\n",
      "Epoch 3 | Step 1804500 | Avg Loss: 0.0156 | Grad Norm: 0.00743653\n",
      "Epoch 3 | Step 1804600 | Avg Loss: 0.0151 | Grad Norm: 0.00825199\n",
      "Epoch 3 | Step 1804700 | Avg Loss: 0.0147 | Grad Norm: 0.00953100\n",
      "Epoch 3 | Step 1804800 | Avg Loss: 0.0148 | Grad Norm: 0.00897595\n",
      "Epoch 3 | Step 1804900 | Avg Loss: 0.0147 | Grad Norm: 0.00799565\n",
      "Epoch 3 | Step 1805000 | Avg Loss: 0.0151 | Grad Norm: 0.00730388\n",
      "Epoch 3 | Step 1805100 | Avg Loss: 0.0149 | Grad Norm: 0.00723525\n",
      "Epoch 3 | Step 1805200 | Avg Loss: 0.0151 | Grad Norm: 0.00754127\n",
      "Epoch 3 | Step 1805300 | Avg Loss: 0.0151 | Grad Norm: 0.00819353\n",
      "Epoch 3 | Step 1805400 | Avg Loss: 0.0147 | Grad Norm: 0.00909643\n",
      "Epoch 3 | Step 1805500 | Avg Loss: 0.0144 | Grad Norm: 0.00704037\n",
      "Epoch 3 | Step 1805600 | Avg Loss: 0.0145 | Grad Norm: 0.00769634\n",
      "Epoch 3 | Step 1805700 | Avg Loss: 0.0145 | Grad Norm: 0.00785242\n",
      "Epoch 3 | Step 1805800 | Avg Loss: 0.0146 | Grad Norm: 0.00762254\n",
      "Epoch 3 | Step 1805900 | Avg Loss: 0.0149 | Grad Norm: 0.00714092\n",
      "Epoch 3 | Step 1806000 | Avg Loss: 0.0152 | Grad Norm: 0.00716612\n",
      "Epoch 3 | Step 1806100 | Avg Loss: 0.0151 | Grad Norm: 0.00797797\n",
      "Epoch 3 | Step 1806200 | Avg Loss: 0.0152 | Grad Norm: 0.00883076\n",
      "Epoch 3 | Step 1806300 | Avg Loss: 0.0147 | Grad Norm: 0.00808495\n",
      "Epoch 3 | Step 1806400 | Avg Loss: 0.0150 | Grad Norm: 0.00697137\n",
      "Epoch 3 | Step 1806500 | Avg Loss: 0.0151 | Grad Norm: 0.00755190\n",
      "Epoch 3 | Step 1806600 | Avg Loss: 0.0152 | Grad Norm: 0.00847987\n",
      "Epoch 3 | Step 1806700 | Avg Loss: 0.0149 | Grad Norm: 0.00850067\n",
      "Epoch 3 | Step 1806800 | Avg Loss: 0.0152 | Grad Norm: 0.00774168\n",
      "Epoch 3 | Step 1806900 | Avg Loss: 0.0153 | Grad Norm: 0.00715523\n",
      "Epoch 3 | Step 1807000 | Avg Loss: 0.0150 | Grad Norm: 0.01057968\n",
      "Epoch 3 | Step 1807100 | Avg Loss: 0.0148 | Grad Norm: 0.00819708\n",
      "Epoch 3 | Step 1807200 | Avg Loss: 0.0146 | Grad Norm: 0.00719806\n",
      "Epoch 3 | Step 1807300 | Avg Loss: 0.0145 | Grad Norm: 0.00797529\n",
      "Epoch 3 | Step 1807400 | Avg Loss: 0.0147 | Grad Norm: 0.00774997\n",
      "Epoch 3 | Step 1807500 | Avg Loss: 0.0145 | Grad Norm: 0.00663248\n",
      "Epoch 3 | Step 1807600 | Avg Loss: 0.0149 | Grad Norm: 0.00794100\n",
      "Epoch 3 | Step 1807700 | Avg Loss: 0.0150 | Grad Norm: 0.00812556\n",
      "Epoch 3 | Step 1807800 | Avg Loss: 0.0145 | Grad Norm: 0.00792006\n",
      "Epoch 3 | Step 1807900 | Avg Loss: 0.0147 | Grad Norm: 0.00723101\n",
      "Epoch 3 | Step 1808000 | Avg Loss: 0.0152 | Grad Norm: 0.00824925\n",
      "Epoch 3 | Step 1808100 | Avg Loss: 0.0151 | Grad Norm: 0.00750225\n",
      "Epoch 3 | Step 1808200 | Avg Loss: 0.0149 | Grad Norm: 0.00737516\n",
      "Epoch 3 | Step 1808300 | Avg Loss: 0.0151 | Grad Norm: 0.01057062\n",
      "Epoch 3 | Step 1808400 | Avg Loss: 0.0155 | Grad Norm: 0.00792965\n",
      "Epoch 3 | Step 1808500 | Avg Loss: 0.0154 | Grad Norm: 0.00827102\n",
      "Epoch 3 | Step 1808600 | Avg Loss: 0.0153 | Grad Norm: 0.00971196\n",
      "Epoch 3 | Step 1808700 | Avg Loss: 0.0151 | Grad Norm: 0.01005402\n",
      "Epoch 3 | Step 1808800 | Avg Loss: 0.0150 | Grad Norm: 0.00868185\n",
      "Epoch 3 | Step 1808900 | Avg Loss: 0.0152 | Grad Norm: 0.00843511\n",
      "Epoch 3 | Step 1809000 | Avg Loss: 0.0150 | Grad Norm: 0.00876652\n",
      "Epoch 3 | Step 1809100 | Avg Loss: 0.0152 | Grad Norm: 0.00736313\n",
      "Epoch 3 | Step 1809200 | Avg Loss: 0.0152 | Grad Norm: 0.00741724\n",
      "Epoch 3 | Step 1809300 | Avg Loss: 0.0151 | Grad Norm: 0.00838779\n",
      "Epoch 3 | Step 1809400 | Avg Loss: 0.0148 | Grad Norm: 0.00739152\n",
      "Epoch 3 | Step 1809500 | Avg Loss: 0.0149 | Grad Norm: 0.00767324\n",
      "Epoch 3 | Step 1809600 | Avg Loss: 0.0151 | Grad Norm: 0.00880941\n",
      "Epoch 3 | Step 1809700 | Avg Loss: 0.0153 | Grad Norm: 0.00816935\n",
      "Epoch 3 | Step 1809800 | Avg Loss: 0.0151 | Grad Norm: 0.00926851\n",
      "Epoch 3 | Step 1809900 | Avg Loss: 0.0150 | Grad Norm: 0.00780677\n",
      "Epoch 3 | Step 1810000 | Avg Loss: 0.0147 | Grad Norm: 0.01165030\n",
      "Epoch 3 | Step 1810100 | Avg Loss: 0.0153 | Grad Norm: 0.00778280\n",
      "Epoch 3 | Step 1810200 | Avg Loss: 0.0154 | Grad Norm: 0.00777274\n",
      "Epoch 3 | Step 1810300 | Avg Loss: 0.0156 | Grad Norm: 0.00807660\n",
      "Epoch 3 | Step 1810400 | Avg Loss: 0.0156 | Grad Norm: 0.00757406\n",
      "Epoch 3 | Step 1810500 | Avg Loss: 0.0153 | Grad Norm: 0.00874707\n",
      "Epoch 3 | Step 1810600 | Avg Loss: 0.0155 | Grad Norm: 0.00790704\n",
      "Epoch 3 | Step 1810700 | Avg Loss: 0.0155 | Grad Norm: 0.00917146\n",
      "Epoch 3 | Step 1810800 | Avg Loss: 0.0155 | Grad Norm: 0.00778913\n",
      "Epoch 3 | Step 1810900 | Avg Loss: 0.0151 | Grad Norm: 0.00941677\n",
      "Epoch 3 | Step 1811000 | Avg Loss: 0.0147 | Grad Norm: 0.00885955\n",
      "Epoch 3 | Step 1811100 | Avg Loss: 0.0151 | Grad Norm: 0.00771130\n",
      "Epoch 3 | Step 1811200 | Avg Loss: 0.0150 | Grad Norm: 0.01260543\n",
      "Epoch 3 | Step 1811300 | Avg Loss: 0.0155 | Grad Norm: 0.00799609\n",
      "Epoch 3 | Step 1811400 | Avg Loss: 0.0153 | Grad Norm: 0.00775474\n",
      "Epoch 3 | Step 1811500 | Avg Loss: 0.0151 | Grad Norm: 0.01070025\n",
      "Epoch 3 | Step 1811600 | Avg Loss: 0.0154 | Grad Norm: 0.01143832\n",
      "Epoch 3 | Step 1811700 | Avg Loss: 0.0149 | Grad Norm: 0.00803821\n",
      "Epoch 3 | Step 1811800 | Avg Loss: 0.0151 | Grad Norm: 0.00891664\n",
      "Epoch 3 | Step 1811900 | Avg Loss: 0.0149 | Grad Norm: 0.00756593\n",
      "Epoch 3 | Step 1812000 | Avg Loss: 0.0150 | Grad Norm: 0.00924816\n",
      "Epoch 3 | Step 1812100 | Avg Loss: 0.0150 | Grad Norm: 0.00827180\n",
      "Epoch 3 | Step 1812200 | Avg Loss: 0.0151 | Grad Norm: 0.00835711\n",
      "Epoch 3 | Step 1812300 | Avg Loss: 0.0151 | Grad Norm: 0.00870489\n",
      "Epoch 3 | Step 1812400 | Avg Loss: 0.0149 | Grad Norm: 0.00792688\n",
      "Epoch 3 | Step 1812500 | Avg Loss: 0.0148 | Grad Norm: 0.00908140\n",
      "Epoch 3 | Step 1812600 | Avg Loss: 0.0148 | Grad Norm: 0.00851967\n",
      "Epoch 3 | Step 1812700 | Avg Loss: 0.0147 | Grad Norm: 0.00899697\n",
      "Epoch 3 | Step 1812800 | Avg Loss: 0.0145 | Grad Norm: 0.00839498\n",
      "Epoch 3 | Step 1812900 | Avg Loss: 0.0143 | Grad Norm: 0.00948172\n",
      "Epoch 3 | Step 1813000 | Avg Loss: 0.0141 | Grad Norm: 0.00909901\n",
      "Epoch 3 | Step 1813100 | Avg Loss: 0.0145 | Grad Norm: 0.00703311\n",
      "Epoch 3 | Step 1813200 | Avg Loss: 0.0145 | Grad Norm: 0.00905328\n",
      "Epoch 3 | Step 1813300 | Avg Loss: 0.0144 | Grad Norm: 0.00749180\n",
      "Epoch 3 | Step 1813400 | Avg Loss: 0.0146 | Grad Norm: 0.00852224\n",
      "Epoch 3 | Step 1813500 | Avg Loss: 0.0142 | Grad Norm: 0.00829272\n",
      "Epoch 3 | Step 1813600 | Avg Loss: 0.0139 | Grad Norm: 0.00571193\n",
      "Epoch 3 | Step 1813700 | Avg Loss: 0.0136 | Grad Norm: 0.00813286\n",
      "Epoch 3 | Step 1813800 | Avg Loss: 0.0143 | Grad Norm: 0.00828549\n",
      "Epoch 3 | Step 1813900 | Avg Loss: 0.0143 | Grad Norm: 0.00701109\n",
      "Epoch 3 | Step 1814000 | Avg Loss: 0.0143 | Grad Norm: 0.00773327\n",
      "Epoch 3 | Step 1814100 | Avg Loss: 0.0147 | Grad Norm: 0.00993440\n",
      "Epoch 3 | Step 1814200 | Avg Loss: 0.0149 | Grad Norm: 0.00787935\n",
      "Epoch 3 | Step 1814300 | Avg Loss: 0.0150 | Grad Norm: 0.00806981\n",
      "Epoch 3 | Step 1814400 | Avg Loss: 0.0152 | Grad Norm: 0.00781225\n",
      "Epoch 3 | Step 1814500 | Avg Loss: 0.0153 | Grad Norm: 0.00925960\n",
      "Epoch 3 | Step 1814600 | Avg Loss: 0.0151 | Grad Norm: 0.00770454\n",
      "Epoch 3 | Step 1814700 | Avg Loss: 0.0148 | Grad Norm: 0.00812376\n",
      "Epoch 3 | Step 1814800 | Avg Loss: 0.0150 | Grad Norm: 0.00817742\n",
      "Epoch 3 | Step 1814900 | Avg Loss: 0.0152 | Grad Norm: 0.00677913\n",
      "Epoch 3 | Step 1815000 | Avg Loss: 0.0154 | Grad Norm: 0.00847584\n",
      "Epoch 3 | Step 1815100 | Avg Loss: 0.0151 | Grad Norm: 0.00961933\n",
      "Epoch 3 | Step 1815200 | Avg Loss: 0.0150 | Grad Norm: 0.00700555\n",
      "Epoch 3 | Step 1815300 | Avg Loss: 0.0148 | Grad Norm: 0.00808493\n",
      "Epoch 3 | Step 1815400 | Avg Loss: 0.0151 | Grad Norm: 0.00839979\n",
      "Epoch 3 | Step 1815500 | Avg Loss: 0.0150 | Grad Norm: 0.00817600\n",
      "Epoch 3 | Step 1815600 | Avg Loss: 0.0150 | Grad Norm: 0.00865468\n",
      "Epoch 3 | Step 1815700 | Avg Loss: 0.0150 | Grad Norm: 0.01008454\n",
      "Epoch 3 | Step 1815800 | Avg Loss: 0.0149 | Grad Norm: 0.00815806\n",
      "Epoch 3 | Step 1815900 | Avg Loss: 0.0151 | Grad Norm: 0.00844360\n",
      "Epoch 3 | Step 1816000 | Avg Loss: 0.0151 | Grad Norm: 0.00894438\n",
      "Epoch 3 | Step 1816100 | Avg Loss: 0.0152 | Grad Norm: 0.00838447\n",
      "Epoch 3 | Step 1816200 | Avg Loss: 0.0147 | Grad Norm: 0.00840413\n",
      "Epoch 3 | Step 1816300 | Avg Loss: 0.0148 | Grad Norm: 0.00834780\n",
      "Epoch 3 | Step 1816400 | Avg Loss: 0.0145 | Grad Norm: 0.00920791\n",
      "Epoch 3 | Step 1816500 | Avg Loss: 0.0146 | Grad Norm: 0.00661878\n",
      "Epoch 3 | Step 1816600 | Avg Loss: 0.0147 | Grad Norm: 0.00794643\n",
      "Epoch 3 | Step 1816700 | Avg Loss: 0.0147 | Grad Norm: 0.00734351\n",
      "Epoch 3 | Step 1816800 | Avg Loss: 0.0153 | Grad Norm: 0.00835486\n",
      "Epoch 3 | Step 1816900 | Avg Loss: 0.0151 | Grad Norm: 0.00756628\n",
      "Epoch 3 | Step 1817000 | Avg Loss: 0.0150 | Grad Norm: 0.00824887\n",
      "Epoch 3 | Step 1817100 | Avg Loss: 0.0150 | Grad Norm: 0.00742024\n",
      "Epoch 3 | Step 1817200 | Avg Loss: 0.0149 | Grad Norm: 0.00862500\n",
      "Epoch 3 | Step 1817300 | Avg Loss: 0.0150 | Grad Norm: 0.00739386\n",
      "Epoch 3 | Step 1817400 | Avg Loss: 0.0151 | Grad Norm: 0.00891992\n",
      "Epoch 3 | Step 1817500 | Avg Loss: 0.0150 | Grad Norm: 0.00730547\n",
      "Epoch 3 | Step 1817600 | Avg Loss: 0.0147 | Grad Norm: 0.00720500\n",
      "Epoch 3 | Step 1817700 | Avg Loss: 0.0148 | Grad Norm: 0.00657091\n",
      "Epoch 3 | Step 1817800 | Avg Loss: 0.0148 | Grad Norm: 0.00683477\n",
      "Epoch 3 | Step 1817900 | Avg Loss: 0.0149 | Grad Norm: 0.00760587\n",
      "Epoch 3 | Step 1818000 | Avg Loss: 0.0149 | Grad Norm: 0.00805458\n",
      "Epoch 3 | Step 1818100 | Avg Loss: 0.0150 | Grad Norm: 0.00853307\n",
      "Epoch 3 | Step 1818200 | Avg Loss: 0.0152 | Grad Norm: 0.00854002\n",
      "Epoch 3 | Step 1818300 | Avg Loss: 0.0151 | Grad Norm: 0.00847157\n",
      "Epoch 3 | Step 1818400 | Avg Loss: 0.0151 | Grad Norm: 0.00964475\n",
      "Epoch 3 | Step 1818500 | Avg Loss: 0.0152 | Grad Norm: 0.00766739\n",
      "Epoch 3 | Step 1818600 | Avg Loss: 0.0151 | Grad Norm: 0.00977129\n",
      "Epoch 3 | Step 1818700 | Avg Loss: 0.0150 | Grad Norm: 0.00709057\n",
      "Epoch 3 | Step 1818800 | Avg Loss: 0.0150 | Grad Norm: 0.00738359\n",
      "Epoch 3 | Step 1818900 | Avg Loss: 0.0152 | Grad Norm: 0.00800516\n",
      "Epoch 3 | Step 1819000 | Avg Loss: 0.0151 | Grad Norm: 0.00656991\n",
      "Epoch 3 | Step 1819100 | Avg Loss: 0.0145 | Grad Norm: 0.00761131\n",
      "Epoch 3 | Step 1819200 | Avg Loss: 0.0148 | Grad Norm: 0.00957013\n",
      "Epoch 3 | Step 1819300 | Avg Loss: 0.0147 | Grad Norm: 0.01022498\n",
      "Epoch 3 | Step 1819400 | Avg Loss: 0.0147 | Grad Norm: 0.00937426\n",
      "Epoch 3 | Step 1819500 | Avg Loss: 0.0150 | Grad Norm: 0.00832895\n",
      "Epoch 3 | Step 1819600 | Avg Loss: 0.0151 | Grad Norm: 0.00957584\n",
      "Epoch 3 | Step 1819700 | Avg Loss: 0.0155 | Grad Norm: 0.00853332\n",
      "Epoch 3 | Step 1819800 | Avg Loss: 0.0151 | Grad Norm: 0.00788716\n",
      "Epoch 3 | Step 1819900 | Avg Loss: 0.0148 | Grad Norm: 0.00713553\n",
      "Epoch 3 | Step 1820000 | Avg Loss: 0.0152 | Grad Norm: 0.00793475\n",
      "Epoch 3 | Step 1820100 | Avg Loss: 0.0152 | Grad Norm: 0.00756196\n",
      "Epoch 3 | Step 1820200 | Avg Loss: 0.0151 | Grad Norm: 0.00746992\n",
      "Epoch 3 | Step 1820300 | Avg Loss: 0.0153 | Grad Norm: 0.00839261\n",
      "Epoch 3 | Step 1820400 | Avg Loss: 0.0154 | Grad Norm: 0.00780011\n",
      "Epoch 3 | Step 1820500 | Avg Loss: 0.0157 | Grad Norm: 0.00920872\n",
      "Epoch 3 | Step 1820600 | Avg Loss: 0.0153 | Grad Norm: 0.00815992\n",
      "Epoch 3 | Step 1820700 | Avg Loss: 0.0148 | Grad Norm: 0.00750476\n",
      "Epoch 3 | Step 1820800 | Avg Loss: 0.0149 | Grad Norm: 0.00768172\n",
      "Epoch 3 | Step 1820900 | Avg Loss: 0.0144 | Grad Norm: 0.00782160\n",
      "Epoch 3 | Step 1821000 | Avg Loss: 0.0148 | Grad Norm: 0.00789249\n",
      "Epoch 3 | Step 1821100 | Avg Loss: 0.0152 | Grad Norm: 0.00764723\n",
      "Epoch 3 | Step 1821200 | Avg Loss: 0.0156 | Grad Norm: 0.00950703\n",
      "Epoch 3 | Step 1821300 | Avg Loss: 0.0156 | Grad Norm: 0.00870567\n",
      "Epoch 3 | Step 1821400 | Avg Loss: 0.0155 | Grad Norm: 0.00749243\n",
      "Epoch 3 | Step 1821500 | Avg Loss: 0.0154 | Grad Norm: 0.00735011\n",
      "Epoch 3 | Step 1821600 | Avg Loss: 0.0153 | Grad Norm: 0.00891342\n",
      "Epoch 3 | Step 1821700 | Avg Loss: 0.0150 | Grad Norm: 0.00929573\n",
      "Epoch 3 | Step 1821800 | Avg Loss: 0.0149 | Grad Norm: 0.00671981\n",
      "Epoch 3 | Step 1821900 | Avg Loss: 0.0148 | Grad Norm: 0.00708502\n",
      "Epoch 3 | Step 1822000 | Avg Loss: 0.0151 | Grad Norm: 0.00797977\n",
      "Epoch 3 | Step 1822100 | Avg Loss: 0.0151 | Grad Norm: 0.00763183\n",
      "Epoch 3 | Step 1822200 | Avg Loss: 0.0149 | Grad Norm: 0.00890018\n",
      "Epoch 3 | Step 1822300 | Avg Loss: 0.0150 | Grad Norm: 0.01546448\n",
      "Epoch 3 | Step 1822400 | Avg Loss: 0.0149 | Grad Norm: 0.00871726\n",
      "Epoch 3 | Step 1822500 | Avg Loss: 0.0149 | Grad Norm: 0.00883776\n",
      "Epoch 3 | Step 1822600 | Avg Loss: 0.0149 | Grad Norm: 0.00745447\n",
      "Epoch 3 | Step 1822700 | Avg Loss: 0.0150 | Grad Norm: 0.00714766\n",
      "Epoch 3 | Step 1822800 | Avg Loss: 0.0146 | Grad Norm: 0.00878041\n",
      "Epoch 3 | Step 1822900 | Avg Loss: 0.0150 | Grad Norm: 0.01212556\n",
      "Epoch 3 | Step 1823000 | Avg Loss: 0.0152 | Grad Norm: 0.01012903\n",
      "Epoch 3 | Step 1823100 | Avg Loss: 0.0154 | Grad Norm: 0.00961038\n",
      "Epoch 3 | Step 1823200 | Avg Loss: 0.0156 | Grad Norm: 0.00756962\n",
      "Epoch 3 | Step 1823300 | Avg Loss: 0.0154 | Grad Norm: 0.00814004\n",
      "Epoch 3 | Step 1823400 | Avg Loss: 0.0152 | Grad Norm: 0.00997057\n",
      "Epoch 3 | Step 1823500 | Avg Loss: 0.0150 | Grad Norm: 0.00838914\n",
      "Epoch 3 | Step 1823600 | Avg Loss: 0.0150 | Grad Norm: 0.00730207\n",
      "Epoch 3 | Step 1823700 | Avg Loss: 0.0153 | Grad Norm: 0.00836697\n",
      "Epoch 3 | Step 1823800 | Avg Loss: 0.0151 | Grad Norm: 0.00780645\n",
      "Epoch 3 | Step 1823900 | Avg Loss: 0.0151 | Grad Norm: 0.00842062\n",
      "Epoch 3 | Step 1824000 | Avg Loss: 0.0152 | Grad Norm: 0.00871625\n",
      "Epoch 3 | Step 1824100 | Avg Loss: 0.0151 | Grad Norm: 0.00815725\n",
      "Epoch 3 | Step 1824200 | Avg Loss: 0.0150 | Grad Norm: 0.00857206\n",
      "Epoch 3 | Step 1824300 | Avg Loss: 0.0150 | Grad Norm: 0.00769072\n",
      "Epoch 3 | Step 1824400 | Avg Loss: 0.0151 | Grad Norm: 0.00762083\n",
      "Epoch 3 | Step 1824500 | Avg Loss: 0.0151 | Grad Norm: 0.00953543\n",
      "Epoch 3 | Step 1824600 | Avg Loss: 0.0154 | Grad Norm: 0.00936341\n",
      "Epoch 3 | Step 1824700 | Avg Loss: 0.0152 | Grad Norm: 0.00789108\n",
      "Epoch 3 | Step 1824800 | Avg Loss: 0.0150 | Grad Norm: 0.00842960\n",
      "Epoch 3 | Step 1824900 | Avg Loss: 0.0149 | Grad Norm: 0.00764588\n",
      "Epoch 3 | Step 1825000 | Avg Loss: 0.0153 | Grad Norm: 0.00896154\n",
      "Epoch 3 | Step 1825100 | Avg Loss: 0.0148 | Grad Norm: 0.00797856\n",
      "Epoch 3 | Step 1825200 | Avg Loss: 0.0150 | Grad Norm: 0.00828331\n",
      "Epoch 3 | Step 1825300 | Avg Loss: 0.0149 | Grad Norm: 0.00840870\n",
      "Epoch 3 | Step 1825400 | Avg Loss: 0.0153 | Grad Norm: 0.00909738\n",
      "Epoch 3 | Step 1825500 | Avg Loss: 0.0153 | Grad Norm: 0.00769683\n",
      "Epoch 3 | Step 1825600 | Avg Loss: 0.0151 | Grad Norm: 0.00933507\n",
      "Epoch 3 | Step 1825700 | Avg Loss: 0.0153 | Grad Norm: 0.00827428\n",
      "Epoch 3 | Step 1825800 | Avg Loss: 0.0152 | Grad Norm: 0.00676780\n",
      "Epoch 3 | Step 1825900 | Avg Loss: 0.0151 | Grad Norm: 0.00921875\n",
      "Epoch 3 | Step 1826000 | Avg Loss: 0.0154 | Grad Norm: 0.00758239\n",
      "Epoch 3 | Step 1826100 | Avg Loss: 0.0154 | Grad Norm: 0.00731077\n",
      "Epoch 3 | Step 1826200 | Avg Loss: 0.0155 | Grad Norm: 0.00700349\n",
      "Epoch 3 | Step 1826300 | Avg Loss: 0.0154 | Grad Norm: 0.00755468\n",
      "Epoch 3 | Step 1826400 | Avg Loss: 0.0154 | Grad Norm: 0.00677662\n",
      "Epoch 3 | Step 1826500 | Avg Loss: 0.0156 | Grad Norm: 0.00763512\n",
      "Epoch 3 | Step 1826600 | Avg Loss: 0.0157 | Grad Norm: 0.00963132\n",
      "Epoch 3 | Step 1826700 | Avg Loss: 0.0156 | Grad Norm: 0.00758391\n",
      "Epoch 3 | Step 1826800 | Avg Loss: 0.0157 | Grad Norm: 0.00910478\n",
      "Epoch 3 | Step 1826900 | Avg Loss: 0.0156 | Grad Norm: 0.00815666\n",
      "Epoch 3 | Step 1827000 | Avg Loss: 0.0157 | Grad Norm: 0.00921487\n",
      "Epoch 3 | Step 1827100 | Avg Loss: 0.0152 | Grad Norm: 0.00832594\n",
      "Epoch 3 | Step 1827200 | Avg Loss: 0.0155 | Grad Norm: 0.00784569\n",
      "Epoch 3 | Step 1827300 | Avg Loss: 0.0156 | Grad Norm: 0.00695997\n",
      "Epoch 3 | Step 1827400 | Avg Loss: 0.0158 | Grad Norm: 0.00864763\n",
      "Epoch 3 | Step 1827500 | Avg Loss: 0.0157 | Grad Norm: 0.00776131\n",
      "Epoch 3 | Step 1827600 | Avg Loss: 0.0156 | Grad Norm: 0.00897411\n",
      "Epoch 3 | Step 1827700 | Avg Loss: 0.0152 | Grad Norm: 0.00749989\n",
      "Epoch 3 | Step 1827800 | Avg Loss: 0.0150 | Grad Norm: 0.00732945\n",
      "Epoch 3 | Step 1827900 | Avg Loss: 0.0145 | Grad Norm: 0.00907827\n",
      "Epoch 3 | Step 1828000 | Avg Loss: 0.0156 | Grad Norm: 0.00759972\n",
      "Epoch 3 | Step 1828100 | Avg Loss: 0.0155 | Grad Norm: 0.00808839\n",
      "Epoch 3 | Step 1828200 | Avg Loss: 0.0152 | Grad Norm: 0.00791335\n",
      "Epoch 3 | Step 1828300 | Avg Loss: 0.0154 | Grad Norm: 0.00837956\n",
      "Epoch 3 | Step 1828400 | Avg Loss: 0.0152 | Grad Norm: 0.00826852\n",
      "Epoch 3 | Step 1828500 | Avg Loss: 0.0149 | Grad Norm: 0.00760948\n",
      "Epoch 3 | Step 1828600 | Avg Loss: 0.0150 | Grad Norm: 0.00709600\n",
      "Epoch 3 | Step 1828700 | Avg Loss: 0.0152 | Grad Norm: 0.01034016\n",
      "Epoch 3 | Step 1828800 | Avg Loss: 0.0152 | Grad Norm: 0.00784213\n",
      "Epoch 3 | Step 1828900 | Avg Loss: 0.0154 | Grad Norm: 0.00821032\n",
      "Epoch 3 | Step 1829000 | Avg Loss: 0.0158 | Grad Norm: 0.00865016\n",
      "Epoch 3 | Step 1829100 | Avg Loss: 0.0154 | Grad Norm: 0.00911031\n",
      "Epoch 3 | Step 1829200 | Avg Loss: 0.0152 | Grad Norm: 0.00815156\n",
      "Epoch 3 | Step 1829300 | Avg Loss: 0.0155 | Grad Norm: 0.00856465\n",
      "Epoch 3 | Step 1829400 | Avg Loss: 0.0150 | Grad Norm: 0.00851889\n",
      "Epoch 3 | Step 1829500 | Avg Loss: 0.0154 | Grad Norm: 0.00729500\n",
      "Epoch 3 | Step 1829600 | Avg Loss: 0.0152 | Grad Norm: 0.00861624\n",
      "Epoch 3 | Step 1829700 | Avg Loss: 0.0148 | Grad Norm: 0.01019117\n",
      "Epoch 3 | Step 1829800 | Avg Loss: 0.0143 | Grad Norm: 0.00738764\n",
      "Epoch 3 | Step 1829900 | Avg Loss: 0.0148 | Grad Norm: 0.00699385\n",
      "Epoch 3 | Step 1830000 | Avg Loss: 0.0145 | Grad Norm: 0.00810205\n",
      "Epoch 3 | Step 1830100 | Avg Loss: 0.0148 | Grad Norm: 0.00814407\n",
      "Epoch 3 | Step 1830200 | Avg Loss: 0.0145 | Grad Norm: 0.00809893\n",
      "Epoch 3 | Step 1830300 | Avg Loss: 0.0148 | Grad Norm: 0.00874585\n",
      "Epoch 3 | Step 1830400 | Avg Loss: 0.0151 | Grad Norm: 0.00700133\n",
      "Epoch 3 | Step 1830500 | Avg Loss: 0.0151 | Grad Norm: 0.00778683\n",
      "Epoch 3 | Step 1830600 | Avg Loss: 0.0151 | Grad Norm: 0.00838917\n",
      "Epoch 3 | Step 1830700 | Avg Loss: 0.0149 | Grad Norm: 0.00698507\n",
      "Epoch 3 | Step 1830800 | Avg Loss: 0.0151 | Grad Norm: 0.00741421\n",
      "Epoch 3 | Step 1830900 | Avg Loss: 0.0153 | Grad Norm: 0.00783038\n",
      "Epoch 3 | Step 1831000 | Avg Loss: 0.0153 | Grad Norm: 0.00808773\n",
      "Epoch 3 | Step 1831100 | Avg Loss: 0.0155 | Grad Norm: 0.00889721\n",
      "Epoch 3 | Step 1831200 | Avg Loss: 0.0154 | Grad Norm: 0.00856002\n",
      "Epoch 3 | Step 1831300 | Avg Loss: 0.0153 | Grad Norm: 0.01018034\n",
      "Epoch 3 | Step 1831400 | Avg Loss: 0.0152 | Grad Norm: 0.00963360\n",
      "Epoch 3 | Step 1831500 | Avg Loss: 0.0154 | Grad Norm: 0.00726658\n",
      "Epoch 3 | Step 1831600 | Avg Loss: 0.0153 | Grad Norm: 0.00745419\n",
      "Epoch 3 | Step 1831700 | Avg Loss: 0.0154 | Grad Norm: 0.00726701\n",
      "Epoch 3 | Step 1831800 | Avg Loss: 0.0154 | Grad Norm: 0.00790470\n",
      "Epoch 3 | Step 1831900 | Avg Loss: 0.0153 | Grad Norm: 0.00749060\n",
      "Epoch 3 | Step 1832000 | Avg Loss: 0.0154 | Grad Norm: 0.00812203\n",
      "Epoch 3 | Step 1832100 | Avg Loss: 0.0153 | Grad Norm: 0.00851565\n",
      "Epoch 3 | Step 1832200 | Avg Loss: 0.0150 | Grad Norm: 0.00791470\n",
      "Epoch 3 | Step 1832300 | Avg Loss: 0.0150 | Grad Norm: 0.00808118\n",
      "Epoch 3 | Step 1832400 | Avg Loss: 0.0148 | Grad Norm: 0.00756105\n",
      "Epoch 3 | Step 1832500 | Avg Loss: 0.0154 | Grad Norm: 0.00883285\n",
      "Epoch 3 | Step 1832600 | Avg Loss: 0.0152 | Grad Norm: 0.00831762\n",
      "Epoch 3 | Step 1832700 | Avg Loss: 0.0153 | Grad Norm: 0.00910999\n",
      "Epoch 3 | Step 1832800 | Avg Loss: 0.0155 | Grad Norm: 0.00830996\n",
      "Epoch 3 | Step 1832900 | Avg Loss: 0.0151 | Grad Norm: 0.00699268\n",
      "Epoch 3 | Step 1833000 | Avg Loss: 0.0151 | Grad Norm: 0.00880033\n",
      "Epoch 3 | Step 1833100 | Avg Loss: 0.0151 | Grad Norm: 0.00713626\n",
      "Epoch 3 | Step 1833200 | Avg Loss: 0.0152 | Grad Norm: 0.00888309\n",
      "Epoch 3 | Step 1833300 | Avg Loss: 0.0149 | Grad Norm: 0.00812885\n",
      "Epoch 3 | Step 1833400 | Avg Loss: 0.0146 | Grad Norm: 0.00740930\n",
      "Epoch 3 | Step 1833500 | Avg Loss: 0.0145 | Grad Norm: 0.00785638\n",
      "Epoch 3 | Step 1833600 | Avg Loss: 0.0148 | Grad Norm: 0.00762766\n",
      "Epoch 3 | Step 1833700 | Avg Loss: 0.0148 | Grad Norm: 0.00709584\n",
      "Epoch 3 | Step 1833800 | Avg Loss: 0.0148 | Grad Norm: 0.00814199\n",
      "Epoch 3 | Step 1833900 | Avg Loss: 0.0148 | Grad Norm: 0.00729885\n",
      "Epoch 3 | Step 1834000 | Avg Loss: 0.0153 | Grad Norm: 0.00879412\n",
      "Epoch 3 | Step 1834100 | Avg Loss: 0.0152 | Grad Norm: 0.00706322\n",
      "Epoch 3 | Step 1834200 | Avg Loss: 0.0149 | Grad Norm: 0.00867490\n",
      "Epoch 3 | Step 1834300 | Avg Loss: 0.0147 | Grad Norm: 0.00963399\n",
      "Epoch 3 | Step 1834400 | Avg Loss: 0.0149 | Grad Norm: 0.00754959\n",
      "Epoch 3 | Step 1834500 | Avg Loss: 0.0148 | Grad Norm: 0.00733462\n",
      "Epoch 3 | Step 1834600 | Avg Loss: 0.0149 | Grad Norm: 0.00870531\n",
      "Epoch 3 | Step 1834700 | Avg Loss: 0.0153 | Grad Norm: 0.00728701\n",
      "Epoch 3 | Step 1834800 | Avg Loss: 0.0154 | Grad Norm: 0.00820400\n",
      "Epoch 3 | Step 1834900 | Avg Loss: 0.0150 | Grad Norm: 0.00776127\n",
      "Epoch 3 | Step 1835000 | Avg Loss: 0.0152 | Grad Norm: 0.00829586\n",
      "Epoch 3 | Step 1835100 | Avg Loss: 0.0152 | Grad Norm: 0.00845040\n",
      "Epoch 3 | Step 1835200 | Avg Loss: 0.0155 | Grad Norm: 0.00997235\n",
      "Epoch 3 | Step 1835300 | Avg Loss: 0.0153 | Grad Norm: 0.00830367\n",
      "Epoch 3 | Step 1835400 | Avg Loss: 0.0151 | Grad Norm: 0.00753243\n",
      "Epoch 3 | Step 1835500 | Avg Loss: 0.0145 | Grad Norm: 0.00785418\n",
      "Epoch 3 | Step 1835600 | Avg Loss: 0.0145 | Grad Norm: 0.00747741\n",
      "Epoch 3 | Step 1835700 | Avg Loss: 0.0148 | Grad Norm: 0.00740346\n",
      "Epoch 3 | Step 1835800 | Avg Loss: 0.0146 | Grad Norm: 0.00852508\n",
      "Epoch 3 | Step 1835900 | Avg Loss: 0.0145 | Grad Norm: 0.00849075\n",
      "Epoch 3 | Step 1836000 | Avg Loss: 0.0146 | Grad Norm: 0.00773045\n",
      "Epoch 3 | Step 1836100 | Avg Loss: 0.0146 | Grad Norm: 0.00782137\n",
      "Epoch 3 | Step 1836200 | Avg Loss: 0.0147 | Grad Norm: 0.00849359\n",
      "Epoch 3 | Step 1836300 | Avg Loss: 0.0149 | Grad Norm: 0.00791172\n",
      "Epoch 3 | Step 1836400 | Avg Loss: 0.0149 | Grad Norm: 0.00760868\n",
      "Epoch 3 | Step 1836500 | Avg Loss: 0.0149 | Grad Norm: 0.00816792\n",
      "Epoch 3 | Step 1836600 | Avg Loss: 0.0148 | Grad Norm: 0.00827753\n",
      "Epoch 3 | Step 1836700 | Avg Loss: 0.0147 | Grad Norm: 0.00742180\n",
      "Epoch 3 | Step 1836800 | Avg Loss: 0.0150 | Grad Norm: 0.00703935\n",
      "Epoch 3 | Step 1836900 | Avg Loss: 0.0147 | Grad Norm: 0.00637170\n",
      "Epoch 3 | Step 1837000 | Avg Loss: 0.0144 | Grad Norm: 0.00812973\n",
      "Epoch 3 | Step 1837100 | Avg Loss: 0.0145 | Grad Norm: 0.00755110\n",
      "Epoch 3 | Step 1837200 | Avg Loss: 0.0146 | Grad Norm: 0.00702338\n",
      "Epoch 3 | Step 1837300 | Avg Loss: 0.0146 | Grad Norm: 0.00823863\n",
      "Epoch 3 | Step 1837400 | Avg Loss: 0.0153 | Grad Norm: 0.00774712\n",
      "Epoch 3 | Step 1837500 | Avg Loss: 0.0152 | Grad Norm: 0.00830595\n",
      "Epoch 3 | Step 1837600 | Avg Loss: 0.0155 | Grad Norm: 0.00674762\n",
      "Epoch 3 | Step 1837700 | Avg Loss: 0.0151 | Grad Norm: 0.00775518\n",
      "Epoch 3 | Step 1837800 | Avg Loss: 0.0150 | Grad Norm: 0.00807539\n",
      "Epoch 3 | Step 1837900 | Avg Loss: 0.0149 | Grad Norm: 0.00816816\n",
      "Epoch 3 | Step 1838000 | Avg Loss: 0.0147 | Grad Norm: 0.00836964\n",
      "Epoch 3 | Step 1838100 | Avg Loss: 0.0150 | Grad Norm: 0.00725786\n",
      "Epoch 3 | Step 1838200 | Avg Loss: 0.0149 | Grad Norm: 0.00851748\n",
      "Epoch 3 | Step 1838300 | Avg Loss: 0.0147 | Grad Norm: 0.01092068\n",
      "Epoch 3 | Step 1838400 | Avg Loss: 0.0153 | Grad Norm: 0.00826622\n",
      "Epoch 3 | Step 1838500 | Avg Loss: 0.0153 | Grad Norm: 0.00840084\n",
      "Epoch 3 | Step 1838600 | Avg Loss: 0.0153 | Grad Norm: 0.00770865\n",
      "Epoch 3 | Step 1838700 | Avg Loss: 0.0152 | Grad Norm: 0.00905278\n",
      "Epoch 3 | Step 1838800 | Avg Loss: 0.0152 | Grad Norm: 0.00796987\n",
      "Epoch 3 | Step 1838900 | Avg Loss: 0.0156 | Grad Norm: 0.00739256\n",
      "Epoch 3 | Step 1839000 | Avg Loss: 0.0154 | Grad Norm: 0.00786253\n",
      "Epoch 3 | Step 1839100 | Avg Loss: 0.0147 | Grad Norm: 0.00664740\n",
      "Epoch 3 | Step 1839200 | Avg Loss: 0.0147 | Grad Norm: 0.00825396\n",
      "Epoch 3 | Step 1839300 | Avg Loss: 0.0148 | Grad Norm: 0.00855193\n",
      "Epoch 3 | Step 1839400 | Avg Loss: 0.0153 | Grad Norm: 0.00988438\n",
      "Epoch 3 | Step 1839500 | Avg Loss: 0.0153 | Grad Norm: 0.00775593\n",
      "Epoch 3 | Step 1839600 | Avg Loss: 0.0150 | Grad Norm: 0.00789805\n",
      "Epoch 3 | Step 1839700 | Avg Loss: 0.0148 | Grad Norm: 0.00727355\n",
      "Epoch 3 | Step 1839800 | Avg Loss: 0.0148 | Grad Norm: 0.00755130\n",
      "Epoch 3 | Step 1839900 | Avg Loss: 0.0149 | Grad Norm: 0.00807187\n",
      "Epoch 3 | Step 1840000 | Avg Loss: 0.0149 | Grad Norm: 0.00723024\n",
      "Epoch 3 | Step 1840100 | Avg Loss: 0.0148 | Grad Norm: 0.00818181\n",
      "Epoch 3 | Step 1840200 | Avg Loss: 0.0145 | Grad Norm: 0.00889939\n",
      "Epoch 3 | Step 1840300 | Avg Loss: 0.0146 | Grad Norm: 0.00743316\n",
      "Epoch 3 | Step 1840400 | Avg Loss: 0.0149 | Grad Norm: 0.00856947\n",
      "Epoch 3 | Step 1840500 | Avg Loss: 0.0149 | Grad Norm: 0.00899106\n",
      "Epoch 3 | Step 1840600 | Avg Loss: 0.0150 | Grad Norm: 0.00795873\n",
      "Epoch 3 | Step 1840700 | Avg Loss: 0.0151 | Grad Norm: 0.00824391\n",
      "Epoch 3 | Step 1840800 | Avg Loss: 0.0150 | Grad Norm: 0.00765672\n",
      "Epoch 3 | Step 1840900 | Avg Loss: 0.0153 | Grad Norm: 0.00728604\n",
      "Epoch 3 | Step 1841000 | Avg Loss: 0.0148 | Grad Norm: 0.00917777\n",
      "Epoch 3 | Step 1841100 | Avg Loss: 0.0149 | Grad Norm: 0.00932322\n",
      "Epoch 3 | Step 1841200 | Avg Loss: 0.0150 | Grad Norm: 0.00789722\n",
      "Epoch 3 | Step 1841300 | Avg Loss: 0.0146 | Grad Norm: 0.00861066\n",
      "Epoch 3 | Step 1841400 | Avg Loss: 0.0152 | Grad Norm: 0.00870236\n",
      "Epoch 3 | Step 1841500 | Avg Loss: 0.0151 | Grad Norm: 0.00878109\n",
      "Epoch 3 | Step 1841600 | Avg Loss: 0.0151 | Grad Norm: 0.00972648\n",
      "Epoch 3 | Step 1841700 | Avg Loss: 0.0151 | Grad Norm: 0.00809422\n",
      "Epoch 3 | Step 1841800 | Avg Loss: 0.0153 | Grad Norm: 0.00763580\n",
      "Epoch 3 | Step 1841900 | Avg Loss: 0.0154 | Grad Norm: 0.00780717\n",
      "Epoch 3 | Step 1842000 | Avg Loss: 0.0158 | Grad Norm: 0.00751502\n",
      "Epoch 3 | Step 1842100 | Avg Loss: 0.0159 | Grad Norm: 0.00852543\n",
      "Epoch 3 | Step 1842200 | Avg Loss: 0.0159 | Grad Norm: 0.00827951\n",
      "Epoch 3 | Step 1842300 | Avg Loss: 0.0156 | Grad Norm: 0.01047871\n",
      "Epoch 3 | Step 1842400 | Avg Loss: 0.0156 | Grad Norm: 0.00831259\n",
      "Epoch 3 | Step 1842500 | Avg Loss: 0.0158 | Grad Norm: 0.00983008\n",
      "Epoch 3 | Step 1842600 | Avg Loss: 0.0152 | Grad Norm: 0.00946836\n",
      "Epoch 3 | Step 1842700 | Avg Loss: 0.0151 | Grad Norm: 0.00706233\n",
      "Epoch 3 | Step 1842800 | Avg Loss: 0.0152 | Grad Norm: 0.00800303\n",
      "Epoch 3 | Step 1842900 | Avg Loss: 0.0150 | Grad Norm: 0.00928975\n",
      "Epoch 3 | Step 1843000 | Avg Loss: 0.0150 | Grad Norm: 0.00903471\n",
      "Epoch 3 | Step 1843100 | Avg Loss: 0.0153 | Grad Norm: 0.00906495\n",
      "Epoch 3 | Step 1843200 | Avg Loss: 0.0152 | Grad Norm: 0.00828286\n",
      "Epoch 3 | Step 1843300 | Avg Loss: 0.0149 | Grad Norm: 0.00693139\n",
      "Epoch 3 | Step 1843400 | Avg Loss: 0.0149 | Grad Norm: 0.00776744\n",
      "Epoch 3 | Step 1843500 | Avg Loss: 0.0152 | Grad Norm: 0.00877057\n",
      "Epoch 3 | Step 1843600 | Avg Loss: 0.0151 | Grad Norm: 0.00941650\n",
      "Epoch 3 | Step 1843700 | Avg Loss: 0.0153 | Grad Norm: 0.00786970\n",
      "Epoch 3 | Step 1843800 | Avg Loss: 0.0152 | Grad Norm: 0.00752834\n",
      "Epoch 3 | Step 1843900 | Avg Loss: 0.0155 | Grad Norm: 0.00833858\n",
      "Epoch 3 | Step 1844000 | Avg Loss: 0.0156 | Grad Norm: 0.00861431\n",
      "Epoch 3 | Step 1844100 | Avg Loss: 0.0153 | Grad Norm: 0.00723089\n",
      "Epoch 3 | Step 1844200 | Avg Loss: 0.0151 | Grad Norm: 0.00866253\n",
      "Epoch 3 | Step 1844300 | Avg Loss: 0.0150 | Grad Norm: 0.00772926\n",
      "Epoch 3 | Step 1844400 | Avg Loss: 0.0152 | Grad Norm: 0.00745436\n",
      "Epoch 3 | Step 1844500 | Avg Loss: 0.0152 | Grad Norm: 0.00805585\n",
      "Epoch 3 | Step 1844600 | Avg Loss: 0.0149 | Grad Norm: 0.00733236\n",
      "Epoch 3 | Step 1844700 | Avg Loss: 0.0149 | Grad Norm: 0.00799442\n",
      "Epoch 3 | Step 1844800 | Avg Loss: 0.0152 | Grad Norm: 0.00809795\n",
      "Epoch 3 | Step 1844900 | Avg Loss: 0.0153 | Grad Norm: 0.00820662\n",
      "Epoch 3 | Step 1845000 | Avg Loss: 0.0151 | Grad Norm: 0.00818828\n",
      "Epoch 3 | Step 1845100 | Avg Loss: 0.0151 | Grad Norm: 0.00708844\n",
      "Epoch 3 | Step 1845200 | Avg Loss: 0.0148 | Grad Norm: 0.00769955\n",
      "Epoch 3 | Step 1845300 | Avg Loss: 0.0148 | Grad Norm: 0.00825271\n",
      "Epoch 3 | Step 1845400 | Avg Loss: 0.0152 | Grad Norm: 0.00844038\n",
      "Epoch 3 | Step 1845500 | Avg Loss: 0.0155 | Grad Norm: 0.00842373\n",
      "Epoch 3 | Step 1845600 | Avg Loss: 0.0154 | Grad Norm: 0.00849895\n",
      "Epoch 3 | Step 1845700 | Avg Loss: 0.0153 | Grad Norm: 0.00734044\n",
      "Epoch 3 | Step 1845800 | Avg Loss: 0.0154 | Grad Norm: 0.00733342\n",
      "Epoch 3 | Step 1845900 | Avg Loss: 0.0153 | Grad Norm: 0.00865441\n",
      "Epoch 3 | Step 1846000 | Avg Loss: 0.0150 | Grad Norm: 0.00767361\n",
      "Epoch 3 | Step 1846100 | Avg Loss: 0.0150 | Grad Norm: 0.00706447\n",
      "Epoch 3 | Step 1846200 | Avg Loss: 0.0149 | Grad Norm: 0.00736333\n",
      "Epoch 3 | Step 1846300 | Avg Loss: 0.0149 | Grad Norm: 0.00663311\n",
      "Epoch 3 | Step 1846400 | Avg Loss: 0.0149 | Grad Norm: 0.00787812\n",
      "Epoch 3 | Step 1846500 | Avg Loss: 0.0148 | Grad Norm: 0.00753262\n",
      "Epoch 3 | Step 1846600 | Avg Loss: 0.0149 | Grad Norm: 0.00767404\n",
      "Epoch 3 | Step 1846700 | Avg Loss: 0.0148 | Grad Norm: 0.00712221\n",
      "Epoch 3 | Step 1846800 | Avg Loss: 0.0151 | Grad Norm: 0.00875110\n",
      "Epoch 3 | Step 1846900 | Avg Loss: 0.0148 | Grad Norm: 0.00734116\n",
      "Epoch 3 | Step 1847000 | Avg Loss: 0.0148 | Grad Norm: 0.00875015\n",
      "Epoch 3 | Step 1847100 | Avg Loss: 0.0152 | Grad Norm: 0.00863798\n",
      "Epoch 3 | Step 1847200 | Avg Loss: 0.0155 | Grad Norm: 0.00958286\n",
      "Epoch 3 | Step 1847300 | Avg Loss: 0.0154 | Grad Norm: 0.00868224\n",
      "Epoch 3 | Step 1847400 | Avg Loss: 0.0152 | Grad Norm: 0.00796937\n",
      "Epoch 3 | Step 1847500 | Avg Loss: 0.0157 | Grad Norm: 0.00956954\n",
      "Epoch 3 | Step 1847600 | Avg Loss: 0.0157 | Grad Norm: 0.00754001\n",
      "Epoch 3 | Step 1847700 | Avg Loss: 0.0154 | Grad Norm: 0.00766923\n",
      "Epoch 3 | Step 1847800 | Avg Loss: 0.0151 | Grad Norm: 0.00865481\n",
      "Epoch 3 | Step 1847900 | Avg Loss: 0.0150 | Grad Norm: 0.00862714\n",
      "Epoch 3 | Step 1848000 | Avg Loss: 0.0147 | Grad Norm: 0.00913900\n",
      "Epoch 3 | Step 1848100 | Avg Loss: 0.0147 | Grad Norm: 0.00971158\n",
      "Epoch 3 | Step 1848200 | Avg Loss: 0.0146 | Grad Norm: 0.00781134\n",
      "Epoch 3 | Step 1848300 | Avg Loss: 0.0148 | Grad Norm: 0.00851570\n",
      "Epoch 3 | Step 1848400 | Avg Loss: 0.0144 | Grad Norm: 0.00926377\n",
      "Epoch 3 | Step 1848500 | Avg Loss: 0.0145 | Grad Norm: 0.00756301\n",
      "Epoch 3 | Step 1848600 | Avg Loss: 0.0145 | Grad Norm: 0.00698575\n",
      "Epoch 3 | Step 1848700 | Avg Loss: 0.0149 | Grad Norm: 0.00746985\n",
      "Epoch 3 | Step 1848800 | Avg Loss: 0.0146 | Grad Norm: 0.00835607\n",
      "Epoch 3 | Step 1848900 | Avg Loss: 0.0146 | Grad Norm: 0.00863997\n",
      "Epoch 3 | Step 1849000 | Avg Loss: 0.0145 | Grad Norm: 0.00963506\n",
      "Epoch 3 | Step 1849100 | Avg Loss: 0.0144 | Grad Norm: 0.00814396\n",
      "Epoch 3 | Step 1849200 | Avg Loss: 0.0147 | Grad Norm: 0.00910672\n",
      "Epoch 3 | Step 1849300 | Avg Loss: 0.0151 | Grad Norm: 0.00830993\n",
      "Epoch 3 | Step 1849400 | Avg Loss: 0.0156 | Grad Norm: 0.00961416\n",
      "Epoch 3 | Step 1849500 | Avg Loss: 0.0155 | Grad Norm: 0.00814942\n",
      "Epoch 3 | Step 1849600 | Avg Loss: 0.0154 | Grad Norm: 0.00829960\n",
      "Epoch 3 | Step 1849700 | Avg Loss: 0.0155 | Grad Norm: 0.00959033\n",
      "Epoch 3 | Step 1849800 | Avg Loss: 0.0156 | Grad Norm: 0.00778512\n",
      "Epoch 3 | Step 1849900 | Avg Loss: 0.0150 | Grad Norm: 0.00894297\n",
      "Epoch 3 | Step 1850000 | Avg Loss: 0.0153 | Grad Norm: 0.00767099\n",
      "Epoch 3 | Step 1850100 | Avg Loss: 0.0156 | Grad Norm: 0.00861866\n",
      "Epoch 3 | Step 1850200 | Avg Loss: 0.0153 | Grad Norm: 0.00799898\n",
      "Epoch 3 | Step 1850300 | Avg Loss: 0.0151 | Grad Norm: 0.00701681\n",
      "Epoch 3 | Step 1850400 | Avg Loss: 0.0153 | Grad Norm: 0.00814418\n",
      "Epoch 3 | Step 1850500 | Avg Loss: 0.0150 | Grad Norm: 0.00918478\n",
      "Epoch 3 | Step 1850600 | Avg Loss: 0.0148 | Grad Norm: 0.00756052\n",
      "Epoch 3 | Step 1850700 | Avg Loss: 0.0147 | Grad Norm: 0.00685672\n",
      "Epoch 3 | Step 1850800 | Avg Loss: 0.0148 | Grad Norm: 0.00783061\n",
      "Epoch 3 | Step 1850900 | Avg Loss: 0.0147 | Grad Norm: 0.00899093\n",
      "Epoch 3 | Step 1851000 | Avg Loss: 0.0148 | Grad Norm: 0.00831451\n",
      "Epoch 3 | Step 1851100 | Avg Loss: 0.0151 | Grad Norm: 0.00818534\n",
      "Epoch 3 | Step 1851200 | Avg Loss: 0.0155 | Grad Norm: 0.00847691\n",
      "Epoch 3 | Step 1851300 | Avg Loss: 0.0153 | Grad Norm: 0.00846175\n",
      "Epoch 3 | Step 1851400 | Avg Loss: 0.0153 | Grad Norm: 0.00816452\n",
      "Epoch 3 | Step 1851500 | Avg Loss: 0.0153 | Grad Norm: 0.00976145\n",
      "Epoch 3 | Step 1851600 | Avg Loss: 0.0158 | Grad Norm: 0.00708825\n",
      "Epoch 3 | Step 1851700 | Avg Loss: 0.0156 | Grad Norm: 0.00775307\n",
      "Epoch 3 | Step 1851800 | Avg Loss: 0.0156 | Grad Norm: 0.00877159\n",
      "Epoch 3 | Step 1851900 | Avg Loss: 0.0150 | Grad Norm: 0.00910669\n",
      "Epoch 3 | Step 1852000 | Avg Loss: 0.0148 | Grad Norm: 0.01073066\n",
      "Epoch 3 | Step 1852100 | Avg Loss: 0.0150 | Grad Norm: 0.00788423\n",
      "Epoch 3 | Step 1852200 | Avg Loss: 0.0151 | Grad Norm: 0.00775418\n",
      "Epoch 3 | Step 1852300 | Avg Loss: 0.0153 | Grad Norm: 0.00875654\n",
      "Epoch 3 | Step 1852400 | Avg Loss: 0.0152 | Grad Norm: 0.00772491\n",
      "Epoch 3 | Step 1852500 | Avg Loss: 0.0154 | Grad Norm: 0.00906815\n",
      "Epoch 3 | Step 1852600 | Avg Loss: 0.0155 | Grad Norm: 0.00997704\n",
      "Epoch 3 | Step 1852700 | Avg Loss: 0.0152 | Grad Norm: 0.00881709\n",
      "Epoch 3 | Step 1852800 | Avg Loss: 0.0151 | Grad Norm: 0.00756866\n",
      "Epoch 3 | Step 1852900 | Avg Loss: 0.0147 | Grad Norm: 0.00841437\n",
      "Epoch 3 | Step 1853000 | Avg Loss: 0.0148 | Grad Norm: 0.00985846\n",
      "Epoch 3 | Step 1853100 | Avg Loss: 0.0150 | Grad Norm: 0.00824306\n",
      "Epoch 3 | Step 1853200 | Avg Loss: 0.0147 | Grad Norm: 0.00716952\n",
      "Epoch 3 | Step 1853300 | Avg Loss: 0.0147 | Grad Norm: 0.00766863\n",
      "Epoch 3 | Step 1853400 | Avg Loss: 0.0148 | Grad Norm: 0.00827505\n",
      "Epoch 3 | Step 1853500 | Avg Loss: 0.0153 | Grad Norm: 0.00871329\n",
      "Epoch 3 | Step 1853600 | Avg Loss: 0.0151 | Grad Norm: 0.00989342\n",
      "Epoch 3 | Step 1853700 | Avg Loss: 0.0152 | Grad Norm: 0.01242586\n",
      "Epoch 3 | Step 1853800 | Avg Loss: 0.0153 | Grad Norm: 0.00832895\n",
      "Epoch 3 | Step 1853900 | Avg Loss: 0.0156 | Grad Norm: 0.00758428\n",
      "Epoch 3 | Step 1854000 | Avg Loss: 0.0153 | Grad Norm: 0.00734530\n",
      "Epoch 3 | Step 1854100 | Avg Loss: 0.0151 | Grad Norm: 0.00812460\n",
      "Epoch 3 | Step 1854200 | Avg Loss: 0.0153 | Grad Norm: 0.01058440\n",
      "Epoch 3 | Step 1854300 | Avg Loss: 0.0154 | Grad Norm: 0.00907925\n",
      "Epoch 3 | Step 1854400 | Avg Loss: 0.0155 | Grad Norm: 0.00841092\n",
      "Epoch 3 | Step 1854500 | Avg Loss: 0.0155 | Grad Norm: 0.00748163\n",
      "Epoch 3 | Step 1854600 | Avg Loss: 0.0156 | Grad Norm: 0.00719712\n",
      "Epoch 3 | Step 1854700 | Avg Loss: 0.0154 | Grad Norm: 0.00816199\n",
      "Epoch 3 | Step 1854800 | Avg Loss: 0.0152 | Grad Norm: 0.00775422\n",
      "Epoch 3 | Step 1854900 | Avg Loss: 0.0150 | Grad Norm: 0.00953851\n",
      "Epoch 3 | Step 1855000 | Avg Loss: 0.0148 | Grad Norm: 0.00680836\n",
      "Epoch 3 | Step 1855100 | Avg Loss: 0.0148 | Grad Norm: 0.01115012\n",
      "Epoch 3 | Step 1855200 | Avg Loss: 0.0147 | Grad Norm: 0.00850987\n",
      "Epoch 3 | Step 1855300 | Avg Loss: 0.0151 | Grad Norm: 0.00712263\n",
      "Epoch 3 | Step 1855400 | Avg Loss: 0.0154 | Grad Norm: 0.00871466\n",
      "Epoch 3 | Step 1855500 | Avg Loss: 0.0156 | Grad Norm: 0.00790201\n",
      "Epoch 3 | Step 1855600 | Avg Loss: 0.0155 | Grad Norm: 0.00717099\n",
      "Epoch 3 | Step 1855700 | Avg Loss: 0.0155 | Grad Norm: 0.00770533\n",
      "Epoch 3 | Step 1855800 | Avg Loss: 0.0155 | Grad Norm: 0.00921141\n",
      "Epoch 3 | Step 1855900 | Avg Loss: 0.0154 | Grad Norm: 0.00801641\n",
      "Epoch 3 | Step 1856000 | Avg Loss: 0.0150 | Grad Norm: 0.00907833\n",
      "Epoch 3 | Step 1856100 | Avg Loss: 0.0149 | Grad Norm: 0.00794670\n",
      "Epoch 3 | Step 1856200 | Avg Loss: 0.0149 | Grad Norm: 0.00741502\n",
      "Epoch 3 | Step 1856300 | Avg Loss: 0.0148 | Grad Norm: 0.00891802\n",
      "Epoch 3 | Step 1856400 | Avg Loss: 0.0147 | Grad Norm: 0.00709903\n",
      "Epoch 3 | Step 1856500 | Avg Loss: 0.0143 | Grad Norm: 0.00851323\n",
      "Epoch 3 | Step 1856600 | Avg Loss: 0.0147 | Grad Norm: 0.00893558\n",
      "Epoch 3 | Step 1856700 | Avg Loss: 0.0148 | Grad Norm: 0.00795288\n",
      "Epoch 3 | Step 1856800 | Avg Loss: 0.0151 | Grad Norm: 0.00909554\n",
      "Epoch 3 | Step 1856900 | Avg Loss: 0.0151 | Grad Norm: 0.00746843\n",
      "Epoch 3 | Step 1857000 | Avg Loss: 0.0153 | Grad Norm: 0.00819348\n",
      "Epoch 3 | Step 1857100 | Avg Loss: 0.0155 | Grad Norm: 0.00758863\n",
      "Epoch 3 | Step 1857200 | Avg Loss: 0.0155 | Grad Norm: 0.01126649\n",
      "Epoch 3 | Step 1857300 | Avg Loss: 0.0159 | Grad Norm: 0.00842342\n",
      "Epoch 3 | Step 1857400 | Avg Loss: 0.0156 | Grad Norm: 0.01026955\n",
      "Epoch 3 | Step 1857500 | Avg Loss: 0.0155 | Grad Norm: 0.00735555\n",
      "Epoch 3 | Step 1857600 | Avg Loss: 0.0152 | Grad Norm: 0.00784937\n",
      "Epoch 3 | Step 1857700 | Avg Loss: 0.0151 | Grad Norm: 0.00788465\n",
      "Epoch 3 | Step 1857800 | Avg Loss: 0.0152 | Grad Norm: 0.00784737\n",
      "Epoch 3 | Step 1857900 | Avg Loss: 0.0149 | Grad Norm: 0.00931993\n",
      "Epoch 3 | Step 1858000 | Avg Loss: 0.0151 | Grad Norm: 0.00858424\n",
      "Epoch 3 | Step 1858100 | Avg Loss: 0.0153 | Grad Norm: 0.00796410\n",
      "Epoch 3 | Step 1858200 | Avg Loss: 0.0151 | Grad Norm: 0.01506137\n",
      "Epoch 3 | Step 1858300 | Avg Loss: 0.0150 | Grad Norm: 0.00786956\n",
      "Epoch 3 | Step 1858400 | Avg Loss: 0.0151 | Grad Norm: 0.00816945\n",
      "Epoch 3 | Step 1858500 | Avg Loss: 0.0155 | Grad Norm: 0.00831208\n",
      "Epoch 3 | Step 1858600 | Avg Loss: 0.0155 | Grad Norm: 0.00877372\n",
      "Epoch 3 | Step 1858700 | Avg Loss: 0.0156 | Grad Norm: 0.00848929\n",
      "Epoch 3 | Step 1858800 | Avg Loss: 0.0158 | Grad Norm: 0.00840196\n",
      "Epoch 3 | Step 1858900 | Avg Loss: 0.0155 | Grad Norm: 0.00892041\n",
      "Epoch 3 | Step 1859000 | Avg Loss: 0.0149 | Grad Norm: 0.01058130\n",
      "Epoch 3 | Step 1859100 | Avg Loss: 0.0151 | Grad Norm: 0.00794853\n",
      "Epoch 3 | Step 1859200 | Avg Loss: 0.0151 | Grad Norm: 0.01094169\n",
      "Epoch 3 | Step 1859300 | Avg Loss: 0.0152 | Grad Norm: 0.00819985\n",
      "Epoch 3 | Step 1859400 | Avg Loss: 0.0150 | Grad Norm: 0.00846643\n",
      "Epoch 3 | Step 1859500 | Avg Loss: 0.0155 | Grad Norm: 0.00844557\n",
      "Epoch 3 | Step 1859600 | Avg Loss: 0.0157 | Grad Norm: 0.00906098\n",
      "Epoch 3 | Step 1859700 | Avg Loss: 0.0156 | Grad Norm: 0.00805723\n",
      "Epoch 3 | Step 1859800 | Avg Loss: 0.0152 | Grad Norm: 0.00816647\n",
      "Epoch 3 | Step 1859900 | Avg Loss: 0.0154 | Grad Norm: 0.00787499\n",
      "Epoch 3 | Step 1860000 | Avg Loss: 0.0159 | Grad Norm: 0.00756571\n",
      "Epoch 3 | Step 1860100 | Avg Loss: 0.0157 | Grad Norm: 0.00850142\n",
      "Epoch 3 | Step 1860200 | Avg Loss: 0.0160 | Grad Norm: 0.00788180\n",
      "Epoch 3 | Step 1860300 | Avg Loss: 0.0159 | Grad Norm: 0.00838734\n",
      "Epoch 3 | Step 1860400 | Avg Loss: 0.0160 | Grad Norm: 0.00790542\n",
      "Epoch 3 | Step 1860500 | Avg Loss: 0.0158 | Grad Norm: 0.00815687\n",
      "Epoch 3 | Step 1860600 | Avg Loss: 0.0156 | Grad Norm: 0.00765467\n",
      "Epoch 3 | Step 1860700 | Avg Loss: 0.0150 | Grad Norm: 0.00870818\n",
      "Epoch 3 | Step 1860800 | Avg Loss: 0.0151 | Grad Norm: 0.00819884\n",
      "Epoch 3 | Step 1860900 | Avg Loss: 0.0152 | Grad Norm: 0.00762796\n",
      "Epoch 3 | Step 1861000 | Avg Loss: 0.0150 | Grad Norm: 0.00765181\n",
      "Epoch 3 | Step 1861100 | Avg Loss: 0.0152 | Grad Norm: 0.00868978\n",
      "Epoch 3 | Step 1861200 | Avg Loss: 0.0152 | Grad Norm: 0.00852260\n",
      "Epoch 3 | Step 1861300 | Avg Loss: 0.0150 | Grad Norm: 0.00726693\n",
      "Epoch 3 | Step 1861400 | Avg Loss: 0.0152 | Grad Norm: 0.00974592\n",
      "Epoch 3 | Step 1861500 | Avg Loss: 0.0148 | Grad Norm: 0.00778105\n",
      "Epoch 3 | Step 1861600 | Avg Loss: 0.0148 | Grad Norm: 0.00736168\n",
      "Epoch 3 | Step 1861700 | Avg Loss: 0.0149 | Grad Norm: 0.00804749\n",
      "Epoch 3 | Step 1861800 | Avg Loss: 0.0149 | Grad Norm: 0.00675704\n",
      "Epoch 3 | Step 1861900 | Avg Loss: 0.0148 | Grad Norm: 0.00950566\n",
      "Epoch 3 | Step 1862000 | Avg Loss: 0.0151 | Grad Norm: 0.00890218\n",
      "Epoch 3 | Step 1862100 | Avg Loss: 0.0147 | Grad Norm: 0.00966331\n",
      "Epoch 3 | Step 1862200 | Avg Loss: 0.0151 | Grad Norm: 0.00940425\n",
      "Epoch 3 | Step 1862300 | Avg Loss: 0.0147 | Grad Norm: 0.00711023\n",
      "Epoch 3 | Step 1862400 | Avg Loss: 0.0148 | Grad Norm: 0.00757084\n",
      "Epoch 3 | Step 1862500 | Avg Loss: 0.0149 | Grad Norm: 0.00772810\n",
      "Epoch 3 | Step 1862600 | Avg Loss: 0.0147 | Grad Norm: 0.00885144\n",
      "Epoch 3 | Step 1862700 | Avg Loss: 0.0150 | Grad Norm: 0.00810236\n",
      "Epoch 3 | Step 1862800 | Avg Loss: 0.0147 | Grad Norm: 0.00774356\n",
      "Epoch 3 | Step 1862900 | Avg Loss: 0.0143 | Grad Norm: 0.00871972\n",
      "Epoch 3 | Step 1863000 | Avg Loss: 0.0142 | Grad Norm: 0.00748848\n",
      "Epoch 3 | Step 1863100 | Avg Loss: 0.0144 | Grad Norm: 0.00792802\n",
      "Epoch 3 | Step 1863200 | Avg Loss: 0.0144 | Grad Norm: 0.00751755\n",
      "Epoch 3 | Step 1863300 | Avg Loss: 0.0143 | Grad Norm: 0.00916182\n",
      "Epoch 3 | Step 1863400 | Avg Loss: 0.0146 | Grad Norm: 0.00795437\n",
      "Epoch 3 | Step 1863500 | Avg Loss: 0.0145 | Grad Norm: 0.00873808\n",
      "Epoch 3 | Step 1863600 | Avg Loss: 0.0146 | Grad Norm: 0.00767475\n",
      "Epoch 3 | Step 1863700 | Avg Loss: 0.0146 | Grad Norm: 0.00838131\n",
      "Epoch 3 | Step 1863800 | Avg Loss: 0.0149 | Grad Norm: 0.00869847\n",
      "Epoch 3 | Step 1863900 | Avg Loss: 0.0151 | Grad Norm: 0.00923504\n",
      "Epoch 3 | Step 1864000 | Avg Loss: 0.0152 | Grad Norm: 0.00850320\n",
      "Epoch 3 | Step 1864100 | Avg Loss: 0.0149 | Grad Norm: 0.00740969\n",
      "Epoch 3 | Step 1864200 | Avg Loss: 0.0152 | Grad Norm: 0.00723092\n",
      "Epoch 3 | Step 1864300 | Avg Loss: 0.0152 | Grad Norm: 0.00782336\n",
      "Epoch 3 | Step 1864400 | Avg Loss: 0.0148 | Grad Norm: 0.00813029\n",
      "Epoch 3 | Step 1864500 | Avg Loss: 0.0146 | Grad Norm: 0.00984806\n",
      "Epoch 3 | Step 1864600 | Avg Loss: 0.0144 | Grad Norm: 0.00804359\n",
      "Epoch 3 | Step 1864700 | Avg Loss: 0.0148 | Grad Norm: 0.00768148\n",
      "Epoch 3 | Step 1864800 | Avg Loss: 0.0144 | Grad Norm: 0.00802434\n",
      "Epoch 3 | Step 1864900 | Avg Loss: 0.0146 | Grad Norm: 0.00779666\n",
      "Epoch 3 | Step 1865000 | Avg Loss: 0.0143 | Grad Norm: 0.00593428\n",
      "Epoch 3 | Step 1865100 | Avg Loss: 0.0148 | Grad Norm: 0.00903674\n",
      "Epoch 3 | Step 1865200 | Avg Loss: 0.0147 | Grad Norm: 0.00746104\n",
      "Epoch 3 | Step 1865300 | Avg Loss: 0.0147 | Grad Norm: 0.00733211\n",
      "Epoch 3 | Step 1865400 | Avg Loss: 0.0149 | Grad Norm: 0.00898721\n",
      "Epoch 3 | Step 1865500 | Avg Loss: 0.0149 | Grad Norm: 0.00871457\n",
      "Epoch 3 | Step 1865600 | Avg Loss: 0.0153 | Grad Norm: 0.00777672\n",
      "Epoch 3 | Step 1865700 | Avg Loss: 0.0149 | Grad Norm: 0.01188241\n",
      "Epoch 3 | Step 1865800 | Avg Loss: 0.0151 | Grad Norm: 0.00830898\n",
      "Epoch 3 | Step 1865900 | Avg Loss: 0.0150 | Grad Norm: 0.00770215\n",
      "Epoch 3 | Step 1866000 | Avg Loss: 0.0148 | Grad Norm: 0.00770460\n",
      "Epoch 3 | Step 1866100 | Avg Loss: 0.0147 | Grad Norm: 0.00821070\n",
      "Epoch 3 | Step 1866200 | Avg Loss: 0.0148 | Grad Norm: 0.00820291\n",
      "Epoch 3 | Step 1866300 | Avg Loss: 0.0149 | Grad Norm: 0.00805865\n",
      "Epoch 3 | Step 1866400 | Avg Loss: 0.0147 | Grad Norm: 0.00727402\n",
      "Epoch 3 | Step 1866500 | Avg Loss: 0.0151 | Grad Norm: 0.00745964\n",
      "Epoch 3 | Step 1866600 | Avg Loss: 0.0151 | Grad Norm: 0.00806452\n",
      "Epoch 3 | Step 1866700 | Avg Loss: 0.0149 | Grad Norm: 0.00805308\n",
      "Epoch 3 | Step 1866800 | Avg Loss: 0.0150 | Grad Norm: 0.00815111\n",
      "Epoch 3 | Step 1866900 | Avg Loss: 0.0147 | Grad Norm: 0.00710836\n",
      "Epoch 3 | Step 1867000 | Avg Loss: 0.0148 | Grad Norm: 0.01053980\n",
      "Epoch 3 | Step 1867100 | Avg Loss: 0.0147 | Grad Norm: 0.00750924\n",
      "Epoch 3 | Step 1867200 | Avg Loss: 0.0148 | Grad Norm: 0.00811407\n",
      "Epoch 3 | Step 1867300 | Avg Loss: 0.0145 | Grad Norm: 0.00995552\n",
      "Epoch 3 | Step 1867400 | Avg Loss: 0.0146 | Grad Norm: 0.00891063\n",
      "Epoch 3 | Step 1867500 | Avg Loss: 0.0147 | Grad Norm: 0.00898888\n",
      "Epoch 3 | Step 1867600 | Avg Loss: 0.0148 | Grad Norm: 0.00758380\n",
      "Epoch 3 | Step 1867700 | Avg Loss: 0.0146 | Grad Norm: 0.00959326\n",
      "Epoch 3 | Step 1867800 | Avg Loss: 0.0148 | Grad Norm: 0.00865862\n",
      "Epoch 3 | Step 1867900 | Avg Loss: 0.0146 | Grad Norm: 0.00692675\n",
      "Epoch 3 | Step 1868000 | Avg Loss: 0.0148 | Grad Norm: 0.00770247\n",
      "Epoch 3 | Step 1868100 | Avg Loss: 0.0149 | Grad Norm: 0.00876370\n",
      "Epoch 3 | Step 1868200 | Avg Loss: 0.0149 | Grad Norm: 0.00729265\n",
      "Epoch 3 | Step 1868300 | Avg Loss: 0.0149 | Grad Norm: 0.00676464\n",
      "Epoch 3 | Step 1868400 | Avg Loss: 0.0150 | Grad Norm: 0.00956472\n",
      "Epoch 3 | Step 1868500 | Avg Loss: 0.0150 | Grad Norm: 0.00902867\n",
      "Epoch 3 | Step 1868600 | Avg Loss: 0.0151 | Grad Norm: 0.00637070\n",
      "Epoch 3 | Step 1868700 | Avg Loss: 0.0145 | Grad Norm: 0.00795475\n",
      "Epoch 3 | Step 1868800 | Avg Loss: 0.0145 | Grad Norm: 0.00767168\n",
      "Epoch 3 | Step 1868900 | Avg Loss: 0.0151 | Grad Norm: 0.00728042\n",
      "Epoch 3 | Step 1869000 | Avg Loss: 0.0148 | Grad Norm: 0.00823616\n",
      "Epoch 3 | Step 1869100 | Avg Loss: 0.0150 | Grad Norm: 0.00864087\n",
      "Epoch 3 | Step 1869200 | Avg Loss: 0.0150 | Grad Norm: 0.00750851\n",
      "Epoch 3 | Step 1869300 | Avg Loss: 0.0148 | Grad Norm: 0.00869763\n",
      "Epoch 3 | Step 1869400 | Avg Loss: 0.0150 | Grad Norm: 0.00748177\n",
      "Epoch 3 | Step 1869500 | Avg Loss: 0.0151 | Grad Norm: 0.00846660\n",
      "Epoch 3 | Step 1869600 | Avg Loss: 0.0152 | Grad Norm: 0.00810749\n",
      "Epoch 3 | Step 1869700 | Avg Loss: 0.0152 | Grad Norm: 0.00872321\n",
      "Epoch 3 | Step 1869800 | Avg Loss: 0.0151 | Grad Norm: 0.00822300\n",
      "Epoch 3 | Step 1869900 | Avg Loss: 0.0151 | Grad Norm: 0.00741739\n",
      "Epoch 3 | Step 1870000 | Avg Loss: 0.0152 | Grad Norm: 0.00747885\n",
      "Epoch 3 | Step 1870100 | Avg Loss: 0.0149 | Grad Norm: 0.00727247\n",
      "Epoch 3 | Step 1870200 | Avg Loss: 0.0150 | Grad Norm: 0.00724292\n",
      "Epoch 3 | Step 1870300 | Avg Loss: 0.0149 | Grad Norm: 0.00880615\n",
      "Epoch 3 | Step 1870400 | Avg Loss: 0.0149 | Grad Norm: 0.00787884\n",
      "Epoch 3 | Step 1870500 | Avg Loss: 0.0153 | Grad Norm: 0.00718535\n",
      "Epoch 3 | Step 1870600 | Avg Loss: 0.0151 | Grad Norm: 0.00769674\n",
      "Epoch 3 | Step 1870700 | Avg Loss: 0.0149 | Grad Norm: 0.00799220\n",
      "Epoch 3 | Step 1870800 | Avg Loss: 0.0147 | Grad Norm: 0.00910030\n",
      "Epoch 3 | Step 1870900 | Avg Loss: 0.0150 | Grad Norm: 0.00814214\n",
      "Epoch 3 | Step 1871000 | Avg Loss: 0.0151 | Grad Norm: 0.00785556\n",
      "Epoch 3 | Step 1871100 | Avg Loss: 0.0149 | Grad Norm: 0.00918178\n",
      "Epoch 3 | Step 1871200 | Avg Loss: 0.0146 | Grad Norm: 0.01071271\n",
      "Epoch 3 | Step 1871300 | Avg Loss: 0.0149 | Grad Norm: 0.00740824\n",
      "Epoch 3 | Step 1871400 | Avg Loss: 0.0148 | Grad Norm: 0.00839633\n",
      "Epoch 3 | Step 1871500 | Avg Loss: 0.0149 | Grad Norm: 0.00842755\n",
      "Epoch 3 | Step 1871600 | Avg Loss: 0.0144 | Grad Norm: 0.00901075\n",
      "Epoch 3 | Step 1871700 | Avg Loss: 0.0146 | Grad Norm: 0.00691862\n",
      "Epoch 3 | Step 1871800 | Avg Loss: 0.0147 | Grad Norm: 0.01036691\n",
      "Epoch 3 | Step 1871900 | Avg Loss: 0.0149 | Grad Norm: 0.00788452\n",
      "Epoch 3 | Step 1872000 | Avg Loss: 0.0146 | Grad Norm: 0.00697781\n",
      "Epoch 3 | Step 1872100 | Avg Loss: 0.0144 | Grad Norm: 0.00824478\n",
      "Epoch 3 | Step 1872200 | Avg Loss: 0.0148 | Grad Norm: 0.00884131\n",
      "Epoch 3 | Step 1872300 | Avg Loss: 0.0149 | Grad Norm: 0.00717158\n",
      "Epoch 3 | Step 1872400 | Avg Loss: 0.0147 | Grad Norm: 0.00909151\n",
      "Epoch 3 | Step 1872500 | Avg Loss: 0.0153 | Grad Norm: 0.00841668\n",
      "Epoch 3 | Step 1872600 | Avg Loss: 0.0153 | Grad Norm: 0.00839689\n",
      "Epoch 3 | Step 1872700 | Avg Loss: 0.0153 | Grad Norm: 0.01165447\n",
      "Epoch 3 | Step 1872800 | Avg Loss: 0.0152 | Grad Norm: 0.00906365\n",
      "Epoch 3 | Step 1872900 | Avg Loss: 0.0148 | Grad Norm: 0.00732000\n",
      "Epoch 3 | Step 1873000 | Avg Loss: 0.0147 | Grad Norm: 0.00815233\n",
      "Epoch 3 | Step 1873100 | Avg Loss: 0.0147 | Grad Norm: 0.00853338\n",
      "Epoch 3 | Step 1873200 | Avg Loss: 0.0152 | Grad Norm: 0.00808409\n",
      "Epoch 3 | Step 1873300 | Avg Loss: 0.0151 | Grad Norm: 0.00783140\n",
      "Epoch 3 | Step 1873400 | Avg Loss: 0.0151 | Grad Norm: 0.00859230\n",
      "Epoch 3 | Step 1873500 | Avg Loss: 0.0149 | Grad Norm: 0.00836443\n",
      "Epoch 3 | Step 1873600 | Avg Loss: 0.0149 | Grad Norm: 0.00747087\n",
      "Epoch 3 | Step 1873700 | Avg Loss: 0.0152 | Grad Norm: 0.00729048\n",
      "Epoch 3 | Step 1873800 | Avg Loss: 0.0155 | Grad Norm: 0.00798947\n",
      "Epoch 3 | Step 1873900 | Avg Loss: 0.0152 | Grad Norm: 0.00751488\n",
      "Epoch 3 | Step 1874000 | Avg Loss: 0.0150 | Grad Norm: 0.00775862\n",
      "Epoch 3 | Step 1874100 | Avg Loss: 0.0151 | Grad Norm: 0.00892816\n",
      "Epoch 3 | Step 1874200 | Avg Loss: 0.0154 | Grad Norm: 0.00830500\n",
      "Epoch 3 | Step 1874300 | Avg Loss: 0.0149 | Grad Norm: 0.00896854\n",
      "Epoch 3 | Step 1874400 | Avg Loss: 0.0150 | Grad Norm: 0.00921692\n",
      "Epoch 3 | Step 1874500 | Avg Loss: 0.0148 | Grad Norm: 0.00787818\n",
      "Epoch 3 | Step 1874600 | Avg Loss: 0.0152 | Grad Norm: 0.00819758\n",
      "Epoch 3 | Step 1874700 | Avg Loss: 0.0154 | Grad Norm: 0.00825814\n",
      "Epoch 3 | Step 1874800 | Avg Loss: 0.0151 | Grad Norm: 0.00758296\n",
      "Epoch 3 | Step 1874900 | Avg Loss: 0.0154 | Grad Norm: 0.00707945\n",
      "Epoch 3 | Step 1875000 | Avg Loss: 0.0152 | Grad Norm: 0.01127140\n",
      "Epoch 3 | Step 1875100 | Avg Loss: 0.0154 | Grad Norm: 0.00831482\n",
      "Epoch 3 | Step 1875200 | Avg Loss: 0.0154 | Grad Norm: 0.00765421\n",
      "Epoch 3 | Step 1875300 | Avg Loss: 0.0152 | Grad Norm: 0.00759001\n",
      "Epoch 3 | Step 1875400 | Avg Loss: 0.0152 | Grad Norm: 0.00788087\n",
      "Epoch 3 | Step 1875500 | Avg Loss: 0.0151 | Grad Norm: 0.00728758\n",
      "Epoch 3 | Step 1875600 | Avg Loss: 0.0149 | Grad Norm: 0.00716025\n",
      "Epoch 3 | Step 1875700 | Avg Loss: 0.0149 | Grad Norm: 0.00803608\n",
      "Epoch 3 | Step 1875800 | Avg Loss: 0.0151 | Grad Norm: 0.00878790\n",
      "Epoch 3 | Step 1875900 | Avg Loss: 0.0147 | Grad Norm: 0.00765164\n",
      "Epoch 3 | Step 1876000 | Avg Loss: 0.0147 | Grad Norm: 0.00791496\n",
      "Epoch 3 | Step 1876100 | Avg Loss: 0.0145 | Grad Norm: 0.00728795\n",
      "Epoch 3 | Step 1876200 | Avg Loss: 0.0147 | Grad Norm: 0.00677164\n",
      "Epoch 3 | Step 1876300 | Avg Loss: 0.0149 | Grad Norm: 0.00769890\n",
      "Epoch 3 | Step 1876400 | Avg Loss: 0.0146 | Grad Norm: 0.00915765\n",
      "Epoch 3 | Step 1876500 | Avg Loss: 0.0146 | Grad Norm: 0.00873221\n",
      "Epoch 3 | Step 1876600 | Avg Loss: 0.0148 | Grad Norm: 0.00688194\n",
      "Epoch 3 | Step 1876700 | Avg Loss: 0.0144 | Grad Norm: 0.00711390\n",
      "Epoch 3 | Step 1876800 | Avg Loss: 0.0141 | Grad Norm: 0.00958700\n",
      "Epoch 3 | Step 1876900 | Avg Loss: 0.0144 | Grad Norm: 0.00820837\n",
      "Epoch 3 | Step 1877000 | Avg Loss: 0.0148 | Grad Norm: 0.00629545\n",
      "Epoch 3 | Step 1877100 | Avg Loss: 0.0151 | Grad Norm: 0.00785328\n",
      "Epoch 3 | Step 1877200 | Avg Loss: 0.0153 | Grad Norm: 0.00661568\n",
      "Epoch 3 | Step 1877300 | Avg Loss: 0.0154 | Grad Norm: 0.00896060\n",
      "Epoch 3 | Step 1877400 | Avg Loss: 0.0155 | Grad Norm: 0.00809401\n",
      "Epoch 3 | Step 1877500 | Avg Loss: 0.0157 | Grad Norm: 0.00990691\n",
      "Epoch 3 | Step 1877600 | Avg Loss: 0.0154 | Grad Norm: 0.00875145\n",
      "Epoch 3 | Step 1877700 | Avg Loss: 0.0155 | Grad Norm: 0.00724923\n",
      "Epoch 3 | Step 1877800 | Avg Loss: 0.0153 | Grad Norm: 0.00865339\n",
      "Epoch 3 | Step 1877900 | Avg Loss: 0.0152 | Grad Norm: 0.00868801\n",
      "Epoch 3 | Step 1878000 | Avg Loss: 0.0150 | Grad Norm: 0.00745784\n",
      "Epoch 3 | Step 1878100 | Avg Loss: 0.0155 | Grad Norm: 0.00761758\n",
      "Epoch 3 | Step 1878200 | Avg Loss: 0.0153 | Grad Norm: 0.00864549\n",
      "Epoch 3 | Step 1878300 | Avg Loss: 0.0154 | Grad Norm: 0.00833374\n",
      "Epoch 3 | Step 1878400 | Avg Loss: 0.0153 | Grad Norm: 0.00842702\n",
      "Epoch 3 | Step 1878500 | Avg Loss: 0.0154 | Grad Norm: 0.00813736\n",
      "Epoch 3 | Step 1878600 | Avg Loss: 0.0154 | Grad Norm: 0.00808333\n",
      "Epoch 3 | Step 1878700 | Avg Loss: 0.0152 | Grad Norm: 0.00727902\n",
      "Epoch 3 | Step 1878800 | Avg Loss: 0.0152 | Grad Norm: 0.00739948\n",
      "Epoch 3 | Step 1878900 | Avg Loss: 0.0150 | Grad Norm: 0.00765335\n",
      "Epoch 3 | Step 1879000 | Avg Loss: 0.0148 | Grad Norm: 0.00848882\n",
      "Epoch 3 | Step 1879100 | Avg Loss: 0.0151 | Grad Norm: 0.00833499\n",
      "Epoch 3 | Step 1879200 | Avg Loss: 0.0151 | Grad Norm: 0.00731144\n",
      "Epoch 3 | Step 1879300 | Avg Loss: 0.0153 | Grad Norm: 0.00979298\n",
      "Epoch 3 | Step 1879400 | Avg Loss: 0.0151 | Grad Norm: 0.00891360\n",
      "Epoch 3 | Step 1879500 | Avg Loss: 0.0152 | Grad Norm: 0.00899031\n",
      "Epoch 3 | Step 1879600 | Avg Loss: 0.0156 | Grad Norm: 0.00912440\n",
      "Epoch 3 | Step 1879700 | Avg Loss: 0.0160 | Grad Norm: 0.00903203\n",
      "Epoch 3 | Step 1879800 | Avg Loss: 0.0157 | Grad Norm: 0.00894007\n",
      "Epoch 3 | Step 1879900 | Avg Loss: 0.0157 | Grad Norm: 0.01033122\n",
      "Epoch 3 | Step 1880000 | Avg Loss: 0.0155 | Grad Norm: 0.00811256\n",
      "Epoch 3 | Step 1880100 | Avg Loss: 0.0153 | Grad Norm: 0.00733108\n",
      "Epoch 3 | Step 1880200 | Avg Loss: 0.0152 | Grad Norm: 0.01065066\n",
      "Epoch 3 | Step 1880300 | Avg Loss: 0.0152 | Grad Norm: 0.00758088\n",
      "Epoch 3 | Step 1880400 | Avg Loss: 0.0149 | Grad Norm: 0.00808115\n",
      "Epoch 3 | Step 1880500 | Avg Loss: 0.0149 | Grad Norm: 0.00919551\n",
      "Epoch 3 | Step 1880600 | Avg Loss: 0.0152 | Grad Norm: 0.00835889\n",
      "Epoch 3 | Step 1880700 | Avg Loss: 0.0151 | Grad Norm: 0.00810579\n",
      "Epoch 3 | Step 1880800 | Avg Loss: 0.0148 | Grad Norm: 0.00859027\n",
      "Epoch 3 | Step 1880900 | Avg Loss: 0.0146 | Grad Norm: 0.00788973\n",
      "Epoch 3 | Step 1881000 | Avg Loss: 0.0148 | Grad Norm: 0.00805096\n",
      "Epoch 3 | Step 1881100 | Avg Loss: 0.0149 | Grad Norm: 0.00778544\n",
      "Epoch 3 | Step 1881200 | Avg Loss: 0.0151 | Grad Norm: 0.00774231\n",
      "Epoch 3 | Step 1881300 | Avg Loss: 0.0151 | Grad Norm: 0.01095851\n",
      "Epoch 3 | Step 1881400 | Avg Loss: 0.0156 | Grad Norm: 0.00948650\n",
      "Epoch 3 | Step 1881500 | Avg Loss: 0.0155 | Grad Norm: 0.00775661\n",
      "Epoch 3 | Step 1881600 | Avg Loss: 0.0153 | Grad Norm: 0.00892140\n",
      "Epoch 3 | Step 1881700 | Avg Loss: 0.0154 | Grad Norm: 0.00771258\n",
      "Epoch 3 | Step 1881800 | Avg Loss: 0.0153 | Grad Norm: 0.00781634\n",
      "Epoch 3 | Step 1881900 | Avg Loss: 0.0151 | Grad Norm: 0.00957639\n",
      "Epoch 3 | Step 1882000 | Avg Loss: 0.0155 | Grad Norm: 0.00797577\n",
      "Epoch 3 | Step 1882100 | Avg Loss: 0.0157 | Grad Norm: 0.00822003\n",
      "Epoch 3 | Step 1882200 | Avg Loss: 0.0160 | Grad Norm: 0.00909600\n",
      "Epoch 3 | Step 1882300 | Avg Loss: 0.0161 | Grad Norm: 0.00998722\n",
      "Epoch 3 | Step 1882400 | Avg Loss: 0.0159 | Grad Norm: 0.00725973\n",
      "Epoch 3 | Step 1882500 | Avg Loss: 0.0160 | Grad Norm: 0.00877082\n",
      "Epoch 3 | Step 1882600 | Avg Loss: 0.0156 | Grad Norm: 0.00737335\n",
      "Epoch 3 | Step 1882700 | Avg Loss: 0.0154 | Grad Norm: 0.00785253\n",
      "Epoch 3 | Step 1882800 | Avg Loss: 0.0156 | Grad Norm: 0.00660782\n",
      "Epoch 3 | Step 1882900 | Avg Loss: 0.0155 | Grad Norm: 0.00923300\n",
      "Epoch 3 | Step 1883000 | Avg Loss: 0.0156 | Grad Norm: 0.00958866\n",
      "Epoch 3 | Step 1883100 | Avg Loss: 0.0161 | Grad Norm: 0.00830042\n",
      "Epoch 3 | Step 1883200 | Avg Loss: 0.0154 | Grad Norm: 0.00792726\n",
      "Epoch 3 | Step 1883300 | Avg Loss: 0.0153 | Grad Norm: 0.00736699\n",
      "Epoch 3 | Step 1883400 | Avg Loss: 0.0152 | Grad Norm: 0.00821220\n",
      "Epoch 3 | Step 1883500 | Avg Loss: 0.0153 | Grad Norm: 0.00783504\n",
      "Epoch 3 | Step 1883600 | Avg Loss: 0.0156 | Grad Norm: 0.00953161\n",
      "Epoch 3 | Step 1883700 | Avg Loss: 0.0155 | Grad Norm: 0.00865567\n",
      "Epoch 3 | Step 1883800 | Avg Loss: 0.0151 | Grad Norm: 0.00754804\n",
      "Epoch 3 | Step 1883900 | Avg Loss: 0.0153 | Grad Norm: 0.00954364\n",
      "Epoch 3 | Step 1884000 | Avg Loss: 0.0152 | Grad Norm: 0.01084380\n",
      "Epoch 3 | Step 1884100 | Avg Loss: 0.0150 | Grad Norm: 0.00999796\n",
      "Epoch 3 | Step 1884200 | Avg Loss: 0.0152 | Grad Norm: 0.00811395\n",
      "Epoch 3 | Step 1884300 | Avg Loss: 0.0155 | Grad Norm: 0.00953655\n",
      "Epoch 3 | Step 1884400 | Avg Loss: 0.0156 | Grad Norm: 0.01008321\n",
      "Epoch 3 | Step 1884500 | Avg Loss: 0.0155 | Grad Norm: 0.00804881\n",
      "Epoch 3 | Step 1884600 | Avg Loss: 0.0151 | Grad Norm: 0.01016926\n",
      "Epoch 3 | Step 1884700 | Avg Loss: 0.0148 | Grad Norm: 0.00799652\n",
      "Epoch 3 | Step 1884800 | Avg Loss: 0.0151 | Grad Norm: 0.00904157\n",
      "Epoch 3 | Step 1884900 | Avg Loss: 0.0151 | Grad Norm: 0.00681522\n",
      "Epoch 3 | Step 1885000 | Avg Loss: 0.0153 | Grad Norm: 0.00975166\n",
      "Epoch 3 | Step 1885100 | Avg Loss: 0.0148 | Grad Norm: 0.00886638\n",
      "Epoch 3 | Step 1885200 | Avg Loss: 0.0149 | Grad Norm: 0.00798479\n",
      "Epoch 3 | Step 1885300 | Avg Loss: 0.0151 | Grad Norm: 0.00907672\n",
      "Epoch 3 | Step 1885400 | Avg Loss: 0.0153 | Grad Norm: 0.00848250\n",
      "Epoch 3 | Step 1885500 | Avg Loss: 0.0151 | Grad Norm: 0.00813411\n",
      "Epoch 3 | Step 1885600 | Avg Loss: 0.0152 | Grad Norm: 0.00875209\n",
      "Epoch 3 | Step 1885700 | Avg Loss: 0.0150 | Grad Norm: 0.00785840\n",
      "Epoch 3 | Step 1885800 | Avg Loss: 0.0151 | Grad Norm: 0.00828838\n",
      "Epoch 3 | Step 1885900 | Avg Loss: 0.0149 | Grad Norm: 0.00934903\n",
      "Epoch 3 | Step 1886000 | Avg Loss: 0.0153 | Grad Norm: 0.00911788\n",
      "Epoch 3 | Step 1886100 | Avg Loss: 0.0156 | Grad Norm: 0.00880957\n",
      "Epoch 3 | Step 1886200 | Avg Loss: 0.0154 | Grad Norm: 0.00896081\n",
      "Epoch 3 | Step 1886300 | Avg Loss: 0.0149 | Grad Norm: 0.00689584\n",
      "Epoch 3 | Step 1886400 | Avg Loss: 0.0147 | Grad Norm: 0.00715377\n",
      "Epoch 3 | Step 1886500 | Avg Loss: 0.0148 | Grad Norm: 0.00892891\n",
      "Epoch 3 | Step 1886600 | Avg Loss: 0.0150 | Grad Norm: 0.00802888\n",
      "Epoch 3 | Step 1886700 | Avg Loss: 0.0150 | Grad Norm: 0.01051324\n",
      "Epoch 3 | Step 1886800 | Avg Loss: 0.0149 | Grad Norm: 0.00944478\n",
      "Epoch 3 | Step 1886900 | Avg Loss: 0.0148 | Grad Norm: 0.00770074\n",
      "Epoch 3 | Step 1887000 | Avg Loss: 0.0150 | Grad Norm: 0.00807990\n",
      "Epoch 3 | Step 1887100 | Avg Loss: 0.0149 | Grad Norm: 0.00871117\n",
      "Epoch 3 | Step 1887200 | Avg Loss: 0.0152 | Grad Norm: 0.00712390\n",
      "Epoch 3 | Step 1887300 | Avg Loss: 0.0152 | Grad Norm: 0.00819991\n",
      "Epoch 3 | Step 1887400 | Avg Loss: 0.0156 | Grad Norm: 0.00829801\n",
      "Epoch 3 | Step 1887500 | Avg Loss: 0.0156 | Grad Norm: 0.00749581\n",
      "Epoch 3 | Step 1887600 | Avg Loss: 0.0155 | Grad Norm: 0.00917687\n",
      "Epoch 3 | Step 1887700 | Avg Loss: 0.0149 | Grad Norm: 0.00976697\n",
      "Epoch 3 | Step 1887800 | Avg Loss: 0.0147 | Grad Norm: 0.00748521\n",
      "Epoch 3 | Step 1887900 | Avg Loss: 0.0146 | Grad Norm: 0.00782832\n",
      "Epoch 3 | Step 1888000 | Avg Loss: 0.0147 | Grad Norm: 0.00916413\n",
      "Epoch 3 | Step 1888100 | Avg Loss: 0.0150 | Grad Norm: 0.00859698\n",
      "Epoch 3 | Step 1888200 | Avg Loss: 0.0149 | Grad Norm: 0.00812971\n",
      "Epoch 3 | Step 1888300 | Avg Loss: 0.0150 | Grad Norm: 0.00754028\n",
      "Epoch 3 | Step 1888400 | Avg Loss: 0.0149 | Grad Norm: 0.00884578\n",
      "Epoch 3 | Step 1888500 | Avg Loss: 0.0148 | Grad Norm: 0.00842324\n",
      "Epoch 3 | Step 1888600 | Avg Loss: 0.0149 | Grad Norm: 0.00795868\n",
      "Epoch 3 | Step 1888700 | Avg Loss: 0.0144 | Grad Norm: 0.00790452\n",
      "Epoch 3 | Step 1888800 | Avg Loss: 0.0141 | Grad Norm: 0.00782578\n",
      "Epoch 3 | Step 1888900 | Avg Loss: 0.0141 | Grad Norm: 0.00807781\n",
      "Epoch 3 | Step 1889000 | Avg Loss: 0.0144 | Grad Norm: 0.00800221\n",
      "Epoch 3 | Step 1889100 | Avg Loss: 0.0147 | Grad Norm: 0.00923950\n",
      "Epoch 3 | Step 1889200 | Avg Loss: 0.0148 | Grad Norm: 0.00858632\n",
      "Epoch 3 | Step 1889300 | Avg Loss: 0.0149 | Grad Norm: 0.00780330\n",
      "Epoch 3 | Step 1889400 | Avg Loss: 0.0151 | Grad Norm: 0.00836076\n",
      "Epoch 3 | Step 1889500 | Avg Loss: 0.0153 | Grad Norm: 0.00753305\n",
      "Epoch 3 | Step 1889600 | Avg Loss: 0.0154 | Grad Norm: 0.00855373\n",
      "Epoch 3 | Step 1889700 | Avg Loss: 0.0157 | Grad Norm: 0.00822128\n",
      "Epoch 3 | Step 1889800 | Avg Loss: 0.0151 | Grad Norm: 0.00706194\n",
      "Epoch 3 | Step 1889900 | Avg Loss: 0.0150 | Grad Norm: 0.00862186\n",
      "Epoch 3 | Step 1890000 | Avg Loss: 0.0152 | Grad Norm: 0.00853431\n",
      "Epoch 3 | Step 1890100 | Avg Loss: 0.0149 | Grad Norm: 0.00906655\n",
      "Epoch 3 | Step 1890200 | Avg Loss: 0.0149 | Grad Norm: 0.00710655\n",
      "Epoch 3 | Step 1890300 | Avg Loss: 0.0145 | Grad Norm: 0.00831060\n",
      "Epoch 3 | Step 1890400 | Avg Loss: 0.0143 | Grad Norm: 0.00755116\n",
      "Epoch 3 | Step 1890500 | Avg Loss: 0.0145 | Grad Norm: 0.00602185\n",
      "Epoch 3 | Step 1890600 | Avg Loss: 0.0146 | Grad Norm: 0.00757818\n",
      "Epoch 3 | Step 1890700 | Avg Loss: 0.0148 | Grad Norm: 0.00728578\n",
      "Epoch 3 | Step 1890800 | Avg Loss: 0.0147 | Grad Norm: 0.00891007\n",
      "Epoch 3 | Step 1890900 | Avg Loss: 0.0147 | Grad Norm: 0.01094425\n",
      "Epoch 3 | Step 1891000 | Avg Loss: 0.0151 | Grad Norm: 0.00680597\n",
      "Epoch 3 | Step 1891100 | Avg Loss: 0.0149 | Grad Norm: 0.00774436\n",
      "Epoch 3 | Step 1891200 | Avg Loss: 0.0150 | Grad Norm: 0.00799775\n",
      "Epoch 3 | Step 1891300 | Avg Loss: 0.0154 | Grad Norm: 0.00766072\n",
      "Epoch 3 | Step 1891400 | Avg Loss: 0.0154 | Grad Norm: 0.01222190\n",
      "Epoch 3 | Step 1891500 | Avg Loss: 0.0149 | Grad Norm: 0.01010752\n",
      "Epoch 3 | Step 1891600 | Avg Loss: 0.0151 | Grad Norm: 0.00755736\n",
      "Epoch 3 | Step 1891700 | Avg Loss: 0.0150 | Grad Norm: 0.00797168\n",
      "Epoch 3 | Step 1891800 | Avg Loss: 0.0153 | Grad Norm: 0.00847891\n",
      "Epoch 3 | Step 1891900 | Avg Loss: 0.0152 | Grad Norm: 0.00951811\n",
      "Epoch 3 | Step 1892000 | Avg Loss: 0.0145 | Grad Norm: 0.00788486\n",
      "Epoch 3 | Step 1892100 | Avg Loss: 0.0148 | Grad Norm: 0.00839881\n",
      "Epoch 3 | Step 1892200 | Avg Loss: 0.0146 | Grad Norm: 0.00677764\n",
      "Epoch 3 | Step 1892300 | Avg Loss: 0.0148 | Grad Norm: 0.01031724\n",
      "Epoch 3 | Step 1892400 | Avg Loss: 0.0150 | Grad Norm: 0.00836420\n",
      "Epoch 3 | Step 1892500 | Avg Loss: 0.0147 | Grad Norm: 0.00801322\n",
      "Epoch 3 | Step 1892600 | Avg Loss: 0.0145 | Grad Norm: 0.00660661\n",
      "Epoch 3 | Step 1892700 | Avg Loss: 0.0146 | Grad Norm: 0.00819077\n",
      "Epoch 3 | Step 1892800 | Avg Loss: 0.0150 | Grad Norm: 0.00768699\n",
      "Epoch 3 | Step 1892900 | Avg Loss: 0.0153 | Grad Norm: 0.00971812\n",
      "Epoch 3 | Step 1893000 | Avg Loss: 0.0153 | Grad Norm: 0.00892569\n",
      "Epoch 3 | Step 1893100 | Avg Loss: 0.0152 | Grad Norm: 0.00772220\n",
      "Epoch 3 | Step 1893200 | Avg Loss: 0.0153 | Grad Norm: 0.00881426\n",
      "Epoch 3 | Step 1893300 | Avg Loss: 0.0159 | Grad Norm: 0.00996636\n",
      "Epoch 3 | Step 1893400 | Avg Loss: 0.0158 | Grad Norm: 0.00807898\n",
      "Epoch 3 | Step 1893500 | Avg Loss: 0.0157 | Grad Norm: 0.00799303\n",
      "Epoch 3 | Step 1893600 | Avg Loss: 0.0159 | Grad Norm: 0.00978874\n",
      "Epoch 3 | Step 1893700 | Avg Loss: 0.0156 | Grad Norm: 0.00759938\n",
      "Epoch 3 | Step 1893800 | Avg Loss: 0.0154 | Grad Norm: 0.00842922\n",
      "Epoch 3 | Step 1893900 | Avg Loss: 0.0152 | Grad Norm: 0.00861700\n",
      "Epoch 3 | Step 1894000 | Avg Loss: 0.0158 | Grad Norm: 0.01061412\n",
      "Epoch 3 | Step 1894100 | Avg Loss: 0.0160 | Grad Norm: 0.00787925\n",
      "Epoch 3 | Step 1894200 | Avg Loss: 0.0157 | Grad Norm: 0.00746219\n",
      "Epoch 3 | Step 1894300 | Avg Loss: 0.0156 | Grad Norm: 0.00890613\n",
      "Epoch 3 | Step 1894400 | Avg Loss: 0.0152 | Grad Norm: 0.00866966\n",
      "Epoch 3 | Step 1894500 | Avg Loss: 0.0151 | Grad Norm: 0.00851267\n",
      "Epoch 3 | Step 1894600 | Avg Loss: 0.0154 | Grad Norm: 0.00848669\n",
      "Epoch 3 | Step 1894700 | Avg Loss: 0.0152 | Grad Norm: 0.00918143\n",
      "Epoch 3 | Step 1894800 | Avg Loss: 0.0150 | Grad Norm: 0.00789358\n",
      "Epoch 3 | Step 1894900 | Avg Loss: 0.0150 | Grad Norm: 0.00958974\n",
      "Epoch 3 | Step 1895000 | Avg Loss: 0.0150 | Grad Norm: 0.00772725\n",
      "Epoch 3 | Step 1895100 | Avg Loss: 0.0151 | Grad Norm: 0.00783452\n",
      "Epoch 3 | Step 1895200 | Avg Loss: 0.0152 | Grad Norm: 0.01132816\n",
      "Epoch 3 | Step 1895300 | Avg Loss: 0.0150 | Grad Norm: 0.00680156\n",
      "Epoch 3 | Step 1895400 | Avg Loss: 0.0156 | Grad Norm: 0.00926774\n",
      "Epoch 3 | Step 1895500 | Avg Loss: 0.0156 | Grad Norm: 0.00764435\n",
      "Epoch 3 | Step 1895600 | Avg Loss: 0.0152 | Grad Norm: 0.00863390\n",
      "Epoch 3 | Step 1895700 | Avg Loss: 0.0151 | Grad Norm: 0.00914666\n",
      "Epoch 3 | Step 1895800 | Avg Loss: 0.0154 | Grad Norm: 0.00734413\n",
      "Epoch 3 | Step 1895900 | Avg Loss: 0.0155 | Grad Norm: 0.00901418\n",
      "Epoch 3 | Step 1896000 | Avg Loss: 0.0153 | Grad Norm: 0.00753029\n",
      "Epoch 3 | Step 1896100 | Avg Loss: 0.0154 | Grad Norm: 0.00934517\n",
      "Epoch 3 | Step 1896200 | Avg Loss: 0.0151 | Grad Norm: 0.00830630\n",
      "Epoch 3 | Step 1896300 | Avg Loss: 0.0151 | Grad Norm: 0.00803742\n",
      "Epoch 3 | Step 1896400 | Avg Loss: 0.0150 | Grad Norm: 0.00700510\n",
      "Epoch 3 | Step 1896500 | Avg Loss: 0.0153 | Grad Norm: 0.00833474\n",
      "Epoch 3 | Step 1896600 | Avg Loss: 0.0153 | Grad Norm: 0.00751685\n",
      "Epoch 3 | Step 1896700 | Avg Loss: 0.0158 | Grad Norm: 0.00683711\n",
      "Epoch 3 | Step 1896800 | Avg Loss: 0.0156 | Grad Norm: 0.00717837\n",
      "Epoch 3 | Step 1896900 | Avg Loss: 0.0152 | Grad Norm: 0.00797820\n",
      "Epoch 3 | Step 1897000 | Avg Loss: 0.0149 | Grad Norm: 0.00700236\n",
      "Epoch 3 | Step 1897100 | Avg Loss: 0.0149 | Grad Norm: 0.00923214\n",
      "Epoch 3 | Step 1897200 | Avg Loss: 0.0151 | Grad Norm: 0.00845124\n",
      "Epoch 3 | Step 1897300 | Avg Loss: 0.0150 | Grad Norm: 0.00850424\n",
      "Epoch 3 | Step 1897400 | Avg Loss: 0.0150 | Grad Norm: 0.00771248\n",
      "Epoch 3 | Step 1897500 | Avg Loss: 0.0154 | Grad Norm: 0.00878341\n",
      "Epoch 3 | Step 1897600 | Avg Loss: 0.0156 | Grad Norm: 0.00865640\n",
      "Epoch 3 | Step 1897700 | Avg Loss: 0.0153 | Grad Norm: 0.00868346\n",
      "Epoch 3 | Step 1897800 | Avg Loss: 0.0153 | Grad Norm: 0.00716285\n",
      "Epoch 3 | Step 1897900 | Avg Loss: 0.0151 | Grad Norm: 0.00960784\n",
      "Epoch 3 | Step 1898000 | Avg Loss: 0.0149 | Grad Norm: 0.00822341\n",
      "Epoch 3 | Step 1898100 | Avg Loss: 0.0152 | Grad Norm: 0.00780835\n",
      "Epoch 3 | Step 1898200 | Avg Loss: 0.0150 | Grad Norm: 0.00888770\n",
      "Epoch 3 | Step 1898300 | Avg Loss: 0.0152 | Grad Norm: 0.00968356\n",
      "Epoch 3 | Step 1898400 | Avg Loss: 0.0150 | Grad Norm: 0.00845925\n",
      "Epoch 3 | Step 1898500 | Avg Loss: 0.0151 | Grad Norm: 0.00841620\n",
      "Epoch 3 | Step 1898600 | Avg Loss: 0.0148 | Grad Norm: 0.00691751\n",
      "Epoch 3 | Step 1898700 | Avg Loss: 0.0151 | Grad Norm: 0.00849249\n",
      "Epoch 3 | Step 1898800 | Avg Loss: 0.0149 | Grad Norm: 0.00849988\n",
      "Epoch 3 | Step 1898900 | Avg Loss: 0.0149 | Grad Norm: 0.01175521\n",
      "Epoch 3 | Step 1899000 | Avg Loss: 0.0149 | Grad Norm: 0.00705009\n",
      "Epoch 3 | Step 1899100 | Avg Loss: 0.0147 | Grad Norm: 0.00922654\n",
      "Epoch 3 | Step 1899200 | Avg Loss: 0.0148 | Grad Norm: 0.01039322\n",
      "Epoch 3 | Step 1899300 | Avg Loss: 0.0149 | Grad Norm: 0.00694723\n",
      "Epoch 3 | Step 1899400 | Avg Loss: 0.0148 | Grad Norm: 0.00940150\n",
      "Epoch 3 | Step 1899500 | Avg Loss: 0.0147 | Grad Norm: 0.00859774\n",
      "Epoch 3 | Step 1899600 | Avg Loss: 0.0150 | Grad Norm: 0.00894319\n",
      "Epoch 3 | Step 1899700 | Avg Loss: 0.0152 | Grad Norm: 0.00670660\n",
      "Epoch 3 | Step 1899800 | Avg Loss: 0.0151 | Grad Norm: 0.00896688\n",
      "Epoch 3 | Step 1899900 | Avg Loss: 0.0148 | Grad Norm: 0.00882545\n",
      "Epoch 3 | Step 1900000 | Avg Loss: 0.0151 | Grad Norm: 0.00811605\n",
      "Saving model at step1900000\n",
      "Epoch 3 | Step 1900100 | Avg Loss: 0.0150 | Grad Norm: 0.00801782\n",
      "Epoch 3 | Step 1900200 | Avg Loss: 0.0147 | Grad Norm: 0.00659567\n",
      "Epoch 3 | Step 1900300 | Avg Loss: 0.0150 | Grad Norm: 0.00816047\n",
      "Epoch 3 | Step 1900400 | Avg Loss: 0.0148 | Grad Norm: 0.00886695\n",
      "Epoch 3 | Step 1900500 | Avg Loss: 0.0153 | Grad Norm: 0.00801108\n",
      "Epoch 3 | Step 1900600 | Avg Loss: 0.0151 | Grad Norm: 0.00734673\n",
      "Epoch 3 | Step 1900700 | Avg Loss: 0.0149 | Grad Norm: 0.00992303\n",
      "Epoch 3 | Step 1900800 | Avg Loss: 0.0147 | Grad Norm: 0.01020340\n",
      "Epoch 3 | Step 1900900 | Avg Loss: 0.0150 | Grad Norm: 0.00757278\n",
      "Epoch 3 | Step 1901000 | Avg Loss: 0.0150 | Grad Norm: 0.00747671\n",
      "Epoch 3 | Step 1901100 | Avg Loss: 0.0150 | Grad Norm: 0.00878346\n",
      "Epoch 3 | Step 1901200 | Avg Loss: 0.0148 | Grad Norm: 0.00787509\n",
      "Epoch 3 | Step 1901300 | Avg Loss: 0.0144 | Grad Norm: 0.00691376\n",
      "Epoch 3 | Step 1901400 | Avg Loss: 0.0149 | Grad Norm: 0.00925157\n",
      "Epoch 3 | Step 1901500 | Avg Loss: 0.0152 | Grad Norm: 0.00853038\n",
      "Epoch 3 | Step 1901600 | Avg Loss: 0.0150 | Grad Norm: 0.00765487\n",
      "Epoch 3 | Step 1901700 | Avg Loss: 0.0146 | Grad Norm: 0.00922818\n",
      "Epoch 3 | Step 1901800 | Avg Loss: 0.0144 | Grad Norm: 0.00841401\n",
      "Epoch 3 | Step 1901900 | Avg Loss: 0.0145 | Grad Norm: 0.00762584\n",
      "Epoch 3 | Step 1902000 | Avg Loss: 0.0143 | Grad Norm: 0.00807137\n",
      "Epoch 3 | Step 1902100 | Avg Loss: 0.0144 | Grad Norm: 0.00762580\n",
      "Epoch 3 | Step 1902200 | Avg Loss: 0.0141 | Grad Norm: 0.00811802\n",
      "Epoch 3 | Step 1902300 | Avg Loss: 0.0141 | Grad Norm: 0.00777345\n",
      "Epoch 3 | Step 1902400 | Avg Loss: 0.0142 | Grad Norm: 0.00870578\n",
      "Epoch 3 | Step 1902500 | Avg Loss: 0.0148 | Grad Norm: 0.00864991\n",
      "Epoch 3 | Step 1902600 | Avg Loss: 0.0150 | Grad Norm: 0.00683849\n",
      "Epoch 3 | Step 1902700 | Avg Loss: 0.0149 | Grad Norm: 0.00708523\n",
      "Epoch 3 | Step 1902800 | Avg Loss: 0.0146 | Grad Norm: 0.00806685\n",
      "Epoch 3 | Step 1902900 | Avg Loss: 0.0144 | Grad Norm: 0.00698485\n",
      "Epoch 3 | Step 1903000 | Avg Loss: 0.0148 | Grad Norm: 0.00811609\n",
      "Epoch 3 | Step 1903100 | Avg Loss: 0.0150 | Grad Norm: 0.00743802\n",
      "Epoch 3 | Step 1903200 | Avg Loss: 0.0151 | Grad Norm: 0.00786291\n",
      "Epoch 3 | Step 1903300 | Avg Loss: 0.0150 | Grad Norm: 0.00802847\n",
      "Epoch 3 | Step 1903400 | Avg Loss: 0.0148 | Grad Norm: 0.00822661\n",
      "Epoch 3 | Step 1903500 | Avg Loss: 0.0146 | Grad Norm: 0.01108582\n",
      "Epoch 3 | Step 1903600 | Avg Loss: 0.0147 | Grad Norm: 0.00858332\n",
      "Epoch 3 | Step 1903700 | Avg Loss: 0.0146 | Grad Norm: 0.00766722\n",
      "Epoch 3 | Step 1903800 | Avg Loss: 0.0146 | Grad Norm: 0.00821621\n",
      "Epoch 3 | Step 1903900 | Avg Loss: 0.0148 | Grad Norm: 0.00818972\n",
      "Epoch 3 | Step 1904000 | Avg Loss: 0.0151 | Grad Norm: 0.00777093\n",
      "Epoch 3 | Step 1904100 | Avg Loss: 0.0150 | Grad Norm: 0.00881706\n",
      "Epoch 3 | Step 1904200 | Avg Loss: 0.0148 | Grad Norm: 0.01373229\n",
      "Epoch 3 | Step 1904300 | Avg Loss: 0.0149 | Grad Norm: 0.00776428\n",
      "Epoch 3 | Step 1904400 | Avg Loss: 0.0155 | Grad Norm: 0.00746616\n",
      "Epoch 3 | Step 1904500 | Avg Loss: 0.0155 | Grad Norm: 0.00803407\n",
      "Epoch 3 | Step 1904600 | Avg Loss: 0.0155 | Grad Norm: 0.00843961\n",
      "Epoch 3 | Step 1904700 | Avg Loss: 0.0152 | Grad Norm: 0.00816570\n",
      "Epoch 3 | Step 1904800 | Avg Loss: 0.0150 | Grad Norm: 0.00862137\n",
      "Epoch 3 | Step 1904900 | Avg Loss: 0.0147 | Grad Norm: 0.00881597\n",
      "Epoch 3 | Step 1905000 | Avg Loss: 0.0146 | Grad Norm: 0.01051622\n",
      "Epoch 3 | Step 1905100 | Avg Loss: 0.0150 | Grad Norm: 0.00774807\n",
      "Epoch 3 | Step 1905200 | Avg Loss: 0.0152 | Grad Norm: 0.01006120\n",
      "Epoch 3 | Step 1905300 | Avg Loss: 0.0153 | Grad Norm: 0.00767356\n",
      "Epoch 3 | Step 1905400 | Avg Loss: 0.0151 | Grad Norm: 0.00822069\n",
      "Epoch 3 | Step 1905500 | Avg Loss: 0.0149 | Grad Norm: 0.00715195\n",
      "Epoch 3 | Step 1905600 | Avg Loss: 0.0150 | Grad Norm: 0.00780933\n",
      "Epoch 3 | Step 1905700 | Avg Loss: 0.0148 | Grad Norm: 0.00861305\n",
      "Epoch 3 | Step 1905800 | Avg Loss: 0.0151 | Grad Norm: 0.00877394\n",
      "Epoch 3 | Step 1905900 | Avg Loss: 0.0151 | Grad Norm: 0.00872562\n",
      "Epoch 3 | Step 1906000 | Avg Loss: 0.0153 | Grad Norm: 0.00951855\n",
      "Epoch 3 | Step 1906100 | Avg Loss: 0.0151 | Grad Norm: 0.00946201\n",
      "Epoch 3 | Step 1906200 | Avg Loss: 0.0151 | Grad Norm: 0.00775039\n",
      "Epoch 3 | Step 1906300 | Avg Loss: 0.0154 | Grad Norm: 0.00864632\n",
      "Epoch 3 | Step 1906400 | Avg Loss: 0.0156 | Grad Norm: 0.00881343\n",
      "Epoch 3 | Step 1906500 | Avg Loss: 0.0154 | Grad Norm: 0.00832377\n",
      "Epoch 3 | Step 1906600 | Avg Loss: 0.0153 | Grad Norm: 0.00795568\n",
      "Epoch 3 | Step 1906700 | Avg Loss: 0.0155 | Grad Norm: 0.00833072\n",
      "Epoch 3 | Step 1906800 | Avg Loss: 0.0155 | Grad Norm: 0.00761369\n",
      "Epoch 3 | Step 1906900 | Avg Loss: 0.0151 | Grad Norm: 0.00742421\n",
      "Epoch 3 | Step 1907000 | Avg Loss: 0.0151 | Grad Norm: 0.00771851\n",
      "Epoch 3 | Step 1907100 | Avg Loss: 0.0155 | Grad Norm: 0.00875849\n",
      "Epoch 3 | Step 1907200 | Avg Loss: 0.0155 | Grad Norm: 0.00735022\n",
      "Epoch 3 | Step 1907300 | Avg Loss: 0.0153 | Grad Norm: 0.00838208\n",
      "Epoch 3 | Step 1907400 | Avg Loss: 0.0151 | Grad Norm: 0.00802293\n",
      "Epoch 3 | Step 1907500 | Avg Loss: 0.0147 | Grad Norm: 0.00886440\n",
      "Epoch 3 | Step 1907600 | Avg Loss: 0.0149 | Grad Norm: 0.00750941\n",
      "Epoch 3 | Step 1907700 | Avg Loss: 0.0149 | Grad Norm: 0.00733970\n",
      "Epoch 3 | Step 1907800 | Avg Loss: 0.0148 | Grad Norm: 0.00823886\n",
      "Epoch 3 | Step 1907900 | Avg Loss: 0.0146 | Grad Norm: 0.00807117\n",
      "Epoch 3 | Step 1908000 | Avg Loss: 0.0150 | Grad Norm: 0.00863470\n",
      "Epoch 3 | Step 1908100 | Avg Loss: 0.0149 | Grad Norm: 0.00838709\n",
      "Epoch 3 | Step 1908200 | Avg Loss: 0.0148 | Grad Norm: 0.00804821\n",
      "Epoch 3 | Step 1908300 | Avg Loss: 0.0152 | Grad Norm: 0.00798168\n",
      "Epoch 3 | Step 1908400 | Avg Loss: 0.0148 | Grad Norm: 0.00733228\n",
      "Epoch 3 | Step 1908500 | Avg Loss: 0.0151 | Grad Norm: 0.00768803\n",
      "Epoch 3 | Step 1908600 | Avg Loss: 0.0152 | Grad Norm: 0.00922218\n",
      "Epoch 3 | Step 1908700 | Avg Loss: 0.0150 | Grad Norm: 0.00895595\n",
      "Epoch 3 | Step 1908800 | Avg Loss: 0.0151 | Grad Norm: 0.00867434\n",
      "Epoch 3 | Step 1908900 | Avg Loss: 0.0147 | Grad Norm: 0.00665477\n",
      "Epoch 3 | Step 1909000 | Avg Loss: 0.0149 | Grad Norm: 0.00816172\n",
      "Epoch 3 | Step 1909100 | Avg Loss: 0.0151 | Grad Norm: 0.00910515\n",
      "Epoch 3 | Step 1909200 | Avg Loss: 0.0147 | Grad Norm: 0.00757385\n",
      "Epoch 3 | Step 1909300 | Avg Loss: 0.0146 | Grad Norm: 0.00742762\n",
      "Epoch 3 | Step 1909400 | Avg Loss: 0.0146 | Grad Norm: 0.00784145\n",
      "Epoch 3 | Step 1909500 | Avg Loss: 0.0150 | Grad Norm: 0.00832412\n",
      "Epoch 3 | Step 1909600 | Avg Loss: 0.0151 | Grad Norm: 0.00769864\n",
      "Epoch 3 | Step 1909700 | Avg Loss: 0.0147 | Grad Norm: 0.00748398\n",
      "Epoch 3 | Step 1909800 | Avg Loss: 0.0146 | Grad Norm: 0.00934754\n",
      "Epoch 3 | Step 1909900 | Avg Loss: 0.0145 | Grad Norm: 0.00797451\n",
      "Epoch 3 | Step 1910000 | Avg Loss: 0.0145 | Grad Norm: 0.00717216\n",
      "Epoch 3 | Step 1910100 | Avg Loss: 0.0150 | Grad Norm: 0.00763813\n",
      "Epoch 3 | Step 1910200 | Avg Loss: 0.0153 | Grad Norm: 0.00753390\n",
      "Epoch 3 | Step 1910300 | Avg Loss: 0.0150 | Grad Norm: 0.00708716\n",
      "Epoch 3 | Step 1910400 | Avg Loss: 0.0150 | Grad Norm: 0.00914499\n",
      "Epoch 3 | Step 1910500 | Avg Loss: 0.0154 | Grad Norm: 0.00886991\n",
      "Epoch 3 | Step 1910600 | Avg Loss: 0.0152 | Grad Norm: 0.00846560\n",
      "Epoch 3 | Step 1910700 | Avg Loss: 0.0151 | Grad Norm: 0.00809836\n",
      "Epoch 3 | Step 1910800 | Avg Loss: 0.0152 | Grad Norm: 0.00813449\n",
      "Epoch 3 | Step 1910900 | Avg Loss: 0.0149 | Grad Norm: 0.00788502\n",
      "Epoch 3 | Step 1911000 | Avg Loss: 0.0149 | Grad Norm: 0.00874900\n",
      "Epoch 3 | Step 1911100 | Avg Loss: 0.0149 | Grad Norm: 0.00768652\n",
      "Epoch 3 | Step 1911200 | Avg Loss: 0.0153 | Grad Norm: 0.00782692\n",
      "Epoch 3 | Step 1911300 | Avg Loss: 0.0150 | Grad Norm: 0.00880661\n",
      "Epoch 3 | Step 1911400 | Avg Loss: 0.0154 | Grad Norm: 0.00719575\n",
      "Epoch 3 | Step 1911500 | Avg Loss: 0.0152 | Grad Norm: 0.00714910\n",
      "Epoch 3 | Step 1911600 | Avg Loss: 0.0147 | Grad Norm: 0.00812327\n",
      "Epoch 3 | Step 1911700 | Avg Loss: 0.0149 | Grad Norm: 0.00713357\n",
      "Epoch 3 | Step 1911800 | Avg Loss: 0.0150 | Grad Norm: 0.00785551\n",
      "Epoch 3 | Step 1911900 | Avg Loss: 0.0152 | Grad Norm: 0.00897096\n",
      "Epoch 3 | Step 1912000 | Avg Loss: 0.0151 | Grad Norm: 0.00815695\n",
      "Epoch 3 | Step 1912100 | Avg Loss: 0.0152 | Grad Norm: 0.00784628\n",
      "Epoch 3 | Step 1912200 | Avg Loss: 0.0151 | Grad Norm: 0.00867326\n",
      "Epoch 3 | Step 1912300 | Avg Loss: 0.0154 | Grad Norm: 0.00757911\n",
      "Epoch 3 | Step 1912400 | Avg Loss: 0.0150 | Grad Norm: 0.00900446\n",
      "Epoch 3 | Step 1912500 | Avg Loss: 0.0149 | Grad Norm: 0.00783310\n",
      "Epoch 3 | Step 1912600 | Avg Loss: 0.0149 | Grad Norm: 0.00699369\n",
      "Epoch 3 | Step 1912700 | Avg Loss: 0.0152 | Grad Norm: 0.00827024\n",
      "Epoch 3 | Step 1912800 | Avg Loss: 0.0152 | Grad Norm: 0.01187252\n",
      "Epoch 3 | Step 1912900 | Avg Loss: 0.0153 | Grad Norm: 0.00947828\n",
      "Epoch 3 | Step 1913000 | Avg Loss: 0.0153 | Grad Norm: 0.00760947\n",
      "Epoch 3 | Step 1913100 | Avg Loss: 0.0153 | Grad Norm: 0.00877535\n",
      "Epoch 3 | Step 1913200 | Avg Loss: 0.0154 | Grad Norm: 0.01118921\n",
      "Epoch 3 | Step 1913300 | Avg Loss: 0.0153 | Grad Norm: 0.00798401\n",
      "Epoch 3 | Step 1913400 | Avg Loss: 0.0155 | Grad Norm: 0.00816377\n",
      "Epoch 3 | Step 1913500 | Avg Loss: 0.0156 | Grad Norm: 0.00879686\n",
      "Epoch 3 | Step 1913600 | Avg Loss: 0.0155 | Grad Norm: 0.00800116\n",
      "Epoch 3 | Step 1913700 | Avg Loss: 0.0152 | Grad Norm: 0.00866777\n",
      "Epoch 3 | Step 1913800 | Avg Loss: 0.0150 | Grad Norm: 0.00759563\n",
      "Epoch 3 | Step 1913900 | Avg Loss: 0.0150 | Grad Norm: 0.00783335\n",
      "Epoch 3 | Step 1914000 | Avg Loss: 0.0149 | Grad Norm: 0.00807413\n",
      "Epoch 3 | Step 1914100 | Avg Loss: 0.0151 | Grad Norm: 0.00839926\n",
      "Epoch 3 | Step 1914200 | Avg Loss: 0.0154 | Grad Norm: 0.00968612\n",
      "Epoch 3 | Step 1914300 | Avg Loss: 0.0154 | Grad Norm: 0.00747847\n",
      "Epoch 3 | Step 1914400 | Avg Loss: 0.0153 | Grad Norm: 0.00737630\n",
      "Epoch 3 | Step 1914500 | Avg Loss: 0.0153 | Grad Norm: 0.00734047\n",
      "Epoch 3 | Step 1914600 | Avg Loss: 0.0155 | Grad Norm: 0.00796409\n",
      "Epoch 3 | Step 1914700 | Avg Loss: 0.0154 | Grad Norm: 0.01019454\n",
      "Epoch 3 | Step 1914800 | Avg Loss: 0.0151 | Grad Norm: 0.00730658\n",
      "Epoch 3 | Step 1914900 | Avg Loss: 0.0150 | Grad Norm: 0.01038595\n",
      "Epoch 3 | Step 1915000 | Avg Loss: 0.0153 | Grad Norm: 0.00801442\n",
      "Epoch 3 | Step 1915100 | Avg Loss: 0.0155 | Grad Norm: 0.00972562\n",
      "Epoch 3 | Step 1915200 | Avg Loss: 0.0152 | Grad Norm: 0.00894692\n",
      "Epoch 3 | Step 1915300 | Avg Loss: 0.0150 | Grad Norm: 0.00820963\n",
      "Epoch 3 | Step 1915400 | Avg Loss: 0.0148 | Grad Norm: 0.00841474\n",
      "Epoch 3 | Step 1915500 | Avg Loss: 0.0148 | Grad Norm: 0.00720028\n",
      "Epoch 3 | Step 1915600 | Avg Loss: 0.0145 | Grad Norm: 0.00922384\n",
      "Epoch 3 | Step 1915700 | Avg Loss: 0.0149 | Grad Norm: 0.00782272\n",
      "Epoch 3 | Step 1915800 | Avg Loss: 0.0150 | Grad Norm: 0.00769475\n",
      "Epoch 3 | Step 1915900 | Avg Loss: 0.0149 | Grad Norm: 0.00773750\n",
      "Epoch 3 | Step 1916000 | Avg Loss: 0.0146 | Grad Norm: 0.00798293\n",
      "Epoch 3 | Step 1916100 | Avg Loss: 0.0144 | Grad Norm: 0.00850453\n",
      "Epoch 3 | Step 1916200 | Avg Loss: 0.0146 | Grad Norm: 0.01023576\n",
      "Epoch 3 | Step 1916300 | Avg Loss: 0.0150 | Grad Norm: 0.00808809\n",
      "Epoch 3 | Step 1916400 | Avg Loss: 0.0151 | Grad Norm: 0.00805324\n",
      "Epoch 3 | Step 1916500 | Avg Loss: 0.0151 | Grad Norm: 0.00721856\n",
      "Epoch 3 | Step 1916600 | Avg Loss: 0.0149 | Grad Norm: 0.00683383\n",
      "Epoch 3 | Step 1916700 | Avg Loss: 0.0153 | Grad Norm: 0.00821831\n",
      "Epoch 3 | Step 1916800 | Avg Loss: 0.0153 | Grad Norm: 0.00881602\n",
      "Epoch 3 | Step 1916900 | Avg Loss: 0.0155 | Grad Norm: 0.00950769\n",
      "Epoch 3 | Step 1917000 | Avg Loss: 0.0151 | Grad Norm: 0.00856729\n",
      "Epoch 3 | Step 1917100 | Avg Loss: 0.0151 | Grad Norm: 0.00707308\n",
      "Epoch 3 | Step 1917200 | Avg Loss: 0.0149 | Grad Norm: 0.01070833\n",
      "Epoch 3 | Step 1917300 | Avg Loss: 0.0150 | Grad Norm: 0.01044775\n",
      "Epoch 3 | Step 1917400 | Avg Loss: 0.0150 | Grad Norm: 0.00754006\n",
      "Epoch 3 | Step 1917500 | Avg Loss: 0.0151 | Grad Norm: 0.00781268\n",
      "Epoch 3 | Step 1917600 | Avg Loss: 0.0148 | Grad Norm: 0.00767594\n",
      "Epoch 3 | Step 1917700 | Avg Loss: 0.0148 | Grad Norm: 0.00865808\n",
      "Epoch 3 | Step 1917800 | Avg Loss: 0.0148 | Grad Norm: 0.00842400\n",
      "Epoch 3 | Step 1917900 | Avg Loss: 0.0149 | Grad Norm: 0.01022150\n",
      "Epoch 3 | Step 1918000 | Avg Loss: 0.0148 | Grad Norm: 0.00741061\n",
      "Epoch 3 | Step 1918100 | Avg Loss: 0.0152 | Grad Norm: 0.00744410\n",
      "Epoch 3 | Step 1918200 | Avg Loss: 0.0149 | Grad Norm: 0.00706652\n",
      "Epoch 3 | Step 1918300 | Avg Loss: 0.0146 | Grad Norm: 0.00878454\n",
      "Epoch 3 | Step 1918400 | Avg Loss: 0.0149 | Grad Norm: 0.00694399\n",
      "Epoch 3 | Step 1918500 | Avg Loss: 0.0153 | Grad Norm: 0.00981751\n",
      "Epoch 3 | Step 1918600 | Avg Loss: 0.0152 | Grad Norm: 0.00851927\n",
      "Epoch 3 | Step 1918700 | Avg Loss: 0.0151 | Grad Norm: 0.00749069\n",
      "Epoch 3 | Step 1918800 | Avg Loss: 0.0153 | Grad Norm: 0.00736779\n",
      "Epoch 3 | Step 1918900 | Avg Loss: 0.0156 | Grad Norm: 0.00935324\n",
      "Epoch 3 | Step 1919000 | Avg Loss: 0.0152 | Grad Norm: 0.00751868\n",
      "Epoch 3 | Step 1919100 | Avg Loss: 0.0148 | Grad Norm: 0.00801034\n",
      "Epoch 3 | Step 1919200 | Avg Loss: 0.0150 | Grad Norm: 0.00812403\n",
      "Epoch 3 | Step 1919300 | Avg Loss: 0.0144 | Grad Norm: 0.00835320\n",
      "Epoch 3 | Step 1919400 | Avg Loss: 0.0146 | Grad Norm: 0.00764864\n",
      "Epoch 3 | Step 1919500 | Avg Loss: 0.0143 | Grad Norm: 0.00732325\n",
      "Epoch 3 | Step 1919600 | Avg Loss: 0.0146 | Grad Norm: 0.00960342\n",
      "Epoch 3 | Step 1919700 | Avg Loss: 0.0147 | Grad Norm: 0.00647536\n",
      "Epoch 3 | Step 1919800 | Avg Loss: 0.0145 | Grad Norm: 0.00873793\n",
      "Epoch 3 | Step 1919900 | Avg Loss: 0.0145 | Grad Norm: 0.00735509\n",
      "Epoch 3 | Step 1920000 | Avg Loss: 0.0143 | Grad Norm: 0.00899204\n",
      "Epoch 3 | Step 1920100 | Avg Loss: 0.0146 | Grad Norm: 0.00784883\n",
      "Epoch 3 | Step 1920200 | Avg Loss: 0.0149 | Grad Norm: 0.00804362\n",
      "Epoch 3 | Step 1920300 | Avg Loss: 0.0149 | Grad Norm: 0.00729307\n",
      "Epoch 3 | Step 1920400 | Avg Loss: 0.0147 | Grad Norm: 0.00773430\n",
      "Epoch 3 | Step 1920500 | Avg Loss: 0.0149 | Grad Norm: 0.00776220\n",
      "Epoch 3 | Step 1920600 | Avg Loss: 0.0149 | Grad Norm: 0.00719330\n",
      "Epoch 3 | Step 1920700 | Avg Loss: 0.0145 | Grad Norm: 0.00747959\n",
      "Epoch 3 | Step 1920800 | Avg Loss: 0.0143 | Grad Norm: 0.00929781\n",
      "Epoch 3 | Step 1920900 | Avg Loss: 0.0145 | Grad Norm: 0.00893424\n",
      "Epoch 3 | Step 1921000 | Avg Loss: 0.0148 | Grad Norm: 0.00750320\n",
      "Epoch 3 | Step 1921100 | Avg Loss: 0.0153 | Grad Norm: 0.00768459\n",
      "Epoch 3 | Step 1921200 | Avg Loss: 0.0152 | Grad Norm: 0.00846814\n",
      "Epoch 3 | Step 1921300 | Avg Loss: 0.0152 | Grad Norm: 0.00765253\n",
      "Epoch 3 | Step 1921400 | Avg Loss: 0.0154 | Grad Norm: 0.00768230\n",
      "Epoch 3 | Step 1921500 | Avg Loss: 0.0149 | Grad Norm: 0.00800592\n",
      "Epoch 3 | Step 1921600 | Avg Loss: 0.0150 | Grad Norm: 0.00802665\n",
      "Epoch 3 | Step 1921700 | Avg Loss: 0.0150 | Grad Norm: 0.00833827\n",
      "Epoch 3 | Step 1921800 | Avg Loss: 0.0149 | Grad Norm: 0.00817126\n",
      "Epoch 3 | Step 1921900 | Avg Loss: 0.0146 | Grad Norm: 0.00673362\n",
      "Epoch 3 | Step 1922000 | Avg Loss: 0.0146 | Grad Norm: 0.00732293\n",
      "Epoch 3 | Step 1922100 | Avg Loss: 0.0148 | Grad Norm: 0.00644175\n",
      "Epoch 3 | Step 1922200 | Avg Loss: 0.0150 | Grad Norm: 0.00779328\n",
      "Epoch 3 | Step 1922300 | Avg Loss: 0.0151 | Grad Norm: 0.00792927\n",
      "Epoch 3 | Step 1922400 | Avg Loss: 0.0153 | Grad Norm: 0.00813368\n",
      "Epoch 3 | Step 1922500 | Avg Loss: 0.0155 | Grad Norm: 0.00794856\n",
      "Epoch 3 | Step 1922600 | Avg Loss: 0.0153 | Grad Norm: 0.00741817\n",
      "Epoch 3 | Step 1922700 | Avg Loss: 0.0151 | Grad Norm: 0.00821504\n",
      "Epoch 3 | Step 1922800 | Avg Loss: 0.0147 | Grad Norm: 0.00781116\n",
      "Epoch 3 | Step 1922900 | Avg Loss: 0.0146 | Grad Norm: 0.00805704\n",
      "Epoch 3 | Step 1923000 | Avg Loss: 0.0149 | Grad Norm: 0.00885487\n",
      "Epoch 3 | Step 1923100 | Avg Loss: 0.0151 | Grad Norm: 0.00758810\n",
      "Epoch 3 | Step 1923200 | Avg Loss: 0.0151 | Grad Norm: 0.00909059\n",
      "Epoch 3 | Step 1923300 | Avg Loss: 0.0152 | Grad Norm: 0.00872848\n",
      "Epoch 3 | Step 1923400 | Avg Loss: 0.0149 | Grad Norm: 0.00713252\n",
      "Epoch 3 | Step 1923500 | Avg Loss: 0.0146 | Grad Norm: 0.00710028\n",
      "Epoch 3 | Step 1923600 | Avg Loss: 0.0148 | Grad Norm: 0.00989692\n",
      "Epoch 3 | Step 1923700 | Avg Loss: 0.0148 | Grad Norm: 0.00760012\n",
      "Epoch 3 | Step 1923800 | Avg Loss: 0.0148 | Grad Norm: 0.00708756\n",
      "Epoch 3 | Step 1923900 | Avg Loss: 0.0152 | Grad Norm: 0.00736472\n",
      "Epoch 3 | Step 1924000 | Avg Loss: 0.0150 | Grad Norm: 0.00899385\n",
      "Epoch 3 | Step 1924100 | Avg Loss: 0.0151 | Grad Norm: 0.00736504\n",
      "Epoch 3 | Step 1924200 | Avg Loss: 0.0148 | Grad Norm: 0.00801225\n",
      "Epoch 3 | Step 1924300 | Avg Loss: 0.0148 | Grad Norm: 0.00726604\n",
      "Epoch 3 | Step 1924400 | Avg Loss: 0.0153 | Grad Norm: 0.00904730\n",
      "Epoch 3 | Step 1924500 | Avg Loss: 0.0153 | Grad Norm: 0.00788401\n",
      "Epoch 3 | Step 1924600 | Avg Loss: 0.0151 | Grad Norm: 0.00783013\n",
      "Epoch 3 | Step 1924700 | Avg Loss: 0.0148 | Grad Norm: 0.00798387\n",
      "Epoch 3 | Step 1924800 | Avg Loss: 0.0148 | Grad Norm: 0.00877977\n",
      "Epoch 3 | Step 1924900 | Avg Loss: 0.0146 | Grad Norm: 0.00844744\n",
      "Epoch 3 | Step 1925000 | Avg Loss: 0.0150 | Grad Norm: 0.01020374\n",
      "Epoch 3 | Step 1925100 | Avg Loss: 0.0151 | Grad Norm: 0.00961628\n",
      "Epoch 3 | Step 1925200 | Avg Loss: 0.0148 | Grad Norm: 0.00822429\n",
      "Epoch 3 | Step 1925300 | Avg Loss: 0.0148 | Grad Norm: 0.00922298\n",
      "Epoch 3 | Step 1925400 | Avg Loss: 0.0147 | Grad Norm: 0.00942427\n",
      "Epoch 3 | Step 1925500 | Avg Loss: 0.0148 | Grad Norm: 0.00900942\n",
      "Epoch 3 | Step 1925600 | Avg Loss: 0.0152 | Grad Norm: 0.00671822\n",
      "Epoch 3 | Step 1925700 | Avg Loss: 0.0155 | Grad Norm: 0.00741409\n",
      "Epoch 3 | Step 1925800 | Avg Loss: 0.0153 | Grad Norm: 0.00719842\n",
      "Epoch 3 | Step 1925900 | Avg Loss: 0.0154 | Grad Norm: 0.00693479\n",
      "Epoch 3 | Step 1926000 | Avg Loss: 0.0152 | Grad Norm: 0.00738207\n",
      "Epoch 3 | Step 1926100 | Avg Loss: 0.0149 | Grad Norm: 0.00772355\n",
      "Epoch 3 | Step 1926200 | Avg Loss: 0.0150 | Grad Norm: 0.00710711\n",
      "Epoch 3 | Step 1926300 | Avg Loss: 0.0149 | Grad Norm: 0.00712297\n",
      "Epoch 3 | Step 1926400 | Avg Loss: 0.0149 | Grad Norm: 0.00850850\n",
      "Epoch 3 | Step 1926500 | Avg Loss: 0.0153 | Grad Norm: 0.00811123\n",
      "Epoch 3 | Step 1926600 | Avg Loss: 0.0152 | Grad Norm: 0.00810684\n",
      "Epoch 3 | Step 1926700 | Avg Loss: 0.0148 | Grad Norm: 0.00989806\n",
      "Epoch 3 | Step 1926800 | Avg Loss: 0.0153 | Grad Norm: 0.00784634\n",
      "Epoch 3 | Step 1926900 | Avg Loss: 0.0151 | Grad Norm: 0.00828985\n",
      "Epoch 3 | Step 1927000 | Avg Loss: 0.0152 | Grad Norm: 0.00836619\n",
      "Epoch 3 | Step 1927100 | Avg Loss: 0.0154 | Grad Norm: 0.00876600\n",
      "Epoch 3 | Step 1927200 | Avg Loss: 0.0155 | Grad Norm: 0.01153333\n",
      "Epoch 3 | Step 1927300 | Avg Loss: 0.0152 | Grad Norm: 0.00807525\n",
      "Epoch 3 | Step 1927400 | Avg Loss: 0.0153 | Grad Norm: 0.00695931\n",
      "Epoch 3 | Step 1927500 | Avg Loss: 0.0149 | Grad Norm: 0.00787842\n",
      "Epoch 3 | Step 1927600 | Avg Loss: 0.0151 | Grad Norm: 0.00754984\n",
      "Epoch 3 | Step 1927700 | Avg Loss: 0.0151 | Grad Norm: 0.00762377\n",
      "Epoch 3 | Step 1927800 | Avg Loss: 0.0147 | Grad Norm: 0.00752240\n",
      "Epoch 3 | Step 1927900 | Avg Loss: 0.0152 | Grad Norm: 0.00887312\n",
      "Epoch 3 | Step 1928000 | Avg Loss: 0.0148 | Grad Norm: 0.01049051\n",
      "Epoch 3 | Step 1928100 | Avg Loss: 0.0149 | Grad Norm: 0.00861904\n",
      "Epoch 3 | Step 1928200 | Avg Loss: 0.0150 | Grad Norm: 0.00867628\n",
      "Epoch 3 | Step 1928300 | Avg Loss: 0.0150 | Grad Norm: 0.00925579\n",
      "Epoch 3 | Step 1928400 | Avg Loss: 0.0153 | Grad Norm: 0.00778382\n",
      "Epoch 3 | Step 1928500 | Avg Loss: 0.0153 | Grad Norm: 0.00737712\n",
      "Epoch 3 | Step 1928600 | Avg Loss: 0.0148 | Grad Norm: 0.00982998\n",
      "Epoch 3 | Step 1928700 | Avg Loss: 0.0150 | Grad Norm: 0.00850791\n",
      "Epoch 3 | Step 1928800 | Avg Loss: 0.0153 | Grad Norm: 0.00931922\n",
      "Epoch 3 | Step 1928900 | Avg Loss: 0.0152 | Grad Norm: 0.00874242\n",
      "Epoch 3 | Step 1929000 | Avg Loss: 0.0149 | Grad Norm: 0.00725665\n",
      "Epoch 3 | Step 1929100 | Avg Loss: 0.0148 | Grad Norm: 0.00859595\n",
      "Epoch 3 | Step 1929200 | Avg Loss: 0.0148 | Grad Norm: 0.00663435\n",
      "Epoch 3 | Step 1929300 | Avg Loss: 0.0147 | Grad Norm: 0.00778917\n",
      "Epoch 3 | Step 1929400 | Avg Loss: 0.0148 | Grad Norm: 0.00879642\n",
      "Epoch 3 | Step 1929500 | Avg Loss: 0.0151 | Grad Norm: 0.01080503\n",
      "Epoch 3 | Step 1929600 | Avg Loss: 0.0151 | Grad Norm: 0.00766923\n",
      "Epoch 3 | Step 1929700 | Avg Loss: 0.0153 | Grad Norm: 0.01199268\n",
      "Epoch 3 | Step 1929800 | Avg Loss: 0.0150 | Grad Norm: 0.00791339\n",
      "Epoch 3 | Step 1929900 | Avg Loss: 0.0146 | Grad Norm: 0.00777670\n",
      "Epoch 3 | Step 1930000 | Avg Loss: 0.0151 | Grad Norm: 0.00906396\n",
      "Epoch 3 | Step 1930100 | Avg Loss: 0.0152 | Grad Norm: 0.00991973\n",
      "Epoch 3 | Step 1930200 | Avg Loss: 0.0150 | Grad Norm: 0.00845579\n",
      "Epoch 3 | Step 1930300 | Avg Loss: 0.0149 | Grad Norm: 0.00792337\n",
      "Epoch 3 | Step 1930400 | Avg Loss: 0.0151 | Grad Norm: 0.00906529\n",
      "Epoch 3 | Step 1930500 | Avg Loss: 0.0150 | Grad Norm: 0.00846805\n",
      "Epoch 3 | Step 1930600 | Avg Loss: 0.0151 | Grad Norm: 0.00741903\n",
      "Epoch 3 | Step 1930700 | Avg Loss: 0.0151 | Grad Norm: 0.00871740\n",
      "Epoch 3 | Step 1930800 | Avg Loss: 0.0152 | Grad Norm: 0.00721935\n",
      "Epoch 3 | Step 1930900 | Avg Loss: 0.0154 | Grad Norm: 0.01108610\n",
      "Epoch 3 | Step 1931000 | Avg Loss: 0.0154 | Grad Norm: 0.00762168\n",
      "Epoch 3 | Step 1931100 | Avg Loss: 0.0152 | Grad Norm: 0.00823388\n",
      "Epoch 3 | Step 1931200 | Avg Loss: 0.0150 | Grad Norm: 0.00712703\n",
      "Epoch 3 | Step 1931300 | Avg Loss: 0.0149 | Grad Norm: 0.00885206\n",
      "Epoch 3 | Step 1931400 | Avg Loss: 0.0146 | Grad Norm: 0.00785533\n",
      "Epoch 3 | Step 1931500 | Avg Loss: 0.0148 | Grad Norm: 0.01087852\n",
      "Epoch 3 | Step 1931600 | Avg Loss: 0.0151 | Grad Norm: 0.00905995\n",
      "Epoch 3 | Step 1931700 | Avg Loss: 0.0147 | Grad Norm: 0.00966337\n",
      "Epoch 3 | Step 1931800 | Avg Loss: 0.0149 | Grad Norm: 0.00768066\n",
      "Epoch 3 | Step 1931900 | Avg Loss: 0.0153 | Grad Norm: 0.00708971\n",
      "Epoch 3 | Step 1932000 | Avg Loss: 0.0155 | Grad Norm: 0.00755612\n",
      "Epoch 3 | Step 1932100 | Avg Loss: 0.0153 | Grad Norm: 0.00911232\n",
      "Epoch 3 | Step 1932200 | Avg Loss: 0.0150 | Grad Norm: 0.00777847\n",
      "Epoch 3 | Step 1932300 | Avg Loss: 0.0149 | Grad Norm: 0.00792605\n",
      "Epoch 3 | Step 1932400 | Avg Loss: 0.0150 | Grad Norm: 0.00810671\n",
      "Epoch 3 | Step 1932500 | Avg Loss: 0.0149 | Grad Norm: 0.00783933\n",
      "Epoch 3 | Step 1932600 | Avg Loss: 0.0145 | Grad Norm: 0.00730237\n",
      "Epoch 3 | Step 1932700 | Avg Loss: 0.0145 | Grad Norm: 0.00798417\n",
      "Epoch 3 | Step 1932800 | Avg Loss: 0.0147 | Grad Norm: 0.00725835\n",
      "Epoch 3 | Step 1932900 | Avg Loss: 0.0144 | Grad Norm: 0.00768667\n",
      "Epoch 3 | Step 1933000 | Avg Loss: 0.0142 | Grad Norm: 0.00772046\n",
      "Epoch 3 | Step 1933100 | Avg Loss: 0.0140 | Grad Norm: 0.00745067\n",
      "Epoch 3 | Step 1933200 | Avg Loss: 0.0141 | Grad Norm: 0.00761030\n",
      "Epoch 3 | Step 1933300 | Avg Loss: 0.0145 | Grad Norm: 0.00921370\n",
      "Epoch 3 | Step 1933400 | Avg Loss: 0.0143 | Grad Norm: 0.00755150\n",
      "Epoch 3 | Step 1933500 | Avg Loss: 0.0147 | Grad Norm: 0.00777028\n",
      "Epoch 3 | Step 1933600 | Avg Loss: 0.0148 | Grad Norm: 0.00874390\n",
      "Epoch 3 | Step 1933700 | Avg Loss: 0.0149 | Grad Norm: 0.00695808\n",
      "Epoch 3 | Step 1933800 | Avg Loss: 0.0149 | Grad Norm: 0.00786558\n",
      "Epoch 3 | Step 1933900 | Avg Loss: 0.0150 | Grad Norm: 0.00755614\n",
      "Epoch 3 | Step 1934000 | Avg Loss: 0.0147 | Grad Norm: 0.00859992\n",
      "Epoch 3 | Step 1934100 | Avg Loss: 0.0148 | Grad Norm: 0.00788592\n",
      "Epoch 3 | Step 1934200 | Avg Loss: 0.0148 | Grad Norm: 0.00848264\n",
      "Epoch 3 | Step 1934300 | Avg Loss: 0.0147 | Grad Norm: 0.00953475\n",
      "Epoch 3 | Step 1934400 | Avg Loss: 0.0147 | Grad Norm: 0.00759946\n",
      "Epoch 3 | Step 1934500 | Avg Loss: 0.0142 | Grad Norm: 0.00792836\n",
      "Epoch 3 | Step 1934600 | Avg Loss: 0.0143 | Grad Norm: 0.00784091\n",
      "Epoch 3 | Step 1934700 | Avg Loss: 0.0145 | Grad Norm: 0.00862959\n",
      "Epoch 3 | Step 1934800 | Avg Loss: 0.0150 | Grad Norm: 0.00747874\n",
      "Epoch 3 | Step 1934900 | Avg Loss: 0.0153 | Grad Norm: 0.00719983\n",
      "Epoch 3 | Step 1935000 | Avg Loss: 0.0150 | Grad Norm: 0.00829757\n",
      "Epoch 3 | Step 1935100 | Avg Loss: 0.0149 | Grad Norm: 0.00806338\n",
      "Epoch 3 | Step 1935200 | Avg Loss: 0.0148 | Grad Norm: 0.00778473\n",
      "Epoch 3 | Step 1935300 | Avg Loss: 0.0148 | Grad Norm: 0.00654872\n",
      "Epoch 3 | Step 1935400 | Avg Loss: 0.0146 | Grad Norm: 0.00658192\n",
      "Epoch 3 | Step 1935500 | Avg Loss: 0.0144 | Grad Norm: 0.00731762\n",
      "Epoch 3 | Step 1935600 | Avg Loss: 0.0149 | Grad Norm: 0.00751844\n",
      "Epoch 3 | Step 1935700 | Avg Loss: 0.0151 | Grad Norm: 0.00815853\n",
      "Epoch 3 | Step 1935800 | Avg Loss: 0.0149 | Grad Norm: 0.00865050\n",
      "Epoch 3 | Step 1935900 | Avg Loss: 0.0147 | Grad Norm: 0.00864896\n",
      "Epoch 3 | Step 1936000 | Avg Loss: 0.0150 | Grad Norm: 0.00823245\n",
      "Epoch 3 | Step 1936100 | Avg Loss: 0.0149 | Grad Norm: 0.00880513\n",
      "Epoch 3 | Step 1936200 | Avg Loss: 0.0148 | Grad Norm: 0.00853459\n",
      "Epoch 3 | Step 1936300 | Avg Loss: 0.0152 | Grad Norm: 0.00826963\n",
      "Epoch 3 | Step 1936400 | Avg Loss: 0.0150 | Grad Norm: 0.00749550\n",
      "Epoch 3 | Step 1936500 | Avg Loss: 0.0150 | Grad Norm: 0.00915528\n",
      "Epoch 3 | Step 1936600 | Avg Loss: 0.0148 | Grad Norm: 0.00717103\n",
      "Epoch 3 | Step 1936700 | Avg Loss: 0.0147 | Grad Norm: 0.00961825\n",
      "Epoch 3 | Step 1936800 | Avg Loss: 0.0148 | Grad Norm: 0.00743947\n",
      "Epoch 3 | Step 1936900 | Avg Loss: 0.0147 | Grad Norm: 0.00717184\n",
      "Epoch 3 | Step 1937000 | Avg Loss: 0.0146 | Grad Norm: 0.00890488\n",
      "Epoch 3 | Step 1937100 | Avg Loss: 0.0147 | Grad Norm: 0.00822641\n",
      "Epoch 3 | Step 1937200 | Avg Loss: 0.0153 | Grad Norm: 0.00861763\n",
      "Epoch 3 | Step 1937300 | Avg Loss: 0.0150 | Grad Norm: 0.00712809\n",
      "Epoch 3 | Step 1937400 | Avg Loss: 0.0149 | Grad Norm: 0.00865626\n",
      "Epoch 3 | Step 1937500 | Avg Loss: 0.0150 | Grad Norm: 0.00757171\n",
      "Epoch 3 | Step 1937600 | Avg Loss: 0.0146 | Grad Norm: 0.00782164\n",
      "Epoch 3 | Step 1937700 | Avg Loss: 0.0148 | Grad Norm: 0.00924036\n",
      "Epoch 3 | Step 1937800 | Avg Loss: 0.0149 | Grad Norm: 0.00878783\n",
      "Epoch 3 | Step 1937900 | Avg Loss: 0.0147 | Grad Norm: 0.00766615\n",
      "Epoch 3 | Step 1938000 | Avg Loss: 0.0145 | Grad Norm: 0.00832873\n",
      "Epoch 3 | Step 1938100 | Avg Loss: 0.0152 | Grad Norm: 0.00845991\n",
      "Epoch 3 | Step 1938200 | Avg Loss: 0.0151 | Grad Norm: 0.00919189\n",
      "Epoch 3 | Step 1938300 | Avg Loss: 0.0155 | Grad Norm: 0.00957737\n",
      "Epoch 3 | Step 1938400 | Avg Loss: 0.0153 | Grad Norm: 0.00983920\n",
      "Epoch 3 | Step 1938500 | Avg Loss: 0.0152 | Grad Norm: 0.00743855\n",
      "Epoch 3 | Step 1938600 | Avg Loss: 0.0158 | Grad Norm: 0.00887761\n",
      "Epoch 3 | Step 1938700 | Avg Loss: 0.0160 | Grad Norm: 0.00817170\n",
      "Epoch 3 | Step 1938800 | Avg Loss: 0.0161 | Grad Norm: 0.00976582\n",
      "Epoch 3 | Step 1938900 | Avg Loss: 0.0161 | Grad Norm: 0.00774480\n",
      "Epoch 3 | Step 1939000 | Avg Loss: 0.0161 | Grad Norm: 0.00906558\n",
      "Epoch 3 | Step 1939100 | Avg Loss: 0.0159 | Grad Norm: 0.00929308\n",
      "Epoch 3 | Step 1939200 | Avg Loss: 0.0159 | Grad Norm: 0.00951950\n",
      "Epoch 3 | Step 1939300 | Avg Loss: 0.0159 | Grad Norm: 0.00722808\n",
      "Epoch 3 | Step 1939400 | Avg Loss: 0.0159 | Grad Norm: 0.00779483\n",
      "Epoch 3 | Step 1939500 | Avg Loss: 0.0158 | Grad Norm: 0.00854989\n",
      "Epoch 3 | Step 1939600 | Avg Loss: 0.0157 | Grad Norm: 0.00780914\n",
      "Epoch 3 | Step 1939700 | Avg Loss: 0.0153 | Grad Norm: 0.00748416\n",
      "Epoch 3 | Step 1939800 | Avg Loss: 0.0149 | Grad Norm: 0.00726426\n",
      "Epoch 3 | Step 1939900 | Avg Loss: 0.0152 | Grad Norm: 0.00744713\n",
      "Epoch 3 | Step 1940000 | Avg Loss: 0.0150 | Grad Norm: 0.00728094\n",
      "Epoch 3 | Step 1940100 | Avg Loss: 0.0149 | Grad Norm: 0.00996555\n",
      "Epoch 3 | Step 1940200 | Avg Loss: 0.0151 | Grad Norm: 0.00956657\n",
      "Epoch 3 | Step 1940300 | Avg Loss: 0.0148 | Grad Norm: 0.00749613\n",
      "Epoch 3 | Step 1940400 | Avg Loss: 0.0154 | Grad Norm: 0.00895308\n",
      "Epoch 3 | Step 1940500 | Avg Loss: 0.0154 | Grad Norm: 0.00949580\n",
      "Epoch 3 | Step 1940600 | Avg Loss: 0.0153 | Grad Norm: 0.00805315\n",
      "Epoch 3 | Step 1940700 | Avg Loss: 0.0151 | Grad Norm: 0.00758953\n",
      "Epoch 3 | Step 1940800 | Avg Loss: 0.0152 | Grad Norm: 0.00778499\n",
      "Epoch 3 | Step 1940900 | Avg Loss: 0.0149 | Grad Norm: 0.00781388\n",
      "Epoch 3 | Step 1941000 | Avg Loss: 0.0149 | Grad Norm: 0.01011381\n",
      "Epoch 3 | Step 1941100 | Avg Loss: 0.0152 | Grad Norm: 0.00745663\n",
      "Epoch 3 | Step 1941200 | Avg Loss: 0.0154 | Grad Norm: 0.00706716\n",
      "Epoch 3 | Step 1941300 | Avg Loss: 0.0156 | Grad Norm: 0.00829513\n",
      "Epoch 3 | Step 1941400 | Avg Loss: 0.0153 | Grad Norm: 0.00811496\n",
      "Epoch 3 | Step 1941500 | Avg Loss: 0.0149 | Grad Norm: 0.00908639\n",
      "Epoch 3 | Step 1941600 | Avg Loss: 0.0153 | Grad Norm: 0.00931322\n",
      "Epoch 3 | Step 1941700 | Avg Loss: 0.0153 | Grad Norm: 0.00742074\n",
      "Epoch 3 | Step 1941800 | Avg Loss: 0.0154 | Grad Norm: 0.00734808\n",
      "Epoch 3 | Step 1941900 | Avg Loss: 0.0154 | Grad Norm: 0.00814240\n",
      "Epoch 3 | Step 1942000 | Avg Loss: 0.0152 | Grad Norm: 0.01070304\n",
      "Epoch 3 | Step 1942100 | Avg Loss: 0.0154 | Grad Norm: 0.00832915\n",
      "Epoch 3 | Step 1942200 | Avg Loss: 0.0149 | Grad Norm: 0.00802047\n",
      "Epoch 3 | Step 1942300 | Avg Loss: 0.0150 | Grad Norm: 0.00784067\n",
      "Epoch 3 | Step 1942400 | Avg Loss: 0.0149 | Grad Norm: 0.00813258\n",
      "Epoch 3 | Step 1942500 | Avg Loss: 0.0148 | Grad Norm: 0.00784472\n",
      "Epoch 3 | Step 1942600 | Avg Loss: 0.0150 | Grad Norm: 0.00978679\n",
      "Epoch 3 | Step 1942700 | Avg Loss: 0.0150 | Grad Norm: 0.00742821\n",
      "Epoch 3 | Step 1942800 | Avg Loss: 0.0152 | Grad Norm: 0.00846285\n",
      "Epoch 3 | Step 1942900 | Avg Loss: 0.0151 | Grad Norm: 0.00699903\n",
      "Epoch 3 | Step 1943000 | Avg Loss: 0.0149 | Grad Norm: 0.00805548\n",
      "Epoch 3 | Step 1943100 | Avg Loss: 0.0147 | Grad Norm: 0.00824994\n",
      "Epoch 3 | Step 1943200 | Avg Loss: 0.0150 | Grad Norm: 0.00826611\n",
      "Epoch 3 | Step 1943300 | Avg Loss: 0.0155 | Grad Norm: 0.00737100\n",
      "Epoch 3 | Step 1943400 | Avg Loss: 0.0158 | Grad Norm: 0.00683799\n",
      "Epoch 3 | Step 1943500 | Avg Loss: 0.0157 | Grad Norm: 0.00819128\n",
      "Epoch 3 | Step 1943600 | Avg Loss: 0.0152 | Grad Norm: 0.00889347\n",
      "Epoch 3 | Step 1943700 | Avg Loss: 0.0150 | Grad Norm: 0.00874766\n",
      "Epoch 3 | Step 1943800 | Avg Loss: 0.0145 | Grad Norm: 0.00668794\n",
      "Epoch 3 | Step 1943900 | Avg Loss: 0.0148 | Grad Norm: 0.00800490\n",
      "Epoch 3 | Step 1944000 | Avg Loss: 0.0147 | Grad Norm: 0.00918416\n",
      "Epoch 3 | Step 1944100 | Avg Loss: 0.0152 | Grad Norm: 0.00708638\n",
      "Epoch 3 | Step 1944200 | Avg Loss: 0.0152 | Grad Norm: 0.00736789\n",
      "Epoch 3 | Step 1944300 | Avg Loss: 0.0149 | Grad Norm: 0.00712486\n",
      "Epoch 3 | Step 1944400 | Avg Loss: 0.0147 | Grad Norm: 0.00751195\n",
      "Epoch 3 | Step 1944500 | Avg Loss: 0.0149 | Grad Norm: 0.00815497\n",
      "Epoch 3 | Step 1944600 | Avg Loss: 0.0149 | Grad Norm: 0.00907650\n",
      "Epoch 3 | Step 1944700 | Avg Loss: 0.0154 | Grad Norm: 0.00691269\n",
      "Epoch 3 | Step 1944800 | Avg Loss: 0.0156 | Grad Norm: 0.00744155\n",
      "Epoch 3 | Step 1944900 | Avg Loss: 0.0152 | Grad Norm: 0.00881561\n",
      "Epoch 3 | Step 1945000 | Avg Loss: 0.0153 | Grad Norm: 0.00780692\n",
      "Epoch 3 | Step 1945100 | Avg Loss: 0.0151 | Grad Norm: 0.00794990\n",
      "Epoch 3 | Step 1945200 | Avg Loss: 0.0149 | Grad Norm: 0.00818932\n",
      "Epoch 3 | Step 1945300 | Avg Loss: 0.0147 | Grad Norm: 0.00817711\n",
      "Epoch 3 | Step 1945400 | Avg Loss: 0.0147 | Grad Norm: 0.00897117\n",
      "Epoch 3 | Step 1945500 | Avg Loss: 0.0149 | Grad Norm: 0.00756332\n",
      "Epoch 3 | Step 1945600 | Avg Loss: 0.0151 | Grad Norm: 0.00758614\n",
      "Epoch 3 | Step 1945700 | Avg Loss: 0.0151 | Grad Norm: 0.00830997\n",
      "Epoch 3 | Step 1945800 | Avg Loss: 0.0149 | Grad Norm: 0.00783853\n",
      "Epoch 3 | Step 1945900 | Avg Loss: 0.0149 | Grad Norm: 0.00829889\n",
      "Epoch 3 | Step 1946000 | Avg Loss: 0.0145 | Grad Norm: 0.00741095\n",
      "Epoch 3 | Step 1946100 | Avg Loss: 0.0148 | Grad Norm: 0.00929464\n",
      "Epoch 3 | Step 1946200 | Avg Loss: 0.0149 | Grad Norm: 0.00783565\n",
      "Epoch 3 | Step 1946300 | Avg Loss: 0.0146 | Grad Norm: 0.00810945\n",
      "Epoch 3 | Step 1946400 | Avg Loss: 0.0146 | Grad Norm: 0.00988285\n",
      "Epoch 3 | Step 1946500 | Avg Loss: 0.0147 | Grad Norm: 0.00832715\n",
      "Epoch 3 | Step 1946600 | Avg Loss: 0.0147 | Grad Norm: 0.00700427\n",
      "Epoch 3 | Step 1946700 | Avg Loss: 0.0151 | Grad Norm: 0.00863502\n",
      "Epoch 3 | Step 1946800 | Avg Loss: 0.0149 | Grad Norm: 0.00854752\n",
      "Epoch 3 | Step 1946900 | Avg Loss: 0.0148 | Grad Norm: 0.00991871\n",
      "Epoch 3 | Step 1947000 | Avg Loss: 0.0150 | Grad Norm: 0.00933041\n",
      "Epoch 3 | Step 1947100 | Avg Loss: 0.0154 | Grad Norm: 0.01017149\n",
      "Epoch 3 | Step 1947200 | Avg Loss: 0.0155 | Grad Norm: 0.00786928\n",
      "Epoch 3 | Step 1947300 | Avg Loss: 0.0154 | Grad Norm: 0.00820607\n",
      "Epoch 3 | Step 1947400 | Avg Loss: 0.0151 | Grad Norm: 0.00781242\n",
      "Epoch 3 | Step 1947500 | Avg Loss: 0.0150 | Grad Norm: 0.00818053\n",
      "Epoch 3 | Step 1947600 | Avg Loss: 0.0148 | Grad Norm: 0.00771589\n",
      "Epoch 3 | Step 1947700 | Avg Loss: 0.0147 | Grad Norm: 0.00887793\n",
      "Epoch 3 | Step 1947800 | Avg Loss: 0.0151 | Grad Norm: 0.00824584\n",
      "Epoch 3 | Step 1947900 | Avg Loss: 0.0151 | Grad Norm: 0.00763760\n",
      "Epoch 3 | Step 1948000 | Avg Loss: 0.0151 | Grad Norm: 0.00857183\n",
      "Epoch 3 | Step 1948100 | Avg Loss: 0.0151 | Grad Norm: 0.00883355\n",
      "Epoch 3 | Step 1948200 | Avg Loss: 0.0153 | Grad Norm: 0.00832836\n",
      "Epoch 3 | Step 1948300 | Avg Loss: 0.0149 | Grad Norm: 0.01038226\n",
      "Epoch 3 | Step 1948400 | Avg Loss: 0.0147 | Grad Norm: 0.00715825\n",
      "Epoch 3 | Step 1948500 | Avg Loss: 0.0146 | Grad Norm: 0.00719083\n",
      "Epoch 3 | Step 1948600 | Avg Loss: 0.0148 | Grad Norm: 0.00746696\n",
      "Epoch 3 | Step 1948700 | Avg Loss: 0.0147 | Grad Norm: 0.00965268\n",
      "Epoch 3 | Step 1948800 | Avg Loss: 0.0145 | Grad Norm: 0.00983996\n",
      "Epoch 3 | Step 1948900 | Avg Loss: 0.0147 | Grad Norm: 0.00744055\n",
      "Epoch 3 | Step 1949000 | Avg Loss: 0.0144 | Grad Norm: 0.00783430\n",
      "Epoch 3 | Step 1949100 | Avg Loss: 0.0145 | Grad Norm: 0.00864331\n",
      "Epoch 3 | Step 1949200 | Avg Loss: 0.0145 | Grad Norm: 0.00732253\n",
      "Epoch 3 | Step 1949300 | Avg Loss: 0.0142 | Grad Norm: 0.00780974\n",
      "Epoch 3 | Step 1949400 | Avg Loss: 0.0148 | Grad Norm: 0.00834981\n",
      "Epoch 3 | Step 1949500 | Avg Loss: 0.0147 | Grad Norm: 0.00740130\n",
      "Epoch 3 | Step 1949600 | Avg Loss: 0.0144 | Grad Norm: 0.00890222\n",
      "Epoch 3 | Step 1949700 | Avg Loss: 0.0144 | Grad Norm: 0.00807868\n",
      "Epoch 3 | Step 1949800 | Avg Loss: 0.0144 | Grad Norm: 0.00827586\n",
      "Epoch 3 | Step 1949900 | Avg Loss: 0.0148 | Grad Norm: 0.00802879\n",
      "Epoch 3 | Step 1950000 | Avg Loss: 0.0147 | Grad Norm: 0.00856448\n",
      "Epoch 3 | Step 1950100 | Avg Loss: 0.0153 | Grad Norm: 0.00894315\n",
      "Epoch 3 | Step 1950200 | Avg Loss: 0.0154 | Grad Norm: 0.00795252\n",
      "Epoch 3 | Step 1950300 | Avg Loss: 0.0152 | Grad Norm: 0.00958376\n",
      "Epoch 3 | Step 1950400 | Avg Loss: 0.0151 | Grad Norm: 0.00666854\n",
      "Epoch 3 | Step 1950500 | Avg Loss: 0.0150 | Grad Norm: 0.00873034\n",
      "Epoch 3 | Step 1950600 | Avg Loss: 0.0154 | Grad Norm: 0.00905044\n",
      "Epoch 3 | Step 1950700 | Avg Loss: 0.0153 | Grad Norm: 0.00774150\n",
      "Epoch 3 | Step 1950800 | Avg Loss: 0.0153 | Grad Norm: 0.01079160\n",
      "Epoch 3 | Step 1950900 | Avg Loss: 0.0155 | Grad Norm: 0.00721253\n",
      "Epoch 3 | Step 1951000 | Avg Loss: 0.0150 | Grad Norm: 0.00761634\n",
      "Epoch 3 | Step 1951100 | Avg Loss: 0.0154 | Grad Norm: 0.00875396\n",
      "Epoch 3 | Step 1951200 | Avg Loss: 0.0154 | Grad Norm: 0.00961549\n",
      "Epoch 3 | Step 1951300 | Avg Loss: 0.0157 | Grad Norm: 0.00755694\n",
      "Epoch 3 | Step 1951400 | Avg Loss: 0.0155 | Grad Norm: 0.00941363\n",
      "Epoch 3 | Step 1951500 | Avg Loss: 0.0155 | Grad Norm: 0.00815262\n",
      "Epoch 3 | Step 1951600 | Avg Loss: 0.0154 | Grad Norm: 0.00651095\n",
      "Epoch 3 | Step 1951700 | Avg Loss: 0.0153 | Grad Norm: 0.00839332\n",
      "Epoch 3 | Step 1951800 | Avg Loss: 0.0155 | Grad Norm: 0.00966165\n",
      "Epoch 3 | Step 1951900 | Avg Loss: 0.0156 | Grad Norm: 0.00923843\n",
      "Epoch 3 | Step 1952000 | Avg Loss: 0.0155 | Grad Norm: 0.00791651\n",
      "Epoch 3 | Step 1952100 | Avg Loss: 0.0154 | Grad Norm: 0.00765627\n",
      "Epoch 3 | Step 1952200 | Avg Loss: 0.0153 | Grad Norm: 0.00865607\n",
      "Epoch 3 | Step 1952300 | Avg Loss: 0.0155 | Grad Norm: 0.00657704\n",
      "Epoch 3 | Step 1952400 | Avg Loss: 0.0151 | Grad Norm: 0.00723782\n",
      "Epoch 3 | Step 1952500 | Avg Loss: 0.0155 | Grad Norm: 0.00749492\n",
      "Epoch 3 | Step 1952600 | Avg Loss: 0.0155 | Grad Norm: 0.00681679\n",
      "Epoch 3 | Step 1952700 | Avg Loss: 0.0157 | Grad Norm: 0.00709249\n",
      "Epoch 3 | Step 1952800 | Avg Loss: 0.0157 | Grad Norm: 0.00959572\n",
      "Epoch 3 | Step 1952900 | Avg Loss: 0.0160 | Grad Norm: 0.00962858\n",
      "Epoch 3 | Step 1953000 | Avg Loss: 0.0158 | Grad Norm: 0.00745700\n",
      "Epoch 3 | Step 1953100 | Avg Loss: 0.0158 | Grad Norm: 0.00996262\n",
      "Epoch 3 | Step 1953200 | Avg Loss: 0.0154 | Grad Norm: 0.01325461\n",
      "Epoch 3 | Step 1953300 | Avg Loss: 0.0154 | Grad Norm: 0.00925365\n",
      "Epoch 3 | Step 1953400 | Avg Loss: 0.0152 | Grad Norm: 0.00720633\n",
      "Epoch 3 | Step 1953500 | Avg Loss: 0.0152 | Grad Norm: 0.00827266\n",
      "Epoch 3 | Step 1953600 | Avg Loss: 0.0153 | Grad Norm: 0.00961569\n",
      "Epoch 3 | Step 1953700 | Avg Loss: 0.0156 | Grad Norm: 0.00649003\n",
      "Epoch 3 | Step 1953800 | Avg Loss: 0.0158 | Grad Norm: 0.00856915\n",
      "Epoch 3 | Step 1953900 | Avg Loss: 0.0155 | Grad Norm: 0.00895877\n",
      "Epoch 3 | Step 1954000 | Avg Loss: 0.0157 | Grad Norm: 0.00891231\n",
      "Epoch 3 | Step 1954100 | Avg Loss: 0.0154 | Grad Norm: 0.01014896\n",
      "Epoch 3 | Step 1954200 | Avg Loss: 0.0155 | Grad Norm: 0.00830597\n",
      "Epoch 3 | Step 1954300 | Avg Loss: 0.0155 | Grad Norm: 0.00830792\n",
      "Epoch 3 | Step 1954400 | Avg Loss: 0.0153 | Grad Norm: 0.00859621\n",
      "Epoch 3 | Step 1954500 | Avg Loss: 0.0148 | Grad Norm: 0.00920853\n",
      "Epoch 3 | Step 1954600 | Avg Loss: 0.0152 | Grad Norm: 0.00734756\n",
      "Epoch 3 | Step 1954700 | Avg Loss: 0.0151 | Grad Norm: 0.00908432\n",
      "Epoch 3 | Step 1954800 | Avg Loss: 0.0150 | Grad Norm: 0.00895596\n",
      "Epoch 3 | Step 1954900 | Avg Loss: 0.0152 | Grad Norm: 0.00835460\n",
      "Epoch 3 | Step 1955000 | Avg Loss: 0.0150 | Grad Norm: 0.00719480\n",
      "Epoch 3 | Step 1955100 | Avg Loss: 0.0149 | Grad Norm: 0.00810081\n",
      "Epoch 3 | Step 1955200 | Avg Loss: 0.0151 | Grad Norm: 0.00860719\n",
      "Epoch 3 | Step 1955300 | Avg Loss: 0.0153 | Grad Norm: 0.01076011\n",
      "Epoch 3 | Step 1955400 | Avg Loss: 0.0150 | Grad Norm: 0.00892131\n",
      "Epoch 3 | Step 1955500 | Avg Loss: 0.0152 | Grad Norm: 0.00836935\n",
      "Epoch 3 | Step 1955600 | Avg Loss: 0.0151 | Grad Norm: 0.00936064\n",
      "Epoch 3 | Step 1955700 | Avg Loss: 0.0151 | Grad Norm: 0.00818918\n",
      "Epoch 3 | Step 1955800 | Avg Loss: 0.0148 | Grad Norm: 0.00835478\n",
      "Epoch 3 | Step 1955900 | Avg Loss: 0.0152 | Grad Norm: 0.00973100\n",
      "Epoch 3 | Step 1956000 | Avg Loss: 0.0157 | Grad Norm: 0.01025507\n",
      "Epoch 3 | Step 1956100 | Avg Loss: 0.0152 | Grad Norm: 0.00765264\n",
      "Epoch 3 | Step 1956200 | Avg Loss: 0.0156 | Grad Norm: 0.00867245\n",
      "Epoch 3 | Step 1956300 | Avg Loss: 0.0157 | Grad Norm: 0.00864574\n",
      "Epoch 3 | Step 1956400 | Avg Loss: 0.0159 | Grad Norm: 0.00727477\n",
      "Epoch 3 | Step 1956500 | Avg Loss: 0.0156 | Grad Norm: 0.00956491\n",
      "Epoch 3 | Step 1956600 | Avg Loss: 0.0159 | Grad Norm: 0.00838941\n",
      "Epoch 3 | Step 1956700 | Avg Loss: 0.0160 | Grad Norm: 0.00920206\n",
      "Epoch 3 | Step 1956800 | Avg Loss: 0.0159 | Grad Norm: 0.00792600\n",
      "Epoch 3 | Step 1956900 | Avg Loss: 0.0155 | Grad Norm: 0.00653675\n",
      "Epoch 3 | Step 1957000 | Avg Loss: 0.0156 | Grad Norm: 0.00913032\n",
      "Epoch 3 | Step 1957100 | Avg Loss: 0.0151 | Grad Norm: 0.00767293\n",
      "Epoch 3 | Step 1957200 | Avg Loss: 0.0153 | Grad Norm: 0.00804388\n",
      "Epoch 3 | Step 1957300 | Avg Loss: 0.0152 | Grad Norm: 0.00741978\n",
      "Epoch 3 | Step 1957400 | Avg Loss: 0.0152 | Grad Norm: 0.01084031\n",
      "Epoch 3 | Step 1957500 | Avg Loss: 0.0148 | Grad Norm: 0.00796456\n",
      "Epoch 3 | Step 1957600 | Avg Loss: 0.0148 | Grad Norm: 0.00845792\n",
      "Epoch 3 | Step 1957700 | Avg Loss: 0.0150 | Grad Norm: 0.00977579\n",
      "Epoch 3 | Step 1957800 | Avg Loss: 0.0150 | Grad Norm: 0.00782494\n",
      "Epoch 3 | Step 1957900 | Avg Loss: 0.0151 | Grad Norm: 0.00767594\n",
      "Epoch 3 | Step 1958000 | Avg Loss: 0.0150 | Grad Norm: 0.00818852\n",
      "Epoch 3 | Step 1958100 | Avg Loss: 0.0151 | Grad Norm: 0.00742512\n",
      "Epoch 3 | Step 1958200 | Avg Loss: 0.0149 | Grad Norm: 0.00682835\n",
      "Epoch 3 | Step 1958300 | Avg Loss: 0.0149 | Grad Norm: 0.00657484\n",
      "Epoch 3 | Step 1958400 | Avg Loss: 0.0152 | Grad Norm: 0.00817473\n",
      "Epoch 3 | Step 1958500 | Avg Loss: 0.0149 | Grad Norm: 0.00789087\n",
      "Epoch 3 | Step 1958600 | Avg Loss: 0.0144 | Grad Norm: 0.00734469\n",
      "Epoch 3 | Step 1958700 | Avg Loss: 0.0149 | Grad Norm: 0.00746647\n",
      "Epoch 3 | Step 1958800 | Avg Loss: 0.0150 | Grad Norm: 0.00847580\n",
      "Epoch 3 | Step 1958900 | Avg Loss: 0.0149 | Grad Norm: 0.00836732\n",
      "Epoch 3 | Step 1959000 | Avg Loss: 0.0149 | Grad Norm: 0.00808335\n",
      "Epoch 3 | Step 1959100 | Avg Loss: 0.0149 | Grad Norm: 0.00815229\n",
      "Epoch 3 | Step 1959200 | Avg Loss: 0.0146 | Grad Norm: 0.00740364\n",
      "Epoch 3 | Step 1959300 | Avg Loss: 0.0148 | Grad Norm: 0.00811992\n",
      "Epoch 3 | Step 1959400 | Avg Loss: 0.0153 | Grad Norm: 0.01124056\n",
      "Epoch 3 | Step 1959500 | Avg Loss: 0.0155 | Grad Norm: 0.00763331\n",
      "Epoch 3 | Step 1959600 | Avg Loss: 0.0149 | Grad Norm: 0.00778236\n",
      "Epoch 3 | Step 1959700 | Avg Loss: 0.0154 | Grad Norm: 0.00759245\n",
      "Epoch 3 | Step 1959800 | Avg Loss: 0.0153 | Grad Norm: 0.00748159\n",
      "Epoch 3 | Step 1959900 | Avg Loss: 0.0156 | Grad Norm: 0.00891065\n",
      "Epoch 3 | Step 1960000 | Avg Loss: 0.0156 | Grad Norm: 0.00789058\n",
      "Epoch 3 | Step 1960100 | Avg Loss: 0.0152 | Grad Norm: 0.00775906\n",
      "Epoch 3 | Step 1960200 | Avg Loss: 0.0150 | Grad Norm: 0.00836996\n",
      "Epoch 3 | Step 1960300 | Avg Loss: 0.0147 | Grad Norm: 0.00828481\n",
      "Epoch 3 | Step 1960400 | Avg Loss: 0.0150 | Grad Norm: 0.00760834\n",
      "Epoch 3 | Step 1960500 | Avg Loss: 0.0151 | Grad Norm: 0.00917444\n",
      "Epoch 3 | Step 1960600 | Avg Loss: 0.0151 | Grad Norm: 0.00731365\n",
      "Epoch 3 | Step 1960700 | Avg Loss: 0.0148 | Grad Norm: 0.00800528\n",
      "Epoch 3 | Step 1960800 | Avg Loss: 0.0150 | Grad Norm: 0.00719180\n",
      "Epoch 3 | Step 1960900 | Avg Loss: 0.0151 | Grad Norm: 0.00813028\n",
      "Epoch 3 | Step 1961000 | Avg Loss: 0.0150 | Grad Norm: 0.00864222\n",
      "Epoch 3 | Step 1961100 | Avg Loss: 0.0152 | Grad Norm: 0.00892473\n",
      "Epoch 3 | Step 1961200 | Avg Loss: 0.0152 | Grad Norm: 0.00825179\n",
      "Epoch 3 | Step 1961300 | Avg Loss: 0.0152 | Grad Norm: 0.00756375\n",
      "Epoch 3 | Step 1961400 | Avg Loss: 0.0156 | Grad Norm: 0.00821746\n",
      "Epoch 3 | Step 1961500 | Avg Loss: 0.0154 | Grad Norm: 0.00891896\n",
      "Epoch 3 | Step 1961600 | Avg Loss: 0.0149 | Grad Norm: 0.00817768\n",
      "Epoch 3 | Step 1961700 | Avg Loss: 0.0151 | Grad Norm: 0.00746649\n",
      "Epoch 3 | Step 1961800 | Avg Loss: 0.0153 | Grad Norm: 0.00762782\n",
      "Epoch 3 | Step 1961900 | Avg Loss: 0.0152 | Grad Norm: 0.00624218\n",
      "Epoch 3 | Step 1962000 | Avg Loss: 0.0159 | Grad Norm: 0.00837354\n",
      "Epoch 3 | Step 1962100 | Avg Loss: 0.0163 | Grad Norm: 0.00800645\n",
      "Epoch 3 | Step 1962200 | Avg Loss: 0.0160 | Grad Norm: 0.01147394\n",
      "Epoch 3 | Step 1962300 | Avg Loss: 0.0158 | Grad Norm: 0.01047818\n",
      "Epoch 3 | Step 1962400 | Avg Loss: 0.0156 | Grad Norm: 0.00804699\n",
      "Epoch 3 | Step 1962500 | Avg Loss: 0.0155 | Grad Norm: 0.00949165\n",
      "Epoch 3 | Step 1962600 | Avg Loss: 0.0157 | Grad Norm: 0.00768254\n",
      "Epoch 3 | Step 1962700 | Avg Loss: 0.0151 | Grad Norm: 0.00841405\n",
      "Epoch 3 | Step 1962800 | Avg Loss: 0.0153 | Grad Norm: 0.00807794\n",
      "Epoch 3 | Step 1962900 | Avg Loss: 0.0156 | Grad Norm: 0.00858161\n",
      "Epoch 3 | Step 1963000 | Avg Loss: 0.0148 | Grad Norm: 0.00733530\n",
      "Epoch 3 | Step 1963100 | Avg Loss: 0.0148 | Grad Norm: 0.00796659\n",
      "Epoch 3 | Step 1963200 | Avg Loss: 0.0147 | Grad Norm: 0.00837717\n",
      "Epoch 3 | Step 1963300 | Avg Loss: 0.0147 | Grad Norm: 0.00794944\n",
      "Epoch 3 | Step 1963400 | Avg Loss: 0.0146 | Grad Norm: 0.00772999\n",
      "Epoch 3 | Step 1963500 | Avg Loss: 0.0147 | Grad Norm: 0.00767642\n",
      "Epoch 3 | Step 1963600 | Avg Loss: 0.0145 | Grad Norm: 0.00684313\n",
      "Epoch 3 | Step 1963700 | Avg Loss: 0.0144 | Grad Norm: 0.00846559\n",
      "Epoch 3 | Step 1963800 | Avg Loss: 0.0147 | Grad Norm: 0.00810749\n",
      "Epoch 3 | Step 1963900 | Avg Loss: 0.0151 | Grad Norm: 0.00895165\n",
      "Epoch 3 | Step 1964000 | Avg Loss: 0.0149 | Grad Norm: 0.00683229\n",
      "Epoch 3 | Step 1964100 | Avg Loss: 0.0150 | Grad Norm: 0.00715594\n",
      "Epoch 3 | Step 1964200 | Avg Loss: 0.0149 | Grad Norm: 0.00925003\n",
      "Epoch 3 | Step 1964300 | Avg Loss: 0.0146 | Grad Norm: 0.00788501\n",
      "Epoch 3 | Step 1964400 | Avg Loss: 0.0146 | Grad Norm: 0.00756059\n",
      "Epoch 3 | Step 1964500 | Avg Loss: 0.0145 | Grad Norm: 0.00679824\n",
      "Epoch 3 | Step 1964600 | Avg Loss: 0.0151 | Grad Norm: 0.00908385\n",
      "Epoch 3 | Step 1964700 | Avg Loss: 0.0153 | Grad Norm: 0.00881704\n",
      "Epoch 3 | Step 1964800 | Avg Loss: 0.0147 | Grad Norm: 0.00743172\n",
      "Epoch 3 | Step 1964900 | Avg Loss: 0.0146 | Grad Norm: 0.00854858\n",
      "Epoch 3 | Step 1965000 | Avg Loss: 0.0145 | Grad Norm: 0.00824249\n",
      "Epoch 3 | Step 1965100 | Avg Loss: 0.0145 | Grad Norm: 0.00829261\n",
      "Epoch 3 | Step 1965200 | Avg Loss: 0.0147 | Grad Norm: 0.00739045\n",
      "Epoch 3 | Step 1965300 | Avg Loss: 0.0145 | Grad Norm: 0.00748407\n",
      "Epoch 3 | Step 1965400 | Avg Loss: 0.0145 | Grad Norm: 0.00878393\n",
      "Epoch 3 | Step 1965500 | Avg Loss: 0.0148 | Grad Norm: 0.00755899\n",
      "Epoch 3 | Step 1965600 | Avg Loss: 0.0145 | Grad Norm: 0.00918691\n",
      "Epoch 3 | Step 1965700 | Avg Loss: 0.0150 | Grad Norm: 0.00830919\n",
      "Epoch 3 | Step 1965800 | Avg Loss: 0.0148 | Grad Norm: 0.00720632\n",
      "Epoch 3 | Step 1965900 | Avg Loss: 0.0146 | Grad Norm: 0.00703788\n",
      "Epoch 3 | Step 1966000 | Avg Loss: 0.0149 | Grad Norm: 0.00745170\n",
      "Epoch 3 | Step 1966100 | Avg Loss: 0.0149 | Grad Norm: 0.00807454\n",
      "Epoch 3 | Step 1966200 | Avg Loss: 0.0149 | Grad Norm: 0.00897335\n",
      "Epoch 3 | Step 1966300 | Avg Loss: 0.0149 | Grad Norm: 0.00802444\n",
      "Epoch 3 | Step 1966400 | Avg Loss: 0.0151 | Grad Norm: 0.01055487\n",
      "Epoch 3 | Step 1966500 | Avg Loss: 0.0153 | Grad Norm: 0.00930306\n",
      "Epoch 3 | Step 1966600 | Avg Loss: 0.0154 | Grad Norm: 0.00773741\n",
      "Epoch 3 | Step 1966700 | Avg Loss: 0.0153 | Grad Norm: 0.00826141\n",
      "Epoch 3 | Step 1966800 | Avg Loss: 0.0149 | Grad Norm: 0.00704533\n",
      "Epoch 3 | Step 1966900 | Avg Loss: 0.0151 | Grad Norm: 0.00859997\n",
      "Epoch 3 | Step 1967000 | Avg Loss: 0.0155 | Grad Norm: 0.00919932\n",
      "Epoch 3 | Step 1967100 | Avg Loss: 0.0161 | Grad Norm: 0.00794866\n",
      "Epoch 3 | Step 1967200 | Avg Loss: 0.0157 | Grad Norm: 0.00973516\n",
      "Epoch 3 | Step 1967300 | Avg Loss: 0.0158 | Grad Norm: 0.00779651\n",
      "Epoch 3 | Step 1967400 | Avg Loss: 0.0154 | Grad Norm: 0.00902627\n",
      "Epoch 3 | Step 1967500 | Avg Loss: 0.0155 | Grad Norm: 0.00812051\n",
      "Epoch 3 | Step 1967600 | Avg Loss: 0.0154 | Grad Norm: 0.00894296\n",
      "Epoch 3 | Step 1967700 | Avg Loss: 0.0151 | Grad Norm: 0.00758286\n",
      "Epoch 3 | Step 1967800 | Avg Loss: 0.0155 | Grad Norm: 0.00769243\n",
      "Epoch 3 | Step 1967900 | Avg Loss: 0.0153 | Grad Norm: 0.00779703\n",
      "Epoch 3 | Step 1968000 | Avg Loss: 0.0153 | Grad Norm: 0.00808439\n",
      "Epoch 3 | Step 1968100 | Avg Loss: 0.0155 | Grad Norm: 0.00661576\n",
      "Epoch 3 | Step 1968200 | Avg Loss: 0.0153 | Grad Norm: 0.00854973\n",
      "Epoch 3 | Step 1968300 | Avg Loss: 0.0153 | Grad Norm: 0.00755390\n",
      "Epoch 3 | Step 1968400 | Avg Loss: 0.0153 | Grad Norm: 0.00812516\n",
      "Epoch 3 | Step 1968500 | Avg Loss: 0.0152 | Grad Norm: 0.00804998\n",
      "Epoch 3 | Step 1968600 | Avg Loss: 0.0152 | Grad Norm: 0.00753506\n",
      "Epoch 3 | Step 1968700 | Avg Loss: 0.0148 | Grad Norm: 0.00717818\n",
      "Epoch 3 | Step 1968800 | Avg Loss: 0.0150 | Grad Norm: 0.00781318\n",
      "Epoch 3 | Step 1968900 | Avg Loss: 0.0151 | Grad Norm: 0.00768544\n",
      "Epoch 3 | Step 1969000 | Avg Loss: 0.0150 | Grad Norm: 0.00785721\n",
      "Epoch 3 | Step 1969100 | Avg Loss: 0.0152 | Grad Norm: 0.01009324\n",
      "Epoch 3 | Step 1969200 | Avg Loss: 0.0153 | Grad Norm: 0.00721287\n",
      "Epoch 3 | Step 1969300 | Avg Loss: 0.0156 | Grad Norm: 0.01268034\n",
      "Epoch 3 | Step 1969400 | Avg Loss: 0.0153 | Grad Norm: 0.01029299\n",
      "Epoch 3 | Step 1969500 | Avg Loss: 0.0156 | Grad Norm: 0.01157762\n",
      "Epoch 3 | Step 1969600 | Avg Loss: 0.0155 | Grad Norm: 0.00727600\n",
      "Epoch 3 | Step 1969700 | Avg Loss: 0.0153 | Grad Norm: 0.00715679\n",
      "Epoch 3 | Step 1969800 | Avg Loss: 0.0154 | Grad Norm: 0.00769190\n",
      "Epoch 3 | Step 1969900 | Avg Loss: 0.0152 | Grad Norm: 0.00912643\n",
      "Epoch 3 | Step 1970000 | Avg Loss: 0.0154 | Grad Norm: 0.01009228\n",
      "Epoch 3 | Step 1970100 | Avg Loss: 0.0152 | Grad Norm: 0.00740125\n",
      "Epoch 3 | Step 1970200 | Avg Loss: 0.0154 | Grad Norm: 0.00802681\n",
      "Epoch 3 | Step 1970300 | Avg Loss: 0.0152 | Grad Norm: 0.00769676\n",
      "Epoch 3 | Step 1970400 | Avg Loss: 0.0153 | Grad Norm: 0.00767585\n",
      "Epoch 3 | Step 1970500 | Avg Loss: 0.0153 | Grad Norm: 0.00805572\n",
      "Epoch 3 | Step 1970600 | Avg Loss: 0.0152 | Grad Norm: 0.00723021\n",
      "Epoch 3 | Step 1970700 | Avg Loss: 0.0151 | Grad Norm: 0.00750445\n",
      "Epoch 3 | Step 1970800 | Avg Loss: 0.0150 | Grad Norm: 0.00746967\n",
      "Epoch 3 | Step 1970900 | Avg Loss: 0.0150 | Grad Norm: 0.00768679\n",
      "Epoch 3 | Step 1971000 | Avg Loss: 0.0152 | Grad Norm: 0.00840669\n",
      "Epoch 3 | Step 1971100 | Avg Loss: 0.0148 | Grad Norm: 0.00741234\n",
      "Epoch 3 | Step 1971200 | Avg Loss: 0.0151 | Grad Norm: 0.00793539\n",
      "Epoch 3 | Step 1971300 | Avg Loss: 0.0156 | Grad Norm: 0.00861452\n",
      "Epoch 3 | Step 1971400 | Avg Loss: 0.0155 | Grad Norm: 0.00788587\n",
      "Epoch 3 | Step 1971500 | Avg Loss: 0.0153 | Grad Norm: 0.00904197\n",
      "Epoch 3 | Step 1971600 | Avg Loss: 0.0151 | Grad Norm: 0.00879452\n",
      "Epoch 3 | Step 1971700 | Avg Loss: 0.0154 | Grad Norm: 0.00856391\n",
      "Epoch 3 | Step 1971800 | Avg Loss: 0.0158 | Grad Norm: 0.00816363\n",
      "Epoch 3 | Step 1971900 | Avg Loss: 0.0157 | Grad Norm: 0.00876018\n",
      "Epoch 3 | Step 1972000 | Avg Loss: 0.0157 | Grad Norm: 0.00944162\n",
      "Epoch 3 | Step 1972100 | Avg Loss: 0.0156 | Grad Norm: 0.00812579\n",
      "Epoch 3 | Step 1972200 | Avg Loss: 0.0154 | Grad Norm: 0.00832365\n",
      "Epoch 3 | Step 1972300 | Avg Loss: 0.0149 | Grad Norm: 0.00705417\n",
      "Epoch 3 | Step 1972400 | Avg Loss: 0.0146 | Grad Norm: 0.00784312\n",
      "Epoch 3 | Step 1972500 | Avg Loss: 0.0147 | Grad Norm: 0.00738299\n",
      "Epoch 3 | Step 1972600 | Avg Loss: 0.0147 | Grad Norm: 0.00797117\n",
      "Epoch 3 | Step 1972700 | Avg Loss: 0.0146 | Grad Norm: 0.00743032\n",
      "Epoch 3 | Step 1972800 | Avg Loss: 0.0148 | Grad Norm: 0.00835146\n",
      "Epoch 3 | Step 1972900 | Avg Loss: 0.0149 | Grad Norm: 0.00909974\n",
      "Epoch 3 | Step 1973000 | Avg Loss: 0.0148 | Grad Norm: 0.00714181\n",
      "Epoch 3 | Step 1973100 | Avg Loss: 0.0149 | Grad Norm: 0.00821606\n",
      "Epoch 3 | Step 1973200 | Avg Loss: 0.0148 | Grad Norm: 0.00830242\n",
      "Epoch 3 | Step 1973300 | Avg Loss: 0.0148 | Grad Norm: 0.00885274\n",
      "Epoch 3 | Step 1973400 | Avg Loss: 0.0150 | Grad Norm: 0.01035460\n",
      "Epoch 3 | Step 1973500 | Avg Loss: 0.0150 | Grad Norm: 0.00733359\n",
      "Epoch 3 | Step 1973600 | Avg Loss: 0.0149 | Grad Norm: 0.00989164\n",
      "Epoch 3 | Step 1973700 | Avg Loss: 0.0155 | Grad Norm: 0.01123069\n",
      "Epoch 3 | Step 1973800 | Avg Loss: 0.0154 | Grad Norm: 0.00985336\n",
      "Epoch 3 | Step 1973900 | Avg Loss: 0.0156 | Grad Norm: 0.00802536\n",
      "Epoch 3 | Step 1974000 | Avg Loss: 0.0156 | Grad Norm: 0.00897698\n",
      "Epoch 3 | Step 1974100 | Avg Loss: 0.0154 | Grad Norm: 0.00982312\n",
      "Epoch 3 | Step 1974200 | Avg Loss: 0.0152 | Grad Norm: 0.00983285\n",
      "Epoch 3 | Step 1974300 | Avg Loss: 0.0151 | Grad Norm: 0.00863000\n",
      "Epoch 3 | Step 1974400 | Avg Loss: 0.0151 | Grad Norm: 0.00944815\n",
      "Epoch 3 | Step 1974500 | Avg Loss: 0.0151 | Grad Norm: 0.00951902\n",
      "Epoch 3 | Step 1974600 | Avg Loss: 0.0151 | Grad Norm: 0.01074196\n",
      "Epoch 3 | Step 1974700 | Avg Loss: 0.0149 | Grad Norm: 0.00862327\n",
      "Epoch 3 | Step 1974800 | Avg Loss: 0.0152 | Grad Norm: 0.00808977\n",
      "Epoch 3 | Step 1974900 | Avg Loss: 0.0154 | Grad Norm: 0.00961969\n",
      "Epoch 3 | Step 1975000 | Avg Loss: 0.0152 | Grad Norm: 0.00874543\n",
      "Epoch 3 | Step 1975100 | Avg Loss: 0.0152 | Grad Norm: 0.00858840\n",
      "Epoch 3 | Step 1975200 | Avg Loss: 0.0149 | Grad Norm: 0.00916390\n",
      "Epoch 3 | Step 1975300 | Avg Loss: 0.0152 | Grad Norm: 0.00778550\n",
      "Epoch 3 | Step 1975400 | Avg Loss: 0.0151 | Grad Norm: 0.00730051\n",
      "Epoch 3 | Step 1975500 | Avg Loss: 0.0151 | Grad Norm: 0.00815622\n",
      "Epoch 3 | Step 1975600 | Avg Loss: 0.0150 | Grad Norm: 0.00855465\n",
      "Epoch 3 | Step 1975700 | Avg Loss: 0.0151 | Grad Norm: 0.00787399\n",
      "Epoch 3 | Step 1975800 | Avg Loss: 0.0152 | Grad Norm: 0.00811733\n",
      "Epoch 3 | Step 1975900 | Avg Loss: 0.0148 | Grad Norm: 0.00757305\n",
      "Epoch 3 | Step 1976000 | Avg Loss: 0.0151 | Grad Norm: 0.00980556\n",
      "Epoch 3 | Step 1976100 | Avg Loss: 0.0150 | Grad Norm: 0.00817588\n",
      "Epoch 3 | Step 1976200 | Avg Loss: 0.0153 | Grad Norm: 0.00893774\n",
      "Epoch 3 | Step 1976300 | Avg Loss: 0.0151 | Grad Norm: 0.00763952\n",
      "Epoch 3 | Step 1976400 | Avg Loss: 0.0152 | Grad Norm: 0.00852159\n",
      "Epoch 3 | Step 1976500 | Avg Loss: 0.0153 | Grad Norm: 0.01012294\n",
      "Epoch 3 | Step 1976600 | Avg Loss: 0.0152 | Grad Norm: 0.01075070\n",
      "Epoch 3 | Step 1976700 | Avg Loss: 0.0151 | Grad Norm: 0.00839471\n",
      "Epoch 3 | Step 1976800 | Avg Loss: 0.0153 | Grad Norm: 0.01033917\n",
      "Epoch 3 | Step 1976900 | Avg Loss: 0.0155 | Grad Norm: 0.00850602\n",
      "Epoch 3 | Step 1977000 | Avg Loss: 0.0158 | Grad Norm: 0.00774188\n",
      "Epoch 3 | Step 1977100 | Avg Loss: 0.0155 | Grad Norm: 0.00826236\n",
      "Epoch 3 | Step 1977200 | Avg Loss: 0.0154 | Grad Norm: 0.00823482\n",
      "Epoch 3 | Step 1977300 | Avg Loss: 0.0156 | Grad Norm: 0.00726638\n",
      "Epoch 3 | Step 1977400 | Avg Loss: 0.0154 | Grad Norm: 0.00734620\n",
      "Epoch 3 | Step 1977500 | Avg Loss: 0.0156 | Grad Norm: 0.00920744\n",
      "Epoch 3 | Step 1977600 | Avg Loss: 0.0151 | Grad Norm: 0.00788476\n",
      "Epoch 3 | Step 1977700 | Avg Loss: 0.0149 | Grad Norm: 0.00906884\n",
      "Epoch 3 | Step 1977800 | Avg Loss: 0.0149 | Grad Norm: 0.00786589\n",
      "Epoch 3 | Step 1977900 | Avg Loss: 0.0151 | Grad Norm: 0.00807750\n",
      "Epoch 3 | Step 1978000 | Avg Loss: 0.0153 | Grad Norm: 0.00739206\n",
      "Epoch 3 | Step 1978100 | Avg Loss: 0.0151 | Grad Norm: 0.00890544\n",
      "Epoch 3 | Step 1978200 | Avg Loss: 0.0153 | Grad Norm: 0.00755119\n",
      "Epoch 3 | Step 1978300 | Avg Loss: 0.0150 | Grad Norm: 0.00869084\n",
      "Epoch 3 | Step 1978400 | Avg Loss: 0.0152 | Grad Norm: 0.00893938\n",
      "Epoch 3 | Step 1978500 | Avg Loss: 0.0153 | Grad Norm: 0.00845464\n",
      "Epoch 3 | Step 1978600 | Avg Loss: 0.0156 | Grad Norm: 0.00706743\n",
      "Epoch 3 | Step 1978700 | Avg Loss: 0.0153 | Grad Norm: 0.00925043\n",
      "Epoch 3 | Step 1978800 | Avg Loss: 0.0151 | Grad Norm: 0.00951199\n",
      "Epoch 3 | Step 1978900 | Avg Loss: 0.0152 | Grad Norm: 0.00751317\n",
      "Epoch 3 | Step 1979000 | Avg Loss: 0.0150 | Grad Norm: 0.00924173\n",
      "Epoch 3 | Step 1979100 | Avg Loss: 0.0146 | Grad Norm: 0.01254901\n",
      "Epoch 3 | Step 1979200 | Avg Loss: 0.0150 | Grad Norm: 0.00868002\n",
      "Epoch 3 | Step 1979300 | Avg Loss: 0.0152 | Grad Norm: 0.00853565\n",
      "Epoch 3 | Step 1979400 | Avg Loss: 0.0156 | Grad Norm: 0.00865630\n",
      "Epoch 3 | Step 1979500 | Avg Loss: 0.0153 | Grad Norm: 0.00914528\n",
      "Epoch 3 | Step 1979600 | Avg Loss: 0.0153 | Grad Norm: 0.00818373\n",
      "Epoch 3 | Step 1979700 | Avg Loss: 0.0156 | Grad Norm: 0.00859767\n",
      "Epoch 3 | Step 1979800 | Avg Loss: 0.0158 | Grad Norm: 0.01221536\n",
      "Epoch 3 | Step 1979900 | Avg Loss: 0.0157 | Grad Norm: 0.00862639\n",
      "Epoch 3 | Step 1980000 | Avg Loss: 0.0156 | Grad Norm: 0.00894357\n",
      "Epoch 3 | Step 1980100 | Avg Loss: 0.0158 | Grad Norm: 0.00805511\n",
      "Epoch 3 | Step 1980200 | Avg Loss: 0.0154 | Grad Norm: 0.00884530\n",
      "Epoch 3 | Step 1980300 | Avg Loss: 0.0156 | Grad Norm: 0.01029217\n",
      "Epoch 3 | Step 1980400 | Avg Loss: 0.0151 | Grad Norm: 0.00771454\n",
      "Epoch 3 | Step 1980500 | Avg Loss: 0.0153 | Grad Norm: 0.00770797\n",
      "Epoch 3 | Step 1980600 | Avg Loss: 0.0155 | Grad Norm: 0.00817867\n",
      "Epoch 3 | Step 1980700 | Avg Loss: 0.0152 | Grad Norm: 0.00867254\n",
      "Epoch 3 | Step 1980800 | Avg Loss: 0.0153 | Grad Norm: 0.00829792\n",
      "Epoch 3 | Step 1980900 | Avg Loss: 0.0154 | Grad Norm: 0.00873229\n",
      "Epoch 3 | Step 1981000 | Avg Loss: 0.0152 | Grad Norm: 0.00704565\n",
      "Epoch 3 | Step 1981100 | Avg Loss: 0.0152 | Grad Norm: 0.00757977\n",
      "Epoch 3 | Step 1981200 | Avg Loss: 0.0159 | Grad Norm: 0.00895858\n",
      "Epoch 3 | Step 1981300 | Avg Loss: 0.0159 | Grad Norm: 0.00872408\n",
      "Epoch 3 | Step 1981400 | Avg Loss: 0.0157 | Grad Norm: 0.00854453\n",
      "Epoch 3 | Step 1981500 | Avg Loss: 0.0156 | Grad Norm: 0.00786847\n",
      "Epoch 3 | Step 1981600 | Avg Loss: 0.0153 | Grad Norm: 0.00781789\n",
      "Epoch 3 | Step 1981700 | Avg Loss: 0.0153 | Grad Norm: 0.00651435\n",
      "Epoch 3 | Step 1981800 | Avg Loss: 0.0152 | Grad Norm: 0.00709379\n",
      "Epoch 3 | Step 1981900 | Avg Loss: 0.0157 | Grad Norm: 0.00899301\n",
      "Epoch 3 | Step 1982000 | Avg Loss: 0.0155 | Grad Norm: 0.00791015\n",
      "Epoch 3 | Step 1982100 | Avg Loss: 0.0156 | Grad Norm: 0.00843612\n",
      "Epoch 3 | Step 1982200 | Avg Loss: 0.0153 | Grad Norm: 0.00942518\n",
      "Epoch 3 | Step 1982300 | Avg Loss: 0.0148 | Grad Norm: 0.00806114\n",
      "Epoch 3 | Step 1982400 | Avg Loss: 0.0143 | Grad Norm: 0.00842953\n",
      "Epoch 3 | Step 1982500 | Avg Loss: 0.0142 | Grad Norm: 0.00788813\n",
      "Epoch 3 | Step 1982600 | Avg Loss: 0.0144 | Grad Norm: 0.00761745\n",
      "Epoch 3 | Step 1982700 | Avg Loss: 0.0147 | Grad Norm: 0.00878773\n",
      "Epoch 3 | Step 1982800 | Avg Loss: 0.0148 | Grad Norm: 0.00867481\n",
      "Epoch 3 | Step 1982900 | Avg Loss: 0.0150 | Grad Norm: 0.01055954\n",
      "Epoch 3 | Step 1983000 | Avg Loss: 0.0152 | Grad Norm: 0.00794512\n",
      "Epoch 3 | Step 1983100 | Avg Loss: 0.0152 | Grad Norm: 0.00687501\n",
      "Epoch 3 | Step 1983200 | Avg Loss: 0.0153 | Grad Norm: 0.00712745\n",
      "Epoch 3 | Step 1983300 | Avg Loss: 0.0153 | Grad Norm: 0.00742133\n",
      "Epoch 3 | Step 1983400 | Avg Loss: 0.0153 | Grad Norm: 0.00681843\n",
      "Epoch 3 | Step 1983500 | Avg Loss: 0.0153 | Grad Norm: 0.00718025\n",
      "Epoch 3 | Step 1983600 | Avg Loss: 0.0154 | Grad Norm: 0.00879521\n",
      "Epoch 3 | Step 1983700 | Avg Loss: 0.0152 | Grad Norm: 0.00760286\n",
      "Epoch 3 | Step 1983800 | Avg Loss: 0.0151 | Grad Norm: 0.00748614\n",
      "Epoch 3 | Step 1983900 | Avg Loss: 0.0149 | Grad Norm: 0.00708508\n",
      "Epoch 3 | Step 1984000 | Avg Loss: 0.0151 | Grad Norm: 0.00829474\n",
      "Epoch 3 | Step 1984100 | Avg Loss: 0.0151 | Grad Norm: 0.00765109\n",
      "Epoch 3 | Step 1984200 | Avg Loss: 0.0152 | Grad Norm: 0.00788193\n",
      "Epoch 3 | Step 1984300 | Avg Loss: 0.0153 | Grad Norm: 0.00774865\n",
      "Epoch 3 | Step 1984400 | Avg Loss: 0.0156 | Grad Norm: 0.00823805\n",
      "Epoch 3 | Step 1984500 | Avg Loss: 0.0152 | Grad Norm: 0.00873140\n",
      "Epoch 3 | Step 1984600 | Avg Loss: 0.0148 | Grad Norm: 0.00916554\n",
      "Epoch 3 | Step 1984700 | Avg Loss: 0.0148 | Grad Norm: 0.00852719\n",
      "Epoch 3 | Step 1984800 | Avg Loss: 0.0147 | Grad Norm: 0.00634499\n",
      "Epoch 3 | Step 1984900 | Avg Loss: 0.0150 | Grad Norm: 0.00840912\n",
      "Epoch 3 | Step 1985000 | Avg Loss: 0.0151 | Grad Norm: 0.00851745\n",
      "Epoch 3 | Step 1985100 | Avg Loss: 0.0152 | Grad Norm: 0.00758182\n",
      "Epoch 3 | Step 1985200 | Avg Loss: 0.0149 | Grad Norm: 0.00735391\n",
      "Epoch 3 | Step 1985300 | Avg Loss: 0.0151 | Grad Norm: 0.00740890\n",
      "Epoch 3 | Step 1985400 | Avg Loss: 0.0153 | Grad Norm: 0.00805783\n",
      "Epoch 3 | Step 1985500 | Avg Loss: 0.0153 | Grad Norm: 0.00756114\n",
      "Epoch 3 | Step 1985600 | Avg Loss: 0.0150 | Grad Norm: 0.00908318\n",
      "Epoch 3 | Step 1985700 | Avg Loss: 0.0153 | Grad Norm: 0.00773562\n",
      "Epoch 3 | Step 1985800 | Avg Loss: 0.0153 | Grad Norm: 0.00783991\n",
      "Epoch 3 | Step 1985900 | Avg Loss: 0.0152 | Grad Norm: 0.00990203\n",
      "Epoch 3 | Step 1986000 | Avg Loss: 0.0151 | Grad Norm: 0.01843781\n",
      "Epoch 3 | Step 1986100 | Avg Loss: 0.0150 | Grad Norm: 0.00832731\n",
      "Epoch 3 | Step 1986200 | Avg Loss: 0.0152 | Grad Norm: 0.00719195\n",
      "Epoch 3 | Step 1986300 | Avg Loss: 0.0149 | Grad Norm: 0.00927342\n",
      "Epoch 3 | Step 1986400 | Avg Loss: 0.0145 | Grad Norm: 0.00855375\n",
      "Epoch 3 | Step 1986500 | Avg Loss: 0.0148 | Grad Norm: 0.00832033\n",
      "Epoch 3 | Step 1986600 | Avg Loss: 0.0149 | Grad Norm: 0.00814608\n",
      "Epoch 3 | Step 1986700 | Avg Loss: 0.0149 | Grad Norm: 0.00773663\n",
      "Epoch 3 | Step 1986800 | Avg Loss: 0.0150 | Grad Norm: 0.00798848\n",
      "Epoch 3 | Step 1986900 | Avg Loss: 0.0150 | Grad Norm: 0.00765748\n",
      "Epoch 3 | Step 1987000 | Avg Loss: 0.0148 | Grad Norm: 0.00850565\n",
      "Epoch 3 | Step 1987100 | Avg Loss: 0.0148 | Grad Norm: 0.00955959\n",
      "Epoch 3 | Step 1987200 | Avg Loss: 0.0153 | Grad Norm: 0.00796428\n",
      "Epoch 3 | Step 1987300 | Avg Loss: 0.0153 | Grad Norm: 0.00766824\n",
      "Epoch 3 | Step 1987400 | Avg Loss: 0.0154 | Grad Norm: 0.00739195\n",
      "Epoch 3 | Step 1987500 | Avg Loss: 0.0151 | Grad Norm: 0.00716091\n",
      "Epoch 3 | Step 1987600 | Avg Loss: 0.0156 | Grad Norm: 0.00843314\n",
      "Epoch 3 | Step 1987700 | Avg Loss: 0.0155 | Grad Norm: 0.00717430\n",
      "Epoch 3 | Step 1987800 | Avg Loss: 0.0152 | Grad Norm: 0.00741868\n",
      "Epoch 3 | Step 1987900 | Avg Loss: 0.0153 | Grad Norm: 0.00732489\n",
      "Epoch 3 | Step 1988000 | Avg Loss: 0.0151 | Grad Norm: 0.00761569\n",
      "Epoch 3 | Step 1988100 | Avg Loss: 0.0153 | Grad Norm: 0.00885996\n",
      "Epoch 3 | Step 1988200 | Avg Loss: 0.0150 | Grad Norm: 0.00645194\n",
      "Epoch 3 | Step 1988300 | Avg Loss: 0.0152 | Grad Norm: 0.00893188\n",
      "Epoch 3 | Step 1988400 | Avg Loss: 0.0153 | Grad Norm: 0.00696098\n",
      "Epoch 3 | Step 1988500 | Avg Loss: 0.0150 | Grad Norm: 0.00773537\n",
      "Epoch 3 | Step 1988600 | Avg Loss: 0.0150 | Grad Norm: 0.00840614\n",
      "Epoch 3 | Step 1988700 | Avg Loss: 0.0147 | Grad Norm: 0.00816583\n",
      "Epoch 3 | Step 1988800 | Avg Loss: 0.0149 | Grad Norm: 0.00731328\n",
      "Epoch 3 | Step 1988900 | Avg Loss: 0.0150 | Grad Norm: 0.00787783\n",
      "Epoch 3 | Step 1989000 | Avg Loss: 0.0146 | Grad Norm: 0.00764455\n",
      "Epoch 3 | Step 1989100 | Avg Loss: 0.0146 | Grad Norm: 0.00683899\n",
      "Epoch 3 | Step 1989200 | Avg Loss: 0.0147 | Grad Norm: 0.00819873\n",
      "Epoch 3 | Step 1989300 | Avg Loss: 0.0150 | Grad Norm: 0.00917828\n",
      "Epoch 3 | Step 1989400 | Avg Loss: 0.0152 | Grad Norm: 0.00797806\n",
      "Epoch 3 | Step 1989500 | Avg Loss: 0.0151 | Grad Norm: 0.00964843\n",
      "Epoch 3 | Step 1989600 | Avg Loss: 0.0149 | Grad Norm: 0.00911326\n",
      "Epoch 3 | Step 1989700 | Avg Loss: 0.0149 | Grad Norm: 0.00842899\n",
      "Epoch 3 | Step 1989800 | Avg Loss: 0.0152 | Grad Norm: 0.00817428\n",
      "Epoch 3 | Step 1989900 | Avg Loss: 0.0152 | Grad Norm: 0.00736806\n",
      "Epoch 3 | Step 1990000 | Avg Loss: 0.0153 | Grad Norm: 0.00875051\n",
      "Epoch 3 | Step 1990100 | Avg Loss: 0.0153 | Grad Norm: 0.00779172\n",
      "Epoch 3 | Step 1990200 | Avg Loss: 0.0153 | Grad Norm: 0.00812414\n",
      "Epoch 3 | Step 1990300 | Avg Loss: 0.0151 | Grad Norm: 0.01003596\n",
      "Epoch 3 | Step 1990400 | Avg Loss: 0.0152 | Grad Norm: 0.00787708\n",
      "Epoch 3 | Step 1990500 | Avg Loss: 0.0152 | Grad Norm: 0.00826772\n",
      "Epoch 3 | Step 1990600 | Avg Loss: 0.0151 | Grad Norm: 0.00684900\n",
      "Epoch 3 | Step 1990700 | Avg Loss: 0.0152 | Grad Norm: 0.00945154\n",
      "Epoch 3 | Step 1990800 | Avg Loss: 0.0153 | Grad Norm: 0.00942582\n",
      "Epoch 3 | Step 1990900 | Avg Loss: 0.0154 | Grad Norm: 0.00978490\n",
      "Epoch 3 | Step 1991000 | Avg Loss: 0.0150 | Grad Norm: 0.00891367\n",
      "Epoch 3 | Step 1991100 | Avg Loss: 0.0148 | Grad Norm: 0.00880650\n",
      "Epoch 3 | Step 1991200 | Avg Loss: 0.0148 | Grad Norm: 0.00869878\n",
      "Epoch 3 | Step 1991300 | Avg Loss: 0.0152 | Grad Norm: 0.00926591\n",
      "Epoch 3 | Step 1991400 | Avg Loss: 0.0150 | Grad Norm: 0.00757977\n",
      "Epoch 3 | Step 1991500 | Avg Loss: 0.0155 | Grad Norm: 0.00831533\n",
      "Epoch 3 | Step 1991600 | Avg Loss: 0.0152 | Grad Norm: 0.00825365\n",
      "Epoch 3 | Step 1991700 | Avg Loss: 0.0151 | Grad Norm: 0.00852265\n",
      "Epoch 3 | Step 1991800 | Avg Loss: 0.0151 | Grad Norm: 0.00747696\n",
      "Epoch 3 | Step 1991900 | Avg Loss: 0.0152 | Grad Norm: 0.00990580\n",
      "Epoch 3 | Step 1992000 | Avg Loss: 0.0153 | Grad Norm: 0.00819520\n",
      "Epoch 3 | Step 1992100 | Avg Loss: 0.0155 | Grad Norm: 0.00775668\n",
      "Epoch 3 | Step 1992200 | Avg Loss: 0.0154 | Grad Norm: 0.00763185\n",
      "Epoch 3 | Step 1992300 | Avg Loss: 0.0153 | Grad Norm: 0.00811492\n",
      "Epoch 3 | Step 1992400 | Avg Loss: 0.0154 | Grad Norm: 0.00885396\n",
      "Epoch 3 | Step 1992500 | Avg Loss: 0.0156 | Grad Norm: 0.00799154\n",
      "Epoch 3 | Step 1992600 | Avg Loss: 0.0155 | Grad Norm: 0.00762670\n",
      "Epoch 3 | Step 1992700 | Avg Loss: 0.0149 | Grad Norm: 0.00865367\n",
      "Epoch 3 | Step 1992800 | Avg Loss: 0.0150 | Grad Norm: 0.00772448\n",
      "Epoch 3 | Step 1992900 | Avg Loss: 0.0148 | Grad Norm: 0.01038150\n",
      "Epoch 3 | Step 1993000 | Avg Loss: 0.0148 | Grad Norm: 0.00783957\n",
      "Epoch 3 | Step 1993100 | Avg Loss: 0.0149 | Grad Norm: 0.00839191\n",
      "Epoch 3 | Step 1993200 | Avg Loss: 0.0149 | Grad Norm: 0.00726115\n",
      "Epoch 3 | Step 1993300 | Avg Loss: 0.0150 | Grad Norm: 0.00766595\n",
      "Epoch 3 | Step 1993400 | Avg Loss: 0.0148 | Grad Norm: 0.00761058\n",
      "Epoch 3 | Step 1993500 | Avg Loss: 0.0149 | Grad Norm: 0.00693262\n",
      "Epoch 3 | Step 1993600 | Avg Loss: 0.0152 | Grad Norm: 0.00950325\n",
      "Epoch 3 | Step 1993700 | Avg Loss: 0.0154 | Grad Norm: 0.00775161\n",
      "Epoch 3 | Step 1993800 | Avg Loss: 0.0154 | Grad Norm: 0.00781854\n",
      "Epoch 3 | Step 1993900 | Avg Loss: 0.0152 | Grad Norm: 0.01102713\n",
      "Epoch 3 | Step 1994000 | Avg Loss: 0.0151 | Grad Norm: 0.00910841\n",
      "Epoch 3 | Step 1994100 | Avg Loss: 0.0151 | Grad Norm: 0.00890423\n",
      "Epoch 3 | Step 1994200 | Avg Loss: 0.0150 | Grad Norm: 0.00916754\n",
      "Epoch 3 | Step 1994300 | Avg Loss: 0.0151 | Grad Norm: 0.00783008\n",
      "Epoch 3 | Step 1994400 | Avg Loss: 0.0154 | Grad Norm: 0.00794609\n",
      "Epoch 3 | Step 1994500 | Avg Loss: 0.0154 | Grad Norm: 0.01182165\n",
      "Epoch 3 | Step 1994600 | Avg Loss: 0.0151 | Grad Norm: 0.00949698\n",
      "Epoch 3 | Step 1994700 | Avg Loss: 0.0151 | Grad Norm: 0.00809225\n",
      "Epoch 3 | Step 1994800 | Avg Loss: 0.0154 | Grad Norm: 0.00785804\n",
      "Epoch 3 | Step 1994900 | Avg Loss: 0.0153 | Grad Norm: 0.00874434\n",
      "Epoch 3 | Step 1995000 | Avg Loss: 0.0151 | Grad Norm: 0.00918331\n",
      "Epoch 3 | Step 1995100 | Avg Loss: 0.0156 | Grad Norm: 0.00916894\n",
      "Epoch 3 | Step 1995200 | Avg Loss: 0.0159 | Grad Norm: 0.00969455\n",
      "Epoch 3 | Step 1995300 | Avg Loss: 0.0157 | Grad Norm: 0.01084317\n",
      "Epoch 3 | Step 1995400 | Avg Loss: 0.0158 | Grad Norm: 0.01118088\n",
      "Epoch 3 | Step 1995500 | Avg Loss: 0.0156 | Grad Norm: 0.00838931\n",
      "Epoch 3 | Step 1995600 | Avg Loss: 0.0152 | Grad Norm: 0.00786290\n",
      "Epoch 3 | Step 1995700 | Avg Loss: 0.0154 | Grad Norm: 0.00824602\n",
      "Epoch 3 | Step 1995800 | Avg Loss: 0.0154 | Grad Norm: 0.01122550\n",
      "Epoch 3 | Step 1995900 | Avg Loss: 0.0155 | Grad Norm: 0.00820116\n",
      "Epoch 3 | Step 1996000 | Avg Loss: 0.0151 | Grad Norm: 0.00819302\n",
      "Epoch 3 | Step 1996100 | Avg Loss: 0.0152 | Grad Norm: 0.00773500\n",
      "Epoch 3 | Step 1996200 | Avg Loss: 0.0151 | Grad Norm: 0.00750794\n",
      "Epoch 3 | Step 1996300 | Avg Loss: 0.0149 | Grad Norm: 0.00757668\n",
      "Epoch 3 | Step 1996400 | Avg Loss: 0.0149 | Grad Norm: 0.00866298\n",
      "Epoch 3 | Step 1996500 | Avg Loss: 0.0152 | Grad Norm: 0.00845699\n",
      "Epoch 3 | Step 1996600 | Avg Loss: 0.0152 | Grad Norm: 0.00849439\n",
      "Epoch 3 | Step 1996700 | Avg Loss: 0.0152 | Grad Norm: 0.00917849\n",
      "Epoch 3 | Step 1996800 | Avg Loss: 0.0150 | Grad Norm: 0.00963014\n",
      "Epoch 3 | Step 1996900 | Avg Loss: 0.0154 | Grad Norm: 0.00806136\n",
      "Epoch 3 | Step 1997000 | Avg Loss: 0.0153 | Grad Norm: 0.00793967\n",
      "Epoch 3 | Step 1997100 | Avg Loss: 0.0151 | Grad Norm: 0.00709582\n",
      "Epoch 3 | Step 1997200 | Avg Loss: 0.0150 | Grad Norm: 0.00835384\n",
      "Epoch 3 | Step 1997300 | Avg Loss: 0.0148 | Grad Norm: 0.00839899\n",
      "Epoch 3 | Step 1997400 | Avg Loss: 0.0153 | Grad Norm: 0.00923232\n",
      "Epoch 3 | Step 1997500 | Avg Loss: 0.0150 | Grad Norm: 0.00849646\n",
      "Epoch 3 | Step 1997600 | Avg Loss: 0.0150 | Grad Norm: 0.00821271\n",
      "Epoch 3 | Step 1997700 | Avg Loss: 0.0148 | Grad Norm: 0.00812879\n",
      "Epoch 3 | Step 1997800 | Avg Loss: 0.0147 | Grad Norm: 0.00792453\n",
      "Epoch 3 | Step 1997900 | Avg Loss: 0.0151 | Grad Norm: 0.00799156\n",
      "Epoch 3 | Step 1998000 | Avg Loss: 0.0149 | Grad Norm: 0.00719008\n",
      "Epoch 3 | Step 1998100 | Avg Loss: 0.0151 | Grad Norm: 0.00804794\n",
      "Epoch 3 | Step 1998200 | Avg Loss: 0.0151 | Grad Norm: 0.00806601\n",
      "Epoch 3 | Step 1998300 | Avg Loss: 0.0152 | Grad Norm: 0.00988054\n",
      "Epoch 3 | Step 1998400 | Avg Loss: 0.0150 | Grad Norm: 0.00907540\n",
      "Epoch 3 | Step 1998500 | Avg Loss: 0.0153 | Grad Norm: 0.00790668\n",
      "Epoch 3 | Step 1998600 | Avg Loss: 0.0151 | Grad Norm: 0.00918537\n",
      "Epoch 3 | Step 1998700 | Avg Loss: 0.0151 | Grad Norm: 0.00814442\n",
      "Epoch 3 | Step 1998800 | Avg Loss: 0.0152 | Grad Norm: 0.00605788\n",
      "Epoch 3 | Step 1998900 | Avg Loss: 0.0155 | Grad Norm: 0.00725859\n",
      "Epoch 3 | Step 1999000 | Avg Loss: 0.0155 | Grad Norm: 0.00742384\n",
      "Epoch 3 | Step 1999100 | Avg Loss: 0.0153 | Grad Norm: 0.00760147\n",
      "Epoch 3 | Step 1999200 | Avg Loss: 0.0152 | Grad Norm: 0.00771757\n",
      "Epoch 3 | Step 1999300 | Avg Loss: 0.0155 | Grad Norm: 0.00852095\n",
      "Epoch 3 | Step 1999400 | Avg Loss: 0.0152 | Grad Norm: 0.00695607\n",
      "Epoch 3 | Step 1999500 | Avg Loss: 0.0149 | Grad Norm: 0.00852978\n",
      "Epoch 3 | Step 1999600 | Avg Loss: 0.0145 | Grad Norm: 0.00772831\n",
      "Epoch 3 | Step 1999700 | Avg Loss: 0.0149 | Grad Norm: 0.00756933\n",
      "Epoch 3 | Step 1999800 | Avg Loss: 0.0153 | Grad Norm: 0.00808717\n",
      "Epoch 3 | Step 1999900 | Avg Loss: 0.0155 | Grad Norm: 0.00915958\n",
      "Epoch 3 | Step 2000000 | Avg Loss: 0.0158 | Grad Norm: 0.00864025\n",
      "Saving model at step2000000\n",
      "Epoch 3 | Step 2000100 | Avg Loss: 0.0152 | Grad Norm: 0.00668413\n",
      "Epoch 3 | Step 2000200 | Avg Loss: 0.0154 | Grad Norm: 0.00688539\n",
      "Epoch 3 | Step 2000300 | Avg Loss: 0.0152 | Grad Norm: 0.00741573\n",
      "Epoch 3 | Step 2000400 | Avg Loss: 0.0150 | Grad Norm: 0.00845739\n",
      "Epoch 3 | Step 2000500 | Avg Loss: 0.0147 | Grad Norm: 0.00684578\n",
      "Epoch 3 | Step 2000600 | Avg Loss: 0.0147 | Grad Norm: 0.00852922\n",
      "Epoch 3 | Step 2000700 | Avg Loss: 0.0151 | Grad Norm: 0.01174240\n",
      "Epoch 3 | Step 2000800 | Avg Loss: 0.0152 | Grad Norm: 0.00875291\n",
      "Epoch 3 | Step 2000900 | Avg Loss: 0.0152 | Grad Norm: 0.00794738\n",
      "Epoch 3 | Step 2001000 | Avg Loss: 0.0146 | Grad Norm: 0.00799803\n",
      "Epoch 3 | Step 2001100 | Avg Loss: 0.0147 | Grad Norm: 0.01036129\n",
      "Epoch 3 | Step 2001200 | Avg Loss: 0.0143 | Grad Norm: 0.00850330\n",
      "Epoch 3 | Step 2001300 | Avg Loss: 0.0144 | Grad Norm: 0.00762607\n",
      "Epoch 3 | Step 2001400 | Avg Loss: 0.0142 | Grad Norm: 0.00768308\n",
      "Epoch 3 | Step 2001500 | Avg Loss: 0.0142 | Grad Norm: 0.00951307\n",
      "Epoch 3 | Step 2001600 | Avg Loss: 0.0146 | Grad Norm: 0.00756961\n",
      "Epoch 3 | Step 2001700 | Avg Loss: 0.0146 | Grad Norm: 0.00786775\n",
      "Epoch 3 | Step 2001800 | Avg Loss: 0.0148 | Grad Norm: 0.00856947\n",
      "Epoch 3 | Step 2001900 | Avg Loss: 0.0151 | Grad Norm: 0.01022701\n",
      "Epoch 3 | Step 2002000 | Avg Loss: 0.0151 | Grad Norm: 0.00770372\n",
      "Epoch 3 | Step 2002100 | Avg Loss: 0.0150 | Grad Norm: 0.00830607\n",
      "Epoch 3 | Step 2002200 | Avg Loss: 0.0157 | Grad Norm: 0.00951507\n",
      "Epoch 3 | Step 2002300 | Avg Loss: 0.0157 | Grad Norm: 0.00929417\n",
      "Epoch 3 | Step 2002400 | Avg Loss: 0.0156 | Grad Norm: 0.00875937\n",
      "Epoch 3 | Step 2002500 | Avg Loss: 0.0158 | Grad Norm: 0.00821805\n",
      "Epoch 3 | Step 2002600 | Avg Loss: 0.0156 | Grad Norm: 0.00916115\n",
      "Epoch 3 | Step 2002700 | Avg Loss: 0.0153 | Grad Norm: 0.00831250\n",
      "Epoch 3 | Step 2002800 | Avg Loss: 0.0148 | Grad Norm: 0.00902166\n",
      "Epoch 3 | Step 2002900 | Avg Loss: 0.0144 | Grad Norm: 0.00784719\n",
      "Epoch 3 | Step 2003000 | Avg Loss: 0.0145 | Grad Norm: 0.00688357\n",
      "Epoch 3 | Step 2003100 | Avg Loss: 0.0145 | Grad Norm: 0.00971065\n",
      "Epoch 3 | Step 2003200 | Avg Loss: 0.0147 | Grad Norm: 0.00842411\n",
      "Epoch 3 | Step 2003300 | Avg Loss: 0.0142 | Grad Norm: 0.00653428\n",
      "Epoch 3 | Step 2003400 | Avg Loss: 0.0143 | Grad Norm: 0.00735436\n",
      "Epoch 3 | Step 2003500 | Avg Loss: 0.0145 | Grad Norm: 0.00784524\n",
      "Epoch 3 | Step 2003600 | Avg Loss: 0.0149 | Grad Norm: 0.00915021\n",
      "Epoch 3 | Step 2003700 | Avg Loss: 0.0153 | Grad Norm: 0.00731321\n",
      "Epoch 3 | Step 2003800 | Avg Loss: 0.0151 | Grad Norm: 0.00626380\n",
      "Epoch 3 | Step 2003900 | Avg Loss: 0.0155 | Grad Norm: 0.00956144\n",
      "Epoch 3 | Step 2004000 | Avg Loss: 0.0158 | Grad Norm: 0.01218013\n",
      "Epoch 3 | Step 2004100 | Avg Loss: 0.0155 | Grad Norm: 0.01046250\n",
      "Epoch 3 | Step 2004200 | Avg Loss: 0.0153 | Grad Norm: 0.00742712\n",
      "Epoch 3 | Step 2004300 | Avg Loss: 0.0153 | Grad Norm: 0.00874327\n",
      "Epoch 3 | Step 2004400 | Avg Loss: 0.0152 | Grad Norm: 0.00753218\n",
      "Epoch 3 | Step 2004500 | Avg Loss: 0.0149 | Grad Norm: 0.00798574\n",
      "Epoch 3 | Step 2004600 | Avg Loss: 0.0148 | Grad Norm: 0.00898007\n",
      "Epoch 3 | Step 2004700 | Avg Loss: 0.0143 | Grad Norm: 0.00661012\n",
      "Epoch 3 | Step 2004800 | Avg Loss: 0.0150 | Grad Norm: 0.00729643\n",
      "Epoch 3 | Step 2004900 | Avg Loss: 0.0148 | Grad Norm: 0.00743950\n",
      "Epoch 3 | Step 2005000 | Avg Loss: 0.0150 | Grad Norm: 0.00836000\n",
      "Epoch 3 | Step 2005100 | Avg Loss: 0.0152 | Grad Norm: 0.00821502\n",
      "Epoch 3 | Step 2005200 | Avg Loss: 0.0152 | Grad Norm: 0.01059607\n",
      "Epoch 3 | Step 2005300 | Avg Loss: 0.0149 | Grad Norm: 0.00844222\n",
      "Epoch 3 | Step 2005400 | Avg Loss: 0.0151 | Grad Norm: 0.00772144\n",
      "Epoch 3 | Step 2005500 | Avg Loss: 0.0146 | Grad Norm: 0.00790213\n",
      "Epoch 3 | Step 2005600 | Avg Loss: 0.0146 | Grad Norm: 0.00825499\n",
      "Epoch 3 | Step 2005700 | Avg Loss: 0.0152 | Grad Norm: 0.00834926\n",
      "Epoch 3 | Step 2005800 | Avg Loss: 0.0155 | Grad Norm: 0.00809733\n",
      "Epoch 3 | Step 2005900 | Avg Loss: 0.0157 | Grad Norm: 0.00646026\n",
      "Epoch 3 | Step 2006000 | Avg Loss: 0.0152 | Grad Norm: 0.01103637\n",
      "Epoch 3 | Step 2006100 | Avg Loss: 0.0154 | Grad Norm: 0.00890874\n",
      "Epoch 3 | Step 2006200 | Avg Loss: 0.0153 | Grad Norm: 0.00757605\n",
      "Epoch 3 | Step 2006300 | Avg Loss: 0.0153 | Grad Norm: 0.00796800\n",
      "Epoch 3 | Step 2006400 | Avg Loss: 0.0156 | Grad Norm: 0.00748079\n",
      "Epoch 3 | Step 2006500 | Avg Loss: 0.0156 | Grad Norm: 0.00785300\n",
      "Epoch 3 | Step 2006600 | Avg Loss: 0.0157 | Grad Norm: 0.00927844\n",
      "Epoch 3 | Step 2006700 | Avg Loss: 0.0154 | Grad Norm: 0.00766999\n",
      "Epoch 3 | Step 2006800 | Avg Loss: 0.0156 | Grad Norm: 0.00706473\n",
      "Epoch 3 | Step 2006900 | Avg Loss: 0.0157 | Grad Norm: 0.00682177\n",
      "Epoch 3 | Step 2007000 | Avg Loss: 0.0156 | Grad Norm: 0.00765851\n",
      "Epoch 3 | Step 2007100 | Avg Loss: 0.0152 | Grad Norm: 0.00820316\n",
      "Epoch 3 | Step 2007200 | Avg Loss: 0.0152 | Grad Norm: 0.00934143\n",
      "Epoch 3 | Step 2007300 | Avg Loss: 0.0156 | Grad Norm: 0.00907278\n",
      "Epoch 3 | Step 2007400 | Avg Loss: 0.0158 | Grad Norm: 0.00822527\n",
      "Epoch 3 | Step 2007500 | Avg Loss: 0.0156 | Grad Norm: 0.00789304\n",
      "Epoch 3 | Step 2007600 | Avg Loss: 0.0151 | Grad Norm: 0.00802075\n",
      "Epoch 3 | Step 2007700 | Avg Loss: 0.0152 | Grad Norm: 0.00739953\n",
      "Epoch 3 | Step 2007800 | Avg Loss: 0.0152 | Grad Norm: 0.00824334\n",
      "Epoch 3 | Step 2007900 | Avg Loss: 0.0155 | Grad Norm: 0.00791550\n",
      "Epoch 3 | Step 2008000 | Avg Loss: 0.0158 | Grad Norm: 0.00894884\n",
      "Epoch 3 | Step 2008100 | Avg Loss: 0.0154 | Grad Norm: 0.00954491\n",
      "Epoch 3 | Step 2008200 | Avg Loss: 0.0154 | Grad Norm: 0.00777506\n",
      "Epoch 3 | Step 2008300 | Avg Loss: 0.0151 | Grad Norm: 0.00822100\n",
      "Epoch 3 | Step 2008400 | Avg Loss: 0.0151 | Grad Norm: 0.00758971\n",
      "Epoch 3 | Step 2008500 | Avg Loss: 0.0153 | Grad Norm: 0.00945526\n",
      "Epoch 3 | Step 2008600 | Avg Loss: 0.0149 | Grad Norm: 0.00869972\n",
      "Epoch 3 | Step 2008700 | Avg Loss: 0.0147 | Grad Norm: 0.00922459\n",
      "Epoch 3 | Step 2008800 | Avg Loss: 0.0149 | Grad Norm: 0.00712535\n",
      "Epoch 3 | Step 2008900 | Avg Loss: 0.0149 | Grad Norm: 0.00842513\n",
      "Epoch 3 | Step 2009000 | Avg Loss: 0.0149 | Grad Norm: 0.00847993\n",
      "Epoch 3 | Step 2009100 | Avg Loss: 0.0151 | Grad Norm: 0.00727893\n",
      "Epoch 3 | Step 2009200 | Avg Loss: 0.0155 | Grad Norm: 0.00800834\n",
      "Epoch 3 | Step 2009300 | Avg Loss: 0.0152 | Grad Norm: 0.00905338\n",
      "Epoch 3 | Step 2009400 | Avg Loss: 0.0152 | Grad Norm: 0.01011786\n",
      "Epoch 3 | Step 2009500 | Avg Loss: 0.0160 | Grad Norm: 0.00757233\n",
      "Epoch 3 | Step 2009600 | Avg Loss: 0.0157 | Grad Norm: 0.00772121\n",
      "Epoch 3 | Step 2009700 | Avg Loss: 0.0153 | Grad Norm: 0.00838653\n",
      "Epoch 3 | Step 2009800 | Avg Loss: 0.0156 | Grad Norm: 0.00903737\n",
      "Epoch 3 | Step 2009900 | Avg Loss: 0.0157 | Grad Norm: 0.00891301\n",
      "Epoch 3 | Step 2010000 | Avg Loss: 0.0151 | Grad Norm: 0.00821310\n",
      "Epoch 3 | Step 2010100 | Avg Loss: 0.0151 | Grad Norm: 0.00708709\n",
      "Epoch 3 | Step 2010200 | Avg Loss: 0.0151 | Grad Norm: 0.00747208\n",
      "Epoch 3 | Step 2010300 | Avg Loss: 0.0155 | Grad Norm: 0.00763999\n",
      "Epoch 3 | Step 2010400 | Avg Loss: 0.0157 | Grad Norm: 0.00794368\n",
      "Epoch 3 | Step 2010500 | Avg Loss: 0.0158 | Grad Norm: 0.00910358\n",
      "Epoch 3 | Step 2010600 | Avg Loss: 0.0158 | Grad Norm: 0.00821673\n",
      "Epoch 3 | Step 2010700 | Avg Loss: 0.0160 | Grad Norm: 0.00890015\n",
      "Epoch 3 | Step 2010800 | Avg Loss: 0.0155 | Grad Norm: 0.00879316\n",
      "Epoch 3 | Step 2010900 | Avg Loss: 0.0151 | Grad Norm: 0.00760835\n",
      "Epoch 3 | Step 2011000 | Avg Loss: 0.0152 | Grad Norm: 0.00868453\n",
      "Epoch 3 | Step 2011100 | Avg Loss: 0.0153 | Grad Norm: 0.00933064\n",
      "Epoch 3 | Step 2011200 | Avg Loss: 0.0149 | Grad Norm: 0.00897463\n",
      "Epoch 3 | Step 2011300 | Avg Loss: 0.0154 | Grad Norm: 0.00826852\n",
      "Epoch 3 | Step 2011400 | Avg Loss: 0.0152 | Grad Norm: 0.00765963\n",
      "Epoch 3 | Step 2011500 | Avg Loss: 0.0151 | Grad Norm: 0.00700828\n",
      "Epoch 3 | Step 2011600 | Avg Loss: 0.0153 | Grad Norm: 0.00808319\n",
      "Epoch 3 | Step 2011700 | Avg Loss: 0.0154 | Grad Norm: 0.00896091\n",
      "Epoch 3 | Step 2011800 | Avg Loss: 0.0154 | Grad Norm: 0.00891991\n",
      "Epoch 3 | Step 2011900 | Avg Loss: 0.0151 | Grad Norm: 0.00729269\n",
      "Epoch 3 | Step 2012000 | Avg Loss: 0.0152 | Grad Norm: 0.01030370\n",
      "Epoch 3 | Step 2012100 | Avg Loss: 0.0153 | Grad Norm: 0.00868952\n",
      "Epoch 3 | Step 2012200 | Avg Loss: 0.0153 | Grad Norm: 0.00971091\n",
      "Epoch 3 | Step 2012300 | Avg Loss: 0.0153 | Grad Norm: 0.00739502\n",
      "Epoch 3 | Step 2012400 | Avg Loss: 0.0151 | Grad Norm: 0.00767986\n",
      "Epoch 3 | Step 2012500 | Avg Loss: 0.0151 | Grad Norm: 0.00842464\n",
      "Epoch 3 | Step 2012600 | Avg Loss: 0.0149 | Grad Norm: 0.00915672\n",
      "Epoch 3 | Step 2012700 | Avg Loss: 0.0151 | Grad Norm: 0.00807379\n",
      "Epoch 3 | Step 2012800 | Avg Loss: 0.0147 | Grad Norm: 0.00696232\n",
      "Epoch 3 | Step 2012900 | Avg Loss: 0.0148 | Grad Norm: 0.00841579\n",
      "Epoch 3 | Step 2013000 | Avg Loss: 0.0149 | Grad Norm: 0.00867173\n",
      "Epoch 3 | Step 2013100 | Avg Loss: 0.0147 | Grad Norm: 0.00768972\n",
      "Epoch 3 | Step 2013200 | Avg Loss: 0.0149 | Grad Norm: 0.00814948\n",
      "Epoch 3 | Step 2013300 | Avg Loss: 0.0148 | Grad Norm: 0.00824834\n",
      "Epoch 3 | Step 2013400 | Avg Loss: 0.0150 | Grad Norm: 0.00765678\n",
      "Epoch 3 | Step 2013500 | Avg Loss: 0.0151 | Grad Norm: 0.00735803\n",
      "Epoch 3 | Step 2013600 | Avg Loss: 0.0151 | Grad Norm: 0.00707387\n",
      "Epoch 3 | Step 2013700 | Avg Loss: 0.0159 | Grad Norm: 0.00992274\n",
      "Epoch 3 | Step 2013800 | Avg Loss: 0.0154 | Grad Norm: 0.00953710\n",
      "Epoch 3 | Step 2013900 | Avg Loss: 0.0153 | Grad Norm: 0.00937978\n",
      "Epoch 3 | Step 2014000 | Avg Loss: 0.0149 | Grad Norm: 0.00679498\n",
      "Epoch 3 | Step 2014100 | Avg Loss: 0.0152 | Grad Norm: 0.00782586\n",
      "Epoch 3 | Step 2014200 | Avg Loss: 0.0152 | Grad Norm: 0.00852316\n",
      "Epoch 3 | Step 2014300 | Avg Loss: 0.0156 | Grad Norm: 0.00814478\n",
      "Epoch 3 | Step 2014400 | Avg Loss: 0.0154 | Grad Norm: 0.00699090\n",
      "Epoch 3 | Step 2014500 | Avg Loss: 0.0154 | Grad Norm: 0.00754564\n",
      "Epoch 3 | Step 2014600 | Avg Loss: 0.0156 | Grad Norm: 0.00820467\n",
      "Epoch 3 | Step 2014700 | Avg Loss: 0.0160 | Grad Norm: 0.00728175\n",
      "Epoch 3 | Step 2014800 | Avg Loss: 0.0158 | Grad Norm: 0.01012603\n",
      "Epoch 3 | Step 2014900 | Avg Loss: 0.0154 | Grad Norm: 0.00754132\n",
      "Epoch 3 | Step 2015000 | Avg Loss: 0.0152 | Grad Norm: 0.00707193\n",
      "Epoch 3 | Step 2015100 | Avg Loss: 0.0154 | Grad Norm: 0.00879668\n",
      "Epoch 3 | Step 2015200 | Avg Loss: 0.0151 | Grad Norm: 0.00841655\n",
      "Epoch 3 | Step 2015300 | Avg Loss: 0.0152 | Grad Norm: 0.00799317\n",
      "Epoch 3 | Step 2015400 | Avg Loss: 0.0150 | Grad Norm: 0.00874825\n",
      "Epoch 3 | Step 2015500 | Avg Loss: 0.0148 | Grad Norm: 0.00801014\n",
      "Epoch 3 | Step 2015600 | Avg Loss: 0.0149 | Grad Norm: 0.00799624\n",
      "Epoch 3 | Step 2015700 | Avg Loss: 0.0152 | Grad Norm: 0.00764992\n",
      "Epoch 3 | Step 2015800 | Avg Loss: 0.0150 | Grad Norm: 0.00683224\n",
      "Epoch 3 | Step 2015900 | Avg Loss: 0.0150 | Grad Norm: 0.01022081\n",
      "Epoch 3 | Step 2016000 | Avg Loss: 0.0150 | Grad Norm: 0.00797747\n",
      "Epoch 3 | Step 2016100 | Avg Loss: 0.0148 | Grad Norm: 0.00999831\n",
      "Epoch 3 | Step 2016200 | Avg Loss: 0.0151 | Grad Norm: 0.00867597\n",
      "Epoch 3 | Step 2016300 | Avg Loss: 0.0151 | Grad Norm: 0.00776524\n",
      "Epoch 3 | Step 2016400 | Avg Loss: 0.0148 | Grad Norm: 0.00814067\n",
      "Epoch 3 | Step 2016500 | Avg Loss: 0.0152 | Grad Norm: 0.00856063\n",
      "Epoch 3 | Step 2016600 | Avg Loss: 0.0150 | Grad Norm: 0.00746330\n",
      "Epoch 3 | Step 2016700 | Avg Loss: 0.0151 | Grad Norm: 0.00837742\n",
      "Epoch 3 | Step 2016800 | Avg Loss: 0.0156 | Grad Norm: 0.00993445\n",
      "Epoch 3 | Step 2016900 | Avg Loss: 0.0158 | Grad Norm: 0.00755289\n",
      "Epoch 3 | Step 2017000 | Avg Loss: 0.0155 | Grad Norm: 0.00803227\n",
      "Epoch 3 | Step 2017100 | Avg Loss: 0.0156 | Grad Norm: 0.00719703\n",
      "Epoch 3 | Step 2017200 | Avg Loss: 0.0159 | Grad Norm: 0.00785480\n",
      "Epoch 3 | Step 2017300 | Avg Loss: 0.0158 | Grad Norm: 0.00751163\n",
      "Epoch 3 | Step 2017400 | Avg Loss: 0.0157 | Grad Norm: 0.00783967\n",
      "Epoch 3 | Step 2017500 | Avg Loss: 0.0158 | Grad Norm: 0.00835870\n",
      "Epoch 3 | Step 2017600 | Avg Loss: 0.0160 | Grad Norm: 0.00829901\n",
      "Epoch 3 | Step 2017700 | Avg Loss: 0.0157 | Grad Norm: 0.00809038\n",
      "Epoch 3 | Step 2017800 | Avg Loss: 0.0153 | Grad Norm: 0.00875141\n",
      "Epoch 3 | Step 2017900 | Avg Loss: 0.0155 | Grad Norm: 0.00867446\n",
      "Epoch 3 | Step 2018000 | Avg Loss: 0.0154 | Grad Norm: 0.00784932\n",
      "Epoch 3 | Step 2018100 | Avg Loss: 0.0149 | Grad Norm: 0.01270116\n",
      "Epoch 3 | Step 2018200 | Avg Loss: 0.0148 | Grad Norm: 0.00668734\n",
      "Epoch 3 | Step 2018300 | Avg Loss: 0.0148 | Grad Norm: 0.00888480\n",
      "Epoch 3 | Step 2018400 | Avg Loss: 0.0147 | Grad Norm: 0.00761256\n",
      "Epoch 3 | Step 2018500 | Avg Loss: 0.0143 | Grad Norm: 0.01131280\n",
      "Epoch 3 | Step 2018600 | Avg Loss: 0.0144 | Grad Norm: 0.00752231\n",
      "Epoch 3 | Step 2018700 | Avg Loss: 0.0146 | Grad Norm: 0.00850770\n",
      "Epoch 3 | Step 2018800 | Avg Loss: 0.0145 | Grad Norm: 0.00732403\n",
      "Epoch 3 | Step 2018900 | Avg Loss: 0.0145 | Grad Norm: 0.00726305\n",
      "Epoch 3 | Step 2019000 | Avg Loss: 0.0148 | Grad Norm: 0.00902110\n",
      "Epoch 3 | Step 2019100 | Avg Loss: 0.0147 | Grad Norm: 0.00840724\n",
      "Epoch 3 | Step 2019200 | Avg Loss: 0.0148 | Grad Norm: 0.00747508\n",
      "Epoch 3 | Step 2019300 | Avg Loss: 0.0148 | Grad Norm: 0.00745725\n",
      "Epoch 3 | Step 2019400 | Avg Loss: 0.0148 | Grad Norm: 0.00766982\n",
      "Epoch 3 | Step 2019500 | Avg Loss: 0.0147 | Grad Norm: 0.00722716\n",
      "Epoch 3 | Step 2019600 | Avg Loss: 0.0149 | Grad Norm: 0.00935391\n",
      "Epoch 3 | Step 2019700 | Avg Loss: 0.0145 | Grad Norm: 0.00787304\n",
      "Epoch 3 | Step 2019800 | Avg Loss: 0.0142 | Grad Norm: 0.00701066\n",
      "Epoch 3 | Step 2019900 | Avg Loss: 0.0148 | Grad Norm: 0.00795016\n",
      "Epoch 3 | Step 2020000 | Avg Loss: 0.0153 | Grad Norm: 0.00741316\n",
      "Epoch 3 | Step 2020100 | Avg Loss: 0.0154 | Grad Norm: 0.00772018\n",
      "Epoch 3 | Step 2020200 | Avg Loss: 0.0150 | Grad Norm: 0.00854788\n",
      "Epoch 3 | Step 2020300 | Avg Loss: 0.0152 | Grad Norm: 0.00717598\n",
      "Epoch 3 | Step 2020400 | Avg Loss: 0.0147 | Grad Norm: 0.00793168\n",
      "Epoch 3 | Step 2020500 | Avg Loss: 0.0146 | Grad Norm: 0.00718234\n",
      "Epoch 3 | Step 2020600 | Avg Loss: 0.0150 | Grad Norm: 0.00786320\n",
      "Epoch 3 | Step 2020700 | Avg Loss: 0.0152 | Grad Norm: 0.00877569\n",
      "Epoch 3 | Step 2020800 | Avg Loss: 0.0150 | Grad Norm: 0.00852762\n",
      "Epoch 3 | Step 2020900 | Avg Loss: 0.0151 | Grad Norm: 0.00776642\n",
      "Epoch 3 | Step 2021000 | Avg Loss: 0.0151 | Grad Norm: 0.00906755\n",
      "Epoch 3 | Step 2021100 | Avg Loss: 0.0149 | Grad Norm: 0.00750266\n",
      "Epoch 3 | Step 2021200 | Avg Loss: 0.0155 | Grad Norm: 0.00780584\n",
      "Epoch 3 | Step 2021300 | Avg Loss: 0.0154 | Grad Norm: 0.00731846\n",
      "Epoch 3 | Step 2021400 | Avg Loss: 0.0154 | Grad Norm: 0.00810311\n",
      "Epoch 3 | Step 2021500 | Avg Loss: 0.0154 | Grad Norm: 0.00879555\n",
      "Epoch 3 | Step 2021600 | Avg Loss: 0.0151 | Grad Norm: 0.00830843\n",
      "Epoch 3 | Step 2021700 | Avg Loss: 0.0152 | Grad Norm: 0.00795136\n",
      "Epoch 3 | Step 2021800 | Avg Loss: 0.0149 | Grad Norm: 0.01031552\n",
      "Epoch 3 | Step 2021900 | Avg Loss: 0.0147 | Grad Norm: 0.00711477\n",
      "Epoch 3 | Step 2022000 | Avg Loss: 0.0149 | Grad Norm: 0.00747098\n",
      "Epoch 3 | Step 2022100 | Avg Loss: 0.0148 | Grad Norm: 0.00948459\n",
      "Epoch 3 | Step 2022200 | Avg Loss: 0.0148 | Grad Norm: 0.00935784\n",
      "Epoch 3 | Step 2022300 | Avg Loss: 0.0148 | Grad Norm: 0.00782713\n",
      "Epoch 3 | Step 2022400 | Avg Loss: 0.0146 | Grad Norm: 0.00813733\n",
      "Epoch 3 | Step 2022500 | Avg Loss: 0.0149 | Grad Norm: 0.00841961\n",
      "Epoch 3 | Step 2022600 | Avg Loss: 0.0148 | Grad Norm: 0.00756745\n",
      "Epoch 3 | Step 2022700 | Avg Loss: 0.0147 | Grad Norm: 0.00755137\n",
      "Epoch 3 | Step 2022800 | Avg Loss: 0.0147 | Grad Norm: 0.00808487\n",
      "Epoch 3 | Step 2022900 | Avg Loss: 0.0151 | Grad Norm: 0.00788282\n",
      "Epoch 3 | Step 2023000 | Avg Loss: 0.0148 | Grad Norm: 0.00760640\n",
      "Epoch 3 | Step 2023100 | Avg Loss: 0.0145 | Grad Norm: 0.00900959\n",
      "Epoch 3 | Step 2023200 | Avg Loss: 0.0146 | Grad Norm: 0.00779788\n",
      "Epoch 3 | Step 2023300 | Avg Loss: 0.0145 | Grad Norm: 0.00729722\n",
      "Epoch 3 | Step 2023400 | Avg Loss: 0.0145 | Grad Norm: 0.00761183\n",
      "Epoch 3 | Step 2023500 | Avg Loss: 0.0148 | Grad Norm: 0.00777899\n",
      "Epoch 3 | Step 2023600 | Avg Loss: 0.0145 | Grad Norm: 0.00790670\n",
      "Epoch 3 | Step 2023700 | Avg Loss: 0.0147 | Grad Norm: 0.00903475\n",
      "Epoch 3 | Step 2023800 | Avg Loss: 0.0146 | Grad Norm: 0.00887730\n",
      "Epoch 3 | Step 2023900 | Avg Loss: 0.0148 | Grad Norm: 0.00703044\n",
      "Epoch 3 | Step 2024000 | Avg Loss: 0.0147 | Grad Norm: 0.00728353\n",
      "Epoch 3 | Step 2024100 | Avg Loss: 0.0145 | Grad Norm: 0.00760148\n",
      "Epoch 3 | Step 2024200 | Avg Loss: 0.0150 | Grad Norm: 0.00784476\n",
      "Epoch 3 | Step 2024300 | Avg Loss: 0.0147 | Grad Norm: 0.00804470\n",
      "Epoch 3 | Step 2024400 | Avg Loss: 0.0145 | Grad Norm: 0.00666405\n",
      "Epoch 3 | Step 2024500 | Avg Loss: 0.0142 | Grad Norm: 0.00803376\n",
      "Epoch 3 | Step 2024600 | Avg Loss: 0.0146 | Grad Norm: 0.00778903\n",
      "Epoch 3 | Step 2024700 | Avg Loss: 0.0148 | Grad Norm: 0.00801639\n",
      "Epoch 3 | Step 2024800 | Avg Loss: 0.0144 | Grad Norm: 0.00687323\n",
      "Epoch 3 | Step 2024900 | Avg Loss: 0.0144 | Grad Norm: 0.00917920\n",
      "Epoch 3 | Step 2025000 | Avg Loss: 0.0148 | Grad Norm: 0.01085081\n",
      "Epoch 3 | Step 2025100 | Avg Loss: 0.0148 | Grad Norm: 0.00837108\n",
      "Epoch 3 | Step 2025200 | Avg Loss: 0.0148 | Grad Norm: 0.00715738\n",
      "Epoch 3 | Step 2025300 | Avg Loss: 0.0146 | Grad Norm: 0.00717995\n",
      "Epoch 3 | Step 2025400 | Avg Loss: 0.0150 | Grad Norm: 0.00775805\n",
      "Epoch 3 | Step 2025500 | Avg Loss: 0.0153 | Grad Norm: 0.00686308\n",
      "Epoch 3 | Step 2025600 | Avg Loss: 0.0153 | Grad Norm: 0.00977321\n",
      "Epoch 3 | Step 2025700 | Avg Loss: 0.0153 | Grad Norm: 0.00866899\n",
      "Epoch 3 | Step 2025800 | Avg Loss: 0.0155 | Grad Norm: 0.00879623\n",
      "Epoch 3 | Step 2025900 | Avg Loss: 0.0155 | Grad Norm: 0.00777041\n",
      "Epoch 3 | Step 2026000 | Avg Loss: 0.0155 | Grad Norm: 0.00879156\n",
      "Epoch 3 | Step 2026100 | Avg Loss: 0.0150 | Grad Norm: 0.00802858\n",
      "Epoch 3 | Step 2026200 | Avg Loss: 0.0149 | Grad Norm: 0.00796962\n",
      "Epoch 3 | Step 2026300 | Avg Loss: 0.0152 | Grad Norm: 0.00820897\n",
      "Epoch 3 | Step 2026400 | Avg Loss: 0.0146 | Grad Norm: 0.00838634\n",
      "Epoch 3 | Step 2026500 | Avg Loss: 0.0145 | Grad Norm: 0.00750160\n",
      "Epoch 3 | Step 2026600 | Avg Loss: 0.0149 | Grad Norm: 0.00836494\n",
      "Epoch 3 | Step 2026700 | Avg Loss: 0.0152 | Grad Norm: 0.00739316\n",
      "Epoch 3 | Step 2026800 | Avg Loss: 0.0154 | Grad Norm: 0.00844474\n",
      "Epoch 3 | Step 2026900 | Avg Loss: 0.0154 | Grad Norm: 0.00760934\n",
      "Epoch 3 | Step 2027000 | Avg Loss: 0.0151 | Grad Norm: 0.00973766\n",
      "Epoch 3 | Step 2027100 | Avg Loss: 0.0150 | Grad Norm: 0.00772164\n",
      "Epoch 3 | Step 2027200 | Avg Loss: 0.0151 | Grad Norm: 0.00748027\n",
      "Epoch 3 | Step 2027300 | Avg Loss: 0.0152 | Grad Norm: 0.00761388\n",
      "Epoch 3 | Step 2027400 | Avg Loss: 0.0152 | Grad Norm: 0.00826701\n",
      "Epoch 3 | Step 2027500 | Avg Loss: 0.0155 | Grad Norm: 0.00723350\n",
      "Epoch 3 | Step 2027600 | Avg Loss: 0.0156 | Grad Norm: 0.00839605\n",
      "Epoch 3 | Step 2027700 | Avg Loss: 0.0153 | Grad Norm: 0.00824618\n",
      "Epoch 3 | Step 2027800 | Avg Loss: 0.0153 | Grad Norm: 0.00871405\n",
      "Epoch 3 | Step 2027900 | Avg Loss: 0.0151 | Grad Norm: 0.00711909\n",
      "Epoch 3 | Step 2028000 | Avg Loss: 0.0154 | Grad Norm: 0.00812634\n",
      "Epoch 3 | Step 2028100 | Avg Loss: 0.0153 | Grad Norm: 0.00689436\n",
      "Epoch 3 | Step 2028200 | Avg Loss: 0.0154 | Grad Norm: 0.00841759\n",
      "Epoch 3 | Step 2028300 | Avg Loss: 0.0154 | Grad Norm: 0.00859337\n",
      "Epoch 3 | Step 2028400 | Avg Loss: 0.0155 | Grad Norm: 0.00813238\n",
      "Epoch 3 | Step 2028500 | Avg Loss: 0.0155 | Grad Norm: 0.00817960\n",
      "Epoch 3 | Step 2028600 | Avg Loss: 0.0155 | Grad Norm: 0.00766556\n",
      "Epoch 3 | Step 2028700 | Avg Loss: 0.0157 | Grad Norm: 0.00933560\n",
      "Epoch 3 | Step 2028800 | Avg Loss: 0.0154 | Grad Norm: 0.00665104\n",
      "Epoch 3 | Step 2028900 | Avg Loss: 0.0155 | Grad Norm: 0.00879011\n",
      "Epoch 3 | Step 2029000 | Avg Loss: 0.0154 | Grad Norm: 0.00889629\n",
      "Epoch 3 | Step 2029100 | Avg Loss: 0.0149 | Grad Norm: 0.00691798\n",
      "Epoch 3 | Step 2029200 | Avg Loss: 0.0152 | Grad Norm: 0.00805102\n",
      "Epoch 3 | Step 2029300 | Avg Loss: 0.0149 | Grad Norm: 0.00771942\n",
      "Epoch 3 | Step 2029400 | Avg Loss: 0.0151 | Grad Norm: 0.00804553\n",
      "Epoch 3 | Step 2029500 | Avg Loss: 0.0151 | Grad Norm: 0.00893394\n",
      "Epoch 3 | Step 2029600 | Avg Loss: 0.0150 | Grad Norm: 0.00683468\n",
      "Epoch 3 | Step 2029700 | Avg Loss: 0.0153 | Grad Norm: 0.00740992\n",
      "Epoch 3 | Step 2029800 | Avg Loss: 0.0154 | Grad Norm: 0.00832093\n",
      "Epoch 3 | Step 2029900 | Avg Loss: 0.0151 | Grad Norm: 0.00703369\n",
      "Epoch 3 | Step 2030000 | Avg Loss: 0.0150 | Grad Norm: 0.00718365\n",
      "Epoch 3 | Step 2030100 | Avg Loss: 0.0150 | Grad Norm: 0.00974319\n",
      "Epoch 3 | Step 2030200 | Avg Loss: 0.0152 | Grad Norm: 0.00807933\n",
      "Epoch 3 | Step 2030300 | Avg Loss: 0.0153 | Grad Norm: 0.00899257\n",
      "Epoch 3 | Step 2030400 | Avg Loss: 0.0153 | Grad Norm: 0.00982110\n",
      "Epoch 3 | Step 2030500 | Avg Loss: 0.0149 | Grad Norm: 0.00762564\n",
      "Epoch 3 | Step 2030600 | Avg Loss: 0.0148 | Grad Norm: 0.00758309\n",
      "Epoch 3 | Step 2030700 | Avg Loss: 0.0151 | Grad Norm: 0.00764118\n",
      "Epoch 3 | Step 2030800 | Avg Loss: 0.0151 | Grad Norm: 0.00910437\n",
      "Epoch 3 | Step 2030900 | Avg Loss: 0.0155 | Grad Norm: 0.00756668\n",
      "Epoch 3 | Step 2031000 | Avg Loss: 0.0152 | Grad Norm: 0.00714038\n",
      "Epoch 3 | Step 2031100 | Avg Loss: 0.0154 | Grad Norm: 0.00769297\n",
      "Epoch 3 | Step 2031200 | Avg Loss: 0.0149 | Grad Norm: 0.00799651\n",
      "Epoch 3 | Step 2031300 | Avg Loss: 0.0149 | Grad Norm: 0.00894489\n",
      "Epoch 3 | Step 2031400 | Avg Loss: 0.0151 | Grad Norm: 0.00716631\n",
      "Epoch 3 | Step 2031500 | Avg Loss: 0.0149 | Grad Norm: 0.00715124\n",
      "Epoch 3 | Step 2031600 | Avg Loss: 0.0153 | Grad Norm: 0.00726121\n",
      "Epoch 3 | Step 2031700 | Avg Loss: 0.0153 | Grad Norm: 0.00806588\n",
      "Epoch 3 | Step 2031800 | Avg Loss: 0.0152 | Grad Norm: 0.00664532\n",
      "Epoch 3 | Step 2031900 | Avg Loss: 0.0154 | Grad Norm: 0.00916429\n",
      "Epoch 3 | Step 2032000 | Avg Loss: 0.0152 | Grad Norm: 0.00872797\n",
      "Epoch 3 | Step 2032100 | Avg Loss: 0.0151 | Grad Norm: 0.00752017\n",
      "Epoch 3 | Step 2032200 | Avg Loss: 0.0152 | Grad Norm: 0.00840143\n",
      "Epoch 3 | Step 2032300 | Avg Loss: 0.0152 | Grad Norm: 0.00796014\n",
      "Epoch 3 | Step 2032400 | Avg Loss: 0.0152 | Grad Norm: 0.00717837\n",
      "Epoch 3 | Step 2032500 | Avg Loss: 0.0152 | Grad Norm: 0.00640189\n",
      "Epoch 3 | Step 2032600 | Avg Loss: 0.0154 | Grad Norm: 0.00919348\n",
      "Epoch 3 | Step 2032700 | Avg Loss: 0.0153 | Grad Norm: 0.00736522\n",
      "Epoch 3 | Step 2032800 | Avg Loss: 0.0151 | Grad Norm: 0.00926614\n",
      "Epoch 3 | Step 2032900 | Avg Loss: 0.0150 | Grad Norm: 0.00938904\n",
      "Epoch 3 | Step 2033000 | Avg Loss: 0.0150 | Grad Norm: 0.00700547\n",
      "Epoch 3 | Step 2033100 | Avg Loss: 0.0152 | Grad Norm: 0.00764238\n",
      "Epoch 3 | Step 2033200 | Avg Loss: 0.0149 | Grad Norm: 0.00676853\n",
      "Epoch 3 | Step 2033300 | Avg Loss: 0.0147 | Grad Norm: 0.00845726\n",
      "Epoch 3 | Step 2033400 | Avg Loss: 0.0146 | Grad Norm: 0.00906151\n",
      "Epoch 3 | Step 2033500 | Avg Loss: 0.0147 | Grad Norm: 0.00675727\n",
      "Epoch 3 | Step 2033600 | Avg Loss: 0.0148 | Grad Norm: 0.00799562\n",
      "Epoch 3 | Step 2033700 | Avg Loss: 0.0152 | Grad Norm: 0.00798642\n",
      "Epoch 3 | Step 2033800 | Avg Loss: 0.0151 | Grad Norm: 0.00781430\n",
      "Epoch 3 | Step 2033900 | Avg Loss: 0.0151 | Grad Norm: 0.00870249\n",
      "Epoch 3 | Step 2034000 | Avg Loss: 0.0148 | Grad Norm: 0.00781937\n",
      "Epoch 3 | Step 2034100 | Avg Loss: 0.0149 | Grad Norm: 0.00646808\n",
      "Epoch 3 | Step 2034200 | Avg Loss: 0.0148 | Grad Norm: 0.00923144\n",
      "Epoch 3 | Step 2034300 | Avg Loss: 0.0151 | Grad Norm: 0.00744234\n",
      "Epoch 3 | Step 2034400 | Avg Loss: 0.0156 | Grad Norm: 0.00924934\n",
      "Epoch 3 | Step 2034500 | Avg Loss: 0.0154 | Grad Norm: 0.00768591\n",
      "Epoch 3 | Step 2034600 | Avg Loss: 0.0151 | Grad Norm: 0.00911070\n",
      "Epoch 3 | Step 2034700 | Avg Loss: 0.0149 | Grad Norm: 0.00910591\n",
      "Epoch 3 | Step 2034800 | Avg Loss: 0.0146 | Grad Norm: 0.00848221\n",
      "Epoch 3 | Step 2034900 | Avg Loss: 0.0147 | Grad Norm: 0.00817116\n",
      "Epoch 3 | Step 2035000 | Avg Loss: 0.0149 | Grad Norm: 0.00770331\n",
      "Epoch 3 | Step 2035100 | Avg Loss: 0.0152 | Grad Norm: 0.00775264\n",
      "Epoch 3 | Step 2035200 | Avg Loss: 0.0149 | Grad Norm: 0.00777421\n",
      "Epoch 3 | Step 2035300 | Avg Loss: 0.0150 | Grad Norm: 0.00852231\n",
      "Epoch 3 | Step 2035400 | Avg Loss: 0.0150 | Grad Norm: 0.00885645\n",
      "Epoch 3 | Step 2035500 | Avg Loss: 0.0151 | Grad Norm: 0.00766724\n",
      "Epoch 3 | Step 2035600 | Avg Loss: 0.0151 | Grad Norm: 0.00849996\n",
      "Epoch 3 | Step 2035700 | Avg Loss: 0.0146 | Grad Norm: 0.00836848\n",
      "Epoch 3 | Step 2035800 | Avg Loss: 0.0149 | Grad Norm: 0.00822124\n",
      "Epoch 3 | Step 2035900 | Avg Loss: 0.0147 | Grad Norm: 0.00818317\n",
      "Epoch 3 | Step 2036000 | Avg Loss: 0.0150 | Grad Norm: 0.00860659\n",
      "Epoch 3 | Step 2036100 | Avg Loss: 0.0146 | Grad Norm: 0.00892029\n",
      "Epoch 3 | Step 2036200 | Avg Loss: 0.0150 | Grad Norm: 0.00864155\n",
      "Epoch 3 | Step 2036300 | Avg Loss: 0.0150 | Grad Norm: 0.00762602\n",
      "Epoch 3 | Step 2036400 | Avg Loss: 0.0152 | Grad Norm: 0.00739521\n",
      "Epoch 3 | Step 2036500 | Avg Loss: 0.0153 | Grad Norm: 0.00926946\n",
      "Epoch 3 | Step 2036600 | Avg Loss: 0.0152 | Grad Norm: 0.00860989\n",
      "Epoch 3 | Step 2036700 | Avg Loss: 0.0152 | Grad Norm: 0.00784054\n",
      "Epoch 3 | Step 2036800 | Avg Loss: 0.0152 | Grad Norm: 0.00840175\n",
      "Epoch 3 | Step 2036900 | Avg Loss: 0.0153 | Grad Norm: 0.00892241\n",
      "Epoch 3 | Step 2037000 | Avg Loss: 0.0156 | Grad Norm: 0.00659304\n",
      "Epoch 3 | Step 2037100 | Avg Loss: 0.0156 | Grad Norm: 0.00806417\n",
      "Epoch 3 | Step 2037200 | Avg Loss: 0.0153 | Grad Norm: 0.00800105\n",
      "Epoch 3 | Step 2037300 | Avg Loss: 0.0157 | Grad Norm: 0.00862613\n",
      "Epoch 3 | Step 2037400 | Avg Loss: 0.0152 | Grad Norm: 0.00791262\n",
      "Epoch 3 | Step 2037500 | Avg Loss: 0.0156 | Grad Norm: 0.00844019\n",
      "Epoch 3 | Step 2037600 | Avg Loss: 0.0155 | Grad Norm: 0.01069552\n",
      "Epoch 3 | Step 2037700 | Avg Loss: 0.0152 | Grad Norm: 0.01418334\n",
      "Epoch 3 | Step 2037800 | Avg Loss: 0.0151 | Grad Norm: 0.00762523\n",
      "Epoch 3 | Step 2037900 | Avg Loss: 0.0149 | Grad Norm: 0.00954964\n",
      "Epoch 3 | Step 2038000 | Avg Loss: 0.0146 | Grad Norm: 0.00721493\n",
      "Epoch 3 | Step 2038100 | Avg Loss: 0.0142 | Grad Norm: 0.00750479\n",
      "Epoch 3 | Step 2038200 | Avg Loss: 0.0144 | Grad Norm: 0.00775001\n",
      "Epoch 3 | Step 2038300 | Avg Loss: 0.0145 | Grad Norm: 0.00734082\n",
      "Epoch 3 | Step 2038400 | Avg Loss: 0.0146 | Grad Norm: 0.00703242\n",
      "Epoch 3 | Step 2038500 | Avg Loss: 0.0144 | Grad Norm: 0.00781588\n",
      "Epoch 3 | Step 2038600 | Avg Loss: 0.0148 | Grad Norm: 0.00671346\n",
      "Epoch 3 | Step 2038700 | Avg Loss: 0.0145 | Grad Norm: 0.00811070\n",
      "Epoch 3 | Step 2038800 | Avg Loss: 0.0149 | Grad Norm: 0.00767068\n",
      "Epoch 3 | Step 2038900 | Avg Loss: 0.0151 | Grad Norm: 0.00711731\n",
      "Epoch 3 | Step 2039000 | Avg Loss: 0.0155 | Grad Norm: 0.00952205\n",
      "Epoch 3 | Step 2039100 | Avg Loss: 0.0155 | Grad Norm: 0.00716496\n",
      "Epoch 3 | Step 2039200 | Avg Loss: 0.0156 | Grad Norm: 0.00730835\n",
      "Epoch 3 | Step 2039300 | Avg Loss: 0.0154 | Grad Norm: 0.00765549\n",
      "Epoch 3 | Step 2039400 | Avg Loss: 0.0152 | Grad Norm: 0.00843991\n",
      "Epoch 3 | Step 2039500 | Avg Loss: 0.0152 | Grad Norm: 0.00926093\n",
      "Epoch 3 | Step 2039600 | Avg Loss: 0.0147 | Grad Norm: 0.00628898\n",
      "Epoch 3 | Step 2039700 | Avg Loss: 0.0148 | Grad Norm: 0.00864077\n",
      "Epoch 3 | Step 2039800 | Avg Loss: 0.0152 | Grad Norm: 0.00829094\n",
      "Epoch 3 | Step 2039900 | Avg Loss: 0.0154 | Grad Norm: 0.00724687\n",
      "Epoch 3 | Step 2040000 | Avg Loss: 0.0152 | Grad Norm: 0.00873708\n",
      "Epoch 3 | Step 2040100 | Avg Loss: 0.0154 | Grad Norm: 0.00745048\n",
      "Epoch 3 | Step 2040200 | Avg Loss: 0.0152 | Grad Norm: 0.00760224\n",
      "Epoch 3 | Step 2040300 | Avg Loss: 0.0153 | Grad Norm: 0.00802667\n",
      "Epoch 3 | Step 2040400 | Avg Loss: 0.0152 | Grad Norm: 0.00688313\n",
      "Epoch 3 | Step 2040500 | Avg Loss: 0.0156 | Grad Norm: 0.00730556\n",
      "Epoch 3 | Step 2040600 | Avg Loss: 0.0157 | Grad Norm: 0.00728170\n",
      "Epoch 3 | Step 2040700 | Avg Loss: 0.0158 | Grad Norm: 0.00827573\n",
      "Epoch 3 | Step 2040800 | Avg Loss: 0.0152 | Grad Norm: 0.00741716\n",
      "Epoch 3 | Step 2040900 | Avg Loss: 0.0155 | Grad Norm: 0.00809149\n",
      "Epoch 3 | Step 2041000 | Avg Loss: 0.0154 | Grad Norm: 0.00894100\n",
      "Epoch 3 | Step 2041100 | Avg Loss: 0.0155 | Grad Norm: 0.00788261\n",
      "Epoch 3 | Step 2041200 | Avg Loss: 0.0151 | Grad Norm: 0.00741763\n",
      "Epoch 3 | Step 2041300 | Avg Loss: 0.0152 | Grad Norm: 0.00723330\n",
      "Epoch 3 | Step 2041400 | Avg Loss: 0.0150 | Grad Norm: 0.00837247\n",
      "Epoch 3 | Step 2041500 | Avg Loss: 0.0150 | Grad Norm: 0.00932276\n",
      "Epoch 3 | Step 2041600 | Avg Loss: 0.0151 | Grad Norm: 0.00737053\n",
      "Epoch 3 | Step 2041700 | Avg Loss: 0.0147 | Grad Norm: 0.00684130\n",
      "Epoch 3 | Step 2041800 | Avg Loss: 0.0147 | Grad Norm: 0.00714787\n",
      "Epoch 3 | Step 2041900 | Avg Loss: 0.0149 | Grad Norm: 0.00740549\n",
      "Epoch 3 | Step 2042000 | Avg Loss: 0.0155 | Grad Norm: 0.00805860\n",
      "Epoch 3 | Step 2042100 | Avg Loss: 0.0153 | Grad Norm: 0.00728710\n",
      "Epoch 3 | Step 2042200 | Avg Loss: 0.0157 | Grad Norm: 0.00728498\n",
      "Epoch 3 | Step 2042300 | Avg Loss: 0.0154 | Grad Norm: 0.00839448\n",
      "Epoch 3 | Step 2042400 | Avg Loss: 0.0156 | Grad Norm: 0.00898043\n",
      "Epoch 3 | Step 2042500 | Avg Loss: 0.0159 | Grad Norm: 0.00771134\n",
      "Epoch 3 | Step 2042600 | Avg Loss: 0.0157 | Grad Norm: 0.00776739\n",
      "Epoch 3 | Step 2042700 | Avg Loss: 0.0158 | Grad Norm: 0.00760440\n",
      "Epoch 3 | Step 2042800 | Avg Loss: 0.0157 | Grad Norm: 0.00830687\n",
      "Epoch 3 | Step 2042900 | Avg Loss: 0.0153 | Grad Norm: 0.00721883\n",
      "Epoch 3 | Step 2043000 | Avg Loss: 0.0151 | Grad Norm: 0.00718928\n",
      "Epoch 3 | Step 2043100 | Avg Loss: 0.0150 | Grad Norm: 0.00750993\n",
      "Epoch 3 | Step 2043200 | Avg Loss: 0.0145 | Grad Norm: 0.00726614\n",
      "Epoch 3 | Step 2043300 | Avg Loss: 0.0149 | Grad Norm: 0.00878141\n",
      "Epoch 3 | Step 2043400 | Avg Loss: 0.0151 | Grad Norm: 0.00843378\n",
      "Epoch 3 | Step 2043500 | Avg Loss: 0.0151 | Grad Norm: 0.00757146\n",
      "Epoch 3 | Step 2043600 | Avg Loss: 0.0154 | Grad Norm: 0.00847655\n",
      "Epoch 3 | Step 2043700 | Avg Loss: 0.0154 | Grad Norm: 0.00859868\n",
      "Epoch 3 | Step 2043800 | Avg Loss: 0.0157 | Grad Norm: 0.00725512\n",
      "Epoch 3 | Step 2043900 | Avg Loss: 0.0156 | Grad Norm: 0.00667160\n",
      "Epoch 3 | Step 2044000 | Avg Loss: 0.0155 | Grad Norm: 0.00792011\n",
      "Epoch 3 | Step 2044100 | Avg Loss: 0.0155 | Grad Norm: 0.00721267\n",
      "Epoch 3 | Step 2044200 | Avg Loss: 0.0153 | Grad Norm: 0.00845453\n",
      "Epoch 3 | Step 2044300 | Avg Loss: 0.0153 | Grad Norm: 0.00930249\n",
      "Epoch 3 | Step 2044400 | Avg Loss: 0.0151 | Grad Norm: 0.00816984\n",
      "Epoch 3 | Step 2044500 | Avg Loss: 0.0149 | Grad Norm: 0.00722855\n",
      "Epoch 3 | Step 2044600 | Avg Loss: 0.0148 | Grad Norm: 0.00731766\n",
      "Epoch 3 | Step 2044700 | Avg Loss: 0.0152 | Grad Norm: 0.00923540\n",
      "Epoch 3 | Step 2044800 | Avg Loss: 0.0146 | Grad Norm: 0.00704623\n",
      "Epoch 3 | Step 2044900 | Avg Loss: 0.0152 | Grad Norm: 0.00764061\n",
      "Epoch 3 | Step 2045000 | Avg Loss: 0.0155 | Grad Norm: 0.00990460\n",
      "Epoch 3 | Step 2045100 | Avg Loss: 0.0156 | Grad Norm: 0.00872709\n",
      "Epoch 3 | Step 2045200 | Avg Loss: 0.0156 | Grad Norm: 0.00727293\n",
      "Epoch 3 | Step 2045300 | Avg Loss: 0.0153 | Grad Norm: 0.00728389\n",
      "Epoch 3 | Step 2045400 | Avg Loss: 0.0156 | Grad Norm: 0.01107991\n",
      "Epoch 3 | Step 2045500 | Avg Loss: 0.0154 | Grad Norm: 0.00786402\n",
      "Epoch 3 | Step 2045600 | Avg Loss: 0.0152 | Grad Norm: 0.01034136\n",
      "Epoch 3 | Step 2045700 | Avg Loss: 0.0153 | Grad Norm: 0.00765169\n",
      "Epoch 3 | Step 2045800 | Avg Loss: 0.0153 | Grad Norm: 0.00827133\n",
      "Epoch 3 | Step 2045900 | Avg Loss: 0.0151 | Grad Norm: 0.00686404\n",
      "Epoch 3 | Step 2046000 | Avg Loss: 0.0150 | Grad Norm: 0.00898840\n",
      "Epoch 3 | Step 2046100 | Avg Loss: 0.0148 | Grad Norm: 0.00739922\n",
      "Epoch 3 | Step 2046200 | Avg Loss: 0.0147 | Grad Norm: 0.00855635\n",
      "Epoch 3 | Step 2046300 | Avg Loss: 0.0150 | Grad Norm: 0.00741515\n",
      "Epoch 3 | Step 2046400 | Avg Loss: 0.0149 | Grad Norm: 0.00724531\n",
      "Epoch 3 | Step 2046500 | Avg Loss: 0.0151 | Grad Norm: 0.00775700\n",
      "Epoch 3 | Step 2046600 | Avg Loss: 0.0150 | Grad Norm: 0.00705005\n",
      "Epoch 3 | Step 2046700 | Avg Loss: 0.0151 | Grad Norm: 0.00911607\n",
      "Epoch 3 | Step 2046800 | Avg Loss: 0.0150 | Grad Norm: 0.01163566\n",
      "Epoch 3 | Step 2046900 | Avg Loss: 0.0151 | Grad Norm: 0.00760688\n",
      "Epoch 3 | Step 2047000 | Avg Loss: 0.0152 | Grad Norm: 0.00694737\n",
      "Epoch 3 | Step 2047100 | Avg Loss: 0.0150 | Grad Norm: 0.00866555\n",
      "Epoch 3 | Step 2047200 | Avg Loss: 0.0150 | Grad Norm: 0.00715944\n",
      "Epoch 3 | Step 2047300 | Avg Loss: 0.0152 | Grad Norm: 0.00771628\n",
      "Epoch 3 | Step 2047400 | Avg Loss: 0.0153 | Grad Norm: 0.00770394\n",
      "Epoch 3 | Step 2047500 | Avg Loss: 0.0154 | Grad Norm: 0.00916457\n",
      "Epoch 3 | Step 2047600 | Avg Loss: 0.0152 | Grad Norm: 0.00737174\n",
      "Epoch 3 | Step 2047700 | Avg Loss: 0.0149 | Grad Norm: 0.00746652\n",
      "Epoch 3 | Step 2047800 | Avg Loss: 0.0146 | Grad Norm: 0.00777204\n",
      "Epoch 3 | Step 2047900 | Avg Loss: 0.0148 | Grad Norm: 0.00802277\n",
      "Epoch 3 | Step 2048000 | Avg Loss: 0.0150 | Grad Norm: 0.00803199\n",
      "Epoch 3 | Step 2048100 | Avg Loss: 0.0149 | Grad Norm: 0.00784422\n",
      "Epoch 3 | Step 2048200 | Avg Loss: 0.0149 | Grad Norm: 0.00794131\n",
      "Epoch 3 | Step 2048300 | Avg Loss: 0.0149 | Grad Norm: 0.00877150\n",
      "Epoch 3 | Step 2048400 | Avg Loss: 0.0146 | Grad Norm: 0.00817058\n",
      "Epoch 3 | Step 2048500 | Avg Loss: 0.0149 | Grad Norm: 0.00852107\n",
      "Epoch 3 | Step 2048600 | Avg Loss: 0.0151 | Grad Norm: 0.01384788\n",
      "Epoch 3 | Step 2048700 | Avg Loss: 0.0152 | Grad Norm: 0.00950651\n",
      "Epoch 3 | Step 2048800 | Avg Loss: 0.0153 | Grad Norm: 0.00761570\n",
      "Epoch 3 | Step 2048900 | Avg Loss: 0.0156 | Grad Norm: 0.00796815\n",
      "Epoch 3 | Step 2049000 | Avg Loss: 0.0152 | Grad Norm: 0.00728786\n",
      "Epoch 3 | Step 2049100 | Avg Loss: 0.0152 | Grad Norm: 0.00808038\n",
      "Epoch 3 | Step 2049200 | Avg Loss: 0.0150 | Grad Norm: 0.00699997\n",
      "Epoch 3 | Step 2049300 | Avg Loss: 0.0150 | Grad Norm: 0.00726834\n",
      "Epoch 3 | Step 2049400 | Avg Loss: 0.0151 | Grad Norm: 0.00880591\n",
      "Epoch 3 | Step 2049500 | Avg Loss: 0.0151 | Grad Norm: 0.01062193\n",
      "Epoch 3 | Step 2049600 | Avg Loss: 0.0157 | Grad Norm: 0.00808392\n",
      "Epoch 3 | Step 2049700 | Avg Loss: 0.0155 | Grad Norm: 0.00958451\n",
      "Epoch 3 | Step 2049800 | Avg Loss: 0.0152 | Grad Norm: 0.00756651\n",
      "Epoch 3 | Step 2049900 | Avg Loss: 0.0151 | Grad Norm: 0.00692139\n",
      "Epoch 3 | Step 2050000 | Avg Loss: 0.0146 | Grad Norm: 0.00795659\n",
      "Epoch 3 | Step 2050100 | Avg Loss: 0.0144 | Grad Norm: 0.00827796\n",
      "Epoch 3 | Step 2050200 | Avg Loss: 0.0144 | Grad Norm: 0.00710048\n",
      "Epoch 3 | Step 2050300 | Avg Loss: 0.0147 | Grad Norm: 0.00734575\n",
      "Epoch 3 | Step 2050400 | Avg Loss: 0.0147 | Grad Norm: 0.00792388\n",
      "Epoch 3 | Step 2050500 | Avg Loss: 0.0148 | Grad Norm: 0.00959767\n",
      "Epoch 3 | Step 2050600 | Avg Loss: 0.0149 | Grad Norm: 0.00843733\n",
      "Epoch 3 | Step 2050700 | Avg Loss: 0.0148 | Grad Norm: 0.00719507\n",
      "Epoch 3 | Step 2050800 | Avg Loss: 0.0147 | Grad Norm: 0.00863315\n",
      "Epoch 3 | Step 2050900 | Avg Loss: 0.0150 | Grad Norm: 0.00959961\n",
      "Epoch 3 | Step 2051000 | Avg Loss: 0.0149 | Grad Norm: 0.00717694\n",
      "Epoch 3 | Step 2051100 | Avg Loss: 0.0149 | Grad Norm: 0.00734004\n",
      "Epoch 3 | Step 2051200 | Avg Loss: 0.0152 | Grad Norm: 0.00852884\n",
      "Epoch 3 | Step 2051300 | Avg Loss: 0.0155 | Grad Norm: 0.00983134\n",
      "Epoch 3 | Step 2051400 | Avg Loss: 0.0155 | Grad Norm: 0.00761129\n",
      "Epoch 3 | Step 2051500 | Avg Loss: 0.0156 | Grad Norm: 0.00809799\n",
      "Epoch 3 | Step 2051600 | Avg Loss: 0.0154 | Grad Norm: 0.00732006\n",
      "Epoch 3 | Step 2051700 | Avg Loss: 0.0149 | Grad Norm: 0.00906117\n",
      "Epoch 3 | Step 2051800 | Avg Loss: 0.0153 | Grad Norm: 0.00652228\n",
      "Epoch 3 | Step 2051900 | Avg Loss: 0.0153 | Grad Norm: 0.00833161\n",
      "Epoch 3 | Step 2052000 | Avg Loss: 0.0151 | Grad Norm: 0.00761274\n",
      "Epoch 3 | Step 2052100 | Avg Loss: 0.0151 | Grad Norm: 0.00811672\n",
      "Epoch 3 | Step 2052200 | Avg Loss: 0.0153 | Grad Norm: 0.00943367\n",
      "Epoch 3 | Step 2052300 | Avg Loss: 0.0154 | Grad Norm: 0.00894269\n",
      "Epoch 3 | Step 2052400 | Avg Loss: 0.0155 | Grad Norm: 0.01033579\n",
      "Epoch 3 | Step 2052500 | Avg Loss: 0.0156 | Grad Norm: 0.00810866\n",
      "Epoch 3 | Step 2052600 | Avg Loss: 0.0158 | Grad Norm: 0.01040304\n",
      "Epoch 3 | Step 2052700 | Avg Loss: 0.0159 | Grad Norm: 0.00809523\n",
      "Epoch 3 | Step 2052800 | Avg Loss: 0.0159 | Grad Norm: 0.00857321\n",
      "Epoch 3 | Step 2052900 | Avg Loss: 0.0158 | Grad Norm: 0.00996173\n",
      "Epoch 3 | Step 2053000 | Avg Loss: 0.0160 | Grad Norm: 0.00847540\n",
      "Epoch 3 | Step 2053100 | Avg Loss: 0.0157 | Grad Norm: 0.01024813\n",
      "Epoch 3 | Step 2053200 | Avg Loss: 0.0154 | Grad Norm: 0.00749029\n",
      "Epoch 3 | Step 2053300 | Avg Loss: 0.0151 | Grad Norm: 0.00989399\n",
      "Epoch 3 | Step 2053400 | Avg Loss: 0.0151 | Grad Norm: 0.00784060\n",
      "Epoch 3 | Step 2053500 | Avg Loss: 0.0155 | Grad Norm: 0.00932364\n",
      "Epoch 3 | Step 2053600 | Avg Loss: 0.0153 | Grad Norm: 0.00757816\n",
      "Epoch 3 | Step 2053700 | Avg Loss: 0.0151 | Grad Norm: 0.00937913\n",
      "Epoch 3 | Step 2053800 | Avg Loss: 0.0152 | Grad Norm: 0.00789122\n",
      "Epoch 3 | Step 2053900 | Avg Loss: 0.0147 | Grad Norm: 0.00737777\n",
      "Epoch 3 | Step 2054000 | Avg Loss: 0.0150 | Grad Norm: 0.00754395\n",
      "Epoch 3 | Step 2054100 | Avg Loss: 0.0150 | Grad Norm: 0.00871657\n",
      "Epoch 3 | Step 2054200 | Avg Loss: 0.0152 | Grad Norm: 0.00790571\n",
      "Epoch 3 | Step 2054300 | Avg Loss: 0.0151 | Grad Norm: 0.00805592\n",
      "Epoch 3 | Step 2054400 | Avg Loss: 0.0149 | Grad Norm: 0.00725087\n",
      "Epoch 3 | Step 2054500 | Avg Loss: 0.0148 | Grad Norm: 0.00809085\n",
      "Epoch 3 | Step 2054600 | Avg Loss: 0.0151 | Grad Norm: 0.00775571\n",
      "Epoch 3 | Step 2054700 | Avg Loss: 0.0149 | Grad Norm: 0.00664290\n",
      "Epoch 3 | Step 2054800 | Avg Loss: 0.0148 | Grad Norm: 0.00717595\n",
      "Epoch 3 | Step 2054900 | Avg Loss: 0.0147 | Grad Norm: 0.00860295\n",
      "Epoch 3 | Step 2055000 | Avg Loss: 0.0153 | Grad Norm: 0.00703221\n",
      "Epoch 3 | Step 2055100 | Avg Loss: 0.0155 | Grad Norm: 0.00940881\n",
      "Epoch 3 | Step 2055200 | Avg Loss: 0.0152 | Grad Norm: 0.00844838\n",
      "Epoch 3 | Step 2055300 | Avg Loss: 0.0150 | Grad Norm: 0.00691666\n",
      "Epoch 3 | Step 2055400 | Avg Loss: 0.0149 | Grad Norm: 0.00788282\n",
      "Epoch 3 | Step 2055500 | Avg Loss: 0.0152 | Grad Norm: 0.00679631\n",
      "Epoch 3 | Step 2055600 | Avg Loss: 0.0153 | Grad Norm: 0.00936186\n",
      "Epoch 3 | Step 2055700 | Avg Loss: 0.0156 | Grad Norm: 0.00715485\n",
      "Epoch 3 | Step 2055800 | Avg Loss: 0.0153 | Grad Norm: 0.00806424\n",
      "Epoch 3 | Step 2055900 | Avg Loss: 0.0157 | Grad Norm: 0.00804257\n",
      "Epoch 3 | Step 2056000 | Avg Loss: 0.0157 | Grad Norm: 0.00894105\n",
      "Epoch 3 | Step 2056100 | Avg Loss: 0.0154 | Grad Norm: 0.00792181\n",
      "Epoch 3 | Step 2056200 | Avg Loss: 0.0153 | Grad Norm: 0.00788214\n",
      "Epoch 3 | Step 2056300 | Avg Loss: 0.0156 | Grad Norm: 0.00867207\n",
      "Epoch 3 | Step 2056400 | Avg Loss: 0.0152 | Grad Norm: 0.00838340\n",
      "Epoch 3 | Step 2056500 | Avg Loss: 0.0152 | Grad Norm: 0.00859192\n",
      "Epoch 3 | Step 2056600 | Avg Loss: 0.0151 | Grad Norm: 0.01004768\n",
      "Epoch 3 | Step 2056700 | Avg Loss: 0.0151 | Grad Norm: 0.00756534\n",
      "Epoch 3 | Step 2056800 | Avg Loss: 0.0151 | Grad Norm: 0.00943343\n",
      "Epoch 3 | Step 2056900 | Avg Loss: 0.0147 | Grad Norm: 0.00702316\n",
      "Epoch 3 | Step 2057000 | Avg Loss: 0.0148 | Grad Norm: 0.00870807\n",
      "Epoch 3 | Step 2057100 | Avg Loss: 0.0149 | Grad Norm: 0.00966734\n",
      "Epoch 3 | Step 2057200 | Avg Loss: 0.0149 | Grad Norm: 0.00809055\n",
      "Epoch 3 | Step 2057300 | Avg Loss: 0.0151 | Grad Norm: 0.00815110\n",
      "Epoch 3 | Step 2057400 | Avg Loss: 0.0153 | Grad Norm: 0.00851465\n",
      "Epoch 3 | Step 2057500 | Avg Loss: 0.0154 | Grad Norm: 0.00778503\n",
      "Epoch 3 | Step 2057600 | Avg Loss: 0.0154 | Grad Norm: 0.00794883\n",
      "Epoch 3 | Step 2057700 | Avg Loss: 0.0155 | Grad Norm: 0.00943273\n",
      "Epoch 3 | Step 2057800 | Avg Loss: 0.0154 | Grad Norm: 0.00721147\n",
      "Epoch 3 | Step 2057900 | Avg Loss: 0.0157 | Grad Norm: 0.00975186\n",
      "Epoch 3 | Step 2058000 | Avg Loss: 0.0154 | Grad Norm: 0.00823213\n",
      "Epoch 3 | Step 2058100 | Avg Loss: 0.0152 | Grad Norm: 0.00769971\n",
      "Epoch 3 | Step 2058200 | Avg Loss: 0.0154 | Grad Norm: 0.00871804\n",
      "Epoch 3 | Step 2058300 | Avg Loss: 0.0152 | Grad Norm: 0.00827377\n",
      "Epoch 3 | Step 2058400 | Avg Loss: 0.0150 | Grad Norm: 0.00854294\n",
      "Epoch 3 | Step 2058500 | Avg Loss: 0.0150 | Grad Norm: 0.00890889\n",
      "Epoch 3 | Step 2058600 | Avg Loss: 0.0149 | Grad Norm: 0.00839115\n",
      "Epoch 3 | Step 2058700 | Avg Loss: 0.0150 | Grad Norm: 0.00735483\n",
      "Epoch 3 | Step 2058800 | Avg Loss: 0.0150 | Grad Norm: 0.00661345\n",
      "Epoch 3 | Step 2058900 | Avg Loss: 0.0146 | Grad Norm: 0.00699039\n",
      "Epoch 3 | Step 2059000 | Avg Loss: 0.0145 | Grad Norm: 0.00699822\n",
      "Epoch 3 | Step 2059100 | Avg Loss: 0.0151 | Grad Norm: 0.00696646\n",
      "Epoch 3 | Step 2059200 | Avg Loss: 0.0156 | Grad Norm: 0.00740476\n",
      "Epoch 3 | Step 2059300 | Avg Loss: 0.0156 | Grad Norm: 0.00648699\n",
      "Epoch 3 | Step 2059400 | Avg Loss: 0.0157 | Grad Norm: 0.00868978\n",
      "Epoch 3 | Step 2059500 | Avg Loss: 0.0150 | Grad Norm: 0.00782748\n",
      "Epoch 3 | Step 2059600 | Avg Loss: 0.0150 | Grad Norm: 0.00853810\n",
      "Epoch 3 | Step 2059700 | Avg Loss: 0.0152 | Grad Norm: 0.00710888\n",
      "Epoch 3 | Step 2059800 | Avg Loss: 0.0150 | Grad Norm: 0.00959693\n",
      "Epoch 3 | Step 2059900 | Avg Loss: 0.0147 | Grad Norm: 0.00759121\n",
      "Epoch 3 | Step 2060000 | Avg Loss: 0.0150 | Grad Norm: 0.00777664\n",
      "Epoch 3 | Step 2060100 | Avg Loss: 0.0150 | Grad Norm: 0.00816937\n",
      "Epoch 3 | Step 2060200 | Avg Loss: 0.0150 | Grad Norm: 0.00847246\n",
      "Epoch 3 | Step 2060300 | Avg Loss: 0.0147 | Grad Norm: 0.00766427\n",
      "Epoch 3 | Step 2060400 | Avg Loss: 0.0145 | Grad Norm: 0.00827805\n",
      "Epoch 3 | Step 2060500 | Avg Loss: 0.0147 | Grad Norm: 0.00875322\n",
      "Epoch 3 | Step 2060600 | Avg Loss: 0.0145 | Grad Norm: 0.00774202\n",
      "Epoch 3 | Step 2060700 | Avg Loss: 0.0149 | Grad Norm: 0.00733796\n",
      "Epoch 3 | Step 2060800 | Avg Loss: 0.0144 | Grad Norm: 0.00900643\n",
      "Epoch 3 | Step 2060900 | Avg Loss: 0.0145 | Grad Norm: 0.00936404\n",
      "Epoch 3 | Step 2061000 | Avg Loss: 0.0146 | Grad Norm: 0.00855006\n",
      "Epoch 3 | Step 2061100 | Avg Loss: 0.0149 | Grad Norm: 0.00692711\n",
      "Epoch 3 | Step 2061200 | Avg Loss: 0.0153 | Grad Norm: 0.00983129\n",
      "Epoch 3 | Step 2061300 | Avg Loss: 0.0156 | Grad Norm: 0.00748246\n",
      "Epoch 3 | Step 2061400 | Avg Loss: 0.0155 | Grad Norm: 0.00725550\n",
      "Epoch 3 | Step 2061500 | Avg Loss: 0.0151 | Grad Norm: 0.00700123\n",
      "Epoch 3 | Step 2061600 | Avg Loss: 0.0155 | Grad Norm: 0.00827441\n",
      "Epoch 3 | Step 2061700 | Avg Loss: 0.0156 | Grad Norm: 0.00859976\n",
      "Epoch 3 | Step 2061800 | Avg Loss: 0.0152 | Grad Norm: 0.00791513\n",
      "Epoch 3 | Step 2061900 | Avg Loss: 0.0151 | Grad Norm: 0.00813193\n",
      "Epoch 3 | Step 2062000 | Avg Loss: 0.0155 | Grad Norm: 0.00809243\n",
      "Epoch 3 | Step 2062100 | Avg Loss: 0.0154 | Grad Norm: 0.00784771\n",
      "Epoch 3 | Step 2062200 | Avg Loss: 0.0155 | Grad Norm: 0.00851379\n",
      "Epoch 3 | Step 2062300 | Avg Loss: 0.0157 | Grad Norm: 0.00858997\n",
      "Epoch 3 | Step 2062400 | Avg Loss: 0.0155 | Grad Norm: 0.00728945\n",
      "Epoch 3 | Step 2062500 | Avg Loss: 0.0160 | Grad Norm: 0.00819743\n",
      "Epoch 3 | Step 2062600 | Avg Loss: 0.0156 | Grad Norm: 0.00747166\n",
      "Epoch 3 | Step 2062700 | Avg Loss: 0.0152 | Grad Norm: 0.00780368\n",
      "Epoch 3 | Step 2062800 | Avg Loss: 0.0155 | Grad Norm: 0.00801597\n",
      "Epoch 3 | Step 2062900 | Avg Loss: 0.0154 | Grad Norm: 0.00883243\n",
      "Epoch 3 | Step 2063000 | Avg Loss: 0.0153 | Grad Norm: 0.00672171\n",
      "Epoch 3 | Step 2063100 | Avg Loss: 0.0151 | Grad Norm: 0.00767666\n",
      "Epoch 3 | Step 2063200 | Avg Loss: 0.0151 | Grad Norm: 0.00810231\n",
      "Epoch 3 | Step 2063300 | Avg Loss: 0.0153 | Grad Norm: 0.00763312\n",
      "Epoch 3 | Step 2063400 | Avg Loss: 0.0148 | Grad Norm: 0.00762458\n",
      "Epoch 3 | Step 2063500 | Avg Loss: 0.0153 | Grad Norm: 0.00720862\n",
      "Epoch 3 | Step 2063600 | Avg Loss: 0.0153 | Grad Norm: 0.00923573\n",
      "Epoch 3 | Step 2063700 | Avg Loss: 0.0151 | Grad Norm: 0.00847027\n",
      "Epoch 3 | Step 2063800 | Avg Loss: 0.0150 | Grad Norm: 0.00810186\n",
      "Epoch 3 | Step 2063900 | Avg Loss: 0.0150 | Grad Norm: 0.00808745\n",
      "Epoch 3 | Step 2064000 | Avg Loss: 0.0147 | Grad Norm: 0.00712805\n",
      "Epoch 3 | Step 2064100 | Avg Loss: 0.0147 | Grad Norm: 0.00894806\n",
      "Epoch 3 | Step 2064200 | Avg Loss: 0.0142 | Grad Norm: 0.00716115\n",
      "Epoch 3 | Step 2064300 | Avg Loss: 0.0141 | Grad Norm: 0.00737526\n",
      "Epoch 3 | Step 2064400 | Avg Loss: 0.0140 | Grad Norm: 0.00739999\n",
      "Epoch 3 | Step 2064500 | Avg Loss: 0.0141 | Grad Norm: 0.00763918\n",
      "Epoch 3 | Step 2064600 | Avg Loss: 0.0142 | Grad Norm: 0.00729593\n",
      "Epoch 3 | Step 2064700 | Avg Loss: 0.0142 | Grad Norm: 0.00830919\n",
      "Epoch 3 | Step 2064800 | Avg Loss: 0.0143 | Grad Norm: 0.00870133\n",
      "Epoch 3 | Step 2064900 | Avg Loss: 0.0146 | Grad Norm: 0.00864724\n",
      "Epoch 3 | Step 2065000 | Avg Loss: 0.0146 | Grad Norm: 0.00774324\n",
      "Epoch 3 | Step 2065100 | Avg Loss: 0.0145 | Grad Norm: 0.00805466\n",
      "Epoch 3 | Step 2065200 | Avg Loss: 0.0143 | Grad Norm: 0.00773560\n",
      "Epoch 3 | Step 2065300 | Avg Loss: 0.0142 | Grad Norm: 0.00740713\n",
      "Epoch 3 | Step 2065400 | Avg Loss: 0.0142 | Grad Norm: 0.00857263\n",
      "Epoch 3 | Step 2065500 | Avg Loss: 0.0145 | Grad Norm: 0.00758280\n",
      "Epoch 3 | Step 2065600 | Avg Loss: 0.0146 | Grad Norm: 0.00689613\n",
      "Epoch 3 | Step 2065700 | Avg Loss: 0.0148 | Grad Norm: 0.00843236\n",
      "Epoch 3 | Step 2065800 | Avg Loss: 0.0150 | Grad Norm: 0.00819734\n",
      "Epoch 3 | Step 2065900 | Avg Loss: 0.0148 | Grad Norm: 0.00846771\n",
      "Epoch 3 | Step 2066000 | Avg Loss: 0.0152 | Grad Norm: 0.00838911\n",
      "Epoch 3 | Step 2066100 | Avg Loss: 0.0153 | Grad Norm: 0.00827652\n",
      "Epoch 3 | Step 2066200 | Avg Loss: 0.0155 | Grad Norm: 0.00876013\n",
      "Epoch 3 | Step 2066300 | Avg Loss: 0.0154 | Grad Norm: 0.00683191\n",
      "Epoch 3 | Step 2066400 | Avg Loss: 0.0152 | Grad Norm: 0.00906789\n",
      "Epoch 3 | Step 2066500 | Avg Loss: 0.0153 | Grad Norm: 0.00891011\n",
      "Epoch 3 | Step 2066600 | Avg Loss: 0.0152 | Grad Norm: 0.00721689\n",
      "Epoch 3 | Step 2066700 | Avg Loss: 0.0153 | Grad Norm: 0.00808235\n",
      "Epoch 3 | Step 2066800 | Avg Loss: 0.0155 | Grad Norm: 0.00894543\n",
      "Epoch 3 | Step 2066900 | Avg Loss: 0.0152 | Grad Norm: 0.00783638\n",
      "Epoch 3 | Step 2067000 | Avg Loss: 0.0154 | Grad Norm: 0.00940189\n",
      "Epoch 3 | Step 2067100 | Avg Loss: 0.0152 | Grad Norm: 0.00753444\n",
      "Epoch 3 | Step 2067200 | Avg Loss: 0.0156 | Grad Norm: 0.00805945\n",
      "Epoch 3 | Step 2067300 | Avg Loss: 0.0159 | Grad Norm: 0.00819357\n",
      "Epoch 3 | Step 2067400 | Avg Loss: 0.0157 | Grad Norm: 0.00797994\n",
      "Epoch 3 | Step 2067500 | Avg Loss: 0.0150 | Grad Norm: 0.00953761\n",
      "Epoch 3 | Step 2067600 | Avg Loss: 0.0147 | Grad Norm: 0.00724476\n",
      "Epoch 3 | Step 2067700 | Avg Loss: 0.0149 | Grad Norm: 0.00790712\n",
      "Epoch 3 | Step 2067800 | Avg Loss: 0.0152 | Grad Norm: 0.00793299\n",
      "Epoch 3 | Step 2067900 | Avg Loss: 0.0153 | Grad Norm: 0.00823150\n",
      "Epoch 3 | Step 2068000 | Avg Loss: 0.0155 | Grad Norm: 0.00879940\n",
      "Epoch 3 | Step 2068100 | Avg Loss: 0.0156 | Grad Norm: 0.00733833\n",
      "Epoch 3 | Step 2068200 | Avg Loss: 0.0158 | Grad Norm: 0.00722253\n",
      "Epoch 3 | Step 2068300 | Avg Loss: 0.0161 | Grad Norm: 0.01180528\n",
      "Epoch 3 | Step 2068400 | Avg Loss: 0.0156 | Grad Norm: 0.00962452\n",
      "Epoch 3 | Step 2068500 | Avg Loss: 0.0160 | Grad Norm: 0.01046655\n",
      "Epoch 3 | Step 2068600 | Avg Loss: 0.0159 | Grad Norm: 0.00814060\n",
      "Epoch 3 | Step 2068700 | Avg Loss: 0.0157 | Grad Norm: 0.01103201\n",
      "Epoch 3 | Step 2068800 | Avg Loss: 0.0153 | Grad Norm: 0.00810653\n",
      "Epoch 3 | Step 2068900 | Avg Loss: 0.0152 | Grad Norm: 0.00862184\n",
      "Epoch 3 | Step 2069000 | Avg Loss: 0.0152 | Grad Norm: 0.00667861\n",
      "Epoch 3 | Step 2069100 | Avg Loss: 0.0152 | Grad Norm: 0.00835408\n",
      "Epoch 3 | Step 2069200 | Avg Loss: 0.0151 | Grad Norm: 0.00797440\n",
      "Epoch 3 | Step 2069300 | Avg Loss: 0.0154 | Grad Norm: 0.00633333\n",
      "Epoch 3 | Step 2069400 | Avg Loss: 0.0155 | Grad Norm: 0.00954060\n",
      "Epoch 3 | Step 2069500 | Avg Loss: 0.0157 | Grad Norm: 0.00782113\n",
      "Epoch 3 | Step 2069600 | Avg Loss: 0.0155 | Grad Norm: 0.00821892\n",
      "Epoch 3 | Step 2069700 | Avg Loss: 0.0151 | Grad Norm: 0.00671824\n",
      "Epoch 3 | Step 2069800 | Avg Loss: 0.0152 | Grad Norm: 0.00868387\n",
      "Epoch 3 | Step 2069900 | Avg Loss: 0.0148 | Grad Norm: 0.00925422\n",
      "Epoch 3 | Step 2070000 | Avg Loss: 0.0154 | Grad Norm: 0.00826045\n",
      "Epoch 3 | Step 2070100 | Avg Loss: 0.0155 | Grad Norm: 0.00774552\n",
      "Epoch 3 | Step 2070200 | Avg Loss: 0.0155 | Grad Norm: 0.00708782\n",
      "Epoch 3 | Step 2070300 | Avg Loss: 0.0156 | Grad Norm: 0.00827611\n",
      "Epoch 3 | Step 2070400 | Avg Loss: 0.0156 | Grad Norm: 0.00818192\n",
      "Epoch 3 | Step 2070500 | Avg Loss: 0.0157 | Grad Norm: 0.00990439\n",
      "Epoch 3 | Step 2070600 | Avg Loss: 0.0155 | Grad Norm: 0.00698572\n",
      "Epoch 3 | Step 2070700 | Avg Loss: 0.0153 | Grad Norm: 0.00802158\n",
      "Epoch 3 | Step 2070800 | Avg Loss: 0.0151 | Grad Norm: 0.00852615\n",
      "Epoch 3 | Step 2070900 | Avg Loss: 0.0149 | Grad Norm: 0.00791319\n",
      "Epoch 3 | Step 2071000 | Avg Loss: 0.0154 | Grad Norm: 0.00849953\n",
      "Epoch 3 | Step 2071100 | Avg Loss: 0.0152 | Grad Norm: 0.00785054\n",
      "Epoch 3 | Step 2071200 | Avg Loss: 0.0151 | Grad Norm: 0.00742252\n",
      "Epoch 3 | Step 2071300 | Avg Loss: 0.0152 | Grad Norm: 0.00770809\n",
      "Epoch 3 | Step 2071400 | Avg Loss: 0.0146 | Grad Norm: 0.00686106\n",
      "Epoch 3 | Step 2071500 | Avg Loss: 0.0147 | Grad Norm: 0.00776521\n",
      "Epoch 3 | Step 2071600 | Avg Loss: 0.0143 | Grad Norm: 0.00717815\n",
      "Epoch 3 | Step 2071700 | Avg Loss: 0.0145 | Grad Norm: 0.00736021\n",
      "Epoch 3 | Step 2071800 | Avg Loss: 0.0147 | Grad Norm: 0.00815688\n",
      "Epoch 3 | Step 2071900 | Avg Loss: 0.0147 | Grad Norm: 0.00734926\n",
      "Epoch 3 | Step 2072000 | Avg Loss: 0.0146 | Grad Norm: 0.00775011\n",
      "Epoch 3 | Step 2072100 | Avg Loss: 0.0142 | Grad Norm: 0.00807961\n",
      "Epoch 3 | Step 2072200 | Avg Loss: 0.0143 | Grad Norm: 0.00734386\n",
      "Epoch 3 | Step 2072300 | Avg Loss: 0.0144 | Grad Norm: 0.00770279\n",
      "Epoch 3 | Step 2072400 | Avg Loss: 0.0143 | Grad Norm: 0.00703055\n",
      "Epoch 3 | Step 2072500 | Avg Loss: 0.0139 | Grad Norm: 0.00692587\n",
      "Epoch 3 | Step 2072600 | Avg Loss: 0.0141 | Grad Norm: 0.00700441\n",
      "Epoch 3 | Step 2072700 | Avg Loss: 0.0145 | Grad Norm: 0.00767851\n",
      "Epoch 3 | Step 2072800 | Avg Loss: 0.0148 | Grad Norm: 0.00707464\n",
      "Epoch 3 | Step 2072900 | Avg Loss: 0.0151 | Grad Norm: 0.00900343\n",
      "Epoch 3 | Step 2073000 | Avg Loss: 0.0151 | Grad Norm: 0.00862650\n",
      "Epoch 3 | Step 2073100 | Avg Loss: 0.0153 | Grad Norm: 0.00796931\n",
      "Epoch 3 | Step 2073200 | Avg Loss: 0.0156 | Grad Norm: 0.00855026\n",
      "Epoch 3 | Step 2073300 | Avg Loss: 0.0153 | Grad Norm: 0.00771207\n",
      "Epoch 3 | Step 2073400 | Avg Loss: 0.0153 | Grad Norm: 0.00873853\n",
      "Epoch 3 | Step 2073500 | Avg Loss: 0.0153 | Grad Norm: 0.00765952\n",
      "Epoch 3 | Step 2073600 | Avg Loss: 0.0154 | Grad Norm: 0.00770605\n",
      "Epoch 3 | Step 2073700 | Avg Loss: 0.0157 | Grad Norm: 0.00995559\n",
      "Epoch 3 | Step 2073800 | Avg Loss: 0.0154 | Grad Norm: 0.00842916\n",
      "Epoch 3 | Step 2073900 | Avg Loss: 0.0156 | Grad Norm: 0.00863003\n",
      "Epoch 3 | Step 2074000 | Avg Loss: 0.0153 | Grad Norm: 0.00711139\n",
      "Epoch 3 | Step 2074100 | Avg Loss: 0.0154 | Grad Norm: 0.00890585\n",
      "Epoch 3 | Step 2074200 | Avg Loss: 0.0152 | Grad Norm: 0.00775424\n",
      "Epoch 3 | Step 2074300 | Avg Loss: 0.0150 | Grad Norm: 0.00802665\n",
      "Epoch 3 | Step 2074400 | Avg Loss: 0.0151 | Grad Norm: 0.00803828\n",
      "Epoch 3 | Step 2074500 | Avg Loss: 0.0152 | Grad Norm: 0.00769040\n",
      "Epoch 3 | Step 2074600 | Avg Loss: 0.0151 | Grad Norm: 0.00847876\n",
      "Epoch 3 | Step 2074700 | Avg Loss: 0.0152 | Grad Norm: 0.00847047\n",
      "Epoch 3 | Step 2074800 | Avg Loss: 0.0154 | Grad Norm: 0.00772721\n",
      "Epoch 3 | Step 2074900 | Avg Loss: 0.0155 | Grad Norm: 0.00755526\n",
      "Epoch 3 | Step 2075000 | Avg Loss: 0.0156 | Grad Norm: 0.00809240\n",
      "Epoch 3 | Step 2075100 | Avg Loss: 0.0155 | Grad Norm: 0.00899903\n",
      "Epoch 3 | Step 2075200 | Avg Loss: 0.0154 | Grad Norm: 0.00985920\n",
      "Epoch 3 | Step 2075300 | Avg Loss: 0.0148 | Grad Norm: 0.00776867\n",
      "Epoch 3 | Step 2075400 | Avg Loss: 0.0150 | Grad Norm: 0.00804089\n",
      "Epoch 3 | Step 2075500 | Avg Loss: 0.0155 | Grad Norm: 0.00846145\n",
      "Epoch 3 | Step 2075600 | Avg Loss: 0.0154 | Grad Norm: 0.00803866\n",
      "Epoch 3 | Step 2075700 | Avg Loss: 0.0152 | Grad Norm: 0.00803656\n",
      "Epoch 3 | Step 2075800 | Avg Loss: 0.0150 | Grad Norm: 0.00829391\n",
      "Epoch 3 | Step 2075900 | Avg Loss: 0.0146 | Grad Norm: 0.00937116\n",
      "Epoch 3 | Step 2076000 | Avg Loss: 0.0146 | Grad Norm: 0.00642649\n",
      "Epoch 3 | Step 2076100 | Avg Loss: 0.0145 | Grad Norm: 0.00832049\n",
      "Epoch 3 | Step 2076200 | Avg Loss: 0.0147 | Grad Norm: 0.00765818\n",
      "Epoch 3 | Step 2076300 | Avg Loss: 0.0151 | Grad Norm: 0.00761427\n",
      "Epoch 3 | Step 2076400 | Avg Loss: 0.0154 | Grad Norm: 0.00772691\n",
      "Epoch 3 | Step 2076500 | Avg Loss: 0.0154 | Grad Norm: 0.00836313\n",
      "Epoch 3 | Step 2076600 | Avg Loss: 0.0153 | Grad Norm: 0.00863535\n",
      "Epoch 3 | Step 2076700 | Avg Loss: 0.0152 | Grad Norm: 0.00695359\n",
      "Epoch 3 | Step 2076800 | Avg Loss: 0.0155 | Grad Norm: 0.00795723\n",
      "Epoch 3 | Step 2076900 | Avg Loss: 0.0156 | Grad Norm: 0.00849288\n",
      "Epoch 3 | Step 2077000 | Avg Loss: 0.0156 | Grad Norm: 0.00897994\n",
      "Epoch 3 | Step 2077100 | Avg Loss: 0.0155 | Grad Norm: 0.00831335\n",
      "Epoch 3 | Step 2077200 | Avg Loss: 0.0157 | Grad Norm: 0.00808423\n",
      "Epoch 3 | Step 2077300 | Avg Loss: 0.0159 | Grad Norm: 0.00798743\n",
      "Epoch 3 | Step 2077400 | Avg Loss: 0.0157 | Grad Norm: 0.00757469\n",
      "Epoch 3 | Step 2077500 | Avg Loss: 0.0159 | Grad Norm: 0.00824451\n",
      "Epoch 3 | Step 2077600 | Avg Loss: 0.0155 | Grad Norm: 0.00954877\n",
      "Epoch 3 | Step 2077700 | Avg Loss: 0.0158 | Grad Norm: 0.00786442\n",
      "Epoch 3 | Step 2077800 | Avg Loss: 0.0153 | Grad Norm: 0.01047444\n",
      "Epoch 3 | Step 2077900 | Avg Loss: 0.0153 | Grad Norm: 0.01523379\n",
      "Epoch 3 | Step 2078000 | Avg Loss: 0.0155 | Grad Norm: 0.01083026\n",
      "Epoch 3 | Step 2078100 | Avg Loss: 0.0159 | Grad Norm: 0.00825432\n",
      "Epoch 3 | Step 2078200 | Avg Loss: 0.0156 | Grad Norm: 0.00712959\n",
      "Epoch 3 | Step 2078300 | Avg Loss: 0.0156 | Grad Norm: 0.00733461\n",
      "Epoch 3 | Step 2078400 | Avg Loss: 0.0158 | Grad Norm: 0.00821530\n",
      "Epoch 3 | Step 2078500 | Avg Loss: 0.0153 | Grad Norm: 0.00830444\n",
      "Epoch 3 | Step 2078600 | Avg Loss: 0.0153 | Grad Norm: 0.00766237\n",
      "Epoch 3 | Step 2078700 | Avg Loss: 0.0153 | Grad Norm: 0.00868338\n",
      "Epoch 3 | Step 2078800 | Avg Loss: 0.0156 | Grad Norm: 0.00695640\n",
      "Epoch 3 | Step 2078900 | Avg Loss: 0.0153 | Grad Norm: 0.00804045\n",
      "Epoch 3 | Step 2079000 | Avg Loss: 0.0153 | Grad Norm: 0.00753213\n",
      "Epoch 3 | Step 2079100 | Avg Loss: 0.0157 | Grad Norm: 0.01100531\n",
      "Epoch 3 | Step 2079200 | Avg Loss: 0.0157 | Grad Norm: 0.00813262\n",
      "Epoch 3 | Step 2079300 | Avg Loss: 0.0155 | Grad Norm: 0.00707423\n",
      "Epoch 3 | Step 2079400 | Avg Loss: 0.0155 | Grad Norm: 0.00866917\n",
      "Epoch 3 | Step 2079500 | Avg Loss: 0.0155 | Grad Norm: 0.00808204\n",
      "Epoch 3 | Step 2079600 | Avg Loss: 0.0151 | Grad Norm: 0.00914742\n",
      "Epoch 3 | Step 2079700 | Avg Loss: 0.0150 | Grad Norm: 0.00912579\n",
      "Epoch 3 | Step 2079800 | Avg Loss: 0.0152 | Grad Norm: 0.00815909\n",
      "Epoch 3 | Step 2079900 | Avg Loss: 0.0149 | Grad Norm: 0.00744524\n",
      "Epoch 3 | Step 2080000 | Avg Loss: 0.0148 | Grad Norm: 0.00843759\n",
      "Epoch 3 | Step 2080100 | Avg Loss: 0.0150 | Grad Norm: 0.01151813\n",
      "Epoch 3 | Step 2080200 | Avg Loss: 0.0151 | Grad Norm: 0.00893700\n",
      "Epoch 3 | Step 2080300 | Avg Loss: 0.0145 | Grad Norm: 0.00734584\n",
      "Epoch 3 | Step 2080400 | Avg Loss: 0.0145 | Grad Norm: 0.00733891\n",
      "Epoch 3 | Step 2080500 | Avg Loss: 0.0144 | Grad Norm: 0.01194961\n",
      "Epoch 3 | Step 2080600 | Avg Loss: 0.0147 | Grad Norm: 0.00701656\n",
      "Epoch 3 | Step 2080700 | Avg Loss: 0.0150 | Grad Norm: 0.00910235\n",
      "Epoch 3 | Step 2080800 | Avg Loss: 0.0152 | Grad Norm: 0.00903420\n",
      "Epoch 3 | Step 2080900 | Avg Loss: 0.0153 | Grad Norm: 0.00822548\n",
      "Epoch 3 | Step 2081000 | Avg Loss: 0.0153 | Grad Norm: 0.00739226\n",
      "Epoch 3 | Step 2081100 | Avg Loss: 0.0152 | Grad Norm: 0.00725254\n",
      "Epoch 3 | Step 2081200 | Avg Loss: 0.0150 | Grad Norm: 0.00740543\n",
      "Epoch 3 | Step 2081300 | Avg Loss: 0.0148 | Grad Norm: 0.00745955\n",
      "Epoch 3 | Step 2081400 | Avg Loss: 0.0149 | Grad Norm: 0.00765648\n",
      "Epoch 3 | Step 2081500 | Avg Loss: 0.0153 | Grad Norm: 0.01118063\n",
      "Epoch 3 | Step 2081600 | Avg Loss: 0.0150 | Grad Norm: 0.00787124\n",
      "Epoch 3 | Step 2081700 | Avg Loss: 0.0147 | Grad Norm: 0.00883414\n",
      "Epoch 3 | Step 2081800 | Avg Loss: 0.0145 | Grad Norm: 0.00737164\n",
      "Epoch 3 | Step 2081900 | Avg Loss: 0.0146 | Grad Norm: 0.00757905\n",
      "Epoch 3 | Step 2082000 | Avg Loss: 0.0148 | Grad Norm: 0.00739899\n",
      "Epoch 3 | Step 2082100 | Avg Loss: 0.0146 | Grad Norm: 0.00906339\n",
      "Epoch 3 | Step 2082200 | Avg Loss: 0.0143 | Grad Norm: 0.00689226\n",
      "Epoch 3 | Step 2082300 | Avg Loss: 0.0146 | Grad Norm: 0.00775413\n",
      "Epoch 3 | Step 2082400 | Avg Loss: 0.0150 | Grad Norm: 0.00720728\n",
      "Epoch 3 | Step 2082500 | Avg Loss: 0.0152 | Grad Norm: 0.00718587\n",
      "Epoch 3 | Step 2082600 | Avg Loss: 0.0153 | Grad Norm: 0.00958280\n",
      "Epoch 3 | Step 2082700 | Avg Loss: 0.0152 | Grad Norm: 0.00854608\n",
      "Epoch 3 | Step 2082800 | Avg Loss: 0.0152 | Grad Norm: 0.00788101\n",
      "Epoch 3 | Step 2082900 | Avg Loss: 0.0154 | Grad Norm: 0.00760886\n",
      "Epoch 3 | Step 2083000 | Avg Loss: 0.0155 | Grad Norm: 0.00859290\n",
      "Epoch 3 | Step 2083100 | Avg Loss: 0.0151 | Grad Norm: 0.00820788\n",
      "Epoch 3 | Step 2083200 | Avg Loss: 0.0155 | Grad Norm: 0.00772044\n",
      "Epoch 3 | Step 2083300 | Avg Loss: 0.0157 | Grad Norm: 0.00753428\n",
      "Epoch 3 | Step 2083400 | Avg Loss: 0.0151 | Grad Norm: 0.00799674\n",
      "Epoch 3 | Step 2083500 | Avg Loss: 0.0148 | Grad Norm: 0.00893824\n",
      "Epoch 3 | Step 2083600 | Avg Loss: 0.0146 | Grad Norm: 0.00977467\n",
      "Epoch 3 | Step 2083700 | Avg Loss: 0.0143 | Grad Norm: 0.00763036\n",
      "Epoch 3 | Step 2083800 | Avg Loss: 0.0150 | Grad Norm: 0.00772916\n",
      "Epoch 3 | Step 2083900 | Avg Loss: 0.0149 | Grad Norm: 0.00757584\n",
      "Epoch 3 | Step 2084000 | Avg Loss: 0.0149 | Grad Norm: 0.00849172\n",
      "Epoch 3 | Step 2084100 | Avg Loss: 0.0151 | Grad Norm: 0.00862141\n",
      "Epoch 3 | Step 2084200 | Avg Loss: 0.0154 | Grad Norm: 0.00818236\n",
      "Epoch 3 | Step 2084300 | Avg Loss: 0.0152 | Grad Norm: 0.00810133\n",
      "Epoch 3 | Step 2084400 | Avg Loss: 0.0152 | Grad Norm: 0.01270757\n",
      "Epoch 3 | Step 2084500 | Avg Loss: 0.0155 | Grad Norm: 0.00935663\n",
      "Epoch 3 | Step 2084600 | Avg Loss: 0.0152 | Grad Norm: 0.00895268\n",
      "Epoch 3 | Step 2084700 | Avg Loss: 0.0147 | Grad Norm: 0.00720230\n",
      "Epoch 3 | Step 2084800 | Avg Loss: 0.0148 | Grad Norm: 0.00798021\n",
      "Epoch 3 | Step 2084900 | Avg Loss: 0.0147 | Grad Norm: 0.00653112\n",
      "Epoch 3 | Step 2085000 | Avg Loss: 0.0150 | Grad Norm: 0.00807617\n",
      "Epoch 3 | Step 2085100 | Avg Loss: 0.0150 | Grad Norm: 0.00801868\n",
      "Epoch 3 | Step 2085200 | Avg Loss: 0.0151 | Grad Norm: 0.00793261\n",
      "Epoch 3 | Step 2085300 | Avg Loss: 0.0149 | Grad Norm: 0.00858646\n",
      "Epoch 3 | Step 2085400 | Avg Loss: 0.0150 | Grad Norm: 0.00961891\n",
      "Epoch 3 | Step 2085500 | Avg Loss: 0.0153 | Grad Norm: 0.00814913\n",
      "Epoch 3 | Step 2085600 | Avg Loss: 0.0151 | Grad Norm: 0.00749285\n",
      "Epoch 3 | Step 2085700 | Avg Loss: 0.0149 | Grad Norm: 0.00885337\n",
      "Epoch 3 | Step 2085800 | Avg Loss: 0.0151 | Grad Norm: 0.00739023\n",
      "Epoch 3 | Step 2085900 | Avg Loss: 0.0152 | Grad Norm: 0.00713468\n",
      "Epoch 3 | Step 2086000 | Avg Loss: 0.0154 | Grad Norm: 0.00797984\n",
      "Epoch 3 | Step 2086100 | Avg Loss: 0.0156 | Grad Norm: 0.00835794\n",
      "Epoch 3 | Step 2086200 | Avg Loss: 0.0153 | Grad Norm: 0.00662418\n",
      "Epoch 3 | Step 2086300 | Avg Loss: 0.0150 | Grad Norm: 0.00766931\n",
      "Epoch 3 | Step 2086400 | Avg Loss: 0.0148 | Grad Norm: 0.00717901\n",
      "Epoch 3 | Step 2086500 | Avg Loss: 0.0148 | Grad Norm: 0.00790285\n",
      "Epoch 3 | Step 2086600 | Avg Loss: 0.0148 | Grad Norm: 0.00852987\n",
      "Epoch 3 | Step 2086700 | Avg Loss: 0.0146 | Grad Norm: 0.00718973\n",
      "Epoch 3 | Step 2086800 | Avg Loss: 0.0149 | Grad Norm: 0.00746264\n",
      "Epoch 3 | Step 2086900 | Avg Loss: 0.0148 | Grad Norm: 0.00876949\n",
      "Epoch 3 | Step 2087000 | Avg Loss: 0.0148 | Grad Norm: 0.00682398\n",
      "Epoch 3 | Step 2087100 | Avg Loss: 0.0146 | Grad Norm: 0.00749842\n",
      "Epoch 3 | Step 2087200 | Avg Loss: 0.0148 | Grad Norm: 0.00810213\n",
      "Epoch 3 | Step 2087300 | Avg Loss: 0.0150 | Grad Norm: 0.00737074\n",
      "Epoch 3 | Step 2087400 | Avg Loss: 0.0152 | Grad Norm: 0.00740404\n",
      "Epoch 3 | Step 2087500 | Avg Loss: 0.0153 | Grad Norm: 0.00706191\n",
      "Epoch 3 | Step 2087600 | Avg Loss: 0.0155 | Grad Norm: 0.00863932\n",
      "Epoch 3 | Step 2087700 | Avg Loss: 0.0153 | Grad Norm: 0.00692307\n",
      "Epoch 3 | Step 2087800 | Avg Loss: 0.0155 | Grad Norm: 0.00971449\n",
      "Epoch 3 | Step 2087900 | Avg Loss: 0.0153 | Grad Norm: 0.00730476\n",
      "Epoch 3 | Step 2088000 | Avg Loss: 0.0153 | Grad Norm: 0.00753994\n",
      "Epoch 3 | Step 2088100 | Avg Loss: 0.0156 | Grad Norm: 0.00893078\n",
      "Epoch 3 | Step 2088200 | Avg Loss: 0.0155 | Grad Norm: 0.00827684\n",
      "Epoch 3 | Step 2088300 | Avg Loss: 0.0154 | Grad Norm: 0.00710047\n",
      "Epoch 3 | Step 2088400 | Avg Loss: 0.0151 | Grad Norm: 0.00904514\n",
      "Epoch 3 | Step 2088500 | Avg Loss: 0.0148 | Grad Norm: 0.00732881\n",
      "Epoch 3 | Step 2088600 | Avg Loss: 0.0151 | Grad Norm: 0.00766558\n",
      "Epoch 3 | Step 2088700 | Avg Loss: 0.0151 | Grad Norm: 0.00758253\n",
      "Epoch 3 | Step 2088800 | Avg Loss: 0.0148 | Grad Norm: 0.00775649\n",
      "Epoch 3 | Step 2088900 | Avg Loss: 0.0147 | Grad Norm: 0.00706025\n",
      "Epoch 3 | Step 2089000 | Avg Loss: 0.0149 | Grad Norm: 0.00916041\n",
      "Epoch 3 | Step 2089100 | Avg Loss: 0.0149 | Grad Norm: 0.00810381\n",
      "Epoch 3 | Step 2089200 | Avg Loss: 0.0147 | Grad Norm: 0.00832649\n",
      "Epoch 3 | Step 2089300 | Avg Loss: 0.0148 | Grad Norm: 0.00703522\n",
      "Epoch 3 | Step 2089400 | Avg Loss: 0.0150 | Grad Norm: 0.00901982\n",
      "Epoch 3 | Step 2089500 | Avg Loss: 0.0152 | Grad Norm: 0.01047832\n",
      "Epoch 3 | Step 2089600 | Avg Loss: 0.0152 | Grad Norm: 0.00987215\n",
      "Epoch 3 | Step 2089700 | Avg Loss: 0.0156 | Grad Norm: 0.00959970\n",
      "Epoch 3 | Step 2089800 | Avg Loss: 0.0157 | Grad Norm: 0.01048802\n",
      "Epoch 3 | Step 2089900 | Avg Loss: 0.0157 | Grad Norm: 0.00845941\n",
      "Epoch 3 | Step 2090000 | Avg Loss: 0.0157 | Grad Norm: 0.00779011\n",
      "Epoch 3 | Step 2090100 | Avg Loss: 0.0157 | Grad Norm: 0.00793001\n",
      "Epoch 3 | Step 2090200 | Avg Loss: 0.0156 | Grad Norm: 0.00733828\n",
      "Epoch 3 | Step 2090300 | Avg Loss: 0.0156 | Grad Norm: 0.00741066\n",
      "Epoch 3 | Step 2090400 | Avg Loss: 0.0151 | Grad Norm: 0.00750141\n",
      "Epoch 3 | Step 2090500 | Avg Loss: 0.0151 | Grad Norm: 0.00881199\n",
      "Epoch 3 | Step 2090600 | Avg Loss: 0.0148 | Grad Norm: 0.00850936\n",
      "Epoch 3 | Step 2090700 | Avg Loss: 0.0146 | Grad Norm: 0.00861942\n",
      "Epoch 3 | Step 2090800 | Avg Loss: 0.0146 | Grad Norm: 0.00820082\n",
      "Epoch 3 | Step 2090900 | Avg Loss: 0.0147 | Grad Norm: 0.00740650\n",
      "Epoch 3 | Step 2091000 | Avg Loss: 0.0146 | Grad Norm: 0.00762513\n",
      "Epoch 3 | Step 2091100 | Avg Loss: 0.0145 | Grad Norm: 0.00800537\n",
      "Epoch 3 | Step 2091200 | Avg Loss: 0.0146 | Grad Norm: 0.00896051\n",
      "Epoch 3 | Step 2091300 | Avg Loss: 0.0149 | Grad Norm: 0.00755132\n",
      "Epoch 3 | Step 2091400 | Avg Loss: 0.0150 | Grad Norm: 0.01250315\n",
      "Epoch 3 | Step 2091500 | Avg Loss: 0.0152 | Grad Norm: 0.00858842\n",
      "Epoch 3 | Step 2091600 | Avg Loss: 0.0153 | Grad Norm: 0.00911285\n",
      "Epoch 3 | Step 2091700 | Avg Loss: 0.0155 | Grad Norm: 0.00827934\n",
      "Epoch 3 | Step 2091800 | Avg Loss: 0.0155 | Grad Norm: 0.00781273\n",
      "Epoch 3 | Step 2091900 | Avg Loss: 0.0155 | Grad Norm: 0.00921380\n",
      "Epoch 3 | Step 2092000 | Avg Loss: 0.0152 | Grad Norm: 0.00800435\n",
      "Epoch 3 | Step 2092100 | Avg Loss: 0.0148 | Grad Norm: 0.00836417\n",
      "Epoch 3 | Step 2092200 | Avg Loss: 0.0154 | Grad Norm: 0.00847335\n",
      "Epoch 3 | Step 2092300 | Avg Loss: 0.0153 | Grad Norm: 0.00644448\n",
      "Epoch 3 | Step 2092400 | Avg Loss: 0.0149 | Grad Norm: 0.00789773\n",
      "Epoch 3 | Step 2092500 | Avg Loss: 0.0150 | Grad Norm: 0.00781721\n",
      "Epoch 3 | Step 2092600 | Avg Loss: 0.0152 | Grad Norm: 0.00792171\n",
      "Epoch 3 | Step 2092700 | Avg Loss: 0.0155 | Grad Norm: 0.00787580\n",
      "Epoch 3 | Step 2092800 | Avg Loss: 0.0152 | Grad Norm: 0.00808956\n",
      "Epoch 3 | Step 2092900 | Avg Loss: 0.0150 | Grad Norm: 0.00800752\n",
      "Epoch 3 | Step 2093000 | Avg Loss: 0.0145 | Grad Norm: 0.00947393\n",
      "Epoch 3 | Step 2093100 | Avg Loss: 0.0152 | Grad Norm: 0.00911112\n",
      "Epoch 3 | Step 2093200 | Avg Loss: 0.0151 | Grad Norm: 0.00782591\n",
      "Epoch 3 | Step 2093300 | Avg Loss: 0.0151 | Grad Norm: 0.00803698\n",
      "Epoch 3 | Step 2093400 | Avg Loss: 0.0148 | Grad Norm: 0.00728287\n",
      "Epoch 3 | Step 2093500 | Avg Loss: 0.0148 | Grad Norm: 0.00759663\n",
      "Epoch 3 | Step 2093600 | Avg Loss: 0.0146 | Grad Norm: 0.01081568\n",
      "Epoch 3 | Step 2093700 | Avg Loss: 0.0145 | Grad Norm: 0.00730094\n",
      "Epoch 3 | Step 2093800 | Avg Loss: 0.0149 | Grad Norm: 0.00722723\n",
      "Epoch 3 | Step 2093900 | Avg Loss: 0.0146 | Grad Norm: 0.00744267\n",
      "Epoch 3 | Step 2094000 | Avg Loss: 0.0146 | Grad Norm: 0.00841570\n",
      "Epoch 3 | Step 2094100 | Avg Loss: 0.0147 | Grad Norm: 0.00698006\n",
      "Epoch 3 | Step 2094200 | Avg Loss: 0.0148 | Grad Norm: 0.00759240\n",
      "Epoch 3 | Step 2094300 | Avg Loss: 0.0150 | Grad Norm: 0.00905962\n",
      "Epoch 3 | Step 2094400 | Avg Loss: 0.0149 | Grad Norm: 0.00782725\n",
      "Epoch 3 | Step 2094500 | Avg Loss: 0.0152 | Grad Norm: 0.01051018\n",
      "Epoch 3 | Step 2094600 | Avg Loss: 0.0155 | Grad Norm: 0.00855303\n",
      "Epoch 3 | Step 2094700 | Avg Loss: 0.0155 | Grad Norm: 0.00780852\n",
      "Epoch 3 | Step 2094800 | Avg Loss: 0.0153 | Grad Norm: 0.00702884\n",
      "Epoch 3 | Step 2094900 | Avg Loss: 0.0153 | Grad Norm: 0.00894464\n",
      "Epoch 3 | Step 2095000 | Avg Loss: 0.0151 | Grad Norm: 0.00742554\n",
      "Epoch 3 | Step 2095100 | Avg Loss: 0.0150 | Grad Norm: 0.00877591\n",
      "Epoch 3 | Step 2095200 | Avg Loss: 0.0150 | Grad Norm: 0.00774822\n",
      "Epoch 3 | Step 2095300 | Avg Loss: 0.0145 | Grad Norm: 0.00781249\n",
      "Epoch 3 | Step 2095400 | Avg Loss: 0.0145 | Grad Norm: 0.00726585\n",
      "Epoch 3 | Step 2095500 | Avg Loss: 0.0144 | Grad Norm: 0.00679156\n",
      "Epoch 3 | Step 2095600 | Avg Loss: 0.0144 | Grad Norm: 0.00776384\n",
      "Epoch 3 | Step 2095700 | Avg Loss: 0.0147 | Grad Norm: 0.00707674\n",
      "Epoch 3 | Step 2095800 | Avg Loss: 0.0149 | Grad Norm: 0.00754726\n",
      "Epoch 3 | Step 2095900 | Avg Loss: 0.0153 | Grad Norm: 0.00679704\n",
      "Epoch 3 | Step 2096000 | Avg Loss: 0.0151 | Grad Norm: 0.00736309\n",
      "Epoch 3 | Step 2096100 | Avg Loss: 0.0148 | Grad Norm: 0.00930558\n",
      "Epoch 3 | Step 2096200 | Avg Loss: 0.0148 | Grad Norm: 0.00758911\n",
      "Epoch 3 | Step 2096300 | Avg Loss: 0.0147 | Grad Norm: 0.00771348\n",
      "Epoch 3 | Step 2096400 | Avg Loss: 0.0148 | Grad Norm: 0.00735134\n",
      "Epoch 3 | Step 2096500 | Avg Loss: 0.0151 | Grad Norm: 0.00678020\n",
      "Epoch 3 | Step 2096600 | Avg Loss: 0.0149 | Grad Norm: 0.00844158\n",
      "Epoch 3 | Step 2096700 | Avg Loss: 0.0147 | Grad Norm: 0.00892351\n",
      "Epoch 3 | Step 2096800 | Avg Loss: 0.0149 | Grad Norm: 0.00685815\n",
      "Epoch 3 | Step 2096900 | Avg Loss: 0.0149 | Grad Norm: 0.00764913\n",
      "Epoch 3 | Step 2097000 | Avg Loss: 0.0149 | Grad Norm: 0.00790532\n",
      "Epoch 3 | Step 2097100 | Avg Loss: 0.0148 | Grad Norm: 0.00759810\n",
      "Epoch 3 | Step 2097200 | Avg Loss: 0.0147 | Grad Norm: 0.00599012\n",
      "Epoch 3 | Step 2097300 | Avg Loss: 0.0147 | Grad Norm: 0.00789157\n",
      "Epoch 3 | Step 2097400 | Avg Loss: 0.0146 | Grad Norm: 0.00767725\n",
      "Epoch 3 | Step 2097500 | Avg Loss: 0.0148 | Grad Norm: 0.00875343\n",
      "Epoch 3 | Step 2097600 | Avg Loss: 0.0151 | Grad Norm: 0.00933482\n",
      "Epoch 3 | Step 2097700 | Avg Loss: 0.0151 | Grad Norm: 0.00775676\n",
      "Epoch 3 | Step 2097800 | Avg Loss: 0.0147 | Grad Norm: 0.00726355\n",
      "Epoch 3 | Step 2097900 | Avg Loss: 0.0145 | Grad Norm: 0.00849468\n",
      "Epoch 3 | Step 2098000 | Avg Loss: 0.0145 | Grad Norm: 0.00827215\n",
      "Epoch 3 | Step 2098100 | Avg Loss: 0.0147 | Grad Norm: 0.00826281\n",
      "Epoch 3 | Step 2098200 | Avg Loss: 0.0148 | Grad Norm: 0.00734014\n",
      "Epoch 3 | Step 2098300 | Avg Loss: 0.0148 | Grad Norm: 0.00707241\n",
      "Epoch 3 | Step 2098400 | Avg Loss: 0.0146 | Grad Norm: 0.00746623\n",
      "Epoch 3 | Step 2098500 | Avg Loss: 0.0145 | Grad Norm: 0.00717762\n",
      "Epoch 3 | Step 2098600 | Avg Loss: 0.0144 | Grad Norm: 0.00814659\n",
      "Epoch 3 | Step 2098700 | Avg Loss: 0.0146 | Grad Norm: 0.00755830\n",
      "Epoch 3 | Step 2098800 | Avg Loss: 0.0147 | Grad Norm: 0.00979119\n",
      "Epoch 3 | Step 2098900 | Avg Loss: 0.0148 | Grad Norm: 0.00714783\n",
      "Epoch 3 | Step 2099000 | Avg Loss: 0.0149 | Grad Norm: 0.00801067\n",
      "Epoch 3 | Step 2099100 | Avg Loss: 0.0150 | Grad Norm: 0.00998052\n",
      "Epoch 3 | Step 2099200 | Avg Loss: 0.0148 | Grad Norm: 0.00732704\n",
      "Epoch 3 | Step 2099300 | Avg Loss: 0.0144 | Grad Norm: 0.00680804\n",
      "Epoch 3 | Step 2099400 | Avg Loss: 0.0142 | Grad Norm: 0.00829539\n",
      "Epoch 3 | Step 2099500 | Avg Loss: 0.0142 | Grad Norm: 0.00760384\n",
      "Epoch 3 | Step 2099600 | Avg Loss: 0.0143 | Grad Norm: 0.00695582\n",
      "Epoch 3 | Step 2099700 | Avg Loss: 0.0146 | Grad Norm: 0.00899130\n",
      "Epoch 3 | Step 2099800 | Avg Loss: 0.0145 | Grad Norm: 0.00745915\n",
      "Epoch 3 | Step 2099900 | Avg Loss: 0.0147 | Grad Norm: 0.00687725\n",
      "Epoch 3 | Step 2100000 | Avg Loss: 0.0147 | Grad Norm: 0.00661105\n",
      "Saving model at step2100000\n",
      "Epoch 3 | Step 2100100 | Avg Loss: 0.0149 | Grad Norm: 0.00887377\n",
      "Epoch 3 | Step 2100200 | Avg Loss: 0.0147 | Grad Norm: 0.00788150\n",
      "Epoch 3 | Step 2100300 | Avg Loss: 0.0150 | Grad Norm: 0.00803471\n",
      "Epoch 3 | Step 2100400 | Avg Loss: 0.0149 | Grad Norm: 0.00915010\n",
      "Epoch 3 | Step 2100500 | Avg Loss: 0.0151 | Grad Norm: 0.00781430\n",
      "Epoch 3 | Step 2100600 | Avg Loss: 0.0152 | Grad Norm: 0.01001795\n",
      "Epoch 3 | Step 2100700 | Avg Loss: 0.0153 | Grad Norm: 0.00773788\n",
      "Epoch 3 | Step 2100800 | Avg Loss: 0.0152 | Grad Norm: 0.00884535\n",
      "Epoch 3 | Step 2100900 | Avg Loss: 0.0152 | Grad Norm: 0.00906502\n",
      "Epoch 3 | Step 2101000 | Avg Loss: 0.0154 | Grad Norm: 0.00729167\n",
      "Epoch 3 | Step 2101100 | Avg Loss: 0.0154 | Grad Norm: 0.00762946\n",
      "Epoch 3 | Step 2101200 | Avg Loss: 0.0156 | Grad Norm: 0.00752655\n",
      "Epoch 3 | Step 2101300 | Avg Loss: 0.0157 | Grad Norm: 0.00779494\n",
      "Epoch 3 | Step 2101400 | Avg Loss: 0.0156 | Grad Norm: 0.00783712\n",
      "Epoch 3 | Step 2101500 | Avg Loss: 0.0155 | Grad Norm: 0.00837650\n",
      "Epoch 3 | Step 2101600 | Avg Loss: 0.0156 | Grad Norm: 0.00747843\n",
      "Epoch 3 | Step 2101700 | Avg Loss: 0.0156 | Grad Norm: 0.00861509\n",
      "Epoch 3 | Step 2101800 | Avg Loss: 0.0156 | Grad Norm: 0.00907877\n",
      "Epoch 3 | Step 2101900 | Avg Loss: 0.0153 | Grad Norm: 0.00881804\n",
      "Epoch 3 | Step 2102000 | Avg Loss: 0.0152 | Grad Norm: 0.00748552\n",
      "Epoch 3 | Step 2102100 | Avg Loss: 0.0152 | Grad Norm: 0.00841998\n",
      "Epoch 3 | Step 2102200 | Avg Loss: 0.0150 | Grad Norm: 0.00883453\n",
      "Epoch 3 | Step 2102300 | Avg Loss: 0.0149 | Grad Norm: 0.00750385\n",
      "Epoch 3 | Step 2102400 | Avg Loss: 0.0147 | Grad Norm: 0.00778269\n",
      "Epoch 3 | Step 2102500 | Avg Loss: 0.0153 | Grad Norm: 0.00790462\n",
      "Epoch 3 | Step 2102600 | Avg Loss: 0.0154 | Grad Norm: 0.00765398\n",
      "Epoch 3 | Step 2102700 | Avg Loss: 0.0157 | Grad Norm: 0.00945075\n",
      "Epoch 3 | Step 2102800 | Avg Loss: 0.0158 | Grad Norm: 0.00761768\n",
      "Epoch 3 | Step 2102900 | Avg Loss: 0.0157 | Grad Norm: 0.00639064\n",
      "Epoch 3 | Step 2103000 | Avg Loss: 0.0158 | Grad Norm: 0.00715029\n",
      "Epoch 3 | Step 2103100 | Avg Loss: 0.0160 | Grad Norm: 0.00737498\n",
      "Epoch 3 | Step 2103200 | Avg Loss: 0.0159 | Grad Norm: 0.00899924\n",
      "Epoch 3 | Step 2103300 | Avg Loss: 0.0160 | Grad Norm: 0.00734035\n",
      "Epoch 3 | Step 2103400 | Avg Loss: 0.0162 | Grad Norm: 0.00781108\n",
      "Epoch 3 | Step 2103500 | Avg Loss: 0.0158 | Grad Norm: 0.01038598\n",
      "Epoch 3 | Step 2103600 | Avg Loss: 0.0156 | Grad Norm: 0.00859635\n",
      "Epoch 3 | Step 2103700 | Avg Loss: 0.0155 | Grad Norm: 0.00742874\n",
      "Epoch 3 | Step 2103800 | Avg Loss: 0.0158 | Grad Norm: 0.00888293\n",
      "Epoch 3 | Step 2103900 | Avg Loss: 0.0155 | Grad Norm: 0.00820616\n",
      "Epoch 3 | Step 2104000 | Avg Loss: 0.0154 | Grad Norm: 0.00750018\n",
      "Epoch 3 | Step 2104100 | Avg Loss: 0.0153 | Grad Norm: 0.00819311\n",
      "Epoch 3 | Step 2104200 | Avg Loss: 0.0151 | Grad Norm: 0.00842472\n",
      "Epoch 3 | Step 2104300 | Avg Loss: 0.0151 | Grad Norm: 0.00882587\n",
      "Epoch 3 | Step 2104400 | Avg Loss: 0.0149 | Grad Norm: 0.00840485\n",
      "Epoch 3 | Step 2104500 | Avg Loss: 0.0152 | Grad Norm: 0.00964608\n",
      "Epoch 3 | Step 2104600 | Avg Loss: 0.0152 | Grad Norm: 0.01011232\n",
      "Epoch 3 | Step 2104700 | Avg Loss: 0.0151 | Grad Norm: 0.00838610\n",
      "Epoch 3 | Step 2104800 | Avg Loss: 0.0151 | Grad Norm: 0.00745200\n",
      "Epoch 3 | Step 2104900 | Avg Loss: 0.0154 | Grad Norm: 0.00753040\n",
      "Epoch 3 | Step 2105000 | Avg Loss: 0.0153 | Grad Norm: 0.01003002\n",
      "Epoch 3 | Step 2105100 | Avg Loss: 0.0155 | Grad Norm: 0.00892414\n",
      "Epoch 3 | Step 2105200 | Avg Loss: 0.0155 | Grad Norm: 0.00686605\n",
      "Epoch 3 | Step 2105300 | Avg Loss: 0.0154 | Grad Norm: 0.00757061\n",
      "Epoch 3 | Step 2105400 | Avg Loss: 0.0153 | Grad Norm: 0.00748246\n",
      "Epoch 3 | Step 2105500 | Avg Loss: 0.0155 | Grad Norm: 0.00808790\n",
      "Epoch 3 | Step 2105600 | Avg Loss: 0.0154 | Grad Norm: 0.00698528\n",
      "Epoch 3 | Step 2105700 | Avg Loss: 0.0156 | Grad Norm: 0.00928132\n",
      "Epoch 3 | Step 2105800 | Avg Loss: 0.0156 | Grad Norm: 0.00770890\n",
      "Epoch 3 | Step 2105900 | Avg Loss: 0.0153 | Grad Norm: 0.00776802\n",
      "Epoch 3 | Step 2106000 | Avg Loss: 0.0152 | Grad Norm: 0.00669552\n",
      "Epoch 3 | Step 2106100 | Avg Loss: 0.0150 | Grad Norm: 0.00688422\n",
      "Epoch 3 | Step 2106200 | Avg Loss: 0.0151 | Grad Norm: 0.01047851\n",
      "Epoch 3 | Step 2106300 | Avg Loss: 0.0150 | Grad Norm: 0.00829599\n",
      "Epoch 3 | Step 2106400 | Avg Loss: 0.0150 | Grad Norm: 0.00753802\n",
      "Epoch 3 | Step 2106500 | Avg Loss: 0.0151 | Grad Norm: 0.00783148\n",
      "Epoch 3 | Step 2106600 | Avg Loss: 0.0152 | Grad Norm: 0.00803894\n",
      "Epoch 3 | Step 2106700 | Avg Loss: 0.0151 | Grad Norm: 0.00995481\n",
      "Epoch 3 | Step 2106800 | Avg Loss: 0.0154 | Grad Norm: 0.00810873\n",
      "Epoch 3 | Step 2106900 | Avg Loss: 0.0156 | Grad Norm: 0.00770016\n",
      "Epoch 3 | Step 2107000 | Avg Loss: 0.0152 | Grad Norm: 0.00781626\n",
      "Epoch 3 | Step 2107100 | Avg Loss: 0.0153 | Grad Norm: 0.00761389\n",
      "Epoch 3 | Step 2107200 | Avg Loss: 0.0152 | Grad Norm: 0.00772695\n",
      "Epoch 3 | Step 2107300 | Avg Loss: 0.0152 | Grad Norm: 0.00905327\n",
      "Epoch 3 | Step 2107400 | Avg Loss: 0.0151 | Grad Norm: 0.00855921\n",
      "Epoch 3 | Step 2107500 | Avg Loss: 0.0151 | Grad Norm: 0.00799665\n",
      "Epoch 3 | Step 2107600 | Avg Loss: 0.0154 | Grad Norm: 0.00778919\n",
      "Epoch 3 | Step 2107700 | Avg Loss: 0.0155 | Grad Norm: 0.00843679\n",
      "Epoch 3 | Step 2107800 | Avg Loss: 0.0156 | Grad Norm: 0.00728994\n",
      "Epoch 3 | Step 2107900 | Avg Loss: 0.0155 | Grad Norm: 0.00814031\n",
      "Epoch 3 | Step 2108000 | Avg Loss: 0.0152 | Grad Norm: 0.00733762\n",
      "Epoch 3 | Step 2108100 | Avg Loss: 0.0150 | Grad Norm: 0.00737388\n",
      "Epoch 3 | Step 2108200 | Avg Loss: 0.0147 | Grad Norm: 0.00794450\n",
      "Epoch 3 | Step 2108300 | Avg Loss: 0.0146 | Grad Norm: 0.00819175\n",
      "Epoch 3 | Step 2108400 | Avg Loss: 0.0144 | Grad Norm: 0.01055506\n",
      "Epoch 3 | Step 2108500 | Avg Loss: 0.0149 | Grad Norm: 0.00900323\n",
      "Epoch 3 | Step 2108600 | Avg Loss: 0.0147 | Grad Norm: 0.00762539\n",
      "Epoch 3 | Step 2108700 | Avg Loss: 0.0149 | Grad Norm: 0.00898339\n",
      "Epoch 3 | Step 2108800 | Avg Loss: 0.0149 | Grad Norm: 0.00867733\n",
      "Epoch 3 | Step 2108900 | Avg Loss: 0.0147 | Grad Norm: 0.00730731\n",
      "Epoch 3 | Step 2109000 | Avg Loss: 0.0146 | Grad Norm: 0.00772193\n",
      "Epoch 3 | Step 2109100 | Avg Loss: 0.0145 | Grad Norm: 0.00734702\n",
      "Epoch 3 | Step 2109200 | Avg Loss: 0.0149 | Grad Norm: 0.00786305\n",
      "Epoch 3 | Step 2109300 | Avg Loss: 0.0150 | Grad Norm: 0.00737206\n",
      "Epoch 3 | Step 2109400 | Avg Loss: 0.0148 | Grad Norm: 0.00841013\n",
      "Epoch 3 | Step 2109500 | Avg Loss: 0.0146 | Grad Norm: 0.00826861\n",
      "Epoch 3 | Step 2109600 | Avg Loss: 0.0144 | Grad Norm: 0.00755202\n",
      "Epoch 3 | Step 2109700 | Avg Loss: 0.0149 | Grad Norm: 0.00950820\n",
      "Epoch 3 | Step 2109800 | Avg Loss: 0.0150 | Grad Norm: 0.00982797\n",
      "Epoch 3 | Step 2109900 | Avg Loss: 0.0152 | Grad Norm: 0.00963303\n",
      "Epoch 3 | Step 2110000 | Avg Loss: 0.0153 | Grad Norm: 0.00855654\n",
      "Epoch 3 | Step 2110100 | Avg Loss: 0.0150 | Grad Norm: 0.00745127\n",
      "Epoch 3 | Step 2110200 | Avg Loss: 0.0148 | Grad Norm: 0.00908889\n",
      "Epoch 3 | Step 2110300 | Avg Loss: 0.0148 | Grad Norm: 0.00854720\n",
      "Epoch 3 | Step 2110400 | Avg Loss: 0.0153 | Grad Norm: 0.00900803\n",
      "Epoch 3 | Step 2110500 | Avg Loss: 0.0154 | Grad Norm: 0.00825247\n",
      "Epoch 3 | Step 2110600 | Avg Loss: 0.0152 | Grad Norm: 0.01026966\n",
      "Epoch 3 | Step 2110700 | Avg Loss: 0.0153 | Grad Norm: 0.00758307\n",
      "Epoch 3 | Step 2110800 | Avg Loss: 0.0148 | Grad Norm: 0.00816462\n",
      "Epoch 3 | Step 2110900 | Avg Loss: 0.0153 | Grad Norm: 0.00840975\n",
      "Epoch 3 | Step 2111000 | Avg Loss: 0.0152 | Grad Norm: 0.00766207\n",
      "Epoch 3 | Step 2111100 | Avg Loss: 0.0151 | Grad Norm: 0.00875349\n",
      "Epoch 3 | Step 2111200 | Avg Loss: 0.0150 | Grad Norm: 0.00756255\n",
      "Epoch 3 | Step 2111300 | Avg Loss: 0.0157 | Grad Norm: 0.00872053\n",
      "Epoch 3 | Step 2111400 | Avg Loss: 0.0154 | Grad Norm: 0.00765844\n",
      "Epoch 3 | Step 2111500 | Avg Loss: 0.0155 | Grad Norm: 0.00811775\n",
      "Epoch 3 | Step 2111600 | Avg Loss: 0.0155 | Grad Norm: 0.00832548\n",
      "Epoch 3 | Step 2111700 | Avg Loss: 0.0151 | Grad Norm: 0.00903375\n",
      "Epoch 3 | Step 2111800 | Avg Loss: 0.0153 | Grad Norm: 0.00809970\n",
      "Epoch 3 | Step 2111900 | Avg Loss: 0.0152 | Grad Norm: 0.00783092\n",
      "Epoch 3 | Step 2112000 | Avg Loss: 0.0153 | Grad Norm: 0.00860935\n",
      "Epoch 3 | Step 2112100 | Avg Loss: 0.0153 | Grad Norm: 0.00862991\n",
      "Epoch 3 | Step 2112200 | Avg Loss: 0.0149 | Grad Norm: 0.00670587\n",
      "Epoch 3 | Step 2112300 | Avg Loss: 0.0152 | Grad Norm: 0.00938585\n",
      "Epoch 3 | Step 2112400 | Avg Loss: 0.0151 | Grad Norm: 0.00805053\n",
      "Epoch 3 | Step 2112500 | Avg Loss: 0.0149 | Grad Norm: 0.00861173\n",
      "Epoch 3 | Step 2112600 | Avg Loss: 0.0149 | Grad Norm: 0.00714260\n",
      "Epoch 3 | Step 2112700 | Avg Loss: 0.0148 | Grad Norm: 0.00709785\n",
      "Epoch 3 | Step 2112800 | Avg Loss: 0.0146 | Grad Norm: 0.00755858\n",
      "Epoch 3 | Step 2112900 | Avg Loss: 0.0147 | Grad Norm: 0.00798767\n",
      "Epoch 3 | Step 2113000 | Avg Loss: 0.0151 | Grad Norm: 0.00788708\n",
      "Epoch 3 | Step 2113100 | Avg Loss: 0.0156 | Grad Norm: 0.00972091\n",
      "Epoch 3 | Step 2113200 | Avg Loss: 0.0154 | Grad Norm: 0.00704475\n",
      "Epoch 3 | Step 2113300 | Avg Loss: 0.0153 | Grad Norm: 0.00690528\n",
      "Epoch 3 | Step 2113400 | Avg Loss: 0.0151 | Grad Norm: 0.00794693\n",
      "Epoch 3 | Step 2113500 | Avg Loss: 0.0152 | Grad Norm: 0.00740728\n",
      "Epoch 3 | Step 2113600 | Avg Loss: 0.0152 | Grad Norm: 0.00748464\n",
      "Epoch 3 | Step 2113700 | Avg Loss: 0.0149 | Grad Norm: 0.00857375\n",
      "Epoch 3 | Step 2113800 | Avg Loss: 0.0148 | Grad Norm: 0.00797208\n",
      "Epoch 3 | Step 2113900 | Avg Loss: 0.0147 | Grad Norm: 0.00821419\n",
      "Epoch 3 | Step 2114000 | Avg Loss: 0.0148 | Grad Norm: 0.00824418\n",
      "Epoch 3 | Step 2114100 | Avg Loss: 0.0152 | Grad Norm: 0.00882338\n",
      "Epoch 3 | Step 2114200 | Avg Loss: 0.0153 | Grad Norm: 0.00759653\n",
      "Epoch 3 | Step 2114300 | Avg Loss: 0.0152 | Grad Norm: 0.00769148\n",
      "Epoch 3 | Step 2114400 | Avg Loss: 0.0154 | Grad Norm: 0.00727843\n",
      "Epoch 3 | Step 2114500 | Avg Loss: 0.0149 | Grad Norm: 0.00825564\n",
      "Epoch 3 | Step 2114600 | Avg Loss: 0.0151 | Grad Norm: 0.00787601\n",
      "Epoch 3 | Step 2114700 | Avg Loss: 0.0150 | Grad Norm: 0.00724012\n",
      "Epoch 3 | Step 2114800 | Avg Loss: 0.0150 | Grad Norm: 0.00783606\n",
      "Epoch 3 | Step 2114900 | Avg Loss: 0.0149 | Grad Norm: 0.00733823\n",
      "Epoch 3 | Step 2115000 | Avg Loss: 0.0147 | Grad Norm: 0.00803405\n",
      "Epoch 3 | Step 2115100 | Avg Loss: 0.0152 | Grad Norm: 0.00815256\n",
      "Epoch 3 | Step 2115200 | Avg Loss: 0.0153 | Grad Norm: 0.00878687\n",
      "Epoch 3 | Step 2115300 | Avg Loss: 0.0157 | Grad Norm: 0.00790326\n",
      "Epoch 3 | Step 2115400 | Avg Loss: 0.0152 | Grad Norm: 0.01252920\n",
      "Epoch 3 | Step 2115500 | Avg Loss: 0.0151 | Grad Norm: 0.00751018\n",
      "Epoch 3 | Step 2115600 | Avg Loss: 0.0149 | Grad Norm: 0.00750766\n",
      "Epoch 3 | Step 2115700 | Avg Loss: 0.0152 | Grad Norm: 0.00883654\n",
      "Epoch 3 | Step 2115800 | Avg Loss: 0.0152 | Grad Norm: 0.00848337\n",
      "Epoch 3 | Step 2115900 | Avg Loss: 0.0157 | Grad Norm: 0.00725972\n",
      "Epoch 3 | Step 2116000 | Avg Loss: 0.0155 | Grad Norm: 0.00848297\n",
      "Epoch 3 | Step 2116100 | Avg Loss: 0.0153 | Grad Norm: 0.00893651\n",
      "Epoch 3 | Step 2116200 | Avg Loss: 0.0152 | Grad Norm: 0.00941173\n",
      "Epoch 3 | Step 2116300 | Avg Loss: 0.0150 | Grad Norm: 0.00810420\n",
      "Epoch 3 | Step 2116400 | Avg Loss: 0.0150 | Grad Norm: 0.00762629\n",
      "Epoch 3 | Step 2116500 | Avg Loss: 0.0148 | Grad Norm: 0.00897895\n",
      "Epoch 3 | Step 2116600 | Avg Loss: 0.0149 | Grad Norm: 0.00853866\n",
      "Epoch 3 | Step 2116700 | Avg Loss: 0.0150 | Grad Norm: 0.00769999\n",
      "Epoch 3 | Step 2116800 | Avg Loss: 0.0150 | Grad Norm: 0.00855061\n",
      "Epoch 3 | Step 2116900 | Avg Loss: 0.0153 | Grad Norm: 0.00888953\n",
      "Epoch 3 | Step 2117000 | Avg Loss: 0.0154 | Grad Norm: 0.00865371\n",
      "Epoch 3 | Step 2117100 | Avg Loss: 0.0154 | Grad Norm: 0.00805334\n",
      "Epoch 3 | Step 2117200 | Avg Loss: 0.0152 | Grad Norm: 0.00813551\n",
      "Epoch 3 | Step 2117300 | Avg Loss: 0.0146 | Grad Norm: 0.00799680\n",
      "Epoch 3 | Step 2117400 | Avg Loss: 0.0152 | Grad Norm: 0.00864468\n",
      "Epoch 3 | Step 2117500 | Avg Loss: 0.0152 | Grad Norm: 0.00680456\n",
      "Epoch 3 | Step 2117600 | Avg Loss: 0.0150 | Grad Norm: 0.00861488\n",
      "Epoch 3 | Step 2117700 | Avg Loss: 0.0152 | Grad Norm: 0.00718851\n",
      "Epoch 3 | Step 2117800 | Avg Loss: 0.0152 | Grad Norm: 0.00739632\n",
      "Epoch 3 | Step 2117900 | Avg Loss: 0.0152 | Grad Norm: 0.00855298\n",
      "Epoch 3 | Step 2118000 | Avg Loss: 0.0153 | Grad Norm: 0.00861544\n",
      "Epoch 3 | Step 2118100 | Avg Loss: 0.0153 | Grad Norm: 0.00700647\n",
      "Epoch 3 | Step 2118200 | Avg Loss: 0.0152 | Grad Norm: 0.00905158\n",
      "Epoch 3 | Step 2118300 | Avg Loss: 0.0152 | Grad Norm: 0.00930257\n",
      "Epoch 3 | Step 2118400 | Avg Loss: 0.0153 | Grad Norm: 0.00894356\n",
      "Epoch 3 | Step 2118500 | Avg Loss: 0.0155 | Grad Norm: 0.01117587\n",
      "Epoch 3 | Step 2118600 | Avg Loss: 0.0155 | Grad Norm: 0.00890155\n",
      "Epoch 3 | Step 2118700 | Avg Loss: 0.0154 | Grad Norm: 0.00788539\n",
      "Epoch 3 | Step 2118800 | Avg Loss: 0.0150 | Grad Norm: 0.00809360\n",
      "Epoch 3 | Step 2118900 | Avg Loss: 0.0152 | Grad Norm: 0.00813459\n",
      "Epoch 3 | Step 2119000 | Avg Loss: 0.0155 | Grad Norm: 0.00818790\n",
      "Epoch 3 | Step 2119100 | Avg Loss: 0.0155 | Grad Norm: 0.00817035\n",
      "Epoch 3 | Step 2119200 | Avg Loss: 0.0151 | Grad Norm: 0.00748711\n",
      "Epoch 3 | Step 2119300 | Avg Loss: 0.0153 | Grad Norm: 0.00945587\n",
      "Epoch 3 | Step 2119400 | Avg Loss: 0.0153 | Grad Norm: 0.00931015\n",
      "Epoch 3 | Step 2119500 | Avg Loss: 0.0151 | Grad Norm: 0.00733948\n",
      "Epoch 3 | Step 2119600 | Avg Loss: 0.0152 | Grad Norm: 0.00950466\n",
      "Epoch 3 | Step 2119700 | Avg Loss: 0.0152 | Grad Norm: 0.00898719\n",
      "Epoch 3 | Step 2119800 | Avg Loss: 0.0154 | Grad Norm: 0.00775731\n",
      "Epoch 3 | Step 2119900 | Avg Loss: 0.0156 | Grad Norm: 0.00998227\n",
      "Epoch 3 | Step 2120000 | Avg Loss: 0.0158 | Grad Norm: 0.00945942\n",
      "Epoch 3 | Step 2120100 | Avg Loss: 0.0161 | Grad Norm: 0.00779164\n",
      "Epoch 3 | Step 2120200 | Avg Loss: 0.0154 | Grad Norm: 0.00885484\n",
      "Epoch 3 | Step 2120300 | Avg Loss: 0.0154 | Grad Norm: 0.00890893\n",
      "Epoch 3 | Step 2120400 | Avg Loss: 0.0154 | Grad Norm: 0.00779164\n",
      "Epoch 3 | Step 2120500 | Avg Loss: 0.0151 | Grad Norm: 0.00775337\n",
      "Epoch 3 | Step 2120600 | Avg Loss: 0.0152 | Grad Norm: 0.00727768\n",
      "Epoch 3 | Step 2120700 | Avg Loss: 0.0152 | Grad Norm: 0.00901478\n",
      "Epoch 3 | Step 2120800 | Avg Loss: 0.0154 | Grad Norm: 0.00796669\n",
      "Epoch 3 | Step 2120900 | Avg Loss: 0.0152 | Grad Norm: 0.00889638\n",
      "Epoch 3 | Step 2121000 | Avg Loss: 0.0151 | Grad Norm: 0.00907799\n",
      "Epoch 3 | Step 2121100 | Avg Loss: 0.0152 | Grad Norm: 0.00842502\n",
      "Epoch 3 | Step 2121200 | Avg Loss: 0.0152 | Grad Norm: 0.00735241\n",
      "Epoch 3 | Step 2121300 | Avg Loss: 0.0153 | Grad Norm: 0.00815274\n",
      "Epoch 3 | Step 2121400 | Avg Loss: 0.0151 | Grad Norm: 0.00892930\n",
      "Epoch 3 | Step 2121500 | Avg Loss: 0.0153 | Grad Norm: 0.00800633\n",
      "Epoch 3 | Step 2121600 | Avg Loss: 0.0155 | Grad Norm: 0.00901977\n",
      "Epoch 3 | Step 2121700 | Avg Loss: 0.0153 | Grad Norm: 0.00746948\n",
      "Epoch 3 | Step 2121800 | Avg Loss: 0.0152 | Grad Norm: 0.00698987\n",
      "Epoch 3 | Step 2121900 | Avg Loss: 0.0155 | Grad Norm: 0.00931156\n",
      "Epoch 3 | Step 2122000 | Avg Loss: 0.0151 | Grad Norm: 0.00803743\n",
      "Epoch 3 | Step 2122100 | Avg Loss: 0.0154 | Grad Norm: 0.00879917\n",
      "Epoch 3 | Step 2122200 | Avg Loss: 0.0154 | Grad Norm: 0.00726276\n",
      "Epoch 3 | Step 2122300 | Avg Loss: 0.0155 | Grad Norm: 0.00787659\n",
      "Epoch 3 | Step 2122400 | Avg Loss: 0.0151 | Grad Norm: 0.00661096\n",
      "Epoch 3 | Step 2122500 | Avg Loss: 0.0152 | Grad Norm: 0.00678272\n",
      "Epoch 3 | Step 2122600 | Avg Loss: 0.0154 | Grad Norm: 0.00741224\n",
      "Epoch 3 | Step 2122700 | Avg Loss: 0.0154 | Grad Norm: 0.00976761\n",
      "Epoch 3 | Step 2122800 | Avg Loss: 0.0151 | Grad Norm: 0.00964829\n",
      "Epoch 3 | Step 2122900 | Avg Loss: 0.0149 | Grad Norm: 0.00824990\n",
      "Epoch 3 | Step 2123000 | Avg Loss: 0.0149 | Grad Norm: 0.00702727\n",
      "Epoch 3 | Step 2123100 | Avg Loss: 0.0149 | Grad Norm: 0.00738410\n",
      "Epoch 3 | Step 2123200 | Avg Loss: 0.0146 | Grad Norm: 0.00814127\n",
      "Epoch 3 | Step 2123300 | Avg Loss: 0.0145 | Grad Norm: 0.00681020\n",
      "Epoch 3 | Step 2123400 | Avg Loss: 0.0143 | Grad Norm: 0.00902385\n",
      "Epoch 3 | Step 2123500 | Avg Loss: 0.0145 | Grad Norm: 0.00731014\n",
      "Epoch 3 | Step 2123600 | Avg Loss: 0.0150 | Grad Norm: 0.00837694\n",
      "Epoch 3 | Step 2123700 | Avg Loss: 0.0155 | Grad Norm: 0.00776977\n",
      "Epoch 3 | Step 2123800 | Avg Loss: 0.0150 | Grad Norm: 0.00919686\n",
      "Epoch 3 | Step 2123900 | Avg Loss: 0.0148 | Grad Norm: 0.00708862\n",
      "Epoch 3 | Step 2124000 | Avg Loss: 0.0146 | Grad Norm: 0.01056421\n",
      "Epoch 3 | Step 2124100 | Avg Loss: 0.0152 | Grad Norm: 0.00708487\n",
      "Epoch 3 | Step 2124200 | Avg Loss: 0.0150 | Grad Norm: 0.00904724\n",
      "Epoch 3 | Step 2124300 | Avg Loss: 0.0152 | Grad Norm: 0.00733790\n",
      "Epoch 3 | Step 2124400 | Avg Loss: 0.0149 | Grad Norm: 0.00868014\n",
      "Epoch 3 | Step 2124500 | Avg Loss: 0.0152 | Grad Norm: 0.00797229\n",
      "Epoch 3 | Step 2124600 | Avg Loss: 0.0150 | Grad Norm: 0.00689295\n",
      "Epoch 3 | Step 2124700 | Avg Loss: 0.0152 | Grad Norm: 0.00854919\n",
      "Epoch 3 | Step 2124800 | Avg Loss: 0.0153 | Grad Norm: 0.00757025\n",
      "Epoch 3 | Step 2124900 | Avg Loss: 0.0149 | Grad Norm: 0.00763063\n",
      "Epoch 3 | Step 2125000 | Avg Loss: 0.0152 | Grad Norm: 0.01034636\n",
      "Epoch 3 | Step 2125100 | Avg Loss: 0.0148 | Grad Norm: 0.00697967\n",
      "Epoch 3 | Step 2125200 | Avg Loss: 0.0153 | Grad Norm: 0.00898019\n",
      "Epoch 3 | Step 2125300 | Avg Loss: 0.0154 | Grad Norm: 0.00816867\n",
      "Epoch 3 | Step 2125400 | Avg Loss: 0.0159 | Grad Norm: 0.00756627\n",
      "Epoch 3 | Step 2125500 | Avg Loss: 0.0158 | Grad Norm: 0.00758448\n",
      "Epoch 3 | Step 2125600 | Avg Loss: 0.0153 | Grad Norm: 0.00691611\n",
      "Epoch 3 | Step 2125700 | Avg Loss: 0.0151 | Grad Norm: 0.00774922\n",
      "Epoch 3 | Step 2125800 | Avg Loss: 0.0150 | Grad Norm: 0.00752288\n",
      "Epoch 3 | Step 2125900 | Avg Loss: 0.0147 | Grad Norm: 0.00742792\n",
      "Epoch 3 | Step 2126000 | Avg Loss: 0.0146 | Grad Norm: 0.00704268\n",
      "Epoch 3 | Step 2126100 | Avg Loss: 0.0146 | Grad Norm: 0.00750781\n",
      "Epoch 3 | Step 2126200 | Avg Loss: 0.0151 | Grad Norm: 0.00885451\n",
      "Epoch 3 | Step 2126300 | Avg Loss: 0.0148 | Grad Norm: 0.00719953\n",
      "Epoch 3 | Step 2126400 | Avg Loss: 0.0149 | Grad Norm: 0.00777827\n",
      "Epoch 3 | Step 2126500 | Avg Loss: 0.0148 | Grad Norm: 0.00868179\n",
      "Epoch 3 | Step 2126600 | Avg Loss: 0.0150 | Grad Norm: 0.00949681\n",
      "Epoch 3 | Step 2126700 | Avg Loss: 0.0151 | Grad Norm: 0.00823194\n",
      "Epoch 3 | Step 2126800 | Avg Loss: 0.0148 | Grad Norm: 0.00793613\n",
      "Epoch 3 | Step 2126900 | Avg Loss: 0.0151 | Grad Norm: 0.01024808\n",
      "Epoch 3 | Step 2127000 | Avg Loss: 0.0151 | Grad Norm: 0.00738373\n",
      "Epoch 3 | Step 2127100 | Avg Loss: 0.0147 | Grad Norm: 0.00757894\n",
      "Epoch 3 | Step 2127200 | Avg Loss: 0.0152 | Grad Norm: 0.00827431\n",
      "Epoch 3 | Step 2127300 | Avg Loss: 0.0149 | Grad Norm: 0.00663632\n",
      "Epoch 3 | Step 2127400 | Avg Loss: 0.0146 | Grad Norm: 0.00741398\n",
      "Epoch 3 | Step 2127500 | Avg Loss: 0.0150 | Grad Norm: 0.00847589\n",
      "Epoch 3 | Step 2127600 | Avg Loss: 0.0155 | Grad Norm: 0.00863133\n",
      "Epoch 3 | Step 2127700 | Avg Loss: 0.0158 | Grad Norm: 0.00794104\n",
      "Epoch 3 | Step 2127800 | Avg Loss: 0.0153 | Grad Norm: 0.00880875\n",
      "Epoch 3 | Step 2127900 | Avg Loss: 0.0152 | Grad Norm: 0.00790188\n",
      "Epoch 3 | Step 2128000 | Avg Loss: 0.0152 | Grad Norm: 0.00949139\n",
      "Epoch 3 | Step 2128100 | Avg Loss: 0.0150 | Grad Norm: 0.00788969\n",
      "Epoch 3 | Step 2128200 | Avg Loss: 0.0150 | Grad Norm: 0.01037268\n",
      "Epoch 3 | Step 2128300 | Avg Loss: 0.0150 | Grad Norm: 0.00805861\n",
      "Epoch 3 | Step 2128400 | Avg Loss: 0.0151 | Grad Norm: 0.00774176\n",
      "Epoch 3 | Step 2128500 | Avg Loss: 0.0149 | Grad Norm: 0.00974474\n",
      "Epoch 3 | Step 2128600 | Avg Loss: 0.0150 | Grad Norm: 0.00827550\n",
      "Epoch 3 | Step 2128700 | Avg Loss: 0.0150 | Grad Norm: 0.00937694\n",
      "Epoch 3 | Step 2128800 | Avg Loss: 0.0151 | Grad Norm: 0.00871810\n",
      "Epoch 3 | Step 2128900 | Avg Loss: 0.0153 | Grad Norm: 0.00880932\n",
      "Epoch 3 | Step 2129000 | Avg Loss: 0.0153 | Grad Norm: 0.00787455\n",
      "Epoch 3 | Step 2129100 | Avg Loss: 0.0153 | Grad Norm: 0.00700436\n",
      "Epoch 3 | Step 2129200 | Avg Loss: 0.0150 | Grad Norm: 0.00860304\n",
      "Epoch 3 | Step 2129300 | Avg Loss: 0.0147 | Grad Norm: 0.00838797\n",
      "Epoch 3 | Step 2129400 | Avg Loss: 0.0146 | Grad Norm: 0.01009479\n",
      "Epoch 3 | Step 2129500 | Avg Loss: 0.0149 | Grad Norm: 0.00800620\n",
      "Epoch 3 | Step 2129600 | Avg Loss: 0.0147 | Grad Norm: 0.00723602\n",
      "Epoch 3 | Step 2129700 | Avg Loss: 0.0150 | Grad Norm: 0.00849778\n",
      "Epoch 3 | Step 2129800 | Avg Loss: 0.0152 | Grad Norm: 0.00790874\n",
      "Epoch 3 | Step 2129900 | Avg Loss: 0.0151 | Grad Norm: 0.00789301\n",
      "Epoch 3 | Step 2130000 | Avg Loss: 0.0155 | Grad Norm: 0.00926500\n",
      "Epoch 3 | Step 2130100 | Avg Loss: 0.0157 | Grad Norm: 0.00788198\n",
      "Epoch 3 | Step 2130200 | Avg Loss: 0.0152 | Grad Norm: 0.00726652\n",
      "Epoch 3 | Step 2130300 | Avg Loss: 0.0150 | Grad Norm: 0.00833515\n",
      "Epoch 3 | Step 2130400 | Avg Loss: 0.0149 | Grad Norm: 0.00804254\n",
      "Epoch 3 | Step 2130500 | Avg Loss: 0.0153 | Grad Norm: 0.00762395\n",
      "Epoch 3 | Step 2130600 | Avg Loss: 0.0152 | Grad Norm: 0.00823174\n",
      "Epoch 3 | Step 2130700 | Avg Loss: 0.0153 | Grad Norm: 0.00771730\n",
      "Epoch 3 | Step 2130800 | Avg Loss: 0.0154 | Grad Norm: 0.00809057\n",
      "Epoch 3 | Step 2130900 | Avg Loss: 0.0154 | Grad Norm: 0.00788864\n",
      "Epoch 3 | Step 2131000 | Avg Loss: 0.0154 | Grad Norm: 0.00762674\n",
      "Epoch 3 | Step 2131100 | Avg Loss: 0.0159 | Grad Norm: 0.00808969\n",
      "Epoch 3 | Step 2131200 | Avg Loss: 0.0153 | Grad Norm: 0.00923498\n",
      "Epoch 3 | Step 2131300 | Avg Loss: 0.0150 | Grad Norm: 0.00810350\n",
      "Epoch 3 | Step 2131400 | Avg Loss: 0.0148 | Grad Norm: 0.00788669\n",
      "Epoch 3 | Step 2131500 | Avg Loss: 0.0149 | Grad Norm: 0.00844073\n",
      "Epoch 3 | Step 2131600 | Avg Loss: 0.0147 | Grad Norm: 0.00747685\n",
      "Epoch 3 | Step 2131700 | Avg Loss: 0.0150 | Grad Norm: 0.00637319\n",
      "Epoch 3 | Step 2131800 | Avg Loss: 0.0149 | Grad Norm: 0.00748472\n",
      "Epoch 3 | Step 2131900 | Avg Loss: 0.0154 | Grad Norm: 0.00768211\n",
      "Epoch 3 | Step 2132000 | Avg Loss: 0.0151 | Grad Norm: 0.00755745\n",
      "Epoch 3 | Step 2132100 | Avg Loss: 0.0151 | Grad Norm: 0.00680233\n",
      "Epoch 3 | Step 2132200 | Avg Loss: 0.0154 | Grad Norm: 0.00787486\n",
      "Epoch 3 | Step 2132300 | Avg Loss: 0.0153 | Grad Norm: 0.00711154\n",
      "Epoch 3 | Step 2132400 | Avg Loss: 0.0152 | Grad Norm: 0.00725088\n",
      "Epoch 3 | Step 2132500 | Avg Loss: 0.0151 | Grad Norm: 0.00778828\n",
      "Epoch 3 | Step 2132600 | Avg Loss: 0.0149 | Grad Norm: 0.00892659\n",
      "Epoch 3 | Step 2132700 | Avg Loss: 0.0152 | Grad Norm: 0.00729674\n",
      "Epoch 3 | Step 2132800 | Avg Loss: 0.0155 | Grad Norm: 0.00812419\n",
      "Epoch 3 | Step 2132900 | Avg Loss: 0.0157 | Grad Norm: 0.00842179\n",
      "Epoch 3 | Step 2133000 | Avg Loss: 0.0154 | Grad Norm: 0.00753865\n",
      "Epoch 3 | Step 2133100 | Avg Loss: 0.0152 | Grad Norm: 0.00851279\n",
      "Epoch 3 | Step 2133200 | Avg Loss: 0.0150 | Grad Norm: 0.00837971\n",
      "Epoch 3 | Step 2133300 | Avg Loss: 0.0152 | Grad Norm: 0.00737061\n",
      "Epoch 3 | Step 2133400 | Avg Loss: 0.0152 | Grad Norm: 0.00804279\n",
      "Epoch 3 | Step 2133500 | Avg Loss: 0.0154 | Grad Norm: 0.00744877\n",
      "Epoch 3 | Step 2133600 | Avg Loss: 0.0154 | Grad Norm: 0.00714691\n",
      "Epoch 3 | Step 2133700 | Avg Loss: 0.0151 | Grad Norm: 0.00926755\n",
      "Epoch 3 | Step 2133800 | Avg Loss: 0.0152 | Grad Norm: 0.01102202\n",
      "Epoch 3 | Step 2133900 | Avg Loss: 0.0156 | Grad Norm: 0.00765831\n",
      "Epoch 3 | Step 2134000 | Avg Loss: 0.0154 | Grad Norm: 0.00851415\n",
      "Epoch 3 | Step 2134100 | Avg Loss: 0.0154 | Grad Norm: 0.00904672\n",
      "Epoch 3 | Step 2134200 | Avg Loss: 0.0152 | Grad Norm: 0.00853591\n",
      "Epoch 3 | Step 2134300 | Avg Loss: 0.0148 | Grad Norm: 0.00742155\n",
      "Epoch 3 | Step 2134400 | Avg Loss: 0.0152 | Grad Norm: 0.00779017\n",
      "Epoch 3 | Step 2134500 | Avg Loss: 0.0151 | Grad Norm: 0.00862119\n",
      "Epoch 3 | Step 2134600 | Avg Loss: 0.0151 | Grad Norm: 0.00790635\n",
      "Epoch 3 | Step 2134700 | Avg Loss: 0.0152 | Grad Norm: 0.00769616\n",
      "Epoch 3 | Step 2134800 | Avg Loss: 0.0152 | Grad Norm: 0.00717536\n",
      "Epoch 3 | Step 2134900 | Avg Loss: 0.0152 | Grad Norm: 0.00872963\n",
      "Epoch 3 | Step 2135000 | Avg Loss: 0.0152 | Grad Norm: 0.00745212\n",
      "Epoch 3 | Step 2135100 | Avg Loss: 0.0151 | Grad Norm: 0.00888431\n",
      "Epoch 3 | Step 2135200 | Avg Loss: 0.0151 | Grad Norm: 0.00774633\n",
      "Epoch 3 | Step 2135300 | Avg Loss: 0.0152 | Grad Norm: 0.00889576\n",
      "Epoch 3 | Step 2135400 | Avg Loss: 0.0152 | Grad Norm: 0.00819602\n",
      "Epoch 3 | Step 2135500 | Avg Loss: 0.0150 | Grad Norm: 0.00658711\n",
      "Epoch 3 | Step 2135600 | Avg Loss: 0.0149 | Grad Norm: 0.00706456\n",
      "Epoch 3 | Step 2135700 | Avg Loss: 0.0147 | Grad Norm: 0.00781122\n",
      "Epoch 3 | Step 2135800 | Avg Loss: 0.0145 | Grad Norm: 0.00944568\n",
      "Epoch 3 | Step 2135900 | Avg Loss: 0.0145 | Grad Norm: 0.00720321\n",
      "Epoch 3 | Step 2136000 | Avg Loss: 0.0147 | Grad Norm: 0.00733462\n",
      "Epoch 3 | Step 2136100 | Avg Loss: 0.0149 | Grad Norm: 0.00708485\n",
      "Epoch 3 | Step 2136200 | Avg Loss: 0.0151 | Grad Norm: 0.00776982\n",
      "Epoch 3 | Step 2136300 | Avg Loss: 0.0151 | Grad Norm: 0.00915702\n",
      "Epoch 3 | Step 2136400 | Avg Loss: 0.0152 | Grad Norm: 0.00827036\n",
      "Epoch 3 | Step 2136500 | Avg Loss: 0.0153 | Grad Norm: 0.01013081\n",
      "Epoch 3 | Step 2136600 | Avg Loss: 0.0148 | Grad Norm: 0.00833352\n",
      "Epoch 3 | Step 2136700 | Avg Loss: 0.0150 | Grad Norm: 0.00817310\n",
      "Epoch 3 | Step 2136800 | Avg Loss: 0.0149 | Grad Norm: 0.00817062\n",
      "Epoch 3 | Step 2136900 | Avg Loss: 0.0150 | Grad Norm: 0.00641216\n",
      "Epoch 3 | Step 2137000 | Avg Loss: 0.0150 | Grad Norm: 0.00836817\n",
      "Epoch 3 | Step 2137100 | Avg Loss: 0.0150 | Grad Norm: 0.00790546\n",
      "Epoch 3 | Step 2137200 | Avg Loss: 0.0149 | Grad Norm: 0.00749586\n",
      "Epoch 3 | Step 2137300 | Avg Loss: 0.0152 | Grad Norm: 0.00909518\n",
      "Epoch 3 | Step 2137400 | Avg Loss: 0.0150 | Grad Norm: 0.01058136\n",
      "Epoch 3 | Step 2137500 | Avg Loss: 0.0151 | Grad Norm: 0.00862514\n",
      "Epoch 3 | Step 2137600 | Avg Loss: 0.0151 | Grad Norm: 0.00778129\n",
      "Epoch 3 | Step 2137700 | Avg Loss: 0.0152 | Grad Norm: 0.00864077\n",
      "Epoch 3 | Step 2137800 | Avg Loss: 0.0151 | Grad Norm: 0.00788871\n",
      "Epoch 3 | Step 2137900 | Avg Loss: 0.0152 | Grad Norm: 0.00816596\n",
      "Epoch 3 | Step 2138000 | Avg Loss: 0.0151 | Grad Norm: 0.00753521\n",
      "Epoch 3 | Step 2138100 | Avg Loss: 0.0152 | Grad Norm: 0.00879890\n",
      "Epoch 3 | Step 2138200 | Avg Loss: 0.0153 | Grad Norm: 0.00788752\n",
      "Epoch 3 | Step 2138300 | Avg Loss: 0.0149 | Grad Norm: 0.00793774\n",
      "Epoch 3 | Step 2138400 | Avg Loss: 0.0149 | Grad Norm: 0.00695472\n",
      "Epoch 3 | Step 2138500 | Avg Loss: 0.0150 | Grad Norm: 0.00876545\n",
      "Epoch 3 | Step 2138600 | Avg Loss: 0.0147 | Grad Norm: 0.00839167\n",
      "Epoch 3 | Step 2138700 | Avg Loss: 0.0151 | Grad Norm: 0.00816474\n",
      "Epoch 3 | Step 2138800 | Avg Loss: 0.0148 | Grad Norm: 0.00724077\n",
      "Epoch 3 | Step 2138900 | Avg Loss: 0.0146 | Grad Norm: 0.00726251\n",
      "Epoch 3 | Step 2139000 | Avg Loss: 0.0147 | Grad Norm: 0.00956713\n",
      "Epoch 3 | Step 2139100 | Avg Loss: 0.0151 | Grad Norm: 0.00791803\n",
      "Epoch 3 | Step 2139200 | Avg Loss: 0.0150 | Grad Norm: 0.00663195\n",
      "Epoch 3 | Step 2139300 | Avg Loss: 0.0150 | Grad Norm: 0.00704297\n",
      "Epoch 3 | Step 2139400 | Avg Loss: 0.0149 | Grad Norm: 0.00787838\n",
      "Epoch 3 | Step 2139500 | Avg Loss: 0.0147 | Grad Norm: 0.00895935\n",
      "Epoch 3 | Step 2139600 | Avg Loss: 0.0147 | Grad Norm: 0.00794697\n",
      "Epoch 3 | Step 2139700 | Avg Loss: 0.0145 | Grad Norm: 0.01062962\n",
      "Epoch 3 | Step 2139800 | Avg Loss: 0.0145 | Grad Norm: 0.00880032\n",
      "Epoch 3 | Step 2139900 | Avg Loss: 0.0147 | Grad Norm: 0.00824651\n",
      "Epoch 3 | Step 2140000 | Avg Loss: 0.0153 | Grad Norm: 0.00938392\n",
      "Epoch 3 | Step 2140100 | Avg Loss: 0.0147 | Grad Norm: 0.00766930\n",
      "Epoch 3 | Step 2140200 | Avg Loss: 0.0146 | Grad Norm: 0.00732119\n",
      "Epoch 3 | Step 2140300 | Avg Loss: 0.0152 | Grad Norm: 0.00752206\n",
      "Epoch 3 | Step 2140400 | Avg Loss: 0.0149 | Grad Norm: 0.00742987\n",
      "Epoch 3 | Step 2140500 | Avg Loss: 0.0149 | Grad Norm: 0.00780453\n",
      "Epoch 3 | Step 2140600 | Avg Loss: 0.0151 | Grad Norm: 0.00815152\n",
      "Epoch 3 | Step 2140700 | Avg Loss: 0.0149 | Grad Norm: 0.00828080\n",
      "Epoch 3 | Step 2140800 | Avg Loss: 0.0145 | Grad Norm: 0.00721170\n",
      "Epoch 3 | Step 2140900 | Avg Loss: 0.0145 | Grad Norm: 0.00742602\n",
      "Epoch 3 | Step 2141000 | Avg Loss: 0.0144 | Grad Norm: 0.00879048\n",
      "Epoch 3 | Step 2141100 | Avg Loss: 0.0147 | Grad Norm: 0.00733381\n",
      "Epoch 3 | Step 2141200 | Avg Loss: 0.0151 | Grad Norm: 0.00976218\n",
      "Epoch 3 | Step 2141300 | Avg Loss: 0.0151 | Grad Norm: 0.00945825\n",
      "Epoch 3 | Step 2141400 | Avg Loss: 0.0150 | Grad Norm: 0.00880560\n",
      "Epoch 3 | Step 2141500 | Avg Loss: 0.0147 | Grad Norm: 0.00743603\n",
      "Epoch 3 | Step 2141600 | Avg Loss: 0.0148 | Grad Norm: 0.00890387\n",
      "Epoch 3 | Step 2141700 | Avg Loss: 0.0150 | Grad Norm: 0.00810994\n",
      "Epoch 3 | Step 2141800 | Avg Loss: 0.0148 | Grad Norm: 0.00817796\n",
      "Epoch 3 | Step 2141900 | Avg Loss: 0.0150 | Grad Norm: 0.00715099\n",
      "Epoch 3 | Step 2142000 | Avg Loss: 0.0150 | Grad Norm: 0.00843955\n",
      "Epoch 3 | Step 2142100 | Avg Loss: 0.0153 | Grad Norm: 0.00719214\n",
      "Epoch 3 | Step 2142200 | Avg Loss: 0.0151 | Grad Norm: 0.00721426\n",
      "Epoch 3 | Step 2142300 | Avg Loss: 0.0149 | Grad Norm: 0.00763827\n",
      "Epoch 3 | Step 2142400 | Avg Loss: 0.0148 | Grad Norm: 0.00716838\n",
      "Epoch 3 | Step 2142500 | Avg Loss: 0.0152 | Grad Norm: 0.00814795\n",
      "Epoch 3 | Step 2142600 | Avg Loss: 0.0149 | Grad Norm: 0.00731378\n",
      "Epoch 3 | Step 2142700 | Avg Loss: 0.0149 | Grad Norm: 0.00734222\n",
      "Epoch 3 | Step 2142800 | Avg Loss: 0.0147 | Grad Norm: 0.00831450\n",
      "Epoch 3 | Step 2142900 | Avg Loss: 0.0146 | Grad Norm: 0.00761790\n",
      "Epoch 3 | Step 2143000 | Avg Loss: 0.0152 | Grad Norm: 0.00843847\n",
      "Epoch 3 | Step 2143100 | Avg Loss: 0.0152 | Grad Norm: 0.00970713\n",
      "Epoch 3 | Step 2143200 | Avg Loss: 0.0149 | Grad Norm: 0.00686002\n",
      "Epoch 3 | Step 2143300 | Avg Loss: 0.0150 | Grad Norm: 0.00681700\n",
      "Epoch 3 | Step 2143400 | Avg Loss: 0.0154 | Grad Norm: 0.00808994\n",
      "Epoch 3 | Step 2143500 | Avg Loss: 0.0149 | Grad Norm: 0.00822918\n",
      "Epoch 3 | Step 2143600 | Avg Loss: 0.0147 | Grad Norm: 0.01028442\n",
      "Epoch 3 | Step 2143700 | Avg Loss: 0.0147 | Grad Norm: 0.00870842\n",
      "Epoch 3 | Step 2143800 | Avg Loss: 0.0151 | Grad Norm: 0.01072347\n",
      "Epoch 3 | Step 2143900 | Avg Loss: 0.0153 | Grad Norm: 0.00887092\n",
      "Epoch 3 | Step 2144000 | Avg Loss: 0.0149 | Grad Norm: 0.00792879\n",
      "Epoch 3 | Step 2144100 | Avg Loss: 0.0148 | Grad Norm: 0.00813427\n",
      "Epoch 3 | Step 2144200 | Avg Loss: 0.0150 | Grad Norm: 0.00825493\n",
      "Epoch 3 | Step 2144300 | Avg Loss: 0.0147 | Grad Norm: 0.00723529\n",
      "Epoch 3 | Step 2144400 | Avg Loss: 0.0145 | Grad Norm: 0.00838269\n",
      "Epoch 3 | Step 2144500 | Avg Loss: 0.0147 | Grad Norm: 0.00828964\n",
      "Epoch 3 | Step 2144600 | Avg Loss: 0.0148 | Grad Norm: 0.00885215\n",
      "Epoch 3 | Step 2144700 | Avg Loss: 0.0149 | Grad Norm: 0.00849874\n",
      "Epoch 3 | Step 2144800 | Avg Loss: 0.0150 | Grad Norm: 0.00891417\n",
      "Epoch 3 | Step 2144900 | Avg Loss: 0.0148 | Grad Norm: 0.01097515\n",
      "Epoch 3 | Step 2145000 | Avg Loss: 0.0150 | Grad Norm: 0.00731954\n",
      "Epoch 3 | Step 2145100 | Avg Loss: 0.0152 | Grad Norm: 0.00668363\n",
      "Epoch 3 | Step 2145200 | Avg Loss: 0.0149 | Grad Norm: 0.00720418\n",
      "Epoch 3 | Step 2145300 | Avg Loss: 0.0150 | Grad Norm: 0.00783360\n",
      "Epoch 3 | Step 2145400 | Avg Loss: 0.0147 | Grad Norm: 0.00927814\n",
      "Epoch 3 | Step 2145500 | Avg Loss: 0.0146 | Grad Norm: 0.00695794\n",
      "Epoch 3 | Step 2145600 | Avg Loss: 0.0147 | Grad Norm: 0.01063862\n",
      "Epoch 3 | Step 2145700 | Avg Loss: 0.0148 | Grad Norm: 0.00659378\n",
      "Epoch 3 | Step 2145800 | Avg Loss: 0.0150 | Grad Norm: 0.00813644\n",
      "Epoch 3 | Step 2145900 | Avg Loss: 0.0150 | Grad Norm: 0.00701614\n",
      "Epoch 3 | Step 2146000 | Avg Loss: 0.0153 | Grad Norm: 0.00874122\n",
      "Epoch 3 | Step 2146100 | Avg Loss: 0.0154 | Grad Norm: 0.00878515\n",
      "Epoch 3 | Step 2146200 | Avg Loss: 0.0153 | Grad Norm: 0.00802734\n",
      "Epoch 3 | Step 2146300 | Avg Loss: 0.0156 | Grad Norm: 0.00934771\n",
      "Epoch 3 | Step 2146400 | Avg Loss: 0.0154 | Grad Norm: 0.00734892\n",
      "Epoch 3 | Step 2146500 | Avg Loss: 0.0152 | Grad Norm: 0.00822723\n",
      "Epoch 3 | Step 2146600 | Avg Loss: 0.0149 | Grad Norm: 0.00890720\n",
      "Epoch 3 | Step 2146700 | Avg Loss: 0.0150 | Grad Norm: 0.00758356\n",
      "Epoch 3 | Step 2146800 | Avg Loss: 0.0150 | Grad Norm: 0.00848510\n",
      "Epoch 3 | Step 2146900 | Avg Loss: 0.0153 | Grad Norm: 0.01054768\n",
      "Epoch 3 | Step 2147000 | Avg Loss: 0.0152 | Grad Norm: 0.00783975\n",
      "Epoch 3 | Step 2147100 | Avg Loss: 0.0155 | Grad Norm: 0.00909155\n",
      "Epoch 3 | Step 2147200 | Avg Loss: 0.0157 | Grad Norm: 0.00850872\n",
      "Epoch 3 | Step 2147300 | Avg Loss: 0.0157 | Grad Norm: 0.00904952\n",
      "Epoch 3 | Step 2147400 | Avg Loss: 0.0157 | Grad Norm: 0.00863082\n",
      "Epoch 3 | Step 2147500 | Avg Loss: 0.0156 | Grad Norm: 0.00756090\n",
      "Epoch 3 | Step 2147600 | Avg Loss: 0.0156 | Grad Norm: 0.00754177\n",
      "Epoch 3 | Step 2147700 | Avg Loss: 0.0160 | Grad Norm: 0.00797489\n",
      "Epoch 3 | Step 2147800 | Avg Loss: 0.0159 | Grad Norm: 0.01057909\n",
      "Epoch 3 | Step 2147900 | Avg Loss: 0.0158 | Grad Norm: 0.00760868\n",
      "Epoch 3 | Step 2148000 | Avg Loss: 0.0155 | Grad Norm: 0.00759608\n",
      "Epoch 3 | Step 2148100 | Avg Loss: 0.0155 | Grad Norm: 0.00841875\n",
      "Epoch 3 | Step 2148200 | Avg Loss: 0.0156 | Grad Norm: 0.00741741\n",
      "Epoch 3 | Step 2148300 | Avg Loss: 0.0156 | Grad Norm: 0.00708764\n",
      "Epoch 3 | Step 2148400 | Avg Loss: 0.0157 | Grad Norm: 0.00831007\n",
      "Epoch 3 | Step 2148500 | Avg Loss: 0.0153 | Grad Norm: 0.00801434\n",
      "Epoch 3 | Step 2148600 | Avg Loss: 0.0148 | Grad Norm: 0.00753522\n",
      "Epoch 3 | Step 2148700 | Avg Loss: 0.0149 | Grad Norm: 0.00740464\n",
      "Epoch 3 | Step 2148800 | Avg Loss: 0.0152 | Grad Norm: 0.00786444\n",
      "Epoch 3 | Step 2148900 | Avg Loss: 0.0149 | Grad Norm: 0.00814633\n",
      "Epoch 3 | Step 2149000 | Avg Loss: 0.0151 | Grad Norm: 0.00936493\n",
      "Epoch 3 | Step 2149100 | Avg Loss: 0.0153 | Grad Norm: 0.00877973\n",
      "Epoch 3 | Step 2149200 | Avg Loss: 0.0155 | Grad Norm: 0.00844185\n",
      "Epoch 3 | Step 2149300 | Avg Loss: 0.0155 | Grad Norm: 0.00900669\n",
      "Epoch 3 | Step 2149400 | Avg Loss: 0.0154 | Grad Norm: 0.00748931\n",
      "Epoch 3 | Step 2149500 | Avg Loss: 0.0152 | Grad Norm: 0.00850486\n",
      "Epoch 3 | Step 2149600 | Avg Loss: 0.0153 | Grad Norm: 0.00865044\n",
      "Epoch 3 | Step 2149700 | Avg Loss: 0.0152 | Grad Norm: 0.00965063\n",
      "Epoch 3 | Step 2149800 | Avg Loss: 0.0150 | Grad Norm: 0.00684979\n",
      "Epoch 3 | Step 2149900 | Avg Loss: 0.0156 | Grad Norm: 0.00783271\n",
      "Epoch 3 | Step 2150000 | Avg Loss: 0.0157 | Grad Norm: 0.00771823\n",
      "Epoch 3 | Step 2150100 | Avg Loss: 0.0156 | Grad Norm: 0.00964502\n",
      "Epoch 3 | Step 2150200 | Avg Loss: 0.0157 | Grad Norm: 0.01139721\n",
      "Epoch 3 | Step 2150300 | Avg Loss: 0.0156 | Grad Norm: 0.00759211\n",
      "Epoch 3 | Step 2150400 | Avg Loss: 0.0155 | Grad Norm: 0.00757465\n",
      "Epoch 3 | Step 2150500 | Avg Loss: 0.0156 | Grad Norm: 0.00949839\n",
      "Epoch 3 | Step 2150600 | Avg Loss: 0.0154 | Grad Norm: 0.00805573\n",
      "Epoch 3 | Step 2150700 | Avg Loss: 0.0152 | Grad Norm: 0.00882858\n",
      "Epoch 3 | Step 2150800 | Avg Loss: 0.0152 | Grad Norm: 0.00782118\n",
      "Epoch 3 | Step 2150900 | Avg Loss: 0.0149 | Grad Norm: 0.01063045\n",
      "Epoch 3 | Step 2151000 | Avg Loss: 0.0151 | Grad Norm: 0.00883730\n",
      "Epoch 3 | Step 2151100 | Avg Loss: 0.0149 | Grad Norm: 0.00841711\n",
      "Epoch 3 | Step 2151200 | Avg Loss: 0.0152 | Grad Norm: 0.00849708\n",
      "Epoch 3 | Step 2151300 | Avg Loss: 0.0152 | Grad Norm: 0.00757790\n",
      "Epoch 3 | Step 2151400 | Avg Loss: 0.0150 | Grad Norm: 0.01035133\n",
      "Epoch 3 | Step 2151500 | Avg Loss: 0.0151 | Grad Norm: 0.00855243\n",
      "Epoch 3 | Step 2151600 | Avg Loss: 0.0150 | Grad Norm: 0.00861709\n",
      "Epoch 3 | Step 2151700 | Avg Loss: 0.0146 | Grad Norm: 0.00657219\n",
      "Epoch 3 | Step 2151800 | Avg Loss: 0.0148 | Grad Norm: 0.00874581\n",
      "Epoch 3 | Step 2151900 | Avg Loss: 0.0148 | Grad Norm: 0.00848111\n",
      "Epoch 3 | Step 2152000 | Avg Loss: 0.0149 | Grad Norm: 0.00906639\n",
      "Epoch 3 | Step 2152100 | Avg Loss: 0.0148 | Grad Norm: 0.00854781\n",
      "Epoch 3 | Step 2152200 | Avg Loss: 0.0149 | Grad Norm: 0.00791212\n",
      "Epoch 3 | Step 2152300 | Avg Loss: 0.0149 | Grad Norm: 0.00671409\n",
      "Epoch 3 | Step 2152400 | Avg Loss: 0.0148 | Grad Norm: 0.00677571\n",
      "Epoch 3 | Step 2152500 | Avg Loss: 0.0148 | Grad Norm: 0.00929710\n",
      "Epoch 3 | Step 2152600 | Avg Loss: 0.0148 | Grad Norm: 0.00796276\n",
      "Epoch 3 | Step 2152700 | Avg Loss: 0.0148 | Grad Norm: 0.00875866\n",
      "Epoch 3 | Step 2152800 | Avg Loss: 0.0148 | Grad Norm: 0.00892851\n",
      "Epoch 3 | Step 2152900 | Avg Loss: 0.0150 | Grad Norm: 0.00787560\n",
      "Epoch 3 | Step 2153000 | Avg Loss: 0.0148 | Grad Norm: 0.00724582\n",
      "Epoch 3 | Step 2153100 | Avg Loss: 0.0149 | Grad Norm: 0.00762336\n",
      "Epoch 3 | Step 2153200 | Avg Loss: 0.0147 | Grad Norm: 0.00788052\n",
      "Epoch 3 | Step 2153300 | Avg Loss: 0.0149 | Grad Norm: 0.00811261\n",
      "Epoch 3 | Step 2153400 | Avg Loss: 0.0148 | Grad Norm: 0.00886806\n",
      "Epoch 3 | Step 2153500 | Avg Loss: 0.0148 | Grad Norm: 0.00798643\n",
      "Epoch 3 | Step 2153600 | Avg Loss: 0.0148 | Grad Norm: 0.00683758\n",
      "Epoch 3 | Step 2153700 | Avg Loss: 0.0151 | Grad Norm: 0.00803514\n",
      "Epoch 3 | Step 2153800 | Avg Loss: 0.0149 | Grad Norm: 0.00683279\n",
      "Epoch 3 | Step 2153900 | Avg Loss: 0.0150 | Grad Norm: 0.00830803\n",
      "Epoch 3 | Step 2154000 | Avg Loss: 0.0154 | Grad Norm: 0.00752294\n",
      "Epoch 3 | Step 2154100 | Avg Loss: 0.0154 | Grad Norm: 0.00720967\n",
      "Epoch 3 | Step 2154200 | Avg Loss: 0.0152 | Grad Norm: 0.00795525\n",
      "Epoch 3 | Step 2154300 | Avg Loss: 0.0153 | Grad Norm: 0.00903464\n",
      "Epoch 3 | Step 2154400 | Avg Loss: 0.0155 | Grad Norm: 0.00953274\n",
      "Epoch 3 | Step 2154500 | Avg Loss: 0.0157 | Grad Norm: 0.00961093\n",
      "Epoch 3 | Step 2154600 | Avg Loss: 0.0156 | Grad Norm: 0.01077943\n",
      "Epoch 3 | Step 2154700 | Avg Loss: 0.0153 | Grad Norm: 0.00689825\n",
      "Epoch 3 | Step 2154800 | Avg Loss: 0.0152 | Grad Norm: 0.00742834\n",
      "Epoch 3 | Step 2154900 | Avg Loss: 0.0153 | Grad Norm: 0.00819995\n",
      "Epoch 3 | Step 2155000 | Avg Loss: 0.0151 | Grad Norm: 0.00899490\n",
      "Epoch 3 | Step 2155100 | Avg Loss: 0.0149 | Grad Norm: 0.00720600\n",
      "Epoch 3 | Step 2155200 | Avg Loss: 0.0149 | Grad Norm: 0.00787963\n",
      "Epoch 3 | Step 2155300 | Avg Loss: 0.0149 | Grad Norm: 0.00811828\n",
      "Epoch 3 | Step 2155400 | Avg Loss: 0.0148 | Grad Norm: 0.00751159\n",
      "Epoch 3 | Step 2155500 | Avg Loss: 0.0150 | Grad Norm: 0.00883873\n",
      "Epoch 3 | Step 2155600 | Avg Loss: 0.0147 | Grad Norm: 0.00681437\n",
      "Epoch 3 | Step 2155700 | Avg Loss: 0.0147 | Grad Norm: 0.00834286\n",
      "Epoch 3 | Step 2155800 | Avg Loss: 0.0149 | Grad Norm: 0.00812867\n",
      "Epoch 3 | Step 2155900 | Avg Loss: 0.0152 | Grad Norm: 0.00784845\n",
      "Epoch 3 | Step 2156000 | Avg Loss: 0.0149 | Grad Norm: 0.00819543\n",
      "Epoch 3 | Step 2156100 | Avg Loss: 0.0153 | Grad Norm: 0.00849777\n",
      "Epoch 3 | Step 2156200 | Avg Loss: 0.0153 | Grad Norm: 0.00811289\n",
      "Epoch 3 | Step 2156300 | Avg Loss: 0.0157 | Grad Norm: 0.00770274\n",
      "Epoch 3 | Step 2156400 | Avg Loss: 0.0153 | Grad Norm: 0.00693654\n",
      "Epoch 3 | Step 2156500 | Avg Loss: 0.0152 | Grad Norm: 0.00741717\n",
      "Epoch 3 | Step 2156600 | Avg Loss: 0.0153 | Grad Norm: 0.00814148\n",
      "Epoch 3 | Step 2156700 | Avg Loss: 0.0151 | Grad Norm: 0.00950761\n",
      "Epoch 3 | Step 2156800 | Avg Loss: 0.0151 | Grad Norm: 0.00880775\n",
      "Epoch 3 | Step 2156900 | Avg Loss: 0.0152 | Grad Norm: 0.00744791\n",
      "Epoch 3 | Step 2157000 | Avg Loss: 0.0151 | Grad Norm: 0.00814478\n",
      "Epoch 3 | Step 2157100 | Avg Loss: 0.0149 | Grad Norm: 0.00818642\n",
      "Epoch 3 | Step 2157200 | Avg Loss: 0.0146 | Grad Norm: 0.00687644\n",
      "Epoch 3 | Step 2157300 | Avg Loss: 0.0147 | Grad Norm: 0.00934593\n",
      "Epoch 3 | Step 2157400 | Avg Loss: 0.0147 | Grad Norm: 0.00984546\n",
      "Epoch 3 | Step 2157500 | Avg Loss: 0.0148 | Grad Norm: 0.00699439\n",
      "Epoch 3 | Step 2157600 | Avg Loss: 0.0153 | Grad Norm: 0.00886773\n",
      "Epoch 3 | Step 2157700 | Avg Loss: 0.0149 | Grad Norm: 0.00815402\n",
      "Epoch 3 | Step 2157800 | Avg Loss: 0.0150 | Grad Norm: 0.00875259\n",
      "Epoch 3 | Step 2157900 | Avg Loss: 0.0152 | Grad Norm: 0.01007528\n",
      "Epoch 3 | Step 2158000 | Avg Loss: 0.0151 | Grad Norm: 0.00713072\n",
      "Epoch 3 | Step 2158100 | Avg Loss: 0.0152 | Grad Norm: 0.00851094\n",
      "Epoch 3 | Step 2158200 | Avg Loss: 0.0155 | Grad Norm: 0.00927406\n",
      "Epoch 3 | Step 2158300 | Avg Loss: 0.0152 | Grad Norm: 0.00778811\n",
      "Epoch 3 | Step 2158400 | Avg Loss: 0.0151 | Grad Norm: 0.00832932\n",
      "Epoch 3 | Step 2158500 | Avg Loss: 0.0151 | Grad Norm: 0.00916834\n",
      "Epoch 3 | Step 2158600 | Avg Loss: 0.0154 | Grad Norm: 0.00834541\n",
      "Epoch 3 | Step 2158700 | Avg Loss: 0.0155 | Grad Norm: 0.00859092\n",
      "Epoch 3 | Step 2158800 | Avg Loss: 0.0155 | Grad Norm: 0.00934509\n",
      "Epoch 3 | Step 2158900 | Avg Loss: 0.0153 | Grad Norm: 0.00767340\n",
      "Epoch 3 | Step 2159000 | Avg Loss: 0.0153 | Grad Norm: 0.00877929\n",
      "Epoch 3 | Step 2159100 | Avg Loss: 0.0152 | Grad Norm: 0.00848756\n",
      "Epoch 3 | Step 2159200 | Avg Loss: 0.0150 | Grad Norm: 0.00664196\n",
      "Epoch 3 | Step 2159300 | Avg Loss: 0.0154 | Grad Norm: 0.00774352\n",
      "Epoch 3 | Step 2159400 | Avg Loss: 0.0153 | Grad Norm: 0.00732805\n",
      "Epoch 3 | Step 2159500 | Avg Loss: 0.0154 | Grad Norm: 0.00883584\n",
      "Epoch 3 | Step 2159600 | Avg Loss: 0.0155 | Grad Norm: 0.00760938\n",
      "Epoch 3 | Step 2159700 | Avg Loss: 0.0153 | Grad Norm: 0.00813063\n",
      "Epoch 3 | Step 2159800 | Avg Loss: 0.0152 | Grad Norm: 0.00738313\n",
      "Epoch 3 | Step 2159900 | Avg Loss: 0.0150 | Grad Norm: 0.00739133\n",
      "Epoch 3 | Step 2160000 | Avg Loss: 0.0150 | Grad Norm: 0.00800459\n",
      "Epoch 3 | Step 2160100 | Avg Loss: 0.0150 | Grad Norm: 0.00838444\n",
      "Epoch 3 | Step 2160200 | Avg Loss: 0.0149 | Grad Norm: 0.00900938\n",
      "Epoch 3 | Step 2160300 | Avg Loss: 0.0150 | Grad Norm: 0.00714989\n",
      "Epoch 3 | Step 2160400 | Avg Loss: 0.0150 | Grad Norm: 0.00758915\n",
      "Epoch 3 | Step 2160500 | Avg Loss: 0.0151 | Grad Norm: 0.00787419\n",
      "Epoch 3 | Step 2160600 | Avg Loss: 0.0150 | Grad Norm: 0.00884335\n",
      "Epoch 3 | Step 2160700 | Avg Loss: 0.0155 | Grad Norm: 0.00727785\n",
      "Epoch 3 | Step 2160800 | Avg Loss: 0.0154 | Grad Norm: 0.00730537\n",
      "Epoch 3 | Step 2160900 | Avg Loss: 0.0153 | Grad Norm: 0.00829892\n",
      "Epoch 3 | Step 2161000 | Avg Loss: 0.0150 | Grad Norm: 0.00870612\n",
      "Epoch 3 | Step 2161100 | Avg Loss: 0.0146 | Grad Norm: 0.00792257\n",
      "Epoch 3 | Step 2161200 | Avg Loss: 0.0146 | Grad Norm: 0.00883031\n",
      "Epoch 3 | Step 2161300 | Avg Loss: 0.0146 | Grad Norm: 0.00886067\n",
      "Epoch 3 | Step 2161400 | Avg Loss: 0.0147 | Grad Norm: 0.00733484\n",
      "Epoch 3 | Step 2161500 | Avg Loss: 0.0149 | Grad Norm: 0.00791676\n",
      "Epoch 3 | Step 2161600 | Avg Loss: 0.0151 | Grad Norm: 0.00806804\n",
      "Epoch 3 | Step 2161700 | Avg Loss: 0.0152 | Grad Norm: 0.00702338\n",
      "Epoch 3 | Step 2161800 | Avg Loss: 0.0153 | Grad Norm: 0.00792040\n",
      "Epoch 3 | Step 2161900 | Avg Loss: 0.0154 | Grad Norm: 0.00766552\n",
      "Epoch 3 | Step 2162000 | Avg Loss: 0.0150 | Grad Norm: 0.00732255\n",
      "Epoch 3 | Step 2162100 | Avg Loss: 0.0148 | Grad Norm: 0.00724037\n",
      "Epoch 3 | Step 2162200 | Avg Loss: 0.0150 | Grad Norm: 0.00776183\n",
      "Epoch 3 | Step 2162300 | Avg Loss: 0.0148 | Grad Norm: 0.00790961\n",
      "Epoch 3 | Step 2162400 | Avg Loss: 0.0151 | Grad Norm: 0.01006665\n",
      "Epoch 3 | Step 2162500 | Avg Loss: 0.0149 | Grad Norm: 0.00699294\n",
      "Epoch 3 | Step 2162600 | Avg Loss: 0.0147 | Grad Norm: 0.00807159\n",
      "Epoch 3 | Step 2162700 | Avg Loss: 0.0147 | Grad Norm: 0.00836759\n",
      "Epoch 3 | Step 2162800 | Avg Loss: 0.0149 | Grad Norm: 0.00779323\n",
      "Epoch 3 | Step 2162900 | Avg Loss: 0.0149 | Grad Norm: 0.00833128\n",
      "Epoch 3 | Step 2163000 | Avg Loss: 0.0150 | Grad Norm: 0.00946229\n",
      "Epoch 3 | Step 2163100 | Avg Loss: 0.0150 | Grad Norm: 0.00890948\n",
      "Epoch 3 | Step 2163200 | Avg Loss: 0.0153 | Grad Norm: 0.00802858\n",
      "Epoch 3 | Step 2163300 | Avg Loss: 0.0154 | Grad Norm: 0.01423737\n",
      "Epoch 3 | Step 2163400 | Avg Loss: 0.0154 | Grad Norm: 0.00724307\n",
      "Epoch 3 | Step 2163500 | Avg Loss: 0.0153 | Grad Norm: 0.00938856\n",
      "Epoch 3 | Step 2163600 | Avg Loss: 0.0152 | Grad Norm: 0.00884513\n",
      "Epoch 3 | Step 2163700 | Avg Loss: 0.0153 | Grad Norm: 0.00848568\n",
      "Epoch 3 | Step 2163800 | Avg Loss: 0.0150 | Grad Norm: 0.00820660\n",
      "Epoch 3 | Step 2163900 | Avg Loss: 0.0150 | Grad Norm: 0.00954076\n",
      "Epoch 3 | Step 2164000 | Avg Loss: 0.0149 | Grad Norm: 0.00847976\n",
      "Epoch 3 | Step 2164100 | Avg Loss: 0.0151 | Grad Norm: 0.00775330\n",
      "Epoch 3 | Step 2164200 | Avg Loss: 0.0152 | Grad Norm: 0.00859761\n",
      "Epoch 3 | Step 2164300 | Avg Loss: 0.0151 | Grad Norm: 0.00724180\n",
      "Epoch 3 | Step 2164400 | Avg Loss: 0.0151 | Grad Norm: 0.00731494\n",
      "Epoch 3 | Step 2164500 | Avg Loss: 0.0149 | Grad Norm: 0.00707971\n",
      "Epoch 3 | Step 2164600 | Avg Loss: 0.0150 | Grad Norm: 0.00782288\n",
      "Epoch 3 | Step 2164700 | Avg Loss: 0.0150 | Grad Norm: 0.00876089\n",
      "Epoch 3 | Step 2164800 | Avg Loss: 0.0144 | Grad Norm: 0.00836253\n",
      "Epoch 3 | Step 2164900 | Avg Loss: 0.0151 | Grad Norm: 0.00741167\n",
      "Epoch 3 | Step 2165000 | Avg Loss: 0.0151 | Grad Norm: 0.00811185\n",
      "Epoch 3 | Step 2165100 | Avg Loss: 0.0154 | Grad Norm: 0.00908021\n",
      "Epoch 3 | Step 2165200 | Avg Loss: 0.0152 | Grad Norm: 0.00730502\n",
      "Epoch 3 | Step 2165300 | Avg Loss: 0.0149 | Grad Norm: 0.00882464\n",
      "Epoch 3 | Step 2165400 | Avg Loss: 0.0147 | Grad Norm: 0.01245489\n",
      "Epoch 3 | Step 2165500 | Avg Loss: 0.0148 | Grad Norm: 0.00787713\n",
      "Epoch 3 | Step 2165600 | Avg Loss: 0.0149 | Grad Norm: 0.01239643\n",
      "Epoch 3 | Step 2165700 | Avg Loss: 0.0150 | Grad Norm: 0.00698627\n",
      "Epoch 3 | Step 2165800 | Avg Loss: 0.0149 | Grad Norm: 0.00968945\n",
      "Epoch 3 | Step 2165900 | Avg Loss: 0.0151 | Grad Norm: 0.00778112\n",
      "Epoch 3 | Step 2166000 | Avg Loss: 0.0148 | Grad Norm: 0.00818834\n",
      "Epoch 3 | Step 2166100 | Avg Loss: 0.0150 | Grad Norm: 0.00917042\n",
      "Epoch 3 | Step 2166200 | Avg Loss: 0.0151 | Grad Norm: 0.00857539\n",
      "Epoch 3 | Step 2166300 | Avg Loss: 0.0150 | Grad Norm: 0.00653687\n",
      "Epoch 3 | Step 2166400 | Avg Loss: 0.0151 | Grad Norm: 0.00739043\n",
      "Epoch 3 | Step 2166500 | Avg Loss: 0.0154 | Grad Norm: 0.00854668\n",
      "Epoch 3 | Step 2166600 | Avg Loss: 0.0153 | Grad Norm: 0.00804540\n",
      "Epoch 3 | Step 2166700 | Avg Loss: 0.0150 | Grad Norm: 0.00721693\n",
      "Epoch 3 | Step 2166800 | Avg Loss: 0.0152 | Grad Norm: 0.00724472\n",
      "Epoch 3 | Step 2166900 | Avg Loss: 0.0152 | Grad Norm: 0.00933725\n",
      "Epoch 3 | Step 2167000 | Avg Loss: 0.0152 | Grad Norm: 0.00821039\n",
      "Epoch 3 | Step 2167100 | Avg Loss: 0.0158 | Grad Norm: 0.00970545\n",
      "Epoch 3 | Step 2167200 | Avg Loss: 0.0155 | Grad Norm: 0.00723743\n",
      "Epoch 3 | Step 2167300 | Avg Loss: 0.0152 | Grad Norm: 0.00879617\n",
      "Epoch 3 | Step 2167400 | Avg Loss: 0.0151 | Grad Norm: 0.00704977\n",
      "Epoch 3 | Step 2167500 | Avg Loss: 0.0152 | Grad Norm: 0.00739924\n",
      "Epoch 3 | Step 2167600 | Avg Loss: 0.0157 | Grad Norm: 0.00783776\n",
      "Epoch 3 | Step 2167700 | Avg Loss: 0.0153 | Grad Norm: 0.00956599\n",
      "Epoch 3 | Step 2167800 | Avg Loss: 0.0150 | Grad Norm: 0.00775310\n",
      "Epoch 3 | Step 2167900 | Avg Loss: 0.0150 | Grad Norm: 0.00859645\n",
      "Epoch 3 | Step 2168000 | Avg Loss: 0.0151 | Grad Norm: 0.00679860\n",
      "Epoch 3 | Step 2168100 | Avg Loss: 0.0154 | Grad Norm: 0.00807375\n",
      "Epoch 3 | Step 2168200 | Avg Loss: 0.0151 | Grad Norm: 0.00807891\n",
      "Epoch 3 | Step 2168300 | Avg Loss: 0.0149 | Grad Norm: 0.00666817\n",
      "Epoch 3 | Step 2168400 | Avg Loss: 0.0143 | Grad Norm: 0.00733600\n",
      "Epoch 3 | Step 2168500 | Avg Loss: 0.0142 | Grad Norm: 0.00813471\n",
      "Epoch 3 | Step 2168600 | Avg Loss: 0.0143 | Grad Norm: 0.00845974\n",
      "Epoch 3 | Step 2168700 | Avg Loss: 0.0146 | Grad Norm: 0.00674288\n",
      "Epoch 3 | Step 2168800 | Avg Loss: 0.0151 | Grad Norm: 0.00790124\n",
      "Epoch 3 | Step 2168900 | Avg Loss: 0.0153 | Grad Norm: 0.00905219\n",
      "Epoch 3 | Step 2169000 | Avg Loss: 0.0151 | Grad Norm: 0.00786785\n",
      "Epoch 3 | Step 2169100 | Avg Loss: 0.0152 | Grad Norm: 0.00915563\n",
      "Epoch 3 | Step 2169200 | Avg Loss: 0.0151 | Grad Norm: 0.00737395\n",
      "Epoch 3 | Step 2169300 | Avg Loss: 0.0151 | Grad Norm: 0.00965926\n",
      "Epoch 3 | Step 2169400 | Avg Loss: 0.0153 | Grad Norm: 0.00899080\n",
      "Epoch 3 | Step 2169500 | Avg Loss: 0.0149 | Grad Norm: 0.00760334\n",
      "Epoch 3 | Step 2169600 | Avg Loss: 0.0151 | Grad Norm: 0.00788891\n",
      "Epoch 3 | Step 2169700 | Avg Loss: 0.0151 | Grad Norm: 0.00895595\n",
      "Epoch 3 | Step 2169800 | Avg Loss: 0.0145 | Grad Norm: 0.00991833\n",
      "Epoch 3 | Step 2169900 | Avg Loss: 0.0146 | Grad Norm: 0.00642244\n",
      "Epoch 3 | Step 2170000 | Avg Loss: 0.0151 | Grad Norm: 0.00881970\n",
      "Epoch 3 | Step 2170100 | Avg Loss: 0.0152 | Grad Norm: 0.00771182\n",
      "Epoch 3 | Step 2170200 | Avg Loss: 0.0153 | Grad Norm: 0.01195097\n",
      "Epoch 3 | Step 2170300 | Avg Loss: 0.0153 | Grad Norm: 0.00809157\n",
      "Epoch 3 | Step 2170400 | Avg Loss: 0.0150 | Grad Norm: 0.00761003\n",
      "Epoch 3 | Step 2170500 | Avg Loss: 0.0153 | Grad Norm: 0.00861791\n",
      "Epoch 3 | Step 2170600 | Avg Loss: 0.0154 | Grad Norm: 0.00790644\n",
      "Epoch 3 | Step 2170700 | Avg Loss: 0.0153 | Grad Norm: 0.00773407\n",
      "Epoch 3 | Step 2170800 | Avg Loss: 0.0149 | Grad Norm: 0.00937401\n",
      "Epoch 3 | Step 2170900 | Avg Loss: 0.0152 | Grad Norm: 0.00804858\n",
      "Epoch 3 | Step 2171000 | Avg Loss: 0.0151 | Grad Norm: 0.00844880\n",
      "Epoch 3 | Step 2171100 | Avg Loss: 0.0153 | Grad Norm: 0.00932531\n",
      "Epoch 3 | Step 2171200 | Avg Loss: 0.0153 | Grad Norm: 0.00842256\n",
      "Epoch 3 | Step 2171300 | Avg Loss: 0.0153 | Grad Norm: 0.00779437\n",
      "Epoch 3 | Step 2171400 | Avg Loss: 0.0152 | Grad Norm: 0.00980123\n",
      "Epoch 3 | Step 2171500 | Avg Loss: 0.0153 | Grad Norm: 0.00798566\n",
      "Epoch 3 | Step 2171600 | Avg Loss: 0.0150 | Grad Norm: 0.00809873\n",
      "Epoch 3 | Step 2171700 | Avg Loss: 0.0148 | Grad Norm: 0.00893830\n",
      "Epoch 3 | Step 2171800 | Avg Loss: 0.0150 | Grad Norm: 0.00751548\n",
      "Epoch 3 | Step 2171900 | Avg Loss: 0.0149 | Grad Norm: 0.00700879\n",
      "Epoch 3 | Step 2172000 | Avg Loss: 0.0151 | Grad Norm: 0.00733448\n",
      "Epoch 3 | Step 2172100 | Avg Loss: 0.0148 | Grad Norm: 0.00818319\n",
      "Epoch 3 | Step 2172200 | Avg Loss: 0.0150 | Grad Norm: 0.00711975\n",
      "Epoch 3 | Step 2172300 | Avg Loss: 0.0150 | Grad Norm: 0.00757079\n",
      "Epoch 3 | Step 2172400 | Avg Loss: 0.0152 | Grad Norm: 0.00798617\n",
      "Epoch 3 | Step 2172500 | Avg Loss: 0.0155 | Grad Norm: 0.00689883\n",
      "Epoch 3 | Step 2172600 | Avg Loss: 0.0156 | Grad Norm: 0.00898968\n",
      "Epoch 3 | Step 2172700 | Avg Loss: 0.0154 | Grad Norm: 0.00694038\n",
      "Epoch 3 | Step 2172800 | Avg Loss: 0.0153 | Grad Norm: 0.00704930\n",
      "Epoch 3 | Step 2172900 | Avg Loss: 0.0151 | Grad Norm: 0.00742489\n",
      "Epoch 3 | Step 2173000 | Avg Loss: 0.0149 | Grad Norm: 0.00908084\n",
      "Epoch 3 | Step 2173100 | Avg Loss: 0.0150 | Grad Norm: 0.00771471\n",
      "Epoch 3 | Step 2173200 | Avg Loss: 0.0152 | Grad Norm: 0.00797740\n",
      "Epoch 3 | Step 2173300 | Avg Loss: 0.0152 | Grad Norm: 0.00830365\n",
      "Epoch 3 | Step 2173400 | Avg Loss: 0.0152 | Grad Norm: 0.00763302\n",
      "Epoch 3 | Step 2173500 | Avg Loss: 0.0151 | Grad Norm: 0.00786145\n",
      "Epoch 3 | Step 2173600 | Avg Loss: 0.0153 | Grad Norm: 0.00931836\n",
      "Epoch 3 | Step 2173700 | Avg Loss: 0.0150 | Grad Norm: 0.00755941\n",
      "Epoch 3 | Step 2173800 | Avg Loss: 0.0150 | Grad Norm: 0.00791963\n",
      "Epoch 3 | Step 2173900 | Avg Loss: 0.0149 | Grad Norm: 0.00875555\n",
      "Epoch 3 | Step 2174000 | Avg Loss: 0.0152 | Grad Norm: 0.00816049\n",
      "Epoch 3 | Step 2174100 | Avg Loss: 0.0154 | Grad Norm: 0.00852598\n",
      "Epoch 3 | Step 2174200 | Avg Loss: 0.0155 | Grad Norm: 0.00779794\n",
      "Epoch 3 | Step 2174300 | Avg Loss: 0.0150 | Grad Norm: 0.00826295\n",
      "Epoch 3 | Step 2174400 | Avg Loss: 0.0154 | Grad Norm: 0.00809680\n",
      "Epoch 3 | Step 2174500 | Avg Loss: 0.0150 | Grad Norm: 0.00925083\n",
      "Epoch 3 | Step 2174600 | Avg Loss: 0.0152 | Grad Norm: 0.01466002\n",
      "Epoch 3 | Step 2174700 | Avg Loss: 0.0155 | Grad Norm: 0.00708760\n",
      "Epoch 3 | Step 2174800 | Avg Loss: 0.0152 | Grad Norm: 0.00745785\n",
      "Epoch 3 | Step 2174900 | Avg Loss: 0.0151 | Grad Norm: 0.00788756\n",
      "Epoch 3 | Step 2175000 | Avg Loss: 0.0149 | Grad Norm: 0.00794042\n",
      "Epoch 3 | Step 2175100 | Avg Loss: 0.0151 | Grad Norm: 0.00762825\n",
      "Epoch 3 | Step 2175200 | Avg Loss: 0.0151 | Grad Norm: 0.00843616\n",
      "Epoch 3 | Step 2175300 | Avg Loss: 0.0149 | Grad Norm: 0.00758456\n",
      "Epoch 3 | Step 2175400 | Avg Loss: 0.0151 | Grad Norm: 0.00706828\n",
      "Epoch 3 | Step 2175500 | Avg Loss: 0.0151 | Grad Norm: 0.00805127\n",
      "Epoch 3 | Step 2175600 | Avg Loss: 0.0153 | Grad Norm: 0.00988670\n",
      "Epoch 3 | Step 2175700 | Avg Loss: 0.0151 | Grad Norm: 0.00858621\n",
      "Epoch 3 | Step 2175800 | Avg Loss: 0.0156 | Grad Norm: 0.00843722\n",
      "Epoch 3 | Step 2175900 | Avg Loss: 0.0154 | Grad Norm: 0.00774608\n",
      "Epoch 3 | Step 2176000 | Avg Loss: 0.0151 | Grad Norm: 0.00772077\n",
      "Epoch 3 | Step 2176100 | Avg Loss: 0.0155 | Grad Norm: 0.00828725\n",
      "Epoch 3 | Step 2176200 | Avg Loss: 0.0155 | Grad Norm: 0.00990769\n",
      "Epoch 3 | Step 2176300 | Avg Loss: 0.0152 | Grad Norm: 0.00921676\n",
      "Epoch 3 | Step 2176400 | Avg Loss: 0.0154 | Grad Norm: 0.00844348\n",
      "Epoch 3 | Step 2176500 | Avg Loss: 0.0154 | Grad Norm: 0.00816547\n",
      "Epoch 3 | Step 2176600 | Avg Loss: 0.0153 | Grad Norm: 0.00846850\n",
      "Epoch 3 | Step 2176700 | Avg Loss: 0.0154 | Grad Norm: 0.01043193\n",
      "Epoch 3 | Step 2176800 | Avg Loss: 0.0152 | Grad Norm: 0.00775418\n",
      "Epoch 3 | Step 2176900 | Avg Loss: 0.0151 | Grad Norm: 0.00795889\n",
      "Epoch 3 | Step 2177000 | Avg Loss: 0.0152 | Grad Norm: 0.00816614\n",
      "Epoch 3 | Step 2177100 | Avg Loss: 0.0155 | Grad Norm: 0.00803339\n",
      "Epoch 3 | Step 2177200 | Avg Loss: 0.0152 | Grad Norm: 0.00804188\n",
      "Epoch 3 | Step 2177300 | Avg Loss: 0.0150 | Grad Norm: 0.00674551\n",
      "Epoch 3 | Step 2177400 | Avg Loss: 0.0153 | Grad Norm: 0.00752059\n",
      "Epoch 3 | Step 2177500 | Avg Loss: 0.0157 | Grad Norm: 0.00824648\n",
      "Epoch 3 | Step 2177600 | Avg Loss: 0.0153 | Grad Norm: 0.00836259\n",
      "Epoch 3 | Step 2177700 | Avg Loss: 0.0152 | Grad Norm: 0.00946046\n",
      "Epoch 3 | Step 2177800 | Avg Loss: 0.0152 | Grad Norm: 0.00854641\n",
      "Epoch 3 | Step 2177900 | Avg Loss: 0.0154 | Grad Norm: 0.00742929\n",
      "Epoch 3 | Step 2178000 | Avg Loss: 0.0150 | Grad Norm: 0.00846920\n",
      "Epoch 3 | Step 2178100 | Avg Loss: 0.0150 | Grad Norm: 0.00769122\n",
      "Epoch 3 | Step 2178200 | Avg Loss: 0.0152 | Grad Norm: 0.00835448\n",
      "Epoch 3 | Step 2178300 | Avg Loss: 0.0151 | Grad Norm: 0.00762809\n",
      "Epoch 3 | Step 2178400 | Avg Loss: 0.0153 | Grad Norm: 0.00782266\n",
      "Epoch 3 | Step 2178500 | Avg Loss: 0.0155 | Grad Norm: 0.00798694\n",
      "Epoch 3 | Step 2178600 | Avg Loss: 0.0156 | Grad Norm: 0.00764093\n",
      "Epoch 3 | Step 2178700 | Avg Loss: 0.0151 | Grad Norm: 0.00855223\n",
      "Epoch 3 | Step 2178800 | Avg Loss: 0.0153 | Grad Norm: 0.00795556\n",
      "Epoch 3 | Step 2178900 | Avg Loss: 0.0153 | Grad Norm: 0.00816711\n",
      "Epoch 3 | Step 2179000 | Avg Loss: 0.0154 | Grad Norm: 0.00766561\n",
      "Epoch 3 | Step 2179100 | Avg Loss: 0.0156 | Grad Norm: 0.00838183\n",
      "Epoch 3 | Step 2179200 | Avg Loss: 0.0155 | Grad Norm: 0.00786527\n",
      "Epoch 3 | Step 2179300 | Avg Loss: 0.0157 | Grad Norm: 0.00777526\n",
      "Epoch 3 | Step 2179400 | Avg Loss: 0.0154 | Grad Norm: 0.00852530\n",
      "Epoch 3 | Step 2179500 | Avg Loss: 0.0157 | Grad Norm: 0.00710870\n",
      "Epoch 3 | Step 2179600 | Avg Loss: 0.0153 | Grad Norm: 0.00731124\n",
      "Epoch 3 | Step 2179700 | Avg Loss: 0.0151 | Grad Norm: 0.00786991\n",
      "Epoch 3 | Step 2179800 | Avg Loss: 0.0151 | Grad Norm: 0.00740707\n",
      "Epoch 3 | Step 2179900 | Avg Loss: 0.0154 | Grad Norm: 0.00775231\n",
      "Epoch 3 | Step 2180000 | Avg Loss: 0.0154 | Grad Norm: 0.00841732\n",
      "Epoch 3 | Step 2180100 | Avg Loss: 0.0152 | Grad Norm: 0.00820485\n",
      "Epoch 3 | Step 2180200 | Avg Loss: 0.0154 | Grad Norm: 0.00805925\n",
      "Epoch 3 | Step 2180300 | Avg Loss: 0.0153 | Grad Norm: 0.00807348\n",
      "Epoch 3 | Step 2180400 | Avg Loss: 0.0156 | Grad Norm: 0.00715109\n",
      "Epoch 3 | Step 2180500 | Avg Loss: 0.0152 | Grad Norm: 0.00991853\n",
      "Epoch 3 | Step 2180600 | Avg Loss: 0.0151 | Grad Norm: 0.00855492\n",
      "Epoch 3 | Step 2180700 | Avg Loss: 0.0150 | Grad Norm: 0.00761656\n",
      "Epoch 3 | Step 2180800 | Avg Loss: 0.0151 | Grad Norm: 0.01069113\n",
      "Epoch 3 | Step 2180900 | Avg Loss: 0.0151 | Grad Norm: 0.00984598\n",
      "Epoch 3 | Step 2181000 | Avg Loss: 0.0148 | Grad Norm: 0.00855616\n",
      "Epoch 3 | Step 2181100 | Avg Loss: 0.0147 | Grad Norm: 0.00809422\n",
      "Epoch 3 | Step 2181200 | Avg Loss: 0.0148 | Grad Norm: 0.00831021\n",
      "Epoch 3 | Step 2181300 | Avg Loss: 0.0152 | Grad Norm: 0.00801103\n",
      "Epoch 3 | Step 2181400 | Avg Loss: 0.0148 | Grad Norm: 0.00824726\n",
      "Epoch 3 | Step 2181500 | Avg Loss: 0.0154 | Grad Norm: 0.00995489\n",
      "Epoch 3 | Step 2181600 | Avg Loss: 0.0149 | Grad Norm: 0.00572489\n",
      "Epoch 3 | Step 2181700 | Avg Loss: 0.0150 | Grad Norm: 0.00778333\n",
      "Epoch 3 | Step 2181800 | Avg Loss: 0.0152 | Grad Norm: 0.00879093\n",
      "Epoch 3 | Step 2181900 | Avg Loss: 0.0153 | Grad Norm: 0.00800954\n",
      "Epoch 3 | Step 2182000 | Avg Loss: 0.0152 | Grad Norm: 0.00787098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f47a5f257c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#criterion = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#for _ in range(1000000):\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get data from the dataloader\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/nnue_training_split_model_200M_2_\")\n",
    "\n",
    "# hyperparameters\n",
    "nr_epochs = 10000\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "scaling_factor = 400\n",
    "ground_truth_scaling_factor = 400\n",
    "lambda_ = 0.2\n",
    "log_interval = 100\n",
    "save_interval = 100000\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "epsilon = 1e-10\n",
    "\n",
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_200M_2100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "\n",
    "        #for _ in range(1000000):\n",
    "    \n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y, result = batch\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        result = result.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = lambda_*result + (1 - lambda_) * torch.sigmoid(batch_y / ground_truth_scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        #loss = (wdl_batch_y * torch.log(wdl_batch_y + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_batch_y + epsilon)) -(wdl_batch_y * torch.log(wdl_pred   + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_pred   + epsilon))\n",
    "        #loss = loss.mean()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(wdl_pred, wdl_batch_y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # make a step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        # calculate the gradient norm\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm of this parameter's gradient\n",
    "                total_norm_sq += param_norm.item() ** 2\n",
    "        total_grad_norm = total_norm_sq ** 0.5\n",
    "        # Now total_grad_norm is the L2 norm of all gradients combined.\n",
    "        #print(f\"Step {step}  Grad Norm = {total_grad_norm:.8f}\")\n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Avg Loss: {avg_loss:.4f} | Grad Norm: {total_grad_norm:.8f}\")\n",
    "            running_loss = 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "            writer.add_scalar(\"Grad Norm\", total_grad_norm, step)\n",
    "            writer.add_scalar(\"WDL Pred\", torch.mean(wdl_pred).item(), step)\n",
    "            writer.add_scalar(\"WDL BatchY\", torch.mean(wdl_batch_y).item(), step)\n",
    "            writer.add_scalar(\"Pred\", torch.median(pred).item(), step)\n",
    "            writer.add_scalar(\"BatchY\", torch.median(batch_y).item(), step)\n",
    "\n",
    "\n",
    "            # log separate grad norms\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    writer.add_scalar(f'GradNorm/{name}', grad_norm, step)\n",
    "\n",
    "        # Save the model every `save_interval` steps\n",
    "        if step % save_interval == 0:\n",
    "            model_name = 'saved_models/split_model_200M_' + str(step) + \".pth\"\n",
    "            print(\"Saving model at step\" + str(step))\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,}, model_name)\n",
    "            \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d09a",
   "metadata": {},
   "source": [
    "### Postprocessing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_200M_2100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def save_layer(layer, name):\n",
    "    w = layer.weight.detach().numpy()\n",
    "    b = layer.bias.detach().numpy()\n",
    "    with open(f\"{name}_weights.txt\", \"w\") as f:\n",
    "        for row in w:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    with open(f\"{name}_biases.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(map(str, b)))\n",
    "\n",
    "save_layer(model.fc1, \"model/layer1\")\n",
    "save_layer(model.fc2, \"model/layer2\")\n",
    "save_layer(model.fc3, \"model/layer3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "fen2 = 'rnbqkbnr/pppppppp/8/8/8/7N/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "fen3 = 'rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "\n",
    "start_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "\n",
    "#torch.tensor(FEN_to_input(fen1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1, b1, stm1 = FEN_to_inputs(fen1)\n",
    "    w1 = w1.unsqueeze(0)\n",
    "    b1 = b1.unsqueeze(0)\n",
    "    stm1 = stm1.unsqueeze(0)\n",
    "    print(w1.shape, b1.shape, stm1.shape)\n",
    "    #in_1 = np.argwhere(input1.numpy() == 1)\n",
    "    #print(sum(input1.numpy()))\n",
    "    w2, b2, stm2 = FEN_to_inputs(fen2)\n",
    "    w2 = w2.unsqueeze(0)\n",
    "    b2 = b2.unsqueeze(0)\n",
    "    stm2 = stm2.unsqueeze(0)\n",
    "    #print(np.argwhere(input2.numpy() == 1))\n",
    "    w3, b3, stm3 = FEN_to_inputs(fen3)\n",
    "    w3 = w3.unsqueeze(0)\n",
    "    b3 = b3.unsqueeze(0)\n",
    "    stm3 = stm3.unsqueeze(0)\n",
    "    #print(np.argwhere(input3.numpy() == 1))\n",
    "\n",
    "    #in_start = np.argwhere(FEN_to_inputs(start_fen).numpy() == 1)\n",
    "\n",
    "    pred1 = model(w1, b1, stm1)\n",
    "    pred2 = model(w2, b2, stm2)\n",
    "    pred3 = model(w3, b3, stm3)\n",
    "\n",
    "    print(pred1.item())\n",
    "    print(pred2.item())\n",
    "    print(pred3.item())\n",
    "\n",
    "    #accumulator = model.fc1(input1)\n",
    "    #ws, bs, stms = FEN_to_inputs(start_fen)\n",
    "    #ws = ws.unsqueeze(0)\n",
    "    #bs = bs.unsqueeze(0)\n",
    "    #stms = stms.unsqueeze(0)\n",
    "    w_accumulator = model.fc1(w1)\n",
    "    b_accumulator = model.fc1(b1)\n",
    "    #print(w_accumulator)\n",
    "    #print(b_accumulator)\n",
    "\n",
    "\n",
    "    cat_wb = torch.cat([w_accumulator, b_accumulator], dim=1)\n",
    "    cat_bw = torch.cat([b_accumulator, w_accumulator], dim=1)\n",
    "\n",
    "    #stm1 = stm1.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "    #print(cat_bw)\n",
    "\n",
    "    accumulator = (1 - stm1) * cat_wb + stm1 * cat_bw\n",
    "    #print(accumulator)\n",
    "\n",
    "    x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "    x = model.fc2(x)\n",
    "\n",
    "    #print(x)\n",
    "\n",
    "    print(model.fc2.bias.detach().numpy())\n",
    "    #print(model.fc2.weight[0][:10])\n",
    "\n",
    "    #print(\"weights[0][0]\")\n",
    "    #print(model.fc1.weight[0][0])\n",
    "    #print(\"weights[1][0]\")\n",
    "    #print(model.fc1.weight[1][0])\n",
    "    #print(model.fc1.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of bins\n",
    "num_bins = 64\n",
    "bin_edges = np.linspace(-32003, 32003, num_bins + 1)\n",
    "counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "filename = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/scores.txt'\n",
    "\n",
    "# Re-read the file and bin values\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            val = float(line.strip())\n",
    "            # Determine bin index\n",
    "            bin_idx = np.searchsorted(bin_edges, val, side='right') - 1\n",
    "            if 0 <= bin_idx < num_bins:\n",
    "                counts[bin_idx] += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Find empty bins\n",
    "empty_bins = []\n",
    "for i, count in enumerate(counts):\n",
    "    if count == 0:\n",
    "        left_edge = bin_edges[i]\n",
    "        right_edge = bin_edges[i + 1]\n",
    "        empty_bins.append((i, left_edge, right_edge))\n",
    "\n",
    "# Print empty bin ranges\n",
    "print(\"Empty bins:\")\n",
    "for i, left, right in empty_bins:\n",
    "    print(f\"Bin {i}: [{left}, {right})\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge')\n",
    "plt.title(\"Histogram (streamed)\")\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP to WDL conversion\n",
    "scaling_factor = 400\n",
    "score = torch.tensor(32000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(-1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(0, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41259",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_2 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 13, 14, 15, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "\n",
    "print(np.sort(in_start.reshape(1, 32)))\n",
    "print(np.sort(in_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481ab8",
   "metadata": {},
   "source": [
    "### HalfKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "\n",
    "def FEN_to_HalfKP(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    position = 0\n",
    "    white_king_position = 0\n",
    "    black_king_position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            elif char == 'K':\n",
    "                white_king_position = position\n",
    "                position += 1\n",
    "            elif char == 'k':\n",
    "                black_king_position = position\n",
    "                position += 1\n",
    "            else:\n",
    "                position += 1\n",
    "\n",
    "    white_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    black_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                if (char != 'K') & (char != 'k'):\n",
    "                    piece_index = (piece_dict[char] % 6) * 2 + (piece_dict[char] > 5)\n",
    "                    white_input_layer[position + (piece_index + white_king_position*10)*64] = 1\n",
    "                    black_input_layer[position + (piece_index + black_king_position*10)*64] = 1\n",
    "                    position += 1\n",
    "                else:\n",
    "                    position += 1\n",
    "\n",
    "    return torch.tensor(white_input_layer, dtype=torch.float32), torch.tensor(black_input_layer, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6378bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class HalfKP_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_HalfKP(row[0].strip())\n",
    "                score = float(row[1].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                buf.append((w_in, b_in, stm, score))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                yield w_in, b_in, stm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_FEATURES = 40960\n",
    "M = 1024\n",
    "N = 32\n",
    "K = 1\n",
    "\n",
    "class HalfKPNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HalfKPNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, M)\n",
    "        self.fc2 = nn.Linear(2*M, N)\n",
    "        self.fc3 = nn.Linear(N, K)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)  # [B, 2*M]\n",
    "        cat_bw = torch.cat([b, w], dim=1)  # [B, 2*M]\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = stm * cat_wb + (1 - stm) * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0.0, max = 1.0)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data.csv'\n",
    "dataset = HalfKP_Dataset(csv_path, shuffle_buffer=1000)\n",
    "loader = DataLoader(dataset, batch_size = 128, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/halfKP\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nr_epochs = 500\n",
    "model = HalfKPNNUE().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.05)\n",
    "total_size = 200000000\n",
    "batch_size = 128\n",
    "steps_per_epoch = total_size // batch_size\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "MAE_loss = nn.L1Loss()\n",
    "lowest_MAE = 10000\n",
    "\n",
    "# Transform the CP scores to the WDL space\n",
    "scaling_factor = 400\n",
    "\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "log_interval = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "        #for _ in range(100000):\n",
    "\n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y = batch\n",
    "\n",
    "        # move data to GPU\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)  # remove the last dimension\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = torch.sigmoid(batch_y / scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        # calculate the MSE loss\n",
    "        loss = criterion(wdl_batch_y, wdl_pred)\n",
    "        MAE = MAE_loss(wdl_batch_y, wdl_pred)\n",
    "        running_loss += loss.item()\n",
    "        running_mae += MAE\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            avg_mae = running_mae / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{steps_per_epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"MAE/train\", avg_mae, step)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "\n",
    "        # calculate MAE\n",
    "        if MAE < 0.0002:\n",
    "            lowest_MAE = MAE\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with MAE: {lowest_MAE.item():.4f}, loss: {loss.item():.4f}\")\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, MAE: {MAE.item():.4f}, lowest MAE: {lowest_MAE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def FEN_to_input(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                input_layer[position + piece_dict[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        #self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.clamp(self.fc1(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc3(x), min = 0, max = 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                fen, score, result = FEN_to_input(row[0].strip()), float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield fen, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for fen, score, result in stream:\n",
    "                buf.append((fen, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for fen, score, result in stream:\n",
    "                yield fen, score, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
