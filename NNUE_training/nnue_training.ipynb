{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5d7453",
   "metadata": {},
   "source": [
    "# NNUE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821225",
   "metadata": {},
   "source": [
    "Great source on NNUE: https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead68bd",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Stockfish has a lot of data available for NNUE training in the .binpack format. They have a repo for training NNUEs (nnue-pytorch) that enables efficient dataloading with this format. I don't want to use nnue-pytorch, i want to make my own NNUE training setup.\n",
    "\n",
    "The nnue-pytorch repo also has information on training datasets for NNUEs: https://github.com/official-stockfish/nnue-pytorch/wiki/Training-datasets. They explain how to make your own dataset and link some of the datasets they generated. I will use some of this data, because generating the data myself would be too time-consuming on my hardware.\n",
    "\n",
    "Currently using training data: test80-2024-01-jan-2tb7p.min-v2.v6.binpack.zst from https://huggingface.co/datasets/linrock/test80-2024/tree/main\n",
    "\n",
    "This file contains billions of positions with evaluations in the .binpack format. The stockfish tools branch has a tool to covert the .binpack data into .plain data (https://github.com/official-stockfish/Stockfish/blob/tools/docs/convert.md). I used this tool and stored the first 200M evaluated positions.\n",
    "\n",
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1c61",
   "metadata": {},
   "source": [
    "### Turn FEN into input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict_w = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "piece_dict_b = {'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K':11, 'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "def FEN_to_inputs(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer_w = np.zeros(768, dtype = np.float32)\n",
    "    input_layer_b = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                alt_pos = 63 - (position ^ 7)\n",
    "                input_layer_w[position + piece_dict_w[char]*64] = 1\n",
    "                input_layer_b[alt_pos + piece_dict_b[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer_w, dtype=torch.float32), torch.tensor(input_layer_b, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1938bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Black Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Side to Move: tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54689/2390657366.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(w_features)))\n",
      "/tmp/ipykernel_54689/2390657366.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(b_features)))\n"
     ]
    }
   ],
   "source": [
    "# testing encoding\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "\n",
    "w_features, b_features, stm = FEN_to_inputs(fen1)\n",
    "print(\"White Features:\", sum(w_features))\n",
    "print(np.nonzero(np.array(w_features)))\n",
    "print(\"Black Features:\", sum(b_features))\n",
    "print(np.nonzero(np.array(b_features)))\n",
    "print(\"Side to Move:\", stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cdf67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test1 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 14, 15, 21, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "test2 = [ 8, 9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192, 199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510, 570, 573, 632, 639, 699, 764]\n",
    "\n",
    "np.sort(test1)\n",
    "np.sort(test2)\n",
    "print(np.array_equal(np.sort(test1), np.sort(test2)))\n",
    "\n",
    "test3 = [  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192, 199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,  570, 573, 632, 639, 699, 764]\n",
    "test4 = [632, 505, 570, 699, 764, 573, 510, 639, 432, 433, 434, 435, 436, 438, 439, 429, 8, 9, 10, 11, 12, 13, 14, 15, 192, 65, 130, 259, 324, 133, 70, 199]\n",
    "print(np.array_equal(np.sort(test3), np.sort(test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c0263",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Input: a sparse, binary array of length 768. Each element of the array represents a possible combination of piece type (6), piece_color (2) and position (64) (6*2*64 = 768).\n",
    "\n",
    "This is a very simple input feature (P feature set) set that will be improved upon later (HalfKP).\n",
    "\n",
    "The fully connected feedfoward network has 4 hidden layers: 768 -> 1024, 1024 -> 8, 8 -> 32 and 32 -> 1.\n",
    "\n",
    "The output is a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Split_NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split_NNUE, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)\n",
    "        cat_bw = torch.cat([b, w], dim=1)\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = (1 - stm) * cat_wb + stm * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Split_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_inputs(row[0].strip())\n",
    "                score, result = float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                buf.append((w_in, b_in, stm, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                yield w_in, b_in, stm, score, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data_full.csv'\n",
    "dataset = Custom_Split_Dataset(csv_path, shuffle_buffer = 100000)\n",
    "loader = DataLoader(dataset, batch_size = 1024, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa7cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Kaiming uniform for piecewise-linear (ReLU-like) activations:\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0.0, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5bc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | Avg Loss: 0.0173 | Grad Norm: 0.00932407\n",
      "Epoch 1 | Step 200 | Avg Loss: 0.0150 | Grad Norm: 0.01012201\n",
      "Epoch 1 | Step 300 | Avg Loss: 0.0140 | Grad Norm: 0.00987712\n",
      "Epoch 1 | Step 400 | Avg Loss: 0.0135 | Grad Norm: 0.01019695\n",
      "Epoch 1 | Step 500 | Avg Loss: 0.0135 | Grad Norm: 0.01116443\n",
      "Epoch 1 | Step 600 | Avg Loss: 0.0136 | Grad Norm: 0.00957682\n",
      "Epoch 1 | Step 700 | Avg Loss: 0.0138 | Grad Norm: 0.01140660\n",
      "Epoch 1 | Step 800 | Avg Loss: 0.0139 | Grad Norm: 0.01087895\n",
      "Epoch 1 | Step 900 | Avg Loss: 0.0142 | Grad Norm: 0.01017237\n",
      "Epoch 1 | Step 1000 | Avg Loss: 0.0138 | Grad Norm: 0.00899415\n",
      "Epoch 1 | Step 1100 | Avg Loss: 0.0134 | Grad Norm: 0.00913476\n",
      "Epoch 1 | Step 1200 | Avg Loss: 0.0133 | Grad Norm: 0.00934295\n",
      "Epoch 1 | Step 1300 | Avg Loss: 0.0135 | Grad Norm: 0.00930398\n",
      "Epoch 1 | Step 1400 | Avg Loss: 0.0133 | Grad Norm: 0.00952346\n",
      "Epoch 1 | Step 1500 | Avg Loss: 0.0132 | Grad Norm: 0.00858753\n",
      "Epoch 1 | Step 1600 | Avg Loss: 0.0131 | Grad Norm: 0.00890785\n",
      "Epoch 1 | Step 1700 | Avg Loss: 0.0130 | Grad Norm: 0.01027594\n",
      "Epoch 1 | Step 1800 | Avg Loss: 0.0131 | Grad Norm: 0.01047141\n",
      "Epoch 1 | Step 1900 | Avg Loss: 0.0134 | Grad Norm: 0.00902084\n",
      "Epoch 1 | Step 2000 | Avg Loss: 0.0134 | Grad Norm: 0.00980479\n",
      "Epoch 1 | Step 2100 | Avg Loss: 0.0131 | Grad Norm: 0.01206486\n",
      "Epoch 1 | Step 2200 | Avg Loss: 0.0130 | Grad Norm: 0.00981093\n",
      "Epoch 1 | Step 2300 | Avg Loss: 0.0135 | Grad Norm: 0.00969714\n",
      "Epoch 1 | Step 2400 | Avg Loss: 0.0132 | Grad Norm: 0.01012011\n",
      "Epoch 1 | Step 2500 | Avg Loss: 0.0134 | Grad Norm: 0.00930186\n",
      "Epoch 1 | Step 2600 | Avg Loss: 0.0134 | Grad Norm: 0.01144373\n",
      "Epoch 1 | Step 2700 | Avg Loss: 0.0136 | Grad Norm: 0.00837465\n",
      "Epoch 1 | Step 2800 | Avg Loss: 0.0134 | Grad Norm: 0.01127145\n",
      "Epoch 1 | Step 2900 | Avg Loss: 0.0135 | Grad Norm: 0.01035194\n",
      "Epoch 1 | Step 3000 | Avg Loss: 0.0136 | Grad Norm: 0.01124440\n",
      "Epoch 1 | Step 3100 | Avg Loss: 0.0133 | Grad Norm: 0.01058685\n",
      "Epoch 1 | Step 3200 | Avg Loss: 0.0129 | Grad Norm: 0.00996814\n",
      "Epoch 1 | Step 3300 | Avg Loss: 0.0128 | Grad Norm: 0.01001168\n",
      "Epoch 1 | Step 3400 | Avg Loss: 0.0129 | Grad Norm: 0.00907419\n",
      "Epoch 1 | Step 3500 | Avg Loss: 0.0126 | Grad Norm: 0.01023806\n",
      "Epoch 1 | Step 3600 | Avg Loss: 0.0128 | Grad Norm: 0.00925439\n",
      "Epoch 1 | Step 3700 | Avg Loss: 0.0131 | Grad Norm: 0.00990511\n",
      "Epoch 1 | Step 3800 | Avg Loss: 0.0131 | Grad Norm: 0.01018004\n",
      "Epoch 1 | Step 3900 | Avg Loss: 0.0130 | Grad Norm: 0.01019431\n",
      "Epoch 1 | Step 4000 | Avg Loss: 0.0130 | Grad Norm: 0.00848500\n",
      "Epoch 1 | Step 4100 | Avg Loss: 0.0131 | Grad Norm: 0.00999713\n",
      "Epoch 1 | Step 4200 | Avg Loss: 0.0129 | Grad Norm: 0.01062197\n",
      "Epoch 1 | Step 4300 | Avg Loss: 0.0129 | Grad Norm: 0.01001448\n",
      "Epoch 1 | Step 4400 | Avg Loss: 0.0132 | Grad Norm: 0.00974218\n",
      "Epoch 1 | Step 4500 | Avg Loss: 0.0130 | Grad Norm: 0.00953110\n",
      "Epoch 1 | Step 4600 | Avg Loss: 0.0128 | Grad Norm: 0.00949509\n",
      "Epoch 1 | Step 4700 | Avg Loss: 0.0128 | Grad Norm: 0.00889651\n",
      "Epoch 1 | Step 4800 | Avg Loss: 0.0132 | Grad Norm: 0.00962879\n",
      "Epoch 1 | Step 4900 | Avg Loss: 0.0131 | Grad Norm: 0.00834251\n",
      "Epoch 1 | Step 5000 | Avg Loss: 0.0132 | Grad Norm: 0.00987062\n",
      "Epoch 1 | Step 5100 | Avg Loss: 0.0131 | Grad Norm: 0.01043554\n",
      "Epoch 1 | Step 5200 | Avg Loss: 0.0127 | Grad Norm: 0.01140115\n",
      "Epoch 1 | Step 5300 | Avg Loss: 0.0132 | Grad Norm: 0.01039038\n",
      "Epoch 1 | Step 5400 | Avg Loss: 0.0134 | Grad Norm: 0.01109358\n",
      "Epoch 1 | Step 5500 | Avg Loss: 0.0134 | Grad Norm: 0.00920126\n",
      "Epoch 1 | Step 5600 | Avg Loss: 0.0131 | Grad Norm: 0.00980215\n",
      "Epoch 1 | Step 5700 | Avg Loss: 0.0132 | Grad Norm: 0.00951441\n",
      "Epoch 1 | Step 5800 | Avg Loss: 0.0133 | Grad Norm: 0.00952897\n",
      "Epoch 1 | Step 5900 | Avg Loss: 0.0131 | Grad Norm: 0.00962636\n",
      "Epoch 1 | Step 6000 | Avg Loss: 0.0131 | Grad Norm: 0.01151502\n",
      "Epoch 1 | Step 6100 | Avg Loss: 0.0128 | Grad Norm: 0.00893094\n",
      "Epoch 1 | Step 6200 | Avg Loss: 0.0132 | Grad Norm: 0.00833125\n",
      "Epoch 1 | Step 6300 | Avg Loss: 0.0136 | Grad Norm: 0.01037445\n",
      "Epoch 1 | Step 6400 | Avg Loss: 0.0134 | Grad Norm: 0.01044576\n",
      "Epoch 1 | Step 6500 | Avg Loss: 0.0133 | Grad Norm: 0.01066391\n",
      "Epoch 1 | Step 6600 | Avg Loss: 0.0134 | Grad Norm: 0.01180595\n",
      "Epoch 1 | Step 6700 | Avg Loss: 0.0136 | Grad Norm: 0.00876131\n",
      "Epoch 1 | Step 6800 | Avg Loss: 0.0134 | Grad Norm: 0.01083394\n",
      "Epoch 1 | Step 6900 | Avg Loss: 0.0134 | Grad Norm: 0.00994394\n",
      "Epoch 1 | Step 7000 | Avg Loss: 0.0132 | Grad Norm: 0.00988973\n",
      "Epoch 1 | Step 7100 | Avg Loss: 0.0130 | Grad Norm: 0.00950855\n",
      "Epoch 1 | Step 7200 | Avg Loss: 0.0127 | Grad Norm: 0.00968026\n",
      "Epoch 1 | Step 7300 | Avg Loss: 0.0127 | Grad Norm: 0.00849572\n",
      "Epoch 1 | Step 7400 | Avg Loss: 0.0129 | Grad Norm: 0.01120591\n",
      "Epoch 1 | Step 7500 | Avg Loss: 0.0128 | Grad Norm: 0.01028604\n",
      "Epoch 1 | Step 7600 | Avg Loss: 0.0132 | Grad Norm: 0.00931555\n",
      "Epoch 1 | Step 7700 | Avg Loss: 0.0128 | Grad Norm: 0.01060744\n",
      "Epoch 1 | Step 7800 | Avg Loss: 0.0130 | Grad Norm: 0.00944187\n",
      "Epoch 1 | Step 7900 | Avg Loss: 0.0131 | Grad Norm: 0.01042960\n",
      "Epoch 1 | Step 8000 | Avg Loss: 0.0134 | Grad Norm: 0.00917493\n",
      "Epoch 1 | Step 8100 | Avg Loss: 0.0130 | Grad Norm: 0.00924676\n",
      "Epoch 1 | Step 8200 | Avg Loss: 0.0131 | Grad Norm: 0.00975618\n",
      "Epoch 1 | Step 8300 | Avg Loss: 0.0131 | Grad Norm: 0.01024142\n",
      "Epoch 1 | Step 8400 | Avg Loss: 0.0130 | Grad Norm: 0.01048856\n",
      "Epoch 1 | Step 8500 | Avg Loss: 0.0132 | Grad Norm: 0.00917605\n",
      "Epoch 1 | Step 8600 | Avg Loss: 0.0135 | Grad Norm: 0.01006366\n",
      "Epoch 1 | Step 8700 | Avg Loss: 0.0134 | Grad Norm: 0.01075833\n",
      "Epoch 1 | Step 8800 | Avg Loss: 0.0133 | Grad Norm: 0.01093521\n",
      "Epoch 1 | Step 8900 | Avg Loss: 0.0132 | Grad Norm: 0.00966494\n",
      "Epoch 1 | Step 9000 | Avg Loss: 0.0133 | Grad Norm: 0.00936433\n",
      "Epoch 1 | Step 9100 | Avg Loss: 0.0132 | Grad Norm: 0.00930856\n",
      "Epoch 1 | Step 9200 | Avg Loss: 0.0132 | Grad Norm: 0.01084351\n",
      "Epoch 1 | Step 9300 | Avg Loss: 0.0131 | Grad Norm: 0.01210154\n",
      "Epoch 1 | Step 9400 | Avg Loss: 0.0131 | Grad Norm: 0.00950477\n",
      "Epoch 1 | Step 9500 | Avg Loss: 0.0134 | Grad Norm: 0.01041308\n",
      "Epoch 1 | Step 9600 | Avg Loss: 0.0133 | Grad Norm: 0.00991980\n",
      "Epoch 1 | Step 9700 | Avg Loss: 0.0133 | Grad Norm: 0.00989332\n",
      "Epoch 1 | Step 9800 | Avg Loss: 0.0132 | Grad Norm: 0.01168334\n",
      "Epoch 1 | Step 9900 | Avg Loss: 0.0132 | Grad Norm: 0.00946968\n",
      "Epoch 1 | Step 10000 | Avg Loss: 0.0133 | Grad Norm: 0.00963879\n",
      "Epoch 1 | Step 10100 | Avg Loss: 0.0138 | Grad Norm: 0.00908813\n",
      "Epoch 1 | Step 10200 | Avg Loss: 0.0137 | Grad Norm: 0.00927300\n",
      "Epoch 1 | Step 10300 | Avg Loss: 0.0135 | Grad Norm: 0.01020013\n",
      "Epoch 1 | Step 10400 | Avg Loss: 0.0133 | Grad Norm: 0.01443277\n",
      "Epoch 1 | Step 10500 | Avg Loss: 0.0136 | Grad Norm: 0.01028367\n",
      "Epoch 1 | Step 10600 | Avg Loss: 0.0138 | Grad Norm: 0.00894020\n",
      "Epoch 1 | Step 10700 | Avg Loss: 0.0136 | Grad Norm: 0.01082697\n",
      "Epoch 1 | Step 10800 | Avg Loss: 0.0136 | Grad Norm: 0.00989437\n",
      "Epoch 1 | Step 10900 | Avg Loss: 0.0131 | Grad Norm: 0.00997695\n",
      "Epoch 1 | Step 11000 | Avg Loss: 0.0131 | Grad Norm: 0.01069600\n",
      "Epoch 1 | Step 11100 | Avg Loss: 0.0136 | Grad Norm: 0.01093740\n",
      "Epoch 1 | Step 11200 | Avg Loss: 0.0132 | Grad Norm: 0.00915300\n",
      "Epoch 1 | Step 11300 | Avg Loss: 0.0136 | Grad Norm: 0.01115845\n",
      "Epoch 1 | Step 11400 | Avg Loss: 0.0132 | Grad Norm: 0.00930369\n",
      "Epoch 1 | Step 11500 | Avg Loss: 0.0134 | Grad Norm: 0.01000717\n",
      "Epoch 1 | Step 11600 | Avg Loss: 0.0132 | Grad Norm: 0.00952124\n",
      "Epoch 1 | Step 11700 | Avg Loss: 0.0130 | Grad Norm: 0.01052510\n",
      "Epoch 1 | Step 11800 | Avg Loss: 0.0129 | Grad Norm: 0.00882547\n",
      "Epoch 1 | Step 11900 | Avg Loss: 0.0129 | Grad Norm: 0.01059271\n",
      "Epoch 1 | Step 12000 | Avg Loss: 0.0129 | Grad Norm: 0.00902159\n",
      "Epoch 1 | Step 12100 | Avg Loss: 0.0127 | Grad Norm: 0.00891704\n",
      "Epoch 1 | Step 12200 | Avg Loss: 0.0129 | Grad Norm: 0.00900192\n",
      "Epoch 1 | Step 12300 | Avg Loss: 0.0130 | Grad Norm: 0.00907889\n",
      "Epoch 1 | Step 12400 | Avg Loss: 0.0132 | Grad Norm: 0.01264248\n",
      "Epoch 1 | Step 12500 | Avg Loss: 0.0136 | Grad Norm: 0.00857453\n",
      "Epoch 1 | Step 12600 | Avg Loss: 0.0131 | Grad Norm: 0.01092764\n",
      "Epoch 1 | Step 12700 | Avg Loss: 0.0130 | Grad Norm: 0.00947372\n",
      "Epoch 1 | Step 12800 | Avg Loss: 0.0130 | Grad Norm: 0.00982039\n",
      "Epoch 1 | Step 12900 | Avg Loss: 0.0130 | Grad Norm: 0.01011965\n",
      "Epoch 1 | Step 13000 | Avg Loss: 0.0134 | Grad Norm: 0.01077256\n",
      "Epoch 1 | Step 13100 | Avg Loss: 0.0131 | Grad Norm: 0.00971312\n",
      "Epoch 1 | Step 13200 | Avg Loss: 0.0131 | Grad Norm: 0.01160048\n",
      "Epoch 1 | Step 13300 | Avg Loss: 0.0129 | Grad Norm: 0.00968786\n",
      "Epoch 1 | Step 13400 | Avg Loss: 0.0129 | Grad Norm: 0.00978038\n",
      "Epoch 1 | Step 13500 | Avg Loss: 0.0132 | Grad Norm: 0.01121595\n",
      "Epoch 1 | Step 13600 | Avg Loss: 0.0130 | Grad Norm: 0.01090568\n",
      "Epoch 1 | Step 13700 | Avg Loss: 0.0128 | Grad Norm: 0.01037434\n",
      "Epoch 1 | Step 13800 | Avg Loss: 0.0129 | Grad Norm: 0.01160308\n",
      "Epoch 1 | Step 13900 | Avg Loss: 0.0136 | Grad Norm: 0.01111768\n",
      "Epoch 1 | Step 14000 | Avg Loss: 0.0133 | Grad Norm: 0.00962723\n",
      "Epoch 1 | Step 14100 | Avg Loss: 0.0130 | Grad Norm: 0.01002348\n",
      "Epoch 1 | Step 14200 | Avg Loss: 0.0132 | Grad Norm: 0.01076564\n",
      "Epoch 1 | Step 14300 | Avg Loss: 0.0129 | Grad Norm: 0.00892262\n",
      "Epoch 1 | Step 14400 | Avg Loss: 0.0130 | Grad Norm: 0.00988588\n",
      "Epoch 1 | Step 14500 | Avg Loss: 0.0130 | Grad Norm: 0.00872182\n",
      "Epoch 1 | Step 14600 | Avg Loss: 0.0130 | Grad Norm: 0.00992408\n",
      "Epoch 1 | Step 14700 | Avg Loss: 0.0132 | Grad Norm: 0.01033337\n",
      "Epoch 1 | Step 14800 | Avg Loss: 0.0129 | Grad Norm: 0.00944366\n",
      "Epoch 1 | Step 14900 | Avg Loss: 0.0130 | Grad Norm: 0.01131607\n",
      "Epoch 1 | Step 15000 | Avg Loss: 0.0131 | Grad Norm: 0.01170614\n",
      "Epoch 1 | Step 15100 | Avg Loss: 0.0130 | Grad Norm: 0.00986502\n",
      "Epoch 1 | Step 15200 | Avg Loss: 0.0129 | Grad Norm: 0.01106269\n",
      "Epoch 1 | Step 15300 | Avg Loss: 0.0130 | Grad Norm: 0.01127192\n",
      "Epoch 1 | Step 15400 | Avg Loss: 0.0129 | Grad Norm: 0.01048238\n",
      "Epoch 1 | Step 15500 | Avg Loss: 0.0129 | Grad Norm: 0.00891389\n",
      "Epoch 1 | Step 15600 | Avg Loss: 0.0133 | Grad Norm: 0.01021230\n",
      "Epoch 1 | Step 15700 | Avg Loss: 0.0137 | Grad Norm: 0.01032627\n",
      "Epoch 1 | Step 15800 | Avg Loss: 0.0141 | Grad Norm: 0.01038778\n",
      "Epoch 1 | Step 15900 | Avg Loss: 0.0139 | Grad Norm: 0.01028934\n",
      "Epoch 1 | Step 16000 | Avg Loss: 0.0138 | Grad Norm: 0.01296634\n",
      "Epoch 1 | Step 16100 | Avg Loss: 0.0138 | Grad Norm: 0.01006111\n",
      "Epoch 1 | Step 16200 | Avg Loss: 0.0138 | Grad Norm: 0.01126075\n",
      "Epoch 1 | Step 16300 | Avg Loss: 0.0136 | Grad Norm: 0.01124198\n",
      "Epoch 1 | Step 16400 | Avg Loss: 0.0136 | Grad Norm: 0.01040573\n",
      "Epoch 1 | Step 16500 | Avg Loss: 0.0136 | Grad Norm: 0.01105780\n",
      "Epoch 1 | Step 16600 | Avg Loss: 0.0131 | Grad Norm: 0.01089148\n",
      "Epoch 1 | Step 16700 | Avg Loss: 0.0130 | Grad Norm: 0.00980272\n",
      "Epoch 1 | Step 16800 | Avg Loss: 0.0133 | Grad Norm: 0.01178726\n",
      "Epoch 1 | Step 16900 | Avg Loss: 0.0133 | Grad Norm: 0.00930918\n",
      "Epoch 1 | Step 17000 | Avg Loss: 0.0135 | Grad Norm: 0.00898919\n",
      "Epoch 1 | Step 17100 | Avg Loss: 0.0132 | Grad Norm: 0.00916877\n",
      "Epoch 1 | Step 17200 | Avg Loss: 0.0130 | Grad Norm: 0.00968905\n",
      "Epoch 1 | Step 17300 | Avg Loss: 0.0131 | Grad Norm: 0.01137693\n",
      "Epoch 1 | Step 17400 | Avg Loss: 0.0132 | Grad Norm: 0.01055212\n",
      "Epoch 1 | Step 17500 | Avg Loss: 0.0130 | Grad Norm: 0.01094976\n",
      "Epoch 1 | Step 17600 | Avg Loss: 0.0128 | Grad Norm: 0.00958738\n",
      "Epoch 1 | Step 17700 | Avg Loss: 0.0128 | Grad Norm: 0.01155762\n",
      "Epoch 1 | Step 17800 | Avg Loss: 0.0130 | Grad Norm: 0.00922498\n",
      "Epoch 1 | Step 17900 | Avg Loss: 0.0130 | Grad Norm: 0.01035190\n",
      "Epoch 1 | Step 18000 | Avg Loss: 0.0129 | Grad Norm: 0.01142252\n",
      "Epoch 1 | Step 18100 | Avg Loss: 0.0129 | Grad Norm: 0.00923541\n",
      "Epoch 1 | Step 18200 | Avg Loss: 0.0129 | Grad Norm: 0.00920224\n",
      "Epoch 1 | Step 18300 | Avg Loss: 0.0129 | Grad Norm: 0.01005644\n",
      "Epoch 1 | Step 18400 | Avg Loss: 0.0131 | Grad Norm: 0.01054704\n",
      "Epoch 1 | Step 18500 | Avg Loss: 0.0133 | Grad Norm: 0.00966711\n",
      "Epoch 1 | Step 18600 | Avg Loss: 0.0133 | Grad Norm: 0.00967200\n",
      "Epoch 1 | Step 18700 | Avg Loss: 0.0132 | Grad Norm: 0.00891360\n",
      "Epoch 1 | Step 18800 | Avg Loss: 0.0134 | Grad Norm: 0.01020623\n",
      "Epoch 1 | Step 18900 | Avg Loss: 0.0134 | Grad Norm: 0.01026314\n",
      "Epoch 1 | Step 19000 | Avg Loss: 0.0134 | Grad Norm: 0.00958573\n",
      "Epoch 1 | Step 19100 | Avg Loss: 0.0136 | Grad Norm: 0.01110867\n",
      "Epoch 1 | Step 19200 | Avg Loss: 0.0135 | Grad Norm: 0.01082614\n",
      "Epoch 1 | Step 19300 | Avg Loss: 0.0133 | Grad Norm: 0.01071538\n",
      "Epoch 1 | Step 19400 | Avg Loss: 0.0128 | Grad Norm: 0.01085732\n",
      "Epoch 1 | Step 19500 | Avg Loss: 0.0127 | Grad Norm: 0.01133713\n",
      "Epoch 1 | Step 19600 | Avg Loss: 0.0124 | Grad Norm: 0.01036871\n",
      "Epoch 1 | Step 19700 | Avg Loss: 0.0129 | Grad Norm: 0.01016871\n",
      "Epoch 1 | Step 19800 | Avg Loss: 0.0127 | Grad Norm: 0.00927775\n",
      "Epoch 1 | Step 19900 | Avg Loss: 0.0128 | Grad Norm: 0.01004366\n",
      "Epoch 1 | Step 20000 | Avg Loss: 0.0130 | Grad Norm: 0.01196444\n",
      "Epoch 1 | Step 20100 | Avg Loss: 0.0127 | Grad Norm: 0.01099575\n",
      "Epoch 1 | Step 20200 | Avg Loss: 0.0124 | Grad Norm: 0.01085960\n",
      "Epoch 1 | Step 20300 | Avg Loss: 0.0129 | Grad Norm: 0.00935011\n",
      "Epoch 1 | Step 20400 | Avg Loss: 0.0130 | Grad Norm: 0.00915249\n",
      "Epoch 1 | Step 20500 | Avg Loss: 0.0133 | Grad Norm: 0.01096413\n",
      "Epoch 1 | Step 20600 | Avg Loss: 0.0136 | Grad Norm: 0.01080527\n",
      "Epoch 1 | Step 20700 | Avg Loss: 0.0131 | Grad Norm: 0.00985198\n",
      "Epoch 1 | Step 20800 | Avg Loss: 0.0130 | Grad Norm: 0.01025182\n",
      "Epoch 1 | Step 20900 | Avg Loss: 0.0130 | Grad Norm: 0.01144013\n",
      "Epoch 1 | Step 21000 | Avg Loss: 0.0131 | Grad Norm: 0.01017891\n",
      "Epoch 1 | Step 21100 | Avg Loss: 0.0129 | Grad Norm: 0.00979135\n",
      "Epoch 1 | Step 21200 | Avg Loss: 0.0129 | Grad Norm: 0.00910952\n",
      "Epoch 1 | Step 21300 | Avg Loss: 0.0129 | Grad Norm: 0.01076666\n",
      "Epoch 1 | Step 21400 | Avg Loss: 0.0130 | Grad Norm: 0.01131521\n",
      "Epoch 1 | Step 21500 | Avg Loss: 0.0134 | Grad Norm: 0.01211882\n",
      "Epoch 1 | Step 21600 | Avg Loss: 0.0134 | Grad Norm: 0.00954311\n",
      "Epoch 1 | Step 21700 | Avg Loss: 0.0135 | Grad Norm: 0.01223096\n",
      "Epoch 1 | Step 21800 | Avg Loss: 0.0137 | Grad Norm: 0.01060261\n",
      "Epoch 1 | Step 21900 | Avg Loss: 0.0135 | Grad Norm: 0.00892427\n",
      "Epoch 1 | Step 22000 | Avg Loss: 0.0134 | Grad Norm: 0.01058560\n",
      "Epoch 1 | Step 22100 | Avg Loss: 0.0135 | Grad Norm: 0.01031107\n",
      "Epoch 1 | Step 22200 | Avg Loss: 0.0131 | Grad Norm: 0.01088830\n",
      "Epoch 1 | Step 22300 | Avg Loss: 0.0134 | Grad Norm: 0.01113795\n",
      "Epoch 1 | Step 22400 | Avg Loss: 0.0135 | Grad Norm: 0.01025277\n",
      "Epoch 1 | Step 22500 | Avg Loss: 0.0136 | Grad Norm: 0.00959427\n",
      "Epoch 1 | Step 22600 | Avg Loss: 0.0134 | Grad Norm: 0.00964577\n",
      "Epoch 1 | Step 22700 | Avg Loss: 0.0137 | Grad Norm: 0.01116968\n",
      "Epoch 1 | Step 22800 | Avg Loss: 0.0138 | Grad Norm: 0.01138054\n",
      "Epoch 1 | Step 22900 | Avg Loss: 0.0138 | Grad Norm: 0.01148656\n",
      "Epoch 1 | Step 23000 | Avg Loss: 0.0134 | Grad Norm: 0.01148977\n",
      "Epoch 1 | Step 23100 | Avg Loss: 0.0135 | Grad Norm: 0.00957015\n",
      "Epoch 1 | Step 23200 | Avg Loss: 0.0131 | Grad Norm: 0.01193283\n",
      "Epoch 1 | Step 23300 | Avg Loss: 0.0133 | Grad Norm: 0.01203240\n",
      "Epoch 1 | Step 23400 | Avg Loss: 0.0137 | Grad Norm: 0.00985676\n",
      "Epoch 1 | Step 23500 | Avg Loss: 0.0140 | Grad Norm: 0.01126902\n",
      "Epoch 1 | Step 23600 | Avg Loss: 0.0144 | Grad Norm: 0.01072042\n",
      "Epoch 1 | Step 23700 | Avg Loss: 0.0143 | Grad Norm: 0.01032186\n",
      "Epoch 1 | Step 23800 | Avg Loss: 0.0143 | Grad Norm: 0.01111055\n",
      "Epoch 1 | Step 23900 | Avg Loss: 0.0141 | Grad Norm: 0.01129205\n",
      "Epoch 1 | Step 24000 | Avg Loss: 0.0141 | Grad Norm: 0.01133377\n",
      "Epoch 1 | Step 24100 | Avg Loss: 0.0141 | Grad Norm: 0.01290906\n",
      "Epoch 1 | Step 24200 | Avg Loss: 0.0141 | Grad Norm: 0.01062398\n",
      "Epoch 1 | Step 24300 | Avg Loss: 0.0141 | Grad Norm: 0.01132849\n",
      "Epoch 1 | Step 24400 | Avg Loss: 0.0139 | Grad Norm: 0.01157929\n",
      "Epoch 1 | Step 24500 | Avg Loss: 0.0137 | Grad Norm: 0.00867242\n",
      "Epoch 1 | Step 24600 | Avg Loss: 0.0142 | Grad Norm: 0.01041191\n",
      "Epoch 1 | Step 24700 | Avg Loss: 0.0145 | Grad Norm: 0.01082423\n",
      "Epoch 1 | Step 24800 | Avg Loss: 0.0140 | Grad Norm: 0.00964234\n",
      "Epoch 1 | Step 24900 | Avg Loss: 0.0136 | Grad Norm: 0.01095300\n",
      "Epoch 1 | Step 25000 | Avg Loss: 0.0138 | Grad Norm: 0.00937407\n",
      "Epoch 1 | Step 25100 | Avg Loss: 0.0141 | Grad Norm: 0.01019897\n",
      "Epoch 1 | Step 25200 | Avg Loss: 0.0145 | Grad Norm: 0.00952346\n",
      "Epoch 1 | Step 25300 | Avg Loss: 0.0144 | Grad Norm: 0.00964540\n",
      "Epoch 1 | Step 25400 | Avg Loss: 0.0144 | Grad Norm: 0.00896689\n",
      "Epoch 1 | Step 25500 | Avg Loss: 0.0143 | Grad Norm: 0.01178592\n",
      "Epoch 1 | Step 25600 | Avg Loss: 0.0140 | Grad Norm: 0.00992158\n",
      "Epoch 1 | Step 25700 | Avg Loss: 0.0140 | Grad Norm: 0.00976575\n",
      "Epoch 1 | Step 25800 | Avg Loss: 0.0143 | Grad Norm: 0.00903711\n",
      "Epoch 1 | Step 25900 | Avg Loss: 0.0145 | Grad Norm: 0.01098643\n",
      "Epoch 1 | Step 26000 | Avg Loss: 0.0141 | Grad Norm: 0.00833669\n",
      "Epoch 1 | Step 26100 | Avg Loss: 0.0137 | Grad Norm: 0.00870726\n",
      "Epoch 1 | Step 26200 | Avg Loss: 0.0137 | Grad Norm: 0.01081097\n",
      "Epoch 1 | Step 26300 | Avg Loss: 0.0141 | Grad Norm: 0.01019653\n",
      "Epoch 1 | Step 26400 | Avg Loss: 0.0137 | Grad Norm: 0.01072853\n",
      "Epoch 1 | Step 26500 | Avg Loss: 0.0138 | Grad Norm: 0.00993008\n",
      "Epoch 1 | Step 26600 | Avg Loss: 0.0135 | Grad Norm: 0.01082775\n",
      "Epoch 1 | Step 26700 | Avg Loss: 0.0139 | Grad Norm: 0.01003744\n",
      "Epoch 1 | Step 26800 | Avg Loss: 0.0139 | Grad Norm: 0.01085709\n",
      "Epoch 1 | Step 26900 | Avg Loss: 0.0140 | Grad Norm: 0.00983888\n",
      "Epoch 1 | Step 27000 | Avg Loss: 0.0141 | Grad Norm: 0.00897634\n",
      "Epoch 1 | Step 27100 | Avg Loss: 0.0141 | Grad Norm: 0.01016613\n",
      "Epoch 1 | Step 27200 | Avg Loss: 0.0137 | Grad Norm: 0.01146776\n",
      "Epoch 1 | Step 27300 | Avg Loss: 0.0137 | Grad Norm: 0.01031614\n",
      "Epoch 1 | Step 27400 | Avg Loss: 0.0141 | Grad Norm: 0.01031016\n",
      "Epoch 1 | Step 27500 | Avg Loss: 0.0143 | Grad Norm: 0.01024551\n",
      "Epoch 1 | Step 27600 | Avg Loss: 0.0140 | Grad Norm: 0.01140971\n",
      "Epoch 1 | Step 27700 | Avg Loss: 0.0140 | Grad Norm: 0.01160119\n",
      "Epoch 1 | Step 27800 | Avg Loss: 0.0143 | Grad Norm: 0.00990352\n",
      "Epoch 1 | Step 27900 | Avg Loss: 0.0144 | Grad Norm: 0.01025339\n",
      "Epoch 1 | Step 28000 | Avg Loss: 0.0142 | Grad Norm: 0.00916855\n",
      "Epoch 1 | Step 28100 | Avg Loss: 0.0140 | Grad Norm: 0.00991372\n",
      "Epoch 1 | Step 28200 | Avg Loss: 0.0138 | Grad Norm: 0.01005337\n",
      "Epoch 1 | Step 28300 | Avg Loss: 0.0135 | Grad Norm: 0.01029194\n",
      "Epoch 1 | Step 28400 | Avg Loss: 0.0136 | Grad Norm: 0.01159612\n",
      "Epoch 1 | Step 28500 | Avg Loss: 0.0141 | Grad Norm: 0.00941454\n",
      "Epoch 1 | Step 28600 | Avg Loss: 0.0142 | Grad Norm: 0.01034863\n",
      "Epoch 1 | Step 28700 | Avg Loss: 0.0135 | Grad Norm: 0.00940852\n",
      "Epoch 1 | Step 28800 | Avg Loss: 0.0135 | Grad Norm: 0.01023059\n",
      "Epoch 1 | Step 28900 | Avg Loss: 0.0136 | Grad Norm: 0.01040951\n",
      "Epoch 1 | Step 29000 | Avg Loss: 0.0134 | Grad Norm: 0.00989883\n",
      "Epoch 1 | Step 29100 | Avg Loss: 0.0135 | Grad Norm: 0.01075524\n",
      "Epoch 1 | Step 29200 | Avg Loss: 0.0140 | Grad Norm: 0.01160258\n",
      "Epoch 1 | Step 29300 | Avg Loss: 0.0134 | Grad Norm: 0.01040611\n",
      "Epoch 1 | Step 29400 | Avg Loss: 0.0135 | Grad Norm: 0.01057795\n",
      "Epoch 1 | Step 29500 | Avg Loss: 0.0140 | Grad Norm: 0.01021304\n",
      "Epoch 1 | Step 29600 | Avg Loss: 0.0136 | Grad Norm: 0.01211872\n",
      "Epoch 1 | Step 29700 | Avg Loss: 0.0138 | Grad Norm: 0.01012347\n",
      "Epoch 1 | Step 29800 | Avg Loss: 0.0140 | Grad Norm: 0.00938838\n",
      "Epoch 1 | Step 29900 | Avg Loss: 0.0142 | Grad Norm: 0.01166516\n",
      "Epoch 1 | Step 30000 | Avg Loss: 0.0142 | Grad Norm: 0.01024813\n",
      "Epoch 1 | Step 30100 | Avg Loss: 0.0139 | Grad Norm: 0.00966574\n",
      "Epoch 1 | Step 30200 | Avg Loss: 0.0142 | Grad Norm: 0.01013698\n",
      "Epoch 1 | Step 30300 | Avg Loss: 0.0140 | Grad Norm: 0.01071931\n",
      "Epoch 1 | Step 30400 | Avg Loss: 0.0141 | Grad Norm: 0.00930175\n",
      "Epoch 1 | Step 30500 | Avg Loss: 0.0138 | Grad Norm: 0.01129574\n",
      "Epoch 1 | Step 30600 | Avg Loss: 0.0137 | Grad Norm: 0.01121642\n",
      "Epoch 1 | Step 30700 | Avg Loss: 0.0137 | Grad Norm: 0.00985069\n",
      "Epoch 1 | Step 30800 | Avg Loss: 0.0137 | Grad Norm: 0.01197222\n",
      "Epoch 1 | Step 30900 | Avg Loss: 0.0140 | Grad Norm: 0.01022221\n",
      "Epoch 1 | Step 31000 | Avg Loss: 0.0136 | Grad Norm: 0.01149504\n",
      "Epoch 1 | Step 31100 | Avg Loss: 0.0137 | Grad Norm: 0.00934755\n",
      "Epoch 1 | Step 31200 | Avg Loss: 0.0138 | Grad Norm: 0.01071870\n",
      "Epoch 1 | Step 31300 | Avg Loss: 0.0136 | Grad Norm: 0.00874970\n",
      "Epoch 1 | Step 31400 | Avg Loss: 0.0139 | Grad Norm: 0.01028118\n",
      "Epoch 1 | Step 31500 | Avg Loss: 0.0137 | Grad Norm: 0.00951795\n",
      "Epoch 1 | Step 31600 | Avg Loss: 0.0137 | Grad Norm: 0.01097316\n",
      "Epoch 1 | Step 31700 | Avg Loss: 0.0135 | Grad Norm: 0.01011335\n",
      "Epoch 1 | Step 31800 | Avg Loss: 0.0140 | Grad Norm: 0.01472520\n",
      "Epoch 1 | Step 31900 | Avg Loss: 0.0141 | Grad Norm: 0.01057629\n",
      "Epoch 1 | Step 32000 | Avg Loss: 0.0139 | Grad Norm: 0.01056596\n",
      "Epoch 1 | Step 32100 | Avg Loss: 0.0139 | Grad Norm: 0.00915012\n",
      "Epoch 1 | Step 32200 | Avg Loss: 0.0141 | Grad Norm: 0.01043014\n",
      "Epoch 1 | Step 32300 | Avg Loss: 0.0141 | Grad Norm: 0.01023081\n",
      "Epoch 1 | Step 32400 | Avg Loss: 0.0141 | Grad Norm: 0.01067499\n",
      "Epoch 1 | Step 32500 | Avg Loss: 0.0141 | Grad Norm: 0.01094815\n",
      "Epoch 1 | Step 32600 | Avg Loss: 0.0139 | Grad Norm: 0.01377840\n",
      "Epoch 1 | Step 32700 | Avg Loss: 0.0141 | Grad Norm: 0.01007898\n",
      "Epoch 1 | Step 32800 | Avg Loss: 0.0139 | Grad Norm: 0.00971550\n",
      "Epoch 1 | Step 32900 | Avg Loss: 0.0140 | Grad Norm: 0.00993978\n",
      "Epoch 1 | Step 33000 | Avg Loss: 0.0138 | Grad Norm: 0.01004354\n",
      "Epoch 1 | Step 33100 | Avg Loss: 0.0137 | Grad Norm: 0.01237740\n",
      "Epoch 1 | Step 33200 | Avg Loss: 0.0134 | Grad Norm: 0.01047788\n",
      "Epoch 1 | Step 33300 | Avg Loss: 0.0136 | Grad Norm: 0.01010425\n",
      "Epoch 1 | Step 33400 | Avg Loss: 0.0139 | Grad Norm: 0.00939423\n",
      "Epoch 1 | Step 33500 | Avg Loss: 0.0138 | Grad Norm: 0.01154991\n",
      "Epoch 1 | Step 33600 | Avg Loss: 0.0139 | Grad Norm: 0.01195516\n",
      "Epoch 1 | Step 33700 | Avg Loss: 0.0138 | Grad Norm: 0.00996644\n",
      "Epoch 1 | Step 33800 | Avg Loss: 0.0136 | Grad Norm: 0.01036679\n",
      "Epoch 1 | Step 33900 | Avg Loss: 0.0134 | Grad Norm: 0.00956269\n",
      "Epoch 1 | Step 34000 | Avg Loss: 0.0133 | Grad Norm: 0.01225809\n",
      "Epoch 1 | Step 34100 | Avg Loss: 0.0129 | Grad Norm: 0.01178062\n",
      "Epoch 1 | Step 34200 | Avg Loss: 0.0130 | Grad Norm: 0.01120793\n",
      "Epoch 1 | Step 34300 | Avg Loss: 0.0132 | Grad Norm: 0.00982612\n",
      "Epoch 1 | Step 34400 | Avg Loss: 0.0134 | Grad Norm: 0.01087761\n",
      "Epoch 1 | Step 34500 | Avg Loss: 0.0135 | Grad Norm: 0.00960514\n",
      "Epoch 1 | Step 34600 | Avg Loss: 0.0138 | Grad Norm: 0.01009525\n",
      "Epoch 1 | Step 34700 | Avg Loss: 0.0135 | Grad Norm: 0.01077571\n",
      "Epoch 1 | Step 34800 | Avg Loss: 0.0134 | Grad Norm: 0.01266868\n",
      "Epoch 1 | Step 34900 | Avg Loss: 0.0138 | Grad Norm: 0.01005729\n",
      "Epoch 1 | Step 35000 | Avg Loss: 0.0137 | Grad Norm: 0.00992909\n",
      "Epoch 1 | Step 35100 | Avg Loss: 0.0137 | Grad Norm: 0.01089847\n",
      "Epoch 1 | Step 35200 | Avg Loss: 0.0141 | Grad Norm: 0.00944927\n",
      "Epoch 1 | Step 35300 | Avg Loss: 0.0139 | Grad Norm: 0.00933170\n",
      "Epoch 1 | Step 35400 | Avg Loss: 0.0139 | Grad Norm: 0.00959021\n",
      "Epoch 1 | Step 35500 | Avg Loss: 0.0140 | Grad Norm: 0.00994266\n",
      "Epoch 1 | Step 35600 | Avg Loss: 0.0142 | Grad Norm: 0.01025671\n",
      "Epoch 1 | Step 35700 | Avg Loss: 0.0138 | Grad Norm: 0.01115991\n",
      "Epoch 1 | Step 35800 | Avg Loss: 0.0136 | Grad Norm: 0.01001224\n",
      "Epoch 1 | Step 35900 | Avg Loss: 0.0139 | Grad Norm: 0.00903789\n",
      "Epoch 1 | Step 36000 | Avg Loss: 0.0133 | Grad Norm: 0.01009287\n",
      "Epoch 1 | Step 36100 | Avg Loss: 0.0137 | Grad Norm: 0.00956012\n",
      "Epoch 1 | Step 36200 | Avg Loss: 0.0143 | Grad Norm: 0.00985719\n",
      "Epoch 1 | Step 36300 | Avg Loss: 0.0146 | Grad Norm: 0.01139574\n",
      "Epoch 1 | Step 36400 | Avg Loss: 0.0147 | Grad Norm: 0.01059663\n",
      "Epoch 1 | Step 36500 | Avg Loss: 0.0143 | Grad Norm: 0.01072288\n",
      "Epoch 1 | Step 36600 | Avg Loss: 0.0141 | Grad Norm: 0.01166675\n",
      "Epoch 1 | Step 36700 | Avg Loss: 0.0142 | Grad Norm: 0.01050261\n",
      "Epoch 1 | Step 36800 | Avg Loss: 0.0141 | Grad Norm: 0.01318335\n",
      "Epoch 1 | Step 36900 | Avg Loss: 0.0143 | Grad Norm: 0.01321692\n",
      "Epoch 1 | Step 37000 | Avg Loss: 0.0144 | Grad Norm: 0.00968692\n",
      "Epoch 1 | Step 37100 | Avg Loss: 0.0142 | Grad Norm: 0.01071377\n",
      "Epoch 1 | Step 37200 | Avg Loss: 0.0139 | Grad Norm: 0.01067607\n",
      "Epoch 1 | Step 37300 | Avg Loss: 0.0138 | Grad Norm: 0.01044773\n",
      "Epoch 1 | Step 37400 | Avg Loss: 0.0139 | Grad Norm: 0.01073528\n",
      "Epoch 1 | Step 37500 | Avg Loss: 0.0136 | Grad Norm: 0.01202500\n",
      "Epoch 1 | Step 37600 | Avg Loss: 0.0136 | Grad Norm: 0.01070374\n",
      "Epoch 1 | Step 37700 | Avg Loss: 0.0137 | Grad Norm: 0.01309955\n",
      "Epoch 1 | Step 37800 | Avg Loss: 0.0141 | Grad Norm: 0.01037149\n",
      "Epoch 1 | Step 37900 | Avg Loss: 0.0141 | Grad Norm: 0.01070847\n",
      "Epoch 1 | Step 38000 | Avg Loss: 0.0142 | Grad Norm: 0.01086461\n",
      "Epoch 1 | Step 38100 | Avg Loss: 0.0143 | Grad Norm: 0.01026002\n",
      "Epoch 1 | Step 38200 | Avg Loss: 0.0145 | Grad Norm: 0.00998362\n",
      "Epoch 1 | Step 38300 | Avg Loss: 0.0144 | Grad Norm: 0.01044239\n",
      "Epoch 1 | Step 38400 | Avg Loss: 0.0144 | Grad Norm: 0.01083222\n",
      "Epoch 1 | Step 38500 | Avg Loss: 0.0143 | Grad Norm: 0.01121787\n",
      "Epoch 1 | Step 38600 | Avg Loss: 0.0141 | Grad Norm: 0.00846013\n",
      "Epoch 1 | Step 38700 | Avg Loss: 0.0143 | Grad Norm: 0.01062395\n",
      "Epoch 1 | Step 38800 | Avg Loss: 0.0142 | Grad Norm: 0.00991469\n",
      "Epoch 1 | Step 38900 | Avg Loss: 0.0139 | Grad Norm: 0.00962356\n",
      "Epoch 1 | Step 39000 | Avg Loss: 0.0138 | Grad Norm: 0.01184316\n",
      "Epoch 1 | Step 39100 | Avg Loss: 0.0141 | Grad Norm: 0.00923375\n",
      "Epoch 1 | Step 39200 | Avg Loss: 0.0140 | Grad Norm: 0.01027109\n",
      "Epoch 1 | Step 39300 | Avg Loss: 0.0138 | Grad Norm: 0.00994208\n",
      "Epoch 1 | Step 39400 | Avg Loss: 0.0141 | Grad Norm: 0.01016516\n",
      "Epoch 1 | Step 39500 | Avg Loss: 0.0139 | Grad Norm: 0.01054086\n",
      "Epoch 1 | Step 39600 | Avg Loss: 0.0136 | Grad Norm: 0.01248864\n",
      "Epoch 1 | Step 39700 | Avg Loss: 0.0140 | Grad Norm: 0.00955382\n",
      "Epoch 1 | Step 39800 | Avg Loss: 0.0138 | Grad Norm: 0.01050424\n",
      "Epoch 1 | Step 39900 | Avg Loss: 0.0135 | Grad Norm: 0.01154159\n",
      "Epoch 1 | Step 40000 | Avg Loss: 0.0137 | Grad Norm: 0.01184924\n",
      "Epoch 1 | Step 40100 | Avg Loss: 0.0138 | Grad Norm: 0.00955031\n",
      "Epoch 1 | Step 40200 | Avg Loss: 0.0137 | Grad Norm: 0.00949610\n",
      "Epoch 1 | Step 40300 | Avg Loss: 0.0136 | Grad Norm: 0.00993992\n",
      "Epoch 1 | Step 40400 | Avg Loss: 0.0134 | Grad Norm: 0.00915349\n",
      "Epoch 1 | Step 40500 | Avg Loss: 0.0136 | Grad Norm: 0.01007815\n",
      "Epoch 1 | Step 40600 | Avg Loss: 0.0137 | Grad Norm: 0.01020021\n",
      "Epoch 1 | Step 40700 | Avg Loss: 0.0136 | Grad Norm: 0.00964639\n",
      "Epoch 1 | Step 40800 | Avg Loss: 0.0134 | Grad Norm: 0.01029189\n",
      "Epoch 1 | Step 40900 | Avg Loss: 0.0134 | Grad Norm: 0.00963020\n",
      "Epoch 1 | Step 41000 | Avg Loss: 0.0138 | Grad Norm: 0.01018640\n",
      "Epoch 1 | Step 41100 | Avg Loss: 0.0138 | Grad Norm: 0.01017012\n",
      "Epoch 1 | Step 41200 | Avg Loss: 0.0142 | Grad Norm: 0.01098425\n",
      "Epoch 1 | Step 41300 | Avg Loss: 0.0142 | Grad Norm: 0.01083103\n",
      "Epoch 1 | Step 41400 | Avg Loss: 0.0138 | Grad Norm: 0.00977161\n",
      "Epoch 1 | Step 41500 | Avg Loss: 0.0136 | Grad Norm: 0.00993952\n",
      "Epoch 1 | Step 41600 | Avg Loss: 0.0139 | Grad Norm: 0.00989021\n",
      "Epoch 1 | Step 41700 | Avg Loss: 0.0141 | Grad Norm: 0.01029677\n",
      "Epoch 1 | Step 41800 | Avg Loss: 0.0140 | Grad Norm: 0.01124481\n",
      "Epoch 1 | Step 41900 | Avg Loss: 0.0143 | Grad Norm: 0.01096187\n",
      "Epoch 1 | Step 42000 | Avg Loss: 0.0147 | Grad Norm: 0.01000158\n",
      "Epoch 1 | Step 42100 | Avg Loss: 0.0141 | Grad Norm: 0.01308625\n",
      "Epoch 1 | Step 42200 | Avg Loss: 0.0144 | Grad Norm: 0.00965974\n",
      "Epoch 1 | Step 42300 | Avg Loss: 0.0146 | Grad Norm: 0.01057100\n",
      "Epoch 1 | Step 42400 | Avg Loss: 0.0148 | Grad Norm: 0.01041827\n",
      "Epoch 1 | Step 42500 | Avg Loss: 0.0144 | Grad Norm: 0.01084899\n",
      "Epoch 1 | Step 42600 | Avg Loss: 0.0140 | Grad Norm: 0.01235857\n",
      "Epoch 1 | Step 42700 | Avg Loss: 0.0139 | Grad Norm: 0.01147269\n",
      "Epoch 1 | Step 42800 | Avg Loss: 0.0138 | Grad Norm: 0.01003923\n",
      "Epoch 1 | Step 42900 | Avg Loss: 0.0136 | Grad Norm: 0.00982958\n",
      "Epoch 1 | Step 43000 | Avg Loss: 0.0136 | Grad Norm: 0.01087626\n",
      "Epoch 1 | Step 43100 | Avg Loss: 0.0141 | Grad Norm: 0.01061269\n",
      "Epoch 1 | Step 43200 | Avg Loss: 0.0141 | Grad Norm: 0.01230232\n",
      "Epoch 1 | Step 43300 | Avg Loss: 0.0139 | Grad Norm: 0.00939587\n",
      "Epoch 1 | Step 43400 | Avg Loss: 0.0140 | Grad Norm: 0.01012922\n",
      "Epoch 1 | Step 43500 | Avg Loss: 0.0142 | Grad Norm: 0.01191918\n",
      "Epoch 1 | Step 43600 | Avg Loss: 0.0141 | Grad Norm: 0.00845601\n",
      "Epoch 1 | Step 43700 | Avg Loss: 0.0139 | Grad Norm: 0.00968147\n",
      "Epoch 1 | Step 43800 | Avg Loss: 0.0139 | Grad Norm: 0.01050435\n",
      "Epoch 1 | Step 43900 | Avg Loss: 0.0139 | Grad Norm: 0.00879405\n",
      "Epoch 1 | Step 44000 | Avg Loss: 0.0140 | Grad Norm: 0.01100042\n",
      "Epoch 1 | Step 44100 | Avg Loss: 0.0141 | Grad Norm: 0.01080471\n",
      "Epoch 1 | Step 44200 | Avg Loss: 0.0141 | Grad Norm: 0.01076657\n",
      "Epoch 1 | Step 44300 | Avg Loss: 0.0144 | Grad Norm: 0.01137034\n",
      "Epoch 1 | Step 44400 | Avg Loss: 0.0146 | Grad Norm: 0.00956183\n",
      "Epoch 1 | Step 44500 | Avg Loss: 0.0145 | Grad Norm: 0.01135183\n",
      "Epoch 1 | Step 44600 | Avg Loss: 0.0142 | Grad Norm: 0.01079775\n",
      "Epoch 1 | Step 44700 | Avg Loss: 0.0147 | Grad Norm: 0.01329100\n",
      "Epoch 1 | Step 44800 | Avg Loss: 0.0147 | Grad Norm: 0.01261471\n",
      "Epoch 1 | Step 44900 | Avg Loss: 0.0144 | Grad Norm: 0.01276383\n",
      "Epoch 1 | Step 45000 | Avg Loss: 0.0140 | Grad Norm: 0.01061585\n",
      "Epoch 1 | Step 45100 | Avg Loss: 0.0140 | Grad Norm: 0.01209728\n",
      "Epoch 1 | Step 45200 | Avg Loss: 0.0137 | Grad Norm: 0.00909660\n",
      "Epoch 1 | Step 45300 | Avg Loss: 0.0137 | Grad Norm: 0.01025744\n",
      "Epoch 1 | Step 45400 | Avg Loss: 0.0139 | Grad Norm: 0.01020956\n",
      "Epoch 1 | Step 45500 | Avg Loss: 0.0139 | Grad Norm: 0.01013216\n",
      "Epoch 1 | Step 45600 | Avg Loss: 0.0141 | Grad Norm: 0.01119957\n",
      "Epoch 1 | Step 45700 | Avg Loss: 0.0142 | Grad Norm: 0.01103144\n",
      "Epoch 1 | Step 45800 | Avg Loss: 0.0141 | Grad Norm: 0.01123198\n",
      "Epoch 1 | Step 45900 | Avg Loss: 0.0134 | Grad Norm: 0.00950154\n",
      "Epoch 1 | Step 46000 | Avg Loss: 0.0136 | Grad Norm: 0.00977702\n",
      "Epoch 1 | Step 46100 | Avg Loss: 0.0135 | Grad Norm: 0.01064588\n",
      "Epoch 1 | Step 46200 | Avg Loss: 0.0134 | Grad Norm: 0.01143647\n",
      "Epoch 1 | Step 46300 | Avg Loss: 0.0138 | Grad Norm: 0.01000295\n",
      "Epoch 1 | Step 46400 | Avg Loss: 0.0141 | Grad Norm: 0.00929691\n",
      "Epoch 1 | Step 46500 | Avg Loss: 0.0136 | Grad Norm: 0.01104325\n",
      "Epoch 1 | Step 46600 | Avg Loss: 0.0136 | Grad Norm: 0.01044357\n",
      "Epoch 1 | Step 46700 | Avg Loss: 0.0137 | Grad Norm: 0.01119260\n",
      "Epoch 1 | Step 46800 | Avg Loss: 0.0137 | Grad Norm: 0.01090241\n",
      "Epoch 1 | Step 46900 | Avg Loss: 0.0138 | Grad Norm: 0.00973291\n",
      "Epoch 1 | Step 47000 | Avg Loss: 0.0138 | Grad Norm: 0.01238608\n",
      "Epoch 1 | Step 47100 | Avg Loss: 0.0141 | Grad Norm: 0.01010640\n",
      "Epoch 1 | Step 47200 | Avg Loss: 0.0140 | Grad Norm: 0.01063306\n",
      "Epoch 1 | Step 47300 | Avg Loss: 0.0142 | Grad Norm: 0.01045230\n",
      "Epoch 1 | Step 47400 | Avg Loss: 0.0147 | Grad Norm: 0.01000374\n",
      "Epoch 1 | Step 47500 | Avg Loss: 0.0144 | Grad Norm: 0.01221387\n",
      "Epoch 1 | Step 47600 | Avg Loss: 0.0145 | Grad Norm: 0.01094197\n",
      "Epoch 1 | Step 47700 | Avg Loss: 0.0146 | Grad Norm: 0.01022296\n",
      "Epoch 1 | Step 47800 | Avg Loss: 0.0147 | Grad Norm: 0.01042638\n",
      "Epoch 1 | Step 47900 | Avg Loss: 0.0143 | Grad Norm: 0.01177709\n",
      "Epoch 1 | Step 48000 | Avg Loss: 0.0140 | Grad Norm: 0.00907881\n",
      "Epoch 1 | Step 48100 | Avg Loss: 0.0139 | Grad Norm: 0.01200452\n",
      "Epoch 1 | Step 48200 | Avg Loss: 0.0141 | Grad Norm: 0.00838117\n",
      "Epoch 1 | Step 48300 | Avg Loss: 0.0138 | Grad Norm: 0.01143639\n",
      "Epoch 1 | Step 48400 | Avg Loss: 0.0141 | Grad Norm: 0.01019669\n",
      "Epoch 1 | Step 48500 | Avg Loss: 0.0139 | Grad Norm: 0.00900146\n",
      "Epoch 1 | Step 48600 | Avg Loss: 0.0137 | Grad Norm: 0.01170036\n",
      "Epoch 1 | Step 48700 | Avg Loss: 0.0141 | Grad Norm: 0.01163084\n",
      "Epoch 1 | Step 48800 | Avg Loss: 0.0138 | Grad Norm: 0.00952770\n",
      "Epoch 1 | Step 48900 | Avg Loss: 0.0137 | Grad Norm: 0.01070219\n",
      "Epoch 1 | Step 49000 | Avg Loss: 0.0140 | Grad Norm: 0.00867249\n",
      "Epoch 1 | Step 49100 | Avg Loss: 0.0137 | Grad Norm: 0.00936422\n",
      "Epoch 1 | Step 49200 | Avg Loss: 0.0141 | Grad Norm: 0.01084537\n",
      "Epoch 1 | Step 49300 | Avg Loss: 0.0144 | Grad Norm: 0.01033918\n",
      "Epoch 1 | Step 49400 | Avg Loss: 0.0145 | Grad Norm: 0.01180737\n",
      "Epoch 1 | Step 49500 | Avg Loss: 0.0140 | Grad Norm: 0.00983441\n",
      "Epoch 1 | Step 49600 | Avg Loss: 0.0138 | Grad Norm: 0.01101163\n",
      "Epoch 1 | Step 49700 | Avg Loss: 0.0139 | Grad Norm: 0.01139908\n",
      "Epoch 1 | Step 49800 | Avg Loss: 0.0138 | Grad Norm: 0.00873984\n",
      "Epoch 1 | Step 49900 | Avg Loss: 0.0138 | Grad Norm: 0.01055379\n",
      "Epoch 1 | Step 50000 | Avg Loss: 0.0139 | Grad Norm: 0.01001638\n",
      "Epoch 1 | Step 50100 | Avg Loss: 0.0142 | Grad Norm: 0.01165475\n",
      "Epoch 1 | Step 50200 | Avg Loss: 0.0140 | Grad Norm: 0.01149303\n",
      "Epoch 1 | Step 50300 | Avg Loss: 0.0142 | Grad Norm: 0.01191682\n",
      "Epoch 1 | Step 50400 | Avg Loss: 0.0145 | Grad Norm: 0.01017584\n",
      "Epoch 1 | Step 50500 | Avg Loss: 0.0143 | Grad Norm: 0.01131978\n",
      "Epoch 1 | Step 50600 | Avg Loss: 0.0142 | Grad Norm: 0.01073731\n",
      "Epoch 1 | Step 50700 | Avg Loss: 0.0137 | Grad Norm: 0.01158397\n",
      "Epoch 1 | Step 50800 | Avg Loss: 0.0139 | Grad Norm: 0.01105165\n",
      "Epoch 1 | Step 50900 | Avg Loss: 0.0139 | Grad Norm: 0.01286698\n",
      "Epoch 1 | Step 51000 | Avg Loss: 0.0141 | Grad Norm: 0.00907011\n",
      "Epoch 1 | Step 51100 | Avg Loss: 0.0139 | Grad Norm: 0.01167145\n",
      "Epoch 1 | Step 51200 | Avg Loss: 0.0141 | Grad Norm: 0.01133209\n",
      "Epoch 1 | Step 51300 | Avg Loss: 0.0141 | Grad Norm: 0.00964529\n",
      "Epoch 1 | Step 51400 | Avg Loss: 0.0140 | Grad Norm: 0.01103942\n",
      "Epoch 1 | Step 51500 | Avg Loss: 0.0142 | Grad Norm: 0.00990712\n",
      "Epoch 1 | Step 51600 | Avg Loss: 0.0143 | Grad Norm: 0.01009636\n",
      "Epoch 1 | Step 51700 | Avg Loss: 0.0144 | Grad Norm: 0.01151214\n",
      "Epoch 1 | Step 51800 | Avg Loss: 0.0145 | Grad Norm: 0.01072183\n",
      "Epoch 1 | Step 51900 | Avg Loss: 0.0146 | Grad Norm: 0.00895587\n",
      "Epoch 1 | Step 52000 | Avg Loss: 0.0143 | Grad Norm: 0.00969810\n",
      "Epoch 1 | Step 52100 | Avg Loss: 0.0143 | Grad Norm: 0.01092776\n",
      "Epoch 1 | Step 52200 | Avg Loss: 0.0144 | Grad Norm: 0.00958968\n",
      "Epoch 1 | Step 52300 | Avg Loss: 0.0142 | Grad Norm: 0.01145843\n",
      "Epoch 1 | Step 52400 | Avg Loss: 0.0136 | Grad Norm: 0.00951071\n",
      "Epoch 1 | Step 52500 | Avg Loss: 0.0139 | Grad Norm: 0.01047506\n",
      "Epoch 1 | Step 52600 | Avg Loss: 0.0138 | Grad Norm: 0.01027802\n",
      "Epoch 1 | Step 52700 | Avg Loss: 0.0138 | Grad Norm: 0.01107039\n",
      "Epoch 1 | Step 52800 | Avg Loss: 0.0136 | Grad Norm: 0.01049598\n",
      "Epoch 1 | Step 52900 | Avg Loss: 0.0136 | Grad Norm: 0.00990715\n",
      "Epoch 1 | Step 53000 | Avg Loss: 0.0135 | Grad Norm: 0.01142913\n",
      "Epoch 1 | Step 53100 | Avg Loss: 0.0136 | Grad Norm: 0.00891493\n",
      "Epoch 1 | Step 53200 | Avg Loss: 0.0139 | Grad Norm: 0.01238345\n",
      "Epoch 1 | Step 53300 | Avg Loss: 0.0140 | Grad Norm: 0.01099849\n",
      "Epoch 1 | Step 53400 | Avg Loss: 0.0138 | Grad Norm: 0.00941601\n",
      "Epoch 1 | Step 53500 | Avg Loss: 0.0142 | Grad Norm: 0.01117392\n",
      "Epoch 1 | Step 53600 | Avg Loss: 0.0135 | Grad Norm: 0.01135010\n",
      "Epoch 1 | Step 53700 | Avg Loss: 0.0138 | Grad Norm: 0.01236554\n",
      "Epoch 1 | Step 53800 | Avg Loss: 0.0141 | Grad Norm: 0.01101599\n",
      "Epoch 1 | Step 53900 | Avg Loss: 0.0143 | Grad Norm: 0.00962002\n",
      "Epoch 1 | Step 54000 | Avg Loss: 0.0147 | Grad Norm: 0.01122565\n",
      "Epoch 1 | Step 54100 | Avg Loss: 0.0146 | Grad Norm: 0.01083791\n",
      "Epoch 1 | Step 54200 | Avg Loss: 0.0144 | Grad Norm: 0.01060959\n",
      "Epoch 1 | Step 54300 | Avg Loss: 0.0146 | Grad Norm: 0.01047274\n",
      "Epoch 1 | Step 54400 | Avg Loss: 0.0146 | Grad Norm: 0.01142636\n",
      "Epoch 1 | Step 54500 | Avg Loss: 0.0148 | Grad Norm: 0.01062488\n",
      "Epoch 1 | Step 54600 | Avg Loss: 0.0146 | Grad Norm: 0.01211075\n",
      "Epoch 1 | Step 54700 | Avg Loss: 0.0144 | Grad Norm: 0.01263341\n",
      "Epoch 1 | Step 54800 | Avg Loss: 0.0144 | Grad Norm: 0.01000609\n",
      "Epoch 1 | Step 54900 | Avg Loss: 0.0144 | Grad Norm: 0.00956258\n",
      "Epoch 1 | Step 55000 | Avg Loss: 0.0147 | Grad Norm: 0.01107910\n",
      "Epoch 1 | Step 55100 | Avg Loss: 0.0147 | Grad Norm: 0.01146861\n",
      "Epoch 1 | Step 55200 | Avg Loss: 0.0142 | Grad Norm: 0.01139203\n",
      "Epoch 1 | Step 55300 | Avg Loss: 0.0143 | Grad Norm: 0.01186829\n",
      "Epoch 1 | Step 55400 | Avg Loss: 0.0140 | Grad Norm: 0.00998024\n",
      "Epoch 1 | Step 55500 | Avg Loss: 0.0145 | Grad Norm: 0.00984630\n",
      "Epoch 1 | Step 55600 | Avg Loss: 0.0141 | Grad Norm: 0.01007620\n",
      "Epoch 1 | Step 55700 | Avg Loss: 0.0143 | Grad Norm: 0.01097042\n",
      "Epoch 1 | Step 55800 | Avg Loss: 0.0144 | Grad Norm: 0.01006222\n",
      "Epoch 1 | Step 55900 | Avg Loss: 0.0142 | Grad Norm: 0.00935694\n",
      "Epoch 1 | Step 56000 | Avg Loss: 0.0143 | Grad Norm: 0.01117455\n",
      "Epoch 1 | Step 56100 | Avg Loss: 0.0149 | Grad Norm: 0.01117519\n",
      "Epoch 1 | Step 56200 | Avg Loss: 0.0149 | Grad Norm: 0.01156303\n",
      "Epoch 1 | Step 56300 | Avg Loss: 0.0143 | Grad Norm: 0.01018831\n",
      "Epoch 1 | Step 56400 | Avg Loss: 0.0143 | Grad Norm: 0.00959952\n",
      "Epoch 1 | Step 56500 | Avg Loss: 0.0140 | Grad Norm: 0.01177947\n",
      "Epoch 1 | Step 56600 | Avg Loss: 0.0143 | Grad Norm: 0.01233584\n",
      "Epoch 1 | Step 56700 | Avg Loss: 0.0144 | Grad Norm: 0.00971227\n",
      "Epoch 1 | Step 56800 | Avg Loss: 0.0147 | Grad Norm: 0.01221469\n",
      "Epoch 1 | Step 56900 | Avg Loss: 0.0153 | Grad Norm: 0.01212189\n",
      "Epoch 1 | Step 57000 | Avg Loss: 0.0146 | Grad Norm: 0.01025685\n",
      "Epoch 1 | Step 57100 | Avg Loss: 0.0141 | Grad Norm: 0.00960697\n",
      "Epoch 1 | Step 57200 | Avg Loss: 0.0140 | Grad Norm: 0.00989192\n",
      "Epoch 1 | Step 57300 | Avg Loss: 0.0138 | Grad Norm: 0.00992786\n",
      "Epoch 1 | Step 57400 | Avg Loss: 0.0139 | Grad Norm: 0.01242161\n",
      "Epoch 1 | Step 57500 | Avg Loss: 0.0140 | Grad Norm: 0.00933803\n",
      "Epoch 1 | Step 57600 | Avg Loss: 0.0137 | Grad Norm: 0.01140166\n",
      "Epoch 1 | Step 57700 | Avg Loss: 0.0141 | Grad Norm: 0.00972235\n",
      "Epoch 1 | Step 57800 | Avg Loss: 0.0138 | Grad Norm: 0.01435438\n",
      "Epoch 1 | Step 57900 | Avg Loss: 0.0135 | Grad Norm: 0.01004130\n",
      "Epoch 1 | Step 58000 | Avg Loss: 0.0137 | Grad Norm: 0.01069274\n",
      "Epoch 1 | Step 58100 | Avg Loss: 0.0142 | Grad Norm: 0.01029624\n",
      "Epoch 1 | Step 58200 | Avg Loss: 0.0138 | Grad Norm: 0.01050397\n",
      "Epoch 1 | Step 58300 | Avg Loss: 0.0141 | Grad Norm: 0.01173052\n",
      "Epoch 1 | Step 58400 | Avg Loss: 0.0141 | Grad Norm: 0.01090716\n",
      "Epoch 1 | Step 58500 | Avg Loss: 0.0143 | Grad Norm: 0.01168055\n",
      "Epoch 1 | Step 58600 | Avg Loss: 0.0140 | Grad Norm: 0.01059155\n",
      "Epoch 1 | Step 58700 | Avg Loss: 0.0136 | Grad Norm: 0.01159811\n",
      "Epoch 1 | Step 58800 | Avg Loss: 0.0143 | Grad Norm: 0.01071065\n",
      "Epoch 1 | Step 58900 | Avg Loss: 0.0143 | Grad Norm: 0.01156221\n",
      "Epoch 1 | Step 59000 | Avg Loss: 0.0141 | Grad Norm: 0.01150391\n",
      "Epoch 1 | Step 59100 | Avg Loss: 0.0140 | Grad Norm: 0.01068407\n",
      "Epoch 1 | Step 59200 | Avg Loss: 0.0140 | Grad Norm: 0.01131314\n",
      "Epoch 1 | Step 59300 | Avg Loss: 0.0138 | Grad Norm: 0.00952554\n",
      "Epoch 1 | Step 59400 | Avg Loss: 0.0137 | Grad Norm: 0.01215845\n",
      "Epoch 1 | Step 59500 | Avg Loss: 0.0137 | Grad Norm: 0.01028617\n",
      "Epoch 1 | Step 59600 | Avg Loss: 0.0141 | Grad Norm: 0.00935181\n",
      "Epoch 1 | Step 59700 | Avg Loss: 0.0139 | Grad Norm: 0.01041591\n",
      "Epoch 1 | Step 59800 | Avg Loss: 0.0136 | Grad Norm: 0.00960037\n",
      "Epoch 1 | Step 59900 | Avg Loss: 0.0138 | Grad Norm: 0.00896334\n",
      "Epoch 1 | Step 60000 | Avg Loss: 0.0140 | Grad Norm: 0.01078833\n",
      "Epoch 1 | Step 60100 | Avg Loss: 0.0144 | Grad Norm: 0.01130119\n",
      "Epoch 1 | Step 60200 | Avg Loss: 0.0140 | Grad Norm: 0.00969899\n",
      "Epoch 1 | Step 60300 | Avg Loss: 0.0138 | Grad Norm: 0.00934080\n",
      "Epoch 1 | Step 60400 | Avg Loss: 0.0140 | Grad Norm: 0.01114815\n",
      "Epoch 1 | Step 60500 | Avg Loss: 0.0137 | Grad Norm: 0.00833198\n",
      "Epoch 1 | Step 60600 | Avg Loss: 0.0137 | Grad Norm: 0.01202015\n",
      "Epoch 1 | Step 60700 | Avg Loss: 0.0136 | Grad Norm: 0.01001525\n",
      "Epoch 1 | Step 60800 | Avg Loss: 0.0139 | Grad Norm: 0.01172019\n",
      "Epoch 1 | Step 60900 | Avg Loss: 0.0142 | Grad Norm: 0.01164193\n",
      "Epoch 1 | Step 61000 | Avg Loss: 0.0140 | Grad Norm: 0.00991775\n",
      "Epoch 1 | Step 61100 | Avg Loss: 0.0141 | Grad Norm: 0.01001993\n",
      "Epoch 1 | Step 61200 | Avg Loss: 0.0141 | Grad Norm: 0.00973466\n",
      "Epoch 1 | Step 61300 | Avg Loss: 0.0139 | Grad Norm: 0.00986127\n",
      "Epoch 1 | Step 61400 | Avg Loss: 0.0140 | Grad Norm: 0.00953433\n",
      "Epoch 1 | Step 61500 | Avg Loss: 0.0141 | Grad Norm: 0.01116974\n",
      "Epoch 1 | Step 61600 | Avg Loss: 0.0140 | Grad Norm: 0.01050003\n",
      "Epoch 1 | Step 61700 | Avg Loss: 0.0138 | Grad Norm: 0.01015426\n",
      "Epoch 1 | Step 61800 | Avg Loss: 0.0141 | Grad Norm: 0.01146917\n",
      "Epoch 1 | Step 61900 | Avg Loss: 0.0141 | Grad Norm: 0.01059684\n",
      "Epoch 1 | Step 62000 | Avg Loss: 0.0145 | Grad Norm: 0.01076291\n",
      "Epoch 1 | Step 62100 | Avg Loss: 0.0148 | Grad Norm: 0.01159229\n",
      "Epoch 1 | Step 62200 | Avg Loss: 0.0143 | Grad Norm: 0.01108131\n",
      "Epoch 1 | Step 62300 | Avg Loss: 0.0139 | Grad Norm: 0.00993088\n",
      "Epoch 1 | Step 62400 | Avg Loss: 0.0132 | Grad Norm: 0.01005377\n",
      "Epoch 1 | Step 62500 | Avg Loss: 0.0136 | Grad Norm: 0.00892963\n",
      "Epoch 1 | Step 62600 | Avg Loss: 0.0141 | Grad Norm: 0.01148486\n",
      "Epoch 1 | Step 62700 | Avg Loss: 0.0142 | Grad Norm: 0.00978432\n",
      "Epoch 1 | Step 62800 | Avg Loss: 0.0141 | Grad Norm: 0.01171358\n",
      "Epoch 1 | Step 62900 | Avg Loss: 0.0142 | Grad Norm: 0.01201560\n",
      "Epoch 1 | Step 63000 | Avg Loss: 0.0145 | Grad Norm: 0.00999951\n",
      "Epoch 1 | Step 63100 | Avg Loss: 0.0146 | Grad Norm: 0.00973359\n",
      "Epoch 1 | Step 63200 | Avg Loss: 0.0143 | Grad Norm: 0.01096787\n",
      "Epoch 1 | Step 63300 | Avg Loss: 0.0141 | Grad Norm: 0.01133189\n",
      "Epoch 1 | Step 63400 | Avg Loss: 0.0142 | Grad Norm: 0.01221199\n",
      "Epoch 1 | Step 63500 | Avg Loss: 0.0136 | Grad Norm: 0.01067837\n",
      "Epoch 1 | Step 63600 | Avg Loss: 0.0134 | Grad Norm: 0.00904913\n",
      "Epoch 1 | Step 63700 | Avg Loss: 0.0136 | Grad Norm: 0.01028727\n",
      "Epoch 1 | Step 63800 | Avg Loss: 0.0136 | Grad Norm: 0.01021627\n",
      "Epoch 1 | Step 63900 | Avg Loss: 0.0134 | Grad Norm: 0.00998801\n",
      "Epoch 1 | Step 64000 | Avg Loss: 0.0137 | Grad Norm: 0.01018923\n",
      "Epoch 1 | Step 64100 | Avg Loss: 0.0136 | Grad Norm: 0.01107311\n",
      "Epoch 1 | Step 64200 | Avg Loss: 0.0137 | Grad Norm: 0.01066654\n",
      "Epoch 1 | Step 64300 | Avg Loss: 0.0139 | Grad Norm: 0.01066200\n",
      "Epoch 1 | Step 64400 | Avg Loss: 0.0144 | Grad Norm: 0.01114097\n",
      "Epoch 1 | Step 64500 | Avg Loss: 0.0145 | Grad Norm: 0.01016311\n",
      "Epoch 1 | Step 64600 | Avg Loss: 0.0142 | Grad Norm: 0.01097039\n",
      "Epoch 1 | Step 64700 | Avg Loss: 0.0141 | Grad Norm: 0.00955710\n",
      "Epoch 1 | Step 64800 | Avg Loss: 0.0141 | Grad Norm: 0.01110201\n",
      "Epoch 1 | Step 64900 | Avg Loss: 0.0140 | Grad Norm: 0.01045053\n",
      "Epoch 1 | Step 65000 | Avg Loss: 0.0140 | Grad Norm: 0.01069987\n",
      "Epoch 1 | Step 65100 | Avg Loss: 0.0140 | Grad Norm: 0.00985290\n",
      "Epoch 1 | Step 65200 | Avg Loss: 0.0137 | Grad Norm: 0.01042911\n",
      "Epoch 1 | Step 65300 | Avg Loss: 0.0136 | Grad Norm: 0.00981224\n",
      "Epoch 1 | Step 65400 | Avg Loss: 0.0135 | Grad Norm: 0.01070223\n",
      "Epoch 1 | Step 65500 | Avg Loss: 0.0135 | Grad Norm: 0.00911286\n",
      "Epoch 1 | Step 65600 | Avg Loss: 0.0136 | Grad Norm: 0.01005638\n",
      "Epoch 1 | Step 65700 | Avg Loss: 0.0135 | Grad Norm: 0.01111181\n",
      "Epoch 1 | Step 65800 | Avg Loss: 0.0136 | Grad Norm: 0.01016854\n",
      "Epoch 1 | Step 65900 | Avg Loss: 0.0135 | Grad Norm: 0.00896815\n",
      "Epoch 1 | Step 66000 | Avg Loss: 0.0136 | Grad Norm: 0.00980241\n",
      "Epoch 1 | Step 66100 | Avg Loss: 0.0139 | Grad Norm: 0.01250148\n",
      "Epoch 1 | Step 66200 | Avg Loss: 0.0139 | Grad Norm: 0.01281189\n",
      "Epoch 1 | Step 66300 | Avg Loss: 0.0139 | Grad Norm: 0.01121063\n",
      "Epoch 1 | Step 66400 | Avg Loss: 0.0143 | Grad Norm: 0.01028204\n",
      "Epoch 1 | Step 66500 | Avg Loss: 0.0147 | Grad Norm: 0.01179130\n",
      "Epoch 1 | Step 66600 | Avg Loss: 0.0146 | Grad Norm: 0.01040463\n",
      "Epoch 1 | Step 66700 | Avg Loss: 0.0140 | Grad Norm: 0.01153249\n",
      "Epoch 1 | Step 66800 | Avg Loss: 0.0133 | Grad Norm: 0.01081793\n",
      "Epoch 1 | Step 66900 | Avg Loss: 0.0135 | Grad Norm: 0.00912615\n",
      "Epoch 1 | Step 67000 | Avg Loss: 0.0138 | Grad Norm: 0.01035110\n",
      "Epoch 1 | Step 67100 | Avg Loss: 0.0136 | Grad Norm: 0.01215193\n",
      "Epoch 1 | Step 67200 | Avg Loss: 0.0139 | Grad Norm: 0.01163974\n",
      "Epoch 1 | Step 67300 | Avg Loss: 0.0139 | Grad Norm: 0.01084769\n",
      "Epoch 1 | Step 67400 | Avg Loss: 0.0140 | Grad Norm: 0.01117152\n",
      "Epoch 1 | Step 67500 | Avg Loss: 0.0142 | Grad Norm: 0.01094240\n",
      "Epoch 1 | Step 67600 | Avg Loss: 0.0148 | Grad Norm: 0.01291149\n",
      "Epoch 1 | Step 67700 | Avg Loss: 0.0146 | Grad Norm: 0.01259954\n",
      "Epoch 1 | Step 67800 | Avg Loss: 0.0146 | Grad Norm: 0.01147934\n",
      "Epoch 1 | Step 67900 | Avg Loss: 0.0139 | Grad Norm: 0.00979902\n",
      "Epoch 1 | Step 68000 | Avg Loss: 0.0136 | Grad Norm: 0.01056245\n",
      "Epoch 1 | Step 68100 | Avg Loss: 0.0142 | Grad Norm: 0.01285534\n",
      "Epoch 1 | Step 68200 | Avg Loss: 0.0139 | Grad Norm: 0.01168622\n",
      "Epoch 1 | Step 68300 | Avg Loss: 0.0141 | Grad Norm: 0.01075860\n",
      "Epoch 1 | Step 68400 | Avg Loss: 0.0141 | Grad Norm: 0.01036156\n",
      "Epoch 1 | Step 68500 | Avg Loss: 0.0138 | Grad Norm: 0.00880497\n",
      "Epoch 1 | Step 68600 | Avg Loss: 0.0141 | Grad Norm: 0.01035973\n",
      "Epoch 1 | Step 68700 | Avg Loss: 0.0139 | Grad Norm: 0.01307103\n",
      "Epoch 1 | Step 68800 | Avg Loss: 0.0143 | Grad Norm: 0.00987081\n",
      "Epoch 1 | Step 68900 | Avg Loss: 0.0143 | Grad Norm: 0.01153675\n",
      "Epoch 1 | Step 69000 | Avg Loss: 0.0146 | Grad Norm: 0.01055913\n",
      "Epoch 1 | Step 69100 | Avg Loss: 0.0146 | Grad Norm: 0.01051107\n",
      "Epoch 1 | Step 69200 | Avg Loss: 0.0141 | Grad Norm: 0.01055577\n",
      "Epoch 1 | Step 69300 | Avg Loss: 0.0139 | Grad Norm: 0.00929567\n",
      "Epoch 1 | Step 69400 | Avg Loss: 0.0139 | Grad Norm: 0.00890195\n",
      "Epoch 1 | Step 69500 | Avg Loss: 0.0144 | Grad Norm: 0.00960810\n",
      "Epoch 1 | Step 69600 | Avg Loss: 0.0144 | Grad Norm: 0.00985849\n",
      "Epoch 1 | Step 69700 | Avg Loss: 0.0141 | Grad Norm: 0.01056714\n",
      "Epoch 1 | Step 69800 | Avg Loss: 0.0140 | Grad Norm: 0.01109121\n",
      "Epoch 1 | Step 69900 | Avg Loss: 0.0140 | Grad Norm: 0.01076819\n",
      "Epoch 1 | Step 70000 | Avg Loss: 0.0141 | Grad Norm: 0.01185001\n",
      "Epoch 1 | Step 70100 | Avg Loss: 0.0145 | Grad Norm: 0.00843214\n",
      "Epoch 1 | Step 70200 | Avg Loss: 0.0139 | Grad Norm: 0.01085603\n",
      "Epoch 1 | Step 70300 | Avg Loss: 0.0141 | Grad Norm: 0.01208874\n",
      "Epoch 1 | Step 70400 | Avg Loss: 0.0139 | Grad Norm: 0.01053960\n",
      "Epoch 1 | Step 70500 | Avg Loss: 0.0142 | Grad Norm: 0.00995613\n",
      "Epoch 1 | Step 70600 | Avg Loss: 0.0143 | Grad Norm: 0.01010313\n",
      "Epoch 1 | Step 70700 | Avg Loss: 0.0140 | Grad Norm: 0.01194710\n",
      "Epoch 1 | Step 70800 | Avg Loss: 0.0134 | Grad Norm: 0.01129240\n",
      "Epoch 1 | Step 70900 | Avg Loss: 0.0138 | Grad Norm: 0.01205858\n",
      "Epoch 1 | Step 71000 | Avg Loss: 0.0144 | Grad Norm: 0.01047897\n",
      "Epoch 1 | Step 71100 | Avg Loss: 0.0144 | Grad Norm: 0.01055969\n",
      "Epoch 1 | Step 71200 | Avg Loss: 0.0145 | Grad Norm: 0.01071388\n",
      "Epoch 1 | Step 71300 | Avg Loss: 0.0142 | Grad Norm: 0.01129187\n",
      "Epoch 1 | Step 71400 | Avg Loss: 0.0140 | Grad Norm: 0.01014623\n",
      "Epoch 1 | Step 71500 | Avg Loss: 0.0140 | Grad Norm: 0.01071676\n",
      "Epoch 1 | Step 71600 | Avg Loss: 0.0146 | Grad Norm: 0.01095405\n",
      "Epoch 1 | Step 71700 | Avg Loss: 0.0144 | Grad Norm: 0.01010306\n",
      "Epoch 1 | Step 71800 | Avg Loss: 0.0139 | Grad Norm: 0.01068718\n",
      "Epoch 1 | Step 71900 | Avg Loss: 0.0141 | Grad Norm: 0.00976263\n",
      "Epoch 1 | Step 72000 | Avg Loss: 0.0145 | Grad Norm: 0.01129200\n",
      "Epoch 1 | Step 72100 | Avg Loss: 0.0143 | Grad Norm: 0.01144733\n",
      "Epoch 1 | Step 72200 | Avg Loss: 0.0140 | Grad Norm: 0.00896773\n",
      "Epoch 1 | Step 72300 | Avg Loss: 0.0144 | Grad Norm: 0.00913534\n",
      "Epoch 1 | Step 72400 | Avg Loss: 0.0144 | Grad Norm: 0.01198604\n",
      "Epoch 1 | Step 72500 | Avg Loss: 0.0144 | Grad Norm: 0.01032997\n",
      "Epoch 1 | Step 72600 | Avg Loss: 0.0143 | Grad Norm: 0.01009414\n",
      "Epoch 1 | Step 72700 | Avg Loss: 0.0143 | Grad Norm: 0.00962355\n",
      "Epoch 1 | Step 72800 | Avg Loss: 0.0142 | Grad Norm: 0.01081505\n",
      "Epoch 1 | Step 72900 | Avg Loss: 0.0147 | Grad Norm: 0.01074105\n",
      "Epoch 1 | Step 73000 | Avg Loss: 0.0149 | Grad Norm: 0.01088139\n",
      "Epoch 1 | Step 73100 | Avg Loss: 0.0149 | Grad Norm: 0.01011413\n",
      "Epoch 1 | Step 73200 | Avg Loss: 0.0148 | Grad Norm: 0.01081920\n",
      "Epoch 1 | Step 73300 | Avg Loss: 0.0147 | Grad Norm: 0.01131240\n",
      "Epoch 1 | Step 73400 | Avg Loss: 0.0143 | Grad Norm: 0.01122758\n",
      "Epoch 1 | Step 73500 | Avg Loss: 0.0138 | Grad Norm: 0.01268387\n",
      "Epoch 1 | Step 73600 | Avg Loss: 0.0142 | Grad Norm: 0.00942685\n",
      "Epoch 1 | Step 73700 | Avg Loss: 0.0143 | Grad Norm: 0.01000801\n",
      "Epoch 1 | Step 73800 | Avg Loss: 0.0143 | Grad Norm: 0.00997690\n",
      "Epoch 1 | Step 73900 | Avg Loss: 0.0138 | Grad Norm: 0.01063779\n",
      "Epoch 1 | Step 74000 | Avg Loss: 0.0138 | Grad Norm: 0.01102156\n",
      "Epoch 1 | Step 74100 | Avg Loss: 0.0141 | Grad Norm: 0.01356660\n",
      "Epoch 1 | Step 74200 | Avg Loss: 0.0138 | Grad Norm: 0.01051127\n",
      "Epoch 1 | Step 74300 | Avg Loss: 0.0140 | Grad Norm: 0.01096352\n",
      "Epoch 1 | Step 74400 | Avg Loss: 0.0141 | Grad Norm: 0.01247366\n",
      "Epoch 1 | Step 74500 | Avg Loss: 0.0143 | Grad Norm: 0.01029743\n",
      "Epoch 1 | Step 74600 | Avg Loss: 0.0145 | Grad Norm: 0.01049093\n",
      "Epoch 1 | Step 74700 | Avg Loss: 0.0141 | Grad Norm: 0.01051784\n",
      "Epoch 1 | Step 74800 | Avg Loss: 0.0144 | Grad Norm: 0.00946640\n",
      "Epoch 1 | Step 74900 | Avg Loss: 0.0141 | Grad Norm: 0.01044232\n",
      "Epoch 1 | Step 75000 | Avg Loss: 0.0147 | Grad Norm: 0.01177907\n",
      "Epoch 1 | Step 75100 | Avg Loss: 0.0146 | Grad Norm: 0.01207478\n",
      "Epoch 1 | Step 75200 | Avg Loss: 0.0140 | Grad Norm: 0.01064850\n",
      "Epoch 1 | Step 75300 | Avg Loss: 0.0138 | Grad Norm: 0.00947547\n",
      "Epoch 1 | Step 75400 | Avg Loss: 0.0137 | Grad Norm: 0.01053659\n",
      "Epoch 1 | Step 75500 | Avg Loss: 0.0137 | Grad Norm: 0.01118625\n",
      "Epoch 1 | Step 75600 | Avg Loss: 0.0137 | Grad Norm: 0.01256736\n",
      "Epoch 1 | Step 75700 | Avg Loss: 0.0137 | Grad Norm: 0.01074735\n",
      "Epoch 1 | Step 75800 | Avg Loss: 0.0142 | Grad Norm: 0.00868353\n",
      "Epoch 1 | Step 75900 | Avg Loss: 0.0143 | Grad Norm: 0.00939340\n",
      "Epoch 1 | Step 76000 | Avg Loss: 0.0145 | Grad Norm: 0.01263696\n",
      "Epoch 1 | Step 76100 | Avg Loss: 0.0140 | Grad Norm: 0.01021363\n",
      "Epoch 1 | Step 76200 | Avg Loss: 0.0142 | Grad Norm: 0.01013644\n",
      "Epoch 1 | Step 76300 | Avg Loss: 0.0145 | Grad Norm: 0.00954209\n",
      "Epoch 1 | Step 76400 | Avg Loss: 0.0141 | Grad Norm: 0.01088767\n",
      "Epoch 1 | Step 76500 | Avg Loss: 0.0141 | Grad Norm: 0.01005696\n",
      "Epoch 1 | Step 76600 | Avg Loss: 0.0137 | Grad Norm: 0.00996627\n",
      "Epoch 1 | Step 76700 | Avg Loss: 0.0140 | Grad Norm: 0.01058009\n",
      "Epoch 1 | Step 76800 | Avg Loss: 0.0140 | Grad Norm: 0.01033945\n",
      "Epoch 1 | Step 76900 | Avg Loss: 0.0143 | Grad Norm: 0.01114044\n",
      "Epoch 1 | Step 77000 | Avg Loss: 0.0145 | Grad Norm: 0.01260418\n",
      "Epoch 1 | Step 77100 | Avg Loss: 0.0142 | Grad Norm: 0.01082667\n",
      "Epoch 1 | Step 77200 | Avg Loss: 0.0140 | Grad Norm: 0.01046073\n",
      "Epoch 1 | Step 77300 | Avg Loss: 0.0143 | Grad Norm: 0.01028644\n",
      "Epoch 1 | Step 77400 | Avg Loss: 0.0142 | Grad Norm: 0.01417834\n",
      "Epoch 1 | Step 77500 | Avg Loss: 0.0145 | Grad Norm: 0.01233037\n",
      "Epoch 1 | Step 77600 | Avg Loss: 0.0144 | Grad Norm: 0.01051217\n",
      "Epoch 1 | Step 77700 | Avg Loss: 0.0149 | Grad Norm: 0.01085431\n",
      "Epoch 1 | Step 77800 | Avg Loss: 0.0148 | Grad Norm: 0.01146481\n",
      "Epoch 1 | Step 77900 | Avg Loss: 0.0147 | Grad Norm: 0.01146987\n",
      "Epoch 1 | Step 78000 | Avg Loss: 0.0144 | Grad Norm: 0.01146601\n",
      "Epoch 1 | Step 78100 | Avg Loss: 0.0144 | Grad Norm: 0.01202699\n",
      "Epoch 1 | Step 78200 | Avg Loss: 0.0143 | Grad Norm: 0.01134076\n",
      "Epoch 1 | Step 78300 | Avg Loss: 0.0147 | Grad Norm: 0.01118228\n",
      "Epoch 1 | Step 78400 | Avg Loss: 0.0146 | Grad Norm: 0.01067007\n",
      "Epoch 1 | Step 78500 | Avg Loss: 0.0146 | Grad Norm: 0.01021192\n",
      "Epoch 1 | Step 78600 | Avg Loss: 0.0144 | Grad Norm: 0.01037109\n",
      "Epoch 1 | Step 78700 | Avg Loss: 0.0143 | Grad Norm: 0.01318644\n",
      "Epoch 1 | Step 78800 | Avg Loss: 0.0148 | Grad Norm: 0.01125520\n",
      "Epoch 1 | Step 78900 | Avg Loss: 0.0145 | Grad Norm: 0.01105405\n",
      "Epoch 1 | Step 79000 | Avg Loss: 0.0148 | Grad Norm: 0.01183315\n",
      "Epoch 1 | Step 79100 | Avg Loss: 0.0143 | Grad Norm: 0.01078592\n",
      "Epoch 1 | Step 79200 | Avg Loss: 0.0140 | Grad Norm: 0.00980103\n",
      "Epoch 1 | Step 79300 | Avg Loss: 0.0139 | Grad Norm: 0.01313988\n",
      "Epoch 1 | Step 79400 | Avg Loss: 0.0142 | Grad Norm: 0.01016149\n",
      "Epoch 1 | Step 79500 | Avg Loss: 0.0143 | Grad Norm: 0.01112148\n",
      "Epoch 1 | Step 79600 | Avg Loss: 0.0142 | Grad Norm: 0.01104640\n",
      "Epoch 1 | Step 79700 | Avg Loss: 0.0148 | Grad Norm: 0.01230415\n",
      "Epoch 1 | Step 79800 | Avg Loss: 0.0148 | Grad Norm: 0.01044451\n",
      "Epoch 1 | Step 79900 | Avg Loss: 0.0148 | Grad Norm: 0.01081576\n",
      "Epoch 1 | Step 80000 | Avg Loss: 0.0146 | Grad Norm: 0.01248211\n",
      "Epoch 1 | Step 80100 | Avg Loss: 0.0146 | Grad Norm: 0.00983630\n",
      "Epoch 1 | Step 80200 | Avg Loss: 0.0144 | Grad Norm: 0.00971257\n",
      "Epoch 1 | Step 80300 | Avg Loss: 0.0142 | Grad Norm: 0.01132961\n",
      "Epoch 1 | Step 80400 | Avg Loss: 0.0142 | Grad Norm: 0.01080248\n",
      "Epoch 1 | Step 80500 | Avg Loss: 0.0140 | Grad Norm: 0.00890625\n",
      "Epoch 1 | Step 80600 | Avg Loss: 0.0141 | Grad Norm: 0.01170605\n",
      "Epoch 1 | Step 80700 | Avg Loss: 0.0144 | Grad Norm: 0.01106559\n",
      "Epoch 1 | Step 80800 | Avg Loss: 0.0138 | Grad Norm: 0.00980918\n",
      "Epoch 1 | Step 80900 | Avg Loss: 0.0135 | Grad Norm: 0.01080691\n",
      "Epoch 1 | Step 81000 | Avg Loss: 0.0138 | Grad Norm: 0.01038165\n",
      "Epoch 1 | Step 81100 | Avg Loss: 0.0139 | Grad Norm: 0.01091545\n",
      "Epoch 1 | Step 81200 | Avg Loss: 0.0136 | Grad Norm: 0.00851437\n",
      "Epoch 1 | Step 81300 | Avg Loss: 0.0135 | Grad Norm: 0.01015288\n",
      "Epoch 1 | Step 81400 | Avg Loss: 0.0138 | Grad Norm: 0.01012841\n",
      "Epoch 1 | Step 81500 | Avg Loss: 0.0141 | Grad Norm: 0.01036214\n",
      "Epoch 1 | Step 81600 | Avg Loss: 0.0137 | Grad Norm: 0.00954407\n",
      "Epoch 1 | Step 81700 | Avg Loss: 0.0138 | Grad Norm: 0.00950965\n",
      "Epoch 1 | Step 81800 | Avg Loss: 0.0141 | Grad Norm: 0.01068061\n",
      "Epoch 1 | Step 81900 | Avg Loss: 0.0140 | Grad Norm: 0.01210244\n",
      "Epoch 1 | Step 82000 | Avg Loss: 0.0143 | Grad Norm: 0.00908253\n",
      "Epoch 1 | Step 82100 | Avg Loss: 0.0140 | Grad Norm: 0.01015616\n",
      "Epoch 1 | Step 82200 | Avg Loss: 0.0136 | Grad Norm: 0.01017472\n",
      "Epoch 1 | Step 82300 | Avg Loss: 0.0137 | Grad Norm: 0.01025910\n",
      "Epoch 1 | Step 82400 | Avg Loss: 0.0139 | Grad Norm: 0.00841310\n",
      "Epoch 1 | Step 82500 | Avg Loss: 0.0139 | Grad Norm: 0.00936071\n",
      "Epoch 1 | Step 82600 | Avg Loss: 0.0139 | Grad Norm: 0.01053332\n",
      "Epoch 1 | Step 82700 | Avg Loss: 0.0141 | Grad Norm: 0.00944553\n",
      "Epoch 1 | Step 82800 | Avg Loss: 0.0141 | Grad Norm: 0.01099524\n",
      "Epoch 1 | Step 82900 | Avg Loss: 0.0140 | Grad Norm: 0.01152135\n",
      "Epoch 1 | Step 83000 | Avg Loss: 0.0143 | Grad Norm: 0.01095831\n",
      "Epoch 1 | Step 83100 | Avg Loss: 0.0138 | Grad Norm: 0.01017590\n",
      "Epoch 1 | Step 83200 | Avg Loss: 0.0140 | Grad Norm: 0.00873030\n",
      "Epoch 1 | Step 83300 | Avg Loss: 0.0141 | Grad Norm: 0.00891661\n",
      "Epoch 1 | Step 83400 | Avg Loss: 0.0141 | Grad Norm: 0.01105212\n",
      "Epoch 1 | Step 83500 | Avg Loss: 0.0145 | Grad Norm: 0.01029930\n",
      "Epoch 1 | Step 83600 | Avg Loss: 0.0143 | Grad Norm: 0.00983856\n",
      "Epoch 1 | Step 83700 | Avg Loss: 0.0140 | Grad Norm: 0.01049031\n",
      "Epoch 1 | Step 83800 | Avg Loss: 0.0147 | Grad Norm: 0.01015591\n",
      "Epoch 1 | Step 83900 | Avg Loss: 0.0145 | Grad Norm: 0.00939573\n",
      "Epoch 1 | Step 84000 | Avg Loss: 0.0142 | Grad Norm: 0.01061929\n",
      "Epoch 1 | Step 84100 | Avg Loss: 0.0141 | Grad Norm: 0.01158377\n",
      "Epoch 1 | Step 84200 | Avg Loss: 0.0137 | Grad Norm: 0.00989084\n",
      "Epoch 1 | Step 84300 | Avg Loss: 0.0144 | Grad Norm: 0.00981077\n",
      "Epoch 1 | Step 84400 | Avg Loss: 0.0143 | Grad Norm: 0.00876331\n",
      "Epoch 1 | Step 84500 | Avg Loss: 0.0143 | Grad Norm: 0.01117775\n",
      "Epoch 1 | Step 84600 | Avg Loss: 0.0142 | Grad Norm: 0.01077467\n",
      "Epoch 1 | Step 84700 | Avg Loss: 0.0143 | Grad Norm: 0.01003647\n",
      "Epoch 1 | Step 84800 | Avg Loss: 0.0143 | Grad Norm: 0.01030073\n",
      "Epoch 1 | Step 84900 | Avg Loss: 0.0142 | Grad Norm: 0.01123690\n",
      "Epoch 1 | Step 85000 | Avg Loss: 0.0136 | Grad Norm: 0.00817539\n",
      "Epoch 1 | Step 85100 | Avg Loss: 0.0139 | Grad Norm: 0.00979712\n",
      "Epoch 1 | Step 85200 | Avg Loss: 0.0142 | Grad Norm: 0.01429827\n",
      "Epoch 1 | Step 85300 | Avg Loss: 0.0139 | Grad Norm: 0.01013841\n",
      "Epoch 1 | Step 85400 | Avg Loss: 0.0137 | Grad Norm: 0.01080606\n",
      "Epoch 1 | Step 85500 | Avg Loss: 0.0135 | Grad Norm: 0.00889014\n",
      "Epoch 1 | Step 85600 | Avg Loss: 0.0134 | Grad Norm: 0.01065200\n",
      "Epoch 1 | Step 85700 | Avg Loss: 0.0133 | Grad Norm: 0.01018018\n",
      "Epoch 1 | Step 85800 | Avg Loss: 0.0136 | Grad Norm: 0.00985581\n",
      "Epoch 1 | Step 85900 | Avg Loss: 0.0139 | Grad Norm: 0.01013910\n",
      "Epoch 1 | Step 86000 | Avg Loss: 0.0142 | Grad Norm: 0.01112805\n",
      "Epoch 1 | Step 86100 | Avg Loss: 0.0141 | Grad Norm: 0.01060584\n",
      "Epoch 1 | Step 86200 | Avg Loss: 0.0141 | Grad Norm: 0.01307719\n",
      "Epoch 1 | Step 86300 | Avg Loss: 0.0142 | Grad Norm: 0.00917118\n",
      "Epoch 1 | Step 86400 | Avg Loss: 0.0141 | Grad Norm: 0.01222464\n",
      "Epoch 1 | Step 86500 | Avg Loss: 0.0141 | Grad Norm: 0.01010581\n",
      "Epoch 1 | Step 86600 | Avg Loss: 0.0138 | Grad Norm: 0.01050925\n",
      "Epoch 1 | Step 86700 | Avg Loss: 0.0138 | Grad Norm: 0.01110621\n",
      "Epoch 1 | Step 86800 | Avg Loss: 0.0140 | Grad Norm: 0.01035279\n",
      "Epoch 1 | Step 86900 | Avg Loss: 0.0139 | Grad Norm: 0.01157204\n",
      "Epoch 1 | Step 87000 | Avg Loss: 0.0142 | Grad Norm: 0.00957485\n",
      "Epoch 1 | Step 87100 | Avg Loss: 0.0142 | Grad Norm: 0.01101020\n",
      "Epoch 1 | Step 87200 | Avg Loss: 0.0143 | Grad Norm: 0.00987239\n",
      "Epoch 1 | Step 87300 | Avg Loss: 0.0143 | Grad Norm: 0.01141116\n",
      "Epoch 1 | Step 87400 | Avg Loss: 0.0144 | Grad Norm: 0.01061196\n",
      "Epoch 1 | Step 87500 | Avg Loss: 0.0148 | Grad Norm: 0.01090914\n",
      "Epoch 1 | Step 87600 | Avg Loss: 0.0146 | Grad Norm: 0.01131967\n",
      "Epoch 1 | Step 87700 | Avg Loss: 0.0150 | Grad Norm: 0.01191207\n",
      "Epoch 1 | Step 87800 | Avg Loss: 0.0147 | Grad Norm: 0.00996276\n",
      "Epoch 1 | Step 87900 | Avg Loss: 0.0144 | Grad Norm: 0.01161397\n",
      "Epoch 1 | Step 88000 | Avg Loss: 0.0145 | Grad Norm: 0.01140423\n",
      "Epoch 1 | Step 88100 | Avg Loss: 0.0145 | Grad Norm: 0.01188779\n",
      "Epoch 1 | Step 88200 | Avg Loss: 0.0143 | Grad Norm: 0.01098127\n",
      "Epoch 1 | Step 88300 | Avg Loss: 0.0140 | Grad Norm: 0.01217018\n",
      "Epoch 1 | Step 88400 | Avg Loss: 0.0140 | Grad Norm: 0.01104739\n",
      "Epoch 1 | Step 88500 | Avg Loss: 0.0139 | Grad Norm: 0.00871822\n",
      "Epoch 1 | Step 88600 | Avg Loss: 0.0135 | Grad Norm: 0.01061821\n",
      "Epoch 1 | Step 88700 | Avg Loss: 0.0138 | Grad Norm: 0.01045994\n",
      "Epoch 1 | Step 88800 | Avg Loss: 0.0142 | Grad Norm: 0.00954175\n",
      "Epoch 1 | Step 88900 | Avg Loss: 0.0141 | Grad Norm: 0.00988297\n",
      "Epoch 1 | Step 89000 | Avg Loss: 0.0139 | Grad Norm: 0.00868523\n",
      "Epoch 1 | Step 89100 | Avg Loss: 0.0141 | Grad Norm: 0.01165811\n",
      "Epoch 1 | Step 89200 | Avg Loss: 0.0145 | Grad Norm: 0.01023630\n",
      "Epoch 1 | Step 89300 | Avg Loss: 0.0143 | Grad Norm: 0.01083040\n",
      "Epoch 1 | Step 89400 | Avg Loss: 0.0144 | Grad Norm: 0.00868116\n",
      "Epoch 1 | Step 89500 | Avg Loss: 0.0144 | Grad Norm: 0.01147932\n",
      "Epoch 1 | Step 89600 | Avg Loss: 0.0142 | Grad Norm: 0.01014509\n",
      "Epoch 1 | Step 89700 | Avg Loss: 0.0132 | Grad Norm: 0.00955905\n",
      "Epoch 1 | Step 89800 | Avg Loss: 0.0134 | Grad Norm: 0.00915965\n",
      "Epoch 1 | Step 89900 | Avg Loss: 0.0139 | Grad Norm: 0.01001308\n",
      "Epoch 1 | Step 90000 | Avg Loss: 0.0144 | Grad Norm: 0.01041617\n",
      "Epoch 1 | Step 90100 | Avg Loss: 0.0147 | Grad Norm: 0.01150832\n",
      "Epoch 1 | Step 90200 | Avg Loss: 0.0145 | Grad Norm: 0.01113090\n",
      "Epoch 1 | Step 90300 | Avg Loss: 0.0142 | Grad Norm: 0.01110551\n",
      "Epoch 1 | Step 90400 | Avg Loss: 0.0145 | Grad Norm: 0.01151301\n",
      "Epoch 1 | Step 90500 | Avg Loss: 0.0145 | Grad Norm: 0.01165095\n",
      "Epoch 1 | Step 90600 | Avg Loss: 0.0146 | Grad Norm: 0.01031043\n",
      "Epoch 1 | Step 90700 | Avg Loss: 0.0144 | Grad Norm: 0.01261960\n",
      "Epoch 1 | Step 90800 | Avg Loss: 0.0140 | Grad Norm: 0.01169116\n",
      "Epoch 1 | Step 90900 | Avg Loss: 0.0136 | Grad Norm: 0.00946789\n",
      "Epoch 1 | Step 91000 | Avg Loss: 0.0136 | Grad Norm: 0.01211302\n",
      "Epoch 1 | Step 91100 | Avg Loss: 0.0136 | Grad Norm: 0.01103551\n",
      "Epoch 1 | Step 91200 | Avg Loss: 0.0144 | Grad Norm: 0.01192045\n",
      "Epoch 1 | Step 91300 | Avg Loss: 0.0144 | Grad Norm: 0.00895858\n",
      "Epoch 1 | Step 91400 | Avg Loss: 0.0142 | Grad Norm: 0.01109581\n",
      "Epoch 1 | Step 91500 | Avg Loss: 0.0139 | Grad Norm: 0.01058660\n",
      "Epoch 1 | Step 91600 | Avg Loss: 0.0141 | Grad Norm: 0.00945130\n",
      "Epoch 1 | Step 91700 | Avg Loss: 0.0140 | Grad Norm: 0.01154762\n",
      "Epoch 1 | Step 91800 | Avg Loss: 0.0138 | Grad Norm: 0.01233869\n",
      "Epoch 1 | Step 91900 | Avg Loss: 0.0141 | Grad Norm: 0.01101939\n",
      "Epoch 1 | Step 92000 | Avg Loss: 0.0142 | Grad Norm: 0.00872954\n",
      "Epoch 1 | Step 92100 | Avg Loss: 0.0141 | Grad Norm: 0.00999295\n",
      "Epoch 1 | Step 92200 | Avg Loss: 0.0141 | Grad Norm: 0.00998180\n",
      "Epoch 1 | Step 92300 | Avg Loss: 0.0143 | Grad Norm: 0.01087914\n",
      "Epoch 1 | Step 92400 | Avg Loss: 0.0143 | Grad Norm: 0.01086203\n",
      "Epoch 1 | Step 92500 | Avg Loss: 0.0144 | Grad Norm: 0.01001390\n",
      "Epoch 1 | Step 92600 | Avg Loss: 0.0143 | Grad Norm: 0.01056309\n",
      "Epoch 1 | Step 92700 | Avg Loss: 0.0142 | Grad Norm: 0.01090369\n",
      "Epoch 1 | Step 92800 | Avg Loss: 0.0138 | Grad Norm: 0.00961658\n",
      "Epoch 1 | Step 92900 | Avg Loss: 0.0137 | Grad Norm: 0.00853640\n",
      "Epoch 1 | Step 93000 | Avg Loss: 0.0139 | Grad Norm: 0.01096259\n",
      "Epoch 1 | Step 93100 | Avg Loss: 0.0140 | Grad Norm: 0.01061116\n",
      "Epoch 1 | Step 93200 | Avg Loss: 0.0138 | Grad Norm: 0.00949120\n",
      "Epoch 1 | Step 93300 | Avg Loss: 0.0134 | Grad Norm: 0.01154638\n",
      "Epoch 1 | Step 93400 | Avg Loss: 0.0136 | Grad Norm: 0.01124983\n",
      "Epoch 1 | Step 93500 | Avg Loss: 0.0139 | Grad Norm: 0.01010324\n",
      "Epoch 1 | Step 93600 | Avg Loss: 0.0138 | Grad Norm: 0.01239749\n",
      "Epoch 1 | Step 93700 | Avg Loss: 0.0135 | Grad Norm: 0.00988616\n",
      "Epoch 1 | Step 93800 | Avg Loss: 0.0141 | Grad Norm: 0.01040156\n",
      "Epoch 1 | Step 93900 | Avg Loss: 0.0145 | Grad Norm: 0.01055261\n",
      "Epoch 1 | Step 94000 | Avg Loss: 0.0143 | Grad Norm: 0.00969319\n",
      "Epoch 1 | Step 94100 | Avg Loss: 0.0142 | Grad Norm: 0.01241021\n",
      "Epoch 1 | Step 94200 | Avg Loss: 0.0145 | Grad Norm: 0.00942779\n",
      "Epoch 1 | Step 94300 | Avg Loss: 0.0139 | Grad Norm: 0.01164120\n",
      "Epoch 1 | Step 94400 | Avg Loss: 0.0142 | Grad Norm: 0.01071691\n",
      "Epoch 1 | Step 94500 | Avg Loss: 0.0141 | Grad Norm: 0.01018231\n",
      "Epoch 1 | Step 94600 | Avg Loss: 0.0141 | Grad Norm: 0.00978520\n",
      "Epoch 1 | Step 94700 | Avg Loss: 0.0139 | Grad Norm: 0.01169859\n",
      "Epoch 1 | Step 94800 | Avg Loss: 0.0139 | Grad Norm: 0.01152795\n",
      "Epoch 1 | Step 94900 | Avg Loss: 0.0140 | Grad Norm: 0.01086477\n",
      "Epoch 1 | Step 95000 | Avg Loss: 0.0141 | Grad Norm: 0.01119427\n",
      "Epoch 1 | Step 95100 | Avg Loss: 0.0141 | Grad Norm: 0.01118517\n",
      "Epoch 1 | Step 95200 | Avg Loss: 0.0144 | Grad Norm: 0.00870778\n",
      "Epoch 1 | Step 95300 | Avg Loss: 0.0140 | Grad Norm: 0.00903326\n",
      "Epoch 1 | Step 95400 | Avg Loss: 0.0142 | Grad Norm: 0.00992236\n",
      "Epoch 1 | Step 95500 | Avg Loss: 0.0143 | Grad Norm: 0.00989465\n",
      "Epoch 1 | Step 95600 | Avg Loss: 0.0136 | Grad Norm: 0.01078411\n",
      "Epoch 1 | Step 95700 | Avg Loss: 0.0139 | Grad Norm: 0.01122254\n",
      "Epoch 1 | Step 95800 | Avg Loss: 0.0136 | Grad Norm: 0.01032774\n",
      "Epoch 1 | Step 95900 | Avg Loss: 0.0135 | Grad Norm: 0.00895169\n",
      "Epoch 1 | Step 96000 | Avg Loss: 0.0136 | Grad Norm: 0.01113240\n",
      "Epoch 1 | Step 96100 | Avg Loss: 0.0136 | Grad Norm: 0.01030191\n",
      "Epoch 1 | Step 96200 | Avg Loss: 0.0133 | Grad Norm: 0.00938121\n",
      "Epoch 1 | Step 96300 | Avg Loss: 0.0136 | Grad Norm: 0.00952901\n",
      "Epoch 1 | Step 96400 | Avg Loss: 0.0139 | Grad Norm: 0.01008137\n",
      "Epoch 1 | Step 96500 | Avg Loss: 0.0141 | Grad Norm: 0.01040215\n",
      "Epoch 1 | Step 96600 | Avg Loss: 0.0140 | Grad Norm: 0.01052151\n",
      "Epoch 1 | Step 96700 | Avg Loss: 0.0142 | Grad Norm: 0.00956799\n",
      "Epoch 1 | Step 96800 | Avg Loss: 0.0142 | Grad Norm: 0.01119845\n",
      "Epoch 1 | Step 96900 | Avg Loss: 0.0142 | Grad Norm: 0.00962564\n",
      "Epoch 1 | Step 97000 | Avg Loss: 0.0143 | Grad Norm: 0.00923436\n",
      "Epoch 1 | Step 97100 | Avg Loss: 0.0141 | Grad Norm: 0.00963473\n",
      "Epoch 1 | Step 97200 | Avg Loss: 0.0144 | Grad Norm: 0.00964693\n",
      "Epoch 1 | Step 97300 | Avg Loss: 0.0144 | Grad Norm: 0.01131498\n",
      "Epoch 1 | Step 97400 | Avg Loss: 0.0143 | Grad Norm: 0.00985708\n",
      "Epoch 1 | Step 97500 | Avg Loss: 0.0147 | Grad Norm: 0.01043291\n",
      "Epoch 1 | Step 97600 | Avg Loss: 0.0144 | Grad Norm: 0.01115684\n",
      "Epoch 1 | Step 97700 | Avg Loss: 0.0144 | Grad Norm: 0.01090776\n",
      "Epoch 1 | Step 97800 | Avg Loss: 0.0148 | Grad Norm: 0.00970435\n",
      "Epoch 1 | Step 97900 | Avg Loss: 0.0153 | Grad Norm: 0.01275229\n",
      "Epoch 1 | Step 98000 | Avg Loss: 0.0145 | Grad Norm: 0.01025894\n",
      "Epoch 1 | Step 98100 | Avg Loss: 0.0143 | Grad Norm: 0.01021921\n",
      "Epoch 1 | Step 98200 | Avg Loss: 0.0142 | Grad Norm: 0.00939212\n",
      "Epoch 1 | Step 98300 | Avg Loss: 0.0143 | Grad Norm: 0.01003364\n",
      "Epoch 1 | Step 98400 | Avg Loss: 0.0142 | Grad Norm: 0.00970599\n",
      "Epoch 1 | Step 98500 | Avg Loss: 0.0140 | Grad Norm: 0.01021703\n",
      "Epoch 1 | Step 98600 | Avg Loss: 0.0139 | Grad Norm: 0.01538583\n",
      "Epoch 1 | Step 98700 | Avg Loss: 0.0144 | Grad Norm: 0.01099121\n",
      "Epoch 1 | Step 98800 | Avg Loss: 0.0144 | Grad Norm: 0.01051594\n",
      "Epoch 1 | Step 98900 | Avg Loss: 0.0144 | Grad Norm: 0.00851122\n",
      "Epoch 1 | Step 99000 | Avg Loss: 0.0143 | Grad Norm: 0.01006908\n",
      "Epoch 1 | Step 99100 | Avg Loss: 0.0142 | Grad Norm: 0.01199858\n",
      "Epoch 1 | Step 99200 | Avg Loss: 0.0142 | Grad Norm: 0.01214617\n",
      "Epoch 1 | Step 99300 | Avg Loss: 0.0139 | Grad Norm: 0.01009741\n",
      "Epoch 1 | Step 99400 | Avg Loss: 0.0136 | Grad Norm: 0.00878254\n",
      "Epoch 1 | Step 99500 | Avg Loss: 0.0134 | Grad Norm: 0.01176776\n",
      "Epoch 1 | Step 99600 | Avg Loss: 0.0137 | Grad Norm: 0.01066372\n",
      "Epoch 1 | Step 99700 | Avg Loss: 0.0136 | Grad Norm: 0.01123356\n",
      "Epoch 1 | Step 99800 | Avg Loss: 0.0135 | Grad Norm: 0.01147367\n",
      "Epoch 1 | Step 99900 | Avg Loss: 0.0138 | Grad Norm: 0.01067761\n",
      "Epoch 1 | Step 100000 | Avg Loss: 0.0137 | Grad Norm: 0.00990265\n",
      "Saving model at step100000\n",
      "Epoch 1 | Step 100100 | Avg Loss: 0.0139 | Grad Norm: 0.01062072\n",
      "Epoch 1 | Step 100200 | Avg Loss: 0.0137 | Grad Norm: 0.00878357\n",
      "Epoch 1 | Step 100300 | Avg Loss: 0.0141 | Grad Norm: 0.01076330\n",
      "Epoch 1 | Step 100400 | Avg Loss: 0.0140 | Grad Norm: 0.00967433\n",
      "Epoch 1 | Step 100500 | Avg Loss: 0.0143 | Grad Norm: 0.00999514\n",
      "Epoch 1 | Step 100600 | Avg Loss: 0.0146 | Grad Norm: 0.01414208\n",
      "Epoch 1 | Step 100700 | Avg Loss: 0.0150 | Grad Norm: 0.01217497\n",
      "Epoch 1 | Step 100800 | Avg Loss: 0.0149 | Grad Norm: 0.01239388\n",
      "Epoch 1 | Step 100900 | Avg Loss: 0.0149 | Grad Norm: 0.01208460\n",
      "Epoch 1 | Step 101000 | Avg Loss: 0.0147 | Grad Norm: 0.01031905\n",
      "Epoch 1 | Step 101100 | Avg Loss: 0.0145 | Grad Norm: 0.01156281\n",
      "Epoch 1 | Step 101200 | Avg Loss: 0.0141 | Grad Norm: 0.00992388\n",
      "Epoch 1 | Step 101300 | Avg Loss: 0.0147 | Grad Norm: 0.01109771\n",
      "Epoch 1 | Step 101400 | Avg Loss: 0.0142 | Grad Norm: 0.01026381\n",
      "Epoch 1 | Step 101500 | Avg Loss: 0.0141 | Grad Norm: 0.00805567\n",
      "Epoch 1 | Step 101600 | Avg Loss: 0.0136 | Grad Norm: 0.01020964\n",
      "Epoch 1 | Step 101700 | Avg Loss: 0.0138 | Grad Norm: 0.00860308\n",
      "Epoch 1 | Step 101800 | Avg Loss: 0.0135 | Grad Norm: 0.01519747\n",
      "Epoch 1 | Step 101900 | Avg Loss: 0.0137 | Grad Norm: 0.01156499\n",
      "Epoch 1 | Step 102000 | Avg Loss: 0.0140 | Grad Norm: 0.01123099\n",
      "Epoch 1 | Step 102100 | Avg Loss: 0.0136 | Grad Norm: 0.00956257\n",
      "Epoch 1 | Step 102200 | Avg Loss: 0.0137 | Grad Norm: 0.00926041\n",
      "Epoch 1 | Step 102300 | Avg Loss: 0.0136 | Grad Norm: 0.01152439\n",
      "Epoch 1 | Step 102400 | Avg Loss: 0.0140 | Grad Norm: 0.01144991\n",
      "Epoch 1 | Step 102500 | Avg Loss: 0.0141 | Grad Norm: 0.00994871\n",
      "Epoch 1 | Step 102600 | Avg Loss: 0.0144 | Grad Norm: 0.00996053\n",
      "Epoch 1 | Step 102700 | Avg Loss: 0.0145 | Grad Norm: 0.00948798\n",
      "Epoch 1 | Step 102800 | Avg Loss: 0.0139 | Grad Norm: 0.01141592\n",
      "Epoch 1 | Step 102900 | Avg Loss: 0.0143 | Grad Norm: 0.01117402\n",
      "Epoch 1 | Step 103000 | Avg Loss: 0.0140 | Grad Norm: 0.00943788\n",
      "Epoch 1 | Step 103100 | Avg Loss: 0.0141 | Grad Norm: 0.00914553\n",
      "Epoch 1 | Step 103200 | Avg Loss: 0.0136 | Grad Norm: 0.00998950\n",
      "Epoch 1 | Step 103300 | Avg Loss: 0.0133 | Grad Norm: 0.01061231\n",
      "Epoch 1 | Step 103400 | Avg Loss: 0.0136 | Grad Norm: 0.01064100\n",
      "Epoch 1 | Step 103500 | Avg Loss: 0.0141 | Grad Norm: 0.01288126\n",
      "Epoch 1 | Step 103600 | Avg Loss: 0.0141 | Grad Norm: 0.00988755\n",
      "Epoch 1 | Step 103700 | Avg Loss: 0.0141 | Grad Norm: 0.00983714\n",
      "Epoch 1 | Step 103800 | Avg Loss: 0.0141 | Grad Norm: 0.01052637\n",
      "Epoch 1 | Step 103900 | Avg Loss: 0.0139 | Grad Norm: 0.01091305\n",
      "Epoch 1 | Step 104000 | Avg Loss: 0.0139 | Grad Norm: 0.01022124\n",
      "Epoch 1 | Step 104100 | Avg Loss: 0.0139 | Grad Norm: 0.01010496\n",
      "Epoch 1 | Step 104200 | Avg Loss: 0.0137 | Grad Norm: 0.00960580\n",
      "Epoch 1 | Step 104300 | Avg Loss: 0.0140 | Grad Norm: 0.01172672\n",
      "Epoch 1 | Step 104400 | Avg Loss: 0.0140 | Grad Norm: 0.00981874\n",
      "Epoch 1 | Step 104500 | Avg Loss: 0.0142 | Grad Norm: 0.00902253\n",
      "Epoch 1 | Step 104600 | Avg Loss: 0.0141 | Grad Norm: 0.01104170\n",
      "Epoch 1 | Step 104700 | Avg Loss: 0.0139 | Grad Norm: 0.01028485\n",
      "Epoch 1 | Step 104800 | Avg Loss: 0.0138 | Grad Norm: 0.01083036\n",
      "Epoch 1 | Step 104900 | Avg Loss: 0.0138 | Grad Norm: 0.01034572\n",
      "Epoch 1 | Step 105000 | Avg Loss: 0.0140 | Grad Norm: 0.01034662\n",
      "Epoch 1 | Step 105100 | Avg Loss: 0.0142 | Grad Norm: 0.01138436\n",
      "Epoch 1 | Step 105200 | Avg Loss: 0.0142 | Grad Norm: 0.01026290\n",
      "Epoch 1 | Step 105300 | Avg Loss: 0.0145 | Grad Norm: 0.01051967\n",
      "Epoch 1 | Step 105400 | Avg Loss: 0.0142 | Grad Norm: 0.01226029\n",
      "Epoch 1 | Step 105500 | Avg Loss: 0.0135 | Grad Norm: 0.00949672\n",
      "Epoch 1 | Step 105600 | Avg Loss: 0.0137 | Grad Norm: 0.01117245\n",
      "Epoch 1 | Step 105700 | Avg Loss: 0.0139 | Grad Norm: 0.01057860\n",
      "Epoch 1 | Step 105800 | Avg Loss: 0.0142 | Grad Norm: 0.01024681\n",
      "Epoch 1 | Step 105900 | Avg Loss: 0.0141 | Grad Norm: 0.00885601\n",
      "Epoch 1 | Step 106000 | Avg Loss: 0.0145 | Grad Norm: 0.01131000\n",
      "Epoch 1 | Step 106100 | Avg Loss: 0.0144 | Grad Norm: 0.01042381\n",
      "Epoch 1 | Step 106200 | Avg Loss: 0.0145 | Grad Norm: 0.01102034\n",
      "Epoch 1 | Step 106300 | Avg Loss: 0.0144 | Grad Norm: 0.01051600\n",
      "Epoch 1 | Step 106400 | Avg Loss: 0.0141 | Grad Norm: 0.00940865\n",
      "Epoch 1 | Step 106500 | Avg Loss: 0.0139 | Grad Norm: 0.00968895\n",
      "Epoch 1 | Step 106600 | Avg Loss: 0.0133 | Grad Norm: 0.00904123\n",
      "Epoch 1 | Step 106700 | Avg Loss: 0.0135 | Grad Norm: 0.01091352\n",
      "Epoch 1 | Step 106800 | Avg Loss: 0.0138 | Grad Norm: 0.01182286\n",
      "Epoch 1 | Step 106900 | Avg Loss: 0.0138 | Grad Norm: 0.00958677\n",
      "Epoch 1 | Step 107000 | Avg Loss: 0.0140 | Grad Norm: 0.01145979\n",
      "Epoch 1 | Step 107100 | Avg Loss: 0.0142 | Grad Norm: 0.00939946\n",
      "Epoch 1 | Step 107200 | Avg Loss: 0.0147 | Grad Norm: 0.00949390\n",
      "Epoch 1 | Step 107300 | Avg Loss: 0.0152 | Grad Norm: 0.01103212\n",
      "Epoch 1 | Step 107400 | Avg Loss: 0.0150 | Grad Norm: 0.01179413\n",
      "Epoch 1 | Step 107500 | Avg Loss: 0.0151 | Grad Norm: 0.01117427\n",
      "Epoch 1 | Step 107600 | Avg Loss: 0.0149 | Grad Norm: 0.01200047\n",
      "Epoch 1 | Step 107700 | Avg Loss: 0.0145 | Grad Norm: 0.01136654\n",
      "Epoch 1 | Step 107800 | Avg Loss: 0.0148 | Grad Norm: 0.01062801\n",
      "Epoch 1 | Step 107900 | Avg Loss: 0.0147 | Grad Norm: 0.01134013\n",
      "Epoch 1 | Step 108000 | Avg Loss: 0.0147 | Grad Norm: 0.01015033\n",
      "Epoch 1 | Step 108100 | Avg Loss: 0.0149 | Grad Norm: 0.01140106\n",
      "Epoch 1 | Step 108200 | Avg Loss: 0.0146 | Grad Norm: 0.00948361\n",
      "Epoch 1 | Step 108300 | Avg Loss: 0.0144 | Grad Norm: 0.01289750\n",
      "Epoch 1 | Step 108400 | Avg Loss: 0.0148 | Grad Norm: 0.01113567\n",
      "Epoch 1 | Step 108500 | Avg Loss: 0.0144 | Grad Norm: 0.00948999\n",
      "Epoch 1 | Step 108600 | Avg Loss: 0.0143 | Grad Norm: 0.01039747\n",
      "Epoch 1 | Step 108700 | Avg Loss: 0.0141 | Grad Norm: 0.01044132\n",
      "Epoch 1 | Step 108800 | Avg Loss: 0.0143 | Grad Norm: 0.01234907\n",
      "Epoch 1 | Step 108900 | Avg Loss: 0.0142 | Grad Norm: 0.00990851\n",
      "Epoch 1 | Step 109000 | Avg Loss: 0.0144 | Grad Norm: 0.01240246\n",
      "Epoch 1 | Step 109100 | Avg Loss: 0.0140 | Grad Norm: 0.00948527\n",
      "Epoch 1 | Step 109200 | Avg Loss: 0.0140 | Grad Norm: 0.01027011\n",
      "Epoch 1 | Step 109300 | Avg Loss: 0.0145 | Grad Norm: 0.01019654\n",
      "Epoch 1 | Step 109400 | Avg Loss: 0.0143 | Grad Norm: 0.01056164\n",
      "Epoch 1 | Step 109500 | Avg Loss: 0.0144 | Grad Norm: 0.00888220\n",
      "Epoch 1 | Step 109600 | Avg Loss: 0.0141 | Grad Norm: 0.01114532\n",
      "Epoch 1 | Step 109700 | Avg Loss: 0.0138 | Grad Norm: 0.00944284\n",
      "Epoch 1 | Step 109800 | Avg Loss: 0.0138 | Grad Norm: 0.01129932\n",
      "Epoch 1 | Step 109900 | Avg Loss: 0.0140 | Grad Norm: 0.01012972\n",
      "Epoch 1 | Step 110000 | Avg Loss: 0.0141 | Grad Norm: 0.01136197\n",
      "Epoch 1 | Step 110100 | Avg Loss: 0.0138 | Grad Norm: 0.01069120\n",
      "Epoch 1 | Step 110200 | Avg Loss: 0.0135 | Grad Norm: 0.00966956\n",
      "Epoch 1 | Step 110300 | Avg Loss: 0.0138 | Grad Norm: 0.00892801\n",
      "Epoch 1 | Step 110400 | Avg Loss: 0.0138 | Grad Norm: 0.01016796\n",
      "Epoch 1 | Step 110500 | Avg Loss: 0.0139 | Grad Norm: 0.01091553\n",
      "Epoch 1 | Step 110600 | Avg Loss: 0.0141 | Grad Norm: 0.01106965\n",
      "Epoch 1 | Step 110700 | Avg Loss: 0.0143 | Grad Norm: 0.01189916\n",
      "Epoch 1 | Step 110800 | Avg Loss: 0.0142 | Grad Norm: 0.01107025\n",
      "Epoch 1 | Step 110900 | Avg Loss: 0.0145 | Grad Norm: 0.01061351\n",
      "Epoch 1 | Step 111000 | Avg Loss: 0.0143 | Grad Norm: 0.01029440\n",
      "Epoch 1 | Step 111100 | Avg Loss: 0.0143 | Grad Norm: 0.01029874\n",
      "Epoch 1 | Step 111200 | Avg Loss: 0.0141 | Grad Norm: 0.00959849\n",
      "Epoch 1 | Step 111300 | Avg Loss: 0.0143 | Grad Norm: 0.01132525\n",
      "Epoch 1 | Step 111400 | Avg Loss: 0.0144 | Grad Norm: 0.01128908\n",
      "Epoch 1 | Step 111500 | Avg Loss: 0.0142 | Grad Norm: 0.00871489\n",
      "Epoch 1 | Step 111600 | Avg Loss: 0.0142 | Grad Norm: 0.01123412\n",
      "Epoch 1 | Step 111700 | Avg Loss: 0.0145 | Grad Norm: 0.01006104\n",
      "Epoch 1 | Step 111800 | Avg Loss: 0.0137 | Grad Norm: 0.01096188\n",
      "Epoch 1 | Step 111900 | Avg Loss: 0.0141 | Grad Norm: 0.01188187\n",
      "Epoch 1 | Step 112000 | Avg Loss: 0.0139 | Grad Norm: 0.01081260\n",
      "Epoch 1 | Step 112100 | Avg Loss: 0.0140 | Grad Norm: 0.01159527\n",
      "Epoch 1 | Step 112200 | Avg Loss: 0.0140 | Grad Norm: 0.01032224\n",
      "Epoch 1 | Step 112300 | Avg Loss: 0.0140 | Grad Norm: 0.01145293\n",
      "Epoch 1 | Step 112400 | Avg Loss: 0.0140 | Grad Norm: 0.00874593\n",
      "Epoch 1 | Step 112500 | Avg Loss: 0.0139 | Grad Norm: 0.00997640\n",
      "Epoch 1 | Step 112600 | Avg Loss: 0.0142 | Grad Norm: 0.01017052\n",
      "Epoch 1 | Step 112700 | Avg Loss: 0.0144 | Grad Norm: 0.00960263\n",
      "Epoch 1 | Step 112800 | Avg Loss: 0.0141 | Grad Norm: 0.01027477\n",
      "Epoch 1 | Step 112900 | Avg Loss: 0.0146 | Grad Norm: 0.01025815\n",
      "Epoch 1 | Step 113000 | Avg Loss: 0.0147 | Grad Norm: 0.01053793\n",
      "Epoch 1 | Step 113100 | Avg Loss: 0.0148 | Grad Norm: 0.01095533\n",
      "Epoch 1 | Step 113200 | Avg Loss: 0.0147 | Grad Norm: 0.01065741\n",
      "Epoch 1 | Step 113300 | Avg Loss: 0.0147 | Grad Norm: 0.01014274\n",
      "Epoch 1 | Step 113400 | Avg Loss: 0.0143 | Grad Norm: 0.00983683\n",
      "Epoch 1 | Step 113500 | Avg Loss: 0.0142 | Grad Norm: 0.01140738\n",
      "Epoch 1 | Step 113600 | Avg Loss: 0.0140 | Grad Norm: 0.00945436\n",
      "Epoch 1 | Step 113700 | Avg Loss: 0.0140 | Grad Norm: 0.01066006\n",
      "Epoch 1 | Step 113800 | Avg Loss: 0.0142 | Grad Norm: 0.00976963\n",
      "Epoch 1 | Step 113900 | Avg Loss: 0.0143 | Grad Norm: 0.00925859\n",
      "Epoch 1 | Step 114000 | Avg Loss: 0.0143 | Grad Norm: 0.00905686\n",
      "Epoch 1 | Step 114100 | Avg Loss: 0.0141 | Grad Norm: 0.01037093\n",
      "Epoch 1 | Step 114200 | Avg Loss: 0.0144 | Grad Norm: 0.01146689\n",
      "Epoch 1 | Step 114300 | Avg Loss: 0.0146 | Grad Norm: 0.01163196\n",
      "Epoch 1 | Step 114400 | Avg Loss: 0.0143 | Grad Norm: 0.01007364\n",
      "Epoch 1 | Step 114500 | Avg Loss: 0.0144 | Grad Norm: 0.01005673\n",
      "Epoch 1 | Step 114600 | Avg Loss: 0.0142 | Grad Norm: 0.01105430\n",
      "Epoch 1 | Step 114700 | Avg Loss: 0.0138 | Grad Norm: 0.01098069\n",
      "Epoch 1 | Step 114800 | Avg Loss: 0.0139 | Grad Norm: 0.01071216\n",
      "Epoch 1 | Step 114900 | Avg Loss: 0.0141 | Grad Norm: 0.01030374\n",
      "Epoch 1 | Step 115000 | Avg Loss: 0.0141 | Grad Norm: 0.00987602\n",
      "Epoch 1 | Step 115100 | Avg Loss: 0.0141 | Grad Norm: 0.00875523\n",
      "Epoch 1 | Step 115200 | Avg Loss: 0.0142 | Grad Norm: 0.00910092\n",
      "Epoch 1 | Step 115300 | Avg Loss: 0.0142 | Grad Norm: 0.01001089\n",
      "Epoch 1 | Step 115400 | Avg Loss: 0.0139 | Grad Norm: 0.01165608\n",
      "Epoch 1 | Step 115500 | Avg Loss: 0.0136 | Grad Norm: 0.00807965\n",
      "Epoch 1 | Step 115600 | Avg Loss: 0.0139 | Grad Norm: 0.01017374\n",
      "Epoch 1 | Step 115700 | Avg Loss: 0.0137 | Grad Norm: 0.00990030\n",
      "Epoch 1 | Step 115800 | Avg Loss: 0.0140 | Grad Norm: 0.01022759\n",
      "Epoch 1 | Step 115900 | Avg Loss: 0.0138 | Grad Norm: 0.00966558\n",
      "Epoch 1 | Step 116000 | Avg Loss: 0.0139 | Grad Norm: 0.01029420\n",
      "Epoch 1 | Step 116100 | Avg Loss: 0.0140 | Grad Norm: 0.01008564\n",
      "Epoch 1 | Step 116200 | Avg Loss: 0.0139 | Grad Norm: 0.01123812\n",
      "Epoch 1 | Step 116300 | Avg Loss: 0.0145 | Grad Norm: 0.01127469\n",
      "Epoch 1 | Step 116400 | Avg Loss: 0.0149 | Grad Norm: 0.01110244\n",
      "Epoch 1 | Step 116500 | Avg Loss: 0.0142 | Grad Norm: 0.01068111\n",
      "Epoch 1 | Step 116600 | Avg Loss: 0.0141 | Grad Norm: 0.01009306\n",
      "Epoch 1 | Step 116700 | Avg Loss: 0.0141 | Grad Norm: 0.00883500\n",
      "Epoch 1 | Step 116800 | Avg Loss: 0.0145 | Grad Norm: 0.01013038\n",
      "Epoch 1 | Step 116900 | Avg Loss: 0.0146 | Grad Norm: 0.00938177\n",
      "Epoch 1 | Step 117000 | Avg Loss: 0.0143 | Grad Norm: 0.01042861\n",
      "Epoch 1 | Step 117100 | Avg Loss: 0.0141 | Grad Norm: 0.01230298\n",
      "Epoch 1 | Step 117200 | Avg Loss: 0.0140 | Grad Norm: 0.00991194\n",
      "Epoch 1 | Step 117300 | Avg Loss: 0.0142 | Grad Norm: 0.01016369\n",
      "Epoch 1 | Step 117400 | Avg Loss: 0.0142 | Grad Norm: 0.01072440\n",
      "Epoch 1 | Step 117500 | Avg Loss: 0.0142 | Grad Norm: 0.01079582\n",
      "Epoch 1 | Step 117600 | Avg Loss: 0.0143 | Grad Norm: 0.01012529\n",
      "Epoch 1 | Step 117700 | Avg Loss: 0.0146 | Grad Norm: 0.01137279\n",
      "Epoch 1 | Step 117800 | Avg Loss: 0.0143 | Grad Norm: 0.00965498\n",
      "Epoch 1 | Step 117900 | Avg Loss: 0.0142 | Grad Norm: 0.01036570\n",
      "Epoch 1 | Step 118000 | Avg Loss: 0.0143 | Grad Norm: 0.01418031\n",
      "Epoch 1 | Step 118100 | Avg Loss: 0.0147 | Grad Norm: 0.01108852\n",
      "Epoch 1 | Step 118200 | Avg Loss: 0.0145 | Grad Norm: 0.01187488\n",
      "Epoch 1 | Step 118300 | Avg Loss: 0.0142 | Grad Norm: 0.01002627\n",
      "Epoch 1 | Step 118400 | Avg Loss: 0.0143 | Grad Norm: 0.01062306\n",
      "Epoch 1 | Step 118500 | Avg Loss: 0.0142 | Grad Norm: 0.01195660\n",
      "Epoch 1 | Step 118600 | Avg Loss: 0.0141 | Grad Norm: 0.00894584\n",
      "Epoch 1 | Step 118700 | Avg Loss: 0.0144 | Grad Norm: 0.01222825\n",
      "Epoch 1 | Step 118800 | Avg Loss: 0.0139 | Grad Norm: 0.01018400\n",
      "Epoch 1 | Step 118900 | Avg Loss: 0.0139 | Grad Norm: 0.00957608\n",
      "Epoch 1 | Step 119000 | Avg Loss: 0.0138 | Grad Norm: 0.01130390\n",
      "Epoch 1 | Step 119100 | Avg Loss: 0.0138 | Grad Norm: 0.01079477\n",
      "Epoch 1 | Step 119200 | Avg Loss: 0.0140 | Grad Norm: 0.00971610\n",
      "Epoch 1 | Step 119300 | Avg Loss: 0.0140 | Grad Norm: 0.00987542\n",
      "Epoch 1 | Step 119400 | Avg Loss: 0.0135 | Grad Norm: 0.00991842\n",
      "Epoch 1 | Step 119500 | Avg Loss: 0.0136 | Grad Norm: 0.00901432\n",
      "Epoch 1 | Step 119600 | Avg Loss: 0.0137 | Grad Norm: 0.00874050\n",
      "Epoch 1 | Step 119700 | Avg Loss: 0.0136 | Grad Norm: 0.01008991\n",
      "Epoch 1 | Step 119800 | Avg Loss: 0.0141 | Grad Norm: 0.01268866\n",
      "Epoch 1 | Step 119900 | Avg Loss: 0.0144 | Grad Norm: 0.01043607\n",
      "Epoch 1 | Step 120000 | Avg Loss: 0.0145 | Grad Norm: 0.01048805\n",
      "Epoch 1 | Step 120100 | Avg Loss: 0.0143 | Grad Norm: 0.01281025\n",
      "Epoch 1 | Step 120200 | Avg Loss: 0.0143 | Grad Norm: 0.01501081\n",
      "Epoch 1 | Step 120300 | Avg Loss: 0.0142 | Grad Norm: 0.00988977\n",
      "Epoch 1 | Step 120400 | Avg Loss: 0.0141 | Grad Norm: 0.00975336\n",
      "Epoch 1 | Step 120500 | Avg Loss: 0.0142 | Grad Norm: 0.01088113\n",
      "Epoch 1 | Step 120600 | Avg Loss: 0.0139 | Grad Norm: 0.00960398\n",
      "Epoch 1 | Step 120700 | Avg Loss: 0.0140 | Grad Norm: 0.01042902\n",
      "Epoch 1 | Step 120800 | Avg Loss: 0.0138 | Grad Norm: 0.01028279\n",
      "Epoch 1 | Step 120900 | Avg Loss: 0.0138 | Grad Norm: 0.00886287\n",
      "Epoch 1 | Step 121000 | Avg Loss: 0.0136 | Grad Norm: 0.01048011\n",
      "Epoch 1 | Step 121100 | Avg Loss: 0.0138 | Grad Norm: 0.01108641\n",
      "Epoch 1 | Step 121200 | Avg Loss: 0.0139 | Grad Norm: 0.01033805\n",
      "Epoch 1 | Step 121300 | Avg Loss: 0.0140 | Grad Norm: 0.01122796\n",
      "Epoch 1 | Step 121400 | Avg Loss: 0.0140 | Grad Norm: 0.01166898\n",
      "Epoch 1 | Step 121500 | Avg Loss: 0.0143 | Grad Norm: 0.01056925\n",
      "Epoch 1 | Step 121600 | Avg Loss: 0.0142 | Grad Norm: 0.01127944\n",
      "Epoch 1 | Step 121700 | Avg Loss: 0.0143 | Grad Norm: 0.01124122\n",
      "Epoch 1 | Step 121800 | Avg Loss: 0.0141 | Grad Norm: 0.00977686\n",
      "Epoch 1 | Step 121900 | Avg Loss: 0.0139 | Grad Norm: 0.01100320\n",
      "Epoch 1 | Step 122000 | Avg Loss: 0.0141 | Grad Norm: 0.01162536\n",
      "Epoch 1 | Step 122100 | Avg Loss: 0.0142 | Grad Norm: 0.01299403\n",
      "Epoch 1 | Step 122200 | Avg Loss: 0.0139 | Grad Norm: 0.01221008\n",
      "Epoch 1 | Step 122300 | Avg Loss: 0.0139 | Grad Norm: 0.00973098\n",
      "Epoch 1 | Step 122400 | Avg Loss: 0.0138 | Grad Norm: 0.01146318\n",
      "Epoch 1 | Step 122500 | Avg Loss: 0.0138 | Grad Norm: 0.00959220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fbb54791f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fbb54791f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fbb54791f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fbb54791f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#criterion = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#for _ in range(1000000):\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get data from the dataloader\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/nnue_training_split_model_200M\")\n",
    "\n",
    "# hyperparameters\n",
    "nr_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "scaling_factor = 400\n",
    "ground_truth_scaling_factor = 400\n",
    "lambda_ = 0.2\n",
    "log_interval = 100\n",
    "save_interval = 100000\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "epsilon = 1e-10\n",
    "\n",
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "\n",
    "        #for _ in range(1000000):\n",
    "    \n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y, result = batch\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        result = result.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = lambda_*result + (1 - lambda_) * torch.sigmoid(batch_y / ground_truth_scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        #loss = (wdl_batch_y * torch.log(wdl_batch_y + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_batch_y + epsilon)) -(wdl_batch_y * torch.log(wdl_pred   + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_pred   + epsilon))\n",
    "        #loss = loss.mean()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(wdl_pred, wdl_batch_y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # make a step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        # calculate the gradient norm\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm of this parameter's gradient\n",
    "                total_norm_sq += param_norm.item() ** 2\n",
    "        total_grad_norm = total_norm_sq ** 0.5\n",
    "        # Now total_grad_norm is the L2 norm of all gradients combined.\n",
    "        #print(f\"Step {step}  Grad Norm = {total_grad_norm:.8f}\")\n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Avg Loss: {avg_loss:.4f} | Grad Norm: {total_grad_norm:.8f}\")\n",
    "            running_loss = 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "            writer.add_scalar(\"Grad Norm\", total_grad_norm, step)\n",
    "            writer.add_scalar(\"WDL Pred\", torch.mean(wdl_pred).item(), step)\n",
    "            writer.add_scalar(\"WDL BatchY\", torch.mean(wdl_batch_y).item(), step)\n",
    "            writer.add_scalar(\"Pred\", torch.median(pred).item(), step)\n",
    "            writer.add_scalar(\"BatchY\", torch.median(batch_y).item(), step)\n",
    "\n",
    "\n",
    "            # log separate grad norms\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    writer.add_scalar(f'GradNorm/{name}', grad_norm, step)\n",
    "\n",
    "        # Save the model every `save_interval` steps\n",
    "        if step % save_interval == 0:\n",
    "            model_name = 'saved_models/split_model_200M_' + str(step) + \".pth\"\n",
    "            print(\"Saving model at step\" + str(step))\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,}, model_name)\n",
    "            \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d09a",
   "metadata": {},
   "source": [
    "### Postprocessing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def save_layer(layer, name):\n",
    "    w = layer.weight.detach().numpy()\n",
    "    b = layer.bias.detach().numpy()\n",
    "    with open(f\"{name}_weights.txt\", \"w\") as f:\n",
    "        for row in w:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    with open(f\"{name}_biases.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(map(str, b)))\n",
    "\n",
    "save_layer(model.fc1, \"model/layer1\")\n",
    "save_layer(model.fc2, \"model/layer2\")\n",
    "save_layer(model.fc3, \"model/layer3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "fen2 = 'rnbqkbnr/pppppppp/8/8/8/7N/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "fen3 = 'rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "\n",
    "start_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "\n",
    "#torch.tensor(FEN_to_input(fen1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1, b1, stm1 = FEN_to_inputs(fen1)\n",
    "    w1 = w1.unsqueeze(0)\n",
    "    b1 = b1.unsqueeze(0)\n",
    "    stm1 = stm1.unsqueeze(0)\n",
    "    print(w1.shape, b1.shape, stm1.shape)\n",
    "    #in_1 = np.argwhere(input1.numpy() == 1)\n",
    "    #print(sum(input1.numpy()))\n",
    "    w2, b2, stm2 = FEN_to_inputs(fen2)\n",
    "    w2 = w2.unsqueeze(0)\n",
    "    b2 = b2.unsqueeze(0)\n",
    "    stm2 = stm2.unsqueeze(0)\n",
    "    #print(np.argwhere(input2.numpy() == 1))\n",
    "    w3, b3, stm3 = FEN_to_inputs(fen3)\n",
    "    w3 = w3.unsqueeze(0)\n",
    "    b3 = b3.unsqueeze(0)\n",
    "    stm3 = stm3.unsqueeze(0)\n",
    "    #print(np.argwhere(input3.numpy() == 1))\n",
    "\n",
    "    #in_start = np.argwhere(FEN_to_inputs(start_fen).numpy() == 1)\n",
    "\n",
    "    pred1 = model(w1, b1, stm1)\n",
    "    pred2 = model(w2, b2, stm2)\n",
    "    pred3 = model(w3, b3, stm3)\n",
    "\n",
    "    print(pred1.item())\n",
    "    print(pred2.item())\n",
    "    print(pred3.item())\n",
    "\n",
    "    #accumulator = model.fc1(input1)\n",
    "    #ws, bs, stms = FEN_to_inputs(start_fen)\n",
    "    #ws = ws.unsqueeze(0)\n",
    "    #bs = bs.unsqueeze(0)\n",
    "    #stms = stms.unsqueeze(0)\n",
    "    w_accumulator = model.fc1(w1)\n",
    "    b_accumulator = model.fc1(b1)\n",
    "    #print(w_accumulator)\n",
    "    #print(b_accumulator)\n",
    "\n",
    "\n",
    "    cat_wb = torch.cat([w_accumulator, b_accumulator], dim=1)\n",
    "    cat_bw = torch.cat([b_accumulator, w_accumulator], dim=1)\n",
    "\n",
    "    #stm1 = stm1.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "    #print(cat_bw)\n",
    "\n",
    "    accumulator = (1 - stm1) * cat_wb + stm1 * cat_bw\n",
    "    #print(accumulator)\n",
    "\n",
    "    x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "    x = model.fc2(x)\n",
    "\n",
    "    #print(x)\n",
    "\n",
    "    print(model.fc2.bias.detach().numpy())\n",
    "    #print(model.fc2.weight[0][:10])\n",
    "\n",
    "    #print(\"weights[0][0]\")\n",
    "    #print(model.fc1.weight[0][0])\n",
    "    #print(\"weights[1][0]\")\n",
    "    #print(model.fc1.weight[1][0])\n",
    "    #print(model.fc1.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of bins\n",
    "num_bins = 64\n",
    "bin_edges = np.linspace(-32003, 32003, num_bins + 1)\n",
    "counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "filename = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/scores.txt'\n",
    "\n",
    "# Re-read the file and bin values\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            val = float(line.strip())\n",
    "            # Determine bin index\n",
    "            bin_idx = np.searchsorted(bin_edges, val, side='right') - 1\n",
    "            if 0 <= bin_idx < num_bins:\n",
    "                counts[bin_idx] += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Find empty bins\n",
    "empty_bins = []\n",
    "for i, count in enumerate(counts):\n",
    "    if count == 0:\n",
    "        left_edge = bin_edges[i]\n",
    "        right_edge = bin_edges[i + 1]\n",
    "        empty_bins.append((i, left_edge, right_edge))\n",
    "\n",
    "# Print empty bin ranges\n",
    "print(\"Empty bins:\")\n",
    "for i, left, right in empty_bins:\n",
    "    print(f\"Bin {i}: [{left}, {right})\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge')\n",
    "plt.title(\"Histogram (streamed)\")\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP to WDL conversion\n",
    "scaling_factor = 400\n",
    "score = torch.tensor(32000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(-1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(0, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41259",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_2 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 13, 14, 15, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "\n",
    "print(np.sort(in_start.reshape(1, 32)))\n",
    "print(np.sort(in_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481ab8",
   "metadata": {},
   "source": [
    "### HalfKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "\n",
    "def FEN_to_HalfKP(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    position = 0\n",
    "    white_king_position = 0\n",
    "    black_king_position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            elif char == 'K':\n",
    "                white_king_position = position\n",
    "                position += 1\n",
    "            elif char == 'k':\n",
    "                black_king_position = position\n",
    "                position += 1\n",
    "            else:\n",
    "                position += 1\n",
    "\n",
    "    white_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    black_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                if (char != 'K') & (char != 'k'):\n",
    "                    piece_index = (piece_dict[char] % 6) * 2 + (piece_dict[char] > 5)\n",
    "                    white_input_layer[position + (piece_index + white_king_position*10)*64] = 1\n",
    "                    black_input_layer[position + (piece_index + black_king_position*10)*64] = 1\n",
    "                    position += 1\n",
    "                else:\n",
    "                    position += 1\n",
    "\n",
    "    return torch.tensor(white_input_layer, dtype=torch.float32), torch.tensor(black_input_layer, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6378bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class HalfKP_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_HalfKP(row[0].strip())\n",
    "                score = float(row[1].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                buf.append((w_in, b_in, stm, score))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                yield w_in, b_in, stm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_FEATURES = 40960\n",
    "M = 1024\n",
    "N = 32\n",
    "K = 1\n",
    "\n",
    "class HalfKPNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HalfKPNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, M)\n",
    "        self.fc2 = nn.Linear(2*M, N)\n",
    "        self.fc3 = nn.Linear(N, K)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)  # [B, 2*M]\n",
    "        cat_bw = torch.cat([b, w], dim=1)  # [B, 2*M]\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = stm * cat_wb + (1 - stm) * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0.0, max = 1.0)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data.csv'\n",
    "dataset = HalfKP_Dataset(csv_path, shuffle_buffer=1000)\n",
    "loader = DataLoader(dataset, batch_size = 128, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/halfKP\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nr_epochs = 500\n",
    "model = HalfKPNNUE().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.05)\n",
    "total_size = 200000000\n",
    "batch_size = 128\n",
    "steps_per_epoch = total_size // batch_size\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "MAE_loss = nn.L1Loss()\n",
    "lowest_MAE = 10000\n",
    "\n",
    "# Transform the CP scores to the WDL space\n",
    "scaling_factor = 400\n",
    "\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "log_interval = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "        #for _ in range(100000):\n",
    "\n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y = batch\n",
    "\n",
    "        # move data to GPU\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)  # remove the last dimension\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = torch.sigmoid(batch_y / scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        # calculate the MSE loss\n",
    "        loss = criterion(wdl_batch_y, wdl_pred)\n",
    "        MAE = MAE_loss(wdl_batch_y, wdl_pred)\n",
    "        running_loss += loss.item()\n",
    "        running_mae += MAE\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            avg_mae = running_mae / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{steps_per_epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"MAE/train\", avg_mae, step)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "\n",
    "        # calculate MAE\n",
    "        if MAE < 0.0002:\n",
    "            lowest_MAE = MAE\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with MAE: {lowest_MAE.item():.4f}, loss: {loss.item():.4f}\")\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, MAE: {MAE.item():.4f}, lowest MAE: {lowest_MAE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def FEN_to_input(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                input_layer[position + piece_dict[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        #self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.clamp(self.fc1(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc3(x), min = 0, max = 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                fen, score, result = FEN_to_input(row[0].strip()), float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield fen, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for fen, score, result in stream:\n",
    "                buf.append((fen, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for fen, score, result in stream:\n",
    "                yield fen, score, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
