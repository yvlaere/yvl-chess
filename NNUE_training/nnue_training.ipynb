{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5d7453",
   "metadata": {},
   "source": [
    "# NNUE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821225",
   "metadata": {},
   "source": [
    "Great source on NNUE: https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead68bd",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Stockfish has a lot of data available for NNUE training in the .binpack format. They have a repo for training NNUEs (nnue-pytorch) that enables efficient dataloading with this format. I don't want to use nnue-pytorch, i want to make my own NNUE training setup.\n",
    "\n",
    "The nnue-pytorch repo also has information on training datasets for NNUEs: https://github.com/official-stockfish/nnue-pytorch/wiki/Training-datasets. They explain how to make your own dataset and link some of the datasets they generated. I will use some of this data, because generating the data myself would be too time-consuming on my hardware.\n",
    "\n",
    "Currently using training data: test80-2024-01-jan-2tb7p.min-v2.v6.binpack.zst from https://huggingface.co/datasets/linrock/test80-2024/tree/main\n",
    "\n",
    "This file contains billions of positions with evaluations in the .binpack format. The stockfish tools branch has a tool to covert the .binpack data into .plain data (https://github.com/official-stockfish/Stockfish/blob/tools/docs/convert.md). I used this tool and stored the first 200M evaluated positions.\n",
    "\n",
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1c61",
   "metadata": {},
   "source": [
    "### Turn FEN into input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict_w = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "piece_dict_b = {'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K':11, 'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "def FEN_to_inputs(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer_w = np.zeros(768, dtype = np.float32)\n",
    "    input_layer_b = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                alt_pos = 63 - (position ^ 7)\n",
    "                input_layer_w[position + piece_dict_w[char]*64] = 1\n",
    "                input_layer_b[alt_pos + piece_dict_b[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer_w, dtype=torch.float32), torch.tensor(input_layer_b, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1938bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Black Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Side to Move: tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051/2390657366.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(w_features)))\n",
      "/tmp/ipykernel_1051/2390657366.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(b_features)))\n"
     ]
    }
   ],
   "source": [
    "# testing encoding\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "\n",
    "w_features, b_features, stm = FEN_to_inputs(fen1)\n",
    "print(\"White Features:\", sum(w_features))\n",
    "print(np.nonzero(np.array(w_features)))\n",
    "print(\"Black Features:\", sum(b_features))\n",
    "print(np.nonzero(np.array(b_features)))\n",
    "print(\"Side to Move:\", stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cdf67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test1 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 14, 15, 21, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "test2 = [ 8, 9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192, 199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510, 570, 573, 632, 639, 699, 764]\n",
    "\n",
    "np.sort(test1)\n",
    "np.sort(test2)\n",
    "print(np.array_equal(np.sort(test1), np.sort(test2)))\n",
    "\n",
    "test3 = [  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192, 199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,  570, 573, 632, 639, 699, 764]\n",
    "test4 = [632, 505, 570, 699, 764, 573, 510, 639, 432, 433, 434, 435, 436, 438, 439, 429, 8, 9, 10, 11, 12, 13, 14, 15, 192, 65, 130, 259, 324, 133, 70, 199]\n",
    "print(np.array_equal(np.sort(test3), np.sort(test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c0263",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Input: a sparse, binary array of length 768. Each element of the array represents a possible combination of piece type (6), piece_color (2) and position (64) (6*2*64 = 768).\n",
    "\n",
    "This is a very simple input feature (P feature set) set that will be improved upon later (HalfKP).\n",
    "\n",
    "The fully connected feedfoward network has 4 hidden layers: 768 -> 1024, 1024 -> 8, 8 -> 32 and 32 -> 1.\n",
    "\n",
    "The output is a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Split_NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split_NNUE, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)\n",
    "        cat_bw = torch.cat([b, w], dim=1)\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = (1 - stm) * cat_wb + stm * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Split_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_inputs(row[0].strip())\n",
    "                score, result = float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                buf.append((w_in, b_in, stm, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                yield w_in, b_in, stm, score, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data_full.csv'\n",
    "dataset = Custom_Split_Dataset(csv_path, shuffle_buffer = 100000)\n",
    "loader = DataLoader(dataset, batch_size = 1024, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa7cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Kaiming uniform for piecewise-linear (ReLU-like) activations:\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0.0, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | Avg Loss: 0.0158 | Grad Norm: 0.00754837\n",
      "Epoch 1 | Step 200 | Avg Loss: 0.0155 | Grad Norm: 0.00764584\n",
      "Epoch 1 | Step 300 | Avg Loss: 0.0155 | Grad Norm: 0.00898548\n",
      "Epoch 1 | Step 400 | Avg Loss: 0.0156 | Grad Norm: 0.01081765\n",
      "Epoch 1 | Step 500 | Avg Loss: 0.0155 | Grad Norm: 0.00929531\n",
      "Epoch 1 | Step 600 | Avg Loss: 0.0157 | Grad Norm: 0.00955345\n",
      "Epoch 1 | Step 700 | Avg Loss: 0.0162 | Grad Norm: 0.00982713\n",
      "Epoch 1 | Step 800 | Avg Loss: 0.0163 | Grad Norm: 0.00804034\n",
      "Epoch 1 | Step 900 | Avg Loss: 0.0165 | Grad Norm: 0.00868121\n",
      "Epoch 1 | Step 1000 | Avg Loss: 0.0162 | Grad Norm: 0.00768275\n",
      "Epoch 1 | Step 1100 | Avg Loss: 0.0159 | Grad Norm: 0.00857947\n",
      "Epoch 1 | Step 1200 | Avg Loss: 0.0158 | Grad Norm: 0.00936676\n",
      "Epoch 1 | Step 1300 | Avg Loss: 0.0157 | Grad Norm: 0.00835045\n",
      "Epoch 1 | Step 1400 | Avg Loss: 0.0155 | Grad Norm: 0.01060518\n",
      "Epoch 1 | Step 1500 | Avg Loss: 0.0151 | Grad Norm: 0.00991394\n",
      "Epoch 1 | Step 1600 | Avg Loss: 0.0151 | Grad Norm: 0.00774505\n",
      "Epoch 1 | Step 1700 | Avg Loss: 0.0151 | Grad Norm: 0.00882798\n",
      "Epoch 1 | Step 1800 | Avg Loss: 0.0155 | Grad Norm: 0.00819961\n",
      "Epoch 1 | Step 1900 | Avg Loss: 0.0155 | Grad Norm: 0.00821493\n",
      "Epoch 1 | Step 2000 | Avg Loss: 0.0157 | Grad Norm: 0.00799139\n",
      "Epoch 1 | Step 2100 | Avg Loss: 0.0156 | Grad Norm: 0.00827516\n",
      "Epoch 1 | Step 2200 | Avg Loss: 0.0156 | Grad Norm: 0.00828191\n",
      "Epoch 1 | Step 2300 | Avg Loss: 0.0159 | Grad Norm: 0.00774164\n",
      "Epoch 1 | Step 2400 | Avg Loss: 0.0157 | Grad Norm: 0.00826730\n",
      "Epoch 1 | Step 2500 | Avg Loss: 0.0161 | Grad Norm: 0.00803569\n",
      "Epoch 1 | Step 2600 | Avg Loss: 0.0160 | Grad Norm: 0.00928526\n",
      "Epoch 1 | Step 2700 | Avg Loss: 0.0163 | Grad Norm: 0.00855579\n",
      "Epoch 1 | Step 2800 | Avg Loss: 0.0161 | Grad Norm: 0.01024861\n",
      "Epoch 1 | Step 2900 | Avg Loss: 0.0158 | Grad Norm: 0.00817276\n",
      "Epoch 1 | Step 3000 | Avg Loss: 0.0160 | Grad Norm: 0.00966301\n",
      "Epoch 1 | Step 3100 | Avg Loss: 0.0157 | Grad Norm: 0.00882067\n",
      "Epoch 1 | Step 3200 | Avg Loss: 0.0152 | Grad Norm: 0.00828751\n",
      "Epoch 1 | Step 3300 | Avg Loss: 0.0155 | Grad Norm: 0.01123685\n",
      "Epoch 1 | Step 3400 | Avg Loss: 0.0155 | Grad Norm: 0.00952436\n",
      "Epoch 1 | Step 3500 | Avg Loss: 0.0154 | Grad Norm: 0.00964498\n",
      "Epoch 1 | Step 3600 | Avg Loss: 0.0153 | Grad Norm: 0.00871789\n",
      "Epoch 1 | Step 3700 | Avg Loss: 0.0158 | Grad Norm: 0.00830468\n",
      "Epoch 1 | Step 3800 | Avg Loss: 0.0159 | Grad Norm: 0.00897344\n",
      "Epoch 1 | Step 3900 | Avg Loss: 0.0157 | Grad Norm: 0.00887128\n",
      "Epoch 1 | Step 4000 | Avg Loss: 0.0156 | Grad Norm: 0.00886964\n",
      "Epoch 1 | Step 4100 | Avg Loss: 0.0152 | Grad Norm: 0.00890275\n",
      "Epoch 1 | Step 4200 | Avg Loss: 0.0152 | Grad Norm: 0.00857646\n",
      "Epoch 1 | Step 4300 | Avg Loss: 0.0152 | Grad Norm: 0.00872100\n",
      "Epoch 1 | Step 4400 | Avg Loss: 0.0156 | Grad Norm: 0.00867647\n",
      "Epoch 1 | Step 4500 | Avg Loss: 0.0155 | Grad Norm: 0.00872085\n",
      "Epoch 1 | Step 4600 | Avg Loss: 0.0150 | Grad Norm: 0.00856324\n",
      "Epoch 1 | Step 4700 | Avg Loss: 0.0150 | Grad Norm: 0.00900241\n",
      "Epoch 1 | Step 4800 | Avg Loss: 0.0153 | Grad Norm: 0.00851228\n",
      "Epoch 1 | Step 4900 | Avg Loss: 0.0153 | Grad Norm: 0.00890825\n",
      "Epoch 1 | Step 5000 | Avg Loss: 0.0156 | Grad Norm: 0.00930823\n",
      "Epoch 1 | Step 5100 | Avg Loss: 0.0156 | Grad Norm: 0.00852141\n",
      "Epoch 1 | Step 5200 | Avg Loss: 0.0152 | Grad Norm: 0.00719757\n",
      "Epoch 1 | Step 5300 | Avg Loss: 0.0154 | Grad Norm: 0.01061558\n",
      "Epoch 1 | Step 5400 | Avg Loss: 0.0160 | Grad Norm: 0.00885549\n",
      "Epoch 1 | Step 5500 | Avg Loss: 0.0161 | Grad Norm: 0.00979028\n",
      "Epoch 1 | Step 5600 | Avg Loss: 0.0161 | Grad Norm: 0.00781548\n",
      "Epoch 1 | Step 5700 | Avg Loss: 0.0158 | Grad Norm: 0.00832815\n",
      "Epoch 1 | Step 5800 | Avg Loss: 0.0160 | Grad Norm: 0.00941766\n",
      "Epoch 1 | Step 5900 | Avg Loss: 0.0154 | Grad Norm: 0.00875255\n",
      "Epoch 1 | Step 6000 | Avg Loss: 0.0156 | Grad Norm: 0.00879455\n",
      "Epoch 1 | Step 6100 | Avg Loss: 0.0157 | Grad Norm: 0.01052275\n",
      "Epoch 1 | Step 6200 | Avg Loss: 0.0156 | Grad Norm: 0.00913383\n",
      "Epoch 1 | Step 6300 | Avg Loss: 0.0158 | Grad Norm: 0.00902274\n",
      "Epoch 1 | Step 6400 | Avg Loss: 0.0161 | Grad Norm: 0.00871795\n",
      "Epoch 1 | Step 6500 | Avg Loss: 0.0159 | Grad Norm: 0.00973827\n",
      "Epoch 1 | Step 6600 | Avg Loss: 0.0160 | Grad Norm: 0.00922714\n",
      "Epoch 1 | Step 6700 | Avg Loss: 0.0160 | Grad Norm: 0.00839531\n",
      "Epoch 1 | Step 6800 | Avg Loss: 0.0158 | Grad Norm: 0.00762345\n",
      "Epoch 1 | Step 6900 | Avg Loss: 0.0158 | Grad Norm: 0.00834177\n",
      "Epoch 1 | Step 7000 | Avg Loss: 0.0157 | Grad Norm: 0.00835474\n",
      "Epoch 1 | Step 7100 | Avg Loss: 0.0154 | Grad Norm: 0.00892209\n",
      "Epoch 1 | Step 7200 | Avg Loss: 0.0152 | Grad Norm: 0.01128343\n",
      "Epoch 1 | Step 7300 | Avg Loss: 0.0153 | Grad Norm: 0.00764733\n",
      "Epoch 1 | Step 7400 | Avg Loss: 0.0154 | Grad Norm: 0.00824871\n",
      "Epoch 1 | Step 7500 | Avg Loss: 0.0154 | Grad Norm: 0.01057535\n",
      "Epoch 1 | Step 7600 | Avg Loss: 0.0157 | Grad Norm: 0.00928270\n",
      "Epoch 1 | Step 7700 | Avg Loss: 0.0156 | Grad Norm: 0.00913889\n",
      "Epoch 1 | Step 7800 | Avg Loss: 0.0155 | Grad Norm: 0.00738720\n",
      "Epoch 1 | Step 7900 | Avg Loss: 0.0157 | Grad Norm: 0.00940313\n",
      "Epoch 1 | Step 8000 | Avg Loss: 0.0159 | Grad Norm: 0.00854505\n",
      "Epoch 1 | Step 8100 | Avg Loss: 0.0158 | Grad Norm: 0.00827400\n",
      "Epoch 1 | Step 8200 | Avg Loss: 0.0156 | Grad Norm: 0.00986143\n",
      "Epoch 1 | Step 8300 | Avg Loss: 0.0158 | Grad Norm: 0.00783168\n",
      "Epoch 1 | Step 8400 | Avg Loss: 0.0157 | Grad Norm: 0.00783318\n",
      "Epoch 1 | Step 8500 | Avg Loss: 0.0158 | Grad Norm: 0.00921504\n",
      "Epoch 1 | Step 8600 | Avg Loss: 0.0160 | Grad Norm: 0.00854437\n",
      "Epoch 1 | Step 8700 | Avg Loss: 0.0162 | Grad Norm: 0.00875230\n",
      "Epoch 1 | Step 8800 | Avg Loss: 0.0159 | Grad Norm: 0.00747075\n",
      "Epoch 1 | Step 8900 | Avg Loss: 0.0160 | Grad Norm: 0.00751885\n",
      "Epoch 1 | Step 9000 | Avg Loss: 0.0160 | Grad Norm: 0.00810542\n",
      "Epoch 1 | Step 9100 | Avg Loss: 0.0156 | Grad Norm: 0.00890355\n",
      "Epoch 1 | Step 9200 | Avg Loss: 0.0156 | Grad Norm: 0.00847541\n",
      "Epoch 1 | Step 9300 | Avg Loss: 0.0157 | Grad Norm: 0.00812785\n",
      "Epoch 1 | Step 9400 | Avg Loss: 0.0153 | Grad Norm: 0.00865446\n",
      "Epoch 1 | Step 9500 | Avg Loss: 0.0156 | Grad Norm: 0.00773178\n",
      "Epoch 1 | Step 9600 | Avg Loss: 0.0158 | Grad Norm: 0.00896615\n",
      "Epoch 1 | Step 9700 | Avg Loss: 0.0158 | Grad Norm: 0.00813288\n",
      "Epoch 1 | Step 9800 | Avg Loss: 0.0158 | Grad Norm: 0.00967141\n",
      "Epoch 1 | Step 9900 | Avg Loss: 0.0159 | Grad Norm: 0.00941480\n",
      "Epoch 1 | Step 10000 | Avg Loss: 0.0159 | Grad Norm: 0.00894480\n",
      "Epoch 1 | Step 10100 | Avg Loss: 0.0162 | Grad Norm: 0.00946958\n",
      "Epoch 1 | Step 10200 | Avg Loss: 0.0161 | Grad Norm: 0.00892993\n",
      "Epoch 1 | Step 10300 | Avg Loss: 0.0160 | Grad Norm: 0.00905564\n",
      "Epoch 1 | Step 10400 | Avg Loss: 0.0161 | Grad Norm: 0.00968024\n",
      "Epoch 1 | Step 10500 | Avg Loss: 0.0162 | Grad Norm: 0.00965946\n",
      "Epoch 1 | Step 10600 | Avg Loss: 0.0165 | Grad Norm: 0.00870206\n",
      "Epoch 1 | Step 10700 | Avg Loss: 0.0166 | Grad Norm: 0.00972586\n",
      "Epoch 1 | Step 10800 | Avg Loss: 0.0166 | Grad Norm: 0.00880897\n",
      "Epoch 1 | Step 10900 | Avg Loss: 0.0160 | Grad Norm: 0.01108370\n",
      "Epoch 1 | Step 11000 | Avg Loss: 0.0159 | Grad Norm: 0.00821680\n",
      "Epoch 1 | Step 11100 | Avg Loss: 0.0163 | Grad Norm: 0.01011478\n",
      "Epoch 1 | Step 11200 | Avg Loss: 0.0160 | Grad Norm: 0.01025333\n",
      "Epoch 1 | Step 11300 | Avg Loss: 0.0163 | Grad Norm: 0.00815662\n",
      "Epoch 1 | Step 11400 | Avg Loss: 0.0158 | Grad Norm: 0.00898548\n",
      "Epoch 1 | Step 11500 | Avg Loss: 0.0161 | Grad Norm: 0.00914273\n",
      "Epoch 1 | Step 11600 | Avg Loss: 0.0161 | Grad Norm: 0.00998709\n",
      "Epoch 1 | Step 11700 | Avg Loss: 0.0156 | Grad Norm: 0.00805665\n",
      "Epoch 1 | Step 11800 | Avg Loss: 0.0155 | Grad Norm: 0.00925602\n",
      "Epoch 1 | Step 11900 | Avg Loss: 0.0154 | Grad Norm: 0.01025185\n",
      "Epoch 1 | Step 12000 | Avg Loss: 0.0153 | Grad Norm: 0.00900919\n",
      "Epoch 1 | Step 12100 | Avg Loss: 0.0153 | Grad Norm: 0.01189656\n",
      "Epoch 1 | Step 12200 | Avg Loss: 0.0155 | Grad Norm: 0.00850207\n",
      "Epoch 1 | Step 12300 | Avg Loss: 0.0154 | Grad Norm: 0.01053396\n",
      "Epoch 1 | Step 12400 | Avg Loss: 0.0158 | Grad Norm: 0.00865234\n",
      "Epoch 1 | Step 12500 | Avg Loss: 0.0162 | Grad Norm: 0.00932564\n",
      "Epoch 1 | Step 12600 | Avg Loss: 0.0162 | Grad Norm: 0.00898484\n",
      "Epoch 1 | Step 12700 | Avg Loss: 0.0156 | Grad Norm: 0.00949075\n",
      "Epoch 1 | Step 12800 | Avg Loss: 0.0155 | Grad Norm: 0.00896030\n",
      "Epoch 1 | Step 12900 | Avg Loss: 0.0159 | Grad Norm: 0.00844116\n",
      "Epoch 1 | Step 13000 | Avg Loss: 0.0160 | Grad Norm: 0.00920363\n",
      "Epoch 1 | Step 13100 | Avg Loss: 0.0158 | Grad Norm: 0.00931005\n",
      "Epoch 1 | Step 13200 | Avg Loss: 0.0157 | Grad Norm: 0.00865218\n",
      "Epoch 1 | Step 13300 | Avg Loss: 0.0158 | Grad Norm: 0.00937385\n",
      "Epoch 1 | Step 13400 | Avg Loss: 0.0156 | Grad Norm: 0.00878354\n",
      "Epoch 1 | Step 13500 | Avg Loss: 0.0158 | Grad Norm: 0.00845246\n",
      "Epoch 1 | Step 13600 | Avg Loss: 0.0155 | Grad Norm: 0.00917078\n",
      "Epoch 1 | Step 13700 | Avg Loss: 0.0154 | Grad Norm: 0.00794473\n",
      "Epoch 1 | Step 13800 | Avg Loss: 0.0155 | Grad Norm: 0.00862438\n",
      "Epoch 1 | Step 13900 | Avg Loss: 0.0159 | Grad Norm: 0.00816286\n",
      "Epoch 1 | Step 14000 | Avg Loss: 0.0159 | Grad Norm: 0.00778980\n",
      "Epoch 1 | Step 14100 | Avg Loss: 0.0155 | Grad Norm: 0.01122735\n",
      "Epoch 1 | Step 14200 | Avg Loss: 0.0156 | Grad Norm: 0.00810280\n",
      "Epoch 1 | Step 14300 | Avg Loss: 0.0155 | Grad Norm: 0.00812675\n",
      "Epoch 1 | Step 14400 | Avg Loss: 0.0155 | Grad Norm: 0.00977446\n",
      "Epoch 1 | Step 14500 | Avg Loss: 0.0158 | Grad Norm: 0.00976815\n",
      "Epoch 1 | Step 14600 | Avg Loss: 0.0160 | Grad Norm: 0.00955459\n",
      "Epoch 1 | Step 14700 | Avg Loss: 0.0160 | Grad Norm: 0.00908616\n",
      "Epoch 1 | Step 14800 | Avg Loss: 0.0158 | Grad Norm: 0.00806181\n",
      "Epoch 1 | Step 14900 | Avg Loss: 0.0158 | Grad Norm: 0.00922260\n",
      "Epoch 1 | Step 15000 | Avg Loss: 0.0155 | Grad Norm: 0.01194557\n",
      "Epoch 1 | Step 15100 | Avg Loss: 0.0157 | Grad Norm: 0.00802473\n",
      "Epoch 1 | Step 15200 | Avg Loss: 0.0160 | Grad Norm: 0.01079513\n",
      "Epoch 1 | Step 15300 | Avg Loss: 0.0156 | Grad Norm: 0.00981945\n",
      "Epoch 1 | Step 15400 | Avg Loss: 0.0158 | Grad Norm: 0.00991171\n",
      "Epoch 1 | Step 15500 | Avg Loss: 0.0158 | Grad Norm: 0.00825981\n",
      "Epoch 1 | Step 15600 | Avg Loss: 0.0160 | Grad Norm: 0.00772245\n",
      "Epoch 1 | Step 15700 | Avg Loss: 0.0162 | Grad Norm: 0.01093664\n",
      "Epoch 1 | Step 15800 | Avg Loss: 0.0166 | Grad Norm: 0.00815682\n",
      "Epoch 1 | Step 15900 | Avg Loss: 0.0171 | Grad Norm: 0.00904992\n",
      "Epoch 1 | Step 16000 | Avg Loss: 0.0166 | Grad Norm: 0.00888180\n",
      "Epoch 1 | Step 16100 | Avg Loss: 0.0168 | Grad Norm: 0.00748326\n",
      "Epoch 1 | Step 16200 | Avg Loss: 0.0167 | Grad Norm: 0.00899458\n",
      "Epoch 1 | Step 16300 | Avg Loss: 0.0165 | Grad Norm: 0.00935239\n",
      "Epoch 1 | Step 16400 | Avg Loss: 0.0164 | Grad Norm: 0.00883575\n",
      "Epoch 1 | Step 16500 | Avg Loss: 0.0164 | Grad Norm: 0.00865258\n",
      "Epoch 1 | Step 16600 | Avg Loss: 0.0158 | Grad Norm: 0.00948979\n",
      "Epoch 1 | Step 16700 | Avg Loss: 0.0155 | Grad Norm: 0.00943600\n",
      "Epoch 1 | Step 16800 | Avg Loss: 0.0158 | Grad Norm: 0.00816245\n",
      "Epoch 1 | Step 16900 | Avg Loss: 0.0161 | Grad Norm: 0.00959706\n",
      "Epoch 1 | Step 17000 | Avg Loss: 0.0161 | Grad Norm: 0.00906524\n",
      "Epoch 1 | Step 17100 | Avg Loss: 0.0160 | Grad Norm: 0.00865091\n",
      "Epoch 1 | Step 17200 | Avg Loss: 0.0158 | Grad Norm: 0.00737613\n",
      "Epoch 1 | Step 17300 | Avg Loss: 0.0158 | Grad Norm: 0.00859232\n",
      "Epoch 1 | Step 17400 | Avg Loss: 0.0159 | Grad Norm: 0.00916393\n",
      "Epoch 1 | Step 17500 | Avg Loss: 0.0155 | Grad Norm: 0.00867845\n",
      "Epoch 1 | Step 17600 | Avg Loss: 0.0155 | Grad Norm: 0.01086223\n",
      "Epoch 1 | Step 17700 | Avg Loss: 0.0154 | Grad Norm: 0.00931264\n",
      "Epoch 1 | Step 17800 | Avg Loss: 0.0157 | Grad Norm: 0.00845003\n",
      "Epoch 1 | Step 17900 | Avg Loss: 0.0157 | Grad Norm: 0.00933782\n",
      "Epoch 1 | Step 18000 | Avg Loss: 0.0155 | Grad Norm: 0.00779400\n",
      "Epoch 1 | Step 18100 | Avg Loss: 0.0153 | Grad Norm: 0.00904252\n",
      "Epoch 1 | Step 18200 | Avg Loss: 0.0156 | Grad Norm: 0.00896766\n",
      "Epoch 1 | Step 18300 | Avg Loss: 0.0157 | Grad Norm: 0.00894210\n",
      "Epoch 1 | Step 18400 | Avg Loss: 0.0155 | Grad Norm: 0.00872330\n",
      "Epoch 1 | Step 18500 | Avg Loss: 0.0158 | Grad Norm: 0.01008859\n",
      "Epoch 1 | Step 18600 | Avg Loss: 0.0160 | Grad Norm: 0.00870823\n",
      "Epoch 1 | Step 18700 | Avg Loss: 0.0159 | Grad Norm: 0.00861857\n",
      "Epoch 1 | Step 18800 | Avg Loss: 0.0161 | Grad Norm: 0.00846951\n",
      "Epoch 1 | Step 18900 | Avg Loss: 0.0165 | Grad Norm: 0.00914616\n",
      "Epoch 1 | Step 19000 | Avg Loss: 0.0165 | Grad Norm: 0.00906568\n",
      "Epoch 1 | Step 19100 | Avg Loss: 0.0165 | Grad Norm: 0.00988184\n",
      "Epoch 1 | Step 19200 | Avg Loss: 0.0165 | Grad Norm: 0.00795287\n",
      "Epoch 1 | Step 19300 | Avg Loss: 0.0162 | Grad Norm: 0.00866263\n",
      "Epoch 1 | Step 19400 | Avg Loss: 0.0158 | Grad Norm: 0.00808997\n",
      "Epoch 1 | Step 19500 | Avg Loss: 0.0157 | Grad Norm: 0.00778750\n",
      "Epoch 1 | Step 19600 | Avg Loss: 0.0155 | Grad Norm: 0.00737945\n",
      "Epoch 1 | Step 19700 | Avg Loss: 0.0156 | Grad Norm: 0.00727728\n",
      "Epoch 1 | Step 19800 | Avg Loss: 0.0155 | Grad Norm: 0.00903599\n",
      "Epoch 1 | Step 19900 | Avg Loss: 0.0154 | Grad Norm: 0.01107872\n",
      "Epoch 1 | Step 20000 | Avg Loss: 0.0154 | Grad Norm: 0.00899247\n",
      "Epoch 1 | Step 20100 | Avg Loss: 0.0157 | Grad Norm: 0.00945437\n",
      "Epoch 1 | Step 20200 | Avg Loss: 0.0154 | Grad Norm: 0.00806630\n",
      "Epoch 1 | Step 20300 | Avg Loss: 0.0157 | Grad Norm: 0.00848075\n",
      "Epoch 1 | Step 20400 | Avg Loss: 0.0159 | Grad Norm: 0.00716311\n",
      "Epoch 1 | Step 20500 | Avg Loss: 0.0160 | Grad Norm: 0.00879016\n",
      "Epoch 1 | Step 20600 | Avg Loss: 0.0161 | Grad Norm: 0.00895876\n",
      "Epoch 1 | Step 20700 | Avg Loss: 0.0158 | Grad Norm: 0.00837580\n",
      "Epoch 1 | Step 20800 | Avg Loss: 0.0156 | Grad Norm: 0.00874937\n",
      "Epoch 1 | Step 20900 | Avg Loss: 0.0155 | Grad Norm: 0.00773971\n",
      "Epoch 1 | Step 21000 | Avg Loss: 0.0156 | Grad Norm: 0.00828005\n",
      "Epoch 1 | Step 21100 | Avg Loss: 0.0158 | Grad Norm: 0.00733857\n",
      "Epoch 1 | Step 21200 | Avg Loss: 0.0157 | Grad Norm: 0.00886504\n",
      "Epoch 1 | Step 21300 | Avg Loss: 0.0158 | Grad Norm: 0.00910080\n",
      "Epoch 1 | Step 21400 | Avg Loss: 0.0157 | Grad Norm: 0.00990079\n",
      "Epoch 1 | Step 21500 | Avg Loss: 0.0161 | Grad Norm: 0.01110645\n",
      "Epoch 1 | Step 21600 | Avg Loss: 0.0161 | Grad Norm: 0.00950777\n",
      "Epoch 1 | Step 21700 | Avg Loss: 0.0162 | Grad Norm: 0.00905461\n",
      "Epoch 1 | Step 21800 | Avg Loss: 0.0165 | Grad Norm: 0.00980295\n",
      "Epoch 1 | Step 21900 | Avg Loss: 0.0161 | Grad Norm: 0.00912129\n",
      "Epoch 1 | Step 22000 | Avg Loss: 0.0161 | Grad Norm: 0.01286937\n",
      "Epoch 1 | Step 22100 | Avg Loss: 0.0160 | Grad Norm: 0.00938738\n",
      "Epoch 1 | Step 22200 | Avg Loss: 0.0156 | Grad Norm: 0.00846799\n",
      "Epoch 1 | Step 22300 | Avg Loss: 0.0158 | Grad Norm: 0.00842434\n",
      "Epoch 1 | Step 22400 | Avg Loss: 0.0159 | Grad Norm: 0.00787139\n",
      "Epoch 1 | Step 22500 | Avg Loss: 0.0157 | Grad Norm: 0.01019823\n",
      "Epoch 1 | Step 22600 | Avg Loss: 0.0158 | Grad Norm: 0.01147091\n",
      "Epoch 1 | Step 22700 | Avg Loss: 0.0158 | Grad Norm: 0.00876369\n",
      "Epoch 1 | Step 22800 | Avg Loss: 0.0160 | Grad Norm: 0.00789103\n",
      "Epoch 1 | Step 22900 | Avg Loss: 0.0161 | Grad Norm: 0.00927333\n",
      "Epoch 1 | Step 23000 | Avg Loss: 0.0159 | Grad Norm: 0.00779495\n",
      "Epoch 1 | Step 23100 | Avg Loss: 0.0158 | Grad Norm: 0.00893602\n",
      "Epoch 1 | Step 23200 | Avg Loss: 0.0156 | Grad Norm: 0.00823354\n",
      "Epoch 1 | Step 23300 | Avg Loss: 0.0158 | Grad Norm: 0.00842193\n",
      "Epoch 1 | Step 23400 | Avg Loss: 0.0159 | Grad Norm: 0.01013632\n",
      "Epoch 1 | Step 23500 | Avg Loss: 0.0161 | Grad Norm: 0.00837990\n",
      "Epoch 1 | Step 23600 | Avg Loss: 0.0163 | Grad Norm: 0.00838002\n",
      "Epoch 1 | Step 23700 | Avg Loss: 0.0163 | Grad Norm: 0.00842648\n",
      "Epoch 1 | Step 23800 | Avg Loss: 0.0163 | Grad Norm: 0.00849232\n",
      "Epoch 1 | Step 23900 | Avg Loss: 0.0161 | Grad Norm: 0.00835463\n",
      "Epoch 1 | Step 24000 | Avg Loss: 0.0161 | Grad Norm: 0.01019850\n",
      "Epoch 1 | Step 24100 | Avg Loss: 0.0160 | Grad Norm: 0.00755902\n",
      "Epoch 1 | Step 24200 | Avg Loss: 0.0161 | Grad Norm: 0.00833198\n",
      "Epoch 1 | Step 24300 | Avg Loss: 0.0159 | Grad Norm: 0.01003745\n",
      "Epoch 1 | Step 24400 | Avg Loss: 0.0156 | Grad Norm: 0.00976846\n",
      "Epoch 1 | Step 24500 | Avg Loss: 0.0155 | Grad Norm: 0.00964010\n",
      "Epoch 1 | Step 24600 | Avg Loss: 0.0159 | Grad Norm: 0.00928575\n",
      "Epoch 1 | Step 24700 | Avg Loss: 0.0162 | Grad Norm: 0.01003457\n",
      "Epoch 1 | Step 24800 | Avg Loss: 0.0162 | Grad Norm: 0.00833656\n",
      "Epoch 1 | Step 24900 | Avg Loss: 0.0161 | Grad Norm: 0.01291747\n",
      "Epoch 1 | Step 25000 | Avg Loss: 0.0161 | Grad Norm: 0.00754242\n",
      "Epoch 1 | Step 25100 | Avg Loss: 0.0161 | Grad Norm: 0.00854399\n",
      "Epoch 1 | Step 25200 | Avg Loss: 0.0163 | Grad Norm: 0.01049114\n",
      "Epoch 1 | Step 25300 | Avg Loss: 0.0166 | Grad Norm: 0.01006320\n",
      "Epoch 1 | Step 25400 | Avg Loss: 0.0164 | Grad Norm: 0.00948104\n",
      "Epoch 1 | Step 25500 | Avg Loss: 0.0164 | Grad Norm: 0.01272562\n",
      "Epoch 1 | Step 25600 | Avg Loss: 0.0165 | Grad Norm: 0.01002663\n",
      "Epoch 1 | Step 25700 | Avg Loss: 0.0161 | Grad Norm: 0.00794670\n",
      "Epoch 1 | Step 25800 | Avg Loss: 0.0163 | Grad Norm: 0.00829525\n",
      "Epoch 1 | Step 25900 | Avg Loss: 0.0163 | Grad Norm: 0.00810499\n",
      "Epoch 1 | Step 26000 | Avg Loss: 0.0160 | Grad Norm: 0.00812167\n",
      "Epoch 1 | Step 26100 | Avg Loss: 0.0155 | Grad Norm: 0.00844573\n",
      "Epoch 1 | Step 26200 | Avg Loss: 0.0157 | Grad Norm: 0.00920156\n",
      "Epoch 1 | Step 26300 | Avg Loss: 0.0159 | Grad Norm: 0.00948314\n",
      "Epoch 1 | Step 26400 | Avg Loss: 0.0157 | Grad Norm: 0.00890261\n",
      "Epoch 1 | Step 26500 | Avg Loss: 0.0157 | Grad Norm: 0.00880110\n",
      "Epoch 1 | Step 26600 | Avg Loss: 0.0154 | Grad Norm: 0.00840362\n",
      "Epoch 1 | Step 26700 | Avg Loss: 0.0157 | Grad Norm: 0.00763725\n",
      "Epoch 1 | Step 26800 | Avg Loss: 0.0161 | Grad Norm: 0.00875271\n",
      "Epoch 1 | Step 26900 | Avg Loss: 0.0158 | Grad Norm: 0.00858929\n",
      "Epoch 1 | Step 27000 | Avg Loss: 0.0161 | Grad Norm: 0.00844618\n",
      "Epoch 1 | Step 27100 | Avg Loss: 0.0161 | Grad Norm: 0.00892137\n",
      "Epoch 1 | Step 27200 | Avg Loss: 0.0159 | Grad Norm: 0.01129779\n",
      "Epoch 1 | Step 27300 | Avg Loss: 0.0157 | Grad Norm: 0.00843293\n",
      "Epoch 1 | Step 27400 | Avg Loss: 0.0160 | Grad Norm: 0.00799417\n",
      "Epoch 1 | Step 27500 | Avg Loss: 0.0161 | Grad Norm: 0.00796229\n",
      "Epoch 1 | Step 27600 | Avg Loss: 0.0160 | Grad Norm: 0.00849258\n",
      "Epoch 1 | Step 27700 | Avg Loss: 0.0160 | Grad Norm: 0.00853698\n",
      "Epoch 1 | Step 27800 | Avg Loss: 0.0163 | Grad Norm: 0.01037394\n",
      "Epoch 1 | Step 27900 | Avg Loss: 0.0161 | Grad Norm: 0.01046136\n",
      "Epoch 1 | Step 28000 | Avg Loss: 0.0161 | Grad Norm: 0.00881068\n",
      "Epoch 1 | Step 28100 | Avg Loss: 0.0161 | Grad Norm: 0.00904849\n",
      "Epoch 1 | Step 28200 | Avg Loss: 0.0162 | Grad Norm: 0.00868553\n",
      "Epoch 1 | Step 28300 | Avg Loss: 0.0154 | Grad Norm: 0.00803792\n",
      "Epoch 1 | Step 28400 | Avg Loss: 0.0157 | Grad Norm: 0.00906302\n",
      "Epoch 1 | Step 28500 | Avg Loss: 0.0160 | Grad Norm: 0.00841009\n",
      "Epoch 1 | Step 28600 | Avg Loss: 0.0160 | Grad Norm: 0.00881442\n",
      "Epoch 1 | Step 28700 | Avg Loss: 0.0159 | Grad Norm: 0.00974012\n",
      "Epoch 1 | Step 28800 | Avg Loss: 0.0156 | Grad Norm: 0.00826731\n",
      "Epoch 1 | Step 28900 | Avg Loss: 0.0156 | Grad Norm: 0.00850881\n",
      "Epoch 1 | Step 29000 | Avg Loss: 0.0154 | Grad Norm: 0.00918775\n",
      "Epoch 1 | Step 29100 | Avg Loss: 0.0158 | Grad Norm: 0.00809369\n",
      "Epoch 1 | Step 29200 | Avg Loss: 0.0158 | Grad Norm: 0.00918721\n",
      "Epoch 1 | Step 29300 | Avg Loss: 0.0155 | Grad Norm: 0.00887603\n",
      "Epoch 1 | Step 29400 | Avg Loss: 0.0156 | Grad Norm: 0.00797980\n",
      "Epoch 1 | Step 29500 | Avg Loss: 0.0158 | Grad Norm: 0.01039115\n",
      "Epoch 1 | Step 29600 | Avg Loss: 0.0159 | Grad Norm: 0.01084204\n",
      "Epoch 1 | Step 29700 | Avg Loss: 0.0159 | Grad Norm: 0.00850057\n",
      "Epoch 1 | Step 29800 | Avg Loss: 0.0158 | Grad Norm: 0.00913069\n",
      "Epoch 1 | Step 29900 | Avg Loss: 0.0160 | Grad Norm: 0.00822673\n",
      "Epoch 1 | Step 30000 | Avg Loss: 0.0161 | Grad Norm: 0.01068003\n",
      "Epoch 1 | Step 30100 | Avg Loss: 0.0158 | Grad Norm: 0.00741480\n",
      "Epoch 1 | Step 30200 | Avg Loss: 0.0160 | Grad Norm: 0.00992844\n",
      "Epoch 1 | Step 30300 | Avg Loss: 0.0160 | Grad Norm: 0.00908341\n",
      "Epoch 1 | Step 30400 | Avg Loss: 0.0160 | Grad Norm: 0.00775343\n",
      "Epoch 1 | Step 30500 | Avg Loss: 0.0158 | Grad Norm: 0.00852067\n",
      "Epoch 1 | Step 30600 | Avg Loss: 0.0156 | Grad Norm: 0.00957575\n",
      "Epoch 1 | Step 30700 | Avg Loss: 0.0157 | Grad Norm: 0.00752215\n",
      "Epoch 1 | Step 30800 | Avg Loss: 0.0157 | Grad Norm: 0.00729713\n",
      "Epoch 1 | Step 30900 | Avg Loss: 0.0162 | Grad Norm: 0.00816802\n",
      "Epoch 1 | Step 31000 | Avg Loss: 0.0159 | Grad Norm: 0.00909361\n",
      "Epoch 1 | Step 31100 | Avg Loss: 0.0158 | Grad Norm: 0.00780878\n",
      "Epoch 1 | Step 31200 | Avg Loss: 0.0156 | Grad Norm: 0.00954346\n",
      "Epoch 1 | Step 31300 | Avg Loss: 0.0153 | Grad Norm: 0.00887659\n",
      "Epoch 1 | Step 31400 | Avg Loss: 0.0157 | Grad Norm: 0.00810539\n",
      "Epoch 1 | Step 31500 | Avg Loss: 0.0157 | Grad Norm: 0.00916358\n",
      "Epoch 1 | Step 31600 | Avg Loss: 0.0155 | Grad Norm: 0.00891893\n",
      "Epoch 1 | Step 31700 | Avg Loss: 0.0154 | Grad Norm: 0.00823813\n",
      "Epoch 1 | Step 31800 | Avg Loss: 0.0158 | Grad Norm: 0.00899895\n",
      "Epoch 1 | Step 31900 | Avg Loss: 0.0161 | Grad Norm: 0.00982751\n",
      "Epoch 1 | Step 32000 | Avg Loss: 0.0159 | Grad Norm: 0.00887905\n",
      "Epoch 1 | Step 32100 | Avg Loss: 0.0158 | Grad Norm: 0.00816630\n",
      "Epoch 1 | Step 32200 | Avg Loss: 0.0157 | Grad Norm: 0.00751889\n",
      "Epoch 1 | Step 32300 | Avg Loss: 0.0165 | Grad Norm: 0.00760081\n",
      "Epoch 1 | Step 32400 | Avg Loss: 0.0163 | Grad Norm: 0.00807501\n",
      "Epoch 1 | Step 32500 | Avg Loss: 0.0162 | Grad Norm: 0.00852640\n",
      "Epoch 1 | Step 32600 | Avg Loss: 0.0162 | Grad Norm: 0.01004041\n",
      "Epoch 1 | Step 32700 | Avg Loss: 0.0161 | Grad Norm: 0.00965535\n",
      "Epoch 1 | Step 32800 | Avg Loss: 0.0159 | Grad Norm: 0.00994933\n",
      "Epoch 1 | Step 32900 | Avg Loss: 0.0160 | Grad Norm: 0.00890035\n",
      "Epoch 1 | Step 33000 | Avg Loss: 0.0158 | Grad Norm: 0.00931449\n",
      "Epoch 1 | Step 33100 | Avg Loss: 0.0156 | Grad Norm: 0.00964177\n",
      "Epoch 1 | Step 33200 | Avg Loss: 0.0155 | Grad Norm: 0.00849161\n",
      "Epoch 1 | Step 33300 | Avg Loss: 0.0155 | Grad Norm: 0.00815069\n",
      "Epoch 1 | Step 33400 | Avg Loss: 0.0158 | Grad Norm: 0.00927061\n",
      "Epoch 1 | Step 33500 | Avg Loss: 0.0156 | Grad Norm: 0.00918562\n",
      "Epoch 1 | Step 33600 | Avg Loss: 0.0158 | Grad Norm: 0.00857033\n",
      "Epoch 1 | Step 33700 | Avg Loss: 0.0159 | Grad Norm: 0.00804622\n",
      "Epoch 1 | Step 33800 | Avg Loss: 0.0157 | Grad Norm: 0.00744133\n",
      "Epoch 1 | Step 33900 | Avg Loss: 0.0157 | Grad Norm: 0.01026869\n",
      "Epoch 1 | Step 34000 | Avg Loss: 0.0154 | Grad Norm: 0.00924479\n",
      "Epoch 1 | Step 34100 | Avg Loss: 0.0149 | Grad Norm: 0.00753513\n",
      "Epoch 1 | Step 34200 | Avg Loss: 0.0148 | Grad Norm: 0.00965630\n",
      "Epoch 1 | Step 34300 | Avg Loss: 0.0150 | Grad Norm: 0.00824444\n",
      "Epoch 1 | Step 34400 | Avg Loss: 0.0154 | Grad Norm: 0.00850690\n",
      "Epoch 1 | Step 34500 | Avg Loss: 0.0157 | Grad Norm: 0.00925236\n",
      "Epoch 1 | Step 34600 | Avg Loss: 0.0157 | Grad Norm: 0.00902587\n",
      "Epoch 1 | Step 34700 | Avg Loss: 0.0154 | Grad Norm: 0.00885029\n",
      "Epoch 1 | Step 34800 | Avg Loss: 0.0154 | Grad Norm: 0.00766548\n",
      "Epoch 1 | Step 34900 | Avg Loss: 0.0159 | Grad Norm: 0.00931832\n",
      "Epoch 1 | Step 35000 | Avg Loss: 0.0156 | Grad Norm: 0.00909237\n",
      "Epoch 1 | Step 35100 | Avg Loss: 0.0157 | Grad Norm: 0.00850551\n",
      "Epoch 1 | Step 35200 | Avg Loss: 0.0161 | Grad Norm: 0.00783898\n",
      "Epoch 1 | Step 35300 | Avg Loss: 0.0159 | Grad Norm: 0.00854141\n",
      "Epoch 1 | Step 35400 | Avg Loss: 0.0159 | Grad Norm: 0.01042610\n",
      "Epoch 1 | Step 35500 | Avg Loss: 0.0162 | Grad Norm: 0.00918351\n",
      "Epoch 1 | Step 35600 | Avg Loss: 0.0164 | Grad Norm: 0.00838967\n",
      "Epoch 1 | Step 35700 | Avg Loss: 0.0161 | Grad Norm: 0.00904550\n",
      "Epoch 1 | Step 35800 | Avg Loss: 0.0157 | Grad Norm: 0.00724273\n",
      "Epoch 1 | Step 35900 | Avg Loss: 0.0157 | Grad Norm: 0.00834453\n",
      "Epoch 1 | Step 36000 | Avg Loss: 0.0153 | Grad Norm: 0.00788454\n",
      "Epoch 1 | Step 36100 | Avg Loss: 0.0157 | Grad Norm: 0.00828573\n",
      "Epoch 1 | Step 36200 | Avg Loss: 0.0157 | Grad Norm: 0.01355384\n",
      "Epoch 1 | Step 36300 | Avg Loss: 0.0162 | Grad Norm: 0.00820944\n",
      "Epoch 1 | Step 36400 | Avg Loss: 0.0163 | Grad Norm: 0.01006474\n",
      "Epoch 1 | Step 36500 | Avg Loss: 0.0163 | Grad Norm: 0.00852316\n",
      "Epoch 1 | Step 36600 | Avg Loss: 0.0162 | Grad Norm: 0.00875737\n",
      "Epoch 1 | Step 36700 | Avg Loss: 0.0160 | Grad Norm: 0.00874175\n",
      "Epoch 1 | Step 36800 | Avg Loss: 0.0162 | Grad Norm: 0.01114617\n",
      "Epoch 1 | Step 36900 | Avg Loss: 0.0162 | Grad Norm: 0.00956428\n",
      "Epoch 1 | Step 37000 | Avg Loss: 0.0166 | Grad Norm: 0.01060142\n",
      "Epoch 1 | Step 37100 | Avg Loss: 0.0161 | Grad Norm: 0.00875453\n",
      "Epoch 1 | Step 37200 | Avg Loss: 0.0162 | Grad Norm: 0.00870070\n",
      "Epoch 1 | Step 37300 | Avg Loss: 0.0159 | Grad Norm: 0.01000966\n",
      "Epoch 1 | Step 37400 | Avg Loss: 0.0162 | Grad Norm: 0.00926915\n",
      "Epoch 1 | Step 37500 | Avg Loss: 0.0160 | Grad Norm: 0.00914273\n",
      "Epoch 1 | Step 37600 | Avg Loss: 0.0159 | Grad Norm: 0.00964325\n",
      "Epoch 1 | Step 37700 | Avg Loss: 0.0161 | Grad Norm: 0.01011165\n",
      "Epoch 1 | Step 37800 | Avg Loss: 0.0162 | Grad Norm: 0.01091524\n",
      "Epoch 1 | Step 37900 | Avg Loss: 0.0161 | Grad Norm: 0.00920459\n",
      "Epoch 1 | Step 38000 | Avg Loss: 0.0163 | Grad Norm: 0.00883119\n",
      "Epoch 1 | Step 38100 | Avg Loss: 0.0165 | Grad Norm: 0.00906902\n",
      "Epoch 1 | Step 38200 | Avg Loss: 0.0165 | Grad Norm: 0.00967454\n",
      "Epoch 1 | Step 38300 | Avg Loss: 0.0166 | Grad Norm: 0.01041334\n",
      "Epoch 1 | Step 38400 | Avg Loss: 0.0168 | Grad Norm: 0.00918989\n",
      "Epoch 1 | Step 38500 | Avg Loss: 0.0166 | Grad Norm: 0.01007841\n",
      "Epoch 1 | Step 38600 | Avg Loss: 0.0161 | Grad Norm: 0.00916922\n",
      "Epoch 1 | Step 38700 | Avg Loss: 0.0160 | Grad Norm: 0.00814977\n",
      "Epoch 1 | Step 38800 | Avg Loss: 0.0161 | Grad Norm: 0.00928603\n",
      "Epoch 1 | Step 38900 | Avg Loss: 0.0161 | Grad Norm: 0.00868512\n",
      "Epoch 1 | Step 39000 | Avg Loss: 0.0159 | Grad Norm: 0.00872475\n",
      "Epoch 1 | Step 39100 | Avg Loss: 0.0162 | Grad Norm: 0.00842882\n",
      "Epoch 1 | Step 39200 | Avg Loss: 0.0159 | Grad Norm: 0.00843545\n",
      "Epoch 1 | Step 39300 | Avg Loss: 0.0156 | Grad Norm: 0.00836913\n",
      "Epoch 1 | Step 39400 | Avg Loss: 0.0160 | Grad Norm: 0.00940887\n",
      "Epoch 1 | Step 39500 | Avg Loss: 0.0159 | Grad Norm: 0.00750892\n",
      "Epoch 1 | Step 39600 | Avg Loss: 0.0156 | Grad Norm: 0.00882536\n",
      "Epoch 1 | Step 39700 | Avg Loss: 0.0160 | Grad Norm: 0.00995010\n",
      "Epoch 1 | Step 39800 | Avg Loss: 0.0159 | Grad Norm: 0.00879587\n",
      "Epoch 1 | Step 39900 | Avg Loss: 0.0159 | Grad Norm: 0.00873754\n",
      "Epoch 1 | Step 40000 | Avg Loss: 0.0158 | Grad Norm: 0.00881923\n",
      "Epoch 1 | Step 40100 | Avg Loss: 0.0158 | Grad Norm: 0.00784566\n",
      "Epoch 1 | Step 40200 | Avg Loss: 0.0159 | Grad Norm: 0.01219905\n",
      "Epoch 1 | Step 40300 | Avg Loss: 0.0156 | Grad Norm: 0.00969287\n",
      "Epoch 1 | Step 40400 | Avg Loss: 0.0155 | Grad Norm: 0.01124169\n",
      "Epoch 1 | Step 40500 | Avg Loss: 0.0159 | Grad Norm: 0.00854394\n",
      "Epoch 1 | Step 40600 | Avg Loss: 0.0155 | Grad Norm: 0.01002664\n",
      "Epoch 1 | Step 40700 | Avg Loss: 0.0154 | Grad Norm: 0.00978475\n",
      "Epoch 1 | Step 40800 | Avg Loss: 0.0155 | Grad Norm: 0.00843851\n",
      "Epoch 1 | Step 40900 | Avg Loss: 0.0153 | Grad Norm: 0.00750773\n",
      "Epoch 1 | Step 41000 | Avg Loss: 0.0156 | Grad Norm: 0.01006472\n",
      "Epoch 1 | Step 41100 | Avg Loss: 0.0153 | Grad Norm: 0.00866390\n",
      "Epoch 1 | Step 41200 | Avg Loss: 0.0158 | Grad Norm: 0.00935422\n",
      "Epoch 1 | Step 41300 | Avg Loss: 0.0158 | Grad Norm: 0.00995419\n",
      "Epoch 1 | Step 41400 | Avg Loss: 0.0155 | Grad Norm: 0.00847319\n",
      "Epoch 1 | Step 41500 | Avg Loss: 0.0154 | Grad Norm: 0.00819506\n",
      "Epoch 1 | Step 41600 | Avg Loss: 0.0156 | Grad Norm: 0.00879556\n",
      "Epoch 1 | Step 41700 | Avg Loss: 0.0160 | Grad Norm: 0.00930474\n",
      "Epoch 1 | Step 41800 | Avg Loss: 0.0156 | Grad Norm: 0.00914239\n",
      "Epoch 1 | Step 41900 | Avg Loss: 0.0158 | Grad Norm: 0.00850587\n",
      "Epoch 1 | Step 42000 | Avg Loss: 0.0163 | Grad Norm: 0.00908776\n",
      "Epoch 1 | Step 42100 | Avg Loss: 0.0163 | Grad Norm: 0.00803969\n",
      "Epoch 1 | Step 42200 | Avg Loss: 0.0162 | Grad Norm: 0.00835037\n",
      "Epoch 1 | Step 42300 | Avg Loss: 0.0162 | Grad Norm: 0.00963165\n",
      "Epoch 1 | Step 42400 | Avg Loss: 0.0167 | Grad Norm: 0.00925573\n",
      "Epoch 1 | Step 42500 | Avg Loss: 0.0164 | Grad Norm: 0.00808179\n",
      "Epoch 1 | Step 42600 | Avg Loss: 0.0160 | Grad Norm: 0.00719308\n",
      "Epoch 1 | Step 42700 | Avg Loss: 0.0157 | Grad Norm: 0.01083548\n",
      "Epoch 1 | Step 42800 | Avg Loss: 0.0157 | Grad Norm: 0.00825045\n",
      "Epoch 1 | Step 42900 | Avg Loss: 0.0154 | Grad Norm: 0.00871294\n",
      "Epoch 1 | Step 43000 | Avg Loss: 0.0153 | Grad Norm: 0.00842230\n",
      "Epoch 1 | Step 43100 | Avg Loss: 0.0157 | Grad Norm: 0.00952202\n",
      "Epoch 1 | Step 43200 | Avg Loss: 0.0159 | Grad Norm: 0.00835998\n",
      "Epoch 1 | Step 43300 | Avg Loss: 0.0158 | Grad Norm: 0.00965779\n",
      "Epoch 1 | Step 43400 | Avg Loss: 0.0157 | Grad Norm: 0.00916876\n",
      "Epoch 1 | Step 43500 | Avg Loss: 0.0161 | Grad Norm: 0.00847200\n",
      "Epoch 1 | Step 43600 | Avg Loss: 0.0158 | Grad Norm: 0.01243791\n",
      "Epoch 1 | Step 43700 | Avg Loss: 0.0157 | Grad Norm: 0.00754405\n",
      "Epoch 1 | Step 43800 | Avg Loss: 0.0155 | Grad Norm: 0.00812085\n",
      "Epoch 1 | Step 43900 | Avg Loss: 0.0158 | Grad Norm: 0.00854101\n",
      "Epoch 1 | Step 44000 | Avg Loss: 0.0156 | Grad Norm: 0.00802690\n",
      "Epoch 1 | Step 44100 | Avg Loss: 0.0160 | Grad Norm: 0.00768978\n",
      "Epoch 1 | Step 44200 | Avg Loss: 0.0159 | Grad Norm: 0.00871585\n",
      "Epoch 1 | Step 44300 | Avg Loss: 0.0161 | Grad Norm: 0.00917658\n",
      "Epoch 1 | Step 44400 | Avg Loss: 0.0160 | Grad Norm: 0.00918142\n",
      "Epoch 1 | Step 44500 | Avg Loss: 0.0161 | Grad Norm: 0.01106491\n",
      "Epoch 1 | Step 44600 | Avg Loss: 0.0159 | Grad Norm: 0.00940640\n",
      "Epoch 1 | Step 44700 | Avg Loss: 0.0163 | Grad Norm: 0.00831529\n",
      "Epoch 1 | Step 44800 | Avg Loss: 0.0165 | Grad Norm: 0.00828135\n",
      "Epoch 1 | Step 44900 | Avg Loss: 0.0161 | Grad Norm: 0.00948594\n",
      "Epoch 1 | Step 45000 | Avg Loss: 0.0159 | Grad Norm: 0.00799316\n",
      "Epoch 1 | Step 45100 | Avg Loss: 0.0157 | Grad Norm: 0.00829327\n",
      "Epoch 1 | Step 45200 | Avg Loss: 0.0153 | Grad Norm: 0.00860981\n",
      "Epoch 1 | Step 45300 | Avg Loss: 0.0155 | Grad Norm: 0.00732180\n",
      "Epoch 1 | Step 45400 | Avg Loss: 0.0154 | Grad Norm: 0.00808368\n",
      "Epoch 1 | Step 45500 | Avg Loss: 0.0156 | Grad Norm: 0.00780753\n",
      "Epoch 1 | Step 45600 | Avg Loss: 0.0158 | Grad Norm: 0.00944594\n",
      "Epoch 1 | Step 45700 | Avg Loss: 0.0158 | Grad Norm: 0.00947448\n",
      "Epoch 1 | Step 45800 | Avg Loss: 0.0159 | Grad Norm: 0.01373608\n",
      "Epoch 1 | Step 45900 | Avg Loss: 0.0150 | Grad Norm: 0.00914335\n",
      "Epoch 1 | Step 46000 | Avg Loss: 0.0153 | Grad Norm: 0.00898269\n",
      "Epoch 1 | Step 46100 | Avg Loss: 0.0154 | Grad Norm: 0.00844469\n",
      "Epoch 1 | Step 46200 | Avg Loss: 0.0150 | Grad Norm: 0.00899973\n",
      "Epoch 1 | Step 46300 | Avg Loss: 0.0153 | Grad Norm: 0.00879846\n",
      "Epoch 1 | Step 46400 | Avg Loss: 0.0155 | Grad Norm: 0.00985610\n",
      "Epoch 1 | Step 46500 | Avg Loss: 0.0152 | Grad Norm: 0.00726803\n",
      "Epoch 1 | Step 46600 | Avg Loss: 0.0151 | Grad Norm: 0.00713409\n",
      "Epoch 1 | Step 46700 | Avg Loss: 0.0154 | Grad Norm: 0.00936808\n",
      "Epoch 1 | Step 46800 | Avg Loss: 0.0153 | Grad Norm: 0.00979180\n",
      "Epoch 1 | Step 46900 | Avg Loss: 0.0155 | Grad Norm: 0.00918527\n",
      "Epoch 1 | Step 47000 | Avg Loss: 0.0152 | Grad Norm: 0.00829269\n",
      "Epoch 1 | Step 47100 | Avg Loss: 0.0159 | Grad Norm: 0.00871607\n",
      "Epoch 1 | Step 47200 | Avg Loss: 0.0157 | Grad Norm: 0.00834457\n",
      "Epoch 1 | Step 47300 | Avg Loss: 0.0156 | Grad Norm: 0.01275113\n",
      "Epoch 1 | Step 47400 | Avg Loss: 0.0160 | Grad Norm: 0.01043424\n",
      "Epoch 1 | Step 47500 | Avg Loss: 0.0159 | Grad Norm: 0.00785020\n",
      "Epoch 1 | Step 47600 | Avg Loss: 0.0162 | Grad Norm: 0.00848697\n",
      "Epoch 1 | Step 47700 | Avg Loss: 0.0166 | Grad Norm: 0.00870758\n",
      "Epoch 1 | Step 47800 | Avg Loss: 0.0163 | Grad Norm: 0.00865583\n",
      "Epoch 1 | Step 47900 | Avg Loss: 0.0162 | Grad Norm: 0.00742338\n",
      "Epoch 1 | Step 48000 | Avg Loss: 0.0159 | Grad Norm: 0.00861089\n",
      "Epoch 1 | Step 48100 | Avg Loss: 0.0158 | Grad Norm: 0.00991366\n",
      "Epoch 1 | Step 48200 | Avg Loss: 0.0158 | Grad Norm: 0.00898601\n",
      "Epoch 1 | Step 48300 | Avg Loss: 0.0157 | Grad Norm: 0.00901683\n",
      "Epoch 1 | Step 48400 | Avg Loss: 0.0158 | Grad Norm: 0.00852755\n",
      "Epoch 1 | Step 48500 | Avg Loss: 0.0160 | Grad Norm: 0.00769868\n",
      "Epoch 1 | Step 48600 | Avg Loss: 0.0156 | Grad Norm: 0.00771853\n",
      "Epoch 1 | Step 48700 | Avg Loss: 0.0158 | Grad Norm: 0.00782451\n",
      "Epoch 1 | Step 48800 | Avg Loss: 0.0155 | Grad Norm: 0.00916586\n",
      "Epoch 1 | Step 48900 | Avg Loss: 0.0155 | Grad Norm: 0.00933445\n",
      "Epoch 1 | Step 49000 | Avg Loss: 0.0156 | Grad Norm: 0.00807045\n",
      "Epoch 1 | Step 49100 | Avg Loss: 0.0156 | Grad Norm: 0.00897446\n",
      "Epoch 1 | Step 49200 | Avg Loss: 0.0157 | Grad Norm: 0.00816940\n",
      "Epoch 1 | Step 49300 | Avg Loss: 0.0159 | Grad Norm: 0.00941854\n",
      "Epoch 1 | Step 49400 | Avg Loss: 0.0164 | Grad Norm: 0.00735835\n",
      "Epoch 1 | Step 49500 | Avg Loss: 0.0161 | Grad Norm: 0.00887115\n",
      "Epoch 1 | Step 49600 | Avg Loss: 0.0160 | Grad Norm: 0.00964932\n",
      "Epoch 1 | Step 49700 | Avg Loss: 0.0158 | Grad Norm: 0.00901719\n",
      "Epoch 1 | Step 49800 | Avg Loss: 0.0157 | Grad Norm: 0.00711103\n",
      "Epoch 1 | Step 49900 | Avg Loss: 0.0156 | Grad Norm: 0.00837793\n",
      "Epoch 1 | Step 50000 | Avg Loss: 0.0157 | Grad Norm: 0.00998275\n",
      "Epoch 1 | Step 50100 | Avg Loss: 0.0160 | Grad Norm: 0.00997224\n",
      "Epoch 1 | Step 50200 | Avg Loss: 0.0157 | Grad Norm: 0.00847057\n",
      "Epoch 1 | Step 50300 | Avg Loss: 0.0158 | Grad Norm: 0.01362400\n",
      "Epoch 1 | Step 50400 | Avg Loss: 0.0158 | Grad Norm: 0.00885480\n",
      "Epoch 1 | Step 50500 | Avg Loss: 0.0156 | Grad Norm: 0.00865343\n",
      "Epoch 1 | Step 50600 | Avg Loss: 0.0158 | Grad Norm: 0.00707795\n",
      "Epoch 1 | Step 50700 | Avg Loss: 0.0156 | Grad Norm: 0.00795917\n",
      "Epoch 1 | Step 50800 | Avg Loss: 0.0155 | Grad Norm: 0.00977630\n",
      "Epoch 1 | Step 50900 | Avg Loss: 0.0158 | Grad Norm: 0.00775915\n",
      "Epoch 1 | Step 51000 | Avg Loss: 0.0159 | Grad Norm: 0.00803661\n",
      "Epoch 1 | Step 51100 | Avg Loss: 0.0159 | Grad Norm: 0.00799145\n",
      "Epoch 1 | Step 51200 | Avg Loss: 0.0159 | Grad Norm: 0.00844113\n",
      "Epoch 1 | Step 51300 | Avg Loss: 0.0159 | Grad Norm: 0.00770877\n",
      "Epoch 1 | Step 51400 | Avg Loss: 0.0158 | Grad Norm: 0.00790587\n",
      "Epoch 1 | Step 51500 | Avg Loss: 0.0157 | Grad Norm: 0.01031060\n",
      "Epoch 1 | Step 51600 | Avg Loss: 0.0161 | Grad Norm: 0.00944955\n",
      "Epoch 1 | Step 51700 | Avg Loss: 0.0157 | Grad Norm: 0.00932616\n",
      "Epoch 1 | Step 51800 | Avg Loss: 0.0158 | Grad Norm: 0.00924978\n",
      "Epoch 1 | Step 51900 | Avg Loss: 0.0159 | Grad Norm: 0.00875989\n",
      "Epoch 1 | Step 52000 | Avg Loss: 0.0158 | Grad Norm: 0.01017897\n",
      "Epoch 1 | Step 52100 | Avg Loss: 0.0160 | Grad Norm: 0.00876782\n",
      "Epoch 1 | Step 52200 | Avg Loss: 0.0161 | Grad Norm: 0.00891720\n",
      "Epoch 1 | Step 52300 | Avg Loss: 0.0158 | Grad Norm: 0.00880180\n",
      "Epoch 1 | Step 52400 | Avg Loss: 0.0157 | Grad Norm: 0.00804587\n",
      "Epoch 1 | Step 52500 | Avg Loss: 0.0155 | Grad Norm: 0.00868319\n",
      "Epoch 1 | Step 52600 | Avg Loss: 0.0156 | Grad Norm: 0.00772607\n",
      "Epoch 1 | Step 52700 | Avg Loss: 0.0156 | Grad Norm: 0.00845383\n",
      "Epoch 1 | Step 52800 | Avg Loss: 0.0154 | Grad Norm: 0.00823369\n",
      "Epoch 1 | Step 52900 | Avg Loss: 0.0154 | Grad Norm: 0.00722669\n",
      "Epoch 1 | Step 53000 | Avg Loss: 0.0151 | Grad Norm: 0.00810536\n",
      "Epoch 1 | Step 53100 | Avg Loss: 0.0153 | Grad Norm: 0.00869996\n",
      "Epoch 1 | Step 53200 | Avg Loss: 0.0153 | Grad Norm: 0.00787215\n",
      "Epoch 1 | Step 53300 | Avg Loss: 0.0152 | Grad Norm: 0.00854927\n",
      "Epoch 1 | Step 53400 | Avg Loss: 0.0156 | Grad Norm: 0.00858837\n",
      "Epoch 1 | Step 53500 | Avg Loss: 0.0160 | Grad Norm: 0.00928559\n",
      "Epoch 1 | Step 53600 | Avg Loss: 0.0156 | Grad Norm: 0.00877368\n",
      "Epoch 1 | Step 53700 | Avg Loss: 0.0159 | Grad Norm: 0.00980877\n",
      "Epoch 1 | Step 53800 | Avg Loss: 0.0159 | Grad Norm: 0.00911145\n",
      "Epoch 1 | Step 53900 | Avg Loss: 0.0159 | Grad Norm: 0.01042046\n",
      "Epoch 1 | Step 54000 | Avg Loss: 0.0162 | Grad Norm: 0.01079676\n",
      "Epoch 1 | Step 54100 | Avg Loss: 0.0161 | Grad Norm: 0.00938934\n",
      "Epoch 1 | Step 54200 | Avg Loss: 0.0161 | Grad Norm: 0.00961958\n",
      "Epoch 1 | Step 54300 | Avg Loss: 0.0164 | Grad Norm: 0.00781949\n",
      "Epoch 1 | Step 54400 | Avg Loss: 0.0165 | Grad Norm: 0.00879565\n",
      "Epoch 1 | Step 54500 | Avg Loss: 0.0166 | Grad Norm: 0.00780535\n",
      "Epoch 1 | Step 54600 | Avg Loss: 0.0162 | Grad Norm: 0.00911638\n",
      "Epoch 1 | Step 54700 | Avg Loss: 0.0163 | Grad Norm: 0.00822698\n",
      "Epoch 1 | Step 54800 | Avg Loss: 0.0164 | Grad Norm: 0.00949915\n",
      "Epoch 1 | Step 54900 | Avg Loss: 0.0162 | Grad Norm: 0.00881602\n",
      "Epoch 1 | Step 55000 | Avg Loss: 0.0162 | Grad Norm: 0.00845025\n",
      "Epoch 1 | Step 55100 | Avg Loss: 0.0162 | Grad Norm: 0.00784378\n",
      "Epoch 1 | Step 55200 | Avg Loss: 0.0162 | Grad Norm: 0.00929995\n",
      "Epoch 1 | Step 55300 | Avg Loss: 0.0161 | Grad Norm: 0.00899315\n",
      "Epoch 1 | Step 55400 | Avg Loss: 0.0159 | Grad Norm: 0.00901967\n",
      "Epoch 1 | Step 55500 | Avg Loss: 0.0161 | Grad Norm: 0.00787524\n",
      "Epoch 1 | Step 55600 | Avg Loss: 0.0157 | Grad Norm: 0.00926522\n",
      "Epoch 1 | Step 55700 | Avg Loss: 0.0160 | Grad Norm: 0.00871613\n",
      "Epoch 1 | Step 55800 | Avg Loss: 0.0160 | Grad Norm: 0.00832051\n",
      "Epoch 1 | Step 55900 | Avg Loss: 0.0159 | Grad Norm: 0.00826592\n",
      "Epoch 1 | Step 56000 | Avg Loss: 0.0159 | Grad Norm: 0.00783676\n",
      "Epoch 1 | Step 56100 | Avg Loss: 0.0161 | Grad Norm: 0.00886177\n",
      "Epoch 1 | Step 56200 | Avg Loss: 0.0163 | Grad Norm: 0.00847324\n",
      "Epoch 1 | Step 56300 | Avg Loss: 0.0163 | Grad Norm: 0.00830957\n",
      "Epoch 1 | Step 56400 | Avg Loss: 0.0163 | Grad Norm: 0.00949771\n",
      "Epoch 1 | Step 56500 | Avg Loss: 0.0160 | Grad Norm: 0.00860700\n",
      "Epoch 1 | Step 56600 | Avg Loss: 0.0163 | Grad Norm: 0.00962113\n",
      "Epoch 1 | Step 56700 | Avg Loss: 0.0162 | Grad Norm: 0.00824332\n",
      "Epoch 1 | Step 56800 | Avg Loss: 0.0167 | Grad Norm: 0.00953681\n",
      "Epoch 1 | Step 56900 | Avg Loss: 0.0169 | Grad Norm: 0.00942124\n",
      "Epoch 1 | Step 57000 | Avg Loss: 0.0164 | Grad Norm: 0.00862571\n",
      "Epoch 1 | Step 57100 | Avg Loss: 0.0160 | Grad Norm: 0.00923245\n",
      "Epoch 1 | Step 57200 | Avg Loss: 0.0158 | Grad Norm: 0.01101644\n",
      "Epoch 1 | Step 57300 | Avg Loss: 0.0157 | Grad Norm: 0.00862813\n",
      "Epoch 1 | Step 57400 | Avg Loss: 0.0156 | Grad Norm: 0.00894277\n",
      "Epoch 1 | Step 57500 | Avg Loss: 0.0156 | Grad Norm: 0.00876221\n",
      "Epoch 1 | Step 57600 | Avg Loss: 0.0154 | Grad Norm: 0.00745445\n",
      "Epoch 1 | Step 57700 | Avg Loss: 0.0157 | Grad Norm: 0.00830659\n",
      "Epoch 1 | Step 57800 | Avg Loss: 0.0156 | Grad Norm: 0.00902682\n",
      "Epoch 1 | Step 57900 | Avg Loss: 0.0153 | Grad Norm: 0.00855272\n",
      "Epoch 1 | Step 58000 | Avg Loss: 0.0154 | Grad Norm: 0.00875462\n",
      "Epoch 1 | Step 58100 | Avg Loss: 0.0155 | Grad Norm: 0.00860892\n",
      "Epoch 1 | Step 58200 | Avg Loss: 0.0157 | Grad Norm: 0.00973612\n",
      "Epoch 1 | Step 58300 | Avg Loss: 0.0158 | Grad Norm: 0.00879374\n",
      "Epoch 1 | Step 58400 | Avg Loss: 0.0159 | Grad Norm: 0.00839542\n",
      "Epoch 1 | Step 58500 | Avg Loss: 0.0159 | Grad Norm: 0.00880574\n",
      "Epoch 1 | Step 58600 | Avg Loss: 0.0159 | Grad Norm: 0.00687830\n",
      "Epoch 1 | Step 58700 | Avg Loss: 0.0155 | Grad Norm: 0.00909594\n",
      "Epoch 1 | Step 58800 | Avg Loss: 0.0157 | Grad Norm: 0.00913509\n",
      "Epoch 1 | Step 58900 | Avg Loss: 0.0157 | Grad Norm: 0.00906237\n",
      "Epoch 1 | Step 59000 | Avg Loss: 0.0154 | Grad Norm: 0.00886990\n",
      "Epoch 1 | Step 59100 | Avg Loss: 0.0158 | Grad Norm: 0.00891448\n",
      "Epoch 1 | Step 59200 | Avg Loss: 0.0160 | Grad Norm: 0.00942355\n",
      "Epoch 1 | Step 59300 | Avg Loss: 0.0158 | Grad Norm: 0.00990567\n",
      "Epoch 1 | Step 59400 | Avg Loss: 0.0158 | Grad Norm: 0.00933908\n",
      "Epoch 1 | Step 59500 | Avg Loss: 0.0155 | Grad Norm: 0.00820420\n",
      "Epoch 1 | Step 59600 | Avg Loss: 0.0157 | Grad Norm: 0.00877365\n",
      "Epoch 1 | Step 59700 | Avg Loss: 0.0154 | Grad Norm: 0.00952115\n",
      "Epoch 1 | Step 59800 | Avg Loss: 0.0151 | Grad Norm: 0.00778510\n",
      "Epoch 1 | Step 59900 | Avg Loss: 0.0155 | Grad Norm: 0.00785677\n",
      "Epoch 1 | Step 60000 | Avg Loss: 0.0156 | Grad Norm: 0.00711853\n",
      "Epoch 1 | Step 60100 | Avg Loss: 0.0157 | Grad Norm: 0.00877854\n",
      "Epoch 1 | Step 60200 | Avg Loss: 0.0158 | Grad Norm: 0.00834393\n",
      "Epoch 1 | Step 60300 | Avg Loss: 0.0157 | Grad Norm: 0.00831898\n",
      "Epoch 1 | Step 60400 | Avg Loss: 0.0157 | Grad Norm: 0.00865702\n",
      "Epoch 1 | Step 60500 | Avg Loss: 0.0155 | Grad Norm: 0.00911184\n",
      "Epoch 1 | Step 60600 | Avg Loss: 0.0154 | Grad Norm: 0.00914996\n",
      "Epoch 1 | Step 60700 | Avg Loss: 0.0154 | Grad Norm: 0.00817677\n",
      "Epoch 1 | Step 60800 | Avg Loss: 0.0154 | Grad Norm: 0.00951452\n",
      "Epoch 1 | Step 60900 | Avg Loss: 0.0158 | Grad Norm: 0.01089253\n",
      "Epoch 1 | Step 61000 | Avg Loss: 0.0156 | Grad Norm: 0.00923170\n",
      "Epoch 1 | Step 61100 | Avg Loss: 0.0156 | Grad Norm: 0.00725870\n",
      "Epoch 1 | Step 61200 | Avg Loss: 0.0157 | Grad Norm: 0.00881993\n",
      "Epoch 1 | Step 61300 | Avg Loss: 0.0156 | Grad Norm: 0.00772977\n",
      "Epoch 1 | Step 61400 | Avg Loss: 0.0155 | Grad Norm: 0.00884599\n",
      "Epoch 1 | Step 61500 | Avg Loss: 0.0158 | Grad Norm: 0.00886171\n",
      "Epoch 1 | Step 61600 | Avg Loss: 0.0158 | Grad Norm: 0.00889285\n",
      "Epoch 1 | Step 61700 | Avg Loss: 0.0156 | Grad Norm: 0.00908201\n",
      "Epoch 1 | Step 61800 | Avg Loss: 0.0157 | Grad Norm: 0.00961749\n",
      "Epoch 1 | Step 61900 | Avg Loss: 0.0158 | Grad Norm: 0.00734658\n",
      "Epoch 1 | Step 62000 | Avg Loss: 0.0160 | Grad Norm: 0.00959883\n",
      "Epoch 1 | Step 62100 | Avg Loss: 0.0167 | Grad Norm: 0.00946601\n",
      "Epoch 1 | Step 62200 | Avg Loss: 0.0163 | Grad Norm: 0.01145099\n",
      "Epoch 1 | Step 62300 | Avg Loss: 0.0157 | Grad Norm: 0.00834959\n",
      "Epoch 1 | Step 62400 | Avg Loss: 0.0152 | Grad Norm: 0.00758367\n",
      "Epoch 1 | Step 62500 | Avg Loss: 0.0153 | Grad Norm: 0.00783217\n",
      "Epoch 1 | Step 62600 | Avg Loss: 0.0157 | Grad Norm: 0.00791939\n",
      "Epoch 1 | Step 62700 | Avg Loss: 0.0157 | Grad Norm: 0.00892789\n",
      "Epoch 1 | Step 62800 | Avg Loss: 0.0157 | Grad Norm: 0.00933115\n",
      "Epoch 1 | Step 62900 | Avg Loss: 0.0160 | Grad Norm: 0.00852106\n",
      "Epoch 1 | Step 63000 | Avg Loss: 0.0161 | Grad Norm: 0.00836365\n",
      "Epoch 1 | Step 63100 | Avg Loss: 0.0163 | Grad Norm: 0.00980936\n",
      "Epoch 1 | Step 63200 | Avg Loss: 0.0160 | Grad Norm: 0.00911847\n",
      "Epoch 1 | Step 63300 | Avg Loss: 0.0159 | Grad Norm: 0.00813569\n",
      "Epoch 1 | Step 63400 | Avg Loss: 0.0157 | Grad Norm: 0.00824041\n",
      "Epoch 1 | Step 63500 | Avg Loss: 0.0155 | Grad Norm: 0.00850953\n",
      "Epoch 1 | Step 63600 | Avg Loss: 0.0152 | Grad Norm: 0.00821423\n",
      "Epoch 1 | Step 63700 | Avg Loss: 0.0151 | Grad Norm: 0.00988738\n",
      "Epoch 1 | Step 63800 | Avg Loss: 0.0150 | Grad Norm: 0.00806975\n",
      "Epoch 1 | Step 63900 | Avg Loss: 0.0148 | Grad Norm: 0.01074179\n",
      "Epoch 1 | Step 64000 | Avg Loss: 0.0155 | Grad Norm: 0.00933494\n",
      "Epoch 1 | Step 64100 | Avg Loss: 0.0151 | Grad Norm: 0.00827755\n",
      "Epoch 1 | Step 64200 | Avg Loss: 0.0151 | Grad Norm: 0.00894002\n",
      "Epoch 1 | Step 64300 | Avg Loss: 0.0156 | Grad Norm: 0.00904916\n",
      "Epoch 1 | Step 64400 | Avg Loss: 0.0156 | Grad Norm: 0.00931871\n",
      "Epoch 1 | Step 64500 | Avg Loss: 0.0159 | Grad Norm: 0.00900611\n",
      "Epoch 1 | Step 64600 | Avg Loss: 0.0159 | Grad Norm: 0.00953960\n",
      "Epoch 1 | Step 64700 | Avg Loss: 0.0160 | Grad Norm: 0.00883487\n",
      "Epoch 1 | Step 64800 | Avg Loss: 0.0157 | Grad Norm: 0.00875434\n",
      "Epoch 1 | Step 64900 | Avg Loss: 0.0159 | Grad Norm: 0.00817817\n",
      "Epoch 1 | Step 65000 | Avg Loss: 0.0157 | Grad Norm: 0.00905142\n",
      "Epoch 1 | Step 65100 | Avg Loss: 0.0157 | Grad Norm: 0.00820629\n",
      "Epoch 1 | Step 65200 | Avg Loss: 0.0155 | Grad Norm: 0.00823795\n",
      "Epoch 1 | Step 65300 | Avg Loss: 0.0152 | Grad Norm: 0.00834839\n",
      "Epoch 1 | Step 65400 | Avg Loss: 0.0152 | Grad Norm: 0.00775116\n",
      "Epoch 1 | Step 65500 | Avg Loss: 0.0150 | Grad Norm: 0.00927817\n",
      "Epoch 1 | Step 65600 | Avg Loss: 0.0151 | Grad Norm: 0.00728963\n",
      "Epoch 1 | Step 65700 | Avg Loss: 0.0151 | Grad Norm: 0.00912702\n",
      "Epoch 1 | Step 65800 | Avg Loss: 0.0151 | Grad Norm: 0.00824807\n",
      "Epoch 1 | Step 65900 | Avg Loss: 0.0153 | Grad Norm: 0.00835996\n",
      "Epoch 1 | Step 66000 | Avg Loss: 0.0153 | Grad Norm: 0.00821130\n",
      "Epoch 1 | Step 66100 | Avg Loss: 0.0154 | Grad Norm: 0.00899459\n",
      "Epoch 1 | Step 66200 | Avg Loss: 0.0155 | Grad Norm: 0.00811866\n",
      "Epoch 1 | Step 66300 | Avg Loss: 0.0154 | Grad Norm: 0.00829377\n",
      "Epoch 1 | Step 66400 | Avg Loss: 0.0158 | Grad Norm: 0.01565559\n",
      "Epoch 1 | Step 66500 | Avg Loss: 0.0161 | Grad Norm: 0.00828037\n",
      "Epoch 1 | Step 66600 | Avg Loss: 0.0161 | Grad Norm: 0.00895758\n",
      "Epoch 1 | Step 66700 | Avg Loss: 0.0158 | Grad Norm: 0.00936879\n",
      "Epoch 1 | Step 66800 | Avg Loss: 0.0152 | Grad Norm: 0.00883654\n",
      "Epoch 1 | Step 66900 | Avg Loss: 0.0153 | Grad Norm: 0.00956007\n",
      "Epoch 1 | Step 67000 | Avg Loss: 0.0157 | Grad Norm: 0.01015476\n",
      "Epoch 1 | Step 67100 | Avg Loss: 0.0156 | Grad Norm: 0.00861982\n",
      "Epoch 1 | Step 67200 | Avg Loss: 0.0157 | Grad Norm: 0.00783969\n",
      "Epoch 1 | Step 67300 | Avg Loss: 0.0154 | Grad Norm: 0.00933303\n",
      "Epoch 1 | Step 67400 | Avg Loss: 0.0155 | Grad Norm: 0.00785848\n",
      "Epoch 1 | Step 67500 | Avg Loss: 0.0158 | Grad Norm: 0.01014574\n",
      "Epoch 1 | Step 67600 | Avg Loss: 0.0162 | Grad Norm: 0.01035808\n",
      "Epoch 1 | Step 67700 | Avg Loss: 0.0161 | Grad Norm: 0.01176195\n",
      "Epoch 1 | Step 67800 | Avg Loss: 0.0162 | Grad Norm: 0.00908810\n",
      "Epoch 1 | Step 67900 | Avg Loss: 0.0159 | Grad Norm: 0.00908934\n",
      "Epoch 1 | Step 68000 | Avg Loss: 0.0154 | Grad Norm: 0.00836254\n",
      "Epoch 1 | Step 68100 | Avg Loss: 0.0156 | Grad Norm: 0.00697695\n",
      "Epoch 1 | Step 68200 | Avg Loss: 0.0155 | Grad Norm: 0.00870298\n",
      "Epoch 1 | Step 68300 | Avg Loss: 0.0156 | Grad Norm: 0.00861700\n",
      "Epoch 1 | Step 68400 | Avg Loss: 0.0156 | Grad Norm: 0.00884573\n",
      "Epoch 1 | Step 68500 | Avg Loss: 0.0157 | Grad Norm: 0.00878659\n",
      "Epoch 1 | Step 68600 | Avg Loss: 0.0157 | Grad Norm: 0.00813172\n",
      "Epoch 1 | Step 68700 | Avg Loss: 0.0155 | Grad Norm: 0.00896620\n",
      "Epoch 1 | Step 68800 | Avg Loss: 0.0159 | Grad Norm: 0.01000439\n",
      "Epoch 1 | Step 68900 | Avg Loss: 0.0161 | Grad Norm: 0.00904420\n",
      "Epoch 1 | Step 69000 | Avg Loss: 0.0165 | Grad Norm: 0.00775011\n",
      "Epoch 1 | Step 69100 | Avg Loss: 0.0161 | Grad Norm: 0.00855078\n",
      "Epoch 1 | Step 69200 | Avg Loss: 0.0159 | Grad Norm: 0.01243429\n",
      "Epoch 1 | Step 69300 | Avg Loss: 0.0156 | Grad Norm: 0.00903652\n",
      "Epoch 1 | Step 69400 | Avg Loss: 0.0157 | Grad Norm: 0.01012642\n",
      "Epoch 1 | Step 69500 | Avg Loss: 0.0158 | Grad Norm: 0.00941575\n",
      "Epoch 1 | Step 69600 | Avg Loss: 0.0157 | Grad Norm: 0.00884220\n",
      "Epoch 1 | Step 69700 | Avg Loss: 0.0155 | Grad Norm: 0.00818931\n",
      "Epoch 1 | Step 69800 | Avg Loss: 0.0157 | Grad Norm: 0.01125952\n",
      "Epoch 1 | Step 69900 | Avg Loss: 0.0158 | Grad Norm: 0.00906753\n",
      "Epoch 1 | Step 70000 | Avg Loss: 0.0161 | Grad Norm: 0.01060896\n",
      "Epoch 1 | Step 70100 | Avg Loss: 0.0163 | Grad Norm: 0.00886569\n",
      "Epoch 1 | Step 70200 | Avg Loss: 0.0158 | Grad Norm: 0.00757890\n",
      "Epoch 1 | Step 70300 | Avg Loss: 0.0157 | Grad Norm: 0.00956946\n",
      "Epoch 1 | Step 70400 | Avg Loss: 0.0156 | Grad Norm: 0.01106688\n",
      "Epoch 1 | Step 70500 | Avg Loss: 0.0158 | Grad Norm: 0.00810385\n",
      "Epoch 1 | Step 70600 | Avg Loss: 0.0158 | Grad Norm: 0.00904609\n",
      "Epoch 1 | Step 70700 | Avg Loss: 0.0157 | Grad Norm: 0.01054760\n",
      "Epoch 1 | Step 70800 | Avg Loss: 0.0150 | Grad Norm: 0.00759321\n",
      "Epoch 1 | Step 70900 | Avg Loss: 0.0155 | Grad Norm: 0.00838870\n",
      "Epoch 1 | Step 71000 | Avg Loss: 0.0158 | Grad Norm: 0.00958329\n",
      "Epoch 1 | Step 71100 | Avg Loss: 0.0155 | Grad Norm: 0.01032180\n",
      "Epoch 1 | Step 71200 | Avg Loss: 0.0160 | Grad Norm: 0.01003562\n",
      "Epoch 1 | Step 71300 | Avg Loss: 0.0158 | Grad Norm: 0.00775308\n",
      "Epoch 1 | Step 71400 | Avg Loss: 0.0157 | Grad Norm: 0.00933351\n",
      "Epoch 1 | Step 71500 | Avg Loss: 0.0156 | Grad Norm: 0.00816038\n",
      "Epoch 1 | Step 71600 | Avg Loss: 0.0160 | Grad Norm: 0.01183483\n",
      "Epoch 1 | Step 71700 | Avg Loss: 0.0159 | Grad Norm: 0.00906949\n",
      "Epoch 1 | Step 71800 | Avg Loss: 0.0157 | Grad Norm: 0.00823227\n",
      "Epoch 1 | Step 71900 | Avg Loss: 0.0155 | Grad Norm: 0.00852579\n",
      "Epoch 1 | Step 72000 | Avg Loss: 0.0159 | Grad Norm: 0.00919378\n",
      "Epoch 1 | Step 72100 | Avg Loss: 0.0160 | Grad Norm: 0.00988440\n",
      "Epoch 1 | Step 72200 | Avg Loss: 0.0157 | Grad Norm: 0.00894102\n",
      "Epoch 1 | Step 72300 | Avg Loss: 0.0155 | Grad Norm: 0.00832370\n",
      "Epoch 1 | Step 72400 | Avg Loss: 0.0159 | Grad Norm: 0.00810836\n",
      "Epoch 1 | Step 72500 | Avg Loss: 0.0156 | Grad Norm: 0.00851345\n",
      "Epoch 1 | Step 72600 | Avg Loss: 0.0159 | Grad Norm: 0.00840463\n",
      "Epoch 1 | Step 72700 | Avg Loss: 0.0157 | Grad Norm: 0.00882731\n",
      "Epoch 1 | Step 72800 | Avg Loss: 0.0156 | Grad Norm: 0.00917284\n",
      "Epoch 1 | Step 72900 | Avg Loss: 0.0160 | Grad Norm: 0.00872303\n",
      "Epoch 1 | Step 73000 | Avg Loss: 0.0164 | Grad Norm: 0.00907593\n",
      "Epoch 1 | Step 73100 | Avg Loss: 0.0161 | Grad Norm: 0.01172748\n",
      "Epoch 1 | Step 73200 | Avg Loss: 0.0163 | Grad Norm: 0.00938147\n",
      "Epoch 1 | Step 73300 | Avg Loss: 0.0160 | Grad Norm: 0.00867878\n",
      "Epoch 1 | Step 73400 | Avg Loss: 0.0158 | Grad Norm: 0.00916147\n",
      "Epoch 1 | Step 73500 | Avg Loss: 0.0154 | Grad Norm: 0.01071384\n",
      "Epoch 1 | Step 73600 | Avg Loss: 0.0157 | Grad Norm: 0.00899375\n",
      "Epoch 1 | Step 73700 | Avg Loss: 0.0155 | Grad Norm: 0.01047597\n",
      "Epoch 1 | Step 73800 | Avg Loss: 0.0158 | Grad Norm: 0.00761133\n",
      "Epoch 1 | Step 73900 | Avg Loss: 0.0153 | Grad Norm: 0.00928463\n",
      "Epoch 1 | Step 74000 | Avg Loss: 0.0153 | Grad Norm: 0.00814027\n",
      "Epoch 1 | Step 74100 | Avg Loss: 0.0156 | Grad Norm: 0.01344292\n",
      "Epoch 1 | Step 74200 | Avg Loss: 0.0154 | Grad Norm: 0.01085866\n",
      "Epoch 1 | Step 74300 | Avg Loss: 0.0153 | Grad Norm: 0.00809922\n",
      "Epoch 1 | Step 74400 | Avg Loss: 0.0157 | Grad Norm: 0.00850995\n",
      "Epoch 1 | Step 74500 | Avg Loss: 0.0157 | Grad Norm: 0.00830958\n",
      "Epoch 1 | Step 74600 | Avg Loss: 0.0161 | Grad Norm: 0.00858164\n",
      "Epoch 1 | Step 74700 | Avg Loss: 0.0159 | Grad Norm: 0.00900045\n",
      "Epoch 1 | Step 74800 | Avg Loss: 0.0158 | Grad Norm: 0.00846519\n",
      "Epoch 1 | Step 74900 | Avg Loss: 0.0160 | Grad Norm: 0.00877538\n",
      "Epoch 1 | Step 75000 | Avg Loss: 0.0162 | Grad Norm: 0.00734500\n",
      "Epoch 1 | Step 75100 | Avg Loss: 0.0160 | Grad Norm: 0.00911281\n",
      "Epoch 1 | Step 75200 | Avg Loss: 0.0156 | Grad Norm: 0.00914496\n",
      "Epoch 1 | Step 75300 | Avg Loss: 0.0156 | Grad Norm: 0.00865908\n",
      "Epoch 1 | Step 75400 | Avg Loss: 0.0154 | Grad Norm: 0.00889853\n",
      "Epoch 1 | Step 75500 | Avg Loss: 0.0154 | Grad Norm: 0.00842780\n",
      "Epoch 1 | Step 75600 | Avg Loss: 0.0153 | Grad Norm: 0.00881410\n",
      "Epoch 1 | Step 75700 | Avg Loss: 0.0156 | Grad Norm: 0.00743069\n",
      "Epoch 1 | Step 75800 | Avg Loss: 0.0157 | Grad Norm: 0.00854175\n",
      "Epoch 1 | Step 75900 | Avg Loss: 0.0159 | Grad Norm: 0.00942201\n",
      "Epoch 1 | Step 76000 | Avg Loss: 0.0161 | Grad Norm: 0.00747541\n",
      "Epoch 1 | Step 76100 | Avg Loss: 0.0159 | Grad Norm: 0.00860506\n",
      "Epoch 1 | Step 76200 | Avg Loss: 0.0160 | Grad Norm: 0.00916799\n",
      "Epoch 1 | Step 76300 | Avg Loss: 0.0161 | Grad Norm: 0.00835746\n",
      "Epoch 1 | Step 76400 | Avg Loss: 0.0158 | Grad Norm: 0.00947663\n",
      "Epoch 1 | Step 76500 | Avg Loss: 0.0156 | Grad Norm: 0.00958107\n",
      "Epoch 1 | Step 76600 | Avg Loss: 0.0155 | Grad Norm: 0.00828105\n",
      "Epoch 1 | Step 76700 | Avg Loss: 0.0157 | Grad Norm: 0.00789300\n",
      "Epoch 1 | Step 76800 | Avg Loss: 0.0156 | Grad Norm: 0.00930511\n",
      "Epoch 1 | Step 76900 | Avg Loss: 0.0158 | Grad Norm: 0.00871550\n",
      "Epoch 1 | Step 77000 | Avg Loss: 0.0158 | Grad Norm: 0.00924234\n",
      "Epoch 1 | Step 77100 | Avg Loss: 0.0158 | Grad Norm: 0.00831703\n",
      "Epoch 1 | Step 77200 | Avg Loss: 0.0159 | Grad Norm: 0.00998027\n",
      "Epoch 1 | Step 77300 | Avg Loss: 0.0163 | Grad Norm: 0.01050394\n",
      "Epoch 1 | Step 77400 | Avg Loss: 0.0159 | Grad Norm: 0.01005701\n",
      "Epoch 1 | Step 77500 | Avg Loss: 0.0161 | Grad Norm: 0.00757469\n",
      "Epoch 1 | Step 77600 | Avg Loss: 0.0164 | Grad Norm: 0.00865739\n",
      "Epoch 1 | Step 77700 | Avg Loss: 0.0163 | Grad Norm: 0.00829699\n",
      "Epoch 1 | Step 77800 | Avg Loss: 0.0164 | Grad Norm: 0.00918416\n",
      "Epoch 1 | Step 77900 | Avg Loss: 0.0165 | Grad Norm: 0.01650356\n",
      "Epoch 1 | Step 78000 | Avg Loss: 0.0162 | Grad Norm: 0.00925615\n",
      "Epoch 1 | Step 78100 | Avg Loss: 0.0163 | Grad Norm: 0.00897503\n",
      "Epoch 1 | Step 78200 | Avg Loss: 0.0161 | Grad Norm: 0.00832688\n",
      "Epoch 1 | Step 78300 | Avg Loss: 0.0164 | Grad Norm: 0.00985727\n",
      "Epoch 1 | Step 78400 | Avg Loss: 0.0165 | Grad Norm: 0.00799769\n",
      "Epoch 1 | Step 78500 | Avg Loss: 0.0164 | Grad Norm: 0.01003431\n",
      "Epoch 1 | Step 78600 | Avg Loss: 0.0162 | Grad Norm: 0.00990980\n",
      "Epoch 1 | Step 78700 | Avg Loss: 0.0161 | Grad Norm: 0.01029079\n",
      "Epoch 1 | Step 78800 | Avg Loss: 0.0164 | Grad Norm: 0.00976881\n",
      "Epoch 1 | Step 78900 | Avg Loss: 0.0164 | Grad Norm: 0.00903723\n",
      "Epoch 1 | Step 79000 | Avg Loss: 0.0164 | Grad Norm: 0.00937913\n",
      "Epoch 1 | Step 79100 | Avg Loss: 0.0160 | Grad Norm: 0.00865503\n",
      "Epoch 1 | Step 79200 | Avg Loss: 0.0158 | Grad Norm: 0.00810576\n",
      "Epoch 1 | Step 79300 | Avg Loss: 0.0156 | Grad Norm: 0.00838283\n",
      "Epoch 1 | Step 79400 | Avg Loss: 0.0159 | Grad Norm: 0.00844170\n",
      "Epoch 1 | Step 79500 | Avg Loss: 0.0158 | Grad Norm: 0.00869023\n",
      "Epoch 1 | Step 79600 | Avg Loss: 0.0158 | Grad Norm: 0.00860289\n",
      "Epoch 1 | Step 79700 | Avg Loss: 0.0162 | Grad Norm: 0.00891064\n",
      "Epoch 1 | Step 79800 | Avg Loss: 0.0164 | Grad Norm: 0.00897762\n",
      "Epoch 1 | Step 79900 | Avg Loss: 0.0165 | Grad Norm: 0.01017345\n",
      "Epoch 1 | Step 80000 | Avg Loss: 0.0163 | Grad Norm: 0.00897399\n",
      "Epoch 1 | Step 80100 | Avg Loss: 0.0161 | Grad Norm: 0.00908389\n",
      "Epoch 1 | Step 80200 | Avg Loss: 0.0160 | Grad Norm: 0.00893717\n",
      "Epoch 1 | Step 80300 | Avg Loss: 0.0160 | Grad Norm: 0.00824594\n",
      "Epoch 1 | Step 80400 | Avg Loss: 0.0159 | Grad Norm: 0.00837356\n",
      "Epoch 1 | Step 80500 | Avg Loss: 0.0162 | Grad Norm: 0.01063891\n",
      "Epoch 1 | Step 80600 | Avg Loss: 0.0160 | Grad Norm: 0.00907322\n",
      "Epoch 1 | Step 80700 | Avg Loss: 0.0159 | Grad Norm: 0.00878515\n",
      "Epoch 1 | Step 80800 | Avg Loss: 0.0155 | Grad Norm: 0.00887406\n",
      "Epoch 1 | Step 80900 | Avg Loss: 0.0153 | Grad Norm: 0.00927383\n",
      "Epoch 1 | Step 81000 | Avg Loss: 0.0155 | Grad Norm: 0.00740215\n",
      "Epoch 1 | Step 81100 | Avg Loss: 0.0155 | Grad Norm: 0.01112487\n",
      "Epoch 1 | Step 81200 | Avg Loss: 0.0152 | Grad Norm: 0.00810842\n",
      "Epoch 1 | Step 81300 | Avg Loss: 0.0150 | Grad Norm: 0.00847492\n",
      "Epoch 1 | Step 81400 | Avg Loss: 0.0153 | Grad Norm: 0.01023160\n",
      "Epoch 1 | Step 81500 | Avg Loss: 0.0156 | Grad Norm: 0.00932171\n",
      "Epoch 1 | Step 81600 | Avg Loss: 0.0154 | Grad Norm: 0.00826828\n",
      "Epoch 1 | Step 81700 | Avg Loss: 0.0155 | Grad Norm: 0.00921588\n",
      "Epoch 1 | Step 81800 | Avg Loss: 0.0158 | Grad Norm: 0.00854469\n",
      "Epoch 1 | Step 81900 | Avg Loss: 0.0158 | Grad Norm: 0.00855240\n",
      "Epoch 1 | Step 82000 | Avg Loss: 0.0158 | Grad Norm: 0.00945873\n",
      "Epoch 1 | Step 82100 | Avg Loss: 0.0156 | Grad Norm: 0.00826324\n",
      "Epoch 1 | Step 82200 | Avg Loss: 0.0153 | Grad Norm: 0.00864264\n",
      "Epoch 1 | Step 82300 | Avg Loss: 0.0154 | Grad Norm: 0.00781685\n",
      "Epoch 1 | Step 82400 | Avg Loss: 0.0155 | Grad Norm: 0.00899787\n",
      "Epoch 1 | Step 82500 | Avg Loss: 0.0155 | Grad Norm: 0.01071496\n",
      "Epoch 1 | Step 82600 | Avg Loss: 0.0153 | Grad Norm: 0.00828732\n",
      "Epoch 1 | Step 82700 | Avg Loss: 0.0155 | Grad Norm: 0.00866475\n",
      "Epoch 1 | Step 82800 | Avg Loss: 0.0155 | Grad Norm: 0.00824141\n",
      "Epoch 1 | Step 82900 | Avg Loss: 0.0157 | Grad Norm: 0.00937530\n",
      "Epoch 1 | Step 83000 | Avg Loss: 0.0157 | Grad Norm: 0.00853557\n",
      "Epoch 1 | Step 83100 | Avg Loss: 0.0157 | Grad Norm: 0.00935063\n",
      "Epoch 1 | Step 83200 | Avg Loss: 0.0155 | Grad Norm: 0.00977573\n",
      "Epoch 1 | Step 83300 | Avg Loss: 0.0157 | Grad Norm: 0.00996796\n",
      "Epoch 1 | Step 83400 | Avg Loss: 0.0156 | Grad Norm: 0.00950973\n",
      "Epoch 1 | Step 83500 | Avg Loss: 0.0162 | Grad Norm: 0.00969726\n",
      "Epoch 1 | Step 83600 | Avg Loss: 0.0158 | Grad Norm: 0.00822832\n",
      "Epoch 1 | Step 83700 | Avg Loss: 0.0160 | Grad Norm: 0.00847085\n",
      "Epoch 1 | Step 83800 | Avg Loss: 0.0162 | Grad Norm: 0.00878416\n",
      "Epoch 1 | Step 83900 | Avg Loss: 0.0162 | Grad Norm: 0.01302575\n",
      "Epoch 1 | Step 84000 | Avg Loss: 0.0159 | Grad Norm: 0.00793778\n",
      "Epoch 1 | Step 84100 | Avg Loss: 0.0156 | Grad Norm: 0.00963523\n",
      "Epoch 1 | Step 84200 | Avg Loss: 0.0155 | Grad Norm: 0.00820636\n",
      "Epoch 1 | Step 84300 | Avg Loss: 0.0159 | Grad Norm: 0.00899959\n",
      "Epoch 1 | Step 84400 | Avg Loss: 0.0163 | Grad Norm: 0.00839846\n",
      "Epoch 1 | Step 84500 | Avg Loss: 0.0161 | Grad Norm: 0.00817920\n",
      "Epoch 1 | Step 84600 | Avg Loss: 0.0159 | Grad Norm: 0.01018102\n",
      "Epoch 1 | Step 84700 | Avg Loss: 0.0157 | Grad Norm: 0.00841868\n",
      "Epoch 1 | Step 84800 | Avg Loss: 0.0159 | Grad Norm: 0.00938659\n",
      "Epoch 1 | Step 84900 | Avg Loss: 0.0159 | Grad Norm: 0.00977419\n",
      "Epoch 1 | Step 85000 | Avg Loss: 0.0156 | Grad Norm: 0.00909306\n",
      "Epoch 1 | Step 85100 | Avg Loss: 0.0156 | Grad Norm: 0.00838632\n",
      "Epoch 1 | Step 85200 | Avg Loss: 0.0155 | Grad Norm: 0.00888735\n",
      "Epoch 1 | Step 85300 | Avg Loss: 0.0155 | Grad Norm: 0.01011140\n",
      "Epoch 1 | Step 85400 | Avg Loss: 0.0153 | Grad Norm: 0.00913959\n",
      "Epoch 1 | Step 85500 | Avg Loss: 0.0151 | Grad Norm: 0.00824106\n",
      "Epoch 1 | Step 85600 | Avg Loss: 0.0149 | Grad Norm: 0.00935989\n",
      "Epoch 1 | Step 85700 | Avg Loss: 0.0150 | Grad Norm: 0.00866324\n",
      "Epoch 1 | Step 85800 | Avg Loss: 0.0154 | Grad Norm: 0.00801033\n",
      "Epoch 1 | Step 85900 | Avg Loss: 0.0155 | Grad Norm: 0.00789019\n",
      "Epoch 1 | Step 86000 | Avg Loss: 0.0155 | Grad Norm: 0.01119542\n",
      "Epoch 1 | Step 86100 | Avg Loss: 0.0159 | Grad Norm: 0.00890215\n",
      "Epoch 1 | Step 86200 | Avg Loss: 0.0157 | Grad Norm: 0.00830925\n",
      "Epoch 1 | Step 86300 | Avg Loss: 0.0158 | Grad Norm: 0.00761123\n",
      "Epoch 1 | Step 86400 | Avg Loss: 0.0156 | Grad Norm: 0.00893301\n",
      "Epoch 1 | Step 86500 | Avg Loss: 0.0158 | Grad Norm: 0.00949653\n",
      "Epoch 1 | Step 86600 | Avg Loss: 0.0159 | Grad Norm: 0.00913405\n",
      "Epoch 1 | Step 86700 | Avg Loss: 0.0155 | Grad Norm: 0.00809550\n",
      "Epoch 1 | Step 86800 | Avg Loss: 0.0158 | Grad Norm: 0.00865637\n",
      "Epoch 1 | Step 86900 | Avg Loss: 0.0158 | Grad Norm: 0.00869966\n",
      "Epoch 1 | Step 87000 | Avg Loss: 0.0161 | Grad Norm: 0.00930088\n",
      "Epoch 1 | Step 87100 | Avg Loss: 0.0160 | Grad Norm: 0.00838485\n",
      "Epoch 1 | Step 87200 | Avg Loss: 0.0162 | Grad Norm: 0.00928616\n",
      "Epoch 1 | Step 87300 | Avg Loss: 0.0163 | Grad Norm: 0.00963373\n",
      "Epoch 1 | Step 87400 | Avg Loss: 0.0163 | Grad Norm: 0.00880182\n",
      "Epoch 1 | Step 87500 | Avg Loss: 0.0162 | Grad Norm: 0.00995968\n",
      "Epoch 1 | Step 87600 | Avg Loss: 0.0161 | Grad Norm: 0.01014426\n",
      "Epoch 1 | Step 87700 | Avg Loss: 0.0164 | Grad Norm: 0.00826552\n",
      "Epoch 1 | Step 87800 | Avg Loss: 0.0162 | Grad Norm: 0.00894831\n",
      "Epoch 1 | Step 87900 | Avg Loss: 0.0161 | Grad Norm: 0.00882899\n",
      "Epoch 1 | Step 88000 | Avg Loss: 0.0160 | Grad Norm: 0.00952010\n",
      "Epoch 1 | Step 88100 | Avg Loss: 0.0160 | Grad Norm: 0.00873653\n",
      "Epoch 1 | Step 88200 | Avg Loss: 0.0160 | Grad Norm: 0.00854664\n",
      "Epoch 1 | Step 88300 | Avg Loss: 0.0158 | Grad Norm: 0.00822080\n",
      "Epoch 1 | Step 88400 | Avg Loss: 0.0158 | Grad Norm: 0.00844435\n",
      "Epoch 1 | Step 88500 | Avg Loss: 0.0156 | Grad Norm: 0.00843128\n",
      "Epoch 1 | Step 88600 | Avg Loss: 0.0152 | Grad Norm: 0.01002721\n",
      "Epoch 1 | Step 88700 | Avg Loss: 0.0156 | Grad Norm: 0.00784619\n",
      "Epoch 1 | Step 88800 | Avg Loss: 0.0155 | Grad Norm: 0.00960542\n",
      "Epoch 1 | Step 88900 | Avg Loss: 0.0157 | Grad Norm: 0.00817514\n",
      "Epoch 1 | Step 89000 | Avg Loss: 0.0157 | Grad Norm: 0.00909974\n",
      "Epoch 1 | Step 89100 | Avg Loss: 0.0159 | Grad Norm: 0.00911433\n",
      "Epoch 1 | Step 89200 | Avg Loss: 0.0161 | Grad Norm: 0.00829691\n",
      "Epoch 1 | Step 89300 | Avg Loss: 0.0161 | Grad Norm: 0.00834088\n",
      "Epoch 1 | Step 89400 | Avg Loss: 0.0162 | Grad Norm: 0.00805110\n",
      "Epoch 1 | Step 89500 | Avg Loss: 0.0160 | Grad Norm: 0.00957815\n",
      "Epoch 1 | Step 89600 | Avg Loss: 0.0159 | Grad Norm: 0.01103604\n",
      "Epoch 1 | Step 89700 | Avg Loss: 0.0152 | Grad Norm: 0.00849632\n",
      "Epoch 1 | Step 89800 | Avg Loss: 0.0151 | Grad Norm: 0.00807776\n",
      "Epoch 1 | Step 89900 | Avg Loss: 0.0156 | Grad Norm: 0.00748862\n",
      "Epoch 1 | Step 90000 | Avg Loss: 0.0159 | Grad Norm: 0.00897758\n",
      "Epoch 1 | Step 90100 | Avg Loss: 0.0160 | Grad Norm: 0.00778215\n",
      "Epoch 1 | Step 90200 | Avg Loss: 0.0159 | Grad Norm: 0.01068892\n",
      "Epoch 1 | Step 90300 | Avg Loss: 0.0159 | Grad Norm: 0.00929282\n",
      "Epoch 1 | Step 90400 | Avg Loss: 0.0164 | Grad Norm: 0.00898910\n",
      "Epoch 1 | Step 90500 | Avg Loss: 0.0159 | Grad Norm: 0.01272458\n",
      "Epoch 1 | Step 90600 | Avg Loss: 0.0160 | Grad Norm: 0.00962208\n",
      "Epoch 1 | Step 90700 | Avg Loss: 0.0160 | Grad Norm: 0.00785476\n",
      "Epoch 1 | Step 90800 | Avg Loss: 0.0156 | Grad Norm: 0.00813615\n",
      "Epoch 1 | Step 90900 | Avg Loss: 0.0152 | Grad Norm: 0.00880109\n",
      "Epoch 1 | Step 91000 | Avg Loss: 0.0151 | Grad Norm: 0.00895769\n",
      "Epoch 1 | Step 91100 | Avg Loss: 0.0150 | Grad Norm: 0.00937046\n",
      "Epoch 1 | Step 91200 | Avg Loss: 0.0157 | Grad Norm: 0.00934480\n",
      "Epoch 1 | Step 91300 | Avg Loss: 0.0161 | Grad Norm: 0.01113584\n",
      "Epoch 1 | Step 91400 | Avg Loss: 0.0160 | Grad Norm: 0.00918641\n",
      "Epoch 1 | Step 91500 | Avg Loss: 0.0154 | Grad Norm: 0.00945073\n",
      "Epoch 1 | Step 91600 | Avg Loss: 0.0157 | Grad Norm: 0.00907291\n",
      "Epoch 1 | Step 91700 | Avg Loss: 0.0157 | Grad Norm: 0.00907689\n",
      "Epoch 1 | Step 91800 | Avg Loss: 0.0154 | Grad Norm: 0.00921327\n",
      "Epoch 1 | Step 91900 | Avg Loss: 0.0156 | Grad Norm: 0.00914797\n",
      "Epoch 1 | Step 92000 | Avg Loss: 0.0159 | Grad Norm: 0.00897482\n",
      "Epoch 1 | Step 92100 | Avg Loss: 0.0157 | Grad Norm: 0.00927768\n",
      "Epoch 1 | Step 92200 | Avg Loss: 0.0158 | Grad Norm: 0.00819733\n",
      "Epoch 1 | Step 92300 | Avg Loss: 0.0161 | Grad Norm: 0.00870471\n",
      "Epoch 1 | Step 92400 | Avg Loss: 0.0157 | Grad Norm: 0.00892001\n",
      "Epoch 1 | Step 92500 | Avg Loss: 0.0159 | Grad Norm: 0.00873062\n",
      "Epoch 1 | Step 92600 | Avg Loss: 0.0157 | Grad Norm: 0.00839824\n",
      "Epoch 1 | Step 92700 | Avg Loss: 0.0157 | Grad Norm: 0.00825966\n",
      "Epoch 1 | Step 92800 | Avg Loss: 0.0157 | Grad Norm: 0.00819822\n",
      "Epoch 1 | Step 92900 | Avg Loss: 0.0153 | Grad Norm: 0.00876573\n",
      "Epoch 1 | Step 93000 | Avg Loss: 0.0154 | Grad Norm: 0.00804757\n",
      "Epoch 1 | Step 93100 | Avg Loss: 0.0155 | Grad Norm: 0.00888513\n",
      "Epoch 1 | Step 93200 | Avg Loss: 0.0154 | Grad Norm: 0.00808905\n",
      "Epoch 1 | Step 93300 | Avg Loss: 0.0150 | Grad Norm: 0.00757128\n",
      "Epoch 1 | Step 93400 | Avg Loss: 0.0152 | Grad Norm: 0.00903135\n",
      "Epoch 1 | Step 93500 | Avg Loss: 0.0153 | Grad Norm: 0.00886552\n",
      "Epoch 1 | Step 93600 | Avg Loss: 0.0155 | Grad Norm: 0.00819826\n",
      "Epoch 1 | Step 93700 | Avg Loss: 0.0155 | Grad Norm: 0.00787628\n",
      "Epoch 1 | Step 93800 | Avg Loss: 0.0157 | Grad Norm: 0.00889378\n",
      "Epoch 1 | Step 93900 | Avg Loss: 0.0159 | Grad Norm: 0.00778022\n",
      "Epoch 1 | Step 94000 | Avg Loss: 0.0159 | Grad Norm: 0.00877893\n",
      "Epoch 1 | Step 94100 | Avg Loss: 0.0159 | Grad Norm: 0.00892219\n",
      "Epoch 1 | Step 94200 | Avg Loss: 0.0157 | Grad Norm: 0.00774334\n",
      "Epoch 1 | Step 94300 | Avg Loss: 0.0157 | Grad Norm: 0.00942682\n",
      "Epoch 1 | Step 94400 | Avg Loss: 0.0158 | Grad Norm: 0.00870314\n",
      "Epoch 1 | Step 94500 | Avg Loss: 0.0156 | Grad Norm: 0.00888468\n",
      "Epoch 1 | Step 94600 | Avg Loss: 0.0153 | Grad Norm: 0.00848404\n",
      "Epoch 1 | Step 94700 | Avg Loss: 0.0153 | Grad Norm: 0.00805994\n",
      "Epoch 1 | Step 94800 | Avg Loss: 0.0153 | Grad Norm: 0.01306157\n",
      "Epoch 1 | Step 94900 | Avg Loss: 0.0153 | Grad Norm: 0.00823696\n",
      "Epoch 1 | Step 95000 | Avg Loss: 0.0156 | Grad Norm: 0.00901519\n",
      "Epoch 1 | Step 95100 | Avg Loss: 0.0155 | Grad Norm: 0.01077616\n",
      "Epoch 1 | Step 95200 | Avg Loss: 0.0159 | Grad Norm: 0.01025474\n",
      "Epoch 1 | Step 95300 | Avg Loss: 0.0156 | Grad Norm: 0.00781588\n",
      "Epoch 1 | Step 95400 | Avg Loss: 0.0158 | Grad Norm: 0.00744568\n",
      "Epoch 1 | Step 95500 | Avg Loss: 0.0158 | Grad Norm: 0.00821551\n",
      "Epoch 1 | Step 95600 | Avg Loss: 0.0153 | Grad Norm: 0.00776293\n",
      "Epoch 1 | Step 95700 | Avg Loss: 0.0154 | Grad Norm: 0.01065880\n",
      "Epoch 1 | Step 95800 | Avg Loss: 0.0152 | Grad Norm: 0.00865012\n",
      "Epoch 1 | Step 95900 | Avg Loss: 0.0153 | Grad Norm: 0.00792501\n",
      "Epoch 1 | Step 96000 | Avg Loss: 0.0151 | Grad Norm: 0.00864275\n",
      "Epoch 1 | Step 96100 | Avg Loss: 0.0149 | Grad Norm: 0.00725014\n",
      "Epoch 1 | Step 96200 | Avg Loss: 0.0147 | Grad Norm: 0.00878091\n",
      "Epoch 1 | Step 96300 | Avg Loss: 0.0150 | Grad Norm: 0.01465737\n",
      "Epoch 1 | Step 96400 | Avg Loss: 0.0151 | Grad Norm: 0.01026666\n",
      "Epoch 1 | Step 96500 | Avg Loss: 0.0157 | Grad Norm: 0.00855154\n",
      "Epoch 1 | Step 96600 | Avg Loss: 0.0153 | Grad Norm: 0.01003825\n",
      "Epoch 1 | Step 96700 | Avg Loss: 0.0158 | Grad Norm: 0.00760386\n",
      "Epoch 1 | Step 96800 | Avg Loss: 0.0158 | Grad Norm: 0.00754704\n",
      "Epoch 1 | Step 96900 | Avg Loss: 0.0157 | Grad Norm: 0.00847936\n",
      "Epoch 1 | Step 97000 | Avg Loss: 0.0158 | Grad Norm: 0.00846632\n",
      "Epoch 1 | Step 97100 | Avg Loss: 0.0158 | Grad Norm: 0.00906156\n",
      "Epoch 1 | Step 97200 | Avg Loss: 0.0162 | Grad Norm: 0.01075713\n",
      "Epoch 1 | Step 97300 | Avg Loss: 0.0162 | Grad Norm: 0.00875846\n",
      "Epoch 1 | Step 97400 | Avg Loss: 0.0161 | Grad Norm: 0.00955992\n",
      "Epoch 1 | Step 97500 | Avg Loss: 0.0162 | Grad Norm: 0.01137143\n",
      "Epoch 1 | Step 97600 | Avg Loss: 0.0160 | Grad Norm: 0.00833951\n",
      "Epoch 1 | Step 97700 | Avg Loss: 0.0162 | Grad Norm: 0.01220312\n",
      "Epoch 1 | Step 97800 | Avg Loss: 0.0161 | Grad Norm: 0.00850179\n",
      "Epoch 1 | Step 97900 | Avg Loss: 0.0169 | Grad Norm: 0.01048159\n",
      "Epoch 1 | Step 98000 | Avg Loss: 0.0165 | Grad Norm: 0.00886727\n",
      "Epoch 1 | Step 98100 | Avg Loss: 0.0159 | Grad Norm: 0.00904108\n",
      "Epoch 1 | Step 98200 | Avg Loss: 0.0160 | Grad Norm: 0.00840637\n",
      "Epoch 1 | Step 98300 | Avg Loss: 0.0161 | Grad Norm: 0.00822749\n",
      "Epoch 1 | Step 98400 | Avg Loss: 0.0162 | Grad Norm: 0.00994798\n",
      "Epoch 1 | Step 98500 | Avg Loss: 0.0158 | Grad Norm: 0.00840788\n",
      "Epoch 1 | Step 98600 | Avg Loss: 0.0156 | Grad Norm: 0.00907690\n",
      "Epoch 1 | Step 98700 | Avg Loss: 0.0162 | Grad Norm: 0.01295451\n",
      "Epoch 1 | Step 98800 | Avg Loss: 0.0160 | Grad Norm: 0.01402220\n",
      "Epoch 1 | Step 98900 | Avg Loss: 0.0157 | Grad Norm: 0.00855355\n",
      "Epoch 1 | Step 99000 | Avg Loss: 0.0159 | Grad Norm: 0.00985073\n",
      "Epoch 1 | Step 99100 | Avg Loss: 0.0159 | Grad Norm: 0.00997429\n",
      "Epoch 1 | Step 99200 | Avg Loss: 0.0155 | Grad Norm: 0.00933814\n",
      "Epoch 1 | Step 99300 | Avg Loss: 0.0153 | Grad Norm: 0.00858790\n",
      "Epoch 1 | Step 99400 | Avg Loss: 0.0151 | Grad Norm: 0.00877624\n",
      "Epoch 1 | Step 99500 | Avg Loss: 0.0150 | Grad Norm: 0.00905509\n",
      "Epoch 1 | Step 99600 | Avg Loss: 0.0152 | Grad Norm: 0.01000499\n",
      "Epoch 1 | Step 99700 | Avg Loss: 0.0149 | Grad Norm: 0.00812964\n",
      "Epoch 1 | Step 99800 | Avg Loss: 0.0148 | Grad Norm: 0.00741730\n",
      "Epoch 1 | Step 99900 | Avg Loss: 0.0154 | Grad Norm: 0.00900387\n",
      "Epoch 1 | Step 100000 | Avg Loss: 0.0153 | Grad Norm: 0.00839867\n",
      "Saving model at step100000\n",
      "Epoch 1 | Step 100100 | Avg Loss: 0.0153 | Grad Norm: 0.00867000\n",
      "Epoch 1 | Step 100200 | Avg Loss: 0.0151 | Grad Norm: 0.00809626\n",
      "Epoch 1 | Step 100300 | Avg Loss: 0.0154 | Grad Norm: 0.00829564\n",
      "Epoch 1 | Step 100400 | Avg Loss: 0.0156 | Grad Norm: 0.00799661\n",
      "Epoch 1 | Step 100500 | Avg Loss: 0.0158 | Grad Norm: 0.00981194\n",
      "Epoch 1 | Step 100600 | Avg Loss: 0.0162 | Grad Norm: 0.01424113\n",
      "Epoch 1 | Step 100700 | Avg Loss: 0.0163 | Grad Norm: 0.00850954\n",
      "Epoch 1 | Step 100800 | Avg Loss: 0.0164 | Grad Norm: 0.00893843\n",
      "Epoch 1 | Step 100900 | Avg Loss: 0.0164 | Grad Norm: 0.00766194\n",
      "Epoch 1 | Step 101000 | Avg Loss: 0.0163 | Grad Norm: 0.00999864\n",
      "Epoch 1 | Step 101100 | Avg Loss: 0.0159 | Grad Norm: 0.00787695\n",
      "Epoch 1 | Step 101200 | Avg Loss: 0.0157 | Grad Norm: 0.00915032\n",
      "Epoch 1 | Step 101300 | Avg Loss: 0.0159 | Grad Norm: 0.00876478\n",
      "Epoch 1 | Step 101400 | Avg Loss: 0.0159 | Grad Norm: 0.00796060\n",
      "Epoch 1 | Step 101500 | Avg Loss: 0.0154 | Grad Norm: 0.00740888\n",
      "Epoch 1 | Step 101600 | Avg Loss: 0.0150 | Grad Norm: 0.00871824\n",
      "Epoch 1 | Step 101700 | Avg Loss: 0.0150 | Grad Norm: 0.00951727\n",
      "Epoch 1 | Step 101800 | Avg Loss: 0.0148 | Grad Norm: 0.00825165\n",
      "Epoch 1 | Step 101900 | Avg Loss: 0.0150 | Grad Norm: 0.00769023\n",
      "Epoch 1 | Step 102000 | Avg Loss: 0.0154 | Grad Norm: 0.00783423\n",
      "Epoch 1 | Step 102100 | Avg Loss: 0.0152 | Grad Norm: 0.00735918\n",
      "Epoch 1 | Step 102200 | Avg Loss: 0.0149 | Grad Norm: 0.00718942\n",
      "Epoch 1 | Step 102300 | Avg Loss: 0.0152 | Grad Norm: 0.00871560\n",
      "Epoch 1 | Step 102400 | Avg Loss: 0.0154 | Grad Norm: 0.00914132\n",
      "Epoch 1 | Step 102500 | Avg Loss: 0.0156 | Grad Norm: 0.00928954\n",
      "Epoch 1 | Step 102600 | Avg Loss: 0.0158 | Grad Norm: 0.00942689\n",
      "Epoch 1 | Step 102700 | Avg Loss: 0.0160 | Grad Norm: 0.00975595\n",
      "Epoch 1 | Step 102800 | Avg Loss: 0.0155 | Grad Norm: 0.00932644\n",
      "Epoch 1 | Step 102900 | Avg Loss: 0.0157 | Grad Norm: 0.00758275\n",
      "Epoch 1 | Step 103000 | Avg Loss: 0.0157 | Grad Norm: 0.00824008\n",
      "Epoch 1 | Step 103100 | Avg Loss: 0.0156 | Grad Norm: 0.00973010\n",
      "Epoch 1 | Step 103200 | Avg Loss: 0.0154 | Grad Norm: 0.00907008\n",
      "Epoch 1 | Step 103300 | Avg Loss: 0.0151 | Grad Norm: 0.00965175\n",
      "Epoch 1 | Step 103400 | Avg Loss: 0.0154 | Grad Norm: 0.00810707\n",
      "Epoch 1 | Step 103500 | Avg Loss: 0.0158 | Grad Norm: 0.00742320\n",
      "Epoch 1 | Step 103600 | Avg Loss: 0.0157 | Grad Norm: 0.00971923\n",
      "Epoch 1 | Step 103700 | Avg Loss: 0.0156 | Grad Norm: 0.00905823\n",
      "Epoch 1 | Step 103800 | Avg Loss: 0.0155 | Grad Norm: 0.00939504\n",
      "Epoch 1 | Step 103900 | Avg Loss: 0.0153 | Grad Norm: 0.01005941\n",
      "Epoch 1 | Step 104000 | Avg Loss: 0.0156 | Grad Norm: 0.00900480\n",
      "Epoch 1 | Step 104100 | Avg Loss: 0.0155 | Grad Norm: 0.00886877\n",
      "Epoch 1 | Step 104200 | Avg Loss: 0.0153 | Grad Norm: 0.00882737\n",
      "Epoch 1 | Step 104300 | Avg Loss: 0.0153 | Grad Norm: 0.00841071\n",
      "Epoch 1 | Step 104400 | Avg Loss: 0.0157 | Grad Norm: 0.00855761\n",
      "Epoch 1 | Step 104500 | Avg Loss: 0.0158 | Grad Norm: 0.00869100\n",
      "Epoch 1 | Step 104600 | Avg Loss: 0.0158 | Grad Norm: 0.00842676\n",
      "Epoch 1 | Step 104700 | Avg Loss: 0.0157 | Grad Norm: 0.00984182\n",
      "Epoch 1 | Step 104800 | Avg Loss: 0.0157 | Grad Norm: 0.00987178\n",
      "Epoch 1 | Step 104900 | Avg Loss: 0.0156 | Grad Norm: 0.00908856\n",
      "Epoch 1 | Step 105000 | Avg Loss: 0.0155 | Grad Norm: 0.00935929\n",
      "Epoch 1 | Step 105100 | Avg Loss: 0.0157 | Grad Norm: 0.00910602\n",
      "Epoch 1 | Step 105200 | Avg Loss: 0.0158 | Grad Norm: 0.00763445\n",
      "Epoch 1 | Step 105300 | Avg Loss: 0.0160 | Grad Norm: 0.01077683\n",
      "Epoch 1 | Step 105400 | Avg Loss: 0.0159 | Grad Norm: 0.00905410\n",
      "Epoch 1 | Step 105500 | Avg Loss: 0.0152 | Grad Norm: 0.01168589\n",
      "Epoch 1 | Step 105600 | Avg Loss: 0.0151 | Grad Norm: 0.00919756\n",
      "Epoch 1 | Step 105700 | Avg Loss: 0.0155 | Grad Norm: 0.00787112\n",
      "Epoch 1 | Step 105800 | Avg Loss: 0.0155 | Grad Norm: 0.00904653\n",
      "Epoch 1 | Step 105900 | Avg Loss: 0.0157 | Grad Norm: 0.00854079\n",
      "Epoch 1 | Step 106000 | Avg Loss: 0.0158 | Grad Norm: 0.00803898\n",
      "Epoch 1 | Step 106100 | Avg Loss: 0.0159 | Grad Norm: 0.00825772\n",
      "Epoch 1 | Step 106200 | Avg Loss: 0.0160 | Grad Norm: 0.00881687\n",
      "Epoch 1 | Step 106300 | Avg Loss: 0.0158 | Grad Norm: 0.00822199\n",
      "Epoch 1 | Step 106400 | Avg Loss: 0.0157 | Grad Norm: 0.00841514\n",
      "Epoch 1 | Step 106500 | Avg Loss: 0.0156 | Grad Norm: 0.00882215\n",
      "Epoch 1 | Step 106600 | Avg Loss: 0.0150 | Grad Norm: 0.00894359\n",
      "Epoch 1 | Step 106700 | Avg Loss: 0.0152 | Grad Norm: 0.00932664\n",
      "Epoch 1 | Step 106800 | Avg Loss: 0.0152 | Grad Norm: 0.01025568\n",
      "Epoch 1 | Step 106900 | Avg Loss: 0.0153 | Grad Norm: 0.00720336\n",
      "Epoch 1 | Step 107000 | Avg Loss: 0.0154 | Grad Norm: 0.00864263\n",
      "Epoch 1 | Step 107100 | Avg Loss: 0.0156 | Grad Norm: 0.00840009\n",
      "Epoch 1 | Step 107200 | Avg Loss: 0.0160 | Grad Norm: 0.00854085\n",
      "Epoch 1 | Step 107300 | Avg Loss: 0.0167 | Grad Norm: 0.00863087\n",
      "Epoch 1 | Step 107400 | Avg Loss: 0.0168 | Grad Norm: 0.00904362\n",
      "Epoch 1 | Step 107500 | Avg Loss: 0.0165 | Grad Norm: 0.00950748\n",
      "Epoch 1 | Step 107600 | Avg Loss: 0.0164 | Grad Norm: 0.00852612\n",
      "Epoch 1 | Step 107700 | Avg Loss: 0.0161 | Grad Norm: 0.00825550\n",
      "Epoch 1 | Step 107800 | Avg Loss: 0.0161 | Grad Norm: 0.00973867\n",
      "Epoch 1 | Step 107900 | Avg Loss: 0.0163 | Grad Norm: 0.00844860\n",
      "Epoch 1 | Step 108000 | Avg Loss: 0.0162 | Grad Norm: 0.00876789\n",
      "Epoch 1 | Step 108100 | Avg Loss: 0.0165 | Grad Norm: 0.00809794\n",
      "Epoch 1 | Step 108200 | Avg Loss: 0.0160 | Grad Norm: 0.01054547\n",
      "Epoch 1 | Step 108300 | Avg Loss: 0.0159 | Grad Norm: 0.00889526\n",
      "Epoch 1 | Step 108400 | Avg Loss: 0.0160 | Grad Norm: 0.00871271\n",
      "Epoch 1 | Step 108500 | Avg Loss: 0.0157 | Grad Norm: 0.00852191\n",
      "Epoch 1 | Step 108600 | Avg Loss: 0.0156 | Grad Norm: 0.00922640\n",
      "Epoch 1 | Step 108700 | Avg Loss: 0.0155 | Grad Norm: 0.00862481\n",
      "Epoch 1 | Step 108800 | Avg Loss: 0.0156 | Grad Norm: 0.00851970\n",
      "Epoch 1 | Step 108900 | Avg Loss: 0.0158 | Grad Norm: 0.00741298\n",
      "Epoch 1 | Step 109000 | Avg Loss: 0.0159 | Grad Norm: 0.00881298\n",
      "Epoch 1 | Step 109100 | Avg Loss: 0.0155 | Grad Norm: 0.00977927\n",
      "Epoch 1 | Step 109200 | Avg Loss: 0.0156 | Grad Norm: 0.00779578\n",
      "Epoch 1 | Step 109300 | Avg Loss: 0.0159 | Grad Norm: 0.00849955\n",
      "Epoch 1 | Step 109400 | Avg Loss: 0.0158 | Grad Norm: 0.00752772\n",
      "Epoch 1 | Step 109500 | Avg Loss: 0.0161 | Grad Norm: 0.01133060\n",
      "Epoch 1 | Step 109600 | Avg Loss: 0.0159 | Grad Norm: 0.00858346\n",
      "Epoch 1 | Step 109700 | Avg Loss: 0.0154 | Grad Norm: 0.00780548\n",
      "Epoch 1 | Step 109800 | Avg Loss: 0.0155 | Grad Norm: 0.00882399\n",
      "Epoch 1 | Step 109900 | Avg Loss: 0.0154 | Grad Norm: 0.00870229\n",
      "Epoch 1 | Step 110000 | Avg Loss: 0.0155 | Grad Norm: 0.00963201\n",
      "Epoch 1 | Step 110100 | Avg Loss: 0.0155 | Grad Norm: 0.00855509\n",
      "Epoch 1 | Step 110200 | Avg Loss: 0.0153 | Grad Norm: 0.00934001\n",
      "Epoch 1 | Step 110300 | Avg Loss: 0.0152 | Grad Norm: 0.01072790\n",
      "Epoch 1 | Step 110400 | Avg Loss: 0.0155 | Grad Norm: 0.00956609\n",
      "Epoch 1 | Step 110500 | Avg Loss: 0.0153 | Grad Norm: 0.00872864\n",
      "Epoch 1 | Step 110600 | Avg Loss: 0.0154 | Grad Norm: 0.00812545\n",
      "Epoch 1 | Step 110700 | Avg Loss: 0.0161 | Grad Norm: 0.01085149\n",
      "Epoch 1 | Step 110800 | Avg Loss: 0.0156 | Grad Norm: 0.00843285\n",
      "Epoch 1 | Step 110900 | Avg Loss: 0.0159 | Grad Norm: 0.00785728\n",
      "Epoch 1 | Step 111000 | Avg Loss: 0.0160 | Grad Norm: 0.00906875\n",
      "Epoch 1 | Step 111100 | Avg Loss: 0.0158 | Grad Norm: 0.00877346\n",
      "Epoch 1 | Step 111200 | Avg Loss: 0.0158 | Grad Norm: 0.01113846\n",
      "Epoch 1 | Step 111300 | Avg Loss: 0.0156 | Grad Norm: 0.00895531\n",
      "Epoch 1 | Step 111400 | Avg Loss: 0.0158 | Grad Norm: 0.00951828\n",
      "Epoch 1 | Step 111500 | Avg Loss: 0.0158 | Grad Norm: 0.00928134\n",
      "Epoch 1 | Step 111600 | Avg Loss: 0.0160 | Grad Norm: 0.00809809\n",
      "Epoch 1 | Step 111700 | Avg Loss: 0.0161 | Grad Norm: 0.00774730\n",
      "Epoch 1 | Step 111800 | Avg Loss: 0.0157 | Grad Norm: 0.00810029\n",
      "Epoch 1 | Step 111900 | Avg Loss: 0.0159 | Grad Norm: 0.00801327\n",
      "Epoch 1 | Step 112000 | Avg Loss: 0.0155 | Grad Norm: 0.00812856\n",
      "Epoch 1 | Step 112100 | Avg Loss: 0.0157 | Grad Norm: 0.00904616\n",
      "Epoch 1 | Step 112200 | Avg Loss: 0.0157 | Grad Norm: 0.00921514\n",
      "Epoch 1 | Step 112300 | Avg Loss: 0.0158 | Grad Norm: 0.00923049\n",
      "Epoch 1 | Step 112400 | Avg Loss: 0.0154 | Grad Norm: 0.00835849\n",
      "Epoch 1 | Step 112500 | Avg Loss: 0.0153 | Grad Norm: 0.01104696\n",
      "Epoch 1 | Step 112600 | Avg Loss: 0.0157 | Grad Norm: 0.00700027\n",
      "Epoch 1 | Step 112700 | Avg Loss: 0.0158 | Grad Norm: 0.00730698\n",
      "Epoch 1 | Step 112800 | Avg Loss: 0.0159 | Grad Norm: 0.00915841\n",
      "Epoch 1 | Step 112900 | Avg Loss: 0.0159 | Grad Norm: 0.00913365\n",
      "Epoch 1 | Step 113000 | Avg Loss: 0.0160 | Grad Norm: 0.00862457\n",
      "Epoch 1 | Step 113100 | Avg Loss: 0.0163 | Grad Norm: 0.01124231\n",
      "Epoch 1 | Step 113200 | Avg Loss: 0.0163 | Grad Norm: 0.00863118\n",
      "Epoch 1 | Step 113300 | Avg Loss: 0.0161 | Grad Norm: 0.00813686\n",
      "Epoch 1 | Step 113400 | Avg Loss: 0.0158 | Grad Norm: 0.00844867\n",
      "Epoch 1 | Step 113500 | Avg Loss: 0.0156 | Grad Norm: 0.00811976\n",
      "Epoch 1 | Step 113600 | Avg Loss: 0.0155 | Grad Norm: 0.00904601\n",
      "Epoch 1 | Step 113700 | Avg Loss: 0.0155 | Grad Norm: 0.00851224\n",
      "Epoch 1 | Step 113800 | Avg Loss: 0.0156 | Grad Norm: 0.00939330\n",
      "Epoch 1 | Step 113900 | Avg Loss: 0.0157 | Grad Norm: 0.00792501\n",
      "Epoch 1 | Step 114000 | Avg Loss: 0.0156 | Grad Norm: 0.00843789\n",
      "Epoch 1 | Step 114100 | Avg Loss: 0.0154 | Grad Norm: 0.00792452\n",
      "Epoch 1 | Step 114200 | Avg Loss: 0.0162 | Grad Norm: 0.00905240\n",
      "Epoch 1 | Step 114300 | Avg Loss: 0.0160 | Grad Norm: 0.00774275\n",
      "Epoch 1 | Step 114400 | Avg Loss: 0.0160 | Grad Norm: 0.00815252\n",
      "Epoch 1 | Step 114500 | Avg Loss: 0.0160 | Grad Norm: 0.00841277\n",
      "Epoch 1 | Step 114600 | Avg Loss: 0.0158 | Grad Norm: 0.00884708\n",
      "Epoch 1 | Step 114700 | Avg Loss: 0.0154 | Grad Norm: 0.00982585\n",
      "Epoch 1 | Step 114800 | Avg Loss: 0.0155 | Grad Norm: 0.01152286\n",
      "Epoch 1 | Step 114900 | Avg Loss: 0.0157 | Grad Norm: 0.01086515\n",
      "Epoch 1 | Step 115000 | Avg Loss: 0.0161 | Grad Norm: 0.00836132\n",
      "Epoch 1 | Step 115100 | Avg Loss: 0.0159 | Grad Norm: 0.00961491\n",
      "Epoch 1 | Step 115200 | Avg Loss: 0.0158 | Grad Norm: 0.00802552\n",
      "Epoch 1 | Step 115300 | Avg Loss: 0.0156 | Grad Norm: 0.00802066\n",
      "Epoch 1 | Step 115400 | Avg Loss: 0.0155 | Grad Norm: 0.00875621\n",
      "Epoch 1 | Step 115500 | Avg Loss: 0.0152 | Grad Norm: 0.00892559\n",
      "Epoch 1 | Step 115600 | Avg Loss: 0.0153 | Grad Norm: 0.00866298\n",
      "Epoch 1 | Step 115700 | Avg Loss: 0.0155 | Grad Norm: 0.00879096\n",
      "Epoch 1 | Step 115800 | Avg Loss: 0.0157 | Grad Norm: 0.00884484\n",
      "Epoch 1 | Step 115900 | Avg Loss: 0.0157 | Grad Norm: 0.00933357\n",
      "Epoch 1 | Step 116000 | Avg Loss: 0.0156 | Grad Norm: 0.00781416\n",
      "Epoch 1 | Step 116100 | Avg Loss: 0.0158 | Grad Norm: 0.00856536\n",
      "Epoch 1 | Step 116200 | Avg Loss: 0.0160 | Grad Norm: 0.00863277\n",
      "Epoch 1 | Step 116300 | Avg Loss: 0.0162 | Grad Norm: 0.00944541\n",
      "Epoch 1 | Step 116400 | Avg Loss: 0.0164 | Grad Norm: 0.01027063\n",
      "Epoch 1 | Step 116500 | Avg Loss: 0.0160 | Grad Norm: 0.00837152\n",
      "Epoch 1 | Step 116600 | Avg Loss: 0.0158 | Grad Norm: 0.00836163\n",
      "Epoch 1 | Step 116700 | Avg Loss: 0.0158 | Grad Norm: 0.00981444\n",
      "Epoch 1 | Step 116800 | Avg Loss: 0.0158 | Grad Norm: 0.00823519\n",
      "Epoch 1 | Step 116900 | Avg Loss: 0.0159 | Grad Norm: 0.00875267\n",
      "Epoch 1 | Step 117000 | Avg Loss: 0.0156 | Grad Norm: 0.00945495\n",
      "Epoch 1 | Step 117100 | Avg Loss: 0.0157 | Grad Norm: 0.00924385\n",
      "Epoch 1 | Step 117200 | Avg Loss: 0.0156 | Grad Norm: 0.00871940\n",
      "Epoch 1 | Step 117300 | Avg Loss: 0.0160 | Grad Norm: 0.01023435\n",
      "Epoch 1 | Step 117400 | Avg Loss: 0.0157 | Grad Norm: 0.00875313\n",
      "Epoch 1 | Step 117500 | Avg Loss: 0.0157 | Grad Norm: 0.00780902\n",
      "Epoch 1 | Step 117600 | Avg Loss: 0.0161 | Grad Norm: 0.00899550\n",
      "Epoch 1 | Step 117700 | Avg Loss: 0.0162 | Grad Norm: 0.00920158\n",
      "Epoch 1 | Step 117800 | Avg Loss: 0.0159 | Grad Norm: 0.00872262\n",
      "Epoch 1 | Step 117900 | Avg Loss: 0.0159 | Grad Norm: 0.00911033\n",
      "Epoch 1 | Step 118000 | Avg Loss: 0.0159 | Grad Norm: 0.00761264\n",
      "Epoch 1 | Step 118100 | Avg Loss: 0.0160 | Grad Norm: 0.00961745\n",
      "Epoch 1 | Step 118200 | Avg Loss: 0.0163 | Grad Norm: 0.00944826\n",
      "Epoch 1 | Step 118300 | Avg Loss: 0.0159 | Grad Norm: 0.00816817\n",
      "Epoch 1 | Step 118400 | Avg Loss: 0.0163 | Grad Norm: 0.00763046\n",
      "Epoch 1 | Step 118500 | Avg Loss: 0.0160 | Grad Norm: 0.00870385\n",
      "Epoch 1 | Step 118600 | Avg Loss: 0.0159 | Grad Norm: 0.00902705\n",
      "Epoch 1 | Step 118700 | Avg Loss: 0.0159 | Grad Norm: 0.00978807\n",
      "Epoch 1 | Step 118800 | Avg Loss: 0.0154 | Grad Norm: 0.01129616\n",
      "Epoch 1 | Step 118900 | Avg Loss: 0.0153 | Grad Norm: 0.00700423\n",
      "Epoch 1 | Step 119000 | Avg Loss: 0.0153 | Grad Norm: 0.01046080\n",
      "Epoch 1 | Step 119100 | Avg Loss: 0.0152 | Grad Norm: 0.00961966\n",
      "Epoch 1 | Step 119200 | Avg Loss: 0.0152 | Grad Norm: 0.00888884\n",
      "Epoch 1 | Step 119300 | Avg Loss: 0.0153 | Grad Norm: 0.00809549\n",
      "Epoch 1 | Step 119400 | Avg Loss: 0.0151 | Grad Norm: 0.01011192\n",
      "Epoch 1 | Step 119500 | Avg Loss: 0.0153 | Grad Norm: 0.00883090\n",
      "Epoch 1 | Step 119600 | Avg Loss: 0.0150 | Grad Norm: 0.00799812\n",
      "Epoch 1 | Step 119700 | Avg Loss: 0.0151 | Grad Norm: 0.00866990\n",
      "Epoch 1 | Step 119800 | Avg Loss: 0.0154 | Grad Norm: 0.00763488\n",
      "Epoch 1 | Step 119900 | Avg Loss: 0.0156 | Grad Norm: 0.00852406\n",
      "Epoch 1 | Step 120000 | Avg Loss: 0.0159 | Grad Norm: 0.00893854\n",
      "Epoch 1 | Step 120100 | Avg Loss: 0.0160 | Grad Norm: 0.00827135\n",
      "Epoch 1 | Step 120200 | Avg Loss: 0.0157 | Grad Norm: 0.00806250\n",
      "Epoch 1 | Step 120300 | Avg Loss: 0.0156 | Grad Norm: 0.00817335\n",
      "Epoch 1 | Step 120400 | Avg Loss: 0.0157 | Grad Norm: 0.00864868\n",
      "Epoch 1 | Step 120500 | Avg Loss: 0.0154 | Grad Norm: 0.00900021\n",
      "Epoch 1 | Step 120600 | Avg Loss: 0.0154 | Grad Norm: 0.00831243\n",
      "Epoch 1 | Step 120700 | Avg Loss: 0.0154 | Grad Norm: 0.00888854\n",
      "Epoch 1 | Step 120800 | Avg Loss: 0.0151 | Grad Norm: 0.00920652\n",
      "Epoch 1 | Step 120900 | Avg Loss: 0.0154 | Grad Norm: 0.00875413\n",
      "Epoch 1 | Step 121000 | Avg Loss: 0.0151 | Grad Norm: 0.00862184\n",
      "Epoch 1 | Step 121100 | Avg Loss: 0.0153 | Grad Norm: 0.00824144\n",
      "Epoch 1 | Step 121200 | Avg Loss: 0.0153 | Grad Norm: 0.00911959\n",
      "Epoch 1 | Step 121300 | Avg Loss: 0.0156 | Grad Norm: 0.00946390\n",
      "Epoch 1 | Step 121400 | Avg Loss: 0.0156 | Grad Norm: 0.00837153\n",
      "Epoch 1 | Step 121500 | Avg Loss: 0.0156 | Grad Norm: 0.00991195\n",
      "Epoch 1 | Step 121600 | Avg Loss: 0.0157 | Grad Norm: 0.00914987\n",
      "Epoch 1 | Step 121700 | Avg Loss: 0.0158 | Grad Norm: 0.01099857\n",
      "Epoch 1 | Step 121800 | Avg Loss: 0.0155 | Grad Norm: 0.00846211\n",
      "Epoch 1 | Step 121900 | Avg Loss: 0.0156 | Grad Norm: 0.00927049\n",
      "Epoch 1 | Step 122000 | Avg Loss: 0.0154 | Grad Norm: 0.00816884\n",
      "Epoch 1 | Step 122100 | Avg Loss: 0.0158 | Grad Norm: 0.00898613\n",
      "Epoch 1 | Step 122200 | Avg Loss: 0.0158 | Grad Norm: 0.01023591\n",
      "Epoch 1 | Step 122300 | Avg Loss: 0.0155 | Grad Norm: 0.00898263\n",
      "Epoch 1 | Step 122400 | Avg Loss: 0.0158 | Grad Norm: 0.00706695\n",
      "Epoch 1 | Step 122500 | Avg Loss: 0.0155 | Grad Norm: 0.01004548\n",
      "Epoch 1 | Step 122600 | Avg Loss: 0.0155 | Grad Norm: 0.00925524\n",
      "Epoch 1 | Step 122700 | Avg Loss: 0.0153 | Grad Norm: 0.00832705\n",
      "Epoch 1 | Step 122800 | Avg Loss: 0.0156 | Grad Norm: 0.00959725\n",
      "Epoch 1 | Step 122900 | Avg Loss: 0.0154 | Grad Norm: 0.00902154\n",
      "Epoch 1 | Step 123000 | Avg Loss: 0.0152 | Grad Norm: 0.00949052\n",
      "Epoch 1 | Step 123100 | Avg Loss: 0.0155 | Grad Norm: 0.00853994\n",
      "Epoch 1 | Step 123200 | Avg Loss: 0.0154 | Grad Norm: 0.00964561\n",
      "Epoch 1 | Step 123300 | Avg Loss: 0.0155 | Grad Norm: 0.00838545\n",
      "Epoch 1 | Step 123400 | Avg Loss: 0.0151 | Grad Norm: 0.00905813\n",
      "Epoch 1 | Step 123500 | Avg Loss: 0.0151 | Grad Norm: 0.00743203\n",
      "Epoch 1 | Step 123600 | Avg Loss: 0.0154 | Grad Norm: 0.00867588\n",
      "Epoch 1 | Step 123700 | Avg Loss: 0.0154 | Grad Norm: 0.00911881\n",
      "Epoch 1 | Step 123800 | Avg Loss: 0.0151 | Grad Norm: 0.00795381\n",
      "Epoch 1 | Step 123900 | Avg Loss: 0.0155 | Grad Norm: 0.00889927\n",
      "Epoch 1 | Step 124000 | Avg Loss: 0.0157 | Grad Norm: 0.00826187\n",
      "Epoch 1 | Step 124100 | Avg Loss: 0.0161 | Grad Norm: 0.01165021\n",
      "Epoch 1 | Step 124200 | Avg Loss: 0.0158 | Grad Norm: 0.00911898\n",
      "Epoch 1 | Step 124300 | Avg Loss: 0.0158 | Grad Norm: 0.00800883\n",
      "Epoch 1 | Step 124400 | Avg Loss: 0.0159 | Grad Norm: 0.00849577\n",
      "Epoch 1 | Step 124500 | Avg Loss: 0.0157 | Grad Norm: 0.00810666\n",
      "Epoch 1 | Step 124600 | Avg Loss: 0.0153 | Grad Norm: 0.00864109\n",
      "Epoch 1 | Step 124700 | Avg Loss: 0.0153 | Grad Norm: 0.00892559\n",
      "Epoch 1 | Step 124800 | Avg Loss: 0.0155 | Grad Norm: 0.00880716\n",
      "Epoch 1 | Step 124900 | Avg Loss: 0.0156 | Grad Norm: 0.00719701\n",
      "Epoch 1 | Step 125000 | Avg Loss: 0.0159 | Grad Norm: 0.00804341\n",
      "Epoch 1 | Step 125100 | Avg Loss: 0.0157 | Grad Norm: 0.00838708\n",
      "Epoch 1 | Step 125200 | Avg Loss: 0.0158 | Grad Norm: 0.01150550\n",
      "Epoch 1 | Step 125300 | Avg Loss: 0.0156 | Grad Norm: 0.00844465\n",
      "Epoch 1 | Step 125400 | Avg Loss: 0.0161 | Grad Norm: 0.00867124\n",
      "Epoch 1 | Step 125500 | Avg Loss: 0.0159 | Grad Norm: 0.00997791\n",
      "Epoch 1 | Step 125600 | Avg Loss: 0.0157 | Grad Norm: 0.00787855\n",
      "Epoch 1 | Step 125700 | Avg Loss: 0.0157 | Grad Norm: 0.00925657\n",
      "Epoch 1 | Step 125800 | Avg Loss: 0.0158 | Grad Norm: 0.00888339\n",
      "Epoch 1 | Step 125900 | Avg Loss: 0.0155 | Grad Norm: 0.01079887\n",
      "Epoch 1 | Step 126000 | Avg Loss: 0.0157 | Grad Norm: 0.00924846\n",
      "Epoch 1 | Step 126100 | Avg Loss: 0.0159 | Grad Norm: 0.00845900\n",
      "Epoch 1 | Step 126200 | Avg Loss: 0.0158 | Grad Norm: 0.01026281\n",
      "Epoch 1 | Step 126300 | Avg Loss: 0.0159 | Grad Norm: 0.01131125\n",
      "Epoch 1 | Step 126400 | Avg Loss: 0.0160 | Grad Norm: 0.00973210\n",
      "Epoch 1 | Step 126500 | Avg Loss: 0.0160 | Grad Norm: 0.00885549\n",
      "Epoch 1 | Step 126600 | Avg Loss: 0.0158 | Grad Norm: 0.00866685\n",
      "Epoch 1 | Step 126700 | Avg Loss: 0.0158 | Grad Norm: 0.00930944\n",
      "Epoch 1 | Step 126800 | Avg Loss: 0.0155 | Grad Norm: 0.00786373\n",
      "Epoch 1 | Step 126900 | Avg Loss: 0.0152 | Grad Norm: 0.00979332\n",
      "Epoch 1 | Step 127000 | Avg Loss: 0.0155 | Grad Norm: 0.00904504\n",
      "Epoch 1 | Step 127100 | Avg Loss: 0.0159 | Grad Norm: 0.00888815\n",
      "Epoch 1 | Step 127200 | Avg Loss: 0.0158 | Grad Norm: 0.00873390\n",
      "Epoch 1 | Step 127300 | Avg Loss: 0.0158 | Grad Norm: 0.01015257\n",
      "Epoch 1 | Step 127400 | Avg Loss: 0.0155 | Grad Norm: 0.00859993\n",
      "Epoch 1 | Step 127500 | Avg Loss: 0.0152 | Grad Norm: 0.00859487\n",
      "Epoch 1 | Step 127600 | Avg Loss: 0.0153 | Grad Norm: 0.00794719\n",
      "Epoch 1 | Step 127700 | Avg Loss: 0.0157 | Grad Norm: 0.00784002\n",
      "Epoch 1 | Step 127800 | Avg Loss: 0.0159 | Grad Norm: 0.00952152\n",
      "Epoch 1 | Step 127900 | Avg Loss: 0.0160 | Grad Norm: 0.00860693\n",
      "Epoch 1 | Step 128000 | Avg Loss: 0.0157 | Grad Norm: 0.00971133\n",
      "Epoch 1 | Step 128100 | Avg Loss: 0.0154 | Grad Norm: 0.00869460\n",
      "Epoch 1 | Step 128200 | Avg Loss: 0.0155 | Grad Norm: 0.00899612\n",
      "Epoch 1 | Step 128300 | Avg Loss: 0.0156 | Grad Norm: 0.00811249\n",
      "Epoch 1 | Step 128400 | Avg Loss: 0.0155 | Grad Norm: 0.00910646\n",
      "Epoch 1 | Step 128500 | Avg Loss: 0.0156 | Grad Norm: 0.00856216\n",
      "Epoch 1 | Step 128600 | Avg Loss: 0.0157 | Grad Norm: 0.01030642\n",
      "Epoch 1 | Step 128700 | Avg Loss: 0.0154 | Grad Norm: 0.00900967\n",
      "Epoch 1 | Step 128800 | Avg Loss: 0.0159 | Grad Norm: 0.00943986\n",
      "Epoch 1 | Step 128900 | Avg Loss: 0.0159 | Grad Norm: 0.00863008\n",
      "Epoch 1 | Step 129000 | Avg Loss: 0.0159 | Grad Norm: 0.00864165\n",
      "Epoch 1 | Step 129100 | Avg Loss: 0.0157 | Grad Norm: 0.00787458\n",
      "Epoch 1 | Step 129200 | Avg Loss: 0.0161 | Grad Norm: 0.01103941\n",
      "Epoch 1 | Step 129300 | Avg Loss: 0.0160 | Grad Norm: 0.00973802\n",
      "Epoch 1 | Step 129400 | Avg Loss: 0.0160 | Grad Norm: 0.00896230\n",
      "Epoch 1 | Step 129500 | Avg Loss: 0.0158 | Grad Norm: 0.00882673\n",
      "Epoch 1 | Step 129600 | Avg Loss: 0.0160 | Grad Norm: 0.01073768\n",
      "Epoch 1 | Step 129700 | Avg Loss: 0.0159 | Grad Norm: 0.00874422\n",
      "Epoch 1 | Step 129800 | Avg Loss: 0.0163 | Grad Norm: 0.00838061\n",
      "Epoch 1 | Step 129900 | Avg Loss: 0.0162 | Grad Norm: 0.00787464\n",
      "Epoch 1 | Step 130000 | Avg Loss: 0.0159 | Grad Norm: 0.00923721\n",
      "Epoch 1 | Step 130100 | Avg Loss: 0.0157 | Grad Norm: 0.00889915\n",
      "Epoch 1 | Step 130200 | Avg Loss: 0.0156 | Grad Norm: 0.00918281\n",
      "Epoch 1 | Step 130300 | Avg Loss: 0.0157 | Grad Norm: 0.00899423\n",
      "Epoch 1 | Step 130400 | Avg Loss: 0.0154 | Grad Norm: 0.00835451\n",
      "Epoch 1 | Step 130500 | Avg Loss: 0.0158 | Grad Norm: 0.00900384\n",
      "Epoch 1 | Step 130600 | Avg Loss: 0.0159 | Grad Norm: 0.00863540\n",
      "Epoch 1 | Step 130700 | Avg Loss: 0.0162 | Grad Norm: 0.00884662\n",
      "Epoch 1 | Step 130800 | Avg Loss: 0.0161 | Grad Norm: 0.00831658\n",
      "Epoch 1 | Step 130900 | Avg Loss: 0.0160 | Grad Norm: 0.00955600\n",
      "Epoch 1 | Step 131000 | Avg Loss: 0.0159 | Grad Norm: 0.00835925\n",
      "Epoch 1 | Step 131100 | Avg Loss: 0.0159 | Grad Norm: 0.01207991\n",
      "Epoch 1 | Step 131200 | Avg Loss: 0.0160 | Grad Norm: 0.00873881\n",
      "Epoch 1 | Step 131300 | Avg Loss: 0.0162 | Grad Norm: 0.00854866\n",
      "Epoch 1 | Step 131400 | Avg Loss: 0.0161 | Grad Norm: 0.00804715\n",
      "Epoch 1 | Step 131500 | Avg Loss: 0.0160 | Grad Norm: 0.00891105\n",
      "Epoch 1 | Step 131600 | Avg Loss: 0.0162 | Grad Norm: 0.00862709\n",
      "Epoch 1 | Step 131700 | Avg Loss: 0.0162 | Grad Norm: 0.00811042\n",
      "Epoch 1 | Step 131800 | Avg Loss: 0.0159 | Grad Norm: 0.00942358\n",
      "Epoch 1 | Step 131900 | Avg Loss: 0.0158 | Grad Norm: 0.00968984\n",
      "Epoch 1 | Step 132000 | Avg Loss: 0.0153 | Grad Norm: 0.00866882\n",
      "Epoch 1 | Step 132100 | Avg Loss: 0.0157 | Grad Norm: 0.00967103\n",
      "Epoch 1 | Step 132200 | Avg Loss: 0.0159 | Grad Norm: 0.00818434\n",
      "Epoch 1 | Step 132300 | Avg Loss: 0.0160 | Grad Norm: 0.00849515\n",
      "Epoch 1 | Step 132400 | Avg Loss: 0.0162 | Grad Norm: 0.00835041\n",
      "Epoch 1 | Step 132500 | Avg Loss: 0.0161 | Grad Norm: 0.00847671\n",
      "Epoch 1 | Step 132600 | Avg Loss: 0.0160 | Grad Norm: 0.00939658\n",
      "Epoch 1 | Step 132700 | Avg Loss: 0.0160 | Grad Norm: 0.00878240\n",
      "Epoch 1 | Step 132800 | Avg Loss: 0.0162 | Grad Norm: 0.00864752\n",
      "Epoch 1 | Step 132900 | Avg Loss: 0.0161 | Grad Norm: 0.01002636\n",
      "Epoch 1 | Step 133000 | Avg Loss: 0.0161 | Grad Norm: 0.01120419\n",
      "Epoch 1 | Step 133100 | Avg Loss: 0.0161 | Grad Norm: 0.00843741\n",
      "Epoch 1 | Step 133200 | Avg Loss: 0.0158 | Grad Norm: 0.00748407\n",
      "Epoch 1 | Step 133300 | Avg Loss: 0.0157 | Grad Norm: 0.00804204\n",
      "Epoch 1 | Step 133400 | Avg Loss: 0.0162 | Grad Norm: 0.01123803\n",
      "Epoch 1 | Step 133500 | Avg Loss: 0.0156 | Grad Norm: 0.00889573\n",
      "Epoch 1 | Step 133600 | Avg Loss: 0.0159 | Grad Norm: 0.00820080\n",
      "Epoch 1 | Step 133700 | Avg Loss: 0.0156 | Grad Norm: 0.00840935\n",
      "Epoch 1 | Step 133800 | Avg Loss: 0.0153 | Grad Norm: 0.00922354\n",
      "Epoch 1 | Step 133900 | Avg Loss: 0.0153 | Grad Norm: 0.00879874\n",
      "Epoch 1 | Step 134000 | Avg Loss: 0.0152 | Grad Norm: 0.00890192\n",
      "Epoch 1 | Step 134100 | Avg Loss: 0.0154 | Grad Norm: 0.00926413\n",
      "Epoch 1 | Step 134200 | Avg Loss: 0.0155 | Grad Norm: 0.00746273\n",
      "Epoch 1 | Step 134300 | Avg Loss: 0.0153 | Grad Norm: 0.00907260\n",
      "Epoch 1 | Step 134400 | Avg Loss: 0.0157 | Grad Norm: 0.00922557\n",
      "Epoch 1 | Step 134500 | Avg Loss: 0.0158 | Grad Norm: 0.00842336\n",
      "Epoch 1 | Step 134600 | Avg Loss: 0.0157 | Grad Norm: 0.00949703\n",
      "Epoch 1 | Step 134700 | Avg Loss: 0.0156 | Grad Norm: 0.00747982\n",
      "Epoch 1 | Step 134800 | Avg Loss: 0.0155 | Grad Norm: 0.00808475\n",
      "Epoch 1 | Step 134900 | Avg Loss: 0.0157 | Grad Norm: 0.00876913\n",
      "Epoch 1 | Step 135000 | Avg Loss: 0.0157 | Grad Norm: 0.00880703\n",
      "Epoch 1 | Step 135100 | Avg Loss: 0.0158 | Grad Norm: 0.00935140\n",
      "Epoch 1 | Step 135200 | Avg Loss: 0.0160 | Grad Norm: 0.00864543\n",
      "Epoch 1 | Step 135300 | Avg Loss: 0.0163 | Grad Norm: 0.00896659\n",
      "Epoch 1 | Step 135400 | Avg Loss: 0.0161 | Grad Norm: 0.00842774\n",
      "Epoch 1 | Step 135500 | Avg Loss: 0.0163 | Grad Norm: 0.00838736\n",
      "Epoch 1 | Step 135600 | Avg Loss: 0.0159 | Grad Norm: 0.00720411\n",
      "Epoch 1 | Step 135700 | Avg Loss: 0.0158 | Grad Norm: 0.00846041\n",
      "Epoch 1 | Step 135800 | Avg Loss: 0.0162 | Grad Norm: 0.01077938\n",
      "Epoch 1 | Step 135900 | Avg Loss: 0.0158 | Grad Norm: 0.00818716\n",
      "Epoch 1 | Step 136000 | Avg Loss: 0.0159 | Grad Norm: 0.00881713\n",
      "Epoch 1 | Step 136100 | Avg Loss: 0.0157 | Grad Norm: 0.00869003\n",
      "Epoch 1 | Step 136200 | Avg Loss: 0.0158 | Grad Norm: 0.00910693\n",
      "Epoch 1 | Step 136300 | Avg Loss: 0.0153 | Grad Norm: 0.00865612\n",
      "Epoch 1 | Step 136400 | Avg Loss: 0.0152 | Grad Norm: 0.00902243\n",
      "Epoch 1 | Step 136500 | Avg Loss: 0.0153 | Grad Norm: 0.00878737\n",
      "Epoch 1 | Step 136600 | Avg Loss: 0.0149 | Grad Norm: 0.00906026\n",
      "Epoch 1 | Step 136700 | Avg Loss: 0.0149 | Grad Norm: 0.00874819\n",
      "Epoch 1 | Step 136800 | Avg Loss: 0.0155 | Grad Norm: 0.00855914\n",
      "Epoch 1 | Step 136900 | Avg Loss: 0.0155 | Grad Norm: 0.00983758\n",
      "Epoch 1 | Step 137000 | Avg Loss: 0.0157 | Grad Norm: 0.00816875\n",
      "Epoch 1 | Step 137100 | Avg Loss: 0.0155 | Grad Norm: 0.00796328\n",
      "Epoch 1 | Step 137200 | Avg Loss: 0.0156 | Grad Norm: 0.00812026\n",
      "Epoch 1 | Step 137300 | Avg Loss: 0.0153 | Grad Norm: 0.00942173\n",
      "Epoch 1 | Step 137400 | Avg Loss: 0.0156 | Grad Norm: 0.00861166\n",
      "Epoch 1 | Step 137500 | Avg Loss: 0.0156 | Grad Norm: 0.00912814\n",
      "Epoch 1 | Step 137600 | Avg Loss: 0.0157 | Grad Norm: 0.00738526\n",
      "Epoch 1 | Step 137700 | Avg Loss: 0.0158 | Grad Norm: 0.00847757\n",
      "Epoch 1 | Step 137800 | Avg Loss: 0.0160 | Grad Norm: 0.01041158\n",
      "Epoch 1 | Step 137900 | Avg Loss: 0.0155 | Grad Norm: 0.00890295\n",
      "Epoch 1 | Step 138000 | Avg Loss: 0.0155 | Grad Norm: 0.00945202\n",
      "Epoch 1 | Step 138100 | Avg Loss: 0.0157 | Grad Norm: 0.00804781\n",
      "Epoch 1 | Step 138200 | Avg Loss: 0.0160 | Grad Norm: 0.01100490\n",
      "Epoch 1 | Step 138300 | Avg Loss: 0.0163 | Grad Norm: 0.00876194\n",
      "Epoch 1 | Step 138400 | Avg Loss: 0.0160 | Grad Norm: 0.00830819\n",
      "Epoch 1 | Step 138500 | Avg Loss: 0.0156 | Grad Norm: 0.00816864\n",
      "Epoch 1 | Step 138600 | Avg Loss: 0.0158 | Grad Norm: 0.00917870\n",
      "Epoch 1 | Step 138700 | Avg Loss: 0.0157 | Grad Norm: 0.00754661\n",
      "Epoch 1 | Step 138800 | Avg Loss: 0.0156 | Grad Norm: 0.00862210\n",
      "Epoch 1 | Step 138900 | Avg Loss: 0.0154 | Grad Norm: 0.00858484\n",
      "Epoch 1 | Step 139000 | Avg Loss: 0.0153 | Grad Norm: 0.00841151\n",
      "Epoch 1 | Step 139100 | Avg Loss: 0.0155 | Grad Norm: 0.00992212\n",
      "Epoch 1 | Step 139200 | Avg Loss: 0.0162 | Grad Norm: 0.00754567\n",
      "Epoch 1 | Step 139300 | Avg Loss: 0.0162 | Grad Norm: 0.00834934\n",
      "Epoch 1 | Step 139400 | Avg Loss: 0.0159 | Grad Norm: 0.00816477\n",
      "Epoch 1 | Step 139500 | Avg Loss: 0.0158 | Grad Norm: 0.00794923\n",
      "Epoch 1 | Step 139600 | Avg Loss: 0.0160 | Grad Norm: 0.00916558\n",
      "Epoch 1 | Step 139700 | Avg Loss: 0.0161 | Grad Norm: 0.00892802\n",
      "Epoch 1 | Step 139800 | Avg Loss: 0.0162 | Grad Norm: 0.00806790\n",
      "Epoch 1 | Step 139900 | Avg Loss: 0.0158 | Grad Norm: 0.00811551\n",
      "Epoch 1 | Step 140000 | Avg Loss: 0.0157 | Grad Norm: 0.00952662\n",
      "Epoch 1 | Step 140100 | Avg Loss: 0.0153 | Grad Norm: 0.00877968\n",
      "Epoch 1 | Step 140200 | Avg Loss: 0.0149 | Grad Norm: 0.01010156\n",
      "Epoch 1 | Step 140300 | Avg Loss: 0.0153 | Grad Norm: 0.00896466\n",
      "Epoch 1 | Step 140400 | Avg Loss: 0.0154 | Grad Norm: 0.00769579\n",
      "Epoch 1 | Step 140500 | Avg Loss: 0.0159 | Grad Norm: 0.00958196\n",
      "Epoch 1 | Step 140600 | Avg Loss: 0.0158 | Grad Norm: 0.00852549\n",
      "Epoch 1 | Step 140700 | Avg Loss: 0.0164 | Grad Norm: 0.00784023\n",
      "Epoch 1 | Step 140800 | Avg Loss: 0.0161 | Grad Norm: 0.00947342\n",
      "Epoch 1 | Step 140900 | Avg Loss: 0.0162 | Grad Norm: 0.00819808\n",
      "Epoch 1 | Step 141000 | Avg Loss: 0.0159 | Grad Norm: 0.00929823\n",
      "Epoch 1 | Step 141100 | Avg Loss: 0.0163 | Grad Norm: 0.00860566\n",
      "Epoch 1 | Step 141200 | Avg Loss: 0.0159 | Grad Norm: 0.01049152\n",
      "Epoch 1 | Step 141300 | Avg Loss: 0.0159 | Grad Norm: 0.00844134\n",
      "Epoch 1 | Step 141400 | Avg Loss: 0.0157 | Grad Norm: 0.01008039\n",
      "Epoch 1 | Step 141500 | Avg Loss: 0.0153 | Grad Norm: 0.00788271\n",
      "Epoch 1 | Step 141600 | Avg Loss: 0.0154 | Grad Norm: 0.00843375\n",
      "Epoch 1 | Step 141700 | Avg Loss: 0.0151 | Grad Norm: 0.00821083\n",
      "Epoch 1 | Step 141800 | Avg Loss: 0.0153 | Grad Norm: 0.00760298\n",
      "Epoch 1 | Step 141900 | Avg Loss: 0.0152 | Grad Norm: 0.00743696\n",
      "Epoch 1 | Step 142000 | Avg Loss: 0.0153 | Grad Norm: 0.00955428\n",
      "Epoch 1 | Step 142100 | Avg Loss: 0.0155 | Grad Norm: 0.00986833\n",
      "Epoch 1 | Step 142200 | Avg Loss: 0.0157 | Grad Norm: 0.00783998\n",
      "Epoch 1 | Step 142300 | Avg Loss: 0.0159 | Grad Norm: 0.00773040\n",
      "Epoch 1 | Step 142400 | Avg Loss: 0.0161 | Grad Norm: 0.00888182\n",
      "Epoch 1 | Step 142500 | Avg Loss: 0.0160 | Grad Norm: 0.00812158\n",
      "Epoch 1 | Step 142600 | Avg Loss: 0.0150 | Grad Norm: 0.00867233\n",
      "Epoch 1 | Step 142700 | Avg Loss: 0.0152 | Grad Norm: 0.01036016\n",
      "Epoch 1 | Step 142800 | Avg Loss: 0.0150 | Grad Norm: 0.00874549\n",
      "Epoch 1 | Step 142900 | Avg Loss: 0.0148 | Grad Norm: 0.00840797\n",
      "Epoch 1 | Step 143000 | Avg Loss: 0.0154 | Grad Norm: 0.00777307\n",
      "Epoch 1 | Step 143100 | Avg Loss: 0.0155 | Grad Norm: 0.00940552\n",
      "Epoch 1 | Step 143200 | Avg Loss: 0.0156 | Grad Norm: 0.00884163\n",
      "Epoch 1 | Step 143300 | Avg Loss: 0.0153 | Grad Norm: 0.01214449\n",
      "Epoch 1 | Step 143400 | Avg Loss: 0.0155 | Grad Norm: 0.00874753\n",
      "Epoch 1 | Step 143500 | Avg Loss: 0.0154 | Grad Norm: 0.00881486\n",
      "Epoch 1 | Step 143600 | Avg Loss: 0.0155 | Grad Norm: 0.00870817\n",
      "Epoch 1 | Step 143700 | Avg Loss: 0.0154 | Grad Norm: 0.00769683\n",
      "Epoch 1 | Step 143800 | Avg Loss: 0.0153 | Grad Norm: 0.00720752\n",
      "Epoch 1 | Step 143900 | Avg Loss: 0.0155 | Grad Norm: 0.00962727\n",
      "Epoch 1 | Step 144000 | Avg Loss: 0.0154 | Grad Norm: 0.00849473\n",
      "Epoch 1 | Step 144100 | Avg Loss: 0.0153 | Grad Norm: 0.00936454\n",
      "Epoch 1 | Step 144200 | Avg Loss: 0.0156 | Grad Norm: 0.00752709\n",
      "Epoch 1 | Step 144300 | Avg Loss: 0.0158 | Grad Norm: 0.00924118\n",
      "Epoch 1 | Step 144400 | Avg Loss: 0.0163 | Grad Norm: 0.00806274\n",
      "Epoch 1 | Step 144500 | Avg Loss: 0.0162 | Grad Norm: 0.01051837\n",
      "Epoch 1 | Step 144600 | Avg Loss: 0.0160 | Grad Norm: 0.00965741\n",
      "Epoch 1 | Step 144700 | Avg Loss: 0.0163 | Grad Norm: 0.01092625\n",
      "Epoch 1 | Step 144800 | Avg Loss: 0.0161 | Grad Norm: 0.00845775\n",
      "Epoch 1 | Step 144900 | Avg Loss: 0.0164 | Grad Norm: 0.01320113\n",
      "Epoch 1 | Step 145000 | Avg Loss: 0.0159 | Grad Norm: 0.00893091\n",
      "Epoch 1 | Step 145100 | Avg Loss: 0.0155 | Grad Norm: 0.00985550\n",
      "Epoch 1 | Step 145200 | Avg Loss: 0.0154 | Grad Norm: 0.00816302\n",
      "Epoch 1 | Step 145300 | Avg Loss: 0.0157 | Grad Norm: 0.00758549\n",
      "Epoch 1 | Step 145400 | Avg Loss: 0.0158 | Grad Norm: 0.00806592\n",
      "Epoch 1 | Step 145500 | Avg Loss: 0.0160 | Grad Norm: 0.00960012\n",
      "Epoch 1 | Step 145600 | Avg Loss: 0.0161 | Grad Norm: 0.00954936\n",
      "Epoch 1 | Step 145700 | Avg Loss: 0.0157 | Grad Norm: 0.01680143\n",
      "Epoch 1 | Step 145800 | Avg Loss: 0.0157 | Grad Norm: 0.00835553\n",
      "Epoch 1 | Step 145900 | Avg Loss: 0.0156 | Grad Norm: 0.00828271\n",
      "Epoch 1 | Step 146000 | Avg Loss: 0.0158 | Grad Norm: 0.01026797\n",
      "Epoch 1 | Step 146100 | Avg Loss: 0.0158 | Grad Norm: 0.00863371\n",
      "Epoch 1 | Step 146200 | Avg Loss: 0.0157 | Grad Norm: 0.01053559\n",
      "Epoch 1 | Step 146300 | Avg Loss: 0.0157 | Grad Norm: 0.01101191\n",
      "Epoch 1 | Step 146400 | Avg Loss: 0.0158 | Grad Norm: 0.00824674\n",
      "Epoch 1 | Step 146500 | Avg Loss: 0.0155 | Grad Norm: 0.00898018\n",
      "Epoch 1 | Step 146600 | Avg Loss: 0.0151 | Grad Norm: 0.00994487\n",
      "Epoch 1 | Step 146700 | Avg Loss: 0.0152 | Grad Norm: 0.00883217\n",
      "Epoch 1 | Step 146800 | Avg Loss: 0.0152 | Grad Norm: 0.00908938\n",
      "Epoch 1 | Step 146900 | Avg Loss: 0.0155 | Grad Norm: 0.01226223\n",
      "Epoch 1 | Step 147000 | Avg Loss: 0.0161 | Grad Norm: 0.00857656\n",
      "Epoch 1 | Step 147100 | Avg Loss: 0.0163 | Grad Norm: 0.00915489\n",
      "Epoch 1 | Step 147200 | Avg Loss: 0.0166 | Grad Norm: 0.00832420\n",
      "Epoch 1 | Step 147300 | Avg Loss: 0.0162 | Grad Norm: 0.01071868\n",
      "Epoch 1 | Step 147400 | Avg Loss: 0.0162 | Grad Norm: 0.00897260\n",
      "Epoch 1 | Step 147500 | Avg Loss: 0.0158 | Grad Norm: 0.00887461\n",
      "Epoch 1 | Step 147600 | Avg Loss: 0.0157 | Grad Norm: 0.00819911\n",
      "Epoch 1 | Step 147700 | Avg Loss: 0.0155 | Grad Norm: 0.00877888\n",
      "Epoch 1 | Step 147800 | Avg Loss: 0.0156 | Grad Norm: 0.00841378\n",
      "Epoch 1 | Step 147900 | Avg Loss: 0.0156 | Grad Norm: 0.00849937\n",
      "Epoch 1 | Step 148000 | Avg Loss: 0.0158 | Grad Norm: 0.01010125\n",
      "Epoch 1 | Step 148100 | Avg Loss: 0.0155 | Grad Norm: 0.00859604\n",
      "Epoch 1 | Step 148200 | Avg Loss: 0.0156 | Grad Norm: 0.00841995\n",
      "Epoch 1 | Step 148300 | Avg Loss: 0.0156 | Grad Norm: 0.00760079\n",
      "Epoch 1 | Step 148400 | Avg Loss: 0.0155 | Grad Norm: 0.00867732\n",
      "Epoch 1 | Step 148500 | Avg Loss: 0.0158 | Grad Norm: 0.00983281\n",
      "Epoch 1 | Step 148600 | Avg Loss: 0.0160 | Grad Norm: 0.00885169\n",
      "Epoch 1 | Step 148700 | Avg Loss: 0.0160 | Grad Norm: 0.00906299\n",
      "Epoch 1 | Step 148800 | Avg Loss: 0.0159 | Grad Norm: 0.00969550\n",
      "Epoch 1 | Step 148900 | Avg Loss: 0.0157 | Grad Norm: 0.01030718\n",
      "Epoch 1 | Step 149000 | Avg Loss: 0.0157 | Grad Norm: 0.01041729\n",
      "Epoch 1 | Step 149100 | Avg Loss: 0.0159 | Grad Norm: 0.00805560\n",
      "Epoch 1 | Step 149200 | Avg Loss: 0.0156 | Grad Norm: 0.00840989\n",
      "Epoch 1 | Step 149300 | Avg Loss: 0.0156 | Grad Norm: 0.00936447\n",
      "Epoch 1 | Step 149400 | Avg Loss: 0.0157 | Grad Norm: 0.00984766\n",
      "Epoch 1 | Step 149500 | Avg Loss: 0.0156 | Grad Norm: 0.00838437\n",
      "Epoch 1 | Step 149600 | Avg Loss: 0.0154 | Grad Norm: 0.00833620\n",
      "Epoch 1 | Step 149700 | Avg Loss: 0.0155 | Grad Norm: 0.00913940\n",
      "Epoch 1 | Step 149800 | Avg Loss: 0.0156 | Grad Norm: 0.00926829\n",
      "Epoch 1 | Step 149900 | Avg Loss: 0.0149 | Grad Norm: 0.00801395\n",
      "Epoch 1 | Step 150000 | Avg Loss: 0.0152 | Grad Norm: 0.00917854\n",
      "Epoch 1 | Step 150100 | Avg Loss: 0.0152 | Grad Norm: 0.01200078\n",
      "Epoch 1 | Step 150200 | Avg Loss: 0.0155 | Grad Norm: 0.00895447\n",
      "Epoch 1 | Step 150300 | Avg Loss: 0.0155 | Grad Norm: 0.00775216\n",
      "Epoch 1 | Step 150400 | Avg Loss: 0.0155 | Grad Norm: 0.00877332\n",
      "Epoch 1 | Step 150500 | Avg Loss: 0.0157 | Grad Norm: 0.00853211\n",
      "Epoch 1 | Step 150600 | Avg Loss: 0.0158 | Grad Norm: 0.00935448\n",
      "Epoch 1 | Step 150700 | Avg Loss: 0.0153 | Grad Norm: 0.01002521\n",
      "Epoch 1 | Step 150800 | Avg Loss: 0.0153 | Grad Norm: 0.00855093\n",
      "Epoch 1 | Step 150900 | Avg Loss: 0.0152 | Grad Norm: 0.00906920\n",
      "Epoch 1 | Step 151000 | Avg Loss: 0.0151 | Grad Norm: 0.00800827\n",
      "Epoch 1 | Step 151100 | Avg Loss: 0.0151 | Grad Norm: 0.00828464\n",
      "Epoch 1 | Step 151200 | Avg Loss: 0.0149 | Grad Norm: 0.00775154\n",
      "Epoch 1 | Step 151300 | Avg Loss: 0.0150 | Grad Norm: 0.00919601\n",
      "Epoch 1 | Step 151400 | Avg Loss: 0.0148 | Grad Norm: 0.00809912\n",
      "Epoch 1 | Step 151500 | Avg Loss: 0.0148 | Grad Norm: 0.00848431\n",
      "Epoch 1 | Step 151600 | Avg Loss: 0.0149 | Grad Norm: 0.00821048\n",
      "Epoch 1 | Step 151700 | Avg Loss: 0.0150 | Grad Norm: 0.00951396\n",
      "Epoch 1 | Step 151800 | Avg Loss: 0.0153 | Grad Norm: 0.00797439\n",
      "Epoch 1 | Step 151900 | Avg Loss: 0.0152 | Grad Norm: 0.00843223\n",
      "Epoch 1 | Step 152000 | Avg Loss: 0.0151 | Grad Norm: 0.01131417\n",
      "Epoch 1 | Step 152100 | Avg Loss: 0.0152 | Grad Norm: 0.00914967\n",
      "Epoch 1 | Step 152200 | Avg Loss: 0.0153 | Grad Norm: 0.00876422\n",
      "Epoch 1 | Step 152300 | Avg Loss: 0.0155 | Grad Norm: 0.00891553\n",
      "Epoch 1 | Step 152400 | Avg Loss: 0.0157 | Grad Norm: 0.00852317\n",
      "Epoch 1 | Step 152500 | Avg Loss: 0.0156 | Grad Norm: 0.00878135\n",
      "Epoch 1 | Step 152600 | Avg Loss: 0.0158 | Grad Norm: 0.00896559\n",
      "Epoch 1 | Step 152700 | Avg Loss: 0.0158 | Grad Norm: 0.00901317\n",
      "Epoch 1 | Step 152800 | Avg Loss: 0.0157 | Grad Norm: 0.00950331\n",
      "Epoch 1 | Step 152900 | Avg Loss: 0.0156 | Grad Norm: 0.01209709\n",
      "Epoch 1 | Step 153000 | Avg Loss: 0.0153 | Grad Norm: 0.00880019\n",
      "Epoch 1 | Step 153100 | Avg Loss: 0.0155 | Grad Norm: 0.00770808\n",
      "Epoch 1 | Step 153200 | Avg Loss: 0.0150 | Grad Norm: 0.00898058\n",
      "Epoch 1 | Step 153300 | Avg Loss: 0.0152 | Grad Norm: 0.00717812\n",
      "Epoch 1 | Step 153400 | Avg Loss: 0.0157 | Grad Norm: 0.00867873\n",
      "Epoch 1 | Step 153500 | Avg Loss: 0.0157 | Grad Norm: 0.00809146\n",
      "Epoch 1 | Step 153600 | Avg Loss: 0.0152 | Grad Norm: 0.00797645\n",
      "Epoch 1 | Step 153700 | Avg Loss: 0.0152 | Grad Norm: 0.00865903\n",
      "Epoch 1 | Step 153800 | Avg Loss: 0.0156 | Grad Norm: 0.01005534\n",
      "Epoch 1 | Step 153900 | Avg Loss: 0.0157 | Grad Norm: 0.01093675\n",
      "Epoch 1 | Step 154000 | Avg Loss: 0.0161 | Grad Norm: 0.00805790\n",
      "Epoch 1 | Step 154100 | Avg Loss: 0.0159 | Grad Norm: 0.00952969\n",
      "Epoch 1 | Step 154200 | Avg Loss: 0.0159 | Grad Norm: 0.00932269\n",
      "Epoch 1 | Step 154300 | Avg Loss: 0.0157 | Grad Norm: 0.00889122\n",
      "Epoch 1 | Step 154400 | Avg Loss: 0.0154 | Grad Norm: 0.00791112\n",
      "Epoch 1 | Step 154500 | Avg Loss: 0.0148 | Grad Norm: 0.00964924\n",
      "Epoch 1 | Step 154600 | Avg Loss: 0.0149 | Grad Norm: 0.00812056\n",
      "Epoch 1 | Step 154700 | Avg Loss: 0.0153 | Grad Norm: 0.00782748\n",
      "Epoch 1 | Step 154800 | Avg Loss: 0.0156 | Grad Norm: 0.00790989\n",
      "Epoch 1 | Step 154900 | Avg Loss: 0.0161 | Grad Norm: 0.00828805\n",
      "Epoch 1 | Step 155000 | Avg Loss: 0.0156 | Grad Norm: 0.00873992\n",
      "Epoch 1 | Step 155100 | Avg Loss: 0.0160 | Grad Norm: 0.00810778\n",
      "Epoch 1 | Step 155200 | Avg Loss: 0.0157 | Grad Norm: 0.00743057\n",
      "Epoch 1 | Step 155300 | Avg Loss: 0.0154 | Grad Norm: 0.00901206\n",
      "Epoch 1 | Step 155400 | Avg Loss: 0.0154 | Grad Norm: 0.00800770\n",
      "Epoch 1 | Step 155500 | Avg Loss: 0.0152 | Grad Norm: 0.00791810\n",
      "Epoch 1 | Step 155600 | Avg Loss: 0.0150 | Grad Norm: 0.00786424\n",
      "Epoch 1 | Step 155700 | Avg Loss: 0.0149 | Grad Norm: 0.00785219\n",
      "Epoch 1 | Step 155800 | Avg Loss: 0.0148 | Grad Norm: 0.00919532\n",
      "Epoch 1 | Step 155900 | Avg Loss: 0.0147 | Grad Norm: 0.00927188\n",
      "Epoch 1 | Step 156000 | Avg Loss: 0.0149 | Grad Norm: 0.00951525\n",
      "Epoch 1 | Step 156100 | Avg Loss: 0.0153 | Grad Norm: 0.00798355\n",
      "Epoch 1 | Step 156200 | Avg Loss: 0.0158 | Grad Norm: 0.00833224\n",
      "Epoch 1 | Step 156300 | Avg Loss: 0.0157 | Grad Norm: 0.00987069\n",
      "Epoch 1 | Step 156400 | Avg Loss: 0.0160 | Grad Norm: 0.00783067\n",
      "Epoch 1 | Step 156500 | Avg Loss: 0.0159 | Grad Norm: 0.00834766\n",
      "Epoch 1 | Step 156600 | Avg Loss: 0.0157 | Grad Norm: 0.00852890\n",
      "Epoch 1 | Step 156700 | Avg Loss: 0.0155 | Grad Norm: 0.00825414\n",
      "Epoch 1 | Step 156800 | Avg Loss: 0.0153 | Grad Norm: 0.00798337\n",
      "Epoch 1 | Step 156900 | Avg Loss: 0.0152 | Grad Norm: 0.00928352\n",
      "Epoch 1 | Step 157000 | Avg Loss: 0.0154 | Grad Norm: 0.01127276\n",
      "Epoch 1 | Step 157100 | Avg Loss: 0.0155 | Grad Norm: 0.00811112\n",
      "Epoch 1 | Step 157200 | Avg Loss: 0.0155 | Grad Norm: 0.00814113\n",
      "Epoch 1 | Step 157300 | Avg Loss: 0.0153 | Grad Norm: 0.00813866\n",
      "Epoch 1 | Step 157400 | Avg Loss: 0.0160 | Grad Norm: 0.00782064\n",
      "Epoch 1 | Step 157500 | Avg Loss: 0.0157 | Grad Norm: 0.00853459\n",
      "Epoch 1 | Step 157600 | Avg Loss: 0.0155 | Grad Norm: 0.01318987\n",
      "Epoch 1 | Step 157700 | Avg Loss: 0.0154 | Grad Norm: 0.00954523\n",
      "Epoch 1 | Step 157800 | Avg Loss: 0.0155 | Grad Norm: 0.00866477\n",
      "Epoch 1 | Step 157900 | Avg Loss: 0.0153 | Grad Norm: 0.00900487\n",
      "Epoch 1 | Step 158000 | Avg Loss: 0.0155 | Grad Norm: 0.00848671\n",
      "Epoch 1 | Step 158100 | Avg Loss: 0.0155 | Grad Norm: 0.00775636\n",
      "Epoch 1 | Step 158200 | Avg Loss: 0.0155 | Grad Norm: 0.00901895\n",
      "Epoch 1 | Step 158300 | Avg Loss: 0.0156 | Grad Norm: 0.00801832\n",
      "Epoch 1 | Step 158400 | Avg Loss: 0.0158 | Grad Norm: 0.00795818\n",
      "Epoch 1 | Step 158500 | Avg Loss: 0.0157 | Grad Norm: 0.00962686\n",
      "Epoch 1 | Step 158600 | Avg Loss: 0.0157 | Grad Norm: 0.00820692\n",
      "Epoch 1 | Step 158700 | Avg Loss: 0.0158 | Grad Norm: 0.00875307\n",
      "Epoch 1 | Step 158800 | Avg Loss: 0.0158 | Grad Norm: 0.00952328\n",
      "Epoch 1 | Step 158900 | Avg Loss: 0.0158 | Grad Norm: 0.00911308\n",
      "Epoch 1 | Step 159000 | Avg Loss: 0.0158 | Grad Norm: 0.00805168\n",
      "Epoch 1 | Step 159100 | Avg Loss: 0.0156 | Grad Norm: 0.00912476\n",
      "Epoch 1 | Step 159200 | Avg Loss: 0.0157 | Grad Norm: 0.00885017\n",
      "Epoch 1 | Step 159300 | Avg Loss: 0.0162 | Grad Norm: 0.00864977\n",
      "Epoch 1 | Step 159400 | Avg Loss: 0.0159 | Grad Norm: 0.00904947\n",
      "Epoch 1 | Step 159500 | Avg Loss: 0.0159 | Grad Norm: 0.00904862\n",
      "Epoch 1 | Step 159600 | Avg Loss: 0.0158 | Grad Norm: 0.00957224\n",
      "Epoch 1 | Step 159700 | Avg Loss: 0.0161 | Grad Norm: 0.00819073\n",
      "Epoch 1 | Step 159800 | Avg Loss: 0.0162 | Grad Norm: 0.00860825\n",
      "Epoch 1 | Step 159900 | Avg Loss: 0.0161 | Grad Norm: 0.00904385\n",
      "Epoch 1 | Step 160000 | Avg Loss: 0.0157 | Grad Norm: 0.00869222\n",
      "Epoch 1 | Step 160100 | Avg Loss: 0.0156 | Grad Norm: 0.00862037\n",
      "Epoch 1 | Step 160200 | Avg Loss: 0.0158 | Grad Norm: 0.00788506\n",
      "Epoch 1 | Step 160300 | Avg Loss: 0.0154 | Grad Norm: 0.00748602\n",
      "Epoch 1 | Step 160400 | Avg Loss: 0.0156 | Grad Norm: 0.00813604\n",
      "Epoch 1 | Step 160500 | Avg Loss: 0.0158 | Grad Norm: 0.01023543\n",
      "Epoch 1 | Step 160600 | Avg Loss: 0.0156 | Grad Norm: 0.00902102\n",
      "Epoch 1 | Step 160700 | Avg Loss: 0.0155 | Grad Norm: 0.00907397\n",
      "Epoch 1 | Step 160800 | Avg Loss: 0.0154 | Grad Norm: 0.00814444\n",
      "Epoch 1 | Step 160900 | Avg Loss: 0.0154 | Grad Norm: 0.00872193\n",
      "Epoch 1 | Step 161000 | Avg Loss: 0.0151 | Grad Norm: 0.00756574\n",
      "Epoch 1 | Step 161100 | Avg Loss: 0.0153 | Grad Norm: 0.00834533\n",
      "Epoch 1 | Step 161200 | Avg Loss: 0.0153 | Grad Norm: 0.00778333\n",
      "Epoch 1 | Step 161300 | Avg Loss: 0.0156 | Grad Norm: 0.00833078\n",
      "Epoch 1 | Step 161400 | Avg Loss: 0.0153 | Grad Norm: 0.00936058\n",
      "Epoch 1 | Step 161500 | Avg Loss: 0.0154 | Grad Norm: 0.01397729\n",
      "Epoch 1 | Step 161600 | Avg Loss: 0.0153 | Grad Norm: 0.01051225\n",
      "Epoch 1 | Step 161700 | Avg Loss: 0.0159 | Grad Norm: 0.00886022\n",
      "Epoch 1 | Step 161800 | Avg Loss: 0.0155 | Grad Norm: 0.00965659\n",
      "Epoch 1 | Step 161900 | Avg Loss: 0.0151 | Grad Norm: 0.00847062\n",
      "Epoch 1 | Step 162000 | Avg Loss: 0.0151 | Grad Norm: 0.01246723\n",
      "Epoch 1 | Step 162100 | Avg Loss: 0.0155 | Grad Norm: 0.00784254\n",
      "Epoch 1 | Step 162200 | Avg Loss: 0.0159 | Grad Norm: 0.00847491\n",
      "Epoch 1 | Step 162300 | Avg Loss: 0.0153 | Grad Norm: 0.00823372\n",
      "Epoch 1 | Step 162400 | Avg Loss: 0.0154 | Grad Norm: 0.00770964\n",
      "Epoch 1 | Step 162500 | Avg Loss: 0.0150 | Grad Norm: 0.00982034\n",
      "Epoch 1 | Step 162600 | Avg Loss: 0.0154 | Grad Norm: 0.00910637\n",
      "Epoch 1 | Step 162700 | Avg Loss: 0.0156 | Grad Norm: 0.01088903\n",
      "Epoch 1 | Step 162800 | Avg Loss: 0.0160 | Grad Norm: 0.00827395\n",
      "Epoch 1 | Step 162900 | Avg Loss: 0.0159 | Grad Norm: 0.00832241\n",
      "Epoch 1 | Step 163000 | Avg Loss: 0.0157 | Grad Norm: 0.00805924\n",
      "Epoch 1 | Step 163100 | Avg Loss: 0.0161 | Grad Norm: 0.00874912\n",
      "Epoch 1 | Step 163200 | Avg Loss: 0.0159 | Grad Norm: 0.00874736\n",
      "Epoch 1 | Step 163300 | Avg Loss: 0.0157 | Grad Norm: 0.00744410\n",
      "Epoch 1 | Step 163400 | Avg Loss: 0.0160 | Grad Norm: 0.00915247\n",
      "Epoch 1 | Step 163500 | Avg Loss: 0.0161 | Grad Norm: 0.01025964\n",
      "Epoch 1 | Step 163600 | Avg Loss: 0.0156 | Grad Norm: 0.00921915\n",
      "Epoch 1 | Step 163700 | Avg Loss: 0.0155 | Grad Norm: 0.00848358\n",
      "Epoch 1 | Step 163800 | Avg Loss: 0.0156 | Grad Norm: 0.00834256\n",
      "Epoch 1 | Step 163900 | Avg Loss: 0.0160 | Grad Norm: 0.00877238\n",
      "Epoch 1 | Step 164000 | Avg Loss: 0.0157 | Grad Norm: 0.00795980\n",
      "Epoch 1 | Step 164100 | Avg Loss: 0.0161 | Grad Norm: 0.00974601\n",
      "Epoch 1 | Step 164200 | Avg Loss: 0.0161 | Grad Norm: 0.01015289\n",
      "Epoch 1 | Step 164300 | Avg Loss: 0.0162 | Grad Norm: 0.00864368\n",
      "Epoch 1 | Step 164400 | Avg Loss: 0.0162 | Grad Norm: 0.00796351\n",
      "Epoch 1 | Step 164500 | Avg Loss: 0.0158 | Grad Norm: 0.00824458\n",
      "Epoch 1 | Step 164600 | Avg Loss: 0.0156 | Grad Norm: 0.00907118\n",
      "Epoch 1 | Step 164700 | Avg Loss: 0.0152 | Grad Norm: 0.00889950\n",
      "Epoch 1 | Step 164800 | Avg Loss: 0.0153 | Grad Norm: 0.00775655\n",
      "Epoch 1 | Step 164900 | Avg Loss: 0.0155 | Grad Norm: 0.01081701\n",
      "Epoch 1 | Step 165000 | Avg Loss: 0.0155 | Grad Norm: 0.00981345\n",
      "Epoch 1 | Step 165100 | Avg Loss: 0.0154 | Grad Norm: 0.00826541\n",
      "Epoch 1 | Step 165200 | Avg Loss: 0.0153 | Grad Norm: 0.00998788\n",
      "Epoch 1 | Step 165300 | Avg Loss: 0.0156 | Grad Norm: 0.00853402\n",
      "Epoch 1 | Step 165400 | Avg Loss: 0.0160 | Grad Norm: 0.00837952\n",
      "Epoch 1 | Step 165500 | Avg Loss: 0.0159 | Grad Norm: 0.00952169\n",
      "Epoch 1 | Step 165600 | Avg Loss: 0.0159 | Grad Norm: 0.00948305\n",
      "Epoch 1 | Step 165700 | Avg Loss: 0.0159 | Grad Norm: 0.00830701\n",
      "Epoch 1 | Step 165800 | Avg Loss: 0.0153 | Grad Norm: 0.00815413\n",
      "Epoch 1 | Step 165900 | Avg Loss: 0.0156 | Grad Norm: 0.00860071\n",
      "Epoch 1 | Step 166000 | Avg Loss: 0.0156 | Grad Norm: 0.00853600\n",
      "Epoch 1 | Step 166100 | Avg Loss: 0.0155 | Grad Norm: 0.00764414\n",
      "Epoch 1 | Step 166200 | Avg Loss: 0.0154 | Grad Norm: 0.00868935\n",
      "Epoch 1 | Step 166300 | Avg Loss: 0.0160 | Grad Norm: 0.00924668\n",
      "Epoch 1 | Step 166400 | Avg Loss: 0.0161 | Grad Norm: 0.01155867\n",
      "Epoch 1 | Step 166500 | Avg Loss: 0.0163 | Grad Norm: 0.01004554\n",
      "Epoch 1 | Step 166600 | Avg Loss: 0.0160 | Grad Norm: 0.00865182\n",
      "Epoch 1 | Step 166700 | Avg Loss: 0.0161 | Grad Norm: 0.01008024\n",
      "Epoch 1 | Step 166800 | Avg Loss: 0.0161 | Grad Norm: 0.00865590\n",
      "Epoch 1 | Step 166900 | Avg Loss: 0.0160 | Grad Norm: 0.00808386\n",
      "Epoch 1 | Step 167000 | Avg Loss: 0.0157 | Grad Norm: 0.00880611\n",
      "Epoch 1 | Step 167100 | Avg Loss: 0.0160 | Grad Norm: 0.00912924\n",
      "Epoch 1 | Step 167200 | Avg Loss: 0.0161 | Grad Norm: 0.00905377\n",
      "Epoch 1 | Step 167300 | Avg Loss: 0.0164 | Grad Norm: 0.00964787\n",
      "Epoch 1 | Step 167400 | Avg Loss: 0.0163 | Grad Norm: 0.00864643\n",
      "Epoch 1 | Step 167500 | Avg Loss: 0.0161 | Grad Norm: 0.00808511\n",
      "Epoch 1 | Step 167600 | Avg Loss: 0.0162 | Grad Norm: 0.00911428\n",
      "Epoch 1 | Step 167700 | Avg Loss: 0.0165 | Grad Norm: 0.01003366\n",
      "Epoch 1 | Step 167800 | Avg Loss: 0.0164 | Grad Norm: 0.00902905\n",
      "Epoch 1 | Step 167900 | Avg Loss: 0.0160 | Grad Norm: 0.00864999\n",
      "Epoch 1 | Step 168000 | Avg Loss: 0.0157 | Grad Norm: 0.01198939\n",
      "Epoch 1 | Step 168100 | Avg Loss: 0.0162 | Grad Norm: 0.00836772\n",
      "Epoch 1 | Step 168200 | Avg Loss: 0.0162 | Grad Norm: 0.00804523\n",
      "Epoch 1 | Step 168300 | Avg Loss: 0.0161 | Grad Norm: 0.00991935\n",
      "Epoch 1 | Step 168400 | Avg Loss: 0.0157 | Grad Norm: 0.00711747\n",
      "Epoch 1 | Step 168500 | Avg Loss: 0.0158 | Grad Norm: 0.00863988\n",
      "Epoch 1 | Step 168600 | Avg Loss: 0.0161 | Grad Norm: 0.00771017\n",
      "Epoch 1 | Step 168700 | Avg Loss: 0.0158 | Grad Norm: 0.01082174\n",
      "Epoch 1 | Step 168800 | Avg Loss: 0.0158 | Grad Norm: 0.00801686\n",
      "Epoch 1 | Step 168900 | Avg Loss: 0.0157 | Grad Norm: 0.00854635\n",
      "Epoch 1 | Step 169000 | Avg Loss: 0.0151 | Grad Norm: 0.00881876\n",
      "Epoch 1 | Step 169100 | Avg Loss: 0.0152 | Grad Norm: 0.00985331\n",
      "Epoch 1 | Step 169200 | Avg Loss: 0.0151 | Grad Norm: 0.00763622\n",
      "Epoch 1 | Step 169300 | Avg Loss: 0.0152 | Grad Norm: 0.00888799\n",
      "Epoch 1 | Step 169400 | Avg Loss: 0.0150 | Grad Norm: 0.00842490\n",
      "Epoch 1 | Step 169500 | Avg Loss: 0.0151 | Grad Norm: 0.00849332\n",
      "Epoch 1 | Step 169600 | Avg Loss: 0.0152 | Grad Norm: 0.00797887\n",
      "Epoch 1 | Step 169700 | Avg Loss: 0.0155 | Grad Norm: 0.00810830\n",
      "Epoch 1 | Step 169800 | Avg Loss: 0.0153 | Grad Norm: 0.00721014\n",
      "Epoch 1 | Step 169900 | Avg Loss: 0.0151 | Grad Norm: 0.01010494\n",
      "Epoch 1 | Step 170000 | Avg Loss: 0.0152 | Grad Norm: 0.00783979\n",
      "Epoch 1 | Step 170100 | Avg Loss: 0.0153 | Grad Norm: 0.00882028\n",
      "Epoch 1 | Step 170200 | Avg Loss: 0.0152 | Grad Norm: 0.00981305\n",
      "Epoch 1 | Step 170300 | Avg Loss: 0.0157 | Grad Norm: 0.00856574\n",
      "Epoch 1 | Step 170400 | Avg Loss: 0.0156 | Grad Norm: 0.00832590\n",
      "Epoch 1 | Step 170500 | Avg Loss: 0.0157 | Grad Norm: 0.00796175\n",
      "Epoch 1 | Step 170600 | Avg Loss: 0.0158 | Grad Norm: 0.01192249\n",
      "Epoch 1 | Step 170700 | Avg Loss: 0.0158 | Grad Norm: 0.00870353\n",
      "Epoch 1 | Step 170800 | Avg Loss: 0.0158 | Grad Norm: 0.00976856\n",
      "Epoch 1 | Step 170900 | Avg Loss: 0.0157 | Grad Norm: 0.00987604\n",
      "Epoch 1 | Step 171000 | Avg Loss: 0.0158 | Grad Norm: 0.00850678\n",
      "Epoch 1 | Step 171100 | Avg Loss: 0.0152 | Grad Norm: 0.00989953\n",
      "Epoch 1 | Step 171200 | Avg Loss: 0.0156 | Grad Norm: 0.00953182\n",
      "Epoch 1 | Step 171300 | Avg Loss: 0.0158 | Grad Norm: 0.00814990\n",
      "Epoch 1 | Step 171400 | Avg Loss: 0.0158 | Grad Norm: 0.00768784\n",
      "Epoch 1 | Step 171500 | Avg Loss: 0.0158 | Grad Norm: 0.00927408\n",
      "Epoch 1 | Step 171600 | Avg Loss: 0.0157 | Grad Norm: 0.00744808\n",
      "Epoch 1 | Step 171700 | Avg Loss: 0.0159 | Grad Norm: 0.00913249\n",
      "Epoch 1 | Step 171800 | Avg Loss: 0.0156 | Grad Norm: 0.00925718\n",
      "Epoch 1 | Step 171900 | Avg Loss: 0.0161 | Grad Norm: 0.00919276\n",
      "Epoch 1 | Step 172000 | Avg Loss: 0.0160 | Grad Norm: 0.00855811\n",
      "Epoch 1 | Step 172100 | Avg Loss: 0.0157 | Grad Norm: 0.00815545\n",
      "Epoch 1 | Step 172200 | Avg Loss: 0.0156 | Grad Norm: 0.00886792\n",
      "Epoch 1 | Step 172300 | Avg Loss: 0.0154 | Grad Norm: 0.00774246\n",
      "Epoch 1 | Step 172400 | Avg Loss: 0.0153 | Grad Norm: 0.00889379\n",
      "Epoch 1 | Step 172500 | Avg Loss: 0.0153 | Grad Norm: 0.00870466\n",
      "Epoch 1 | Step 172600 | Avg Loss: 0.0151 | Grad Norm: 0.00814015\n",
      "Epoch 1 | Step 172700 | Avg Loss: 0.0150 | Grad Norm: 0.00791282\n",
      "Epoch 1 | Step 172800 | Avg Loss: 0.0148 | Grad Norm: 0.00866602\n",
      "Epoch 1 | Step 172900 | Avg Loss: 0.0152 | Grad Norm: 0.00896628\n",
      "Epoch 1 | Step 173000 | Avg Loss: 0.0150 | Grad Norm: 0.00787014\n",
      "Epoch 1 | Step 173100 | Avg Loss: 0.0154 | Grad Norm: 0.00873796\n",
      "Epoch 1 | Step 173200 | Avg Loss: 0.0157 | Grad Norm: 0.00868775\n",
      "Epoch 1 | Step 173300 | Avg Loss: 0.0157 | Grad Norm: 0.00900644\n",
      "Epoch 1 | Step 173400 | Avg Loss: 0.0161 | Grad Norm: 0.01053537\n",
      "Epoch 1 | Step 173500 | Avg Loss: 0.0160 | Grad Norm: 0.00799628\n",
      "Epoch 1 | Step 173600 | Avg Loss: 0.0157 | Grad Norm: 0.00906676\n",
      "Epoch 1 | Step 173700 | Avg Loss: 0.0160 | Grad Norm: 0.00797885\n",
      "Epoch 1 | Step 173800 | Avg Loss: 0.0158 | Grad Norm: 0.00823768\n",
      "Epoch 1 | Step 173900 | Avg Loss: 0.0160 | Grad Norm: 0.00928757\n",
      "Epoch 1 | Step 174000 | Avg Loss: 0.0161 | Grad Norm: 0.00816137\n",
      "Epoch 1 | Step 174100 | Avg Loss: 0.0164 | Grad Norm: 0.00947336\n",
      "Epoch 1 | Step 174200 | Avg Loss: 0.0159 | Grad Norm: 0.00813750\n",
      "Epoch 1 | Step 174300 | Avg Loss: 0.0158 | Grad Norm: 0.00887124\n",
      "Epoch 1 | Step 174400 | Avg Loss: 0.0163 | Grad Norm: 0.00871465\n",
      "Epoch 1 | Step 174500 | Avg Loss: 0.0159 | Grad Norm: 0.00774930\n",
      "Epoch 1 | Step 174600 | Avg Loss: 0.0160 | Grad Norm: 0.00913639\n",
      "Epoch 1 | Step 174700 | Avg Loss: 0.0158 | Grad Norm: 0.00921026\n",
      "Epoch 1 | Step 174800 | Avg Loss: 0.0161 | Grad Norm: 0.00869796\n",
      "Epoch 1 | Step 174900 | Avg Loss: 0.0160 | Grad Norm: 0.00968679\n",
      "Epoch 1 | Step 175000 | Avg Loss: 0.0160 | Grad Norm: 0.00910857\n",
      "Epoch 1 | Step 175100 | Avg Loss: 0.0160 | Grad Norm: 0.00924198\n",
      "Epoch 1 | Step 175200 | Avg Loss: 0.0155 | Grad Norm: 0.00783597\n",
      "Epoch 1 | Step 175300 | Avg Loss: 0.0155 | Grad Norm: 0.00924703\n",
      "Epoch 1 | Step 175400 | Avg Loss: 0.0154 | Grad Norm: 0.00850610\n",
      "Epoch 1 | Step 175500 | Avg Loss: 0.0154 | Grad Norm: 0.00839131\n",
      "Epoch 1 | Step 175600 | Avg Loss: 0.0156 | Grad Norm: 0.00780690\n",
      "Epoch 1 | Step 175700 | Avg Loss: 0.0160 | Grad Norm: 0.00901537\n",
      "Epoch 1 | Step 175800 | Avg Loss: 0.0161 | Grad Norm: 0.00898410\n",
      "Epoch 1 | Step 175900 | Avg Loss: 0.0156 | Grad Norm: 0.00918939\n",
      "Epoch 1 | Step 176000 | Avg Loss: 0.0157 | Grad Norm: 0.00966160\n",
      "Epoch 1 | Step 176100 | Avg Loss: 0.0157 | Grad Norm: 0.01050834\n",
      "Epoch 1 | Step 176200 | Avg Loss: 0.0155 | Grad Norm: 0.00917711\n",
      "Epoch 1 | Step 176300 | Avg Loss: 0.0156 | Grad Norm: 0.00829869\n",
      "Epoch 1 | Step 176400 | Avg Loss: 0.0155 | Grad Norm: 0.00812019\n",
      "Epoch 1 | Step 176500 | Avg Loss: 0.0157 | Grad Norm: 0.01034780\n",
      "Epoch 1 | Step 176600 | Avg Loss: 0.0157 | Grad Norm: 0.01003107\n",
      "Epoch 1 | Step 176700 | Avg Loss: 0.0155 | Grad Norm: 0.00823025\n",
      "Epoch 1 | Step 176800 | Avg Loss: 0.0163 | Grad Norm: 0.01017697\n",
      "Epoch 1 | Step 176900 | Avg Loss: 0.0163 | Grad Norm: 0.00810427\n",
      "Epoch 1 | Step 177000 | Avg Loss: 0.0161 | Grad Norm: 0.00899709\n",
      "Epoch 1 | Step 177100 | Avg Loss: 0.0156 | Grad Norm: 0.00819545\n",
      "Epoch 1 | Step 177200 | Avg Loss: 0.0158 | Grad Norm: 0.01009982\n",
      "Epoch 1 | Step 177300 | Avg Loss: 0.0162 | Grad Norm: 0.00817741\n",
      "Epoch 1 | Step 177400 | Avg Loss: 0.0162 | Grad Norm: 0.00830984\n",
      "Epoch 1 | Step 177500 | Avg Loss: 0.0158 | Grad Norm: 0.00792791\n",
      "Epoch 1 | Step 177600 | Avg Loss: 0.0156 | Grad Norm: 0.00909223\n",
      "Epoch 1 | Step 177700 | Avg Loss: 0.0155 | Grad Norm: 0.00807889\n",
      "Epoch 1 | Step 177800 | Avg Loss: 0.0153 | Grad Norm: 0.00903012\n",
      "Epoch 1 | Step 177900 | Avg Loss: 0.0157 | Grad Norm: 0.00879093\n",
      "Epoch 1 | Step 178000 | Avg Loss: 0.0160 | Grad Norm: 0.00812054\n",
      "Epoch 1 | Step 178100 | Avg Loss: 0.0159 | Grad Norm: 0.00926612\n",
      "Epoch 1 | Step 178200 | Avg Loss: 0.0159 | Grad Norm: 0.00891070\n",
      "Epoch 1 | Step 178300 | Avg Loss: 0.0156 | Grad Norm: 0.00873003\n",
      "Epoch 1 | Step 178400 | Avg Loss: 0.0156 | Grad Norm: 0.01001741\n",
      "Epoch 1 | Step 178500 | Avg Loss: 0.0156 | Grad Norm: 0.00863925\n",
      "Epoch 1 | Step 178600 | Avg Loss: 0.0155 | Grad Norm: 0.00773804\n",
      "Epoch 1 | Step 178700 | Avg Loss: 0.0156 | Grad Norm: 0.00871966\n",
      "Epoch 1 | Step 178800 | Avg Loss: 0.0156 | Grad Norm: 0.01008334\n",
      "Epoch 1 | Step 178900 | Avg Loss: 0.0161 | Grad Norm: 0.00807002\n",
      "Epoch 1 | Step 179000 | Avg Loss: 0.0161 | Grad Norm: 0.00839095\n",
      "Epoch 1 | Step 179100 | Avg Loss: 0.0158 | Grad Norm: 0.00937796\n",
      "Epoch 1 | Step 179200 | Avg Loss: 0.0163 | Grad Norm: 0.00860735\n",
      "Epoch 1 | Step 179300 | Avg Loss: 0.0161 | Grad Norm: 0.00942498\n",
      "Epoch 1 | Step 179400 | Avg Loss: 0.0159 | Grad Norm: 0.00973379\n",
      "Epoch 1 | Step 179500 | Avg Loss: 0.0162 | Grad Norm: 0.00857936\n",
      "Epoch 1 | Step 179600 | Avg Loss: 0.0164 | Grad Norm: 0.00891601\n",
      "Epoch 1 | Step 179700 | Avg Loss: 0.0162 | Grad Norm: 0.01082087\n",
      "Epoch 1 | Step 179800 | Avg Loss: 0.0163 | Grad Norm: 0.00824287\n",
      "Epoch 1 | Step 179900 | Avg Loss: 0.0162 | Grad Norm: 0.01134657\n",
      "Epoch 1 | Step 180000 | Avg Loss: 0.0162 | Grad Norm: 0.00791297\n",
      "Epoch 1 | Step 180100 | Avg Loss: 0.0161 | Grad Norm: 0.00791034\n",
      "Epoch 1 | Step 180200 | Avg Loss: 0.0160 | Grad Norm: 0.00875108\n",
      "Epoch 1 | Step 180300 | Avg Loss: 0.0162 | Grad Norm: 0.00873957\n",
      "Epoch 1 | Step 180400 | Avg Loss: 0.0164 | Grad Norm: 0.00852721\n",
      "Epoch 1 | Step 180500 | Avg Loss: 0.0164 | Grad Norm: 0.00753716\n",
      "Epoch 1 | Step 180600 | Avg Loss: 0.0161 | Grad Norm: 0.00834698\n",
      "Epoch 1 | Step 180700 | Avg Loss: 0.0161 | Grad Norm: 0.00945745\n",
      "Epoch 1 | Step 180800 | Avg Loss: 0.0158 | Grad Norm: 0.01063016\n",
      "Epoch 1 | Step 180900 | Avg Loss: 0.0158 | Grad Norm: 0.00888534\n",
      "Epoch 1 | Step 181000 | Avg Loss: 0.0159 | Grad Norm: 0.00761642\n",
      "Epoch 1 | Step 181100 | Avg Loss: 0.0159 | Grad Norm: 0.00843269\n",
      "Epoch 1 | Step 181200 | Avg Loss: 0.0162 | Grad Norm: 0.00850355\n",
      "Epoch 1 | Step 181300 | Avg Loss: 0.0165 | Grad Norm: 0.00910681\n",
      "Epoch 1 | Step 181400 | Avg Loss: 0.0164 | Grad Norm: 0.01018553\n",
      "Epoch 1 | Step 181500 | Avg Loss: 0.0158 | Grad Norm: 0.00907018\n",
      "Epoch 1 | Step 181600 | Avg Loss: 0.0158 | Grad Norm: 0.00816758\n",
      "Epoch 1 | Step 181700 | Avg Loss: 0.0159 | Grad Norm: 0.01027109\n",
      "Epoch 1 | Step 181800 | Avg Loss: 0.0157 | Grad Norm: 0.00781159\n",
      "Epoch 1 | Step 181900 | Avg Loss: 0.0155 | Grad Norm: 0.00813208\n",
      "Epoch 1 | Step 182000 | Avg Loss: 0.0156 | Grad Norm: 0.01048790\n",
      "Epoch 1 | Step 182100 | Avg Loss: 0.0155 | Grad Norm: 0.00900244\n",
      "Epoch 1 | Step 182200 | Avg Loss: 0.0156 | Grad Norm: 0.00754809\n",
      "Epoch 1 | Step 182300 | Avg Loss: 0.0159 | Grad Norm: 0.00984033\n",
      "Epoch 1 | Step 182400 | Avg Loss: 0.0157 | Grad Norm: 0.00830198\n",
      "Epoch 1 | Step 182500 | Avg Loss: 0.0155 | Grad Norm: 0.00945413\n",
      "Epoch 1 | Step 182600 | Avg Loss: 0.0154 | Grad Norm: 0.00845710\n",
      "Epoch 1 | Step 182700 | Avg Loss: 0.0151 | Grad Norm: 0.00884215\n",
      "Epoch 1 | Step 182800 | Avg Loss: 0.0152 | Grad Norm: 0.00887003\n",
      "Epoch 1 | Step 182900 | Avg Loss: 0.0152 | Grad Norm: 0.00948090\n",
      "Epoch 1 | Step 183000 | Avg Loss: 0.0154 | Grad Norm: 0.00879367\n",
      "Epoch 1 | Step 183100 | Avg Loss: 0.0155 | Grad Norm: 0.00758970\n",
      "Epoch 1 | Step 183200 | Avg Loss: 0.0153 | Grad Norm: 0.00915697\n",
      "Epoch 1 | Step 183300 | Avg Loss: 0.0155 | Grad Norm: 0.00845849\n",
      "Epoch 1 | Step 183400 | Avg Loss: 0.0156 | Grad Norm: 0.00838232\n",
      "Epoch 1 | Step 183500 | Avg Loss: 0.0154 | Grad Norm: 0.00887432\n",
      "Epoch 1 | Step 183600 | Avg Loss: 0.0158 | Grad Norm: 0.00904887\n",
      "Epoch 1 | Step 183700 | Avg Loss: 0.0157 | Grad Norm: 0.00802019\n",
      "Epoch 1 | Step 183800 | Avg Loss: 0.0158 | Grad Norm: 0.00845187\n",
      "Epoch 1 | Step 183900 | Avg Loss: 0.0155 | Grad Norm: 0.00844959\n",
      "Epoch 1 | Step 184000 | Avg Loss: 0.0154 | Grad Norm: 0.00858297\n",
      "Epoch 1 | Step 184100 | Avg Loss: 0.0156 | Grad Norm: 0.00992665\n",
      "Epoch 1 | Step 184200 | Avg Loss: 0.0159 | Grad Norm: 0.01065656\n",
      "Epoch 1 | Step 184300 | Avg Loss: 0.0154 | Grad Norm: 0.00780989\n",
      "Epoch 1 | Step 184400 | Avg Loss: 0.0157 | Grad Norm: 0.00936893\n",
      "Epoch 1 | Step 184500 | Avg Loss: 0.0157 | Grad Norm: 0.01024873\n",
      "Epoch 1 | Step 184600 | Avg Loss: 0.0154 | Grad Norm: 0.00979706\n",
      "Epoch 1 | Step 184700 | Avg Loss: 0.0156 | Grad Norm: 0.00837807\n",
      "Epoch 1 | Step 184800 | Avg Loss: 0.0157 | Grad Norm: 0.00780252\n",
      "Epoch 1 | Step 184900 | Avg Loss: 0.0155 | Grad Norm: 0.00842635\n",
      "Epoch 1 | Step 185000 | Avg Loss: 0.0155 | Grad Norm: 0.00968211\n",
      "Epoch 1 | Step 185100 | Avg Loss: 0.0157 | Grad Norm: 0.00870155\n",
      "Epoch 1 | Step 185200 | Avg Loss: 0.0153 | Grad Norm: 0.00838178\n",
      "Epoch 1 | Step 185300 | Avg Loss: 0.0152 | Grad Norm: 0.00777978\n",
      "Epoch 1 | Step 185400 | Avg Loss: 0.0152 | Grad Norm: 0.00867729\n",
      "Epoch 1 | Step 185500 | Avg Loss: 0.0154 | Grad Norm: 0.00890233\n",
      "Epoch 1 | Step 185600 | Avg Loss: 0.0155 | Grad Norm: 0.00984865\n",
      "Epoch 1 | Step 185700 | Avg Loss: 0.0154 | Grad Norm: 0.00880169\n",
      "Epoch 1 | Step 185800 | Avg Loss: 0.0151 | Grad Norm: 0.00842576\n",
      "Epoch 1 | Step 185900 | Avg Loss: 0.0151 | Grad Norm: 0.00815831\n",
      "Epoch 1 | Step 186000 | Avg Loss: 0.0152 | Grad Norm: 0.00900337\n",
      "Epoch 1 | Step 186100 | Avg Loss: 0.0153 | Grad Norm: 0.00880263\n",
      "Epoch 1 | Step 186200 | Avg Loss: 0.0151 | Grad Norm: 0.00765142\n",
      "Epoch 1 | Step 186300 | Avg Loss: 0.0154 | Grad Norm: 0.00902881\n",
      "Epoch 1 | Step 186400 | Avg Loss: 0.0151 | Grad Norm: 0.00780234\n",
      "Epoch 1 | Step 186500 | Avg Loss: 0.0148 | Grad Norm: 0.00810676\n",
      "Epoch 1 | Step 186600 | Avg Loss: 0.0152 | Grad Norm: 0.00765426\n",
      "Epoch 1 | Step 186700 | Avg Loss: 0.0158 | Grad Norm: 0.00894033\n",
      "Epoch 1 | Step 186800 | Avg Loss: 0.0156 | Grad Norm: 0.00816670\n",
      "Epoch 1 | Step 186900 | Avg Loss: 0.0161 | Grad Norm: 0.00886389\n",
      "Epoch 1 | Step 187000 | Avg Loss: 0.0157 | Grad Norm: 0.00788847\n",
      "Epoch 1 | Step 187100 | Avg Loss: 0.0155 | Grad Norm: 0.00795438\n",
      "Epoch 1 | Step 187200 | Avg Loss: 0.0159 | Grad Norm: 0.01070606\n",
      "Epoch 1 | Step 187300 | Avg Loss: 0.0163 | Grad Norm: 0.00815878\n",
      "Epoch 1 | Step 187400 | Avg Loss: 0.0162 | Grad Norm: 0.00799422\n",
      "Epoch 1 | Step 187500 | Avg Loss: 0.0161 | Grad Norm: 0.00912196\n",
      "Epoch 1 | Step 187600 | Avg Loss: 0.0161 | Grad Norm: 0.00747978\n",
      "Epoch 1 | Step 187700 | Avg Loss: 0.0161 | Grad Norm: 0.00857402\n",
      "Epoch 1 | Step 187800 | Avg Loss: 0.0162 | Grad Norm: 0.01039699\n",
      "Epoch 1 | Step 187900 | Avg Loss: 0.0164 | Grad Norm: 0.00873145\n",
      "Epoch 1 | Step 188000 | Avg Loss: 0.0166 | Grad Norm: 0.00941836\n",
      "Epoch 1 | Step 188100 | Avg Loss: 0.0162 | Grad Norm: 0.00907191\n",
      "Epoch 1 | Step 188200 | Avg Loss: 0.0163 | Grad Norm: 0.00886429\n",
      "Epoch 1 | Step 188300 | Avg Loss: 0.0160 | Grad Norm: 0.00950495\n",
      "Epoch 1 | Step 188400 | Avg Loss: 0.0156 | Grad Norm: 0.00880006\n",
      "Epoch 1 | Step 188500 | Avg Loss: 0.0160 | Grad Norm: 0.00820365\n",
      "Epoch 1 | Step 188600 | Avg Loss: 0.0161 | Grad Norm: 0.00864988\n",
      "Epoch 1 | Step 188700 | Avg Loss: 0.0161 | Grad Norm: 0.00874163\n",
      "Epoch 1 | Step 188800 | Avg Loss: 0.0157 | Grad Norm: 0.00929436\n",
      "Epoch 1 | Step 188900 | Avg Loss: 0.0156 | Grad Norm: 0.00783037\n",
      "Epoch 1 | Step 189000 | Avg Loss: 0.0160 | Grad Norm: 0.00991221\n",
      "Epoch 1 | Step 189100 | Avg Loss: 0.0160 | Grad Norm: 0.00788186\n",
      "Epoch 1 | Step 189200 | Avg Loss: 0.0162 | Grad Norm: 0.00874539\n",
      "Epoch 1 | Step 189300 | Avg Loss: 0.0162 | Grad Norm: 0.00993894\n",
      "Epoch 1 | Step 189400 | Avg Loss: 0.0161 | Grad Norm: 0.00868914\n",
      "Epoch 1 | Step 189500 | Avg Loss: 0.0154 | Grad Norm: 0.00884307\n",
      "Epoch 1 | Step 189600 | Avg Loss: 0.0155 | Grad Norm: 0.00859898\n",
      "Epoch 1 | Step 189700 | Avg Loss: 0.0155 | Grad Norm: 0.00819078\n",
      "Epoch 1 | Step 189800 | Avg Loss: 0.0154 | Grad Norm: 0.00958189\n",
      "Epoch 1 | Step 189900 | Avg Loss: 0.0152 | Grad Norm: 0.00842032\n",
      "Epoch 1 | Step 190000 | Avg Loss: 0.0154 | Grad Norm: 0.00878779\n",
      "Epoch 1 | Step 190100 | Avg Loss: 0.0153 | Grad Norm: 0.00822880\n",
      "Epoch 1 | Step 190200 | Avg Loss: 0.0156 | Grad Norm: 0.00814904\n",
      "Epoch 1 | Step 190300 | Avg Loss: 0.0159 | Grad Norm: 0.00996686\n",
      "Epoch 1 | Step 190400 | Avg Loss: 0.0160 | Grad Norm: 0.00856176\n",
      "Epoch 1 | Step 190500 | Avg Loss: 0.0159 | Grad Norm: 0.00839262\n",
      "Epoch 1 | Step 190600 | Avg Loss: 0.0156 | Grad Norm: 0.00885926\n",
      "Epoch 1 | Step 190700 | Avg Loss: 0.0159 | Grad Norm: 0.00913034\n",
      "Epoch 1 | Step 190800 | Avg Loss: 0.0155 | Grad Norm: 0.00861412\n",
      "Epoch 1 | Step 190900 | Avg Loss: 0.0155 | Grad Norm: 0.00850044\n",
      "Epoch 1 | Step 191000 | Avg Loss: 0.0155 | Grad Norm: 0.00907872\n",
      "Epoch 1 | Step 191100 | Avg Loss: 0.0156 | Grad Norm: 0.00925328\n",
      "Epoch 1 | Step 191200 | Avg Loss: 0.0154 | Grad Norm: 0.00799301\n",
      "Epoch 1 | Step 191300 | Avg Loss: 0.0153 | Grad Norm: 0.00870443\n",
      "Epoch 1 | Step 191400 | Avg Loss: 0.0153 | Grad Norm: 0.00712222\n",
      "Epoch 1 | Step 191500 | Avg Loss: 0.0158 | Grad Norm: 0.00912042\n",
      "Epoch 1 | Step 191600 | Avg Loss: 0.0155 | Grad Norm: 0.01028023\n",
      "Epoch 1 | Step 191700 | Avg Loss: 0.0156 | Grad Norm: 0.00879692\n",
      "Epoch 1 | Step 191800 | Avg Loss: 0.0155 | Grad Norm: 0.00965674\n",
      "Epoch 1 | Step 191900 | Avg Loss: 0.0159 | Grad Norm: 0.00820628\n",
      "Epoch 1 | Step 192000 | Avg Loss: 0.0158 | Grad Norm: 0.00965694\n",
      "Epoch 1 | Step 192100 | Avg Loss: 0.0159 | Grad Norm: 0.01038567\n",
      "Epoch 1 | Step 192200 | Avg Loss: 0.0157 | Grad Norm: 0.00861080\n",
      "Epoch 1 | Step 192300 | Avg Loss: 0.0157 | Grad Norm: 0.00749542\n",
      "Epoch 1 | Step 192400 | Avg Loss: 0.0158 | Grad Norm: 0.00939740\n",
      "Epoch 1 | Step 192500 | Avg Loss: 0.0162 | Grad Norm: 0.01047474\n",
      "Epoch 1 | Step 192600 | Avg Loss: 0.0159 | Grad Norm: 0.00852043\n",
      "Epoch 1 | Step 192700 | Avg Loss: 0.0155 | Grad Norm: 0.00797034\n",
      "Epoch 1 | Step 192800 | Avg Loss: 0.0155 | Grad Norm: 0.00823998\n",
      "Epoch 1 | Step 192900 | Avg Loss: 0.0158 | Grad Norm: 0.00816554\n",
      "Epoch 1 | Step 193000 | Avg Loss: 0.0160 | Grad Norm: 0.00865430\n",
      "Epoch 1 | Step 193100 | Avg Loss: 0.0159 | Grad Norm: 0.00944878\n",
      "Epoch 1 | Step 193200 | Avg Loss: 0.0159 | Grad Norm: 0.00857845\n",
      "Epoch 1 | Step 193300 | Avg Loss: 0.0162 | Grad Norm: 0.00840183\n",
      "Epoch 1 | Step 193400 | Avg Loss: 0.0162 | Grad Norm: 0.00822113\n",
      "Epoch 1 | Step 193500 | Avg Loss: 0.0160 | Grad Norm: 0.00964841\n",
      "Epoch 1 | Step 193600 | Avg Loss: 0.0162 | Grad Norm: 0.00925806\n",
      "Epoch 1 | Step 193700 | Avg Loss: 0.0160 | Grad Norm: 0.00845798\n",
      "Epoch 1 | Step 193800 | Avg Loss: 0.0156 | Grad Norm: 0.00842225\n",
      "Epoch 1 | Step 193900 | Avg Loss: 0.0156 | Grad Norm: 0.00827582\n",
      "Epoch 1 | Step 194000 | Avg Loss: 0.0162 | Grad Norm: 0.00821429\n",
      "Epoch 1 | Step 194100 | Avg Loss: 0.0160 | Grad Norm: 0.00940734\n",
      "Epoch 1 | Step 194200 | Avg Loss: 0.0159 | Grad Norm: 0.00926590\n",
      "Epoch 1 | Step 194300 | Avg Loss: 0.0158 | Grad Norm: 0.01050913\n",
      "Epoch 1 | Step 194400 | Avg Loss: 0.0156 | Grad Norm: 0.00789538\n",
      "Epoch 1 | Step 194500 | Avg Loss: 0.0153 | Grad Norm: 0.01265441\n",
      "Epoch 1 | Step 194600 | Avg Loss: 0.0155 | Grad Norm: 0.00877225\n",
      "Epoch 1 | Step 194700 | Avg Loss: 0.0155 | Grad Norm: 0.00792189\n",
      "Epoch 1 | Step 194800 | Avg Loss: 0.0155 | Grad Norm: 0.00912770\n",
      "Epoch 1 | Step 194900 | Avg Loss: 0.0157 | Grad Norm: 0.00764264\n",
      "Epoch 1 | Step 195000 | Avg Loss: 0.0156 | Grad Norm: 0.00759159\n",
      "Epoch 1 | Step 195100 | Avg Loss: 0.0160 | Grad Norm: 0.00889303\n",
      "Epoch 1 | Step 195200 | Avg Loss: 0.0161 | Grad Norm: 0.00889358\n",
      "Epoch 1 | Step 195300 | Avg Loss: 0.0161 | Grad Norm: 0.00917339\n",
      "Epoch 1 | Step 195400 | Avg Loss: 0.0163 | Grad Norm: 0.00999572\n",
      "Epoch 1 | Step 195500 | Avg Loss: 0.0164 | Grad Norm: 0.00835008\n",
      "Epoch 1 | Step 195600 | Avg Loss: 0.0162 | Grad Norm: 0.00895592\n",
      "Epoch 1 | Step 195700 | Avg Loss: 0.0161 | Grad Norm: 0.00813037\n",
      "Epoch 1 | Step 195800 | Avg Loss: 0.0158 | Grad Norm: 0.00820334\n",
      "Epoch 1 | Step 195900 | Avg Loss: 0.0158 | Grad Norm: 0.00876073\n",
      "Epoch 1 | Step 196000 | Avg Loss: 0.0159 | Grad Norm: 0.00861200\n",
      "Epoch 1 | Step 196100 | Avg Loss: 0.0159 | Grad Norm: 0.01089482\n",
      "Epoch 1 | Step 196200 | Avg Loss: 0.0160 | Grad Norm: 0.00922051\n",
      "Epoch 1 | Step 196300 | Avg Loss: 0.0157 | Grad Norm: 0.00731195\n",
      "Epoch 1 | Step 196400 | Avg Loss: 0.0154 | Grad Norm: 0.00795203\n",
      "Epoch 1 | Step 196500 | Avg Loss: 0.0155 | Grad Norm: 0.00868460\n",
      "Epoch 1 | Step 196600 | Avg Loss: 0.0152 | Grad Norm: 0.00934520\n",
      "Epoch 1 | Step 196700 | Avg Loss: 0.0154 | Grad Norm: 0.00916205\n",
      "Epoch 1 | Step 196800 | Avg Loss: 0.0155 | Grad Norm: 0.00831092\n",
      "Epoch 1 | Step 196900 | Avg Loss: 0.0157 | Grad Norm: 0.00764311\n",
      "Epoch 1 | Step 197000 | Avg Loss: 0.0156 | Grad Norm: 0.00805762\n",
      "Epoch 1 | Step 197100 | Avg Loss: 0.0160 | Grad Norm: 0.00833790\n",
      "Epoch 1 | Step 197200 | Avg Loss: 0.0156 | Grad Norm: 0.00780115\n",
      "Epoch 1 | Step 197300 | Avg Loss: 0.0156 | Grad Norm: 0.00803391\n",
      "Epoch 1 | Step 197400 | Avg Loss: 0.0154 | Grad Norm: 0.00772718\n",
      "Epoch 1 | Step 197500 | Avg Loss: 0.0154 | Grad Norm: 0.00795483\n",
      "Epoch 1 | Step 197600 | Avg Loss: 0.0157 | Grad Norm: 0.00838345\n",
      "Epoch 1 | Step 197700 | Avg Loss: 0.0156 | Grad Norm: 0.00838923\n",
      "Epoch 1 | Step 197800 | Avg Loss: 0.0159 | Grad Norm: 0.00947139\n",
      "Epoch 1 | Step 197900 | Avg Loss: 0.0164 | Grad Norm: 0.00875590\n",
      "Epoch 1 | Step 198000 | Avg Loss: 0.0164 | Grad Norm: 0.00943275\n",
      "Epoch 1 | Step 198100 | Avg Loss: 0.0160 | Grad Norm: 0.00846039\n",
      "Epoch 1 | Step 198200 | Avg Loss: 0.0156 | Grad Norm: 0.00803325\n",
      "Epoch 1 | Step 198300 | Avg Loss: 0.0154 | Grad Norm: 0.00798224\n",
      "Epoch 1 | Step 198400 | Avg Loss: 0.0158 | Grad Norm: 0.01002087\n",
      "Epoch 1 | Step 198500 | Avg Loss: 0.0158 | Grad Norm: 0.00953102\n",
      "Epoch 1 | Step 198600 | Avg Loss: 0.0158 | Grad Norm: 0.00811059\n",
      "Epoch 1 | Step 198700 | Avg Loss: 0.0156 | Grad Norm: 0.00772122\n",
      "Epoch 1 | Step 198800 | Avg Loss: 0.0153 | Grad Norm: 0.00826012\n",
      "Epoch 1 | Step 198900 | Avg Loss: 0.0158 | Grad Norm: 0.01123483\n",
      "Epoch 1 | Step 199000 | Avg Loss: 0.0158 | Grad Norm: 0.00929324\n",
      "Epoch 1 | Step 199100 | Avg Loss: 0.0158 | Grad Norm: 0.00869951\n",
      "Epoch 1 | Step 199200 | Avg Loss: 0.0160 | Grad Norm: 0.00921200\n",
      "Epoch 1 | Step 199300 | Avg Loss: 0.0158 | Grad Norm: 0.00975471\n",
      "Epoch 1 | Step 199400 | Avg Loss: 0.0165 | Grad Norm: 0.00815917\n",
      "Epoch 1 | Step 199500 | Avg Loss: 0.0162 | Grad Norm: 0.01033758\n",
      "Epoch 1 | Step 199600 | Avg Loss: 0.0163 | Grad Norm: 0.00908465\n",
      "Epoch 1 | Step 199700 | Avg Loss: 0.0163 | Grad Norm: 0.01070802\n",
      "Epoch 1 | Step 199800 | Avg Loss: 0.0157 | Grad Norm: 0.00831329\n",
      "Epoch 1 | Step 199900 | Avg Loss: 0.0154 | Grad Norm: 0.00892255\n",
      "Epoch 1 | Step 200000 | Avg Loss: 0.0157 | Grad Norm: 0.00879451\n",
      "Saving model at step200000\n",
      "Epoch 1 | Step 200100 | Avg Loss: 0.0156 | Grad Norm: 0.00857498\n",
      "Epoch 1 | Step 200200 | Avg Loss: 0.0156 | Grad Norm: 0.00874438\n",
      "Epoch 1 | Step 200300 | Avg Loss: 0.0155 | Grad Norm: 0.00880466\n",
      "Epoch 1 | Step 200400 | Avg Loss: 0.0155 | Grad Norm: 0.00835016\n",
      "Epoch 1 | Step 200500 | Avg Loss: 0.0157 | Grad Norm: 0.00832507\n",
      "Epoch 1 | Step 200600 | Avg Loss: 0.0159 | Grad Norm: 0.00901344\n",
      "Epoch 1 | Step 200700 | Avg Loss: 0.0155 | Grad Norm: 0.01017708\n",
      "Epoch 1 | Step 200800 | Avg Loss: 0.0159 | Grad Norm: 0.00863694\n",
      "Epoch 1 | Step 200900 | Avg Loss: 0.0158 | Grad Norm: 0.01059514\n",
      "Epoch 1 | Step 201000 | Avg Loss: 0.0159 | Grad Norm: 0.00971276\n",
      "Epoch 1 | Step 201100 | Avg Loss: 0.0156 | Grad Norm: 0.00857545\n",
      "Epoch 1 | Step 201200 | Avg Loss: 0.0159 | Grad Norm: 0.00826334\n",
      "Epoch 1 | Step 201300 | Avg Loss: 0.0159 | Grad Norm: 0.00823712\n",
      "Epoch 1 | Step 201400 | Avg Loss: 0.0156 | Grad Norm: 0.00724232\n",
      "Epoch 1 | Step 201500 | Avg Loss: 0.0156 | Grad Norm: 0.00912691\n",
      "Epoch 1 | Step 201600 | Avg Loss: 0.0159 | Grad Norm: 0.00908061\n",
      "Epoch 1 | Step 201700 | Avg Loss: 0.0156 | Grad Norm: 0.00816364\n",
      "Epoch 1 | Step 201800 | Avg Loss: 0.0155 | Grad Norm: 0.00822615\n",
      "Epoch 1 | Step 201900 | Avg Loss: 0.0155 | Grad Norm: 0.00804152\n",
      "Epoch 1 | Step 202000 | Avg Loss: 0.0156 | Grad Norm: 0.00890113\n",
      "Epoch 1 | Step 202100 | Avg Loss: 0.0155 | Grad Norm: 0.00851406\n",
      "Epoch 1 | Step 202200 | Avg Loss: 0.0159 | Grad Norm: 0.00788785\n",
      "Epoch 1 | Step 202300 | Avg Loss: 0.0155 | Grad Norm: 0.00899909\n",
      "Epoch 1 | Step 202400 | Avg Loss: 0.0154 | Grad Norm: 0.00882682\n",
      "Epoch 1 | Step 202500 | Avg Loss: 0.0157 | Grad Norm: 0.00964586\n",
      "Epoch 1 | Step 202600 | Avg Loss: 0.0157 | Grad Norm: 0.01004111\n",
      "Epoch 1 | Step 202700 | Avg Loss: 0.0159 | Grad Norm: 0.00939144\n",
      "Epoch 1 | Step 202800 | Avg Loss: 0.0167 | Grad Norm: 0.00802519\n",
      "Epoch 1 | Step 202900 | Avg Loss: 0.0165 | Grad Norm: 0.00864449\n",
      "Epoch 1 | Step 203000 | Avg Loss: 0.0163 | Grad Norm: 0.00847271\n",
      "Epoch 1 | Step 203100 | Avg Loss: 0.0163 | Grad Norm: 0.00779731\n",
      "Epoch 1 | Step 203200 | Avg Loss: 0.0163 | Grad Norm: 0.01005431\n",
      "Epoch 1 | Step 203300 | Avg Loss: 0.0158 | Grad Norm: 0.00788168\n",
      "Epoch 1 | Step 203400 | Avg Loss: 0.0159 | Grad Norm: 0.00773704\n",
      "Epoch 1 | Step 203500 | Avg Loss: 0.0160 | Grad Norm: 0.00758649\n",
      "Epoch 1 | Step 203600 | Avg Loss: 0.0159 | Grad Norm: 0.00775222\n",
      "Epoch 1 | Step 203700 | Avg Loss: 0.0164 | Grad Norm: 0.00835080\n",
      "Epoch 1 | Step 203800 | Avg Loss: 0.0163 | Grad Norm: 0.00986217\n",
      "Epoch 1 | Step 203900 | Avg Loss: 0.0161 | Grad Norm: 0.00901830\n",
      "Epoch 1 | Step 204000 | Avg Loss: 0.0157 | Grad Norm: 0.00920853\n",
      "Epoch 1 | Step 204100 | Avg Loss: 0.0162 | Grad Norm: 0.01014334\n",
      "Epoch 1 | Step 204200 | Avg Loss: 0.0160 | Grad Norm: 0.01011931\n",
      "Epoch 1 | Step 204300 | Avg Loss: 0.0157 | Grad Norm: 0.01068926\n",
      "Epoch 1 | Step 204400 | Avg Loss: 0.0157 | Grad Norm: 0.00932060\n",
      "Epoch 1 | Step 204500 | Avg Loss: 0.0156 | Grad Norm: 0.00801526\n",
      "Epoch 1 | Step 204600 | Avg Loss: 0.0159 | Grad Norm: 0.01039100\n",
      "Epoch 1 | Step 204700 | Avg Loss: 0.0159 | Grad Norm: 0.00917140\n",
      "Epoch 1 | Step 204800 | Avg Loss: 0.0156 | Grad Norm: 0.00866753\n",
      "Epoch 1 | Step 204900 | Avg Loss: 0.0153 | Grad Norm: 0.00959995\n",
      "Epoch 1 | Step 205000 | Avg Loss: 0.0155 | Grad Norm: 0.00836363\n",
      "Epoch 1 | Step 205100 | Avg Loss: 0.0153 | Grad Norm: 0.01005332\n",
      "Epoch 1 | Step 205200 | Avg Loss: 0.0153 | Grad Norm: 0.00861922\n",
      "Epoch 1 | Step 205300 | Avg Loss: 0.0156 | Grad Norm: 0.00912644\n",
      "Epoch 1 | Step 205400 | Avg Loss: 0.0156 | Grad Norm: 0.00877566\n",
      "Epoch 1 | Step 205500 | Avg Loss: 0.0156 | Grad Norm: 0.00898157\n",
      "Epoch 1 | Step 205600 | Avg Loss: 0.0154 | Grad Norm: 0.01184667\n",
      "Epoch 1 | Step 205700 | Avg Loss: 0.0155 | Grad Norm: 0.00872521\n",
      "Epoch 1 | Step 205800 | Avg Loss: 0.0160 | Grad Norm: 0.00948543\n",
      "Epoch 1 | Step 205900 | Avg Loss: 0.0162 | Grad Norm: 0.00940719\n",
      "Epoch 1 | Step 206000 | Avg Loss: 0.0159 | Grad Norm: 0.00730042\n",
      "Epoch 1 | Step 206100 | Avg Loss: 0.0163 | Grad Norm: 0.00903984\n",
      "Epoch 1 | Step 206200 | Avg Loss: 0.0160 | Grad Norm: 0.00913907\n",
      "Epoch 1 | Step 206300 | Avg Loss: 0.0161 | Grad Norm: 0.00881280\n",
      "Epoch 1 | Step 206400 | Avg Loss: 0.0158 | Grad Norm: 0.00882663\n",
      "Epoch 1 | Step 206500 | Avg Loss: 0.0158 | Grad Norm: 0.00957974\n",
      "Epoch 1 | Step 206600 | Avg Loss: 0.0159 | Grad Norm: 0.00749173\n",
      "Epoch 1 | Step 206700 | Avg Loss: 0.0162 | Grad Norm: 0.00904146\n",
      "Epoch 1 | Step 206800 | Avg Loss: 0.0160 | Grad Norm: 0.00965468\n",
      "Epoch 1 | Step 206900 | Avg Loss: 0.0160 | Grad Norm: 0.00832353\n",
      "Epoch 1 | Step 207000 | Avg Loss: 0.0157 | Grad Norm: 0.01141861\n",
      "Epoch 1 | Step 207100 | Avg Loss: 0.0157 | Grad Norm: 0.00847251\n",
      "Epoch 1 | Step 207200 | Avg Loss: 0.0159 | Grad Norm: 0.00760785\n",
      "Epoch 1 | Step 207300 | Avg Loss: 0.0158 | Grad Norm: 0.00835998\n",
      "Epoch 1 | Step 207400 | Avg Loss: 0.0160 | Grad Norm: 0.01149051\n",
      "Epoch 1 | Step 207500 | Avg Loss: 0.0160 | Grad Norm: 0.00840034\n",
      "Epoch 1 | Step 207600 | Avg Loss: 0.0156 | Grad Norm: 0.00927848\n",
      "Epoch 1 | Step 207700 | Avg Loss: 0.0156 | Grad Norm: 0.00826131\n",
      "Epoch 1 | Step 207800 | Avg Loss: 0.0156 | Grad Norm: 0.00891915\n",
      "Epoch 1 | Step 207900 | Avg Loss: 0.0159 | Grad Norm: 0.00884942\n",
      "Epoch 1 | Step 208000 | Avg Loss: 0.0156 | Grad Norm: 0.00985200\n",
      "Epoch 1 | Step 208100 | Avg Loss: 0.0161 | Grad Norm: 0.00861629\n",
      "Epoch 1 | Step 208200 | Avg Loss: 0.0162 | Grad Norm: 0.00853091\n",
      "Epoch 1 | Step 208300 | Avg Loss: 0.0162 | Grad Norm: 0.01004536\n",
      "Epoch 1 | Step 208400 | Avg Loss: 0.0161 | Grad Norm: 0.00787166\n",
      "Epoch 1 | Step 208500 | Avg Loss: 0.0162 | Grad Norm: 0.00980239\n",
      "Epoch 1 | Step 208600 | Avg Loss: 0.0165 | Grad Norm: 0.00854167\n",
      "Epoch 1 | Step 208700 | Avg Loss: 0.0161 | Grad Norm: 0.00912730\n",
      "Epoch 1 | Step 208800 | Avg Loss: 0.0156 | Grad Norm: 0.00931272\n",
      "Epoch 1 | Step 208900 | Avg Loss: 0.0161 | Grad Norm: 0.00831125\n",
      "Epoch 1 | Step 209000 | Avg Loss: 0.0162 | Grad Norm: 0.00980952\n",
      "Epoch 1 | Step 209100 | Avg Loss: 0.0161 | Grad Norm: 0.00854934\n",
      "Epoch 1 | Step 209200 | Avg Loss: 0.0163 | Grad Norm: 0.00985370\n",
      "Epoch 1 | Step 209300 | Avg Loss: 0.0162 | Grad Norm: 0.00856371\n",
      "Epoch 1 | Step 209400 | Avg Loss: 0.0166 | Grad Norm: 0.00856711\n",
      "Epoch 1 | Step 209500 | Avg Loss: 0.0161 | Grad Norm: 0.00875978\n",
      "Epoch 1 | Step 209600 | Avg Loss: 0.0158 | Grad Norm: 0.00959561\n",
      "Epoch 1 | Step 209700 | Avg Loss: 0.0155 | Grad Norm: 0.00943779\n",
      "Epoch 1 | Step 209800 | Avg Loss: 0.0157 | Grad Norm: 0.00830593\n",
      "Epoch 1 | Step 209900 | Avg Loss: 0.0160 | Grad Norm: 0.00875082\n",
      "Epoch 1 | Step 210000 | Avg Loss: 0.0160 | Grad Norm: 0.01061922\n",
      "Epoch 1 | Step 210100 | Avg Loss: 0.0160 | Grad Norm: 0.01287714\n",
      "Epoch 1 | Step 210200 | Avg Loss: 0.0161 | Grad Norm: 0.00812256\n",
      "Epoch 1 | Step 210300 | Avg Loss: 0.0157 | Grad Norm: 0.00993225\n",
      "Epoch 1 | Step 210400 | Avg Loss: 0.0158 | Grad Norm: 0.00836027\n",
      "Epoch 1 | Step 210500 | Avg Loss: 0.0160 | Grad Norm: 0.00959353\n",
      "Epoch 1 | Step 210600 | Avg Loss: 0.0160 | Grad Norm: 0.00912043\n",
      "Epoch 1 | Step 210700 | Avg Loss: 0.0158 | Grad Norm: 0.00971243\n",
      "Epoch 1 | Step 210800 | Avg Loss: 0.0157 | Grad Norm: 0.00910092\n",
      "Epoch 1 | Step 210900 | Avg Loss: 0.0158 | Grad Norm: 0.00846081\n",
      "Epoch 1 | Step 211000 | Avg Loss: 0.0154 | Grad Norm: 0.00791817\n",
      "Epoch 1 | Step 211100 | Avg Loss: 0.0151 | Grad Norm: 0.00839337\n",
      "Epoch 1 | Step 211200 | Avg Loss: 0.0153 | Grad Norm: 0.01094233\n",
      "Epoch 1 | Step 211300 | Avg Loss: 0.0152 | Grad Norm: 0.00939158\n",
      "Epoch 1 | Step 211400 | Avg Loss: 0.0154 | Grad Norm: 0.00849716\n",
      "Epoch 1 | Step 211500 | Avg Loss: 0.0152 | Grad Norm: 0.00895135\n",
      "Epoch 1 | Step 211600 | Avg Loss: 0.0152 | Grad Norm: 0.00786022\n",
      "Epoch 1 | Step 211700 | Avg Loss: 0.0155 | Grad Norm: 0.00821828\n",
      "Epoch 1 | Step 211800 | Avg Loss: 0.0153 | Grad Norm: 0.00776915\n",
      "Epoch 1 | Step 211900 | Avg Loss: 0.0156 | Grad Norm: 0.00873898\n",
      "Epoch 1 | Step 212000 | Avg Loss: 0.0161 | Grad Norm: 0.01263692\n",
      "Epoch 1 | Step 212100 | Avg Loss: 0.0163 | Grad Norm: 0.00837792\n",
      "Epoch 1 | Step 212200 | Avg Loss: 0.0162 | Grad Norm: 0.00808390\n",
      "Epoch 1 | Step 212300 | Avg Loss: 0.0161 | Grad Norm: 0.00882938\n",
      "Epoch 1 | Step 212400 | Avg Loss: 0.0163 | Grad Norm: 0.00893285\n",
      "Epoch 1 | Step 212500 | Avg Loss: 0.0162 | Grad Norm: 0.00860002\n",
      "Epoch 1 | Step 212600 | Avg Loss: 0.0160 | Grad Norm: 0.00947172\n",
      "Epoch 1 | Step 212700 | Avg Loss: 0.0157 | Grad Norm: 0.00812862\n",
      "Epoch 1 | Step 212800 | Avg Loss: 0.0157 | Grad Norm: 0.00879013\n",
      "Epoch 1 | Step 212900 | Avg Loss: 0.0155 | Grad Norm: 0.00900509\n",
      "Epoch 1 | Step 213000 | Avg Loss: 0.0155 | Grad Norm: 0.00903926\n",
      "Epoch 1 | Step 213100 | Avg Loss: 0.0153 | Grad Norm: 0.00882168\n",
      "Epoch 1 | Step 213200 | Avg Loss: 0.0153 | Grad Norm: 0.00791801\n",
      "Epoch 1 | Step 213300 | Avg Loss: 0.0154 | Grad Norm: 0.00919704\n",
      "Epoch 1 | Step 213400 | Avg Loss: 0.0151 | Grad Norm: 0.01072286\n",
      "Epoch 1 | Step 213500 | Avg Loss: 0.0153 | Grad Norm: 0.00951144\n",
      "Epoch 1 | Step 213600 | Avg Loss: 0.0156 | Grad Norm: 0.00933193\n",
      "Epoch 1 | Step 213700 | Avg Loss: 0.0159 | Grad Norm: 0.00889191\n",
      "Epoch 1 | Step 213800 | Avg Loss: 0.0159 | Grad Norm: 0.00846130\n",
      "Epoch 1 | Step 213900 | Avg Loss: 0.0157 | Grad Norm: 0.00812138\n",
      "Epoch 1 | Step 214000 | Avg Loss: 0.0157 | Grad Norm: 0.00849047\n",
      "Epoch 1 | Step 214100 | Avg Loss: 0.0158 | Grad Norm: 0.00925963\n",
      "Epoch 1 | Step 214200 | Avg Loss: 0.0160 | Grad Norm: 0.00973572\n",
      "Epoch 1 | Step 214300 | Avg Loss: 0.0159 | Grad Norm: 0.00981866\n",
      "Epoch 1 | Step 214400 | Avg Loss: 0.0158 | Grad Norm: 0.00859029\n",
      "Epoch 1 | Step 214500 | Avg Loss: 0.0160 | Grad Norm: 0.00879764\n",
      "Epoch 1 | Step 214600 | Avg Loss: 0.0154 | Grad Norm: 0.00750748\n",
      "Epoch 1 | Step 214700 | Avg Loss: 0.0151 | Grad Norm: 0.00820345\n",
      "Epoch 1 | Step 214800 | Avg Loss: 0.0151 | Grad Norm: 0.00776404\n",
      "Epoch 1 | Step 214900 | Avg Loss: 0.0154 | Grad Norm: 0.00880798\n",
      "Epoch 1 | Step 215000 | Avg Loss: 0.0158 | Grad Norm: 0.00884294\n",
      "Epoch 1 | Step 215100 | Avg Loss: 0.0158 | Grad Norm: 0.00894415\n",
      "Epoch 1 | Step 215200 | Avg Loss: 0.0158 | Grad Norm: 0.00810370\n",
      "Epoch 1 | Step 215300 | Avg Loss: 0.0154 | Grad Norm: 0.00804867\n",
      "Epoch 1 | Step 215400 | Avg Loss: 0.0153 | Grad Norm: 0.00938394\n",
      "Epoch 1 | Step 215500 | Avg Loss: 0.0154 | Grad Norm: 0.00768660\n",
      "Epoch 1 | Step 215600 | Avg Loss: 0.0157 | Grad Norm: 0.00862232\n",
      "Epoch 1 | Step 215700 | Avg Loss: 0.0157 | Grad Norm: 0.00798424\n",
      "Epoch 1 | Step 215800 | Avg Loss: 0.0154 | Grad Norm: 0.00800451\n",
      "Epoch 1 | Step 215900 | Avg Loss: 0.0152 | Grad Norm: 0.00902281\n",
      "Epoch 1 | Step 216000 | Avg Loss: 0.0152 | Grad Norm: 0.00819014\n",
      "Epoch 1 | Step 216100 | Avg Loss: 0.0155 | Grad Norm: 0.00878070\n",
      "Epoch 1 | Step 216200 | Avg Loss: 0.0154 | Grad Norm: 0.00814239\n",
      "Epoch 1 | Step 216300 | Avg Loss: 0.0156 | Grad Norm: 0.00925718\n",
      "Epoch 1 | Step 216400 | Avg Loss: 0.0157 | Grad Norm: 0.00953591\n",
      "Epoch 1 | Step 216500 | Avg Loss: 0.0152 | Grad Norm: 0.00894780\n",
      "Epoch 1 | Step 216600 | Avg Loss: 0.0158 | Grad Norm: 0.00839400\n",
      "Epoch 1 | Step 216700 | Avg Loss: 0.0154 | Grad Norm: 0.00737718\n",
      "Epoch 1 | Step 216800 | Avg Loss: 0.0156 | Grad Norm: 0.00985306\n",
      "Epoch 1 | Step 216900 | Avg Loss: 0.0155 | Grad Norm: 0.00710179\n",
      "Epoch 1 | Step 217000 | Avg Loss: 0.0156 | Grad Norm: 0.00879274\n",
      "Epoch 1 | Step 217100 | Avg Loss: 0.0159 | Grad Norm: 0.00838564\n",
      "Epoch 1 | Step 217200 | Avg Loss: 0.0161 | Grad Norm: 0.00877939\n",
      "Epoch 1 | Step 217300 | Avg Loss: 0.0159 | Grad Norm: 0.01034307\n",
      "Epoch 1 | Step 217400 | Avg Loss: 0.0164 | Grad Norm: 0.01007135\n",
      "Epoch 1 | Step 217500 | Avg Loss: 0.0161 | Grad Norm: 0.00833286\n",
      "Epoch 1 | Step 217600 | Avg Loss: 0.0161 | Grad Norm: 0.00959088\n",
      "Epoch 1 | Step 217700 | Avg Loss: 0.0160 | Grad Norm: 0.00985653\n",
      "Epoch 1 | Step 217800 | Avg Loss: 0.0165 | Grad Norm: 0.00923831\n",
      "Epoch 1 | Step 217900 | Avg Loss: 0.0163 | Grad Norm: 0.00859019\n",
      "Epoch 1 | Step 218000 | Avg Loss: 0.0163 | Grad Norm: 0.00885293\n",
      "Epoch 1 | Step 218100 | Avg Loss: 0.0158 | Grad Norm: 0.00813503\n",
      "Epoch 1 | Step 218200 | Avg Loss: 0.0160 | Grad Norm: 0.00832432\n",
      "Epoch 1 | Step 218300 | Avg Loss: 0.0158 | Grad Norm: 0.00841546\n",
      "Epoch 1 | Step 218400 | Avg Loss: 0.0160 | Grad Norm: 0.01123975\n",
      "Epoch 1 | Step 218500 | Avg Loss: 0.0158 | Grad Norm: 0.00931967\n",
      "Epoch 1 | Step 218600 | Avg Loss: 0.0154 | Grad Norm: 0.00825870\n",
      "Epoch 1 | Step 218700 | Avg Loss: 0.0152 | Grad Norm: 0.00960863\n",
      "Epoch 1 | Step 218800 | Avg Loss: 0.0155 | Grad Norm: 0.00837394\n",
      "Epoch 1 | Step 218900 | Avg Loss: 0.0156 | Grad Norm: 0.00857687\n",
      "Epoch 1 | Step 219000 | Avg Loss: 0.0160 | Grad Norm: 0.00959898\n",
      "Epoch 1 | Step 219100 | Avg Loss: 0.0163 | Grad Norm: 0.00931841\n",
      "Epoch 1 | Step 219200 | Avg Loss: 0.0160 | Grad Norm: 0.00985927\n",
      "Epoch 1 | Step 219300 | Avg Loss: 0.0159 | Grad Norm: 0.00875311\n",
      "Epoch 1 | Step 219400 | Avg Loss: 0.0159 | Grad Norm: 0.00968663\n",
      "Epoch 1 | Step 219500 | Avg Loss: 0.0159 | Grad Norm: 0.00830075\n",
      "Epoch 1 | Step 219600 | Avg Loss: 0.0158 | Grad Norm: 0.00786001\n",
      "Epoch 1 | Step 219700 | Avg Loss: 0.0157 | Grad Norm: 0.00805308\n",
      "Epoch 1 | Step 219800 | Avg Loss: 0.0159 | Grad Norm: 0.00920155\n",
      "Epoch 1 | Step 219900 | Avg Loss: 0.0156 | Grad Norm: 0.00898578\n",
      "Epoch 1 | Step 220000 | Avg Loss: 0.0157 | Grad Norm: 0.00810831\n",
      "Epoch 1 | Step 220100 | Avg Loss: 0.0155 | Grad Norm: 0.00770316\n",
      "Epoch 1 | Step 220200 | Avg Loss: 0.0153 | Grad Norm: 0.00903009\n",
      "Epoch 1 | Step 220300 | Avg Loss: 0.0152 | Grad Norm: 0.00807010\n",
      "Epoch 1 | Step 220400 | Avg Loss: 0.0157 | Grad Norm: 0.00864258\n",
      "Epoch 1 | Step 220500 | Avg Loss: 0.0157 | Grad Norm: 0.01062866\n",
      "Epoch 1 | Step 220600 | Avg Loss: 0.0154 | Grad Norm: 0.01021874\n",
      "Epoch 1 | Step 220700 | Avg Loss: 0.0154 | Grad Norm: 0.01092974\n",
      "Epoch 1 | Step 220800 | Avg Loss: 0.0151 | Grad Norm: 0.00846968\n",
      "Epoch 1 | Step 220900 | Avg Loss: 0.0154 | Grad Norm: 0.00844896\n",
      "Epoch 1 | Step 221000 | Avg Loss: 0.0155 | Grad Norm: 0.00970360\n",
      "Epoch 1 | Step 221100 | Avg Loss: 0.0155 | Grad Norm: 0.00928167\n",
      "Epoch 1 | Step 221200 | Avg Loss: 0.0154 | Grad Norm: 0.00938502\n",
      "Epoch 1 | Step 221300 | Avg Loss: 0.0157 | Grad Norm: 0.01064695\n",
      "Epoch 1 | Step 221400 | Avg Loss: 0.0159 | Grad Norm: 0.00793514\n",
      "Epoch 1 | Step 221500 | Avg Loss: 0.0161 | Grad Norm: 0.01052965\n",
      "Epoch 1 | Step 221600 | Avg Loss: 0.0160 | Grad Norm: 0.00868041\n",
      "Epoch 1 | Step 221700 | Avg Loss: 0.0162 | Grad Norm: 0.00798292\n",
      "Epoch 1 | Step 221800 | Avg Loss: 0.0164 | Grad Norm: 0.00850395\n",
      "Epoch 1 | Step 221900 | Avg Loss: 0.0158 | Grad Norm: 0.00852528\n",
      "Epoch 1 | Step 222000 | Avg Loss: 0.0160 | Grad Norm: 0.00796315\n",
      "Epoch 1 | Step 222100 | Avg Loss: 0.0156 | Grad Norm: 0.01085434\n",
      "Epoch 1 | Step 222200 | Avg Loss: 0.0155 | Grad Norm: 0.00841523\n",
      "Epoch 1 | Step 222300 | Avg Loss: 0.0158 | Grad Norm: 0.01069486\n",
      "Epoch 1 | Step 222400 | Avg Loss: 0.0158 | Grad Norm: 0.00846288\n",
      "Epoch 1 | Step 222500 | Avg Loss: 0.0157 | Grad Norm: 0.00945687\n",
      "Epoch 1 | Step 222600 | Avg Loss: 0.0154 | Grad Norm: 0.00929188\n",
      "Epoch 1 | Step 222700 | Avg Loss: 0.0155 | Grad Norm: 0.00870094\n",
      "Epoch 1 | Step 222800 | Avg Loss: 0.0156 | Grad Norm: 0.00755696\n",
      "Epoch 1 | Step 222900 | Avg Loss: 0.0158 | Grad Norm: 0.01004940\n",
      "Epoch 1 | Step 223000 | Avg Loss: 0.0160 | Grad Norm: 0.00851665\n",
      "Epoch 1 | Step 223100 | Avg Loss: 0.0161 | Grad Norm: 0.00729946\n",
      "Epoch 1 | Step 223200 | Avg Loss: 0.0158 | Grad Norm: 0.00977950\n",
      "Epoch 1 | Step 223300 | Avg Loss: 0.0157 | Grad Norm: 0.00852296\n",
      "Epoch 1 | Step 223400 | Avg Loss: 0.0156 | Grad Norm: 0.00858953\n",
      "Epoch 1 | Step 223500 | Avg Loss: 0.0155 | Grad Norm: 0.00940358\n",
      "Epoch 1 | Step 223600 | Avg Loss: 0.0155 | Grad Norm: 0.00874583\n",
      "Epoch 1 | Step 223700 | Avg Loss: 0.0154 | Grad Norm: 0.00843142\n",
      "Epoch 1 | Step 223800 | Avg Loss: 0.0153 | Grad Norm: 0.01001781\n",
      "Epoch 1 | Step 223900 | Avg Loss: 0.0151 | Grad Norm: 0.00916308\n",
      "Epoch 1 | Step 224000 | Avg Loss: 0.0154 | Grad Norm: 0.00859225\n",
      "Epoch 1 | Step 224100 | Avg Loss: 0.0153 | Grad Norm: 0.00868209\n",
      "Epoch 1 | Step 224200 | Avg Loss: 0.0156 | Grad Norm: 0.00977041\n",
      "Epoch 1 | Step 224300 | Avg Loss: 0.0158 | Grad Norm: 0.01028959\n",
      "Epoch 1 | Step 224400 | Avg Loss: 0.0156 | Grad Norm: 0.00961538\n",
      "Epoch 1 | Step 224500 | Avg Loss: 0.0157 | Grad Norm: 0.00846900\n",
      "Epoch 1 | Step 224600 | Avg Loss: 0.0159 | Grad Norm: 0.00766971\n",
      "Epoch 1 | Step 224700 | Avg Loss: 0.0162 | Grad Norm: 0.00978795\n",
      "Epoch 1 | Step 224800 | Avg Loss: 0.0160 | Grad Norm: 0.00951779\n",
      "Epoch 1 | Step 224900 | Avg Loss: 0.0161 | Grad Norm: 0.00840223\n",
      "Epoch 1 | Step 225000 | Avg Loss: 0.0164 | Grad Norm: 0.00956386\n",
      "Epoch 1 | Step 225100 | Avg Loss: 0.0162 | Grad Norm: 0.00811437\n",
      "Epoch 1 | Step 225200 | Avg Loss: 0.0154 | Grad Norm: 0.00867528\n",
      "Epoch 1 | Step 225300 | Avg Loss: 0.0157 | Grad Norm: 0.00823956\n",
      "Epoch 1 | Step 225400 | Avg Loss: 0.0158 | Grad Norm: 0.00870502\n",
      "Epoch 1 | Step 225500 | Avg Loss: 0.0156 | Grad Norm: 0.00737723\n",
      "Epoch 1 | Step 225600 | Avg Loss: 0.0160 | Grad Norm: 0.00992909\n",
      "Epoch 1 | Step 225700 | Avg Loss: 0.0162 | Grad Norm: 0.00921998\n",
      "Epoch 1 | Step 225800 | Avg Loss: 0.0160 | Grad Norm: 0.00838514\n",
      "Epoch 1 | Step 225900 | Avg Loss: 0.0159 | Grad Norm: 0.00903144\n",
      "Epoch 1 | Step 226000 | Avg Loss: 0.0160 | Grad Norm: 0.00806419\n",
      "Epoch 1 | Step 226100 | Avg Loss: 0.0158 | Grad Norm: 0.00895230\n",
      "Epoch 1 | Step 226200 | Avg Loss: 0.0159 | Grad Norm: 0.00914153\n",
      "Epoch 1 | Step 226300 | Avg Loss: 0.0156 | Grad Norm: 0.00855337\n",
      "Epoch 1 | Step 226400 | Avg Loss: 0.0159 | Grad Norm: 0.00842963\n",
      "Epoch 1 | Step 226500 | Avg Loss: 0.0158 | Grad Norm: 0.00830115\n",
      "Epoch 1 | Step 226600 | Avg Loss: 0.0159 | Grad Norm: 0.00916358\n",
      "Epoch 1 | Step 226700 | Avg Loss: 0.0154 | Grad Norm: 0.00888745\n",
      "Epoch 1 | Step 226800 | Avg Loss: 0.0155 | Grad Norm: 0.01021327\n",
      "Epoch 1 | Step 226900 | Avg Loss: 0.0151 | Grad Norm: 0.00923914\n",
      "Epoch 1 | Step 227000 | Avg Loss: 0.0151 | Grad Norm: 0.00766600\n",
      "Epoch 1 | Step 227100 | Avg Loss: 0.0150 | Grad Norm: 0.00806740\n",
      "Epoch 1 | Step 227200 | Avg Loss: 0.0153 | Grad Norm: 0.00882880\n",
      "Epoch 1 | Step 227300 | Avg Loss: 0.0155 | Grad Norm: 0.01026902\n",
      "Epoch 1 | Step 227400 | Avg Loss: 0.0157 | Grad Norm: 0.00918529\n",
      "Epoch 1 | Step 227500 | Avg Loss: 0.0161 | Grad Norm: 0.00979135\n",
      "Epoch 1 | Step 227600 | Avg Loss: 0.0164 | Grad Norm: 0.00812833\n",
      "Epoch 1 | Step 227700 | Avg Loss: 0.0159 | Grad Norm: 0.00925627\n",
      "Epoch 1 | Step 227800 | Avg Loss: 0.0159 | Grad Norm: 0.00824303\n",
      "Epoch 1 | Step 227900 | Avg Loss: 0.0160 | Grad Norm: 0.00867970\n",
      "Epoch 1 | Step 228000 | Avg Loss: 0.0158 | Grad Norm: 0.00851600\n",
      "Epoch 1 | Step 228100 | Avg Loss: 0.0160 | Grad Norm: 0.00799512\n",
      "Epoch 1 | Step 228200 | Avg Loss: 0.0158 | Grad Norm: 0.00898954\n",
      "Epoch 1 | Step 228300 | Avg Loss: 0.0160 | Grad Norm: 0.00939298\n",
      "Epoch 1 | Step 228400 | Avg Loss: 0.0163 | Grad Norm: 0.01053704\n",
      "Epoch 1 | Step 228500 | Avg Loss: 0.0160 | Grad Norm: 0.00816121\n",
      "Epoch 1 | Step 228600 | Avg Loss: 0.0158 | Grad Norm: 0.00857985\n",
      "Epoch 1 | Step 228700 | Avg Loss: 0.0161 | Grad Norm: 0.00847075\n",
      "Epoch 1 | Step 228800 | Avg Loss: 0.0157 | Grad Norm: 0.00791466\n",
      "Epoch 1 | Step 228900 | Avg Loss: 0.0158 | Grad Norm: 0.00893377\n",
      "Epoch 1 | Step 229000 | Avg Loss: 0.0154 | Grad Norm: 0.00781195\n",
      "Epoch 1 | Step 229100 | Avg Loss: 0.0155 | Grad Norm: 0.00817333\n",
      "Epoch 1 | Step 229200 | Avg Loss: 0.0157 | Grad Norm: 0.00748004\n",
      "Epoch 1 | Step 229300 | Avg Loss: 0.0157 | Grad Norm: 0.00851290\n",
      "Epoch 1 | Step 229400 | Avg Loss: 0.0152 | Grad Norm: 0.00917703\n",
      "Epoch 1 | Step 229500 | Avg Loss: 0.0152 | Grad Norm: 0.00907601\n",
      "Epoch 1 | Step 229600 | Avg Loss: 0.0157 | Grad Norm: 0.00982176\n",
      "Epoch 1 | Step 229700 | Avg Loss: 0.0156 | Grad Norm: 0.00839049\n",
      "Epoch 1 | Step 229800 | Avg Loss: 0.0154 | Grad Norm: 0.01026629\n",
      "Epoch 1 | Step 229900 | Avg Loss: 0.0154 | Grad Norm: 0.00897093\n",
      "Epoch 1 | Step 230000 | Avg Loss: 0.0152 | Grad Norm: 0.00760763\n",
      "Epoch 1 | Step 230100 | Avg Loss: 0.0153 | Grad Norm: 0.00816237\n",
      "Epoch 1 | Step 230200 | Avg Loss: 0.0150 | Grad Norm: 0.00822296\n",
      "Epoch 1 | Step 230300 | Avg Loss: 0.0150 | Grad Norm: 0.00990639\n",
      "Epoch 1 | Step 230400 | Avg Loss: 0.0150 | Grad Norm: 0.00763608\n",
      "Epoch 1 | Step 230500 | Avg Loss: 0.0149 | Grad Norm: 0.00754389\n",
      "Epoch 1 | Step 230600 | Avg Loss: 0.0155 | Grad Norm: 0.00883372\n",
      "Epoch 1 | Step 230700 | Avg Loss: 0.0154 | Grad Norm: 0.01071032\n",
      "Epoch 1 | Step 230800 | Avg Loss: 0.0155 | Grad Norm: 0.00810477\n",
      "Epoch 1 | Step 230900 | Avg Loss: 0.0155 | Grad Norm: 0.01153638\n",
      "Epoch 1 | Step 231000 | Avg Loss: 0.0154 | Grad Norm: 0.00996847\n",
      "Epoch 1 | Step 231100 | Avg Loss: 0.0154 | Grad Norm: 0.00821496\n",
      "Epoch 1 | Step 231200 | Avg Loss: 0.0154 | Grad Norm: 0.00901076\n",
      "Epoch 1 | Step 231300 | Avg Loss: 0.0158 | Grad Norm: 0.00878368\n",
      "Epoch 1 | Step 231400 | Avg Loss: 0.0157 | Grad Norm: 0.00819336\n",
      "Epoch 1 | Step 231500 | Avg Loss: 0.0160 | Grad Norm: 0.00925870\n",
      "Epoch 1 | Step 231600 | Avg Loss: 0.0156 | Grad Norm: 0.00839500\n",
      "Epoch 1 | Step 231700 | Avg Loss: 0.0159 | Grad Norm: 0.00925681\n",
      "Epoch 1 | Step 231800 | Avg Loss: 0.0158 | Grad Norm: 0.00778785\n",
      "Epoch 1 | Step 231900 | Avg Loss: 0.0157 | Grad Norm: 0.00860231\n",
      "Epoch 1 | Step 232000 | Avg Loss: 0.0161 | Grad Norm: 0.00912667\n",
      "Epoch 1 | Step 232100 | Avg Loss: 0.0160 | Grad Norm: 0.00778308\n",
      "Epoch 1 | Step 232200 | Avg Loss: 0.0159 | Grad Norm: 0.00860747\n",
      "Epoch 1 | Step 232300 | Avg Loss: 0.0158 | Grad Norm: 0.00949319\n",
      "Epoch 1 | Step 232400 | Avg Loss: 0.0157 | Grad Norm: 0.00969735\n",
      "Epoch 1 | Step 232500 | Avg Loss: 0.0153 | Grad Norm: 0.00817644\n",
      "Epoch 1 | Step 232600 | Avg Loss: 0.0157 | Grad Norm: 0.00842535\n",
      "Epoch 1 | Step 232700 | Avg Loss: 0.0156 | Grad Norm: 0.00776970\n",
      "Epoch 1 | Step 232800 | Avg Loss: 0.0159 | Grad Norm: 0.01085928\n",
      "Epoch 1 | Step 232900 | Avg Loss: 0.0158 | Grad Norm: 0.00812964\n",
      "Epoch 1 | Step 233000 | Avg Loss: 0.0160 | Grad Norm: 0.00979065\n",
      "Epoch 1 | Step 233100 | Avg Loss: 0.0160 | Grad Norm: 0.00853366\n",
      "Epoch 1 | Step 233200 | Avg Loss: 0.0160 | Grad Norm: 0.00969185\n",
      "Epoch 1 | Step 233300 | Avg Loss: 0.0156 | Grad Norm: 0.00906250\n",
      "Epoch 1 | Step 233400 | Avg Loss: 0.0154 | Grad Norm: 0.00856867\n",
      "Epoch 1 | Step 233500 | Avg Loss: 0.0159 | Grad Norm: 0.00939441\n",
      "Epoch 1 | Step 233600 | Avg Loss: 0.0160 | Grad Norm: 0.00982269\n",
      "Epoch 1 | Step 233700 | Avg Loss: 0.0156 | Grad Norm: 0.00772160\n",
      "Epoch 1 | Step 233800 | Avg Loss: 0.0154 | Grad Norm: 0.00904066\n",
      "Epoch 1 | Step 233900 | Avg Loss: 0.0154 | Grad Norm: 0.01074831\n",
      "Epoch 1 | Step 234000 | Avg Loss: 0.0155 | Grad Norm: 0.00889955\n",
      "Epoch 1 | Step 234100 | Avg Loss: 0.0157 | Grad Norm: 0.00902689\n",
      "Epoch 1 | Step 234200 | Avg Loss: 0.0163 | Grad Norm: 0.00900608\n",
      "Epoch 1 | Step 234300 | Avg Loss: 0.0156 | Grad Norm: 0.01277389\n",
      "Epoch 1 | Step 234400 | Avg Loss: 0.0154 | Grad Norm: 0.00866652\n",
      "Epoch 1 | Step 234500 | Avg Loss: 0.0159 | Grad Norm: 0.00822616\n",
      "Epoch 1 | Step 234600 | Avg Loss: 0.0154 | Grad Norm: 0.01021744\n",
      "Epoch 1 | Step 234700 | Avg Loss: 0.0157 | Grad Norm: 0.00761878\n",
      "Epoch 1 | Step 234800 | Avg Loss: 0.0160 | Grad Norm: 0.00796484\n",
      "Epoch 1 | Step 234900 | Avg Loss: 0.0158 | Grad Norm: 0.01035703\n",
      "Epoch 1 | Step 235000 | Avg Loss: 0.0156 | Grad Norm: 0.00969201\n",
      "Epoch 1 | Step 235100 | Avg Loss: 0.0156 | Grad Norm: 0.00897568\n",
      "Epoch 1 | Step 235200 | Avg Loss: 0.0155 | Grad Norm: 0.00983792\n",
      "Epoch 1 | Step 235300 | Avg Loss: 0.0156 | Grad Norm: 0.01043260\n",
      "Epoch 1 | Step 235400 | Avg Loss: 0.0158 | Grad Norm: 0.00984975\n",
      "Epoch 1 | Step 235500 | Avg Loss: 0.0155 | Grad Norm: 0.01097117\n",
      "Epoch 1 | Step 235600 | Avg Loss: 0.0155 | Grad Norm: 0.01231943\n",
      "Epoch 1 | Step 235700 | Avg Loss: 0.0156 | Grad Norm: 0.00859948\n",
      "Epoch 1 | Step 235800 | Avg Loss: 0.0156 | Grad Norm: 0.00883051\n",
      "Epoch 1 | Step 235900 | Avg Loss: 0.0157 | Grad Norm: 0.00954099\n",
      "Epoch 1 | Step 236000 | Avg Loss: 0.0156 | Grad Norm: 0.00958753\n",
      "Epoch 1 | Step 236100 | Avg Loss: 0.0155 | Grad Norm: 0.00926617\n",
      "Epoch 1 | Step 236200 | Avg Loss: 0.0155 | Grad Norm: 0.00773390\n",
      "Epoch 1 | Step 236300 | Avg Loss: 0.0154 | Grad Norm: 0.00830965\n",
      "Epoch 1 | Step 236400 | Avg Loss: 0.0158 | Grad Norm: 0.00931932\n",
      "Epoch 1 | Step 236500 | Avg Loss: 0.0156 | Grad Norm: 0.00919491\n",
      "Epoch 1 | Step 236600 | Avg Loss: 0.0153 | Grad Norm: 0.00799960\n",
      "Epoch 1 | Step 236700 | Avg Loss: 0.0151 | Grad Norm: 0.00880805\n",
      "Epoch 1 | Step 236800 | Avg Loss: 0.0155 | Grad Norm: 0.00822116\n",
      "Epoch 1 | Step 236900 | Avg Loss: 0.0156 | Grad Norm: 0.01018724\n",
      "Epoch 1 | Step 237000 | Avg Loss: 0.0159 | Grad Norm: 0.00843324\n",
      "Epoch 1 | Step 237100 | Avg Loss: 0.0159 | Grad Norm: 0.00814513\n",
      "Epoch 1 | Step 237200 | Avg Loss: 0.0161 | Grad Norm: 0.00790253\n",
      "Epoch 1 | Step 237300 | Avg Loss: 0.0160 | Grad Norm: 0.00851363\n",
      "Epoch 1 | Step 237400 | Avg Loss: 0.0159 | Grad Norm: 0.00870677\n",
      "Epoch 1 | Step 237500 | Avg Loss: 0.0162 | Grad Norm: 0.00937993\n",
      "Epoch 1 | Step 237600 | Avg Loss: 0.0158 | Grad Norm: 0.00871844\n",
      "Epoch 1 | Step 237700 | Avg Loss: 0.0160 | Grad Norm: 0.00888383\n",
      "Epoch 1 | Step 237800 | Avg Loss: 0.0158 | Grad Norm: 0.00884313\n",
      "Epoch 1 | Step 237900 | Avg Loss: 0.0156 | Grad Norm: 0.00769413\n",
      "Epoch 1 | Step 238000 | Avg Loss: 0.0158 | Grad Norm: 0.00802039\n",
      "Epoch 1 | Step 238100 | Avg Loss: 0.0156 | Grad Norm: 0.00906320\n",
      "Epoch 1 | Step 238200 | Avg Loss: 0.0159 | Grad Norm: 0.00784279\n",
      "Epoch 1 | Step 238300 | Avg Loss: 0.0155 | Grad Norm: 0.00769414\n",
      "Epoch 1 | Step 238400 | Avg Loss: 0.0156 | Grad Norm: 0.00972241\n",
      "Epoch 1 | Step 238500 | Avg Loss: 0.0155 | Grad Norm: 0.00962910\n",
      "Epoch 1 | Step 238600 | Avg Loss: 0.0157 | Grad Norm: 0.00869390\n",
      "Epoch 1 | Step 238700 | Avg Loss: 0.0159 | Grad Norm: 0.00877654\n",
      "Epoch 1 | Step 238800 | Avg Loss: 0.0160 | Grad Norm: 0.00858120\n",
      "Epoch 1 | Step 238900 | Avg Loss: 0.0157 | Grad Norm: 0.00814663\n",
      "Epoch 1 | Step 239000 | Avg Loss: 0.0157 | Grad Norm: 0.00851285\n",
      "Epoch 1 | Step 239100 | Avg Loss: 0.0152 | Grad Norm: 0.00916032\n",
      "Epoch 1 | Step 239200 | Avg Loss: 0.0152 | Grad Norm: 0.00812734\n",
      "Epoch 1 | Step 239300 | Avg Loss: 0.0155 | Grad Norm: 0.00819638\n",
      "Epoch 1 | Step 239400 | Avg Loss: 0.0155 | Grad Norm: 0.00935383\n",
      "Epoch 1 | Step 239500 | Avg Loss: 0.0159 | Grad Norm: 0.00904329\n",
      "Epoch 1 | Step 239600 | Avg Loss: 0.0159 | Grad Norm: 0.00863608\n",
      "Epoch 1 | Step 239700 | Avg Loss: 0.0156 | Grad Norm: 0.01037956\n",
      "Epoch 1 | Step 239800 | Avg Loss: 0.0155 | Grad Norm: 0.00866757\n",
      "Epoch 1 | Step 239900 | Avg Loss: 0.0156 | Grad Norm: 0.00807498\n",
      "Epoch 1 | Step 240000 | Avg Loss: 0.0158 | Grad Norm: 0.00997737\n",
      "Epoch 1 | Step 240100 | Avg Loss: 0.0154 | Grad Norm: 0.00721484\n",
      "Epoch 1 | Step 240200 | Avg Loss: 0.0154 | Grad Norm: 0.00895915\n",
      "Epoch 1 | Step 240300 | Avg Loss: 0.0157 | Grad Norm: 0.01026420\n",
      "Epoch 1 | Step 240400 | Avg Loss: 0.0160 | Grad Norm: 0.00881228\n",
      "Epoch 1 | Step 240500 | Avg Loss: 0.0161 | Grad Norm: 0.00988467\n",
      "Epoch 1 | Step 240600 | Avg Loss: 0.0158 | Grad Norm: 0.01010970\n",
      "Epoch 1 | Step 240700 | Avg Loss: 0.0158 | Grad Norm: 0.01065469\n",
      "Epoch 1 | Step 240800 | Avg Loss: 0.0158 | Grad Norm: 0.01159448\n",
      "Epoch 1 | Step 240900 | Avg Loss: 0.0160 | Grad Norm: 0.00907171\n",
      "Epoch 1 | Step 241000 | Avg Loss: 0.0159 | Grad Norm: 0.00849776\n",
      "Epoch 1 | Step 241100 | Avg Loss: 0.0158 | Grad Norm: 0.00967185\n",
      "Epoch 1 | Step 241200 | Avg Loss: 0.0159 | Grad Norm: 0.00727906\n",
      "Epoch 1 | Step 241300 | Avg Loss: 0.0160 | Grad Norm: 0.00792959\n",
      "Epoch 1 | Step 241400 | Avg Loss: 0.0160 | Grad Norm: 0.00999919\n",
      "Epoch 1 | Step 241500 | Avg Loss: 0.0159 | Grad Norm: 0.00787207\n",
      "Epoch 1 | Step 241600 | Avg Loss: 0.0158 | Grad Norm: 0.00918174\n",
      "Epoch 1 | Step 241700 | Avg Loss: 0.0159 | Grad Norm: 0.00783992\n",
      "Epoch 1 | Step 241800 | Avg Loss: 0.0160 | Grad Norm: 0.01020013\n",
      "Epoch 1 | Step 241900 | Avg Loss: 0.0163 | Grad Norm: 0.00756956\n",
      "Epoch 1 | Step 242000 | Avg Loss: 0.0161 | Grad Norm: 0.00789049\n",
      "Epoch 1 | Step 242100 | Avg Loss: 0.0160 | Grad Norm: 0.00875636\n",
      "Epoch 1 | Step 242200 | Avg Loss: 0.0157 | Grad Norm: 0.00915807\n",
      "Epoch 1 | Step 242300 | Avg Loss: 0.0154 | Grad Norm: 0.00845247\n",
      "Epoch 1 | Step 242400 | Avg Loss: 0.0155 | Grad Norm: 0.00972077\n",
      "Epoch 1 | Step 242500 | Avg Loss: 0.0153 | Grad Norm: 0.00796200\n",
      "Epoch 1 | Step 242600 | Avg Loss: 0.0155 | Grad Norm: 0.00965951\n",
      "Epoch 1 | Step 242700 | Avg Loss: 0.0156 | Grad Norm: 0.00875330\n",
      "Epoch 1 | Step 242800 | Avg Loss: 0.0155 | Grad Norm: 0.01007062\n",
      "Epoch 1 | Step 242900 | Avg Loss: 0.0154 | Grad Norm: 0.00926115\n",
      "Epoch 1 | Step 243000 | Avg Loss: 0.0149 | Grad Norm: 0.00848604\n",
      "Epoch 1 | Step 243100 | Avg Loss: 0.0152 | Grad Norm: 0.00828619\n",
      "Epoch 1 | Step 243200 | Avg Loss: 0.0152 | Grad Norm: 0.00938019\n",
      "Epoch 1 | Step 243300 | Avg Loss: 0.0152 | Grad Norm: 0.00806652\n",
      "Epoch 1 | Step 243400 | Avg Loss: 0.0155 | Grad Norm: 0.00799903\n",
      "Epoch 1 | Step 243500 | Avg Loss: 0.0159 | Grad Norm: 0.00983355\n",
      "Epoch 1 | Step 243600 | Avg Loss: 0.0158 | Grad Norm: 0.00788718\n",
      "Epoch 1 | Step 243700 | Avg Loss: 0.0156 | Grad Norm: 0.00845000\n",
      "Epoch 1 | Step 243800 | Avg Loss: 0.0154 | Grad Norm: 0.00826743\n",
      "Epoch 1 | Step 243900 | Avg Loss: 0.0156 | Grad Norm: 0.00798342\n",
      "Epoch 1 | Step 244000 | Avg Loss: 0.0156 | Grad Norm: 0.00940134\n",
      "Epoch 1 | Step 244100 | Avg Loss: 0.0159 | Grad Norm: 0.00850185\n",
      "Epoch 1 | Step 244200 | Avg Loss: 0.0156 | Grad Norm: 0.00710414\n",
      "Epoch 1 | Step 244300 | Avg Loss: 0.0158 | Grad Norm: 0.00814854\n",
      "Epoch 1 | Step 244400 | Avg Loss: 0.0156 | Grad Norm: 0.00821158\n",
      "Epoch 1 | Step 244500 | Avg Loss: 0.0154 | Grad Norm: 0.00905749\n",
      "Epoch 1 | Step 244600 | Avg Loss: 0.0152 | Grad Norm: 0.00811486\n",
      "Epoch 1 | Step 244700 | Avg Loss: 0.0152 | Grad Norm: 0.00839625\n",
      "Epoch 1 | Step 244800 | Avg Loss: 0.0152 | Grad Norm: 0.00854549\n",
      "Epoch 1 | Step 244900 | Avg Loss: 0.0151 | Grad Norm: 0.00934141\n",
      "Epoch 1 | Step 245000 | Avg Loss: 0.0151 | Grad Norm: 0.00818619\n",
      "Epoch 1 | Step 245100 | Avg Loss: 0.0154 | Grad Norm: 0.00812289\n",
      "Epoch 1 | Step 245200 | Avg Loss: 0.0156 | Grad Norm: 0.00855870\n",
      "Epoch 1 | Step 245300 | Avg Loss: 0.0152 | Grad Norm: 0.00803584\n",
      "Epoch 1 | Step 245400 | Avg Loss: 0.0154 | Grad Norm: 0.00925608\n",
      "Epoch 1 | Step 245500 | Avg Loss: 0.0156 | Grad Norm: 0.00839145\n",
      "Epoch 1 | Step 245600 | Avg Loss: 0.0155 | Grad Norm: 0.00919338\n",
      "Epoch 1 | Step 245700 | Avg Loss: 0.0156 | Grad Norm: 0.00842826\n",
      "Epoch 1 | Step 245800 | Avg Loss: 0.0161 | Grad Norm: 0.00985236\n",
      "Epoch 1 | Step 245900 | Avg Loss: 0.0161 | Grad Norm: 0.00865736\n",
      "Epoch 1 | Step 246000 | Avg Loss: 0.0161 | Grad Norm: 0.00893324\n",
      "Epoch 1 | Step 246100 | Avg Loss: 0.0160 | Grad Norm: 0.00977676\n",
      "Epoch 1 | Step 246200 | Avg Loss: 0.0159 | Grad Norm: 0.00906111\n",
      "Epoch 1 | Step 246300 | Avg Loss: 0.0159 | Grad Norm: 0.00796533\n",
      "Epoch 1 | Step 246400 | Avg Loss: 0.0158 | Grad Norm: 0.00945862\n",
      "Epoch 1 | Step 246500 | Avg Loss: 0.0158 | Grad Norm: 0.00825449\n",
      "Epoch 1 | Step 246600 | Avg Loss: 0.0159 | Grad Norm: 0.00850260\n",
      "Epoch 1 | Step 246700 | Avg Loss: 0.0158 | Grad Norm: 0.00853197\n",
      "Epoch 1 | Step 246800 | Avg Loss: 0.0158 | Grad Norm: 0.00884797\n",
      "Epoch 1 | Step 246900 | Avg Loss: 0.0155 | Grad Norm: 0.00804841\n",
      "Epoch 1 | Step 247000 | Avg Loss: 0.0157 | Grad Norm: 0.00879917\n",
      "Epoch 1 | Step 247100 | Avg Loss: 0.0160 | Grad Norm: 0.01098254\n",
      "Epoch 1 | Step 247200 | Avg Loss: 0.0159 | Grad Norm: 0.00792512\n",
      "Epoch 1 | Step 247300 | Avg Loss: 0.0156 | Grad Norm: 0.00814129\n",
      "Epoch 1 | Step 247400 | Avg Loss: 0.0156 | Grad Norm: 0.00799860\n",
      "Epoch 1 | Step 247500 | Avg Loss: 0.0154 | Grad Norm: 0.00941237\n",
      "Epoch 1 | Step 247600 | Avg Loss: 0.0159 | Grad Norm: 0.00946024\n",
      "Epoch 1 | Step 247700 | Avg Loss: 0.0158 | Grad Norm: 0.00806483\n",
      "Epoch 1 | Step 247800 | Avg Loss: 0.0162 | Grad Norm: 0.00994178\n",
      "Epoch 1 | Step 247900 | Avg Loss: 0.0162 | Grad Norm: 0.00887343\n",
      "Epoch 1 | Step 248000 | Avg Loss: 0.0161 | Grad Norm: 0.00889456\n",
      "Epoch 1 | Step 248100 | Avg Loss: 0.0160 | Grad Norm: 0.00826116\n",
      "Epoch 1 | Step 248200 | Avg Loss: 0.0163 | Grad Norm: 0.00795526\n",
      "Epoch 1 | Step 248300 | Avg Loss: 0.0160 | Grad Norm: 0.00873837\n",
      "Epoch 1 | Step 248400 | Avg Loss: 0.0156 | Grad Norm: 0.00812889\n",
      "Epoch 1 | Step 248500 | Avg Loss: 0.0154 | Grad Norm: 0.00859084\n",
      "Epoch 1 | Step 248600 | Avg Loss: 0.0155 | Grad Norm: 0.00872913\n",
      "Epoch 1 | Step 248700 | Avg Loss: 0.0157 | Grad Norm: 0.00856795\n",
      "Epoch 1 | Step 248800 | Avg Loss: 0.0160 | Grad Norm: 0.01057880\n",
      "Epoch 1 | Step 248900 | Avg Loss: 0.0160 | Grad Norm: 0.00942931\n",
      "Epoch 1 | Step 249000 | Avg Loss: 0.0158 | Grad Norm: 0.00876976\n",
      "Epoch 1 | Step 249100 | Avg Loss: 0.0159 | Grad Norm: 0.01008748\n",
      "Epoch 1 | Step 249200 | Avg Loss: 0.0156 | Grad Norm: 0.01013563\n",
      "Epoch 1 | Step 249300 | Avg Loss: 0.0157 | Grad Norm: 0.00977620\n",
      "Epoch 1 | Step 249400 | Avg Loss: 0.0155 | Grad Norm: 0.00952620\n",
      "Epoch 1 | Step 249500 | Avg Loss: 0.0156 | Grad Norm: 0.00811554\n",
      "Epoch 1 | Step 249600 | Avg Loss: 0.0157 | Grad Norm: 0.00878396\n",
      "Epoch 1 | Step 249700 | Avg Loss: 0.0155 | Grad Norm: 0.00924833\n",
      "Epoch 1 | Step 249800 | Avg Loss: 0.0158 | Grad Norm: 0.00835549\n",
      "Epoch 1 | Step 249900 | Avg Loss: 0.0157 | Grad Norm: 0.00885646\n",
      "Epoch 1 | Step 250000 | Avg Loss: 0.0153 | Grad Norm: 0.00711909\n",
      "Epoch 1 | Step 250100 | Avg Loss: 0.0154 | Grad Norm: 0.00810608\n",
      "Epoch 1 | Step 250200 | Avg Loss: 0.0152 | Grad Norm: 0.01011639\n",
      "Epoch 1 | Step 250300 | Avg Loss: 0.0150 | Grad Norm: 0.00974653\n",
      "Epoch 1 | Step 250400 | Avg Loss: 0.0146 | Grad Norm: 0.00837427\n",
      "Epoch 1 | Step 250500 | Avg Loss: 0.0148 | Grad Norm: 0.00985795\n",
      "Epoch 1 | Step 250600 | Avg Loss: 0.0152 | Grad Norm: 0.00715715\n",
      "Epoch 1 | Step 250700 | Avg Loss: 0.0151 | Grad Norm: 0.00805623\n",
      "Epoch 1 | Step 250800 | Avg Loss: 0.0148 | Grad Norm: 0.00807833\n",
      "Epoch 1 | Step 250900 | Avg Loss: 0.0150 | Grad Norm: 0.01179668\n",
      "Epoch 1 | Step 251000 | Avg Loss: 0.0149 | Grad Norm: 0.00830614\n",
      "Epoch 1 | Step 251100 | Avg Loss: 0.0144 | Grad Norm: 0.00905840\n",
      "Epoch 1 | Step 251200 | Avg Loss: 0.0144 | Grad Norm: 0.00888897\n",
      "Epoch 1 | Step 251300 | Avg Loss: 0.0147 | Grad Norm: 0.00801853\n",
      "Epoch 1 | Step 251400 | Avg Loss: 0.0149 | Grad Norm: 0.00875045\n",
      "Epoch 1 | Step 251500 | Avg Loss: 0.0149 | Grad Norm: 0.00955976\n",
      "Epoch 1 | Step 251600 | Avg Loss: 0.0153 | Grad Norm: 0.00861784\n",
      "Epoch 1 | Step 251700 | Avg Loss: 0.0156 | Grad Norm: 0.00872061\n",
      "Epoch 1 | Step 251800 | Avg Loss: 0.0153 | Grad Norm: 0.00794921\n",
      "Epoch 1 | Step 251900 | Avg Loss: 0.0157 | Grad Norm: 0.01088311\n",
      "Epoch 1 | Step 252000 | Avg Loss: 0.0158 | Grad Norm: 0.00778719\n",
      "Epoch 1 | Step 252100 | Avg Loss: 0.0158 | Grad Norm: 0.00833757\n",
      "Epoch 1 | Step 252200 | Avg Loss: 0.0157 | Grad Norm: 0.00865179\n",
      "Epoch 1 | Step 252300 | Avg Loss: 0.0155 | Grad Norm: 0.01044034\n",
      "Epoch 1 | Step 252400 | Avg Loss: 0.0159 | Grad Norm: 0.01341135\n",
      "Epoch 1 | Step 252500 | Avg Loss: 0.0161 | Grad Norm: 0.00954048\n",
      "Epoch 1 | Step 252600 | Avg Loss: 0.0158 | Grad Norm: 0.00782678\n",
      "Epoch 1 | Step 252700 | Avg Loss: 0.0156 | Grad Norm: 0.00971546\n",
      "Epoch 1 | Step 252800 | Avg Loss: 0.0157 | Grad Norm: 0.00867491\n",
      "Epoch 1 | Step 252900 | Avg Loss: 0.0156 | Grad Norm: 0.01164730\n",
      "Epoch 1 | Step 253000 | Avg Loss: 0.0156 | Grad Norm: 0.00927443\n",
      "Epoch 1 | Step 253100 | Avg Loss: 0.0156 | Grad Norm: 0.00935972\n",
      "Epoch 1 | Step 253200 | Avg Loss: 0.0153 | Grad Norm: 0.00813412\n",
      "Epoch 1 | Step 253300 | Avg Loss: 0.0156 | Grad Norm: 0.00948534\n",
      "Epoch 1 | Step 253400 | Avg Loss: 0.0155 | Grad Norm: 0.00888928\n",
      "Epoch 1 | Step 253500 | Avg Loss: 0.0157 | Grad Norm: 0.00983217\n",
      "Epoch 1 | Step 253600 | Avg Loss: 0.0155 | Grad Norm: 0.00919791\n",
      "Epoch 1 | Step 253700 | Avg Loss: 0.0155 | Grad Norm: 0.00842292\n",
      "Epoch 1 | Step 253800 | Avg Loss: 0.0154 | Grad Norm: 0.00781881\n",
      "Epoch 1 | Step 253900 | Avg Loss: 0.0152 | Grad Norm: 0.00887473\n",
      "Epoch 1 | Step 254000 | Avg Loss: 0.0155 | Grad Norm: 0.00850951\n",
      "Epoch 1 | Step 254100 | Avg Loss: 0.0151 | Grad Norm: 0.00999745\n",
      "Epoch 1 | Step 254200 | Avg Loss: 0.0154 | Grad Norm: 0.00956859\n",
      "Epoch 1 | Step 254300 | Avg Loss: 0.0160 | Grad Norm: 0.00907085\n",
      "Epoch 1 | Step 254400 | Avg Loss: 0.0160 | Grad Norm: 0.00849680\n",
      "Epoch 1 | Step 254500 | Avg Loss: 0.0156 | Grad Norm: 0.00825318\n",
      "Epoch 1 | Step 254600 | Avg Loss: 0.0161 | Grad Norm: 0.00987122\n",
      "Epoch 1 | Step 254700 | Avg Loss: 0.0157 | Grad Norm: 0.00903743\n",
      "Epoch 1 | Step 254800 | Avg Loss: 0.0156 | Grad Norm: 0.00836894\n",
      "Epoch 1 | Step 254900 | Avg Loss: 0.0156 | Grad Norm: 0.00965455\n",
      "Epoch 1 | Step 255000 | Avg Loss: 0.0155 | Grad Norm: 0.00852373\n",
      "Epoch 1 | Step 255100 | Avg Loss: 0.0154 | Grad Norm: 0.01003532\n",
      "Epoch 1 | Step 255200 | Avg Loss: 0.0153 | Grad Norm: 0.00974747\n",
      "Epoch 1 | Step 255300 | Avg Loss: 0.0153 | Grad Norm: 0.00763192\n",
      "Epoch 1 | Step 255400 | Avg Loss: 0.0155 | Grad Norm: 0.00873358\n",
      "Epoch 1 | Step 255500 | Avg Loss: 0.0155 | Grad Norm: 0.01120249\n",
      "Epoch 1 | Step 255600 | Avg Loss: 0.0155 | Grad Norm: 0.00969231\n",
      "Epoch 1 | Step 255700 | Avg Loss: 0.0158 | Grad Norm: 0.00886179\n",
      "Epoch 1 | Step 255800 | Avg Loss: 0.0156 | Grad Norm: 0.00735388\n",
      "Epoch 1 | Step 255900 | Avg Loss: 0.0156 | Grad Norm: 0.00959268\n",
      "Epoch 1 | Step 256000 | Avg Loss: 0.0157 | Grad Norm: 0.00929038\n",
      "Epoch 1 | Step 256100 | Avg Loss: 0.0157 | Grad Norm: 0.00902598\n",
      "Epoch 1 | Step 256200 | Avg Loss: 0.0155 | Grad Norm: 0.00845010\n",
      "Epoch 1 | Step 256300 | Avg Loss: 0.0156 | Grad Norm: 0.00832151\n",
      "Epoch 1 | Step 256400 | Avg Loss: 0.0156 | Grad Norm: 0.00845993\n",
      "Epoch 1 | Step 256500 | Avg Loss: 0.0158 | Grad Norm: 0.00886510\n",
      "Epoch 1 | Step 256600 | Avg Loss: 0.0152 | Grad Norm: 0.00779398\n",
      "Epoch 1 | Step 256700 | Avg Loss: 0.0155 | Grad Norm: 0.00915712\n",
      "Epoch 1 | Step 256800 | Avg Loss: 0.0152 | Grad Norm: 0.00946297\n",
      "Epoch 1 | Step 256900 | Avg Loss: 0.0153 | Grad Norm: 0.00947914\n",
      "Epoch 1 | Step 257000 | Avg Loss: 0.0154 | Grad Norm: 0.00972828\n",
      "Epoch 1 | Step 257100 | Avg Loss: 0.0155 | Grad Norm: 0.00947575\n",
      "Epoch 1 | Step 257200 | Avg Loss: 0.0157 | Grad Norm: 0.00821386\n",
      "Epoch 1 | Step 257300 | Avg Loss: 0.0157 | Grad Norm: 0.00882458\n",
      "Epoch 1 | Step 257400 | Avg Loss: 0.0155 | Grad Norm: 0.00744270\n",
      "Epoch 1 | Step 257500 | Avg Loss: 0.0157 | Grad Norm: 0.00810409\n",
      "Epoch 1 | Step 257600 | Avg Loss: 0.0158 | Grad Norm: 0.00892172\n",
      "Epoch 1 | Step 257700 | Avg Loss: 0.0157 | Grad Norm: 0.00810018\n",
      "Epoch 1 | Step 257800 | Avg Loss: 0.0161 | Grad Norm: 0.00822876\n",
      "Epoch 1 | Step 257900 | Avg Loss: 0.0163 | Grad Norm: 0.00892135\n",
      "Epoch 1 | Step 258000 | Avg Loss: 0.0160 | Grad Norm: 0.00990156\n",
      "Epoch 1 | Step 258100 | Avg Loss: 0.0159 | Grad Norm: 0.00859662\n",
      "Epoch 1 | Step 258200 | Avg Loss: 0.0154 | Grad Norm: 0.00905500\n",
      "Epoch 1 | Step 258300 | Avg Loss: 0.0154 | Grad Norm: 0.00835832\n",
      "Epoch 1 | Step 258400 | Avg Loss: 0.0150 | Grad Norm: 0.00951549\n",
      "Epoch 1 | Step 258500 | Avg Loss: 0.0153 | Grad Norm: 0.00857125\n",
      "Epoch 1 | Step 258600 | Avg Loss: 0.0157 | Grad Norm: 0.01084290\n",
      "Epoch 1 | Step 258700 | Avg Loss: 0.0159 | Grad Norm: 0.00869330\n",
      "Epoch 1 | Step 258800 | Avg Loss: 0.0161 | Grad Norm: 0.00795176\n",
      "Epoch 1 | Step 258900 | Avg Loss: 0.0162 | Grad Norm: 0.00869208\n",
      "Epoch 1 | Step 259000 | Avg Loss: 0.0157 | Grad Norm: 0.00852022\n",
      "Epoch 1 | Step 259100 | Avg Loss: 0.0159 | Grad Norm: 0.00934562\n",
      "Epoch 1 | Step 259200 | Avg Loss: 0.0158 | Grad Norm: 0.00951373\n",
      "Epoch 1 | Step 259300 | Avg Loss: 0.0154 | Grad Norm: 0.00886200\n",
      "Epoch 1 | Step 259400 | Avg Loss: 0.0153 | Grad Norm: 0.00756151\n",
      "Epoch 1 | Step 259500 | Avg Loss: 0.0155 | Grad Norm: 0.01000409\n",
      "Epoch 1 | Step 259600 | Avg Loss: 0.0154 | Grad Norm: 0.00820733\n",
      "Epoch 1 | Step 259700 | Avg Loss: 0.0155 | Grad Norm: 0.00999011\n",
      "Epoch 1 | Step 259800 | Avg Loss: 0.0152 | Grad Norm: 0.01027053\n",
      "Epoch 1 | Step 259900 | Avg Loss: 0.0155 | Grad Norm: 0.00821891\n",
      "Epoch 1 | Step 260000 | Avg Loss: 0.0155 | Grad Norm: 0.01089354\n",
      "Epoch 1 | Step 260100 | Avg Loss: 0.0154 | Grad Norm: 0.00873278\n",
      "Epoch 1 | Step 260200 | Avg Loss: 0.0155 | Grad Norm: 0.01219499\n",
      "Epoch 1 | Step 260300 | Avg Loss: 0.0152 | Grad Norm: 0.00832281\n",
      "Epoch 1 | Step 260400 | Avg Loss: 0.0155 | Grad Norm: 0.00786340\n",
      "Epoch 1 | Step 260500 | Avg Loss: 0.0155 | Grad Norm: 0.00843415\n",
      "Epoch 1 | Step 260600 | Avg Loss: 0.0156 | Grad Norm: 0.00797838\n",
      "Epoch 1 | Step 260700 | Avg Loss: 0.0159 | Grad Norm: 0.00824024\n",
      "Epoch 1 | Step 260800 | Avg Loss: 0.0161 | Grad Norm: 0.00809417\n",
      "Epoch 1 | Step 260900 | Avg Loss: 0.0158 | Grad Norm: 0.00846764\n",
      "Epoch 1 | Step 261000 | Avg Loss: 0.0157 | Grad Norm: 0.00881254\n",
      "Epoch 1 | Step 261100 | Avg Loss: 0.0156 | Grad Norm: 0.00908940\n",
      "Epoch 1 | Step 261200 | Avg Loss: 0.0158 | Grad Norm: 0.00795025\n",
      "Epoch 1 | Step 261300 | Avg Loss: 0.0157 | Grad Norm: 0.00802153\n",
      "Epoch 1 | Step 261400 | Avg Loss: 0.0159 | Grad Norm: 0.00810810\n",
      "Epoch 1 | Step 261500 | Avg Loss: 0.0160 | Grad Norm: 0.00863541\n",
      "Epoch 1 | Step 261600 | Avg Loss: 0.0156 | Grad Norm: 0.01100209\n",
      "Epoch 1 | Step 261700 | Avg Loss: 0.0155 | Grad Norm: 0.00869990\n",
      "Epoch 1 | Step 261800 | Avg Loss: 0.0157 | Grad Norm: 0.01114818\n",
      "Epoch 1 | Step 261900 | Avg Loss: 0.0155 | Grad Norm: 0.00899058\n",
      "Epoch 1 | Step 262000 | Avg Loss: 0.0157 | Grad Norm: 0.01035344\n",
      "Epoch 1 | Step 262100 | Avg Loss: 0.0160 | Grad Norm: 0.00868502\n",
      "Epoch 1 | Step 262200 | Avg Loss: 0.0158 | Grad Norm: 0.00917934\n",
      "Epoch 1 | Step 262300 | Avg Loss: 0.0157 | Grad Norm: 0.00978538\n",
      "Epoch 1 | Step 262400 | Avg Loss: 0.0155 | Grad Norm: 0.01036387\n",
      "Epoch 1 | Step 262500 | Avg Loss: 0.0157 | Grad Norm: 0.00859000\n",
      "Epoch 1 | Step 262600 | Avg Loss: 0.0156 | Grad Norm: 0.00881366\n",
      "Epoch 1 | Step 262700 | Avg Loss: 0.0156 | Grad Norm: 0.00939247\n",
      "Epoch 1 | Step 262800 | Avg Loss: 0.0157 | Grad Norm: 0.00907985\n",
      "Epoch 1 | Step 262900 | Avg Loss: 0.0160 | Grad Norm: 0.00826964\n",
      "Epoch 1 | Step 263000 | Avg Loss: 0.0159 | Grad Norm: 0.00864622\n",
      "Epoch 1 | Step 263100 | Avg Loss: 0.0158 | Grad Norm: 0.01009079\n",
      "Epoch 1 | Step 263200 | Avg Loss: 0.0157 | Grad Norm: 0.00826885\n",
      "Epoch 1 | Step 263300 | Avg Loss: 0.0158 | Grad Norm: 0.00985682\n",
      "Epoch 1 | Step 263400 | Avg Loss: 0.0157 | Grad Norm: 0.00908604\n",
      "Epoch 1 | Step 263500 | Avg Loss: 0.0160 | Grad Norm: 0.01629840\n",
      "Epoch 1 | Step 263600 | Avg Loss: 0.0159 | Grad Norm: 0.00810207\n",
      "Epoch 1 | Step 263700 | Avg Loss: 0.0161 | Grad Norm: 0.00792773\n",
      "Epoch 1 | Step 263800 | Avg Loss: 0.0160 | Grad Norm: 0.00745670\n",
      "Epoch 1 | Step 263900 | Avg Loss: 0.0161 | Grad Norm: 0.00962010\n",
      "Epoch 1 | Step 264000 | Avg Loss: 0.0160 | Grad Norm: 0.00709590\n",
      "Epoch 1 | Step 264100 | Avg Loss: 0.0162 | Grad Norm: 0.00921990\n",
      "Epoch 1 | Step 264200 | Avg Loss: 0.0163 | Grad Norm: 0.00844946\n",
      "Epoch 1 | Step 264300 | Avg Loss: 0.0165 | Grad Norm: 0.00768667\n",
      "Epoch 1 | Step 264400 | Avg Loss: 0.0162 | Grad Norm: 0.00859891\n",
      "Epoch 1 | Step 264500 | Avg Loss: 0.0162 | Grad Norm: 0.00899707\n",
      "Epoch 1 | Step 264600 | Avg Loss: 0.0158 | Grad Norm: 0.00922309\n",
      "Epoch 1 | Step 264700 | Avg Loss: 0.0159 | Grad Norm: 0.00944861\n",
      "Epoch 1 | Step 264800 | Avg Loss: 0.0159 | Grad Norm: 0.01070461\n",
      "Epoch 1 | Step 264900 | Avg Loss: 0.0163 | Grad Norm: 0.00900932\n",
      "Epoch 1 | Step 265000 | Avg Loss: 0.0163 | Grad Norm: 0.00816013\n",
      "Epoch 1 | Step 265100 | Avg Loss: 0.0162 | Grad Norm: 0.00804679\n",
      "Epoch 1 | Step 265200 | Avg Loss: 0.0160 | Grad Norm: 0.00902701\n",
      "Epoch 1 | Step 265300 | Avg Loss: 0.0156 | Grad Norm: 0.00849228\n",
      "Epoch 1 | Step 265400 | Avg Loss: 0.0154 | Grad Norm: 0.00867457\n",
      "Epoch 1 | Step 265500 | Avg Loss: 0.0163 | Grad Norm: 0.00966136\n",
      "Epoch 1 | Step 265600 | Avg Loss: 0.0161 | Grad Norm: 0.00889608\n",
      "Epoch 1 | Step 265700 | Avg Loss: 0.0161 | Grad Norm: 0.00858134\n",
      "Epoch 1 | Step 265800 | Avg Loss: 0.0160 | Grad Norm: 0.00748365\n",
      "Epoch 1 | Step 265900 | Avg Loss: 0.0158 | Grad Norm: 0.00891623\n",
      "Epoch 1 | Step 266000 | Avg Loss: 0.0156 | Grad Norm: 0.00935772\n",
      "Epoch 1 | Step 266100 | Avg Loss: 0.0159 | Grad Norm: 0.01080927\n",
      "Epoch 1 | Step 266200 | Avg Loss: 0.0158 | Grad Norm: 0.00845398\n",
      "Epoch 1 | Step 266300 | Avg Loss: 0.0159 | Grad Norm: 0.00787445\n",
      "Epoch 1 | Step 266400 | Avg Loss: 0.0160 | Grad Norm: 0.00920935\n",
      "Epoch 1 | Step 266500 | Avg Loss: 0.0162 | Grad Norm: 0.00915301\n",
      "Epoch 1 | Step 266600 | Avg Loss: 0.0162 | Grad Norm: 0.00963165\n",
      "Epoch 1 | Step 266700 | Avg Loss: 0.0157 | Grad Norm: 0.00815624\n",
      "Epoch 1 | Step 266800 | Avg Loss: 0.0160 | Grad Norm: 0.00790791\n",
      "Epoch 1 | Step 266900 | Avg Loss: 0.0157 | Grad Norm: 0.00891686\n",
      "Epoch 1 | Step 267000 | Avg Loss: 0.0158 | Grad Norm: 0.00833770\n",
      "Epoch 1 | Step 267100 | Avg Loss: 0.0157 | Grad Norm: 0.00940930\n",
      "Epoch 1 | Step 267200 | Avg Loss: 0.0156 | Grad Norm: 0.00784043\n",
      "Epoch 1 | Step 267300 | Avg Loss: 0.0152 | Grad Norm: 0.00854648\n",
      "Epoch 1 | Step 267400 | Avg Loss: 0.0151 | Grad Norm: 0.00916512\n",
      "Epoch 1 | Step 267500 | Avg Loss: 0.0154 | Grad Norm: 0.00834471\n",
      "Epoch 1 | Step 267600 | Avg Loss: 0.0154 | Grad Norm: 0.00788929\n",
      "Epoch 1 | Step 267700 | Avg Loss: 0.0154 | Grad Norm: 0.00822657\n",
      "Epoch 1 | Step 267800 | Avg Loss: 0.0154 | Grad Norm: 0.00814528\n",
      "Epoch 1 | Step 267900 | Avg Loss: 0.0155 | Grad Norm: 0.00832921\n",
      "Epoch 1 | Step 268000 | Avg Loss: 0.0157 | Grad Norm: 0.00751042\n",
      "Epoch 1 | Step 268100 | Avg Loss: 0.0156 | Grad Norm: 0.00834235\n",
      "Epoch 1 | Step 268200 | Avg Loss: 0.0155 | Grad Norm: 0.00912486\n",
      "Epoch 1 | Step 268300 | Avg Loss: 0.0156 | Grad Norm: 0.01197090\n",
      "Epoch 1 | Step 268400 | Avg Loss: 0.0158 | Grad Norm: 0.00962076\n",
      "Epoch 1 | Step 268500 | Avg Loss: 0.0158 | Grad Norm: 0.00950340\n",
      "Epoch 1 | Step 268600 | Avg Loss: 0.0159 | Grad Norm: 0.00872340\n",
      "Epoch 1 | Step 268700 | Avg Loss: 0.0158 | Grad Norm: 0.00828625\n",
      "Epoch 1 | Step 268800 | Avg Loss: 0.0160 | Grad Norm: 0.00835653\n",
      "Epoch 1 | Step 268900 | Avg Loss: 0.0158 | Grad Norm: 0.00959455\n",
      "Epoch 1 | Step 269000 | Avg Loss: 0.0160 | Grad Norm: 0.00859179\n",
      "Epoch 1 | Step 269100 | Avg Loss: 0.0159 | Grad Norm: 0.00877810\n",
      "Epoch 1 | Step 269200 | Avg Loss: 0.0159 | Grad Norm: 0.00963066\n",
      "Epoch 1 | Step 269300 | Avg Loss: 0.0158 | Grad Norm: 0.00795822\n",
      "Epoch 1 | Step 269400 | Avg Loss: 0.0160 | Grad Norm: 0.00789036\n",
      "Epoch 1 | Step 269500 | Avg Loss: 0.0161 | Grad Norm: 0.00822468\n",
      "Epoch 1 | Step 269600 | Avg Loss: 0.0157 | Grad Norm: 0.00981739\n",
      "Epoch 1 | Step 269700 | Avg Loss: 0.0156 | Grad Norm: 0.00848624\n",
      "Epoch 1 | Step 269800 | Avg Loss: 0.0158 | Grad Norm: 0.00778233\n",
      "Epoch 1 | Step 269900 | Avg Loss: 0.0153 | Grad Norm: 0.01085394\n",
      "Epoch 1 | Step 270000 | Avg Loss: 0.0157 | Grad Norm: 0.00959386\n",
      "Epoch 1 | Step 270100 | Avg Loss: 0.0157 | Grad Norm: 0.01025677\n",
      "Epoch 1 | Step 270200 | Avg Loss: 0.0158 | Grad Norm: 0.00835433\n",
      "Epoch 1 | Step 270300 | Avg Loss: 0.0158 | Grad Norm: 0.00839873\n",
      "Epoch 1 | Step 270400 | Avg Loss: 0.0157 | Grad Norm: 0.00898081\n",
      "Epoch 1 | Step 270500 | Avg Loss: 0.0156 | Grad Norm: 0.00777126\n",
      "Epoch 1 | Step 270600 | Avg Loss: 0.0159 | Grad Norm: 0.00911713\n",
      "Epoch 1 | Step 270700 | Avg Loss: 0.0156 | Grad Norm: 0.00928079\n",
      "Epoch 1 | Step 270800 | Avg Loss: 0.0155 | Grad Norm: 0.00809476\n",
      "Epoch 1 | Step 270900 | Avg Loss: 0.0152 | Grad Norm: 0.00800409\n",
      "Epoch 1 | Step 271000 | Avg Loss: 0.0150 | Grad Norm: 0.00755048\n",
      "Epoch 1 | Step 271100 | Avg Loss: 0.0151 | Grad Norm: 0.00930663\n",
      "Epoch 1 | Step 271200 | Avg Loss: 0.0155 | Grad Norm: 0.00854784\n",
      "Epoch 1 | Step 271300 | Avg Loss: 0.0153 | Grad Norm: 0.00830852\n",
      "Epoch 1 | Step 271400 | Avg Loss: 0.0154 | Grad Norm: 0.00998512\n",
      "Epoch 1 | Step 271500 | Avg Loss: 0.0156 | Grad Norm: 0.00808026\n",
      "Epoch 1 | Step 271600 | Avg Loss: 0.0154 | Grad Norm: 0.00797988\n",
      "Epoch 1 | Step 271700 | Avg Loss: 0.0154 | Grad Norm: 0.00794957\n",
      "Epoch 1 | Step 271800 | Avg Loss: 0.0155 | Grad Norm: 0.01022327\n",
      "Epoch 1 | Step 271900 | Avg Loss: 0.0157 | Grad Norm: 0.00801856\n",
      "Epoch 1 | Step 272000 | Avg Loss: 0.0155 | Grad Norm: 0.00783645\n",
      "Epoch 1 | Step 272100 | Avg Loss: 0.0154 | Grad Norm: 0.00830404\n",
      "Epoch 1 | Step 272200 | Avg Loss: 0.0159 | Grad Norm: 0.00958812\n",
      "Epoch 1 | Step 272300 | Avg Loss: 0.0162 | Grad Norm: 0.01032345\n",
      "Epoch 1 | Step 272400 | Avg Loss: 0.0157 | Grad Norm: 0.01125379\n",
      "Epoch 1 | Step 272500 | Avg Loss: 0.0156 | Grad Norm: 0.00808865\n",
      "Epoch 1 | Step 272600 | Avg Loss: 0.0159 | Grad Norm: 0.00917379\n",
      "Epoch 1 | Step 272700 | Avg Loss: 0.0162 | Grad Norm: 0.00774910\n",
      "Epoch 1 | Step 272800 | Avg Loss: 0.0160 | Grad Norm: 0.00962902\n",
      "Epoch 1 | Step 272900 | Avg Loss: 0.0155 | Grad Norm: 0.00788436\n",
      "Epoch 1 | Step 273000 | Avg Loss: 0.0151 | Grad Norm: 0.00686725\n",
      "Epoch 1 | Step 273100 | Avg Loss: 0.0150 | Grad Norm: 0.00987603\n",
      "Epoch 1 | Step 273200 | Avg Loss: 0.0154 | Grad Norm: 0.00846349\n",
      "Epoch 1 | Step 273300 | Avg Loss: 0.0155 | Grad Norm: 0.00852357\n",
      "Epoch 1 | Step 273400 | Avg Loss: 0.0151 | Grad Norm: 0.00841768\n",
      "Epoch 1 | Step 273500 | Avg Loss: 0.0154 | Grad Norm: 0.00945637\n",
      "Epoch 1 | Step 273600 | Avg Loss: 0.0150 | Grad Norm: 0.00925009\n",
      "Epoch 1 | Step 273700 | Avg Loss: 0.0152 | Grad Norm: 0.00884455\n",
      "Epoch 1 | Step 273800 | Avg Loss: 0.0155 | Grad Norm: 0.00834744\n",
      "Epoch 1 | Step 273900 | Avg Loss: 0.0153 | Grad Norm: 0.00993971\n",
      "Epoch 1 | Step 274000 | Avg Loss: 0.0156 | Grad Norm: 0.00805358\n",
      "Epoch 1 | Step 274100 | Avg Loss: 0.0156 | Grad Norm: 0.00875590\n",
      "Epoch 1 | Step 274200 | Avg Loss: 0.0152 | Grad Norm: 0.00828667\n",
      "Epoch 1 | Step 274300 | Avg Loss: 0.0156 | Grad Norm: 0.00769285\n",
      "Epoch 1 | Step 274400 | Avg Loss: 0.0152 | Grad Norm: 0.00811021\n",
      "Epoch 1 | Step 274500 | Avg Loss: 0.0150 | Grad Norm: 0.00863929\n",
      "Epoch 1 | Step 274600 | Avg Loss: 0.0153 | Grad Norm: 0.00963470\n",
      "Epoch 1 | Step 274700 | Avg Loss: 0.0151 | Grad Norm: 0.00814155\n",
      "Epoch 1 | Step 274800 | Avg Loss: 0.0152 | Grad Norm: 0.00766118\n",
      "Epoch 1 | Step 274900 | Avg Loss: 0.0158 | Grad Norm: 0.00865489\n",
      "Epoch 1 | Step 275000 | Avg Loss: 0.0158 | Grad Norm: 0.00807278\n",
      "Epoch 1 | Step 275100 | Avg Loss: 0.0160 | Grad Norm: 0.00842587\n",
      "Epoch 1 | Step 275200 | Avg Loss: 0.0158 | Grad Norm: 0.00754033\n",
      "Epoch 1 | Step 275300 | Avg Loss: 0.0157 | Grad Norm: 0.00922181\n",
      "Epoch 1 | Step 275400 | Avg Loss: 0.0156 | Grad Norm: 0.00957581\n",
      "Epoch 1 | Step 275500 | Avg Loss: 0.0154 | Grad Norm: 0.00973155\n",
      "Epoch 1 | Step 275600 | Avg Loss: 0.0156 | Grad Norm: 0.00900246\n",
      "Epoch 1 | Step 275700 | Avg Loss: 0.0153 | Grad Norm: 0.00811084\n",
      "Epoch 1 | Step 275800 | Avg Loss: 0.0153 | Grad Norm: 0.00949450\n",
      "Epoch 1 | Step 275900 | Avg Loss: 0.0158 | Grad Norm: 0.00786909\n",
      "Epoch 1 | Step 276000 | Avg Loss: 0.0160 | Grad Norm: 0.00893363\n",
      "Epoch 1 | Step 276100 | Avg Loss: 0.0160 | Grad Norm: 0.00885501\n",
      "Epoch 1 | Step 276200 | Avg Loss: 0.0158 | Grad Norm: 0.00800493\n",
      "Epoch 1 | Step 276300 | Avg Loss: 0.0159 | Grad Norm: 0.00859543\n",
      "Epoch 1 | Step 276400 | Avg Loss: 0.0161 | Grad Norm: 0.00787388\n",
      "Epoch 1 | Step 276500 | Avg Loss: 0.0160 | Grad Norm: 0.01152715\n",
      "Epoch 1 | Step 276600 | Avg Loss: 0.0154 | Grad Norm: 0.00852209\n",
      "Epoch 1 | Step 276700 | Avg Loss: 0.0153 | Grad Norm: 0.00952900\n",
      "Epoch 1 | Step 276800 | Avg Loss: 0.0154 | Grad Norm: 0.00933704\n",
      "Epoch 1 | Step 276900 | Avg Loss: 0.0159 | Grad Norm: 0.00843922\n",
      "Epoch 1 | Step 277000 | Avg Loss: 0.0157 | Grad Norm: 0.00878831\n",
      "Epoch 1 | Step 277100 | Avg Loss: 0.0155 | Grad Norm: 0.00866742\n",
      "Epoch 1 | Step 277200 | Avg Loss: 0.0153 | Grad Norm: 0.00898554\n",
      "Epoch 1 | Step 277300 | Avg Loss: 0.0153 | Grad Norm: 0.00888419\n",
      "Epoch 1 | Step 277400 | Avg Loss: 0.0154 | Grad Norm: 0.00922473\n",
      "Epoch 1 | Step 277500 | Avg Loss: 0.0154 | Grad Norm: 0.00944587\n",
      "Epoch 1 | Step 277600 | Avg Loss: 0.0153 | Grad Norm: 0.00760971\n",
      "Epoch 1 | Step 277700 | Avg Loss: 0.0152 | Grad Norm: 0.00733755\n",
      "Epoch 1 | Step 277800 | Avg Loss: 0.0153 | Grad Norm: 0.00967794\n",
      "Epoch 1 | Step 277900 | Avg Loss: 0.0153 | Grad Norm: 0.00820641\n",
      "Epoch 1 | Step 278000 | Avg Loss: 0.0155 | Grad Norm: 0.00979900\n",
      "Epoch 1 | Step 278100 | Avg Loss: 0.0157 | Grad Norm: 0.00923252\n",
      "Epoch 1 | Step 278200 | Avg Loss: 0.0157 | Grad Norm: 0.00874243\n",
      "Epoch 1 | Step 278300 | Avg Loss: 0.0157 | Grad Norm: 0.00875142\n",
      "Epoch 1 | Step 278400 | Avg Loss: 0.0156 | Grad Norm: 0.00860633\n",
      "Epoch 1 | Step 278500 | Avg Loss: 0.0154 | Grad Norm: 0.00927710\n",
      "Epoch 1 | Step 278600 | Avg Loss: 0.0152 | Grad Norm: 0.00781956\n",
      "Epoch 1 | Step 278700 | Avg Loss: 0.0154 | Grad Norm: 0.00869860\n",
      "Epoch 1 | Step 278800 | Avg Loss: 0.0154 | Grad Norm: 0.00918312\n",
      "Epoch 1 | Step 278900 | Avg Loss: 0.0157 | Grad Norm: 0.00822949\n",
      "Epoch 1 | Step 279000 | Avg Loss: 0.0158 | Grad Norm: 0.00859478\n",
      "Epoch 1 | Step 279100 | Avg Loss: 0.0156 | Grad Norm: 0.00865629\n",
      "Epoch 1 | Step 279200 | Avg Loss: 0.0157 | Grad Norm: 0.01010890\n",
      "Epoch 1 | Step 279300 | Avg Loss: 0.0159 | Grad Norm: 0.00956005\n",
      "Epoch 1 | Step 279400 | Avg Loss: 0.0158 | Grad Norm: 0.00816611\n",
      "Epoch 1 | Step 279500 | Avg Loss: 0.0162 | Grad Norm: 0.00941450\n",
      "Epoch 1 | Step 279600 | Avg Loss: 0.0162 | Grad Norm: 0.00727396\n",
      "Epoch 1 | Step 279700 | Avg Loss: 0.0160 | Grad Norm: 0.00835896\n",
      "Epoch 1 | Step 279800 | Avg Loss: 0.0161 | Grad Norm: 0.01029157\n",
      "Epoch 1 | Step 279900 | Avg Loss: 0.0165 | Grad Norm: 0.00907777\n",
      "Epoch 1 | Step 280000 | Avg Loss: 0.0163 | Grad Norm: 0.00933893\n",
      "Epoch 1 | Step 280100 | Avg Loss: 0.0157 | Grad Norm: 0.00977205\n",
      "Epoch 1 | Step 280200 | Avg Loss: 0.0158 | Grad Norm: 0.00946269\n",
      "Epoch 1 | Step 280300 | Avg Loss: 0.0159 | Grad Norm: 0.00891742\n",
      "Epoch 1 | Step 280400 | Avg Loss: 0.0157 | Grad Norm: 0.00839561\n",
      "Epoch 1 | Step 280500 | Avg Loss: 0.0156 | Grad Norm: 0.00963087\n",
      "Epoch 1 | Step 280600 | Avg Loss: 0.0157 | Grad Norm: 0.00934735\n",
      "Epoch 1 | Step 280700 | Avg Loss: 0.0159 | Grad Norm: 0.00969412\n",
      "Epoch 1 | Step 280800 | Avg Loss: 0.0155 | Grad Norm: 0.00932517\n",
      "Epoch 1 | Step 280900 | Avg Loss: 0.0156 | Grad Norm: 0.00911460\n",
      "Epoch 1 | Step 281000 | Avg Loss: 0.0160 | Grad Norm: 0.00829798\n",
      "Epoch 1 | Step 281100 | Avg Loss: 0.0159 | Grad Norm: 0.00913224\n",
      "Epoch 1 | Step 281200 | Avg Loss: 0.0160 | Grad Norm: 0.00910434\n",
      "Epoch 1 | Step 281300 | Avg Loss: 0.0162 | Grad Norm: 0.00938808\n",
      "Epoch 1 | Step 281400 | Avg Loss: 0.0162 | Grad Norm: 0.00866063\n",
      "Epoch 1 | Step 281500 | Avg Loss: 0.0162 | Grad Norm: 0.00756380\n",
      "Epoch 1 | Step 281600 | Avg Loss: 0.0159 | Grad Norm: 0.00903289\n",
      "Epoch 1 | Step 281700 | Avg Loss: 0.0157 | Grad Norm: 0.00831909\n",
      "Epoch 1 | Step 281800 | Avg Loss: 0.0159 | Grad Norm: 0.01173855\n",
      "Epoch 1 | Step 281900 | Avg Loss: 0.0159 | Grad Norm: 0.01025332\n",
      "Epoch 1 | Step 282000 | Avg Loss: 0.0159 | Grad Norm: 0.00790203\n",
      "Epoch 1 | Step 282100 | Avg Loss: 0.0159 | Grad Norm: 0.00746559\n",
      "Epoch 1 | Step 282200 | Avg Loss: 0.0156 | Grad Norm: 0.00898588\n",
      "Epoch 1 | Step 282300 | Avg Loss: 0.0156 | Grad Norm: 0.00893226\n",
      "Epoch 1 | Step 282400 | Avg Loss: 0.0156 | Grad Norm: 0.00792296\n",
      "Epoch 1 | Step 282500 | Avg Loss: 0.0157 | Grad Norm: 0.00801164\n",
      "Epoch 1 | Step 282600 | Avg Loss: 0.0156 | Grad Norm: 0.00871628\n",
      "Epoch 1 | Step 282700 | Avg Loss: 0.0154 | Grad Norm: 0.00876702\n",
      "Epoch 1 | Step 282800 | Avg Loss: 0.0154 | Grad Norm: 0.00895442\n",
      "Epoch 1 | Step 282900 | Avg Loss: 0.0158 | Grad Norm: 0.00847050\n",
      "Epoch 1 | Step 283000 | Avg Loss: 0.0159 | Grad Norm: 0.01086266\n",
      "Epoch 1 | Step 283100 | Avg Loss: 0.0159 | Grad Norm: 0.00917367\n",
      "Epoch 1 | Step 283200 | Avg Loss: 0.0157 | Grad Norm: 0.00780816\n",
      "Epoch 1 | Step 283300 | Avg Loss: 0.0159 | Grad Norm: 0.00989024\n",
      "Epoch 1 | Step 283400 | Avg Loss: 0.0157 | Grad Norm: 0.00913145\n",
      "Epoch 1 | Step 283500 | Avg Loss: 0.0156 | Grad Norm: 0.00772640\n",
      "Epoch 1 | Step 283600 | Avg Loss: 0.0155 | Grad Norm: 0.00967290\n",
      "Epoch 1 | Step 283700 | Avg Loss: 0.0156 | Grad Norm: 0.00959557\n",
      "Epoch 1 | Step 283800 | Avg Loss: 0.0158 | Grad Norm: 0.00792105\n",
      "Epoch 1 | Step 283900 | Avg Loss: 0.0157 | Grad Norm: 0.01073940\n",
      "Epoch 1 | Step 284000 | Avg Loss: 0.0153 | Grad Norm: 0.00963665\n",
      "Epoch 1 | Step 284100 | Avg Loss: 0.0155 | Grad Norm: 0.00862035\n",
      "Epoch 1 | Step 284200 | Avg Loss: 0.0154 | Grad Norm: 0.00850820\n",
      "Epoch 1 | Step 284300 | Avg Loss: 0.0157 | Grad Norm: 0.00870567\n",
      "Epoch 1 | Step 284400 | Avg Loss: 0.0154 | Grad Norm: 0.00842079\n",
      "Epoch 1 | Step 284500 | Avg Loss: 0.0153 | Grad Norm: 0.00759347\n",
      "Epoch 1 | Step 284600 | Avg Loss: 0.0158 | Grad Norm: 0.00737126\n",
      "Epoch 1 | Step 284700 | Avg Loss: 0.0161 | Grad Norm: 0.01189515\n",
      "Epoch 1 | Step 284800 | Avg Loss: 0.0161 | Grad Norm: 0.00793848\n",
      "Epoch 1 | Step 284900 | Avg Loss: 0.0161 | Grad Norm: 0.00753071\n",
      "Epoch 1 | Step 285000 | Avg Loss: 0.0165 | Grad Norm: 0.01022300\n",
      "Epoch 1 | Step 285100 | Avg Loss: 0.0163 | Grad Norm: 0.00982052\n",
      "Epoch 1 | Step 285200 | Avg Loss: 0.0159 | Grad Norm: 0.00898951\n",
      "Epoch 1 | Step 285300 | Avg Loss: 0.0158 | Grad Norm: 0.00768896\n",
      "Epoch 1 | Step 285400 | Avg Loss: 0.0156 | Grad Norm: 0.00916202\n",
      "Epoch 1 | Step 285500 | Avg Loss: 0.0153 | Grad Norm: 0.00744525\n",
      "Epoch 1 | Step 285600 | Avg Loss: 0.0153 | Grad Norm: 0.00999571\n",
      "Epoch 1 | Step 285700 | Avg Loss: 0.0150 | Grad Norm: 0.00776831\n",
      "Epoch 1 | Step 285800 | Avg Loss: 0.0152 | Grad Norm: 0.00803452\n",
      "Epoch 1 | Step 285900 | Avg Loss: 0.0151 | Grad Norm: 0.00915731\n",
      "Epoch 1 | Step 286000 | Avg Loss: 0.0150 | Grad Norm: 0.00878549\n",
      "Epoch 1 | Step 286100 | Avg Loss: 0.0154 | Grad Norm: 0.00793234\n",
      "Epoch 1 | Step 286200 | Avg Loss: 0.0156 | Grad Norm: 0.01027058\n",
      "Epoch 1 | Step 286300 | Avg Loss: 0.0154 | Grad Norm: 0.00900677\n",
      "Epoch 1 | Step 286400 | Avg Loss: 0.0153 | Grad Norm: 0.00885722\n",
      "Epoch 1 | Step 286500 | Avg Loss: 0.0152 | Grad Norm: 0.00784039\n",
      "Epoch 1 | Step 286600 | Avg Loss: 0.0153 | Grad Norm: 0.01058008\n",
      "Epoch 1 | Step 286700 | Avg Loss: 0.0155 | Grad Norm: 0.00851581\n",
      "Epoch 1 | Step 286800 | Avg Loss: 0.0158 | Grad Norm: 0.00998020\n",
      "Epoch 1 | Step 286900 | Avg Loss: 0.0165 | Grad Norm: 0.01059239\n",
      "Epoch 1 | Step 287000 | Avg Loss: 0.0163 | Grad Norm: 0.00986284\n",
      "Epoch 1 | Step 287100 | Avg Loss: 0.0160 | Grad Norm: 0.00906873\n",
      "Epoch 1 | Step 287200 | Avg Loss: 0.0163 | Grad Norm: 0.00856893\n",
      "Epoch 1 | Step 287300 | Avg Loss: 0.0163 | Grad Norm: 0.00824994\n",
      "Epoch 1 | Step 287400 | Avg Loss: 0.0159 | Grad Norm: 0.00844306\n",
      "Epoch 1 | Step 287500 | Avg Loss: 0.0163 | Grad Norm: 0.00873090\n",
      "Epoch 1 | Step 287600 | Avg Loss: 0.0163 | Grad Norm: 0.00958529\n",
      "Epoch 1 | Step 287700 | Avg Loss: 0.0159 | Grad Norm: 0.00971686\n",
      "Epoch 1 | Step 287800 | Avg Loss: 0.0158 | Grad Norm: 0.00754827\n",
      "Epoch 1 | Step 287900 | Avg Loss: 0.0161 | Grad Norm: 0.01014400\n",
      "Epoch 1 | Step 288000 | Avg Loss: 0.0156 | Grad Norm: 0.00876439\n",
      "Epoch 1 | Step 288100 | Avg Loss: 0.0155 | Grad Norm: 0.00816230\n",
      "Epoch 1 | Step 288200 | Avg Loss: 0.0154 | Grad Norm: 0.01005395\n",
      "Epoch 1 | Step 288300 | Avg Loss: 0.0154 | Grad Norm: 0.00772645\n",
      "Epoch 1 | Step 288400 | Avg Loss: 0.0155 | Grad Norm: 0.00824819\n",
      "Epoch 1 | Step 288500 | Avg Loss: 0.0155 | Grad Norm: 0.00805403\n",
      "Epoch 1 | Step 288600 | Avg Loss: 0.0157 | Grad Norm: 0.00849342\n",
      "Epoch 1 | Step 288700 | Avg Loss: 0.0161 | Grad Norm: 0.00865591\n",
      "Epoch 1 | Step 288800 | Avg Loss: 0.0159 | Grad Norm: 0.01056220\n",
      "Epoch 1 | Step 288900 | Avg Loss: 0.0156 | Grad Norm: 0.00995799\n",
      "Epoch 1 | Step 289000 | Avg Loss: 0.0160 | Grad Norm: 0.00799117\n",
      "Epoch 1 | Step 289100 | Avg Loss: 0.0162 | Grad Norm: 0.00944249\n",
      "Epoch 1 | Step 289200 | Avg Loss: 0.0163 | Grad Norm: 0.00884812\n",
      "Epoch 1 | Step 289300 | Avg Loss: 0.0159 | Grad Norm: 0.00860944\n",
      "Epoch 1 | Step 289400 | Avg Loss: 0.0154 | Grad Norm: 0.00820315\n",
      "Epoch 1 | Step 289500 | Avg Loss: 0.0153 | Grad Norm: 0.00928489\n",
      "Epoch 1 | Step 289600 | Avg Loss: 0.0156 | Grad Norm: 0.00860853\n",
      "Epoch 1 | Step 289700 | Avg Loss: 0.0155 | Grad Norm: 0.00847235\n",
      "Epoch 1 | Step 289800 | Avg Loss: 0.0158 | Grad Norm: 0.00948781\n",
      "Epoch 1 | Step 289900 | Avg Loss: 0.0160 | Grad Norm: 0.00933076\n",
      "Epoch 1 | Step 290000 | Avg Loss: 0.0160 | Grad Norm: 0.00987222\n",
      "Epoch 1 | Step 290100 | Avg Loss: 0.0161 | Grad Norm: 0.00830713\n",
      "Epoch 1 | Step 290200 | Avg Loss: 0.0158 | Grad Norm: 0.00924769\n",
      "Epoch 1 | Step 290300 | Avg Loss: 0.0157 | Grad Norm: 0.00877806\n",
      "Epoch 1 | Step 290400 | Avg Loss: 0.0151 | Grad Norm: 0.00793114\n",
      "Epoch 1 | Step 290500 | Avg Loss: 0.0155 | Grad Norm: 0.00783593\n",
      "Epoch 1 | Step 290600 | Avg Loss: 0.0153 | Grad Norm: 0.00921327\n",
      "Epoch 1 | Step 290700 | Avg Loss: 0.0151 | Grad Norm: 0.01043988\n",
      "Epoch 1 | Step 290800 | Avg Loss: 0.0150 | Grad Norm: 0.00773944\n",
      "Epoch 1 | Step 290900 | Avg Loss: 0.0152 | Grad Norm: 0.00926661\n",
      "Epoch 1 | Step 291000 | Avg Loss: 0.0158 | Grad Norm: 0.00922752\n",
      "Epoch 1 | Step 291100 | Avg Loss: 0.0155 | Grad Norm: 0.00894447\n",
      "Epoch 1 | Step 291200 | Avg Loss: 0.0157 | Grad Norm: 0.00832234\n",
      "Epoch 1 | Step 291300 | Avg Loss: 0.0160 | Grad Norm: 0.00933582\n",
      "Epoch 1 | Step 291400 | Avg Loss: 0.0161 | Grad Norm: 0.00861904\n",
      "Epoch 1 | Step 291500 | Avg Loss: 0.0156 | Grad Norm: 0.00826173\n",
      "Epoch 1 | Step 291600 | Avg Loss: 0.0155 | Grad Norm: 0.01028658\n",
      "Epoch 1 | Step 291700 | Avg Loss: 0.0159 | Grad Norm: 0.00889385\n",
      "Epoch 1 | Step 291800 | Avg Loss: 0.0161 | Grad Norm: 0.00812789\n",
      "Epoch 1 | Step 291900 | Avg Loss: 0.0160 | Grad Norm: 0.00943390\n",
      "Epoch 1 | Step 292000 | Avg Loss: 0.0163 | Grad Norm: 0.00827270\n",
      "Epoch 1 | Step 292100 | Avg Loss: 0.0162 | Grad Norm: 0.00887696\n",
      "Epoch 1 | Step 292200 | Avg Loss: 0.0158 | Grad Norm: 0.00871374\n",
      "Epoch 1 | Step 292300 | Avg Loss: 0.0159 | Grad Norm: 0.00865893\n",
      "Epoch 1 | Step 292400 | Avg Loss: 0.0155 | Grad Norm: 0.01115027\n",
      "Epoch 1 | Step 292500 | Avg Loss: 0.0154 | Grad Norm: 0.00977007\n",
      "Epoch 1 | Step 292600 | Avg Loss: 0.0154 | Grad Norm: 0.00861323\n",
      "Epoch 1 | Step 292700 | Avg Loss: 0.0153 | Grad Norm: 0.00743592\n",
      "Epoch 1 | Step 292800 | Avg Loss: 0.0159 | Grad Norm: 0.01001248\n",
      "Epoch 1 | Step 292900 | Avg Loss: 0.0160 | Grad Norm: 0.00865431\n",
      "Epoch 1 | Step 293000 | Avg Loss: 0.0159 | Grad Norm: 0.00880528\n",
      "Epoch 1 | Step 293100 | Avg Loss: 0.0160 | Grad Norm: 0.00939210\n",
      "Epoch 1 | Step 293200 | Avg Loss: 0.0163 | Grad Norm: 0.00827715\n",
      "Epoch 1 | Step 293300 | Avg Loss: 0.0160 | Grad Norm: 0.00808473\n",
      "Epoch 1 | Step 293400 | Avg Loss: 0.0159 | Grad Norm: 0.00858311\n",
      "Epoch 1 | Step 293500 | Avg Loss: 0.0159 | Grad Norm: 0.00904774\n",
      "Epoch 1 | Step 293600 | Avg Loss: 0.0155 | Grad Norm: 0.00860169\n",
      "Epoch 1 | Step 293700 | Avg Loss: 0.0156 | Grad Norm: 0.00837415\n",
      "Epoch 1 | Step 293800 | Avg Loss: 0.0156 | Grad Norm: 0.00851674\n",
      "Epoch 1 | Step 293900 | Avg Loss: 0.0155 | Grad Norm: 0.00859707\n",
      "Epoch 1 | Step 294000 | Avg Loss: 0.0151 | Grad Norm: 0.00889644\n",
      "Epoch 1 | Step 294100 | Avg Loss: 0.0152 | Grad Norm: 0.00822255\n",
      "Epoch 1 | Step 294200 | Avg Loss: 0.0153 | Grad Norm: 0.00830234\n",
      "Epoch 1 | Step 294300 | Avg Loss: 0.0160 | Grad Norm: 0.00911997\n",
      "Epoch 1 | Step 294400 | Avg Loss: 0.0156 | Grad Norm: 0.00749301\n",
      "Epoch 1 | Step 294500 | Avg Loss: 0.0159 | Grad Norm: 0.00925496\n",
      "Epoch 1 | Step 294600 | Avg Loss: 0.0161 | Grad Norm: 0.00917694\n",
      "Epoch 1 | Step 294700 | Avg Loss: 0.0164 | Grad Norm: 0.00820463\n",
      "Epoch 1 | Step 294800 | Avg Loss: 0.0162 | Grad Norm: 0.01012584\n",
      "Epoch 1 | Step 294900 | Avg Loss: 0.0162 | Grad Norm: 0.00785280\n",
      "Epoch 1 | Step 295000 | Avg Loss: 0.0160 | Grad Norm: 0.00904240\n",
      "Epoch 1 | Step 295100 | Avg Loss: 0.0158 | Grad Norm: 0.00938109\n",
      "Epoch 1 | Step 295200 | Avg Loss: 0.0157 | Grad Norm: 0.00842083\n",
      "Epoch 1 | Step 295300 | Avg Loss: 0.0156 | Grad Norm: 0.00987810\n",
      "Epoch 1 | Step 295400 | Avg Loss: 0.0156 | Grad Norm: 0.00860051\n",
      "Epoch 1 | Step 295500 | Avg Loss: 0.0159 | Grad Norm: 0.00867820\n",
      "Epoch 1 | Step 295600 | Avg Loss: 0.0159 | Grad Norm: 0.00814681\n",
      "Epoch 1 | Step 295700 | Avg Loss: 0.0155 | Grad Norm: 0.00862308\n",
      "Epoch 1 | Step 295800 | Avg Loss: 0.0157 | Grad Norm: 0.00792274\n",
      "Epoch 1 | Step 295900 | Avg Loss: 0.0158 | Grad Norm: 0.00798162\n",
      "Epoch 1 | Step 296000 | Avg Loss: 0.0161 | Grad Norm: 0.00843062\n",
      "Epoch 1 | Step 296100 | Avg Loss: 0.0162 | Grad Norm: 0.00851565\n",
      "Epoch 1 | Step 296200 | Avg Loss: 0.0163 | Grad Norm: 0.01034283\n",
      "Epoch 1 | Step 296300 | Avg Loss: 0.0167 | Grad Norm: 0.00925347\n",
      "Epoch 1 | Step 296400 | Avg Loss: 0.0162 | Grad Norm: 0.01070606\n",
      "Epoch 1 | Step 296500 | Avg Loss: 0.0158 | Grad Norm: 0.01022520\n",
      "Epoch 1 | Step 296600 | Avg Loss: 0.0158 | Grad Norm: 0.00956070\n",
      "Epoch 1 | Step 296700 | Avg Loss: 0.0157 | Grad Norm: 0.01000232\n",
      "Epoch 1 | Step 296800 | Avg Loss: 0.0157 | Grad Norm: 0.00906458\n",
      "Epoch 1 | Step 296900 | Avg Loss: 0.0155 | Grad Norm: 0.00804642\n",
      "Epoch 1 | Step 297000 | Avg Loss: 0.0160 | Grad Norm: 0.00875692\n",
      "Epoch 1 | Step 297100 | Avg Loss: 0.0163 | Grad Norm: 0.01195703\n",
      "Epoch 1 | Step 297200 | Avg Loss: 0.0160 | Grad Norm: 0.00905971\n",
      "Epoch 1 | Step 297300 | Avg Loss: 0.0159 | Grad Norm: 0.00867007\n",
      "Epoch 1 | Step 297400 | Avg Loss: 0.0163 | Grad Norm: 0.00849893\n",
      "Epoch 1 | Step 297500 | Avg Loss: 0.0163 | Grad Norm: 0.00890262\n",
      "Epoch 1 | Step 297600 | Avg Loss: 0.0166 | Grad Norm: 0.01111579\n",
      "Epoch 1 | Step 297700 | Avg Loss: 0.0166 | Grad Norm: 0.00868487\n",
      "Epoch 1 | Step 297800 | Avg Loss: 0.0167 | Grad Norm: 0.00911014\n",
      "Epoch 1 | Step 297900 | Avg Loss: 0.0167 | Grad Norm: 0.01068160\n",
      "Epoch 1 | Step 298000 | Avg Loss: 0.0162 | Grad Norm: 0.00970643\n",
      "Epoch 1 | Step 298100 | Avg Loss: 0.0162 | Grad Norm: 0.00888700\n",
      "Epoch 1 | Step 298200 | Avg Loss: 0.0159 | Grad Norm: 0.00881908\n",
      "Epoch 1 | Step 298300 | Avg Loss: 0.0156 | Grad Norm: 0.00781367\n",
      "Epoch 1 | Step 298400 | Avg Loss: 0.0158 | Grad Norm: 0.00990838\n",
      "Epoch 1 | Step 298500 | Avg Loss: 0.0157 | Grad Norm: 0.00812441\n",
      "Epoch 1 | Step 298600 | Avg Loss: 0.0158 | Grad Norm: 0.01058048\n",
      "Epoch 1 | Step 298700 | Avg Loss: 0.0159 | Grad Norm: 0.00942688\n",
      "Epoch 1 | Step 298800 | Avg Loss: 0.0159 | Grad Norm: 0.00882193\n",
      "Epoch 1 | Step 298900 | Avg Loss: 0.0155 | Grad Norm: 0.00790509\n",
      "Epoch 1 | Step 299000 | Avg Loss: 0.0153 | Grad Norm: 0.00731785\n",
      "Epoch 1 | Step 299100 | Avg Loss: 0.0155 | Grad Norm: 0.00828949\n",
      "Epoch 1 | Step 299200 | Avg Loss: 0.0154 | Grad Norm: 0.00811387\n",
      "Epoch 1 | Step 299300 | Avg Loss: 0.0153 | Grad Norm: 0.01004834\n",
      "Epoch 1 | Step 299400 | Avg Loss: 0.0156 | Grad Norm: 0.01000188\n",
      "Epoch 1 | Step 299500 | Avg Loss: 0.0154 | Grad Norm: 0.01429832\n",
      "Epoch 1 | Step 299600 | Avg Loss: 0.0156 | Grad Norm: 0.00983617\n",
      "Epoch 1 | Step 299700 | Avg Loss: 0.0153 | Grad Norm: 0.00962971\n",
      "Epoch 1 | Step 299800 | Avg Loss: 0.0153 | Grad Norm: 0.00919463\n",
      "Epoch 1 | Step 299900 | Avg Loss: 0.0154 | Grad Norm: 0.00751862\n",
      "Epoch 1 | Step 300000 | Avg Loss: 0.0154 | Grad Norm: 0.00852491\n",
      "Saving model at step300000\n",
      "Epoch 1 | Step 300100 | Avg Loss: 0.0153 | Grad Norm: 0.00857224\n",
      "Epoch 1 | Step 300200 | Avg Loss: 0.0155 | Grad Norm: 0.00943091\n",
      "Epoch 1 | Step 300300 | Avg Loss: 0.0151 | Grad Norm: 0.01040703\n",
      "Epoch 1 | Step 300400 | Avg Loss: 0.0151 | Grad Norm: 0.00851461\n",
      "Epoch 1 | Step 300500 | Avg Loss: 0.0149 | Grad Norm: 0.00741781\n",
      "Epoch 1 | Step 300600 | Avg Loss: 0.0150 | Grad Norm: 0.00905808\n",
      "Epoch 1 | Step 300700 | Avg Loss: 0.0148 | Grad Norm: 0.00868257\n",
      "Epoch 1 | Step 300800 | Avg Loss: 0.0149 | Grad Norm: 0.00872098\n",
      "Epoch 1 | Step 300900 | Avg Loss: 0.0151 | Grad Norm: 0.00885930\n",
      "Epoch 1 | Step 301000 | Avg Loss: 0.0153 | Grad Norm: 0.00803500\n",
      "Epoch 1 | Step 301100 | Avg Loss: 0.0153 | Grad Norm: 0.00854515\n",
      "Epoch 1 | Step 301200 | Avg Loss: 0.0154 | Grad Norm: 0.00829367\n",
      "Epoch 1 | Step 301300 | Avg Loss: 0.0154 | Grad Norm: 0.00720486\n",
      "Epoch 1 | Step 301400 | Avg Loss: 0.0158 | Grad Norm: 0.00894666\n",
      "Epoch 1 | Step 301500 | Avg Loss: 0.0157 | Grad Norm: 0.00721472\n",
      "Epoch 1 | Step 301600 | Avg Loss: 0.0156 | Grad Norm: 0.00825483\n",
      "Epoch 1 | Step 301700 | Avg Loss: 0.0157 | Grad Norm: 0.00736985\n",
      "Epoch 1 | Step 301800 | Avg Loss: 0.0158 | Grad Norm: 0.00821273\n",
      "Epoch 1 | Step 301900 | Avg Loss: 0.0156 | Grad Norm: 0.00970113\n",
      "Epoch 1 | Step 302000 | Avg Loss: 0.0154 | Grad Norm: 0.00780274\n",
      "Epoch 1 | Step 302100 | Avg Loss: 0.0153 | Grad Norm: 0.01172512\n",
      "Epoch 1 | Step 302200 | Avg Loss: 0.0153 | Grad Norm: 0.00912989\n",
      "Epoch 1 | Step 302300 | Avg Loss: 0.0151 | Grad Norm: 0.00923870\n",
      "Epoch 1 | Step 302400 | Avg Loss: 0.0152 | Grad Norm: 0.00912082\n",
      "Epoch 1 | Step 302500 | Avg Loss: 0.0150 | Grad Norm: 0.00859925\n",
      "Epoch 1 | Step 302600 | Avg Loss: 0.0153 | Grad Norm: 0.00938683\n",
      "Epoch 1 | Step 302700 | Avg Loss: 0.0152 | Grad Norm: 0.00734050\n",
      "Epoch 1 | Step 302800 | Avg Loss: 0.0153 | Grad Norm: 0.00826517\n",
      "Epoch 1 | Step 302900 | Avg Loss: 0.0155 | Grad Norm: 0.00856927\n",
      "Epoch 1 | Step 303000 | Avg Loss: 0.0154 | Grad Norm: 0.01042422\n",
      "Epoch 1 | Step 303100 | Avg Loss: 0.0157 | Grad Norm: 0.00943653\n",
      "Epoch 1 | Step 303200 | Avg Loss: 0.0158 | Grad Norm: 0.00844089\n",
      "Epoch 1 | Step 303300 | Avg Loss: 0.0158 | Grad Norm: 0.00761224\n",
      "Epoch 1 | Step 303400 | Avg Loss: 0.0156 | Grad Norm: 0.00905896\n",
      "Epoch 1 | Step 303500 | Avg Loss: 0.0153 | Grad Norm: 0.00823140\n",
      "Epoch 1 | Step 303600 | Avg Loss: 0.0153 | Grad Norm: 0.00928475\n",
      "Epoch 1 | Step 303700 | Avg Loss: 0.0153 | Grad Norm: 0.01020658\n",
      "Epoch 1 | Step 303800 | Avg Loss: 0.0153 | Grad Norm: 0.00875773\n",
      "Epoch 1 | Step 303900 | Avg Loss: 0.0153 | Grad Norm: 0.01488320\n",
      "Epoch 1 | Step 304000 | Avg Loss: 0.0156 | Grad Norm: 0.00920551\n",
      "Epoch 1 | Step 304100 | Avg Loss: 0.0157 | Grad Norm: 0.00910375\n",
      "Epoch 1 | Step 304200 | Avg Loss: 0.0156 | Grad Norm: 0.00846502\n",
      "Epoch 1 | Step 304300 | Avg Loss: 0.0156 | Grad Norm: 0.01018541\n",
      "Epoch 1 | Step 304400 | Avg Loss: 0.0153 | Grad Norm: 0.00778627\n",
      "Epoch 1 | Step 304500 | Avg Loss: 0.0154 | Grad Norm: 0.01149086\n",
      "Epoch 1 | Step 304600 | Avg Loss: 0.0155 | Grad Norm: 0.00932060\n",
      "Epoch 1 | Step 304700 | Avg Loss: 0.0156 | Grad Norm: 0.00995440\n",
      "Epoch 1 | Step 304800 | Avg Loss: 0.0155 | Grad Norm: 0.00888200\n",
      "Epoch 1 | Step 304900 | Avg Loss: 0.0153 | Grad Norm: 0.00781993\n",
      "Epoch 1 | Step 305000 | Avg Loss: 0.0154 | Grad Norm: 0.00878831\n",
      "Epoch 1 | Step 305100 | Avg Loss: 0.0154 | Grad Norm: 0.00849405\n",
      "Epoch 1 | Step 305200 | Avg Loss: 0.0155 | Grad Norm: 0.00800078\n",
      "Epoch 1 | Step 305300 | Avg Loss: 0.0155 | Grad Norm: 0.00954241\n",
      "Epoch 1 | Step 305400 | Avg Loss: 0.0153 | Grad Norm: 0.00806270\n",
      "Epoch 1 | Step 305500 | Avg Loss: 0.0156 | Grad Norm: 0.00958451\n",
      "Epoch 1 | Step 305600 | Avg Loss: 0.0157 | Grad Norm: 0.01032557\n",
      "Epoch 1 | Step 305700 | Avg Loss: 0.0156 | Grad Norm: 0.00962728\n",
      "Epoch 1 | Step 305800 | Avg Loss: 0.0159 | Grad Norm: 0.00751711\n",
      "Epoch 1 | Step 305900 | Avg Loss: 0.0156 | Grad Norm: 0.01069098\n",
      "Epoch 1 | Step 306000 | Avg Loss: 0.0158 | Grad Norm: 0.00851771\n",
      "Epoch 1 | Step 306100 | Avg Loss: 0.0156 | Grad Norm: 0.00794305\n",
      "Epoch 1 | Step 306200 | Avg Loss: 0.0152 | Grad Norm: 0.00811322\n",
      "Epoch 1 | Step 306300 | Avg Loss: 0.0150 | Grad Norm: 0.01023944\n",
      "Epoch 1 | Step 306400 | Avg Loss: 0.0155 | Grad Norm: 0.00990056\n",
      "Epoch 1 | Step 306500 | Avg Loss: 0.0154 | Grad Norm: 0.00784529\n",
      "Epoch 1 | Step 306600 | Avg Loss: 0.0157 | Grad Norm: 0.00762913\n",
      "Epoch 1 | Step 306700 | Avg Loss: 0.0155 | Grad Norm: 0.00912818\n",
      "Epoch 1 | Step 306800 | Avg Loss: 0.0154 | Grad Norm: 0.00880479\n",
      "Epoch 1 | Step 306900 | Avg Loss: 0.0156 | Grad Norm: 0.01228053\n",
      "Epoch 1 | Step 307000 | Avg Loss: 0.0156 | Grad Norm: 0.00937353\n",
      "Epoch 1 | Step 307100 | Avg Loss: 0.0160 | Grad Norm: 0.00800781\n",
      "Epoch 1 | Step 307200 | Avg Loss: 0.0158 | Grad Norm: 0.00877866\n",
      "Epoch 1 | Step 307300 | Avg Loss: 0.0157 | Grad Norm: 0.00890006\n",
      "Epoch 1 | Step 307400 | Avg Loss: 0.0159 | Grad Norm: 0.00837813\n",
      "Epoch 1 | Step 307500 | Avg Loss: 0.0158 | Grad Norm: 0.00960122\n",
      "Epoch 1 | Step 307600 | Avg Loss: 0.0156 | Grad Norm: 0.00809731\n",
      "Epoch 1 | Step 307700 | Avg Loss: 0.0155 | Grad Norm: 0.00882307\n",
      "Epoch 1 | Step 307800 | Avg Loss: 0.0156 | Grad Norm: 0.00861929\n",
      "Epoch 1 | Step 307900 | Avg Loss: 0.0157 | Grad Norm: 0.01077414\n",
      "Epoch 1 | Step 308000 | Avg Loss: 0.0159 | Grad Norm: 0.00998791\n",
      "Epoch 1 | Step 308100 | Avg Loss: 0.0158 | Grad Norm: 0.00757280\n",
      "Epoch 1 | Step 308200 | Avg Loss: 0.0157 | Grad Norm: 0.01020937\n",
      "Epoch 1 | Step 308300 | Avg Loss: 0.0153 | Grad Norm: 0.00914606\n",
      "Epoch 1 | Step 308400 | Avg Loss: 0.0157 | Grad Norm: 0.00906318\n",
      "Epoch 1 | Step 308500 | Avg Loss: 0.0158 | Grad Norm: 0.00961409\n",
      "Epoch 1 | Step 308600 | Avg Loss: 0.0156 | Grad Norm: 0.01018387\n",
      "Epoch 1 | Step 308700 | Avg Loss: 0.0152 | Grad Norm: 0.00950306\n",
      "Epoch 1 | Step 308800 | Avg Loss: 0.0157 | Grad Norm: 0.00750946\n",
      "Epoch 1 | Step 308900 | Avg Loss: 0.0155 | Grad Norm: 0.00851988\n",
      "Epoch 1 | Step 309000 | Avg Loss: 0.0156 | Grad Norm: 0.00982767\n",
      "Epoch 1 | Step 309100 | Avg Loss: 0.0152 | Grad Norm: 0.00763842\n",
      "Epoch 1 | Step 309200 | Avg Loss: 0.0154 | Grad Norm: 0.00784529\n",
      "Epoch 1 | Step 309300 | Avg Loss: 0.0154 | Grad Norm: 0.00941184\n",
      "Epoch 1 | Step 309400 | Avg Loss: 0.0156 | Grad Norm: 0.00920542\n",
      "Epoch 1 | Step 309500 | Avg Loss: 0.0150 | Grad Norm: 0.00809480\n",
      "Epoch 1 | Step 309600 | Avg Loss: 0.0150 | Grad Norm: 0.00949114\n",
      "Epoch 1 | Step 309700 | Avg Loss: 0.0153 | Grad Norm: 0.00799907\n",
      "Epoch 1 | Step 309800 | Avg Loss: 0.0154 | Grad Norm: 0.00923504\n",
      "Epoch 1 | Step 309900 | Avg Loss: 0.0153 | Grad Norm: 0.00861338\n",
      "Epoch 1 | Step 310000 | Avg Loss: 0.0157 | Grad Norm: 0.00759648\n",
      "Epoch 1 | Step 310100 | Avg Loss: 0.0156 | Grad Norm: 0.00845326\n",
      "Epoch 1 | Step 310200 | Avg Loss: 0.0158 | Grad Norm: 0.00935792\n",
      "Epoch 1 | Step 310300 | Avg Loss: 0.0158 | Grad Norm: 0.00863492\n",
      "Epoch 1 | Step 310400 | Avg Loss: 0.0155 | Grad Norm: 0.00800614\n",
      "Epoch 1 | Step 310500 | Avg Loss: 0.0155 | Grad Norm: 0.00815427\n",
      "Epoch 1 | Step 310600 | Avg Loss: 0.0152 | Grad Norm: 0.00931615\n",
      "Epoch 1 | Step 310700 | Avg Loss: 0.0153 | Grad Norm: 0.01059152\n",
      "Epoch 1 | Step 310800 | Avg Loss: 0.0156 | Grad Norm: 0.00908787\n",
      "Epoch 1 | Step 310900 | Avg Loss: 0.0154 | Grad Norm: 0.00764387\n",
      "Epoch 1 | Step 311000 | Avg Loss: 0.0153 | Grad Norm: 0.00825171\n",
      "Epoch 1 | Step 311100 | Avg Loss: 0.0156 | Grad Norm: 0.00978698\n",
      "Epoch 1 | Step 311200 | Avg Loss: 0.0160 | Grad Norm: 0.00928897\n",
      "Epoch 1 | Step 311300 | Avg Loss: 0.0163 | Grad Norm: 0.01054705\n",
      "Epoch 1 | Step 311400 | Avg Loss: 0.0158 | Grad Norm: 0.00970018\n",
      "Epoch 1 | Step 311500 | Avg Loss: 0.0154 | Grad Norm: 0.00915995\n",
      "Epoch 1 | Step 311600 | Avg Loss: 0.0158 | Grad Norm: 0.00865714\n",
      "Epoch 1 | Step 311700 | Avg Loss: 0.0157 | Grad Norm: 0.00848875\n",
      "Epoch 1 | Step 311800 | Avg Loss: 0.0153 | Grad Norm: 0.01233124\n",
      "Epoch 1 | Step 311900 | Avg Loss: 0.0154 | Grad Norm: 0.00927342\n",
      "Epoch 1 | Step 312000 | Avg Loss: 0.0155 | Grad Norm: 0.00889465\n",
      "Epoch 1 | Step 312100 | Avg Loss: 0.0157 | Grad Norm: 0.00846434\n",
      "Epoch 1 | Step 312200 | Avg Loss: 0.0160 | Grad Norm: 0.00958805\n",
      "Epoch 1 | Step 312300 | Avg Loss: 0.0159 | Grad Norm: 0.01019243\n",
      "Epoch 1 | Step 312400 | Avg Loss: 0.0160 | Grad Norm: 0.01016158\n",
      "Epoch 1 | Step 312500 | Avg Loss: 0.0158 | Grad Norm: 0.00876682\n",
      "Epoch 1 | Step 312600 | Avg Loss: 0.0160 | Grad Norm: 0.00905586\n",
      "Epoch 1 | Step 312700 | Avg Loss: 0.0159 | Grad Norm: 0.00815455\n",
      "Epoch 1 | Step 312800 | Avg Loss: 0.0158 | Grad Norm: 0.00928340\n",
      "Epoch 1 | Step 312900 | Avg Loss: 0.0159 | Grad Norm: 0.00820436\n",
      "Epoch 1 | Step 313000 | Avg Loss: 0.0158 | Grad Norm: 0.00888896\n",
      "Epoch 1 | Step 313100 | Avg Loss: 0.0154 | Grad Norm: 0.00919511\n",
      "Epoch 1 | Step 313200 | Avg Loss: 0.0153 | Grad Norm: 0.00894533\n",
      "Epoch 1 | Step 313300 | Avg Loss: 0.0154 | Grad Norm: 0.00917408\n",
      "Epoch 1 | Step 313400 | Avg Loss: 0.0152 | Grad Norm: 0.01014617\n",
      "Epoch 1 | Step 313500 | Avg Loss: 0.0152 | Grad Norm: 0.00835740\n",
      "Epoch 1 | Step 313600 | Avg Loss: 0.0152 | Grad Norm: 0.00776637\n",
      "Epoch 1 | Step 313700 | Avg Loss: 0.0151 | Grad Norm: 0.00873735\n",
      "Epoch 1 | Step 313800 | Avg Loss: 0.0155 | Grad Norm: 0.00925710\n",
      "Epoch 1 | Step 313900 | Avg Loss: 0.0152 | Grad Norm: 0.00805143\n",
      "Epoch 1 | Step 314000 | Avg Loss: 0.0152 | Grad Norm: 0.00800384\n",
      "Epoch 1 | Step 314100 | Avg Loss: 0.0154 | Grad Norm: 0.00863264\n",
      "Epoch 1 | Step 314200 | Avg Loss: 0.0149 | Grad Norm: 0.00891896\n",
      "Epoch 1 | Step 314300 | Avg Loss: 0.0148 | Grad Norm: 0.00861105\n",
      "Epoch 1 | Step 314400 | Avg Loss: 0.0149 | Grad Norm: 0.00880659\n",
      "Epoch 1 | Step 314500 | Avg Loss: 0.0153 | Grad Norm: 0.00942393\n",
      "Epoch 1 | Step 314600 | Avg Loss: 0.0155 | Grad Norm: 0.00962974\n",
      "Epoch 1 | Step 314700 | Avg Loss: 0.0157 | Grad Norm: 0.00881204\n",
      "Epoch 1 | Step 314800 | Avg Loss: 0.0160 | Grad Norm: 0.00916513\n",
      "Epoch 1 | Step 314900 | Avg Loss: 0.0161 | Grad Norm: 0.01153769\n",
      "Epoch 1 | Step 315000 | Avg Loss: 0.0161 | Grad Norm: 0.00925418\n",
      "Epoch 1 | Step 315100 | Avg Loss: 0.0161 | Grad Norm: 0.00833435\n",
      "Epoch 1 | Step 315200 | Avg Loss: 0.0163 | Grad Norm: 0.00772632\n",
      "Epoch 1 | Step 315300 | Avg Loss: 0.0158 | Grad Norm: 0.00846663\n",
      "Epoch 1 | Step 315400 | Avg Loss: 0.0161 | Grad Norm: 0.01017378\n",
      "Epoch 1 | Step 315500 | Avg Loss: 0.0158 | Grad Norm: 0.01047009\n",
      "Epoch 1 | Step 315600 | Avg Loss: 0.0159 | Grad Norm: 0.00819526\n",
      "Epoch 1 | Step 315700 | Avg Loss: 0.0158 | Grad Norm: 0.00913900\n",
      "Epoch 1 | Step 315800 | Avg Loss: 0.0163 | Grad Norm: 0.01248783\n",
      "Epoch 1 | Step 315900 | Avg Loss: 0.0160 | Grad Norm: 0.00898972\n",
      "Epoch 1 | Step 316000 | Avg Loss: 0.0162 | Grad Norm: 0.00870583\n",
      "Epoch 1 | Step 316100 | Avg Loss: 0.0158 | Grad Norm: 0.01003481\n",
      "Epoch 1 | Step 316200 | Avg Loss: 0.0156 | Grad Norm: 0.00950750\n",
      "Epoch 1 | Step 316300 | Avg Loss: 0.0158 | Grad Norm: 0.00818251\n",
      "Epoch 1 | Step 316400 | Avg Loss: 0.0155 | Grad Norm: 0.00983457\n",
      "Epoch 1 | Step 316500 | Avg Loss: 0.0155 | Grad Norm: 0.00760549\n",
      "Epoch 1 | Step 316600 | Avg Loss: 0.0159 | Grad Norm: 0.00933127\n",
      "Epoch 1 | Step 316700 | Avg Loss: 0.0157 | Grad Norm: 0.01120001\n",
      "Epoch 1 | Step 316800 | Avg Loss: 0.0159 | Grad Norm: 0.00949218\n",
      "Epoch 1 | Step 316900 | Avg Loss: 0.0157 | Grad Norm: 0.01018545\n",
      "Epoch 1 | Step 317000 | Avg Loss: 0.0158 | Grad Norm: 0.00916603\n",
      "Epoch 1 | Step 317100 | Avg Loss: 0.0163 | Grad Norm: 0.01005407\n",
      "Epoch 1 | Step 317200 | Avg Loss: 0.0166 | Grad Norm: 0.00974618\n",
      "Epoch 1 | Step 317300 | Avg Loss: 0.0162 | Grad Norm: 0.01097670\n",
      "Epoch 1 | Step 317400 | Avg Loss: 0.0162 | Grad Norm: 0.00933696\n",
      "Epoch 1 | Step 317500 | Avg Loss: 0.0160 | Grad Norm: 0.00864360\n",
      "Epoch 1 | Step 317600 | Avg Loss: 0.0158 | Grad Norm: 0.01067203\n",
      "Epoch 1 | Step 317700 | Avg Loss: 0.0160 | Grad Norm: 0.00785666\n",
      "Epoch 1 | Step 317800 | Avg Loss: 0.0156 | Grad Norm: 0.00837030\n",
      "Epoch 1 | Step 317900 | Avg Loss: 0.0156 | Grad Norm: 0.00741603\n",
      "Epoch 1 | Step 318000 | Avg Loss: 0.0155 | Grad Norm: 0.00986372\n",
      "Epoch 1 | Step 318100 | Avg Loss: 0.0159 | Grad Norm: 0.00903589\n",
      "Epoch 1 | Step 318200 | Avg Loss: 0.0156 | Grad Norm: 0.00799086\n",
      "Epoch 1 | Step 318300 | Avg Loss: 0.0154 | Grad Norm: 0.01003932\n",
      "Epoch 1 | Step 318400 | Avg Loss: 0.0155 | Grad Norm: 0.00901069\n",
      "Epoch 1 | Step 318500 | Avg Loss: 0.0154 | Grad Norm: 0.00829118\n",
      "Epoch 1 | Step 318600 | Avg Loss: 0.0155 | Grad Norm: 0.00915307\n",
      "Epoch 1 | Step 318700 | Avg Loss: 0.0156 | Grad Norm: 0.00913810\n",
      "Epoch 1 | Step 318800 | Avg Loss: 0.0157 | Grad Norm: 0.00895475\n",
      "Epoch 1 | Step 318900 | Avg Loss: 0.0162 | Grad Norm: 0.01021975\n",
      "Epoch 1 | Step 319000 | Avg Loss: 0.0161 | Grad Norm: 0.00887746\n",
      "Epoch 1 | Step 319100 | Avg Loss: 0.0160 | Grad Norm: 0.00808508\n",
      "Epoch 1 | Step 319200 | Avg Loss: 0.0161 | Grad Norm: 0.01055289\n",
      "Epoch 1 | Step 319300 | Avg Loss: 0.0158 | Grad Norm: 0.01013167\n",
      "Epoch 1 | Step 319400 | Avg Loss: 0.0158 | Grad Norm: 0.00965236\n",
      "Epoch 1 | Step 319500 | Avg Loss: 0.0158 | Grad Norm: 0.00854627\n",
      "Epoch 1 | Step 319600 | Avg Loss: 0.0161 | Grad Norm: 0.00869356\n",
      "Epoch 1 | Step 319700 | Avg Loss: 0.0165 | Grad Norm: 0.00937811\n",
      "Epoch 1 | Step 319800 | Avg Loss: 0.0164 | Grad Norm: 0.00830284\n",
      "Epoch 1 | Step 319900 | Avg Loss: 0.0167 | Grad Norm: 0.00782217\n",
      "Epoch 1 | Step 320000 | Avg Loss: 0.0163 | Grad Norm: 0.00785204\n",
      "Epoch 1 | Step 320100 | Avg Loss: 0.0161 | Grad Norm: 0.00908105\n",
      "Epoch 1 | Step 320200 | Avg Loss: 0.0157 | Grad Norm: 0.00944870\n",
      "Epoch 1 | Step 320300 | Avg Loss: 0.0160 | Grad Norm: 0.00847175\n",
      "Epoch 1 | Step 320400 | Avg Loss: 0.0158 | Grad Norm: 0.00723783\n",
      "Epoch 1 | Step 320500 | Avg Loss: 0.0163 | Grad Norm: 0.00855640\n",
      "Epoch 1 | Step 320600 | Avg Loss: 0.0164 | Grad Norm: 0.00902577\n",
      "Epoch 1 | Step 320700 | Avg Loss: 0.0160 | Grad Norm: 0.00938781\n",
      "Epoch 1 | Step 320800 | Avg Loss: 0.0156 | Grad Norm: 0.00832163\n",
      "Epoch 1 | Step 320900 | Avg Loss: 0.0158 | Grad Norm: 0.00750586\n",
      "Epoch 1 | Step 321000 | Avg Loss: 0.0156 | Grad Norm: 0.00923514\n",
      "Epoch 1 | Step 321100 | Avg Loss: 0.0161 | Grad Norm: 0.00923025\n",
      "Epoch 1 | Step 321200 | Avg Loss: 0.0158 | Grad Norm: 0.00916432\n",
      "Epoch 1 | Step 321300 | Avg Loss: 0.0156 | Grad Norm: 0.00981992\n",
      "Epoch 1 | Step 321400 | Avg Loss: 0.0158 | Grad Norm: 0.00911624\n",
      "Epoch 1 | Step 321500 | Avg Loss: 0.0157 | Grad Norm: 0.00826080\n",
      "Epoch 1 | Step 321600 | Avg Loss: 0.0157 | Grad Norm: 0.00863107\n",
      "Epoch 1 | Step 321700 | Avg Loss: 0.0156 | Grad Norm: 0.00938025\n",
      "Epoch 1 | Step 321800 | Avg Loss: 0.0159 | Grad Norm: 0.00846558\n",
      "Epoch 1 | Step 321900 | Avg Loss: 0.0160 | Grad Norm: 0.00846337\n",
      "Epoch 1 | Step 322000 | Avg Loss: 0.0159 | Grad Norm: 0.00822265\n",
      "Epoch 1 | Step 322100 | Avg Loss: 0.0157 | Grad Norm: 0.00993323\n",
      "Epoch 1 | Step 322200 | Avg Loss: 0.0157 | Grad Norm: 0.00891604\n",
      "Epoch 1 | Step 322300 | Avg Loss: 0.0157 | Grad Norm: 0.00853384\n",
      "Epoch 1 | Step 322400 | Avg Loss: 0.0158 | Grad Norm: 0.00887763\n",
      "Epoch 1 | Step 322500 | Avg Loss: 0.0157 | Grad Norm: 0.00815733\n",
      "Epoch 1 | Step 322600 | Avg Loss: 0.0156 | Grad Norm: 0.00798797\n",
      "Epoch 1 | Step 322700 | Avg Loss: 0.0155 | Grad Norm: 0.00899336\n",
      "Epoch 1 | Step 322800 | Avg Loss: 0.0155 | Grad Norm: 0.01304483\n",
      "Epoch 1 | Step 322900 | Avg Loss: 0.0156 | Grad Norm: 0.00818491\n",
      "Epoch 1 | Step 323000 | Avg Loss: 0.0156 | Grad Norm: 0.00865346\n",
      "Epoch 1 | Step 323100 | Avg Loss: 0.0157 | Grad Norm: 0.00882047\n",
      "Epoch 1 | Step 323200 | Avg Loss: 0.0156 | Grad Norm: 0.00893666\n",
      "Epoch 1 | Step 323300 | Avg Loss: 0.0156 | Grad Norm: 0.00932427\n",
      "Epoch 1 | Step 323400 | Avg Loss: 0.0156 | Grad Norm: 0.00845643\n",
      "Epoch 1 | Step 323500 | Avg Loss: 0.0158 | Grad Norm: 0.00930898\n",
      "Epoch 1 | Step 323600 | Avg Loss: 0.0163 | Grad Norm: 0.00908686\n",
      "Epoch 1 | Step 323700 | Avg Loss: 0.0162 | Grad Norm: 0.00799046\n",
      "Epoch 1 | Step 323800 | Avg Loss: 0.0156 | Grad Norm: 0.00897070\n",
      "Epoch 1 | Step 323900 | Avg Loss: 0.0151 | Grad Norm: 0.00892625\n",
      "Epoch 1 | Step 324000 | Avg Loss: 0.0153 | Grad Norm: 0.00847535\n",
      "Epoch 1 | Step 324100 | Avg Loss: 0.0153 | Grad Norm: 0.00882978\n",
      "Epoch 1 | Step 324200 | Avg Loss: 0.0155 | Grad Norm: 0.00985725\n",
      "Epoch 1 | Step 324300 | Avg Loss: 0.0154 | Grad Norm: 0.00830430\n",
      "Epoch 1 | Step 324400 | Avg Loss: 0.0155 | Grad Norm: 0.00792764\n",
      "Epoch 1 | Step 324500 | Avg Loss: 0.0156 | Grad Norm: 0.00959272\n",
      "Epoch 1 | Step 324600 | Avg Loss: 0.0153 | Grad Norm: 0.00773181\n",
      "Epoch 1 | Step 324700 | Avg Loss: 0.0156 | Grad Norm: 0.00980219\n",
      "Epoch 1 | Step 324800 | Avg Loss: 0.0157 | Grad Norm: 0.01002933\n",
      "Epoch 1 | Step 324900 | Avg Loss: 0.0163 | Grad Norm: 0.01061102\n",
      "Epoch 1 | Step 325000 | Avg Loss: 0.0161 | Grad Norm: 0.00798288\n",
      "Epoch 1 | Step 325100 | Avg Loss: 0.0162 | Grad Norm: 0.00883338\n",
      "Epoch 1 | Step 325200 | Avg Loss: 0.0156 | Grad Norm: 0.00820954\n",
      "Epoch 1 | Step 325300 | Avg Loss: 0.0152 | Grad Norm: 0.00724782\n",
      "Epoch 1 | Step 325400 | Avg Loss: 0.0154 | Grad Norm: 0.00872480\n",
      "Epoch 1 | Step 325500 | Avg Loss: 0.0154 | Grad Norm: 0.00917844\n",
      "Epoch 1 | Step 325600 | Avg Loss: 0.0156 | Grad Norm: 0.00877267\n",
      "Epoch 1 | Step 325700 | Avg Loss: 0.0158 | Grad Norm: 0.00897987\n",
      "Epoch 1 | Step 325800 | Avg Loss: 0.0157 | Grad Norm: 0.00853343\n",
      "Epoch 1 | Step 325900 | Avg Loss: 0.0155 | Grad Norm: 0.00902180\n",
      "Epoch 1 | Step 326000 | Avg Loss: 0.0155 | Grad Norm: 0.00794097\n",
      "Epoch 1 | Step 326100 | Avg Loss: 0.0152 | Grad Norm: 0.00948199\n",
      "Epoch 1 | Step 326200 | Avg Loss: 0.0149 | Grad Norm: 0.00829689\n",
      "Epoch 1 | Step 326300 | Avg Loss: 0.0147 | Grad Norm: 0.00882844\n",
      "Epoch 1 | Step 326400 | Avg Loss: 0.0146 | Grad Norm: 0.00812456\n",
      "Epoch 1 | Step 326500 | Avg Loss: 0.0151 | Grad Norm: 0.00851249\n",
      "Epoch 1 | Step 326600 | Avg Loss: 0.0154 | Grad Norm: 0.00799304\n",
      "Epoch 1 | Step 326700 | Avg Loss: 0.0155 | Grad Norm: 0.00848901\n",
      "Epoch 1 | Step 326800 | Avg Loss: 0.0155 | Grad Norm: 0.00865810\n",
      "Epoch 1 | Step 326900 | Avg Loss: 0.0156 | Grad Norm: 0.00883967\n",
      "Epoch 1 | Step 327000 | Avg Loss: 0.0158 | Grad Norm: 0.00933167\n",
      "Epoch 1 | Step 327100 | Avg Loss: 0.0160 | Grad Norm: 0.00834831\n",
      "Epoch 1 | Step 327200 | Avg Loss: 0.0163 | Grad Norm: 0.00892735\n",
      "Epoch 1 | Step 327300 | Avg Loss: 0.0159 | Grad Norm: 0.01034166\n",
      "Epoch 1 | Step 327400 | Avg Loss: 0.0160 | Grad Norm: 0.00775512\n",
      "Epoch 1 | Step 327500 | Avg Loss: 0.0159 | Grad Norm: 0.00780047\n",
      "Epoch 1 | Step 327600 | Avg Loss: 0.0155 | Grad Norm: 0.00810900\n",
      "Epoch 1 | Step 327700 | Avg Loss: 0.0156 | Grad Norm: 0.00856954\n",
      "Epoch 1 | Step 327800 | Avg Loss: 0.0152 | Grad Norm: 0.00768698\n",
      "Epoch 1 | Step 327900 | Avg Loss: 0.0150 | Grad Norm: 0.00794575\n",
      "Epoch 1 | Step 328000 | Avg Loss: 0.0149 | Grad Norm: 0.00794771\n",
      "Epoch 1 | Step 328100 | Avg Loss: 0.0150 | Grad Norm: 0.00919239\n",
      "Epoch 1 | Step 328200 | Avg Loss: 0.0151 | Grad Norm: 0.00818122\n",
      "Epoch 1 | Step 328300 | Avg Loss: 0.0153 | Grad Norm: 0.01181228\n",
      "Epoch 1 | Step 328400 | Avg Loss: 0.0151 | Grad Norm: 0.00818715\n",
      "Epoch 1 | Step 328500 | Avg Loss: 0.0154 | Grad Norm: 0.00858028\n",
      "Epoch 1 | Step 328600 | Avg Loss: 0.0154 | Grad Norm: 0.00970183\n",
      "Epoch 1 | Step 328700 | Avg Loss: 0.0156 | Grad Norm: 0.00784219\n",
      "Epoch 1 | Step 328800 | Avg Loss: 0.0158 | Grad Norm: 0.00721932\n",
      "Epoch 1 | Step 328900 | Avg Loss: 0.0161 | Grad Norm: 0.00962065\n",
      "Epoch 1 | Step 329000 | Avg Loss: 0.0156 | Grad Norm: 0.00974703\n",
      "Epoch 1 | Step 329100 | Avg Loss: 0.0158 | Grad Norm: 0.00922678\n",
      "Epoch 1 | Step 329200 | Avg Loss: 0.0157 | Grad Norm: 0.00794766\n",
      "Epoch 1 | Step 329300 | Avg Loss: 0.0155 | Grad Norm: 0.00878175\n",
      "Epoch 1 | Step 329400 | Avg Loss: 0.0154 | Grad Norm: 0.00842184\n",
      "Epoch 1 | Step 329500 | Avg Loss: 0.0151 | Grad Norm: 0.00969248\n",
      "Epoch 1 | Step 329600 | Avg Loss: 0.0156 | Grad Norm: 0.00875674\n",
      "Epoch 1 | Step 329700 | Avg Loss: 0.0154 | Grad Norm: 0.01260682\n",
      "Epoch 1 | Step 329800 | Avg Loss: 0.0154 | Grad Norm: 0.00911922\n",
      "Epoch 1 | Step 329900 | Avg Loss: 0.0154 | Grad Norm: 0.00900182\n",
      "Epoch 1 | Step 330000 | Avg Loss: 0.0153 | Grad Norm: 0.00883181\n",
      "Epoch 1 | Step 330100 | Avg Loss: 0.0153 | Grad Norm: 0.00849071\n",
      "Epoch 1 | Step 330200 | Avg Loss: 0.0154 | Grad Norm: 0.00924457\n",
      "Epoch 1 | Step 330300 | Avg Loss: 0.0155 | Grad Norm: 0.00812669\n",
      "Epoch 1 | Step 330400 | Avg Loss: 0.0158 | Grad Norm: 0.00827406\n",
      "Epoch 1 | Step 330500 | Avg Loss: 0.0157 | Grad Norm: 0.00855117\n",
      "Epoch 1 | Step 330600 | Avg Loss: 0.0158 | Grad Norm: 0.00949725\n",
      "Epoch 1 | Step 330700 | Avg Loss: 0.0162 | Grad Norm: 0.00933022\n",
      "Epoch 1 | Step 330800 | Avg Loss: 0.0163 | Grad Norm: 0.00962144\n",
      "Epoch 1 | Step 330900 | Avg Loss: 0.0166 | Grad Norm: 0.01027596\n",
      "Epoch 1 | Step 331000 | Avg Loss: 0.0162 | Grad Norm: 0.00880606\n",
      "Epoch 1 | Step 331100 | Avg Loss: 0.0163 | Grad Norm: 0.00800929\n",
      "Epoch 1 | Step 331200 | Avg Loss: 0.0162 | Grad Norm: 0.00935825\n",
      "Epoch 1 | Step 331300 | Avg Loss: 0.0159 | Grad Norm: 0.01008928\n",
      "Epoch 1 | Step 331400 | Avg Loss: 0.0159 | Grad Norm: 0.00799199\n",
      "Epoch 1 | Step 331500 | Avg Loss: 0.0164 | Grad Norm: 0.00832702\n",
      "Epoch 1 | Step 331600 | Avg Loss: 0.0169 | Grad Norm: 0.00846464\n",
      "Epoch 1 | Step 331700 | Avg Loss: 0.0163 | Grad Norm: 0.00787162\n",
      "Epoch 1 | Step 331800 | Avg Loss: 0.0163 | Grad Norm: 0.00874574\n",
      "Epoch 1 | Step 331900 | Avg Loss: 0.0160 | Grad Norm: 0.00751078\n",
      "Epoch 1 | Step 332000 | Avg Loss: 0.0158 | Grad Norm: 0.00802006\n",
      "Epoch 1 | Step 332100 | Avg Loss: 0.0162 | Grad Norm: 0.00962363\n",
      "Epoch 1 | Step 332200 | Avg Loss: 0.0159 | Grad Norm: 0.00994050\n",
      "Epoch 1 | Step 332300 | Avg Loss: 0.0159 | Grad Norm: 0.00882105\n",
      "Epoch 1 | Step 332400 | Avg Loss: 0.0155 | Grad Norm: 0.00927495\n",
      "Epoch 1 | Step 332500 | Avg Loss: 0.0158 | Grad Norm: 0.00877875\n",
      "Epoch 1 | Step 332600 | Avg Loss: 0.0158 | Grad Norm: 0.00993955\n",
      "Epoch 1 | Step 332700 | Avg Loss: 0.0158 | Grad Norm: 0.00944179\n",
      "Epoch 1 | Step 332800 | Avg Loss: 0.0156 | Grad Norm: 0.01029824\n",
      "Epoch 1 | Step 332900 | Avg Loss: 0.0161 | Grad Norm: 0.00747026\n",
      "Epoch 1 | Step 333000 | Avg Loss: 0.0162 | Grad Norm: 0.00905179\n",
      "Epoch 1 | Step 333100 | Avg Loss: 0.0161 | Grad Norm: 0.00837937\n",
      "Epoch 1 | Step 333200 | Avg Loss: 0.0157 | Grad Norm: 0.00788739\n",
      "Epoch 1 | Step 333300 | Avg Loss: 0.0163 | Grad Norm: 0.01027879\n",
      "Epoch 1 | Step 333400 | Avg Loss: 0.0162 | Grad Norm: 0.00843653\n",
      "Epoch 1 | Step 333500 | Avg Loss: 0.0162 | Grad Norm: 0.01149903\n",
      "Epoch 1 | Step 333600 | Avg Loss: 0.0159 | Grad Norm: 0.00740631\n",
      "Epoch 1 | Step 333700 | Avg Loss: 0.0157 | Grad Norm: 0.00969567\n",
      "Epoch 1 | Step 333800 | Avg Loss: 0.0156 | Grad Norm: 0.01193676\n",
      "Epoch 1 | Step 333900 | Avg Loss: 0.0157 | Grad Norm: 0.00997076\n",
      "Epoch 1 | Step 334000 | Avg Loss: 0.0155 | Grad Norm: 0.00776126\n",
      "Epoch 1 | Step 334100 | Avg Loss: 0.0160 | Grad Norm: 0.00784932\n",
      "Epoch 1 | Step 334200 | Avg Loss: 0.0162 | Grad Norm: 0.01060063\n",
      "Epoch 1 | Step 334300 | Avg Loss: 0.0160 | Grad Norm: 0.00865084\n",
      "Epoch 1 | Step 334400 | Avg Loss: 0.0159 | Grad Norm: 0.00837079\n",
      "Epoch 1 | Step 334500 | Avg Loss: 0.0156 | Grad Norm: 0.00866524\n",
      "Epoch 1 | Step 334600 | Avg Loss: 0.0157 | Grad Norm: 0.00899601\n",
      "Epoch 1 | Step 334700 | Avg Loss: 0.0156 | Grad Norm: 0.01026544\n",
      "Epoch 1 | Step 334800 | Avg Loss: 0.0157 | Grad Norm: 0.00882519\n",
      "Epoch 1 | Step 334900 | Avg Loss: 0.0155 | Grad Norm: 0.00915919\n",
      "Epoch 1 | Step 335000 | Avg Loss: 0.0158 | Grad Norm: 0.00943475\n",
      "Epoch 1 | Step 335100 | Avg Loss: 0.0162 | Grad Norm: 0.01080372\n",
      "Epoch 1 | Step 335200 | Avg Loss: 0.0158 | Grad Norm: 0.01065477\n",
      "Epoch 1 | Step 335300 | Avg Loss: 0.0158 | Grad Norm: 0.00827950\n",
      "Epoch 1 | Step 335400 | Avg Loss: 0.0156 | Grad Norm: 0.00946971\n",
      "Epoch 1 | Step 335500 | Avg Loss: 0.0155 | Grad Norm: 0.01010314\n",
      "Epoch 1 | Step 335600 | Avg Loss: 0.0159 | Grad Norm: 0.00930053\n",
      "Epoch 1 | Step 335700 | Avg Loss: 0.0158 | Grad Norm: 0.00796677\n",
      "Epoch 1 | Step 335800 | Avg Loss: 0.0156 | Grad Norm: 0.00924310\n",
      "Epoch 1 | Step 335900 | Avg Loss: 0.0157 | Grad Norm: 0.01058826\n",
      "Epoch 1 | Step 336000 | Avg Loss: 0.0158 | Grad Norm: 0.00818158\n",
      "Epoch 1 | Step 336100 | Avg Loss: 0.0157 | Grad Norm: 0.00990919\n",
      "Epoch 1 | Step 336200 | Avg Loss: 0.0157 | Grad Norm: 0.01003597\n",
      "Epoch 1 | Step 336300 | Avg Loss: 0.0155 | Grad Norm: 0.00856819\n",
      "Epoch 1 | Step 336400 | Avg Loss: 0.0154 | Grad Norm: 0.01077123\n",
      "Epoch 1 | Step 336500 | Avg Loss: 0.0155 | Grad Norm: 0.00946656\n",
      "Epoch 1 | Step 336600 | Avg Loss: 0.0153 | Grad Norm: 0.01079631\n",
      "Epoch 1 | Step 336700 | Avg Loss: 0.0155 | Grad Norm: 0.00851954\n",
      "Epoch 1 | Step 336800 | Avg Loss: 0.0156 | Grad Norm: 0.00968297\n",
      "Epoch 1 | Step 336900 | Avg Loss: 0.0156 | Grad Norm: 0.00925548\n",
      "Epoch 1 | Step 337000 | Avg Loss: 0.0156 | Grad Norm: 0.00822846\n",
      "Epoch 1 | Step 337100 | Avg Loss: 0.0157 | Grad Norm: 0.00795907\n",
      "Epoch 1 | Step 337200 | Avg Loss: 0.0157 | Grad Norm: 0.01030182\n",
      "Epoch 1 | Step 337300 | Avg Loss: 0.0156 | Grad Norm: 0.00795719\n",
      "Epoch 1 | Step 337400 | Avg Loss: 0.0156 | Grad Norm: 0.00793814\n",
      "Epoch 1 | Step 337500 | Avg Loss: 0.0155 | Grad Norm: 0.00893705\n",
      "Epoch 1 | Step 337600 | Avg Loss: 0.0155 | Grad Norm: 0.00791942\n",
      "Epoch 1 | Step 337700 | Avg Loss: 0.0155 | Grad Norm: 0.00792209\n",
      "Epoch 1 | Step 337800 | Avg Loss: 0.0154 | Grad Norm: 0.00799228\n",
      "Epoch 1 | Step 337900 | Avg Loss: 0.0155 | Grad Norm: 0.01003194\n",
      "Epoch 1 | Step 338000 | Avg Loss: 0.0156 | Grad Norm: 0.00877147\n",
      "Epoch 1 | Step 338100 | Avg Loss: 0.0159 | Grad Norm: 0.01053855\n",
      "Epoch 1 | Step 338200 | Avg Loss: 0.0158 | Grad Norm: 0.00907744\n",
      "Epoch 1 | Step 338300 | Avg Loss: 0.0154 | Grad Norm: 0.00783715\n",
      "Epoch 1 | Step 338400 | Avg Loss: 0.0160 | Grad Norm: 0.00814331\n",
      "Epoch 1 | Step 338500 | Avg Loss: 0.0158 | Grad Norm: 0.00867793\n",
      "Epoch 1 | Step 338600 | Avg Loss: 0.0159 | Grad Norm: 0.00792081\n",
      "Epoch 1 | Step 338700 | Avg Loss: 0.0156 | Grad Norm: 0.00969179\n",
      "Epoch 1 | Step 338800 | Avg Loss: 0.0151 | Grad Norm: 0.00883219\n",
      "Epoch 1 | Step 338900 | Avg Loss: 0.0154 | Grad Norm: 0.00896577\n",
      "Epoch 1 | Step 339000 | Avg Loss: 0.0158 | Grad Norm: 0.00873385\n",
      "Epoch 1 | Step 339100 | Avg Loss: 0.0155 | Grad Norm: 0.00885886\n",
      "Epoch 1 | Step 339200 | Avg Loss: 0.0153 | Grad Norm: 0.01079634\n",
      "Epoch 1 | Step 339300 | Avg Loss: 0.0151 | Grad Norm: 0.00917650\n",
      "Epoch 1 | Step 339400 | Avg Loss: 0.0154 | Grad Norm: 0.00995099\n",
      "Epoch 1 | Step 339500 | Avg Loss: 0.0149 | Grad Norm: 0.00921888\n",
      "Epoch 1 | Step 339600 | Avg Loss: 0.0149 | Grad Norm: 0.00988506\n",
      "Epoch 1 | Step 339700 | Avg Loss: 0.0148 | Grad Norm: 0.00762588\n",
      "Epoch 1 | Step 339800 | Avg Loss: 0.0147 | Grad Norm: 0.00786769\n",
      "Epoch 1 | Step 339900 | Avg Loss: 0.0149 | Grad Norm: 0.01002882\n",
      "Epoch 1 | Step 340000 | Avg Loss: 0.0154 | Grad Norm: 0.00819947\n",
      "Epoch 1 | Step 340100 | Avg Loss: 0.0157 | Grad Norm: 0.00813884\n",
      "Epoch 1 | Step 340200 | Avg Loss: 0.0154 | Grad Norm: 0.00787701\n",
      "Epoch 1 | Step 340300 | Avg Loss: 0.0151 | Grad Norm: 0.00961697\n",
      "Epoch 1 | Step 340400 | Avg Loss: 0.0150 | Grad Norm: 0.00904516\n",
      "Epoch 1 | Step 340500 | Avg Loss: 0.0156 | Grad Norm: 0.00804129\n",
      "Epoch 1 | Step 340600 | Avg Loss: 0.0153 | Grad Norm: 0.00775690\n",
      "Epoch 1 | Step 340700 | Avg Loss: 0.0155 | Grad Norm: 0.00862595\n",
      "Epoch 1 | Step 340800 | Avg Loss: 0.0153 | Grad Norm: 0.00834635\n",
      "Epoch 1 | Step 340900 | Avg Loss: 0.0157 | Grad Norm: 0.00861694\n",
      "Epoch 1 | Step 341000 | Avg Loss: 0.0154 | Grad Norm: 0.00758687\n",
      "Epoch 1 | Step 341100 | Avg Loss: 0.0153 | Grad Norm: 0.00741763\n",
      "Epoch 1 | Step 341200 | Avg Loss: 0.0152 | Grad Norm: 0.00826026\n",
      "Epoch 1 | Step 341300 | Avg Loss: 0.0153 | Grad Norm: 0.00837849\n",
      "Epoch 1 | Step 341400 | Avg Loss: 0.0155 | Grad Norm: 0.00759547\n",
      "Epoch 1 | Step 341500 | Avg Loss: 0.0157 | Grad Norm: 0.00954554\n",
      "Epoch 1 | Step 341600 | Avg Loss: 0.0155 | Grad Norm: 0.00976186\n",
      "Epoch 1 | Step 341700 | Avg Loss: 0.0155 | Grad Norm: 0.00792126\n",
      "Epoch 1 | Step 341800 | Avg Loss: 0.0155 | Grad Norm: 0.00716189\n",
      "Epoch 1 | Step 341900 | Avg Loss: 0.0158 | Grad Norm: 0.00912611\n",
      "Epoch 1 | Step 342000 | Avg Loss: 0.0161 | Grad Norm: 0.00879923\n",
      "Epoch 1 | Step 342100 | Avg Loss: 0.0162 | Grad Norm: 0.00731808\n",
      "Epoch 1 | Step 342200 | Avg Loss: 0.0158 | Grad Norm: 0.00797063\n",
      "Epoch 1 | Step 342300 | Avg Loss: 0.0154 | Grad Norm: 0.00888500\n",
      "Epoch 1 | Step 342400 | Avg Loss: 0.0153 | Grad Norm: 0.00890160\n",
      "Epoch 1 | Step 342500 | Avg Loss: 0.0154 | Grad Norm: 0.00948790\n",
      "Epoch 1 | Step 342600 | Avg Loss: 0.0155 | Grad Norm: 0.00803622\n",
      "Epoch 1 | Step 342700 | Avg Loss: 0.0159 | Grad Norm: 0.00931906\n",
      "Epoch 1 | Step 342800 | Avg Loss: 0.0158 | Grad Norm: 0.00799841\n",
      "Epoch 1 | Step 342900 | Avg Loss: 0.0158 | Grad Norm: 0.00921319\n",
      "Epoch 1 | Step 343000 | Avg Loss: 0.0154 | Grad Norm: 0.00797662\n",
      "Epoch 1 | Step 343100 | Avg Loss: 0.0156 | Grad Norm: 0.00833144\n",
      "Epoch 1 | Step 343200 | Avg Loss: 0.0154 | Grad Norm: 0.00709431\n",
      "Epoch 1 | Step 343300 | Avg Loss: 0.0155 | Grad Norm: 0.01075561\n",
      "Epoch 1 | Step 343400 | Avg Loss: 0.0157 | Grad Norm: 0.00934278\n",
      "Epoch 1 | Step 343500 | Avg Loss: 0.0156 | Grad Norm: 0.00889675\n",
      "Epoch 1 | Step 343600 | Avg Loss: 0.0156 | Grad Norm: 0.00843377\n",
      "Epoch 1 | Step 343700 | Avg Loss: 0.0158 | Grad Norm: 0.00863162\n",
      "Epoch 1 | Step 343800 | Avg Loss: 0.0158 | Grad Norm: 0.00827148\n",
      "Epoch 1 | Step 343900 | Avg Loss: 0.0160 | Grad Norm: 0.00880282\n",
      "Epoch 1 | Step 344000 | Avg Loss: 0.0160 | Grad Norm: 0.00950826\n",
      "Epoch 1 | Step 344100 | Avg Loss: 0.0158 | Grad Norm: 0.00765580\n",
      "Epoch 1 | Step 344200 | Avg Loss: 0.0159 | Grad Norm: 0.00972920\n",
      "Epoch 1 | Step 344300 | Avg Loss: 0.0161 | Grad Norm: 0.00835679\n",
      "Epoch 1 | Step 344400 | Avg Loss: 0.0158 | Grad Norm: 0.00823331\n",
      "Epoch 1 | Step 344500 | Avg Loss: 0.0160 | Grad Norm: 0.00820155\n",
      "Epoch 1 | Step 344600 | Avg Loss: 0.0161 | Grad Norm: 0.00864248\n",
      "Epoch 1 | Step 344700 | Avg Loss: 0.0160 | Grad Norm: 0.00968446\n",
      "Epoch 1 | Step 344800 | Avg Loss: 0.0157 | Grad Norm: 0.00891589\n",
      "Epoch 1 | Step 344900 | Avg Loss: 0.0158 | Grad Norm: 0.00956688\n",
      "Epoch 1 | Step 345000 | Avg Loss: 0.0156 | Grad Norm: 0.00838908\n",
      "Epoch 1 | Step 345100 | Avg Loss: 0.0155 | Grad Norm: 0.00794817\n",
      "Epoch 1 | Step 345200 | Avg Loss: 0.0157 | Grad Norm: 0.00938447\n",
      "Epoch 1 | Step 345300 | Avg Loss: 0.0153 | Grad Norm: 0.00816525\n",
      "Epoch 1 | Step 345400 | Avg Loss: 0.0155 | Grad Norm: 0.00928555\n",
      "Epoch 1 | Step 345500 | Avg Loss: 0.0154 | Grad Norm: 0.00984905\n",
      "Epoch 1 | Step 345600 | Avg Loss: 0.0152 | Grad Norm: 0.00777541\n",
      "Epoch 1 | Step 345700 | Avg Loss: 0.0156 | Grad Norm: 0.00975482\n",
      "Epoch 1 | Step 345800 | Avg Loss: 0.0157 | Grad Norm: 0.01042633\n",
      "Epoch 1 | Step 345900 | Avg Loss: 0.0154 | Grad Norm: 0.00867372\n",
      "Epoch 1 | Step 346000 | Avg Loss: 0.0158 | Grad Norm: 0.00846149\n",
      "Epoch 1 | Step 346100 | Avg Loss: 0.0156 | Grad Norm: 0.00928133\n",
      "Epoch 1 | Step 346200 | Avg Loss: 0.0159 | Grad Norm: 0.00844775\n",
      "Epoch 1 | Step 346300 | Avg Loss: 0.0156 | Grad Norm: 0.00740678\n",
      "Epoch 1 | Step 346400 | Avg Loss: 0.0156 | Grad Norm: 0.00882554\n",
      "Epoch 1 | Step 346500 | Avg Loss: 0.0155 | Grad Norm: 0.01002619\n",
      "Epoch 1 | Step 346600 | Avg Loss: 0.0155 | Grad Norm: 0.00902766\n",
      "Epoch 1 | Step 346700 | Avg Loss: 0.0153 | Grad Norm: 0.00997082\n",
      "Epoch 1 | Step 346800 | Avg Loss: 0.0153 | Grad Norm: 0.00845947\n",
      "Epoch 1 | Step 346900 | Avg Loss: 0.0153 | Grad Norm: 0.00822558\n",
      "Epoch 1 | Step 347000 | Avg Loss: 0.0155 | Grad Norm: 0.00816005\n",
      "Epoch 1 | Step 347100 | Avg Loss: 0.0153 | Grad Norm: 0.00973973\n",
      "Epoch 1 | Step 347200 | Avg Loss: 0.0153 | Grad Norm: 0.00943216\n",
      "Epoch 1 | Step 347300 | Avg Loss: 0.0152 | Grad Norm: 0.00882959\n",
      "Epoch 1 | Step 347400 | Avg Loss: 0.0151 | Grad Norm: 0.00904429\n",
      "Epoch 1 | Step 347500 | Avg Loss: 0.0153 | Grad Norm: 0.00801960\n",
      "Epoch 1 | Step 347600 | Avg Loss: 0.0155 | Grad Norm: 0.00978126\n",
      "Epoch 1 | Step 347700 | Avg Loss: 0.0160 | Grad Norm: 0.01065706\n",
      "Epoch 1 | Step 347800 | Avg Loss: 0.0158 | Grad Norm: 0.00999965\n",
      "Epoch 1 | Step 347900 | Avg Loss: 0.0158 | Grad Norm: 0.00935209\n",
      "Epoch 1 | Step 348000 | Avg Loss: 0.0160 | Grad Norm: 0.01017993\n",
      "Epoch 1 | Step 348100 | Avg Loss: 0.0156 | Grad Norm: 0.01130705\n",
      "Epoch 1 | Step 348200 | Avg Loss: 0.0157 | Grad Norm: 0.00848407\n",
      "Epoch 1 | Step 348300 | Avg Loss: 0.0158 | Grad Norm: 0.00805701\n",
      "Epoch 1 | Step 348400 | Avg Loss: 0.0156 | Grad Norm: 0.00913579\n",
      "Epoch 1 | Step 348500 | Avg Loss: 0.0155 | Grad Norm: 0.00922198\n",
      "Epoch 1 | Step 348600 | Avg Loss: 0.0156 | Grad Norm: 0.00804549\n",
      "Epoch 1 | Step 348700 | Avg Loss: 0.0157 | Grad Norm: 0.00906081\n",
      "Epoch 1 | Step 348800 | Avg Loss: 0.0155 | Grad Norm: 0.00888883\n",
      "Epoch 1 | Step 348900 | Avg Loss: 0.0158 | Grad Norm: 0.00816623\n",
      "Epoch 1 | Step 349000 | Avg Loss: 0.0157 | Grad Norm: 0.00951336\n",
      "Epoch 1 | Step 349100 | Avg Loss: 0.0155 | Grad Norm: 0.00936852\n",
      "Epoch 1 | Step 349200 | Avg Loss: 0.0156 | Grad Norm: 0.00980626\n",
      "Epoch 1 | Step 349300 | Avg Loss: 0.0157 | Grad Norm: 0.01056358\n",
      "Epoch 1 | Step 349400 | Avg Loss: 0.0159 | Grad Norm: 0.00857415\n",
      "Epoch 1 | Step 349500 | Avg Loss: 0.0157 | Grad Norm: 0.00795808\n",
      "Epoch 1 | Step 349600 | Avg Loss: 0.0160 | Grad Norm: 0.00940277\n",
      "Epoch 1 | Step 349700 | Avg Loss: 0.0156 | Grad Norm: 0.00783629\n",
      "Epoch 1 | Step 349800 | Avg Loss: 0.0159 | Grad Norm: 0.00823229\n",
      "Epoch 1 | Step 349900 | Avg Loss: 0.0158 | Grad Norm: 0.00769006\n",
      "Epoch 1 | Step 350000 | Avg Loss: 0.0157 | Grad Norm: 0.00987469\n",
      "Epoch 1 | Step 350100 | Avg Loss: 0.0155 | Grad Norm: 0.00960799\n",
      "Epoch 1 | Step 350200 | Avg Loss: 0.0159 | Grad Norm: 0.00906149\n",
      "Epoch 1 | Step 350300 | Avg Loss: 0.0158 | Grad Norm: 0.00964791\n",
      "Epoch 1 | Step 350400 | Avg Loss: 0.0159 | Grad Norm: 0.00823878\n",
      "Epoch 1 | Step 350500 | Avg Loss: 0.0160 | Grad Norm: 0.00870291\n",
      "Epoch 1 | Step 350600 | Avg Loss: 0.0161 | Grad Norm: 0.00847583\n",
      "Epoch 1 | Step 350700 | Avg Loss: 0.0160 | Grad Norm: 0.00880750\n",
      "Epoch 1 | Step 350800 | Avg Loss: 0.0161 | Grad Norm: 0.00966727\n",
      "Epoch 1 | Step 350900 | Avg Loss: 0.0159 | Grad Norm: 0.00846347\n",
      "Epoch 1 | Step 351000 | Avg Loss: 0.0160 | Grad Norm: 0.00866767\n",
      "Epoch 1 | Step 351100 | Avg Loss: 0.0161 | Grad Norm: 0.00867915\n",
      "Epoch 1 | Step 351200 | Avg Loss: 0.0159 | Grad Norm: 0.00894433\n",
      "Epoch 1 | Step 351300 | Avg Loss: 0.0155 | Grad Norm: 0.00845772\n",
      "Epoch 1 | Step 351400 | Avg Loss: 0.0156 | Grad Norm: 0.00789501\n",
      "Epoch 1 | Step 351500 | Avg Loss: 0.0151 | Grad Norm: 0.00878552\n",
      "Epoch 1 | Step 351600 | Avg Loss: 0.0156 | Grad Norm: 0.00836075\n",
      "Epoch 1 | Step 351700 | Avg Loss: 0.0158 | Grad Norm: 0.00820936\n",
      "Epoch 1 | Step 351800 | Avg Loss: 0.0161 | Grad Norm: 0.00791369\n",
      "Epoch 1 | Step 351900 | Avg Loss: 0.0161 | Grad Norm: 0.00870533\n",
      "Epoch 1 | Step 352000 | Avg Loss: 0.0162 | Grad Norm: 0.00793742\n",
      "Epoch 1 | Step 352100 | Avg Loss: 0.0159 | Grad Norm: 0.00864422\n",
      "Epoch 1 | Step 352200 | Avg Loss: 0.0161 | Grad Norm: 0.00863108\n",
      "Epoch 1 | Step 352300 | Avg Loss: 0.0158 | Grad Norm: 0.00895095\n",
      "Epoch 1 | Step 352400 | Avg Loss: 0.0157 | Grad Norm: 0.00843269\n",
      "Epoch 1 | Step 352500 | Avg Loss: 0.0156 | Grad Norm: 0.00880421\n",
      "Epoch 1 | Step 352600 | Avg Loss: 0.0159 | Grad Norm: 0.00930056\n",
      "Epoch 1 | Step 352700 | Avg Loss: 0.0159 | Grad Norm: 0.01102983\n",
      "Epoch 1 | Step 352800 | Avg Loss: 0.0157 | Grad Norm: 0.01029978\n",
      "Epoch 1 | Step 352900 | Avg Loss: 0.0155 | Grad Norm: 0.00909242\n",
      "Epoch 1 | Step 353000 | Avg Loss: 0.0153 | Grad Norm: 0.00804708\n",
      "Epoch 1 | Step 353100 | Avg Loss: 0.0152 | Grad Norm: 0.00791210\n",
      "Epoch 1 | Step 353200 | Avg Loss: 0.0156 | Grad Norm: 0.00807401\n",
      "Epoch 1 | Step 353300 | Avg Loss: 0.0158 | Grad Norm: 0.00728438\n",
      "Epoch 1 | Step 353400 | Avg Loss: 0.0155 | Grad Norm: 0.00823256\n",
      "Epoch 1 | Step 353500 | Avg Loss: 0.0152 | Grad Norm: 0.00954653\n",
      "Epoch 1 | Step 353600 | Avg Loss: 0.0151 | Grad Norm: 0.00770773\n",
      "Epoch 1 | Step 353700 | Avg Loss: 0.0152 | Grad Norm: 0.00988017\n",
      "Epoch 1 | Step 353800 | Avg Loss: 0.0154 | Grad Norm: 0.01063747\n",
      "Epoch 1 | Step 353900 | Avg Loss: 0.0157 | Grad Norm: 0.00955378\n",
      "Epoch 1 | Step 354000 | Avg Loss: 0.0156 | Grad Norm: 0.00860204\n",
      "Epoch 1 | Step 354100 | Avg Loss: 0.0153 | Grad Norm: 0.00949556\n",
      "Epoch 1 | Step 354200 | Avg Loss: 0.0155 | Grad Norm: 0.00904472\n",
      "Epoch 1 | Step 354300 | Avg Loss: 0.0156 | Grad Norm: 0.00876950\n",
      "Epoch 1 | Step 354400 | Avg Loss: 0.0157 | Grad Norm: 0.00840516\n",
      "Epoch 1 | Step 354500 | Avg Loss: 0.0156 | Grad Norm: 0.00954537\n",
      "Epoch 1 | Step 354600 | Avg Loss: 0.0158 | Grad Norm: 0.00882122\n",
      "Epoch 1 | Step 354700 | Avg Loss: 0.0155 | Grad Norm: 0.00755292\n",
      "Epoch 1 | Step 354800 | Avg Loss: 0.0157 | Grad Norm: 0.00833938\n",
      "Epoch 1 | Step 354900 | Avg Loss: 0.0157 | Grad Norm: 0.00861031\n",
      "Epoch 1 | Step 355000 | Avg Loss: 0.0156 | Grad Norm: 0.00897841\n",
      "Epoch 1 | Step 355100 | Avg Loss: 0.0153 | Grad Norm: 0.00977622\n",
      "Epoch 1 | Step 355200 | Avg Loss: 0.0154 | Grad Norm: 0.00875301\n",
      "Epoch 1 | Step 355300 | Avg Loss: 0.0154 | Grad Norm: 0.00800816\n",
      "Epoch 1 | Step 355400 | Avg Loss: 0.0156 | Grad Norm: 0.00891269\n",
      "Epoch 1 | Step 355500 | Avg Loss: 0.0153 | Grad Norm: 0.00819190\n",
      "Epoch 1 | Step 355600 | Avg Loss: 0.0155 | Grad Norm: 0.00844332\n",
      "Epoch 1 | Step 355700 | Avg Loss: 0.0153 | Grad Norm: 0.00866026\n",
      "Epoch 1 | Step 355800 | Avg Loss: 0.0150 | Grad Norm: 0.00860013\n",
      "Epoch 1 | Step 355900 | Avg Loss: 0.0151 | Grad Norm: 0.00757746\n",
      "Epoch 1 | Step 356000 | Avg Loss: 0.0156 | Grad Norm: 0.00940677\n",
      "Epoch 1 | Step 356100 | Avg Loss: 0.0156 | Grad Norm: 0.00831700\n",
      "Epoch 1 | Step 356200 | Avg Loss: 0.0155 | Grad Norm: 0.00952543\n",
      "Epoch 1 | Step 356300 | Avg Loss: 0.0158 | Grad Norm: 0.01174418\n",
      "Epoch 1 | Step 356400 | Avg Loss: 0.0158 | Grad Norm: 0.00888827\n",
      "Epoch 1 | Step 356500 | Avg Loss: 0.0157 | Grad Norm: 0.00896818\n",
      "Epoch 1 | Step 356600 | Avg Loss: 0.0153 | Grad Norm: 0.00726139\n",
      "Epoch 1 | Step 356700 | Avg Loss: 0.0154 | Grad Norm: 0.00747134\n",
      "Epoch 1 | Step 356800 | Avg Loss: 0.0151 | Grad Norm: 0.00734452\n",
      "Epoch 1 | Step 356900 | Avg Loss: 0.0150 | Grad Norm: 0.00701988\n",
      "Epoch 1 | Step 357000 | Avg Loss: 0.0149 | Grad Norm: 0.00759892\n",
      "Epoch 1 | Step 357100 | Avg Loss: 0.0153 | Grad Norm: 0.00875232\n",
      "Epoch 1 | Step 357200 | Avg Loss: 0.0152 | Grad Norm: 0.00874955\n",
      "Epoch 1 | Step 357300 | Avg Loss: 0.0149 | Grad Norm: 0.00759535\n",
      "Epoch 1 | Step 357400 | Avg Loss: 0.0152 | Grad Norm: 0.00803364\n",
      "Epoch 1 | Step 357500 | Avg Loss: 0.0146 | Grad Norm: 0.00807176\n",
      "Epoch 1 | Step 357600 | Avg Loss: 0.0152 | Grad Norm: 0.00837425\n",
      "Epoch 1 | Step 357700 | Avg Loss: 0.0154 | Grad Norm: 0.00913833\n",
      "Epoch 1 | Step 357800 | Avg Loss: 0.0154 | Grad Norm: 0.00930170\n",
      "Epoch 1 | Step 357900 | Avg Loss: 0.0153 | Grad Norm: 0.00809081\n",
      "Epoch 1 | Step 358000 | Avg Loss: 0.0154 | Grad Norm: 0.00813171\n",
      "Epoch 1 | Step 358100 | Avg Loss: 0.0154 | Grad Norm: 0.01010994\n",
      "Epoch 1 | Step 358200 | Avg Loss: 0.0150 | Grad Norm: 0.00929050\n",
      "Epoch 1 | Step 358300 | Avg Loss: 0.0147 | Grad Norm: 0.00778375\n",
      "Epoch 1 | Step 358400 | Avg Loss: 0.0151 | Grad Norm: 0.00860803\n",
      "Epoch 1 | Step 358500 | Avg Loss: 0.0156 | Grad Norm: 0.00824810\n",
      "Epoch 1 | Step 358600 | Avg Loss: 0.0159 | Grad Norm: 0.00810230\n",
      "Epoch 1 | Step 358700 | Avg Loss: 0.0161 | Grad Norm: 0.00794423\n",
      "Epoch 1 | Step 358800 | Avg Loss: 0.0158 | Grad Norm: 0.00870582\n",
      "Epoch 1 | Step 358900 | Avg Loss: 0.0159 | Grad Norm: 0.00857370\n",
      "Epoch 1 | Step 359000 | Avg Loss: 0.0156 | Grad Norm: 0.00814314\n",
      "Epoch 1 | Step 359100 | Avg Loss: 0.0157 | Grad Norm: 0.01000587\n",
      "Epoch 1 | Step 359200 | Avg Loss: 0.0157 | Grad Norm: 0.00760766\n",
      "Epoch 1 | Step 359300 | Avg Loss: 0.0153 | Grad Norm: 0.00835670\n",
      "Epoch 1 | Step 359400 | Avg Loss: 0.0155 | Grad Norm: 0.00817940\n",
      "Epoch 1 | Step 359500 | Avg Loss: 0.0153 | Grad Norm: 0.00921808\n",
      "Epoch 1 | Step 359600 | Avg Loss: 0.0153 | Grad Norm: 0.00897885\n",
      "Epoch 1 | Step 359700 | Avg Loss: 0.0156 | Grad Norm: 0.00770855\n",
      "Epoch 1 | Step 359800 | Avg Loss: 0.0158 | Grad Norm: 0.00920475\n",
      "Epoch 1 | Step 359900 | Avg Loss: 0.0157 | Grad Norm: 0.00844721\n",
      "Epoch 1 | Step 360000 | Avg Loss: 0.0157 | Grad Norm: 0.00864076\n",
      "Epoch 1 | Step 360100 | Avg Loss: 0.0158 | Grad Norm: 0.00883141\n",
      "Epoch 1 | Step 360200 | Avg Loss: 0.0157 | Grad Norm: 0.00771074\n",
      "Epoch 1 | Step 360300 | Avg Loss: 0.0152 | Grad Norm: 0.00785623\n",
      "Epoch 1 | Step 360400 | Avg Loss: 0.0151 | Grad Norm: 0.00887752\n",
      "Epoch 1 | Step 360500 | Avg Loss: 0.0156 | Grad Norm: 0.00952408\n",
      "Epoch 1 | Step 360600 | Avg Loss: 0.0159 | Grad Norm: 0.00856158\n",
      "Epoch 1 | Step 360700 | Avg Loss: 0.0158 | Grad Norm: 0.00769375\n",
      "Epoch 1 | Step 360800 | Avg Loss: 0.0161 | Grad Norm: 0.00867902\n",
      "Epoch 1 | Step 360900 | Avg Loss: 0.0154 | Grad Norm: 0.00751320\n",
      "Epoch 1 | Step 361000 | Avg Loss: 0.0154 | Grad Norm: 0.00919508\n",
      "Epoch 1 | Step 361100 | Avg Loss: 0.0155 | Grad Norm: 0.00885257\n",
      "Epoch 1 | Step 361200 | Avg Loss: 0.0153 | Grad Norm: 0.00833454\n",
      "Epoch 1 | Step 361300 | Avg Loss: 0.0155 | Grad Norm: 0.00892110\n",
      "Epoch 1 | Step 361400 | Avg Loss: 0.0157 | Grad Norm: 0.00877880\n",
      "Epoch 1 | Step 361500 | Avg Loss: 0.0156 | Grad Norm: 0.00859280\n",
      "Epoch 1 | Step 361600 | Avg Loss: 0.0154 | Grad Norm: 0.00876004\n",
      "Epoch 1 | Step 361700 | Avg Loss: 0.0153 | Grad Norm: 0.00837278\n",
      "Epoch 1 | Step 361800 | Avg Loss: 0.0155 | Grad Norm: 0.00934269\n",
      "Epoch 1 | Step 361900 | Avg Loss: 0.0157 | Grad Norm: 0.00794836\n",
      "Epoch 1 | Step 362000 | Avg Loss: 0.0160 | Grad Norm: 0.00801510\n",
      "Epoch 1 | Step 362100 | Avg Loss: 0.0157 | Grad Norm: 0.00864016\n",
      "Epoch 1 | Step 362200 | Avg Loss: 0.0152 | Grad Norm: 0.01082570\n",
      "Epoch 1 | Step 362300 | Avg Loss: 0.0152 | Grad Norm: 0.00876997\n",
      "Epoch 1 | Step 362400 | Avg Loss: 0.0153 | Grad Norm: 0.00851078\n",
      "Epoch 1 | Step 362500 | Avg Loss: 0.0155 | Grad Norm: 0.00930425\n",
      "Epoch 1 | Step 362600 | Avg Loss: 0.0158 | Grad Norm: 0.00857721\n",
      "Epoch 1 | Step 362700 | Avg Loss: 0.0153 | Grad Norm: 0.00985312\n",
      "Epoch 1 | Step 362800 | Avg Loss: 0.0153 | Grad Norm: 0.01101004\n",
      "Epoch 1 | Step 362900 | Avg Loss: 0.0153 | Grad Norm: 0.01007802\n",
      "Epoch 1 | Step 363000 | Avg Loss: 0.0154 | Grad Norm: 0.00742094\n",
      "Epoch 1 | Step 363100 | Avg Loss: 0.0158 | Grad Norm: 0.00769991\n",
      "Epoch 1 | Step 363200 | Avg Loss: 0.0159 | Grad Norm: 0.00985493\n",
      "Epoch 1 | Step 363300 | Avg Loss: 0.0158 | Grad Norm: 0.00810236\n",
      "Epoch 1 | Step 363400 | Avg Loss: 0.0160 | Grad Norm: 0.00877211\n",
      "Epoch 1 | Step 363500 | Avg Loss: 0.0155 | Grad Norm: 0.00835456\n",
      "Epoch 1 | Step 363600 | Avg Loss: 0.0155 | Grad Norm: 0.00929415\n",
      "Epoch 1 | Step 363700 | Avg Loss: 0.0156 | Grad Norm: 0.00904466\n",
      "Epoch 1 | Step 363800 | Avg Loss: 0.0158 | Grad Norm: 0.00926209\n",
      "Epoch 1 | Step 363900 | Avg Loss: 0.0154 | Grad Norm: 0.01087445\n",
      "Epoch 1 | Step 364000 | Avg Loss: 0.0156 | Grad Norm: 0.00936372\n",
      "Epoch 1 | Step 364100 | Avg Loss: 0.0157 | Grad Norm: 0.00789337\n",
      "Epoch 1 | Step 364200 | Avg Loss: 0.0153 | Grad Norm: 0.00874175\n",
      "Epoch 1 | Step 364300 | Avg Loss: 0.0157 | Grad Norm: 0.01025180\n",
      "Epoch 1 | Step 364400 | Avg Loss: 0.0158 | Grad Norm: 0.01091671\n",
      "Epoch 1 | Step 364500 | Avg Loss: 0.0161 | Grad Norm: 0.00986904\n",
      "Epoch 1 | Step 364600 | Avg Loss: 0.0159 | Grad Norm: 0.00779608\n",
      "Epoch 1 | Step 364700 | Avg Loss: 0.0158 | Grad Norm: 0.00978503\n",
      "Epoch 1 | Step 364800 | Avg Loss: 0.0159 | Grad Norm: 0.00933891\n",
      "Epoch 1 | Step 364900 | Avg Loss: 0.0160 | Grad Norm: 0.00849794\n",
      "Epoch 1 | Step 365000 | Avg Loss: 0.0158 | Grad Norm: 0.00821398\n",
      "Epoch 1 | Step 365100 | Avg Loss: 0.0156 | Grad Norm: 0.00736549\n",
      "Epoch 1 | Step 365200 | Avg Loss: 0.0156 | Grad Norm: 0.00874561\n",
      "Epoch 1 | Step 365300 | Avg Loss: 0.0156 | Grad Norm: 0.00801434\n",
      "Epoch 1 | Step 365400 | Avg Loss: 0.0157 | Grad Norm: 0.00869138\n",
      "Epoch 1 | Step 365500 | Avg Loss: 0.0152 | Grad Norm: 0.00870156\n",
      "Epoch 1 | Step 365600 | Avg Loss: 0.0153 | Grad Norm: 0.00870152\n",
      "Epoch 1 | Step 365700 | Avg Loss: 0.0154 | Grad Norm: 0.00814670\n",
      "Epoch 1 | Step 365800 | Avg Loss: 0.0154 | Grad Norm: 0.00858832\n",
      "Epoch 1 | Step 365900 | Avg Loss: 0.0159 | Grad Norm: 0.00936892\n",
      "Epoch 1 | Step 366000 | Avg Loss: 0.0156 | Grad Norm: 0.00825994\n",
      "Epoch 1 | Step 366100 | Avg Loss: 0.0153 | Grad Norm: 0.00915860\n",
      "Epoch 1 | Step 366200 | Avg Loss: 0.0154 | Grad Norm: 0.00899768\n",
      "Epoch 1 | Step 366300 | Avg Loss: 0.0159 | Grad Norm: 0.01025502\n",
      "Epoch 1 | Step 366400 | Avg Loss: 0.0156 | Grad Norm: 0.01013242\n",
      "Epoch 1 | Step 366500 | Avg Loss: 0.0155 | Grad Norm: 0.00876390\n",
      "Epoch 1 | Step 366600 | Avg Loss: 0.0157 | Grad Norm: 0.00854198\n",
      "Epoch 1 | Step 366700 | Avg Loss: 0.0154 | Grad Norm: 0.00938599\n",
      "Epoch 1 | Step 366800 | Avg Loss: 0.0153 | Grad Norm: 0.00899491\n",
      "Epoch 1 | Step 366900 | Avg Loss: 0.0150 | Grad Norm: 0.01027060\n",
      "Epoch 1 | Step 367000 | Avg Loss: 0.0156 | Grad Norm: 0.00891805\n",
      "Epoch 1 | Step 367100 | Avg Loss: 0.0156 | Grad Norm: 0.00834705\n",
      "Epoch 1 | Step 367200 | Avg Loss: 0.0158 | Grad Norm: 0.00856723\n",
      "Epoch 1 | Step 367300 | Avg Loss: 0.0154 | Grad Norm: 0.00912790\n",
      "Epoch 1 | Step 367400 | Avg Loss: 0.0152 | Grad Norm: 0.00898739\n",
      "Epoch 1 | Step 367500 | Avg Loss: 0.0156 | Grad Norm: 0.00854913\n",
      "Epoch 1 | Step 367600 | Avg Loss: 0.0155 | Grad Norm: 0.00958405\n",
      "Epoch 1 | Step 367700 | Avg Loss: 0.0156 | Grad Norm: 0.00711495\n",
      "Epoch 1 | Step 367800 | Avg Loss: 0.0155 | Grad Norm: 0.00870230\n",
      "Epoch 1 | Step 367900 | Avg Loss: 0.0154 | Grad Norm: 0.00921938\n",
      "Epoch 1 | Step 368000 | Avg Loss: 0.0159 | Grad Norm: 0.00881457\n",
      "Epoch 1 | Step 368100 | Avg Loss: 0.0155 | Grad Norm: 0.00873724\n",
      "Epoch 1 | Step 368200 | Avg Loss: 0.0156 | Grad Norm: 0.00949762\n",
      "Epoch 1 | Step 368300 | Avg Loss: 0.0159 | Grad Norm: 0.01047573\n",
      "Epoch 1 | Step 368400 | Avg Loss: 0.0160 | Grad Norm: 0.00911953\n",
      "Epoch 1 | Step 368500 | Avg Loss: 0.0162 | Grad Norm: 0.00862958\n",
      "Epoch 1 | Step 368600 | Avg Loss: 0.0157 | Grad Norm: 0.00936760\n",
      "Epoch 1 | Step 368700 | Avg Loss: 0.0155 | Grad Norm: 0.01080537\n",
      "Epoch 1 | Step 368800 | Avg Loss: 0.0155 | Grad Norm: 0.00769213\n",
      "Epoch 1 | Step 368900 | Avg Loss: 0.0152 | Grad Norm: 0.00920162\n",
      "Epoch 1 | Step 369000 | Avg Loss: 0.0155 | Grad Norm: 0.00819636\n",
      "Epoch 1 | Step 369100 | Avg Loss: 0.0153 | Grad Norm: 0.00832699\n",
      "Epoch 1 | Step 369200 | Avg Loss: 0.0151 | Grad Norm: 0.00811326\n",
      "Epoch 1 | Step 369300 | Avg Loss: 0.0157 | Grad Norm: 0.00844627\n",
      "Epoch 1 | Step 369400 | Avg Loss: 0.0160 | Grad Norm: 0.00736084\n",
      "Epoch 1 | Step 369500 | Avg Loss: 0.0159 | Grad Norm: 0.01005059\n",
      "Epoch 1 | Step 369600 | Avg Loss: 0.0161 | Grad Norm: 0.00881846\n",
      "Epoch 1 | Step 369700 | Avg Loss: 0.0158 | Grad Norm: 0.00866301\n",
      "Epoch 1 | Step 369800 | Avg Loss: 0.0157 | Grad Norm: 0.00896661\n",
      "Epoch 1 | Step 369900 | Avg Loss: 0.0158 | Grad Norm: 0.00979345\n",
      "Epoch 1 | Step 370000 | Avg Loss: 0.0157 | Grad Norm: 0.00957731\n",
      "Epoch 1 | Step 370100 | Avg Loss: 0.0153 | Grad Norm: 0.00881824\n",
      "Epoch 1 | Step 370200 | Avg Loss: 0.0151 | Grad Norm: 0.00753624\n",
      "Epoch 1 | Step 370300 | Avg Loss: 0.0153 | Grad Norm: 0.01076812\n",
      "Epoch 1 | Step 370400 | Avg Loss: 0.0150 | Grad Norm: 0.00988479\n",
      "Epoch 1 | Step 370500 | Avg Loss: 0.0147 | Grad Norm: 0.00884393\n",
      "Epoch 1 | Step 370600 | Avg Loss: 0.0146 | Grad Norm: 0.00764917\n",
      "Epoch 1 | Step 370700 | Avg Loss: 0.0149 | Grad Norm: 0.00903857\n",
      "Epoch 1 | Step 370800 | Avg Loss: 0.0150 | Grad Norm: 0.00829241\n",
      "Epoch 1 | Step 370900 | Avg Loss: 0.0150 | Grad Norm: 0.00898895\n",
      "Epoch 1 | Step 371000 | Avg Loss: 0.0154 | Grad Norm: 0.00817222\n",
      "Epoch 1 | Step 371100 | Avg Loss: 0.0154 | Grad Norm: 0.00823179\n",
      "Epoch 1 | Step 371200 | Avg Loss: 0.0156 | Grad Norm: 0.00825430\n",
      "Epoch 1 | Step 371300 | Avg Loss: 0.0156 | Grad Norm: 0.00904964\n",
      "Epoch 1 | Step 371400 | Avg Loss: 0.0156 | Grad Norm: 0.00886194\n",
      "Epoch 1 | Step 371500 | Avg Loss: 0.0154 | Grad Norm: 0.00803657\n",
      "Epoch 1 | Step 371600 | Avg Loss: 0.0155 | Grad Norm: 0.00883944\n",
      "Epoch 1 | Step 371700 | Avg Loss: 0.0154 | Grad Norm: 0.00866209\n",
      "Epoch 1 | Step 371800 | Avg Loss: 0.0155 | Grad Norm: 0.00879444\n",
      "Epoch 1 | Step 371900 | Avg Loss: 0.0152 | Grad Norm: 0.00720370\n",
      "Epoch 1 | Step 372000 | Avg Loss: 0.0147 | Grad Norm: 0.00981777\n",
      "Epoch 1 | Step 372100 | Avg Loss: 0.0150 | Grad Norm: 0.00986952\n",
      "Epoch 1 | Step 372200 | Avg Loss: 0.0151 | Grad Norm: 0.00857601\n",
      "Epoch 1 | Step 372300 | Avg Loss: 0.0154 | Grad Norm: 0.00841338\n",
      "Epoch 1 | Step 372400 | Avg Loss: 0.0158 | Grad Norm: 0.00988426\n",
      "Epoch 1 | Step 372500 | Avg Loss: 0.0156 | Grad Norm: 0.00796277\n",
      "Epoch 1 | Step 372600 | Avg Loss: 0.0155 | Grad Norm: 0.00820755\n",
      "Epoch 1 | Step 372700 | Avg Loss: 0.0155 | Grad Norm: 0.00894904\n",
      "Epoch 1 | Step 372800 | Avg Loss: 0.0151 | Grad Norm: 0.00814800\n",
      "Epoch 1 | Step 372900 | Avg Loss: 0.0152 | Grad Norm: 0.00971274\n",
      "Epoch 1 | Step 373000 | Avg Loss: 0.0151 | Grad Norm: 0.00799702\n",
      "Epoch 1 | Step 373100 | Avg Loss: 0.0154 | Grad Norm: 0.00849565\n",
      "Epoch 1 | Step 373200 | Avg Loss: 0.0157 | Grad Norm: 0.00845228\n",
      "Epoch 1 | Step 373300 | Avg Loss: 0.0156 | Grad Norm: 0.00837206\n",
      "Epoch 1 | Step 373400 | Avg Loss: 0.0158 | Grad Norm: 0.00835151\n",
      "Epoch 1 | Step 373500 | Avg Loss: 0.0156 | Grad Norm: 0.00994284\n",
      "Epoch 1 | Step 373600 | Avg Loss: 0.0155 | Grad Norm: 0.01036551\n",
      "Epoch 1 | Step 373700 | Avg Loss: 0.0156 | Grad Norm: 0.00911758\n",
      "Epoch 1 | Step 373800 | Avg Loss: 0.0158 | Grad Norm: 0.00874095\n",
      "Epoch 1 | Step 373900 | Avg Loss: 0.0156 | Grad Norm: 0.00898647\n",
      "Epoch 1 | Step 374000 | Avg Loss: 0.0158 | Grad Norm: 0.00890928\n",
      "Epoch 1 | Step 374100 | Avg Loss: 0.0156 | Grad Norm: 0.00936722\n",
      "Epoch 1 | Step 374200 | Avg Loss: 0.0151 | Grad Norm: 0.00856275\n",
      "Epoch 1 | Step 374300 | Avg Loss: 0.0152 | Grad Norm: 0.00899040\n",
      "Epoch 1 | Step 374400 | Avg Loss: 0.0152 | Grad Norm: 0.00871374\n",
      "Epoch 1 | Step 374500 | Avg Loss: 0.0153 | Grad Norm: 0.00907548\n",
      "Epoch 1 | Step 374600 | Avg Loss: 0.0153 | Grad Norm: 0.00896813\n",
      "Epoch 1 | Step 374700 | Avg Loss: 0.0159 | Grad Norm: 0.01001530\n",
      "Epoch 1 | Step 374800 | Avg Loss: 0.0158 | Grad Norm: 0.00823041\n",
      "Epoch 1 | Step 374900 | Avg Loss: 0.0154 | Grad Norm: 0.00904522\n",
      "Epoch 1 | Step 375000 | Avg Loss: 0.0154 | Grad Norm: 0.00792373\n",
      "Epoch 1 | Step 375100 | Avg Loss: 0.0153 | Grad Norm: 0.00886924\n",
      "Epoch 1 | Step 375200 | Avg Loss: 0.0153 | Grad Norm: 0.00778089\n",
      "Epoch 1 | Step 375300 | Avg Loss: 0.0154 | Grad Norm: 0.00876004\n",
      "Epoch 1 | Step 375400 | Avg Loss: 0.0153 | Grad Norm: 0.00862982\n",
      "Epoch 1 | Step 375500 | Avg Loss: 0.0153 | Grad Norm: 0.00884451\n",
      "Epoch 1 | Step 375600 | Avg Loss: 0.0156 | Grad Norm: 0.00948382\n",
      "Epoch 1 | Step 375700 | Avg Loss: 0.0156 | Grad Norm: 0.01147377\n",
      "Epoch 1 | Step 375800 | Avg Loss: 0.0159 | Grad Norm: 0.00909928\n",
      "Epoch 1 | Step 375900 | Avg Loss: 0.0158 | Grad Norm: 0.00806553\n",
      "Epoch 1 | Step 376000 | Avg Loss: 0.0158 | Grad Norm: 0.00875008\n",
      "Epoch 1 | Step 376100 | Avg Loss: 0.0162 | Grad Norm: 0.01098329\n",
      "Epoch 1 | Step 376200 | Avg Loss: 0.0164 | Grad Norm: 0.00816309\n",
      "Epoch 1 | Step 376300 | Avg Loss: 0.0167 | Grad Norm: 0.00776831\n",
      "Epoch 1 | Step 376400 | Avg Loss: 0.0166 | Grad Norm: 0.00918011\n",
      "Epoch 1 | Step 376500 | Avg Loss: 0.0167 | Grad Norm: 0.00818321\n",
      "Epoch 1 | Step 376600 | Avg Loss: 0.0162 | Grad Norm: 0.00918358\n",
      "Epoch 1 | Step 376700 | Avg Loss: 0.0165 | Grad Norm: 0.01044642\n",
      "Epoch 1 | Step 376800 | Avg Loss: 0.0164 | Grad Norm: 0.00852512\n",
      "Epoch 1 | Step 376900 | Avg Loss: 0.0164 | Grad Norm: 0.00916553\n",
      "Epoch 1 | Step 377000 | Avg Loss: 0.0167 | Grad Norm: 0.00827510\n",
      "Epoch 1 | Step 377100 | Avg Loss: 0.0165 | Grad Norm: 0.01071879\n",
      "Epoch 1 | Step 377200 | Avg Loss: 0.0158 | Grad Norm: 0.00887924\n",
      "Epoch 1 | Step 377300 | Avg Loss: 0.0154 | Grad Norm: 0.00775187\n",
      "Epoch 1 | Step 377400 | Avg Loss: 0.0155 | Grad Norm: 0.00810790\n",
      "Epoch 1 | Step 377500 | Avg Loss: 0.0155 | Grad Norm: 0.00948389\n",
      "Epoch 1 | Step 377600 | Avg Loss: 0.0155 | Grad Norm: 0.00851336\n",
      "Epoch 1 | Step 377700 | Avg Loss: 0.0155 | Grad Norm: 0.00908536\n",
      "Epoch 1 | Step 377800 | Avg Loss: 0.0154 | Grad Norm: 0.00914071\n",
      "Epoch 1 | Step 377900 | Avg Loss: 0.0158 | Grad Norm: 0.00807006\n",
      "Epoch 1 | Step 378000 | Avg Loss: 0.0156 | Grad Norm: 0.01054006\n",
      "Epoch 1 | Step 378100 | Avg Loss: 0.0157 | Grad Norm: 0.01018087\n",
      "Epoch 1 | Step 378200 | Avg Loss: 0.0155 | Grad Norm: 0.00917947\n",
      "Epoch 1 | Step 378300 | Avg Loss: 0.0158 | Grad Norm: 0.00938654\n",
      "Epoch 1 | Step 378400 | Avg Loss: 0.0154 | Grad Norm: 0.00719269\n",
      "Epoch 1 | Step 378500 | Avg Loss: 0.0158 | Grad Norm: 0.01077812\n",
      "Epoch 1 | Step 378600 | Avg Loss: 0.0158 | Grad Norm: 0.00844246\n",
      "Epoch 1 | Step 378700 | Avg Loss: 0.0159 | Grad Norm: 0.00871420\n",
      "Epoch 1 | Step 378800 | Avg Loss: 0.0160 | Grad Norm: 0.00767117\n",
      "Epoch 1 | Step 378900 | Avg Loss: 0.0157 | Grad Norm: 0.00961192\n",
      "Epoch 1 | Step 379000 | Avg Loss: 0.0157 | Grad Norm: 0.00959848\n",
      "Epoch 1 | Step 379100 | Avg Loss: 0.0160 | Grad Norm: 0.00978292\n",
      "Epoch 1 | Step 379200 | Avg Loss: 0.0160 | Grad Norm: 0.00936396\n",
      "Epoch 1 | Step 379300 | Avg Loss: 0.0159 | Grad Norm: 0.01008839\n",
      "Epoch 1 | Step 379400 | Avg Loss: 0.0159 | Grad Norm: 0.00818596\n",
      "Epoch 1 | Step 379500 | Avg Loss: 0.0159 | Grad Norm: 0.00860914\n",
      "Epoch 1 | Step 379600 | Avg Loss: 0.0159 | Grad Norm: 0.00812101\n",
      "Epoch 1 | Step 379700 | Avg Loss: 0.0156 | Grad Norm: 0.00732421\n",
      "Epoch 1 | Step 379800 | Avg Loss: 0.0155 | Grad Norm: 0.00854544\n",
      "Epoch 1 | Step 379900 | Avg Loss: 0.0154 | Grad Norm: 0.00996399\n",
      "Epoch 1 | Step 380000 | Avg Loss: 0.0154 | Grad Norm: 0.00918041\n",
      "Epoch 1 | Step 380100 | Avg Loss: 0.0154 | Grad Norm: 0.00831218\n",
      "Epoch 1 | Step 380200 | Avg Loss: 0.0158 | Grad Norm: 0.01189074\n",
      "Epoch 1 | Step 380300 | Avg Loss: 0.0159 | Grad Norm: 0.00924329\n",
      "Epoch 1 | Step 380400 | Avg Loss: 0.0156 | Grad Norm: 0.00871093\n",
      "Epoch 1 | Step 380500 | Avg Loss: 0.0156 | Grad Norm: 0.00823013\n",
      "Epoch 1 | Step 380600 | Avg Loss: 0.0155 | Grad Norm: 0.00871809\n",
      "Epoch 1 | Step 380700 | Avg Loss: 0.0158 | Grad Norm: 0.00926632\n",
      "Epoch 1 | Step 380800 | Avg Loss: 0.0164 | Grad Norm: 0.01035018\n",
      "Epoch 1 | Step 380900 | Avg Loss: 0.0164 | Grad Norm: 0.00937258\n",
      "Epoch 1 | Step 381000 | Avg Loss: 0.0164 | Grad Norm: 0.00937154\n",
      "Epoch 1 | Step 381100 | Avg Loss: 0.0161 | Grad Norm: 0.01017975\n",
      "Epoch 1 | Step 381200 | Avg Loss: 0.0159 | Grad Norm: 0.00951614\n",
      "Epoch 1 | Step 381300 | Avg Loss: 0.0155 | Grad Norm: 0.00830791\n",
      "Epoch 1 | Step 381400 | Avg Loss: 0.0154 | Grad Norm: 0.01027510\n",
      "Epoch 1 | Step 381500 | Avg Loss: 0.0156 | Grad Norm: 0.00787092\n",
      "Epoch 1 | Step 381600 | Avg Loss: 0.0159 | Grad Norm: 0.00877563\n",
      "Epoch 1 | Step 381700 | Avg Loss: 0.0156 | Grad Norm: 0.00968435\n",
      "Epoch 1 | Step 381800 | Avg Loss: 0.0156 | Grad Norm: 0.00871023\n",
      "Epoch 1 | Step 381900 | Avg Loss: 0.0154 | Grad Norm: 0.01333157\n",
      "Epoch 1 | Step 382000 | Avg Loss: 0.0154 | Grad Norm: 0.00914716\n",
      "Epoch 1 | Step 382100 | Avg Loss: 0.0154 | Grad Norm: 0.01113428\n",
      "Epoch 1 | Step 382200 | Avg Loss: 0.0159 | Grad Norm: 0.00791087\n",
      "Epoch 1 | Step 382300 | Avg Loss: 0.0161 | Grad Norm: 0.00924643\n",
      "Epoch 1 | Step 382400 | Avg Loss: 0.0159 | Grad Norm: 0.00990135\n",
      "Epoch 1 | Step 382500 | Avg Loss: 0.0160 | Grad Norm: 0.00848877\n",
      "Epoch 1 | Step 382600 | Avg Loss: 0.0157 | Grad Norm: 0.00971607\n",
      "Epoch 1 | Step 382700 | Avg Loss: 0.0155 | Grad Norm: 0.00837041\n",
      "Epoch 1 | Step 382800 | Avg Loss: 0.0154 | Grad Norm: 0.01068732\n",
      "Epoch 1 | Step 382900 | Avg Loss: 0.0154 | Grad Norm: 0.00814075\n",
      "Epoch 1 | Step 383000 | Avg Loss: 0.0155 | Grad Norm: 0.00884884\n",
      "Epoch 1 | Step 383100 | Avg Loss: 0.0157 | Grad Norm: 0.00868617\n",
      "Epoch 1 | Step 383200 | Avg Loss: 0.0156 | Grad Norm: 0.00858796\n",
      "Epoch 1 | Step 383300 | Avg Loss: 0.0156 | Grad Norm: 0.00914903\n",
      "Epoch 1 | Step 383400 | Avg Loss: 0.0155 | Grad Norm: 0.00766414\n",
      "Epoch 1 | Step 383500 | Avg Loss: 0.0153 | Grad Norm: 0.00900550\n",
      "Epoch 1 | Step 383600 | Avg Loss: 0.0153 | Grad Norm: 0.00867032\n",
      "Epoch 1 | Step 383700 | Avg Loss: 0.0154 | Grad Norm: 0.01013745\n",
      "Epoch 1 | Step 383800 | Avg Loss: 0.0151 | Grad Norm: 0.00936908\n",
      "Epoch 1 | Step 383900 | Avg Loss: 0.0153 | Grad Norm: 0.00826846\n",
      "Epoch 1 | Step 384000 | Avg Loss: 0.0151 | Grad Norm: 0.01017666\n",
      "Epoch 1 | Step 384100 | Avg Loss: 0.0153 | Grad Norm: 0.00746600\n",
      "Epoch 1 | Step 384200 | Avg Loss: 0.0155 | Grad Norm: 0.00927644\n",
      "Epoch 1 | Step 384300 | Avg Loss: 0.0152 | Grad Norm: 0.01033059\n",
      "Epoch 1 | Step 384400 | Avg Loss: 0.0154 | Grad Norm: 0.00991685\n",
      "Epoch 1 | Step 384500 | Avg Loss: 0.0158 | Grad Norm: 0.00934928\n",
      "Epoch 1 | Step 384600 | Avg Loss: 0.0159 | Grad Norm: 0.01106074\n",
      "Epoch 1 | Step 384700 | Avg Loss: 0.0162 | Grad Norm: 0.00952880\n",
      "Epoch 1 | Step 384800 | Avg Loss: 0.0161 | Grad Norm: 0.00906876\n",
      "Epoch 1 | Step 384900 | Avg Loss: 0.0159 | Grad Norm: 0.00934348\n",
      "Epoch 1 | Step 385000 | Avg Loss: 0.0156 | Grad Norm: 0.00793655\n",
      "Epoch 1 | Step 385100 | Avg Loss: 0.0153 | Grad Norm: 0.00871243\n",
      "Epoch 1 | Step 385200 | Avg Loss: 0.0154 | Grad Norm: 0.00931623\n",
      "Epoch 1 | Step 385300 | Avg Loss: 0.0155 | Grad Norm: 0.00970307\n",
      "Epoch 1 | Step 385400 | Avg Loss: 0.0155 | Grad Norm: 0.00877471\n",
      "Epoch 1 | Step 385500 | Avg Loss: 0.0157 | Grad Norm: 0.00786786\n",
      "Epoch 1 | Step 385600 | Avg Loss: 0.0158 | Grad Norm: 0.00816409\n",
      "Epoch 1 | Step 385700 | Avg Loss: 0.0159 | Grad Norm: 0.00835389\n",
      "Epoch 1 | Step 385800 | Avg Loss: 0.0158 | Grad Norm: 0.00860647\n",
      "Epoch 1 | Step 385900 | Avg Loss: 0.0155 | Grad Norm: 0.00824287\n",
      "Epoch 1 | Step 386000 | Avg Loss: 0.0155 | Grad Norm: 0.00856463\n",
      "Epoch 1 | Step 386100 | Avg Loss: 0.0152 | Grad Norm: 0.01057792\n",
      "Epoch 1 | Step 386200 | Avg Loss: 0.0153 | Grad Norm: 0.01053996\n",
      "Epoch 1 | Step 386300 | Avg Loss: 0.0151 | Grad Norm: 0.00854590\n",
      "Epoch 1 | Step 386400 | Avg Loss: 0.0154 | Grad Norm: 0.00759995\n",
      "Epoch 1 | Step 386500 | Avg Loss: 0.0152 | Grad Norm: 0.00810919\n",
      "Epoch 1 | Step 386600 | Avg Loss: 0.0150 | Grad Norm: 0.00823465\n",
      "Epoch 1 | Step 386700 | Avg Loss: 0.0152 | Grad Norm: 0.00761474\n",
      "Epoch 1 | Step 386800 | Avg Loss: 0.0149 | Grad Norm: 0.00838415\n",
      "Epoch 1 | Step 386900 | Avg Loss: 0.0152 | Grad Norm: 0.00919035\n",
      "Epoch 1 | Step 387000 | Avg Loss: 0.0150 | Grad Norm: 0.00828372\n",
      "Epoch 1 | Step 387100 | Avg Loss: 0.0152 | Grad Norm: 0.00942665\n",
      "Epoch 1 | Step 387200 | Avg Loss: 0.0149 | Grad Norm: 0.00834345\n",
      "Epoch 1 | Step 387300 | Avg Loss: 0.0151 | Grad Norm: 0.00788413\n",
      "Epoch 1 | Step 387400 | Avg Loss: 0.0152 | Grad Norm: 0.00806355\n",
      "Epoch 1 | Step 387500 | Avg Loss: 0.0154 | Grad Norm: 0.00914837\n",
      "Epoch 1 | Step 387600 | Avg Loss: 0.0160 | Grad Norm: 0.00915003\n",
      "Epoch 1 | Step 387700 | Avg Loss: 0.0159 | Grad Norm: 0.01018851\n",
      "Epoch 1 | Step 387800 | Avg Loss: 0.0160 | Grad Norm: 0.00822727\n",
      "Epoch 1 | Step 387900 | Avg Loss: 0.0159 | Grad Norm: 0.00902096\n",
      "Epoch 1 | Step 388000 | Avg Loss: 0.0157 | Grad Norm: 0.00992935\n",
      "Epoch 1 | Step 388100 | Avg Loss: 0.0159 | Grad Norm: 0.00919262\n",
      "Epoch 1 | Step 388200 | Avg Loss: 0.0159 | Grad Norm: 0.00960536\n",
      "Epoch 1 | Step 388300 | Avg Loss: 0.0158 | Grad Norm: 0.00829569\n",
      "Epoch 1 | Step 388400 | Avg Loss: 0.0161 | Grad Norm: 0.00876383\n",
      "Epoch 1 | Step 388500 | Avg Loss: 0.0158 | Grad Norm: 0.00876762\n",
      "Epoch 1 | Step 388600 | Avg Loss: 0.0160 | Grad Norm: 0.00932192\n",
      "Epoch 1 | Step 388700 | Avg Loss: 0.0163 | Grad Norm: 0.00794417\n",
      "Epoch 1 | Step 388800 | Avg Loss: 0.0163 | Grad Norm: 0.01026068\n",
      "Epoch 1 | Step 388900 | Avg Loss: 0.0160 | Grad Norm: 0.01051202\n",
      "Epoch 1 | Step 389000 | Avg Loss: 0.0159 | Grad Norm: 0.00855398\n",
      "Epoch 1 | Step 389100 | Avg Loss: 0.0159 | Grad Norm: 0.00876815\n",
      "Epoch 1 | Step 389200 | Avg Loss: 0.0159 | Grad Norm: 0.00829499\n",
      "Epoch 1 | Step 389300 | Avg Loss: 0.0161 | Grad Norm: 0.00814081\n",
      "Epoch 1 | Step 389400 | Avg Loss: 0.0161 | Grad Norm: 0.00804092\n",
      "Epoch 1 | Step 389500 | Avg Loss: 0.0160 | Grad Norm: 0.00871448\n",
      "Epoch 1 | Step 389600 | Avg Loss: 0.0158 | Grad Norm: 0.00734170\n",
      "Epoch 1 | Step 389700 | Avg Loss: 0.0158 | Grad Norm: 0.00878932\n",
      "Epoch 1 | Step 389800 | Avg Loss: 0.0159 | Grad Norm: 0.00877063\n",
      "Epoch 1 | Step 389900 | Avg Loss: 0.0156 | Grad Norm: 0.00872214\n",
      "Epoch 1 | Step 390000 | Avg Loss: 0.0157 | Grad Norm: 0.00787207\n",
      "Epoch 1 | Step 390100 | Avg Loss: 0.0163 | Grad Norm: 0.00887088\n",
      "Epoch 1 | Step 390200 | Avg Loss: 0.0161 | Grad Norm: 0.01044856\n",
      "Epoch 1 | Step 390300 | Avg Loss: 0.0165 | Grad Norm: 0.00818806\n",
      "Epoch 1 | Step 390400 | Avg Loss: 0.0167 | Grad Norm: 0.00879508\n",
      "Epoch 1 | Step 390500 | Avg Loss: 0.0164 | Grad Norm: 0.00966796\n",
      "Epoch 1 | Step 390600 | Avg Loss: 0.0165 | Grad Norm: 0.00785523\n",
      "Epoch 1 | Step 390700 | Avg Loss: 0.0162 | Grad Norm: 0.00906362\n",
      "Epoch 1 | Step 390800 | Avg Loss: 0.0161 | Grad Norm: 0.00815169\n",
      "Epoch 1 | Step 390900 | Avg Loss: 0.0161 | Grad Norm: 0.00874168\n",
      "Epoch 1 | Step 391000 | Avg Loss: 0.0160 | Grad Norm: 0.00878443\n",
      "Epoch 1 | Step 391100 | Avg Loss: 0.0161 | Grad Norm: 0.00912258\n",
      "Epoch 1 | Step 391200 | Avg Loss: 0.0161 | Grad Norm: 0.00843273\n",
      "Epoch 1 | Step 391300 | Avg Loss: 0.0162 | Grad Norm: 0.00829886\n",
      "Epoch 1 | Step 391400 | Avg Loss: 0.0161 | Grad Norm: 0.00793668\n",
      "Epoch 1 | Step 391500 | Avg Loss: 0.0162 | Grad Norm: 0.01029383\n",
      "Epoch 1 | Step 391600 | Avg Loss: 0.0159 | Grad Norm: 0.00887479\n",
      "Epoch 1 | Step 391700 | Avg Loss: 0.0159 | Grad Norm: 0.00947595\n",
      "Epoch 1 | Step 391800 | Avg Loss: 0.0160 | Grad Norm: 0.00874243\n",
      "Epoch 1 | Step 391900 | Avg Loss: 0.0159 | Grad Norm: 0.00846652\n",
      "Epoch 1 | Step 392000 | Avg Loss: 0.0159 | Grad Norm: 0.00877972\n",
      "Epoch 1 | Step 392100 | Avg Loss: 0.0158 | Grad Norm: 0.00945087\n",
      "Epoch 1 | Step 392200 | Avg Loss: 0.0159 | Grad Norm: 0.00773340\n",
      "Epoch 1 | Step 392300 | Avg Loss: 0.0156 | Grad Norm: 0.00945220\n",
      "Epoch 1 | Step 392400 | Avg Loss: 0.0157 | Grad Norm: 0.00788778\n",
      "Epoch 1 | Step 392500 | Avg Loss: 0.0157 | Grad Norm: 0.00900790\n",
      "Epoch 1 | Step 392600 | Avg Loss: 0.0155 | Grad Norm: 0.01113737\n",
      "Epoch 1 | Step 392700 | Avg Loss: 0.0159 | Grad Norm: 0.01019764\n",
      "Epoch 1 | Step 392800 | Avg Loss: 0.0158 | Grad Norm: 0.00787118\n",
      "Epoch 1 | Step 392900 | Avg Loss: 0.0156 | Grad Norm: 0.00993184\n",
      "Epoch 1 | Step 393000 | Avg Loss: 0.0161 | Grad Norm: 0.00976619\n",
      "Epoch 1 | Step 393100 | Avg Loss: 0.0160 | Grad Norm: 0.00922924\n",
      "Epoch 1 | Step 393200 | Avg Loss: 0.0156 | Grad Norm: 0.00948768\n",
      "Epoch 1 | Step 393300 | Avg Loss: 0.0156 | Grad Norm: 0.00851206\n",
      "Epoch 1 | Step 393400 | Avg Loss: 0.0159 | Grad Norm: 0.00786610\n",
      "Epoch 1 | Step 393500 | Avg Loss: 0.0161 | Grad Norm: 0.01018006\n",
      "Epoch 1 | Step 393600 | Avg Loss: 0.0158 | Grad Norm: 0.00982974\n",
      "Epoch 1 | Step 393700 | Avg Loss: 0.0161 | Grad Norm: 0.00828004\n",
      "Epoch 1 | Step 393800 | Avg Loss: 0.0161 | Grad Norm: 0.01067810\n",
      "Epoch 1 | Step 393900 | Avg Loss: 0.0167 | Grad Norm: 0.00934253\n",
      "Epoch 1 | Step 394000 | Avg Loss: 0.0164 | Grad Norm: 0.00966064\n",
      "Epoch 1 | Step 394100 | Avg Loss: 0.0163 | Grad Norm: 0.00934131\n",
      "Epoch 1 | Step 394200 | Avg Loss: 0.0163 | Grad Norm: 0.00968637\n",
      "Epoch 1 | Step 394300 | Avg Loss: 0.0164 | Grad Norm: 0.00840602\n",
      "Epoch 1 | Step 394400 | Avg Loss: 0.0161 | Grad Norm: 0.00838288\n",
      "Epoch 1 | Step 394500 | Avg Loss: 0.0163 | Grad Norm: 0.00806598\n",
      "Epoch 1 | Step 394600 | Avg Loss: 0.0160 | Grad Norm: 0.00872448\n",
      "Epoch 1 | Step 394700 | Avg Loss: 0.0161 | Grad Norm: 0.00943620\n",
      "Epoch 1 | Step 394800 | Avg Loss: 0.0157 | Grad Norm: 0.00926303\n",
      "Epoch 1 | Step 394900 | Avg Loss: 0.0156 | Grad Norm: 0.00957675\n",
      "Epoch 1 | Step 395000 | Avg Loss: 0.0156 | Grad Norm: 0.01024854\n",
      "Epoch 1 | Step 395100 | Avg Loss: 0.0155 | Grad Norm: 0.00860768\n",
      "Epoch 1 | Step 395200 | Avg Loss: 0.0154 | Grad Norm: 0.00893560\n",
      "Epoch 1 | Step 395300 | Avg Loss: 0.0156 | Grad Norm: 0.00826922\n",
      "Epoch 1 | Step 395400 | Avg Loss: 0.0157 | Grad Norm: 0.00786879\n",
      "Epoch 1 | Step 395500 | Avg Loss: 0.0157 | Grad Norm: 0.00992722\n",
      "Epoch 1 | Step 395600 | Avg Loss: 0.0157 | Grad Norm: 0.00886732\n",
      "Epoch 1 | Step 395700 | Avg Loss: 0.0156 | Grad Norm: 0.01102859\n",
      "Epoch 1 | Step 395800 | Avg Loss: 0.0155 | Grad Norm: 0.01121223\n",
      "Epoch 1 | Step 395900 | Avg Loss: 0.0156 | Grad Norm: 0.00910216\n",
      "Epoch 1 | Step 396000 | Avg Loss: 0.0156 | Grad Norm: 0.01051376\n",
      "Epoch 1 | Step 396100 | Avg Loss: 0.0153 | Grad Norm: 0.01060244\n",
      "Epoch 1 | Step 396200 | Avg Loss: 0.0156 | Grad Norm: 0.01068991\n",
      "Epoch 1 | Step 396300 | Avg Loss: 0.0156 | Grad Norm: 0.00852239\n",
      "Epoch 1 | Step 396400 | Avg Loss: 0.0154 | Grad Norm: 0.01034635\n",
      "Epoch 1 | Step 396500 | Avg Loss: 0.0153 | Grad Norm: 0.00924522\n",
      "Epoch 1 | Step 396600 | Avg Loss: 0.0155 | Grad Norm: 0.00816125\n",
      "Epoch 1 | Step 396700 | Avg Loss: 0.0154 | Grad Norm: 0.00795395\n",
      "Epoch 1 | Step 396800 | Avg Loss: 0.0154 | Grad Norm: 0.00903671\n",
      "Epoch 1 | Step 396900 | Avg Loss: 0.0158 | Grad Norm: 0.00959865\n",
      "Epoch 1 | Step 397000 | Avg Loss: 0.0160 | Grad Norm: 0.00840974\n",
      "Epoch 1 | Step 397100 | Avg Loss: 0.0156 | Grad Norm: 0.01167082\n",
      "Epoch 1 | Step 397200 | Avg Loss: 0.0159 | Grad Norm: 0.00822537\n",
      "Epoch 1 | Step 397300 | Avg Loss: 0.0158 | Grad Norm: 0.00903293\n",
      "Epoch 1 | Step 397400 | Avg Loss: 0.0160 | Grad Norm: 0.00891543\n",
      "Epoch 1 | Step 397500 | Avg Loss: 0.0159 | Grad Norm: 0.00864170\n",
      "Epoch 1 | Step 397600 | Avg Loss: 0.0157 | Grad Norm: 0.00856295\n",
      "Epoch 1 | Step 397700 | Avg Loss: 0.0154 | Grad Norm: 0.00734407\n",
      "Epoch 1 | Step 397800 | Avg Loss: 0.0153 | Grad Norm: 0.00918049\n",
      "Epoch 1 | Step 397900 | Avg Loss: 0.0155 | Grad Norm: 0.00784040\n",
      "Epoch 1 | Step 398000 | Avg Loss: 0.0155 | Grad Norm: 0.00835980\n",
      "Epoch 1 | Step 398100 | Avg Loss: 0.0157 | Grad Norm: 0.00764055\n",
      "Epoch 1 | Step 398200 | Avg Loss: 0.0153 | Grad Norm: 0.00795021\n",
      "Epoch 1 | Step 398300 | Avg Loss: 0.0155 | Grad Norm: 0.00842078\n",
      "Epoch 1 | Step 398400 | Avg Loss: 0.0156 | Grad Norm: 0.00890005\n",
      "Epoch 1 | Step 398500 | Avg Loss: 0.0159 | Grad Norm: 0.00851689\n",
      "Epoch 1 | Step 398600 | Avg Loss: 0.0159 | Grad Norm: 0.00995926\n",
      "Epoch 1 | Step 398700 | Avg Loss: 0.0160 | Grad Norm: 0.00806205\n",
      "Epoch 1 | Step 398800 | Avg Loss: 0.0156 | Grad Norm: 0.00868794\n",
      "Epoch 1 | Step 398900 | Avg Loss: 0.0161 | Grad Norm: 0.00828653\n",
      "Epoch 1 | Step 399000 | Avg Loss: 0.0160 | Grad Norm: 0.01008607\n",
      "Epoch 1 | Step 399100 | Avg Loss: 0.0155 | Grad Norm: 0.00890319\n",
      "Epoch 1 | Step 399200 | Avg Loss: 0.0157 | Grad Norm: 0.00829539\n",
      "Epoch 1 | Step 399300 | Avg Loss: 0.0157 | Grad Norm: 0.00926833\n",
      "Epoch 1 | Step 399400 | Avg Loss: 0.0160 | Grad Norm: 0.00817682\n",
      "Epoch 1 | Step 399500 | Avg Loss: 0.0165 | Grad Norm: 0.00839348\n",
      "Epoch 1 | Step 399600 | Avg Loss: 0.0167 | Grad Norm: 0.00917357\n",
      "Epoch 1 | Step 399700 | Avg Loss: 0.0167 | Grad Norm: 0.01150464\n",
      "Epoch 1 | Step 399800 | Avg Loss: 0.0166 | Grad Norm: 0.00985561\n",
      "Epoch 1 | Step 399900 | Avg Loss: 0.0162 | Grad Norm: 0.00986940\n",
      "Epoch 1 | Step 400000 | Avg Loss: 0.0162 | Grad Norm: 0.00869923\n",
      "Saving model at step400000\n",
      "Epoch 1 | Step 400100 | Avg Loss: 0.0162 | Grad Norm: 0.00843185\n",
      "Epoch 1 | Step 400200 | Avg Loss: 0.0159 | Grad Norm: 0.00963626\n",
      "Epoch 1 | Step 400300 | Avg Loss: 0.0160 | Grad Norm: 0.00862133\n",
      "Epoch 1 | Step 400400 | Avg Loss: 0.0161 | Grad Norm: 0.00825968\n",
      "Epoch 1 | Step 400500 | Avg Loss: 0.0156 | Grad Norm: 0.00981289\n",
      "Epoch 1 | Step 400600 | Avg Loss: 0.0152 | Grad Norm: 0.00862820\n",
      "Epoch 1 | Step 400700 | Avg Loss: 0.0155 | Grad Norm: 0.00760303\n",
      "Epoch 1 | Step 400800 | Avg Loss: 0.0152 | Grad Norm: 0.00810426\n",
      "Epoch 1 | Step 400900 | Avg Loss: 0.0151 | Grad Norm: 0.00714700\n",
      "Epoch 1 | Step 401000 | Avg Loss: 0.0153 | Grad Norm: 0.00895028\n",
      "Epoch 1 | Step 401100 | Avg Loss: 0.0151 | Grad Norm: 0.00815230\n",
      "Epoch 1 | Step 401200 | Avg Loss: 0.0151 | Grad Norm: 0.00986369\n",
      "Epoch 1 | Step 401300 | Avg Loss: 0.0151 | Grad Norm: 0.00734344\n",
      "Epoch 1 | Step 401400 | Avg Loss: 0.0154 | Grad Norm: 0.00904908\n",
      "Epoch 1 | Step 401500 | Avg Loss: 0.0154 | Grad Norm: 0.00783215\n",
      "Epoch 1 | Step 401600 | Avg Loss: 0.0156 | Grad Norm: 0.00709150\n",
      "Epoch 1 | Step 401700 | Avg Loss: 0.0154 | Grad Norm: 0.00819151\n",
      "Epoch 1 | Step 401800 | Avg Loss: 0.0151 | Grad Norm: 0.00816502\n",
      "Epoch 1 | Step 401900 | Avg Loss: 0.0152 | Grad Norm: 0.00927219\n",
      "Epoch 1 | Step 402000 | Avg Loss: 0.0153 | Grad Norm: 0.00936047\n",
      "Epoch 1 | Step 402100 | Avg Loss: 0.0155 | Grad Norm: 0.00983883\n",
      "Epoch 1 | Step 402200 | Avg Loss: 0.0158 | Grad Norm: 0.00825616\n",
      "Epoch 1 | Step 402300 | Avg Loss: 0.0155 | Grad Norm: 0.00901432\n",
      "Epoch 1 | Step 402400 | Avg Loss: 0.0153 | Grad Norm: 0.00836842\n",
      "Epoch 1 | Step 402500 | Avg Loss: 0.0155 | Grad Norm: 0.00927128\n",
      "Epoch 1 | Step 402600 | Avg Loss: 0.0153 | Grad Norm: 0.00852243\n",
      "Epoch 1 | Step 402700 | Avg Loss: 0.0153 | Grad Norm: 0.00731205\n",
      "Epoch 1 | Step 402800 | Avg Loss: 0.0153 | Grad Norm: 0.00948129\n",
      "Epoch 1 | Step 402900 | Avg Loss: 0.0153 | Grad Norm: 0.00985179\n",
      "Epoch 1 | Step 403000 | Avg Loss: 0.0155 | Grad Norm: 0.00878415\n",
      "Epoch 1 | Step 403100 | Avg Loss: 0.0154 | Grad Norm: 0.00797249\n",
      "Epoch 1 | Step 403200 | Avg Loss: 0.0152 | Grad Norm: 0.00822579\n",
      "Epoch 1 | Step 403300 | Avg Loss: 0.0152 | Grad Norm: 0.00782695\n",
      "Epoch 1 | Step 403400 | Avg Loss: 0.0154 | Grad Norm: 0.00852871\n",
      "Epoch 1 | Step 403500 | Avg Loss: 0.0156 | Grad Norm: 0.00923637\n",
      "Epoch 1 | Step 403600 | Avg Loss: 0.0155 | Grad Norm: 0.00867337\n",
      "Epoch 1 | Step 403700 | Avg Loss: 0.0156 | Grad Norm: 0.00961970\n",
      "Epoch 1 | Step 403800 | Avg Loss: 0.0159 | Grad Norm: 0.00881160\n",
      "Epoch 1 | Step 403900 | Avg Loss: 0.0158 | Grad Norm: 0.00884795\n",
      "Epoch 1 | Step 404000 | Avg Loss: 0.0158 | Grad Norm: 0.00863732\n",
      "Epoch 1 | Step 404100 | Avg Loss: 0.0158 | Grad Norm: 0.01100655\n",
      "Epoch 1 | Step 404200 | Avg Loss: 0.0157 | Grad Norm: 0.00868697\n",
      "Epoch 1 | Step 404300 | Avg Loss: 0.0156 | Grad Norm: 0.00919849\n",
      "Epoch 1 | Step 404400 | Avg Loss: 0.0160 | Grad Norm: 0.00799496\n",
      "Epoch 1 | Step 404500 | Avg Loss: 0.0164 | Grad Norm: 0.00922329\n",
      "Epoch 1 | Step 404600 | Avg Loss: 0.0165 | Grad Norm: 0.00875029\n",
      "Epoch 1 | Step 404700 | Avg Loss: 0.0164 | Grad Norm: 0.00896895\n",
      "Epoch 1 | Step 404800 | Avg Loss: 0.0166 | Grad Norm: 0.00971764\n",
      "Epoch 1 | Step 404900 | Avg Loss: 0.0160 | Grad Norm: 0.00932382\n",
      "Epoch 1 | Step 405000 | Avg Loss: 0.0158 | Grad Norm: 0.01011588\n",
      "Epoch 1 | Step 405100 | Avg Loss: 0.0159 | Grad Norm: 0.00885107\n",
      "Epoch 1 | Step 405200 | Avg Loss: 0.0156 | Grad Norm: 0.00753355\n",
      "Epoch 1 | Step 405300 | Avg Loss: 0.0162 | Grad Norm: 0.01256185\n",
      "Epoch 1 | Step 405400 | Avg Loss: 0.0160 | Grad Norm: 0.01057014\n",
      "Epoch 1 | Step 405500 | Avg Loss: 0.0158 | Grad Norm: 0.01036619\n",
      "Epoch 1 | Step 405600 | Avg Loss: 0.0159 | Grad Norm: 0.00926071\n",
      "Epoch 1 | Step 405700 | Avg Loss: 0.0160 | Grad Norm: 0.00964650\n",
      "Epoch 1 | Step 405800 | Avg Loss: 0.0159 | Grad Norm: 0.00834440\n",
      "Epoch 1 | Step 405900 | Avg Loss: 0.0161 | Grad Norm: 0.01072920\n",
      "Epoch 1 | Step 406000 | Avg Loss: 0.0155 | Grad Norm: 0.00973487\n",
      "Epoch 1 | Step 406100 | Avg Loss: 0.0158 | Grad Norm: 0.00823866\n",
      "Epoch 1 | Step 406200 | Avg Loss: 0.0156 | Grad Norm: 0.00822599\n",
      "Epoch 1 | Step 406300 | Avg Loss: 0.0154 | Grad Norm: 0.00721048\n",
      "Epoch 1 | Step 406400 | Avg Loss: 0.0157 | Grad Norm: 0.00870977\n",
      "Epoch 1 | Step 406500 | Avg Loss: 0.0157 | Grad Norm: 0.00857569\n",
      "Epoch 1 | Step 406600 | Avg Loss: 0.0158 | Grad Norm: 0.00958350\n",
      "Epoch 1 | Step 406700 | Avg Loss: 0.0159 | Grad Norm: 0.00892222\n",
      "Epoch 1 | Step 406800 | Avg Loss: 0.0164 | Grad Norm: 0.00817550\n",
      "Epoch 1 | Step 406900 | Avg Loss: 0.0159 | Grad Norm: 0.00880525\n",
      "Epoch 1 | Step 407000 | Avg Loss: 0.0161 | Grad Norm: 0.00850605\n",
      "Epoch 1 | Step 407100 | Avg Loss: 0.0160 | Grad Norm: 0.00836522\n",
      "Epoch 1 | Step 407200 | Avg Loss: 0.0159 | Grad Norm: 0.00770389\n",
      "Epoch 1 | Step 407300 | Avg Loss: 0.0159 | Grad Norm: 0.00772265\n",
      "Epoch 1 | Step 407400 | Avg Loss: 0.0157 | Grad Norm: 0.00853798\n",
      "Epoch 1 | Step 407500 | Avg Loss: 0.0157 | Grad Norm: 0.00812643\n",
      "Epoch 1 | Step 407600 | Avg Loss: 0.0159 | Grad Norm: 0.00793501\n",
      "Epoch 1 | Step 407700 | Avg Loss: 0.0161 | Grad Norm: 0.00855701\n",
      "Epoch 1 | Step 407800 | Avg Loss: 0.0156 | Grad Norm: 0.01055174\n",
      "Epoch 1 | Step 407900 | Avg Loss: 0.0156 | Grad Norm: 0.00865595\n",
      "Epoch 1 | Step 408000 | Avg Loss: 0.0156 | Grad Norm: 0.00897592\n",
      "Epoch 1 | Step 408100 | Avg Loss: 0.0158 | Grad Norm: 0.00774108\n",
      "Epoch 1 | Step 408200 | Avg Loss: 0.0158 | Grad Norm: 0.00928809\n",
      "Epoch 1 | Step 408300 | Avg Loss: 0.0156 | Grad Norm: 0.00848443\n",
      "Epoch 1 | Step 408400 | Avg Loss: 0.0156 | Grad Norm: 0.01041516\n",
      "Epoch 1 | Step 408500 | Avg Loss: 0.0154 | Grad Norm: 0.00899039\n",
      "Epoch 1 | Step 408600 | Avg Loss: 0.0153 | Grad Norm: 0.01214031\n",
      "Epoch 1 | Step 408700 | Avg Loss: 0.0157 | Grad Norm: 0.00916510\n",
      "Epoch 1 | Step 408800 | Avg Loss: 0.0160 | Grad Norm: 0.00999475\n",
      "Epoch 1 | Step 408900 | Avg Loss: 0.0160 | Grad Norm: 0.00867343\n",
      "Epoch 1 | Step 409000 | Avg Loss: 0.0157 | Grad Norm: 0.00811236\n",
      "Epoch 1 | Step 409100 | Avg Loss: 0.0157 | Grad Norm: 0.00949156\n",
      "Epoch 1 | Step 409200 | Avg Loss: 0.0160 | Grad Norm: 0.00957886\n",
      "Epoch 1 | Step 409300 | Avg Loss: 0.0162 | Grad Norm: 0.00893803\n",
      "Epoch 1 | Step 409400 | Avg Loss: 0.0162 | Grad Norm: 0.00837466\n",
      "Epoch 1 | Step 409500 | Avg Loss: 0.0162 | Grad Norm: 0.00866332\n",
      "Epoch 1 | Step 409600 | Avg Loss: 0.0163 | Grad Norm: 0.00869747\n",
      "Epoch 1 | Step 409700 | Avg Loss: 0.0161 | Grad Norm: 0.00845360\n",
      "Epoch 1 | Step 409800 | Avg Loss: 0.0154 | Grad Norm: 0.00801054\n",
      "Epoch 1 | Step 409900 | Avg Loss: 0.0155 | Grad Norm: 0.00834177\n",
      "Epoch 1 | Step 410000 | Avg Loss: 0.0156 | Grad Norm: 0.00755117\n",
      "Epoch 1 | Step 410100 | Avg Loss: 0.0154 | Grad Norm: 0.00824602\n",
      "Epoch 1 | Step 410200 | Avg Loss: 0.0153 | Grad Norm: 0.00890486\n",
      "Epoch 1 | Step 410300 | Avg Loss: 0.0153 | Grad Norm: 0.00894618\n",
      "Epoch 1 | Step 410400 | Avg Loss: 0.0152 | Grad Norm: 0.00825615\n",
      "Epoch 1 | Step 410500 | Avg Loss: 0.0155 | Grad Norm: 0.00881709\n",
      "Epoch 1 | Step 410600 | Avg Loss: 0.0154 | Grad Norm: 0.00796744\n",
      "Epoch 1 | Step 410700 | Avg Loss: 0.0152 | Grad Norm: 0.00906651\n",
      "Epoch 1 | Step 410800 | Avg Loss: 0.0154 | Grad Norm: 0.00741057\n",
      "Epoch 1 | Step 410900 | Avg Loss: 0.0155 | Grad Norm: 0.01188689\n",
      "Epoch 1 | Step 411000 | Avg Loss: 0.0156 | Grad Norm: 0.00882754\n",
      "Epoch 1 | Step 411100 | Avg Loss: 0.0156 | Grad Norm: 0.01069433\n",
      "Epoch 1 | Step 411200 | Avg Loss: 0.0159 | Grad Norm: 0.00775268\n",
      "Epoch 1 | Step 411300 | Avg Loss: 0.0161 | Grad Norm: 0.00910878\n",
      "Epoch 1 | Step 411400 | Avg Loss: 0.0163 | Grad Norm: 0.00829105\n",
      "Epoch 1 | Step 411500 | Avg Loss: 0.0162 | Grad Norm: 0.00910094\n",
      "Epoch 1 | Step 411600 | Avg Loss: 0.0161 | Grad Norm: 0.00769893\n",
      "Epoch 1 | Step 411700 | Avg Loss: 0.0159 | Grad Norm: 0.00861247\n",
      "Epoch 1 | Step 411800 | Avg Loss: 0.0157 | Grad Norm: 0.00978526\n",
      "Epoch 1 | Step 411900 | Avg Loss: 0.0158 | Grad Norm: 0.00743651\n",
      "Epoch 1 | Step 412000 | Avg Loss: 0.0157 | Grad Norm: 0.00855530\n",
      "Epoch 1 | Step 412100 | Avg Loss: 0.0156 | Grad Norm: 0.00918244\n",
      "Epoch 1 | Step 412200 | Avg Loss: 0.0156 | Grad Norm: 0.00856503\n",
      "Epoch 1 | Step 412300 | Avg Loss: 0.0157 | Grad Norm: 0.01300201\n",
      "Epoch 1 | Step 412400 | Avg Loss: 0.0158 | Grad Norm: 0.00820226\n",
      "Epoch 1 | Step 412500 | Avg Loss: 0.0158 | Grad Norm: 0.00780306\n",
      "Epoch 1 | Step 412600 | Avg Loss: 0.0156 | Grad Norm: 0.00996639\n",
      "Epoch 1 | Step 412700 | Avg Loss: 0.0155 | Grad Norm: 0.00952697\n",
      "Epoch 1 | Step 412800 | Avg Loss: 0.0155 | Grad Norm: 0.00838641\n",
      "Epoch 1 | Step 412900 | Avg Loss: 0.0159 | Grad Norm: 0.00858696\n",
      "Epoch 1 | Step 413000 | Avg Loss: 0.0156 | Grad Norm: 0.01017261\n",
      "Epoch 1 | Step 413100 | Avg Loss: 0.0157 | Grad Norm: 0.01047397\n",
      "Epoch 1 | Step 413200 | Avg Loss: 0.0156 | Grad Norm: 0.00928336\n",
      "Epoch 1 | Step 413300 | Avg Loss: 0.0155 | Grad Norm: 0.00863862\n",
      "Epoch 1 | Step 413400 | Avg Loss: 0.0155 | Grad Norm: 0.00882926\n",
      "Epoch 1 | Step 413500 | Avg Loss: 0.0159 | Grad Norm: 0.00849955\n",
      "Epoch 1 | Step 413600 | Avg Loss: 0.0155 | Grad Norm: 0.01024841\n",
      "Epoch 1 | Step 413700 | Avg Loss: 0.0158 | Grad Norm: 0.00915733\n",
      "Epoch 1 | Step 413800 | Avg Loss: 0.0156 | Grad Norm: 0.00947685\n",
      "Epoch 1 | Step 413900 | Avg Loss: 0.0156 | Grad Norm: 0.00826652\n",
      "Epoch 1 | Step 414000 | Avg Loss: 0.0156 | Grad Norm: 0.00884502\n",
      "Epoch 1 | Step 414100 | Avg Loss: 0.0159 | Grad Norm: 0.00763590\n",
      "Epoch 1 | Step 414200 | Avg Loss: 0.0158 | Grad Norm: 0.00961093\n",
      "Epoch 1 | Step 414300 | Avg Loss: 0.0162 | Grad Norm: 0.00863492\n",
      "Epoch 1 | Step 414400 | Avg Loss: 0.0162 | Grad Norm: 0.00907449\n",
      "Epoch 1 | Step 414500 | Avg Loss: 0.0163 | Grad Norm: 0.00972906\n",
      "Epoch 1 | Step 414600 | Avg Loss: 0.0161 | Grad Norm: 0.01095332\n",
      "Epoch 1 | Step 414700 | Avg Loss: 0.0160 | Grad Norm: 0.00831940\n",
      "Epoch 1 | Step 414800 | Avg Loss: 0.0160 | Grad Norm: 0.01025444\n",
      "Epoch 1 | Step 414900 | Avg Loss: 0.0160 | Grad Norm: 0.00924515\n",
      "Epoch 1 | Step 415000 | Avg Loss: 0.0158 | Grad Norm: 0.00861305\n",
      "Epoch 1 | Step 415100 | Avg Loss: 0.0156 | Grad Norm: 0.00830059\n",
      "Epoch 1 | Step 415200 | Avg Loss: 0.0155 | Grad Norm: 0.00903032\n",
      "Epoch 1 | Step 415300 | Avg Loss: 0.0153 | Grad Norm: 0.00709890\n",
      "Epoch 1 | Step 415400 | Avg Loss: 0.0155 | Grad Norm: 0.00773915\n",
      "Epoch 1 | Step 415500 | Avg Loss: 0.0156 | Grad Norm: 0.00895377\n",
      "Epoch 1 | Step 415600 | Avg Loss: 0.0156 | Grad Norm: 0.01091423\n",
      "Epoch 1 | Step 415700 | Avg Loss: 0.0158 | Grad Norm: 0.00803865\n",
      "Epoch 1 | Step 415800 | Avg Loss: 0.0157 | Grad Norm: 0.00770372\n",
      "Epoch 1 | Step 415900 | Avg Loss: 0.0160 | Grad Norm: 0.00833763\n",
      "Epoch 1 | Step 416000 | Avg Loss: 0.0161 | Grad Norm: 0.00943977\n",
      "Epoch 1 | Step 416100 | Avg Loss: 0.0161 | Grad Norm: 0.00936564\n",
      "Epoch 1 | Step 416200 | Avg Loss: 0.0158 | Grad Norm: 0.00888158\n",
      "Epoch 1 | Step 416300 | Avg Loss: 0.0159 | Grad Norm: 0.00857338\n",
      "Epoch 1 | Step 416400 | Avg Loss: 0.0157 | Grad Norm: 0.00954937\n",
      "Epoch 1 | Step 416500 | Avg Loss: 0.0155 | Grad Norm: 0.00844334\n",
      "Epoch 1 | Step 416600 | Avg Loss: 0.0152 | Grad Norm: 0.00992719\n",
      "Epoch 1 | Step 416700 | Avg Loss: 0.0157 | Grad Norm: 0.00819558\n",
      "Epoch 1 | Step 416800 | Avg Loss: 0.0157 | Grad Norm: 0.00884096\n",
      "Epoch 1 | Step 416900 | Avg Loss: 0.0162 | Grad Norm: 0.00840469\n",
      "Epoch 1 | Step 417000 | Avg Loss: 0.0163 | Grad Norm: 0.00971648\n",
      "Epoch 1 | Step 417100 | Avg Loss: 0.0158 | Grad Norm: 0.00856768\n",
      "Epoch 1 | Step 417200 | Avg Loss: 0.0160 | Grad Norm: 0.00806663\n",
      "Epoch 1 | Step 417300 | Avg Loss: 0.0161 | Grad Norm: 0.01010161\n",
      "Epoch 1 | Step 417400 | Avg Loss: 0.0162 | Grad Norm: 0.00903476\n",
      "Epoch 1 | Step 417500 | Avg Loss: 0.0163 | Grad Norm: 0.01006106\n",
      "Epoch 1 | Step 417600 | Avg Loss: 0.0163 | Grad Norm: 0.00873543\n",
      "Epoch 1 | Step 417700 | Avg Loss: 0.0162 | Grad Norm: 0.00827440\n",
      "Epoch 1 | Step 417800 | Avg Loss: 0.0159 | Grad Norm: 0.00871758\n",
      "Epoch 1 | Step 417900 | Avg Loss: 0.0158 | Grad Norm: 0.00940782\n",
      "Epoch 1 | Step 418000 | Avg Loss: 0.0158 | Grad Norm: 0.00824541\n",
      "Epoch 1 | Step 418100 | Avg Loss: 0.0158 | Grad Norm: 0.00829887\n",
      "Epoch 1 | Step 418200 | Avg Loss: 0.0160 | Grad Norm: 0.01000727\n",
      "Epoch 1 | Step 418300 | Avg Loss: 0.0160 | Grad Norm: 0.00798019\n",
      "Epoch 1 | Step 418400 | Avg Loss: 0.0158 | Grad Norm: 0.00899418\n",
      "Epoch 1 | Step 418500 | Avg Loss: 0.0158 | Grad Norm: 0.00976848\n",
      "Epoch 1 | Step 418600 | Avg Loss: 0.0160 | Grad Norm: 0.00945431\n",
      "Epoch 1 | Step 418700 | Avg Loss: 0.0164 | Grad Norm: 0.01042187\n",
      "Epoch 1 | Step 418800 | Avg Loss: 0.0165 | Grad Norm: 0.00892981\n",
      "Epoch 1 | Step 418900 | Avg Loss: 0.0162 | Grad Norm: 0.00926069\n",
      "Epoch 1 | Step 419000 | Avg Loss: 0.0163 | Grad Norm: 0.00907786\n",
      "Epoch 1 | Step 419100 | Avg Loss: 0.0162 | Grad Norm: 0.00848135\n",
      "Epoch 1 | Step 419200 | Avg Loss: 0.0160 | Grad Norm: 0.00949686\n",
      "Epoch 1 | Step 419300 | Avg Loss: 0.0160 | Grad Norm: 0.00846745\n",
      "Epoch 1 | Step 419400 | Avg Loss: 0.0164 | Grad Norm: 0.00794928\n",
      "Epoch 1 | Step 419500 | Avg Loss: 0.0159 | Grad Norm: 0.00840818\n",
      "Epoch 1 | Step 419600 | Avg Loss: 0.0161 | Grad Norm: 0.00864995\n",
      "Epoch 1 | Step 419700 | Avg Loss: 0.0159 | Grad Norm: 0.00878871\n",
      "Epoch 1 | Step 419800 | Avg Loss: 0.0154 | Grad Norm: 0.00895113\n",
      "Epoch 1 | Step 419900 | Avg Loss: 0.0149 | Grad Norm: 0.00967292\n",
      "Epoch 1 | Step 420000 | Avg Loss: 0.0149 | Grad Norm: 0.00734604\n",
      "Epoch 1 | Step 420100 | Avg Loss: 0.0151 | Grad Norm: 0.00972662\n",
      "Epoch 1 | Step 420200 | Avg Loss: 0.0152 | Grad Norm: 0.00893287\n",
      "Epoch 1 | Step 420300 | Avg Loss: 0.0155 | Grad Norm: 0.00833297\n",
      "Epoch 1 | Step 420400 | Avg Loss: 0.0158 | Grad Norm: 0.00910607\n",
      "Epoch 1 | Step 420500 | Avg Loss: 0.0156 | Grad Norm: 0.00754838\n",
      "Epoch 1 | Step 420600 | Avg Loss: 0.0157 | Grad Norm: 0.00952554\n",
      "Epoch 1 | Step 420700 | Avg Loss: 0.0160 | Grad Norm: 0.00775106\n",
      "Epoch 1 | Step 420800 | Avg Loss: 0.0162 | Grad Norm: 0.00882722\n",
      "Epoch 1 | Step 420900 | Avg Loss: 0.0160 | Grad Norm: 0.01145022\n",
      "Epoch 1 | Step 421000 | Avg Loss: 0.0158 | Grad Norm: 0.00883510\n",
      "Epoch 1 | Step 421100 | Avg Loss: 0.0160 | Grad Norm: 0.00841781\n",
      "Epoch 1 | Step 421200 | Avg Loss: 0.0157 | Grad Norm: 0.01020784\n",
      "Epoch 1 | Step 421300 | Avg Loss: 0.0156 | Grad Norm: 0.00795612\n",
      "Epoch 1 | Step 421400 | Avg Loss: 0.0155 | Grad Norm: 0.01058721\n",
      "Epoch 1 | Step 421500 | Avg Loss: 0.0155 | Grad Norm: 0.00870455\n",
      "Epoch 1 | Step 421600 | Avg Loss: 0.0156 | Grad Norm: 0.00837751\n",
      "Epoch 1 | Step 421700 | Avg Loss: 0.0158 | Grad Norm: 0.01034476\n",
      "Epoch 1 | Step 421800 | Avg Loss: 0.0158 | Grad Norm: 0.00896813\n",
      "Epoch 1 | Step 421900 | Avg Loss: 0.0160 | Grad Norm: 0.00962566\n",
      "Epoch 1 | Step 422000 | Avg Loss: 0.0156 | Grad Norm: 0.00876134\n",
      "Epoch 1 | Step 422100 | Avg Loss: 0.0156 | Grad Norm: 0.00849645\n",
      "Epoch 1 | Step 422200 | Avg Loss: 0.0154 | Grad Norm: 0.00908219\n",
      "Epoch 1 | Step 422300 | Avg Loss: 0.0153 | Grad Norm: 0.00919350\n",
      "Epoch 1 | Step 422400 | Avg Loss: 0.0153 | Grad Norm: 0.01075350\n",
      "Epoch 1 | Step 422500 | Avg Loss: 0.0159 | Grad Norm: 0.01024818\n",
      "Epoch 1 | Step 422600 | Avg Loss: 0.0155 | Grad Norm: 0.00814662\n",
      "Epoch 1 | Step 422700 | Avg Loss: 0.0155 | Grad Norm: 0.01044651\n",
      "Epoch 1 | Step 422800 | Avg Loss: 0.0159 | Grad Norm: 0.00905836\n",
      "Epoch 1 | Step 422900 | Avg Loss: 0.0159 | Grad Norm: 0.00875942\n",
      "Epoch 1 | Step 423000 | Avg Loss: 0.0160 | Grad Norm: 0.01225282\n",
      "Epoch 1 | Step 423100 | Avg Loss: 0.0157 | Grad Norm: 0.00848015\n",
      "Epoch 1 | Step 423200 | Avg Loss: 0.0158 | Grad Norm: 0.00879181\n",
      "Epoch 1 | Step 423300 | Avg Loss: 0.0160 | Grad Norm: 0.00881773\n",
      "Epoch 1 | Step 423400 | Avg Loss: 0.0158 | Grad Norm: 0.00785788\n",
      "Epoch 1 | Step 423500 | Avg Loss: 0.0157 | Grad Norm: 0.00859386\n",
      "Epoch 1 | Step 423600 | Avg Loss: 0.0159 | Grad Norm: 0.01226977\n",
      "Epoch 1 | Step 423700 | Avg Loss: 0.0159 | Grad Norm: 0.00830060\n",
      "Epoch 1 | Step 423800 | Avg Loss: 0.0156 | Grad Norm: 0.01025778\n",
      "Epoch 1 | Step 423900 | Avg Loss: 0.0151 | Grad Norm: 0.01078853\n",
      "Epoch 1 | Step 424000 | Avg Loss: 0.0157 | Grad Norm: 0.00772367\n",
      "Epoch 1 | Step 424100 | Avg Loss: 0.0154 | Grad Norm: 0.00851470\n",
      "Epoch 1 | Step 424200 | Avg Loss: 0.0155 | Grad Norm: 0.00790021\n",
      "Epoch 1 | Step 424300 | Avg Loss: 0.0156 | Grad Norm: 0.00811706\n",
      "Epoch 1 | Step 424400 | Avg Loss: 0.0156 | Grad Norm: 0.00989200\n",
      "Epoch 1 | Step 424500 | Avg Loss: 0.0155 | Grad Norm: 0.00867226\n",
      "Epoch 1 | Step 424600 | Avg Loss: 0.0153 | Grad Norm: 0.00910820\n",
      "Epoch 1 | Step 424700 | Avg Loss: 0.0160 | Grad Norm: 0.00938101\n",
      "Epoch 1 | Step 424800 | Avg Loss: 0.0158 | Grad Norm: 0.00892461\n",
      "Epoch 1 | Step 424900 | Avg Loss: 0.0158 | Grad Norm: 0.00937988\n",
      "Epoch 1 | Step 425000 | Avg Loss: 0.0157 | Grad Norm: 0.00868989\n",
      "Epoch 1 | Step 425100 | Avg Loss: 0.0163 | Grad Norm: 0.01008150\n",
      "Epoch 1 | Step 425200 | Avg Loss: 0.0161 | Grad Norm: 0.01024046\n",
      "Epoch 1 | Step 425300 | Avg Loss: 0.0155 | Grad Norm: 0.00890627\n",
      "Epoch 1 | Step 425400 | Avg Loss: 0.0159 | Grad Norm: 0.00883972\n",
      "Epoch 1 | Step 425500 | Avg Loss: 0.0159 | Grad Norm: 0.00973322\n",
      "Epoch 1 | Step 425600 | Avg Loss: 0.0158 | Grad Norm: 0.01029185\n",
      "Epoch 1 | Step 425700 | Avg Loss: 0.0156 | Grad Norm: 0.00726821\n",
      "Epoch 1 | Step 425800 | Avg Loss: 0.0157 | Grad Norm: 0.00897177\n",
      "Epoch 1 | Step 425900 | Avg Loss: 0.0156 | Grad Norm: 0.00885237\n",
      "Epoch 1 | Step 426000 | Avg Loss: 0.0158 | Grad Norm: 0.00840316\n",
      "Epoch 1 | Step 426100 | Avg Loss: 0.0154 | Grad Norm: 0.00810745\n",
      "Epoch 1 | Step 426200 | Avg Loss: 0.0155 | Grad Norm: 0.00761598\n",
      "Epoch 1 | Step 426300 | Avg Loss: 0.0151 | Grad Norm: 0.01021444\n",
      "Epoch 1 | Step 426400 | Avg Loss: 0.0155 | Grad Norm: 0.01048737\n",
      "Epoch 1 | Step 426500 | Avg Loss: 0.0153 | Grad Norm: 0.00845776\n",
      "Epoch 1 | Step 426600 | Avg Loss: 0.0149 | Grad Norm: 0.00745337\n",
      "Epoch 1 | Step 426700 | Avg Loss: 0.0151 | Grad Norm: 0.00913645\n",
      "Epoch 1 | Step 426800 | Avg Loss: 0.0155 | Grad Norm: 0.00802054\n",
      "Epoch 1 | Step 426900 | Avg Loss: 0.0158 | Grad Norm: 0.01126809\n",
      "Epoch 1 | Step 427000 | Avg Loss: 0.0157 | Grad Norm: 0.00911396\n",
      "Epoch 1 | Step 427100 | Avg Loss: 0.0154 | Grad Norm: 0.01007235\n",
      "Epoch 1 | Step 427200 | Avg Loss: 0.0155 | Grad Norm: 0.00901184\n",
      "Epoch 1 | Step 427300 | Avg Loss: 0.0158 | Grad Norm: 0.00786919\n",
      "Epoch 1 | Step 427400 | Avg Loss: 0.0158 | Grad Norm: 0.01070480\n",
      "Epoch 1 | Step 427500 | Avg Loss: 0.0161 | Grad Norm: 0.00900152\n",
      "Epoch 1 | Step 427600 | Avg Loss: 0.0161 | Grad Norm: 0.00908282\n",
      "Epoch 1 | Step 427700 | Avg Loss: 0.0160 | Grad Norm: 0.01064379\n",
      "Epoch 1 | Step 427800 | Avg Loss: 0.0157 | Grad Norm: 0.00928852\n",
      "Epoch 1 | Step 427900 | Avg Loss: 0.0157 | Grad Norm: 0.01008493\n",
      "Epoch 1 | Step 428000 | Avg Loss: 0.0156 | Grad Norm: 0.00911097\n",
      "Epoch 1 | Step 428100 | Avg Loss: 0.0157 | Grad Norm: 0.00878610\n",
      "Epoch 1 | Step 428200 | Avg Loss: 0.0159 | Grad Norm: 0.00920910\n",
      "Epoch 1 | Step 428300 | Avg Loss: 0.0160 | Grad Norm: 0.00848525\n",
      "Epoch 1 | Step 428400 | Avg Loss: 0.0159 | Grad Norm: 0.00896363\n",
      "Epoch 1 | Step 428500 | Avg Loss: 0.0153 | Grad Norm: 0.00901331\n",
      "Epoch 1 | Step 428600 | Avg Loss: 0.0156 | Grad Norm: 0.00789956\n",
      "Epoch 1 | Step 428700 | Avg Loss: 0.0157 | Grad Norm: 0.00995088\n",
      "Epoch 1 | Step 428800 | Avg Loss: 0.0159 | Grad Norm: 0.00988599\n",
      "Epoch 1 | Step 428900 | Avg Loss: 0.0156 | Grad Norm: 0.00980837\n",
      "Epoch 1 | Step 429000 | Avg Loss: 0.0157 | Grad Norm: 0.00881817\n",
      "Epoch 1 | Step 429100 | Avg Loss: 0.0161 | Grad Norm: 0.01002162\n",
      "Epoch 1 | Step 429200 | Avg Loss: 0.0160 | Grad Norm: 0.00848608\n",
      "Epoch 1 | Step 429300 | Avg Loss: 0.0157 | Grad Norm: 0.00848409\n",
      "Epoch 1 | Step 429400 | Avg Loss: 0.0159 | Grad Norm: 0.00801459\n",
      "Epoch 1 | Step 429500 | Avg Loss: 0.0159 | Grad Norm: 0.00811490\n",
      "Epoch 1 | Step 429600 | Avg Loss: 0.0159 | Grad Norm: 0.00828086\n",
      "Epoch 1 | Step 429700 | Avg Loss: 0.0160 | Grad Norm: 0.00890678\n",
      "Epoch 1 | Step 429800 | Avg Loss: 0.0161 | Grad Norm: 0.01173888\n",
      "Epoch 1 | Step 429900 | Avg Loss: 0.0162 | Grad Norm: 0.00812303\n",
      "Epoch 1 | Step 430000 | Avg Loss: 0.0159 | Grad Norm: 0.00837192\n",
      "Epoch 1 | Step 430100 | Avg Loss: 0.0162 | Grad Norm: 0.00842818\n",
      "Epoch 1 | Step 430200 | Avg Loss: 0.0156 | Grad Norm: 0.00855871\n",
      "Epoch 1 | Step 430300 | Avg Loss: 0.0158 | Grad Norm: 0.00745068\n",
      "Epoch 1 | Step 430400 | Avg Loss: 0.0155 | Grad Norm: 0.00842160\n",
      "Epoch 1 | Step 430500 | Avg Loss: 0.0156 | Grad Norm: 0.01085798\n",
      "Epoch 1 | Step 430600 | Avg Loss: 0.0155 | Grad Norm: 0.00867347\n",
      "Epoch 1 | Step 430700 | Avg Loss: 0.0156 | Grad Norm: 0.00857842\n",
      "Epoch 1 | Step 430800 | Avg Loss: 0.0153 | Grad Norm: 0.01087315\n",
      "Epoch 1 | Step 430900 | Avg Loss: 0.0152 | Grad Norm: 0.00849183\n",
      "Epoch 1 | Step 431000 | Avg Loss: 0.0154 | Grad Norm: 0.00932444\n",
      "Epoch 1 | Step 431100 | Avg Loss: 0.0158 | Grad Norm: 0.01080356\n",
      "Epoch 1 | Step 431200 | Avg Loss: 0.0162 | Grad Norm: 0.00884407\n",
      "Epoch 1 | Step 431300 | Avg Loss: 0.0157 | Grad Norm: 0.00901785\n",
      "Epoch 1 | Step 431400 | Avg Loss: 0.0156 | Grad Norm: 0.00870866\n",
      "Epoch 1 | Step 431500 | Avg Loss: 0.0158 | Grad Norm: 0.00972787\n",
      "Epoch 1 | Step 431600 | Avg Loss: 0.0157 | Grad Norm: 0.00845612\n",
      "Epoch 1 | Step 431700 | Avg Loss: 0.0157 | Grad Norm: 0.00844277\n",
      "Epoch 1 | Step 431800 | Avg Loss: 0.0158 | Grad Norm: 0.00798457\n",
      "Epoch 1 | Step 431900 | Avg Loss: 0.0161 | Grad Norm: 0.01149014\n",
      "Epoch 1 | Step 432000 | Avg Loss: 0.0161 | Grad Norm: 0.00923817\n",
      "Epoch 1 | Step 432100 | Avg Loss: 0.0157 | Grad Norm: 0.01000917\n",
      "Epoch 1 | Step 432200 | Avg Loss: 0.0157 | Grad Norm: 0.00839243\n",
      "Epoch 1 | Step 432300 | Avg Loss: 0.0161 | Grad Norm: 0.00912405\n",
      "Epoch 1 | Step 432400 | Avg Loss: 0.0158 | Grad Norm: 0.00923372\n",
      "Epoch 1 | Step 432500 | Avg Loss: 0.0160 | Grad Norm: 0.00968743\n",
      "Epoch 1 | Step 432600 | Avg Loss: 0.0163 | Grad Norm: 0.00882980\n",
      "Epoch 1 | Step 432700 | Avg Loss: 0.0165 | Grad Norm: 0.00922900\n",
      "Epoch 1 | Step 432800 | Avg Loss: 0.0165 | Grad Norm: 0.00941306\n",
      "Epoch 1 | Step 432900 | Avg Loss: 0.0162 | Grad Norm: 0.01151131\n",
      "Epoch 1 | Step 433000 | Avg Loss: 0.0161 | Grad Norm: 0.00890476\n",
      "Epoch 1 | Step 433100 | Avg Loss: 0.0157 | Grad Norm: 0.00837344\n",
      "Epoch 1 | Step 433200 | Avg Loss: 0.0159 | Grad Norm: 0.00962811\n",
      "Epoch 1 | Step 433300 | Avg Loss: 0.0157 | Grad Norm: 0.00822589\n",
      "Epoch 1 | Step 433400 | Avg Loss: 0.0158 | Grad Norm: 0.00830598\n",
      "Epoch 1 | Step 433500 | Avg Loss: 0.0155 | Grad Norm: 0.00840064\n",
      "Epoch 1 | Step 433600 | Avg Loss: 0.0156 | Grad Norm: 0.00881609\n",
      "Epoch 1 | Step 433700 | Avg Loss: 0.0156 | Grad Norm: 0.00864951\n",
      "Epoch 1 | Step 433800 | Avg Loss: 0.0153 | Grad Norm: 0.00812726\n",
      "Epoch 1 | Step 433900 | Avg Loss: 0.0153 | Grad Norm: 0.00938880\n",
      "Epoch 1 | Step 434000 | Avg Loss: 0.0158 | Grad Norm: 0.00858269\n",
      "Epoch 1 | Step 434100 | Avg Loss: 0.0158 | Grad Norm: 0.00830858\n",
      "Epoch 1 | Step 434200 | Avg Loss: 0.0159 | Grad Norm: 0.00749858\n",
      "Epoch 1 | Step 434300 | Avg Loss: 0.0158 | Grad Norm: 0.01013931\n",
      "Epoch 1 | Step 434400 | Avg Loss: 0.0162 | Grad Norm: 0.00928750\n",
      "Epoch 1 | Step 434500 | Avg Loss: 0.0159 | Grad Norm: 0.00822280\n",
      "Epoch 1 | Step 434600 | Avg Loss: 0.0156 | Grad Norm: 0.00846721\n",
      "Epoch 1 | Step 434700 | Avg Loss: 0.0155 | Grad Norm: 0.00882977\n",
      "Epoch 1 | Step 434800 | Avg Loss: 0.0157 | Grad Norm: 0.00861372\n",
      "Epoch 1 | Step 434900 | Avg Loss: 0.0155 | Grad Norm: 0.00827852\n",
      "Epoch 1 | Step 435000 | Avg Loss: 0.0155 | Grad Norm: 0.00940455\n",
      "Epoch 1 | Step 435100 | Avg Loss: 0.0155 | Grad Norm: 0.00828820\n",
      "Epoch 1 | Step 435200 | Avg Loss: 0.0153 | Grad Norm: 0.00836150\n",
      "Epoch 1 | Step 435300 | Avg Loss: 0.0153 | Grad Norm: 0.00855134\n",
      "Epoch 1 | Step 435400 | Avg Loss: 0.0155 | Grad Norm: 0.00911106\n",
      "Epoch 1 | Step 435500 | Avg Loss: 0.0155 | Grad Norm: 0.00895230\n",
      "Epoch 1 | Step 435600 | Avg Loss: 0.0155 | Grad Norm: 0.00931097\n",
      "Epoch 1 | Step 435700 | Avg Loss: 0.0156 | Grad Norm: 0.00751352\n",
      "Epoch 1 | Step 435800 | Avg Loss: 0.0158 | Grad Norm: 0.00748069\n",
      "Epoch 1 | Step 435900 | Avg Loss: 0.0154 | Grad Norm: 0.00855311\n",
      "Epoch 1 | Step 436000 | Avg Loss: 0.0157 | Grad Norm: 0.00855749\n",
      "Epoch 1 | Step 436100 | Avg Loss: 0.0154 | Grad Norm: 0.00908367\n",
      "Epoch 1 | Step 436200 | Avg Loss: 0.0155 | Grad Norm: 0.00769295\n",
      "Epoch 1 | Step 436300 | Avg Loss: 0.0157 | Grad Norm: 0.00852814\n",
      "Epoch 1 | Step 436400 | Avg Loss: 0.0160 | Grad Norm: 0.00817828\n",
      "Epoch 1 | Step 436500 | Avg Loss: 0.0158 | Grad Norm: 0.00922777\n",
      "Epoch 1 | Step 436600 | Avg Loss: 0.0160 | Grad Norm: 0.00937897\n",
      "Epoch 1 | Step 436700 | Avg Loss: 0.0158 | Grad Norm: 0.00823053\n",
      "Epoch 1 | Step 436800 | Avg Loss: 0.0161 | Grad Norm: 0.00801758\n",
      "Epoch 1 | Step 436900 | Avg Loss: 0.0158 | Grad Norm: 0.00919011\n",
      "Epoch 1 | Step 437000 | Avg Loss: 0.0156 | Grad Norm: 0.00835985\n",
      "Epoch 1 | Step 437100 | Avg Loss: 0.0154 | Grad Norm: 0.00827812\n",
      "Epoch 1 | Step 437200 | Avg Loss: 0.0155 | Grad Norm: 0.00819579\n",
      "Epoch 1 | Step 437300 | Avg Loss: 0.0161 | Grad Norm: 0.00906462\n",
      "Epoch 1 | Step 437400 | Avg Loss: 0.0163 | Grad Norm: 0.00931762\n",
      "Epoch 1 | Step 437500 | Avg Loss: 0.0161 | Grad Norm: 0.00813092\n",
      "Epoch 1 | Step 437600 | Avg Loss: 0.0160 | Grad Norm: 0.00826019\n",
      "Epoch 1 | Step 437700 | Avg Loss: 0.0161 | Grad Norm: 0.00859506\n",
      "Epoch 1 | Step 437800 | Avg Loss: 0.0161 | Grad Norm: 0.00871033\n",
      "Epoch 1 | Step 437900 | Avg Loss: 0.0156 | Grad Norm: 0.00824735\n",
      "Epoch 1 | Step 438000 | Avg Loss: 0.0153 | Grad Norm: 0.00892868\n",
      "Epoch 1 | Step 438100 | Avg Loss: 0.0156 | Grad Norm: 0.00965381\n",
      "Epoch 1 | Step 438200 | Avg Loss: 0.0156 | Grad Norm: 0.00806420\n",
      "Epoch 1 | Step 438300 | Avg Loss: 0.0159 | Grad Norm: 0.00793373\n",
      "Epoch 1 | Step 438400 | Avg Loss: 0.0159 | Grad Norm: 0.00875789\n",
      "Epoch 1 | Step 438500 | Avg Loss: 0.0155 | Grad Norm: 0.00753763\n",
      "Epoch 1 | Step 438600 | Avg Loss: 0.0153 | Grad Norm: 0.00905336\n",
      "Epoch 1 | Step 438700 | Avg Loss: 0.0151 | Grad Norm: 0.00756169\n",
      "Epoch 1 | Step 438800 | Avg Loss: 0.0148 | Grad Norm: 0.00797364\n",
      "Epoch 1 | Step 438900 | Avg Loss: 0.0149 | Grad Norm: 0.00842085\n",
      "Epoch 1 | Step 439000 | Avg Loss: 0.0149 | Grad Norm: 0.00866071\n",
      "Epoch 1 | Step 439100 | Avg Loss: 0.0152 | Grad Norm: 0.00706320\n",
      "Epoch 1 | Step 439200 | Avg Loss: 0.0153 | Grad Norm: 0.00882326\n",
      "Epoch 1 | Step 439300 | Avg Loss: 0.0158 | Grad Norm: 0.00874508\n",
      "Epoch 1 | Step 439400 | Avg Loss: 0.0157 | Grad Norm: 0.01011171\n",
      "Epoch 1 | Step 439500 | Avg Loss: 0.0159 | Grad Norm: 0.00784388\n",
      "Epoch 1 | Step 439600 | Avg Loss: 0.0157 | Grad Norm: 0.00875001\n",
      "Epoch 1 | Step 439700 | Avg Loss: 0.0163 | Grad Norm: 0.00883067\n",
      "Epoch 1 | Step 439800 | Avg Loss: 0.0165 | Grad Norm: 0.00945073\n",
      "Epoch 1 | Step 439900 | Avg Loss: 0.0163 | Grad Norm: 0.01025892\n",
      "Epoch 1 | Step 440000 | Avg Loss: 0.0166 | Grad Norm: 0.00873307\n",
      "Epoch 1 | Step 440100 | Avg Loss: 0.0161 | Grad Norm: 0.00870297\n",
      "Epoch 1 | Step 440200 | Avg Loss: 0.0158 | Grad Norm: 0.00890752\n",
      "Epoch 1 | Step 440300 | Avg Loss: 0.0154 | Grad Norm: 0.00929560\n",
      "Epoch 1 | Step 440400 | Avg Loss: 0.0150 | Grad Norm: 0.00965077\n",
      "Epoch 1 | Step 440500 | Avg Loss: 0.0152 | Grad Norm: 0.00917938\n",
      "Epoch 1 | Step 440600 | Avg Loss: 0.0152 | Grad Norm: 0.01014047\n",
      "Epoch 1 | Step 440700 | Avg Loss: 0.0152 | Grad Norm: 0.00818695\n",
      "Epoch 1 | Step 440800 | Avg Loss: 0.0146 | Grad Norm: 0.00755964\n",
      "Epoch 1 | Step 440900 | Avg Loss: 0.0147 | Grad Norm: 0.00887740\n",
      "Epoch 1 | Step 441000 | Avg Loss: 0.0150 | Grad Norm: 0.00853781\n",
      "Epoch 1 | Step 441100 | Avg Loss: 0.0156 | Grad Norm: 0.00871226\n",
      "Epoch 1 | Step 441200 | Avg Loss: 0.0156 | Grad Norm: 0.00850733\n",
      "Epoch 1 | Step 441300 | Avg Loss: 0.0156 | Grad Norm: 0.00954536\n",
      "Epoch 1 | Step 441400 | Avg Loss: 0.0159 | Grad Norm: 0.00920872\n",
      "Epoch 1 | Step 441500 | Avg Loss: 0.0162 | Grad Norm: 0.00799065\n",
      "Epoch 1 | Step 441600 | Avg Loss: 0.0159 | Grad Norm: 0.00929200\n",
      "Epoch 1 | Step 441700 | Avg Loss: 0.0159 | Grad Norm: 0.00955930\n",
      "Epoch 1 | Step 441800 | Avg Loss: 0.0157 | Grad Norm: 0.01026960\n",
      "Epoch 1 | Step 441900 | Avg Loss: 0.0158 | Grad Norm: 0.00987175\n",
      "Epoch 1 | Step 442000 | Avg Loss: 0.0157 | Grad Norm: 0.00955258\n",
      "Epoch 1 | Step 442100 | Avg Loss: 0.0153 | Grad Norm: 0.00813135\n",
      "Epoch 1 | Step 442200 | Avg Loss: 0.0150 | Grad Norm: 0.00827035\n",
      "Epoch 1 | Step 442300 | Avg Loss: 0.0156 | Grad Norm: 0.00890038\n",
      "Epoch 1 | Step 442400 | Avg Loss: 0.0154 | Grad Norm: 0.00831987\n",
      "Epoch 1 | Step 442500 | Avg Loss: 0.0155 | Grad Norm: 0.00758359\n",
      "Epoch 1 | Step 442600 | Avg Loss: 0.0160 | Grad Norm: 0.00913852\n",
      "Epoch 1 | Step 442700 | Avg Loss: 0.0159 | Grad Norm: 0.00838229\n",
      "Epoch 1 | Step 442800 | Avg Loss: 0.0155 | Grad Norm: 0.00783390\n",
      "Epoch 1 | Step 442900 | Avg Loss: 0.0155 | Grad Norm: 0.00847864\n",
      "Epoch 1 | Step 443000 | Avg Loss: 0.0151 | Grad Norm: 0.00773816\n",
      "Epoch 1 | Step 443100 | Avg Loss: 0.0152 | Grad Norm: 0.00740067\n",
      "Epoch 1 | Step 443200 | Avg Loss: 0.0157 | Grad Norm: 0.00834602\n",
      "Epoch 1 | Step 443300 | Avg Loss: 0.0159 | Grad Norm: 0.00925052\n",
      "Epoch 1 | Step 443400 | Avg Loss: 0.0162 | Grad Norm: 0.00792075\n",
      "Epoch 1 | Step 443500 | Avg Loss: 0.0159 | Grad Norm: 0.00836554\n",
      "Epoch 1 | Step 443600 | Avg Loss: 0.0158 | Grad Norm: 0.00867746\n",
      "Epoch 1 | Step 443700 | Avg Loss: 0.0161 | Grad Norm: 0.00904432\n",
      "Epoch 1 | Step 443800 | Avg Loss: 0.0158 | Grad Norm: 0.00905145\n",
      "Epoch 1 | Step 443900 | Avg Loss: 0.0162 | Grad Norm: 0.00929943\n",
      "Epoch 1 | Step 444000 | Avg Loss: 0.0162 | Grad Norm: 0.00781519\n",
      "Epoch 1 | Step 444100 | Avg Loss: 0.0163 | Grad Norm: 0.00942533\n",
      "Epoch 1 | Step 444200 | Avg Loss: 0.0161 | Grad Norm: 0.00730034\n",
      "Epoch 1 | Step 444300 | Avg Loss: 0.0162 | Grad Norm: 0.01028723\n",
      "Epoch 1 | Step 444400 | Avg Loss: 0.0164 | Grad Norm: 0.00975376\n",
      "Epoch 1 | Step 444500 | Avg Loss: 0.0159 | Grad Norm: 0.01194910\n",
      "Epoch 1 | Step 444600 | Avg Loss: 0.0159 | Grad Norm: 0.00832101\n",
      "Epoch 1 | Step 444700 | Avg Loss: 0.0158 | Grad Norm: 0.00842584\n",
      "Epoch 1 | Step 444800 | Avg Loss: 0.0161 | Grad Norm: 0.00932341\n",
      "Epoch 1 | Step 444900 | Avg Loss: 0.0166 | Grad Norm: 0.00845174\n",
      "Epoch 1 | Step 445000 | Avg Loss: 0.0162 | Grad Norm: 0.00822045\n",
      "Epoch 1 | Step 445100 | Avg Loss: 0.0161 | Grad Norm: 0.00887254\n",
      "Epoch 1 | Step 445200 | Avg Loss: 0.0160 | Grad Norm: 0.00967174\n",
      "Epoch 1 | Step 445300 | Avg Loss: 0.0159 | Grad Norm: 0.00697107\n",
      "Epoch 1 | Step 445400 | Avg Loss: 0.0158 | Grad Norm: 0.01007985\n",
      "Epoch 1 | Step 445500 | Avg Loss: 0.0161 | Grad Norm: 0.00847463\n",
      "Epoch 1 | Step 445600 | Avg Loss: 0.0161 | Grad Norm: 0.00928183\n",
      "Epoch 1 | Step 445700 | Avg Loss: 0.0160 | Grad Norm: 0.00821145\n",
      "Epoch 1 | Step 445800 | Avg Loss: 0.0156 | Grad Norm: 0.00768058\n",
      "Epoch 1 | Step 445900 | Avg Loss: 0.0160 | Grad Norm: 0.01042232\n",
      "Epoch 1 | Step 446000 | Avg Loss: 0.0159 | Grad Norm: 0.00859217\n",
      "Epoch 1 | Step 446100 | Avg Loss: 0.0158 | Grad Norm: 0.00822085\n",
      "Epoch 1 | Step 446200 | Avg Loss: 0.0154 | Grad Norm: 0.00691607\n",
      "Epoch 1 | Step 446300 | Avg Loss: 0.0155 | Grad Norm: 0.00866043\n",
      "Epoch 1 | Step 446400 | Avg Loss: 0.0156 | Grad Norm: 0.00815431\n",
      "Epoch 1 | Step 446500 | Avg Loss: 0.0156 | Grad Norm: 0.00856103\n",
      "Epoch 1 | Step 446600 | Avg Loss: 0.0159 | Grad Norm: 0.00915207\n",
      "Epoch 1 | Step 446700 | Avg Loss: 0.0159 | Grad Norm: 0.00891276\n",
      "Epoch 1 | Step 446800 | Avg Loss: 0.0157 | Grad Norm: 0.00887821\n",
      "Epoch 1 | Step 446900 | Avg Loss: 0.0158 | Grad Norm: 0.00749965\n",
      "Epoch 1 | Step 447000 | Avg Loss: 0.0164 | Grad Norm: 0.00898970\n",
      "Epoch 1 | Step 447100 | Avg Loss: 0.0164 | Grad Norm: 0.00881608\n",
      "Epoch 1 | Step 447200 | Avg Loss: 0.0161 | Grad Norm: 0.00945820\n",
      "Epoch 1 | Step 447300 | Avg Loss: 0.0161 | Grad Norm: 0.01038608\n",
      "Epoch 1 | Step 447400 | Avg Loss: 0.0160 | Grad Norm: 0.00792885\n",
      "Epoch 1 | Step 447500 | Avg Loss: 0.0157 | Grad Norm: 0.00792714\n",
      "Epoch 1 | Step 447600 | Avg Loss: 0.0156 | Grad Norm: 0.00957054\n",
      "Epoch 1 | Step 447700 | Avg Loss: 0.0160 | Grad Norm: 0.00876151\n",
      "Epoch 1 | Step 447800 | Avg Loss: 0.0159 | Grad Norm: 0.00771526\n",
      "Epoch 1 | Step 447900 | Avg Loss: 0.0163 | Grad Norm: 0.00947637\n",
      "Epoch 1 | Step 448000 | Avg Loss: 0.0163 | Grad Norm: 0.00950692\n",
      "Epoch 1 | Step 448100 | Avg Loss: 0.0165 | Grad Norm: 0.00850151\n",
      "Epoch 1 | Step 448200 | Avg Loss: 0.0167 | Grad Norm: 0.00713742\n",
      "Epoch 1 | Step 448300 | Avg Loss: 0.0162 | Grad Norm: 0.00931953\n",
      "Epoch 1 | Step 448400 | Avg Loss: 0.0159 | Grad Norm: 0.00951449\n",
      "Epoch 1 | Step 448500 | Avg Loss: 0.0157 | Grad Norm: 0.00894725\n",
      "Epoch 1 | Step 448600 | Avg Loss: 0.0158 | Grad Norm: 0.00874013\n",
      "Epoch 1 | Step 448700 | Avg Loss: 0.0155 | Grad Norm: 0.00905075\n",
      "Epoch 1 | Step 448800 | Avg Loss: 0.0158 | Grad Norm: 0.00859122\n",
      "Epoch 1 | Step 448900 | Avg Loss: 0.0158 | Grad Norm: 0.00836454\n",
      "Epoch 1 | Step 449000 | Avg Loss: 0.0155 | Grad Norm: 0.00768647\n",
      "Epoch 1 | Step 449100 | Avg Loss: 0.0157 | Grad Norm: 0.00878647\n",
      "Epoch 1 | Step 449200 | Avg Loss: 0.0161 | Grad Norm: 0.00985993\n",
      "Epoch 1 | Step 449300 | Avg Loss: 0.0161 | Grad Norm: 0.01295821\n",
      "Epoch 1 | Step 449400 | Avg Loss: 0.0157 | Grad Norm: 0.00871644\n",
      "Epoch 1 | Step 449500 | Avg Loss: 0.0157 | Grad Norm: 0.00890730\n",
      "Epoch 1 | Step 449600 | Avg Loss: 0.0158 | Grad Norm: 0.00798817\n",
      "Epoch 1 | Step 449700 | Avg Loss: 0.0160 | Grad Norm: 0.00846037\n",
      "Epoch 1 | Step 449800 | Avg Loss: 0.0160 | Grad Norm: 0.00865560\n",
      "Epoch 1 | Step 449900 | Avg Loss: 0.0157 | Grad Norm: 0.01080699\n",
      "Epoch 1 | Step 450000 | Avg Loss: 0.0156 | Grad Norm: 0.00815036\n",
      "Epoch 1 | Step 450100 | Avg Loss: 0.0154 | Grad Norm: 0.00773084\n",
      "Epoch 1 | Step 450200 | Avg Loss: 0.0155 | Grad Norm: 0.01024575\n",
      "Epoch 1 | Step 450300 | Avg Loss: 0.0152 | Grad Norm: 0.00906961\n",
      "Epoch 1 | Step 450400 | Avg Loss: 0.0153 | Grad Norm: 0.00820203\n",
      "Epoch 1 | Step 450500 | Avg Loss: 0.0155 | Grad Norm: 0.00765417\n",
      "Epoch 1 | Step 450600 | Avg Loss: 0.0152 | Grad Norm: 0.00817794\n",
      "Epoch 1 | Step 450700 | Avg Loss: 0.0154 | Grad Norm: 0.01044271\n",
      "Epoch 1 | Step 450800 | Avg Loss: 0.0152 | Grad Norm: 0.00956975\n",
      "Epoch 1 | Step 450900 | Avg Loss: 0.0156 | Grad Norm: 0.00828227\n",
      "Epoch 1 | Step 451000 | Avg Loss: 0.0156 | Grad Norm: 0.00802849\n",
      "Epoch 1 | Step 451100 | Avg Loss: 0.0158 | Grad Norm: 0.00839961\n",
      "Epoch 1 | Step 451200 | Avg Loss: 0.0161 | Grad Norm: 0.00784270\n",
      "Epoch 1 | Step 451300 | Avg Loss: 0.0159 | Grad Norm: 0.01112772\n",
      "Epoch 1 | Step 451400 | Avg Loss: 0.0159 | Grad Norm: 0.00827502\n",
      "Epoch 1 | Step 451500 | Avg Loss: 0.0156 | Grad Norm: 0.00795688\n",
      "Epoch 1 | Step 451600 | Avg Loss: 0.0157 | Grad Norm: 0.00937501\n",
      "Epoch 1 | Step 451700 | Avg Loss: 0.0159 | Grad Norm: 0.00908892\n",
      "Epoch 1 | Step 451800 | Avg Loss: 0.0160 | Grad Norm: 0.00944947\n",
      "Epoch 1 | Step 451900 | Avg Loss: 0.0161 | Grad Norm: 0.00889887\n",
      "Epoch 1 | Step 452000 | Avg Loss: 0.0162 | Grad Norm: 0.00932839\n",
      "Epoch 1 | Step 452100 | Avg Loss: 0.0160 | Grad Norm: 0.01126324\n",
      "Epoch 1 | Step 452200 | Avg Loss: 0.0168 | Grad Norm: 0.00891300\n",
      "Epoch 1 | Step 452300 | Avg Loss: 0.0167 | Grad Norm: 0.00926511\n",
      "Epoch 1 | Step 452400 | Avg Loss: 0.0162 | Grad Norm: 0.00855135\n",
      "Epoch 1 | Step 452500 | Avg Loss: 0.0161 | Grad Norm: 0.01117424\n",
      "Epoch 1 | Step 452600 | Avg Loss: 0.0162 | Grad Norm: 0.00956671\n",
      "Epoch 1 | Step 452700 | Avg Loss: 0.0161 | Grad Norm: 0.00989080\n",
      "Epoch 1 | Step 452800 | Avg Loss: 0.0158 | Grad Norm: 0.00880518\n",
      "Epoch 1 | Step 452900 | Avg Loss: 0.0160 | Grad Norm: 0.00831775\n",
      "Epoch 1 | Step 453000 | Avg Loss: 0.0155 | Grad Norm: 0.00792919\n",
      "Epoch 1 | Step 453100 | Avg Loss: 0.0158 | Grad Norm: 0.00749901\n",
      "Epoch 1 | Step 453200 | Avg Loss: 0.0160 | Grad Norm: 0.00953108\n",
      "Epoch 1 | Step 453300 | Avg Loss: 0.0159 | Grad Norm: 0.00908366\n",
      "Epoch 1 | Step 453400 | Avg Loss: 0.0156 | Grad Norm: 0.00867444\n",
      "Epoch 1 | Step 453500 | Avg Loss: 0.0157 | Grad Norm: 0.00943463\n",
      "Epoch 1 | Step 453600 | Avg Loss: 0.0156 | Grad Norm: 0.00814955\n",
      "Epoch 1 | Step 453700 | Avg Loss: 0.0157 | Grad Norm: 0.00921543\n",
      "Epoch 1 | Step 453800 | Avg Loss: 0.0156 | Grad Norm: 0.00771419\n",
      "Epoch 1 | Step 453900 | Avg Loss: 0.0154 | Grad Norm: 0.00901379\n",
      "Epoch 1 | Step 454000 | Avg Loss: 0.0159 | Grad Norm: 0.00922990\n",
      "Epoch 1 | Step 454100 | Avg Loss: 0.0156 | Grad Norm: 0.00966842\n",
      "Epoch 1 | Step 454200 | Avg Loss: 0.0160 | Grad Norm: 0.01103945\n",
      "Epoch 1 | Step 454300 | Avg Loss: 0.0162 | Grad Norm: 0.00916412\n",
      "Epoch 1 | Step 454400 | Avg Loss: 0.0165 | Grad Norm: 0.00932369\n",
      "Epoch 1 | Step 454500 | Avg Loss: 0.0162 | Grad Norm: 0.01285920\n",
      "Epoch 1 | Step 454600 | Avg Loss: 0.0165 | Grad Norm: 0.00892383\n",
      "Epoch 1 | Step 454700 | Avg Loss: 0.0166 | Grad Norm: 0.00948477\n",
      "Epoch 1 | Step 454800 | Avg Loss: 0.0165 | Grad Norm: 0.00812298\n",
      "Epoch 1 | Step 454900 | Avg Loss: 0.0162 | Grad Norm: 0.00876756\n",
      "Epoch 1 | Step 455000 | Avg Loss: 0.0165 | Grad Norm: 0.00899371\n",
      "Epoch 1 | Step 455100 | Avg Loss: 0.0167 | Grad Norm: 0.00957965\n",
      "Epoch 1 | Step 455200 | Avg Loss: 0.0162 | Grad Norm: 0.00821245\n",
      "Epoch 1 | Step 455300 | Avg Loss: 0.0160 | Grad Norm: 0.00881965\n",
      "Epoch 1 | Step 455400 | Avg Loss: 0.0161 | Grad Norm: 0.01036368\n",
      "Epoch 1 | Step 455500 | Avg Loss: 0.0160 | Grad Norm: 0.00828357\n",
      "Epoch 1 | Step 455600 | Avg Loss: 0.0159 | Grad Norm: 0.00831185\n",
      "Epoch 1 | Step 455700 | Avg Loss: 0.0156 | Grad Norm: 0.00866049\n",
      "Epoch 1 | Step 455800 | Avg Loss: 0.0153 | Grad Norm: 0.00770684\n",
      "Epoch 1 | Step 455900 | Avg Loss: 0.0153 | Grad Norm: 0.00966482\n",
      "Epoch 1 | Step 456000 | Avg Loss: 0.0149 | Grad Norm: 0.00787931\n",
      "Epoch 1 | Step 456100 | Avg Loss: 0.0149 | Grad Norm: 0.01179508\n",
      "Epoch 1 | Step 456200 | Avg Loss: 0.0151 | Grad Norm: 0.00834892\n",
      "Epoch 1 | Step 456300 | Avg Loss: 0.0152 | Grad Norm: 0.00836461\n",
      "Epoch 1 | Step 456400 | Avg Loss: 0.0152 | Grad Norm: 0.00850368\n",
      "Epoch 1 | Step 456500 | Avg Loss: 0.0153 | Grad Norm: 0.00728912\n",
      "Epoch 1 | Step 456600 | Avg Loss: 0.0150 | Grad Norm: 0.00724563\n",
      "Epoch 1 | Step 456700 | Avg Loss: 0.0155 | Grad Norm: 0.00922329\n",
      "Epoch 1 | Step 456800 | Avg Loss: 0.0154 | Grad Norm: 0.00834031\n",
      "Epoch 1 | Step 456900 | Avg Loss: 0.0155 | Grad Norm: 0.00798696\n",
      "Epoch 1 | Step 457000 | Avg Loss: 0.0153 | Grad Norm: 0.00817846\n",
      "Epoch 1 | Step 457100 | Avg Loss: 0.0155 | Grad Norm: 0.00919089\n",
      "Epoch 1 | Step 457200 | Avg Loss: 0.0151 | Grad Norm: 0.00857607\n",
      "Epoch 1 | Step 457300 | Avg Loss: 0.0149 | Grad Norm: 0.00970020\n",
      "Epoch 1 | Step 457400 | Avg Loss: 0.0151 | Grad Norm: 0.01194322\n",
      "Epoch 1 | Step 457500 | Avg Loss: 0.0157 | Grad Norm: 0.00921717\n",
      "Epoch 1 | Step 457600 | Avg Loss: 0.0160 | Grad Norm: 0.00910629\n",
      "Epoch 1 | Step 457700 | Avg Loss: 0.0156 | Grad Norm: 0.00792113\n",
      "Epoch 1 | Step 457800 | Avg Loss: 0.0156 | Grad Norm: 0.00986768\n",
      "Epoch 1 | Step 457900 | Avg Loss: 0.0153 | Grad Norm: 0.00870348\n",
      "Epoch 1 | Step 458000 | Avg Loss: 0.0153 | Grad Norm: 0.00905685\n",
      "Epoch 1 | Step 458100 | Avg Loss: 0.0155 | Grad Norm: 0.00817954\n",
      "Epoch 1 | Step 458200 | Avg Loss: 0.0155 | Grad Norm: 0.00955762\n",
      "Epoch 1 | Step 458300 | Avg Loss: 0.0158 | Grad Norm: 0.00983984\n",
      "Epoch 1 | Step 458400 | Avg Loss: 0.0157 | Grad Norm: 0.00813405\n",
      "Epoch 1 | Step 458500 | Avg Loss: 0.0157 | Grad Norm: 0.00868869\n",
      "Epoch 1 | Step 458600 | Avg Loss: 0.0158 | Grad Norm: 0.00956405\n",
      "Epoch 1 | Step 458700 | Avg Loss: 0.0162 | Grad Norm: 0.00872080\n",
      "Epoch 1 | Step 458800 | Avg Loss: 0.0161 | Grad Norm: 0.00760210\n",
      "Epoch 1 | Step 458900 | Avg Loss: 0.0161 | Grad Norm: 0.00910582\n",
      "Epoch 1 | Step 459000 | Avg Loss: 0.0157 | Grad Norm: 0.00872267\n",
      "Epoch 1 | Step 459100 | Avg Loss: 0.0158 | Grad Norm: 0.00815050\n",
      "Epoch 1 | Step 459200 | Avg Loss: 0.0158 | Grad Norm: 0.00816672\n",
      "Epoch 1 | Step 459300 | Avg Loss: 0.0153 | Grad Norm: 0.00892458\n",
      "Epoch 1 | Step 459400 | Avg Loss: 0.0155 | Grad Norm: 0.00917432\n",
      "Epoch 1 | Step 459500 | Avg Loss: 0.0154 | Grad Norm: 0.00834840\n",
      "Epoch 1 | Step 459600 | Avg Loss: 0.0156 | Grad Norm: 0.00928129\n",
      "Epoch 1 | Step 459700 | Avg Loss: 0.0156 | Grad Norm: 0.00828354\n",
      "Epoch 1 | Step 459800 | Avg Loss: 0.0153 | Grad Norm: 0.00857693\n",
      "Epoch 1 | Step 459900 | Avg Loss: 0.0153 | Grad Norm: 0.00741485\n",
      "Epoch 1 | Step 460000 | Avg Loss: 0.0154 | Grad Norm: 0.00800598\n",
      "Epoch 1 | Step 460100 | Avg Loss: 0.0152 | Grad Norm: 0.01272626\n",
      "Epoch 1 | Step 460200 | Avg Loss: 0.0152 | Grad Norm: 0.00902752\n",
      "Epoch 1 | Step 460300 | Avg Loss: 0.0154 | Grad Norm: 0.00801164\n",
      "Epoch 1 | Step 460400 | Avg Loss: 0.0153 | Grad Norm: 0.00938347\n",
      "Epoch 1 | Step 460500 | Avg Loss: 0.0154 | Grad Norm: 0.00960529\n",
      "Epoch 1 | Step 460600 | Avg Loss: 0.0151 | Grad Norm: 0.00797826\n",
      "Epoch 1 | Step 460700 | Avg Loss: 0.0153 | Grad Norm: 0.00965924\n",
      "Epoch 1 | Step 460800 | Avg Loss: 0.0149 | Grad Norm: 0.00837939\n",
      "Epoch 1 | Step 460900 | Avg Loss: 0.0150 | Grad Norm: 0.00961653\n",
      "Epoch 1 | Step 461000 | Avg Loss: 0.0152 | Grad Norm: 0.00858213\n",
      "Epoch 1 | Step 461100 | Avg Loss: 0.0151 | Grad Norm: 0.01006981\n",
      "Epoch 1 | Step 461200 | Avg Loss: 0.0151 | Grad Norm: 0.00787295\n",
      "Epoch 1 | Step 461300 | Avg Loss: 0.0153 | Grad Norm: 0.00946837\n",
      "Epoch 1 | Step 461400 | Avg Loss: 0.0151 | Grad Norm: 0.01025465\n",
      "Epoch 1 | Step 461500 | Avg Loss: 0.0152 | Grad Norm: 0.00840579\n",
      "Epoch 1 | Step 461600 | Avg Loss: 0.0149 | Grad Norm: 0.01165552\n",
      "Epoch 1 | Step 461700 | Avg Loss: 0.0155 | Grad Norm: 0.00879965\n",
      "Epoch 1 | Step 461800 | Avg Loss: 0.0152 | Grad Norm: 0.00975483\n",
      "Epoch 1 | Step 461900 | Avg Loss: 0.0149 | Grad Norm: 0.00915650\n",
      "Epoch 1 | Step 462000 | Avg Loss: 0.0149 | Grad Norm: 0.00818597\n",
      "Epoch 1 | Step 462100 | Avg Loss: 0.0152 | Grad Norm: 0.00925309\n",
      "Epoch 1 | Step 462200 | Avg Loss: 0.0157 | Grad Norm: 0.00824074\n",
      "Epoch 1 | Step 462300 | Avg Loss: 0.0152 | Grad Norm: 0.01066964\n",
      "Epoch 1 | Step 462400 | Avg Loss: 0.0150 | Grad Norm: 0.00884824\n",
      "Epoch 1 | Step 462500 | Avg Loss: 0.0154 | Grad Norm: 0.00791540\n",
      "Epoch 1 | Step 462600 | Avg Loss: 0.0155 | Grad Norm: 0.00890541\n",
      "Epoch 1 | Step 462700 | Avg Loss: 0.0157 | Grad Norm: 0.00913419\n",
      "Epoch 1 | Step 462800 | Avg Loss: 0.0154 | Grad Norm: 0.00919892\n",
      "Epoch 1 | Step 462900 | Avg Loss: 0.0157 | Grad Norm: 0.00904768\n",
      "Epoch 1 | Step 463000 | Avg Loss: 0.0159 | Grad Norm: 0.00858437\n",
      "Epoch 1 | Step 463100 | Avg Loss: 0.0157 | Grad Norm: 0.00846503\n",
      "Epoch 1 | Step 463200 | Avg Loss: 0.0160 | Grad Norm: 0.00998412\n",
      "Epoch 1 | Step 463300 | Avg Loss: 0.0160 | Grad Norm: 0.00883918\n",
      "Epoch 1 | Step 463400 | Avg Loss: 0.0157 | Grad Norm: 0.00903316\n",
      "Epoch 1 | Step 463500 | Avg Loss: 0.0160 | Grad Norm: 0.00992533\n",
      "Epoch 1 | Step 463600 | Avg Loss: 0.0159 | Grad Norm: 0.00907824\n",
      "Epoch 1 | Step 463700 | Avg Loss: 0.0159 | Grad Norm: 0.01237461\n",
      "Epoch 1 | Step 463800 | Avg Loss: 0.0155 | Grad Norm: 0.00912610\n",
      "Epoch 1 | Step 463900 | Avg Loss: 0.0154 | Grad Norm: 0.00831090\n",
      "Epoch 1 | Step 464000 | Avg Loss: 0.0153 | Grad Norm: 0.00932478\n",
      "Epoch 1 | Step 464100 | Avg Loss: 0.0157 | Grad Norm: 0.00957621\n",
      "Epoch 1 | Step 464200 | Avg Loss: 0.0158 | Grad Norm: 0.00751046\n",
      "Epoch 1 | Step 464300 | Avg Loss: 0.0160 | Grad Norm: 0.00823330\n",
      "Epoch 1 | Step 464400 | Avg Loss: 0.0159 | Grad Norm: 0.00841495\n",
      "Epoch 1 | Step 464500 | Avg Loss: 0.0157 | Grad Norm: 0.00790949\n",
      "Epoch 1 | Step 464600 | Avg Loss: 0.0154 | Grad Norm: 0.00717866\n",
      "Epoch 1 | Step 464700 | Avg Loss: 0.0157 | Grad Norm: 0.00821096\n",
      "Epoch 1 | Step 464800 | Avg Loss: 0.0156 | Grad Norm: 0.00858971\n",
      "Epoch 1 | Step 464900 | Avg Loss: 0.0154 | Grad Norm: 0.01000314\n",
      "Epoch 1 | Step 465000 | Avg Loss: 0.0160 | Grad Norm: 0.00981101\n",
      "Epoch 1 | Step 465100 | Avg Loss: 0.0159 | Grad Norm: 0.01063507\n",
      "Epoch 1 | Step 465200 | Avg Loss: 0.0160 | Grad Norm: 0.00889286\n",
      "Epoch 1 | Step 465300 | Avg Loss: 0.0160 | Grad Norm: 0.00952356\n",
      "Epoch 1 | Step 465400 | Avg Loss: 0.0158 | Grad Norm: 0.00937072\n",
      "Epoch 1 | Step 465500 | Avg Loss: 0.0158 | Grad Norm: 0.00899518\n",
      "Epoch 1 | Step 465600 | Avg Loss: 0.0160 | Grad Norm: 0.00783162\n",
      "Epoch 1 | Step 465700 | Avg Loss: 0.0158 | Grad Norm: 0.00909316\n",
      "Epoch 1 | Step 465800 | Avg Loss: 0.0158 | Grad Norm: 0.00828924\n",
      "Epoch 1 | Step 465900 | Avg Loss: 0.0160 | Grad Norm: 0.00839382\n",
      "Epoch 1 | Step 466000 | Avg Loss: 0.0161 | Grad Norm: 0.00760245\n",
      "Epoch 1 | Step 466100 | Avg Loss: 0.0161 | Grad Norm: 0.00853814\n",
      "Epoch 1 | Step 466200 | Avg Loss: 0.0160 | Grad Norm: 0.00891836\n",
      "Epoch 1 | Step 466300 | Avg Loss: 0.0158 | Grad Norm: 0.01200189\n",
      "Epoch 1 | Step 466400 | Avg Loss: 0.0161 | Grad Norm: 0.00837232\n",
      "Epoch 1 | Step 466500 | Avg Loss: 0.0157 | Grad Norm: 0.00903761\n",
      "Epoch 1 | Step 466600 | Avg Loss: 0.0155 | Grad Norm: 0.00923725\n",
      "Epoch 1 | Step 466700 | Avg Loss: 0.0157 | Grad Norm: 0.00865395\n",
      "Epoch 1 | Step 466800 | Avg Loss: 0.0154 | Grad Norm: 0.01036117\n",
      "Epoch 1 | Step 466900 | Avg Loss: 0.0157 | Grad Norm: 0.00815785\n",
      "Epoch 1 | Step 467000 | Avg Loss: 0.0156 | Grad Norm: 0.00868578\n",
      "Epoch 1 | Step 467100 | Avg Loss: 0.0158 | Grad Norm: 0.00829738\n",
      "Epoch 1 | Step 467200 | Avg Loss: 0.0155 | Grad Norm: 0.00889546\n",
      "Epoch 1 | Step 467300 | Avg Loss: 0.0158 | Grad Norm: 0.00858054\n",
      "Epoch 1 | Step 467400 | Avg Loss: 0.0157 | Grad Norm: 0.01085619\n",
      "Epoch 1 | Step 467500 | Avg Loss: 0.0155 | Grad Norm: 0.00865047\n",
      "Epoch 1 | Step 467600 | Avg Loss: 0.0157 | Grad Norm: 0.00903952\n",
      "Epoch 1 | Step 467700 | Avg Loss: 0.0156 | Grad Norm: 0.00945841\n",
      "Epoch 1 | Step 467800 | Avg Loss: 0.0158 | Grad Norm: 0.00849419\n",
      "Epoch 1 | Step 467900 | Avg Loss: 0.0159 | Grad Norm: 0.00857169\n",
      "Epoch 1 | Step 468000 | Avg Loss: 0.0156 | Grad Norm: 0.00882713\n",
      "Epoch 1 | Step 468100 | Avg Loss: 0.0153 | Grad Norm: 0.01000543\n",
      "Epoch 1 | Step 468200 | Avg Loss: 0.0155 | Grad Norm: 0.00907159\n",
      "Epoch 1 | Step 468300 | Avg Loss: 0.0157 | Grad Norm: 0.00968716\n",
      "Epoch 1 | Step 468400 | Avg Loss: 0.0160 | Grad Norm: 0.00924487\n",
      "Epoch 1 | Step 468500 | Avg Loss: 0.0157 | Grad Norm: 0.00994334\n",
      "Epoch 1 | Step 468600 | Avg Loss: 0.0161 | Grad Norm: 0.00866350\n",
      "Epoch 1 | Step 468700 | Avg Loss: 0.0159 | Grad Norm: 0.00759132\n",
      "Epoch 1 | Step 468800 | Avg Loss: 0.0155 | Grad Norm: 0.00868550\n",
      "Epoch 1 | Step 468900 | Avg Loss: 0.0155 | Grad Norm: 0.00828364\n",
      "Epoch 1 | Step 469000 | Avg Loss: 0.0152 | Grad Norm: 0.00872144\n",
      "Epoch 1 | Step 469100 | Avg Loss: 0.0156 | Grad Norm: 0.00770470\n",
      "Epoch 1 | Step 469200 | Avg Loss: 0.0159 | Grad Norm: 0.00884945\n",
      "Epoch 1 | Step 469300 | Avg Loss: 0.0158 | Grad Norm: 0.00970935\n",
      "Epoch 1 | Step 469400 | Avg Loss: 0.0157 | Grad Norm: 0.00922683\n",
      "Epoch 1 | Step 469500 | Avg Loss: 0.0159 | Grad Norm: 0.00866088\n",
      "Epoch 1 | Step 469600 | Avg Loss: 0.0156 | Grad Norm: 0.00780639\n",
      "Epoch 1 | Step 469700 | Avg Loss: 0.0160 | Grad Norm: 0.00907587\n",
      "Epoch 1 | Step 469800 | Avg Loss: 0.0158 | Grad Norm: 0.00958111\n",
      "Epoch 1 | Step 469900 | Avg Loss: 0.0159 | Grad Norm: 0.00907198\n",
      "Epoch 1 | Step 470000 | Avg Loss: 0.0156 | Grad Norm: 0.00832577\n",
      "Epoch 1 | Step 470100 | Avg Loss: 0.0157 | Grad Norm: 0.00886271\n",
      "Epoch 1 | Step 470200 | Avg Loss: 0.0157 | Grad Norm: 0.00871276\n",
      "Epoch 1 | Step 470300 | Avg Loss: 0.0158 | Grad Norm: 0.00899310\n",
      "Epoch 1 | Step 470400 | Avg Loss: 0.0156 | Grad Norm: 0.01037644\n",
      "Epoch 1 | Step 470500 | Avg Loss: 0.0155 | Grad Norm: 0.00817397\n",
      "Epoch 1 | Step 470600 | Avg Loss: 0.0156 | Grad Norm: 0.00815626\n",
      "Epoch 1 | Step 470700 | Avg Loss: 0.0154 | Grad Norm: 0.00852420\n",
      "Epoch 1 | Step 470800 | Avg Loss: 0.0154 | Grad Norm: 0.00985091\n",
      "Epoch 1 | Step 470900 | Avg Loss: 0.0153 | Grad Norm: 0.00874377\n",
      "Epoch 1 | Step 471000 | Avg Loss: 0.0152 | Grad Norm: 0.00695743\n",
      "Epoch 1 | Step 471100 | Avg Loss: 0.0155 | Grad Norm: 0.00867669\n",
      "Epoch 1 | Step 471200 | Avg Loss: 0.0158 | Grad Norm: 0.00871806\n",
      "Epoch 1 | Step 471300 | Avg Loss: 0.0155 | Grad Norm: 0.00929435\n",
      "Epoch 1 | Step 471400 | Avg Loss: 0.0158 | Grad Norm: 0.00931849\n",
      "Epoch 1 | Step 471500 | Avg Loss: 0.0152 | Grad Norm: 0.00985746\n",
      "Epoch 1 | Step 471600 | Avg Loss: 0.0154 | Grad Norm: 0.00835733\n",
      "Epoch 1 | Step 471700 | Avg Loss: 0.0156 | Grad Norm: 0.00880077\n",
      "Epoch 1 | Step 471800 | Avg Loss: 0.0158 | Grad Norm: 0.00957464\n",
      "Epoch 1 | Step 471900 | Avg Loss: 0.0161 | Grad Norm: 0.00931201\n",
      "Epoch 1 | Step 472000 | Avg Loss: 0.0157 | Grad Norm: 0.00955047\n",
      "Epoch 1 | Step 472100 | Avg Loss: 0.0154 | Grad Norm: 0.00857547\n",
      "Epoch 1 | Step 472200 | Avg Loss: 0.0152 | Grad Norm: 0.00776079\n",
      "Epoch 1 | Step 472300 | Avg Loss: 0.0153 | Grad Norm: 0.00768191\n",
      "Epoch 1 | Step 472400 | Avg Loss: 0.0154 | Grad Norm: 0.00922241\n",
      "Epoch 1 | Step 472500 | Avg Loss: 0.0155 | Grad Norm: 0.00749008\n",
      "Epoch 1 | Step 472600 | Avg Loss: 0.0160 | Grad Norm: 0.00844773\n",
      "Epoch 1 | Step 472700 | Avg Loss: 0.0157 | Grad Norm: 0.00864824\n",
      "Epoch 1 | Step 472800 | Avg Loss: 0.0157 | Grad Norm: 0.00903098\n",
      "Epoch 1 | Step 472900 | Avg Loss: 0.0157 | Grad Norm: 0.00841036\n",
      "Epoch 1 | Step 473000 | Avg Loss: 0.0157 | Grad Norm: 0.01062440\n",
      "Epoch 1 | Step 473100 | Avg Loss: 0.0157 | Grad Norm: 0.00841772\n",
      "Epoch 1 | Step 473200 | Avg Loss: 0.0151 | Grad Norm: 0.00895778\n",
      "Epoch 1 | Step 473300 | Avg Loss: 0.0155 | Grad Norm: 0.00905903\n",
      "Epoch 1 | Step 473400 | Avg Loss: 0.0154 | Grad Norm: 0.00827204\n",
      "Epoch 1 | Step 473500 | Avg Loss: 0.0155 | Grad Norm: 0.00875897\n",
      "Epoch 1 | Step 473600 | Avg Loss: 0.0153 | Grad Norm: 0.00860738\n",
      "Epoch 1 | Step 473700 | Avg Loss: 0.0157 | Grad Norm: 0.00768108\n",
      "Epoch 1 | Step 473800 | Avg Loss: 0.0158 | Grad Norm: 0.01135325\n",
      "Epoch 1 | Step 473900 | Avg Loss: 0.0159 | Grad Norm: 0.00780163\n",
      "Epoch 1 | Step 474000 | Avg Loss: 0.0158 | Grad Norm: 0.00815251\n",
      "Epoch 1 | Step 474100 | Avg Loss: 0.0158 | Grad Norm: 0.00679989\n",
      "Epoch 1 | Step 474200 | Avg Loss: 0.0160 | Grad Norm: 0.00866807\n",
      "Epoch 1 | Step 474300 | Avg Loss: 0.0161 | Grad Norm: 0.00742533\n",
      "Epoch 1 | Step 474400 | Avg Loss: 0.0161 | Grad Norm: 0.00834594\n",
      "Epoch 1 | Step 474500 | Avg Loss: 0.0166 | Grad Norm: 0.00942717\n",
      "Epoch 1 | Step 474600 | Avg Loss: 0.0164 | Grad Norm: 0.00841069\n",
      "Epoch 1 | Step 474700 | Avg Loss: 0.0160 | Grad Norm: 0.00933624\n",
      "Epoch 1 | Step 474800 | Avg Loss: 0.0164 | Grad Norm: 0.00741840\n",
      "Epoch 1 | Step 474900 | Avg Loss: 0.0160 | Grad Norm: 0.00851904\n",
      "Epoch 1 | Step 475000 | Avg Loss: 0.0162 | Grad Norm: 0.00894580\n",
      "Epoch 1 | Step 475100 | Avg Loss: 0.0163 | Grad Norm: 0.00948471\n",
      "Epoch 1 | Step 475200 | Avg Loss: 0.0158 | Grad Norm: 0.00839449\n",
      "Epoch 1 | Step 475300 | Avg Loss: 0.0158 | Grad Norm: 0.00914073\n",
      "Epoch 1 | Step 475400 | Avg Loss: 0.0156 | Grad Norm: 0.00852048\n",
      "Epoch 1 | Step 475500 | Avg Loss: 0.0153 | Grad Norm: 0.00764115\n",
      "Epoch 1 | Step 475600 | Avg Loss: 0.0150 | Grad Norm: 0.00806688\n",
      "Epoch 1 | Step 475700 | Avg Loss: 0.0152 | Grad Norm: 0.00758649\n",
      "Epoch 1 | Step 475800 | Avg Loss: 0.0151 | Grad Norm: 0.00845609\n",
      "Epoch 1 | Step 475900 | Avg Loss: 0.0152 | Grad Norm: 0.01117781\n",
      "Epoch 1 | Step 476000 | Avg Loss: 0.0151 | Grad Norm: 0.00707138\n",
      "Epoch 1 | Step 476100 | Avg Loss: 0.0153 | Grad Norm: 0.01171072\n",
      "Epoch 1 | Step 476200 | Avg Loss: 0.0153 | Grad Norm: 0.00824095\n",
      "Epoch 1 | Step 476300 | Avg Loss: 0.0156 | Grad Norm: 0.00894529\n",
      "Epoch 1 | Step 476400 | Avg Loss: 0.0157 | Grad Norm: 0.00895964\n",
      "Epoch 1 | Step 476500 | Avg Loss: 0.0161 | Grad Norm: 0.00913075\n",
      "Epoch 1 | Step 476600 | Avg Loss: 0.0160 | Grad Norm: 0.00885596\n",
      "Epoch 1 | Step 476700 | Avg Loss: 0.0157 | Grad Norm: 0.00813547\n",
      "Epoch 1 | Step 476800 | Avg Loss: 0.0158 | Grad Norm: 0.01076292\n",
      "Epoch 1 | Step 476900 | Avg Loss: 0.0157 | Grad Norm: 0.00961456\n",
      "Epoch 1 | Step 477000 | Avg Loss: 0.0158 | Grad Norm: 0.00848899\n",
      "Epoch 1 | Step 477100 | Avg Loss: 0.0156 | Grad Norm: 0.00762696\n",
      "Epoch 1 | Step 477200 | Avg Loss: 0.0156 | Grad Norm: 0.00921502\n",
      "Epoch 1 | Step 477300 | Avg Loss: 0.0157 | Grad Norm: 0.00752032\n",
      "Epoch 1 | Step 477400 | Avg Loss: 0.0158 | Grad Norm: 0.00838473\n",
      "Epoch 1 | Step 477500 | Avg Loss: 0.0161 | Grad Norm: 0.00934762\n",
      "Epoch 1 | Step 477600 | Avg Loss: 0.0162 | Grad Norm: 0.00993717\n",
      "Epoch 1 | Step 477700 | Avg Loss: 0.0157 | Grad Norm: 0.00840962\n",
      "Epoch 1 | Step 477800 | Avg Loss: 0.0159 | Grad Norm: 0.00888711\n",
      "Epoch 1 | Step 477900 | Avg Loss: 0.0159 | Grad Norm: 0.01059070\n",
      "Epoch 1 | Step 478000 | Avg Loss: 0.0162 | Grad Norm: 0.00887080\n",
      "Epoch 1 | Step 478100 | Avg Loss: 0.0163 | Grad Norm: 0.00903753\n",
      "Epoch 1 | Step 478200 | Avg Loss: 0.0162 | Grad Norm: 0.00819938\n",
      "Epoch 1 | Step 478300 | Avg Loss: 0.0161 | Grad Norm: 0.00923848\n",
      "Epoch 1 | Step 478400 | Avg Loss: 0.0159 | Grad Norm: 0.00744697\n",
      "Epoch 1 | Step 478500 | Avg Loss: 0.0161 | Grad Norm: 0.00977941\n",
      "Epoch 1 | Step 478600 | Avg Loss: 0.0161 | Grad Norm: 0.00852576\n",
      "Epoch 1 | Step 478700 | Avg Loss: 0.0160 | Grad Norm: 0.00870117\n",
      "Epoch 1 | Step 478800 | Avg Loss: 0.0156 | Grad Norm: 0.00887883\n",
      "Epoch 1 | Step 478900 | Avg Loss: 0.0154 | Grad Norm: 0.00884316\n",
      "Epoch 1 | Step 479000 | Avg Loss: 0.0156 | Grad Norm: 0.00711206\n",
      "Epoch 1 | Step 479100 | Avg Loss: 0.0157 | Grad Norm: 0.00879801\n",
      "Epoch 1 | Step 479200 | Avg Loss: 0.0153 | Grad Norm: 0.00847631\n",
      "Epoch 1 | Step 479300 | Avg Loss: 0.0153 | Grad Norm: 0.00901658\n",
      "Epoch 1 | Step 479400 | Avg Loss: 0.0157 | Grad Norm: 0.00832465\n",
      "Epoch 1 | Step 479500 | Avg Loss: 0.0162 | Grad Norm: 0.00923339\n",
      "Epoch 1 | Step 479600 | Avg Loss: 0.0161 | Grad Norm: 0.00869006\n",
      "Epoch 1 | Step 479700 | Avg Loss: 0.0161 | Grad Norm: 0.00947835\n",
      "Epoch 1 | Step 479800 | Avg Loss: 0.0162 | Grad Norm: 0.00808682\n",
      "Epoch 1 | Step 479900 | Avg Loss: 0.0161 | Grad Norm: 0.00833929\n",
      "Epoch 1 | Step 480000 | Avg Loss: 0.0167 | Grad Norm: 0.00833480\n",
      "Epoch 1 | Step 480100 | Avg Loss: 0.0166 | Grad Norm: 0.00938114\n",
      "Epoch 1 | Step 480200 | Avg Loss: 0.0163 | Grad Norm: 0.00802404\n",
      "Epoch 1 | Step 480300 | Avg Loss: 0.0163 | Grad Norm: 0.01098592\n",
      "Epoch 1 | Step 480400 | Avg Loss: 0.0159 | Grad Norm: 0.01036647\n",
      "Epoch 1 | Step 480500 | Avg Loss: 0.0155 | Grad Norm: 0.00820985\n",
      "Epoch 1 | Step 480600 | Avg Loss: 0.0157 | Grad Norm: 0.00921040\n",
      "Epoch 1 | Step 480700 | Avg Loss: 0.0152 | Grad Norm: 0.00848739\n",
      "Epoch 1 | Step 480800 | Avg Loss: 0.0157 | Grad Norm: 0.00769210\n",
      "Epoch 1 | Step 480900 | Avg Loss: 0.0156 | Grad Norm: 0.00992358\n",
      "Epoch 1 | Step 481000 | Avg Loss: 0.0156 | Grad Norm: 0.00988722\n",
      "Epoch 1 | Step 481100 | Avg Loss: 0.0162 | Grad Norm: 0.00951710\n",
      "Epoch 1 | Step 481200 | Avg Loss: 0.0159 | Grad Norm: 0.00937269\n",
      "Epoch 1 | Step 481300 | Avg Loss: 0.0163 | Grad Norm: 0.00799027\n",
      "Epoch 1 | Step 481400 | Avg Loss: 0.0165 | Grad Norm: 0.01057784\n",
      "Epoch 1 | Step 481500 | Avg Loss: 0.0161 | Grad Norm: 0.00819543\n",
      "Epoch 1 | Step 481600 | Avg Loss: 0.0161 | Grad Norm: 0.00972330\n",
      "Epoch 1 | Step 481700 | Avg Loss: 0.0161 | Grad Norm: 0.00904930\n",
      "Epoch 1 | Step 481800 | Avg Loss: 0.0161 | Grad Norm: 0.00981692\n",
      "Epoch 1 | Step 481900 | Avg Loss: 0.0156 | Grad Norm: 0.00805508\n",
      "Epoch 1 | Step 482000 | Avg Loss: 0.0150 | Grad Norm: 0.00779936\n",
      "Epoch 1 | Step 482100 | Avg Loss: 0.0154 | Grad Norm: 0.00955158\n",
      "Epoch 1 | Step 482200 | Avg Loss: 0.0155 | Grad Norm: 0.00909208\n",
      "Epoch 1 | Step 482300 | Avg Loss: 0.0149 | Grad Norm: 0.00851438\n",
      "Epoch 1 | Step 482400 | Avg Loss: 0.0156 | Grad Norm: 0.00867440\n",
      "Epoch 1 | Step 482500 | Avg Loss: 0.0161 | Grad Norm: 0.00812834\n",
      "Epoch 1 | Step 482600 | Avg Loss: 0.0160 | Grad Norm: 0.00883816\n",
      "Epoch 1 | Step 482700 | Avg Loss: 0.0163 | Grad Norm: 0.00828664\n",
      "Epoch 1 | Step 482800 | Avg Loss: 0.0160 | Grad Norm: 0.00927586\n",
      "Epoch 1 | Step 482900 | Avg Loss: 0.0161 | Grad Norm: 0.00949380\n",
      "Epoch 1 | Step 483000 | Avg Loss: 0.0161 | Grad Norm: 0.00884986\n",
      "Epoch 1 | Step 483100 | Avg Loss: 0.0158 | Grad Norm: 0.00796668\n",
      "Epoch 1 | Step 483200 | Avg Loss: 0.0159 | Grad Norm: 0.00918871\n",
      "Epoch 1 | Step 483300 | Avg Loss: 0.0159 | Grad Norm: 0.00790912\n",
      "Epoch 1 | Step 483400 | Avg Loss: 0.0158 | Grad Norm: 0.01110346\n",
      "Epoch 1 | Step 483500 | Avg Loss: 0.0154 | Grad Norm: 0.00922079\n",
      "Epoch 1 | Step 483600 | Avg Loss: 0.0155 | Grad Norm: 0.00778317\n",
      "Epoch 1 | Step 483700 | Avg Loss: 0.0154 | Grad Norm: 0.00841595\n",
      "Epoch 1 | Step 483800 | Avg Loss: 0.0156 | Grad Norm: 0.00775822\n",
      "Epoch 1 | Step 483900 | Avg Loss: 0.0154 | Grad Norm: 0.00852911\n",
      "Epoch 1 | Step 484000 | Avg Loss: 0.0157 | Grad Norm: 0.01029358\n",
      "Epoch 1 | Step 484100 | Avg Loss: 0.0153 | Grad Norm: 0.00866147\n",
      "Epoch 1 | Step 484200 | Avg Loss: 0.0157 | Grad Norm: 0.00913979\n",
      "Epoch 1 | Step 484300 | Avg Loss: 0.0157 | Grad Norm: 0.00873175\n",
      "Epoch 1 | Step 484400 | Avg Loss: 0.0157 | Grad Norm: 0.00846387\n",
      "Epoch 1 | Step 484500 | Avg Loss: 0.0157 | Grad Norm: 0.00839945\n",
      "Epoch 1 | Step 484600 | Avg Loss: 0.0156 | Grad Norm: 0.00800464\n",
      "Epoch 1 | Step 484700 | Avg Loss: 0.0157 | Grad Norm: 0.00868051\n",
      "Epoch 1 | Step 484800 | Avg Loss: 0.0156 | Grad Norm: 0.00805487\n",
      "Epoch 1 | Step 484900 | Avg Loss: 0.0158 | Grad Norm: 0.01301232\n",
      "Epoch 1 | Step 485000 | Avg Loss: 0.0156 | Grad Norm: 0.01027058\n",
      "Epoch 1 | Step 485100 | Avg Loss: 0.0155 | Grad Norm: 0.00795312\n",
      "Epoch 1 | Step 485200 | Avg Loss: 0.0154 | Grad Norm: 0.00715574\n",
      "Epoch 1 | Step 485300 | Avg Loss: 0.0152 | Grad Norm: 0.00778139\n",
      "Epoch 1 | Step 485400 | Avg Loss: 0.0154 | Grad Norm: 0.00835708\n",
      "Epoch 1 | Step 485500 | Avg Loss: 0.0155 | Grad Norm: 0.00836737\n",
      "Epoch 1 | Step 485600 | Avg Loss: 0.0154 | Grad Norm: 0.00730366\n",
      "Epoch 1 | Step 485700 | Avg Loss: 0.0152 | Grad Norm: 0.00856486\n",
      "Epoch 1 | Step 485800 | Avg Loss: 0.0155 | Grad Norm: 0.00809097\n",
      "Epoch 1 | Step 485900 | Avg Loss: 0.0153 | Grad Norm: 0.00853684\n",
      "Epoch 1 | Step 486000 | Avg Loss: 0.0153 | Grad Norm: 0.00853716\n",
      "Epoch 1 | Step 486100 | Avg Loss: 0.0156 | Grad Norm: 0.00886280\n",
      "Epoch 1 | Step 486200 | Avg Loss: 0.0157 | Grad Norm: 0.00889352\n",
      "Epoch 1 | Step 486300 | Avg Loss: 0.0157 | Grad Norm: 0.00869866\n",
      "Epoch 1 | Step 486400 | Avg Loss: 0.0164 | Grad Norm: 0.00967180\n",
      "Epoch 1 | Step 486500 | Avg Loss: 0.0158 | Grad Norm: 0.00847294\n",
      "Epoch 1 | Step 486600 | Avg Loss: 0.0157 | Grad Norm: 0.00783035\n",
      "Epoch 1 | Step 486700 | Avg Loss: 0.0156 | Grad Norm: 0.01059416\n",
      "Epoch 1 | Step 486800 | Avg Loss: 0.0156 | Grad Norm: 0.01049082\n",
      "Epoch 1 | Step 486900 | Avg Loss: 0.0157 | Grad Norm: 0.00742694\n",
      "Epoch 1 | Step 487000 | Avg Loss: 0.0159 | Grad Norm: 0.00853439\n",
      "Epoch 1 | Step 487100 | Avg Loss: 0.0161 | Grad Norm: 0.00822258\n",
      "Epoch 1 | Step 487200 | Avg Loss: 0.0160 | Grad Norm: 0.00861914\n",
      "Epoch 1 | Step 487300 | Avg Loss: 0.0156 | Grad Norm: 0.01071334\n",
      "Epoch 1 | Step 487400 | Avg Loss: 0.0156 | Grad Norm: 0.00868107\n",
      "Epoch 1 | Step 487500 | Avg Loss: 0.0152 | Grad Norm: 0.00744581\n",
      "Epoch 1 | Step 487600 | Avg Loss: 0.0150 | Grad Norm: 0.00868604\n",
      "Epoch 1 | Step 487700 | Avg Loss: 0.0148 | Grad Norm: 0.00951934\n",
      "Epoch 1 | Step 487800 | Avg Loss: 0.0153 | Grad Norm: 0.00819941\n",
      "Epoch 1 | Step 487900 | Avg Loss: 0.0153 | Grad Norm: 0.00886877\n",
      "Epoch 1 | Step 488000 | Avg Loss: 0.0152 | Grad Norm: 0.00973532\n",
      "Epoch 1 | Step 488100 | Avg Loss: 0.0153 | Grad Norm: 0.00749725\n",
      "Epoch 1 | Step 488200 | Avg Loss: 0.0152 | Grad Norm: 0.00934398\n",
      "Epoch 1 | Step 488300 | Avg Loss: 0.0153 | Grad Norm: 0.00861454\n",
      "Epoch 1 | Step 488400 | Avg Loss: 0.0158 | Grad Norm: 0.00886725\n",
      "Epoch 1 | Step 488500 | Avg Loss: 0.0155 | Grad Norm: 0.00786764\n",
      "Epoch 1 | Step 488600 | Avg Loss: 0.0155 | Grad Norm: 0.01018807\n",
      "Epoch 1 | Step 488700 | Avg Loss: 0.0158 | Grad Norm: 0.00892450\n",
      "Epoch 1 | Step 488800 | Avg Loss: 0.0161 | Grad Norm: 0.00772500\n",
      "Epoch 1 | Step 488900 | Avg Loss: 0.0165 | Grad Norm: 0.00928125\n",
      "Epoch 1 | Step 489000 | Avg Loss: 0.0163 | Grad Norm: 0.00947219\n",
      "Epoch 1 | Step 489100 | Avg Loss: 0.0158 | Grad Norm: 0.00823933\n",
      "Epoch 1 | Step 489200 | Avg Loss: 0.0156 | Grad Norm: 0.00874578\n",
      "Epoch 1 | Step 489300 | Avg Loss: 0.0158 | Grad Norm: 0.01230062\n",
      "Epoch 1 | Step 489400 | Avg Loss: 0.0157 | Grad Norm: 0.00855080\n",
      "Epoch 1 | Step 489500 | Avg Loss: 0.0158 | Grad Norm: 0.00936756\n",
      "Epoch 1 | Step 489600 | Avg Loss: 0.0156 | Grad Norm: 0.00920443\n",
      "Epoch 1 | Step 489700 | Avg Loss: 0.0157 | Grad Norm: 0.00860883\n",
      "Epoch 1 | Step 489800 | Avg Loss: 0.0158 | Grad Norm: 0.00834614\n",
      "Epoch 1 | Step 489900 | Avg Loss: 0.0160 | Grad Norm: 0.00894422\n",
      "Epoch 1 | Step 490000 | Avg Loss: 0.0160 | Grad Norm: 0.01032764\n",
      "Epoch 1 | Step 490100 | Avg Loss: 0.0165 | Grad Norm: 0.00825809\n",
      "Epoch 1 | Step 490200 | Avg Loss: 0.0166 | Grad Norm: 0.00912599\n",
      "Epoch 1 | Step 490300 | Avg Loss: 0.0165 | Grad Norm: 0.00975577\n",
      "Epoch 1 | Step 490400 | Avg Loss: 0.0164 | Grad Norm: 0.00793355\n",
      "Epoch 1 | Step 490500 | Avg Loss: 0.0163 | Grad Norm: 0.01208165\n",
      "Epoch 1 | Step 490600 | Avg Loss: 0.0162 | Grad Norm: 0.00864458\n",
      "Epoch 1 | Step 490700 | Avg Loss: 0.0159 | Grad Norm: 0.00986040\n",
      "Epoch 1 | Step 490800 | Avg Loss: 0.0157 | Grad Norm: 0.01008343\n",
      "Epoch 1 | Step 490900 | Avg Loss: 0.0158 | Grad Norm: 0.00764198\n",
      "Epoch 1 | Step 491000 | Avg Loss: 0.0159 | Grad Norm: 0.01004851\n",
      "Epoch 1 | Step 491100 | Avg Loss: 0.0160 | Grad Norm: 0.00935704\n",
      "Epoch 1 | Step 491200 | Avg Loss: 0.0157 | Grad Norm: 0.00908469\n",
      "Epoch 1 | Step 491300 | Avg Loss: 0.0156 | Grad Norm: 0.01001414\n",
      "Epoch 1 | Step 491400 | Avg Loss: 0.0153 | Grad Norm: 0.01024111\n",
      "Epoch 1 | Step 491500 | Avg Loss: 0.0155 | Grad Norm: 0.00763088\n",
      "Epoch 1 | Step 491600 | Avg Loss: 0.0158 | Grad Norm: 0.00815618\n",
      "Epoch 1 | Step 491700 | Avg Loss: 0.0156 | Grad Norm: 0.00908004\n",
      "Epoch 1 | Step 491800 | Avg Loss: 0.0157 | Grad Norm: 0.00972845\n",
      "Epoch 1 | Step 491900 | Avg Loss: 0.0153 | Grad Norm: 0.00954865\n",
      "Epoch 1 | Step 492000 | Avg Loss: 0.0151 | Grad Norm: 0.01030517\n",
      "Epoch 1 | Step 492100 | Avg Loss: 0.0154 | Grad Norm: 0.00809321\n",
      "Epoch 1 | Step 492200 | Avg Loss: 0.0154 | Grad Norm: 0.00920027\n",
      "Epoch 1 | Step 492300 | Avg Loss: 0.0152 | Grad Norm: 0.00800412\n",
      "Epoch 1 | Step 492400 | Avg Loss: 0.0153 | Grad Norm: 0.00872016\n",
      "Epoch 1 | Step 492500 | Avg Loss: 0.0158 | Grad Norm: 0.00900304\n",
      "Epoch 1 | Step 492600 | Avg Loss: 0.0159 | Grad Norm: 0.00898846\n",
      "Epoch 1 | Step 492700 | Avg Loss: 0.0157 | Grad Norm: 0.00841192\n",
      "Epoch 1 | Step 492800 | Avg Loss: 0.0155 | Grad Norm: 0.00894627\n",
      "Epoch 1 | Step 492900 | Avg Loss: 0.0155 | Grad Norm: 0.00792891\n",
      "Epoch 1 | Step 493000 | Avg Loss: 0.0158 | Grad Norm: 0.00791871\n",
      "Epoch 1 | Step 493100 | Avg Loss: 0.0159 | Grad Norm: 0.00943382\n",
      "Epoch 1 | Step 493200 | Avg Loss: 0.0161 | Grad Norm: 0.00878603\n",
      "Epoch 1 | Step 493300 | Avg Loss: 0.0159 | Grad Norm: 0.00811579\n",
      "Epoch 1 | Step 493400 | Avg Loss: 0.0159 | Grad Norm: 0.00830731\n",
      "Epoch 1 | Step 493500 | Avg Loss: 0.0162 | Grad Norm: 0.00837546\n",
      "Epoch 1 | Step 493600 | Avg Loss: 0.0159 | Grad Norm: 0.00834036\n",
      "Epoch 1 | Step 493700 | Avg Loss: 0.0159 | Grad Norm: 0.00847093\n",
      "Epoch 1 | Step 493800 | Avg Loss: 0.0161 | Grad Norm: 0.00875933\n",
      "Epoch 1 | Step 493900 | Avg Loss: 0.0158 | Grad Norm: 0.00781752\n",
      "Epoch 1 | Step 494000 | Avg Loss: 0.0157 | Grad Norm: 0.01027404\n",
      "Epoch 1 | Step 494100 | Avg Loss: 0.0157 | Grad Norm: 0.00838118\n",
      "Epoch 1 | Step 494200 | Avg Loss: 0.0157 | Grad Norm: 0.01076311\n",
      "Epoch 1 | Step 494300 | Avg Loss: 0.0157 | Grad Norm: 0.00833583\n",
      "Epoch 1 | Step 494400 | Avg Loss: 0.0153 | Grad Norm: 0.00850135\n",
      "Epoch 1 | Step 494500 | Avg Loss: 0.0153 | Grad Norm: 0.00880031\n",
      "Epoch 1 | Step 494600 | Avg Loss: 0.0155 | Grad Norm: 0.00872270\n",
      "Epoch 1 | Step 494700 | Avg Loss: 0.0152 | Grad Norm: 0.01019610\n",
      "Epoch 1 | Step 494800 | Avg Loss: 0.0156 | Grad Norm: 0.00857120\n",
      "Epoch 1 | Step 494900 | Avg Loss: 0.0160 | Grad Norm: 0.00866042\n",
      "Epoch 1 | Step 495000 | Avg Loss: 0.0160 | Grad Norm: 0.00863184\n",
      "Epoch 1 | Step 495100 | Avg Loss: 0.0161 | Grad Norm: 0.01147296\n",
      "Epoch 1 | Step 495200 | Avg Loss: 0.0161 | Grad Norm: 0.01055861\n",
      "Epoch 1 | Step 495300 | Avg Loss: 0.0161 | Grad Norm: 0.00894753\n",
      "Epoch 1 | Step 495400 | Avg Loss: 0.0161 | Grad Norm: 0.01001783\n",
      "Epoch 1 | Step 495500 | Avg Loss: 0.0162 | Grad Norm: 0.00927782\n",
      "Epoch 1 | Step 495600 | Avg Loss: 0.0158 | Grad Norm: 0.00978555\n",
      "Epoch 1 | Step 495700 | Avg Loss: 0.0159 | Grad Norm: 0.00918194\n",
      "Epoch 1 | Step 495800 | Avg Loss: 0.0156 | Grad Norm: 0.01043011\n",
      "Epoch 1 | Step 495900 | Avg Loss: 0.0157 | Grad Norm: 0.00869736\n",
      "Epoch 1 | Step 496000 | Avg Loss: 0.0155 | Grad Norm: 0.00806483\n",
      "Epoch 1 | Step 496100 | Avg Loss: 0.0154 | Grad Norm: 0.00866913\n",
      "Epoch 1 | Step 496200 | Avg Loss: 0.0154 | Grad Norm: 0.00936753\n",
      "Epoch 1 | Step 496300 | Avg Loss: 0.0158 | Grad Norm: 0.00891382\n",
      "Epoch 1 | Step 496400 | Avg Loss: 0.0152 | Grad Norm: 0.00865289\n",
      "Epoch 1 | Step 496500 | Avg Loss: 0.0152 | Grad Norm: 0.00880448\n",
      "Epoch 1 | Step 496600 | Avg Loss: 0.0157 | Grad Norm: 0.00901935\n",
      "Epoch 1 | Step 496700 | Avg Loss: 0.0161 | Grad Norm: 0.00964180\n",
      "Epoch 1 | Step 496800 | Avg Loss: 0.0160 | Grad Norm: 0.00720882\n",
      "Epoch 1 | Step 496900 | Avg Loss: 0.0161 | Grad Norm: 0.00904949\n",
      "Epoch 1 | Step 497000 | Avg Loss: 0.0156 | Grad Norm: 0.00813032\n",
      "Epoch 1 | Step 497100 | Avg Loss: 0.0156 | Grad Norm: 0.00916009\n",
      "Epoch 1 | Step 497200 | Avg Loss: 0.0158 | Grad Norm: 0.00834892\n",
      "Epoch 1 | Step 497300 | Avg Loss: 0.0155 | Grad Norm: 0.00815817\n",
      "Epoch 1 | Step 497400 | Avg Loss: 0.0156 | Grad Norm: 0.01166159\n",
      "Epoch 1 | Step 497500 | Avg Loss: 0.0155 | Grad Norm: 0.00974710\n",
      "Epoch 1 | Step 497600 | Avg Loss: 0.0155 | Grad Norm: 0.00934240\n",
      "Epoch 1 | Step 497700 | Avg Loss: 0.0152 | Grad Norm: 0.00861450\n",
      "Epoch 1 | Step 497800 | Avg Loss: 0.0152 | Grad Norm: 0.00827467\n",
      "Epoch 1 | Step 497900 | Avg Loss: 0.0150 | Grad Norm: 0.00903876\n",
      "Epoch 1 | Step 498000 | Avg Loss: 0.0151 | Grad Norm: 0.00793869\n",
      "Epoch 1 | Step 498100 | Avg Loss: 0.0150 | Grad Norm: 0.00856999\n",
      "Epoch 1 | Step 498200 | Avg Loss: 0.0153 | Grad Norm: 0.00957885\n",
      "Epoch 1 | Step 498300 | Avg Loss: 0.0148 | Grad Norm: 0.00777798\n",
      "Epoch 1 | Step 498400 | Avg Loss: 0.0151 | Grad Norm: 0.00846416\n",
      "Epoch 1 | Step 498500 | Avg Loss: 0.0153 | Grad Norm: 0.00944659\n",
      "Epoch 1 | Step 498600 | Avg Loss: 0.0154 | Grad Norm: 0.00753227\n",
      "Epoch 1 | Step 498700 | Avg Loss: 0.0159 | Grad Norm: 0.00901308\n",
      "Epoch 1 | Step 498800 | Avg Loss: 0.0161 | Grad Norm: 0.00908285\n",
      "Epoch 1 | Step 498900 | Avg Loss: 0.0157 | Grad Norm: 0.00834578\n",
      "Epoch 1 | Step 499000 | Avg Loss: 0.0157 | Grad Norm: 0.01064596\n",
      "Epoch 1 | Step 499100 | Avg Loss: 0.0161 | Grad Norm: 0.00889852\n",
      "Epoch 1 | Step 499200 | Avg Loss: 0.0162 | Grad Norm: 0.00804474\n",
      "Epoch 1 | Step 499300 | Avg Loss: 0.0157 | Grad Norm: 0.01026596\n",
      "Epoch 1 | Step 499400 | Avg Loss: 0.0157 | Grad Norm: 0.00816441\n",
      "Epoch 1 | Step 499500 | Avg Loss: 0.0159 | Grad Norm: 0.01003617\n",
      "Epoch 1 | Step 499600 | Avg Loss: 0.0162 | Grad Norm: 0.00950079\n",
      "Epoch 1 | Step 499700 | Avg Loss: 0.0161 | Grad Norm: 0.00794099\n",
      "Epoch 1 | Step 499800 | Avg Loss: 0.0161 | Grad Norm: 0.01046864\n",
      "Epoch 1 | Step 499900 | Avg Loss: 0.0163 | Grad Norm: 0.00864297\n",
      "Epoch 1 | Step 500000 | Avg Loss: 0.0164 | Grad Norm: 0.00814254\n",
      "Saving model at step500000\n",
      "Epoch 1 | Step 500100 | Avg Loss: 0.0163 | Grad Norm: 0.00852397\n",
      "Epoch 1 | Step 500200 | Avg Loss: 0.0159 | Grad Norm: 0.00840723\n",
      "Epoch 1 | Step 500300 | Avg Loss: 0.0162 | Grad Norm: 0.00858195\n",
      "Epoch 1 | Step 500400 | Avg Loss: 0.0160 | Grad Norm: 0.00997082\n",
      "Epoch 1 | Step 500500 | Avg Loss: 0.0158 | Grad Norm: 0.00863598\n",
      "Epoch 1 | Step 500600 | Avg Loss: 0.0158 | Grad Norm: 0.00872545\n",
      "Epoch 1 | Step 500700 | Avg Loss: 0.0158 | Grad Norm: 0.00834946\n",
      "Epoch 1 | Step 500800 | Avg Loss: 0.0156 | Grad Norm: 0.00989777\n",
      "Epoch 1 | Step 500900 | Avg Loss: 0.0154 | Grad Norm: 0.00946343\n",
      "Epoch 1 | Step 501000 | Avg Loss: 0.0155 | Grad Norm: 0.00929994\n",
      "Epoch 1 | Step 501100 | Avg Loss: 0.0158 | Grad Norm: 0.00926615\n",
      "Epoch 1 | Step 501200 | Avg Loss: 0.0155 | Grad Norm: 0.00966419\n",
      "Epoch 1 | Step 501300 | Avg Loss: 0.0156 | Grad Norm: 0.00801485\n",
      "Epoch 1 | Step 501400 | Avg Loss: 0.0158 | Grad Norm: 0.00787086\n",
      "Epoch 1 | Step 501500 | Avg Loss: 0.0154 | Grad Norm: 0.00858412\n",
      "Epoch 1 | Step 501600 | Avg Loss: 0.0153 | Grad Norm: 0.00956271\n",
      "Epoch 1 | Step 501700 | Avg Loss: 0.0151 | Grad Norm: 0.00948987\n",
      "Epoch 1 | Step 501800 | Avg Loss: 0.0145 | Grad Norm: 0.00762353\n",
      "Epoch 1 | Step 501900 | Avg Loss: 0.0147 | Grad Norm: 0.00836001\n",
      "Epoch 1 | Step 502000 | Avg Loss: 0.0146 | Grad Norm: 0.00924200\n",
      "Epoch 1 | Step 502100 | Avg Loss: 0.0145 | Grad Norm: 0.00819230\n",
      "Epoch 1 | Step 502200 | Avg Loss: 0.0147 | Grad Norm: 0.00812475\n",
      "Epoch 1 | Step 502300 | Avg Loss: 0.0150 | Grad Norm: 0.00880020\n",
      "Epoch 1 | Step 502400 | Avg Loss: 0.0154 | Grad Norm: 0.01021887\n",
      "Epoch 1 | Step 502500 | Avg Loss: 0.0153 | Grad Norm: 0.00947556\n",
      "Epoch 1 | Step 502600 | Avg Loss: 0.0149 | Grad Norm: 0.00831425\n",
      "Epoch 1 | Step 502700 | Avg Loss: 0.0147 | Grad Norm: 0.00913458\n",
      "Epoch 1 | Step 502800 | Avg Loss: 0.0145 | Grad Norm: 0.00798997\n",
      "Epoch 1 | Step 502900 | Avg Loss: 0.0148 | Grad Norm: 0.00841037\n",
      "Epoch 1 | Step 503000 | Avg Loss: 0.0149 | Grad Norm: 0.00845071\n",
      "Epoch 1 | Step 503100 | Avg Loss: 0.0152 | Grad Norm: 0.01069598\n",
      "Epoch 1 | Step 503200 | Avg Loss: 0.0154 | Grad Norm: 0.01008713\n",
      "Epoch 1 | Step 503300 | Avg Loss: 0.0156 | Grad Norm: 0.00811475\n",
      "Epoch 1 | Step 503400 | Avg Loss: 0.0152 | Grad Norm: 0.00858850\n",
      "Epoch 1 | Step 503500 | Avg Loss: 0.0156 | Grad Norm: 0.00890897\n",
      "Epoch 1 | Step 503600 | Avg Loss: 0.0159 | Grad Norm: 0.01048993\n",
      "Epoch 1 | Step 503700 | Avg Loss: 0.0160 | Grad Norm: 0.00838529\n",
      "Epoch 1 | Step 503800 | Avg Loss: 0.0161 | Grad Norm: 0.00979601\n",
      "Epoch 1 | Step 503900 | Avg Loss: 0.0156 | Grad Norm: 0.00810973\n",
      "Epoch 1 | Step 504000 | Avg Loss: 0.0158 | Grad Norm: 0.00812464\n",
      "Epoch 1 | Step 504100 | Avg Loss: 0.0158 | Grad Norm: 0.00904896\n",
      "Epoch 1 | Step 504200 | Avg Loss: 0.0159 | Grad Norm: 0.00907612\n",
      "Epoch 1 | Step 504300 | Avg Loss: 0.0160 | Grad Norm: 0.00900680\n",
      "Epoch 1 | Step 504400 | Avg Loss: 0.0159 | Grad Norm: 0.00970182\n",
      "Epoch 1 | Step 504500 | Avg Loss: 0.0159 | Grad Norm: 0.00874012\n",
      "Epoch 1 | Step 504600 | Avg Loss: 0.0160 | Grad Norm: 0.01035560\n",
      "Epoch 1 | Step 504700 | Avg Loss: 0.0159 | Grad Norm: 0.00749632\n",
      "Epoch 1 | Step 504800 | Avg Loss: 0.0164 | Grad Norm: 0.00999951\n",
      "Epoch 1 | Step 504900 | Avg Loss: 0.0161 | Grad Norm: 0.00881361\n",
      "Epoch 1 | Step 505000 | Avg Loss: 0.0157 | Grad Norm: 0.00881035\n",
      "Epoch 1 | Step 505100 | Avg Loss: 0.0154 | Grad Norm: 0.01000527\n",
      "Epoch 1 | Step 505200 | Avg Loss: 0.0154 | Grad Norm: 0.00928069\n",
      "Epoch 1 | Step 505300 | Avg Loss: 0.0157 | Grad Norm: 0.00983707\n",
      "Epoch 1 | Step 505400 | Avg Loss: 0.0157 | Grad Norm: 0.00900503\n",
      "Epoch 1 | Step 505500 | Avg Loss: 0.0160 | Grad Norm: 0.00964606\n",
      "Epoch 1 | Step 505600 | Avg Loss: 0.0159 | Grad Norm: 0.00948386\n",
      "Epoch 1 | Step 505700 | Avg Loss: 0.0165 | Grad Norm: 0.01063171\n",
      "Epoch 1 | Step 505800 | Avg Loss: 0.0168 | Grad Norm: 0.00950208\n",
      "Epoch 1 | Step 505900 | Avg Loss: 0.0164 | Grad Norm: 0.01021945\n",
      "Epoch 1 | Step 506000 | Avg Loss: 0.0165 | Grad Norm: 0.00860590\n",
      "Epoch 1 | Step 506100 | Avg Loss: 0.0165 | Grad Norm: 0.00835334\n",
      "Epoch 1 | Step 506200 | Avg Loss: 0.0164 | Grad Norm: 0.00891282\n",
      "Epoch 1 | Step 506300 | Avg Loss: 0.0160 | Grad Norm: 0.00828149\n",
      "Epoch 1 | Step 506400 | Avg Loss: 0.0159 | Grad Norm: 0.00855112\n",
      "Epoch 1 | Step 506500 | Avg Loss: 0.0157 | Grad Norm: 0.00802470\n",
      "Epoch 1 | Step 506600 | Avg Loss: 0.0157 | Grad Norm: 0.00752203\n",
      "Epoch 1 | Step 506700 | Avg Loss: 0.0157 | Grad Norm: 0.00851103\n",
      "Epoch 1 | Step 506800 | Avg Loss: 0.0160 | Grad Norm: 0.00899208\n",
      "Epoch 1 | Step 506900 | Avg Loss: 0.0162 | Grad Norm: 0.00914186\n",
      "Epoch 1 | Step 507000 | Avg Loss: 0.0164 | Grad Norm: 0.00878195\n",
      "Epoch 1 | Step 507100 | Avg Loss: 0.0162 | Grad Norm: 0.00877656\n",
      "Epoch 1 | Step 507200 | Avg Loss: 0.0158 | Grad Norm: 0.00800206\n",
      "Epoch 1 | Step 507300 | Avg Loss: 0.0157 | Grad Norm: 0.01155545\n",
      "Epoch 1 | Step 507400 | Avg Loss: 0.0152 | Grad Norm: 0.00739470\n",
      "Epoch 1 | Step 507500 | Avg Loss: 0.0160 | Grad Norm: 0.00797142\n",
      "Epoch 1 | Step 507600 | Avg Loss: 0.0161 | Grad Norm: 0.00983916\n",
      "Epoch 1 | Step 507700 | Avg Loss: 0.0161 | Grad Norm: 0.00904713\n",
      "Epoch 1 | Step 507800 | Avg Loss: 0.0161 | Grad Norm: 0.00819180\n",
      "Epoch 1 | Step 507900 | Avg Loss: 0.0161 | Grad Norm: 0.00916002\n",
      "Epoch 1 | Step 508000 | Avg Loss: 0.0163 | Grad Norm: 0.00963803\n",
      "Epoch 1 | Step 508100 | Avg Loss: 0.0159 | Grad Norm: 0.00925587\n",
      "Epoch 1 | Step 508200 | Avg Loss: 0.0158 | Grad Norm: 0.01031532\n",
      "Epoch 1 | Step 508300 | Avg Loss: 0.0156 | Grad Norm: 0.00900025\n",
      "Epoch 1 | Step 508400 | Avg Loss: 0.0157 | Grad Norm: 0.00933803\n",
      "Epoch 1 | Step 508500 | Avg Loss: 0.0160 | Grad Norm: 0.00808640\n",
      "Epoch 1 | Step 508600 | Avg Loss: 0.0159 | Grad Norm: 0.01024781\n",
      "Epoch 1 | Step 508700 | Avg Loss: 0.0157 | Grad Norm: 0.00878935\n",
      "Epoch 1 | Step 508800 | Avg Loss: 0.0158 | Grad Norm: 0.00864739\n",
      "Epoch 1 | Step 508900 | Avg Loss: 0.0154 | Grad Norm: 0.00979043\n",
      "Epoch 1 | Step 509000 | Avg Loss: 0.0156 | Grad Norm: 0.00902293\n",
      "Epoch 1 | Step 509100 | Avg Loss: 0.0152 | Grad Norm: 0.00875972\n",
      "Epoch 1 | Step 509200 | Avg Loss: 0.0152 | Grad Norm: 0.00902743\n",
      "Epoch 1 | Step 509300 | Avg Loss: 0.0153 | Grad Norm: 0.00779480\n",
      "Epoch 1 | Step 509400 | Avg Loss: 0.0151 | Grad Norm: 0.00864314\n",
      "Epoch 1 | Step 509500 | Avg Loss: 0.0148 | Grad Norm: 0.00870114\n",
      "Epoch 1 | Step 509600 | Avg Loss: 0.0148 | Grad Norm: 0.00747238\n",
      "Epoch 1 | Step 509700 | Avg Loss: 0.0147 | Grad Norm: 0.00866332\n",
      "Epoch 1 | Step 509800 | Avg Loss: 0.0148 | Grad Norm: 0.00882455\n",
      "Epoch 1 | Step 509900 | Avg Loss: 0.0150 | Grad Norm: 0.00934253\n",
      "Epoch 1 | Step 510000 | Avg Loss: 0.0145 | Grad Norm: 0.00736505\n",
      "Epoch 1 | Step 510100 | Avg Loss: 0.0145 | Grad Norm: 0.00814189\n",
      "Epoch 1 | Step 510200 | Avg Loss: 0.0150 | Grad Norm: 0.00799133\n",
      "Epoch 1 | Step 510300 | Avg Loss: 0.0152 | Grad Norm: 0.00843299\n",
      "Epoch 1 | Step 510400 | Avg Loss: 0.0152 | Grad Norm: 0.00893569\n",
      "Epoch 1 | Step 510500 | Avg Loss: 0.0154 | Grad Norm: 0.00772062\n",
      "Epoch 1 | Step 510600 | Avg Loss: 0.0158 | Grad Norm: 0.00873096\n",
      "Epoch 1 | Step 510700 | Avg Loss: 0.0162 | Grad Norm: 0.00898829\n",
      "Epoch 1 | Step 510800 | Avg Loss: 0.0159 | Grad Norm: 0.00776932\n",
      "Epoch 1 | Step 510900 | Avg Loss: 0.0158 | Grad Norm: 0.00958682\n",
      "Epoch 1 | Step 511000 | Avg Loss: 0.0161 | Grad Norm: 0.00874661\n",
      "Epoch 1 | Step 511100 | Avg Loss: 0.0160 | Grad Norm: 0.00905044\n",
      "Epoch 1 | Step 511200 | Avg Loss: 0.0160 | Grad Norm: 0.00792461\n",
      "Epoch 1 | Step 511300 | Avg Loss: 0.0160 | Grad Norm: 0.00887986\n",
      "Epoch 1 | Step 511400 | Avg Loss: 0.0162 | Grad Norm: 0.00857170\n",
      "Epoch 1 | Step 511500 | Avg Loss: 0.0159 | Grad Norm: 0.00883032\n",
      "Epoch 1 | Step 511600 | Avg Loss: 0.0160 | Grad Norm: 0.00973742\n",
      "Epoch 1 | Step 511700 | Avg Loss: 0.0158 | Grad Norm: 0.00918768\n",
      "Epoch 1 | Step 511800 | Avg Loss: 0.0158 | Grad Norm: 0.00753385\n",
      "Epoch 1 | Step 511900 | Avg Loss: 0.0158 | Grad Norm: 0.00855381\n",
      "Epoch 1 | Step 512000 | Avg Loss: 0.0160 | Grad Norm: 0.00904576\n",
      "Epoch 1 | Step 512100 | Avg Loss: 0.0158 | Grad Norm: 0.00976939\n",
      "Epoch 1 | Step 512200 | Avg Loss: 0.0157 | Grad Norm: 0.01020891\n",
      "Epoch 1 | Step 512300 | Avg Loss: 0.0161 | Grad Norm: 0.00868596\n",
      "Epoch 1 | Step 512400 | Avg Loss: 0.0162 | Grad Norm: 0.00991097\n",
      "Epoch 1 | Step 512500 | Avg Loss: 0.0163 | Grad Norm: 0.00842708\n",
      "Epoch 1 | Step 512600 | Avg Loss: 0.0159 | Grad Norm: 0.01115076\n",
      "Epoch 1 | Step 512700 | Avg Loss: 0.0157 | Grad Norm: 0.00867317\n",
      "Epoch 1 | Step 512800 | Avg Loss: 0.0154 | Grad Norm: 0.01234176\n",
      "Epoch 1 | Step 512900 | Avg Loss: 0.0155 | Grad Norm: 0.00816248\n",
      "Epoch 1 | Step 513000 | Avg Loss: 0.0159 | Grad Norm: 0.00913265\n",
      "Epoch 1 | Step 513100 | Avg Loss: 0.0160 | Grad Norm: 0.00830339\n",
      "Epoch 1 | Step 513200 | Avg Loss: 0.0158 | Grad Norm: 0.00801506\n",
      "Epoch 1 | Step 513300 | Avg Loss: 0.0156 | Grad Norm: 0.00910207\n",
      "Epoch 1 | Step 513400 | Avg Loss: 0.0153 | Grad Norm: 0.00762596\n",
      "Epoch 1 | Step 513500 | Avg Loss: 0.0151 | Grad Norm: 0.00794355\n",
      "Epoch 1 | Step 513600 | Avg Loss: 0.0151 | Grad Norm: 0.00821927\n",
      "Epoch 1 | Step 513700 | Avg Loss: 0.0155 | Grad Norm: 0.01010784\n",
      "Epoch 1 | Step 513800 | Avg Loss: 0.0157 | Grad Norm: 0.00857684\n",
      "Epoch 1 | Step 513900 | Avg Loss: 0.0161 | Grad Norm: 0.00843075\n",
      "Epoch 1 | Step 514000 | Avg Loss: 0.0160 | Grad Norm: 0.01151850\n",
      "Epoch 1 | Step 514100 | Avg Loss: 0.0159 | Grad Norm: 0.01086059\n",
      "Epoch 1 | Step 514200 | Avg Loss: 0.0158 | Grad Norm: 0.00901407\n",
      "Epoch 1 | Step 514300 | Avg Loss: 0.0160 | Grad Norm: 0.00940951\n",
      "Epoch 1 | Step 514400 | Avg Loss: 0.0162 | Grad Norm: 0.00876559\n",
      "Epoch 1 | Step 514500 | Avg Loss: 0.0162 | Grad Norm: 0.00790364\n",
      "Epoch 1 | Step 514600 | Avg Loss: 0.0160 | Grad Norm: 0.00815978\n",
      "Epoch 1 | Step 514700 | Avg Loss: 0.0162 | Grad Norm: 0.00757168\n",
      "Epoch 1 | Step 514800 | Avg Loss: 0.0163 | Grad Norm: 0.00858947\n",
      "Epoch 1 | Step 514900 | Avg Loss: 0.0163 | Grad Norm: 0.00897910\n",
      "Epoch 1 | Step 515000 | Avg Loss: 0.0164 | Grad Norm: 0.00885067\n",
      "Epoch 1 | Step 515100 | Avg Loss: 0.0163 | Grad Norm: 0.00945951\n",
      "Epoch 1 | Step 515200 | Avg Loss: 0.0162 | Grad Norm: 0.01085986\n",
      "Epoch 1 | Step 515300 | Avg Loss: 0.0160 | Grad Norm: 0.01022746\n",
      "Epoch 1 | Step 515400 | Avg Loss: 0.0159 | Grad Norm: 0.00826304\n",
      "Epoch 1 | Step 515500 | Avg Loss: 0.0161 | Grad Norm: 0.01401130\n",
      "Epoch 1 | Step 515600 | Avg Loss: 0.0163 | Grad Norm: 0.00929564\n",
      "Epoch 1 | Step 515700 | Avg Loss: 0.0162 | Grad Norm: 0.00957204\n",
      "Epoch 1 | Step 515800 | Avg Loss: 0.0162 | Grad Norm: 0.00796753\n",
      "Epoch 1 | Step 515900 | Avg Loss: 0.0164 | Grad Norm: 0.01238828\n",
      "Epoch 1 | Step 516000 | Avg Loss: 0.0161 | Grad Norm: 0.00923681\n",
      "Epoch 1 | Step 516100 | Avg Loss: 0.0159 | Grad Norm: 0.00837990\n",
      "Epoch 1 | Step 516200 | Avg Loss: 0.0158 | Grad Norm: 0.00834216\n",
      "Epoch 1 | Step 516300 | Avg Loss: 0.0159 | Grad Norm: 0.01019407\n",
      "Epoch 1 | Step 516400 | Avg Loss: 0.0160 | Grad Norm: 0.00945721\n",
      "Epoch 1 | Step 516500 | Avg Loss: 0.0157 | Grad Norm: 0.00778025\n",
      "Epoch 1 | Step 516600 | Avg Loss: 0.0161 | Grad Norm: 0.00879288\n",
      "Epoch 1 | Step 516700 | Avg Loss: 0.0161 | Grad Norm: 0.00862087\n",
      "Epoch 1 | Step 516800 | Avg Loss: 0.0160 | Grad Norm: 0.01034014\n",
      "Epoch 1 | Step 516900 | Avg Loss: 0.0161 | Grad Norm: 0.00873400\n",
      "Epoch 1 | Step 517000 | Avg Loss: 0.0159 | Grad Norm: 0.00835121\n",
      "Epoch 1 | Step 517100 | Avg Loss: 0.0154 | Grad Norm: 0.00990624\n",
      "Epoch 1 | Step 517200 | Avg Loss: 0.0155 | Grad Norm: 0.00866711\n",
      "Epoch 1 | Step 517300 | Avg Loss: 0.0157 | Grad Norm: 0.00853593\n",
      "Epoch 1 | Step 517400 | Avg Loss: 0.0153 | Grad Norm: 0.00814671\n",
      "Epoch 1 | Step 517500 | Avg Loss: 0.0152 | Grad Norm: 0.00804540\n",
      "Epoch 1 | Step 517600 | Avg Loss: 0.0156 | Grad Norm: 0.00834172\n",
      "Epoch 1 | Step 517700 | Avg Loss: 0.0156 | Grad Norm: 0.00987905\n",
      "Epoch 1 | Step 517800 | Avg Loss: 0.0152 | Grad Norm: 0.00935216\n",
      "Epoch 1 | Step 517900 | Avg Loss: 0.0151 | Grad Norm: 0.00816126\n",
      "Epoch 1 | Step 518000 | Avg Loss: 0.0151 | Grad Norm: 0.00824826\n",
      "Epoch 1 | Step 518100 | Avg Loss: 0.0155 | Grad Norm: 0.01046642\n",
      "Epoch 1 | Step 518200 | Avg Loss: 0.0157 | Grad Norm: 0.00918945\n",
      "Epoch 1 | Step 518300 | Avg Loss: 0.0159 | Grad Norm: 0.00876522\n",
      "Epoch 1 | Step 518400 | Avg Loss: 0.0161 | Grad Norm: 0.00793968\n",
      "Epoch 1 | Step 518500 | Avg Loss: 0.0159 | Grad Norm: 0.00836336\n",
      "Epoch 1 | Step 518600 | Avg Loss: 0.0157 | Grad Norm: 0.01055499\n",
      "Epoch 1 | Step 518700 | Avg Loss: 0.0154 | Grad Norm: 0.00874082\n",
      "Epoch 1 | Step 518800 | Avg Loss: 0.0154 | Grad Norm: 0.00979726\n",
      "Epoch 1 | Step 518900 | Avg Loss: 0.0153 | Grad Norm: 0.00908554\n",
      "Epoch 1 | Step 519000 | Avg Loss: 0.0159 | Grad Norm: 0.00936526\n",
      "Epoch 1 | Step 519100 | Avg Loss: 0.0158 | Grad Norm: 0.00922080\n",
      "Epoch 1 | Step 519200 | Avg Loss: 0.0153 | Grad Norm: 0.00874837\n",
      "Epoch 1 | Step 519300 | Avg Loss: 0.0153 | Grad Norm: 0.00936997\n",
      "Epoch 1 | Step 519400 | Avg Loss: 0.0152 | Grad Norm: 0.00791192\n",
      "Epoch 1 | Step 519500 | Avg Loss: 0.0155 | Grad Norm: 0.00778498\n",
      "Epoch 1 | Step 519600 | Avg Loss: 0.0151 | Grad Norm: 0.00714733\n",
      "Epoch 1 | Step 519700 | Avg Loss: 0.0149 | Grad Norm: 0.00880664\n",
      "Epoch 1 | Step 519800 | Avg Loss: 0.0152 | Grad Norm: 0.00859107\n",
      "Epoch 1 | Step 519900 | Avg Loss: 0.0153 | Grad Norm: 0.01002040\n",
      "Epoch 1 | Step 520000 | Avg Loss: 0.0159 | Grad Norm: 0.01130209\n",
      "Epoch 1 | Step 520100 | Avg Loss: 0.0158 | Grad Norm: 0.00835638\n",
      "Epoch 1 | Step 520200 | Avg Loss: 0.0158 | Grad Norm: 0.00825075\n",
      "Epoch 1 | Step 520300 | Avg Loss: 0.0159 | Grad Norm: 0.00772782\n",
      "Epoch 1 | Step 520400 | Avg Loss: 0.0156 | Grad Norm: 0.01013437\n",
      "Epoch 1 | Step 520500 | Avg Loss: 0.0159 | Grad Norm: 0.00933355\n",
      "Epoch 1 | Step 520600 | Avg Loss: 0.0156 | Grad Norm: 0.00961568\n",
      "Epoch 1 | Step 520700 | Avg Loss: 0.0160 | Grad Norm: 0.00895302\n",
      "Epoch 1 | Step 520800 | Avg Loss: 0.0162 | Grad Norm: 0.00880747\n",
      "Epoch 1 | Step 520900 | Avg Loss: 0.0158 | Grad Norm: 0.01015704\n",
      "Epoch 1 | Step 521000 | Avg Loss: 0.0154 | Grad Norm: 0.00937718\n",
      "Epoch 1 | Step 521100 | Avg Loss: 0.0152 | Grad Norm: 0.00964267\n",
      "Epoch 1 | Step 521200 | Avg Loss: 0.0150 | Grad Norm: 0.00881806\n",
      "Epoch 1 | Step 521300 | Avg Loss: 0.0153 | Grad Norm: 0.00872203\n",
      "Epoch 1 | Step 521400 | Avg Loss: 0.0152 | Grad Norm: 0.00900083\n",
      "Epoch 1 | Step 521500 | Avg Loss: 0.0153 | Grad Norm: 0.00823852\n",
      "Epoch 1 | Step 521600 | Avg Loss: 0.0155 | Grad Norm: 0.01200390\n",
      "Epoch 1 | Step 521700 | Avg Loss: 0.0158 | Grad Norm: 0.00794437\n",
      "Epoch 1 | Step 521800 | Avg Loss: 0.0159 | Grad Norm: 0.00819693\n",
      "Epoch 1 | Step 521900 | Avg Loss: 0.0158 | Grad Norm: 0.00955087\n",
      "Epoch 1 | Step 522000 | Avg Loss: 0.0160 | Grad Norm: 0.00840971\n",
      "Epoch 1 | Step 522100 | Avg Loss: 0.0156 | Grad Norm: 0.00984470\n",
      "Epoch 1 | Step 522200 | Avg Loss: 0.0153 | Grad Norm: 0.00912269\n",
      "Epoch 1 | Step 522300 | Avg Loss: 0.0153 | Grad Norm: 0.00833677\n",
      "Epoch 1 | Step 522400 | Avg Loss: 0.0153 | Grad Norm: 0.00873127\n",
      "Epoch 1 | Step 522500 | Avg Loss: 0.0159 | Grad Norm: 0.00905834\n",
      "Epoch 1 | Step 522600 | Avg Loss: 0.0155 | Grad Norm: 0.00932898\n",
      "Epoch 1 | Step 522700 | Avg Loss: 0.0156 | Grad Norm: 0.00922048\n",
      "Epoch 1 | Step 522800 | Avg Loss: 0.0154 | Grad Norm: 0.01017748\n",
      "Epoch 1 | Step 522900 | Avg Loss: 0.0156 | Grad Norm: 0.01033882\n",
      "Epoch 1 | Step 523000 | Avg Loss: 0.0160 | Grad Norm: 0.00906519\n",
      "Epoch 1 | Step 523100 | Avg Loss: 0.0157 | Grad Norm: 0.00772790\n",
      "Epoch 1 | Step 523200 | Avg Loss: 0.0158 | Grad Norm: 0.00774418\n",
      "Epoch 1 | Step 523300 | Avg Loss: 0.0159 | Grad Norm: 0.00764468\n",
      "Epoch 1 | Step 523400 | Avg Loss: 0.0157 | Grad Norm: 0.00804406\n",
      "Epoch 1 | Step 523500 | Avg Loss: 0.0159 | Grad Norm: 0.00979466\n",
      "Epoch 1 | Step 523600 | Avg Loss: 0.0162 | Grad Norm: 0.00995153\n",
      "Epoch 1 | Step 523700 | Avg Loss: 0.0157 | Grad Norm: 0.00834571\n",
      "Epoch 1 | Step 523800 | Avg Loss: 0.0153 | Grad Norm: 0.00862805\n",
      "Epoch 1 | Step 523900 | Avg Loss: 0.0153 | Grad Norm: 0.00902949\n",
      "Epoch 1 | Step 524000 | Avg Loss: 0.0154 | Grad Norm: 0.00806274\n",
      "Epoch 1 | Step 524100 | Avg Loss: 0.0151 | Grad Norm: 0.00790679\n",
      "Epoch 1 | Step 524200 | Avg Loss: 0.0153 | Grad Norm: 0.00873527\n",
      "Epoch 1 | Step 524300 | Avg Loss: 0.0156 | Grad Norm: 0.00818141\n",
      "Epoch 1 | Step 524400 | Avg Loss: 0.0155 | Grad Norm: 0.00898933\n",
      "Epoch 1 | Step 524500 | Avg Loss: 0.0153 | Grad Norm: 0.00816653\n",
      "Epoch 1 | Step 524600 | Avg Loss: 0.0155 | Grad Norm: 0.00738477\n",
      "Epoch 1 | Step 524700 | Avg Loss: 0.0154 | Grad Norm: 0.00922988\n",
      "Epoch 1 | Step 524800 | Avg Loss: 0.0155 | Grad Norm: 0.00901847\n",
      "Epoch 1 | Step 524900 | Avg Loss: 0.0157 | Grad Norm: 0.00822880\n",
      "Epoch 1 | Step 525000 | Avg Loss: 0.0158 | Grad Norm: 0.00837398\n",
      "Epoch 1 | Step 525100 | Avg Loss: 0.0161 | Grad Norm: 0.00761485\n",
      "Epoch 1 | Step 525200 | Avg Loss: 0.0158 | Grad Norm: 0.00878185\n",
      "Epoch 1 | Step 525300 | Avg Loss: 0.0160 | Grad Norm: 0.00859746\n",
      "Epoch 1 | Step 525400 | Avg Loss: 0.0158 | Grad Norm: 0.00825541\n",
      "Epoch 1 | Step 525500 | Avg Loss: 0.0158 | Grad Norm: 0.00954711\n",
      "Epoch 1 | Step 525600 | Avg Loss: 0.0160 | Grad Norm: 0.00826867\n",
      "Epoch 1 | Step 525700 | Avg Loss: 0.0160 | Grad Norm: 0.00823747\n",
      "Epoch 1 | Step 525800 | Avg Loss: 0.0159 | Grad Norm: 0.00925577\n",
      "Epoch 1 | Step 525900 | Avg Loss: 0.0157 | Grad Norm: 0.00831637\n",
      "Epoch 1 | Step 526000 | Avg Loss: 0.0154 | Grad Norm: 0.00731937\n",
      "Epoch 1 | Step 526100 | Avg Loss: 0.0156 | Grad Norm: 0.00888437\n",
      "Epoch 1 | Step 526200 | Avg Loss: 0.0154 | Grad Norm: 0.00899575\n",
      "Epoch 1 | Step 526300 | Avg Loss: 0.0154 | Grad Norm: 0.01088452\n",
      "Epoch 1 | Step 526400 | Avg Loss: 0.0151 | Grad Norm: 0.00972872\n",
      "Epoch 1 | Step 526500 | Avg Loss: 0.0155 | Grad Norm: 0.00850513\n",
      "Epoch 1 | Step 526600 | Avg Loss: 0.0153 | Grad Norm: 0.00930972\n",
      "Epoch 1 | Step 526700 | Avg Loss: 0.0151 | Grad Norm: 0.01041395\n",
      "Epoch 1 | Step 526800 | Avg Loss: 0.0152 | Grad Norm: 0.00838469\n",
      "Epoch 1 | Step 526900 | Avg Loss: 0.0154 | Grad Norm: 0.00904906\n",
      "Epoch 1 | Step 527000 | Avg Loss: 0.0157 | Grad Norm: 0.01047585\n",
      "Epoch 1 | Step 527100 | Avg Loss: 0.0157 | Grad Norm: 0.00928729\n",
      "Epoch 1 | Step 527200 | Avg Loss: 0.0159 | Grad Norm: 0.01003237\n",
      "Epoch 1 | Step 527300 | Avg Loss: 0.0160 | Grad Norm: 0.00875192\n",
      "Epoch 1 | Step 527400 | Avg Loss: 0.0165 | Grad Norm: 0.00949171\n",
      "Epoch 1 | Step 527500 | Avg Loss: 0.0162 | Grad Norm: 0.00898087\n",
      "Epoch 1 | Step 527600 | Avg Loss: 0.0163 | Grad Norm: 0.01110940\n",
      "Epoch 1 | Step 527700 | Avg Loss: 0.0163 | Grad Norm: 0.01718115\n",
      "Epoch 1 | Step 527800 | Avg Loss: 0.0160 | Grad Norm: 0.00821483\n",
      "Epoch 1 | Step 527900 | Avg Loss: 0.0157 | Grad Norm: 0.00837793\n",
      "Epoch 1 | Step 528000 | Avg Loss: 0.0157 | Grad Norm: 0.00852668\n",
      "Epoch 1 | Step 528100 | Avg Loss: 0.0155 | Grad Norm: 0.00761259\n",
      "Epoch 1 | Step 528200 | Avg Loss: 0.0150 | Grad Norm: 0.00789676\n",
      "Epoch 1 | Step 528300 | Avg Loss: 0.0152 | Grad Norm: 0.00787643\n",
      "Epoch 1 | Step 528400 | Avg Loss: 0.0151 | Grad Norm: 0.01089061\n",
      "Epoch 1 | Step 528500 | Avg Loss: 0.0154 | Grad Norm: 0.00848993\n",
      "Epoch 1 | Step 528600 | Avg Loss: 0.0153 | Grad Norm: 0.00770819\n",
      "Epoch 1 | Step 528700 | Avg Loss: 0.0153 | Grad Norm: 0.01143554\n",
      "Epoch 1 | Step 528800 | Avg Loss: 0.0155 | Grad Norm: 0.00790599\n",
      "Epoch 1 | Step 528900 | Avg Loss: 0.0155 | Grad Norm: 0.01062958\n",
      "Epoch 1 | Step 529000 | Avg Loss: 0.0159 | Grad Norm: 0.00884250\n",
      "Epoch 1 | Step 529100 | Avg Loss: 0.0159 | Grad Norm: 0.00777512\n",
      "Epoch 1 | Step 529200 | Avg Loss: 0.0158 | Grad Norm: 0.00786227\n",
      "Epoch 1 | Step 529300 | Avg Loss: 0.0160 | Grad Norm: 0.00941472\n",
      "Epoch 1 | Step 529400 | Avg Loss: 0.0160 | Grad Norm: 0.00803356\n",
      "Epoch 1 | Step 529500 | Avg Loss: 0.0158 | Grad Norm: 0.00855723\n",
      "Epoch 1 | Step 529600 | Avg Loss: 0.0157 | Grad Norm: 0.00814405\n",
      "Epoch 1 | Step 529700 | Avg Loss: 0.0158 | Grad Norm: 0.00878848\n",
      "Epoch 1 | Step 529800 | Avg Loss: 0.0160 | Grad Norm: 0.00858300\n",
      "Epoch 1 | Step 529900 | Avg Loss: 0.0155 | Grad Norm: 0.00873355\n",
      "Epoch 1 | Step 530000 | Avg Loss: 0.0158 | Grad Norm: 0.01074610\n",
      "Epoch 1 | Step 530100 | Avg Loss: 0.0158 | Grad Norm: 0.00806331\n",
      "Epoch 1 | Step 530200 | Avg Loss: 0.0161 | Grad Norm: 0.00797852\n",
      "Epoch 1 | Step 530300 | Avg Loss: 0.0157 | Grad Norm: 0.00993751\n",
      "Epoch 1 | Step 530400 | Avg Loss: 0.0156 | Grad Norm: 0.00988924\n",
      "Epoch 1 | Step 530500 | Avg Loss: 0.0155 | Grad Norm: 0.00945381\n",
      "Epoch 1 | Step 530600 | Avg Loss: 0.0158 | Grad Norm: 0.00881137\n",
      "Epoch 1 | Step 530700 | Avg Loss: 0.0155 | Grad Norm: 0.00979119\n",
      "Epoch 1 | Step 530800 | Avg Loss: 0.0156 | Grad Norm: 0.00902379\n",
      "Epoch 1 | Step 530900 | Avg Loss: 0.0151 | Grad Norm: 0.00838077\n",
      "Epoch 1 | Step 531000 | Avg Loss: 0.0152 | Grad Norm: 0.00935222\n",
      "Epoch 1 | Step 531100 | Avg Loss: 0.0151 | Grad Norm: 0.00778426\n",
      "Epoch 1 | Step 531200 | Avg Loss: 0.0152 | Grad Norm: 0.00893770\n",
      "Epoch 1 | Step 531300 | Avg Loss: 0.0153 | Grad Norm: 0.00936801\n",
      "Epoch 1 | Step 531400 | Avg Loss: 0.0153 | Grad Norm: 0.00867599\n",
      "Epoch 1 | Step 531500 | Avg Loss: 0.0150 | Grad Norm: 0.00902976\n",
      "Epoch 1 | Step 531600 | Avg Loss: 0.0152 | Grad Norm: 0.00861225\n",
      "Epoch 1 | Step 531700 | Avg Loss: 0.0153 | Grad Norm: 0.00843106\n",
      "Epoch 1 | Step 531800 | Avg Loss: 0.0156 | Grad Norm: 0.00950720\n",
      "Epoch 1 | Step 531900 | Avg Loss: 0.0155 | Grad Norm: 0.00828631\n",
      "Epoch 1 | Step 532000 | Avg Loss: 0.0158 | Grad Norm: 0.00862076\n",
      "Epoch 1 | Step 532100 | Avg Loss: 0.0159 | Grad Norm: 0.00764130\n",
      "Epoch 1 | Step 532200 | Avg Loss: 0.0159 | Grad Norm: 0.00922852\n",
      "Epoch 1 | Step 532300 | Avg Loss: 0.0160 | Grad Norm: 0.00960601\n",
      "Epoch 1 | Step 532400 | Avg Loss: 0.0160 | Grad Norm: 0.01035864\n",
      "Epoch 1 | Step 532500 | Avg Loss: 0.0155 | Grad Norm: 0.00911079\n",
      "Epoch 1 | Step 532600 | Avg Loss: 0.0156 | Grad Norm: 0.00894472\n",
      "Epoch 1 | Step 532700 | Avg Loss: 0.0156 | Grad Norm: 0.00787063\n",
      "Epoch 1 | Step 532800 | Avg Loss: 0.0154 | Grad Norm: 0.01030472\n",
      "Epoch 1 | Step 532900 | Avg Loss: 0.0150 | Grad Norm: 0.00913781\n",
      "Epoch 1 | Step 533000 | Avg Loss: 0.0147 | Grad Norm: 0.00780392\n",
      "Epoch 1 | Step 533100 | Avg Loss: 0.0150 | Grad Norm: 0.00997076\n",
      "Epoch 1 | Step 533200 | Avg Loss: 0.0152 | Grad Norm: 0.00801263\n",
      "Epoch 1 | Step 533300 | Avg Loss: 0.0154 | Grad Norm: 0.00894648\n",
      "Epoch 1 | Step 533400 | Avg Loss: 0.0156 | Grad Norm: 0.00823653\n",
      "Epoch 1 | Step 533500 | Avg Loss: 0.0157 | Grad Norm: 0.00916028\n",
      "Epoch 1 | Step 533600 | Avg Loss: 0.0156 | Grad Norm: 0.00841615\n",
      "Epoch 1 | Step 533700 | Avg Loss: 0.0152 | Grad Norm: 0.00879849\n",
      "Epoch 1 | Step 533800 | Avg Loss: 0.0154 | Grad Norm: 0.00807861\n",
      "Epoch 1 | Step 533900 | Avg Loss: 0.0155 | Grad Norm: 0.00829583\n",
      "Epoch 1 | Step 534000 | Avg Loss: 0.0155 | Grad Norm: 0.01062838\n",
      "Epoch 1 | Step 534100 | Avg Loss: 0.0153 | Grad Norm: 0.00928858\n",
      "Epoch 1 | Step 534200 | Avg Loss: 0.0154 | Grad Norm: 0.00915842\n",
      "Epoch 1 | Step 534300 | Avg Loss: 0.0152 | Grad Norm: 0.00866067\n",
      "Epoch 1 | Step 534400 | Avg Loss: 0.0154 | Grad Norm: 0.00997876\n",
      "Epoch 1 | Step 534500 | Avg Loss: 0.0154 | Grad Norm: 0.00711334\n",
      "Epoch 1 | Step 534600 | Avg Loss: 0.0155 | Grad Norm: 0.00935180\n",
      "Epoch 1 | Step 534700 | Avg Loss: 0.0153 | Grad Norm: 0.00755815\n",
      "Epoch 1 | Step 534800 | Avg Loss: 0.0153 | Grad Norm: 0.00868361\n",
      "Epoch 1 | Step 534900 | Avg Loss: 0.0153 | Grad Norm: 0.00806961\n",
      "Epoch 1 | Step 535000 | Avg Loss: 0.0155 | Grad Norm: 0.00897323\n",
      "Epoch 1 | Step 535100 | Avg Loss: 0.0154 | Grad Norm: 0.00866292\n",
      "Epoch 1 | Step 535200 | Avg Loss: 0.0155 | Grad Norm: 0.00769453\n",
      "Epoch 1 | Step 535300 | Avg Loss: 0.0152 | Grad Norm: 0.00879837\n",
      "Epoch 1 | Step 535400 | Avg Loss: 0.0149 | Grad Norm: 0.00903533\n",
      "Epoch 1 | Step 535500 | Avg Loss: 0.0153 | Grad Norm: 0.00883689\n",
      "Epoch 1 | Step 535600 | Avg Loss: 0.0151 | Grad Norm: 0.00869039\n",
      "Epoch 1 | Step 535700 | Avg Loss: 0.0152 | Grad Norm: 0.00895180\n",
      "Epoch 1 | Step 535800 | Avg Loss: 0.0153 | Grad Norm: 0.00989143\n",
      "Epoch 1 | Step 535900 | Avg Loss: 0.0153 | Grad Norm: 0.00825312\n",
      "Epoch 1 | Step 536000 | Avg Loss: 0.0151 | Grad Norm: 0.00868530\n",
      "Epoch 1 | Step 536100 | Avg Loss: 0.0151 | Grad Norm: 0.00978219\n",
      "Epoch 1 | Step 536200 | Avg Loss: 0.0152 | Grad Norm: 0.00804215\n",
      "Epoch 1 | Step 536300 | Avg Loss: 0.0152 | Grad Norm: 0.00899850\n",
      "Epoch 1 | Step 536400 | Avg Loss: 0.0151 | Grad Norm: 0.00930101\n",
      "Epoch 1 | Step 536500 | Avg Loss: 0.0153 | Grad Norm: 0.00828630\n",
      "Epoch 1 | Step 536600 | Avg Loss: 0.0154 | Grad Norm: 0.00879599\n",
      "Epoch 1 | Step 536700 | Avg Loss: 0.0152 | Grad Norm: 0.01105641\n",
      "Epoch 1 | Step 536800 | Avg Loss: 0.0149 | Grad Norm: 0.00829751\n",
      "Epoch 1 | Step 536900 | Avg Loss: 0.0149 | Grad Norm: 0.00914313\n",
      "Epoch 1 | Step 537000 | Avg Loss: 0.0148 | Grad Norm: 0.00812324\n",
      "Epoch 1 | Step 537100 | Avg Loss: 0.0149 | Grad Norm: 0.00913107\n",
      "Epoch 1 | Step 537200 | Avg Loss: 0.0150 | Grad Norm: 0.00966468\n",
      "Epoch 1 | Step 537300 | Avg Loss: 0.0149 | Grad Norm: 0.00841031\n",
      "Epoch 1 | Step 537400 | Avg Loss: 0.0153 | Grad Norm: 0.00813534\n",
      "Epoch 1 | Step 537500 | Avg Loss: 0.0151 | Grad Norm: 0.00826832\n",
      "Epoch 1 | Step 537600 | Avg Loss: 0.0153 | Grad Norm: 0.00893440\n",
      "Epoch 1 | Step 537700 | Avg Loss: 0.0152 | Grad Norm: 0.00968197\n",
      "Epoch 1 | Step 537800 | Avg Loss: 0.0154 | Grad Norm: 0.00982670\n",
      "Epoch 1 | Step 537900 | Avg Loss: 0.0157 | Grad Norm: 0.00826991\n",
      "Epoch 1 | Step 538000 | Avg Loss: 0.0158 | Grad Norm: 0.00748901\n",
      "Epoch 1 | Step 538100 | Avg Loss: 0.0161 | Grad Norm: 0.00840199\n",
      "Epoch 1 | Step 538200 | Avg Loss: 0.0159 | Grad Norm: 0.00947004\n",
      "Epoch 1 | Step 538300 | Avg Loss: 0.0161 | Grad Norm: 0.00851436\n",
      "Epoch 1 | Step 538400 | Avg Loss: 0.0157 | Grad Norm: 0.00930770\n",
      "Epoch 1 | Step 538500 | Avg Loss: 0.0160 | Grad Norm: 0.01242936\n",
      "Epoch 1 | Step 538600 | Avg Loss: 0.0163 | Grad Norm: 0.00854569\n",
      "Epoch 1 | Step 538700 | Avg Loss: 0.0160 | Grad Norm: 0.00820275\n",
      "Epoch 1 | Step 538800 | Avg Loss: 0.0162 | Grad Norm: 0.00975931\n",
      "Epoch 1 | Step 538900 | Avg Loss: 0.0161 | Grad Norm: 0.00879311\n",
      "Epoch 1 | Step 539000 | Avg Loss: 0.0162 | Grad Norm: 0.00816669\n",
      "Epoch 1 | Step 539100 | Avg Loss: 0.0161 | Grad Norm: 0.01134348\n",
      "Epoch 1 | Step 539200 | Avg Loss: 0.0165 | Grad Norm: 0.01089748\n",
      "Epoch 1 | Step 539300 | Avg Loss: 0.0164 | Grad Norm: 0.00866842\n",
      "Epoch 1 | Step 539400 | Avg Loss: 0.0160 | Grad Norm: 0.00936735\n",
      "Epoch 1 | Step 539500 | Avg Loss: 0.0158 | Grad Norm: 0.00779381\n",
      "Epoch 1 | Step 539600 | Avg Loss: 0.0156 | Grad Norm: 0.00795789\n",
      "Epoch 1 | Step 539700 | Avg Loss: 0.0155 | Grad Norm: 0.01083986\n",
      "Epoch 1 | Step 539800 | Avg Loss: 0.0156 | Grad Norm: 0.00830838\n",
      "Epoch 1 | Step 539900 | Avg Loss: 0.0155 | Grad Norm: 0.00943107\n",
      "Epoch 1 | Step 540000 | Avg Loss: 0.0161 | Grad Norm: 0.01048236\n",
      "Epoch 1 | Step 540100 | Avg Loss: 0.0160 | Grad Norm: 0.00809812\n",
      "Epoch 1 | Step 540200 | Avg Loss: 0.0164 | Grad Norm: 0.01130638\n",
      "Epoch 1 | Step 540300 | Avg Loss: 0.0163 | Grad Norm: 0.00886326\n",
      "Epoch 1 | Step 540400 | Avg Loss: 0.0163 | Grad Norm: 0.01084157\n",
      "Epoch 1 | Step 540500 | Avg Loss: 0.0162 | Grad Norm: 0.00974200\n",
      "Epoch 1 | Step 540600 | Avg Loss: 0.0164 | Grad Norm: 0.00970798\n",
      "Epoch 1 | Step 540700 | Avg Loss: 0.0165 | Grad Norm: 0.00841547\n",
      "Epoch 1 | Step 540800 | Avg Loss: 0.0166 | Grad Norm: 0.00960862\n",
      "Epoch 1 | Step 540900 | Avg Loss: 0.0165 | Grad Norm: 0.00914518\n",
      "Epoch 1 | Step 541000 | Avg Loss: 0.0164 | Grad Norm: 0.00901399\n",
      "Epoch 1 | Step 541100 | Avg Loss: 0.0161 | Grad Norm: 0.00965931\n",
      "Epoch 1 | Step 541200 | Avg Loss: 0.0163 | Grad Norm: 0.00869438\n",
      "Epoch 1 | Step 541300 | Avg Loss: 0.0165 | Grad Norm: 0.00901733\n",
      "Epoch 1 | Step 541400 | Avg Loss: 0.0165 | Grad Norm: 0.01012745\n",
      "Epoch 1 | Step 541500 | Avg Loss: 0.0160 | Grad Norm: 0.00793468\n",
      "Epoch 1 | Step 541600 | Avg Loss: 0.0160 | Grad Norm: 0.00903947\n",
      "Epoch 1 | Step 541700 | Avg Loss: 0.0158 | Grad Norm: 0.00890742\n",
      "Epoch 1 | Step 541800 | Avg Loss: 0.0160 | Grad Norm: 0.00802127\n",
      "Epoch 1 | Step 541900 | Avg Loss: 0.0159 | Grad Norm: 0.00929935\n",
      "Epoch 1 | Step 542000 | Avg Loss: 0.0157 | Grad Norm: 0.01002104\n",
      "Epoch 1 | Step 542100 | Avg Loss: 0.0160 | Grad Norm: 0.00956122\n",
      "Epoch 1 | Step 542200 | Avg Loss: 0.0162 | Grad Norm: 0.01042141\n",
      "Epoch 1 | Step 542300 | Avg Loss: 0.0159 | Grad Norm: 0.01075868\n",
      "Epoch 1 | Step 542400 | Avg Loss: 0.0161 | Grad Norm: 0.00917243\n",
      "Epoch 1 | Step 542500 | Avg Loss: 0.0159 | Grad Norm: 0.00910432\n",
      "Epoch 1 | Step 542600 | Avg Loss: 0.0160 | Grad Norm: 0.00974905\n",
      "Epoch 1 | Step 542700 | Avg Loss: 0.0161 | Grad Norm: 0.00817120\n",
      "Epoch 1 | Step 542800 | Avg Loss: 0.0159 | Grad Norm: 0.00864527\n",
      "Epoch 1 | Step 542900 | Avg Loss: 0.0162 | Grad Norm: 0.00975567\n",
      "Epoch 1 | Step 543000 | Avg Loss: 0.0158 | Grad Norm: 0.00982034\n",
      "Epoch 1 | Step 543100 | Avg Loss: 0.0160 | Grad Norm: 0.00859667\n",
      "Epoch 1 | Step 543200 | Avg Loss: 0.0163 | Grad Norm: 0.00957942\n",
      "Epoch 1 | Step 543300 | Avg Loss: 0.0163 | Grad Norm: 0.00896692\n",
      "Epoch 1 | Step 543400 | Avg Loss: 0.0162 | Grad Norm: 0.00954106\n",
      "Epoch 1 | Step 543500 | Avg Loss: 0.0160 | Grad Norm: 0.00993568\n",
      "Epoch 1 | Step 543600 | Avg Loss: 0.0159 | Grad Norm: 0.00927195\n",
      "Epoch 1 | Step 543700 | Avg Loss: 0.0155 | Grad Norm: 0.00782051\n",
      "Epoch 1 | Step 543800 | Avg Loss: 0.0156 | Grad Norm: 0.00864150\n",
      "Epoch 1 | Step 543900 | Avg Loss: 0.0158 | Grad Norm: 0.00839788\n",
      "Epoch 1 | Step 544000 | Avg Loss: 0.0159 | Grad Norm: 0.00740395\n",
      "Epoch 1 | Step 544100 | Avg Loss: 0.0157 | Grad Norm: 0.00910394\n",
      "Epoch 1 | Step 544200 | Avg Loss: 0.0158 | Grad Norm: 0.00979875\n",
      "Epoch 1 | Step 544300 | Avg Loss: 0.0159 | Grad Norm: 0.00940762\n",
      "Epoch 1 | Step 544400 | Avg Loss: 0.0160 | Grad Norm: 0.00786589\n",
      "Epoch 1 | Step 544500 | Avg Loss: 0.0159 | Grad Norm: 0.00765604\n",
      "Epoch 1 | Step 544600 | Avg Loss: 0.0156 | Grad Norm: 0.00869195\n",
      "Epoch 1 | Step 544700 | Avg Loss: 0.0159 | Grad Norm: 0.00967215\n",
      "Epoch 1 | Step 544800 | Avg Loss: 0.0156 | Grad Norm: 0.00854432\n",
      "Epoch 1 | Step 544900 | Avg Loss: 0.0157 | Grad Norm: 0.00823978\n",
      "Epoch 1 | Step 545000 | Avg Loss: 0.0154 | Grad Norm: 0.00959220\n",
      "Epoch 1 | Step 545100 | Avg Loss: 0.0157 | Grad Norm: 0.00966080\n",
      "Epoch 1 | Step 545200 | Avg Loss: 0.0159 | Grad Norm: 0.00740062\n",
      "Epoch 1 | Step 545300 | Avg Loss: 0.0161 | Grad Norm: 0.01099756\n",
      "Epoch 1 | Step 545400 | Avg Loss: 0.0160 | Grad Norm: 0.00775104\n",
      "Epoch 1 | Step 545500 | Avg Loss: 0.0159 | Grad Norm: 0.00867468\n",
      "Epoch 1 | Step 545600 | Avg Loss: 0.0153 | Grad Norm: 0.00769998\n",
      "Epoch 1 | Step 545700 | Avg Loss: 0.0153 | Grad Norm: 0.00988168\n",
      "Epoch 1 | Step 545800 | Avg Loss: 0.0151 | Grad Norm: 0.00927520\n",
      "Epoch 1 | Step 545900 | Avg Loss: 0.0150 | Grad Norm: 0.00840425\n",
      "Epoch 1 | Step 546000 | Avg Loss: 0.0156 | Grad Norm: 0.00852402\n",
      "Epoch 1 | Step 546100 | Avg Loss: 0.0154 | Grad Norm: 0.00787026\n",
      "Epoch 1 | Step 546200 | Avg Loss: 0.0154 | Grad Norm: 0.00874489\n",
      "Epoch 1 | Step 546300 | Avg Loss: 0.0154 | Grad Norm: 0.00920960\n",
      "Epoch 1 | Step 546400 | Avg Loss: 0.0152 | Grad Norm: 0.00849716\n",
      "Epoch 1 | Step 546500 | Avg Loss: 0.0149 | Grad Norm: 0.00899431\n",
      "Epoch 1 | Step 546600 | Avg Loss: 0.0151 | Grad Norm: 0.00953689\n",
      "Epoch 1 | Step 546700 | Avg Loss: 0.0152 | Grad Norm: 0.00977816\n",
      "Epoch 1 | Step 546800 | Avg Loss: 0.0155 | Grad Norm: 0.00854045\n",
      "Epoch 1 | Step 546900 | Avg Loss: 0.0153 | Grad Norm: 0.00764859\n",
      "Epoch 1 | Step 547000 | Avg Loss: 0.0152 | Grad Norm: 0.00825719\n",
      "Epoch 1 | Step 547100 | Avg Loss: 0.0153 | Grad Norm: 0.00717271\n",
      "Epoch 1 | Step 547200 | Avg Loss: 0.0155 | Grad Norm: 0.00914697\n",
      "Epoch 1 | Step 547300 | Avg Loss: 0.0156 | Grad Norm: 0.00864437\n",
      "Epoch 1 | Step 547400 | Avg Loss: 0.0158 | Grad Norm: 0.00932367\n",
      "Epoch 1 | Step 547500 | Avg Loss: 0.0154 | Grad Norm: 0.00808356\n",
      "Epoch 1 | Step 547600 | Avg Loss: 0.0154 | Grad Norm: 0.00996211\n",
      "Epoch 1 | Step 547700 | Avg Loss: 0.0154 | Grad Norm: 0.00922225\n",
      "Epoch 1 | Step 547800 | Avg Loss: 0.0154 | Grad Norm: 0.00767346\n",
      "Epoch 1 | Step 547900 | Avg Loss: 0.0158 | Grad Norm: 0.00825896\n",
      "Epoch 1 | Step 548000 | Avg Loss: 0.0159 | Grad Norm: 0.00868392\n",
      "Epoch 1 | Step 548100 | Avg Loss: 0.0158 | Grad Norm: 0.00883784\n",
      "Epoch 1 | Step 548200 | Avg Loss: 0.0159 | Grad Norm: 0.00905179\n",
      "Epoch 1 | Step 548300 | Avg Loss: 0.0154 | Grad Norm: 0.00807981\n",
      "Epoch 1 | Step 548400 | Avg Loss: 0.0159 | Grad Norm: 0.00942082\n",
      "Epoch 1 | Step 548500 | Avg Loss: 0.0158 | Grad Norm: 0.00761343\n",
      "Epoch 1 | Step 548600 | Avg Loss: 0.0157 | Grad Norm: 0.00900907\n",
      "Epoch 1 | Step 548700 | Avg Loss: 0.0158 | Grad Norm: 0.01032583\n",
      "Epoch 1 | Step 548800 | Avg Loss: 0.0162 | Grad Norm: 0.00819249\n",
      "Epoch 1 | Step 548900 | Avg Loss: 0.0162 | Grad Norm: 0.00911744\n",
      "Epoch 1 | Step 549000 | Avg Loss: 0.0162 | Grad Norm: 0.00944615\n",
      "Epoch 1 | Step 549100 | Avg Loss: 0.0162 | Grad Norm: 0.00786742\n",
      "Epoch 1 | Step 549200 | Avg Loss: 0.0159 | Grad Norm: 0.01076466\n",
      "Epoch 1 | Step 549300 | Avg Loss: 0.0160 | Grad Norm: 0.00911898\n",
      "Epoch 1 | Step 549400 | Avg Loss: 0.0160 | Grad Norm: 0.00884774\n",
      "Epoch 1 | Step 549500 | Avg Loss: 0.0159 | Grad Norm: 0.00872817\n",
      "Epoch 1 | Step 549600 | Avg Loss: 0.0159 | Grad Norm: 0.00892411\n",
      "Epoch 1 | Step 549700 | Avg Loss: 0.0156 | Grad Norm: 0.00779242\n",
      "Epoch 1 | Step 549800 | Avg Loss: 0.0158 | Grad Norm: 0.00804850\n",
      "Epoch 1 | Step 549900 | Avg Loss: 0.0156 | Grad Norm: 0.00847246\n",
      "Epoch 1 | Step 550000 | Avg Loss: 0.0155 | Grad Norm: 0.00888125\n",
      "Epoch 1 | Step 550100 | Avg Loss: 0.0156 | Grad Norm: 0.00890445\n",
      "Epoch 1 | Step 550200 | Avg Loss: 0.0155 | Grad Norm: 0.00936703\n",
      "Epoch 1 | Step 550300 | Avg Loss: 0.0153 | Grad Norm: 0.00903288\n",
      "Epoch 1 | Step 550400 | Avg Loss: 0.0153 | Grad Norm: 0.00952696\n",
      "Epoch 1 | Step 550500 | Avg Loss: 0.0157 | Grad Norm: 0.01016146\n",
      "Epoch 1 | Step 550600 | Avg Loss: 0.0160 | Grad Norm: 0.00782782\n",
      "Epoch 1 | Step 550700 | Avg Loss: 0.0160 | Grad Norm: 0.00787706\n",
      "Epoch 1 | Step 550800 | Avg Loss: 0.0158 | Grad Norm: 0.00953689\n",
      "Epoch 1 | Step 550900 | Avg Loss: 0.0159 | Grad Norm: 0.00912561\n",
      "Epoch 1 | Step 551000 | Avg Loss: 0.0159 | Grad Norm: 0.01111392\n",
      "Epoch 1 | Step 551100 | Avg Loss: 0.0157 | Grad Norm: 0.00870965\n",
      "Epoch 1 | Step 551200 | Avg Loss: 0.0155 | Grad Norm: 0.00859061\n",
      "Epoch 1 | Step 551300 | Avg Loss: 0.0154 | Grad Norm: 0.00736546\n",
      "Epoch 1 | Step 551400 | Avg Loss: 0.0156 | Grad Norm: 0.00862868\n",
      "Epoch 1 | Step 551500 | Avg Loss: 0.0155 | Grad Norm: 0.00898608\n",
      "Epoch 1 | Step 551600 | Avg Loss: 0.0159 | Grad Norm: 0.00841664\n",
      "Epoch 1 | Step 551700 | Avg Loss: 0.0159 | Grad Norm: 0.01056581\n",
      "Epoch 1 | Step 551800 | Avg Loss: 0.0161 | Grad Norm: 0.00861739\n",
      "Epoch 1 | Step 551900 | Avg Loss: 0.0159 | Grad Norm: 0.00883968\n",
      "Epoch 1 | Step 552000 | Avg Loss: 0.0157 | Grad Norm: 0.00842933\n",
      "Epoch 1 | Step 552100 | Avg Loss: 0.0158 | Grad Norm: 0.00976899\n",
      "Epoch 1 | Step 552200 | Avg Loss: 0.0157 | Grad Norm: 0.00821798\n",
      "Epoch 1 | Step 552300 | Avg Loss: 0.0158 | Grad Norm: 0.00908659\n",
      "Epoch 1 | Step 552400 | Avg Loss: 0.0153 | Grad Norm: 0.00842102\n",
      "Epoch 1 | Step 552500 | Avg Loss: 0.0153 | Grad Norm: 0.00753563\n",
      "Epoch 1 | Step 552600 | Avg Loss: 0.0158 | Grad Norm: 0.00978873\n",
      "Epoch 1 | Step 552700 | Avg Loss: 0.0160 | Grad Norm: 0.00804189\n",
      "Epoch 1 | Step 552800 | Avg Loss: 0.0163 | Grad Norm: 0.00784638\n",
      "Epoch 1 | Step 552900 | Avg Loss: 0.0158 | Grad Norm: 0.01100788\n",
      "Epoch 1 | Step 553000 | Avg Loss: 0.0158 | Grad Norm: 0.00876476\n",
      "Epoch 1 | Step 553100 | Avg Loss: 0.0154 | Grad Norm: 0.00829424\n",
      "Epoch 1 | Step 553200 | Avg Loss: 0.0155 | Grad Norm: 0.00773743\n",
      "Epoch 1 | Step 553300 | Avg Loss: 0.0159 | Grad Norm: 0.00968160\n",
      "Epoch 1 | Step 553400 | Avg Loss: 0.0160 | Grad Norm: 0.00874354\n",
      "Epoch 1 | Step 553500 | Avg Loss: 0.0160 | Grad Norm: 0.00826478\n",
      "Epoch 1 | Step 553600 | Avg Loss: 0.0159 | Grad Norm: 0.00841834\n",
      "Epoch 1 | Step 553700 | Avg Loss: 0.0158 | Grad Norm: 0.00874620\n",
      "Epoch 1 | Step 553800 | Avg Loss: 0.0159 | Grad Norm: 0.00860451\n",
      "Epoch 1 | Step 553900 | Avg Loss: 0.0156 | Grad Norm: 0.00871802\n",
      "Epoch 1 | Step 554000 | Avg Loss: 0.0154 | Grad Norm: 0.00857941\n",
      "Epoch 1 | Step 554100 | Avg Loss: 0.0158 | Grad Norm: 0.00810597\n",
      "Epoch 1 | Step 554200 | Avg Loss: 0.0155 | Grad Norm: 0.00844833\n",
      "Epoch 1 | Step 554300 | Avg Loss: 0.0155 | Grad Norm: 0.00750425\n",
      "Epoch 1 | Step 554400 | Avg Loss: 0.0155 | Grad Norm: 0.00985008\n",
      "Epoch 1 | Step 554500 | Avg Loss: 0.0161 | Grad Norm: 0.01017726\n",
      "Epoch 1 | Step 554600 | Avg Loss: 0.0160 | Grad Norm: 0.00873599\n",
      "Epoch 1 | Step 554700 | Avg Loss: 0.0158 | Grad Norm: 0.00808160\n",
      "Epoch 1 | Step 554800 | Avg Loss: 0.0154 | Grad Norm: 0.00907104\n",
      "Epoch 1 | Step 554900 | Avg Loss: 0.0155 | Grad Norm: 0.00747529\n",
      "Epoch 1 | Step 555000 | Avg Loss: 0.0158 | Grad Norm: 0.00791595\n",
      "Epoch 1 | Step 555100 | Avg Loss: 0.0157 | Grad Norm: 0.01008484\n",
      "Epoch 1 | Step 555200 | Avg Loss: 0.0160 | Grad Norm: 0.00855320\n",
      "Epoch 1 | Step 555300 | Avg Loss: 0.0158 | Grad Norm: 0.00864088\n",
      "Epoch 1 | Step 555400 | Avg Loss: 0.0157 | Grad Norm: 0.00892013\n",
      "Epoch 1 | Step 555500 | Avg Loss: 0.0159 | Grad Norm: 0.00957338\n",
      "Epoch 1 | Step 555600 | Avg Loss: 0.0159 | Grad Norm: 0.00984068\n",
      "Epoch 1 | Step 555700 | Avg Loss: 0.0159 | Grad Norm: 0.00922226\n",
      "Epoch 1 | Step 555800 | Avg Loss: 0.0158 | Grad Norm: 0.00869173\n",
      "Epoch 1 | Step 555900 | Avg Loss: 0.0161 | Grad Norm: 0.00797390\n",
      "Epoch 1 | Step 556000 | Avg Loss: 0.0161 | Grad Norm: 0.00868356\n",
      "Epoch 1 | Step 556100 | Avg Loss: 0.0162 | Grad Norm: 0.01034267\n",
      "Epoch 1 | Step 556200 | Avg Loss: 0.0160 | Grad Norm: 0.00961125\n",
      "Epoch 1 | Step 556300 | Avg Loss: 0.0159 | Grad Norm: 0.00770831\n",
      "Epoch 1 | Step 556400 | Avg Loss: 0.0159 | Grad Norm: 0.00841666\n",
      "Epoch 1 | Step 556500 | Avg Loss: 0.0162 | Grad Norm: 0.00808350\n",
      "Epoch 1 | Step 556600 | Avg Loss: 0.0160 | Grad Norm: 0.00987713\n",
      "Epoch 1 | Step 556700 | Avg Loss: 0.0159 | Grad Norm: 0.00833260\n",
      "Epoch 1 | Step 556800 | Avg Loss: 0.0159 | Grad Norm: 0.00842889\n",
      "Epoch 1 | Step 556900 | Avg Loss: 0.0162 | Grad Norm: 0.00973738\n",
      "Epoch 1 | Step 557000 | Avg Loss: 0.0158 | Grad Norm: 0.00852570\n",
      "Epoch 1 | Step 557100 | Avg Loss: 0.0157 | Grad Norm: 0.00917259\n",
      "Epoch 1 | Step 557200 | Avg Loss: 0.0158 | Grad Norm: 0.00873071\n",
      "Epoch 1 | Step 557300 | Avg Loss: 0.0163 | Grad Norm: 0.00818936\n",
      "Epoch 1 | Step 557400 | Avg Loss: 0.0164 | Grad Norm: 0.00980904\n",
      "Epoch 1 | Step 557500 | Avg Loss: 0.0164 | Grad Norm: 0.00953060\n",
      "Epoch 1 | Step 557600 | Avg Loss: 0.0166 | Grad Norm: 0.00952328\n",
      "Epoch 1 | Step 557700 | Avg Loss: 0.0161 | Grad Norm: 0.00807990\n",
      "Epoch 1 | Step 557800 | Avg Loss: 0.0161 | Grad Norm: 0.00992365\n",
      "Epoch 1 | Step 557900 | Avg Loss: 0.0159 | Grad Norm: 0.00950441\n",
      "Epoch 1 | Step 558000 | Avg Loss: 0.0158 | Grad Norm: 0.00776975\n",
      "Epoch 1 | Step 558100 | Avg Loss: 0.0159 | Grad Norm: 0.00816502\n",
      "Epoch 1 | Step 558200 | Avg Loss: 0.0158 | Grad Norm: 0.00878794\n",
      "Epoch 1 | Step 558300 | Avg Loss: 0.0159 | Grad Norm: 0.00970658\n",
      "Epoch 1 | Step 558400 | Avg Loss: 0.0159 | Grad Norm: 0.00818771\n",
      "Epoch 1 | Step 558500 | Avg Loss: 0.0157 | Grad Norm: 0.00779885\n",
      "Epoch 1 | Step 558600 | Avg Loss: 0.0161 | Grad Norm: 0.00876691\n",
      "Epoch 1 | Step 558700 | Avg Loss: 0.0157 | Grad Norm: 0.00862230\n",
      "Epoch 1 | Step 558800 | Avg Loss: 0.0160 | Grad Norm: 0.00774243\n",
      "Epoch 1 | Step 558900 | Avg Loss: 0.0161 | Grad Norm: 0.00918948\n",
      "Epoch 1 | Step 559000 | Avg Loss: 0.0161 | Grad Norm: 0.00880666\n",
      "Epoch 1 | Step 559100 | Avg Loss: 0.0161 | Grad Norm: 0.01016680\n",
      "Epoch 1 | Step 559200 | Avg Loss: 0.0161 | Grad Norm: 0.00924143\n",
      "Epoch 1 | Step 559300 | Avg Loss: 0.0159 | Grad Norm: 0.00792954\n",
      "Epoch 1 | Step 559400 | Avg Loss: 0.0162 | Grad Norm: 0.00889700\n",
      "Epoch 1 | Step 559500 | Avg Loss: 0.0161 | Grad Norm: 0.00876973\n",
      "Epoch 1 | Step 559600 | Avg Loss: 0.0162 | Grad Norm: 0.00825674\n",
      "Epoch 1 | Step 559700 | Avg Loss: 0.0161 | Grad Norm: 0.00927213\n",
      "Epoch 1 | Step 559800 | Avg Loss: 0.0162 | Grad Norm: 0.00810981\n",
      "Epoch 1 | Step 559900 | Avg Loss: 0.0160 | Grad Norm: 0.00889743\n",
      "Epoch 1 | Step 560000 | Avg Loss: 0.0158 | Grad Norm: 0.00837643\n",
      "Epoch 1 | Step 560100 | Avg Loss: 0.0160 | Grad Norm: 0.00883959\n",
      "Epoch 1 | Step 560200 | Avg Loss: 0.0160 | Grad Norm: 0.00901890\n",
      "Epoch 1 | Step 560300 | Avg Loss: 0.0156 | Grad Norm: 0.00956241\n",
      "Epoch 1 | Step 560400 | Avg Loss: 0.0157 | Grad Norm: 0.00964859\n",
      "Epoch 1 | Step 560500 | Avg Loss: 0.0156 | Grad Norm: 0.00822276\n",
      "Epoch 1 | Step 560600 | Avg Loss: 0.0155 | Grad Norm: 0.00906379\n",
      "Epoch 1 | Step 560700 | Avg Loss: 0.0155 | Grad Norm: 0.00918187\n",
      "Epoch 1 | Step 560800 | Avg Loss: 0.0152 | Grad Norm: 0.00843157\n",
      "Epoch 1 | Step 560900 | Avg Loss: 0.0150 | Grad Norm: 0.00770951\n",
      "Epoch 1 | Step 561000 | Avg Loss: 0.0151 | Grad Norm: 0.00877412\n",
      "Epoch 1 | Step 561100 | Avg Loss: 0.0156 | Grad Norm: 0.00927944\n",
      "Epoch 1 | Step 561200 | Avg Loss: 0.0160 | Grad Norm: 0.00922005\n",
      "Epoch 1 | Step 561300 | Avg Loss: 0.0157 | Grad Norm: 0.00959805\n",
      "Epoch 1 | Step 561400 | Avg Loss: 0.0156 | Grad Norm: 0.00897576\n",
      "Epoch 1 | Step 561500 | Avg Loss: 0.0153 | Grad Norm: 0.00761231\n",
      "Epoch 1 | Step 561600 | Avg Loss: 0.0158 | Grad Norm: 0.00977145\n",
      "Epoch 1 | Step 561700 | Avg Loss: 0.0156 | Grad Norm: 0.00885252\n",
      "Epoch 1 | Step 561800 | Avg Loss: 0.0158 | Grad Norm: 0.00806617\n",
      "Epoch 1 | Step 561900 | Avg Loss: 0.0159 | Grad Norm: 0.00870188\n",
      "Epoch 1 | Step 562000 | Avg Loss: 0.0161 | Grad Norm: 0.00846956\n",
      "Epoch 1 | Step 562100 | Avg Loss: 0.0159 | Grad Norm: 0.00806863\n",
      "Epoch 1 | Step 562200 | Avg Loss: 0.0159 | Grad Norm: 0.00982546\n",
      "Epoch 1 | Step 562300 | Avg Loss: 0.0159 | Grad Norm: 0.00788846\n",
      "Epoch 1 | Step 562400 | Avg Loss: 0.0158 | Grad Norm: 0.00811202\n",
      "Epoch 1 | Step 562500 | Avg Loss: 0.0157 | Grad Norm: 0.00832103\n",
      "Epoch 1 | Step 562600 | Avg Loss: 0.0156 | Grad Norm: 0.00898204\n",
      "Epoch 1 | Step 562700 | Avg Loss: 0.0159 | Grad Norm: 0.00852033\n",
      "Epoch 1 | Step 562800 | Avg Loss: 0.0160 | Grad Norm: 0.00914169\n",
      "Epoch 1 | Step 562900 | Avg Loss: 0.0164 | Grad Norm: 0.00867081\n",
      "Epoch 1 | Step 563000 | Avg Loss: 0.0163 | Grad Norm: 0.00784320\n",
      "Epoch 1 | Step 563100 | Avg Loss: 0.0159 | Grad Norm: 0.00851303\n",
      "Epoch 1 | Step 563200 | Avg Loss: 0.0156 | Grad Norm: 0.00836926\n",
      "Epoch 1 | Step 563300 | Avg Loss: 0.0153 | Grad Norm: 0.00814524\n",
      "Epoch 1 | Step 563400 | Avg Loss: 0.0155 | Grad Norm: 0.01090328\n",
      "Epoch 1 | Step 563500 | Avg Loss: 0.0153 | Grad Norm: 0.00780686\n",
      "Epoch 1 | Step 563600 | Avg Loss: 0.0151 | Grad Norm: 0.00883759\n",
      "Epoch 1 | Step 563700 | Avg Loss: 0.0155 | Grad Norm: 0.01072187\n",
      "Epoch 1 | Step 563800 | Avg Loss: 0.0154 | Grad Norm: 0.00949942\n",
      "Epoch 1 | Step 563900 | Avg Loss: 0.0154 | Grad Norm: 0.00899320\n",
      "Epoch 1 | Step 564000 | Avg Loss: 0.0154 | Grad Norm: 0.00905383\n",
      "Epoch 1 | Step 564100 | Avg Loss: 0.0155 | Grad Norm: 0.00905405\n",
      "Epoch 1 | Step 564200 | Avg Loss: 0.0155 | Grad Norm: 0.00769068\n",
      "Epoch 1 | Step 564300 | Avg Loss: 0.0157 | Grad Norm: 0.00985671\n",
      "Epoch 1 | Step 564400 | Avg Loss: 0.0156 | Grad Norm: 0.00929220\n",
      "Epoch 1 | Step 564500 | Avg Loss: 0.0156 | Grad Norm: 0.00888142\n",
      "Epoch 1 | Step 564600 | Avg Loss: 0.0154 | Grad Norm: 0.00907552\n",
      "Epoch 1 | Step 564700 | Avg Loss: 0.0156 | Grad Norm: 0.00836853\n",
      "Epoch 1 | Step 564800 | Avg Loss: 0.0154 | Grad Norm: 0.00894634\n",
      "Epoch 1 | Step 564900 | Avg Loss: 0.0155 | Grad Norm: 0.00990719\n",
      "Epoch 1 | Step 565000 | Avg Loss: 0.0156 | Grad Norm: 0.00887353\n",
      "Epoch 1 | Step 565100 | Avg Loss: 0.0160 | Grad Norm: 0.00975560\n",
      "Epoch 1 | Step 565200 | Avg Loss: 0.0164 | Grad Norm: 0.00972461\n",
      "Epoch 1 | Step 565300 | Avg Loss: 0.0159 | Grad Norm: 0.00897650\n",
      "Epoch 1 | Step 565400 | Avg Loss: 0.0158 | Grad Norm: 0.00788520\n",
      "Epoch 1 | Step 565500 | Avg Loss: 0.0156 | Grad Norm: 0.00900332\n",
      "Epoch 1 | Step 565600 | Avg Loss: 0.0157 | Grad Norm: 0.00875626\n",
      "Epoch 1 | Step 565700 | Avg Loss: 0.0156 | Grad Norm: 0.00816819\n",
      "Epoch 1 | Step 565800 | Avg Loss: 0.0154 | Grad Norm: 0.00925862\n",
      "Epoch 1 | Step 565900 | Avg Loss: 0.0153 | Grad Norm: 0.00784918\n",
      "Epoch 1 | Step 566000 | Avg Loss: 0.0156 | Grad Norm: 0.00854807\n",
      "Epoch 1 | Step 566100 | Avg Loss: 0.0155 | Grad Norm: 0.00877159\n",
      "Epoch 1 | Step 566200 | Avg Loss: 0.0158 | Grad Norm: 0.00793682\n",
      "Epoch 1 | Step 566300 | Avg Loss: 0.0158 | Grad Norm: 0.01008342\n",
      "Epoch 1 | Step 566400 | Avg Loss: 0.0159 | Grad Norm: 0.00885212\n",
      "Epoch 1 | Step 566500 | Avg Loss: 0.0159 | Grad Norm: 0.00902655\n",
      "Epoch 1 | Step 566600 | Avg Loss: 0.0160 | Grad Norm: 0.00919711\n",
      "Epoch 1 | Step 566700 | Avg Loss: 0.0157 | Grad Norm: 0.00823921\n",
      "Epoch 1 | Step 566800 | Avg Loss: 0.0151 | Grad Norm: 0.01013952\n",
      "Epoch 1 | Step 566900 | Avg Loss: 0.0154 | Grad Norm: 0.00786609\n",
      "Epoch 1 | Step 567000 | Avg Loss: 0.0156 | Grad Norm: 0.00804102\n",
      "Epoch 1 | Step 567100 | Avg Loss: 0.0152 | Grad Norm: 0.00976984\n",
      "Epoch 1 | Step 567200 | Avg Loss: 0.0159 | Grad Norm: 0.01119011\n",
      "Epoch 1 | Step 567300 | Avg Loss: 0.0158 | Grad Norm: 0.01069265\n",
      "Epoch 1 | Step 567400 | Avg Loss: 0.0158 | Grad Norm: 0.00826539\n",
      "Epoch 1 | Step 567500 | Avg Loss: 0.0162 | Grad Norm: 0.00920560\n",
      "Epoch 1 | Step 567600 | Avg Loss: 0.0161 | Grad Norm: 0.00772629\n",
      "Epoch 1 | Step 567700 | Avg Loss: 0.0159 | Grad Norm: 0.00940178\n",
      "Epoch 1 | Step 567800 | Avg Loss: 0.0155 | Grad Norm: 0.00850570\n",
      "Epoch 1 | Step 567900 | Avg Loss: 0.0158 | Grad Norm: 0.00982915\n",
      "Epoch 1 | Step 568000 | Avg Loss: 0.0162 | Grad Norm: 0.00869437\n",
      "Epoch 1 | Step 568100 | Avg Loss: 0.0161 | Grad Norm: 0.00838548\n",
      "Epoch 1 | Step 568200 | Avg Loss: 0.0161 | Grad Norm: 0.00862575\n",
      "Epoch 1 | Step 568300 | Avg Loss: 0.0162 | Grad Norm: 0.01155684\n",
      "Epoch 1 | Step 568400 | Avg Loss: 0.0160 | Grad Norm: 0.00941309\n",
      "Epoch 1 | Step 568500 | Avg Loss: 0.0160 | Grad Norm: 0.00855111\n",
      "Epoch 1 | Step 568600 | Avg Loss: 0.0163 | Grad Norm: 0.00844160\n",
      "Epoch 1 | Step 568700 | Avg Loss: 0.0159 | Grad Norm: 0.00872454\n",
      "Epoch 1 | Step 568800 | Avg Loss: 0.0156 | Grad Norm: 0.00809417\n",
      "Epoch 1 | Step 568900 | Avg Loss: 0.0154 | Grad Norm: 0.00766637\n",
      "Epoch 1 | Step 569000 | Avg Loss: 0.0156 | Grad Norm: 0.00784657\n",
      "Epoch 1 | Step 569100 | Avg Loss: 0.0154 | Grad Norm: 0.00697068\n",
      "Epoch 1 | Step 569200 | Avg Loss: 0.0154 | Grad Norm: 0.00953985\n",
      "Epoch 1 | Step 569300 | Avg Loss: 0.0154 | Grad Norm: 0.00847925\n",
      "Epoch 1 | Step 569400 | Avg Loss: 0.0159 | Grad Norm: 0.01090021\n",
      "Epoch 1 | Step 569500 | Avg Loss: 0.0158 | Grad Norm: 0.00748961\n",
      "Epoch 1 | Step 569600 | Avg Loss: 0.0158 | Grad Norm: 0.00825078\n",
      "Epoch 1 | Step 569700 | Avg Loss: 0.0160 | Grad Norm: 0.00795682\n",
      "Epoch 1 | Step 569800 | Avg Loss: 0.0158 | Grad Norm: 0.00934999\n",
      "Epoch 1 | Step 569900 | Avg Loss: 0.0157 | Grad Norm: 0.00930475\n",
      "Epoch 1 | Step 570000 | Avg Loss: 0.0155 | Grad Norm: 0.01093547\n",
      "Epoch 1 | Step 570100 | Avg Loss: 0.0156 | Grad Norm: 0.00710798\n",
      "Epoch 1 | Step 570200 | Avg Loss: 0.0159 | Grad Norm: 0.00835660\n",
      "Epoch 1 | Step 570300 | Avg Loss: 0.0161 | Grad Norm: 0.00989626\n",
      "Epoch 1 | Step 570400 | Avg Loss: 0.0161 | Grad Norm: 0.00886503\n",
      "Epoch 1 | Step 570500 | Avg Loss: 0.0159 | Grad Norm: 0.00853125\n",
      "Epoch 1 | Step 570600 | Avg Loss: 0.0159 | Grad Norm: 0.00766136\n",
      "Epoch 1 | Step 570700 | Avg Loss: 0.0157 | Grad Norm: 0.00882998\n",
      "Epoch 1 | Step 570800 | Avg Loss: 0.0157 | Grad Norm: 0.00901776\n",
      "Epoch 1 | Step 570900 | Avg Loss: 0.0158 | Grad Norm: 0.00916130\n",
      "Epoch 1 | Step 571000 | Avg Loss: 0.0157 | Grad Norm: 0.00788815\n",
      "Epoch 1 | Step 571100 | Avg Loss: 0.0160 | Grad Norm: 0.00867100\n",
      "Epoch 1 | Step 571200 | Avg Loss: 0.0158 | Grad Norm: 0.00918059\n",
      "Epoch 1 | Step 571300 | Avg Loss: 0.0160 | Grad Norm: 0.00799400\n",
      "Epoch 1 | Step 571400 | Avg Loss: 0.0162 | Grad Norm: 0.01051438\n",
      "Epoch 1 | Step 571500 | Avg Loss: 0.0160 | Grad Norm: 0.00830235\n",
      "Epoch 1 | Step 571600 | Avg Loss: 0.0162 | Grad Norm: 0.00867835\n",
      "Epoch 1 | Step 571700 | Avg Loss: 0.0158 | Grad Norm: 0.00914884\n",
      "Epoch 1 | Step 571800 | Avg Loss: 0.0155 | Grad Norm: 0.00848896\n",
      "Epoch 1 | Step 571900 | Avg Loss: 0.0156 | Grad Norm: 0.00953958\n",
      "Epoch 1 | Step 572000 | Avg Loss: 0.0156 | Grad Norm: 0.01037351\n",
      "Epoch 1 | Step 572100 | Avg Loss: 0.0157 | Grad Norm: 0.00712480\n",
      "Epoch 1 | Step 572200 | Avg Loss: 0.0160 | Grad Norm: 0.00907072\n",
      "Epoch 1 | Step 572300 | Avg Loss: 0.0155 | Grad Norm: 0.00933230\n",
      "Epoch 1 | Step 572400 | Avg Loss: 0.0157 | Grad Norm: 0.00773816\n",
      "Epoch 1 | Step 572500 | Avg Loss: 0.0156 | Grad Norm: 0.00933874\n",
      "Epoch 1 | Step 572600 | Avg Loss: 0.0157 | Grad Norm: 0.00887775\n",
      "Epoch 1 | Step 572700 | Avg Loss: 0.0156 | Grad Norm: 0.00922817\n",
      "Epoch 1 | Step 572800 | Avg Loss: 0.0157 | Grad Norm: 0.00961169\n",
      "Epoch 1 | Step 572900 | Avg Loss: 0.0155 | Grad Norm: 0.00851173\n",
      "Epoch 1 | Step 573000 | Avg Loss: 0.0156 | Grad Norm: 0.00946272\n",
      "Epoch 1 | Step 573100 | Avg Loss: 0.0154 | Grad Norm: 0.00855967\n",
      "Epoch 1 | Step 573200 | Avg Loss: 0.0153 | Grad Norm: 0.00821136\n",
      "Epoch 1 | Step 573300 | Avg Loss: 0.0152 | Grad Norm: 0.00787871\n",
      "Epoch 1 | Step 573400 | Avg Loss: 0.0152 | Grad Norm: 0.00924158\n",
      "Epoch 1 | Step 573500 | Avg Loss: 0.0155 | Grad Norm: 0.00834194\n",
      "Epoch 1 | Step 573600 | Avg Loss: 0.0154 | Grad Norm: 0.00877779\n",
      "Epoch 1 | Step 573700 | Avg Loss: 0.0156 | Grad Norm: 0.02025265\n",
      "Epoch 1 | Step 573800 | Avg Loss: 0.0154 | Grad Norm: 0.00952130\n",
      "Epoch 1 | Step 573900 | Avg Loss: 0.0157 | Grad Norm: 0.00883994\n",
      "Epoch 1 | Step 574000 | Avg Loss: 0.0159 | Grad Norm: 0.00806804\n",
      "Epoch 1 | Step 574100 | Avg Loss: 0.0156 | Grad Norm: 0.00953867\n",
      "Epoch 1 | Step 574200 | Avg Loss: 0.0154 | Grad Norm: 0.00796927\n",
      "Epoch 1 | Step 574300 | Avg Loss: 0.0156 | Grad Norm: 0.00956269\n",
      "Epoch 1 | Step 574400 | Avg Loss: 0.0156 | Grad Norm: 0.00779750\n",
      "Epoch 1 | Step 574500 | Avg Loss: 0.0158 | Grad Norm: 0.00897681\n",
      "Epoch 1 | Step 574600 | Avg Loss: 0.0156 | Grad Norm: 0.00920556\n",
      "Epoch 1 | Step 574700 | Avg Loss: 0.0155 | Grad Norm: 0.00884917\n",
      "Epoch 1 | Step 574800 | Avg Loss: 0.0157 | Grad Norm: 0.00938310\n",
      "Epoch 1 | Step 574900 | Avg Loss: 0.0155 | Grad Norm: 0.01063812\n",
      "Epoch 1 | Step 575000 | Avg Loss: 0.0157 | Grad Norm: 0.01096843\n",
      "Epoch 1 | Step 575100 | Avg Loss: 0.0156 | Grad Norm: 0.00937540\n",
      "Epoch 1 | Step 575200 | Avg Loss: 0.0156 | Grad Norm: 0.00999900\n",
      "Epoch 1 | Step 575300 | Avg Loss: 0.0160 | Grad Norm: 0.01014207\n",
      "Epoch 1 | Step 575400 | Avg Loss: 0.0158 | Grad Norm: 0.00785415\n",
      "Epoch 1 | Step 575500 | Avg Loss: 0.0156 | Grad Norm: 0.00918620\n",
      "Epoch 1 | Step 575600 | Avg Loss: 0.0160 | Grad Norm: 0.00868044\n",
      "Epoch 1 | Step 575700 | Avg Loss: 0.0157 | Grad Norm: 0.00896663\n",
      "Epoch 1 | Step 575800 | Avg Loss: 0.0155 | Grad Norm: 0.00896486\n",
      "Epoch 1 | Step 575900 | Avg Loss: 0.0156 | Grad Norm: 0.01078517\n",
      "Epoch 1 | Step 576000 | Avg Loss: 0.0157 | Grad Norm: 0.00883495\n",
      "Epoch 1 | Step 576100 | Avg Loss: 0.0155 | Grad Norm: 0.00864550\n",
      "Epoch 1 | Step 576200 | Avg Loss: 0.0156 | Grad Norm: 0.00767987\n",
      "Epoch 1 | Step 576300 | Avg Loss: 0.0153 | Grad Norm: 0.00753619\n",
      "Epoch 1 | Step 576400 | Avg Loss: 0.0152 | Grad Norm: 0.01337766\n",
      "Epoch 1 | Step 576500 | Avg Loss: 0.0153 | Grad Norm: 0.00813150\n",
      "Epoch 1 | Step 576600 | Avg Loss: 0.0156 | Grad Norm: 0.00974261\n",
      "Epoch 1 | Step 576700 | Avg Loss: 0.0155 | Grad Norm: 0.01054602\n",
      "Epoch 1 | Step 576800 | Avg Loss: 0.0154 | Grad Norm: 0.00756765\n",
      "Epoch 1 | Step 576900 | Avg Loss: 0.0153 | Grad Norm: 0.00859111\n",
      "Epoch 1 | Step 577000 | Avg Loss: 0.0153 | Grad Norm: 0.00875624\n",
      "Epoch 1 | Step 577100 | Avg Loss: 0.0152 | Grad Norm: 0.00877046\n",
      "Epoch 1 | Step 577200 | Avg Loss: 0.0149 | Grad Norm: 0.00868365\n",
      "Epoch 1 | Step 577300 | Avg Loss: 0.0150 | Grad Norm: 0.00822162\n",
      "Epoch 1 | Step 577400 | Avg Loss: 0.0154 | Grad Norm: 0.00909363\n",
      "Epoch 1 | Step 577500 | Avg Loss: 0.0156 | Grad Norm: 0.00816473\n",
      "Epoch 1 | Step 577600 | Avg Loss: 0.0153 | Grad Norm: 0.00819257\n",
      "Epoch 1 | Step 577700 | Avg Loss: 0.0153 | Grad Norm: 0.00940933\n",
      "Epoch 1 | Step 577800 | Avg Loss: 0.0157 | Grad Norm: 0.01045443\n",
      "Epoch 1 | Step 577900 | Avg Loss: 0.0155 | Grad Norm: 0.00791205\n",
      "Epoch 1 | Step 578000 | Avg Loss: 0.0155 | Grad Norm: 0.00928719\n",
      "Epoch 1 | Step 578100 | Avg Loss: 0.0155 | Grad Norm: 0.00820494\n",
      "Epoch 1 | Step 578200 | Avg Loss: 0.0154 | Grad Norm: 0.00824908\n",
      "Epoch 1 | Step 578300 | Avg Loss: 0.0153 | Grad Norm: 0.00878719\n",
      "Epoch 1 | Step 578400 | Avg Loss: 0.0152 | Grad Norm: 0.00983924\n",
      "Epoch 1 | Step 578500 | Avg Loss: 0.0150 | Grad Norm: 0.00951457\n",
      "Epoch 1 | Step 578600 | Avg Loss: 0.0151 | Grad Norm: 0.00816109\n",
      "Epoch 1 | Step 578700 | Avg Loss: 0.0154 | Grad Norm: 0.00753647\n",
      "Epoch 1 | Step 578800 | Avg Loss: 0.0156 | Grad Norm: 0.00948895\n",
      "Epoch 1 | Step 578900 | Avg Loss: 0.0156 | Grad Norm: 0.00858951\n",
      "Epoch 1 | Step 579000 | Avg Loss: 0.0155 | Grad Norm: 0.01057205\n",
      "Epoch 1 | Step 579100 | Avg Loss: 0.0153 | Grad Norm: 0.00869992\n",
      "Epoch 1 | Step 579200 | Avg Loss: 0.0154 | Grad Norm: 0.00864169\n",
      "Epoch 1 | Step 579300 | Avg Loss: 0.0155 | Grad Norm: 0.00837434\n",
      "Epoch 1 | Step 579400 | Avg Loss: 0.0156 | Grad Norm: 0.01062853\n",
      "Epoch 1 | Step 579500 | Avg Loss: 0.0155 | Grad Norm: 0.00975480\n",
      "Epoch 1 | Step 579600 | Avg Loss: 0.0158 | Grad Norm: 0.01035866\n",
      "Epoch 1 | Step 579700 | Avg Loss: 0.0156 | Grad Norm: 0.00921292\n",
      "Epoch 1 | Step 579800 | Avg Loss: 0.0153 | Grad Norm: 0.00869774\n",
      "Epoch 1 | Step 579900 | Avg Loss: 0.0153 | Grad Norm: 0.00783591\n",
      "Epoch 1 | Step 580000 | Avg Loss: 0.0154 | Grad Norm: 0.00876285\n",
      "Epoch 1 | Step 580100 | Avg Loss: 0.0156 | Grad Norm: 0.00929613\n",
      "Epoch 1 | Step 580200 | Avg Loss: 0.0156 | Grad Norm: 0.00883995\n",
      "Epoch 1 | Step 580300 | Avg Loss: 0.0155 | Grad Norm: 0.00901558\n",
      "Epoch 1 | Step 580400 | Avg Loss: 0.0151 | Grad Norm: 0.00928708\n",
      "Epoch 1 | Step 580500 | Avg Loss: 0.0158 | Grad Norm: 0.00791584\n",
      "Epoch 1 | Step 580600 | Avg Loss: 0.0155 | Grad Norm: 0.00787412\n",
      "Epoch 1 | Step 580700 | Avg Loss: 0.0157 | Grad Norm: 0.00818406\n",
      "Epoch 1 | Step 580800 | Avg Loss: 0.0154 | Grad Norm: 0.00999708\n",
      "Epoch 1 | Step 580900 | Avg Loss: 0.0158 | Grad Norm: 0.00965420\n",
      "Epoch 1 | Step 581000 | Avg Loss: 0.0156 | Grad Norm: 0.00837901\n",
      "Epoch 1 | Step 581100 | Avg Loss: 0.0154 | Grad Norm: 0.00777166\n",
      "Epoch 1 | Step 581200 | Avg Loss: 0.0154 | Grad Norm: 0.00809846\n",
      "Epoch 1 | Step 581300 | Avg Loss: 0.0157 | Grad Norm: 0.00914809\n",
      "Epoch 1 | Step 581400 | Avg Loss: 0.0158 | Grad Norm: 0.01137875\n",
      "Epoch 1 | Step 581500 | Avg Loss: 0.0155 | Grad Norm: 0.00828493\n",
      "Epoch 1 | Step 581600 | Avg Loss: 0.0154 | Grad Norm: 0.00849622\n",
      "Epoch 1 | Step 581700 | Avg Loss: 0.0156 | Grad Norm: 0.00834307\n",
      "Epoch 1 | Step 581800 | Avg Loss: 0.0153 | Grad Norm: 0.00885754\n",
      "Epoch 1 | Step 581900 | Avg Loss: 0.0151 | Grad Norm: 0.00883523\n",
      "Epoch 1 | Step 582000 | Avg Loss: 0.0153 | Grad Norm: 0.00812522\n",
      "Epoch 1 | Step 582100 | Avg Loss: 0.0155 | Grad Norm: 0.00896071\n",
      "Epoch 1 | Step 582200 | Avg Loss: 0.0153 | Grad Norm: 0.00910608\n",
      "Epoch 1 | Step 582300 | Avg Loss: 0.0157 | Grad Norm: 0.00879547\n",
      "Epoch 1 | Step 582400 | Avg Loss: 0.0155 | Grad Norm: 0.01011765\n",
      "Epoch 1 | Step 582500 | Avg Loss: 0.0155 | Grad Norm: 0.00870999\n",
      "Epoch 1 | Step 582600 | Avg Loss: 0.0156 | Grad Norm: 0.00770595\n",
      "Epoch 1 | Step 582700 | Avg Loss: 0.0154 | Grad Norm: 0.00929662\n",
      "Epoch 1 | Step 582800 | Avg Loss: 0.0157 | Grad Norm: 0.00916110\n",
      "Epoch 1 | Step 582900 | Avg Loss: 0.0154 | Grad Norm: 0.00782105\n",
      "Epoch 1 | Step 583000 | Avg Loss: 0.0153 | Grad Norm: 0.00943643\n",
      "Epoch 1 | Step 583100 | Avg Loss: 0.0152 | Grad Norm: 0.00992426\n",
      "Epoch 1 | Step 583200 | Avg Loss: 0.0154 | Grad Norm: 0.00847292\n",
      "Epoch 1 | Step 583300 | Avg Loss: 0.0156 | Grad Norm: 0.00858472\n",
      "Epoch 1 | Step 583400 | Avg Loss: 0.0157 | Grad Norm: 0.01000972\n",
      "Epoch 1 | Step 583500 | Avg Loss: 0.0161 | Grad Norm: 0.00962898\n",
      "Epoch 1 | Step 583600 | Avg Loss: 0.0160 | Grad Norm: 0.00890376\n",
      "Epoch 1 | Step 583700 | Avg Loss: 0.0162 | Grad Norm: 0.00875818\n",
      "Epoch 1 | Step 583800 | Avg Loss: 0.0160 | Grad Norm: 0.00849500\n",
      "Epoch 1 | Step 583900 | Avg Loss: 0.0162 | Grad Norm: 0.01099380\n",
      "Epoch 1 | Step 584000 | Avg Loss: 0.0159 | Grad Norm: 0.00938326\n",
      "Epoch 1 | Step 584100 | Avg Loss: 0.0155 | Grad Norm: 0.00903829\n",
      "Epoch 1 | Step 584200 | Avg Loss: 0.0159 | Grad Norm: 0.01007513\n",
      "Epoch 1 | Step 584300 | Avg Loss: 0.0158 | Grad Norm: 0.00807644\n",
      "Epoch 1 | Step 584400 | Avg Loss: 0.0159 | Grad Norm: 0.00921938\n",
      "Epoch 1 | Step 584500 | Avg Loss: 0.0159 | Grad Norm: 0.01262547\n",
      "Epoch 1 | Step 584600 | Avg Loss: 0.0163 | Grad Norm: 0.00799805\n",
      "Epoch 1 | Step 584700 | Avg Loss: 0.0162 | Grad Norm: 0.01086330\n",
      "Epoch 1 | Step 584800 | Avg Loss: 0.0165 | Grad Norm: 0.00975082\n",
      "Epoch 1 | Step 584900 | Avg Loss: 0.0165 | Grad Norm: 0.00874896\n",
      "Epoch 1 | Step 585000 | Avg Loss: 0.0163 | Grad Norm: 0.00874256\n",
      "Epoch 1 | Step 585100 | Avg Loss: 0.0164 | Grad Norm: 0.01074957\n",
      "Epoch 1 | Step 585200 | Avg Loss: 0.0167 | Grad Norm: 0.00809236\n",
      "Epoch 1 | Step 585300 | Avg Loss: 0.0166 | Grad Norm: 0.00952294\n",
      "Epoch 1 | Step 585400 | Avg Loss: 0.0164 | Grad Norm: 0.00916671\n",
      "Epoch 1 | Step 585500 | Avg Loss: 0.0163 | Grad Norm: 0.00819060\n",
      "Epoch 1 | Step 585600 | Avg Loss: 0.0166 | Grad Norm: 0.00970782\n",
      "Epoch 1 | Step 585700 | Avg Loss: 0.0161 | Grad Norm: 0.00863713\n",
      "Epoch 1 | Step 585800 | Avg Loss: 0.0162 | Grad Norm: 0.00892094\n",
      "Epoch 1 | Step 585900 | Avg Loss: 0.0163 | Grad Norm: 0.00930398\n",
      "Epoch 1 | Step 586000 | Avg Loss: 0.0161 | Grad Norm: 0.00839185\n",
      "Epoch 1 | Step 586100 | Avg Loss: 0.0156 | Grad Norm: 0.00867150\n",
      "Epoch 1 | Step 586200 | Avg Loss: 0.0155 | Grad Norm: 0.00917665\n",
      "Epoch 1 | Step 586300 | Avg Loss: 0.0156 | Grad Norm: 0.00832687\n",
      "Epoch 1 | Step 586400 | Avg Loss: 0.0157 | Grad Norm: 0.01027289\n",
      "Epoch 1 | Step 586500 | Avg Loss: 0.0155 | Grad Norm: 0.00990863\n",
      "Epoch 1 | Step 586600 | Avg Loss: 0.0160 | Grad Norm: 0.00751199\n",
      "Epoch 1 | Step 586700 | Avg Loss: 0.0160 | Grad Norm: 0.00883959\n",
      "Epoch 1 | Step 586800 | Avg Loss: 0.0159 | Grad Norm: 0.00832759\n",
      "Epoch 1 | Step 586900 | Avg Loss: 0.0162 | Grad Norm: 0.00797390\n",
      "Epoch 1 | Step 587000 | Avg Loss: 0.0159 | Grad Norm: 0.00832040\n",
      "Epoch 1 | Step 587100 | Avg Loss: 0.0159 | Grad Norm: 0.00824824\n",
      "Epoch 1 | Step 587200 | Avg Loss: 0.0160 | Grad Norm: 0.00898482\n",
      "Epoch 1 | Step 587300 | Avg Loss: 0.0157 | Grad Norm: 0.00833607\n",
      "Epoch 1 | Step 587400 | Avg Loss: 0.0161 | Grad Norm: 0.00880591\n",
      "Epoch 1 | Step 587500 | Avg Loss: 0.0160 | Grad Norm: 0.00834820\n",
      "Epoch 1 | Step 587600 | Avg Loss: 0.0164 | Grad Norm: 0.00802987\n",
      "Epoch 1 | Step 587700 | Avg Loss: 0.0163 | Grad Norm: 0.00955799\n",
      "Epoch 1 | Step 587800 | Avg Loss: 0.0160 | Grad Norm: 0.00916945\n",
      "Epoch 1 | Step 587900 | Avg Loss: 0.0161 | Grad Norm: 0.01042669\n",
      "Epoch 1 | Step 588000 | Avg Loss: 0.0161 | Grad Norm: 0.00811384\n",
      "Epoch 1 | Step 588100 | Avg Loss: 0.0160 | Grad Norm: 0.00741880\n",
      "Epoch 1 | Step 588200 | Avg Loss: 0.0160 | Grad Norm: 0.00861732\n",
      "Epoch 1 | Step 588300 | Avg Loss: 0.0155 | Grad Norm: 0.00810288\n",
      "Epoch 1 | Step 588400 | Avg Loss: 0.0155 | Grad Norm: 0.01012641\n",
      "Epoch 1 | Step 588500 | Avg Loss: 0.0157 | Grad Norm: 0.00854533\n",
      "Epoch 1 | Step 588600 | Avg Loss: 0.0157 | Grad Norm: 0.00958293\n",
      "Epoch 1 | Step 588700 | Avg Loss: 0.0158 | Grad Norm: 0.00918831\n",
      "Epoch 1 | Step 588800 | Avg Loss: 0.0158 | Grad Norm: 0.00938371\n",
      "Epoch 1 | Step 588900 | Avg Loss: 0.0155 | Grad Norm: 0.00926231\n",
      "Epoch 1 | Step 589000 | Avg Loss: 0.0157 | Grad Norm: 0.00792465\n",
      "Epoch 1 | Step 589100 | Avg Loss: 0.0154 | Grad Norm: 0.00818010\n",
      "Epoch 1 | Step 589200 | Avg Loss: 0.0151 | Grad Norm: 0.00839755\n",
      "Epoch 1 | Step 589300 | Avg Loss: 0.0155 | Grad Norm: 0.00857254\n",
      "Epoch 1 | Step 589400 | Avg Loss: 0.0155 | Grad Norm: 0.00915104\n",
      "Epoch 1 | Step 589500 | Avg Loss: 0.0155 | Grad Norm: 0.00742924\n",
      "Epoch 1 | Step 589600 | Avg Loss: 0.0156 | Grad Norm: 0.00807311\n",
      "Epoch 1 | Step 589700 | Avg Loss: 0.0156 | Grad Norm: 0.01248374\n",
      "Epoch 1 | Step 589800 | Avg Loss: 0.0158 | Grad Norm: 0.00876659\n",
      "Epoch 1 | Step 589900 | Avg Loss: 0.0154 | Grad Norm: 0.00810802\n",
      "Epoch 1 | Step 590000 | Avg Loss: 0.0156 | Grad Norm: 0.00846247\n",
      "Epoch 1 | Step 590100 | Avg Loss: 0.0155 | Grad Norm: 0.00925217\n",
      "Epoch 1 | Step 590200 | Avg Loss: 0.0153 | Grad Norm: 0.00992043\n",
      "Epoch 1 | Step 590300 | Avg Loss: 0.0153 | Grad Norm: 0.00790123\n",
      "Epoch 1 | Step 590400 | Avg Loss: 0.0156 | Grad Norm: 0.00914903\n",
      "Epoch 1 | Step 590500 | Avg Loss: 0.0155 | Grad Norm: 0.00796454\n",
      "Epoch 1 | Step 590600 | Avg Loss: 0.0156 | Grad Norm: 0.00839773\n",
      "Epoch 1 | Step 590700 | Avg Loss: 0.0155 | Grad Norm: 0.00849844\n",
      "Epoch 1 | Step 590800 | Avg Loss: 0.0155 | Grad Norm: 0.00982744\n",
      "Epoch 1 | Step 590900 | Avg Loss: 0.0153 | Grad Norm: 0.00856108\n",
      "Epoch 1 | Step 591000 | Avg Loss: 0.0154 | Grad Norm: 0.00880708\n",
      "Epoch 1 | Step 591100 | Avg Loss: 0.0152 | Grad Norm: 0.00872818\n",
      "Epoch 1 | Step 591200 | Avg Loss: 0.0156 | Grad Norm: 0.00845013\n",
      "Epoch 1 | Step 591300 | Avg Loss: 0.0156 | Grad Norm: 0.00840605\n",
      "Epoch 1 | Step 591400 | Avg Loss: 0.0158 | Grad Norm: 0.00914012\n",
      "Epoch 1 | Step 591500 | Avg Loss: 0.0162 | Grad Norm: 0.00961837\n",
      "Epoch 1 | Step 591600 | Avg Loss: 0.0160 | Grad Norm: 0.00897700\n",
      "Epoch 1 | Step 591700 | Avg Loss: 0.0162 | Grad Norm: 0.00870009\n",
      "Epoch 1 | Step 591800 | Avg Loss: 0.0161 | Grad Norm: 0.01010575\n",
      "Epoch 1 | Step 591900 | Avg Loss: 0.0166 | Grad Norm: 0.00991204\n",
      "Epoch 1 | Step 592000 | Avg Loss: 0.0164 | Grad Norm: 0.00912310\n",
      "Epoch 1 | Step 592100 | Avg Loss: 0.0164 | Grad Norm: 0.00815476\n",
      "Epoch 1 | Step 592200 | Avg Loss: 0.0159 | Grad Norm: 0.00907433\n",
      "Epoch 1 | Step 592300 | Avg Loss: 0.0159 | Grad Norm: 0.00856146\n",
      "Epoch 1 | Step 592400 | Avg Loss: 0.0158 | Grad Norm: 0.00899827\n",
      "Epoch 1 | Step 592500 | Avg Loss: 0.0156 | Grad Norm: 0.00715281\n",
      "Epoch 1 | Step 592600 | Avg Loss: 0.0155 | Grad Norm: 0.00862614\n",
      "Epoch 1 | Step 592700 | Avg Loss: 0.0157 | Grad Norm: 0.00863691\n",
      "Epoch 1 | Step 592800 | Avg Loss: 0.0152 | Grad Norm: 0.01069836\n",
      "Epoch 1 | Step 592900 | Avg Loss: 0.0152 | Grad Norm: 0.00775305\n",
      "Epoch 1 | Step 593000 | Avg Loss: 0.0157 | Grad Norm: 0.00822233\n",
      "Epoch 1 | Step 593100 | Avg Loss: 0.0154 | Grad Norm: 0.00913677\n",
      "Epoch 1 | Step 593200 | Avg Loss: 0.0153 | Grad Norm: 0.00844066\n",
      "Epoch 1 | Step 593300 | Avg Loss: 0.0156 | Grad Norm: 0.00915975\n",
      "Epoch 1 | Step 593400 | Avg Loss: 0.0159 | Grad Norm: 0.00965755\n",
      "Epoch 1 | Step 593500 | Avg Loss: 0.0157 | Grad Norm: 0.00807401\n",
      "Epoch 1 | Step 593600 | Avg Loss: 0.0156 | Grad Norm: 0.00896333\n",
      "Epoch 1 | Step 593700 | Avg Loss: 0.0158 | Grad Norm: 0.00778640\n",
      "Epoch 1 | Step 593800 | Avg Loss: 0.0159 | Grad Norm: 0.00882995\n",
      "Epoch 1 | Step 593900 | Avg Loss: 0.0157 | Grad Norm: 0.00884778\n",
      "Epoch 1 | Step 594000 | Avg Loss: 0.0158 | Grad Norm: 0.00892801\n",
      "Epoch 1 | Step 594100 | Avg Loss: 0.0159 | Grad Norm: 0.00900188\n",
      "Epoch 1 | Step 594200 | Avg Loss: 0.0156 | Grad Norm: 0.00911587\n",
      "Epoch 1 | Step 594300 | Avg Loss: 0.0158 | Grad Norm: 0.00774411\n",
      "Epoch 1 | Step 594400 | Avg Loss: 0.0157 | Grad Norm: 0.00807335\n",
      "Epoch 1 | Step 594500 | Avg Loss: 0.0156 | Grad Norm: 0.00823109\n",
      "Epoch 1 | Step 594600 | Avg Loss: 0.0155 | Grad Norm: 0.00846063\n",
      "Epoch 1 | Step 594700 | Avg Loss: 0.0153 | Grad Norm: 0.00800735\n",
      "Epoch 1 | Step 594800 | Avg Loss: 0.0155 | Grad Norm: 0.00993690\n",
      "Epoch 1 | Step 594900 | Avg Loss: 0.0154 | Grad Norm: 0.00814514\n",
      "Epoch 1 | Step 595000 | Avg Loss: 0.0156 | Grad Norm: 0.00774726\n",
      "Epoch 1 | Step 595100 | Avg Loss: 0.0160 | Grad Norm: 0.00941259\n",
      "Epoch 1 | Step 595200 | Avg Loss: 0.0156 | Grad Norm: 0.01073286\n",
      "Epoch 1 | Step 595300 | Avg Loss: 0.0156 | Grad Norm: 0.00897138\n",
      "Epoch 1 | Step 595400 | Avg Loss: 0.0158 | Grad Norm: 0.00964283\n",
      "Epoch 1 | Step 595500 | Avg Loss: 0.0160 | Grad Norm: 0.00791875\n",
      "Epoch 1 | Step 595600 | Avg Loss: 0.0158 | Grad Norm: 0.00865834\n",
      "Epoch 1 | Step 595700 | Avg Loss: 0.0161 | Grad Norm: 0.00923434\n",
      "Epoch 1 | Step 595800 | Avg Loss: 0.0160 | Grad Norm: 0.01053157\n",
      "Epoch 1 | Step 595900 | Avg Loss: 0.0160 | Grad Norm: 0.00824501\n",
      "Epoch 1 | Step 596000 | Avg Loss: 0.0160 | Grad Norm: 0.00869889\n",
      "Epoch 1 | Step 596100 | Avg Loss: 0.0163 | Grad Norm: 0.00888808\n",
      "Epoch 1 | Step 596200 | Avg Loss: 0.0163 | Grad Norm: 0.00912206\n",
      "Epoch 1 | Step 596300 | Avg Loss: 0.0160 | Grad Norm: 0.00864720\n",
      "Epoch 1 | Step 596400 | Avg Loss: 0.0161 | Grad Norm: 0.00885628\n",
      "Epoch 1 | Step 596500 | Avg Loss: 0.0161 | Grad Norm: 0.00872252\n",
      "Epoch 1 | Step 596600 | Avg Loss: 0.0159 | Grad Norm: 0.00836688\n",
      "Epoch 1 | Step 596700 | Avg Loss: 0.0158 | Grad Norm: 0.00977939\n",
      "Epoch 1 | Step 596800 | Avg Loss: 0.0159 | Grad Norm: 0.00873419\n",
      "Epoch 1 | Step 596900 | Avg Loss: 0.0161 | Grad Norm: 0.00937410\n",
      "Epoch 1 | Step 597000 | Avg Loss: 0.0160 | Grad Norm: 0.00910675\n",
      "Epoch 1 | Step 597100 | Avg Loss: 0.0158 | Grad Norm: 0.00894446\n",
      "Epoch 1 | Step 597200 | Avg Loss: 0.0157 | Grad Norm: 0.00892508\n",
      "Epoch 1 | Step 597300 | Avg Loss: 0.0158 | Grad Norm: 0.00856815\n",
      "Epoch 1 | Step 597400 | Avg Loss: 0.0155 | Grad Norm: 0.00847761\n",
      "Epoch 1 | Step 597500 | Avg Loss: 0.0156 | Grad Norm: 0.00833381\n",
      "Epoch 1 | Step 597600 | Avg Loss: 0.0156 | Grad Norm: 0.00883625\n",
      "Epoch 1 | Step 597700 | Avg Loss: 0.0156 | Grad Norm: 0.01131117\n",
      "Epoch 1 | Step 597800 | Avg Loss: 0.0156 | Grad Norm: 0.00804555\n",
      "Epoch 1 | Step 597900 | Avg Loss: 0.0156 | Grad Norm: 0.01111942\n",
      "Epoch 1 | Step 598000 | Avg Loss: 0.0156 | Grad Norm: 0.00808707\n",
      "Epoch 1 | Step 598100 | Avg Loss: 0.0158 | Grad Norm: 0.00795189\n",
      "Epoch 1 | Step 598200 | Avg Loss: 0.0162 | Grad Norm: 0.00823288\n",
      "Epoch 1 | Step 598300 | Avg Loss: 0.0160 | Grad Norm: 0.00812043\n",
      "Epoch 1 | Step 598400 | Avg Loss: 0.0157 | Grad Norm: 0.00930699\n",
      "Epoch 1 | Step 598500 | Avg Loss: 0.0158 | Grad Norm: 0.00801357\n",
      "Epoch 1 | Step 598600 | Avg Loss: 0.0154 | Grad Norm: 0.00866916\n",
      "Epoch 1 | Step 598700 | Avg Loss: 0.0150 | Grad Norm: 0.00944420\n",
      "Epoch 1 | Step 598800 | Avg Loss: 0.0150 | Grad Norm: 0.01031634\n",
      "Epoch 1 | Step 598900 | Avg Loss: 0.0155 | Grad Norm: 0.00850984\n",
      "Epoch 1 | Step 599000 | Avg Loss: 0.0156 | Grad Norm: 0.00872640\n",
      "Epoch 1 | Step 599100 | Avg Loss: 0.0154 | Grad Norm: 0.00882237\n",
      "Epoch 1 | Step 599200 | Avg Loss: 0.0156 | Grad Norm: 0.00870659\n",
      "Epoch 1 | Step 599300 | Avg Loss: 0.0159 | Grad Norm: 0.00814873\n",
      "Epoch 1 | Step 599400 | Avg Loss: 0.0161 | Grad Norm: 0.00813129\n",
      "Epoch 1 | Step 599500 | Avg Loss: 0.0156 | Grad Norm: 0.00856940\n",
      "Epoch 1 | Step 599600 | Avg Loss: 0.0155 | Grad Norm: 0.00741053\n",
      "Epoch 1 | Step 599700 | Avg Loss: 0.0154 | Grad Norm: 0.00960892\n",
      "Epoch 1 | Step 599800 | Avg Loss: 0.0154 | Grad Norm: 0.00832810\n",
      "Epoch 1 | Step 599900 | Avg Loss: 0.0156 | Grad Norm: 0.00841717\n",
      "Epoch 1 | Step 600000 | Avg Loss: 0.0155 | Grad Norm: 0.00880378\n",
      "Saving model at step600000\n",
      "Epoch 1 | Step 600100 | Avg Loss: 0.0152 | Grad Norm: 0.00904743\n",
      "Epoch 1 | Step 600200 | Avg Loss: 0.0156 | Grad Norm: 0.00944664\n",
      "Epoch 1 | Step 600300 | Avg Loss: 0.0152 | Grad Norm: 0.00840433\n",
      "Epoch 1 | Step 600400 | Avg Loss: 0.0152 | Grad Norm: 0.00940596\n",
      "Epoch 1 | Step 600500 | Avg Loss: 0.0154 | Grad Norm: 0.00855359\n",
      "Epoch 1 | Step 600600 | Avg Loss: 0.0157 | Grad Norm: 0.00872442\n",
      "Epoch 1 | Step 600700 | Avg Loss: 0.0158 | Grad Norm: 0.00800099\n",
      "Epoch 1 | Step 600800 | Avg Loss: 0.0160 | Grad Norm: 0.00870950\n",
      "Epoch 1 | Step 600900 | Avg Loss: 0.0158 | Grad Norm: 0.00811162\n",
      "Epoch 1 | Step 601000 | Avg Loss: 0.0162 | Grad Norm: 0.00972673\n",
      "Epoch 1 | Step 601100 | Avg Loss: 0.0159 | Grad Norm: 0.00840923\n",
      "Epoch 1 | Step 601200 | Avg Loss: 0.0158 | Grad Norm: 0.00872207\n",
      "Epoch 1 | Step 601300 | Avg Loss: 0.0157 | Grad Norm: 0.00896126\n",
      "Epoch 1 | Step 601400 | Avg Loss: 0.0158 | Grad Norm: 0.00896374\n",
      "Epoch 1 | Step 601500 | Avg Loss: 0.0156 | Grad Norm: 0.00901617\n",
      "Epoch 1 | Step 601600 | Avg Loss: 0.0157 | Grad Norm: 0.00879351\n",
      "Epoch 1 | Step 601700 | Avg Loss: 0.0158 | Grad Norm: 0.00848467\n",
      "Epoch 1 | Step 601800 | Avg Loss: 0.0159 | Grad Norm: 0.00904463\n",
      "Epoch 1 | Step 601900 | Avg Loss: 0.0158 | Grad Norm: 0.00818041\n",
      "Epoch 1 | Step 602000 | Avg Loss: 0.0157 | Grad Norm: 0.00879911\n",
      "Epoch 1 | Step 602100 | Avg Loss: 0.0156 | Grad Norm: 0.00749242\n",
      "Epoch 1 | Step 602200 | Avg Loss: 0.0156 | Grad Norm: 0.00774225\n",
      "Epoch 1 | Step 602300 | Avg Loss: 0.0152 | Grad Norm: 0.01082520\n",
      "Epoch 1 | Step 602400 | Avg Loss: 0.0157 | Grad Norm: 0.00950794\n",
      "Epoch 1 | Step 602500 | Avg Loss: 0.0157 | Grad Norm: 0.00954515\n",
      "Epoch 1 | Step 602600 | Avg Loss: 0.0159 | Grad Norm: 0.00919691\n",
      "Epoch 1 | Step 602700 | Avg Loss: 0.0156 | Grad Norm: 0.00899932\n",
      "Epoch 1 | Step 602800 | Avg Loss: 0.0155 | Grad Norm: 0.00843940\n",
      "Epoch 1 | Step 602900 | Avg Loss: 0.0154 | Grad Norm: 0.00829043\n",
      "Epoch 1 | Step 603000 | Avg Loss: 0.0154 | Grad Norm: 0.00829153\n",
      "Epoch 1 | Step 603100 | Avg Loss: 0.0157 | Grad Norm: 0.00830791\n",
      "Epoch 1 | Step 603200 | Avg Loss: 0.0157 | Grad Norm: 0.00818608\n",
      "Epoch 1 | Step 603300 | Avg Loss: 0.0153 | Grad Norm: 0.00854378\n",
      "Epoch 1 | Step 603400 | Avg Loss: 0.0156 | Grad Norm: 0.00837277\n",
      "Epoch 1 | Step 603500 | Avg Loss: 0.0153 | Grad Norm: 0.00779195\n",
      "Epoch 1 | Step 603600 | Avg Loss: 0.0156 | Grad Norm: 0.00902920\n",
      "Epoch 1 | Step 603700 | Avg Loss: 0.0156 | Grad Norm: 0.01095093\n",
      "Epoch 1 | Step 603800 | Avg Loss: 0.0157 | Grad Norm: 0.00879626\n",
      "Epoch 1 | Step 603900 | Avg Loss: 0.0156 | Grad Norm: 0.00961373\n",
      "Epoch 1 | Step 604000 | Avg Loss: 0.0159 | Grad Norm: 0.01100467\n",
      "Epoch 1 | Step 604100 | Avg Loss: 0.0159 | Grad Norm: 0.00874229\n",
      "Epoch 1 | Step 604200 | Avg Loss: 0.0156 | Grad Norm: 0.00766790\n",
      "Epoch 1 | Step 604300 | Avg Loss: 0.0156 | Grad Norm: 0.01035815\n",
      "Epoch 1 | Step 604400 | Avg Loss: 0.0156 | Grad Norm: 0.00781157\n",
      "Epoch 1 | Step 604500 | Avg Loss: 0.0158 | Grad Norm: 0.00778377\n",
      "Epoch 1 | Step 604600 | Avg Loss: 0.0161 | Grad Norm: 0.00810090\n",
      "Epoch 1 | Step 604700 | Avg Loss: 0.0160 | Grad Norm: 0.00929735\n",
      "Epoch 1 | Step 604800 | Avg Loss: 0.0158 | Grad Norm: 0.00980812\n",
      "Epoch 1 | Step 604900 | Avg Loss: 0.0156 | Grad Norm: 0.00822677\n",
      "Epoch 1 | Step 605000 | Avg Loss: 0.0160 | Grad Norm: 0.00977636\n",
      "Epoch 1 | Step 605100 | Avg Loss: 0.0163 | Grad Norm: 0.01044035\n",
      "Epoch 1 | Step 605200 | Avg Loss: 0.0159 | Grad Norm: 0.00855160\n",
      "Epoch 1 | Step 605300 | Avg Loss: 0.0154 | Grad Norm: 0.00778160\n",
      "Epoch 1 | Step 605400 | Avg Loss: 0.0157 | Grad Norm: 0.01204822\n",
      "Epoch 1 | Step 605500 | Avg Loss: 0.0159 | Grad Norm: 0.00978440\n",
      "Epoch 1 | Step 605600 | Avg Loss: 0.0160 | Grad Norm: 0.00859815\n",
      "Epoch 1 | Step 605700 | Avg Loss: 0.0160 | Grad Norm: 0.00907386\n",
      "Epoch 1 | Step 605800 | Avg Loss: 0.0154 | Grad Norm: 0.00808350\n",
      "Epoch 1 | Step 605900 | Avg Loss: 0.0150 | Grad Norm: 0.00828753\n",
      "Epoch 1 | Step 606000 | Avg Loss: 0.0147 | Grad Norm: 0.00934952\n",
      "Epoch 1 | Step 606100 | Avg Loss: 0.0149 | Grad Norm: 0.00818862\n",
      "Epoch 1 | Step 606200 | Avg Loss: 0.0154 | Grad Norm: 0.00934006\n",
      "Epoch 1 | Step 606300 | Avg Loss: 0.0158 | Grad Norm: 0.00848235\n",
      "Epoch 1 | Step 606400 | Avg Loss: 0.0160 | Grad Norm: 0.00792579\n",
      "Epoch 1 | Step 606500 | Avg Loss: 0.0158 | Grad Norm: 0.00880993\n",
      "Epoch 1 | Step 606600 | Avg Loss: 0.0161 | Grad Norm: 0.01077673\n",
      "Epoch 1 | Step 606700 | Avg Loss: 0.0159 | Grad Norm: 0.00857103\n",
      "Epoch 1 | Step 606800 | Avg Loss: 0.0158 | Grad Norm: 0.00791437\n",
      "Epoch 1 | Step 606900 | Avg Loss: 0.0158 | Grad Norm: 0.00867783\n",
      "Epoch 1 | Step 607000 | Avg Loss: 0.0156 | Grad Norm: 0.00871592\n",
      "Epoch 1 | Step 607100 | Avg Loss: 0.0157 | Grad Norm: 0.00818800\n",
      "Epoch 1 | Step 607200 | Avg Loss: 0.0160 | Grad Norm: 0.00996333\n",
      "Epoch 1 | Step 607300 | Avg Loss: 0.0154 | Grad Norm: 0.00946218\n",
      "Epoch 1 | Step 607400 | Avg Loss: 0.0155 | Grad Norm: 0.00793988\n",
      "Epoch 1 | Step 607500 | Avg Loss: 0.0154 | Grad Norm: 0.00867576\n",
      "Epoch 1 | Step 607600 | Avg Loss: 0.0157 | Grad Norm: 0.00803686\n",
      "Epoch 1 | Step 607700 | Avg Loss: 0.0158 | Grad Norm: 0.01023860\n",
      "Epoch 1 | Step 607800 | Avg Loss: 0.0157 | Grad Norm: 0.00813767\n",
      "Epoch 1 | Step 607900 | Avg Loss: 0.0158 | Grad Norm: 0.00866776\n",
      "Epoch 1 | Step 608000 | Avg Loss: 0.0157 | Grad Norm: 0.00918544\n",
      "Epoch 1 | Step 608100 | Avg Loss: 0.0159 | Grad Norm: 0.01224932\n",
      "Epoch 1 | Step 608200 | Avg Loss: 0.0155 | Grad Norm: 0.00830136\n",
      "Epoch 1 | Step 608300 | Avg Loss: 0.0156 | Grad Norm: 0.00919340\n",
      "Epoch 1 | Step 608400 | Avg Loss: 0.0157 | Grad Norm: 0.00843340\n",
      "Epoch 1 | Step 608500 | Avg Loss: 0.0158 | Grad Norm: 0.00909645\n",
      "Epoch 1 | Step 608600 | Avg Loss: 0.0159 | Grad Norm: 0.01008755\n",
      "Epoch 1 | Step 608700 | Avg Loss: 0.0160 | Grad Norm: 0.01029626\n",
      "Epoch 1 | Step 608800 | Avg Loss: 0.0158 | Grad Norm: 0.00944368\n",
      "Epoch 1 | Step 608900 | Avg Loss: 0.0160 | Grad Norm: 0.00927356\n",
      "Epoch 1 | Step 609000 | Avg Loss: 0.0159 | Grad Norm: 0.00824196\n",
      "Epoch 1 | Step 609100 | Avg Loss: 0.0157 | Grad Norm: 0.00967058\n",
      "Epoch 1 | Step 609200 | Avg Loss: 0.0156 | Grad Norm: 0.00948897\n",
      "Epoch 1 | Step 609300 | Avg Loss: 0.0153 | Grad Norm: 0.00975369\n",
      "Epoch 1 | Step 609400 | Avg Loss: 0.0156 | Grad Norm: 0.00997639\n",
      "Epoch 1 | Step 609500 | Avg Loss: 0.0153 | Grad Norm: 0.00785793\n",
      "Epoch 1 | Step 609600 | Avg Loss: 0.0154 | Grad Norm: 0.00928692\n",
      "Epoch 1 | Step 609700 | Avg Loss: 0.0158 | Grad Norm: 0.00797869\n",
      "Epoch 1 | Step 609800 | Avg Loss: 0.0157 | Grad Norm: 0.00879291\n",
      "Epoch 1 | Step 609900 | Avg Loss: 0.0158 | Grad Norm: 0.00882303\n",
      "Epoch 1 | Step 610000 | Avg Loss: 0.0157 | Grad Norm: 0.00894597\n",
      "Epoch 1 | Step 610100 | Avg Loss: 0.0160 | Grad Norm: 0.00994308\n",
      "Epoch 1 | Step 610200 | Avg Loss: 0.0162 | Grad Norm: 0.00818906\n",
      "Epoch 1 | Step 610300 | Avg Loss: 0.0158 | Grad Norm: 0.01087271\n",
      "Epoch 1 | Step 610400 | Avg Loss: 0.0156 | Grad Norm: 0.00947027\n",
      "Epoch 1 | Step 610500 | Avg Loss: 0.0155 | Grad Norm: 0.00904103\n",
      "Epoch 1 | Step 610600 | Avg Loss: 0.0158 | Grad Norm: 0.00941968\n",
      "Epoch 1 | Step 610700 | Avg Loss: 0.0158 | Grad Norm: 0.00822174\n",
      "Epoch 1 | Step 610800 | Avg Loss: 0.0157 | Grad Norm: 0.01052996\n",
      "Epoch 1 | Step 610900 | Avg Loss: 0.0156 | Grad Norm: 0.00930765\n",
      "Epoch 1 | Step 611000 | Avg Loss: 0.0159 | Grad Norm: 0.00973399\n",
      "Epoch 1 | Step 611100 | Avg Loss: 0.0157 | Grad Norm: 0.00849127\n",
      "Epoch 1 | Step 611200 | Avg Loss: 0.0157 | Grad Norm: 0.00929931\n",
      "Epoch 1 | Step 611300 | Avg Loss: 0.0155 | Grad Norm: 0.00949324\n",
      "Epoch 1 | Step 611400 | Avg Loss: 0.0153 | Grad Norm: 0.00901811\n",
      "Epoch 1 | Step 611500 | Avg Loss: 0.0157 | Grad Norm: 0.01248815\n",
      "Epoch 1 | Step 611600 | Avg Loss: 0.0158 | Grad Norm: 0.00971491\n",
      "Epoch 1 | Step 611700 | Avg Loss: 0.0161 | Grad Norm: 0.01055695\n",
      "Epoch 1 | Step 611800 | Avg Loss: 0.0157 | Grad Norm: 0.00898739\n",
      "Epoch 1 | Step 611900 | Avg Loss: 0.0159 | Grad Norm: 0.00923783\n",
      "Epoch 1 | Step 612000 | Avg Loss: 0.0156 | Grad Norm: 0.00821900\n",
      "Epoch 1 | Step 612100 | Avg Loss: 0.0160 | Grad Norm: 0.00963267\n",
      "Epoch 1 | Step 612200 | Avg Loss: 0.0159 | Grad Norm: 0.01085236\n",
      "Epoch 1 | Step 612300 | Avg Loss: 0.0159 | Grad Norm: 0.01010610\n",
      "Epoch 1 | Step 612400 | Avg Loss: 0.0158 | Grad Norm: 0.00852948\n",
      "Epoch 1 | Step 612500 | Avg Loss: 0.0155 | Grad Norm: 0.00870465\n",
      "Epoch 1 | Step 612600 | Avg Loss: 0.0158 | Grad Norm: 0.00884321\n",
      "Epoch 1 | Step 612700 | Avg Loss: 0.0157 | Grad Norm: 0.01026633\n",
      "Epoch 1 | Step 612800 | Avg Loss: 0.0156 | Grad Norm: 0.00936335\n",
      "Epoch 1 | Step 612900 | Avg Loss: 0.0156 | Grad Norm: 0.00763425\n",
      "Epoch 1 | Step 613000 | Avg Loss: 0.0155 | Grad Norm: 0.00873001\n",
      "Epoch 1 | Step 613100 | Avg Loss: 0.0159 | Grad Norm: 0.00948482\n",
      "Epoch 1 | Step 613200 | Avg Loss: 0.0157 | Grad Norm: 0.00754441\n",
      "Epoch 1 | Step 613300 | Avg Loss: 0.0163 | Grad Norm: 0.00976972\n",
      "Epoch 1 | Step 613400 | Avg Loss: 0.0160 | Grad Norm: 0.01077367\n",
      "Epoch 1 | Step 613500 | Avg Loss: 0.0157 | Grad Norm: 0.00918374\n",
      "Epoch 1 | Step 613600 | Avg Loss: 0.0161 | Grad Norm: 0.00825563\n",
      "Epoch 1 | Step 613700 | Avg Loss: 0.0161 | Grad Norm: 0.00969074\n",
      "Epoch 1 | Step 613800 | Avg Loss: 0.0160 | Grad Norm: 0.00963758\n",
      "Epoch 1 | Step 613900 | Avg Loss: 0.0159 | Grad Norm: 0.00840403\n",
      "Epoch 1 | Step 614000 | Avg Loss: 0.0159 | Grad Norm: 0.00939481\n",
      "Epoch 1 | Step 614100 | Avg Loss: 0.0159 | Grad Norm: 0.00866582\n",
      "Epoch 1 | Step 614200 | Avg Loss: 0.0160 | Grad Norm: 0.00862511\n",
      "Epoch 1 | Step 614300 | Avg Loss: 0.0160 | Grad Norm: 0.00850642\n",
      "Epoch 1 | Step 614400 | Avg Loss: 0.0157 | Grad Norm: 0.00966906\n",
      "Epoch 1 | Step 614500 | Avg Loss: 0.0155 | Grad Norm: 0.00821617\n",
      "Epoch 1 | Step 614600 | Avg Loss: 0.0158 | Grad Norm: 0.00870075\n",
      "Epoch 1 | Step 614700 | Avg Loss: 0.0157 | Grad Norm: 0.00833648\n",
      "Epoch 1 | Step 614800 | Avg Loss: 0.0155 | Grad Norm: 0.00946756\n",
      "Epoch 1 | Step 614900 | Avg Loss: 0.0158 | Grad Norm: 0.00967411\n",
      "Epoch 1 | Step 615000 | Avg Loss: 0.0161 | Grad Norm: 0.00832553\n",
      "Epoch 1 | Step 615100 | Avg Loss: 0.0159 | Grad Norm: 0.00953497\n",
      "Epoch 1 | Step 615200 | Avg Loss: 0.0156 | Grad Norm: 0.00831716\n",
      "Epoch 1 | Step 615300 | Avg Loss: 0.0160 | Grad Norm: 0.00832413\n",
      "Epoch 1 | Step 615400 | Avg Loss: 0.0158 | Grad Norm: 0.00940337\n",
      "Epoch 1 | Step 615500 | Avg Loss: 0.0157 | Grad Norm: 0.00842768\n",
      "Epoch 1 | Step 615600 | Avg Loss: 0.0160 | Grad Norm: 0.00846157\n",
      "Epoch 1 | Step 615700 | Avg Loss: 0.0159 | Grad Norm: 0.00873397\n",
      "Epoch 1 | Step 615800 | Avg Loss: 0.0159 | Grad Norm: 0.00722196\n",
      "Epoch 1 | Step 615900 | Avg Loss: 0.0160 | Grad Norm: 0.00915607\n",
      "Epoch 1 | Step 616000 | Avg Loss: 0.0160 | Grad Norm: 0.00915734\n",
      "Epoch 1 | Step 616100 | Avg Loss: 0.0160 | Grad Norm: 0.00839663\n",
      "Epoch 1 | Step 616200 | Avg Loss: 0.0158 | Grad Norm: 0.00930860\n",
      "Epoch 1 | Step 616300 | Avg Loss: 0.0161 | Grad Norm: 0.00885699\n",
      "Epoch 1 | Step 616400 | Avg Loss: 0.0158 | Grad Norm: 0.00972483\n",
      "Epoch 1 | Step 616500 | Avg Loss: 0.0159 | Grad Norm: 0.00802582\n",
      "Epoch 1 | Step 616600 | Avg Loss: 0.0162 | Grad Norm: 0.00772134\n",
      "Epoch 1 | Step 616700 | Avg Loss: 0.0159 | Grad Norm: 0.00852642\n",
      "Epoch 1 | Step 616800 | Avg Loss: 0.0160 | Grad Norm: 0.00955885\n",
      "Epoch 1 | Step 616900 | Avg Loss: 0.0163 | Grad Norm: 0.01278767\n",
      "Epoch 1 | Step 617000 | Avg Loss: 0.0163 | Grad Norm: 0.00847456\n",
      "Epoch 1 | Step 617100 | Avg Loss: 0.0158 | Grad Norm: 0.00967241\n",
      "Epoch 1 | Step 617200 | Avg Loss: 0.0158 | Grad Norm: 0.00878833\n",
      "Epoch 1 | Step 617300 | Avg Loss: 0.0157 | Grad Norm: 0.00839330\n",
      "Epoch 1 | Step 617400 | Avg Loss: 0.0157 | Grad Norm: 0.00944323\n",
      "Epoch 1 | Step 617500 | Avg Loss: 0.0161 | Grad Norm: 0.00895678\n",
      "Epoch 1 | Step 617600 | Avg Loss: 0.0159 | Grad Norm: 0.00863389\n",
      "Epoch 1 | Step 617700 | Avg Loss: 0.0162 | Grad Norm: 0.01083842\n",
      "Epoch 1 | Step 617800 | Avg Loss: 0.0160 | Grad Norm: 0.00833815\n",
      "Epoch 1 | Step 617900 | Avg Loss: 0.0162 | Grad Norm: 0.00936043\n",
      "Epoch 1 | Step 618000 | Avg Loss: 0.0161 | Grad Norm: 0.00845625\n",
      "Epoch 1 | Step 618100 | Avg Loss: 0.0157 | Grad Norm: 0.00838328\n",
      "Epoch 1 | Step 618200 | Avg Loss: 0.0157 | Grad Norm: 0.00912445\n",
      "Epoch 1 | Step 618300 | Avg Loss: 0.0153 | Grad Norm: 0.01011902\n",
      "Epoch 1 | Step 618400 | Avg Loss: 0.0155 | Grad Norm: 0.00809743\n",
      "Epoch 1 | Step 618500 | Avg Loss: 0.0154 | Grad Norm: 0.00826313\n",
      "Epoch 1 | Step 618600 | Avg Loss: 0.0155 | Grad Norm: 0.00886009\n",
      "Epoch 1 | Step 618700 | Avg Loss: 0.0154 | Grad Norm: 0.00997066\n",
      "Epoch 1 | Step 618800 | Avg Loss: 0.0157 | Grad Norm: 0.01064105\n",
      "Epoch 1 | Step 618900 | Avg Loss: 0.0155 | Grad Norm: 0.00906808\n",
      "Epoch 1 | Step 619000 | Avg Loss: 0.0157 | Grad Norm: 0.00943786\n",
      "Epoch 1 | Step 619100 | Avg Loss: 0.0156 | Grad Norm: 0.00917256\n",
      "Epoch 1 | Step 619200 | Avg Loss: 0.0158 | Grad Norm: 0.01040346\n",
      "Epoch 1 | Step 619300 | Avg Loss: 0.0157 | Grad Norm: 0.00861442\n",
      "Epoch 1 | Step 619400 | Avg Loss: 0.0160 | Grad Norm: 0.00794428\n",
      "Epoch 1 | Step 619500 | Avg Loss: 0.0159 | Grad Norm: 0.00809096\n",
      "Epoch 1 | Step 619600 | Avg Loss: 0.0160 | Grad Norm: 0.00925251\n",
      "Epoch 1 | Step 619700 | Avg Loss: 0.0162 | Grad Norm: 0.00842964\n",
      "Epoch 1 | Step 619800 | Avg Loss: 0.0161 | Grad Norm: 0.00756514\n",
      "Epoch 1 | Step 619900 | Avg Loss: 0.0160 | Grad Norm: 0.00911558\n",
      "Epoch 1 | Step 620000 | Avg Loss: 0.0158 | Grad Norm: 0.00867556\n",
      "Epoch 1 | Step 620100 | Avg Loss: 0.0158 | Grad Norm: 0.00900459\n",
      "Epoch 1 | Step 620200 | Avg Loss: 0.0157 | Grad Norm: 0.00849688\n",
      "Epoch 1 | Step 620300 | Avg Loss: 0.0157 | Grad Norm: 0.00914517\n",
      "Epoch 1 | Step 620400 | Avg Loss: 0.0162 | Grad Norm: 0.00964929\n",
      "Epoch 1 | Step 620500 | Avg Loss: 0.0159 | Grad Norm: 0.00913891\n",
      "Epoch 1 | Step 620600 | Avg Loss: 0.0156 | Grad Norm: 0.00807183\n",
      "Epoch 1 | Step 620700 | Avg Loss: 0.0159 | Grad Norm: 0.00820222\n",
      "Epoch 1 | Step 620800 | Avg Loss: 0.0159 | Grad Norm: 0.00871852\n",
      "Epoch 1 | Step 620900 | Avg Loss: 0.0158 | Grad Norm: 0.00918824\n",
      "Epoch 1 | Step 621000 | Avg Loss: 0.0160 | Grad Norm: 0.00824220\n",
      "Epoch 1 | Step 621100 | Avg Loss: 0.0158 | Grad Norm: 0.00829662\n",
      "Epoch 1 | Step 621200 | Avg Loss: 0.0154 | Grad Norm: 0.01005329\n",
      "Epoch 1 | Step 621300 | Avg Loss: 0.0155 | Grad Norm: 0.00862601\n",
      "Epoch 1 | Step 621400 | Avg Loss: 0.0152 | Grad Norm: 0.00850957\n",
      "Epoch 1 | Step 621500 | Avg Loss: 0.0154 | Grad Norm: 0.00848774\n",
      "Epoch 1 | Step 621600 | Avg Loss: 0.0154 | Grad Norm: 0.00907677\n",
      "Epoch 1 | Step 621700 | Avg Loss: 0.0158 | Grad Norm: 0.00832455\n",
      "Epoch 1 | Step 621800 | Avg Loss: 0.0157 | Grad Norm: 0.00779370\n",
      "Epoch 1 | Step 621900 | Avg Loss: 0.0156 | Grad Norm: 0.00920155\n",
      "Epoch 1 | Step 622000 | Avg Loss: 0.0158 | Grad Norm: 0.00753738\n",
      "Epoch 1 | Step 622100 | Avg Loss: 0.0155 | Grad Norm: 0.00936877\n",
      "Epoch 1 | Step 622200 | Avg Loss: 0.0154 | Grad Norm: 0.01005045\n",
      "Epoch 1 | Step 622300 | Avg Loss: 0.0157 | Grad Norm: 0.00811495\n",
      "Epoch 1 | Step 622400 | Avg Loss: 0.0154 | Grad Norm: 0.00806319\n",
      "Epoch 1 | Step 622500 | Avg Loss: 0.0155 | Grad Norm: 0.01089736\n",
      "Epoch 1 | Step 622600 | Avg Loss: 0.0158 | Grad Norm: 0.00826943\n",
      "Epoch 1 | Step 622700 | Avg Loss: 0.0154 | Grad Norm: 0.00848094\n",
      "Epoch 1 | Step 622800 | Avg Loss: 0.0153 | Grad Norm: 0.00884247\n",
      "Epoch 1 | Step 622900 | Avg Loss: 0.0153 | Grad Norm: 0.00793034\n",
      "Epoch 1 | Step 623000 | Avg Loss: 0.0154 | Grad Norm: 0.00814590\n",
      "Epoch 1 | Step 623100 | Avg Loss: 0.0153 | Grad Norm: 0.00813726\n",
      "Epoch 1 | Step 623200 | Avg Loss: 0.0151 | Grad Norm: 0.01338117\n",
      "Epoch 1 | Step 623300 | Avg Loss: 0.0153 | Grad Norm: 0.00918925\n",
      "Epoch 1 | Step 623400 | Avg Loss: 0.0151 | Grad Norm: 0.00804634\n",
      "Epoch 1 | Step 623500 | Avg Loss: 0.0149 | Grad Norm: 0.00824352\n",
      "Epoch 1 | Step 623600 | Avg Loss: 0.0148 | Grad Norm: 0.00701293\n",
      "Epoch 1 | Step 623700 | Avg Loss: 0.0151 | Grad Norm: 0.00951451\n",
      "Epoch 1 | Step 623800 | Avg Loss: 0.0156 | Grad Norm: 0.00804833\n",
      "Epoch 1 | Step 623900 | Avg Loss: 0.0158 | Grad Norm: 0.00913480\n",
      "Epoch 1 | Step 624000 | Avg Loss: 0.0154 | Grad Norm: 0.00836018\n",
      "Epoch 1 | Step 624100 | Avg Loss: 0.0155 | Grad Norm: 0.00960078\n",
      "Epoch 1 | Step 624200 | Avg Loss: 0.0155 | Grad Norm: 0.01091114\n",
      "Epoch 1 | Step 624300 | Avg Loss: 0.0160 | Grad Norm: 0.00851363\n",
      "Epoch 1 | Step 624400 | Avg Loss: 0.0157 | Grad Norm: 0.00903667\n",
      "Epoch 1 | Step 624500 | Avg Loss: 0.0157 | Grad Norm: 0.01017816\n",
      "Epoch 1 | Step 624600 | Avg Loss: 0.0157 | Grad Norm: 0.00912872\n",
      "Epoch 1 | Step 624700 | Avg Loss: 0.0156 | Grad Norm: 0.01023198\n",
      "Epoch 1 | Step 624800 | Avg Loss: 0.0156 | Grad Norm: 0.00939618\n",
      "Epoch 1 | Step 624900 | Avg Loss: 0.0155 | Grad Norm: 0.00910849\n",
      "Epoch 1 | Step 625000 | Avg Loss: 0.0155 | Grad Norm: 0.00879439\n",
      "Epoch 1 | Step 625100 | Avg Loss: 0.0153 | Grad Norm: 0.00779782\n",
      "Epoch 1 | Step 625200 | Avg Loss: 0.0149 | Grad Norm: 0.00940755\n",
      "Epoch 1 | Step 625300 | Avg Loss: 0.0148 | Grad Norm: 0.00907567\n",
      "Epoch 1 | Step 625400 | Avg Loss: 0.0148 | Grad Norm: 0.00917452\n",
      "Epoch 1 | Step 625500 | Avg Loss: 0.0151 | Grad Norm: 0.00922046\n",
      "Epoch 1 | Step 625600 | Avg Loss: 0.0154 | Grad Norm: 0.01071961\n",
      "Epoch 1 | Step 625700 | Avg Loss: 0.0154 | Grad Norm: 0.00959262\n",
      "Epoch 1 | Step 625800 | Avg Loss: 0.0154 | Grad Norm: 0.00878749\n",
      "Epoch 1 | Step 625900 | Avg Loss: 0.0151 | Grad Norm: 0.00867753\n",
      "Epoch 1 | Step 626000 | Avg Loss: 0.0153 | Grad Norm: 0.00910501\n",
      "Epoch 1 | Step 626100 | Avg Loss: 0.0153 | Grad Norm: 0.00868711\n",
      "Epoch 1 | Step 626200 | Avg Loss: 0.0153 | Grad Norm: 0.00767684\n",
      "Epoch 1 | Step 626300 | Avg Loss: 0.0153 | Grad Norm: 0.00790959\n",
      "Epoch 1 | Step 626400 | Avg Loss: 0.0156 | Grad Norm: 0.00932338\n",
      "Epoch 1 | Step 626500 | Avg Loss: 0.0158 | Grad Norm: 0.00953486\n",
      "Epoch 1 | Step 626600 | Avg Loss: 0.0158 | Grad Norm: 0.00877365\n",
      "Epoch 1 | Step 626700 | Avg Loss: 0.0160 | Grad Norm: 0.00935218\n",
      "Epoch 1 | Step 626800 | Avg Loss: 0.0159 | Grad Norm: 0.00840357\n",
      "Epoch 1 | Step 626900 | Avg Loss: 0.0158 | Grad Norm: 0.00846506\n",
      "Epoch 1 | Step 627000 | Avg Loss: 0.0156 | Grad Norm: 0.00977859\n",
      "Epoch 1 | Step 627100 | Avg Loss: 0.0156 | Grad Norm: 0.00924314\n",
      "Epoch 1 | Step 627200 | Avg Loss: 0.0155 | Grad Norm: 0.00905059\n",
      "Epoch 1 | Step 627300 | Avg Loss: 0.0156 | Grad Norm: 0.00838443\n",
      "Epoch 1 | Step 627400 | Avg Loss: 0.0156 | Grad Norm: 0.00874290\n",
      "Epoch 1 | Step 627500 | Avg Loss: 0.0156 | Grad Norm: 0.00905554\n",
      "Epoch 1 | Step 627600 | Avg Loss: 0.0151 | Grad Norm: 0.00966862\n",
      "Epoch 1 | Step 627700 | Avg Loss: 0.0154 | Grad Norm: 0.00848344\n",
      "Epoch 1 | Step 627800 | Avg Loss: 0.0156 | Grad Norm: 0.00859070\n",
      "Epoch 1 | Step 627900 | Avg Loss: 0.0160 | Grad Norm: 0.00824012\n",
      "Epoch 1 | Step 628000 | Avg Loss: 0.0157 | Grad Norm: 0.00862904\n",
      "Epoch 1 | Step 628100 | Avg Loss: 0.0154 | Grad Norm: 0.00807642\n",
      "Epoch 1 | Step 628200 | Avg Loss: 0.0153 | Grad Norm: 0.00953895\n",
      "Epoch 1 | Step 628300 | Avg Loss: 0.0152 | Grad Norm: 0.00852931\n",
      "Epoch 1 | Step 628400 | Avg Loss: 0.0150 | Grad Norm: 0.00955890\n",
      "Epoch 1 | Step 628500 | Avg Loss: 0.0153 | Grad Norm: 0.00937293\n",
      "Epoch 1 | Step 628600 | Avg Loss: 0.0157 | Grad Norm: 0.00888915\n",
      "Epoch 1 | Step 628700 | Avg Loss: 0.0159 | Grad Norm: 0.00846947\n",
      "Epoch 1 | Step 628800 | Avg Loss: 0.0157 | Grad Norm: 0.01122614\n",
      "Epoch 1 | Step 628900 | Avg Loss: 0.0155 | Grad Norm: 0.00797069\n",
      "Epoch 1 | Step 629000 | Avg Loss: 0.0154 | Grad Norm: 0.00945159\n",
      "Epoch 1 | Step 629100 | Avg Loss: 0.0156 | Grad Norm: 0.00785065\n",
      "Epoch 1 | Step 629200 | Avg Loss: 0.0155 | Grad Norm: 0.00791310\n",
      "Epoch 1 | Step 629300 | Avg Loss: 0.0156 | Grad Norm: 0.00857240\n",
      "Epoch 1 | Step 629400 | Avg Loss: 0.0156 | Grad Norm: 0.00891491\n",
      "Epoch 1 | Step 629500 | Avg Loss: 0.0156 | Grad Norm: 0.00882671\n",
      "Epoch 1 | Step 629600 | Avg Loss: 0.0159 | Grad Norm: 0.00845688\n",
      "Epoch 1 | Step 629700 | Avg Loss: 0.0160 | Grad Norm: 0.00732948\n",
      "Epoch 1 | Step 629800 | Avg Loss: 0.0162 | Grad Norm: 0.00928653\n",
      "Epoch 1 | Step 629900 | Avg Loss: 0.0161 | Grad Norm: 0.00872715\n",
      "Epoch 1 | Step 630000 | Avg Loss: 0.0156 | Grad Norm: 0.00800177\n",
      "Epoch 1 | Step 630100 | Avg Loss: 0.0157 | Grad Norm: 0.00899114\n",
      "Epoch 1 | Step 630200 | Avg Loss: 0.0157 | Grad Norm: 0.00793017\n",
      "Epoch 1 | Step 630300 | Avg Loss: 0.0157 | Grad Norm: 0.00821250\n",
      "Epoch 1 | Step 630400 | Avg Loss: 0.0161 | Grad Norm: 0.00984267\n",
      "Epoch 1 | Step 630500 | Avg Loss: 0.0158 | Grad Norm: 0.00943917\n",
      "Epoch 1 | Step 630600 | Avg Loss: 0.0156 | Grad Norm: 0.00834042\n",
      "Epoch 1 | Step 630700 | Avg Loss: 0.0158 | Grad Norm: 0.00905429\n",
      "Epoch 1 | Step 630800 | Avg Loss: 0.0158 | Grad Norm: 0.00989503\n",
      "Epoch 1 | Step 630900 | Avg Loss: 0.0159 | Grad Norm: 0.00994686\n",
      "Epoch 1 | Step 631000 | Avg Loss: 0.0163 | Grad Norm: 0.00883297\n",
      "Epoch 1 | Step 631100 | Avg Loss: 0.0163 | Grad Norm: 0.00896962\n",
      "Epoch 1 | Step 631200 | Avg Loss: 0.0161 | Grad Norm: 0.00871556\n",
      "Epoch 1 | Step 631300 | Avg Loss: 0.0160 | Grad Norm: 0.00952388\n",
      "Epoch 1 | Step 631400 | Avg Loss: 0.0162 | Grad Norm: 0.00918835\n",
      "Epoch 1 | Step 631500 | Avg Loss: 0.0162 | Grad Norm: 0.00958066\n",
      "Epoch 1 | Step 631600 | Avg Loss: 0.0161 | Grad Norm: 0.00931909\n",
      "Epoch 1 | Step 631700 | Avg Loss: 0.0162 | Grad Norm: 0.01162965\n",
      "Epoch 1 | Step 631800 | Avg Loss: 0.0157 | Grad Norm: 0.00956297\n",
      "Epoch 1 | Step 631900 | Avg Loss: 0.0158 | Grad Norm: 0.00903458\n",
      "Epoch 1 | Step 632000 | Avg Loss: 0.0158 | Grad Norm: 0.01054330\n",
      "Epoch 1 | Step 632100 | Avg Loss: 0.0156 | Grad Norm: 0.00813823\n",
      "Epoch 1 | Step 632200 | Avg Loss: 0.0154 | Grad Norm: 0.00894889\n",
      "Epoch 1 | Step 632300 | Avg Loss: 0.0154 | Grad Norm: 0.00936290\n",
      "Epoch 1 | Step 632400 | Avg Loss: 0.0154 | Grad Norm: 0.00909568\n",
      "Epoch 1 | Step 632500 | Avg Loss: 0.0154 | Grad Norm: 0.01036755\n",
      "Epoch 1 | Step 632600 | Avg Loss: 0.0157 | Grad Norm: 0.00742863\n",
      "Epoch 1 | Step 632700 | Avg Loss: 0.0157 | Grad Norm: 0.00866987\n",
      "Epoch 1 | Step 632800 | Avg Loss: 0.0158 | Grad Norm: 0.00984230\n",
      "Epoch 1 | Step 632900 | Avg Loss: 0.0157 | Grad Norm: 0.00863806\n",
      "Epoch 1 | Step 633000 | Avg Loss: 0.0159 | Grad Norm: 0.00829765\n",
      "Epoch 1 | Step 633100 | Avg Loss: 0.0156 | Grad Norm: 0.01078208\n",
      "Epoch 1 | Step 633200 | Avg Loss: 0.0153 | Grad Norm: 0.00885549\n",
      "Epoch 1 | Step 633300 | Avg Loss: 0.0154 | Grad Norm: 0.00852234\n",
      "Epoch 1 | Step 633400 | Avg Loss: 0.0155 | Grad Norm: 0.00880206\n",
      "Epoch 1 | Step 633500 | Avg Loss: 0.0157 | Grad Norm: 0.00852239\n",
      "Epoch 1 | Step 633600 | Avg Loss: 0.0156 | Grad Norm: 0.00841929\n",
      "Epoch 1 | Step 633700 | Avg Loss: 0.0153 | Grad Norm: 0.00855415\n",
      "Epoch 1 | Step 633800 | Avg Loss: 0.0154 | Grad Norm: 0.00754694\n",
      "Epoch 1 | Step 633900 | Avg Loss: 0.0153 | Grad Norm: 0.00866155\n",
      "Epoch 1 | Step 634000 | Avg Loss: 0.0154 | Grad Norm: 0.00928223\n",
      "Epoch 1 | Step 634100 | Avg Loss: 0.0156 | Grad Norm: 0.01131035\n",
      "Epoch 1 | Step 634200 | Avg Loss: 0.0154 | Grad Norm: 0.01159242\n",
      "Epoch 1 | Step 634300 | Avg Loss: 0.0156 | Grad Norm: 0.00843972\n",
      "Epoch 1 | Step 634400 | Avg Loss: 0.0156 | Grad Norm: 0.00980088\n",
      "Epoch 1 | Step 634500 | Avg Loss: 0.0159 | Grad Norm: 0.00997451\n",
      "Epoch 1 | Step 634600 | Avg Loss: 0.0157 | Grad Norm: 0.00836181\n",
      "Epoch 1 | Step 634700 | Avg Loss: 0.0156 | Grad Norm: 0.00844322\n",
      "Epoch 1 | Step 634800 | Avg Loss: 0.0159 | Grad Norm: 0.00871770\n",
      "Epoch 1 | Step 634900 | Avg Loss: 0.0163 | Grad Norm: 0.00903587\n",
      "Epoch 1 | Step 635000 | Avg Loss: 0.0161 | Grad Norm: 0.00883049\n",
      "Epoch 1 | Step 635100 | Avg Loss: 0.0160 | Grad Norm: 0.00939337\n",
      "Epoch 1 | Step 635200 | Avg Loss: 0.0160 | Grad Norm: 0.01054509\n",
      "Epoch 1 | Step 635300 | Avg Loss: 0.0162 | Grad Norm: 0.00919875\n",
      "Epoch 1 | Step 635400 | Avg Loss: 0.0157 | Grad Norm: 0.00882419\n",
      "Epoch 1 | Step 635500 | Avg Loss: 0.0155 | Grad Norm: 0.00920411\n",
      "Epoch 1 | Step 635600 | Avg Loss: 0.0162 | Grad Norm: 0.00891290\n",
      "Epoch 1 | Step 635700 | Avg Loss: 0.0164 | Grad Norm: 0.00784934\n",
      "Epoch 1 | Step 635800 | Avg Loss: 0.0159 | Grad Norm: 0.00877750\n",
      "Epoch 1 | Step 635900 | Avg Loss: 0.0162 | Grad Norm: 0.00898848\n",
      "Epoch 1 | Step 636000 | Avg Loss: 0.0161 | Grad Norm: 0.00825584\n",
      "Epoch 1 | Step 636100 | Avg Loss: 0.0161 | Grad Norm: 0.01105205\n",
      "Epoch 1 | Step 636200 | Avg Loss: 0.0161 | Grad Norm: 0.00798107\n",
      "Epoch 1 | Step 636300 | Avg Loss: 0.0162 | Grad Norm: 0.01112923\n",
      "Epoch 1 | Step 636400 | Avg Loss: 0.0160 | Grad Norm: 0.00878643\n",
      "Epoch 1 | Step 636500 | Avg Loss: 0.0159 | Grad Norm: 0.00745670\n",
      "Epoch 1 | Step 636600 | Avg Loss: 0.0158 | Grad Norm: 0.00928107\n",
      "Epoch 1 | Step 636700 | Avg Loss: 0.0156 | Grad Norm: 0.00865194\n",
      "Epoch 1 | Step 636800 | Avg Loss: 0.0154 | Grad Norm: 0.00819213\n",
      "Epoch 1 | Step 636900 | Avg Loss: 0.0153 | Grad Norm: 0.00942002\n",
      "Epoch 1 | Step 637000 | Avg Loss: 0.0156 | Grad Norm: 0.00911221\n",
      "Epoch 1 | Step 637100 | Avg Loss: 0.0153 | Grad Norm: 0.00898757\n",
      "Epoch 1 | Step 637200 | Avg Loss: 0.0159 | Grad Norm: 0.00802160\n",
      "Epoch 1 | Step 637300 | Avg Loss: 0.0154 | Grad Norm: 0.00932253\n",
      "Epoch 1 | Step 637400 | Avg Loss: 0.0157 | Grad Norm: 0.00779065\n",
      "Epoch 1 | Step 637500 | Avg Loss: 0.0156 | Grad Norm: 0.00982128\n",
      "Epoch 1 | Step 637600 | Avg Loss: 0.0156 | Grad Norm: 0.00832562\n",
      "Epoch 1 | Step 637700 | Avg Loss: 0.0158 | Grad Norm: 0.00867004\n",
      "Epoch 1 | Step 637800 | Avg Loss: 0.0160 | Grad Norm: 0.00910432\n",
      "Epoch 1 | Step 637900 | Avg Loss: 0.0159 | Grad Norm: 0.00768929\n",
      "Epoch 1 | Step 638000 | Avg Loss: 0.0157 | Grad Norm: 0.00977446\n",
      "Epoch 1 | Step 638100 | Avg Loss: 0.0161 | Grad Norm: 0.00948204\n",
      "Epoch 1 | Step 638200 | Avg Loss: 0.0155 | Grad Norm: 0.00797687\n",
      "Epoch 1 | Step 638300 | Avg Loss: 0.0157 | Grad Norm: 0.00801703\n",
      "Epoch 1 | Step 638400 | Avg Loss: 0.0159 | Grad Norm: 0.00892521\n",
      "Epoch 1 | Step 638500 | Avg Loss: 0.0157 | Grad Norm: 0.00882838\n",
      "Epoch 1 | Step 638600 | Avg Loss: 0.0159 | Grad Norm: 0.00801851\n",
      "Epoch 1 | Step 638700 | Avg Loss: 0.0155 | Grad Norm: 0.00774099\n",
      "Epoch 1 | Step 638800 | Avg Loss: 0.0154 | Grad Norm: 0.00850110\n",
      "Epoch 1 | Step 638900 | Avg Loss: 0.0158 | Grad Norm: 0.01144192\n",
      "Epoch 1 | Step 639000 | Avg Loss: 0.0156 | Grad Norm: 0.00831101\n",
      "Epoch 1 | Step 639100 | Avg Loss: 0.0156 | Grad Norm: 0.00911888\n",
      "Epoch 1 | Step 639200 | Avg Loss: 0.0156 | Grad Norm: 0.00822693\n",
      "Epoch 1 | Step 639300 | Avg Loss: 0.0157 | Grad Norm: 0.00836069\n",
      "Epoch 1 | Step 639400 | Avg Loss: 0.0160 | Grad Norm: 0.00816954\n",
      "Epoch 1 | Step 639500 | Avg Loss: 0.0161 | Grad Norm: 0.00847563\n",
      "Epoch 1 | Step 639600 | Avg Loss: 0.0159 | Grad Norm: 0.00743265\n",
      "Epoch 1 | Step 639700 | Avg Loss: 0.0155 | Grad Norm: 0.01006179\n",
      "Epoch 1 | Step 639800 | Avg Loss: 0.0159 | Grad Norm: 0.01006827\n",
      "Epoch 1 | Step 639900 | Avg Loss: 0.0158 | Grad Norm: 0.00878738\n",
      "Epoch 1 | Step 640000 | Avg Loss: 0.0161 | Grad Norm: 0.00939267\n",
      "Epoch 1 | Step 640100 | Avg Loss: 0.0161 | Grad Norm: 0.00891934\n",
      "Epoch 1 | Step 640200 | Avg Loss: 0.0156 | Grad Norm: 0.01246359\n",
      "Epoch 1 | Step 640300 | Avg Loss: 0.0156 | Grad Norm: 0.00784685\n",
      "Epoch 1 | Step 640400 | Avg Loss: 0.0154 | Grad Norm: 0.01171014\n",
      "Epoch 1 | Step 640500 | Avg Loss: 0.0154 | Grad Norm: 0.00922457\n",
      "Epoch 1 | Step 640600 | Avg Loss: 0.0154 | Grad Norm: 0.00855657\n",
      "Epoch 1 | Step 640700 | Avg Loss: 0.0154 | Grad Norm: 0.00822211\n",
      "Epoch 1 | Step 640800 | Avg Loss: 0.0150 | Grad Norm: 0.00843689\n",
      "Epoch 1 | Step 640900 | Avg Loss: 0.0149 | Grad Norm: 0.00894042\n",
      "Epoch 1 | Step 641000 | Avg Loss: 0.0154 | Grad Norm: 0.00787032\n",
      "Epoch 1 | Step 641100 | Avg Loss: 0.0155 | Grad Norm: 0.00798138\n",
      "Epoch 1 | Step 641200 | Avg Loss: 0.0154 | Grad Norm: 0.00700838\n",
      "Epoch 1 | Step 641300 | Avg Loss: 0.0154 | Grad Norm: 0.00871893\n",
      "Epoch 1 | Step 641400 | Avg Loss: 0.0158 | Grad Norm: 0.01051205\n",
      "Epoch 1 | Step 641500 | Avg Loss: 0.0159 | Grad Norm: 0.00884181\n",
      "Epoch 1 | Step 641600 | Avg Loss: 0.0156 | Grad Norm: 0.00821055\n",
      "Epoch 1 | Step 641700 | Avg Loss: 0.0152 | Grad Norm: 0.00810871\n",
      "Epoch 1 | Step 641800 | Avg Loss: 0.0153 | Grad Norm: 0.00807138\n",
      "Epoch 1 | Step 641900 | Avg Loss: 0.0153 | Grad Norm: 0.00868842\n",
      "Epoch 1 | Step 642000 | Avg Loss: 0.0155 | Grad Norm: 0.00810812\n",
      "Epoch 1 | Step 642100 | Avg Loss: 0.0158 | Grad Norm: 0.00970053\n",
      "Epoch 1 | Step 642200 | Avg Loss: 0.0155 | Grad Norm: 0.00832142\n",
      "Epoch 1 | Step 642300 | Avg Loss: 0.0157 | Grad Norm: 0.00772629\n",
      "Epoch 1 | Step 642400 | Avg Loss: 0.0156 | Grad Norm: 0.00833715\n",
      "Epoch 1 | Step 642500 | Avg Loss: 0.0156 | Grad Norm: 0.01020268\n",
      "Epoch 1 | Step 642600 | Avg Loss: 0.0157 | Grad Norm: 0.00805801\n",
      "Epoch 1 | Step 642700 | Avg Loss: 0.0156 | Grad Norm: 0.00817942\n",
      "Epoch 1 | Step 642800 | Avg Loss: 0.0157 | Grad Norm: 0.01473567\n",
      "Epoch 1 | Step 642900 | Avg Loss: 0.0154 | Grad Norm: 0.00865254\n",
      "Epoch 1 | Step 643000 | Avg Loss: 0.0157 | Grad Norm: 0.00888067\n",
      "Epoch 1 | Step 643100 | Avg Loss: 0.0157 | Grad Norm: 0.00872765\n",
      "Epoch 1 | Step 643200 | Avg Loss: 0.0155 | Grad Norm: 0.00889506\n",
      "Epoch 1 | Step 643300 | Avg Loss: 0.0155 | Grad Norm: 0.00769436\n",
      "Epoch 1 | Step 643400 | Avg Loss: 0.0155 | Grad Norm: 0.00765537\n",
      "Epoch 1 | Step 643500 | Avg Loss: 0.0156 | Grad Norm: 0.00912106\n",
      "Epoch 1 | Step 643600 | Avg Loss: 0.0158 | Grad Norm: 0.00889745\n",
      "Epoch 1 | Step 643700 | Avg Loss: 0.0157 | Grad Norm: 0.00802520\n",
      "Epoch 1 | Step 643800 | Avg Loss: 0.0157 | Grad Norm: 0.00900824\n",
      "Epoch 1 | Step 643900 | Avg Loss: 0.0157 | Grad Norm: 0.01071580\n",
      "Epoch 1 | Step 644000 | Avg Loss: 0.0159 | Grad Norm: 0.00882211\n",
      "Epoch 1 | Step 644100 | Avg Loss: 0.0158 | Grad Norm: 0.00825889\n",
      "Epoch 1 | Step 644200 | Avg Loss: 0.0158 | Grad Norm: 0.00934191\n",
      "Epoch 1 | Step 644300 | Avg Loss: 0.0156 | Grad Norm: 0.00776400\n",
      "Epoch 1 | Step 644400 | Avg Loss: 0.0157 | Grad Norm: 0.00813606\n",
      "Epoch 1 | Step 644500 | Avg Loss: 0.0157 | Grad Norm: 0.00942082\n",
      "Epoch 1 | Step 644600 | Avg Loss: 0.0157 | Grad Norm: 0.00870385\n",
      "Epoch 1 | Step 644700 | Avg Loss: 0.0157 | Grad Norm: 0.00808087\n",
      "Epoch 1 | Step 644800 | Avg Loss: 0.0159 | Grad Norm: 0.00923679\n",
      "Epoch 1 | Step 644900 | Avg Loss: 0.0157 | Grad Norm: 0.00945882\n",
      "Epoch 1 | Step 645000 | Avg Loss: 0.0157 | Grad Norm: 0.01024210\n",
      "Epoch 1 | Step 645100 | Avg Loss: 0.0154 | Grad Norm: 0.00886125\n",
      "Epoch 1 | Step 645200 | Avg Loss: 0.0152 | Grad Norm: 0.00899486\n",
      "Epoch 1 | Step 645300 | Avg Loss: 0.0152 | Grad Norm: 0.00893780\n",
      "Epoch 1 | Step 645400 | Avg Loss: 0.0157 | Grad Norm: 0.00896619\n",
      "Epoch 1 | Step 645500 | Avg Loss: 0.0153 | Grad Norm: 0.00806083\n",
      "Epoch 1 | Step 645600 | Avg Loss: 0.0153 | Grad Norm: 0.00751326\n",
      "Epoch 1 | Step 645700 | Avg Loss: 0.0154 | Grad Norm: 0.00852488\n",
      "Epoch 1 | Step 645800 | Avg Loss: 0.0154 | Grad Norm: 0.00900247\n",
      "Epoch 1 | Step 645900 | Avg Loss: 0.0154 | Grad Norm: 0.00950274\n",
      "Epoch 1 | Step 646000 | Avg Loss: 0.0153 | Grad Norm: 0.01062262\n",
      "Epoch 1 | Step 646100 | Avg Loss: 0.0153 | Grad Norm: 0.00839096\n",
      "Epoch 1 | Step 646200 | Avg Loss: 0.0153 | Grad Norm: 0.01077681\n",
      "Epoch 1 | Step 646300 | Avg Loss: 0.0153 | Grad Norm: 0.00837237\n",
      "Epoch 1 | Step 646400 | Avg Loss: 0.0155 | Grad Norm: 0.00882782\n",
      "Epoch 1 | Step 646500 | Avg Loss: 0.0153 | Grad Norm: 0.00863656\n",
      "Epoch 1 | Step 646600 | Avg Loss: 0.0157 | Grad Norm: 0.00794011\n",
      "Epoch 1 | Step 646700 | Avg Loss: 0.0156 | Grad Norm: 0.00906173\n",
      "Epoch 1 | Step 646800 | Avg Loss: 0.0152 | Grad Norm: 0.00824311\n",
      "Epoch 1 | Step 646900 | Avg Loss: 0.0150 | Grad Norm: 0.00988307\n",
      "Epoch 1 | Step 647000 | Avg Loss: 0.0156 | Grad Norm: 0.01039376\n",
      "Epoch 1 | Step 647100 | Avg Loss: 0.0157 | Grad Norm: 0.00786131\n",
      "Epoch 1 | Step 647200 | Avg Loss: 0.0157 | Grad Norm: 0.00924673\n",
      "Epoch 1 | Step 647300 | Avg Loss: 0.0158 | Grad Norm: 0.01074859\n",
      "Epoch 1 | Step 647400 | Avg Loss: 0.0158 | Grad Norm: 0.00917697\n",
      "Epoch 1 | Step 647500 | Avg Loss: 0.0156 | Grad Norm: 0.00984654\n",
      "Epoch 1 | Step 647600 | Avg Loss: 0.0152 | Grad Norm: 0.00913165\n",
      "Epoch 1 | Step 647700 | Avg Loss: 0.0152 | Grad Norm: 0.00862000\n",
      "Epoch 1 | Step 647800 | Avg Loss: 0.0154 | Grad Norm: 0.00899009\n",
      "Epoch 1 | Step 647900 | Avg Loss: 0.0153 | Grad Norm: 0.00920702\n",
      "Epoch 1 | Step 648000 | Avg Loss: 0.0155 | Grad Norm: 0.00903320\n",
      "Epoch 1 | Step 648100 | Avg Loss: 0.0157 | Grad Norm: 0.00874985\n",
      "Epoch 1 | Step 648200 | Avg Loss: 0.0158 | Grad Norm: 0.01007659\n",
      "Epoch 1 | Step 648300 | Avg Loss: 0.0160 | Grad Norm: 0.00860866\n",
      "Epoch 1 | Step 648400 | Avg Loss: 0.0157 | Grad Norm: 0.00983183\n",
      "Epoch 1 | Step 648500 | Avg Loss: 0.0157 | Grad Norm: 0.00906396\n",
      "Epoch 1 | Step 648600 | Avg Loss: 0.0158 | Grad Norm: 0.00846022\n",
      "Epoch 1 | Step 648700 | Avg Loss: 0.0155 | Grad Norm: 0.01126862\n",
      "Epoch 1 | Step 648800 | Avg Loss: 0.0157 | Grad Norm: 0.00993426\n",
      "Epoch 1 | Step 648900 | Avg Loss: 0.0157 | Grad Norm: 0.01199202\n",
      "Epoch 1 | Step 649000 | Avg Loss: 0.0159 | Grad Norm: 0.00924860\n",
      "Epoch 1 | Step 649100 | Avg Loss: 0.0163 | Grad Norm: 0.00890695\n",
      "Epoch 1 | Step 649200 | Avg Loss: 0.0161 | Grad Norm: 0.00869814\n",
      "Epoch 1 | Step 649300 | Avg Loss: 0.0159 | Grad Norm: 0.00877057\n",
      "Epoch 1 | Step 649400 | Avg Loss: 0.0162 | Grad Norm: 0.00898408\n",
      "Epoch 1 | Step 649500 | Avg Loss: 0.0164 | Grad Norm: 0.01039407\n",
      "Epoch 1 | Step 649600 | Avg Loss: 0.0162 | Grad Norm: 0.00820923\n",
      "Epoch 1 | Step 649700 | Avg Loss: 0.0161 | Grad Norm: 0.00860012\n",
      "Epoch 1 | Step 649800 | Avg Loss: 0.0159 | Grad Norm: 0.00792327\n",
      "Epoch 1 | Step 649900 | Avg Loss: 0.0156 | Grad Norm: 0.00800686\n",
      "Epoch 1 | Step 650000 | Avg Loss: 0.0156 | Grad Norm: 0.00938718\n",
      "Epoch 1 | Step 650100 | Avg Loss: 0.0154 | Grad Norm: 0.00851005\n",
      "Epoch 1 | Step 650200 | Avg Loss: 0.0154 | Grad Norm: 0.00834300\n",
      "Epoch 1 | Step 650300 | Avg Loss: 0.0157 | Grad Norm: 0.00887943\n",
      "Epoch 1 | Step 650400 | Avg Loss: 0.0159 | Grad Norm: 0.00872115\n",
      "Epoch 1 | Step 650500 | Avg Loss: 0.0161 | Grad Norm: 0.00843539\n",
      "Epoch 1 | Step 650600 | Avg Loss: 0.0163 | Grad Norm: 0.00931878\n",
      "Epoch 1 | Step 650700 | Avg Loss: 0.0164 | Grad Norm: 0.00778579\n",
      "Epoch 1 | Step 650800 | Avg Loss: 0.0164 | Grad Norm: 0.00860114\n",
      "Epoch 1 | Step 650900 | Avg Loss: 0.0162 | Grad Norm: 0.00828963\n",
      "Epoch 1 | Step 651000 | Avg Loss: 0.0161 | Grad Norm: 0.00878774\n",
      "Epoch 1 | Step 651100 | Avg Loss: 0.0166 | Grad Norm: 0.00970873\n",
      "Epoch 1 | Step 651200 | Avg Loss: 0.0164 | Grad Norm: 0.00786731\n",
      "Epoch 1 | Step 651300 | Avg Loss: 0.0162 | Grad Norm: 0.00822296\n",
      "Epoch 1 | Step 651400 | Avg Loss: 0.0157 | Grad Norm: 0.00805256\n",
      "Epoch 1 | Step 651500 | Avg Loss: 0.0159 | Grad Norm: 0.00963328\n",
      "Epoch 1 | Step 651600 | Avg Loss: 0.0156 | Grad Norm: 0.00936835\n",
      "Epoch 1 | Step 651700 | Avg Loss: 0.0155 | Grad Norm: 0.00831698\n",
      "Epoch 1 | Step 651800 | Avg Loss: 0.0155 | Grad Norm: 0.01019924\n",
      "Epoch 1 | Step 651900 | Avg Loss: 0.0154 | Grad Norm: 0.00930742\n",
      "Epoch 1 | Step 652000 | Avg Loss: 0.0150 | Grad Norm: 0.00822194\n",
      "Epoch 1 | Step 652100 | Avg Loss: 0.0153 | Grad Norm: 0.01179262\n",
      "Epoch 1 | Step 652200 | Avg Loss: 0.0152 | Grad Norm: 0.00806937\n",
      "Epoch 1 | Step 652300 | Avg Loss: 0.0153 | Grad Norm: 0.00887637\n",
      "Epoch 1 | Step 652400 | Avg Loss: 0.0157 | Grad Norm: 0.00819968\n",
      "Epoch 1 | Step 652500 | Avg Loss: 0.0158 | Grad Norm: 0.00792636\n",
      "Epoch 1 | Step 652600 | Avg Loss: 0.0156 | Grad Norm: 0.00702625\n",
      "Epoch 1 | Step 652700 | Avg Loss: 0.0159 | Grad Norm: 0.00897629\n",
      "Epoch 1 | Step 652800 | Avg Loss: 0.0157 | Grad Norm: 0.00826424\n",
      "Epoch 1 | Step 652900 | Avg Loss: 0.0153 | Grad Norm: 0.00809293\n",
      "Epoch 1 | Step 653000 | Avg Loss: 0.0155 | Grad Norm: 0.00833357\n",
      "Epoch 1 | Step 653100 | Avg Loss: 0.0156 | Grad Norm: 0.00863143\n",
      "Epoch 1 | Step 653200 | Avg Loss: 0.0162 | Grad Norm: 0.01141471\n",
      "Epoch 1 | Step 653300 | Avg Loss: 0.0157 | Grad Norm: 0.00974470\n",
      "Epoch 1 | Step 653400 | Avg Loss: 0.0161 | Grad Norm: 0.00856666\n",
      "Epoch 1 | Step 653500 | Avg Loss: 0.0159 | Grad Norm: 0.00886411\n",
      "Epoch 1 | Step 653600 | Avg Loss: 0.0155 | Grad Norm: 0.00778385\n",
      "Epoch 1 | Step 653700 | Avg Loss: 0.0156 | Grad Norm: 0.00904262\n",
      "Epoch 1 | Step 653800 | Avg Loss: 0.0158 | Grad Norm: 0.00874744\n",
      "Epoch 1 | Step 653900 | Avg Loss: 0.0157 | Grad Norm: 0.00894061\n",
      "Epoch 1 | Step 654000 | Avg Loss: 0.0152 | Grad Norm: 0.00825354\n",
      "Epoch 1 | Step 654100 | Avg Loss: 0.0154 | Grad Norm: 0.00727478\n",
      "Epoch 1 | Step 654200 | Avg Loss: 0.0153 | Grad Norm: 0.00807745\n",
      "Epoch 1 | Step 654300 | Avg Loss: 0.0153 | Grad Norm: 0.00867693\n",
      "Epoch 1 | Step 654400 | Avg Loss: 0.0155 | Grad Norm: 0.00870680\n",
      "Epoch 1 | Step 654500 | Avg Loss: 0.0157 | Grad Norm: 0.00907652\n",
      "Epoch 1 | Step 654600 | Avg Loss: 0.0157 | Grad Norm: 0.00792579\n",
      "Epoch 1 | Step 654700 | Avg Loss: 0.0161 | Grad Norm: 0.00890912\n",
      "Epoch 1 | Step 654800 | Avg Loss: 0.0162 | Grad Norm: 0.01012828\n",
      "Epoch 1 | Step 654900 | Avg Loss: 0.0159 | Grad Norm: 0.00877900\n",
      "Epoch 1 | Step 655000 | Avg Loss: 0.0158 | Grad Norm: 0.00903464\n",
      "Epoch 1 | Step 655100 | Avg Loss: 0.0157 | Grad Norm: 0.01085332\n",
      "Epoch 1 | Step 655200 | Avg Loss: 0.0158 | Grad Norm: 0.00942172\n",
      "Epoch 1 | Step 655300 | Avg Loss: 0.0161 | Grad Norm: 0.01285737\n",
      "Epoch 1 | Step 655400 | Avg Loss: 0.0162 | Grad Norm: 0.01005475\n",
      "Epoch 1 | Step 655500 | Avg Loss: 0.0159 | Grad Norm: 0.00847900\n",
      "Epoch 1 | Step 655600 | Avg Loss: 0.0159 | Grad Norm: 0.00964961\n",
      "Epoch 1 | Step 655700 | Avg Loss: 0.0156 | Grad Norm: 0.00872818\n",
      "Epoch 1 | Step 655800 | Avg Loss: 0.0159 | Grad Norm: 0.00814978\n",
      "Epoch 1 | Step 655900 | Avg Loss: 0.0157 | Grad Norm: 0.00968990\n",
      "Epoch 1 | Step 656000 | Avg Loss: 0.0159 | Grad Norm: 0.00802210\n",
      "Epoch 1 | Step 656100 | Avg Loss: 0.0159 | Grad Norm: 0.00858091\n",
      "Epoch 1 | Step 656200 | Avg Loss: 0.0157 | Grad Norm: 0.00857598\n",
      "Epoch 1 | Step 656300 | Avg Loss: 0.0156 | Grad Norm: 0.00819822\n",
      "Epoch 1 | Step 656400 | Avg Loss: 0.0153 | Grad Norm: 0.00899715\n",
      "Epoch 1 | Step 656500 | Avg Loss: 0.0152 | Grad Norm: 0.00898668\n",
      "Epoch 1 | Step 656600 | Avg Loss: 0.0153 | Grad Norm: 0.00886906\n",
      "Epoch 1 | Step 656700 | Avg Loss: 0.0162 | Grad Norm: 0.00916959\n",
      "Epoch 1 | Step 656800 | Avg Loss: 0.0161 | Grad Norm: 0.01112422\n",
      "Epoch 1 | Step 656900 | Avg Loss: 0.0160 | Grad Norm: 0.00780059\n",
      "Epoch 1 | Step 657000 | Avg Loss: 0.0159 | Grad Norm: 0.00973813\n",
      "Epoch 1 | Step 657100 | Avg Loss: 0.0158 | Grad Norm: 0.00862649\n",
      "Epoch 1 | Step 657200 | Avg Loss: 0.0160 | Grad Norm: 0.00844341\n",
      "Epoch 1 | Step 657300 | Avg Loss: 0.0158 | Grad Norm: 0.00871863\n",
      "Epoch 1 | Step 657400 | Avg Loss: 0.0157 | Grad Norm: 0.00901047\n",
      "Epoch 1 | Step 657500 | Avg Loss: 0.0156 | Grad Norm: 0.00810707\n",
      "Epoch 1 | Step 657600 | Avg Loss: 0.0159 | Grad Norm: 0.00860619\n",
      "Epoch 1 | Step 657700 | Avg Loss: 0.0158 | Grad Norm: 0.01045604\n",
      "Epoch 1 | Step 657800 | Avg Loss: 0.0160 | Grad Norm: 0.00845951\n",
      "Epoch 1 | Step 657900 | Avg Loss: 0.0156 | Grad Norm: 0.00812268\n",
      "Epoch 1 | Step 658000 | Avg Loss: 0.0156 | Grad Norm: 0.00812959\n",
      "Epoch 1 | Step 658100 | Avg Loss: 0.0155 | Grad Norm: 0.00841112\n",
      "Epoch 1 | Step 658200 | Avg Loss: 0.0154 | Grad Norm: 0.00874840\n",
      "Epoch 1 | Step 658300 | Avg Loss: 0.0150 | Grad Norm: 0.00754171\n",
      "Epoch 1 | Step 658400 | Avg Loss: 0.0150 | Grad Norm: 0.00873267\n",
      "Epoch 1 | Step 658500 | Avg Loss: 0.0147 | Grad Norm: 0.00836121\n",
      "Epoch 1 | Step 658600 | Avg Loss: 0.0150 | Grad Norm: 0.00777983\n",
      "Epoch 1 | Step 658700 | Avg Loss: 0.0152 | Grad Norm: 0.00997548\n",
      "Epoch 1 | Step 658800 | Avg Loss: 0.0152 | Grad Norm: 0.00888294\n",
      "Epoch 1 | Step 658900 | Avg Loss: 0.0153 | Grad Norm: 0.00830105\n",
      "Epoch 1 | Step 659000 | Avg Loss: 0.0153 | Grad Norm: 0.00874439\n",
      "Epoch 1 | Step 659100 | Avg Loss: 0.0159 | Grad Norm: 0.00882261\n",
      "Epoch 1 | Step 659200 | Avg Loss: 0.0158 | Grad Norm: 0.00841260\n",
      "Epoch 1 | Step 659300 | Avg Loss: 0.0165 | Grad Norm: 0.01135249\n",
      "Epoch 1 | Step 659400 | Avg Loss: 0.0165 | Grad Norm: 0.01018518\n",
      "Epoch 1 | Step 659500 | Avg Loss: 0.0161 | Grad Norm: 0.00885435\n",
      "Epoch 1 | Step 659600 | Avg Loss: 0.0158 | Grad Norm: 0.00857242\n",
      "Epoch 1 | Step 659700 | Avg Loss: 0.0158 | Grad Norm: 0.00818391\n",
      "Epoch 1 | Step 659800 | Avg Loss: 0.0160 | Grad Norm: 0.01022727\n",
      "Epoch 1 | Step 659900 | Avg Loss: 0.0163 | Grad Norm: 0.01016507\n",
      "Epoch 1 | Step 660000 | Avg Loss: 0.0162 | Grad Norm: 0.01018146\n",
      "Epoch 1 | Step 660100 | Avg Loss: 0.0165 | Grad Norm: 0.00900542\n",
      "Epoch 1 | Step 660200 | Avg Loss: 0.0162 | Grad Norm: 0.00936457\n",
      "Epoch 1 | Step 660300 | Avg Loss: 0.0158 | Grad Norm: 0.00909908\n",
      "Epoch 1 | Step 660400 | Avg Loss: 0.0158 | Grad Norm: 0.00937684\n",
      "Epoch 1 | Step 660500 | Avg Loss: 0.0159 | Grad Norm: 0.01024691\n",
      "Epoch 1 | Step 660600 | Avg Loss: 0.0160 | Grad Norm: 0.00910092\n",
      "Epoch 1 | Step 660700 | Avg Loss: 0.0160 | Grad Norm: 0.00858194\n",
      "Epoch 1 | Step 660800 | Avg Loss: 0.0158 | Grad Norm: 0.00841145\n",
      "Epoch 1 | Step 660900 | Avg Loss: 0.0158 | Grad Norm: 0.00821652\n",
      "Epoch 1 | Step 661000 | Avg Loss: 0.0158 | Grad Norm: 0.00794928\n",
      "Epoch 1 | Step 661100 | Avg Loss: 0.0159 | Grad Norm: 0.01116702\n",
      "Epoch 1 | Step 661200 | Avg Loss: 0.0161 | Grad Norm: 0.00829707\n",
      "Epoch 1 | Step 661300 | Avg Loss: 0.0159 | Grad Norm: 0.00849826\n",
      "Epoch 1 | Step 661400 | Avg Loss: 0.0160 | Grad Norm: 0.00873021\n",
      "Epoch 1 | Step 661500 | Avg Loss: 0.0160 | Grad Norm: 0.00910477\n",
      "Epoch 1 | Step 661600 | Avg Loss: 0.0156 | Grad Norm: 0.00876451\n",
      "Epoch 1 | Step 661700 | Avg Loss: 0.0155 | Grad Norm: 0.00885197\n",
      "Epoch 1 | Step 661800 | Avg Loss: 0.0150 | Grad Norm: 0.00848850\n",
      "Epoch 1 | Step 661900 | Avg Loss: 0.0152 | Grad Norm: 0.01051547\n",
      "Epoch 1 | Step 662000 | Avg Loss: 0.0153 | Grad Norm: 0.00875831\n",
      "Epoch 1 | Step 662100 | Avg Loss: 0.0155 | Grad Norm: 0.00958471\n",
      "Epoch 1 | Step 662200 | Avg Loss: 0.0156 | Grad Norm: 0.00885618\n",
      "Epoch 1 | Step 662300 | Avg Loss: 0.0156 | Grad Norm: 0.00902258\n",
      "Epoch 1 | Step 662400 | Avg Loss: 0.0154 | Grad Norm: 0.00827096\n",
      "Epoch 1 | Step 662500 | Avg Loss: 0.0153 | Grad Norm: 0.01010249\n",
      "Epoch 1 | Step 662600 | Avg Loss: 0.0151 | Grad Norm: 0.00876902\n",
      "Epoch 1 | Step 662700 | Avg Loss: 0.0151 | Grad Norm: 0.00780098\n",
      "Epoch 1 | Step 662800 | Avg Loss: 0.0154 | Grad Norm: 0.00779004\n",
      "Epoch 1 | Step 662900 | Avg Loss: 0.0153 | Grad Norm: 0.00960888\n",
      "Epoch 1 | Step 663000 | Avg Loss: 0.0154 | Grad Norm: 0.00877824\n",
      "Epoch 1 | Step 663100 | Avg Loss: 0.0156 | Grad Norm: 0.00845717\n",
      "Epoch 1 | Step 663200 | Avg Loss: 0.0156 | Grad Norm: 0.00992106\n",
      "Epoch 1 | Step 663300 | Avg Loss: 0.0160 | Grad Norm: 0.00962988\n",
      "Epoch 1 | Step 663400 | Avg Loss: 0.0159 | Grad Norm: 0.00785195\n",
      "Epoch 1 | Step 663500 | Avg Loss: 0.0163 | Grad Norm: 0.00935323\n",
      "Epoch 1 | Step 663600 | Avg Loss: 0.0162 | Grad Norm: 0.00844587\n",
      "Epoch 1 | Step 663700 | Avg Loss: 0.0163 | Grad Norm: 0.00914149\n",
      "Epoch 1 | Step 663800 | Avg Loss: 0.0164 | Grad Norm: 0.00883605\n",
      "Epoch 1 | Step 663900 | Avg Loss: 0.0161 | Grad Norm: 0.01137295\n",
      "Epoch 1 | Step 664000 | Avg Loss: 0.0166 | Grad Norm: 0.00765153\n",
      "Epoch 1 | Step 664100 | Avg Loss: 0.0163 | Grad Norm: 0.00884616\n",
      "Epoch 1 | Step 664200 | Avg Loss: 0.0154 | Grad Norm: 0.00887088\n",
      "Epoch 1 | Step 664300 | Avg Loss: 0.0153 | Grad Norm: 0.00876052\n",
      "Epoch 1 | Step 664400 | Avg Loss: 0.0151 | Grad Norm: 0.00826380\n",
      "Epoch 1 | Step 664500 | Avg Loss: 0.0148 | Grad Norm: 0.01006084\n",
      "Epoch 1 | Step 664600 | Avg Loss: 0.0149 | Grad Norm: 0.00834253\n",
      "Epoch 1 | Step 664700 | Avg Loss: 0.0149 | Grad Norm: 0.00873072\n",
      "Epoch 1 | Step 664800 | Avg Loss: 0.0148 | Grad Norm: 0.00877608\n",
      "Epoch 1 | Step 664900 | Avg Loss: 0.0150 | Grad Norm: 0.00975809\n",
      "Epoch 1 | Step 665000 | Avg Loss: 0.0148 | Grad Norm: 0.00816531\n",
      "Epoch 1 | Step 665100 | Avg Loss: 0.0150 | Grad Norm: 0.00975776\n",
      "Epoch 1 | Step 665200 | Avg Loss: 0.0152 | Grad Norm: 0.00849558\n",
      "Epoch 1 | Step 665300 | Avg Loss: 0.0154 | Grad Norm: 0.00914306\n",
      "Epoch 1 | Step 665400 | Avg Loss: 0.0157 | Grad Norm: 0.00722693\n",
      "Epoch 1 | Step 665500 | Avg Loss: 0.0155 | Grad Norm: 0.00761876\n",
      "Epoch 1 | Step 665600 | Avg Loss: 0.0158 | Grad Norm: 0.00734695\n",
      "Epoch 1 | Step 665700 | Avg Loss: 0.0156 | Grad Norm: 0.00949143\n",
      "Epoch 1 | Step 665800 | Avg Loss: 0.0160 | Grad Norm: 0.00945538\n",
      "Epoch 1 | Step 665900 | Avg Loss: 0.0162 | Grad Norm: 0.00872252\n",
      "Epoch 1 | Step 666000 | Avg Loss: 0.0158 | Grad Norm: 0.00940138\n",
      "Epoch 1 | Step 666100 | Avg Loss: 0.0154 | Grad Norm: 0.00808215\n",
      "Epoch 1 | Step 666200 | Avg Loss: 0.0155 | Grad Norm: 0.00774116\n",
      "Epoch 1 | Step 666300 | Avg Loss: 0.0157 | Grad Norm: 0.00804308\n",
      "Epoch 1 | Step 666400 | Avg Loss: 0.0158 | Grad Norm: 0.00860472\n",
      "Epoch 1 | Step 666500 | Avg Loss: 0.0160 | Grad Norm: 0.00851423\n",
      "Epoch 1 | Step 666600 | Avg Loss: 0.0156 | Grad Norm: 0.00939956\n",
      "Epoch 1 | Step 666700 | Avg Loss: 0.0157 | Grad Norm: 0.00908001\n",
      "Epoch 1 | Step 666800 | Avg Loss: 0.0158 | Grad Norm: 0.00900416\n",
      "Epoch 1 | Step 666900 | Avg Loss: 0.0162 | Grad Norm: 0.00795927\n",
      "Epoch 1 | Step 667000 | Avg Loss: 0.0159 | Grad Norm: 0.00850404\n",
      "Epoch 1 | Step 667100 | Avg Loss: 0.0161 | Grad Norm: 0.00862886\n",
      "Epoch 1 | Step 667200 | Avg Loss: 0.0160 | Grad Norm: 0.01028739\n",
      "Epoch 1 | Step 667300 | Avg Loss: 0.0160 | Grad Norm: 0.00992822\n",
      "Epoch 1 | Step 667400 | Avg Loss: 0.0158 | Grad Norm: 0.00988438\n",
      "Epoch 1 | Step 667500 | Avg Loss: 0.0156 | Grad Norm: 0.00928400\n",
      "Epoch 1 | Step 667600 | Avg Loss: 0.0157 | Grad Norm: 0.00824863\n",
      "Epoch 1 | Step 667700 | Avg Loss: 0.0158 | Grad Norm: 0.00813761\n",
      "Epoch 1 | Step 667800 | Avg Loss: 0.0159 | Grad Norm: 0.00946048\n",
      "Epoch 1 | Step 667900 | Avg Loss: 0.0156 | Grad Norm: 0.00777127\n",
      "Epoch 1 | Step 668000 | Avg Loss: 0.0154 | Grad Norm: 0.00693057\n",
      "Epoch 1 | Step 668100 | Avg Loss: 0.0152 | Grad Norm: 0.01092533\n",
      "Epoch 1 | Step 668200 | Avg Loss: 0.0149 | Grad Norm: 0.01005024\n",
      "Epoch 1 | Step 668300 | Avg Loss: 0.0153 | Grad Norm: 0.00755143\n",
      "Epoch 1 | Step 668400 | Avg Loss: 0.0157 | Grad Norm: 0.00913004\n",
      "Epoch 1 | Step 668500 | Avg Loss: 0.0158 | Grad Norm: 0.00970412\n",
      "Epoch 1 | Step 668600 | Avg Loss: 0.0157 | Grad Norm: 0.00875999\n",
      "Epoch 1 | Step 668700 | Avg Loss: 0.0159 | Grad Norm: 0.00995787\n",
      "Epoch 1 | Step 668800 | Avg Loss: 0.0158 | Grad Norm: 0.00981876\n",
      "Epoch 1 | Step 668900 | Avg Loss: 0.0156 | Grad Norm: 0.00910187\n",
      "Epoch 1 | Step 669000 | Avg Loss: 0.0160 | Grad Norm: 0.00822919\n",
      "Epoch 1 | Step 669100 | Avg Loss: 0.0157 | Grad Norm: 0.00878107\n",
      "Epoch 1 | Step 669200 | Avg Loss: 0.0158 | Grad Norm: 0.00875635\n",
      "Epoch 1 | Step 669300 | Avg Loss: 0.0161 | Grad Norm: 0.00827717\n",
      "Epoch 1 | Step 669400 | Avg Loss: 0.0161 | Grad Norm: 0.00983649\n",
      "Epoch 1 | Step 669500 | Avg Loss: 0.0158 | Grad Norm: 0.00920018\n",
      "Epoch 1 | Step 669600 | Avg Loss: 0.0162 | Grad Norm: 0.01028589\n",
      "Epoch 1 | Step 669700 | Avg Loss: 0.0161 | Grad Norm: 0.01044607\n",
      "Epoch 1 | Step 669800 | Avg Loss: 0.0160 | Grad Norm: 0.00854325\n",
      "Epoch 1 | Step 669900 | Avg Loss: 0.0158 | Grad Norm: 0.00781313\n",
      "Epoch 1 | Step 670000 | Avg Loss: 0.0160 | Grad Norm: 0.00833844\n",
      "Epoch 1 | Step 670100 | Avg Loss: 0.0160 | Grad Norm: 0.01215309\n",
      "Epoch 1 | Step 670200 | Avg Loss: 0.0160 | Grad Norm: 0.00858300\n",
      "Epoch 1 | Step 670300 | Avg Loss: 0.0159 | Grad Norm: 0.00831423\n",
      "Epoch 1 | Step 670400 | Avg Loss: 0.0160 | Grad Norm: 0.00883772\n",
      "Epoch 1 | Step 670500 | Avg Loss: 0.0158 | Grad Norm: 0.00813878\n",
      "Epoch 1 | Step 670600 | Avg Loss: 0.0160 | Grad Norm: 0.00958392\n",
      "Epoch 1 | Step 670700 | Avg Loss: 0.0160 | Grad Norm: 0.00824717\n",
      "Epoch 1 | Step 670800 | Avg Loss: 0.0159 | Grad Norm: 0.00786719\n",
      "Epoch 1 | Step 670900 | Avg Loss: 0.0156 | Grad Norm: 0.00785026\n",
      "Epoch 1 | Step 671000 | Avg Loss: 0.0154 | Grad Norm: 0.00866124\n",
      "Epoch 1 | Step 671100 | Avg Loss: 0.0155 | Grad Norm: 0.00867987\n",
      "Epoch 1 | Step 671200 | Avg Loss: 0.0152 | Grad Norm: 0.00817840\n",
      "Epoch 1 | Step 671300 | Avg Loss: 0.0150 | Grad Norm: 0.00750203\n",
      "Epoch 1 | Step 671400 | Avg Loss: 0.0151 | Grad Norm: 0.01066111\n",
      "Epoch 1 | Step 671500 | Avg Loss: 0.0153 | Grad Norm: 0.00813420\n",
      "Epoch 1 | Step 671600 | Avg Loss: 0.0152 | Grad Norm: 0.00963626\n",
      "Epoch 1 | Step 671700 | Avg Loss: 0.0154 | Grad Norm: 0.00730797\n",
      "Epoch 1 | Step 671800 | Avg Loss: 0.0157 | Grad Norm: 0.00819472\n",
      "Epoch 1 | Step 671900 | Avg Loss: 0.0157 | Grad Norm: 0.01058837\n",
      "Epoch 1 | Step 672000 | Avg Loss: 0.0158 | Grad Norm: 0.00791614\n",
      "Epoch 1 | Step 672100 | Avg Loss: 0.0158 | Grad Norm: 0.00936895\n",
      "Epoch 1 | Step 672200 | Avg Loss: 0.0162 | Grad Norm: 0.01001494\n",
      "Epoch 1 | Step 672300 | Avg Loss: 0.0159 | Grad Norm: 0.00826789\n",
      "Epoch 1 | Step 672400 | Avg Loss: 0.0159 | Grad Norm: 0.00946099\n",
      "Epoch 1 | Step 672500 | Avg Loss: 0.0157 | Grad Norm: 0.00843582\n",
      "Epoch 1 | Step 672600 | Avg Loss: 0.0159 | Grad Norm: 0.00917493\n",
      "Epoch 1 | Step 672700 | Avg Loss: 0.0159 | Grad Norm: 0.00997114\n",
      "Epoch 1 | Step 672800 | Avg Loss: 0.0160 | Grad Norm: 0.00874570\n",
      "Epoch 1 | Step 672900 | Avg Loss: 0.0163 | Grad Norm: 0.00802327\n",
      "Epoch 1 | Step 673000 | Avg Loss: 0.0159 | Grad Norm: 0.00878266\n",
      "Epoch 1 | Step 673100 | Avg Loss: 0.0160 | Grad Norm: 0.00985638\n",
      "Epoch 1 | Step 673200 | Avg Loss: 0.0157 | Grad Norm: 0.00880617\n",
      "Epoch 1 | Step 673300 | Avg Loss: 0.0156 | Grad Norm: 0.00902560\n",
      "Epoch 1 | Step 673400 | Avg Loss: 0.0154 | Grad Norm: 0.00848763\n",
      "Epoch 1 | Step 673500 | Avg Loss: 0.0157 | Grad Norm: 0.00920997\n",
      "Epoch 1 | Step 673600 | Avg Loss: 0.0154 | Grad Norm: 0.00791103\n",
      "Epoch 1 | Step 673700 | Avg Loss: 0.0157 | Grad Norm: 0.00864125\n",
      "Epoch 1 | Step 673800 | Avg Loss: 0.0157 | Grad Norm: 0.00928427\n",
      "Epoch 1 | Step 673900 | Avg Loss: 0.0155 | Grad Norm: 0.00975547\n",
      "Epoch 1 | Step 674000 | Avg Loss: 0.0155 | Grad Norm: 0.00948987\n",
      "Epoch 1 | Step 674100 | Avg Loss: 0.0156 | Grad Norm: 0.00848033\n",
      "Epoch 1 | Step 674200 | Avg Loss: 0.0160 | Grad Norm: 0.00804881\n",
      "Epoch 1 | Step 674300 | Avg Loss: 0.0160 | Grad Norm: 0.00948597\n",
      "Epoch 1 | Step 674400 | Avg Loss: 0.0158 | Grad Norm: 0.00890292\n",
      "Epoch 1 | Step 674500 | Avg Loss: 0.0154 | Grad Norm: 0.00911583\n",
      "Epoch 1 | Step 674600 | Avg Loss: 0.0155 | Grad Norm: 0.01137771\n",
      "Epoch 1 | Step 674700 | Avg Loss: 0.0156 | Grad Norm: 0.00788267\n",
      "Epoch 1 | Step 674800 | Avg Loss: 0.0157 | Grad Norm: 0.01003700\n",
      "Epoch 1 | Step 674900 | Avg Loss: 0.0156 | Grad Norm: 0.00929324\n",
      "Epoch 1 | Step 675000 | Avg Loss: 0.0155 | Grad Norm: 0.00850883\n",
      "Epoch 1 | Step 675100 | Avg Loss: 0.0151 | Grad Norm: 0.00943889\n",
      "Epoch 1 | Step 675200 | Avg Loss: 0.0150 | Grad Norm: 0.00840886\n",
      "Epoch 1 | Step 675300 | Avg Loss: 0.0149 | Grad Norm: 0.01103790\n",
      "Epoch 1 | Step 675400 | Avg Loss: 0.0148 | Grad Norm: 0.00964818\n",
      "Epoch 1 | Step 675500 | Avg Loss: 0.0151 | Grad Norm: 0.00896021\n",
      "Epoch 1 | Step 675600 | Avg Loss: 0.0152 | Grad Norm: 0.00946709\n",
      "Epoch 1 | Step 675700 | Avg Loss: 0.0150 | Grad Norm: 0.00933455\n",
      "Epoch 1 | Step 675800 | Avg Loss: 0.0151 | Grad Norm: 0.00846825\n",
      "Epoch 1 | Step 675900 | Avg Loss: 0.0153 | Grad Norm: 0.00901031\n",
      "Epoch 1 | Step 676000 | Avg Loss: 0.0154 | Grad Norm: 0.00926143\n",
      "Epoch 1 | Step 676100 | Avg Loss: 0.0154 | Grad Norm: 0.00863457\n",
      "Epoch 1 | Step 676200 | Avg Loss: 0.0153 | Grad Norm: 0.01042724\n",
      "Epoch 1 | Step 676300 | Avg Loss: 0.0158 | Grad Norm: 0.00871107\n",
      "Epoch 1 | Step 676400 | Avg Loss: 0.0153 | Grad Norm: 0.00846803\n",
      "Epoch 1 | Step 676500 | Avg Loss: 0.0147 | Grad Norm: 0.00988097\n",
      "Epoch 1 | Step 676600 | Avg Loss: 0.0151 | Grad Norm: 0.00960155\n",
      "Epoch 1 | Step 676700 | Avg Loss: 0.0149 | Grad Norm: 0.01356634\n",
      "Epoch 1 | Step 676800 | Avg Loss: 0.0149 | Grad Norm: 0.00928881\n",
      "Epoch 1 | Step 676900 | Avg Loss: 0.0153 | Grad Norm: 0.00840328\n",
      "Epoch 1 | Step 677000 | Avg Loss: 0.0152 | Grad Norm: 0.00827909\n",
      "Epoch 1 | Step 677100 | Avg Loss: 0.0158 | Grad Norm: 0.00883443\n",
      "Epoch 1 | Step 677200 | Avg Loss: 0.0158 | Grad Norm: 0.00738085\n",
      "Epoch 1 | Step 677300 | Avg Loss: 0.0160 | Grad Norm: 0.00850354\n",
      "Epoch 1 | Step 677400 | Avg Loss: 0.0161 | Grad Norm: 0.01009382\n",
      "Epoch 1 | Step 677500 | Avg Loss: 0.0159 | Grad Norm: 0.00902619\n",
      "Epoch 1 | Step 677600 | Avg Loss: 0.0156 | Grad Norm: 0.00865480\n",
      "Epoch 1 | Step 677700 | Avg Loss: 0.0157 | Grad Norm: 0.00856990\n",
      "Epoch 1 | Step 677800 | Avg Loss: 0.0154 | Grad Norm: 0.00819083\n",
      "Epoch 1 | Step 677900 | Avg Loss: 0.0153 | Grad Norm: 0.00982308\n",
      "Epoch 1 | Step 678000 | Avg Loss: 0.0156 | Grad Norm: 0.00938821\n",
      "Epoch 1 | Step 678100 | Avg Loss: 0.0156 | Grad Norm: 0.00908950\n",
      "Epoch 1 | Step 678200 | Avg Loss: 0.0155 | Grad Norm: 0.00751064\n",
      "Epoch 1 | Step 678300 | Avg Loss: 0.0152 | Grad Norm: 0.00910056\n",
      "Epoch 1 | Step 678400 | Avg Loss: 0.0153 | Grad Norm: 0.00907239\n",
      "Epoch 1 | Step 678500 | Avg Loss: 0.0156 | Grad Norm: 0.00828474\n",
      "Epoch 1 | Step 678600 | Avg Loss: 0.0161 | Grad Norm: 0.00788874\n",
      "Epoch 1 | Step 678700 | Avg Loss: 0.0161 | Grad Norm: 0.00992844\n",
      "Epoch 1 | Step 678800 | Avg Loss: 0.0162 | Grad Norm: 0.00848778\n",
      "Epoch 1 | Step 678900 | Avg Loss: 0.0160 | Grad Norm: 0.00823892\n",
      "Epoch 1 | Step 679000 | Avg Loss: 0.0160 | Grad Norm: 0.01249112\n",
      "Epoch 1 | Step 679100 | Avg Loss: 0.0159 | Grad Norm: 0.01059630\n",
      "Epoch 1 | Step 679200 | Avg Loss: 0.0160 | Grad Norm: 0.00854377\n",
      "Epoch 1 | Step 679300 | Avg Loss: 0.0166 | Grad Norm: 0.00938767\n",
      "Epoch 1 | Step 679400 | Avg Loss: 0.0163 | Grad Norm: 0.00842746\n",
      "Epoch 1 | Step 679500 | Avg Loss: 0.0162 | Grad Norm: 0.00857993\n",
      "Epoch 1 | Step 679600 | Avg Loss: 0.0160 | Grad Norm: 0.00902928\n",
      "Epoch 1 | Step 679700 | Avg Loss: 0.0159 | Grad Norm: 0.01025646\n",
      "Epoch 1 | Step 679800 | Avg Loss: 0.0161 | Grad Norm: 0.00803854\n",
      "Epoch 1 | Step 679900 | Avg Loss: 0.0161 | Grad Norm: 0.00962198\n",
      "Epoch 1 | Step 680000 | Avg Loss: 0.0160 | Grad Norm: 0.00873211\n",
      "Epoch 1 | Step 680100 | Avg Loss: 0.0164 | Grad Norm: 0.01525940\n",
      "Epoch 1 | Step 680200 | Avg Loss: 0.0163 | Grad Norm: 0.00856872\n",
      "Epoch 1 | Step 680300 | Avg Loss: 0.0165 | Grad Norm: 0.00838386\n",
      "Epoch 1 | Step 680400 | Avg Loss: 0.0165 | Grad Norm: 0.00968480\n",
      "Epoch 1 | Step 680500 | Avg Loss: 0.0166 | Grad Norm: 0.00863882\n",
      "Epoch 1 | Step 680600 | Avg Loss: 0.0160 | Grad Norm: 0.00906447\n",
      "Epoch 1 | Step 680700 | Avg Loss: 0.0161 | Grad Norm: 0.00950790\n",
      "Epoch 1 | Step 680800 | Avg Loss: 0.0160 | Grad Norm: 0.00862063\n",
      "Epoch 1 | Step 680900 | Avg Loss: 0.0158 | Grad Norm: 0.00835849\n",
      "Epoch 1 | Step 681000 | Avg Loss: 0.0157 | Grad Norm: 0.00821002\n",
      "Epoch 1 | Step 681100 | Avg Loss: 0.0158 | Grad Norm: 0.00937677\n",
      "Epoch 1 | Step 681200 | Avg Loss: 0.0157 | Grad Norm: 0.00801574\n",
      "Epoch 1 | Step 681300 | Avg Loss: 0.0159 | Grad Norm: 0.00778638\n",
      "Epoch 1 | Step 681400 | Avg Loss: 0.0159 | Grad Norm: 0.00991673\n",
      "Epoch 1 | Step 681500 | Avg Loss: 0.0158 | Grad Norm: 0.00861727\n",
      "Epoch 1 | Step 681600 | Avg Loss: 0.0157 | Grad Norm: 0.00917603\n",
      "Epoch 1 | Step 681700 | Avg Loss: 0.0158 | Grad Norm: 0.00877733\n",
      "Epoch 1 | Step 681800 | Avg Loss: 0.0156 | Grad Norm: 0.00759214\n",
      "Epoch 1 | Step 681900 | Avg Loss: 0.0153 | Grad Norm: 0.00845363\n",
      "Epoch 1 | Step 682000 | Avg Loss: 0.0156 | Grad Norm: 0.00915809\n",
      "Epoch 1 | Step 682100 | Avg Loss: 0.0154 | Grad Norm: 0.00811218\n",
      "Epoch 1 | Step 682200 | Avg Loss: 0.0158 | Grad Norm: 0.00964583\n",
      "Epoch 1 | Step 682300 | Avg Loss: 0.0155 | Grad Norm: 0.01034522\n",
      "Epoch 1 | Step 682400 | Avg Loss: 0.0153 | Grad Norm: 0.00831034\n",
      "Epoch 1 | Step 682500 | Avg Loss: 0.0150 | Grad Norm: 0.00737422\n",
      "Epoch 1 | Step 682600 | Avg Loss: 0.0151 | Grad Norm: 0.00755107\n",
      "Epoch 1 | Step 682700 | Avg Loss: 0.0154 | Grad Norm: 0.01006410\n",
      "Epoch 1 | Step 682800 | Avg Loss: 0.0156 | Grad Norm: 0.00819802\n",
      "Epoch 1 | Step 682900 | Avg Loss: 0.0159 | Grad Norm: 0.00926301\n",
      "Epoch 1 | Step 683000 | Avg Loss: 0.0161 | Grad Norm: 0.00927106\n",
      "Epoch 1 | Step 683100 | Avg Loss: 0.0160 | Grad Norm: 0.00866380\n",
      "Epoch 1 | Step 683200 | Avg Loss: 0.0154 | Grad Norm: 0.00997607\n",
      "Epoch 1 | Step 683300 | Avg Loss: 0.0156 | Grad Norm: 0.00804106\n",
      "Epoch 1 | Step 683400 | Avg Loss: 0.0149 | Grad Norm: 0.00991723\n",
      "Epoch 1 | Step 683500 | Avg Loss: 0.0147 | Grad Norm: 0.00921251\n",
      "Epoch 1 | Step 683600 | Avg Loss: 0.0149 | Grad Norm: 0.00930057\n",
      "Epoch 1 | Step 683700 | Avg Loss: 0.0150 | Grad Norm: 0.00956895\n",
      "Epoch 1 | Step 683800 | Avg Loss: 0.0152 | Grad Norm: 0.00921778\n",
      "Epoch 1 | Step 683900 | Avg Loss: 0.0151 | Grad Norm: 0.00738824\n",
      "Epoch 1 | Step 684000 | Avg Loss: 0.0156 | Grad Norm: 0.00889921\n",
      "Epoch 1 | Step 684100 | Avg Loss: 0.0150 | Grad Norm: 0.00852932\n",
      "Epoch 1 | Step 684200 | Avg Loss: 0.0152 | Grad Norm: 0.00910608\n",
      "Epoch 1 | Step 684300 | Avg Loss: 0.0154 | Grad Norm: 0.00769325\n",
      "Epoch 1 | Step 684400 | Avg Loss: 0.0157 | Grad Norm: 0.00954251\n",
      "Epoch 1 | Step 684500 | Avg Loss: 0.0157 | Grad Norm: 0.00856828\n",
      "Epoch 1 | Step 684600 | Avg Loss: 0.0160 | Grad Norm: 0.00995801\n",
      "Epoch 1 | Step 684700 | Avg Loss: 0.0160 | Grad Norm: 0.01023455\n",
      "Epoch 1 | Step 684800 | Avg Loss: 0.0162 | Grad Norm: 0.00806397\n",
      "Epoch 1 | Step 684900 | Avg Loss: 0.0161 | Grad Norm: 0.00788305\n",
      "Epoch 1 | Step 685000 | Avg Loss: 0.0161 | Grad Norm: 0.00862640\n",
      "Epoch 1 | Step 685100 | Avg Loss: 0.0159 | Grad Norm: 0.00863626\n",
      "Epoch 1 | Step 685200 | Avg Loss: 0.0159 | Grad Norm: 0.00928226\n",
      "Epoch 1 | Step 685300 | Avg Loss: 0.0160 | Grad Norm: 0.00817612\n",
      "Epoch 1 | Step 685400 | Avg Loss: 0.0160 | Grad Norm: 0.00972738\n",
      "Epoch 1 | Step 685500 | Avg Loss: 0.0161 | Grad Norm: 0.00875470\n",
      "Epoch 1 | Step 685600 | Avg Loss: 0.0159 | Grad Norm: 0.00908692\n",
      "Epoch 1 | Step 685700 | Avg Loss: 0.0158 | Grad Norm: 0.00837046\n",
      "Epoch 1 | Step 685800 | Avg Loss: 0.0157 | Grad Norm: 0.00831218\n",
      "Epoch 1 | Step 685900 | Avg Loss: 0.0154 | Grad Norm: 0.00902389\n",
      "Epoch 1 | Step 686000 | Avg Loss: 0.0153 | Grad Norm: 0.00856438\n",
      "Epoch 1 | Step 686100 | Avg Loss: 0.0154 | Grad Norm: 0.01024782\n",
      "Epoch 1 | Step 686200 | Avg Loss: 0.0153 | Grad Norm: 0.00856295\n",
      "Epoch 1 | Step 686300 | Avg Loss: 0.0155 | Grad Norm: 0.00814543\n",
      "Epoch 1 | Step 686400 | Avg Loss: 0.0159 | Grad Norm: 0.00866732\n",
      "Epoch 1 | Step 686500 | Avg Loss: 0.0158 | Grad Norm: 0.00830281\n",
      "Epoch 1 | Step 686600 | Avg Loss: 0.0159 | Grad Norm: 0.00835797\n",
      "Epoch 1 | Step 686700 | Avg Loss: 0.0158 | Grad Norm: 0.00858659\n",
      "Epoch 1 | Step 686800 | Avg Loss: 0.0160 | Grad Norm: 0.00897775\n",
      "Epoch 1 | Step 686900 | Avg Loss: 0.0163 | Grad Norm: 0.00871017\n",
      "Epoch 1 | Step 687000 | Avg Loss: 0.0163 | Grad Norm: 0.00833059\n",
      "Epoch 1 | Step 687100 | Avg Loss: 0.0163 | Grad Norm: 0.00935409\n",
      "Epoch 1 | Step 687200 | Avg Loss: 0.0159 | Grad Norm: 0.00792386\n",
      "Epoch 1 | Step 687300 | Avg Loss: 0.0161 | Grad Norm: 0.01078768\n",
      "Epoch 1 | Step 687400 | Avg Loss: 0.0157 | Grad Norm: 0.00909073\n",
      "Epoch 1 | Step 687500 | Avg Loss: 0.0162 | Grad Norm: 0.01163306\n",
      "Epoch 1 | Step 687600 | Avg Loss: 0.0164 | Grad Norm: 0.00922767\n",
      "Epoch 1 | Step 687700 | Avg Loss: 0.0164 | Grad Norm: 0.00838264\n",
      "Epoch 1 | Step 687800 | Avg Loss: 0.0162 | Grad Norm: 0.00879132\n",
      "Epoch 1 | Step 687900 | Avg Loss: 0.0163 | Grad Norm: 0.00839422\n",
      "Epoch 1 | Step 688000 | Avg Loss: 0.0161 | Grad Norm: 0.00762756\n",
      "Epoch 1 | Step 688100 | Avg Loss: 0.0156 | Grad Norm: 0.00899118\n",
      "Epoch 1 | Step 688200 | Avg Loss: 0.0160 | Grad Norm: 0.00991369\n",
      "Epoch 1 | Step 688300 | Avg Loss: 0.0162 | Grad Norm: 0.00977316\n",
      "Epoch 1 | Step 688400 | Avg Loss: 0.0160 | Grad Norm: 0.01115026\n",
      "Epoch 1 | Step 688500 | Avg Loss: 0.0159 | Grad Norm: 0.00820382\n",
      "Epoch 1 | Step 688600 | Avg Loss: 0.0159 | Grad Norm: 0.00893605\n",
      "Epoch 1 | Step 688700 | Avg Loss: 0.0157 | Grad Norm: 0.00774288\n",
      "Epoch 1 | Step 688800 | Avg Loss: 0.0158 | Grad Norm: 0.00806641\n",
      "Epoch 1 | Step 688900 | Avg Loss: 0.0158 | Grad Norm: 0.00888756\n",
      "Epoch 1 | Step 689000 | Avg Loss: 0.0154 | Grad Norm: 0.00936236\n",
      "Epoch 1 | Step 689100 | Avg Loss: 0.0155 | Grad Norm: 0.00811785\n",
      "Epoch 1 | Step 689200 | Avg Loss: 0.0154 | Grad Norm: 0.00893162\n",
      "Epoch 1 | Step 689300 | Avg Loss: 0.0156 | Grad Norm: 0.00868118\n",
      "Epoch 1 | Step 689400 | Avg Loss: 0.0156 | Grad Norm: 0.00980328\n",
      "Epoch 1 | Step 689500 | Avg Loss: 0.0159 | Grad Norm: 0.00832312\n",
      "Epoch 1 | Step 689600 | Avg Loss: 0.0154 | Grad Norm: 0.00929825\n",
      "Epoch 1 | Step 689700 | Avg Loss: 0.0155 | Grad Norm: 0.00947329\n",
      "Epoch 1 | Step 689800 | Avg Loss: 0.0152 | Grad Norm: 0.00830832\n",
      "Epoch 1 | Step 689900 | Avg Loss: 0.0152 | Grad Norm: 0.00912397\n",
      "Epoch 1 | Step 690000 | Avg Loss: 0.0150 | Grad Norm: 0.00808366\n",
      "Epoch 1 | Step 690100 | Avg Loss: 0.0148 | Grad Norm: 0.00767937\n",
      "Epoch 1 | Step 690200 | Avg Loss: 0.0149 | Grad Norm: 0.00893975\n",
      "Epoch 1 | Step 690300 | Avg Loss: 0.0150 | Grad Norm: 0.00845802\n",
      "Epoch 1 | Step 690400 | Avg Loss: 0.0155 | Grad Norm: 0.00992367\n",
      "Epoch 1 | Step 690500 | Avg Loss: 0.0156 | Grad Norm: 0.00904302\n",
      "Epoch 1 | Step 690600 | Avg Loss: 0.0156 | Grad Norm: 0.01054161\n",
      "Epoch 1 | Step 690700 | Avg Loss: 0.0158 | Grad Norm: 0.00796118\n",
      "Epoch 1 | Step 690800 | Avg Loss: 0.0158 | Grad Norm: 0.00887852\n",
      "Epoch 1 | Step 690900 | Avg Loss: 0.0157 | Grad Norm: 0.00830489\n",
      "Epoch 1 | Step 691000 | Avg Loss: 0.0156 | Grad Norm: 0.00845530\n",
      "Epoch 1 | Step 691100 | Avg Loss: 0.0152 | Grad Norm: 0.00816928\n",
      "Epoch 1 | Step 691200 | Avg Loss: 0.0151 | Grad Norm: 0.00820941\n",
      "Epoch 1 | Step 691300 | Avg Loss: 0.0155 | Grad Norm: 0.01061273\n",
      "Epoch 1 | Step 691400 | Avg Loss: 0.0155 | Grad Norm: 0.00837675\n",
      "Epoch 1 | Step 691500 | Avg Loss: 0.0155 | Grad Norm: 0.00863942\n",
      "Epoch 1 | Step 691600 | Avg Loss: 0.0155 | Grad Norm: 0.00942332\n",
      "Epoch 1 | Step 691700 | Avg Loss: 0.0157 | Grad Norm: 0.00896063\n",
      "Epoch 1 | Step 691800 | Avg Loss: 0.0155 | Grad Norm: 0.00820830\n",
      "Epoch 1 | Step 691900 | Avg Loss: 0.0157 | Grad Norm: 0.00930490\n",
      "Epoch 1 | Step 692000 | Avg Loss: 0.0155 | Grad Norm: 0.00843394\n",
      "Epoch 1 | Step 692100 | Avg Loss: 0.0157 | Grad Norm: 0.00847943\n",
      "Epoch 1 | Step 692200 | Avg Loss: 0.0158 | Grad Norm: 0.00918852\n",
      "Epoch 1 | Step 692300 | Avg Loss: 0.0159 | Grad Norm: 0.00822343\n",
      "Epoch 1 | Step 692400 | Avg Loss: 0.0160 | Grad Norm: 0.00800113\n",
      "Epoch 1 | Step 692500 | Avg Loss: 0.0162 | Grad Norm: 0.00926153\n",
      "Epoch 1 | Step 692600 | Avg Loss: 0.0158 | Grad Norm: 0.00810651\n",
      "Epoch 1 | Step 692700 | Avg Loss: 0.0155 | Grad Norm: 0.00895105\n",
      "Epoch 1 | Step 692800 | Avg Loss: 0.0161 | Grad Norm: 0.01086325\n",
      "Epoch 1 | Step 692900 | Avg Loss: 0.0161 | Grad Norm: 0.00922383\n",
      "Epoch 1 | Step 693000 | Avg Loss: 0.0159 | Grad Norm: 0.01044099\n",
      "Epoch 1 | Step 693100 | Avg Loss: 0.0160 | Grad Norm: 0.00951130\n",
      "Epoch 1 | Step 693200 | Avg Loss: 0.0162 | Grad Norm: 0.00905032\n",
      "Epoch 1 | Step 693300 | Avg Loss: 0.0163 | Grad Norm: 0.00726333\n",
      "Epoch 1 | Step 693400 | Avg Loss: 0.0158 | Grad Norm: 0.00762455\n",
      "Epoch 1 | Step 693500 | Avg Loss: 0.0158 | Grad Norm: 0.00812954\n",
      "Epoch 1 | Step 693600 | Avg Loss: 0.0157 | Grad Norm: 0.00806666\n",
      "Epoch 1 | Step 693700 | Avg Loss: 0.0158 | Grad Norm: 0.00893929\n",
      "Epoch 1 | Step 693800 | Avg Loss: 0.0159 | Grad Norm: 0.00866812\n",
      "Epoch 1 | Step 693900 | Avg Loss: 0.0160 | Grad Norm: 0.00868709\n",
      "Epoch 1 | Step 694000 | Avg Loss: 0.0160 | Grad Norm: 0.00866839\n",
      "Epoch 1 | Step 694100 | Avg Loss: 0.0160 | Grad Norm: 0.00768920\n",
      "Epoch 1 | Step 694200 | Avg Loss: 0.0163 | Grad Norm: 0.01209192\n",
      "Epoch 1 | Step 694300 | Avg Loss: 0.0166 | Grad Norm: 0.00843789\n",
      "Epoch 1 | Step 694400 | Avg Loss: 0.0163 | Grad Norm: 0.00958789\n",
      "Epoch 1 | Step 694500 | Avg Loss: 0.0163 | Grad Norm: 0.01037246\n",
      "Epoch 1 | Step 694600 | Avg Loss: 0.0162 | Grad Norm: 0.01187651\n",
      "Epoch 1 | Step 694700 | Avg Loss: 0.0162 | Grad Norm: 0.00849195\n",
      "Epoch 1 | Step 694800 | Avg Loss: 0.0160 | Grad Norm: 0.00840128\n",
      "Epoch 1 | Step 694900 | Avg Loss: 0.0161 | Grad Norm: 0.01026826\n",
      "Epoch 1 | Step 695000 | Avg Loss: 0.0162 | Grad Norm: 0.00989399\n",
      "Epoch 1 | Step 695100 | Avg Loss: 0.0160 | Grad Norm: 0.01102297\n",
      "Epoch 1 | Step 695200 | Avg Loss: 0.0159 | Grad Norm: 0.00852663\n",
      "Epoch 1 | Step 695300 | Avg Loss: 0.0158 | Grad Norm: 0.00858896\n",
      "Epoch 1 | Step 695400 | Avg Loss: 0.0157 | Grad Norm: 0.00936302\n",
      "Epoch 1 | Step 695500 | Avg Loss: 0.0157 | Grad Norm: 0.00841961\n",
      "Epoch 1 | Step 695600 | Avg Loss: 0.0159 | Grad Norm: 0.00938775\n",
      "Epoch 1 | Step 695700 | Avg Loss: 0.0157 | Grad Norm: 0.00848342\n",
      "Epoch 1 | Step 695800 | Avg Loss: 0.0157 | Grad Norm: 0.00851721\n",
      "Epoch 1 | Step 695900 | Avg Loss: 0.0157 | Grad Norm: 0.00781673\n",
      "Epoch 1 | Step 696000 | Avg Loss: 0.0160 | Grad Norm: 0.00820595\n",
      "Epoch 1 | Step 696100 | Avg Loss: 0.0158 | Grad Norm: 0.01043436\n",
      "Epoch 1 | Step 696200 | Avg Loss: 0.0157 | Grad Norm: 0.00804799\n",
      "Epoch 1 | Step 696300 | Avg Loss: 0.0157 | Grad Norm: 0.00894326\n",
      "Epoch 1 | Step 696400 | Avg Loss: 0.0158 | Grad Norm: 0.01013584\n",
      "Epoch 1 | Step 696500 | Avg Loss: 0.0155 | Grad Norm: 0.00855720\n",
      "Epoch 1 | Step 696600 | Avg Loss: 0.0159 | Grad Norm: 0.00935446\n",
      "Epoch 1 | Step 696700 | Avg Loss: 0.0156 | Grad Norm: 0.00974584\n",
      "Epoch 1 | Step 696800 | Avg Loss: 0.0156 | Grad Norm: 0.00974901\n",
      "Epoch 1 | Step 696900 | Avg Loss: 0.0158 | Grad Norm: 0.00835810\n",
      "Epoch 1 | Step 697000 | Avg Loss: 0.0157 | Grad Norm: 0.00835984\n",
      "Epoch 1 | Step 697100 | Avg Loss: 0.0156 | Grad Norm: 0.00869495\n",
      "Epoch 1 | Step 697200 | Avg Loss: 0.0160 | Grad Norm: 0.01033406\n",
      "Epoch 1 | Step 697300 | Avg Loss: 0.0159 | Grad Norm: 0.01073779\n",
      "Epoch 1 | Step 697400 | Avg Loss: 0.0163 | Grad Norm: 0.01106464\n",
      "Epoch 1 | Step 697500 | Avg Loss: 0.0162 | Grad Norm: 0.00887672\n",
      "Epoch 1 | Step 697600 | Avg Loss: 0.0163 | Grad Norm: 0.00863482\n",
      "Epoch 1 | Step 697700 | Avg Loss: 0.0163 | Grad Norm: 0.01303889\n",
      "Epoch 1 | Step 697800 | Avg Loss: 0.0165 | Grad Norm: 0.00942744\n",
      "Epoch 1 | Step 697900 | Avg Loss: 0.0162 | Grad Norm: 0.00957696\n",
      "Epoch 1 | Step 698000 | Avg Loss: 0.0163 | Grad Norm: 0.00915624\n",
      "Epoch 1 | Step 698100 | Avg Loss: 0.0162 | Grad Norm: 0.00997283\n",
      "Epoch 1 | Step 698200 | Avg Loss: 0.0161 | Grad Norm: 0.00842083\n",
      "Epoch 1 | Step 698300 | Avg Loss: 0.0162 | Grad Norm: 0.00976309\n",
      "Epoch 1 | Step 698400 | Avg Loss: 0.0164 | Grad Norm: 0.01103702\n",
      "Epoch 1 | Step 698500 | Avg Loss: 0.0161 | Grad Norm: 0.00846597\n",
      "Epoch 1 | Step 698600 | Avg Loss: 0.0160 | Grad Norm: 0.00924109\n",
      "Epoch 1 | Step 698700 | Avg Loss: 0.0164 | Grad Norm: 0.00888634\n",
      "Epoch 1 | Step 698800 | Avg Loss: 0.0161 | Grad Norm: 0.00837969\n",
      "Epoch 1 | Step 698900 | Avg Loss: 0.0157 | Grad Norm: 0.00858883\n",
      "Epoch 1 | Step 699000 | Avg Loss: 0.0154 | Grad Norm: 0.00826590\n",
      "Epoch 1 | Step 699100 | Avg Loss: 0.0157 | Grad Norm: 0.00864769\n",
      "Epoch 1 | Step 699200 | Avg Loss: 0.0158 | Grad Norm: 0.01000763\n",
      "Epoch 1 | Step 699300 | Avg Loss: 0.0160 | Grad Norm: 0.00826860\n",
      "Epoch 1 | Step 699400 | Avg Loss: 0.0156 | Grad Norm: 0.00843259\n",
      "Epoch 1 | Step 699500 | Avg Loss: 0.0158 | Grad Norm: 0.00801203\n",
      "Epoch 1 | Step 699600 | Avg Loss: 0.0161 | Grad Norm: 0.01068382\n",
      "Epoch 1 | Step 699700 | Avg Loss: 0.0157 | Grad Norm: 0.00815528\n",
      "Epoch 1 | Step 699800 | Avg Loss: 0.0158 | Grad Norm: 0.00892410\n",
      "Epoch 1 | Step 699900 | Avg Loss: 0.0157 | Grad Norm: 0.00824833\n",
      "Epoch 1 | Step 700000 | Avg Loss: 0.0157 | Grad Norm: 0.00842947\n",
      "Saving model at step700000\n",
      "Epoch 1 | Step 700100 | Avg Loss: 0.0156 | Grad Norm: 0.00844054\n",
      "Epoch 1 | Step 700200 | Avg Loss: 0.0156 | Grad Norm: 0.00899843\n",
      "Epoch 1 | Step 700300 | Avg Loss: 0.0159 | Grad Norm: 0.00869250\n",
      "Epoch 1 | Step 700400 | Avg Loss: 0.0162 | Grad Norm: 0.00834694\n",
      "Epoch 1 | Step 700500 | Avg Loss: 0.0161 | Grad Norm: 0.00840310\n",
      "Epoch 1 | Step 700600 | Avg Loss: 0.0159 | Grad Norm: 0.00806719\n",
      "Epoch 1 | Step 700700 | Avg Loss: 0.0154 | Grad Norm: 0.00795052\n",
      "Epoch 1 | Step 700800 | Avg Loss: 0.0153 | Grad Norm: 0.00937976\n",
      "Epoch 1 | Step 700900 | Avg Loss: 0.0153 | Grad Norm: 0.00956125\n",
      "Epoch 1 | Step 701000 | Avg Loss: 0.0153 | Grad Norm: 0.00821590\n",
      "Epoch 1 | Step 701100 | Avg Loss: 0.0157 | Grad Norm: 0.00855075\n",
      "Epoch 1 | Step 701200 | Avg Loss: 0.0160 | Grad Norm: 0.00874782\n",
      "Epoch 1 | Step 701300 | Avg Loss: 0.0157 | Grad Norm: 0.01029726\n",
      "Epoch 1 | Step 701400 | Avg Loss: 0.0156 | Grad Norm: 0.00814560\n",
      "Epoch 1 | Step 701500 | Avg Loss: 0.0158 | Grad Norm: 0.00927570\n",
      "Epoch 1 | Step 701600 | Avg Loss: 0.0155 | Grad Norm: 0.00761568\n",
      "Epoch 1 | Step 701700 | Avg Loss: 0.0159 | Grad Norm: 0.00823171\n",
      "Epoch 1 | Step 701800 | Avg Loss: 0.0157 | Grad Norm: 0.00879166\n",
      "Epoch 1 | Step 701900 | Avg Loss: 0.0159 | Grad Norm: 0.00846082\n",
      "Epoch 1 | Step 702000 | Avg Loss: 0.0157 | Grad Norm: 0.00838311\n",
      "Epoch 1 | Step 702100 | Avg Loss: 0.0157 | Grad Norm: 0.00854783\n",
      "Epoch 1 | Step 702200 | Avg Loss: 0.0159 | Grad Norm: 0.00843557\n",
      "Epoch 1 | Step 702300 | Avg Loss: 0.0157 | Grad Norm: 0.00804162\n",
      "Epoch 1 | Step 702400 | Avg Loss: 0.0154 | Grad Norm: 0.00798178\n",
      "Epoch 1 | Step 702500 | Avg Loss: 0.0156 | Grad Norm: 0.00865389\n",
      "Epoch 1 | Step 702600 | Avg Loss: 0.0157 | Grad Norm: 0.00960668\n",
      "Epoch 1 | Step 702700 | Avg Loss: 0.0158 | Grad Norm: 0.00864745\n",
      "Epoch 1 | Step 702800 | Avg Loss: 0.0156 | Grad Norm: 0.00773980\n",
      "Epoch 1 | Step 702900 | Avg Loss: 0.0154 | Grad Norm: 0.00872465\n",
      "Epoch 1 | Step 703000 | Avg Loss: 0.0151 | Grad Norm: 0.00807020\n",
      "Epoch 1 | Step 703100 | Avg Loss: 0.0158 | Grad Norm: 0.00810904\n",
      "Epoch 1 | Step 703200 | Avg Loss: 0.0155 | Grad Norm: 0.00858886\n",
      "Epoch 1 | Step 703300 | Avg Loss: 0.0155 | Grad Norm: 0.00913169\n",
      "Epoch 1 | Step 703400 | Avg Loss: 0.0158 | Grad Norm: 0.00874153\n",
      "Epoch 1 | Step 703500 | Avg Loss: 0.0160 | Grad Norm: 0.00961687\n",
      "Epoch 1 | Step 703600 | Avg Loss: 0.0155 | Grad Norm: 0.00974422\n",
      "Epoch 1 | Step 703700 | Avg Loss: 0.0158 | Grad Norm: 0.00952161\n",
      "Epoch 1 | Step 703800 | Avg Loss: 0.0154 | Grad Norm: 0.00837840\n",
      "Epoch 1 | Step 703900 | Avg Loss: 0.0156 | Grad Norm: 0.00747797\n",
      "Epoch 1 | Step 704000 | Avg Loss: 0.0155 | Grad Norm: 0.00800908\n",
      "Epoch 1 | Step 704100 | Avg Loss: 0.0154 | Grad Norm: 0.00900880\n",
      "Epoch 1 | Step 704200 | Avg Loss: 0.0155 | Grad Norm: 0.00909473\n",
      "Epoch 1 | Step 704300 | Avg Loss: 0.0157 | Grad Norm: 0.00905269\n",
      "Epoch 1 | Step 704400 | Avg Loss: 0.0151 | Grad Norm: 0.00963336\n",
      "Epoch 1 | Step 704500 | Avg Loss: 0.0153 | Grad Norm: 0.00843834\n",
      "Epoch 1 | Step 704600 | Avg Loss: 0.0159 | Grad Norm: 0.00873886\n",
      "Epoch 1 | Step 704700 | Avg Loss: 0.0156 | Grad Norm: 0.01034364\n",
      "Epoch 1 | Step 704800 | Avg Loss: 0.0156 | Grad Norm: 0.00839199\n",
      "Epoch 1 | Step 704900 | Avg Loss: 0.0154 | Grad Norm: 0.00869024\n",
      "Epoch 1 | Step 705000 | Avg Loss: 0.0155 | Grad Norm: 0.00981408\n",
      "Epoch 1 | Step 705100 | Avg Loss: 0.0155 | Grad Norm: 0.00898019\n",
      "Epoch 1 | Step 705200 | Avg Loss: 0.0155 | Grad Norm: 0.00977979\n",
      "Epoch 1 | Step 705300 | Avg Loss: 0.0155 | Grad Norm: 0.00823124\n",
      "Epoch 1 | Step 705400 | Avg Loss: 0.0156 | Grad Norm: 0.00917461\n",
      "Epoch 1 | Step 705500 | Avg Loss: 0.0153 | Grad Norm: 0.00807007\n",
      "Epoch 1 | Step 705600 | Avg Loss: 0.0155 | Grad Norm: 0.00799437\n",
      "Epoch 1 | Step 705700 | Avg Loss: 0.0158 | Grad Norm: 0.01009818\n",
      "Epoch 1 | Step 705800 | Avg Loss: 0.0156 | Grad Norm: 0.00944924\n",
      "Epoch 1 | Step 705900 | Avg Loss: 0.0159 | Grad Norm: 0.01029488\n",
      "Epoch 1 | Step 706000 | Avg Loss: 0.0156 | Grad Norm: 0.00843066\n",
      "Epoch 1 | Step 706100 | Avg Loss: 0.0153 | Grad Norm: 0.00971928\n",
      "Epoch 1 | Step 706200 | Avg Loss: 0.0157 | Grad Norm: 0.00949874\n",
      "Epoch 1 | Step 706300 | Avg Loss: 0.0156 | Grad Norm: 0.00897774\n",
      "Epoch 1 | Step 706400 | Avg Loss: 0.0156 | Grad Norm: 0.00934846\n",
      "Epoch 1 | Step 706500 | Avg Loss: 0.0159 | Grad Norm: 0.02084855\n",
      "Epoch 1 | Step 706600 | Avg Loss: 0.0162 | Grad Norm: 0.00845316\n",
      "Epoch 1 | Step 706700 | Avg Loss: 0.0159 | Grad Norm: 0.00861892\n",
      "Epoch 1 | Step 706800 | Avg Loss: 0.0159 | Grad Norm: 0.00815955\n",
      "Epoch 1 | Step 706900 | Avg Loss: 0.0162 | Grad Norm: 0.00875229\n",
      "Epoch 1 | Step 707000 | Avg Loss: 0.0158 | Grad Norm: 0.00839065\n",
      "Epoch 1 | Step 707100 | Avg Loss: 0.0159 | Grad Norm: 0.00878075\n",
      "Epoch 1 | Step 707200 | Avg Loss: 0.0158 | Grad Norm: 0.00868409\n",
      "Epoch 1 | Step 707300 | Avg Loss: 0.0158 | Grad Norm: 0.00975863\n",
      "Epoch 1 | Step 707400 | Avg Loss: 0.0158 | Grad Norm: 0.00950801\n",
      "Epoch 1 | Step 707500 | Avg Loss: 0.0158 | Grad Norm: 0.00893917\n",
      "Epoch 1 | Step 707600 | Avg Loss: 0.0157 | Grad Norm: 0.00889376\n",
      "Epoch 1 | Step 707700 | Avg Loss: 0.0158 | Grad Norm: 0.00837819\n",
      "Epoch 1 | Step 707800 | Avg Loss: 0.0163 | Grad Norm: 0.00913488\n",
      "Epoch 1 | Step 707900 | Avg Loss: 0.0160 | Grad Norm: 0.00872918\n",
      "Epoch 1 | Step 708000 | Avg Loss: 0.0156 | Grad Norm: 0.00973572\n",
      "Epoch 1 | Step 708100 | Avg Loss: 0.0156 | Grad Norm: 0.00876069\n",
      "Epoch 1 | Step 708200 | Avg Loss: 0.0159 | Grad Norm: 0.00867721\n",
      "Epoch 1 | Step 708300 | Avg Loss: 0.0159 | Grad Norm: 0.01027750\n",
      "Epoch 1 | Step 708400 | Avg Loss: 0.0163 | Grad Norm: 0.01117032\n",
      "Epoch 1 | Step 708500 | Avg Loss: 0.0163 | Grad Norm: 0.00881476\n",
      "Epoch 1 | Step 708600 | Avg Loss: 0.0160 | Grad Norm: 0.00867826\n",
      "Epoch 1 | Step 708700 | Avg Loss: 0.0158 | Grad Norm: 0.00881699\n",
      "Epoch 1 | Step 708800 | Avg Loss: 0.0159 | Grad Norm: 0.00922533\n",
      "Epoch 1 | Step 708900 | Avg Loss: 0.0156 | Grad Norm: 0.00932495\n",
      "Epoch 1 | Step 709000 | Avg Loss: 0.0157 | Grad Norm: 0.00878833\n",
      "Epoch 1 | Step 709100 | Avg Loss: 0.0158 | Grad Norm: 0.00968149\n",
      "Epoch 1 | Step 709200 | Avg Loss: 0.0156 | Grad Norm: 0.01003833\n",
      "Epoch 1 | Step 709300 | Avg Loss: 0.0155 | Grad Norm: 0.00851138\n",
      "Epoch 1 | Step 709400 | Avg Loss: 0.0157 | Grad Norm: 0.00914060\n",
      "Epoch 1 | Step 709500 | Avg Loss: 0.0156 | Grad Norm: 0.00843807\n",
      "Epoch 1 | Step 709600 | Avg Loss: 0.0156 | Grad Norm: 0.00911368\n",
      "Epoch 1 | Step 709700 | Avg Loss: 0.0154 | Grad Norm: 0.00942689\n",
      "Epoch 1 | Step 709800 | Avg Loss: 0.0153 | Grad Norm: 0.00936316\n",
      "Epoch 1 | Step 709900 | Avg Loss: 0.0154 | Grad Norm: 0.01041724\n",
      "Epoch 1 | Step 710000 | Avg Loss: 0.0157 | Grad Norm: 0.00857358\n",
      "Epoch 1 | Step 710100 | Avg Loss: 0.0161 | Grad Norm: 0.00889077\n",
      "Epoch 1 | Step 710200 | Avg Loss: 0.0160 | Grad Norm: 0.00884887\n",
      "Epoch 1 | Step 710300 | Avg Loss: 0.0157 | Grad Norm: 0.01218746\n",
      "Epoch 1 | Step 710400 | Avg Loss: 0.0158 | Grad Norm: 0.01003884\n",
      "Epoch 1 | Step 710500 | Avg Loss: 0.0157 | Grad Norm: 0.00890922\n",
      "Epoch 1 | Step 710600 | Avg Loss: 0.0156 | Grad Norm: 0.00852808\n",
      "Epoch 1 | Step 710700 | Avg Loss: 0.0160 | Grad Norm: 0.00901656\n",
      "Epoch 1 | Step 710800 | Avg Loss: 0.0163 | Grad Norm: 0.00815351\n",
      "Epoch 1 | Step 710900 | Avg Loss: 0.0156 | Grad Norm: 0.00890531\n",
      "Epoch 1 | Step 711000 | Avg Loss: 0.0157 | Grad Norm: 0.00911428\n",
      "Epoch 1 | Step 711100 | Avg Loss: 0.0158 | Grad Norm: 0.00708105\n",
      "Epoch 1 | Step 711200 | Avg Loss: 0.0152 | Grad Norm: 0.00840062\n",
      "Epoch 1 | Step 711300 | Avg Loss: 0.0152 | Grad Norm: 0.01004197\n",
      "Epoch 1 | Step 711400 | Avg Loss: 0.0154 | Grad Norm: 0.00939621\n",
      "Epoch 1 | Step 711500 | Avg Loss: 0.0152 | Grad Norm: 0.00786527\n",
      "Epoch 1 | Step 711600 | Avg Loss: 0.0154 | Grad Norm: 0.00950538\n",
      "Epoch 1 | Step 711700 | Avg Loss: 0.0153 | Grad Norm: 0.00778686\n",
      "Epoch 1 | Step 711800 | Avg Loss: 0.0153 | Grad Norm: 0.00881153\n",
      "Epoch 1 | Step 711900 | Avg Loss: 0.0154 | Grad Norm: 0.00824149\n",
      "Epoch 1 | Step 712000 | Avg Loss: 0.0159 | Grad Norm: 0.00860857\n",
      "Epoch 1 | Step 712100 | Avg Loss: 0.0161 | Grad Norm: 0.01101891\n",
      "Epoch 1 | Step 712200 | Avg Loss: 0.0157 | Grad Norm: 0.00989037\n",
      "Epoch 1 | Step 712300 | Avg Loss: 0.0158 | Grad Norm: 0.00838866\n",
      "Epoch 1 | Step 712400 | Avg Loss: 0.0158 | Grad Norm: 0.00872652\n",
      "Epoch 1 | Step 712500 | Avg Loss: 0.0162 | Grad Norm: 0.01031941\n",
      "Epoch 1 | Step 712600 | Avg Loss: 0.0156 | Grad Norm: 0.00944446\n",
      "Epoch 1 | Step 712700 | Avg Loss: 0.0159 | Grad Norm: 0.00828620\n",
      "Epoch 1 | Step 712800 | Avg Loss: 0.0157 | Grad Norm: 0.00895402\n",
      "Epoch 1 | Step 712900 | Avg Loss: 0.0158 | Grad Norm: 0.00838119\n",
      "Epoch 1 | Step 713000 | Avg Loss: 0.0158 | Grad Norm: 0.01307515\n",
      "Epoch 1 | Step 713100 | Avg Loss: 0.0159 | Grad Norm: 0.00915651\n",
      "Epoch 1 | Step 713200 | Avg Loss: 0.0158 | Grad Norm: 0.00715448\n",
      "Epoch 1 | Step 713300 | Avg Loss: 0.0158 | Grad Norm: 0.00827413\n",
      "Epoch 1 | Step 713400 | Avg Loss: 0.0159 | Grad Norm: 0.00875225\n",
      "Epoch 1 | Step 713500 | Avg Loss: 0.0160 | Grad Norm: 0.00922634\n",
      "Epoch 1 | Step 713600 | Avg Loss: 0.0162 | Grad Norm: 0.00730546\n",
      "Epoch 1 | Step 713700 | Avg Loss: 0.0158 | Grad Norm: 0.00796714\n",
      "Epoch 1 | Step 713800 | Avg Loss: 0.0161 | Grad Norm: 0.00966200\n",
      "Epoch 1 | Step 713900 | Avg Loss: 0.0161 | Grad Norm: 0.00955496\n",
      "Epoch 1 | Step 714000 | Avg Loss: 0.0158 | Grad Norm: 0.00842712\n",
      "Epoch 1 | Step 714100 | Avg Loss: 0.0158 | Grad Norm: 0.00991496\n",
      "Epoch 1 | Step 714200 | Avg Loss: 0.0158 | Grad Norm: 0.00824591\n",
      "Epoch 1 | Step 714300 | Avg Loss: 0.0156 | Grad Norm: 0.00889484\n",
      "Epoch 1 | Step 714400 | Avg Loss: 0.0154 | Grad Norm: 0.00820040\n",
      "Epoch 1 | Step 714500 | Avg Loss: 0.0155 | Grad Norm: 0.00889665\n",
      "Epoch 1 | Step 714600 | Avg Loss: 0.0157 | Grad Norm: 0.00862779\n",
      "Epoch 1 | Step 714700 | Avg Loss: 0.0158 | Grad Norm: 0.00829943\n",
      "Epoch 1 | Step 714800 | Avg Loss: 0.0157 | Grad Norm: 0.00863336\n",
      "Epoch 1 | Step 714900 | Avg Loss: 0.0154 | Grad Norm: 0.00756863\n",
      "Epoch 1 | Step 715000 | Avg Loss: 0.0154 | Grad Norm: 0.00883418\n",
      "Epoch 1 | Step 715100 | Avg Loss: 0.0160 | Grad Norm: 0.00789684\n",
      "Epoch 1 | Step 715200 | Avg Loss: 0.0157 | Grad Norm: 0.00807881\n",
      "Epoch 1 | Step 715300 | Avg Loss: 0.0155 | Grad Norm: 0.00765061\n",
      "Epoch 1 | Step 715400 | Avg Loss: 0.0155 | Grad Norm: 0.00947044\n",
      "Epoch 1 | Step 715500 | Avg Loss: 0.0158 | Grad Norm: 0.00955623\n",
      "Epoch 1 | Step 715600 | Avg Loss: 0.0159 | Grad Norm: 0.00843223\n",
      "Epoch 1 | Step 715700 | Avg Loss: 0.0159 | Grad Norm: 0.00931172\n",
      "Epoch 1 | Step 715800 | Avg Loss: 0.0163 | Grad Norm: 0.00838677\n",
      "Epoch 1 | Step 715900 | Avg Loss: 0.0161 | Grad Norm: 0.00843858\n",
      "Epoch 1 | Step 716000 | Avg Loss: 0.0161 | Grad Norm: 0.01006554\n",
      "Epoch 1 | Step 716100 | Avg Loss: 0.0165 | Grad Norm: 0.00926195\n",
      "Epoch 1 | Step 716200 | Avg Loss: 0.0163 | Grad Norm: 0.00746685\n",
      "Epoch 1 | Step 716300 | Avg Loss: 0.0163 | Grad Norm: 0.01011872\n",
      "Epoch 1 | Step 716400 | Avg Loss: 0.0160 | Grad Norm: 0.00844377\n",
      "Epoch 1 | Step 716500 | Avg Loss: 0.0162 | Grad Norm: 0.00965437\n",
      "Epoch 1 | Step 716600 | Avg Loss: 0.0158 | Grad Norm: 0.00883772\n",
      "Epoch 1 | Step 716700 | Avg Loss: 0.0153 | Grad Norm: 0.00917586\n",
      "Epoch 1 | Step 716800 | Avg Loss: 0.0158 | Grad Norm: 0.00783733\n",
      "Epoch 1 | Step 716900 | Avg Loss: 0.0161 | Grad Norm: 0.00949150\n",
      "Epoch 1 | Step 717000 | Avg Loss: 0.0160 | Grad Norm: 0.00963214\n",
      "Epoch 1 | Step 717100 | Avg Loss: 0.0159 | Grad Norm: 0.00968632\n",
      "Epoch 1 | Step 717200 | Avg Loss: 0.0159 | Grad Norm: 0.00915960\n",
      "Epoch 1 | Step 717300 | Avg Loss: 0.0160 | Grad Norm: 0.00895411\n",
      "Epoch 1 | Step 717400 | Avg Loss: 0.0159 | Grad Norm: 0.01090232\n",
      "Epoch 1 | Step 717500 | Avg Loss: 0.0159 | Grad Norm: 0.00940999\n",
      "Epoch 1 | Step 717600 | Avg Loss: 0.0161 | Grad Norm: 0.00777788\n",
      "Epoch 1 | Step 717700 | Avg Loss: 0.0163 | Grad Norm: 0.01132083\n",
      "Epoch 1 | Step 717800 | Avg Loss: 0.0164 | Grad Norm: 0.01021956\n",
      "Epoch 1 | Step 717900 | Avg Loss: 0.0163 | Grad Norm: 0.00805969\n",
      "Epoch 1 | Step 718000 | Avg Loss: 0.0161 | Grad Norm: 0.01001405\n",
      "Epoch 1 | Step 718100 | Avg Loss: 0.0161 | Grad Norm: 0.00839898\n",
      "Epoch 1 | Step 718200 | Avg Loss: 0.0160 | Grad Norm: 0.00853106\n",
      "Epoch 1 | Step 718300 | Avg Loss: 0.0162 | Grad Norm: 0.00910019\n",
      "Epoch 1 | Step 718400 | Avg Loss: 0.0162 | Grad Norm: 0.00904697\n",
      "Epoch 1 | Step 718500 | Avg Loss: 0.0163 | Grad Norm: 0.00837335\n",
      "Epoch 1 | Step 718600 | Avg Loss: 0.0161 | Grad Norm: 0.00826109\n",
      "Epoch 1 | Step 718700 | Avg Loss: 0.0160 | Grad Norm: 0.01004939\n",
      "Epoch 1 | Step 718800 | Avg Loss: 0.0158 | Grad Norm: 0.00937581\n",
      "Epoch 1 | Step 718900 | Avg Loss: 0.0159 | Grad Norm: 0.00802615\n",
      "Epoch 1 | Step 719000 | Avg Loss: 0.0155 | Grad Norm: 0.00933186\n",
      "Epoch 1 | Step 719100 | Avg Loss: 0.0155 | Grad Norm: 0.00978958\n",
      "Epoch 1 | Step 719200 | Avg Loss: 0.0158 | Grad Norm: 0.00903526\n",
      "Epoch 1 | Step 719300 | Avg Loss: 0.0156 | Grad Norm: 0.01123072\n",
      "Epoch 1 | Step 719400 | Avg Loss: 0.0155 | Grad Norm: 0.00983136\n",
      "Epoch 1 | Step 719500 | Avg Loss: 0.0155 | Grad Norm: 0.00920289\n",
      "Epoch 1 | Step 719600 | Avg Loss: 0.0155 | Grad Norm: 0.01246195\n",
      "Epoch 1 | Step 719700 | Avg Loss: 0.0153 | Grad Norm: 0.00918999\n",
      "Epoch 1 | Step 719800 | Avg Loss: 0.0153 | Grad Norm: 0.01001599\n",
      "Epoch 1 | Step 719900 | Avg Loss: 0.0156 | Grad Norm: 0.00885132\n",
      "Epoch 1 | Step 720000 | Avg Loss: 0.0154 | Grad Norm: 0.00808768\n",
      "Epoch 1 | Step 720100 | Avg Loss: 0.0157 | Grad Norm: 0.00929078\n",
      "Epoch 1 | Step 720200 | Avg Loss: 0.0153 | Grad Norm: 0.01072713\n",
      "Epoch 1 | Step 720300 | Avg Loss: 0.0154 | Grad Norm: 0.00831840\n",
      "Epoch 1 | Step 720400 | Avg Loss: 0.0156 | Grad Norm: 0.01106077\n",
      "Epoch 1 | Step 720500 | Avg Loss: 0.0158 | Grad Norm: 0.00915898\n",
      "Epoch 1 | Step 720600 | Avg Loss: 0.0152 | Grad Norm: 0.00977599\n",
      "Epoch 1 | Step 720700 | Avg Loss: 0.0150 | Grad Norm: 0.00847042\n",
      "Epoch 1 | Step 720800 | Avg Loss: 0.0149 | Grad Norm: 0.00939572\n",
      "Epoch 1 | Step 720900 | Avg Loss: 0.0153 | Grad Norm: 0.01005263\n",
      "Epoch 1 | Step 721000 | Avg Loss: 0.0153 | Grad Norm: 0.00820733\n",
      "Epoch 1 | Step 721100 | Avg Loss: 0.0152 | Grad Norm: 0.00817300\n",
      "Epoch 1 | Step 721200 | Avg Loss: 0.0152 | Grad Norm: 0.00762780\n",
      "Epoch 1 | Step 721300 | Avg Loss: 0.0154 | Grad Norm: 0.00884954\n",
      "Epoch 1 | Step 721400 | Avg Loss: 0.0155 | Grad Norm: 0.00842913\n",
      "Epoch 1 | Step 721500 | Avg Loss: 0.0157 | Grad Norm: 0.00830234\n",
      "Epoch 1 | Step 721600 | Avg Loss: 0.0159 | Grad Norm: 0.00948467\n",
      "Epoch 1 | Step 721700 | Avg Loss: 0.0157 | Grad Norm: 0.00897295\n",
      "Epoch 1 | Step 721800 | Avg Loss: 0.0156 | Grad Norm: 0.00853705\n",
      "Epoch 1 | Step 721900 | Avg Loss: 0.0158 | Grad Norm: 0.00944608\n",
      "Epoch 1 | Step 722000 | Avg Loss: 0.0153 | Grad Norm: 0.00927897\n",
      "Epoch 1 | Step 722100 | Avg Loss: 0.0154 | Grad Norm: 0.01168944\n",
      "Epoch 1 | Step 722200 | Avg Loss: 0.0153 | Grad Norm: 0.00935729\n",
      "Epoch 1 | Step 722300 | Avg Loss: 0.0155 | Grad Norm: 0.00982391\n",
      "Epoch 1 | Step 722400 | Avg Loss: 0.0156 | Grad Norm: 0.00774369\n",
      "Epoch 1 | Step 722500 | Avg Loss: 0.0160 | Grad Norm: 0.00801245\n",
      "Epoch 1 | Step 722600 | Avg Loss: 0.0161 | Grad Norm: 0.00845697\n",
      "Epoch 1 | Step 722700 | Avg Loss: 0.0159 | Grad Norm: 0.00891845\n",
      "Epoch 1 | Step 722800 | Avg Loss: 0.0156 | Grad Norm: 0.01019565\n",
      "Epoch 1 | Step 722900 | Avg Loss: 0.0157 | Grad Norm: 0.01038300\n",
      "Epoch 1 | Step 723000 | Avg Loss: 0.0158 | Grad Norm: 0.00939635\n",
      "Epoch 1 | Step 723100 | Avg Loss: 0.0158 | Grad Norm: 0.00902134\n",
      "Epoch 1 | Step 723200 | Avg Loss: 0.0157 | Grad Norm: 0.00890960\n",
      "Epoch 1 | Step 723300 | Avg Loss: 0.0157 | Grad Norm: 0.00928505\n",
      "Epoch 1 | Step 723400 | Avg Loss: 0.0155 | Grad Norm: 0.00922368\n",
      "Epoch 1 | Step 723500 | Avg Loss: 0.0156 | Grad Norm: 0.00898863\n",
      "Epoch 1 | Step 723600 | Avg Loss: 0.0150 | Grad Norm: 0.00824207\n",
      "Epoch 1 | Step 723700 | Avg Loss: 0.0153 | Grad Norm: 0.00906732\n",
      "Epoch 1 | Step 723800 | Avg Loss: 0.0153 | Grad Norm: 0.00872595\n",
      "Epoch 1 | Step 723900 | Avg Loss: 0.0157 | Grad Norm: 0.00861412\n",
      "Epoch 1 | Step 724000 | Avg Loss: 0.0158 | Grad Norm: 0.00835793\n",
      "Epoch 1 | Step 724100 | Avg Loss: 0.0160 | Grad Norm: 0.01020824\n",
      "Epoch 1 | Step 724200 | Avg Loss: 0.0156 | Grad Norm: 0.00905140\n",
      "Epoch 1 | Step 724300 | Avg Loss: 0.0159 | Grad Norm: 0.00827581\n",
      "Epoch 1 | Step 724400 | Avg Loss: 0.0161 | Grad Norm: 0.00840362\n",
      "Epoch 1 | Step 724500 | Avg Loss: 0.0159 | Grad Norm: 0.00849803\n",
      "Epoch 1 | Step 724600 | Avg Loss: 0.0159 | Grad Norm: 0.01142729\n",
      "Epoch 1 | Step 724700 | Avg Loss: 0.0160 | Grad Norm: 0.00848195\n",
      "Epoch 1 | Step 724800 | Avg Loss: 0.0160 | Grad Norm: 0.01078232\n",
      "Epoch 1 | Step 724900 | Avg Loss: 0.0156 | Grad Norm: 0.00892480\n",
      "Epoch 1 | Step 725000 | Avg Loss: 0.0154 | Grad Norm: 0.00825588\n",
      "Epoch 1 | Step 725100 | Avg Loss: 0.0157 | Grad Norm: 0.00803263\n",
      "Epoch 1 | Step 725200 | Avg Loss: 0.0154 | Grad Norm: 0.00776233\n",
      "Epoch 1 | Step 725300 | Avg Loss: 0.0156 | Grad Norm: 0.00896351\n",
      "Epoch 1 | Step 725400 | Avg Loss: 0.0153 | Grad Norm: 0.00886376\n",
      "Epoch 1 | Step 725500 | Avg Loss: 0.0154 | Grad Norm: 0.00864580\n",
      "Epoch 1 | Step 725600 | Avg Loss: 0.0151 | Grad Norm: 0.00866099\n",
      "Epoch 1 | Step 725700 | Avg Loss: 0.0151 | Grad Norm: 0.00921610\n",
      "Epoch 1 | Step 725800 | Avg Loss: 0.0147 | Grad Norm: 0.00867998\n",
      "Epoch 1 | Step 725900 | Avg Loss: 0.0151 | Grad Norm: 0.00793789\n",
      "Epoch 1 | Step 726000 | Avg Loss: 0.0151 | Grad Norm: 0.00884084\n",
      "Epoch 1 | Step 726100 | Avg Loss: 0.0155 | Grad Norm: 0.00957095\n",
      "Epoch 1 | Step 726200 | Avg Loss: 0.0156 | Grad Norm: 0.00786997\n",
      "Epoch 1 | Step 726300 | Avg Loss: 0.0156 | Grad Norm: 0.00918261\n",
      "Epoch 1 | Step 726400 | Avg Loss: 0.0152 | Grad Norm: 0.00763678\n",
      "Epoch 1 | Step 726500 | Avg Loss: 0.0153 | Grad Norm: 0.00820653\n",
      "Epoch 1 | Step 726600 | Avg Loss: 0.0154 | Grad Norm: 0.00853896\n",
      "Epoch 1 | Step 726700 | Avg Loss: 0.0157 | Grad Norm: 0.00875120\n",
      "Epoch 1 | Step 726800 | Avg Loss: 0.0155 | Grad Norm: 0.00900204\n",
      "Epoch 1 | Step 726900 | Avg Loss: 0.0157 | Grad Norm: 0.01165549\n",
      "Epoch 1 | Step 727000 | Avg Loss: 0.0158 | Grad Norm: 0.00780056\n",
      "Epoch 1 | Step 727100 | Avg Loss: 0.0158 | Grad Norm: 0.01076708\n",
      "Epoch 1 | Step 727200 | Avg Loss: 0.0157 | Grad Norm: 0.01114266\n",
      "Epoch 1 | Step 727300 | Avg Loss: 0.0155 | Grad Norm: 0.00866707\n",
      "Epoch 1 | Step 727400 | Avg Loss: 0.0156 | Grad Norm: 0.00863007\n",
      "Epoch 1 | Step 727500 | Avg Loss: 0.0155 | Grad Norm: 0.00815434\n",
      "Epoch 1 | Step 727600 | Avg Loss: 0.0155 | Grad Norm: 0.00871467\n",
      "Epoch 1 | Step 727700 | Avg Loss: 0.0156 | Grad Norm: 0.00791168\n",
      "Epoch 1 | Step 727800 | Avg Loss: 0.0158 | Grad Norm: 0.00774861\n",
      "Epoch 1 | Step 727900 | Avg Loss: 0.0156 | Grad Norm: 0.00871119\n",
      "Epoch 1 | Step 728000 | Avg Loss: 0.0163 | Grad Norm: 0.01024705\n",
      "Epoch 1 | Step 728100 | Avg Loss: 0.0159 | Grad Norm: 0.00930845\n",
      "Epoch 1 | Step 728200 | Avg Loss: 0.0154 | Grad Norm: 0.00842909\n",
      "Epoch 1 | Step 728300 | Avg Loss: 0.0152 | Grad Norm: 0.00884611\n",
      "Epoch 1 | Step 728400 | Avg Loss: 0.0153 | Grad Norm: 0.00907602\n",
      "Epoch 1 | Step 728500 | Avg Loss: 0.0155 | Grad Norm: 0.00784775\n",
      "Epoch 1 | Step 728600 | Avg Loss: 0.0157 | Grad Norm: 0.01089290\n",
      "Epoch 1 | Step 728700 | Avg Loss: 0.0156 | Grad Norm: 0.00846654\n",
      "Epoch 1 | Step 728800 | Avg Loss: 0.0157 | Grad Norm: 0.00778039\n",
      "Epoch 1 | Step 728900 | Avg Loss: 0.0160 | Grad Norm: 0.00940353\n",
      "Epoch 1 | Step 729000 | Avg Loss: 0.0160 | Grad Norm: 0.00962331\n",
      "Epoch 1 | Step 729100 | Avg Loss: 0.0158 | Grad Norm: 0.00735453\n",
      "Epoch 1 | Step 729200 | Avg Loss: 0.0159 | Grad Norm: 0.00753462\n",
      "Epoch 1 | Step 729300 | Avg Loss: 0.0157 | Grad Norm: 0.00777392\n",
      "Epoch 1 | Step 729400 | Avg Loss: 0.0158 | Grad Norm: 0.00997180\n",
      "Epoch 1 | Step 729500 | Avg Loss: 0.0160 | Grad Norm: 0.00884313\n",
      "Epoch 1 | Step 729600 | Avg Loss: 0.0160 | Grad Norm: 0.00952019\n",
      "Epoch 1 | Step 729700 | Avg Loss: 0.0160 | Grad Norm: 0.00880586\n",
      "Epoch 1 | Step 729800 | Avg Loss: 0.0163 | Grad Norm: 0.00848854\n",
      "Epoch 1 | Step 729900 | Avg Loss: 0.0162 | Grad Norm: 0.00852824\n",
      "Epoch 1 | Step 730000 | Avg Loss: 0.0159 | Grad Norm: 0.00943767\n",
      "Epoch 1 | Step 730100 | Avg Loss: 0.0157 | Grad Norm: 0.00922162\n",
      "Epoch 1 | Step 730200 | Avg Loss: 0.0159 | Grad Norm: 0.00850243\n",
      "Epoch 1 | Step 730300 | Avg Loss: 0.0158 | Grad Norm: 0.00914843\n",
      "Epoch 1 | Step 730400 | Avg Loss: 0.0163 | Grad Norm: 0.00900713\n",
      "Epoch 1 | Step 730500 | Avg Loss: 0.0165 | Grad Norm: 0.00866059\n",
      "Epoch 1 | Step 730600 | Avg Loss: 0.0161 | Grad Norm: 0.01052990\n",
      "Epoch 1 | Step 730700 | Avg Loss: 0.0161 | Grad Norm: 0.00986996\n",
      "Epoch 1 | Step 730800 | Avg Loss: 0.0159 | Grad Norm: 0.00940624\n",
      "Epoch 1 | Step 730900 | Avg Loss: 0.0158 | Grad Norm: 0.00876443\n",
      "Epoch 1 | Step 731000 | Avg Loss: 0.0158 | Grad Norm: 0.00899930\n",
      "Epoch 1 | Step 731100 | Avg Loss: 0.0154 | Grad Norm: 0.00885922\n",
      "Epoch 1 | Step 731200 | Avg Loss: 0.0155 | Grad Norm: 0.00876637\n",
      "Epoch 1 | Step 731300 | Avg Loss: 0.0153 | Grad Norm: 0.00862924\n",
      "Epoch 1 | Step 731400 | Avg Loss: 0.0157 | Grad Norm: 0.00922510\n",
      "Epoch 1 | Step 731500 | Avg Loss: 0.0161 | Grad Norm: 0.00861296\n",
      "Epoch 1 | Step 731600 | Avg Loss: 0.0160 | Grad Norm: 0.00887587\n",
      "Epoch 1 | Step 731700 | Avg Loss: 0.0157 | Grad Norm: 0.00814417\n",
      "Epoch 1 | Step 731800 | Avg Loss: 0.0159 | Grad Norm: 0.00836682\n",
      "Epoch 1 | Step 731900 | Avg Loss: 0.0158 | Grad Norm: 0.01054122\n",
      "Epoch 1 | Step 732000 | Avg Loss: 0.0155 | Grad Norm: 0.00959200\n",
      "Epoch 1 | Step 732100 | Avg Loss: 0.0156 | Grad Norm: 0.00865066\n",
      "Epoch 1 | Step 732200 | Avg Loss: 0.0156 | Grad Norm: 0.00835114\n",
      "Epoch 1 | Step 732300 | Avg Loss: 0.0157 | Grad Norm: 0.00850366\n",
      "Epoch 1 | Step 732400 | Avg Loss: 0.0156 | Grad Norm: 0.00815910\n",
      "Epoch 1 | Step 732500 | Avg Loss: 0.0151 | Grad Norm: 0.00826583\n",
      "Epoch 1 | Step 732600 | Avg Loss: 0.0149 | Grad Norm: 0.00811207\n",
      "Epoch 1 | Step 732700 | Avg Loss: 0.0154 | Grad Norm: 0.00954044\n",
      "Epoch 1 | Step 732800 | Avg Loss: 0.0151 | Grad Norm: 0.00803373\n",
      "Epoch 1 | Step 732900 | Avg Loss: 0.0153 | Grad Norm: 0.00895055\n",
      "Epoch 1 | Step 733000 | Avg Loss: 0.0154 | Grad Norm: 0.00937201\n",
      "Epoch 1 | Step 733100 | Avg Loss: 0.0155 | Grad Norm: 0.00933989\n",
      "Epoch 1 | Step 733200 | Avg Loss: 0.0154 | Grad Norm: 0.00859342\n",
      "Epoch 1 | Step 733300 | Avg Loss: 0.0152 | Grad Norm: 0.00823932\n",
      "Epoch 1 | Step 733400 | Avg Loss: 0.0154 | Grad Norm: 0.00873497\n",
      "Epoch 1 | Step 733500 | Avg Loss: 0.0155 | Grad Norm: 0.01045333\n",
      "Epoch 1 | Step 733600 | Avg Loss: 0.0156 | Grad Norm: 0.00804141\n",
      "Epoch 1 | Step 733700 | Avg Loss: 0.0157 | Grad Norm: 0.00981239\n",
      "Epoch 1 | Step 733800 | Avg Loss: 0.0159 | Grad Norm: 0.00840332\n",
      "Epoch 1 | Step 733900 | Avg Loss: 0.0159 | Grad Norm: 0.00938679\n",
      "Epoch 1 | Step 734000 | Avg Loss: 0.0160 | Grad Norm: 0.00784433\n",
      "Epoch 1 | Step 734100 | Avg Loss: 0.0158 | Grad Norm: 0.00890538\n",
      "Epoch 1 | Step 734200 | Avg Loss: 0.0160 | Grad Norm: 0.00877384\n",
      "Epoch 1 | Step 734300 | Avg Loss: 0.0159 | Grad Norm: 0.01155129\n",
      "Epoch 1 | Step 734400 | Avg Loss: 0.0158 | Grad Norm: 0.00941786\n",
      "Epoch 1 | Step 734500 | Avg Loss: 0.0158 | Grad Norm: 0.00826318\n",
      "Epoch 1 | Step 734600 | Avg Loss: 0.0159 | Grad Norm: 0.00857350\n",
      "Epoch 1 | Step 734700 | Avg Loss: 0.0159 | Grad Norm: 0.00998797\n",
      "Epoch 1 | Step 734800 | Avg Loss: 0.0163 | Grad Norm: 0.01094764\n",
      "Epoch 1 | Step 734900 | Avg Loss: 0.0161 | Grad Norm: 0.00867315\n",
      "Epoch 1 | Step 735000 | Avg Loss: 0.0161 | Grad Norm: 0.00940220\n",
      "Epoch 1 | Step 735100 | Avg Loss: 0.0161 | Grad Norm: 0.00929446\n",
      "Epoch 1 | Step 735200 | Avg Loss: 0.0160 | Grad Norm: 0.00787428\n",
      "Epoch 1 | Step 735300 | Avg Loss: 0.0160 | Grad Norm: 0.00832721\n",
      "Epoch 1 | Step 735400 | Avg Loss: 0.0159 | Grad Norm: 0.00890405\n",
      "Epoch 1 | Step 735500 | Avg Loss: 0.0153 | Grad Norm: 0.01102402\n",
      "Epoch 1 | Step 735600 | Avg Loss: 0.0152 | Grad Norm: 0.00838111\n",
      "Epoch 1 | Step 735700 | Avg Loss: 0.0154 | Grad Norm: 0.00898177\n",
      "Epoch 1 | Step 735800 | Avg Loss: 0.0157 | Grad Norm: 0.00806132\n",
      "Epoch 1 | Step 735900 | Avg Loss: 0.0155 | Grad Norm: 0.00903695\n",
      "Epoch 1 | Step 736000 | Avg Loss: 0.0154 | Grad Norm: 0.00874093\n",
      "Epoch 1 | Step 736100 | Avg Loss: 0.0155 | Grad Norm: 0.00832097\n",
      "Epoch 1 | Step 736200 | Avg Loss: 0.0156 | Grad Norm: 0.00986171\n",
      "Epoch 1 | Step 736300 | Avg Loss: 0.0154 | Grad Norm: 0.00839332\n",
      "Epoch 1 | Step 736400 | Avg Loss: 0.0156 | Grad Norm: 0.00958296\n",
      "Epoch 1 | Step 736500 | Avg Loss: 0.0155 | Grad Norm: 0.00837355\n",
      "Epoch 1 | Step 736600 | Avg Loss: 0.0158 | Grad Norm: 0.00908158\n",
      "Epoch 1 | Step 736700 | Avg Loss: 0.0156 | Grad Norm: 0.00844480\n",
      "Epoch 1 | Step 736800 | Avg Loss: 0.0159 | Grad Norm: 0.00945777\n",
      "Epoch 1 | Step 736900 | Avg Loss: 0.0161 | Grad Norm: 0.00947602\n",
      "Epoch 1 | Step 737000 | Avg Loss: 0.0158 | Grad Norm: 0.01033533\n",
      "Epoch 1 | Step 737100 | Avg Loss: 0.0160 | Grad Norm: 0.00853283\n",
      "Epoch 1 | Step 737200 | Avg Loss: 0.0162 | Grad Norm: 0.00959062\n",
      "Epoch 1 | Step 737300 | Avg Loss: 0.0162 | Grad Norm: 0.00873672\n",
      "Epoch 1 | Step 737400 | Avg Loss: 0.0166 | Grad Norm: 0.00906543\n",
      "Epoch 1 | Step 737500 | Avg Loss: 0.0165 | Grad Norm: 0.00927007\n",
      "Epoch 1 | Step 737600 | Avg Loss: 0.0165 | Grad Norm: 0.00978474\n",
      "Epoch 1 | Step 737700 | Avg Loss: 0.0162 | Grad Norm: 0.00969033\n",
      "Epoch 1 | Step 737800 | Avg Loss: 0.0162 | Grad Norm: 0.00807195\n",
      "Epoch 1 | Step 737900 | Avg Loss: 0.0162 | Grad Norm: 0.01047089\n",
      "Epoch 1 | Step 738000 | Avg Loss: 0.0154 | Grad Norm: 0.00906165\n",
      "Epoch 1 | Step 738100 | Avg Loss: 0.0156 | Grad Norm: 0.00994617\n",
      "Epoch 1 | Step 738200 | Avg Loss: 0.0160 | Grad Norm: 0.00944073\n",
      "Epoch 1 | Step 738300 | Avg Loss: 0.0158 | Grad Norm: 0.00958266\n",
      "Epoch 1 | Step 738400 | Avg Loss: 0.0157 | Grad Norm: 0.00784767\n",
      "Epoch 1 | Step 738500 | Avg Loss: 0.0154 | Grad Norm: 0.00872178\n",
      "Epoch 1 | Step 738600 | Avg Loss: 0.0156 | Grad Norm: 0.00738185\n",
      "Epoch 1 | Step 738700 | Avg Loss: 0.0153 | Grad Norm: 0.00833515\n",
      "Epoch 1 | Step 738800 | Avg Loss: 0.0157 | Grad Norm: 0.00917729\n",
      "Epoch 1 | Step 738900 | Avg Loss: 0.0161 | Grad Norm: 0.00829662\n",
      "Epoch 1 | Step 739000 | Avg Loss: 0.0161 | Grad Norm: 0.00921254\n",
      "Epoch 1 | Step 739100 | Avg Loss: 0.0164 | Grad Norm: 0.01117777\n",
      "Epoch 1 | Step 739200 | Avg Loss: 0.0167 | Grad Norm: 0.00884475\n",
      "Epoch 1 | Step 739300 | Avg Loss: 0.0167 | Grad Norm: 0.00877147\n",
      "Epoch 1 | Step 739400 | Avg Loss: 0.0163 | Grad Norm: 0.00982790\n",
      "Epoch 1 | Step 739500 | Avg Loss: 0.0164 | Grad Norm: 0.01010351\n",
      "Epoch 1 | Step 739600 | Avg Loss: 0.0163 | Grad Norm: 0.00882051\n",
      "Epoch 1 | Step 739700 | Avg Loss: 0.0160 | Grad Norm: 0.01047294\n",
      "Epoch 1 | Step 739800 | Avg Loss: 0.0158 | Grad Norm: 0.00860320\n",
      "Epoch 1 | Step 739900 | Avg Loss: 0.0158 | Grad Norm: 0.00779311\n",
      "Epoch 1 | Step 740000 | Avg Loss: 0.0156 | Grad Norm: 0.00905001\n",
      "Epoch 1 | Step 740100 | Avg Loss: 0.0158 | Grad Norm: 0.00743475\n",
      "Epoch 1 | Step 740200 | Avg Loss: 0.0161 | Grad Norm: 0.01087666\n",
      "Epoch 1 | Step 740300 | Avg Loss: 0.0163 | Grad Norm: 0.01022155\n",
      "Epoch 1 | Step 740400 | Avg Loss: 0.0164 | Grad Norm: 0.00898482\n",
      "Epoch 1 | Step 740500 | Avg Loss: 0.0165 | Grad Norm: 0.00948418\n",
      "Epoch 1 | Step 740600 | Avg Loss: 0.0162 | Grad Norm: 0.01013212\n",
      "Epoch 1 | Step 740700 | Avg Loss: 0.0162 | Grad Norm: 0.00760632\n",
      "Epoch 1 | Step 740800 | Avg Loss: 0.0161 | Grad Norm: 0.00910010\n",
      "Epoch 1 | Step 740900 | Avg Loss: 0.0157 | Grad Norm: 0.00873297\n",
      "Epoch 1 | Step 741000 | Avg Loss: 0.0161 | Grad Norm: 0.00866209\n",
      "Epoch 1 | Step 741100 | Avg Loss: 0.0162 | Grad Norm: 0.00825707\n",
      "Epoch 1 | Step 741200 | Avg Loss: 0.0162 | Grad Norm: 0.01741438\n",
      "Epoch 1 | Step 741300 | Avg Loss: 0.0163 | Grad Norm: 0.00794977\n",
      "Epoch 1 | Step 741400 | Avg Loss: 0.0163 | Grad Norm: 0.00923093\n",
      "Epoch 1 | Step 741500 | Avg Loss: 0.0157 | Grad Norm: 0.00837072\n",
      "Epoch 1 | Step 741600 | Avg Loss: 0.0161 | Grad Norm: 0.01160870\n",
      "Epoch 1 | Step 741700 | Avg Loss: 0.0161 | Grad Norm: 0.00806082\n",
      "Epoch 1 | Step 741800 | Avg Loss: 0.0160 | Grad Norm: 0.00941407\n",
      "Epoch 1 | Step 741900 | Avg Loss: 0.0159 | Grad Norm: 0.00802036\n",
      "Epoch 1 | Step 742000 | Avg Loss: 0.0161 | Grad Norm: 0.00858452\n",
      "Epoch 1 | Step 742100 | Avg Loss: 0.0160 | Grad Norm: 0.00920848\n",
      "Epoch 1 | Step 742200 | Avg Loss: 0.0158 | Grad Norm: 0.00854375\n",
      "Epoch 1 | Step 742300 | Avg Loss: 0.0156 | Grad Norm: 0.01024426\n",
      "Epoch 1 | Step 742400 | Avg Loss: 0.0156 | Grad Norm: 0.00805608\n",
      "Epoch 1 | Step 742500 | Avg Loss: 0.0158 | Grad Norm: 0.00900540\n",
      "Epoch 1 | Step 742600 | Avg Loss: 0.0156 | Grad Norm: 0.00885723\n",
      "Epoch 1 | Step 742700 | Avg Loss: 0.0159 | Grad Norm: 0.00987428\n",
      "Epoch 1 | Step 742800 | Avg Loss: 0.0160 | Grad Norm: 0.00895687\n",
      "Epoch 1 | Step 742900 | Avg Loss: 0.0160 | Grad Norm: 0.00835734\n",
      "Epoch 1 | Step 743000 | Avg Loss: 0.0157 | Grad Norm: 0.01025318\n",
      "Epoch 1 | Step 743100 | Avg Loss: 0.0165 | Grad Norm: 0.00974469\n",
      "Epoch 1 | Step 743200 | Avg Loss: 0.0163 | Grad Norm: 0.00884185\n",
      "Epoch 1 | Step 743300 | Avg Loss: 0.0160 | Grad Norm: 0.00867985\n",
      "Epoch 1 | Step 743400 | Avg Loss: 0.0163 | Grad Norm: 0.01189251\n",
      "Epoch 1 | Step 743500 | Avg Loss: 0.0160 | Grad Norm: 0.01062705\n",
      "Epoch 1 | Step 743600 | Avg Loss: 0.0162 | Grad Norm: 0.00912715\n",
      "Epoch 1 | Step 743700 | Avg Loss: 0.0160 | Grad Norm: 0.00860030\n",
      "Epoch 1 | Step 743800 | Avg Loss: 0.0157 | Grad Norm: 0.00839837\n",
      "Epoch 1 | Step 743900 | Avg Loss: 0.0156 | Grad Norm: 0.00893335\n",
      "Epoch 1 | Step 744000 | Avg Loss: 0.0159 | Grad Norm: 0.00913841\n",
      "Epoch 1 | Step 744100 | Avg Loss: 0.0157 | Grad Norm: 0.00868286\n",
      "Epoch 1 | Step 744200 | Avg Loss: 0.0159 | Grad Norm: 0.00841800\n",
      "Epoch 1 | Step 744300 | Avg Loss: 0.0164 | Grad Norm: 0.00889543\n",
      "Epoch 1 | Step 744400 | Avg Loss: 0.0163 | Grad Norm: 0.00922073\n",
      "Epoch 1 | Step 744500 | Avg Loss: 0.0160 | Grad Norm: 0.00806968\n",
      "Epoch 1 | Step 744600 | Avg Loss: 0.0161 | Grad Norm: 0.00809804\n",
      "Epoch 1 | Step 744700 | Avg Loss: 0.0161 | Grad Norm: 0.01084327\n",
      "Epoch 1 | Step 744800 | Avg Loss: 0.0162 | Grad Norm: 0.00852146\n",
      "Epoch 1 | Step 744900 | Avg Loss: 0.0161 | Grad Norm: 0.00826859\n",
      "Epoch 1 | Step 745000 | Avg Loss: 0.0160 | Grad Norm: 0.00831928\n",
      "Epoch 1 | Step 745100 | Avg Loss: 0.0157 | Grad Norm: 0.00775756\n",
      "Epoch 1 | Step 745200 | Avg Loss: 0.0159 | Grad Norm: 0.00934728\n",
      "Epoch 1 | Step 745300 | Avg Loss: 0.0157 | Grad Norm: 0.00903918\n",
      "Epoch 1 | Step 745400 | Avg Loss: 0.0152 | Grad Norm: 0.00862982\n",
      "Epoch 1 | Step 745500 | Avg Loss: 0.0153 | Grad Norm: 0.00886670\n",
      "Epoch 1 | Step 745600 | Avg Loss: 0.0151 | Grad Norm: 0.00782825\n",
      "Epoch 1 | Step 745700 | Avg Loss: 0.0149 | Grad Norm: 0.00828425\n",
      "Epoch 1 | Step 745800 | Avg Loss: 0.0150 | Grad Norm: 0.00940312\n",
      "Epoch 1 | Step 745900 | Avg Loss: 0.0153 | Grad Norm: 0.00883586\n",
      "Epoch 1 | Step 746000 | Avg Loss: 0.0156 | Grad Norm: 0.00868218\n",
      "Epoch 1 | Step 746100 | Avg Loss: 0.0159 | Grad Norm: 0.00938764\n",
      "Epoch 1 | Step 746200 | Avg Loss: 0.0159 | Grad Norm: 0.00811760\n",
      "Epoch 1 | Step 746300 | Avg Loss: 0.0161 | Grad Norm: 0.00839321\n",
      "Epoch 1 | Step 746400 | Avg Loss: 0.0161 | Grad Norm: 0.00852685\n",
      "Epoch 1 | Step 746500 | Avg Loss: 0.0162 | Grad Norm: 0.01183782\n",
      "Epoch 1 | Step 746600 | Avg Loss: 0.0159 | Grad Norm: 0.00870329\n",
      "Epoch 1 | Step 746700 | Avg Loss: 0.0156 | Grad Norm: 0.00861025\n",
      "Epoch 1 | Step 746800 | Avg Loss: 0.0158 | Grad Norm: 0.00905618\n",
      "Epoch 1 | Step 746900 | Avg Loss: 0.0160 | Grad Norm: 0.01065626\n",
      "Epoch 1 | Step 747000 | Avg Loss: 0.0154 | Grad Norm: 0.00812118\n",
      "Epoch 1 | Step 747100 | Avg Loss: 0.0159 | Grad Norm: 0.00862672\n",
      "Epoch 1 | Step 747200 | Avg Loss: 0.0160 | Grad Norm: 0.00851570\n",
      "Epoch 1 | Step 747300 | Avg Loss: 0.0161 | Grad Norm: 0.00936874\n",
      "Epoch 1 | Step 747400 | Avg Loss: 0.0156 | Grad Norm: 0.00902110\n",
      "Epoch 1 | Step 747500 | Avg Loss: 0.0158 | Grad Norm: 0.00960573\n",
      "Epoch 1 | Step 747600 | Avg Loss: 0.0162 | Grad Norm: 0.00932232\n",
      "Epoch 1 | Step 747700 | Avg Loss: 0.0161 | Grad Norm: 0.00803325\n",
      "Epoch 1 | Step 747800 | Avg Loss: 0.0160 | Grad Norm: 0.00800317\n",
      "Epoch 1 | Step 747900 | Avg Loss: 0.0157 | Grad Norm: 0.00851527\n",
      "Epoch 1 | Step 748000 | Avg Loss: 0.0162 | Grad Norm: 0.00766339\n",
      "Epoch 1 | Step 748100 | Avg Loss: 0.0163 | Grad Norm: 0.00897775\n",
      "Epoch 1 | Step 748200 | Avg Loss: 0.0161 | Grad Norm: 0.00785541\n",
      "Epoch 1 | Step 748300 | Avg Loss: 0.0162 | Grad Norm: 0.00785152\n",
      "Epoch 1 | Step 748400 | Avg Loss: 0.0160 | Grad Norm: 0.00955828\n",
      "Epoch 1 | Step 748500 | Avg Loss: 0.0157 | Grad Norm: 0.00930346\n",
      "Epoch 1 | Step 748600 | Avg Loss: 0.0158 | Grad Norm: 0.00865736\n",
      "Epoch 1 | Step 748700 | Avg Loss: 0.0157 | Grad Norm: 0.00854960\n",
      "Epoch 1 | Step 748800 | Avg Loss: 0.0161 | Grad Norm: 0.01030154\n",
      "Epoch 1 | Step 748900 | Avg Loss: 0.0159 | Grad Norm: 0.00888073\n",
      "Epoch 1 | Step 749000 | Avg Loss: 0.0159 | Grad Norm: 0.00887947\n",
      "Epoch 1 | Step 749100 | Avg Loss: 0.0157 | Grad Norm: 0.00976506\n",
      "Epoch 1 | Step 749200 | Avg Loss: 0.0159 | Grad Norm: 0.00822080\n",
      "Epoch 1 | Step 749300 | Avg Loss: 0.0158 | Grad Norm: 0.00895503\n",
      "Epoch 1 | Step 749400 | Avg Loss: 0.0159 | Grad Norm: 0.00827691\n",
      "Epoch 1 | Step 749500 | Avg Loss: 0.0159 | Grad Norm: 0.01128560\n",
      "Epoch 1 | Step 749600 | Avg Loss: 0.0164 | Grad Norm: 0.00981937\n",
      "Epoch 1 | Step 749700 | Avg Loss: 0.0161 | Grad Norm: 0.00779596\n",
      "Epoch 1 | Step 749800 | Avg Loss: 0.0159 | Grad Norm: 0.00809384\n",
      "Epoch 1 | Step 749900 | Avg Loss: 0.0156 | Grad Norm: 0.00836510\n",
      "Epoch 1 | Step 750000 | Avg Loss: 0.0155 | Grad Norm: 0.00953758\n",
      "Epoch 1 | Step 750100 | Avg Loss: 0.0157 | Grad Norm: 0.00935023\n",
      "Epoch 1 | Step 750200 | Avg Loss: 0.0155 | Grad Norm: 0.00878418\n",
      "Epoch 1 | Step 750300 | Avg Loss: 0.0155 | Grad Norm: 0.00906122\n",
      "Epoch 1 | Step 750400 | Avg Loss: 0.0156 | Grad Norm: 0.00918962\n",
      "Epoch 1 | Step 750500 | Avg Loss: 0.0161 | Grad Norm: 0.00858707\n",
      "Epoch 1 | Step 750600 | Avg Loss: 0.0163 | Grad Norm: 0.01046629\n",
      "Epoch 1 | Step 750700 | Avg Loss: 0.0162 | Grad Norm: 0.00968306\n",
      "Epoch 1 | Step 750800 | Avg Loss: 0.0160 | Grad Norm: 0.00928725\n",
      "Epoch 1 | Step 750900 | Avg Loss: 0.0160 | Grad Norm: 0.00831714\n",
      "Epoch 1 | Step 751000 | Avg Loss: 0.0161 | Grad Norm: 0.00813551\n",
      "Epoch 1 | Step 751100 | Avg Loss: 0.0158 | Grad Norm: 0.00843490\n",
      "Epoch 1 | Step 751200 | Avg Loss: 0.0157 | Grad Norm: 0.00794454\n",
      "Epoch 1 | Step 751300 | Avg Loss: 0.0153 | Grad Norm: 0.01132014\n",
      "Epoch 1 | Step 751400 | Avg Loss: 0.0156 | Grad Norm: 0.00873288\n",
      "Epoch 1 | Step 751500 | Avg Loss: 0.0156 | Grad Norm: 0.00760529\n",
      "Epoch 1 | Step 751600 | Avg Loss: 0.0159 | Grad Norm: 0.00815401\n",
      "Epoch 1 | Step 751700 | Avg Loss: 0.0160 | Grad Norm: 0.00924559\n",
      "Epoch 1 | Step 751800 | Avg Loss: 0.0165 | Grad Norm: 0.00894991\n",
      "Epoch 1 | Step 751900 | Avg Loss: 0.0164 | Grad Norm: 0.00913653\n",
      "Epoch 1 | Step 752000 | Avg Loss: 0.0167 | Grad Norm: 0.00764941\n",
      "Epoch 1 | Step 752100 | Avg Loss: 0.0168 | Grad Norm: 0.00940812\n",
      "Epoch 1 | Step 752200 | Avg Loss: 0.0169 | Grad Norm: 0.00937484\n",
      "Epoch 1 | Step 752300 | Avg Loss: 0.0166 | Grad Norm: 0.00851716\n",
      "Epoch 1 | Step 752400 | Avg Loss: 0.0164 | Grad Norm: 0.00848069\n",
      "Epoch 1 | Step 752500 | Avg Loss: 0.0166 | Grad Norm: 0.00918260\n",
      "Epoch 1 | Step 752600 | Avg Loss: 0.0166 | Grad Norm: 0.00895247\n",
      "Epoch 1 | Step 752700 | Avg Loss: 0.0164 | Grad Norm: 0.00826218\n",
      "Epoch 1 | Step 752800 | Avg Loss: 0.0163 | Grad Norm: 0.01244623\n",
      "Epoch 1 | Step 752900 | Avg Loss: 0.0159 | Grad Norm: 0.00910573\n",
      "Epoch 1 | Step 753000 | Avg Loss: 0.0159 | Grad Norm: 0.00843476\n",
      "Epoch 1 | Step 753100 | Avg Loss: 0.0159 | Grad Norm: 0.00888259\n",
      "Epoch 1 | Step 753200 | Avg Loss: 0.0161 | Grad Norm: 0.00832610\n",
      "Epoch 1 | Step 753300 | Avg Loss: 0.0161 | Grad Norm: 0.00834981\n",
      "Epoch 1 | Step 753400 | Avg Loss: 0.0159 | Grad Norm: 0.00948403\n",
      "Epoch 1 | Step 753500 | Avg Loss: 0.0162 | Grad Norm: 0.00852261\n",
      "Epoch 1 | Step 753600 | Avg Loss: 0.0159 | Grad Norm: 0.00809130\n",
      "Epoch 1 | Step 753700 | Avg Loss: 0.0159 | Grad Norm: 0.01094036\n",
      "Epoch 1 | Step 753800 | Avg Loss: 0.0157 | Grad Norm: 0.00878852\n",
      "Epoch 1 | Step 753900 | Avg Loss: 0.0155 | Grad Norm: 0.00861533\n",
      "Epoch 1 | Step 754000 | Avg Loss: 0.0157 | Grad Norm: 0.01009795\n",
      "Epoch 1 | Step 754100 | Avg Loss: 0.0157 | Grad Norm: 0.00875030\n",
      "Epoch 1 | Step 754200 | Avg Loss: 0.0158 | Grad Norm: 0.00785049\n",
      "Epoch 1 | Step 754300 | Avg Loss: 0.0161 | Grad Norm: 0.00872508\n",
      "Epoch 1 | Step 754400 | Avg Loss: 0.0162 | Grad Norm: 0.00801972\n",
      "Epoch 1 | Step 754500 | Avg Loss: 0.0162 | Grad Norm: 0.00817246\n",
      "Epoch 1 | Step 754600 | Avg Loss: 0.0163 | Grad Norm: 0.01174902\n",
      "Epoch 1 | Step 754700 | Avg Loss: 0.0161 | Grad Norm: 0.00900634\n",
      "Epoch 1 | Step 754800 | Avg Loss: 0.0158 | Grad Norm: 0.00862929\n",
      "Epoch 1 | Step 754900 | Avg Loss: 0.0160 | Grad Norm: 0.00752821\n",
      "Epoch 1 | Step 755000 | Avg Loss: 0.0162 | Grad Norm: 0.00864895\n",
      "Epoch 1 | Step 755100 | Avg Loss: 0.0160 | Grad Norm: 0.00816139\n",
      "Epoch 1 | Step 755200 | Avg Loss: 0.0160 | Grad Norm: 0.00824795\n",
      "Epoch 1 | Step 755300 | Avg Loss: 0.0157 | Grad Norm: 0.00878606\n",
      "Epoch 1 | Step 755400 | Avg Loss: 0.0161 | Grad Norm: 0.00821909\n",
      "Epoch 1 | Step 755500 | Avg Loss: 0.0154 | Grad Norm: 0.00865997\n",
      "Epoch 1 | Step 755600 | Avg Loss: 0.0152 | Grad Norm: 0.00885065\n",
      "Epoch 1 | Step 755700 | Avg Loss: 0.0159 | Grad Norm: 0.00986627\n",
      "Epoch 1 | Step 755800 | Avg Loss: 0.0160 | Grad Norm: 0.00883360\n",
      "Epoch 1 | Step 755900 | Avg Loss: 0.0161 | Grad Norm: 0.00897944\n",
      "Epoch 1 | Step 756000 | Avg Loss: 0.0156 | Grad Norm: 0.00763452\n",
      "Epoch 1 | Step 756100 | Avg Loss: 0.0159 | Grad Norm: 0.01022384\n",
      "Epoch 1 | Step 756200 | Avg Loss: 0.0161 | Grad Norm: 0.00930150\n",
      "Epoch 1 | Step 756300 | Avg Loss: 0.0162 | Grad Norm: 0.00909717\n",
      "Epoch 1 | Step 756400 | Avg Loss: 0.0161 | Grad Norm: 0.00954988\n",
      "Epoch 1 | Step 756500 | Avg Loss: 0.0160 | Grad Norm: 0.00925963\n",
      "Epoch 1 | Step 756600 | Avg Loss: 0.0161 | Grad Norm: 0.00776492\n",
      "Epoch 1 | Step 756700 | Avg Loss: 0.0162 | Grad Norm: 0.00903938\n",
      "Epoch 1 | Step 756800 | Avg Loss: 0.0162 | Grad Norm: 0.00847254\n",
      "Epoch 1 | Step 756900 | Avg Loss: 0.0158 | Grad Norm: 0.01012604\n",
      "Epoch 1 | Step 757000 | Avg Loss: 0.0163 | Grad Norm: 0.01044650\n",
      "Epoch 1 | Step 757100 | Avg Loss: 0.0159 | Grad Norm: 0.00968539\n",
      "Epoch 1 | Step 757200 | Avg Loss: 0.0158 | Grad Norm: 0.00789009\n",
      "Epoch 1 | Step 757300 | Avg Loss: 0.0156 | Grad Norm: 0.01060946\n",
      "Epoch 1 | Step 757400 | Avg Loss: 0.0155 | Grad Norm: 0.00838932\n",
      "Epoch 1 | Step 757500 | Avg Loss: 0.0155 | Grad Norm: 0.00768390\n",
      "Epoch 1 | Step 757600 | Avg Loss: 0.0156 | Grad Norm: 0.00897157\n",
      "Epoch 1 | Step 757700 | Avg Loss: 0.0156 | Grad Norm: 0.01128223\n",
      "Epoch 1 | Step 757800 | Avg Loss: 0.0155 | Grad Norm: 0.00793460\n",
      "Epoch 1 | Step 757900 | Avg Loss: 0.0154 | Grad Norm: 0.00798500\n",
      "Epoch 1 | Step 758000 | Avg Loss: 0.0156 | Grad Norm: 0.01058658\n",
      "Epoch 1 | Step 758100 | Avg Loss: 0.0155 | Grad Norm: 0.00751609\n",
      "Epoch 1 | Step 758200 | Avg Loss: 0.0153 | Grad Norm: 0.00869020\n",
      "Epoch 1 | Step 758300 | Avg Loss: 0.0153 | Grad Norm: 0.00972440\n",
      "Epoch 1 | Step 758400 | Avg Loss: 0.0157 | Grad Norm: 0.00946118\n",
      "Epoch 1 | Step 758500 | Avg Loss: 0.0160 | Grad Norm: 0.00990286\n",
      "Epoch 1 | Step 758600 | Avg Loss: 0.0158 | Grad Norm: 0.00855874\n",
      "Epoch 1 | Step 758700 | Avg Loss: 0.0158 | Grad Norm: 0.01063333\n",
      "Epoch 1 | Step 758800 | Avg Loss: 0.0163 | Grad Norm: 0.00882359\n",
      "Epoch 1 | Step 758900 | Avg Loss: 0.0159 | Grad Norm: 0.00955474\n",
      "Epoch 1 | Step 759000 | Avg Loss: 0.0159 | Grad Norm: 0.00817078\n",
      "Epoch 1 | Step 759100 | Avg Loss: 0.0160 | Grad Norm: 0.00928743\n",
      "Epoch 1 | Step 759200 | Avg Loss: 0.0157 | Grad Norm: 0.00811398\n",
      "Epoch 1 | Step 759300 | Avg Loss: 0.0157 | Grad Norm: 0.00785672\n",
      "Epoch 1 | Step 759400 | Avg Loss: 0.0159 | Grad Norm: 0.00900217\n",
      "Epoch 1 | Step 759500 | Avg Loss: 0.0162 | Grad Norm: 0.00839836\n",
      "Epoch 1 | Step 759600 | Avg Loss: 0.0158 | Grad Norm: 0.00797524\n",
      "Epoch 1 | Step 759700 | Avg Loss: 0.0157 | Grad Norm: 0.00848170\n",
      "Epoch 1 | Step 759800 | Avg Loss: 0.0153 | Grad Norm: 0.00911338\n",
      "Epoch 1 | Step 759900 | Avg Loss: 0.0154 | Grad Norm: 0.00758677\n",
      "Epoch 1 | Step 760000 | Avg Loss: 0.0155 | Grad Norm: 0.00715753\n",
      "Epoch 1 | Step 760100 | Avg Loss: 0.0153 | Grad Norm: 0.00781869\n",
      "Epoch 1 | Step 760200 | Avg Loss: 0.0155 | Grad Norm: 0.00955801\n",
      "Epoch 1 | Step 760300 | Avg Loss: 0.0152 | Grad Norm: 0.00809627\n",
      "Epoch 1 | Step 760400 | Avg Loss: 0.0155 | Grad Norm: 0.00857172\n",
      "Epoch 1 | Step 760500 | Avg Loss: 0.0158 | Grad Norm: 0.00893978\n",
      "Epoch 1 | Step 760600 | Avg Loss: 0.0159 | Grad Norm: 0.00900078\n",
      "Epoch 1 | Step 760700 | Avg Loss: 0.0160 | Grad Norm: 0.01613187\n",
      "Epoch 1 | Step 760800 | Avg Loss: 0.0158 | Grad Norm: 0.00869685\n",
      "Epoch 1 | Step 760900 | Avg Loss: 0.0157 | Grad Norm: 0.00809648\n",
      "Epoch 1 | Step 761000 | Avg Loss: 0.0154 | Grad Norm: 0.01007141\n",
      "Epoch 1 | Step 761100 | Avg Loss: 0.0155 | Grad Norm: 0.00825651\n",
      "Epoch 1 | Step 761200 | Avg Loss: 0.0160 | Grad Norm: 0.00890739\n",
      "Epoch 1 | Step 761300 | Avg Loss: 0.0159 | Grad Norm: 0.01144637\n",
      "Epoch 1 | Step 761400 | Avg Loss: 0.0164 | Grad Norm: 0.00865889\n",
      "Epoch 1 | Step 761500 | Avg Loss: 0.0162 | Grad Norm: 0.00891057\n",
      "Epoch 1 | Step 761600 | Avg Loss: 0.0160 | Grad Norm: 0.00941132\n",
      "Epoch 1 | Step 761700 | Avg Loss: 0.0162 | Grad Norm: 0.00859237\n",
      "Epoch 1 | Step 761800 | Avg Loss: 0.0159 | Grad Norm: 0.00857991\n",
      "Epoch 1 | Step 761900 | Avg Loss: 0.0161 | Grad Norm: 0.00896906\n",
      "Epoch 1 | Step 762000 | Avg Loss: 0.0160 | Grad Norm: 0.01022032\n",
      "Epoch 1 | Step 762100 | Avg Loss: 0.0160 | Grad Norm: 0.01029763\n",
      "Epoch 1 | Step 762200 | Avg Loss: 0.0159 | Grad Norm: 0.00962287\n",
      "Epoch 1 | Step 762300 | Avg Loss: 0.0158 | Grad Norm: 0.00788613\n",
      "Epoch 1 | Step 762400 | Avg Loss: 0.0158 | Grad Norm: 0.01064029\n",
      "Epoch 1 | Step 762500 | Avg Loss: 0.0160 | Grad Norm: 0.01043381\n",
      "Epoch 1 | Step 762600 | Avg Loss: 0.0160 | Grad Norm: 0.00804625\n",
      "Epoch 1 | Step 762700 | Avg Loss: 0.0162 | Grad Norm: 0.00830527\n",
      "Epoch 1 | Step 762800 | Avg Loss: 0.0164 | Grad Norm: 0.00922771\n",
      "Epoch 1 | Step 762900 | Avg Loss: 0.0163 | Grad Norm: 0.00841800\n",
      "Epoch 1 | Step 763000 | Avg Loss: 0.0161 | Grad Norm: 0.00961837\n",
      "Epoch 1 | Step 763100 | Avg Loss: 0.0161 | Grad Norm: 0.00980709\n",
      "Epoch 1 | Step 763200 | Avg Loss: 0.0163 | Grad Norm: 0.00985013\n",
      "Epoch 1 | Step 763300 | Avg Loss: 0.0164 | Grad Norm: 0.01084366\n",
      "Epoch 1 | Step 763400 | Avg Loss: 0.0162 | Grad Norm: 0.00925215\n",
      "Epoch 1 | Step 763500 | Avg Loss: 0.0161 | Grad Norm: 0.00949252\n",
      "Epoch 1 | Step 763600 | Avg Loss: 0.0161 | Grad Norm: 0.00914159\n",
      "Epoch 1 | Step 763700 | Avg Loss: 0.0157 | Grad Norm: 0.00857636\n",
      "Epoch 1 | Step 763800 | Avg Loss: 0.0157 | Grad Norm: 0.00882753\n",
      "Epoch 1 | Step 763900 | Avg Loss: 0.0156 | Grad Norm: 0.00864694\n",
      "Epoch 1 | Step 764000 | Avg Loss: 0.0154 | Grad Norm: 0.00993196\n",
      "Epoch 1 | Step 764100 | Avg Loss: 0.0150 | Grad Norm: 0.00831007\n",
      "Epoch 1 | Step 764200 | Avg Loss: 0.0150 | Grad Norm: 0.00818989\n",
      "Epoch 1 | Step 764300 | Avg Loss: 0.0153 | Grad Norm: 0.00785728\n",
      "Epoch 1 | Step 764400 | Avg Loss: 0.0152 | Grad Norm: 0.00815148\n",
      "Epoch 1 | Step 764500 | Avg Loss: 0.0154 | Grad Norm: 0.01030738\n",
      "Epoch 1 | Step 764600 | Avg Loss: 0.0151 | Grad Norm: 0.01103289\n",
      "Epoch 1 | Step 764700 | Avg Loss: 0.0156 | Grad Norm: 0.01002970\n",
      "Epoch 1 | Step 764800 | Avg Loss: 0.0156 | Grad Norm: 0.01368360\n",
      "Epoch 1 | Step 764900 | Avg Loss: 0.0152 | Grad Norm: 0.00847217\n",
      "Epoch 1 | Step 765000 | Avg Loss: 0.0155 | Grad Norm: 0.00826638\n",
      "Epoch 1 | Step 765100 | Avg Loss: 0.0157 | Grad Norm: 0.00863307\n",
      "Epoch 1 | Step 765200 | Avg Loss: 0.0156 | Grad Norm: 0.00900425\n",
      "Epoch 1 | Step 765300 | Avg Loss: 0.0156 | Grad Norm: 0.01024072\n",
      "Epoch 1 | Step 765400 | Avg Loss: 0.0156 | Grad Norm: 0.00962699\n",
      "Epoch 1 | Step 765500 | Avg Loss: 0.0159 | Grad Norm: 0.00789676\n",
      "Epoch 1 | Step 765600 | Avg Loss: 0.0158 | Grad Norm: 0.00892823\n",
      "Epoch 1 | Step 765700 | Avg Loss: 0.0154 | Grad Norm: 0.01089952\n",
      "Epoch 1 | Step 765800 | Avg Loss: 0.0153 | Grad Norm: 0.00944520\n",
      "Epoch 1 | Step 765900 | Avg Loss: 0.0152 | Grad Norm: 0.00765572\n",
      "Epoch 1 | Step 766000 | Avg Loss: 0.0152 | Grad Norm: 0.00881187\n",
      "Epoch 1 | Step 766100 | Avg Loss: 0.0154 | Grad Norm: 0.00740676\n",
      "Epoch 1 | Step 766200 | Avg Loss: 0.0159 | Grad Norm: 0.00847024\n",
      "Epoch 1 | Step 766300 | Avg Loss: 0.0162 | Grad Norm: 0.00924528\n",
      "Epoch 1 | Step 766400 | Avg Loss: 0.0161 | Grad Norm: 0.00850062\n",
      "Epoch 1 | Step 766500 | Avg Loss: 0.0160 | Grad Norm: 0.00832074\n",
      "Epoch 1 | Step 766600 | Avg Loss: 0.0161 | Grad Norm: 0.00763134\n",
      "Epoch 1 | Step 766700 | Avg Loss: 0.0156 | Grad Norm: 0.00824830\n",
      "Epoch 1 | Step 766800 | Avg Loss: 0.0156 | Grad Norm: 0.00864340\n",
      "Epoch 1 | Step 766900 | Avg Loss: 0.0158 | Grad Norm: 0.00899393\n",
      "Epoch 1 | Step 767000 | Avg Loss: 0.0157 | Grad Norm: 0.00964070\n",
      "Epoch 1 | Step 767100 | Avg Loss: 0.0156 | Grad Norm: 0.00951769\n",
      "Epoch 1 | Step 767200 | Avg Loss: 0.0156 | Grad Norm: 0.00819388\n",
      "Epoch 1 | Step 767300 | Avg Loss: 0.0156 | Grad Norm: 0.00801221\n",
      "Epoch 1 | Step 767400 | Avg Loss: 0.0158 | Grad Norm: 0.00818294\n",
      "Epoch 1 | Step 767500 | Avg Loss: 0.0155 | Grad Norm: 0.00752895\n",
      "Epoch 1 | Step 767600 | Avg Loss: 0.0158 | Grad Norm: 0.00942590\n",
      "Epoch 1 | Step 767700 | Avg Loss: 0.0160 | Grad Norm: 0.00796205\n",
      "Epoch 1 | Step 767800 | Avg Loss: 0.0155 | Grad Norm: 0.00894693\n",
      "Epoch 1 | Step 767900 | Avg Loss: 0.0152 | Grad Norm: 0.00963345\n",
      "Epoch 1 | Step 768000 | Avg Loss: 0.0152 | Grad Norm: 0.00855761\n",
      "Epoch 1 | Step 768100 | Avg Loss: 0.0155 | Grad Norm: 0.00764006\n",
      "Epoch 1 | Step 768200 | Avg Loss: 0.0155 | Grad Norm: 0.00824600\n",
      "Epoch 1 | Step 768300 | Avg Loss: 0.0158 | Grad Norm: 0.00805122\n",
      "Epoch 1 | Step 768400 | Avg Loss: 0.0159 | Grad Norm: 0.00855859\n",
      "Epoch 1 | Step 768500 | Avg Loss: 0.0160 | Grad Norm: 0.01017934\n",
      "Epoch 1 | Step 768600 | Avg Loss: 0.0164 | Grad Norm: 0.00878939\n",
      "Epoch 1 | Step 768700 | Avg Loss: 0.0160 | Grad Norm: 0.00919053\n",
      "Epoch 1 | Step 768800 | Avg Loss: 0.0164 | Grad Norm: 0.00888728\n",
      "Epoch 1 | Step 768900 | Avg Loss: 0.0160 | Grad Norm: 0.00979715\n",
      "Epoch 1 | Step 769000 | Avg Loss: 0.0160 | Grad Norm: 0.00968096\n",
      "Epoch 1 | Step 769100 | Avg Loss: 0.0159 | Grad Norm: 0.00899959\n",
      "Epoch 1 | Step 769200 | Avg Loss: 0.0157 | Grad Norm: 0.00795373\n",
      "Epoch 1 | Step 769300 | Avg Loss: 0.0155 | Grad Norm: 0.00852333\n",
      "Epoch 1 | Step 769400 | Avg Loss: 0.0157 | Grad Norm: 0.01088747\n",
      "Epoch 1 | Step 769500 | Avg Loss: 0.0158 | Grad Norm: 0.00982166\n",
      "Epoch 1 | Step 769600 | Avg Loss: 0.0154 | Grad Norm: 0.00798156\n",
      "Epoch 1 | Step 769700 | Avg Loss: 0.0159 | Grad Norm: 0.00971483\n",
      "Epoch 1 | Step 769800 | Avg Loss: 0.0157 | Grad Norm: 0.00862688\n",
      "Epoch 1 | Step 769900 | Avg Loss: 0.0158 | Grad Norm: 0.00858799\n",
      "Epoch 1 | Step 770000 | Avg Loss: 0.0159 | Grad Norm: 0.00900696\n",
      "Epoch 1 | Step 770100 | Avg Loss: 0.0158 | Grad Norm: 0.00954914\n",
      "Epoch 1 | Step 770200 | Avg Loss: 0.0158 | Grad Norm: 0.00880357\n",
      "Epoch 1 | Step 770300 | Avg Loss: 0.0161 | Grad Norm: 0.00796359\n",
      "Epoch 1 | Step 770400 | Avg Loss: 0.0159 | Grad Norm: 0.00908221\n",
      "Epoch 1 | Step 770500 | Avg Loss: 0.0161 | Grad Norm: 0.00967349\n",
      "Epoch 1 | Step 770600 | Avg Loss: 0.0158 | Grad Norm: 0.00833326\n",
      "Epoch 1 | Step 770700 | Avg Loss: 0.0160 | Grad Norm: 0.00892931\n",
      "Epoch 1 | Step 770800 | Avg Loss: 0.0158 | Grad Norm: 0.00893139\n",
      "Epoch 1 | Step 770900 | Avg Loss: 0.0157 | Grad Norm: 0.00840800\n",
      "Epoch 1 | Step 771000 | Avg Loss: 0.0156 | Grad Norm: 0.00853923\n",
      "Epoch 1 | Step 771100 | Avg Loss: 0.0156 | Grad Norm: 0.00943629\n",
      "Epoch 1 | Step 771200 | Avg Loss: 0.0154 | Grad Norm: 0.00831290\n",
      "Epoch 1 | Step 771300 | Avg Loss: 0.0156 | Grad Norm: 0.00886721\n",
      "Epoch 1 | Step 771400 | Avg Loss: 0.0158 | Grad Norm: 0.00858389\n",
      "Epoch 1 | Step 771500 | Avg Loss: 0.0158 | Grad Norm: 0.00837133\n",
      "Epoch 1 | Step 771600 | Avg Loss: 0.0156 | Grad Norm: 0.00891018\n",
      "Epoch 1 | Step 771700 | Avg Loss: 0.0152 | Grad Norm: 0.00782353\n",
      "Epoch 1 | Step 771800 | Avg Loss: 0.0152 | Grad Norm: 0.00755926\n",
      "Epoch 1 | Step 771900 | Avg Loss: 0.0153 | Grad Norm: 0.00773814\n",
      "Epoch 1 | Step 772000 | Avg Loss: 0.0148 | Grad Norm: 0.01002251\n",
      "Epoch 1 | Step 772100 | Avg Loss: 0.0154 | Grad Norm: 0.01081324\n",
      "Epoch 1 | Step 772200 | Avg Loss: 0.0155 | Grad Norm: 0.00814904\n",
      "Epoch 1 | Step 772300 | Avg Loss: 0.0157 | Grad Norm: 0.01004962\n",
      "Epoch 1 | Step 772400 | Avg Loss: 0.0155 | Grad Norm: 0.00791413\n",
      "Epoch 1 | Step 772500 | Avg Loss: 0.0156 | Grad Norm: 0.00872249\n",
      "Epoch 1 | Step 772600 | Avg Loss: 0.0157 | Grad Norm: 0.00865214\n",
      "Epoch 1 | Step 772700 | Avg Loss: 0.0155 | Grad Norm: 0.00859748\n",
      "Epoch 1 | Step 772800 | Avg Loss: 0.0156 | Grad Norm: 0.00826579\n",
      "Epoch 1 | Step 772900 | Avg Loss: 0.0159 | Grad Norm: 0.00755347\n",
      "Epoch 1 | Step 773000 | Avg Loss: 0.0160 | Grad Norm: 0.00926541\n",
      "Epoch 1 | Step 773100 | Avg Loss: 0.0160 | Grad Norm: 0.00799789\n",
      "Epoch 1 | Step 773200 | Avg Loss: 0.0158 | Grad Norm: 0.00876146\n",
      "Epoch 1 | Step 773300 | Avg Loss: 0.0158 | Grad Norm: 0.00883783\n",
      "Epoch 1 | Step 773400 | Avg Loss: 0.0159 | Grad Norm: 0.00897854\n",
      "Epoch 1 | Step 773500 | Avg Loss: 0.0161 | Grad Norm: 0.00755408\n",
      "Epoch 1 | Step 773600 | Avg Loss: 0.0160 | Grad Norm: 0.00923441\n",
      "Epoch 1 | Step 773700 | Avg Loss: 0.0162 | Grad Norm: 0.00886552\n",
      "Epoch 1 | Step 773800 | Avg Loss: 0.0161 | Grad Norm: 0.00844828\n",
      "Epoch 1 | Step 773900 | Avg Loss: 0.0158 | Grad Norm: 0.00892266\n",
      "Epoch 1 | Step 774000 | Avg Loss: 0.0157 | Grad Norm: 0.00806111\n",
      "Epoch 1 | Step 774100 | Avg Loss: 0.0159 | Grad Norm: 0.00825867\n",
      "Epoch 1 | Step 774200 | Avg Loss: 0.0160 | Grad Norm: 0.00995007\n",
      "Epoch 1 | Step 774300 | Avg Loss: 0.0159 | Grad Norm: 0.00924746\n",
      "Epoch 1 | Step 774400 | Avg Loss: 0.0158 | Grad Norm: 0.00916801\n",
      "Epoch 1 | Step 774500 | Avg Loss: 0.0164 | Grad Norm: 0.00894094\n",
      "Epoch 1 | Step 774600 | Avg Loss: 0.0155 | Grad Norm: 0.00959193\n",
      "Epoch 1 | Step 774700 | Avg Loss: 0.0157 | Grad Norm: 0.00905167\n",
      "Epoch 1 | Step 774800 | Avg Loss: 0.0160 | Grad Norm: 0.00831857\n",
      "Epoch 1 | Step 774900 | Avg Loss: 0.0160 | Grad Norm: 0.00963049\n",
      "Epoch 1 | Step 775000 | Avg Loss: 0.0161 | Grad Norm: 0.00872723\n",
      "Epoch 1 | Step 775100 | Avg Loss: 0.0161 | Grad Norm: 0.00960214\n",
      "Epoch 1 | Step 775200 | Avg Loss: 0.0159 | Grad Norm: 0.00819627\n",
      "Epoch 1 | Step 775300 | Avg Loss: 0.0157 | Grad Norm: 0.00908241\n",
      "Epoch 1 | Step 775400 | Avg Loss: 0.0158 | Grad Norm: 0.00927012\n",
      "Epoch 1 | Step 775500 | Avg Loss: 0.0162 | Grad Norm: 0.00933426\n",
      "Epoch 1 | Step 775600 | Avg Loss: 0.0162 | Grad Norm: 0.00836293\n",
      "Epoch 1 | Step 775700 | Avg Loss: 0.0163 | Grad Norm: 0.00865604\n",
      "Epoch 1 | Step 775800 | Avg Loss: 0.0159 | Grad Norm: 0.00886095\n",
      "Epoch 1 | Step 775900 | Avg Loss: 0.0159 | Grad Norm: 0.00862411\n",
      "Epoch 1 | Step 776000 | Avg Loss: 0.0158 | Grad Norm: 0.00976788\n",
      "Epoch 1 | Step 776100 | Avg Loss: 0.0159 | Grad Norm: 0.01040607\n",
      "Epoch 1 | Step 776200 | Avg Loss: 0.0157 | Grad Norm: 0.00962377\n",
      "Epoch 1 | Step 776300 | Avg Loss: 0.0153 | Grad Norm: 0.00913438\n",
      "Epoch 1 | Step 776400 | Avg Loss: 0.0158 | Grad Norm: 0.00888959\n",
      "Epoch 1 | Step 776500 | Avg Loss: 0.0162 | Grad Norm: 0.00849208\n",
      "Epoch 1 | Step 776600 | Avg Loss: 0.0159 | Grad Norm: 0.00914940\n",
      "Epoch 1 | Step 776700 | Avg Loss: 0.0157 | Grad Norm: 0.00830352\n",
      "Epoch 1 | Step 776800 | Avg Loss: 0.0157 | Grad Norm: 0.01035546\n",
      "Epoch 1 | Step 776900 | Avg Loss: 0.0159 | Grad Norm: 0.00866980\n",
      "Epoch 1 | Step 777000 | Avg Loss: 0.0156 | Grad Norm: 0.00824413\n",
      "Epoch 1 | Step 777100 | Avg Loss: 0.0157 | Grad Norm: 0.01208008\n",
      "Epoch 1 | Step 777200 | Avg Loss: 0.0156 | Grad Norm: 0.00860706\n",
      "Epoch 1 | Step 777300 | Avg Loss: 0.0159 | Grad Norm: 0.00912764\n",
      "Epoch 1 | Step 777400 | Avg Loss: 0.0157 | Grad Norm: 0.01077266\n",
      "Epoch 1 | Step 777500 | Avg Loss: 0.0158 | Grad Norm: 0.00919368\n",
      "Epoch 1 | Step 777600 | Avg Loss: 0.0161 | Grad Norm: 0.00827600\n",
      "Epoch 1 | Step 777700 | Avg Loss: 0.0157 | Grad Norm: 0.01385255\n",
      "Epoch 1 | Step 777800 | Avg Loss: 0.0155 | Grad Norm: 0.00855634\n",
      "Epoch 1 | Step 777900 | Avg Loss: 0.0152 | Grad Norm: 0.00879424\n",
      "Epoch 1 | Step 778000 | Avg Loss: 0.0154 | Grad Norm: 0.00871981\n",
      "Epoch 1 | Step 778100 | Avg Loss: 0.0153 | Grad Norm: 0.00867474\n",
      "Epoch 1 | Step 778200 | Avg Loss: 0.0154 | Grad Norm: 0.00830995\n",
      "Epoch 1 | Step 778300 | Avg Loss: 0.0154 | Grad Norm: 0.01004247\n",
      "Epoch 1 | Step 778400 | Avg Loss: 0.0155 | Grad Norm: 0.00817750\n",
      "Epoch 1 | Step 778500 | Avg Loss: 0.0154 | Grad Norm: 0.00947466\n",
      "Epoch 1 | Step 778600 | Avg Loss: 0.0154 | Grad Norm: 0.00916061\n",
      "Epoch 1 | Step 778700 | Avg Loss: 0.0153 | Grad Norm: 0.00862445\n",
      "Epoch 1 | Step 778800 | Avg Loss: 0.0154 | Grad Norm: 0.00791666\n",
      "Epoch 1 | Step 778900 | Avg Loss: 0.0154 | Grad Norm: 0.00741486\n",
      "Epoch 1 | Step 779000 | Avg Loss: 0.0153 | Grad Norm: 0.00771074\n",
      "Epoch 1 | Step 779100 | Avg Loss: 0.0157 | Grad Norm: 0.00852294\n",
      "Epoch 1 | Step 779200 | Avg Loss: 0.0157 | Grad Norm: 0.00864130\n",
      "Epoch 1 | Step 779300 | Avg Loss: 0.0155 | Grad Norm: 0.00854963\n",
      "Epoch 1 | Step 779400 | Avg Loss: 0.0157 | Grad Norm: 0.00880704\n",
      "Epoch 1 | Step 779500 | Avg Loss: 0.0159 | Grad Norm: 0.00916848\n",
      "Epoch 1 | Step 779600 | Avg Loss: 0.0157 | Grad Norm: 0.00917359\n",
      "Epoch 1 | Step 779700 | Avg Loss: 0.0159 | Grad Norm: 0.00881750\n",
      "Epoch 1 | Step 779800 | Avg Loss: 0.0156 | Grad Norm: 0.00791311\n",
      "Epoch 1 | Step 779900 | Avg Loss: 0.0155 | Grad Norm: 0.00984489\n",
      "Epoch 1 | Step 780000 | Avg Loss: 0.0154 | Grad Norm: 0.00916698\n",
      "Epoch 1 | Step 780100 | Avg Loss: 0.0154 | Grad Norm: 0.00802962\n",
      "Epoch 1 | Step 780200 | Avg Loss: 0.0152 | Grad Norm: 0.00930506\n",
      "Epoch 1 | Step 780300 | Avg Loss: 0.0156 | Grad Norm: 0.00956577\n",
      "Epoch 1 | Step 780400 | Avg Loss: 0.0155 | Grad Norm: 0.00938722\n",
      "Epoch 1 | Step 780500 | Avg Loss: 0.0157 | Grad Norm: 0.00802743\n",
      "Epoch 1 | Step 780600 | Avg Loss: 0.0157 | Grad Norm: 0.00951025\n",
      "Epoch 1 | Step 780700 | Avg Loss: 0.0152 | Grad Norm: 0.00857523\n",
      "Epoch 1 | Step 780800 | Avg Loss: 0.0149 | Grad Norm: 0.00760171\n",
      "Epoch 1 | Step 780900 | Avg Loss: 0.0151 | Grad Norm: 0.00874784\n",
      "Epoch 1 | Step 781000 | Avg Loss: 0.0154 | Grad Norm: 0.00946508\n",
      "Epoch 1 | Step 781100 | Avg Loss: 0.0149 | Grad Norm: 0.00774723\n",
      "Epoch 1 | Step 781200 | Avg Loss: 0.0149 | Grad Norm: 0.00902113\n",
      "Epoch 1, Loss: 0.0140\n",
      "Epoch 2 | Step 781300 | Avg Loss: 0.0152 | Grad Norm: 0.00849437\n",
      "Epoch 2 | Step 781400 | Avg Loss: 0.0156 | Grad Norm: 0.00807564\n",
      "Epoch 2 | Step 781500 | Avg Loss: 0.0154 | Grad Norm: 0.00811426\n",
      "Epoch 2 | Step 781600 | Avg Loss: 0.0153 | Grad Norm: 0.00824816\n",
      "Epoch 2 | Step 781700 | Avg Loss: 0.0154 | Grad Norm: 0.01025320\n",
      "Epoch 2 | Step 781800 | Avg Loss: 0.0153 | Grad Norm: 0.01082006\n",
      "Epoch 2 | Step 781900 | Avg Loss: 0.0158 | Grad Norm: 0.00900630\n",
      "Epoch 2 | Step 782000 | Avg Loss: 0.0161 | Grad Norm: 0.00984095\n",
      "Epoch 2 | Step 782100 | Avg Loss: 0.0165 | Grad Norm: 0.00827929\n",
      "Epoch 2 | Step 782200 | Avg Loss: 0.0164 | Grad Norm: 0.00852384\n",
      "Epoch 2 | Step 782300 | Avg Loss: 0.0160 | Grad Norm: 0.00899045\n",
      "Epoch 2 | Step 782400 | Avg Loss: 0.0157 | Grad Norm: 0.00900366\n",
      "Epoch 2 | Step 782500 | Avg Loss: 0.0157 | Grad Norm: 0.00873057\n",
      "Epoch 2 | Step 782600 | Avg Loss: 0.0156 | Grad Norm: 0.00949745\n",
      "Epoch 2 | Step 782700 | Avg Loss: 0.0152 | Grad Norm: 0.00899764\n",
      "Epoch 2 | Step 782800 | Avg Loss: 0.0154 | Grad Norm: 0.00972880\n",
      "Epoch 2 | Step 782900 | Avg Loss: 0.0150 | Grad Norm: 0.00834220\n",
      "Epoch 2 | Step 783000 | Avg Loss: 0.0151 | Grad Norm: 0.01146037\n",
      "Epoch 2 | Step 783100 | Avg Loss: 0.0152 | Grad Norm: 0.00848564\n",
      "Epoch 2 | Step 783200 | Avg Loss: 0.0158 | Grad Norm: 0.00784467\n",
      "Epoch 2 | Step 783300 | Avg Loss: 0.0155 | Grad Norm: 0.00967025\n",
      "Epoch 2 | Step 783400 | Avg Loss: 0.0156 | Grad Norm: 0.00895882\n",
      "Epoch 2 | Step 783500 | Avg Loss: 0.0155 | Grad Norm: 0.00812115\n",
      "Epoch 2 | Step 783600 | Avg Loss: 0.0157 | Grad Norm: 0.00809215\n",
      "Epoch 2 | Step 783700 | Avg Loss: 0.0157 | Grad Norm: 0.00834625\n",
      "Epoch 2 | Step 783800 | Avg Loss: 0.0159 | Grad Norm: 0.01080701\n",
      "Epoch 2 | Step 783900 | Avg Loss: 0.0161 | Grad Norm: 0.00868613\n",
      "Epoch 2 | Step 784000 | Avg Loss: 0.0159 | Grad Norm: 0.00955802\n",
      "Epoch 2 | Step 784100 | Avg Loss: 0.0159 | Grad Norm: 0.00876339\n",
      "Epoch 2 | Step 784200 | Avg Loss: 0.0157 | Grad Norm: 0.00993926\n",
      "Epoch 2 | Step 784300 | Avg Loss: 0.0159 | Grad Norm: 0.00990844\n",
      "Epoch 2 | Step 784400 | Avg Loss: 0.0155 | Grad Norm: 0.00944963\n",
      "Epoch 2 | Step 784500 | Avg Loss: 0.0154 | Grad Norm: 0.00936245\n",
      "Epoch 2 | Step 784600 | Avg Loss: 0.0153 | Grad Norm: 0.00819826\n",
      "Epoch 2 | Step 784700 | Avg Loss: 0.0153 | Grad Norm: 0.00884205\n",
      "Epoch 2 | Step 784800 | Avg Loss: 0.0151 | Grad Norm: 0.00921478\n",
      "Epoch 2 | Step 784900 | Avg Loss: 0.0154 | Grad Norm: 0.00950509\n",
      "Epoch 2 | Step 785000 | Avg Loss: 0.0159 | Grad Norm: 0.00866883\n",
      "Epoch 2 | Step 785100 | Avg Loss: 0.0159 | Grad Norm: 0.00807315\n",
      "Epoch 2 | Step 785200 | Avg Loss: 0.0156 | Grad Norm: 0.00845861\n",
      "Epoch 2 | Step 785300 | Avg Loss: 0.0153 | Grad Norm: 0.00982447\n",
      "Epoch 2 | Step 785400 | Avg Loss: 0.0151 | Grad Norm: 0.00778725\n",
      "Epoch 2 | Step 785500 | Avg Loss: 0.0152 | Grad Norm: 0.01043250\n",
      "Epoch 2 | Step 785600 | Avg Loss: 0.0155 | Grad Norm: 0.01037186\n",
      "Epoch 2 | Step 785700 | Avg Loss: 0.0154 | Grad Norm: 0.00853448\n",
      "Epoch 2 | Step 785800 | Avg Loss: 0.0151 | Grad Norm: 0.00897998\n",
      "Epoch 2 | Step 785900 | Avg Loss: 0.0152 | Grad Norm: 0.00857680\n",
      "Epoch 2 | Step 786000 | Avg Loss: 0.0151 | Grad Norm: 0.00828443\n",
      "Epoch 2 | Step 786100 | Avg Loss: 0.0152 | Grad Norm: 0.00824474\n",
      "Epoch 2 | Step 786200 | Avg Loss: 0.0154 | Grad Norm: 0.01041324\n",
      "Epoch 2 | Step 786300 | Avg Loss: 0.0155 | Grad Norm: 0.00814803\n",
      "Epoch 2 | Step 786400 | Avg Loss: 0.0156 | Grad Norm: 0.00741848\n",
      "Epoch 2 | Step 786500 | Avg Loss: 0.0152 | Grad Norm: 0.00855769\n",
      "Epoch 2 | Step 786600 | Avg Loss: 0.0157 | Grad Norm: 0.00834461\n",
      "Epoch 2 | Step 786700 | Avg Loss: 0.0159 | Grad Norm: 0.00922626\n",
      "Epoch 2 | Step 786800 | Avg Loss: 0.0157 | Grad Norm: 0.00879020\n",
      "Epoch 2 | Step 786900 | Avg Loss: 0.0157 | Grad Norm: 0.00833199\n",
      "Epoch 2 | Step 787000 | Avg Loss: 0.0157 | Grad Norm: 0.00885864\n",
      "Epoch 2 | Step 787100 | Avg Loss: 0.0157 | Grad Norm: 0.00795880\n",
      "Epoch 2 | Step 787200 | Avg Loss: 0.0158 | Grad Norm: 0.00936750\n",
      "Epoch 2 | Step 787300 | Avg Loss: 0.0157 | Grad Norm: 0.01263498\n",
      "Epoch 2 | Step 787400 | Avg Loss: 0.0153 | Grad Norm: 0.00918344\n",
      "Epoch 2 | Step 787500 | Avg Loss: 0.0157 | Grad Norm: 0.00850312\n",
      "Epoch 2 | Step 787600 | Avg Loss: 0.0158 | Grad Norm: 0.00903476\n",
      "Epoch 2 | Step 787700 | Avg Loss: 0.0158 | Grad Norm: 0.00816117\n",
      "Epoch 2 | Step 787800 | Avg Loss: 0.0160 | Grad Norm: 0.01027414\n",
      "Epoch 2 | Step 787900 | Avg Loss: 0.0160 | Grad Norm: 0.01067914\n",
      "Epoch 2 | Step 788000 | Avg Loss: 0.0154 | Grad Norm: 0.00938856\n",
      "Epoch 2 | Step 788100 | Avg Loss: 0.0160 | Grad Norm: 0.01005387\n",
      "Epoch 2 | Step 788200 | Avg Loss: 0.0156 | Grad Norm: 0.00890204\n",
      "Epoch 2 | Step 788300 | Avg Loss: 0.0155 | Grad Norm: 0.00787430\n",
      "Epoch 2 | Step 788400 | Avg Loss: 0.0154 | Grad Norm: 0.00862763\n",
      "Epoch 2 | Step 788500 | Avg Loss: 0.0150 | Grad Norm: 0.00770429\n",
      "Epoch 2 | Step 788600 | Avg Loss: 0.0154 | Grad Norm: 0.00914447\n",
      "Epoch 2 | Step 788700 | Avg Loss: 0.0156 | Grad Norm: 0.00777465\n",
      "Epoch 2 | Step 788800 | Avg Loss: 0.0154 | Grad Norm: 0.00918729\n",
      "Epoch 2 | Step 788900 | Avg Loss: 0.0157 | Grad Norm: 0.00789671\n",
      "Epoch 2 | Step 789000 | Avg Loss: 0.0156 | Grad Norm: 0.00977948\n",
      "Epoch 2 | Step 789100 | Avg Loss: 0.0156 | Grad Norm: 0.00790256\n",
      "Epoch 2 | Step 789200 | Avg Loss: 0.0160 | Grad Norm: 0.00852652\n",
      "Epoch 2 | Step 789300 | Avg Loss: 0.0157 | Grad Norm: 0.01014931\n",
      "Epoch 2 | Step 789400 | Avg Loss: 0.0156 | Grad Norm: 0.00923985\n",
      "Epoch 2 | Step 789500 | Avg Loss: 0.0156 | Grad Norm: 0.00876002\n",
      "Epoch 2 | Step 789600 | Avg Loss: 0.0158 | Grad Norm: 0.00903795\n",
      "Epoch 2 | Step 789700 | Avg Loss: 0.0155 | Grad Norm: 0.00790265\n",
      "Epoch 2 | Step 789800 | Avg Loss: 0.0157 | Grad Norm: 0.01031973\n",
      "Epoch 2 | Step 789900 | Avg Loss: 0.0161 | Grad Norm: 0.01017905\n",
      "Epoch 2 | Step 790000 | Avg Loss: 0.0161 | Grad Norm: 0.00880314\n",
      "Epoch 2 | Step 790100 | Avg Loss: 0.0159 | Grad Norm: 0.00801561\n",
      "Epoch 2 | Step 790200 | Avg Loss: 0.0159 | Grad Norm: 0.00847876\n",
      "Epoch 2 | Step 790300 | Avg Loss: 0.0157 | Grad Norm: 0.00969678\n",
      "Epoch 2 | Step 790400 | Avg Loss: 0.0157 | Grad Norm: 0.00982370\n",
      "Epoch 2 | Step 790500 | Avg Loss: 0.0154 | Grad Norm: 0.00928325\n",
      "Epoch 2 | Step 790600 | Avg Loss: 0.0154 | Grad Norm: 0.00859410\n",
      "Epoch 2 | Step 790700 | Avg Loss: 0.0156 | Grad Norm: 0.00849291\n",
      "Epoch 2 | Step 790800 | Avg Loss: 0.0157 | Grad Norm: 0.01137028\n",
      "Epoch 2 | Step 790900 | Avg Loss: 0.0156 | Grad Norm: 0.00855512\n",
      "Epoch 2 | Step 791000 | Avg Loss: 0.0156 | Grad Norm: 0.01008734\n",
      "Epoch 2 | Step 791100 | Avg Loss: 0.0158 | Grad Norm: 0.00713772\n",
      "Epoch 2 | Step 791200 | Avg Loss: 0.0159 | Grad Norm: 0.00999389\n",
      "Epoch 2 | Step 791300 | Avg Loss: 0.0159 | Grad Norm: 0.00888120\n",
      "Epoch 2 | Step 791400 | Avg Loss: 0.0160 | Grad Norm: 0.00870429\n",
      "Epoch 2 | Step 791500 | Avg Loss: 0.0159 | Grad Norm: 0.00855030\n",
      "Epoch 2 | Step 791600 | Avg Loss: 0.0158 | Grad Norm: 0.00918476\n",
      "Epoch 2 | Step 791700 | Avg Loss: 0.0160 | Grad Norm: 0.00831000\n",
      "Epoch 2 | Step 791800 | Avg Loss: 0.0166 | Grad Norm: 0.00896183\n",
      "Epoch 2 | Step 791900 | Avg Loss: 0.0165 | Grad Norm: 0.00901687\n",
      "Epoch 2 | Step 792000 | Avg Loss: 0.0163 | Grad Norm: 0.00935342\n",
      "Epoch 2 | Step 792100 | Avg Loss: 0.0163 | Grad Norm: 0.00872226\n",
      "Epoch 2 | Step 792200 | Avg Loss: 0.0158 | Grad Norm: 0.00842059\n",
      "Epoch 2 | Step 792300 | Avg Loss: 0.0162 | Grad Norm: 0.00864076\n",
      "Epoch 2 | Step 792400 | Avg Loss: 0.0163 | Grad Norm: 0.00836268\n",
      "Epoch 2 | Step 792500 | Avg Loss: 0.0160 | Grad Norm: 0.01009768\n",
      "Epoch 2 | Step 792600 | Avg Loss: 0.0160 | Grad Norm: 0.00842758\n",
      "Epoch 2 | Step 792700 | Avg Loss: 0.0159 | Grad Norm: 0.00835348\n",
      "Epoch 2 | Step 792800 | Avg Loss: 0.0163 | Grad Norm: 0.00881112\n",
      "Epoch 2 | Step 792900 | Avg Loss: 0.0157 | Grad Norm: 0.00967504\n",
      "Epoch 2 | Step 793000 | Avg Loss: 0.0157 | Grad Norm: 0.00779125\n",
      "Epoch 2 | Step 793100 | Avg Loss: 0.0155 | Grad Norm: 0.00713692\n",
      "Epoch 2 | Step 793200 | Avg Loss: 0.0151 | Grad Norm: 0.00930469\n",
      "Epoch 2 | Step 793300 | Avg Loss: 0.0152 | Grad Norm: 0.00798594\n",
      "Epoch 2 | Step 793400 | Avg Loss: 0.0151 | Grad Norm: 0.00798998\n",
      "Epoch 2 | Step 793500 | Avg Loss: 0.0153 | Grad Norm: 0.00918709\n",
      "Epoch 2 | Step 793600 | Avg Loss: 0.0157 | Grad Norm: 0.00950742\n",
      "Epoch 2 | Step 793700 | Avg Loss: 0.0160 | Grad Norm: 0.00951181\n",
      "Epoch 2 | Step 793800 | Avg Loss: 0.0160 | Grad Norm: 0.00855708\n",
      "Epoch 2 | Step 793900 | Avg Loss: 0.0159 | Grad Norm: 0.00842357\n",
      "Epoch 2 | Step 794000 | Avg Loss: 0.0156 | Grad Norm: 0.00800786\n",
      "Epoch 2 | Step 794100 | Avg Loss: 0.0156 | Grad Norm: 0.00801598\n",
      "Epoch 2 | Step 794200 | Avg Loss: 0.0159 | Grad Norm: 0.00867696\n",
      "Epoch 2 | Step 794300 | Avg Loss: 0.0156 | Grad Norm: 0.00879223\n",
      "Epoch 2 | Step 794400 | Avg Loss: 0.0154 | Grad Norm: 0.00856742\n",
      "Epoch 2 | Step 794500 | Avg Loss: 0.0157 | Grad Norm: 0.01206278\n",
      "Epoch 2 | Step 794600 | Avg Loss: 0.0155 | Grad Norm: 0.00832290\n",
      "Epoch 2 | Step 794700 | Avg Loss: 0.0159 | Grad Norm: 0.00946673\n",
      "Epoch 2 | Step 794800 | Avg Loss: 0.0154 | Grad Norm: 0.00948103\n",
      "Epoch 2 | Step 794900 | Avg Loss: 0.0155 | Grad Norm: 0.00904454\n",
      "Epoch 2 | Step 795000 | Avg Loss: 0.0153 | Grad Norm: 0.00895871\n",
      "Epoch 2 | Step 795100 | Avg Loss: 0.0155 | Grad Norm: 0.00791632\n",
      "Epoch 2 | Step 795200 | Avg Loss: 0.0159 | Grad Norm: 0.00968990\n",
      "Epoch 2 | Step 795300 | Avg Loss: 0.0156 | Grad Norm: 0.00961010\n",
      "Epoch 2 | Step 795400 | Avg Loss: 0.0156 | Grad Norm: 0.00928473\n",
      "Epoch 2 | Step 795500 | Avg Loss: 0.0155 | Grad Norm: 0.00887503\n",
      "Epoch 2 | Step 795600 | Avg Loss: 0.0155 | Grad Norm: 0.00959545\n",
      "Epoch 2 | Step 795700 | Avg Loss: 0.0158 | Grad Norm: 0.00773327\n",
      "Epoch 2 | Step 795800 | Avg Loss: 0.0158 | Grad Norm: 0.01016305\n",
      "Epoch 2 | Step 795900 | Avg Loss: 0.0159 | Grad Norm: 0.00924769\n",
      "Epoch 2 | Step 796000 | Avg Loss: 0.0157 | Grad Norm: 0.00928737\n",
      "Epoch 2 | Step 796100 | Avg Loss: 0.0158 | Grad Norm: 0.01105905\n",
      "Epoch 2 | Step 796200 | Avg Loss: 0.0157 | Grad Norm: 0.00845865\n",
      "Epoch 2 | Step 796300 | Avg Loss: 0.0156 | Grad Norm: 0.00799655\n",
      "Epoch 2 | Step 796400 | Avg Loss: 0.0157 | Grad Norm: 0.00984196\n",
      "Epoch 2 | Step 796500 | Avg Loss: 0.0156 | Grad Norm: 0.01001993\n",
      "Epoch 2 | Step 796600 | Avg Loss: 0.0159 | Grad Norm: 0.01121122\n",
      "Epoch 2 | Step 796700 | Avg Loss: 0.0155 | Grad Norm: 0.00888147\n",
      "Epoch 2 | Step 796800 | Avg Loss: 0.0159 | Grad Norm: 0.00911026\n",
      "Epoch 2 | Step 796900 | Avg Loss: 0.0161 | Grad Norm: 0.00813059\n",
      "Epoch 2 | Step 797000 | Avg Loss: 0.0164 | Grad Norm: 0.00923525\n",
      "Epoch 2 | Step 797100 | Avg Loss: 0.0168 | Grad Norm: 0.00888209\n",
      "Epoch 2 | Step 797200 | Avg Loss: 0.0167 | Grad Norm: 0.00931224\n",
      "Epoch 2 | Step 797300 | Avg Loss: 0.0164 | Grad Norm: 0.00809109\n",
      "Epoch 2 | Step 797400 | Avg Loss: 0.0167 | Grad Norm: 0.00861084\n",
      "Epoch 2 | Step 797500 | Avg Loss: 0.0166 | Grad Norm: 0.01004975\n",
      "Epoch 2 | Step 797600 | Avg Loss: 0.0162 | Grad Norm: 0.00819608\n",
      "Epoch 2 | Step 797700 | Avg Loss: 0.0163 | Grad Norm: 0.00890083\n",
      "Epoch 2 | Step 797800 | Avg Loss: 0.0164 | Grad Norm: 0.01057848\n",
      "Epoch 2 | Step 797900 | Avg Loss: 0.0157 | Grad Norm: 0.01012093\n",
      "Epoch 2 | Step 798000 | Avg Loss: 0.0156 | Grad Norm: 0.00858083\n",
      "Epoch 2 | Step 798100 | Avg Loss: 0.0158 | Grad Norm: 0.00993622\n",
      "Epoch 2 | Step 798200 | Avg Loss: 0.0161 | Grad Norm: 0.01027232\n",
      "Epoch 2 | Step 798300 | Avg Loss: 0.0157 | Grad Norm: 0.00867803\n",
      "Epoch 2 | Step 798400 | Avg Loss: 0.0157 | Grad Norm: 0.00946959\n",
      "Epoch 2 | Step 798500 | Avg Loss: 0.0159 | Grad Norm: 0.01063427\n",
      "Epoch 2 | Step 798600 | Avg Loss: 0.0156 | Grad Norm: 0.00808396\n",
      "Epoch 2 | Step 798700 | Avg Loss: 0.0155 | Grad Norm: 0.01011128\n",
      "Epoch 2 | Step 798800 | Avg Loss: 0.0156 | Grad Norm: 0.00862082\n",
      "Epoch 2 | Step 798900 | Avg Loss: 0.0154 | Grad Norm: 0.00817483\n",
      "Epoch 2 | Step 799000 | Avg Loss: 0.0156 | Grad Norm: 0.00874691\n",
      "Epoch 2 | Step 799100 | Avg Loss: 0.0152 | Grad Norm: 0.00964460\n",
      "Epoch 2 | Step 799200 | Avg Loss: 0.0157 | Grad Norm: 0.01148498\n",
      "Epoch 2 | Step 799300 | Avg Loss: 0.0153 | Grad Norm: 0.01005151\n",
      "Epoch 2 | Step 799400 | Avg Loss: 0.0153 | Grad Norm: 0.00912137\n",
      "Epoch 2 | Step 799500 | Avg Loss: 0.0155 | Grad Norm: 0.00881006\n",
      "Epoch 2 | Step 799600 | Avg Loss: 0.0154 | Grad Norm: 0.00919436\n",
      "Epoch 2 | Step 799700 | Avg Loss: 0.0155 | Grad Norm: 0.00884801\n",
      "Epoch 2 | Step 799800 | Avg Loss: 0.0159 | Grad Norm: 0.00835758\n",
      "Epoch 2 | Step 799900 | Avg Loss: 0.0159 | Grad Norm: 0.01151068\n",
      "Epoch 2 | Step 800000 | Avg Loss: 0.0161 | Grad Norm: 0.01038971\n",
      "Saving model at step800000\n",
      "Epoch 2 | Step 800100 | Avg Loss: 0.0164 | Grad Norm: 0.01138480\n",
      "Epoch 2 | Step 800200 | Avg Loss: 0.0165 | Grad Norm: 0.00961634\n",
      "Epoch 2 | Step 800300 | Avg Loss: 0.0164 | Grad Norm: 0.00835735\n",
      "Epoch 2 | Step 800400 | Avg Loss: 0.0164 | Grad Norm: 0.00791941\n",
      "Epoch 2 | Step 800500 | Avg Loss: 0.0162 | Grad Norm: 0.00832005\n",
      "Epoch 2 | Step 800600 | Avg Loss: 0.0159 | Grad Norm: 0.00836392\n",
      "Epoch 2 | Step 800700 | Avg Loss: 0.0157 | Grad Norm: 0.00828549\n",
      "Epoch 2 | Step 800800 | Avg Loss: 0.0154 | Grad Norm: 0.00993753\n",
      "Epoch 2 | Step 800900 | Avg Loss: 0.0156 | Grad Norm: 0.00938258\n",
      "Epoch 2 | Step 801000 | Avg Loss: 0.0155 | Grad Norm: 0.00738041\n",
      "Epoch 2 | Step 801100 | Avg Loss: 0.0153 | Grad Norm: 0.00890041\n",
      "Epoch 2 | Step 801200 | Avg Loss: 0.0154 | Grad Norm: 0.00967094\n",
      "Epoch 2 | Step 801300 | Avg Loss: 0.0157 | Grad Norm: 0.00818554\n",
      "Epoch 2 | Step 801400 | Avg Loss: 0.0153 | Grad Norm: 0.00774999\n",
      "Epoch 2 | Step 801500 | Avg Loss: 0.0154 | Grad Norm: 0.00760860\n",
      "Epoch 2 | Step 801600 | Avg Loss: 0.0157 | Grad Norm: 0.00871829\n",
      "Epoch 2 | Step 801700 | Avg Loss: 0.0160 | Grad Norm: 0.01035487\n",
      "Epoch 2 | Step 801800 | Avg Loss: 0.0159 | Grad Norm: 0.00838360\n",
      "Epoch 2 | Step 801900 | Avg Loss: 0.0161 | Grad Norm: 0.01048806\n",
      "Epoch 2 | Step 802000 | Avg Loss: 0.0159 | Grad Norm: 0.00806535\n",
      "Epoch 2 | Step 802100 | Avg Loss: 0.0153 | Grad Norm: 0.00810001\n",
      "Epoch 2 | Step 802200 | Avg Loss: 0.0156 | Grad Norm: 0.00761540\n",
      "Epoch 2 | Step 802300 | Avg Loss: 0.0156 | Grad Norm: 0.00982920\n",
      "Epoch 2 | Step 802400 | Avg Loss: 0.0156 | Grad Norm: 0.00816817\n",
      "Epoch 2 | Step 802500 | Avg Loss: 0.0155 | Grad Norm: 0.00874999\n",
      "Epoch 2 | Step 802600 | Avg Loss: 0.0158 | Grad Norm: 0.00895739\n",
      "Epoch 2 | Step 802700 | Avg Loss: 0.0161 | Grad Norm: 0.01016250\n",
      "Epoch 2 | Step 802800 | Avg Loss: 0.0159 | Grad Norm: 0.00960098\n",
      "Epoch 2 | Step 802900 | Avg Loss: 0.0159 | Grad Norm: 0.00851649\n",
      "Epoch 2 | Step 803000 | Avg Loss: 0.0164 | Grad Norm: 0.00763115\n",
      "Epoch 2 | Step 803100 | Avg Loss: 0.0160 | Grad Norm: 0.00882528\n",
      "Epoch 2 | Step 803200 | Avg Loss: 0.0161 | Grad Norm: 0.01000197\n",
      "Epoch 2 | Step 803300 | Avg Loss: 0.0159 | Grad Norm: 0.00928315\n",
      "Epoch 2 | Step 803400 | Avg Loss: 0.0159 | Grad Norm: 0.00989861\n",
      "Epoch 2 | Step 803500 | Avg Loss: 0.0157 | Grad Norm: 0.00953489\n",
      "Epoch 2 | Step 803600 | Avg Loss: 0.0158 | Grad Norm: 0.00906656\n",
      "Epoch 2 | Step 803700 | Avg Loss: 0.0158 | Grad Norm: 0.00866349\n",
      "Epoch 2 | Step 803800 | Avg Loss: 0.0157 | Grad Norm: 0.00874280\n",
      "Epoch 2 | Step 803900 | Avg Loss: 0.0158 | Grad Norm: 0.00797862\n",
      "Epoch 2 | Step 804000 | Avg Loss: 0.0158 | Grad Norm: 0.00794955\n",
      "Epoch 2 | Step 804100 | Avg Loss: 0.0164 | Grad Norm: 0.00859212\n",
      "Epoch 2 | Step 804200 | Avg Loss: 0.0158 | Grad Norm: 0.00862588\n",
      "Epoch 2 | Step 804300 | Avg Loss: 0.0157 | Grad Norm: 0.00931524\n",
      "Epoch 2 | Step 804400 | Avg Loss: 0.0156 | Grad Norm: 0.00803524\n",
      "Epoch 2 | Step 804500 | Avg Loss: 0.0154 | Grad Norm: 0.00855167\n",
      "Epoch 2 | Step 804600 | Avg Loss: 0.0157 | Grad Norm: 0.00800537\n",
      "Epoch 2 | Step 804700 | Avg Loss: 0.0158 | Grad Norm: 0.00885598\n",
      "Epoch 2 | Step 804800 | Avg Loss: 0.0161 | Grad Norm: 0.01051754\n",
      "Epoch 2 | Step 804900 | Avg Loss: 0.0163 | Grad Norm: 0.00942393\n",
      "Epoch 2 | Step 805000 | Avg Loss: 0.0164 | Grad Norm: 0.00791465\n",
      "Epoch 2 | Step 805100 | Avg Loss: 0.0160 | Grad Norm: 0.00856672\n",
      "Epoch 2 | Step 805200 | Avg Loss: 0.0160 | Grad Norm: 0.00856205\n",
      "Epoch 2 | Step 805300 | Avg Loss: 0.0158 | Grad Norm: 0.00908024\n",
      "Epoch 2 | Step 805400 | Avg Loss: 0.0161 | Grad Norm: 0.01063606\n",
      "Epoch 2 | Step 805500 | Avg Loss: 0.0160 | Grad Norm: 0.00863600\n",
      "Epoch 2 | Step 805600 | Avg Loss: 0.0159 | Grad Norm: 0.00960337\n",
      "Epoch 2 | Step 805700 | Avg Loss: 0.0156 | Grad Norm: 0.00965547\n",
      "Epoch 2 | Step 805800 | Avg Loss: 0.0157 | Grad Norm: 0.01220359\n",
      "Epoch 2 | Step 805900 | Avg Loss: 0.0162 | Grad Norm: 0.01346727\n",
      "Epoch 2 | Step 806000 | Avg Loss: 0.0160 | Grad Norm: 0.00839455\n",
      "Epoch 2 | Step 806100 | Avg Loss: 0.0158 | Grad Norm: 0.00921454\n",
      "Epoch 2 | Step 806200 | Avg Loss: 0.0159 | Grad Norm: 0.01044011\n",
      "Epoch 2 | Step 806300 | Avg Loss: 0.0160 | Grad Norm: 0.00764194\n",
      "Epoch 2 | Step 806400 | Avg Loss: 0.0161 | Grad Norm: 0.00748651\n",
      "Epoch 2 | Step 806500 | Avg Loss: 0.0168 | Grad Norm: 0.01014741\n",
      "Epoch 2 | Step 806600 | Avg Loss: 0.0165 | Grad Norm: 0.00976187\n",
      "Epoch 2 | Step 806700 | Avg Loss: 0.0163 | Grad Norm: 0.01034476\n",
      "Epoch 2 | Step 806800 | Avg Loss: 0.0164 | Grad Norm: 0.00936308\n",
      "Epoch 2 | Step 806900 | Avg Loss: 0.0163 | Grad Norm: 0.00943924\n",
      "Epoch 2 | Step 807000 | Avg Loss: 0.0161 | Grad Norm: 0.01036176\n",
      "Epoch 2 | Step 807100 | Avg Loss: 0.0163 | Grad Norm: 0.01024879\n",
      "Epoch 2 | Step 807200 | Avg Loss: 0.0163 | Grad Norm: 0.00908904\n",
      "Epoch 2 | Step 807300 | Avg Loss: 0.0157 | Grad Norm: 0.00929570\n",
      "Epoch 2 | Step 807400 | Avg Loss: 0.0155 | Grad Norm: 0.01348624\n",
      "Epoch 2 | Step 807500 | Avg Loss: 0.0159 | Grad Norm: 0.00812963\n",
      "Epoch 2 | Step 807600 | Avg Loss: 0.0158 | Grad Norm: 0.00832469\n",
      "Epoch 2 | Step 807700 | Avg Loss: 0.0156 | Grad Norm: 0.00826542\n",
      "Epoch 2 | Step 807800 | Avg Loss: 0.0155 | Grad Norm: 0.01000207\n",
      "Epoch 2 | Step 807900 | Avg Loss: 0.0154 | Grad Norm: 0.00922621\n",
      "Epoch 2 | Step 808000 | Avg Loss: 0.0158 | Grad Norm: 0.00941994\n",
      "Epoch 2 | Step 808100 | Avg Loss: 0.0158 | Grad Norm: 0.00853470\n",
      "Epoch 2 | Step 808200 | Avg Loss: 0.0161 | Grad Norm: 0.00808336\n",
      "Epoch 2 | Step 808300 | Avg Loss: 0.0161 | Grad Norm: 0.00919178\n",
      "Epoch 2 | Step 808400 | Avg Loss: 0.0158 | Grad Norm: 0.00899442\n",
      "Epoch 2 | Step 808500 | Avg Loss: 0.0156 | Grad Norm: 0.00814479\n",
      "Epoch 2 | Step 808600 | Avg Loss: 0.0157 | Grad Norm: 0.00919292\n",
      "Epoch 2 | Step 808700 | Avg Loss: 0.0160 | Grad Norm: 0.01042803\n",
      "Epoch 2 | Step 808800 | Avg Loss: 0.0160 | Grad Norm: 0.00817470\n",
      "Epoch 2 | Step 808900 | Avg Loss: 0.0158 | Grad Norm: 0.00912275\n",
      "Epoch 2 | Step 809000 | Avg Loss: 0.0161 | Grad Norm: 0.00902016\n",
      "Epoch 2 | Step 809100 | Avg Loss: 0.0161 | Grad Norm: 0.00892950\n",
      "Epoch 2 | Step 809200 | Avg Loss: 0.0161 | Grad Norm: 0.00889167\n",
      "Epoch 2 | Step 809300 | Avg Loss: 0.0160 | Grad Norm: 0.00966302\n",
      "Epoch 2 | Step 809400 | Avg Loss: 0.0160 | Grad Norm: 0.00979022\n",
      "Epoch 2 | Step 809500 | Avg Loss: 0.0156 | Grad Norm: 0.00895391\n",
      "Epoch 2 | Step 809600 | Avg Loss: 0.0154 | Grad Norm: 0.00830419\n",
      "Epoch 2 | Step 809700 | Avg Loss: 0.0157 | Grad Norm: 0.00865217\n",
      "Epoch 2 | Step 809800 | Avg Loss: 0.0162 | Grad Norm: 0.00895477\n",
      "Epoch 2 | Step 809900 | Avg Loss: 0.0157 | Grad Norm: 0.00935888\n",
      "Epoch 2 | Step 810000 | Avg Loss: 0.0157 | Grad Norm: 0.00855840\n",
      "Epoch 2 | Step 810100 | Avg Loss: 0.0156 | Grad Norm: 0.00834772\n",
      "Epoch 2 | Step 810200 | Avg Loss: 0.0155 | Grad Norm: 0.00902913\n",
      "Epoch 2 | Step 810300 | Avg Loss: 0.0156 | Grad Norm: 0.00869382\n",
      "Epoch 2 | Step 810400 | Avg Loss: 0.0157 | Grad Norm: 0.00914790\n",
      "Epoch 2 | Step 810500 | Avg Loss: 0.0155 | Grad Norm: 0.00742177\n",
      "Epoch 2 | Step 810600 | Avg Loss: 0.0155 | Grad Norm: 0.00968608\n",
      "Epoch 2 | Step 810700 | Avg Loss: 0.0154 | Grad Norm: 0.00759244\n",
      "Epoch 2 | Step 810800 | Avg Loss: 0.0157 | Grad Norm: 0.00827411\n",
      "Epoch 2 | Step 810900 | Avg Loss: 0.0155 | Grad Norm: 0.00941841\n",
      "Epoch 2 | Step 811000 | Avg Loss: 0.0159 | Grad Norm: 0.00914973\n",
      "Epoch 2 | Step 811100 | Avg Loss: 0.0160 | Grad Norm: 0.00863240\n",
      "Epoch 2 | Step 811200 | Avg Loss: 0.0161 | Grad Norm: 0.00778031\n",
      "Epoch 2 | Step 811300 | Avg Loss: 0.0156 | Grad Norm: 0.00916717\n",
      "Epoch 2 | Step 811400 | Avg Loss: 0.0157 | Grad Norm: 0.01027057\n",
      "Epoch 2 | Step 811500 | Avg Loss: 0.0161 | Grad Norm: 0.00929162\n",
      "Epoch 2 | Step 811600 | Avg Loss: 0.0161 | Grad Norm: 0.00826707\n",
      "Epoch 2 | Step 811700 | Avg Loss: 0.0158 | Grad Norm: 0.00752288\n",
      "Epoch 2 | Step 811800 | Avg Loss: 0.0157 | Grad Norm: 0.00875179\n",
      "Epoch 2 | Step 811900 | Avg Loss: 0.0154 | Grad Norm: 0.00834352\n",
      "Epoch 2 | Step 812000 | Avg Loss: 0.0155 | Grad Norm: 0.00892964\n",
      "Epoch 2 | Step 812100 | Avg Loss: 0.0160 | Grad Norm: 0.00959807\n",
      "Epoch 2 | Step 812200 | Avg Loss: 0.0159 | Grad Norm: 0.00886221\n",
      "Epoch 2 | Step 812300 | Avg Loss: 0.0157 | Grad Norm: 0.01044158\n",
      "Epoch 2 | Step 812400 | Avg Loss: 0.0156 | Grad Norm: 0.00935395\n",
      "Epoch 2 | Step 812500 | Avg Loss: 0.0156 | Grad Norm: 0.00806099\n",
      "Epoch 2 | Step 812600 | Avg Loss: 0.0155 | Grad Norm: 0.00946983\n",
      "Epoch 2 | Step 812700 | Avg Loss: 0.0157 | Grad Norm: 0.00860971\n",
      "Epoch 2 | Step 812800 | Avg Loss: 0.0155 | Grad Norm: 0.00995868\n",
      "Epoch 2 | Step 812900 | Avg Loss: 0.0155 | Grad Norm: 0.00888801\n",
      "Epoch 2 | Step 813000 | Avg Loss: 0.0155 | Grad Norm: 0.01061284\n",
      "Epoch 2 | Step 813100 | Avg Loss: 0.0160 | Grad Norm: 0.00811945\n",
      "Epoch 2 | Step 813200 | Avg Loss: 0.0160 | Grad Norm: 0.00982034\n",
      "Epoch 2 | Step 813300 | Avg Loss: 0.0158 | Grad Norm: 0.01063206\n",
      "Epoch 2 | Step 813400 | Avg Loss: 0.0159 | Grad Norm: 0.00924175\n",
      "Epoch 2 | Step 813500 | Avg Loss: 0.0159 | Grad Norm: 0.01024091\n",
      "Epoch 2 | Step 813600 | Avg Loss: 0.0161 | Grad Norm: 0.00870867\n",
      "Epoch 2 | Step 813700 | Avg Loss: 0.0162 | Grad Norm: 0.00980664\n",
      "Epoch 2 | Step 813800 | Avg Loss: 0.0162 | Grad Norm: 0.00861851\n",
      "Epoch 2 | Step 813900 | Avg Loss: 0.0159 | Grad Norm: 0.00767667\n",
      "Epoch 2 | Step 814000 | Avg Loss: 0.0163 | Grad Norm: 0.01097284\n",
      "Epoch 2 | Step 814100 | Avg Loss: 0.0159 | Grad Norm: 0.00752474\n",
      "Epoch 2 | Step 814200 | Avg Loss: 0.0157 | Grad Norm: 0.00981458\n",
      "Epoch 2 | Step 814300 | Avg Loss: 0.0158 | Grad Norm: 0.00875157\n",
      "Epoch 2 | Step 814400 | Avg Loss: 0.0157 | Grad Norm: 0.00916282\n",
      "Epoch 2 | Step 814500 | Avg Loss: 0.0154 | Grad Norm: 0.00807022\n",
      "Epoch 2 | Step 814600 | Avg Loss: 0.0156 | Grad Norm: 0.00828921\n",
      "Epoch 2 | Step 814700 | Avg Loss: 0.0155 | Grad Norm: 0.00848961\n",
      "Epoch 2 | Step 814800 | Avg Loss: 0.0156 | Grad Norm: 0.00930854\n",
      "Epoch 2 | Step 814900 | Avg Loss: 0.0159 | Grad Norm: 0.00852783\n",
      "Epoch 2 | Step 815000 | Avg Loss: 0.0159 | Grad Norm: 0.00867885\n",
      "Epoch 2 | Step 815100 | Avg Loss: 0.0155 | Grad Norm: 0.00872452\n",
      "Epoch 2 | Step 815200 | Avg Loss: 0.0155 | Grad Norm: 0.00934833\n",
      "Epoch 2 | Step 815300 | Avg Loss: 0.0151 | Grad Norm: 0.00817623\n",
      "Epoch 2 | Step 815400 | Avg Loss: 0.0147 | Grad Norm: 0.00853631\n",
      "Epoch 2 | Step 815500 | Avg Loss: 0.0146 | Grad Norm: 0.00788922\n",
      "Epoch 2 | Step 815600 | Avg Loss: 0.0151 | Grad Norm: 0.00827054\n",
      "Epoch 2 | Step 815700 | Avg Loss: 0.0157 | Grad Norm: 0.00904714\n",
      "Epoch 2 | Step 815800 | Avg Loss: 0.0158 | Grad Norm: 0.00913676\n",
      "Epoch 2 | Step 815900 | Avg Loss: 0.0157 | Grad Norm: 0.00862060\n",
      "Epoch 2 | Step 816000 | Avg Loss: 0.0154 | Grad Norm: 0.00829366\n",
      "Epoch 2 | Step 816100 | Avg Loss: 0.0155 | Grad Norm: 0.00842671\n",
      "Epoch 2 | Step 816200 | Avg Loss: 0.0156 | Grad Norm: 0.00875065\n",
      "Epoch 2 | Step 816300 | Avg Loss: 0.0156 | Grad Norm: 0.01004708\n",
      "Epoch 2 | Step 816400 | Avg Loss: 0.0160 | Grad Norm: 0.00954504\n",
      "Epoch 2 | Step 816500 | Avg Loss: 0.0159 | Grad Norm: 0.01063032\n",
      "Epoch 2 | Step 816600 | Avg Loss: 0.0158 | Grad Norm: 0.01218838\n",
      "Epoch 2 | Step 816700 | Avg Loss: 0.0159 | Grad Norm: 0.00876753\n",
      "Epoch 2 | Step 816800 | Avg Loss: 0.0162 | Grad Norm: 0.00923355\n",
      "Epoch 2 | Step 816900 | Avg Loss: 0.0163 | Grad Norm: 0.00844509\n",
      "Epoch 2 | Step 817000 | Avg Loss: 0.0161 | Grad Norm: 0.00841654\n",
      "Epoch 2 | Step 817100 | Avg Loss: 0.0157 | Grad Norm: 0.00910798\n",
      "Epoch 2 | Step 817200 | Avg Loss: 0.0156 | Grad Norm: 0.01008249\n",
      "Epoch 2 | Step 817300 | Avg Loss: 0.0152 | Grad Norm: 0.00965193\n",
      "Epoch 2 | Step 817400 | Avg Loss: 0.0156 | Grad Norm: 0.01011834\n",
      "Epoch 2 | Step 817500 | Avg Loss: 0.0157 | Grad Norm: 0.00906010\n",
      "Epoch 2 | Step 817600 | Avg Loss: 0.0159 | Grad Norm: 0.01028172\n",
      "Epoch 2 | Step 817700 | Avg Loss: 0.0163 | Grad Norm: 0.00849110\n",
      "Epoch 2 | Step 817800 | Avg Loss: 0.0161 | Grad Norm: 0.00875945\n",
      "Epoch 2 | Step 817900 | Avg Loss: 0.0162 | Grad Norm: 0.00901184\n",
      "Epoch 2 | Step 818000 | Avg Loss: 0.0160 | Grad Norm: 0.01026512\n",
      "Epoch 2 | Step 818100 | Avg Loss: 0.0161 | Grad Norm: 0.00889929\n",
      "Epoch 2 | Step 818200 | Avg Loss: 0.0163 | Grad Norm: 0.00780323\n",
      "Epoch 2 | Step 818300 | Avg Loss: 0.0162 | Grad Norm: 0.00939102\n",
      "Epoch 2 | Step 818400 | Avg Loss: 0.0162 | Grad Norm: 0.00824174\n",
      "Epoch 2 | Step 818500 | Avg Loss: 0.0159 | Grad Norm: 0.00837141\n",
      "Epoch 2 | Step 818600 | Avg Loss: 0.0160 | Grad Norm: 0.00876928\n",
      "Epoch 2 | Step 818700 | Avg Loss: 0.0159 | Grad Norm: 0.01015232\n",
      "Epoch 2 | Step 818800 | Avg Loss: 0.0159 | Grad Norm: 0.00916805\n",
      "Epoch 2 | Step 818900 | Avg Loss: 0.0161 | Grad Norm: 0.00783135\n",
      "Epoch 2 | Step 819000 | Avg Loss: 0.0162 | Grad Norm: 0.00955211\n",
      "Epoch 2 | Step 819100 | Avg Loss: 0.0159 | Grad Norm: 0.00956615\n",
      "Epoch 2 | Step 819200 | Avg Loss: 0.0159 | Grad Norm: 0.00912226\n",
      "Epoch 2 | Step 819300 | Avg Loss: 0.0163 | Grad Norm: 0.00969743\n",
      "Epoch 2 | Step 819400 | Avg Loss: 0.0164 | Grad Norm: 0.00902290\n",
      "Epoch 2 | Step 819500 | Avg Loss: 0.0165 | Grad Norm: 0.00942704\n",
      "Epoch 2 | Step 819600 | Avg Loss: 0.0165 | Grad Norm: 0.00829987\n",
      "Epoch 2 | Step 819700 | Avg Loss: 0.0163 | Grad Norm: 0.00817173\n",
      "Epoch 2 | Step 819800 | Avg Loss: 0.0163 | Grad Norm: 0.00910682\n",
      "Epoch 2 | Step 819900 | Avg Loss: 0.0163 | Grad Norm: 0.00801589\n",
      "Epoch 2 | Step 820000 | Avg Loss: 0.0163 | Grad Norm: 0.00930885\n",
      "Epoch 2 | Step 820100 | Avg Loss: 0.0158 | Grad Norm: 0.00863215\n",
      "Epoch 2 | Step 820200 | Avg Loss: 0.0158 | Grad Norm: 0.01099807\n",
      "Epoch 2 | Step 820300 | Avg Loss: 0.0158 | Grad Norm: 0.00894291\n",
      "Epoch 2 | Step 820400 | Avg Loss: 0.0161 | Grad Norm: 0.00848994\n",
      "Epoch 2 | Step 820500 | Avg Loss: 0.0156 | Grad Norm: 0.00798456\n",
      "Epoch 2 | Step 820600 | Avg Loss: 0.0156 | Grad Norm: 0.00890168\n",
      "Epoch 2 | Step 820700 | Avg Loss: 0.0159 | Grad Norm: 0.00959246\n",
      "Epoch 2 | Step 820800 | Avg Loss: 0.0155 | Grad Norm: 0.00864973\n",
      "Epoch 2 | Step 820900 | Avg Loss: 0.0159 | Grad Norm: 0.00981471\n",
      "Epoch 2 | Step 821000 | Avg Loss: 0.0160 | Grad Norm: 0.00904871\n",
      "Epoch 2 | Step 821100 | Avg Loss: 0.0159 | Grad Norm: 0.00935925\n",
      "Epoch 2 | Step 821200 | Avg Loss: 0.0156 | Grad Norm: 0.00803182\n",
      "Epoch 2 | Step 821300 | Avg Loss: 0.0156 | Grad Norm: 0.00940500\n",
      "Epoch 2 | Step 821400 | Avg Loss: 0.0157 | Grad Norm: 0.00890531\n",
      "Epoch 2 | Step 821500 | Avg Loss: 0.0158 | Grad Norm: 0.00889819\n",
      "Epoch 2 | Step 821600 | Avg Loss: 0.0155 | Grad Norm: 0.00879690\n",
      "Epoch 2 | Step 821700 | Avg Loss: 0.0155 | Grad Norm: 0.00841427\n",
      "Epoch 2 | Step 821800 | Avg Loss: 0.0157 | Grad Norm: 0.00858529\n",
      "Epoch 2 | Step 821900 | Avg Loss: 0.0153 | Grad Norm: 0.00873489\n",
      "Epoch 2 | Step 822000 | Avg Loss: 0.0154 | Grad Norm: 0.00832822\n",
      "Epoch 2 | Step 822100 | Avg Loss: 0.0152 | Grad Norm: 0.00917083\n",
      "Epoch 2 | Step 822200 | Avg Loss: 0.0154 | Grad Norm: 0.00917901\n",
      "Epoch 2 | Step 822300 | Avg Loss: 0.0153 | Grad Norm: 0.00819764\n",
      "Epoch 2 | Step 822400 | Avg Loss: 0.0154 | Grad Norm: 0.01188504\n",
      "Epoch 2 | Step 822500 | Avg Loss: 0.0159 | Grad Norm: 0.00910279\n",
      "Epoch 2 | Step 822600 | Avg Loss: 0.0158 | Grad Norm: 0.01001496\n",
      "Epoch 2 | Step 822700 | Avg Loss: 0.0154 | Grad Norm: 0.00942528\n",
      "Epoch 2 | Step 822800 | Avg Loss: 0.0153 | Grad Norm: 0.00799519\n",
      "Epoch 2 | Step 822900 | Avg Loss: 0.0158 | Grad Norm: 0.00820059\n",
      "Epoch 2 | Step 823000 | Avg Loss: 0.0156 | Grad Norm: 0.00920692\n",
      "Epoch 2 | Step 823100 | Avg Loss: 0.0157 | Grad Norm: 0.00959583\n",
      "Epoch 2 | Step 823200 | Avg Loss: 0.0161 | Grad Norm: 0.00859255\n",
      "Epoch 2 | Step 823300 | Avg Loss: 0.0162 | Grad Norm: 0.00897648\n",
      "Epoch 2 | Step 823400 | Avg Loss: 0.0163 | Grad Norm: 0.00934750\n",
      "Epoch 2 | Step 823500 | Avg Loss: 0.0163 | Grad Norm: 0.00859834\n",
      "Epoch 2 | Step 823600 | Avg Loss: 0.0164 | Grad Norm: 0.01034147\n",
      "Epoch 2 | Step 823700 | Avg Loss: 0.0165 | Grad Norm: 0.00789880\n",
      "Epoch 2 | Step 823800 | Avg Loss: 0.0163 | Grad Norm: 0.00945515\n",
      "Epoch 2 | Step 823900 | Avg Loss: 0.0158 | Grad Norm: 0.00990783\n",
      "Epoch 2 | Step 824000 | Avg Loss: 0.0156 | Grad Norm: 0.00800050\n",
      "Epoch 2 | Step 824100 | Avg Loss: 0.0155 | Grad Norm: 0.00800395\n",
      "Epoch 2 | Step 824200 | Avg Loss: 0.0153 | Grad Norm: 0.00790533\n",
      "Epoch 2 | Step 824300 | Avg Loss: 0.0155 | Grad Norm: 0.00886302\n",
      "Epoch 2 | Step 824400 | Avg Loss: 0.0158 | Grad Norm: 0.00789986\n",
      "Epoch 2 | Step 824500 | Avg Loss: 0.0158 | Grad Norm: 0.00787752\n",
      "Epoch 2 | Step 824600 | Avg Loss: 0.0156 | Grad Norm: 0.00937162\n",
      "Epoch 2 | Step 824700 | Avg Loss: 0.0159 | Grad Norm: 0.00813629\n",
      "Epoch 2 | Step 824800 | Avg Loss: 0.0158 | Grad Norm: 0.00861345\n",
      "Epoch 2 | Step 824900 | Avg Loss: 0.0157 | Grad Norm: 0.00736972\n",
      "Epoch 2 | Step 825000 | Avg Loss: 0.0154 | Grad Norm: 0.00893480\n",
      "Epoch 2 | Step 825100 | Avg Loss: 0.0158 | Grad Norm: 0.00816140\n",
      "Epoch 2 | Step 825200 | Avg Loss: 0.0155 | Grad Norm: 0.00869530\n",
      "Epoch 2 | Step 825300 | Avg Loss: 0.0156 | Grad Norm: 0.00782941\n",
      "Epoch 2 | Step 825400 | Avg Loss: 0.0155 | Grad Norm: 0.01011448\n",
      "Epoch 2 | Step 825500 | Avg Loss: 0.0159 | Grad Norm: 0.01053179\n",
      "Epoch 2 | Step 825600 | Avg Loss: 0.0162 | Grad Norm: 0.00993495\n",
      "Epoch 2 | Step 825700 | Avg Loss: 0.0160 | Grad Norm: 0.01468239\n",
      "Epoch 2 | Step 825800 | Avg Loss: 0.0159 | Grad Norm: 0.00868920\n",
      "Epoch 2 | Step 825900 | Avg Loss: 0.0162 | Grad Norm: 0.01069018\n",
      "Epoch 2 | Step 826000 | Avg Loss: 0.0166 | Grad Norm: 0.01020367\n",
      "Epoch 2 | Step 826100 | Avg Loss: 0.0161 | Grad Norm: 0.00939862\n",
      "Epoch 2 | Step 826200 | Avg Loss: 0.0159 | Grad Norm: 0.01028719\n",
      "Epoch 2 | Step 826300 | Avg Loss: 0.0158 | Grad Norm: 0.00887482\n",
      "Epoch 2 | Step 826400 | Avg Loss: 0.0154 | Grad Norm: 0.00963263\n",
      "Epoch 2 | Step 826500 | Avg Loss: 0.0154 | Grad Norm: 0.00932482\n",
      "Epoch 2 | Step 826600 | Avg Loss: 0.0154 | Grad Norm: 0.01054599\n",
      "Epoch 2 | Step 826700 | Avg Loss: 0.0154 | Grad Norm: 0.00771000\n",
      "Epoch 2 | Step 826800 | Avg Loss: 0.0155 | Grad Norm: 0.00849025\n",
      "Epoch 2 | Step 826900 | Avg Loss: 0.0157 | Grad Norm: 0.00773036\n",
      "Epoch 2 | Step 827000 | Avg Loss: 0.0157 | Grad Norm: 0.00886510\n",
      "Epoch 2 | Step 827100 | Avg Loss: 0.0155 | Grad Norm: 0.00935053\n",
      "Epoch 2 | Step 827200 | Avg Loss: 0.0153 | Grad Norm: 0.01043159\n",
      "Epoch 2 | Step 827300 | Avg Loss: 0.0151 | Grad Norm: 0.00793506\n",
      "Epoch 2 | Step 827400 | Avg Loss: 0.0152 | Grad Norm: 0.00995231\n",
      "Epoch 2 | Step 827500 | Avg Loss: 0.0151 | Grad Norm: 0.00809846\n",
      "Epoch 2 | Step 827600 | Avg Loss: 0.0155 | Grad Norm: 0.00838772\n",
      "Epoch 2 | Step 827700 | Avg Loss: 0.0154 | Grad Norm: 0.01040229\n",
      "Epoch 2 | Step 827800 | Avg Loss: 0.0151 | Grad Norm: 0.00797088\n",
      "Epoch 2 | Step 827900 | Avg Loss: 0.0152 | Grad Norm: 0.00934177\n",
      "Epoch 2 | Step 828000 | Avg Loss: 0.0153 | Grad Norm: 0.00900244\n",
      "Epoch 2 | Step 828100 | Avg Loss: 0.0154 | Grad Norm: 0.00858729\n",
      "Epoch 2 | Step 828200 | Avg Loss: 0.0150 | Grad Norm: 0.00876092\n",
      "Epoch 2 | Step 828300 | Avg Loss: 0.0156 | Grad Norm: 0.01024034\n",
      "Epoch 2 | Step 828400 | Avg Loss: 0.0155 | Grad Norm: 0.00891744\n",
      "Epoch 2 | Step 828500 | Avg Loss: 0.0155 | Grad Norm: 0.00864561\n",
      "Epoch 2 | Step 828600 | Avg Loss: 0.0158 | Grad Norm: 0.00929635\n",
      "Epoch 2 | Step 828700 | Avg Loss: 0.0160 | Grad Norm: 0.01027032\n",
      "Epoch 2 | Step 828800 | Avg Loss: 0.0158 | Grad Norm: 0.00924702\n",
      "Epoch 2 | Step 828900 | Avg Loss: 0.0163 | Grad Norm: 0.00963034\n",
      "Epoch 2 | Step 829000 | Avg Loss: 0.0167 | Grad Norm: 0.00852042\n",
      "Epoch 2 | Step 829100 | Avg Loss: 0.0160 | Grad Norm: 0.00960962\n",
      "Epoch 2 | Step 829200 | Avg Loss: 0.0162 | Grad Norm: 0.00864317\n",
      "Epoch 2 | Step 829300 | Avg Loss: 0.0158 | Grad Norm: 0.00848648\n",
      "Epoch 2 | Step 829400 | Avg Loss: 0.0159 | Grad Norm: 0.00945601\n",
      "Epoch 2 | Step 829500 | Avg Loss: 0.0155 | Grad Norm: 0.00802681\n",
      "Epoch 2 | Step 829600 | Avg Loss: 0.0154 | Grad Norm: 0.00873888\n",
      "Epoch 2 | Step 829700 | Avg Loss: 0.0155 | Grad Norm: 0.00986640\n",
      "Epoch 2 | Step 829800 | Avg Loss: 0.0159 | Grad Norm: 0.00869602\n",
      "Epoch 2 | Step 829900 | Avg Loss: 0.0157 | Grad Norm: 0.00711427\n",
      "Epoch 2 | Step 830000 | Avg Loss: 0.0157 | Grad Norm: 0.00874944\n",
      "Epoch 2 | Step 830100 | Avg Loss: 0.0153 | Grad Norm: 0.00943826\n",
      "Epoch 2 | Step 830200 | Avg Loss: 0.0157 | Grad Norm: 0.01034870\n",
      "Epoch 2 | Step 830300 | Avg Loss: 0.0155 | Grad Norm: 0.00816459\n",
      "Epoch 2 | Step 830400 | Avg Loss: 0.0155 | Grad Norm: 0.00821143\n",
      "Epoch 2 | Step 830500 | Avg Loss: 0.0156 | Grad Norm: 0.00957748\n",
      "Epoch 2 | Step 830600 | Avg Loss: 0.0161 | Grad Norm: 0.00914198\n",
      "Epoch 2 | Step 830700 | Avg Loss: 0.0162 | Grad Norm: 0.00838643\n",
      "Epoch 2 | Step 830800 | Avg Loss: 0.0159 | Grad Norm: 0.00911874\n",
      "Epoch 2 | Step 830900 | Avg Loss: 0.0160 | Grad Norm: 0.00823561\n",
      "Epoch 2 | Step 831000 | Avg Loss: 0.0159 | Grad Norm: 0.00803991\n",
      "Epoch 2 | Step 831100 | Avg Loss: 0.0156 | Grad Norm: 0.00956175\n",
      "Epoch 2 | Step 831200 | Avg Loss: 0.0156 | Grad Norm: 0.00883487\n",
      "Epoch 2 | Step 831300 | Avg Loss: 0.0156 | Grad Norm: 0.01076651\n",
      "Epoch 2 | Step 831400 | Avg Loss: 0.0158 | Grad Norm: 0.00826851\n",
      "Epoch 2 | Step 831500 | Avg Loss: 0.0158 | Grad Norm: 0.01153565\n",
      "Epoch 2 | Step 831600 | Avg Loss: 0.0156 | Grad Norm: 0.00893088\n",
      "Epoch 2 | Step 831700 | Avg Loss: 0.0159 | Grad Norm: 0.00734454\n",
      "Epoch 2 | Step 831800 | Avg Loss: 0.0156 | Grad Norm: 0.00777878\n",
      "Epoch 2 | Step 831900 | Avg Loss: 0.0155 | Grad Norm: 0.00886810\n",
      "Epoch 2 | Step 832000 | Avg Loss: 0.0153 | Grad Norm: 0.00776630\n",
      "Epoch 2 | Step 832100 | Avg Loss: 0.0157 | Grad Norm: 0.00941502\n",
      "Epoch 2 | Step 832200 | Avg Loss: 0.0156 | Grad Norm: 0.00964524\n",
      "Epoch 2 | Step 832300 | Avg Loss: 0.0159 | Grad Norm: 0.00852092\n",
      "Epoch 2 | Step 832400 | Avg Loss: 0.0159 | Grad Norm: 0.00781413\n",
      "Epoch 2 | Step 832500 | Avg Loss: 0.0158 | Grad Norm: 0.00926829\n",
      "Epoch 2 | Step 832600 | Avg Loss: 0.0158 | Grad Norm: 0.00853368\n",
      "Epoch 2 | Step 832700 | Avg Loss: 0.0157 | Grad Norm: 0.00978751\n",
      "Epoch 2 | Step 832800 | Avg Loss: 0.0156 | Grad Norm: 0.00991620\n",
      "Epoch 2 | Step 832900 | Avg Loss: 0.0158 | Grad Norm: 0.01116063\n",
      "Epoch 2 | Step 833000 | Avg Loss: 0.0158 | Grad Norm: 0.00938480\n",
      "Epoch 2 | Step 833100 | Avg Loss: 0.0159 | Grad Norm: 0.00897748\n",
      "Epoch 2 | Step 833200 | Avg Loss: 0.0160 | Grad Norm: 0.00964803\n",
      "Epoch 2 | Step 833300 | Avg Loss: 0.0158 | Grad Norm: 0.00739617\n",
      "Epoch 2 | Step 833400 | Avg Loss: 0.0160 | Grad Norm: 0.00931022\n",
      "Epoch 2 | Step 833500 | Avg Loss: 0.0160 | Grad Norm: 0.00989820\n",
      "Epoch 2 | Step 833600 | Avg Loss: 0.0156 | Grad Norm: 0.00779371\n",
      "Epoch 2 | Step 833700 | Avg Loss: 0.0156 | Grad Norm: 0.01123110\n",
      "Epoch 2 | Step 833800 | Avg Loss: 0.0156 | Grad Norm: 0.00803570\n",
      "Epoch 2 | Step 833900 | Avg Loss: 0.0155 | Grad Norm: 0.00975134\n",
      "Epoch 2 | Step 834000 | Avg Loss: 0.0153 | Grad Norm: 0.00902367\n",
      "Epoch 2 | Step 834100 | Avg Loss: 0.0153 | Grad Norm: 0.00866972\n",
      "Epoch 2 | Step 834200 | Avg Loss: 0.0152 | Grad Norm: 0.00737346\n",
      "Epoch 2 | Step 834300 | Avg Loss: 0.0152 | Grad Norm: 0.00848376\n",
      "Epoch 2 | Step 834400 | Avg Loss: 0.0151 | Grad Norm: 0.01009710\n",
      "Epoch 2 | Step 834500 | Avg Loss: 0.0152 | Grad Norm: 0.00935375\n",
      "Epoch 2 | Step 834600 | Avg Loss: 0.0156 | Grad Norm: 0.00784266\n",
      "Epoch 2 | Step 834700 | Avg Loss: 0.0157 | Grad Norm: 0.00912199\n",
      "Epoch 2 | Step 834800 | Avg Loss: 0.0156 | Grad Norm: 0.00820637\n",
      "Epoch 2 | Step 834900 | Avg Loss: 0.0155 | Grad Norm: 0.00745641\n",
      "Epoch 2 | Step 835000 | Avg Loss: 0.0156 | Grad Norm: 0.00703907\n",
      "Epoch 2 | Step 835100 | Avg Loss: 0.0160 | Grad Norm: 0.00894675\n",
      "Epoch 2 | Step 835200 | Avg Loss: 0.0159 | Grad Norm: 0.00799985\n",
      "Epoch 2 | Step 835300 | Avg Loss: 0.0163 | Grad Norm: 0.00945478\n",
      "Epoch 2 | Step 835400 | Avg Loss: 0.0161 | Grad Norm: 0.00791121\n",
      "Epoch 2 | Step 835500 | Avg Loss: 0.0163 | Grad Norm: 0.01044159\n",
      "Epoch 2 | Step 835600 | Avg Loss: 0.0161 | Grad Norm: 0.00992205\n",
      "Epoch 2 | Step 835700 | Avg Loss: 0.0163 | Grad Norm: 0.01046114\n",
      "Epoch 2 | Step 835800 | Avg Loss: 0.0162 | Grad Norm: 0.00911453\n",
      "Epoch 2 | Step 835900 | Avg Loss: 0.0162 | Grad Norm: 0.00918400\n",
      "Epoch 2 | Step 836000 | Avg Loss: 0.0161 | Grad Norm: 0.00913022\n",
      "Epoch 2 | Step 836100 | Avg Loss: 0.0161 | Grad Norm: 0.00859188\n",
      "Epoch 2 | Step 836200 | Avg Loss: 0.0163 | Grad Norm: 0.01039381\n",
      "Epoch 2 | Step 836300 | Avg Loss: 0.0163 | Grad Norm: 0.00887021\n",
      "Epoch 2 | Step 836400 | Avg Loss: 0.0164 | Grad Norm: 0.00937454\n",
      "Epoch 2 | Step 836500 | Avg Loss: 0.0161 | Grad Norm: 0.00936748\n",
      "Epoch 2 | Step 836600 | Avg Loss: 0.0158 | Grad Norm: 0.00799773\n",
      "Epoch 2 | Step 836700 | Avg Loss: 0.0159 | Grad Norm: 0.00940786\n",
      "Epoch 2 | Step 836800 | Avg Loss: 0.0160 | Grad Norm: 0.00874733\n",
      "Epoch 2 | Step 836900 | Avg Loss: 0.0157 | Grad Norm: 0.00840177\n",
      "Epoch 2 | Step 837000 | Avg Loss: 0.0158 | Grad Norm: 0.01123615\n",
      "Epoch 2 | Step 837100 | Avg Loss: 0.0158 | Grad Norm: 0.00742255\n",
      "Epoch 2 | Step 837200 | Avg Loss: 0.0158 | Grad Norm: 0.00955139\n",
      "Epoch 2 | Step 837300 | Avg Loss: 0.0161 | Grad Norm: 0.00837031\n",
      "Epoch 2 | Step 837400 | Avg Loss: 0.0163 | Grad Norm: 0.00919806\n",
      "Epoch 2 | Step 837500 | Avg Loss: 0.0161 | Grad Norm: 0.00967790\n",
      "Epoch 2 | Step 837600 | Avg Loss: 0.0161 | Grad Norm: 0.00853069\n",
      "Epoch 2 | Step 837700 | Avg Loss: 0.0161 | Grad Norm: 0.00789216\n",
      "Epoch 2 | Step 837800 | Avg Loss: 0.0161 | Grad Norm: 0.01031413\n",
      "Epoch 2 | Step 837900 | Avg Loss: 0.0162 | Grad Norm: 0.01024738\n",
      "Epoch 2 | Step 838000 | Avg Loss: 0.0164 | Grad Norm: 0.01170848\n",
      "Epoch 2 | Step 838100 | Avg Loss: 0.0168 | Grad Norm: 0.00926425\n",
      "Epoch 2 | Step 838200 | Avg Loss: 0.0165 | Grad Norm: 0.01276876\n",
      "Epoch 2 | Step 838300 | Avg Loss: 0.0160 | Grad Norm: 0.00958495\n",
      "Epoch 2 | Step 838400 | Avg Loss: 0.0158 | Grad Norm: 0.00877266\n",
      "Epoch 2 | Step 838500 | Avg Loss: 0.0159 | Grad Norm: 0.00858443\n",
      "Epoch 2 | Step 838600 | Avg Loss: 0.0156 | Grad Norm: 0.00886315\n",
      "Epoch 2 | Step 838700 | Avg Loss: 0.0155 | Grad Norm: 0.00768540\n",
      "Epoch 2 | Step 838800 | Avg Loss: 0.0152 | Grad Norm: 0.00854997\n",
      "Epoch 2 | Step 838900 | Avg Loss: 0.0154 | Grad Norm: 0.01103401\n",
      "Epoch 2 | Step 839000 | Avg Loss: 0.0155 | Grad Norm: 0.00811635\n",
      "Epoch 2 | Step 839100 | Avg Loss: 0.0153 | Grad Norm: 0.00815752\n",
      "Epoch 2 | Step 839200 | Avg Loss: 0.0154 | Grad Norm: 0.00978114\n",
      "Epoch 2 | Step 839300 | Avg Loss: 0.0153 | Grad Norm: 0.00937979\n",
      "Epoch 2 | Step 839400 | Avg Loss: 0.0155 | Grad Norm: 0.00861041\n",
      "Epoch 2 | Step 839500 | Avg Loss: 0.0157 | Grad Norm: 0.00858008\n",
      "Epoch 2 | Step 839600 | Avg Loss: 0.0158 | Grad Norm: 0.00961794\n",
      "Epoch 2 | Step 839700 | Avg Loss: 0.0159 | Grad Norm: 0.00837252\n",
      "Epoch 2 | Step 839800 | Avg Loss: 0.0158 | Grad Norm: 0.00930012\n",
      "Epoch 2 | Step 839900 | Avg Loss: 0.0153 | Grad Norm: 0.00781340\n",
      "Epoch 2 | Step 840000 | Avg Loss: 0.0155 | Grad Norm: 0.00837105\n",
      "Epoch 2 | Step 840100 | Avg Loss: 0.0157 | Grad Norm: 0.00882815\n",
      "Epoch 2 | Step 840200 | Avg Loss: 0.0156 | Grad Norm: 0.00838610\n",
      "Epoch 2 | Step 840300 | Avg Loss: 0.0158 | Grad Norm: 0.00998566\n",
      "Epoch 2 | Step 840400 | Avg Loss: 0.0158 | Grad Norm: 0.00894103\n",
      "Epoch 2 | Step 840500 | Avg Loss: 0.0161 | Grad Norm: 0.00892385\n",
      "Epoch 2 | Step 840600 | Avg Loss: 0.0156 | Grad Norm: 0.00947202\n",
      "Epoch 2 | Step 840700 | Avg Loss: 0.0156 | Grad Norm: 0.00978300\n",
      "Epoch 2 | Step 840800 | Avg Loss: 0.0156 | Grad Norm: 0.00824988\n",
      "Epoch 2 | Step 840900 | Avg Loss: 0.0155 | Grad Norm: 0.00825436\n",
      "Epoch 2 | Step 841000 | Avg Loss: 0.0151 | Grad Norm: 0.00946804\n",
      "Epoch 2 | Step 841100 | Avg Loss: 0.0153 | Grad Norm: 0.00772870\n",
      "Epoch 2 | Step 841200 | Avg Loss: 0.0155 | Grad Norm: 0.00868838\n",
      "Epoch 2 | Step 841300 | Avg Loss: 0.0157 | Grad Norm: 0.00936437\n",
      "Epoch 2 | Step 841400 | Avg Loss: 0.0160 | Grad Norm: 0.00793573\n",
      "Epoch 2 | Step 841500 | Avg Loss: 0.0155 | Grad Norm: 0.00868778\n",
      "Epoch 2 | Step 841600 | Avg Loss: 0.0157 | Grad Norm: 0.00887062\n",
      "Epoch 2 | Step 841700 | Avg Loss: 0.0156 | Grad Norm: 0.00849995\n",
      "Epoch 2 | Step 841800 | Avg Loss: 0.0156 | Grad Norm: 0.00960262\n",
      "Epoch 2 | Step 841900 | Avg Loss: 0.0154 | Grad Norm: 0.00815406\n",
      "Epoch 2 | Step 842000 | Avg Loss: 0.0152 | Grad Norm: 0.00776749\n",
      "Epoch 2 | Step 842100 | Avg Loss: 0.0154 | Grad Norm: 0.00887808\n",
      "Epoch 2 | Step 842200 | Avg Loss: 0.0157 | Grad Norm: 0.00965039\n",
      "Epoch 2 | Step 842300 | Avg Loss: 0.0157 | Grad Norm: 0.00990743\n",
      "Epoch 2 | Step 842400 | Avg Loss: 0.0155 | Grad Norm: 0.00889738\n",
      "Epoch 2 | Step 842500 | Avg Loss: 0.0156 | Grad Norm: 0.01043384\n",
      "Epoch 2 | Step 842600 | Avg Loss: 0.0157 | Grad Norm: 0.01063808\n",
      "Epoch 2 | Step 842700 | Avg Loss: 0.0155 | Grad Norm: 0.00756206\n",
      "Epoch 2 | Step 842800 | Avg Loss: 0.0156 | Grad Norm: 0.00757417\n",
      "Epoch 2 | Step 842900 | Avg Loss: 0.0152 | Grad Norm: 0.00843738\n",
      "Epoch 2 | Step 843000 | Avg Loss: 0.0154 | Grad Norm: 0.00928393\n",
      "Epoch 2 | Step 843100 | Avg Loss: 0.0156 | Grad Norm: 0.01033445\n",
      "Epoch 2 | Step 843200 | Avg Loss: 0.0159 | Grad Norm: 0.00976081\n",
      "Epoch 2 | Step 843300 | Avg Loss: 0.0164 | Grad Norm: 0.00922977\n",
      "Epoch 2 | Step 843400 | Avg Loss: 0.0165 | Grad Norm: 0.01005291\n",
      "Epoch 2 | Step 843500 | Avg Loss: 0.0158 | Grad Norm: 0.00768849\n",
      "Epoch 2 | Step 843600 | Avg Loss: 0.0154 | Grad Norm: 0.01043432\n",
      "Epoch 2 | Step 843700 | Avg Loss: 0.0150 | Grad Norm: 0.00790606\n",
      "Epoch 2 | Step 843800 | Avg Loss: 0.0154 | Grad Norm: 0.00984140\n",
      "Epoch 2 | Step 843900 | Avg Loss: 0.0155 | Grad Norm: 0.00930080\n",
      "Epoch 2 | Step 844000 | Avg Loss: 0.0158 | Grad Norm: 0.00893723\n",
      "Epoch 2 | Step 844100 | Avg Loss: 0.0157 | Grad Norm: 0.00836412\n",
      "Epoch 2 | Step 844200 | Avg Loss: 0.0157 | Grad Norm: 0.00889289\n",
      "Epoch 2 | Step 844300 | Avg Loss: 0.0162 | Grad Norm: 0.00964476\n",
      "Epoch 2 | Step 844400 | Avg Loss: 0.0161 | Grad Norm: 0.00971224\n",
      "Epoch 2 | Step 844500 | Avg Loss: 0.0159 | Grad Norm: 0.00955240\n",
      "Epoch 2 | Step 844600 | Avg Loss: 0.0159 | Grad Norm: 0.00924101\n",
      "Epoch 2 | Step 844700 | Avg Loss: 0.0157 | Grad Norm: 0.00855238\n",
      "Epoch 2 | Step 844800 | Avg Loss: 0.0151 | Grad Norm: 0.00888934\n",
      "Epoch 2 | Step 844900 | Avg Loss: 0.0149 | Grad Norm: 0.00795196\n",
      "Epoch 2 | Step 845000 | Avg Loss: 0.0150 | Grad Norm: 0.00803304\n",
      "Epoch 2 | Step 845100 | Avg Loss: 0.0152 | Grad Norm: 0.00921258\n",
      "Epoch 2 | Step 845200 | Avg Loss: 0.0151 | Grad Norm: 0.00894073\n",
      "Epoch 2 | Step 845300 | Avg Loss: 0.0150 | Grad Norm: 0.00939236\n",
      "Epoch 2 | Step 845400 | Avg Loss: 0.0151 | Grad Norm: 0.00890712\n",
      "Epoch 2 | Step 845500 | Avg Loss: 0.0153 | Grad Norm: 0.00887936\n",
      "Epoch 2 | Step 845600 | Avg Loss: 0.0155 | Grad Norm: 0.00892720\n",
      "Epoch 2 | Step 845700 | Avg Loss: 0.0160 | Grad Norm: 0.00899857\n",
      "Epoch 2 | Step 845800 | Avg Loss: 0.0158 | Grad Norm: 0.01100935\n",
      "Epoch 2 | Step 845900 | Avg Loss: 0.0158 | Grad Norm: 0.01097313\n",
      "Epoch 2 | Step 846000 | Avg Loss: 0.0158 | Grad Norm: 0.00888969\n",
      "Epoch 2 | Step 846100 | Avg Loss: 0.0157 | Grad Norm: 0.00854252\n",
      "Epoch 2 | Step 846200 | Avg Loss: 0.0156 | Grad Norm: 0.00816659\n",
      "Epoch 2 | Step 846300 | Avg Loss: 0.0156 | Grad Norm: 0.00829927\n",
      "Epoch 2 | Step 846400 | Avg Loss: 0.0155 | Grad Norm: 0.00916934\n",
      "Epoch 2 | Step 846500 | Avg Loss: 0.0153 | Grad Norm: 0.00781951\n",
      "Epoch 2 | Step 846600 | Avg Loss: 0.0152 | Grad Norm: 0.00915369\n",
      "Epoch 2 | Step 846700 | Avg Loss: 0.0149 | Grad Norm: 0.00796243\n",
      "Epoch 2 | Step 846800 | Avg Loss: 0.0152 | Grad Norm: 0.00887634\n",
      "Epoch 2 | Step 846900 | Avg Loss: 0.0151 | Grad Norm: 0.00895686\n",
      "Epoch 2 | Step 847000 | Avg Loss: 0.0150 | Grad Norm: 0.01041762\n",
      "Epoch 2 | Step 847100 | Avg Loss: 0.0149 | Grad Norm: 0.00818526\n",
      "Epoch 2 | Step 847200 | Avg Loss: 0.0152 | Grad Norm: 0.00906648\n",
      "Epoch 2 | Step 847300 | Avg Loss: 0.0155 | Grad Norm: 0.00883113\n",
      "Epoch 2 | Step 847400 | Avg Loss: 0.0153 | Grad Norm: 0.01129026\n",
      "Epoch 2 | Step 847500 | Avg Loss: 0.0155 | Grad Norm: 0.01046957\n",
      "Epoch 2 | Step 847600 | Avg Loss: 0.0154 | Grad Norm: 0.00799467\n",
      "Epoch 2 | Step 847700 | Avg Loss: 0.0159 | Grad Norm: 0.00947315\n",
      "Epoch 2 | Step 847800 | Avg Loss: 0.0161 | Grad Norm: 0.00883574\n",
      "Epoch 2 | Step 847900 | Avg Loss: 0.0159 | Grad Norm: 0.00928327\n",
      "Epoch 2 | Step 848000 | Avg Loss: 0.0154 | Grad Norm: 0.00783091\n",
      "Epoch 2 | Step 848100 | Avg Loss: 0.0150 | Grad Norm: 0.00880603\n",
      "Epoch 2 | Step 848200 | Avg Loss: 0.0156 | Grad Norm: 0.00851370\n",
      "Epoch 2 | Step 848300 | Avg Loss: 0.0155 | Grad Norm: 0.01059240\n",
      "Epoch 2 | Step 848400 | Avg Loss: 0.0154 | Grad Norm: 0.00787533\n",
      "Epoch 2 | Step 848500 | Avg Loss: 0.0156 | Grad Norm: 0.00852583\n",
      "Epoch 2 | Step 848600 | Avg Loss: 0.0154 | Grad Norm: 0.00644625\n",
      "Epoch 2 | Step 848700 | Avg Loss: 0.0156 | Grad Norm: 0.00817668\n",
      "Epoch 2 | Step 848800 | Avg Loss: 0.0161 | Grad Norm: 0.00886393\n",
      "Epoch 2 | Step 848900 | Avg Loss: 0.0162 | Grad Norm: 0.01002309\n",
      "Epoch 2 | Step 849000 | Avg Loss: 0.0161 | Grad Norm: 0.00797308\n",
      "Epoch 2 | Step 849100 | Avg Loss: 0.0159 | Grad Norm: 0.00892400\n",
      "Epoch 2 | Step 849200 | Avg Loss: 0.0152 | Grad Norm: 0.00733310\n",
      "Epoch 2 | Step 849300 | Avg Loss: 0.0153 | Grad Norm: 0.00980554\n",
      "Epoch 2 | Step 849400 | Avg Loss: 0.0155 | Grad Norm: 0.00934951\n",
      "Epoch 2 | Step 849500 | Avg Loss: 0.0155 | Grad Norm: 0.00868621\n",
      "Epoch 2 | Step 849600 | Avg Loss: 0.0159 | Grad Norm: 0.00810910\n",
      "Epoch 2 | Step 849700 | Avg Loss: 0.0157 | Grad Norm: 0.01032128\n",
      "Epoch 2 | Step 849800 | Avg Loss: 0.0156 | Grad Norm: 0.00927263\n",
      "Epoch 2 | Step 849900 | Avg Loss: 0.0154 | Grad Norm: 0.00703280\n",
      "Epoch 2 | Step 850000 | Avg Loss: 0.0157 | Grad Norm: 0.00974518\n",
      "Epoch 2 | Step 850100 | Avg Loss: 0.0156 | Grad Norm: 0.01308017\n",
      "Epoch 2 | Step 850200 | Avg Loss: 0.0164 | Grad Norm: 0.00844586\n",
      "Epoch 2 | Step 850300 | Avg Loss: 0.0162 | Grad Norm: 0.00829925\n",
      "Epoch 2 | Step 850400 | Avg Loss: 0.0159 | Grad Norm: 0.00863071\n",
      "Epoch 2 | Step 850500 | Avg Loss: 0.0156 | Grad Norm: 0.00820154\n",
      "Epoch 2 | Step 850600 | Avg Loss: 0.0156 | Grad Norm: 0.00821100\n",
      "Epoch 2 | Step 850700 | Avg Loss: 0.0157 | Grad Norm: 0.00789454\n",
      "Epoch 2 | Step 850800 | Avg Loss: 0.0158 | Grad Norm: 0.00826642\n",
      "Epoch 2 | Step 850900 | Avg Loss: 0.0155 | Grad Norm: 0.01209321\n",
      "Epoch 2 | Step 851000 | Avg Loss: 0.0156 | Grad Norm: 0.00926854\n",
      "Epoch 2 | Step 851100 | Avg Loss: 0.0156 | Grad Norm: 0.00945754\n",
      "Epoch 2 | Step 851200 | Avg Loss: 0.0158 | Grad Norm: 0.00830045\n",
      "Epoch 2 | Step 851300 | Avg Loss: 0.0161 | Grad Norm: 0.00976805\n",
      "Epoch 2 | Step 851400 | Avg Loss: 0.0161 | Grad Norm: 0.00871737\n",
      "Epoch 2 | Step 851500 | Avg Loss: 0.0157 | Grad Norm: 0.00718327\n",
      "Epoch 2 | Step 851600 | Avg Loss: 0.0155 | Grad Norm: 0.00777300\n",
      "Epoch 2 | Step 851700 | Avg Loss: 0.0156 | Grad Norm: 0.00994134\n",
      "Epoch 2 | Step 851800 | Avg Loss: 0.0155 | Grad Norm: 0.00954331\n",
      "Epoch 2 | Step 851900 | Avg Loss: 0.0157 | Grad Norm: 0.01062376\n",
      "Epoch 2 | Step 852000 | Avg Loss: 0.0154 | Grad Norm: 0.00808683\n",
      "Epoch 2 | Step 852100 | Avg Loss: 0.0153 | Grad Norm: 0.00878762\n",
      "Epoch 2 | Step 852200 | Avg Loss: 0.0155 | Grad Norm: 0.01073462\n",
      "Epoch 2 | Step 852300 | Avg Loss: 0.0157 | Grad Norm: 0.00873765\n",
      "Epoch 2 | Step 852400 | Avg Loss: 0.0157 | Grad Norm: 0.00820587\n",
      "Epoch 2 | Step 852500 | Avg Loss: 0.0159 | Grad Norm: 0.00964142\n",
      "Epoch 2 | Step 852600 | Avg Loss: 0.0157 | Grad Norm: 0.00973623\n",
      "Epoch 2 | Step 852700 | Avg Loss: 0.0155 | Grad Norm: 0.00842419\n",
      "Epoch 2 | Step 852800 | Avg Loss: 0.0155 | Grad Norm: 0.00897388\n",
      "Epoch 2 | Step 852900 | Avg Loss: 0.0158 | Grad Norm: 0.00829490\n",
      "Epoch 2 | Step 853000 | Avg Loss: 0.0157 | Grad Norm: 0.00858898\n",
      "Epoch 2 | Step 853100 | Avg Loss: 0.0155 | Grad Norm: 0.00960967\n",
      "Epoch 2 | Step 853200 | Avg Loss: 0.0157 | Grad Norm: 0.01104455\n",
      "Epoch 2 | Step 853300 | Avg Loss: 0.0160 | Grad Norm: 0.00911613\n",
      "Epoch 2 | Step 853400 | Avg Loss: 0.0157 | Grad Norm: 0.00800367\n",
      "Epoch 2 | Step 853500 | Avg Loss: 0.0159 | Grad Norm: 0.00847557\n",
      "Epoch 2 | Step 853600 | Avg Loss: 0.0155 | Grad Norm: 0.01083349\n",
      "Epoch 2 | Step 853700 | Avg Loss: 0.0157 | Grad Norm: 0.00859392\n",
      "Epoch 2 | Step 853800 | Avg Loss: 0.0157 | Grad Norm: 0.00899279\n",
      "Epoch 2 | Step 853900 | Avg Loss: 0.0159 | Grad Norm: 0.01239031\n",
      "Epoch 2 | Step 854000 | Avg Loss: 0.0154 | Grad Norm: 0.00883198\n",
      "Epoch 2 | Step 854100 | Avg Loss: 0.0156 | Grad Norm: 0.00907586\n",
      "Epoch 2 | Step 854200 | Avg Loss: 0.0162 | Grad Norm: 0.00966958\n",
      "Epoch 2 | Step 854300 | Avg Loss: 0.0162 | Grad Norm: 0.00893191\n",
      "Epoch 2 | Step 854400 | Avg Loss: 0.0162 | Grad Norm: 0.00894092\n",
      "Epoch 2 | Step 854500 | Avg Loss: 0.0160 | Grad Norm: 0.00862912\n",
      "Epoch 2 | Step 854600 | Avg Loss: 0.0160 | Grad Norm: 0.00953398\n",
      "Epoch 2 | Step 854700 | Avg Loss: 0.0156 | Grad Norm: 0.00866894\n",
      "Epoch 2 | Step 854800 | Avg Loss: 0.0155 | Grad Norm: 0.00894601\n",
      "Epoch 2 | Step 854900 | Avg Loss: 0.0155 | Grad Norm: 0.00808831\n",
      "Epoch 2 | Step 855000 | Avg Loss: 0.0154 | Grad Norm: 0.00847784\n",
      "Epoch 2 | Step 855100 | Avg Loss: 0.0154 | Grad Norm: 0.00960962\n",
      "Epoch 2 | Step 855200 | Avg Loss: 0.0150 | Grad Norm: 0.00918024\n",
      "Epoch 2 | Step 855300 | Avg Loss: 0.0155 | Grad Norm: 0.00917162\n",
      "Epoch 2 | Step 855400 | Avg Loss: 0.0154 | Grad Norm: 0.00989444\n",
      "Epoch 2 | Step 855500 | Avg Loss: 0.0151 | Grad Norm: 0.00846797\n",
      "Epoch 2 | Step 855600 | Avg Loss: 0.0153 | Grad Norm: 0.00853699\n",
      "Epoch 2 | Step 855700 | Avg Loss: 0.0157 | Grad Norm: 0.00899972\n",
      "Epoch 2 | Step 855800 | Avg Loss: 0.0159 | Grad Norm: 0.00837808\n",
      "Epoch 2 | Step 855900 | Avg Loss: 0.0161 | Grad Norm: 0.00842738\n",
      "Epoch 2 | Step 856000 | Avg Loss: 0.0156 | Grad Norm: 0.00859812\n",
      "Epoch 2 | Step 856100 | Avg Loss: 0.0157 | Grad Norm: 0.01021104\n",
      "Epoch 2 | Step 856200 | Avg Loss: 0.0160 | Grad Norm: 0.00916912\n",
      "Epoch 2 | Step 856300 | Avg Loss: 0.0160 | Grad Norm: 0.00884445\n",
      "Epoch 2 | Step 856400 | Avg Loss: 0.0159 | Grad Norm: 0.00807096\n",
      "Epoch 2 | Step 856500 | Avg Loss: 0.0155 | Grad Norm: 0.01028349\n",
      "Epoch 2 | Step 856600 | Avg Loss: 0.0155 | Grad Norm: 0.00848947\n",
      "Epoch 2 | Step 856700 | Avg Loss: 0.0155 | Grad Norm: 0.01000419\n",
      "Epoch 2 | Step 856800 | Avg Loss: 0.0153 | Grad Norm: 0.00855519\n",
      "Epoch 2 | Step 856900 | Avg Loss: 0.0155 | Grad Norm: 0.00949797\n",
      "Epoch 2 | Step 857000 | Avg Loss: 0.0156 | Grad Norm: 0.00911152\n",
      "Epoch 2 | Step 857100 | Avg Loss: 0.0157 | Grad Norm: 0.00858540\n",
      "Epoch 2 | Step 857200 | Avg Loss: 0.0160 | Grad Norm: 0.00826497\n",
      "Epoch 2 | Step 857300 | Avg Loss: 0.0159 | Grad Norm: 0.00885083\n",
      "Epoch 2 | Step 857400 | Avg Loss: 0.0157 | Grad Norm: 0.00935899\n",
      "Epoch 2 | Step 857500 | Avg Loss: 0.0160 | Grad Norm: 0.00800219\n",
      "Epoch 2 | Step 857600 | Avg Loss: 0.0160 | Grad Norm: 0.00883096\n",
      "Epoch 2 | Step 857700 | Avg Loss: 0.0157 | Grad Norm: 0.00817588\n",
      "Epoch 2 | Step 857800 | Avg Loss: 0.0156 | Grad Norm: 0.00833388\n",
      "Epoch 2 | Step 857900 | Avg Loss: 0.0154 | Grad Norm: 0.00924540\n",
      "Epoch 2 | Step 858000 | Avg Loss: 0.0155 | Grad Norm: 0.00750385\n",
      "Epoch 2 | Step 858100 | Avg Loss: 0.0158 | Grad Norm: 0.00839464\n",
      "Epoch 2 | Step 858200 | Avg Loss: 0.0158 | Grad Norm: 0.00901959\n",
      "Epoch 2 | Step 858300 | Avg Loss: 0.0157 | Grad Norm: 0.01191111\n",
      "Epoch 2 | Step 858400 | Avg Loss: 0.0158 | Grad Norm: 0.00869074\n",
      "Epoch 2 | Step 858500 | Avg Loss: 0.0158 | Grad Norm: 0.00911230\n",
      "Epoch 2 | Step 858600 | Avg Loss: 0.0160 | Grad Norm: 0.00950570\n",
      "Epoch 2 | Step 858700 | Avg Loss: 0.0160 | Grad Norm: 0.00950055\n",
      "Epoch 2 | Step 858800 | Avg Loss: 0.0162 | Grad Norm: 0.01028279\n",
      "Epoch 2 | Step 858900 | Avg Loss: 0.0164 | Grad Norm: 0.00887376\n",
      "Epoch 2 | Step 859000 | Avg Loss: 0.0164 | Grad Norm: 0.00964492\n",
      "Epoch 2 | Step 859100 | Avg Loss: 0.0165 | Grad Norm: 0.00863343\n",
      "Epoch 2 | Step 859200 | Avg Loss: 0.0163 | Grad Norm: 0.00823818\n",
      "Epoch 2 | Step 859300 | Avg Loss: 0.0161 | Grad Norm: 0.00982492\n",
      "Epoch 2 | Step 859400 | Avg Loss: 0.0159 | Grad Norm: 0.00812705\n",
      "Epoch 2 | Step 859500 | Avg Loss: 0.0161 | Grad Norm: 0.00746575\n",
      "Epoch 2 | Step 859600 | Avg Loss: 0.0163 | Grad Norm: 0.00918272\n",
      "Epoch 2 | Step 859700 | Avg Loss: 0.0162 | Grad Norm: 0.00879352\n",
      "Epoch 2 | Step 859800 | Avg Loss: 0.0163 | Grad Norm: 0.00973799\n",
      "Epoch 2 | Step 859900 | Avg Loss: 0.0161 | Grad Norm: 0.00925569\n",
      "Epoch 2 | Step 860000 | Avg Loss: 0.0164 | Grad Norm: 0.00930871\n",
      "Epoch 2 | Step 860100 | Avg Loss: 0.0164 | Grad Norm: 0.00926668\n",
      "Epoch 2 | Step 860200 | Avg Loss: 0.0163 | Grad Norm: 0.00806766\n",
      "Epoch 2 | Step 860300 | Avg Loss: 0.0162 | Grad Norm: 0.00946231\n",
      "Epoch 2 | Step 860400 | Avg Loss: 0.0158 | Grad Norm: 0.01080923\n",
      "Epoch 2 | Step 860500 | Avg Loss: 0.0156 | Grad Norm: 0.00761539\n",
      "Epoch 2 | Step 860600 | Avg Loss: 0.0156 | Grad Norm: 0.00924558\n",
      "Epoch 2 | Step 860700 | Avg Loss: 0.0155 | Grad Norm: 0.00786049\n",
      "Epoch 2 | Step 860800 | Avg Loss: 0.0156 | Grad Norm: 0.00888297\n",
      "Epoch 2 | Step 860900 | Avg Loss: 0.0159 | Grad Norm: 0.01032243\n",
      "Epoch 2 | Step 861000 | Avg Loss: 0.0163 | Grad Norm: 0.00868350\n",
      "Epoch 2 | Step 861100 | Avg Loss: 0.0163 | Grad Norm: 0.01018659\n",
      "Epoch 2 | Step 861200 | Avg Loss: 0.0166 | Grad Norm: 0.00919886\n",
      "Epoch 2 | Step 861300 | Avg Loss: 0.0161 | Grad Norm: 0.01035068\n",
      "Epoch 2 | Step 861400 | Avg Loss: 0.0159 | Grad Norm: 0.00834051\n",
      "Epoch 2 | Step 861500 | Avg Loss: 0.0160 | Grad Norm: 0.00888729\n",
      "Epoch 2 | Step 861600 | Avg Loss: 0.0158 | Grad Norm: 0.00969962\n",
      "Epoch 2 | Step 861700 | Avg Loss: 0.0159 | Grad Norm: 0.00926476\n",
      "Epoch 2 | Step 861800 | Avg Loss: 0.0160 | Grad Norm: 0.00972200\n",
      "Epoch 2 | Step 861900 | Avg Loss: 0.0159 | Grad Norm: 0.00850680\n",
      "Epoch 2 | Step 862000 | Avg Loss: 0.0159 | Grad Norm: 0.00874031\n",
      "Epoch 2 | Step 862100 | Avg Loss: 0.0153 | Grad Norm: 0.00842647\n",
      "Epoch 2 | Step 862200 | Avg Loss: 0.0153 | Grad Norm: 0.00855547\n",
      "Epoch 2 | Step 862300 | Avg Loss: 0.0156 | Grad Norm: 0.00848360\n",
      "Epoch 2 | Step 862400 | Avg Loss: 0.0154 | Grad Norm: 0.00990003\n",
      "Epoch 2 | Step 862500 | Avg Loss: 0.0151 | Grad Norm: 0.00940817\n",
      "Epoch 2 | Step 862600 | Avg Loss: 0.0151 | Grad Norm: 0.00927695\n",
      "Epoch 2 | Step 862700 | Avg Loss: 0.0153 | Grad Norm: 0.01030235\n",
      "Epoch 2 | Step 862800 | Avg Loss: 0.0154 | Grad Norm: 0.00956951\n",
      "Epoch 2 | Step 862900 | Avg Loss: 0.0155 | Grad Norm: 0.00884124\n",
      "Epoch 2 | Step 863000 | Avg Loss: 0.0155 | Grad Norm: 0.00942815\n",
      "Epoch 2 | Step 863100 | Avg Loss: 0.0156 | Grad Norm: 0.00876186\n",
      "Epoch 2 | Step 863200 | Avg Loss: 0.0158 | Grad Norm: 0.00907806\n",
      "Epoch 2 | Step 863300 | Avg Loss: 0.0154 | Grad Norm: 0.00916993\n",
      "Epoch 2 | Step 863400 | Avg Loss: 0.0156 | Grad Norm: 0.00974048\n",
      "Epoch 2 | Step 863500 | Avg Loss: 0.0153 | Grad Norm: 0.00766939\n",
      "Epoch 2 | Step 863600 | Avg Loss: 0.0156 | Grad Norm: 0.00838903\n",
      "Epoch 2 | Step 863700 | Avg Loss: 0.0157 | Grad Norm: 0.00905870\n",
      "Epoch 2 | Step 863800 | Avg Loss: 0.0152 | Grad Norm: 0.00779973\n",
      "Epoch 2 | Step 863900 | Avg Loss: 0.0154 | Grad Norm: 0.00865943\n",
      "Epoch 2 | Step 864000 | Avg Loss: 0.0155 | Grad Norm: 0.00793520\n",
      "Epoch 2 | Step 864100 | Avg Loss: 0.0155 | Grad Norm: 0.00874733\n",
      "Epoch 2 | Step 864200 | Avg Loss: 0.0155 | Grad Norm: 0.00796557\n",
      "Epoch 2 | Step 864300 | Avg Loss: 0.0156 | Grad Norm: 0.01012881\n",
      "Epoch 2 | Step 864400 | Avg Loss: 0.0154 | Grad Norm: 0.00919462\n",
      "Epoch 2 | Step 864500 | Avg Loss: 0.0158 | Grad Norm: 0.00806253\n",
      "Epoch 2 | Step 864600 | Avg Loss: 0.0155 | Grad Norm: 0.00841518\n",
      "Epoch 2 | Step 864700 | Avg Loss: 0.0160 | Grad Norm: 0.00946046\n",
      "Epoch 2 | Step 864800 | Avg Loss: 0.0160 | Grad Norm: 0.00986347\n",
      "Epoch 2 | Step 864900 | Avg Loss: 0.0154 | Grad Norm: 0.00966839\n",
      "Epoch 2 | Step 865000 | Avg Loss: 0.0158 | Grad Norm: 0.00775685\n",
      "Epoch 2 | Step 865100 | Avg Loss: 0.0161 | Grad Norm: 0.00758874\n",
      "Epoch 2 | Step 865200 | Avg Loss: 0.0160 | Grad Norm: 0.00867003\n",
      "Epoch 2 | Step 865300 | Avg Loss: 0.0160 | Grad Norm: 0.00912319\n",
      "Epoch 2 | Step 865400 | Avg Loss: 0.0157 | Grad Norm: 0.00868905\n",
      "Epoch 2 | Step 865500 | Avg Loss: 0.0155 | Grad Norm: 0.00896857\n",
      "Epoch 2 | Step 865600 | Avg Loss: 0.0161 | Grad Norm: 0.01022351\n",
      "Epoch 2 | Step 865700 | Avg Loss: 0.0161 | Grad Norm: 0.00927175\n",
      "Epoch 2 | Step 865800 | Avg Loss: 0.0158 | Grad Norm: 0.00891747\n",
      "Epoch 2 | Step 865900 | Avg Loss: 0.0157 | Grad Norm: 0.00791080\n",
      "Epoch 2 | Step 866000 | Avg Loss: 0.0158 | Grad Norm: 0.00868634\n",
      "Epoch 2 | Step 866100 | Avg Loss: 0.0160 | Grad Norm: 0.00900424\n",
      "Epoch 2 | Step 866200 | Avg Loss: 0.0156 | Grad Norm: 0.00884211\n",
      "Epoch 2 | Step 866300 | Avg Loss: 0.0155 | Grad Norm: 0.01266814\n",
      "Epoch 2 | Step 866400 | Avg Loss: 0.0156 | Grad Norm: 0.00776461\n",
      "Epoch 2 | Step 866500 | Avg Loss: 0.0157 | Grad Norm: 0.00915310\n",
      "Epoch 2 | Step 866600 | Avg Loss: 0.0153 | Grad Norm: 0.00871594\n",
      "Epoch 2 | Step 866700 | Avg Loss: 0.0151 | Grad Norm: 0.00971307\n",
      "Epoch 2 | Step 866800 | Avg Loss: 0.0151 | Grad Norm: 0.00847162\n",
      "Epoch 2 | Step 866900 | Avg Loss: 0.0148 | Grad Norm: 0.00773610\n",
      "Epoch 2 | Step 867000 | Avg Loss: 0.0151 | Grad Norm: 0.00822787\n",
      "Epoch 2 | Step 867100 | Avg Loss: 0.0154 | Grad Norm: 0.00930559\n",
      "Epoch 2 | Step 867200 | Avg Loss: 0.0155 | Grad Norm: 0.00881546\n",
      "Epoch 2 | Step 867300 | Avg Loss: 0.0158 | Grad Norm: 0.00946735\n",
      "Epoch 2 | Step 867400 | Avg Loss: 0.0156 | Grad Norm: 0.00866620\n",
      "Epoch 2 | Step 867500 | Avg Loss: 0.0158 | Grad Norm: 0.00877230\n",
      "Epoch 2 | Step 867600 | Avg Loss: 0.0157 | Grad Norm: 0.00957397\n",
      "Epoch 2 | Step 867700 | Avg Loss: 0.0157 | Grad Norm: 0.00874036\n",
      "Epoch 2 | Step 867800 | Avg Loss: 0.0157 | Grad Norm: 0.00900046\n",
      "Epoch 2 | Step 867900 | Avg Loss: 0.0155 | Grad Norm: 0.01126459\n",
      "Epoch 2 | Step 868000 | Avg Loss: 0.0155 | Grad Norm: 0.00824152\n",
      "Epoch 2 | Step 868100 | Avg Loss: 0.0157 | Grad Norm: 0.00784032\n",
      "Epoch 2 | Step 868200 | Avg Loss: 0.0157 | Grad Norm: 0.00919773\n",
      "Epoch 2 | Step 868300 | Avg Loss: 0.0161 | Grad Norm: 0.00916815\n",
      "Epoch 2 | Step 868400 | Avg Loss: 0.0159 | Grad Norm: 0.00855329\n",
      "Epoch 2 | Step 868500 | Avg Loss: 0.0161 | Grad Norm: 0.00835274\n",
      "Epoch 2 | Step 868600 | Avg Loss: 0.0162 | Grad Norm: 0.00924251\n",
      "Epoch 2 | Step 868700 | Avg Loss: 0.0161 | Grad Norm: 0.00866576\n",
      "Epoch 2 | Step 868800 | Avg Loss: 0.0161 | Grad Norm: 0.00972539\n",
      "Epoch 2 | Step 868900 | Avg Loss: 0.0162 | Grad Norm: 0.00928411\n",
      "Epoch 2 | Step 869000 | Avg Loss: 0.0165 | Grad Norm: 0.00904728\n",
      "Epoch 2 | Step 869100 | Avg Loss: 0.0160 | Grad Norm: 0.00939069\n",
      "Epoch 2 | Step 869200 | Avg Loss: 0.0162 | Grad Norm: 0.00935658\n",
      "Epoch 2 | Step 869300 | Avg Loss: 0.0160 | Grad Norm: 0.00774186\n",
      "Epoch 2 | Step 869400 | Avg Loss: 0.0159 | Grad Norm: 0.00968457\n",
      "Epoch 2 | Step 869500 | Avg Loss: 0.0159 | Grad Norm: 0.00910012\n",
      "Epoch 2 | Step 869600 | Avg Loss: 0.0157 | Grad Norm: 0.00896350\n",
      "Epoch 2 | Step 869700 | Avg Loss: 0.0156 | Grad Norm: 0.00978289\n",
      "Epoch 2 | Step 869800 | Avg Loss: 0.0154 | Grad Norm: 0.00842039\n",
      "Epoch 2 | Step 869900 | Avg Loss: 0.0155 | Grad Norm: 0.00961459\n",
      "Epoch 2 | Step 870000 | Avg Loss: 0.0154 | Grad Norm: 0.00847990\n",
      "Epoch 2 | Step 870100 | Avg Loss: 0.0155 | Grad Norm: 0.00839964\n",
      "Epoch 2 | Step 870200 | Avg Loss: 0.0157 | Grad Norm: 0.00774737\n",
      "Epoch 2 | Step 870300 | Avg Loss: 0.0158 | Grad Norm: 0.00844386\n",
      "Epoch 2 | Step 870400 | Avg Loss: 0.0157 | Grad Norm: 0.00878196\n",
      "Epoch 2 | Step 870500 | Avg Loss: 0.0161 | Grad Norm: 0.00774355\n",
      "Epoch 2 | Step 870600 | Avg Loss: 0.0160 | Grad Norm: 0.01037786\n",
      "Epoch 2 | Step 870700 | Avg Loss: 0.0160 | Grad Norm: 0.00993253\n",
      "Epoch 2 | Step 870800 | Avg Loss: 0.0163 | Grad Norm: 0.00876254\n",
      "Epoch 2 | Step 870900 | Avg Loss: 0.0153 | Grad Norm: 0.00803551\n",
      "Epoch 2 | Step 871000 | Avg Loss: 0.0148 | Grad Norm: 0.00779127\n",
      "Epoch 2 | Step 871100 | Avg Loss: 0.0152 | Grad Norm: 0.00832849\n",
      "Epoch 2 | Step 871200 | Avg Loss: 0.0160 | Grad Norm: 0.00957375\n",
      "Epoch 2 | Step 871300 | Avg Loss: 0.0157 | Grad Norm: 0.00905049\n",
      "Epoch 2 | Step 871400 | Avg Loss: 0.0158 | Grad Norm: 0.00862983\n",
      "Epoch 2 | Step 871500 | Avg Loss: 0.0159 | Grad Norm: 0.00817771\n",
      "Epoch 2 | Step 871600 | Avg Loss: 0.0158 | Grad Norm: 0.00837704\n",
      "Epoch 2 | Step 871700 | Avg Loss: 0.0161 | Grad Norm: 0.00812490\n",
      "Epoch 2 | Step 871800 | Avg Loss: 0.0161 | Grad Norm: 0.00933200\n",
      "Epoch 2 | Step 871900 | Avg Loss: 0.0159 | Grad Norm: 0.00863709\n",
      "Epoch 2 | Step 872000 | Avg Loss: 0.0159 | Grad Norm: 0.01022736\n",
      "Epoch 2 | Step 872100 | Avg Loss: 0.0152 | Grad Norm: 0.00841274\n",
      "Epoch 2 | Step 872200 | Avg Loss: 0.0150 | Grad Norm: 0.00799529\n",
      "Epoch 2 | Step 872300 | Avg Loss: 0.0150 | Grad Norm: 0.00870331\n",
      "Epoch 2 | Step 872400 | Avg Loss: 0.0154 | Grad Norm: 0.00948071\n",
      "Epoch 2 | Step 872500 | Avg Loss: 0.0157 | Grad Norm: 0.00847222\n",
      "Epoch 2 | Step 872600 | Avg Loss: 0.0159 | Grad Norm: 0.01098592\n",
      "Epoch 2 | Step 872700 | Avg Loss: 0.0156 | Grad Norm: 0.00885890\n",
      "Epoch 2 | Step 872800 | Avg Loss: 0.0154 | Grad Norm: 0.01005787\n",
      "Epoch 2 | Step 872900 | Avg Loss: 0.0157 | Grad Norm: 0.00859866\n",
      "Epoch 2 | Step 873000 | Avg Loss: 0.0156 | Grad Norm: 0.00887986\n",
      "Epoch 2 | Step 873100 | Avg Loss: 0.0154 | Grad Norm: 0.00859474\n",
      "Epoch 2 | Step 873200 | Avg Loss: 0.0156 | Grad Norm: 0.00845730\n",
      "Epoch 2 | Step 873300 | Avg Loss: 0.0157 | Grad Norm: 0.00985861\n",
      "Epoch 2 | Step 873400 | Avg Loss: 0.0157 | Grad Norm: 0.00894549\n",
      "Epoch 2 | Step 873500 | Avg Loss: 0.0157 | Grad Norm: 0.00841380\n",
      "Epoch 2 | Step 873600 | Avg Loss: 0.0159 | Grad Norm: 0.01113597\n",
      "Epoch 2 | Step 873700 | Avg Loss: 0.0157 | Grad Norm: 0.00835799\n",
      "Epoch 2 | Step 873800 | Avg Loss: 0.0156 | Grad Norm: 0.00891949\n",
      "Epoch 2 | Step 873900 | Avg Loss: 0.0158 | Grad Norm: 0.00871782\n",
      "Epoch 2 | Step 874000 | Avg Loss: 0.0159 | Grad Norm: 0.00830417\n",
      "Epoch 2 | Step 874100 | Avg Loss: 0.0155 | Grad Norm: 0.00970217\n",
      "Epoch 2 | Step 874200 | Avg Loss: 0.0151 | Grad Norm: 0.00908242\n",
      "Epoch 2 | Step 874300 | Avg Loss: 0.0154 | Grad Norm: 0.00835372\n",
      "Epoch 2 | Step 874400 | Avg Loss: 0.0155 | Grad Norm: 0.00930234\n",
      "Epoch 2 | Step 874500 | Avg Loss: 0.0149 | Grad Norm: 0.00979382\n",
      "Epoch 2 | Step 874600 | Avg Loss: 0.0148 | Grad Norm: 0.00764664\n",
      "Epoch 2 | Step 874700 | Avg Loss: 0.0151 | Grad Norm: 0.00899230\n",
      "Epoch 2 | Step 874800 | Avg Loss: 0.0155 | Grad Norm: 0.00835550\n",
      "Epoch 2 | Step 874900 | Avg Loss: 0.0154 | Grad Norm: 0.00805148\n",
      "Epoch 2 | Step 875000 | Avg Loss: 0.0151 | Grad Norm: 0.00822498\n",
      "Epoch 2 | Step 875100 | Avg Loss: 0.0159 | Grad Norm: 0.00988064\n",
      "Epoch 2 | Step 875200 | Avg Loss: 0.0159 | Grad Norm: 0.00919765\n",
      "Epoch 2 | Step 875300 | Avg Loss: 0.0160 | Grad Norm: 0.00907175\n",
      "Epoch 2 | Step 875400 | Avg Loss: 0.0159 | Grad Norm: 0.00864982\n",
      "Epoch 2 | Step 875500 | Avg Loss: 0.0157 | Grad Norm: 0.00770863\n",
      "Epoch 2 | Step 875600 | Avg Loss: 0.0156 | Grad Norm: 0.00949440\n",
      "Epoch 2 | Step 875700 | Avg Loss: 0.0155 | Grad Norm: 0.01014219\n",
      "Epoch 2 | Step 875800 | Avg Loss: 0.0155 | Grad Norm: 0.00836663\n",
      "Epoch 2 | Step 875900 | Avg Loss: 0.0151 | Grad Norm: 0.00972633\n",
      "Epoch 2 | Step 876000 | Avg Loss: 0.0151 | Grad Norm: 0.01056557\n",
      "Epoch 2 | Step 876100 | Avg Loss: 0.0152 | Grad Norm: 0.00930674\n",
      "Epoch 2 | Step 876200 | Avg Loss: 0.0153 | Grad Norm: 0.00889978\n",
      "Epoch 2 | Step 876300 | Avg Loss: 0.0155 | Grad Norm: 0.00843026\n",
      "Epoch 2 | Step 876400 | Avg Loss: 0.0156 | Grad Norm: 0.00914367\n",
      "Epoch 2 | Step 876500 | Avg Loss: 0.0157 | Grad Norm: 0.01041082\n",
      "Epoch 2 | Step 876600 | Avg Loss: 0.0156 | Grad Norm: 0.00832050\n",
      "Epoch 2 | Step 876700 | Avg Loss: 0.0158 | Grad Norm: 0.01031440\n",
      "Epoch 2 | Step 876800 | Avg Loss: 0.0156 | Grad Norm: 0.00856708\n",
      "Epoch 2 | Step 876900 | Avg Loss: 0.0153 | Grad Norm: 0.00787181\n",
      "Epoch 2 | Step 877000 | Avg Loss: 0.0154 | Grad Norm: 0.00927581\n",
      "Epoch 2 | Step 877100 | Avg Loss: 0.0151 | Grad Norm: 0.00836378\n",
      "Epoch 2 | Step 877200 | Avg Loss: 0.0150 | Grad Norm: 0.00776959\n",
      "Epoch 2 | Step 877300 | Avg Loss: 0.0149 | Grad Norm: 0.00944904\n",
      "Epoch 2 | Step 877400 | Avg Loss: 0.0147 | Grad Norm: 0.00908261\n",
      "Epoch 2 | Step 877500 | Avg Loss: 0.0146 | Grad Norm: 0.00905623\n",
      "Epoch 2 | Step 877600 | Avg Loss: 0.0150 | Grad Norm: 0.00835322\n",
      "Epoch 2 | Step 877700 | Avg Loss: 0.0155 | Grad Norm: 0.00905221\n",
      "Epoch 2 | Step 877800 | Avg Loss: 0.0156 | Grad Norm: 0.00882861\n",
      "Epoch 2 | Step 877900 | Avg Loss: 0.0157 | Grad Norm: 0.01009902\n",
      "Epoch 2 | Step 878000 | Avg Loss: 0.0158 | Grad Norm: 0.00847087\n",
      "Epoch 2 | Step 878100 | Avg Loss: 0.0155 | Grad Norm: 0.00960351\n",
      "Epoch 2 | Step 878200 | Avg Loss: 0.0156 | Grad Norm: 0.00973986\n",
      "Epoch 2 | Step 878300 | Avg Loss: 0.0158 | Grad Norm: 0.00805283\n",
      "Epoch 2 | Step 878400 | Avg Loss: 0.0160 | Grad Norm: 0.00859028\n",
      "Epoch 2 | Step 878500 | Avg Loss: 0.0160 | Grad Norm: 0.00932361\n",
      "Epoch 2 | Step 878600 | Avg Loss: 0.0160 | Grad Norm: 0.00906083\n",
      "Epoch 2 | Step 878700 | Avg Loss: 0.0161 | Grad Norm: 0.00914349\n",
      "Epoch 2 | Step 878800 | Avg Loss: 0.0160 | Grad Norm: 0.01101139\n",
      "Epoch 2 | Step 878900 | Avg Loss: 0.0160 | Grad Norm: 0.00875619\n",
      "Epoch 2 | Step 879000 | Avg Loss: 0.0162 | Grad Norm: 0.01040357\n",
      "Epoch 2 | Step 879100 | Avg Loss: 0.0165 | Grad Norm: 0.00874057\n",
      "Epoch 2 | Step 879200 | Avg Loss: 0.0168 | Grad Norm: 0.00957770\n",
      "Epoch 2 | Step 879300 | Avg Loss: 0.0160 | Grad Norm: 0.00864775\n",
      "Epoch 2 | Step 879400 | Avg Loss: 0.0159 | Grad Norm: 0.00936513\n",
      "Epoch 2 | Step 879500 | Avg Loss: 0.0159 | Grad Norm: 0.00935965\n",
      "Epoch 2 | Step 879600 | Avg Loss: 0.0160 | Grad Norm: 0.00970124\n",
      "Epoch 2 | Step 879700 | Avg Loss: 0.0159 | Grad Norm: 0.00902191\n",
      "Epoch 2 | Step 879800 | Avg Loss: 0.0155 | Grad Norm: 0.00918482\n",
      "Epoch 2 | Step 879900 | Avg Loss: 0.0155 | Grad Norm: 0.00849887\n",
      "Epoch 2 | Step 880000 | Avg Loss: 0.0159 | Grad Norm: 0.01133840\n",
      "Epoch 2 | Step 880100 | Avg Loss: 0.0158 | Grad Norm: 0.00968398\n",
      "Epoch 2 | Step 880200 | Avg Loss: 0.0157 | Grad Norm: 0.00936419\n",
      "Epoch 2 | Step 880300 | Avg Loss: 0.0159 | Grad Norm: 0.01029235\n",
      "Epoch 2 | Step 880400 | Avg Loss: 0.0156 | Grad Norm: 0.00841857\n",
      "Epoch 2 | Step 880500 | Avg Loss: 0.0157 | Grad Norm: 0.00833703\n",
      "Epoch 2 | Step 880600 | Avg Loss: 0.0153 | Grad Norm: 0.00900905\n",
      "Epoch 2 | Step 880700 | Avg Loss: 0.0149 | Grad Norm: 0.00699230\n",
      "Epoch 2 | Step 880800 | Avg Loss: 0.0150 | Grad Norm: 0.00955731\n",
      "Epoch 2 | Step 880900 | Avg Loss: 0.0150 | Grad Norm: 0.00975346\n",
      "Epoch 2 | Step 881000 | Avg Loss: 0.0148 | Grad Norm: 0.00787643\n",
      "Epoch 2 | Step 881100 | Avg Loss: 0.0151 | Grad Norm: 0.00822014\n",
      "Epoch 2 | Step 881200 | Avg Loss: 0.0152 | Grad Norm: 0.00912453\n",
      "Epoch 2 | Step 881300 | Avg Loss: 0.0153 | Grad Norm: 0.00825498\n",
      "Epoch 2 | Step 881400 | Avg Loss: 0.0151 | Grad Norm: 0.00861962\n",
      "Epoch 2 | Step 881500 | Avg Loss: 0.0153 | Grad Norm: 0.00930030\n",
      "Epoch 2 | Step 881600 | Avg Loss: 0.0152 | Grad Norm: 0.00853625\n",
      "Epoch 2 | Step 881700 | Avg Loss: 0.0156 | Grad Norm: 0.00907856\n",
      "Epoch 2 | Step 881800 | Avg Loss: 0.0159 | Grad Norm: 0.00938571\n",
      "Epoch 2 | Step 881900 | Avg Loss: 0.0163 | Grad Norm: 0.01009826\n",
      "Epoch 2 | Step 882000 | Avg Loss: 0.0163 | Grad Norm: 0.00994348\n",
      "Epoch 2 | Step 882100 | Avg Loss: 0.0164 | Grad Norm: 0.00925200\n",
      "Epoch 2 | Step 882200 | Avg Loss: 0.0164 | Grad Norm: 0.00974656\n",
      "Epoch 2 | Step 882300 | Avg Loss: 0.0160 | Grad Norm: 0.00980060\n",
      "Epoch 2 | Step 882400 | Avg Loss: 0.0157 | Grad Norm: 0.00878047\n",
      "Epoch 2 | Step 882500 | Avg Loss: 0.0158 | Grad Norm: 0.00812634\n",
      "Epoch 2 | Step 882600 | Avg Loss: 0.0158 | Grad Norm: 0.00843432\n",
      "Epoch 2 | Step 882700 | Avg Loss: 0.0155 | Grad Norm: 0.00826968\n",
      "Epoch 2 | Step 882800 | Avg Loss: 0.0149 | Grad Norm: 0.00902964\n",
      "Epoch 2 | Step 882900 | Avg Loss: 0.0150 | Grad Norm: 0.00800744\n",
      "Epoch 2 | Step 883000 | Avg Loss: 0.0148 | Grad Norm: 0.00826033\n",
      "Epoch 2 | Step 883100 | Avg Loss: 0.0149 | Grad Norm: 0.00873684\n",
      "Epoch 2 | Step 883200 | Avg Loss: 0.0151 | Grad Norm: 0.00823618\n",
      "Epoch 2 | Step 883300 | Avg Loss: 0.0153 | Grad Norm: 0.00856175\n",
      "Epoch 2 | Step 883400 | Avg Loss: 0.0151 | Grad Norm: 0.00890361\n",
      "Epoch 2 | Step 883500 | Avg Loss: 0.0149 | Grad Norm: 0.00846075\n",
      "Epoch 2 | Step 883600 | Avg Loss: 0.0151 | Grad Norm: 0.00853664\n",
      "Epoch 2 | Step 883700 | Avg Loss: 0.0155 | Grad Norm: 0.00862021\n",
      "Epoch 2 | Step 883800 | Avg Loss: 0.0157 | Grad Norm: 0.01008005\n",
      "Epoch 2 | Step 883900 | Avg Loss: 0.0159 | Grad Norm: 0.00933160\n",
      "Epoch 2 | Step 884000 | Avg Loss: 0.0156 | Grad Norm: 0.00826399\n",
      "Epoch 2 | Step 884100 | Avg Loss: 0.0156 | Grad Norm: 0.00825142\n",
      "Epoch 2 | Step 884200 | Avg Loss: 0.0156 | Grad Norm: 0.00924383\n",
      "Epoch 2 | Step 884300 | Avg Loss: 0.0155 | Grad Norm: 0.01123072\n",
      "Epoch 2 | Step 884400 | Avg Loss: 0.0157 | Grad Norm: 0.00803173\n",
      "Epoch 2 | Step 884500 | Avg Loss: 0.0152 | Grad Norm: 0.00920842\n",
      "Epoch 2 | Step 884600 | Avg Loss: 0.0150 | Grad Norm: 0.01026071\n",
      "Epoch 2 | Step 884700 | Avg Loss: 0.0155 | Grad Norm: 0.00951832\n",
      "Epoch 2 | Step 884800 | Avg Loss: 0.0157 | Grad Norm: 0.00946277\n",
      "Epoch 2 | Step 884900 | Avg Loss: 0.0155 | Grad Norm: 0.00887800\n",
      "Epoch 2 | Step 885000 | Avg Loss: 0.0155 | Grad Norm: 0.00812159\n",
      "Epoch 2 | Step 885100 | Avg Loss: 0.0153 | Grad Norm: 0.00815912\n",
      "Epoch 2 | Step 885200 | Avg Loss: 0.0155 | Grad Norm: 0.00764439\n",
      "Epoch 2 | Step 885300 | Avg Loss: 0.0155 | Grad Norm: 0.01060993\n",
      "Epoch 2 | Step 885400 | Avg Loss: 0.0155 | Grad Norm: 0.01148235\n",
      "Epoch 2 | Step 885500 | Avg Loss: 0.0155 | Grad Norm: 0.00991357\n",
      "Epoch 2 | Step 885600 | Avg Loss: 0.0152 | Grad Norm: 0.00889484\n",
      "Epoch 2 | Step 885700 | Avg Loss: 0.0159 | Grad Norm: 0.01045791\n",
      "Epoch 2 | Step 885800 | Avg Loss: 0.0158 | Grad Norm: 0.00736991\n",
      "Epoch 2 | Step 885900 | Avg Loss: 0.0158 | Grad Norm: 0.01261889\n",
      "Epoch 2 | Step 886000 | Avg Loss: 0.0154 | Grad Norm: 0.00865657\n",
      "Epoch 2 | Step 886100 | Avg Loss: 0.0156 | Grad Norm: 0.00889874\n",
      "Epoch 2 | Step 886200 | Avg Loss: 0.0154 | Grad Norm: 0.00885190\n",
      "Epoch 2 | Step 886300 | Avg Loss: 0.0156 | Grad Norm: 0.00879165\n",
      "Epoch 2 | Step 886400 | Avg Loss: 0.0157 | Grad Norm: 0.00938847\n",
      "Epoch 2 | Step 886500 | Avg Loss: 0.0160 | Grad Norm: 0.00957254\n",
      "Epoch 2 | Step 886600 | Avg Loss: 0.0160 | Grad Norm: 0.00820675\n",
      "Epoch 2 | Step 886700 | Avg Loss: 0.0155 | Grad Norm: 0.01040377\n",
      "Epoch 2 | Step 886800 | Avg Loss: 0.0148 | Grad Norm: 0.01214739\n",
      "Epoch 2 | Step 886900 | Avg Loss: 0.0155 | Grad Norm: 0.00865673\n",
      "Epoch 2 | Step 887000 | Avg Loss: 0.0155 | Grad Norm: 0.01133704\n",
      "Epoch 2 | Step 887100 | Avg Loss: 0.0155 | Grad Norm: 0.00875355\n",
      "Epoch 2 | Step 887200 | Avg Loss: 0.0157 | Grad Norm: 0.00830850\n",
      "Epoch 2 | Step 887300 | Avg Loss: 0.0159 | Grad Norm: 0.00848446\n",
      "Epoch 2 | Step 887400 | Avg Loss: 0.0157 | Grad Norm: 0.00903569\n",
      "Epoch 2 | Step 887500 | Avg Loss: 0.0156 | Grad Norm: 0.01192230\n",
      "Epoch 2 | Step 887600 | Avg Loss: 0.0157 | Grad Norm: 0.01072312\n",
      "Epoch 2 | Step 887700 | Avg Loss: 0.0155 | Grad Norm: 0.00884238\n",
      "Epoch 2 | Step 887800 | Avg Loss: 0.0152 | Grad Norm: 0.00906360\n",
      "Epoch 2 | Step 887900 | Avg Loss: 0.0149 | Grad Norm: 0.00827711\n",
      "Epoch 2 | Step 888000 | Avg Loss: 0.0151 | Grad Norm: 0.01174251\n",
      "Epoch 2 | Step 888100 | Avg Loss: 0.0154 | Grad Norm: 0.01268479\n",
      "Epoch 2 | Step 888200 | Avg Loss: 0.0152 | Grad Norm: 0.00871642\n",
      "Epoch 2 | Step 888300 | Avg Loss: 0.0153 | Grad Norm: 0.00799571\n",
      "Epoch 2 | Step 888400 | Avg Loss: 0.0158 | Grad Norm: 0.00860109\n",
      "Epoch 2 | Step 888500 | Avg Loss: 0.0164 | Grad Norm: 0.01018652\n",
      "Epoch 2 | Step 888600 | Avg Loss: 0.0166 | Grad Norm: 0.00906108\n",
      "Epoch 2 | Step 888700 | Avg Loss: 0.0166 | Grad Norm: 0.00931549\n",
      "Epoch 2 | Step 888800 | Avg Loss: 0.0164 | Grad Norm: 0.01068929\n",
      "Epoch 2 | Step 888900 | Avg Loss: 0.0161 | Grad Norm: 0.00853973\n",
      "Epoch 2 | Step 889000 | Avg Loss: 0.0159 | Grad Norm: 0.00860119\n",
      "Epoch 2 | Step 889100 | Avg Loss: 0.0161 | Grad Norm: 0.00772746\n",
      "Epoch 2 | Step 889200 | Avg Loss: 0.0160 | Grad Norm: 0.00758163\n",
      "Epoch 2 | Step 889300 | Avg Loss: 0.0162 | Grad Norm: 0.00943423\n",
      "Epoch 2 | Step 889400 | Avg Loss: 0.0162 | Grad Norm: 0.00886676\n",
      "Epoch 2 | Step 889500 | Avg Loss: 0.0158 | Grad Norm: 0.00927208\n",
      "Epoch 2 | Step 889600 | Avg Loss: 0.0160 | Grad Norm: 0.01017115\n",
      "Epoch 2 | Step 889700 | Avg Loss: 0.0160 | Grad Norm: 0.00971897\n",
      "Epoch 2 | Step 889800 | Avg Loss: 0.0153 | Grad Norm: 0.00883506\n",
      "Epoch 2 | Step 889900 | Avg Loss: 0.0154 | Grad Norm: 0.00958718\n",
      "Epoch 2 | Step 890000 | Avg Loss: 0.0155 | Grad Norm: 0.00857520\n",
      "Epoch 2 | Step 890100 | Avg Loss: 0.0157 | Grad Norm: 0.00988348\n",
      "Epoch 2 | Step 890200 | Avg Loss: 0.0159 | Grad Norm: 0.00883049\n",
      "Epoch 2 | Step 890300 | Avg Loss: 0.0158 | Grad Norm: 0.00795403\n",
      "Epoch 2 | Step 890400 | Avg Loss: 0.0157 | Grad Norm: 0.01018610\n",
      "Epoch 2 | Step 890500 | Avg Loss: 0.0155 | Grad Norm: 0.00762117\n",
      "Epoch 2 | Step 890600 | Avg Loss: 0.0157 | Grad Norm: 0.00877186\n",
      "Epoch 2 | Step 890700 | Avg Loss: 0.0156 | Grad Norm: 0.00966293\n",
      "Epoch 2 | Step 890800 | Avg Loss: 0.0159 | Grad Norm: 0.01062646\n",
      "Epoch 2 | Step 890900 | Avg Loss: 0.0157 | Grad Norm: 0.00910657\n",
      "Epoch 2 | Step 891000 | Avg Loss: 0.0157 | Grad Norm: 0.01156956\n",
      "Epoch 2 | Step 891100 | Avg Loss: 0.0155 | Grad Norm: 0.00977485\n",
      "Epoch 2 | Step 891200 | Avg Loss: 0.0155 | Grad Norm: 0.00798196\n",
      "Epoch 2 | Step 891300 | Avg Loss: 0.0153 | Grad Norm: 0.00880079\n",
      "Epoch 2 | Step 891400 | Avg Loss: 0.0152 | Grad Norm: 0.01095116\n",
      "Epoch 2 | Step 891500 | Avg Loss: 0.0151 | Grad Norm: 0.00879010\n",
      "Epoch 2 | Step 891600 | Avg Loss: 0.0152 | Grad Norm: 0.00830168\n",
      "Epoch 2 | Step 891700 | Avg Loss: 0.0153 | Grad Norm: 0.00983134\n",
      "Epoch 2 | Step 891800 | Avg Loss: 0.0153 | Grad Norm: 0.00941219\n",
      "Epoch 2 | Step 891900 | Avg Loss: 0.0156 | Grad Norm: 0.00914284\n",
      "Epoch 2 | Step 892000 | Avg Loss: 0.0159 | Grad Norm: 0.00824269\n",
      "Epoch 2 | Step 892100 | Avg Loss: 0.0159 | Grad Norm: 0.00925672\n",
      "Epoch 2 | Step 892200 | Avg Loss: 0.0161 | Grad Norm: 0.00954167\n",
      "Epoch 2 | Step 892300 | Avg Loss: 0.0157 | Grad Norm: 0.00831279\n",
      "Epoch 2 | Step 892400 | Avg Loss: 0.0157 | Grad Norm: 0.00822460\n",
      "Epoch 2 | Step 892500 | Avg Loss: 0.0159 | Grad Norm: 0.01022611\n",
      "Epoch 2 | Step 892600 | Avg Loss: 0.0158 | Grad Norm: 0.00831219\n",
      "Epoch 2 | Step 892700 | Avg Loss: 0.0157 | Grad Norm: 0.00913591\n",
      "Epoch 2 | Step 892800 | Avg Loss: 0.0159 | Grad Norm: 0.00839555\n",
      "Epoch 2 | Step 892900 | Avg Loss: 0.0159 | Grad Norm: 0.00891569\n",
      "Epoch 2 | Step 893000 | Avg Loss: 0.0154 | Grad Norm: 0.00818061\n",
      "Epoch 2 | Step 893100 | Avg Loss: 0.0155 | Grad Norm: 0.00809396\n",
      "Epoch 2 | Step 893200 | Avg Loss: 0.0155 | Grad Norm: 0.00914378\n",
      "Epoch 2 | Step 893300 | Avg Loss: 0.0154 | Grad Norm: 0.00810305\n",
      "Epoch 2 | Step 893400 | Avg Loss: 0.0155 | Grad Norm: 0.00937337\n",
      "Epoch 2 | Step 893500 | Avg Loss: 0.0157 | Grad Norm: 0.00898229\n",
      "Epoch 2 | Step 893600 | Avg Loss: 0.0156 | Grad Norm: 0.00866948\n",
      "Epoch 2 | Step 893700 | Avg Loss: 0.0154 | Grad Norm: 0.00857059\n",
      "Epoch 2 | Step 893800 | Avg Loss: 0.0154 | Grad Norm: 0.00777295\n",
      "Epoch 2 | Step 893900 | Avg Loss: 0.0156 | Grad Norm: 0.00904070\n",
      "Epoch 2 | Step 894000 | Avg Loss: 0.0157 | Grad Norm: 0.00947143\n",
      "Epoch 2 | Step 894100 | Avg Loss: 0.0157 | Grad Norm: 0.00982874\n",
      "Epoch 2 | Step 894200 | Avg Loss: 0.0160 | Grad Norm: 0.01135079\n",
      "Epoch 2 | Step 894300 | Avg Loss: 0.0163 | Grad Norm: 0.01039650\n",
      "Epoch 2 | Step 894400 | Avg Loss: 0.0162 | Grad Norm: 0.00907235\n",
      "Epoch 2 | Step 894500 | Avg Loss: 0.0163 | Grad Norm: 0.00870435\n",
      "Epoch 2 | Step 894600 | Avg Loss: 0.0159 | Grad Norm: 0.00883225\n",
      "Epoch 2 | Step 894700 | Avg Loss: 0.0157 | Grad Norm: 0.00887486\n",
      "Epoch 2 | Step 894800 | Avg Loss: 0.0153 | Grad Norm: 0.00897760\n",
      "Epoch 2 | Step 894900 | Avg Loss: 0.0154 | Grad Norm: 0.00822951\n",
      "Epoch 2 | Step 895000 | Avg Loss: 0.0154 | Grad Norm: 0.00815312\n",
      "Epoch 2 | Step 895100 | Avg Loss: 0.0156 | Grad Norm: 0.00868324\n",
      "Epoch 2 | Step 895200 | Avg Loss: 0.0156 | Grad Norm: 0.00887232\n",
      "Epoch 2 | Step 895300 | Avg Loss: 0.0155 | Grad Norm: 0.00889675\n",
      "Epoch 2 | Step 895400 | Avg Loss: 0.0156 | Grad Norm: 0.00846255\n",
      "Epoch 2 | Step 895500 | Avg Loss: 0.0160 | Grad Norm: 0.00897048\n",
      "Epoch 2 | Step 895600 | Avg Loss: 0.0159 | Grad Norm: 0.01021573\n",
      "Epoch 2 | Step 895700 | Avg Loss: 0.0158 | Grad Norm: 0.01046465\n",
      "Epoch 2 | Step 895800 | Avg Loss: 0.0158 | Grad Norm: 0.00789103\n",
      "Epoch 2 | Step 895900 | Avg Loss: 0.0157 | Grad Norm: 0.00895410\n",
      "Epoch 2 | Step 896000 | Avg Loss: 0.0153 | Grad Norm: 0.00998386\n",
      "Epoch 2 | Step 896100 | Avg Loss: 0.0154 | Grad Norm: 0.00931090\n",
      "Epoch 2 | Step 896200 | Avg Loss: 0.0157 | Grad Norm: 0.00910641\n",
      "Epoch 2 | Step 896300 | Avg Loss: 0.0160 | Grad Norm: 0.00847331\n",
      "Epoch 2 | Step 896400 | Avg Loss: 0.0159 | Grad Norm: 0.00825630\n",
      "Epoch 2 | Step 896500 | Avg Loss: 0.0157 | Grad Norm: 0.00892704\n",
      "Epoch 2 | Step 896600 | Avg Loss: 0.0154 | Grad Norm: 0.00913797\n",
      "Epoch 2 | Step 896700 | Avg Loss: 0.0153 | Grad Norm: 0.01059360\n",
      "Epoch 2 | Step 896800 | Avg Loss: 0.0152 | Grad Norm: 0.00920003\n",
      "Epoch 2 | Step 896900 | Avg Loss: 0.0156 | Grad Norm: 0.00950330\n",
      "Epoch 2 | Step 897000 | Avg Loss: 0.0153 | Grad Norm: 0.01109009\n",
      "Epoch 2 | Step 897100 | Avg Loss: 0.0155 | Grad Norm: 0.00914633\n",
      "Epoch 2 | Step 897200 | Avg Loss: 0.0155 | Grad Norm: 0.00805198\n",
      "Epoch 2 | Step 897300 | Avg Loss: 0.0154 | Grad Norm: 0.00833581\n",
      "Epoch 2 | Step 897400 | Avg Loss: 0.0158 | Grad Norm: 0.00949940\n",
      "Epoch 2 | Step 897500 | Avg Loss: 0.0161 | Grad Norm: 0.00779693\n",
      "Epoch 2 | Step 897600 | Avg Loss: 0.0163 | Grad Norm: 0.00822710\n",
      "Epoch 2 | Step 897700 | Avg Loss: 0.0160 | Grad Norm: 0.01170191\n",
      "Epoch 2 | Step 897800 | Avg Loss: 0.0158 | Grad Norm: 0.00893290\n",
      "Epoch 2 | Step 897900 | Avg Loss: 0.0158 | Grad Norm: 0.00887508\n",
      "Epoch 2 | Step 898000 | Avg Loss: 0.0157 | Grad Norm: 0.00871056\n",
      "Epoch 2 | Step 898100 | Avg Loss: 0.0160 | Grad Norm: 0.00899250\n",
      "Epoch 2 | Step 898200 | Avg Loss: 0.0157 | Grad Norm: 0.00821902\n",
      "Epoch 2 | Step 898300 | Avg Loss: 0.0156 | Grad Norm: 0.01076963\n",
      "Epoch 2 | Step 898400 | Avg Loss: 0.0156 | Grad Norm: 0.00865881\n",
      "Epoch 2 | Step 898500 | Avg Loss: 0.0158 | Grad Norm: 0.00959487\n",
      "Epoch 2 | Step 898600 | Avg Loss: 0.0158 | Grad Norm: 0.01029044\n",
      "Epoch 2 | Step 898700 | Avg Loss: 0.0157 | Grad Norm: 0.01032656\n",
      "Epoch 2 | Step 898800 | Avg Loss: 0.0156 | Grad Norm: 0.00903262\n",
      "Epoch 2 | Step 898900 | Avg Loss: 0.0162 | Grad Norm: 0.00876971\n",
      "Epoch 2 | Step 899000 | Avg Loss: 0.0159 | Grad Norm: 0.00925274\n",
      "Epoch 2 | Step 899100 | Avg Loss: 0.0159 | Grad Norm: 0.00877659\n",
      "Epoch 2 | Step 899200 | Avg Loss: 0.0158 | Grad Norm: 0.00847824\n",
      "Epoch 2 | Step 899300 | Avg Loss: 0.0160 | Grad Norm: 0.00897494\n",
      "Epoch 2 | Step 899400 | Avg Loss: 0.0161 | Grad Norm: 0.01061736\n",
      "Epoch 2 | Step 899500 | Avg Loss: 0.0161 | Grad Norm: 0.00898349\n",
      "Epoch 2 | Step 899600 | Avg Loss: 0.0160 | Grad Norm: 0.00930907\n",
      "Epoch 2 | Step 899700 | Avg Loss: 0.0159 | Grad Norm: 0.00939138\n",
      "Epoch 2 | Step 899800 | Avg Loss: 0.0159 | Grad Norm: 0.00910194\n",
      "Epoch 2 | Step 899900 | Avg Loss: 0.0159 | Grad Norm: 0.00770935\n",
      "Epoch 2 | Step 900000 | Avg Loss: 0.0158 | Grad Norm: 0.00877776\n",
      "Saving model at step900000\n",
      "Epoch 2 | Step 900100 | Avg Loss: 0.0152 | Grad Norm: 0.00979079\n",
      "Epoch 2 | Step 900200 | Avg Loss: 0.0153 | Grad Norm: 0.00918553\n",
      "Epoch 2 | Step 900300 | Avg Loss: 0.0152 | Grad Norm: 0.00835752\n",
      "Epoch 2 | Step 900400 | Avg Loss: 0.0150 | Grad Norm: 0.00784876\n",
      "Epoch 2 | Step 900500 | Avg Loss: 0.0154 | Grad Norm: 0.01002225\n",
      "Epoch 2 | Step 900600 | Avg Loss: 0.0153 | Grad Norm: 0.00825349\n",
      "Epoch 2 | Step 900700 | Avg Loss: 0.0150 | Grad Norm: 0.00879460\n",
      "Epoch 2 | Step 900800 | Avg Loss: 0.0149 | Grad Norm: 0.00754317\n",
      "Epoch 2 | Step 900900 | Avg Loss: 0.0151 | Grad Norm: 0.00806866\n",
      "Epoch 2 | Step 901000 | Avg Loss: 0.0153 | Grad Norm: 0.00965760\n",
      "Epoch 2 | Step 901100 | Avg Loss: 0.0157 | Grad Norm: 0.00805873\n",
      "Epoch 2 | Step 901200 | Avg Loss: 0.0157 | Grad Norm: 0.00876195\n",
      "Epoch 2 | Step 901300 | Avg Loss: 0.0159 | Grad Norm: 0.00988934\n",
      "Epoch 2 | Step 901400 | Avg Loss: 0.0157 | Grad Norm: 0.00828634\n",
      "Epoch 2 | Step 901500 | Avg Loss: 0.0158 | Grad Norm: 0.00972867\n",
      "Epoch 2 | Step 901600 | Avg Loss: 0.0154 | Grad Norm: 0.00946686\n",
      "Epoch 2 | Step 901700 | Avg Loss: 0.0157 | Grad Norm: 0.01007412\n",
      "Epoch 2 | Step 901800 | Avg Loss: 0.0155 | Grad Norm: 0.00913163\n",
      "Epoch 2 | Step 901900 | Avg Loss: 0.0153 | Grad Norm: 0.00823088\n",
      "Epoch 2 | Step 902000 | Avg Loss: 0.0150 | Grad Norm: 0.00770519\n",
      "Epoch 2 | Step 902100 | Avg Loss: 0.0152 | Grad Norm: 0.00940165\n",
      "Epoch 2 | Step 902200 | Avg Loss: 0.0153 | Grad Norm: 0.00891695\n",
      "Epoch 2 | Step 902300 | Avg Loss: 0.0152 | Grad Norm: 0.00944056\n",
      "Epoch 2 | Step 902400 | Avg Loss: 0.0151 | Grad Norm: 0.00834826\n",
      "Epoch 2 | Step 902500 | Avg Loss: 0.0153 | Grad Norm: 0.00848382\n",
      "Epoch 2 | Step 902600 | Avg Loss: 0.0154 | Grad Norm: 0.00786028\n",
      "Epoch 2 | Step 902700 | Avg Loss: 0.0156 | Grad Norm: 0.00880098\n",
      "Epoch 2 | Step 902800 | Avg Loss: 0.0158 | Grad Norm: 0.00794668\n",
      "Epoch 2 | Step 902900 | Avg Loss: 0.0157 | Grad Norm: 0.00898988\n",
      "Epoch 2 | Step 903000 | Avg Loss: 0.0157 | Grad Norm: 0.00875031\n",
      "Epoch 2 | Step 903100 | Avg Loss: 0.0156 | Grad Norm: 0.00978605\n",
      "Epoch 2 | Step 903200 | Avg Loss: 0.0154 | Grad Norm: 0.01006886\n",
      "Epoch 2 | Step 903300 | Avg Loss: 0.0155 | Grad Norm: 0.00877941\n",
      "Epoch 2 | Step 903400 | Avg Loss: 0.0154 | Grad Norm: 0.00893559\n",
      "Epoch 2 | Step 903500 | Avg Loss: 0.0156 | Grad Norm: 0.00795922\n",
      "Epoch 2 | Step 903600 | Avg Loss: 0.0153 | Grad Norm: 0.00948337\n",
      "Epoch 2 | Step 903700 | Avg Loss: 0.0155 | Grad Norm: 0.00817067\n",
      "Epoch 2 | Step 903800 | Avg Loss: 0.0157 | Grad Norm: 0.00778059\n",
      "Epoch 2 | Step 903900 | Avg Loss: 0.0152 | Grad Norm: 0.00883918\n",
      "Epoch 2 | Step 904000 | Avg Loss: 0.0153 | Grad Norm: 0.01131433\n",
      "Epoch 2 | Step 904100 | Avg Loss: 0.0153 | Grad Norm: 0.00841133\n",
      "Epoch 2 | Step 904200 | Avg Loss: 0.0152 | Grad Norm: 0.01147156\n",
      "Epoch 2 | Step 904300 | Avg Loss: 0.0154 | Grad Norm: 0.00954126\n",
      "Epoch 2 | Step 904400 | Avg Loss: 0.0154 | Grad Norm: 0.00869235\n",
      "Epoch 2 | Step 904500 | Avg Loss: 0.0155 | Grad Norm: 0.00855512\n",
      "Epoch 2 | Step 904600 | Avg Loss: 0.0152 | Grad Norm: 0.00747057\n",
      "Epoch 2 | Step 904700 | Avg Loss: 0.0151 | Grad Norm: 0.00881332\n",
      "Epoch 2 | Step 904800 | Avg Loss: 0.0155 | Grad Norm: 0.00874380\n",
      "Epoch 2 | Step 904900 | Avg Loss: 0.0154 | Grad Norm: 0.00967822\n",
      "Epoch 2 | Step 905000 | Avg Loss: 0.0153 | Grad Norm: 0.00900045\n",
      "Epoch 2 | Step 905100 | Avg Loss: 0.0151 | Grad Norm: 0.00872599\n",
      "Epoch 2 | Step 905200 | Avg Loss: 0.0156 | Grad Norm: 0.01142945\n",
      "Epoch 2 | Step 905300 | Avg Loss: 0.0157 | Grad Norm: 0.00782578\n",
      "Epoch 2 | Step 905400 | Avg Loss: 0.0157 | Grad Norm: 0.00833006\n",
      "Epoch 2 | Step 905500 | Avg Loss: 0.0156 | Grad Norm: 0.00975348\n",
      "Epoch 2 | Step 905600 | Avg Loss: 0.0158 | Grad Norm: 0.00805951\n",
      "Epoch 2 | Step 905700 | Avg Loss: 0.0157 | Grad Norm: 0.00768856\n",
      "Epoch 2 | Step 905800 | Avg Loss: 0.0156 | Grad Norm: 0.00764808\n",
      "Epoch 2 | Step 905900 | Avg Loss: 0.0155 | Grad Norm: 0.00816331\n",
      "Epoch 2 | Step 906000 | Avg Loss: 0.0152 | Grad Norm: 0.00859835\n",
      "Epoch 2 | Step 906100 | Avg Loss: 0.0155 | Grad Norm: 0.00972939\n",
      "Epoch 2 | Step 906200 | Avg Loss: 0.0155 | Grad Norm: 0.00967974\n",
      "Epoch 2 | Step 906300 | Avg Loss: 0.0158 | Grad Norm: 0.01127970\n",
      "Epoch 2 | Step 906400 | Avg Loss: 0.0156 | Grad Norm: 0.00828737\n",
      "Epoch 2 | Step 906500 | Avg Loss: 0.0158 | Grad Norm: 0.00953534\n",
      "Epoch 2 | Step 906600 | Avg Loss: 0.0157 | Grad Norm: 0.00813551\n",
      "Epoch 2 | Step 906700 | Avg Loss: 0.0159 | Grad Norm: 0.00833180\n",
      "Epoch 2 | Step 906800 | Avg Loss: 0.0157 | Grad Norm: 0.01018354\n",
      "Epoch 2 | Step 906900 | Avg Loss: 0.0158 | Grad Norm: 0.01024103\n",
      "Epoch 2 | Step 907000 | Avg Loss: 0.0155 | Grad Norm: 0.00932635\n",
      "Epoch 2 | Step 907100 | Avg Loss: 0.0155 | Grad Norm: 0.00841840\n",
      "Epoch 2 | Step 907200 | Avg Loss: 0.0157 | Grad Norm: 0.00884273\n",
      "Epoch 2 | Step 907300 | Avg Loss: 0.0156 | Grad Norm: 0.00848235\n",
      "Epoch 2 | Step 907400 | Avg Loss: 0.0158 | Grad Norm: 0.01160138\n",
      "Epoch 2 | Step 907500 | Avg Loss: 0.0159 | Grad Norm: 0.00891199\n",
      "Epoch 2 | Step 907600 | Avg Loss: 0.0159 | Grad Norm: 0.00802516\n",
      "Epoch 2 | Step 907700 | Avg Loss: 0.0159 | Grad Norm: 0.00781903\n",
      "Epoch 2 | Step 907800 | Avg Loss: 0.0160 | Grad Norm: 0.00901289\n",
      "Epoch 2 | Step 907900 | Avg Loss: 0.0157 | Grad Norm: 0.00832611\n",
      "Epoch 2 | Step 908000 | Avg Loss: 0.0156 | Grad Norm: 0.00998074\n",
      "Epoch 2 | Step 908100 | Avg Loss: 0.0152 | Grad Norm: 0.00833094\n",
      "Epoch 2 | Step 908200 | Avg Loss: 0.0154 | Grad Norm: 0.01058089\n",
      "Epoch 2 | Step 908300 | Avg Loss: 0.0157 | Grad Norm: 0.00972991\n",
      "Epoch 2 | Step 908400 | Avg Loss: 0.0158 | Grad Norm: 0.00855457\n",
      "Epoch 2 | Step 908500 | Avg Loss: 0.0158 | Grad Norm: 0.00848957\n",
      "Epoch 2 | Step 908600 | Avg Loss: 0.0155 | Grad Norm: 0.00862633\n",
      "Epoch 2 | Step 908700 | Avg Loss: 0.0152 | Grad Norm: 0.00886108\n",
      "Epoch 2 | Step 908800 | Avg Loss: 0.0154 | Grad Norm: 0.01044533\n",
      "Epoch 2 | Step 908900 | Avg Loss: 0.0155 | Grad Norm: 0.00985081\n",
      "Epoch 2 | Step 909000 | Avg Loss: 0.0159 | Grad Norm: 0.00806895\n",
      "Epoch 2 | Step 909100 | Avg Loss: 0.0159 | Grad Norm: 0.00860131\n",
      "Epoch 2 | Step 909200 | Avg Loss: 0.0159 | Grad Norm: 0.00799486\n",
      "Epoch 2 | Step 909300 | Avg Loss: 0.0156 | Grad Norm: 0.00914917\n",
      "Epoch 2 | Step 909400 | Avg Loss: 0.0154 | Grad Norm: 0.00885325\n",
      "Epoch 2 | Step 909500 | Avg Loss: 0.0154 | Grad Norm: 0.00905285\n",
      "Epoch 2 | Step 909600 | Avg Loss: 0.0156 | Grad Norm: 0.00804183\n",
      "Epoch 2 | Step 909700 | Avg Loss: 0.0155 | Grad Norm: 0.00944439\n",
      "Epoch 2 | Step 909800 | Avg Loss: 0.0154 | Grad Norm: 0.00899253\n",
      "Epoch 2 | Step 909900 | Avg Loss: 0.0154 | Grad Norm: 0.00951356\n",
      "Epoch 2 | Step 910000 | Avg Loss: 0.0155 | Grad Norm: 0.00834039\n",
      "Epoch 2 | Step 910100 | Avg Loss: 0.0157 | Grad Norm: 0.00855591\n",
      "Epoch 2 | Step 910200 | Avg Loss: 0.0158 | Grad Norm: 0.00849706\n",
      "Epoch 2 | Step 910300 | Avg Loss: 0.0158 | Grad Norm: 0.00859525\n",
      "Epoch 2 | Step 910400 | Avg Loss: 0.0161 | Grad Norm: 0.01200804\n",
      "Epoch 2 | Step 910500 | Avg Loss: 0.0159 | Grad Norm: 0.00909438\n",
      "Epoch 2 | Step 910600 | Avg Loss: 0.0157 | Grad Norm: 0.00796918\n",
      "Epoch 2 | Step 910700 | Avg Loss: 0.0157 | Grad Norm: 0.00955777\n",
      "Epoch 2 | Step 910800 | Avg Loss: 0.0158 | Grad Norm: 0.00823268\n",
      "Epoch 2 | Step 910900 | Avg Loss: 0.0160 | Grad Norm: 0.00958170\n",
      "Epoch 2 | Step 911000 | Avg Loss: 0.0160 | Grad Norm: 0.00995540\n",
      "Epoch 2 | Step 911100 | Avg Loss: 0.0163 | Grad Norm: 0.00848382\n",
      "Epoch 2 | Step 911200 | Avg Loss: 0.0160 | Grad Norm: 0.00833968\n",
      "Epoch 2 | Step 911300 | Avg Loss: 0.0157 | Grad Norm: 0.00751067\n",
      "Epoch 2 | Step 911400 | Avg Loss: 0.0155 | Grad Norm: 0.00899677\n",
      "Epoch 2 | Step 911500 | Avg Loss: 0.0156 | Grad Norm: 0.00836598\n",
      "Epoch 2 | Step 911600 | Avg Loss: 0.0154 | Grad Norm: 0.01051320\n",
      "Epoch 2 | Step 911700 | Avg Loss: 0.0154 | Grad Norm: 0.01112142\n",
      "Epoch 2 | Step 911800 | Avg Loss: 0.0159 | Grad Norm: 0.00872387\n",
      "Epoch 2 | Step 911900 | Avg Loss: 0.0164 | Grad Norm: 0.00859987\n",
      "Epoch 2 | Step 912000 | Avg Loss: 0.0160 | Grad Norm: 0.01032052\n",
      "Epoch 2 | Step 912100 | Avg Loss: 0.0158 | Grad Norm: 0.00971785\n",
      "Epoch 2 | Step 912200 | Avg Loss: 0.0158 | Grad Norm: 0.01056422\n",
      "Epoch 2 | Step 912300 | Avg Loss: 0.0158 | Grad Norm: 0.00837571\n",
      "Epoch 2 | Step 912400 | Avg Loss: 0.0158 | Grad Norm: 0.00844518\n",
      "Epoch 2 | Step 912500 | Avg Loss: 0.0161 | Grad Norm: 0.00972478\n",
      "Epoch 2 | Step 912600 | Avg Loss: 0.0161 | Grad Norm: 0.01318592\n",
      "Epoch 2 | Step 912700 | Avg Loss: 0.0160 | Grad Norm: 0.00791941\n",
      "Epoch 2 | Step 912800 | Avg Loss: 0.0158 | Grad Norm: 0.01008493\n",
      "Epoch 2 | Step 912900 | Avg Loss: 0.0161 | Grad Norm: 0.00856158\n",
      "Epoch 2 | Step 913000 | Avg Loss: 0.0161 | Grad Norm: 0.00943678\n",
      "Epoch 2 | Step 913100 | Avg Loss: 0.0156 | Grad Norm: 0.00938021\n",
      "Epoch 2 | Step 913200 | Avg Loss: 0.0153 | Grad Norm: 0.00916254\n",
      "Epoch 2 | Step 913300 | Avg Loss: 0.0154 | Grad Norm: 0.00810092\n",
      "Epoch 2 | Step 913400 | Avg Loss: 0.0158 | Grad Norm: 0.00956532\n",
      "Epoch 2 | Step 913500 | Avg Loss: 0.0161 | Grad Norm: 0.00915490\n",
      "Epoch 2 | Step 913600 | Avg Loss: 0.0160 | Grad Norm: 0.01040542\n",
      "Epoch 2 | Step 913700 | Avg Loss: 0.0162 | Grad Norm: 0.01215162\n",
      "Epoch 2 | Step 913800 | Avg Loss: 0.0161 | Grad Norm: 0.00803565\n",
      "Epoch 2 | Step 913900 | Avg Loss: 0.0160 | Grad Norm: 0.00846910\n",
      "Epoch 2 | Step 914000 | Avg Loss: 0.0160 | Grad Norm: 0.00888532\n",
      "Epoch 2 | Step 914100 | Avg Loss: 0.0161 | Grad Norm: 0.00983968\n",
      "Epoch 2 | Step 914200 | Avg Loss: 0.0160 | Grad Norm: 0.00961162\n",
      "Epoch 2 | Step 914300 | Avg Loss: 0.0160 | Grad Norm: 0.00900222\n",
      "Epoch 2 | Step 914400 | Avg Loss: 0.0156 | Grad Norm: 0.00964550\n",
      "Epoch 2 | Step 914500 | Avg Loss: 0.0158 | Grad Norm: 0.00983408\n",
      "Epoch 2 | Step 914600 | Avg Loss: 0.0159 | Grad Norm: 0.00949896\n",
      "Epoch 2 | Step 914700 | Avg Loss: 0.0160 | Grad Norm: 0.01161587\n",
      "Epoch 2 | Step 914800 | Avg Loss: 0.0158 | Grad Norm: 0.00836462\n",
      "Epoch 2 | Step 914900 | Avg Loss: 0.0156 | Grad Norm: 0.00850167\n",
      "Epoch 2 | Step 915000 | Avg Loss: 0.0154 | Grad Norm: 0.00978670\n",
      "Epoch 2 | Step 915100 | Avg Loss: 0.0152 | Grad Norm: 0.00944295\n",
      "Epoch 2 | Step 915200 | Avg Loss: 0.0151 | Grad Norm: 0.00857209\n",
      "Epoch 2 | Step 915300 | Avg Loss: 0.0150 | Grad Norm: 0.00791923\n",
      "Epoch 2 | Step 915400 | Avg Loss: 0.0153 | Grad Norm: 0.00857445\n",
      "Epoch 2 | Step 915500 | Avg Loss: 0.0151 | Grad Norm: 0.00845084\n",
      "Epoch 2 | Step 915600 | Avg Loss: 0.0155 | Grad Norm: 0.00866267\n",
      "Epoch 2 | Step 915700 | Avg Loss: 0.0156 | Grad Norm: 0.01022618\n",
      "Epoch 2 | Step 915800 | Avg Loss: 0.0158 | Grad Norm: 0.00838236\n",
      "Epoch 2 | Step 915900 | Avg Loss: 0.0154 | Grad Norm: 0.00884654\n",
      "Epoch 2 | Step 916000 | Avg Loss: 0.0154 | Grad Norm: 0.01112818\n",
      "Epoch 2 | Step 916100 | Avg Loss: 0.0157 | Grad Norm: 0.00835101\n",
      "Epoch 2 | Step 916200 | Avg Loss: 0.0157 | Grad Norm: 0.00884400\n",
      "Epoch 2 | Step 916300 | Avg Loss: 0.0157 | Grad Norm: 0.00836907\n",
      "Epoch 2 | Step 916400 | Avg Loss: 0.0160 | Grad Norm: 0.01026764\n",
      "Epoch 2 | Step 916500 | Avg Loss: 0.0157 | Grad Norm: 0.01081459\n",
      "Epoch 2 | Step 916600 | Avg Loss: 0.0160 | Grad Norm: 0.00946370\n",
      "Epoch 2 | Step 916700 | Avg Loss: 0.0163 | Grad Norm: 0.01064384\n",
      "Epoch 2 | Step 916800 | Avg Loss: 0.0160 | Grad Norm: 0.00748709\n",
      "Epoch 2 | Step 916900 | Avg Loss: 0.0158 | Grad Norm: 0.00946752\n",
      "Epoch 2 | Step 917000 | Avg Loss: 0.0159 | Grad Norm: 0.00992811\n",
      "Epoch 2 | Step 917100 | Avg Loss: 0.0158 | Grad Norm: 0.00916522\n",
      "Epoch 2 | Step 917200 | Avg Loss: 0.0159 | Grad Norm: 0.01086878\n",
      "Epoch 2 | Step 917300 | Avg Loss: 0.0157 | Grad Norm: 0.00886049\n",
      "Epoch 2 | Step 917400 | Avg Loss: 0.0157 | Grad Norm: 0.00975613\n",
      "Epoch 2 | Step 917500 | Avg Loss: 0.0156 | Grad Norm: 0.00811691\n",
      "Epoch 2 | Step 917600 | Avg Loss: 0.0152 | Grad Norm: 0.00902092\n",
      "Epoch 2 | Step 917700 | Avg Loss: 0.0152 | Grad Norm: 0.00833484\n",
      "Epoch 2 | Step 917800 | Avg Loss: 0.0151 | Grad Norm: 0.01032709\n",
      "Epoch 2 | Step 917900 | Avg Loss: 0.0149 | Grad Norm: 0.00810180\n",
      "Epoch 2 | Step 918000 | Avg Loss: 0.0148 | Grad Norm: 0.00947615\n",
      "Epoch 2 | Step 918100 | Avg Loss: 0.0153 | Grad Norm: 0.00916104\n",
      "Epoch 2 | Step 918200 | Avg Loss: 0.0155 | Grad Norm: 0.00966775\n",
      "Epoch 2 | Step 918300 | Avg Loss: 0.0156 | Grad Norm: 0.01020987\n",
      "Epoch 2 | Step 918400 | Avg Loss: 0.0154 | Grad Norm: 0.00761683\n",
      "Epoch 2 | Step 918500 | Avg Loss: 0.0154 | Grad Norm: 0.00823616\n",
      "Epoch 2 | Step 918600 | Avg Loss: 0.0154 | Grad Norm: 0.00810811\n",
      "Epoch 2 | Step 918700 | Avg Loss: 0.0155 | Grad Norm: 0.00823012\n",
      "Epoch 2 | Step 918800 | Avg Loss: 0.0155 | Grad Norm: 0.01083516\n",
      "Epoch 2 | Step 918900 | Avg Loss: 0.0157 | Grad Norm: 0.00808451\n",
      "Epoch 2 | Step 919000 | Avg Loss: 0.0157 | Grad Norm: 0.00876982\n",
      "Epoch 2 | Step 919100 | Avg Loss: 0.0157 | Grad Norm: 0.00855800\n",
      "Epoch 2 | Step 919200 | Avg Loss: 0.0155 | Grad Norm: 0.00874730\n",
      "Epoch 2 | Step 919300 | Avg Loss: 0.0158 | Grad Norm: 0.00894686\n",
      "Epoch 2 | Step 919400 | Avg Loss: 0.0157 | Grad Norm: 0.00876018\n",
      "Epoch 2 | Step 919500 | Avg Loss: 0.0160 | Grad Norm: 0.01058030\n",
      "Epoch 2 | Step 919600 | Avg Loss: 0.0160 | Grad Norm: 0.00893197\n",
      "Epoch 2 | Step 919700 | Avg Loss: 0.0157 | Grad Norm: 0.00744051\n",
      "Epoch 2 | Step 919800 | Avg Loss: 0.0156 | Grad Norm: 0.00840789\n",
      "Epoch 2 | Step 919900 | Avg Loss: 0.0158 | Grad Norm: 0.00812830\n",
      "Epoch 2 | Step 920000 | Avg Loss: 0.0156 | Grad Norm: 0.00908663\n",
      "Epoch 2 | Step 920100 | Avg Loss: 0.0154 | Grad Norm: 0.00851875\n",
      "Epoch 2 | Step 920200 | Avg Loss: 0.0153 | Grad Norm: 0.00963403\n",
      "Epoch 2 | Step 920300 | Avg Loss: 0.0153 | Grad Norm: 0.01083269\n",
      "Epoch 2 | Step 920400 | Avg Loss: 0.0159 | Grad Norm: 0.00896447\n",
      "Epoch 2 | Step 920500 | Avg Loss: 0.0160 | Grad Norm: 0.00911191\n",
      "Epoch 2 | Step 920600 | Avg Loss: 0.0161 | Grad Norm: 0.00968016\n",
      "Epoch 2 | Step 920700 | Avg Loss: 0.0157 | Grad Norm: 0.00877925\n",
      "Epoch 2 | Step 920800 | Avg Loss: 0.0157 | Grad Norm: 0.00786983\n",
      "Epoch 2 | Step 920900 | Avg Loss: 0.0160 | Grad Norm: 0.00925791\n",
      "Epoch 2 | Step 921000 | Avg Loss: 0.0161 | Grad Norm: 0.00889506\n",
      "Epoch 2 | Step 921100 | Avg Loss: 0.0160 | Grad Norm: 0.00917706\n",
      "Epoch 2 | Step 921200 | Avg Loss: 0.0156 | Grad Norm: 0.00994186\n",
      "Epoch 2 | Step 921300 | Avg Loss: 0.0153 | Grad Norm: 0.00900633\n",
      "Epoch 2 | Step 921400 | Avg Loss: 0.0154 | Grad Norm: 0.00995011\n",
      "Epoch 2 | Step 921500 | Avg Loss: 0.0148 | Grad Norm: 0.00835639\n",
      "Epoch 2 | Step 921600 | Avg Loss: 0.0153 | Grad Norm: 0.00963835\n",
      "Epoch 2 | Step 921700 | Avg Loss: 0.0155 | Grad Norm: 0.00752706\n",
      "Epoch 2 | Step 921800 | Avg Loss: 0.0161 | Grad Norm: 0.01060706\n",
      "Epoch 2 | Step 921900 | Avg Loss: 0.0159 | Grad Norm: 0.00946460\n",
      "Epoch 2 | Step 922000 | Avg Loss: 0.0162 | Grad Norm: 0.00857590\n",
      "Epoch 2 | Step 922100 | Avg Loss: 0.0162 | Grad Norm: 0.00853040\n",
      "Epoch 2 | Step 922200 | Avg Loss: 0.0160 | Grad Norm: 0.00977602\n",
      "Epoch 2 | Step 922300 | Avg Loss: 0.0160 | Grad Norm: 0.00804187\n",
      "Epoch 2 | Step 922400 | Avg Loss: 0.0160 | Grad Norm: 0.00873295\n",
      "Epoch 2 | Step 922500 | Avg Loss: 0.0159 | Grad Norm: 0.00893833\n",
      "Epoch 2 | Step 922600 | Avg Loss: 0.0158 | Grad Norm: 0.00854970\n",
      "Epoch 2 | Step 922700 | Avg Loss: 0.0153 | Grad Norm: 0.00846073\n",
      "Epoch 2 | Step 922800 | Avg Loss: 0.0154 | Grad Norm: 0.00897727\n",
      "Epoch 2 | Step 922900 | Avg Loss: 0.0153 | Grad Norm: 0.00875324\n",
      "Epoch 2 | Step 923000 | Avg Loss: 0.0153 | Grad Norm: 0.00812249\n",
      "Epoch 2 | Step 923100 | Avg Loss: 0.0150 | Grad Norm: 0.00828047\n",
      "Epoch 2 | Step 923200 | Avg Loss: 0.0152 | Grad Norm: 0.00826730\n",
      "Epoch 2 | Step 923300 | Avg Loss: 0.0151 | Grad Norm: 0.00885608\n",
      "Epoch 2 | Step 923400 | Avg Loss: 0.0156 | Grad Norm: 0.00860529\n",
      "Epoch 2 | Step 923500 | Avg Loss: 0.0157 | Grad Norm: 0.00861466\n",
      "Epoch 2 | Step 923600 | Avg Loss: 0.0159 | Grad Norm: 0.00773030\n",
      "Epoch 2 | Step 923700 | Avg Loss: 0.0158 | Grad Norm: 0.00787034\n",
      "Epoch 2 | Step 923800 | Avg Loss: 0.0155 | Grad Norm: 0.00942396\n",
      "Epoch 2 | Step 923900 | Avg Loss: 0.0151 | Grad Norm: 0.00891306\n",
      "Epoch 2 | Step 924000 | Avg Loss: 0.0150 | Grad Norm: 0.00879557\n",
      "Epoch 2 | Step 924100 | Avg Loss: 0.0148 | Grad Norm: 0.00869531\n",
      "Epoch 2 | Step 924200 | Avg Loss: 0.0150 | Grad Norm: 0.00846141\n",
      "Epoch 2 | Step 924300 | Avg Loss: 0.0154 | Grad Norm: 0.00993842\n",
      "Epoch 2 | Step 924400 | Avg Loss: 0.0158 | Grad Norm: 0.00947534\n",
      "Epoch 2 | Step 924500 | Avg Loss: 0.0154 | Grad Norm: 0.00808910\n",
      "Epoch 2 | Step 924600 | Avg Loss: 0.0154 | Grad Norm: 0.00807219\n",
      "Epoch 2 | Step 924700 | Avg Loss: 0.0156 | Grad Norm: 0.00777841\n",
      "Epoch 2 | Step 924800 | Avg Loss: 0.0155 | Grad Norm: 0.00890920\n",
      "Epoch 2 | Step 924900 | Avg Loss: 0.0151 | Grad Norm: 0.00836040\n",
      "Epoch 2 | Step 925000 | Avg Loss: 0.0152 | Grad Norm: 0.00770186\n",
      "Epoch 2 | Step 925100 | Avg Loss: 0.0153 | Grad Norm: 0.00862929\n",
      "Epoch 2 | Step 925200 | Avg Loss: 0.0155 | Grad Norm: 0.00980489\n",
      "Epoch 2 | Step 925300 | Avg Loss: 0.0152 | Grad Norm: 0.00917103\n",
      "Epoch 2 | Step 925400 | Avg Loss: 0.0153 | Grad Norm: 0.00840013\n",
      "Epoch 2 | Step 925500 | Avg Loss: 0.0155 | Grad Norm: 0.00875203\n",
      "Epoch 2 | Step 925600 | Avg Loss: 0.0160 | Grad Norm: 0.00997943\n",
      "Epoch 2 | Step 925700 | Avg Loss: 0.0162 | Grad Norm: 0.00781637\n",
      "Epoch 2 | Step 925800 | Avg Loss: 0.0161 | Grad Norm: 0.00849941\n",
      "Epoch 2 | Step 925900 | Avg Loss: 0.0162 | Grad Norm: 0.00943729\n",
      "Epoch 2 | Step 926000 | Avg Loss: 0.0162 | Grad Norm: 0.00933513\n",
      "Epoch 2 | Step 926100 | Avg Loss: 0.0162 | Grad Norm: 0.00978882\n",
      "Epoch 2 | Step 926200 | Avg Loss: 0.0160 | Grad Norm: 0.01182856\n",
      "Epoch 2 | Step 926300 | Avg Loss: 0.0157 | Grad Norm: 0.00853220\n",
      "Epoch 2 | Step 926400 | Avg Loss: 0.0156 | Grad Norm: 0.00938984\n",
      "Epoch 2 | Step 926500 | Avg Loss: 0.0157 | Grad Norm: 0.01012358\n",
      "Epoch 2 | Step 926600 | Avg Loss: 0.0157 | Grad Norm: 0.00767879\n",
      "Epoch 2 | Step 926700 | Avg Loss: 0.0159 | Grad Norm: 0.00884580\n",
      "Epoch 2 | Step 926800 | Avg Loss: 0.0159 | Grad Norm: 0.00905428\n",
      "Epoch 2 | Step 926900 | Avg Loss: 0.0155 | Grad Norm: 0.00814283\n",
      "Epoch 2 | Step 927000 | Avg Loss: 0.0156 | Grad Norm: 0.00804793\n",
      "Epoch 2 | Step 927100 | Avg Loss: 0.0157 | Grad Norm: 0.01019159\n",
      "Epoch 2 | Step 927200 | Avg Loss: 0.0158 | Grad Norm: 0.00988699\n",
      "Epoch 2 | Step 927300 | Avg Loss: 0.0155 | Grad Norm: 0.00715676\n",
      "Epoch 2 | Step 927400 | Avg Loss: 0.0156 | Grad Norm: 0.00888645\n",
      "Epoch 2 | Step 927500 | Avg Loss: 0.0155 | Grad Norm: 0.00819139\n",
      "Epoch 2 | Step 927600 | Avg Loss: 0.0157 | Grad Norm: 0.00875428\n",
      "Epoch 2 | Step 927700 | Avg Loss: 0.0160 | Grad Norm: 0.01122453\n",
      "Epoch 2 | Step 927800 | Avg Loss: 0.0152 | Grad Norm: 0.00811571\n",
      "Epoch 2 | Step 927900 | Avg Loss: 0.0150 | Grad Norm: 0.00796651\n",
      "Epoch 2 | Step 928000 | Avg Loss: 0.0153 | Grad Norm: 0.00860769\n",
      "Epoch 2 | Step 928100 | Avg Loss: 0.0153 | Grad Norm: 0.00953785\n",
      "Epoch 2 | Step 928200 | Avg Loss: 0.0157 | Grad Norm: 0.00757851\n",
      "Epoch 2 | Step 928300 | Avg Loss: 0.0163 | Grad Norm: 0.01008970\n",
      "Epoch 2 | Step 928400 | Avg Loss: 0.0163 | Grad Norm: 0.00846467\n",
      "Epoch 2 | Step 928500 | Avg Loss: 0.0163 | Grad Norm: 0.01179043\n",
      "Epoch 2 | Step 928600 | Avg Loss: 0.0161 | Grad Norm: 0.00899786\n",
      "Epoch 2 | Step 928700 | Avg Loss: 0.0157 | Grad Norm: 0.01244857\n",
      "Epoch 2 | Step 928800 | Avg Loss: 0.0155 | Grad Norm: 0.00980462\n",
      "Epoch 2 | Step 928900 | Avg Loss: 0.0155 | Grad Norm: 0.00964946\n",
      "Epoch 2 | Step 929000 | Avg Loss: 0.0155 | Grad Norm: 0.00729562\n",
      "Epoch 2 | Step 929100 | Avg Loss: 0.0154 | Grad Norm: 0.00778478\n",
      "Epoch 2 | Step 929200 | Avg Loss: 0.0155 | Grad Norm: 0.01027310\n",
      "Epoch 2 | Step 929300 | Avg Loss: 0.0156 | Grad Norm: 0.00899555\n",
      "Epoch 2 | Step 929400 | Avg Loss: 0.0155 | Grad Norm: 0.00710805\n",
      "Epoch 2 | Step 929500 | Avg Loss: 0.0156 | Grad Norm: 0.01025330\n",
      "Epoch 2 | Step 929600 | Avg Loss: 0.0155 | Grad Norm: 0.00832905\n",
      "Epoch 2 | Step 929700 | Avg Loss: 0.0154 | Grad Norm: 0.01215714\n",
      "Epoch 2 | Step 929800 | Avg Loss: 0.0158 | Grad Norm: 0.00904249\n",
      "Epoch 2 | Step 929900 | Avg Loss: 0.0160 | Grad Norm: 0.00846628\n",
      "Epoch 2 | Step 930000 | Avg Loss: 0.0160 | Grad Norm: 0.00993295\n",
      "Epoch 2 | Step 930100 | Avg Loss: 0.0158 | Grad Norm: 0.01023670\n",
      "Epoch 2 | Step 930200 | Avg Loss: 0.0157 | Grad Norm: 0.00806269\n",
      "Epoch 2 | Step 930300 | Avg Loss: 0.0154 | Grad Norm: 0.00830930\n",
      "Epoch 2 | Step 930400 | Avg Loss: 0.0156 | Grad Norm: 0.00726575\n",
      "Epoch 2 | Step 930500 | Avg Loss: 0.0157 | Grad Norm: 0.00951178\n",
      "Epoch 2 | Step 930600 | Avg Loss: 0.0157 | Grad Norm: 0.00870613\n",
      "Epoch 2 | Step 930700 | Avg Loss: 0.0155 | Grad Norm: 0.00756858\n",
      "Epoch 2 | Step 930800 | Avg Loss: 0.0155 | Grad Norm: 0.00783690\n",
      "Epoch 2 | Step 930900 | Avg Loss: 0.0155 | Grad Norm: 0.00724064\n",
      "Epoch 2 | Step 931000 | Avg Loss: 0.0155 | Grad Norm: 0.00765504\n",
      "Epoch 2 | Step 931100 | Avg Loss: 0.0153 | Grad Norm: 0.01001838\n",
      "Epoch 2 | Step 931200 | Avg Loss: 0.0149 | Grad Norm: 0.00953245\n",
      "Epoch 2 | Step 931300 | Avg Loss: 0.0152 | Grad Norm: 0.00844718\n",
      "Epoch 2 | Step 931400 | Avg Loss: 0.0151 | Grad Norm: 0.00788329\n",
      "Epoch 2 | Step 931500 | Avg Loss: 0.0153 | Grad Norm: 0.00753615\n",
      "Epoch 2 | Step 931600 | Avg Loss: 0.0154 | Grad Norm: 0.00768543\n",
      "Epoch 2 | Step 931700 | Avg Loss: 0.0157 | Grad Norm: 0.00928469\n",
      "Epoch 2 | Step 931800 | Avg Loss: 0.0154 | Grad Norm: 0.00919728\n",
      "Epoch 2 | Step 931900 | Avg Loss: 0.0154 | Grad Norm: 0.00908729\n",
      "Epoch 2 | Step 932000 | Avg Loss: 0.0153 | Grad Norm: 0.01014282\n",
      "Epoch 2 | Step 932100 | Avg Loss: 0.0153 | Grad Norm: 0.00809086\n",
      "Epoch 2 | Step 932200 | Avg Loss: 0.0149 | Grad Norm: 0.00832612\n",
      "Epoch 2 | Step 932300 | Avg Loss: 0.0150 | Grad Norm: 0.00985046\n",
      "Epoch 2 | Step 932400 | Avg Loss: 0.0149 | Grad Norm: 0.00850485\n",
      "Epoch 2 | Step 932500 | Avg Loss: 0.0149 | Grad Norm: 0.00985055\n",
      "Epoch 2 | Step 932600 | Avg Loss: 0.0149 | Grad Norm: 0.00902828\n",
      "Epoch 2 | Step 932700 | Avg Loss: 0.0149 | Grad Norm: 0.00822216\n",
      "Epoch 2 | Step 932800 | Avg Loss: 0.0149 | Grad Norm: 0.00939975\n",
      "Epoch 2 | Step 932900 | Avg Loss: 0.0150 | Grad Norm: 0.00846817\n",
      "Epoch 2 | Step 933000 | Avg Loss: 0.0150 | Grad Norm: 0.01136705\n",
      "Epoch 2 | Step 933100 | Avg Loss: 0.0152 | Grad Norm: 0.01000647\n",
      "Epoch 2 | Step 933200 | Avg Loss: 0.0153 | Grad Norm: 0.00755794\n",
      "Epoch 2 | Step 933300 | Avg Loss: 0.0150 | Grad Norm: 0.00769422\n",
      "Epoch 2 | Step 933400 | Avg Loss: 0.0151 | Grad Norm: 0.00923508\n",
      "Epoch 2 | Step 933500 | Avg Loss: 0.0152 | Grad Norm: 0.00843743\n",
      "Epoch 2 | Step 933600 | Avg Loss: 0.0157 | Grad Norm: 0.00785845\n",
      "Epoch 2 | Step 933700 | Avg Loss: 0.0156 | Grad Norm: 0.00888942\n",
      "Epoch 2 | Step 933800 | Avg Loss: 0.0157 | Grad Norm: 0.00772657\n",
      "Epoch 2 | Step 933900 | Avg Loss: 0.0155 | Grad Norm: 0.00788774\n",
      "Epoch 2 | Step 934000 | Avg Loss: 0.0157 | Grad Norm: 0.00929280\n",
      "Epoch 2 | Step 934100 | Avg Loss: 0.0155 | Grad Norm: 0.00841652\n",
      "Epoch 2 | Step 934200 | Avg Loss: 0.0154 | Grad Norm: 0.00945795\n",
      "Epoch 2 | Step 934300 | Avg Loss: 0.0153 | Grad Norm: 0.00902580\n",
      "Epoch 2 | Step 934400 | Avg Loss: 0.0154 | Grad Norm: 0.00813359\n",
      "Epoch 2 | Step 934500 | Avg Loss: 0.0152 | Grad Norm: 0.00858154\n",
      "Epoch 2 | Step 934600 | Avg Loss: 0.0151 | Grad Norm: 0.00907027\n",
      "Epoch 2 | Step 934700 | Avg Loss: 0.0157 | Grad Norm: 0.00793243\n",
      "Epoch 2 | Step 934800 | Avg Loss: 0.0156 | Grad Norm: 0.00862993\n",
      "Epoch 2 | Step 934900 | Avg Loss: 0.0151 | Grad Norm: 0.00826114\n",
      "Epoch 2 | Step 935000 | Avg Loss: 0.0153 | Grad Norm: 0.00896114\n",
      "Epoch 2 | Step 935100 | Avg Loss: 0.0156 | Grad Norm: 0.00879257\n",
      "Epoch 2 | Step 935200 | Avg Loss: 0.0159 | Grad Norm: 0.00888615\n",
      "Epoch 2 | Step 935300 | Avg Loss: 0.0158 | Grad Norm: 0.00852553\n",
      "Epoch 2 | Step 935400 | Avg Loss: 0.0160 | Grad Norm: 0.00777651\n",
      "Epoch 2 | Step 935500 | Avg Loss: 0.0157 | Grad Norm: 0.00998444\n",
      "Epoch 2 | Step 935600 | Avg Loss: 0.0153 | Grad Norm: 0.00870950\n",
      "Epoch 2 | Step 935700 | Avg Loss: 0.0149 | Grad Norm: 0.00770587\n",
      "Epoch 2 | Step 935800 | Avg Loss: 0.0151 | Grad Norm: 0.00804598\n",
      "Epoch 2 | Step 935900 | Avg Loss: 0.0150 | Grad Norm: 0.00937995\n",
      "Epoch 2 | Step 936000 | Avg Loss: 0.0154 | Grad Norm: 0.00866122\n",
      "Epoch 2 | Step 936100 | Avg Loss: 0.0158 | Grad Norm: 0.00975641\n",
      "Epoch 2 | Step 936200 | Avg Loss: 0.0158 | Grad Norm: 0.00902619\n",
      "Epoch 2 | Step 936300 | Avg Loss: 0.0156 | Grad Norm: 0.00840592\n",
      "Epoch 2 | Step 936400 | Avg Loss: 0.0157 | Grad Norm: 0.00879441\n",
      "Epoch 2 | Step 936500 | Avg Loss: 0.0156 | Grad Norm: 0.00894262\n",
      "Epoch 2 | Step 936600 | Avg Loss: 0.0155 | Grad Norm: 0.00906514\n",
      "Epoch 2 | Step 936700 | Avg Loss: 0.0152 | Grad Norm: 0.01153173\n",
      "Epoch 2 | Step 936800 | Avg Loss: 0.0151 | Grad Norm: 0.00978498\n",
      "Epoch 2 | Step 936900 | Avg Loss: 0.0149 | Grad Norm: 0.00915755\n",
      "Epoch 2 | Step 937000 | Avg Loss: 0.0148 | Grad Norm: 0.00741460\n",
      "Epoch 2 | Step 937100 | Avg Loss: 0.0146 | Grad Norm: 0.00759367\n",
      "Epoch 2 | Step 937200 | Avg Loss: 0.0147 | Grad Norm: 0.00844077\n",
      "Epoch 2 | Step 937300 | Avg Loss: 0.0149 | Grad Norm: 0.00843569\n",
      "Epoch 2 | Step 937400 | Avg Loss: 0.0154 | Grad Norm: 0.00862604\n",
      "Epoch 2 | Step 937500 | Avg Loss: 0.0158 | Grad Norm: 0.00739460\n",
      "Epoch 2 | Step 937600 | Avg Loss: 0.0157 | Grad Norm: 0.00857505\n",
      "Epoch 2 | Step 937700 | Avg Loss: 0.0158 | Grad Norm: 0.00928522\n",
      "Epoch 2 | Step 937800 | Avg Loss: 0.0157 | Grad Norm: 0.00913183\n",
      "Epoch 2 | Step 937900 | Avg Loss: 0.0157 | Grad Norm: 0.01025222\n",
      "Epoch 2 | Step 938000 | Avg Loss: 0.0153 | Grad Norm: 0.00834176\n",
      "Epoch 2 | Step 938100 | Avg Loss: 0.0153 | Grad Norm: 0.00836571\n",
      "Epoch 2 | Step 938200 | Avg Loss: 0.0154 | Grad Norm: 0.00908706\n",
      "Epoch 2 | Step 938300 | Avg Loss: 0.0153 | Grad Norm: 0.00978739\n",
      "Epoch 2 | Step 938400 | Avg Loss: 0.0153 | Grad Norm: 0.01176395\n",
      "Epoch 2 | Step 938500 | Avg Loss: 0.0154 | Grad Norm: 0.00793102\n",
      "Epoch 2 | Step 938600 | Avg Loss: 0.0155 | Grad Norm: 0.00842228\n",
      "Epoch 2 | Step 938700 | Avg Loss: 0.0157 | Grad Norm: 0.00807691\n",
      "Epoch 2 | Step 938800 | Avg Loss: 0.0156 | Grad Norm: 0.00909513\n",
      "Epoch 2 | Step 938900 | Avg Loss: 0.0153 | Grad Norm: 0.00966483\n",
      "Epoch 2 | Step 939000 | Avg Loss: 0.0154 | Grad Norm: 0.00797102\n",
      "Epoch 2 | Step 939100 | Avg Loss: 0.0152 | Grad Norm: 0.00889573\n",
      "Epoch 2 | Step 939200 | Avg Loss: 0.0154 | Grad Norm: 0.00894288\n",
      "Epoch 2 | Step 939300 | Avg Loss: 0.0154 | Grad Norm: 0.00828387\n",
      "Epoch 2 | Step 939400 | Avg Loss: 0.0153 | Grad Norm: 0.00782863\n",
      "Epoch 2 | Step 939500 | Avg Loss: 0.0156 | Grad Norm: 0.00910358\n",
      "Epoch 2 | Step 939600 | Avg Loss: 0.0157 | Grad Norm: 0.00886291\n",
      "Epoch 2 | Step 939700 | Avg Loss: 0.0157 | Grad Norm: 0.00844453\n",
      "Epoch 2 | Step 939800 | Avg Loss: 0.0156 | Grad Norm: 0.00801141\n",
      "Epoch 2 | Step 939900 | Avg Loss: 0.0156 | Grad Norm: 0.00877403\n",
      "Epoch 2 | Step 940000 | Avg Loss: 0.0158 | Grad Norm: 0.00833694\n",
      "Epoch 2 | Step 940100 | Avg Loss: 0.0157 | Grad Norm: 0.00848090\n",
      "Epoch 2 | Step 940200 | Avg Loss: 0.0157 | Grad Norm: 0.00822077\n",
      "Epoch 2 | Step 940300 | Avg Loss: 0.0160 | Grad Norm: 0.00804067\n",
      "Epoch 2 | Step 940400 | Avg Loss: 0.0156 | Grad Norm: 0.01051864\n",
      "Epoch 2 | Step 940500 | Avg Loss: 0.0159 | Grad Norm: 0.00785050\n",
      "Epoch 2 | Step 940600 | Avg Loss: 0.0158 | Grad Norm: 0.00752057\n",
      "Epoch 2 | Step 940700 | Avg Loss: 0.0160 | Grad Norm: 0.00854041\n",
      "Epoch 2 | Step 940800 | Avg Loss: 0.0158 | Grad Norm: 0.00870239\n",
      "Epoch 2 | Step 940900 | Avg Loss: 0.0159 | Grad Norm: 0.00930746\n",
      "Epoch 2 | Step 941000 | Avg Loss: 0.0158 | Grad Norm: 0.00812332\n",
      "Epoch 2 | Step 941100 | Avg Loss: 0.0161 | Grad Norm: 0.00852386\n",
      "Epoch 2 | Step 941200 | Avg Loss: 0.0158 | Grad Norm: 0.00787884\n",
      "Epoch 2 | Step 941300 | Avg Loss: 0.0156 | Grad Norm: 0.00919844\n",
      "Epoch 2 | Step 941400 | Avg Loss: 0.0158 | Grad Norm: 0.00950956\n",
      "Epoch 2 | Step 941500 | Avg Loss: 0.0156 | Grad Norm: 0.00766062\n",
      "Epoch 2 | Step 941600 | Avg Loss: 0.0153 | Grad Norm: 0.00847550\n",
      "Epoch 2 | Step 941700 | Avg Loss: 0.0156 | Grad Norm: 0.00804481\n",
      "Epoch 2 | Step 941800 | Avg Loss: 0.0154 | Grad Norm: 0.00885505\n",
      "Epoch 2 | Step 941900 | Avg Loss: 0.0155 | Grad Norm: 0.00876320\n",
      "Epoch 2 | Step 942000 | Avg Loss: 0.0154 | Grad Norm: 0.00830248\n",
      "Epoch 2 | Step 942100 | Avg Loss: 0.0154 | Grad Norm: 0.00988995\n",
      "Epoch 2 | Step 942200 | Avg Loss: 0.0152 | Grad Norm: 0.01076099\n",
      "Epoch 2 | Step 942300 | Avg Loss: 0.0153 | Grad Norm: 0.00908637\n",
      "Epoch 2 | Step 942400 | Avg Loss: 0.0154 | Grad Norm: 0.00854890\n",
      "Epoch 2 | Step 942500 | Avg Loss: 0.0153 | Grad Norm: 0.00828537\n",
      "Epoch 2 | Step 942600 | Avg Loss: 0.0156 | Grad Norm: 0.01275645\n",
      "Epoch 2 | Step 942700 | Avg Loss: 0.0152 | Grad Norm: 0.00883566\n",
      "Epoch 2 | Step 942800 | Avg Loss: 0.0152 | Grad Norm: 0.00795279\n",
      "Epoch 2 | Step 942900 | Avg Loss: 0.0153 | Grad Norm: 0.00861258\n",
      "Epoch 2 | Step 943000 | Avg Loss: 0.0158 | Grad Norm: 0.01161190\n",
      "Epoch 2 | Step 943100 | Avg Loss: 0.0151 | Grad Norm: 0.01040687\n",
      "Epoch 2 | Step 943200 | Avg Loss: 0.0151 | Grad Norm: 0.01071688\n",
      "Epoch 2 | Step 943300 | Avg Loss: 0.0152 | Grad Norm: 0.00915583\n",
      "Epoch 2 | Step 943400 | Avg Loss: 0.0156 | Grad Norm: 0.00853216\n",
      "Epoch 2 | Step 943500 | Avg Loss: 0.0157 | Grad Norm: 0.00872324\n",
      "Epoch 2 | Step 943600 | Avg Loss: 0.0154 | Grad Norm: 0.00936901\n",
      "Epoch 2 | Step 943700 | Avg Loss: 0.0152 | Grad Norm: 0.00877142\n",
      "Epoch 2 | Step 943800 | Avg Loss: 0.0150 | Grad Norm: 0.00976779\n",
      "Epoch 2 | Step 943900 | Avg Loss: 0.0153 | Grad Norm: 0.00944544\n",
      "Epoch 2 | Step 944000 | Avg Loss: 0.0156 | Grad Norm: 0.01013791\n",
      "Epoch 2 | Step 944100 | Avg Loss: 0.0159 | Grad Norm: 0.00794686\n",
      "Epoch 2 | Step 944200 | Avg Loss: 0.0157 | Grad Norm: 0.00838338\n",
      "Epoch 2 | Step 944300 | Avg Loss: 0.0156 | Grad Norm: 0.00838031\n",
      "Epoch 2 | Step 944400 | Avg Loss: 0.0160 | Grad Norm: 0.00861301\n",
      "Epoch 2 | Step 944500 | Avg Loss: 0.0157 | Grad Norm: 0.00854556\n",
      "Epoch 2 | Step 944600 | Avg Loss: 0.0158 | Grad Norm: 0.00993556\n",
      "Epoch 2 | Step 944700 | Avg Loss: 0.0158 | Grad Norm: 0.00893366\n",
      "Epoch 2 | Step 944800 | Avg Loss: 0.0160 | Grad Norm: 0.00972770\n",
      "Epoch 2 | Step 944900 | Avg Loss: 0.0156 | Grad Norm: 0.00821183\n",
      "Epoch 2 | Step 945000 | Avg Loss: 0.0157 | Grad Norm: 0.00966957\n",
      "Epoch 2 | Step 945100 | Avg Loss: 0.0156 | Grad Norm: 0.00837824\n",
      "Epoch 2 | Step 945200 | Avg Loss: 0.0159 | Grad Norm: 0.00823986\n",
      "Epoch 2 | Step 945300 | Avg Loss: 0.0159 | Grad Norm: 0.00844192\n",
      "Epoch 2 | Step 945400 | Avg Loss: 0.0161 | Grad Norm: 0.00863095\n",
      "Epoch 2 | Step 945500 | Avg Loss: 0.0164 | Grad Norm: 0.00792166\n",
      "Epoch 2 | Step 945600 | Avg Loss: 0.0162 | Grad Norm: 0.00933182\n",
      "Epoch 2 | Step 945700 | Avg Loss: 0.0157 | Grad Norm: 0.00935968\n",
      "Epoch 2 | Step 945800 | Avg Loss: 0.0154 | Grad Norm: 0.00755295\n",
      "Epoch 2 | Step 945900 | Avg Loss: 0.0155 | Grad Norm: 0.01000635\n",
      "Epoch 2 | Step 946000 | Avg Loss: 0.0151 | Grad Norm: 0.00838242\n",
      "Epoch 2 | Step 946100 | Avg Loss: 0.0154 | Grad Norm: 0.00877878\n",
      "Epoch 2 | Step 946200 | Avg Loss: 0.0155 | Grad Norm: 0.00901353\n",
      "Epoch 2 | Step 946300 | Avg Loss: 0.0154 | Grad Norm: 0.00959308\n",
      "Epoch 2 | Step 946400 | Avg Loss: 0.0151 | Grad Norm: 0.00882867\n",
      "Epoch 2 | Step 946500 | Avg Loss: 0.0153 | Grad Norm: 0.00835798\n",
      "Epoch 2 | Step 946600 | Avg Loss: 0.0159 | Grad Norm: 0.00882763\n",
      "Epoch 2 | Step 946700 | Avg Loss: 0.0159 | Grad Norm: 0.00924534\n",
      "Epoch 2 | Step 946800 | Avg Loss: 0.0158 | Grad Norm: 0.00781398\n",
      "Epoch 2 | Step 946900 | Avg Loss: 0.0156 | Grad Norm: 0.00818751\n",
      "Epoch 2 | Step 947000 | Avg Loss: 0.0155 | Grad Norm: 0.00852585\n",
      "Epoch 2 | Step 947100 | Avg Loss: 0.0155 | Grad Norm: 0.01011800\n",
      "Epoch 2 | Step 947200 | Avg Loss: 0.0153 | Grad Norm: 0.01025037\n",
      "Epoch 2 | Step 947300 | Avg Loss: 0.0156 | Grad Norm: 0.00723181\n",
      "Epoch 2 | Step 947400 | Avg Loss: 0.0152 | Grad Norm: 0.01077198\n",
      "Epoch 2 | Step 947500 | Avg Loss: 0.0156 | Grad Norm: 0.00862733\n",
      "Epoch 2 | Step 947600 | Avg Loss: 0.0162 | Grad Norm: 0.01257405\n",
      "Epoch 2 | Step 947700 | Avg Loss: 0.0162 | Grad Norm: 0.01224448\n",
      "Epoch 2 | Step 947800 | Avg Loss: 0.0161 | Grad Norm: 0.00821447\n",
      "Epoch 2 | Step 947900 | Avg Loss: 0.0159 | Grad Norm: 0.00878331\n",
      "Epoch 2 | Step 948000 | Avg Loss: 0.0162 | Grad Norm: 0.00817298\n",
      "Epoch 2 | Step 948100 | Avg Loss: 0.0160 | Grad Norm: 0.00876606\n",
      "Epoch 2 | Step 948200 | Avg Loss: 0.0157 | Grad Norm: 0.00727143\n",
      "Epoch 2 | Step 948300 | Avg Loss: 0.0157 | Grad Norm: 0.00839770\n",
      "Epoch 2 | Step 948400 | Avg Loss: 0.0163 | Grad Norm: 0.00797088\n",
      "Epoch 2 | Step 948500 | Avg Loss: 0.0164 | Grad Norm: 0.00816991\n",
      "Epoch 2 | Step 948600 | Avg Loss: 0.0161 | Grad Norm: 0.01047283\n",
      "Epoch 2 | Step 948700 | Avg Loss: 0.0160 | Grad Norm: 0.00852348\n",
      "Epoch 2 | Step 948800 | Avg Loss: 0.0161 | Grad Norm: 0.00913069\n",
      "Epoch 2 | Step 948900 | Avg Loss: 0.0162 | Grad Norm: 0.00904558\n",
      "Epoch 2 | Step 949000 | Avg Loss: 0.0163 | Grad Norm: 0.00879346\n",
      "Epoch 2 | Step 949100 | Avg Loss: 0.0162 | Grad Norm: 0.00851488\n",
      "Epoch 2 | Step 949200 | Avg Loss: 0.0157 | Grad Norm: 0.00872001\n",
      "Epoch 2 | Step 949300 | Avg Loss: 0.0160 | Grad Norm: 0.00980226\n",
      "Epoch 2 | Step 949400 | Avg Loss: 0.0162 | Grad Norm: 0.01065704\n",
      "Epoch 2 | Step 949500 | Avg Loss: 0.0162 | Grad Norm: 0.00893666\n",
      "Epoch 2 | Step 949600 | Avg Loss: 0.0160 | Grad Norm: 0.00880309\n",
      "Epoch 2 | Step 949700 | Avg Loss: 0.0156 | Grad Norm: 0.00886807\n",
      "Epoch 2 | Step 949800 | Avg Loss: 0.0157 | Grad Norm: 0.00931087\n",
      "Epoch 2 | Step 949900 | Avg Loss: 0.0160 | Grad Norm: 0.00865911\n",
      "Epoch 2 | Step 950000 | Avg Loss: 0.0155 | Grad Norm: 0.01016588\n",
      "Epoch 2 | Step 950100 | Avg Loss: 0.0158 | Grad Norm: 0.00795967\n",
      "Epoch 2 | Step 950200 | Avg Loss: 0.0155 | Grad Norm: 0.00820806\n",
      "Epoch 2 | Step 950300 | Avg Loss: 0.0152 | Grad Norm: 0.00865724\n",
      "Epoch 2 | Step 950400 | Avg Loss: 0.0152 | Grad Norm: 0.00888458\n",
      "Epoch 2 | Step 950500 | Avg Loss: 0.0153 | Grad Norm: 0.00931753\n",
      "Epoch 2 | Step 950600 | Avg Loss: 0.0151 | Grad Norm: 0.00917974\n",
      "Epoch 2 | Step 950700 | Avg Loss: 0.0152 | Grad Norm: 0.00945127\n",
      "Epoch 2 | Step 950800 | Avg Loss: 0.0150 | Grad Norm: 0.00869714\n",
      "Epoch 2 | Step 950900 | Avg Loss: 0.0151 | Grad Norm: 0.00942252\n",
      "Epoch 2 | Step 951000 | Avg Loss: 0.0154 | Grad Norm: 0.00783631\n",
      "Epoch 2 | Step 951100 | Avg Loss: 0.0151 | Grad Norm: 0.00809318\n",
      "Epoch 2 | Step 951200 | Avg Loss: 0.0151 | Grad Norm: 0.00855886\n",
      "Epoch 2 | Step 951300 | Avg Loss: 0.0152 | Grad Norm: 0.00897439\n",
      "Epoch 2 | Step 951400 | Avg Loss: 0.0152 | Grad Norm: 0.00820160\n",
      "Epoch 2 | Step 951500 | Avg Loss: 0.0153 | Grad Norm: 0.00907072\n",
      "Epoch 2 | Step 951600 | Avg Loss: 0.0157 | Grad Norm: 0.00825906\n",
      "Epoch 2 | Step 951700 | Avg Loss: 0.0154 | Grad Norm: 0.00913471\n",
      "Epoch 2 | Step 951800 | Avg Loss: 0.0156 | Grad Norm: 0.00744720\n",
      "Epoch 2 | Step 951900 | Avg Loss: 0.0158 | Grad Norm: 0.00859108\n",
      "Epoch 2 | Step 952000 | Avg Loss: 0.0157 | Grad Norm: 0.00915272\n",
      "Epoch 2 | Step 952100 | Avg Loss: 0.0156 | Grad Norm: 0.01000762\n",
      "Epoch 2 | Step 952200 | Avg Loss: 0.0157 | Grad Norm: 0.00843791\n",
      "Epoch 2 | Step 952300 | Avg Loss: 0.0153 | Grad Norm: 0.00795025\n",
      "Epoch 2 | Step 952400 | Avg Loss: 0.0154 | Grad Norm: 0.01032109\n",
      "Epoch 2 | Step 952500 | Avg Loss: 0.0155 | Grad Norm: 0.00866473\n",
      "Epoch 2 | Step 952600 | Avg Loss: 0.0158 | Grad Norm: 0.00895309\n",
      "Epoch 2 | Step 952700 | Avg Loss: 0.0159 | Grad Norm: 0.00929329\n",
      "Epoch 2 | Step 952800 | Avg Loss: 0.0158 | Grad Norm: 0.00804289\n",
      "Epoch 2 | Step 952900 | Avg Loss: 0.0156 | Grad Norm: 0.00853625\n",
      "Epoch 2 | Step 953000 | Avg Loss: 0.0158 | Grad Norm: 0.00847591\n",
      "Epoch 2 | Step 953100 | Avg Loss: 0.0159 | Grad Norm: 0.00763752\n",
      "Epoch 2 | Step 953200 | Avg Loss: 0.0159 | Grad Norm: 0.00764109\n",
      "Epoch 2 | Step 953300 | Avg Loss: 0.0157 | Grad Norm: 0.00957000\n",
      "Epoch 2 | Step 953400 | Avg Loss: 0.0157 | Grad Norm: 0.00882817\n",
      "Epoch 2 | Step 953500 | Avg Loss: 0.0155 | Grad Norm: 0.00843068\n",
      "Epoch 2 | Step 953600 | Avg Loss: 0.0153 | Grad Norm: 0.00901241\n",
      "Epoch 2 | Step 953700 | Avg Loss: 0.0153 | Grad Norm: 0.00930516\n",
      "Epoch 2 | Step 953800 | Avg Loss: 0.0148 | Grad Norm: 0.00867706\n",
      "Epoch 2 | Step 953900 | Avg Loss: 0.0148 | Grad Norm: 0.00939623\n",
      "Epoch 2 | Step 954000 | Avg Loss: 0.0150 | Grad Norm: 0.00815306\n",
      "Epoch 2 | Step 954100 | Avg Loss: 0.0148 | Grad Norm: 0.00816310\n",
      "Epoch 2 | Step 954200 | Avg Loss: 0.0150 | Grad Norm: 0.00959527\n",
      "Epoch 2 | Step 954300 | Avg Loss: 0.0150 | Grad Norm: 0.00795438\n",
      "Epoch 2 | Step 954400 | Avg Loss: 0.0156 | Grad Norm: 0.00881399\n",
      "Epoch 2 | Step 954500 | Avg Loss: 0.0160 | Grad Norm: 0.00761743\n",
      "Epoch 2 | Step 954600 | Avg Loss: 0.0159 | Grad Norm: 0.00866619\n",
      "Epoch 2 | Step 954700 | Avg Loss: 0.0160 | Grad Norm: 0.00909380\n",
      "Epoch 2 | Step 954800 | Avg Loss: 0.0158 | Grad Norm: 0.00856329\n",
      "Epoch 2 | Step 954900 | Avg Loss: 0.0158 | Grad Norm: 0.00749447\n",
      "Epoch 2 | Step 955000 | Avg Loss: 0.0157 | Grad Norm: 0.00868850\n",
      "Epoch 2 | Step 955100 | Avg Loss: 0.0159 | Grad Norm: 0.01133807\n",
      "Epoch 2 | Step 955200 | Avg Loss: 0.0162 | Grad Norm: 0.00909452\n",
      "Epoch 2 | Step 955300 | Avg Loss: 0.0164 | Grad Norm: 0.00894918\n",
      "Epoch 2 | Step 955400 | Avg Loss: 0.0160 | Grad Norm: 0.00852752\n",
      "Epoch 2 | Step 955500 | Avg Loss: 0.0155 | Grad Norm: 0.00846310\n",
      "Epoch 2 | Step 955600 | Avg Loss: 0.0161 | Grad Norm: 0.01020805\n",
      "Epoch 2 | Step 955700 | Avg Loss: 0.0160 | Grad Norm: 0.01018425\n",
      "Epoch 2 | Step 955800 | Avg Loss: 0.0160 | Grad Norm: 0.00865094\n",
      "Epoch 2 | Step 955900 | Avg Loss: 0.0157 | Grad Norm: 0.00809336\n",
      "Epoch 2 | Step 956000 | Avg Loss: 0.0161 | Grad Norm: 0.00858400\n",
      "Epoch 2 | Step 956100 | Avg Loss: 0.0160 | Grad Norm: 0.00826600\n",
      "Epoch 2 | Step 956200 | Avg Loss: 0.0160 | Grad Norm: 0.00918624\n",
      "Epoch 2 | Step 956300 | Avg Loss: 0.0160 | Grad Norm: 0.00972659\n",
      "Epoch 2 | Step 956400 | Avg Loss: 0.0156 | Grad Norm: 0.00961645\n",
      "Epoch 2 | Step 956500 | Avg Loss: 0.0154 | Grad Norm: 0.00832007\n",
      "Epoch 2 | Step 956600 | Avg Loss: 0.0154 | Grad Norm: 0.00938842\n",
      "Epoch 2 | Step 956700 | Avg Loss: 0.0152 | Grad Norm: 0.00881243\n",
      "Epoch 2 | Step 956800 | Avg Loss: 0.0154 | Grad Norm: 0.00880345\n",
      "Epoch 2 | Step 956900 | Avg Loss: 0.0156 | Grad Norm: 0.00786276\n",
      "Epoch 2 | Step 957000 | Avg Loss: 0.0160 | Grad Norm: 0.01085987\n",
      "Epoch 2 | Step 957100 | Avg Loss: 0.0160 | Grad Norm: 0.00862233\n",
      "Epoch 2 | Step 957200 | Avg Loss: 0.0156 | Grad Norm: 0.00849721\n",
      "Epoch 2 | Step 957300 | Avg Loss: 0.0157 | Grad Norm: 0.00845183\n",
      "Epoch 2 | Step 957400 | Avg Loss: 0.0155 | Grad Norm: 0.00884752\n",
      "Epoch 2 | Step 957500 | Avg Loss: 0.0155 | Grad Norm: 0.00900949\n",
      "Epoch 2 | Step 957600 | Avg Loss: 0.0154 | Grad Norm: 0.00864576\n",
      "Epoch 2 | Step 957700 | Avg Loss: 0.0155 | Grad Norm: 0.00995075\n",
      "Epoch 2 | Step 957800 | Avg Loss: 0.0156 | Grad Norm: 0.00877137\n",
      "Epoch 2 | Step 957900 | Avg Loss: 0.0157 | Grad Norm: 0.00914216\n",
      "Epoch 2 | Step 958000 | Avg Loss: 0.0161 | Grad Norm: 0.00864725\n",
      "Epoch 2 | Step 958100 | Avg Loss: 0.0161 | Grad Norm: 0.00844615\n",
      "Epoch 2 | Step 958200 | Avg Loss: 0.0161 | Grad Norm: 0.01078617\n",
      "Epoch 2 | Step 958300 | Avg Loss: 0.0157 | Grad Norm: 0.00898558\n",
      "Epoch 2 | Step 958400 | Avg Loss: 0.0157 | Grad Norm: 0.00869762\n",
      "Epoch 2 | Step 958500 | Avg Loss: 0.0160 | Grad Norm: 0.01272482\n",
      "Epoch 2 | Step 958600 | Avg Loss: 0.0161 | Grad Norm: 0.00902106\n",
      "Epoch 2 | Step 958700 | Avg Loss: 0.0159 | Grad Norm: 0.00836132\n",
      "Epoch 2 | Step 958800 | Avg Loss: 0.0157 | Grad Norm: 0.00905469\n",
      "Epoch 2 | Step 958900 | Avg Loss: 0.0156 | Grad Norm: 0.01003824\n",
      "Epoch 2 | Step 959000 | Avg Loss: 0.0153 | Grad Norm: 0.00817395\n",
      "Epoch 2 | Step 959100 | Avg Loss: 0.0153 | Grad Norm: 0.00786095\n",
      "Epoch 2 | Step 959200 | Avg Loss: 0.0158 | Grad Norm: 0.00839270\n",
      "Epoch 2 | Step 959300 | Avg Loss: 0.0159 | Grad Norm: 0.00948541\n",
      "Epoch 2 | Step 959400 | Avg Loss: 0.0160 | Grad Norm: 0.00954336\n",
      "Epoch 2 | Step 959500 | Avg Loss: 0.0158 | Grad Norm: 0.01008128\n",
      "Epoch 2 | Step 959600 | Avg Loss: 0.0153 | Grad Norm: 0.00867265\n",
      "Epoch 2 | Step 959700 | Avg Loss: 0.0154 | Grad Norm: 0.00742499\n",
      "Epoch 2 | Step 959800 | Avg Loss: 0.0153 | Grad Norm: 0.00807016\n",
      "Epoch 2 | Step 959900 | Avg Loss: 0.0153 | Grad Norm: 0.01027661\n",
      "Epoch 2 | Step 960000 | Avg Loss: 0.0156 | Grad Norm: 0.00824420\n",
      "Epoch 2 | Step 960100 | Avg Loss: 0.0158 | Grad Norm: 0.00819203\n",
      "Epoch 2 | Step 960200 | Avg Loss: 0.0160 | Grad Norm: 0.00993427\n",
      "Epoch 2 | Step 960300 | Avg Loss: 0.0160 | Grad Norm: 0.00865220\n",
      "Epoch 2 | Step 960400 | Avg Loss: 0.0161 | Grad Norm: 0.00817174\n",
      "Epoch 2 | Step 960500 | Avg Loss: 0.0162 | Grad Norm: 0.00970862\n",
      "Epoch 2 | Step 960600 | Avg Loss: 0.0158 | Grad Norm: 0.00914829\n",
      "Epoch 2 | Step 960700 | Avg Loss: 0.0160 | Grad Norm: 0.00963679\n",
      "Epoch 2 | Step 960800 | Avg Loss: 0.0162 | Grad Norm: 0.00887803\n",
      "Epoch 2 | Step 960900 | Avg Loss: 0.0161 | Grad Norm: 0.00999958\n",
      "Epoch 2 | Step 961000 | Avg Loss: 0.0161 | Grad Norm: 0.00848358\n",
      "Epoch 2 | Step 961100 | Avg Loss: 0.0162 | Grad Norm: 0.00924009\n",
      "Epoch 2 | Step 961200 | Avg Loss: 0.0160 | Grad Norm: 0.01010010\n",
      "Epoch 2 | Step 961300 | Avg Loss: 0.0162 | Grad Norm: 0.00940259\n",
      "Epoch 2 | Step 961400 | Avg Loss: 0.0159 | Grad Norm: 0.00845315\n",
      "Epoch 2 | Step 961500 | Avg Loss: 0.0163 | Grad Norm: 0.01243234\n",
      "Epoch 2 | Step 961600 | Avg Loss: 0.0164 | Grad Norm: 0.00973629\n",
      "Epoch 2 | Step 961700 | Avg Loss: 0.0163 | Grad Norm: 0.00871276\n",
      "Epoch 2 | Step 961800 | Avg Loss: 0.0161 | Grad Norm: 0.00943235\n",
      "Epoch 2 | Step 961900 | Avg Loss: 0.0158 | Grad Norm: 0.00893385\n",
      "Epoch 2 | Step 962000 | Avg Loss: 0.0159 | Grad Norm: 0.00971573\n",
      "Epoch 2 | Step 962100 | Avg Loss: 0.0160 | Grad Norm: 0.00805437\n",
      "Epoch 2 | Step 962200 | Avg Loss: 0.0159 | Grad Norm: 0.00811486\n",
      "Epoch 2 | Step 962300 | Avg Loss: 0.0156 | Grad Norm: 0.00932069\n",
      "Epoch 2 | Step 962400 | Avg Loss: 0.0160 | Grad Norm: 0.00848636\n",
      "Epoch 2 | Step 962500 | Avg Loss: 0.0163 | Grad Norm: 0.00953928\n",
      "Epoch 2 | Step 962600 | Avg Loss: 0.0163 | Grad Norm: 0.00904085\n",
      "Epoch 2 | Step 962700 | Avg Loss: 0.0160 | Grad Norm: 0.00976194\n",
      "Epoch 2 | Step 962800 | Avg Loss: 0.0159 | Grad Norm: 0.00964283\n",
      "Epoch 2 | Step 962900 | Avg Loss: 0.0160 | Grad Norm: 0.01182980\n",
      "Epoch 2 | Step 963000 | Avg Loss: 0.0156 | Grad Norm: 0.00913342\n",
      "Epoch 2 | Step 963100 | Avg Loss: 0.0154 | Grad Norm: 0.00980233\n",
      "Epoch 2 | Step 963200 | Avg Loss: 0.0155 | Grad Norm: 0.00842850\n",
      "Epoch 2 | Step 963300 | Avg Loss: 0.0157 | Grad Norm: 0.00898268\n",
      "Epoch 2 | Step 963400 | Avg Loss: 0.0156 | Grad Norm: 0.00802708\n",
      "Epoch 2 | Step 963500 | Avg Loss: 0.0156 | Grad Norm: 0.00900551\n",
      "Epoch 2 | Step 963600 | Avg Loss: 0.0157 | Grad Norm: 0.01452278\n",
      "Epoch 2 | Step 963700 | Avg Loss: 0.0155 | Grad Norm: 0.00878796\n",
      "Epoch 2 | Step 963800 | Avg Loss: 0.0154 | Grad Norm: 0.00899249\n",
      "Epoch 2 | Step 963900 | Avg Loss: 0.0151 | Grad Norm: 0.00912420\n",
      "Epoch 2 | Step 964000 | Avg Loss: 0.0150 | Grad Norm: 0.01053129\n",
      "Epoch 2 | Step 964100 | Avg Loss: 0.0151 | Grad Norm: 0.00822535\n",
      "Epoch 2 | Step 964200 | Avg Loss: 0.0152 | Grad Norm: 0.00875332\n",
      "Epoch 2 | Step 964300 | Avg Loss: 0.0155 | Grad Norm: 0.00842225\n",
      "Epoch 2 | Step 964400 | Avg Loss: 0.0153 | Grad Norm: 0.00762066\n",
      "Epoch 2 | Step 964500 | Avg Loss: 0.0156 | Grad Norm: 0.00817456\n",
      "Epoch 2 | Step 964600 | Avg Loss: 0.0153 | Grad Norm: 0.00867271\n",
      "Epoch 2 | Step 964700 | Avg Loss: 0.0155 | Grad Norm: 0.00821536\n",
      "Epoch 2 | Step 964800 | Avg Loss: 0.0154 | Grad Norm: 0.00840196\n",
      "Epoch 2 | Step 964900 | Avg Loss: 0.0155 | Grad Norm: 0.00819188\n",
      "Epoch 2 | Step 965000 | Avg Loss: 0.0157 | Grad Norm: 0.00941107\n",
      "Epoch 2 | Step 965100 | Avg Loss: 0.0156 | Grad Norm: 0.00931599\n",
      "Epoch 2 | Step 965200 | Avg Loss: 0.0154 | Grad Norm: 0.00994289\n",
      "Epoch 2 | Step 965300 | Avg Loss: 0.0154 | Grad Norm: 0.00891440\n",
      "Epoch 2 | Step 965400 | Avg Loss: 0.0157 | Grad Norm: 0.00766214\n",
      "Epoch 2 | Step 965500 | Avg Loss: 0.0157 | Grad Norm: 0.00862710\n",
      "Epoch 2 | Step 965600 | Avg Loss: 0.0158 | Grad Norm: 0.01123949\n",
      "Epoch 2 | Step 965700 | Avg Loss: 0.0156 | Grad Norm: 0.00949625\n",
      "Epoch 2 | Step 965800 | Avg Loss: 0.0156 | Grad Norm: 0.00861225\n",
      "Epoch 2 | Step 965900 | Avg Loss: 0.0155 | Grad Norm: 0.00863032\n",
      "Epoch 2 | Step 966000 | Avg Loss: 0.0153 | Grad Norm: 0.00865169\n",
      "Epoch 2 | Step 966100 | Avg Loss: 0.0155 | Grad Norm: 0.00981983\n",
      "Epoch 2 | Step 966200 | Avg Loss: 0.0154 | Grad Norm: 0.00836096\n",
      "Epoch 2 | Step 966300 | Avg Loss: 0.0154 | Grad Norm: 0.00908421\n",
      "Epoch 2 | Step 966400 | Avg Loss: 0.0154 | Grad Norm: 0.00806389\n",
      "Epoch 2 | Step 966500 | Avg Loss: 0.0151 | Grad Norm: 0.00905273\n",
      "Epoch 2 | Step 966600 | Avg Loss: 0.0151 | Grad Norm: 0.01181758\n",
      "Epoch 2 | Step 966700 | Avg Loss: 0.0153 | Grad Norm: 0.00990970\n",
      "Epoch 2 | Step 966800 | Avg Loss: 0.0153 | Grad Norm: 0.00845993\n",
      "Epoch 2 | Step 966900 | Avg Loss: 0.0153 | Grad Norm: 0.00985427\n",
      "Epoch 2 | Step 967000 | Avg Loss: 0.0153 | Grad Norm: 0.00909670\n",
      "Epoch 2 | Step 967100 | Avg Loss: 0.0151 | Grad Norm: 0.00873094\n",
      "Epoch 2 | Step 967200 | Avg Loss: 0.0153 | Grad Norm: 0.00945376\n",
      "Epoch 2 | Step 967300 | Avg Loss: 0.0151 | Grad Norm: 0.00884444\n",
      "Epoch 2 | Step 967400 | Avg Loss: 0.0153 | Grad Norm: 0.00896582\n",
      "Epoch 2 | Step 967500 | Avg Loss: 0.0153 | Grad Norm: 0.00887060\n",
      "Epoch 2 | Step 967600 | Avg Loss: 0.0152 | Grad Norm: 0.00804702\n",
      "Epoch 2 | Step 967700 | Avg Loss: 0.0147 | Grad Norm: 0.00864158\n",
      "Epoch 2 | Step 967800 | Avg Loss: 0.0152 | Grad Norm: 0.00784311\n",
      "Epoch 2 | Step 967900 | Avg Loss: 0.0155 | Grad Norm: 0.01173898\n",
      "Epoch 2 | Step 968000 | Avg Loss: 0.0156 | Grad Norm: 0.00940541\n",
      "Epoch 2 | Step 968100 | Avg Loss: 0.0155 | Grad Norm: 0.00795920\n",
      "Epoch 2 | Step 968200 | Avg Loss: 0.0157 | Grad Norm: 0.00923632\n",
      "Epoch 2 | Step 968300 | Avg Loss: 0.0156 | Grad Norm: 0.00842269\n",
      "Epoch 2 | Step 968400 | Avg Loss: 0.0155 | Grad Norm: 0.00983139\n",
      "Epoch 2 | Step 968500 | Avg Loss: 0.0160 | Grad Norm: 0.00761881\n",
      "Epoch 2 | Step 968600 | Avg Loss: 0.0164 | Grad Norm: 0.00937686\n",
      "Epoch 2 | Step 968700 | Avg Loss: 0.0161 | Grad Norm: 0.00920504\n",
      "Epoch 2 | Step 968800 | Avg Loss: 0.0160 | Grad Norm: 0.01109598\n",
      "Epoch 2 | Step 968900 | Avg Loss: 0.0161 | Grad Norm: 0.01073678\n",
      "Epoch 2 | Step 969000 | Avg Loss: 0.0159 | Grad Norm: 0.00980527\n",
      "Epoch 2 | Step 969100 | Avg Loss: 0.0162 | Grad Norm: 0.01018712\n",
      "Epoch 2 | Step 969200 | Avg Loss: 0.0166 | Grad Norm: 0.01094986\n",
      "Epoch 2 | Step 969300 | Avg Loss: 0.0162 | Grad Norm: 0.00791951\n",
      "Epoch 2 | Step 969400 | Avg Loss: 0.0161 | Grad Norm: 0.00800420\n",
      "Epoch 2 | Step 969500 | Avg Loss: 0.0161 | Grad Norm: 0.00852575\n",
      "Epoch 2 | Step 969600 | Avg Loss: 0.0160 | Grad Norm: 0.00967268\n",
      "Epoch 2 | Step 969700 | Avg Loss: 0.0159 | Grad Norm: 0.00851757\n",
      "Epoch 2 | Step 969800 | Avg Loss: 0.0161 | Grad Norm: 0.00834645\n",
      "Epoch 2 | Step 969900 | Avg Loss: 0.0160 | Grad Norm: 0.00755993\n",
      "Epoch 2 | Step 970000 | Avg Loss: 0.0157 | Grad Norm: 0.01003441\n",
      "Epoch 2 | Step 970100 | Avg Loss: 0.0157 | Grad Norm: 0.00972785\n",
      "Epoch 2 | Step 970200 | Avg Loss: 0.0157 | Grad Norm: 0.00950296\n",
      "Epoch 2 | Step 970300 | Avg Loss: 0.0158 | Grad Norm: 0.00941992\n",
      "Epoch 2 | Step 970400 | Avg Loss: 0.0162 | Grad Norm: 0.00952265\n",
      "Epoch 2 | Step 970500 | Avg Loss: 0.0160 | Grad Norm: 0.01054744\n",
      "Epoch 2 | Step 970600 | Avg Loss: 0.0158 | Grad Norm: 0.00870404\n",
      "Epoch 2 | Step 970700 | Avg Loss: 0.0158 | Grad Norm: 0.00912983\n",
      "Epoch 2 | Step 970800 | Avg Loss: 0.0154 | Grad Norm: 0.00872562\n",
      "Epoch 2 | Step 970900 | Avg Loss: 0.0154 | Grad Norm: 0.00877953\n",
      "Epoch 2 | Step 971000 | Avg Loss: 0.0154 | Grad Norm: 0.00758381\n",
      "Epoch 2 | Step 971100 | Avg Loss: 0.0154 | Grad Norm: 0.00801727\n",
      "Epoch 2 | Step 971200 | Avg Loss: 0.0153 | Grad Norm: 0.00901326\n",
      "Epoch 2 | Step 971300 | Avg Loss: 0.0154 | Grad Norm: 0.00799485\n",
      "Epoch 2 | Step 971400 | Avg Loss: 0.0152 | Grad Norm: 0.00995651\n",
      "Epoch 2 | Step 971500 | Avg Loss: 0.0156 | Grad Norm: 0.00886755\n",
      "Epoch 2 | Step 971600 | Avg Loss: 0.0160 | Grad Norm: 0.00951599\n",
      "Epoch 2 | Step 971700 | Avg Loss: 0.0162 | Grad Norm: 0.00814562\n",
      "Epoch 2 | Step 971800 | Avg Loss: 0.0158 | Grad Norm: 0.00823003\n",
      "Epoch 2 | Step 971900 | Avg Loss: 0.0157 | Grad Norm: 0.00833354\n",
      "Epoch 2 | Step 972000 | Avg Loss: 0.0156 | Grad Norm: 0.00935281\n",
      "Epoch 2 | Step 972100 | Avg Loss: 0.0154 | Grad Norm: 0.00831077\n",
      "Epoch 2 | Step 972200 | Avg Loss: 0.0154 | Grad Norm: 0.00782627\n",
      "Epoch 2 | Step 972300 | Avg Loss: 0.0153 | Grad Norm: 0.00915481\n",
      "Epoch 2 | Step 972400 | Avg Loss: 0.0156 | Grad Norm: 0.00842937\n",
      "Epoch 2 | Step 972500 | Avg Loss: 0.0154 | Grad Norm: 0.01021874\n",
      "Epoch 2 | Step 972600 | Avg Loss: 0.0152 | Grad Norm: 0.00987689\n",
      "Epoch 2 | Step 972700 | Avg Loss: 0.0154 | Grad Norm: 0.00834819\n",
      "Epoch 2 | Step 972800 | Avg Loss: 0.0157 | Grad Norm: 0.01133811\n",
      "Epoch 2 | Step 972900 | Avg Loss: 0.0155 | Grad Norm: 0.00850983\n",
      "Epoch 2 | Step 973000 | Avg Loss: 0.0156 | Grad Norm: 0.01176727\n",
      "Epoch 2 | Step 973100 | Avg Loss: 0.0155 | Grad Norm: 0.00924843\n",
      "Epoch 2 | Step 973200 | Avg Loss: 0.0157 | Grad Norm: 0.00885429\n",
      "Epoch 2 | Step 973300 | Avg Loss: 0.0157 | Grad Norm: 0.01098442\n",
      "Epoch 2 | Step 973400 | Avg Loss: 0.0157 | Grad Norm: 0.00860478\n",
      "Epoch 2 | Step 973500 | Avg Loss: 0.0156 | Grad Norm: 0.00976503\n",
      "Epoch 2 | Step 973600 | Avg Loss: 0.0153 | Grad Norm: 0.00880007\n",
      "Epoch 2 | Step 973700 | Avg Loss: 0.0159 | Grad Norm: 0.00830854\n",
      "Epoch 2 | Step 973800 | Avg Loss: 0.0161 | Grad Norm: 0.00894908\n",
      "Epoch 2 | Step 973900 | Avg Loss: 0.0157 | Grad Norm: 0.01179199\n",
      "Epoch 2 | Step 974000 | Avg Loss: 0.0155 | Grad Norm: 0.00904535\n",
      "Epoch 2 | Step 974100 | Avg Loss: 0.0156 | Grad Norm: 0.00946519\n",
      "Epoch 2 | Step 974200 | Avg Loss: 0.0160 | Grad Norm: 0.00905433\n",
      "Epoch 2 | Step 974300 | Avg Loss: 0.0161 | Grad Norm: 0.00919530\n",
      "Epoch 2 | Step 974400 | Avg Loss: 0.0157 | Grad Norm: 0.00926011\n",
      "Epoch 2 | Step 974500 | Avg Loss: 0.0162 | Grad Norm: 0.00895476\n",
      "Epoch 2 | Step 974600 | Avg Loss: 0.0161 | Grad Norm: 0.00856734\n",
      "Epoch 2 | Step 974700 | Avg Loss: 0.0158 | Grad Norm: 0.00840208\n",
      "Epoch 2 | Step 974800 | Avg Loss: 0.0160 | Grad Norm: 0.00893136\n",
      "Epoch 2 | Step 974900 | Avg Loss: 0.0161 | Grad Norm: 0.00864433\n",
      "Epoch 2 | Step 975000 | Avg Loss: 0.0154 | Grad Norm: 0.00847361\n",
      "Epoch 2 | Step 975100 | Avg Loss: 0.0158 | Grad Norm: 0.00879922\n",
      "Epoch 2 | Step 975200 | Avg Loss: 0.0159 | Grad Norm: 0.01041051\n",
      "Epoch 2 | Step 975300 | Avg Loss: 0.0162 | Grad Norm: 0.00886452\n",
      "Epoch 2 | Step 975400 | Avg Loss: 0.0159 | Grad Norm: 0.00979369\n",
      "Epoch 2 | Step 975500 | Avg Loss: 0.0157 | Grad Norm: 0.01011548\n",
      "Epoch 2 | Step 975600 | Avg Loss: 0.0157 | Grad Norm: 0.00921249\n",
      "Epoch 2 | Step 975700 | Avg Loss: 0.0153 | Grad Norm: 0.00781688\n",
      "Epoch 2 | Step 975800 | Avg Loss: 0.0154 | Grad Norm: 0.00833983\n",
      "Epoch 2 | Step 975900 | Avg Loss: 0.0156 | Grad Norm: 0.00993693\n",
      "Epoch 2 | Step 976000 | Avg Loss: 0.0156 | Grad Norm: 0.00868502\n",
      "Epoch 2 | Step 976100 | Avg Loss: 0.0154 | Grad Norm: 0.00929947\n",
      "Epoch 2 | Step 976200 | Avg Loss: 0.0156 | Grad Norm: 0.00865506\n",
      "Epoch 2 | Step 976300 | Avg Loss: 0.0155 | Grad Norm: 0.01063593\n",
      "Epoch 2 | Step 976400 | Avg Loss: 0.0162 | Grad Norm: 0.00829224\n",
      "Epoch 2 | Step 976500 | Avg Loss: 0.0158 | Grad Norm: 0.00790693\n",
      "Epoch 2 | Step 976600 | Avg Loss: 0.0160 | Grad Norm: 0.00878752\n",
      "Epoch 2 | Step 976700 | Avg Loss: 0.0161 | Grad Norm: 0.00898437\n",
      "Epoch 2 | Step 976800 | Avg Loss: 0.0162 | Grad Norm: 0.00865231\n",
      "Epoch 2 | Step 976900 | Avg Loss: 0.0158 | Grad Norm: 0.00826137\n",
      "Epoch 2 | Step 977000 | Avg Loss: 0.0158 | Grad Norm: 0.00940731\n",
      "Epoch 2 | Step 977100 | Avg Loss: 0.0158 | Grad Norm: 0.00826154\n",
      "Epoch 2 | Step 977200 | Avg Loss: 0.0159 | Grad Norm: 0.01032792\n",
      "Epoch 2 | Step 977300 | Avg Loss: 0.0157 | Grad Norm: 0.00931801\n",
      "Epoch 2 | Step 977400 | Avg Loss: 0.0159 | Grad Norm: 0.00932986\n",
      "Epoch 2 | Step 977500 | Avg Loss: 0.0157 | Grad Norm: 0.00787203\n",
      "Epoch 2 | Step 977600 | Avg Loss: 0.0155 | Grad Norm: 0.00979843\n",
      "Epoch 2 | Step 977700 | Avg Loss: 0.0154 | Grad Norm: 0.00832009\n",
      "Epoch 2 | Step 977800 | Avg Loss: 0.0155 | Grad Norm: 0.00809216\n",
      "Epoch 2 | Step 977900 | Avg Loss: 0.0154 | Grad Norm: 0.00808873\n",
      "Epoch 2 | Step 978000 | Avg Loss: 0.0154 | Grad Norm: 0.00844488\n",
      "Epoch 2 | Step 978100 | Avg Loss: 0.0156 | Grad Norm: 0.01115269\n",
      "Epoch 2 | Step 978200 | Avg Loss: 0.0157 | Grad Norm: 0.00932384\n",
      "Epoch 2 | Step 978300 | Avg Loss: 0.0156 | Grad Norm: 0.00950393\n",
      "Epoch 2 | Step 978400 | Avg Loss: 0.0159 | Grad Norm: 0.00778192\n",
      "Epoch 2 | Step 978500 | Avg Loss: 0.0157 | Grad Norm: 0.01295597\n",
      "Epoch 2 | Step 978600 | Avg Loss: 0.0153 | Grad Norm: 0.00879920\n",
      "Epoch 2 | Step 978700 | Avg Loss: 0.0152 | Grad Norm: 0.00844528\n",
      "Epoch 2 | Step 978800 | Avg Loss: 0.0153 | Grad Norm: 0.00841044\n",
      "Epoch 2 | Step 978900 | Avg Loss: 0.0156 | Grad Norm: 0.00886402\n",
      "Epoch 2 | Step 979000 | Avg Loss: 0.0156 | Grad Norm: 0.00901597\n",
      "Epoch 2 | Step 979100 | Avg Loss: 0.0162 | Grad Norm: 0.00878293\n",
      "Epoch 2 | Step 979200 | Avg Loss: 0.0162 | Grad Norm: 0.01101170\n",
      "Epoch 2 | Step 979300 | Avg Loss: 0.0162 | Grad Norm: 0.00902429\n",
      "Epoch 2 | Step 979400 | Avg Loss: 0.0157 | Grad Norm: 0.01071381\n",
      "Epoch 2 | Step 979500 | Avg Loss: 0.0152 | Grad Norm: 0.00862840\n",
      "Epoch 2 | Step 979600 | Avg Loss: 0.0156 | Grad Norm: 0.00915445\n",
      "Epoch 2 | Step 979700 | Avg Loss: 0.0156 | Grad Norm: 0.00971504\n",
      "Epoch 2 | Step 979800 | Avg Loss: 0.0157 | Grad Norm: 0.00938815\n",
      "Epoch 2 | Step 979900 | Avg Loss: 0.0156 | Grad Norm: 0.00831457\n",
      "Epoch 2 | Step 980000 | Avg Loss: 0.0155 | Grad Norm: 0.01136587\n",
      "Epoch 2 | Step 980100 | Avg Loss: 0.0156 | Grad Norm: 0.00894860\n",
      "Epoch 2 | Step 980200 | Avg Loss: 0.0159 | Grad Norm: 0.00894377\n",
      "Epoch 2 | Step 980300 | Avg Loss: 0.0156 | Grad Norm: 0.00886744\n",
      "Epoch 2 | Step 980400 | Avg Loss: 0.0156 | Grad Norm: 0.00906094\n",
      "Epoch 2 | Step 980500 | Avg Loss: 0.0159 | Grad Norm: 0.01060233\n",
      "Epoch 2 | Step 980600 | Avg Loss: 0.0160 | Grad Norm: 0.00926878\n",
      "Epoch 2 | Step 980700 | Avg Loss: 0.0162 | Grad Norm: 0.00881110\n",
      "Epoch 2 | Step 980800 | Avg Loss: 0.0162 | Grad Norm: 0.00924741\n",
      "Epoch 2 | Step 980900 | Avg Loss: 0.0164 | Grad Norm: 0.01045843\n",
      "Epoch 2 | Step 981000 | Avg Loss: 0.0160 | Grad Norm: 0.00866767\n",
      "Epoch 2 | Step 981100 | Avg Loss: 0.0155 | Grad Norm: 0.00976144\n",
      "Epoch 2 | Step 981200 | Avg Loss: 0.0155 | Grad Norm: 0.01039407\n",
      "Epoch 2 | Step 981300 | Avg Loss: 0.0155 | Grad Norm: 0.00994250\n",
      "Epoch 2 | Step 981400 | Avg Loss: 0.0154 | Grad Norm: 0.00931947\n",
      "Epoch 2 | Step 981500 | Avg Loss: 0.0154 | Grad Norm: 0.00841420\n",
      "Epoch 2 | Step 981600 | Avg Loss: 0.0155 | Grad Norm: 0.01004489\n",
      "Epoch 2 | Step 981700 | Avg Loss: 0.0155 | Grad Norm: 0.00856985\n",
      "Epoch 2 | Step 981800 | Avg Loss: 0.0158 | Grad Norm: 0.00773599\n",
      "Epoch 2 | Step 981900 | Avg Loss: 0.0155 | Grad Norm: 0.01062850\n",
      "Epoch 2 | Step 982000 | Avg Loss: 0.0158 | Grad Norm: 0.00868935\n",
      "Epoch 2 | Step 982100 | Avg Loss: 0.0157 | Grad Norm: 0.00946975\n",
      "Epoch 2 | Step 982200 | Avg Loss: 0.0156 | Grad Norm: 0.00853985\n",
      "Epoch 2 | Step 982300 | Avg Loss: 0.0159 | Grad Norm: 0.00919534\n",
      "Epoch 2 | Step 982400 | Avg Loss: 0.0160 | Grad Norm: 0.00800298\n",
      "Epoch 2 | Step 982500 | Avg Loss: 0.0158 | Grad Norm: 0.00861535\n",
      "Epoch 2 | Step 982600 | Avg Loss: 0.0157 | Grad Norm: 0.00846092\n",
      "Epoch 2 | Step 982700 | Avg Loss: 0.0154 | Grad Norm: 0.00947250\n",
      "Epoch 2 | Step 982800 | Avg Loss: 0.0157 | Grad Norm: 0.00809143\n",
      "Epoch 2 | Step 982900 | Avg Loss: 0.0159 | Grad Norm: 0.00941659\n",
      "Epoch 2 | Step 983000 | Avg Loss: 0.0156 | Grad Norm: 0.00744361\n",
      "Epoch 2 | Step 983100 | Avg Loss: 0.0154 | Grad Norm: 0.00926885\n",
      "Epoch 2 | Step 983200 | Avg Loss: 0.0153 | Grad Norm: 0.01028385\n",
      "Epoch 2 | Step 983300 | Avg Loss: 0.0154 | Grad Norm: 0.00819335\n",
      "Epoch 2 | Step 983400 | Avg Loss: 0.0155 | Grad Norm: 0.00943776\n",
      "Epoch 2 | Step 983500 | Avg Loss: 0.0156 | Grad Norm: 0.00975122\n",
      "Epoch 2 | Step 983600 | Avg Loss: 0.0155 | Grad Norm: 0.00896760\n",
      "Epoch 2 | Step 983700 | Avg Loss: 0.0156 | Grad Norm: 0.00764461\n",
      "Epoch 2 | Step 983800 | Avg Loss: 0.0157 | Grad Norm: 0.00940446\n",
      "Epoch 2 | Step 983900 | Avg Loss: 0.0156 | Grad Norm: 0.01013589\n",
      "Epoch 2 | Step 984000 | Avg Loss: 0.0163 | Grad Norm: 0.00918894\n",
      "Epoch 2 | Step 984100 | Avg Loss: 0.0166 | Grad Norm: 0.00753535\n",
      "Epoch 2 | Step 984200 | Avg Loss: 0.0162 | Grad Norm: 0.00946548\n",
      "Epoch 2 | Step 984300 | Avg Loss: 0.0162 | Grad Norm: 0.00917860\n",
      "Epoch 2 | Step 984400 | Avg Loss: 0.0163 | Grad Norm: 0.00835503\n",
      "Epoch 2 | Step 984500 | Avg Loss: 0.0159 | Grad Norm: 0.00957219\n",
      "Epoch 2 | Step 984600 | Avg Loss: 0.0160 | Grad Norm: 0.00919452\n",
      "Epoch 2 | Step 984700 | Avg Loss: 0.0159 | Grad Norm: 0.00922727\n",
      "Epoch 2 | Step 984800 | Avg Loss: 0.0158 | Grad Norm: 0.00981626\n",
      "Epoch 2 | Step 984900 | Avg Loss: 0.0160 | Grad Norm: 0.00962183\n",
      "Epoch 2 | Step 985000 | Avg Loss: 0.0164 | Grad Norm: 0.00823821\n",
      "Epoch 2 | Step 985100 | Avg Loss: 0.0162 | Grad Norm: 0.00812849\n",
      "Epoch 2 | Step 985200 | Avg Loss: 0.0157 | Grad Norm: 0.01042606\n",
      "Epoch 2 | Step 985300 | Avg Loss: 0.0157 | Grad Norm: 0.00863465\n",
      "Epoch 2 | Step 985400 | Avg Loss: 0.0162 | Grad Norm: 0.00952835\n",
      "Epoch 2 | Step 985500 | Avg Loss: 0.0158 | Grad Norm: 0.00930621\n",
      "Epoch 2 | Step 985600 | Avg Loss: 0.0157 | Grad Norm: 0.00901999\n",
      "Epoch 2 | Step 985700 | Avg Loss: 0.0160 | Grad Norm: 0.00824398\n",
      "Epoch 2 | Step 985800 | Avg Loss: 0.0157 | Grad Norm: 0.00863438\n",
      "Epoch 2 | Step 985900 | Avg Loss: 0.0158 | Grad Norm: 0.00854480\n",
      "Epoch 2 | Step 986000 | Avg Loss: 0.0157 | Grad Norm: 0.01160162\n",
      "Epoch 2 | Step 986100 | Avg Loss: 0.0156 | Grad Norm: 0.01035618\n",
      "Epoch 2 | Step 986200 | Avg Loss: 0.0153 | Grad Norm: 0.01074192\n",
      "Epoch 2 | Step 986300 | Avg Loss: 0.0154 | Grad Norm: 0.00823276\n",
      "Epoch 2 | Step 986400 | Avg Loss: 0.0153 | Grad Norm: 0.00902453\n",
      "Epoch 2 | Step 986500 | Avg Loss: 0.0153 | Grad Norm: 0.00847998\n",
      "Epoch 2 | Step 986600 | Avg Loss: 0.0154 | Grad Norm: 0.00899966\n",
      "Epoch 2 | Step 986700 | Avg Loss: 0.0158 | Grad Norm: 0.01244532\n",
      "Epoch 2 | Step 986800 | Avg Loss: 0.0156 | Grad Norm: 0.00866114\n",
      "Epoch 2 | Step 986900 | Avg Loss: 0.0154 | Grad Norm: 0.00873660\n",
      "Epoch 2 | Step 987000 | Avg Loss: 0.0157 | Grad Norm: 0.01037768\n",
      "Epoch 2 | Step 987100 | Avg Loss: 0.0161 | Grad Norm: 0.00803561\n",
      "Epoch 2 | Step 987200 | Avg Loss: 0.0160 | Grad Norm: 0.00941567\n",
      "Epoch 2 | Step 987300 | Avg Loss: 0.0159 | Grad Norm: 0.00799499\n",
      "Epoch 2 | Step 987400 | Avg Loss: 0.0161 | Grad Norm: 0.00809125\n",
      "Epoch 2 | Step 987500 | Avg Loss: 0.0161 | Grad Norm: 0.00869862\n",
      "Epoch 2 | Step 987600 | Avg Loss: 0.0156 | Grad Norm: 0.00839273\n",
      "Epoch 2 | Step 987700 | Avg Loss: 0.0157 | Grad Norm: 0.00905489\n",
      "Epoch 2 | Step 987800 | Avg Loss: 0.0156 | Grad Norm: 0.00868301\n",
      "Epoch 2 | Step 987900 | Avg Loss: 0.0156 | Grad Norm: 0.00870097\n",
      "Epoch 2 | Step 988000 | Avg Loss: 0.0162 | Grad Norm: 0.01068702\n",
      "Epoch 2 | Step 988100 | Avg Loss: 0.0161 | Grad Norm: 0.01017155\n",
      "Epoch 2 | Step 988200 | Avg Loss: 0.0157 | Grad Norm: 0.00993141\n",
      "Epoch 2 | Step 988300 | Avg Loss: 0.0159 | Grad Norm: 0.00968312\n",
      "Epoch 2 | Step 988400 | Avg Loss: 0.0159 | Grad Norm: 0.00869438\n",
      "Epoch 2 | Step 988500 | Avg Loss: 0.0158 | Grad Norm: 0.00884371\n",
      "Epoch 2 | Step 988600 | Avg Loss: 0.0158 | Grad Norm: 0.00893262\n",
      "Epoch 2 | Step 988700 | Avg Loss: 0.0160 | Grad Norm: 0.00973045\n",
      "Epoch 2 | Step 988800 | Avg Loss: 0.0156 | Grad Norm: 0.00845638\n",
      "Epoch 2 | Step 988900 | Avg Loss: 0.0152 | Grad Norm: 0.00857548\n",
      "Epoch 2 | Step 989000 | Avg Loss: 0.0155 | Grad Norm: 0.00794774\n",
      "Epoch 2 | Step 989100 | Avg Loss: 0.0157 | Grad Norm: 0.00873559\n",
      "Epoch 2 | Step 989200 | Avg Loss: 0.0160 | Grad Norm: 0.00989859\n",
      "Epoch 2 | Step 989300 | Avg Loss: 0.0160 | Grad Norm: 0.00865071\n",
      "Epoch 2 | Step 989400 | Avg Loss: 0.0160 | Grad Norm: 0.01038282\n",
      "Epoch 2 | Step 989500 | Avg Loss: 0.0162 | Grad Norm: 0.00984818\n",
      "Epoch 2 | Step 989600 | Avg Loss: 0.0160 | Grad Norm: 0.00832737\n",
      "Epoch 2 | Step 989700 | Avg Loss: 0.0161 | Grad Norm: 0.00965356\n",
      "Epoch 2 | Step 989800 | Avg Loss: 0.0165 | Grad Norm: 0.00951680\n",
      "Epoch 2 | Step 989900 | Avg Loss: 0.0163 | Grad Norm: 0.00897478\n",
      "Epoch 2 | Step 990000 | Avg Loss: 0.0156 | Grad Norm: 0.00979859\n",
      "Epoch 2 | Step 990100 | Avg Loss: 0.0160 | Grad Norm: 0.00873939\n",
      "Epoch 2 | Step 990200 | Avg Loss: 0.0162 | Grad Norm: 0.00843662\n",
      "Epoch 2 | Step 990300 | Avg Loss: 0.0159 | Grad Norm: 0.00931488\n",
      "Epoch 2 | Step 990400 | Avg Loss: 0.0161 | Grad Norm: 0.00909637\n",
      "Epoch 2 | Step 990500 | Avg Loss: 0.0162 | Grad Norm: 0.00918672\n",
      "Epoch 2 | Step 990600 | Avg Loss: 0.0162 | Grad Norm: 0.00868096\n",
      "Epoch 2 | Step 990700 | Avg Loss: 0.0161 | Grad Norm: 0.01077577\n",
      "Epoch 2 | Step 990800 | Avg Loss: 0.0157 | Grad Norm: 0.00947716\n",
      "Epoch 2 | Step 990900 | Avg Loss: 0.0156 | Grad Norm: 0.00888540\n",
      "Epoch 2 | Step 991000 | Avg Loss: 0.0155 | Grad Norm: 0.00879224\n",
      "Epoch 2 | Step 991100 | Avg Loss: 0.0156 | Grad Norm: 0.00956457\n",
      "Epoch 2 | Step 991200 | Avg Loss: 0.0159 | Grad Norm: 0.00831073\n",
      "Epoch 2 | Step 991300 | Avg Loss: 0.0159 | Grad Norm: 0.00816689\n",
      "Epoch 2 | Step 991400 | Avg Loss: 0.0160 | Grad Norm: 0.00816109\n",
      "Epoch 2 | Step 991500 | Avg Loss: 0.0158 | Grad Norm: 0.00975377\n",
      "Epoch 2 | Step 991600 | Avg Loss: 0.0159 | Grad Norm: 0.00968282\n",
      "Epoch 2 | Step 991700 | Avg Loss: 0.0159 | Grad Norm: 0.00826297\n",
      "Epoch 2 | Step 991800 | Avg Loss: 0.0157 | Grad Norm: 0.00854505\n",
      "Epoch 2 | Step 991900 | Avg Loss: 0.0158 | Grad Norm: 0.00737168\n",
      "Epoch 2 | Step 992000 | Avg Loss: 0.0158 | Grad Norm: 0.00856149\n",
      "Epoch 2 | Step 992100 | Avg Loss: 0.0158 | Grad Norm: 0.01180424\n",
      "Epoch 2 | Step 992200 | Avg Loss: 0.0154 | Grad Norm: 0.00844001\n",
      "Epoch 2 | Step 992300 | Avg Loss: 0.0150 | Grad Norm: 0.00921037\n",
      "Epoch 2 | Step 992400 | Avg Loss: 0.0151 | Grad Norm: 0.00891494\n",
      "Epoch 2 | Step 992500 | Avg Loss: 0.0152 | Grad Norm: 0.00857187\n",
      "Epoch 2 | Step 992600 | Avg Loss: 0.0152 | Grad Norm: 0.00889048\n",
      "Epoch 2 | Step 992700 | Avg Loss: 0.0154 | Grad Norm: 0.00796644\n",
      "Epoch 2 | Step 992800 | Avg Loss: 0.0152 | Grad Norm: 0.01023556\n",
      "Epoch 2 | Step 992900 | Avg Loss: 0.0152 | Grad Norm: 0.00995442\n",
      "Epoch 2 | Step 993000 | Avg Loss: 0.0156 | Grad Norm: 0.01262861\n",
      "Epoch 2 | Step 993100 | Avg Loss: 0.0152 | Grad Norm: 0.00934462\n",
      "Epoch 2 | Step 993200 | Avg Loss: 0.0160 | Grad Norm: 0.00829259\n",
      "Epoch 2 | Step 993300 | Avg Loss: 0.0163 | Grad Norm: 0.00944472\n",
      "Epoch 2 | Step 993400 | Avg Loss: 0.0162 | Grad Norm: 0.00973443\n",
      "Epoch 2 | Step 993500 | Avg Loss: 0.0159 | Grad Norm: 0.00850641\n",
      "Epoch 2 | Step 993600 | Avg Loss: 0.0162 | Grad Norm: 0.00864354\n",
      "Epoch 2 | Step 993700 | Avg Loss: 0.0160 | Grad Norm: 0.00843933\n",
      "Epoch 2 | Step 993800 | Avg Loss: 0.0161 | Grad Norm: 0.00890425\n",
      "Epoch 2 | Step 993900 | Avg Loss: 0.0158 | Grad Norm: 0.00944630\n",
      "Epoch 2 | Step 994000 | Avg Loss: 0.0157 | Grad Norm: 0.01022795\n",
      "Epoch 2 | Step 994100 | Avg Loss: 0.0155 | Grad Norm: 0.01003897\n",
      "Epoch 2 | Step 994200 | Avg Loss: 0.0156 | Grad Norm: 0.00899377\n",
      "Epoch 2 | Step 994300 | Avg Loss: 0.0155 | Grad Norm: 0.00935607\n",
      "Epoch 2 | Step 994400 | Avg Loss: 0.0153 | Grad Norm: 0.00957974\n",
      "Epoch 2 | Step 994500 | Avg Loss: 0.0153 | Grad Norm: 0.00863442\n",
      "Epoch 2 | Step 994600 | Avg Loss: 0.0150 | Grad Norm: 0.01012075\n",
      "Epoch 2 | Step 994700 | Avg Loss: 0.0152 | Grad Norm: 0.01013586\n",
      "Epoch 2 | Step 994800 | Avg Loss: 0.0155 | Grad Norm: 0.01060358\n",
      "Epoch 2 | Step 994900 | Avg Loss: 0.0156 | Grad Norm: 0.00854736\n",
      "Epoch 2 | Step 995000 | Avg Loss: 0.0158 | Grad Norm: 0.00805966\n",
      "Epoch 2 | Step 995100 | Avg Loss: 0.0160 | Grad Norm: 0.00949783\n",
      "Epoch 2 | Step 995200 | Avg Loss: 0.0154 | Grad Norm: 0.00845249\n",
      "Epoch 2 | Step 995300 | Avg Loss: 0.0158 | Grad Norm: 0.00804023\n",
      "Epoch 2 | Step 995400 | Avg Loss: 0.0157 | Grad Norm: 0.00768128\n",
      "Epoch 2 | Step 995500 | Avg Loss: 0.0160 | Grad Norm: 0.00986990\n",
      "Epoch 2 | Step 995600 | Avg Loss: 0.0158 | Grad Norm: 0.01002785\n",
      "Epoch 2 | Step 995700 | Avg Loss: 0.0158 | Grad Norm: 0.01008700\n",
      "Epoch 2 | Step 995800 | Avg Loss: 0.0154 | Grad Norm: 0.01012455\n",
      "Epoch 2 | Step 995900 | Avg Loss: 0.0151 | Grad Norm: 0.00844988\n",
      "Epoch 2 | Step 996000 | Avg Loss: 0.0151 | Grad Norm: 0.00963922\n",
      "Epoch 2 | Step 996100 | Avg Loss: 0.0152 | Grad Norm: 0.00766820\n",
      "Epoch 2 | Step 996200 | Avg Loss: 0.0153 | Grad Norm: 0.00895845\n",
      "Epoch 2 | Step 996300 | Avg Loss: 0.0157 | Grad Norm: 0.00938571\n",
      "Epoch 2 | Step 996400 | Avg Loss: 0.0157 | Grad Norm: 0.00870989\n",
      "Epoch 2 | Step 996500 | Avg Loss: 0.0155 | Grad Norm: 0.00855031\n",
      "Epoch 2 | Step 996600 | Avg Loss: 0.0154 | Grad Norm: 0.00906781\n",
      "Epoch 2 | Step 996700 | Avg Loss: 0.0154 | Grad Norm: 0.00829816\n",
      "Epoch 2 | Step 996800 | Avg Loss: 0.0156 | Grad Norm: 0.00790505\n",
      "Epoch 2 | Step 996900 | Avg Loss: 0.0156 | Grad Norm: 0.00874578\n",
      "Epoch 2 | Step 997000 | Avg Loss: 0.0155 | Grad Norm: 0.00869541\n",
      "Epoch 2 | Step 997100 | Avg Loss: 0.0154 | Grad Norm: 0.00746195\n",
      "Epoch 2 | Step 997200 | Avg Loss: 0.0150 | Grad Norm: 0.00805668\n",
      "Epoch 2 | Step 997300 | Avg Loss: 0.0150 | Grad Norm: 0.00894576\n",
      "Epoch 2 | Step 997400 | Avg Loss: 0.0155 | Grad Norm: 0.00841313\n",
      "Epoch 2 | Step 997500 | Avg Loss: 0.0155 | Grad Norm: 0.01037171\n",
      "Epoch 2 | Step 997600 | Avg Loss: 0.0155 | Grad Norm: 0.00990384\n",
      "Epoch 2 | Step 997700 | Avg Loss: 0.0155 | Grad Norm: 0.00848455\n",
      "Epoch 2 | Step 997800 | Avg Loss: 0.0153 | Grad Norm: 0.00851275\n",
      "Epoch 2 | Step 997900 | Avg Loss: 0.0154 | Grad Norm: 0.00810861\n",
      "Epoch 2 | Step 998000 | Avg Loss: 0.0154 | Grad Norm: 0.00943339\n",
      "Epoch 2 | Step 998100 | Avg Loss: 0.0154 | Grad Norm: 0.00843298\n",
      "Epoch 2 | Step 998200 | Avg Loss: 0.0156 | Grad Norm: 0.00918920\n",
      "Epoch 2 | Step 998300 | Avg Loss: 0.0157 | Grad Norm: 0.00865380\n",
      "Epoch 2 | Step 998400 | Avg Loss: 0.0159 | Grad Norm: 0.00962699\n",
      "Epoch 2 | Step 998500 | Avg Loss: 0.0161 | Grad Norm: 0.01118999\n",
      "Epoch 2 | Step 998600 | Avg Loss: 0.0163 | Grad Norm: 0.00812482\n",
      "Epoch 2 | Step 998700 | Avg Loss: 0.0157 | Grad Norm: 0.00823519\n",
      "Epoch 2 | Step 998800 | Avg Loss: 0.0159 | Grad Norm: 0.01053250\n",
      "Epoch 2 | Step 998900 | Avg Loss: 0.0161 | Grad Norm: 0.00878051\n",
      "Epoch 2 | Step 999000 | Avg Loss: 0.0161 | Grad Norm: 0.00923456\n",
      "Epoch 2 | Step 999100 | Avg Loss: 0.0166 | Grad Norm: 0.00964490\n",
      "Epoch 2 | Step 999200 | Avg Loss: 0.0161 | Grad Norm: 0.00849104\n",
      "Epoch 2 | Step 999300 | Avg Loss: 0.0160 | Grad Norm: 0.00828953\n",
      "Epoch 2 | Step 999400 | Avg Loss: 0.0158 | Grad Norm: 0.00848766\n",
      "Epoch 2 | Step 999500 | Avg Loss: 0.0156 | Grad Norm: 0.00959888\n",
      "Epoch 2 | Step 999600 | Avg Loss: 0.0160 | Grad Norm: 0.01068945\n",
      "Epoch 2 | Step 999700 | Avg Loss: 0.0159 | Grad Norm: 0.00890064\n",
      "Epoch 2 | Step 999800 | Avg Loss: 0.0156 | Grad Norm: 0.00977139\n",
      "Epoch 2 | Step 999900 | Avg Loss: 0.0153 | Grad Norm: 0.00772640\n",
      "Epoch 2 | Step 1000000 | Avg Loss: 0.0152 | Grad Norm: 0.00924646\n",
      "Saving model at step1000000\n",
      "Epoch 2 | Step 1000100 | Avg Loss: 0.0155 | Grad Norm: 0.00967949\n",
      "Epoch 2 | Step 1000200 | Avg Loss: 0.0156 | Grad Norm: 0.00800572\n",
      "Epoch 2 | Step 1000300 | Avg Loss: 0.0159 | Grad Norm: 0.01019144\n",
      "Epoch 2 | Step 1000400 | Avg Loss: 0.0160 | Grad Norm: 0.01034862\n",
      "Epoch 2 | Step 1000500 | Avg Loss: 0.0161 | Grad Norm: 0.01055834\n",
      "Epoch 2 | Step 1000600 | Avg Loss: 0.0157 | Grad Norm: 0.01080310\n",
      "Epoch 2 | Step 1000700 | Avg Loss: 0.0158 | Grad Norm: 0.01028283\n",
      "Epoch 2 | Step 1000800 | Avg Loss: 0.0157 | Grad Norm: 0.00895858\n",
      "Epoch 2 | Step 1000900 | Avg Loss: 0.0157 | Grad Norm: 0.01048946\n",
      "Epoch 2 | Step 1001000 | Avg Loss: 0.0157 | Grad Norm: 0.00828422\n",
      "Epoch 2 | Step 1001100 | Avg Loss: 0.0157 | Grad Norm: 0.00853741\n",
      "Epoch 2 | Step 1001200 | Avg Loss: 0.0157 | Grad Norm: 0.00875363\n",
      "Epoch 2 | Step 1001300 | Avg Loss: 0.0156 | Grad Norm: 0.00986436\n",
      "Epoch 2 | Step 1001400 | Avg Loss: 0.0154 | Grad Norm: 0.00858163\n",
      "Epoch 2 | Step 1001500 | Avg Loss: 0.0153 | Grad Norm: 0.00809889\n",
      "Epoch 2 | Step 1001600 | Avg Loss: 0.0156 | Grad Norm: 0.00809045\n",
      "Epoch 2 | Step 1001700 | Avg Loss: 0.0154 | Grad Norm: 0.00918364\n",
      "Epoch 2 | Step 1001800 | Avg Loss: 0.0153 | Grad Norm: 0.01092130\n",
      "Epoch 2 | Step 1001900 | Avg Loss: 0.0154 | Grad Norm: 0.00771029\n",
      "Epoch 2 | Step 1002000 | Avg Loss: 0.0152 | Grad Norm: 0.00900957\n",
      "Epoch 2 | Step 1002100 | Avg Loss: 0.0152 | Grad Norm: 0.00828407\n",
      "Epoch 2 | Step 1002200 | Avg Loss: 0.0154 | Grad Norm: 0.00893029\n",
      "Epoch 2 | Step 1002300 | Avg Loss: 0.0154 | Grad Norm: 0.00773741\n",
      "Epoch 2 | Step 1002400 | Avg Loss: 0.0154 | Grad Norm: 0.00863295\n",
      "Epoch 2 | Step 1002500 | Avg Loss: 0.0154 | Grad Norm: 0.00808021\n",
      "Epoch 2 | Step 1002600 | Avg Loss: 0.0157 | Grad Norm: 0.01005133\n",
      "Epoch 2 | Step 1002700 | Avg Loss: 0.0158 | Grad Norm: 0.01073392\n",
      "Epoch 2 | Step 1002800 | Avg Loss: 0.0157 | Grad Norm: 0.00898595\n",
      "Epoch 2 | Step 1002900 | Avg Loss: 0.0159 | Grad Norm: 0.00956220\n",
      "Epoch 2 | Step 1003000 | Avg Loss: 0.0164 | Grad Norm: 0.00873236\n",
      "Epoch 2 | Step 1003100 | Avg Loss: 0.0160 | Grad Norm: 0.00948754\n",
      "Epoch 2 | Step 1003200 | Avg Loss: 0.0159 | Grad Norm: 0.00863370\n",
      "Epoch 2 | Step 1003300 | Avg Loss: 0.0160 | Grad Norm: 0.00891681\n",
      "Epoch 2 | Step 1003400 | Avg Loss: 0.0157 | Grad Norm: 0.01117797\n",
      "Epoch 2 | Step 1003500 | Avg Loss: 0.0155 | Grad Norm: 0.00988944\n",
      "Epoch 2 | Step 1003600 | Avg Loss: 0.0157 | Grad Norm: 0.00793266\n",
      "Epoch 2 | Step 1003700 | Avg Loss: 0.0159 | Grad Norm: 0.00955716\n",
      "Epoch 2 | Step 1003800 | Avg Loss: 0.0154 | Grad Norm: 0.01019518\n",
      "Epoch 2 | Step 1003900 | Avg Loss: 0.0155 | Grad Norm: 0.00885304\n",
      "Epoch 2 | Step 1004000 | Avg Loss: 0.0154 | Grad Norm: 0.00783468\n",
      "Epoch 2 | Step 1004100 | Avg Loss: 0.0158 | Grad Norm: 0.00999751\n",
      "Epoch 2 | Step 1004200 | Avg Loss: 0.0158 | Grad Norm: 0.00855257\n",
      "Epoch 2 | Step 1004300 | Avg Loss: 0.0158 | Grad Norm: 0.00853306\n",
      "Epoch 2 | Step 1004400 | Avg Loss: 0.0158 | Grad Norm: 0.00878631\n",
      "Epoch 2 | Step 1004500 | Avg Loss: 0.0156 | Grad Norm: 0.00903474\n",
      "Epoch 2 | Step 1004600 | Avg Loss: 0.0156 | Grad Norm: 0.00863962\n",
      "Epoch 2 | Step 1004700 | Avg Loss: 0.0157 | Grad Norm: 0.00750450\n",
      "Epoch 2 | Step 1004800 | Avg Loss: 0.0153 | Grad Norm: 0.00826620\n",
      "Epoch 2 | Step 1004900 | Avg Loss: 0.0155 | Grad Norm: 0.00915925\n",
      "Epoch 2 | Step 1005000 | Avg Loss: 0.0152 | Grad Norm: 0.00934506\n",
      "Epoch 2 | Step 1005100 | Avg Loss: 0.0152 | Grad Norm: 0.00867555\n",
      "Epoch 2 | Step 1005200 | Avg Loss: 0.0151 | Grad Norm: 0.00759954\n",
      "Epoch 2 | Step 1005300 | Avg Loss: 0.0154 | Grad Norm: 0.00774290\n",
      "Epoch 2 | Step 1005400 | Avg Loss: 0.0155 | Grad Norm: 0.00852241\n",
      "Epoch 2 | Step 1005500 | Avg Loss: 0.0156 | Grad Norm: 0.01024471\n",
      "Epoch 2 | Step 1005600 | Avg Loss: 0.0156 | Grad Norm: 0.00827884\n",
      "Epoch 2 | Step 1005700 | Avg Loss: 0.0155 | Grad Norm: 0.00914665\n",
      "Epoch 2 | Step 1005800 | Avg Loss: 0.0158 | Grad Norm: 0.00910672\n",
      "Epoch 2 | Step 1005900 | Avg Loss: 0.0161 | Grad Norm: 0.00829370\n",
      "Epoch 2 | Step 1006000 | Avg Loss: 0.0160 | Grad Norm: 0.00826284\n",
      "Epoch 2 | Step 1006100 | Avg Loss: 0.0161 | Grad Norm: 0.00970963\n",
      "Epoch 2 | Step 1006200 | Avg Loss: 0.0163 | Grad Norm: 0.00986775\n",
      "Epoch 2 | Step 1006300 | Avg Loss: 0.0163 | Grad Norm: 0.01220102\n",
      "Epoch 2 | Step 1006400 | Avg Loss: 0.0157 | Grad Norm: 0.00961592\n",
      "Epoch 2 | Step 1006500 | Avg Loss: 0.0155 | Grad Norm: 0.00803171\n",
      "Epoch 2 | Step 1006600 | Avg Loss: 0.0155 | Grad Norm: 0.00893364\n",
      "Epoch 2 | Step 1006700 | Avg Loss: 0.0157 | Grad Norm: 0.01087879\n",
      "Epoch 2 | Step 1006800 | Avg Loss: 0.0158 | Grad Norm: 0.00785288\n",
      "Epoch 2 | Step 1006900 | Avg Loss: 0.0161 | Grad Norm: 0.00860111\n",
      "Epoch 2 | Step 1007000 | Avg Loss: 0.0160 | Grad Norm: 0.00824049\n",
      "Epoch 2 | Step 1007100 | Avg Loss: 0.0160 | Grad Norm: 0.00982445\n",
      "Epoch 2 | Step 1007200 | Avg Loss: 0.0161 | Grad Norm: 0.00863169\n",
      "Epoch 2 | Step 1007300 | Avg Loss: 0.0158 | Grad Norm: 0.00822402\n",
      "Epoch 2 | Step 1007400 | Avg Loss: 0.0159 | Grad Norm: 0.00910469\n",
      "Epoch 2 | Step 1007500 | Avg Loss: 0.0155 | Grad Norm: 0.00769309\n",
      "Epoch 2 | Step 1007600 | Avg Loss: 0.0155 | Grad Norm: 0.00935718\n",
      "Epoch 2 | Step 1007700 | Avg Loss: 0.0157 | Grad Norm: 0.00914804\n",
      "Epoch 2 | Step 1007800 | Avg Loss: 0.0157 | Grad Norm: 0.00809263\n",
      "Epoch 2 | Step 1007900 | Avg Loss: 0.0156 | Grad Norm: 0.00858744\n",
      "Epoch 2 | Step 1008000 | Avg Loss: 0.0152 | Grad Norm: 0.00953527\n",
      "Epoch 2 | Step 1008100 | Avg Loss: 0.0153 | Grad Norm: 0.00866121\n",
      "Epoch 2 | Step 1008200 | Avg Loss: 0.0149 | Grad Norm: 0.00747793\n",
      "Epoch 2 | Step 1008300 | Avg Loss: 0.0151 | Grad Norm: 0.00805487\n",
      "Epoch 2 | Step 1008400 | Avg Loss: 0.0154 | Grad Norm: 0.00794682\n",
      "Epoch 2 | Step 1008500 | Avg Loss: 0.0153 | Grad Norm: 0.00949556\n",
      "Epoch 2 | Step 1008600 | Avg Loss: 0.0153 | Grad Norm: 0.00894734\n",
      "Epoch 2 | Step 1008700 | Avg Loss: 0.0159 | Grad Norm: 0.00910283\n",
      "Epoch 2 | Step 1008800 | Avg Loss: 0.0162 | Grad Norm: 0.00976770\n",
      "Epoch 2 | Step 1008900 | Avg Loss: 0.0161 | Grad Norm: 0.00923669\n",
      "Epoch 2 | Step 1009000 | Avg Loss: 0.0159 | Grad Norm: 0.00937020\n",
      "Epoch 2 | Step 1009100 | Avg Loss: 0.0160 | Grad Norm: 0.00924118\n",
      "Epoch 2 | Step 1009200 | Avg Loss: 0.0157 | Grad Norm: 0.00935590\n",
      "Epoch 2 | Step 1009300 | Avg Loss: 0.0158 | Grad Norm: 0.00857968\n",
      "Epoch 2 | Step 1009400 | Avg Loss: 0.0160 | Grad Norm: 0.00849225\n",
      "Epoch 2 | Step 1009500 | Avg Loss: 0.0160 | Grad Norm: 0.00878248\n",
      "Epoch 2 | Step 1009600 | Avg Loss: 0.0160 | Grad Norm: 0.00937872\n",
      "Epoch 2 | Step 1009700 | Avg Loss: 0.0159 | Grad Norm: 0.00813257\n",
      "Epoch 2 | Step 1009800 | Avg Loss: 0.0156 | Grad Norm: 0.00838125\n",
      "Epoch 2 | Step 1009900 | Avg Loss: 0.0160 | Grad Norm: 0.00852759\n",
      "Epoch 2 | Step 1010000 | Avg Loss: 0.0155 | Grad Norm: 0.00818633\n",
      "Epoch 2 | Step 1010100 | Avg Loss: 0.0157 | Grad Norm: 0.00858695\n",
      "Epoch 2 | Step 1010200 | Avg Loss: 0.0155 | Grad Norm: 0.00870659\n",
      "Epoch 2 | Step 1010300 | Avg Loss: 0.0154 | Grad Norm: 0.00796062\n",
      "Epoch 2 | Step 1010400 | Avg Loss: 0.0157 | Grad Norm: 0.00856630\n",
      "Epoch 2 | Step 1010500 | Avg Loss: 0.0158 | Grad Norm: 0.00866307\n",
      "Epoch 2 | Step 1010600 | Avg Loss: 0.0155 | Grad Norm: 0.00733843\n",
      "Epoch 2 | Step 1010700 | Avg Loss: 0.0151 | Grad Norm: 0.00729863\n",
      "Epoch 2 | Step 1010800 | Avg Loss: 0.0154 | Grad Norm: 0.00822211\n",
      "Epoch 2 | Step 1010900 | Avg Loss: 0.0157 | Grad Norm: 0.01247299\n",
      "Epoch 2 | Step 1011000 | Avg Loss: 0.0155 | Grad Norm: 0.00854479\n",
      "Epoch 2 | Step 1011100 | Avg Loss: 0.0152 | Grad Norm: 0.00695412\n",
      "Epoch 2 | Step 1011200 | Avg Loss: 0.0152 | Grad Norm: 0.00941197\n",
      "Epoch 2 | Step 1011300 | Avg Loss: 0.0153 | Grad Norm: 0.00919771\n",
      "Epoch 2 | Step 1011400 | Avg Loss: 0.0151 | Grad Norm: 0.00948558\n",
      "Epoch 2 | Step 1011500 | Avg Loss: 0.0149 | Grad Norm: 0.00891821\n",
      "Epoch 2 | Step 1011600 | Avg Loss: 0.0150 | Grad Norm: 0.00842779\n",
      "Epoch 2 | Step 1011700 | Avg Loss: 0.0147 | Grad Norm: 0.00948721\n",
      "Epoch 2 | Step 1011800 | Avg Loss: 0.0151 | Grad Norm: 0.00844983\n",
      "Epoch 2 | Step 1011900 | Avg Loss: 0.0154 | Grad Norm: 0.00820635\n",
      "Epoch 2 | Step 1012000 | Avg Loss: 0.0156 | Grad Norm: 0.00985060\n",
      "Epoch 2 | Step 1012100 | Avg Loss: 0.0155 | Grad Norm: 0.00802635\n",
      "Epoch 2 | Step 1012200 | Avg Loss: 0.0155 | Grad Norm: 0.00741265\n",
      "Epoch 2 | Step 1012300 | Avg Loss: 0.0154 | Grad Norm: 0.00887253\n",
      "Epoch 2 | Step 1012400 | Avg Loss: 0.0152 | Grad Norm: 0.00920276\n",
      "Epoch 2 | Step 1012500 | Avg Loss: 0.0155 | Grad Norm: 0.00907240\n",
      "Epoch 2 | Step 1012600 | Avg Loss: 0.0157 | Grad Norm: 0.00902315\n",
      "Epoch 2 | Step 1012700 | Avg Loss: 0.0158 | Grad Norm: 0.00833119\n",
      "Epoch 2 | Step 1012800 | Avg Loss: 0.0158 | Grad Norm: 0.00857157\n",
      "Epoch 2 | Step 1012900 | Avg Loss: 0.0155 | Grad Norm: 0.01042917\n",
      "Epoch 2 | Step 1013000 | Avg Loss: 0.0160 | Grad Norm: 0.00889900\n",
      "Epoch 2 | Step 1013100 | Avg Loss: 0.0157 | Grad Norm: 0.00841666\n",
      "Epoch 2 | Step 1013200 | Avg Loss: 0.0160 | Grad Norm: 0.00829420\n",
      "Epoch 2 | Step 1013300 | Avg Loss: 0.0157 | Grad Norm: 0.00976097\n",
      "Epoch 2 | Step 1013400 | Avg Loss: 0.0160 | Grad Norm: 0.00947304\n",
      "Epoch 2 | Step 1013500 | Avg Loss: 0.0159 | Grad Norm: 0.00923112\n",
      "Epoch 2 | Step 1013600 | Avg Loss: 0.0156 | Grad Norm: 0.00787015\n",
      "Epoch 2 | Step 1013700 | Avg Loss: 0.0154 | Grad Norm: 0.00873199\n",
      "Epoch 2 | Step 1013800 | Avg Loss: 0.0152 | Grad Norm: 0.00981627\n",
      "Epoch 2 | Step 1013900 | Avg Loss: 0.0157 | Grad Norm: 0.00916504\n",
      "Epoch 2 | Step 1014000 | Avg Loss: 0.0157 | Grad Norm: 0.00833970\n",
      "Epoch 2 | Step 1014100 | Avg Loss: 0.0156 | Grad Norm: 0.00897620\n",
      "Epoch 2 | Step 1014200 | Avg Loss: 0.0159 | Grad Norm: 0.01284814\n",
      "Epoch 2 | Step 1014300 | Avg Loss: 0.0160 | Grad Norm: 0.00829948\n",
      "Epoch 2 | Step 1014400 | Avg Loss: 0.0163 | Grad Norm: 0.01099519\n",
      "Epoch 2 | Step 1014500 | Avg Loss: 0.0159 | Grad Norm: 0.00969309\n",
      "Epoch 2 | Step 1014600 | Avg Loss: 0.0153 | Grad Norm: 0.00810360\n",
      "Epoch 2 | Step 1014700 | Avg Loss: 0.0155 | Grad Norm: 0.00994058\n",
      "Epoch 2 | Step 1014800 | Avg Loss: 0.0159 | Grad Norm: 0.00862841\n",
      "Epoch 2 | Step 1014900 | Avg Loss: 0.0158 | Grad Norm: 0.00857997\n",
      "Epoch 2 | Step 1015000 | Avg Loss: 0.0154 | Grad Norm: 0.00828963\n",
      "Epoch 2 | Step 1015100 | Avg Loss: 0.0151 | Grad Norm: 0.00886144\n",
      "Epoch 2 | Step 1015200 | Avg Loss: 0.0155 | Grad Norm: 0.00991387\n",
      "Epoch 2 | Step 1015300 | Avg Loss: 0.0154 | Grad Norm: 0.01002688\n",
      "Epoch 2 | Step 1015400 | Avg Loss: 0.0159 | Grad Norm: 0.00872263\n",
      "Epoch 2 | Step 1015500 | Avg Loss: 0.0159 | Grad Norm: 0.00982483\n",
      "Epoch 2 | Step 1015600 | Avg Loss: 0.0155 | Grad Norm: 0.00888143\n",
      "Epoch 2 | Step 1015700 | Avg Loss: 0.0155 | Grad Norm: 0.00974278\n",
      "Epoch 2 | Step 1015800 | Avg Loss: 0.0154 | Grad Norm: 0.00909244\n",
      "Epoch 2 | Step 1015900 | Avg Loss: 0.0156 | Grad Norm: 0.00965383\n",
      "Epoch 2 | Step 1016000 | Avg Loss: 0.0159 | Grad Norm: 0.00821256\n",
      "Epoch 2 | Step 1016100 | Avg Loss: 0.0159 | Grad Norm: 0.00774375\n",
      "Epoch 2 | Step 1016200 | Avg Loss: 0.0156 | Grad Norm: 0.00788297\n",
      "Epoch 2 | Step 1016300 | Avg Loss: 0.0154 | Grad Norm: 0.00858645\n",
      "Epoch 2 | Step 1016400 | Avg Loss: 0.0155 | Grad Norm: 0.00820749\n",
      "Epoch 2 | Step 1016500 | Avg Loss: 0.0155 | Grad Norm: 0.00862453\n",
      "Epoch 2 | Step 1016600 | Avg Loss: 0.0155 | Grad Norm: 0.00915052\n",
      "Epoch 2 | Step 1016700 | Avg Loss: 0.0155 | Grad Norm: 0.00885267\n",
      "Epoch 2 | Step 1016800 | Avg Loss: 0.0154 | Grad Norm: 0.01098124\n",
      "Epoch 2 | Step 1016900 | Avg Loss: 0.0156 | Grad Norm: 0.00923611\n",
      "Epoch 2 | Step 1017000 | Avg Loss: 0.0153 | Grad Norm: 0.00942709\n",
      "Epoch 2 | Step 1017100 | Avg Loss: 0.0155 | Grad Norm: 0.00904538\n",
      "Epoch 2 | Step 1017200 | Avg Loss: 0.0157 | Grad Norm: 0.00849678\n",
      "Epoch 2 | Step 1017300 | Avg Loss: 0.0156 | Grad Norm: 0.00957254\n",
      "Epoch 2 | Step 1017400 | Avg Loss: 0.0154 | Grad Norm: 0.00898436\n",
      "Epoch 2 | Step 1017500 | Avg Loss: 0.0155 | Grad Norm: 0.00851948\n",
      "Epoch 2 | Step 1017600 | Avg Loss: 0.0155 | Grad Norm: 0.00964144\n",
      "Epoch 2 | Step 1017700 | Avg Loss: 0.0155 | Grad Norm: 0.00996652\n",
      "Epoch 2 | Step 1017800 | Avg Loss: 0.0153 | Grad Norm: 0.00817142\n",
      "Epoch 2 | Step 1017900 | Avg Loss: 0.0151 | Grad Norm: 0.00992229\n",
      "Epoch 2 | Step 1018000 | Avg Loss: 0.0153 | Grad Norm: 0.01060226\n",
      "Epoch 2 | Step 1018100 | Avg Loss: 0.0155 | Grad Norm: 0.00853010\n",
      "Epoch 2 | Step 1018200 | Avg Loss: 0.0157 | Grad Norm: 0.00862778\n",
      "Epoch 2 | Step 1018300 | Avg Loss: 0.0158 | Grad Norm: 0.00878205\n",
      "Epoch 2 | Step 1018400 | Avg Loss: 0.0160 | Grad Norm: 0.00830759\n",
      "Epoch 2 | Step 1018500 | Avg Loss: 0.0159 | Grad Norm: 0.00863290\n",
      "Epoch 2 | Step 1018600 | Avg Loss: 0.0158 | Grad Norm: 0.00909161\n",
      "Epoch 2 | Step 1018700 | Avg Loss: 0.0160 | Grad Norm: 0.00872230\n",
      "Epoch 2 | Step 1018800 | Avg Loss: 0.0160 | Grad Norm: 0.00979162\n",
      "Epoch 2 | Step 1018900 | Avg Loss: 0.0157 | Grad Norm: 0.00805483\n",
      "Epoch 2 | Step 1019000 | Avg Loss: 0.0157 | Grad Norm: 0.00861382\n",
      "Epoch 2 | Step 1019100 | Avg Loss: 0.0156 | Grad Norm: 0.00900170\n",
      "Epoch 2 | Step 1019200 | Avg Loss: 0.0155 | Grad Norm: 0.00820831\n",
      "Epoch 2 | Step 1019300 | Avg Loss: 0.0158 | Grad Norm: 0.00992340\n",
      "Epoch 2 | Step 1019400 | Avg Loss: 0.0158 | Grad Norm: 0.01035447\n",
      "Epoch 2 | Step 1019500 | Avg Loss: 0.0158 | Grad Norm: 0.00875742\n",
      "Epoch 2 | Step 1019600 | Avg Loss: 0.0155 | Grad Norm: 0.00923220\n",
      "Epoch 2 | Step 1019700 | Avg Loss: 0.0153 | Grad Norm: 0.00815302\n",
      "Epoch 2 | Step 1019800 | Avg Loss: 0.0154 | Grad Norm: 0.00771275\n",
      "Epoch 2 | Step 1019900 | Avg Loss: 0.0156 | Grad Norm: 0.00906484\n",
      "Epoch 2 | Step 1020000 | Avg Loss: 0.0159 | Grad Norm: 0.00926240\n",
      "Epoch 2 | Step 1020100 | Avg Loss: 0.0158 | Grad Norm: 0.01036065\n",
      "Epoch 2 | Step 1020200 | Avg Loss: 0.0156 | Grad Norm: 0.00796590\n",
      "Epoch 2 | Step 1020300 | Avg Loss: 0.0155 | Grad Norm: 0.00892185\n",
      "Epoch 2 | Step 1020400 | Avg Loss: 0.0153 | Grad Norm: 0.00875735\n",
      "Epoch 2 | Step 1020500 | Avg Loss: 0.0154 | Grad Norm: 0.00813073\n",
      "Epoch 2 | Step 1020600 | Avg Loss: 0.0152 | Grad Norm: 0.00966463\n",
      "Epoch 2 | Step 1020700 | Avg Loss: 0.0157 | Grad Norm: 0.00779946\n",
      "Epoch 2 | Step 1020800 | Avg Loss: 0.0157 | Grad Norm: 0.00858101\n",
      "Epoch 2 | Step 1020900 | Avg Loss: 0.0158 | Grad Norm: 0.00847475\n",
      "Epoch 2 | Step 1021000 | Avg Loss: 0.0155 | Grad Norm: 0.00825933\n",
      "Epoch 2 | Step 1021100 | Avg Loss: 0.0154 | Grad Norm: 0.00879826\n",
      "Epoch 2 | Step 1021200 | Avg Loss: 0.0156 | Grad Norm: 0.00868903\n",
      "Epoch 2 | Step 1021300 | Avg Loss: 0.0156 | Grad Norm: 0.00985630\n",
      "Epoch 2 | Step 1021400 | Avg Loss: 0.0153 | Grad Norm: 0.00970726\n",
      "Epoch 2 | Step 1021500 | Avg Loss: 0.0155 | Grad Norm: 0.00810712\n",
      "Epoch 2 | Step 1021600 | Avg Loss: 0.0158 | Grad Norm: 0.00881014\n",
      "Epoch 2 | Step 1021700 | Avg Loss: 0.0157 | Grad Norm: 0.00864032\n",
      "Epoch 2 | Step 1021800 | Avg Loss: 0.0159 | Grad Norm: 0.00886290\n",
      "Epoch 2 | Step 1021900 | Avg Loss: 0.0158 | Grad Norm: 0.00953597\n",
      "Epoch 2 | Step 1022000 | Avg Loss: 0.0156 | Grad Norm: 0.00900841\n",
      "Epoch 2 | Step 1022100 | Avg Loss: 0.0158 | Grad Norm: 0.00975693\n",
      "Epoch 2 | Step 1022200 | Avg Loss: 0.0161 | Grad Norm: 0.01080525\n",
      "Epoch 2 | Step 1022300 | Avg Loss: 0.0158 | Grad Norm: 0.00781420\n",
      "Epoch 2 | Step 1022400 | Avg Loss: 0.0158 | Grad Norm: 0.00854918\n",
      "Epoch 2 | Step 1022500 | Avg Loss: 0.0160 | Grad Norm: 0.00808330\n",
      "Epoch 2 | Step 1022600 | Avg Loss: 0.0158 | Grad Norm: 0.01024477\n",
      "Epoch 2 | Step 1022700 | Avg Loss: 0.0160 | Grad Norm: 0.00888623\n",
      "Epoch 2 | Step 1022800 | Avg Loss: 0.0159 | Grad Norm: 0.00947818\n",
      "Epoch 2 | Step 1022900 | Avg Loss: 0.0156 | Grad Norm: 0.00807765\n",
      "Epoch 2 | Step 1023000 | Avg Loss: 0.0159 | Grad Norm: 0.00892496\n",
      "Epoch 2 | Step 1023100 | Avg Loss: 0.0159 | Grad Norm: 0.00908673\n",
      "Epoch 2 | Step 1023200 | Avg Loss: 0.0164 | Grad Norm: 0.01008302\n",
      "Epoch 2 | Step 1023300 | Avg Loss: 0.0161 | Grad Norm: 0.00923868\n",
      "Epoch 2 | Step 1023400 | Avg Loss: 0.0157 | Grad Norm: 0.01715324\n",
      "Epoch 2 | Step 1023500 | Avg Loss: 0.0155 | Grad Norm: 0.00834170\n",
      "Epoch 2 | Step 1023600 | Avg Loss: 0.0154 | Grad Norm: 0.00862539\n",
      "Epoch 2 | Step 1023700 | Avg Loss: 0.0154 | Grad Norm: 0.00838453\n",
      "Epoch 2 | Step 1023800 | Avg Loss: 0.0156 | Grad Norm: 0.00962951\n",
      "Epoch 2 | Step 1023900 | Avg Loss: 0.0154 | Grad Norm: 0.00873626\n",
      "Epoch 2 | Step 1024000 | Avg Loss: 0.0155 | Grad Norm: 0.00926985\n",
      "Epoch 2 | Step 1024100 | Avg Loss: 0.0154 | Grad Norm: 0.00836843\n",
      "Epoch 2 | Step 1024200 | Avg Loss: 0.0151 | Grad Norm: 0.00814645\n",
      "Epoch 2 | Step 1024300 | Avg Loss: 0.0150 | Grad Norm: 0.00842404\n",
      "Epoch 2 | Step 1024400 | Avg Loss: 0.0151 | Grad Norm: 0.00787777\n",
      "Epoch 2 | Step 1024500 | Avg Loss: 0.0151 | Grad Norm: 0.00817596\n",
      "Epoch 2 | Step 1024600 | Avg Loss: 0.0154 | Grad Norm: 0.00840064\n",
      "Epoch 2 | Step 1024700 | Avg Loss: 0.0156 | Grad Norm: 0.01008630\n",
      "Epoch 2 | Step 1024800 | Avg Loss: 0.0158 | Grad Norm: 0.00995607\n",
      "Epoch 2 | Step 1024900 | Avg Loss: 0.0155 | Grad Norm: 0.00848662\n",
      "Epoch 2 | Step 1025000 | Avg Loss: 0.0156 | Grad Norm: 0.00844283\n",
      "Epoch 2 | Step 1025100 | Avg Loss: 0.0152 | Grad Norm: 0.00827749\n",
      "Epoch 2 | Step 1025200 | Avg Loss: 0.0157 | Grad Norm: 0.00832124\n",
      "Epoch 2 | Step 1025300 | Avg Loss: 0.0158 | Grad Norm: 0.00779243\n",
      "Epoch 2 | Step 1025400 | Avg Loss: 0.0155 | Grad Norm: 0.00949631\n",
      "Epoch 2 | Step 1025500 | Avg Loss: 0.0156 | Grad Norm: 0.00885642\n",
      "Epoch 2 | Step 1025600 | Avg Loss: 0.0156 | Grad Norm: 0.00910900\n",
      "Epoch 2 | Step 1025700 | Avg Loss: 0.0153 | Grad Norm: 0.00805927\n",
      "Epoch 2 | Step 1025800 | Avg Loss: 0.0152 | Grad Norm: 0.00898541\n",
      "Epoch 2 | Step 1025900 | Avg Loss: 0.0151 | Grad Norm: 0.00962098\n",
      "Epoch 2 | Step 1026000 | Avg Loss: 0.0150 | Grad Norm: 0.00850841\n",
      "Epoch 2 | Step 1026100 | Avg Loss: 0.0151 | Grad Norm: 0.00780717\n",
      "Epoch 2 | Step 1026200 | Avg Loss: 0.0152 | Grad Norm: 0.00749450\n",
      "Epoch 2 | Step 1026300 | Avg Loss: 0.0152 | Grad Norm: 0.00842747\n",
      "Epoch 2 | Step 1026400 | Avg Loss: 0.0155 | Grad Norm: 0.00832274\n",
      "Epoch 2 | Step 1026500 | Avg Loss: 0.0154 | Grad Norm: 0.00979311\n",
      "Epoch 2 | Step 1026600 | Avg Loss: 0.0152 | Grad Norm: 0.00825508\n",
      "Epoch 2 | Step 1026700 | Avg Loss: 0.0154 | Grad Norm: 0.01011010\n",
      "Epoch 2 | Step 1026800 | Avg Loss: 0.0156 | Grad Norm: 0.00814149\n",
      "Epoch 2 | Step 1026900 | Avg Loss: 0.0155 | Grad Norm: 0.00811991\n",
      "Epoch 2 | Step 1027000 | Avg Loss: 0.0158 | Grad Norm: 0.01087529\n",
      "Epoch 2 | Step 1027100 | Avg Loss: 0.0158 | Grad Norm: 0.00857811\n",
      "Epoch 2 | Step 1027200 | Avg Loss: 0.0161 | Grad Norm: 0.01019963\n",
      "Epoch 2 | Step 1027300 | Avg Loss: 0.0160 | Grad Norm: 0.00886086\n",
      "Epoch 2 | Step 1027400 | Avg Loss: 0.0157 | Grad Norm: 0.00987268\n",
      "Epoch 2 | Step 1027500 | Avg Loss: 0.0158 | Grad Norm: 0.01012487\n",
      "Epoch 2 | Step 1027600 | Avg Loss: 0.0158 | Grad Norm: 0.00806693\n",
      "Epoch 2 | Step 1027700 | Avg Loss: 0.0158 | Grad Norm: 0.00826464\n",
      "Epoch 2 | Step 1027800 | Avg Loss: 0.0157 | Grad Norm: 0.00914519\n",
      "Epoch 2 | Step 1027900 | Avg Loss: 0.0158 | Grad Norm: 0.00870573\n",
      "Epoch 2 | Step 1028000 | Avg Loss: 0.0158 | Grad Norm: 0.00926380\n",
      "Epoch 2 | Step 1028100 | Avg Loss: 0.0155 | Grad Norm: 0.00884903\n",
      "Epoch 2 | Step 1028200 | Avg Loss: 0.0153 | Grad Norm: 0.00871456\n",
      "Epoch 2 | Step 1028300 | Avg Loss: 0.0157 | Grad Norm: 0.00889811\n",
      "Epoch 2 | Step 1028400 | Avg Loss: 0.0158 | Grad Norm: 0.00926290\n",
      "Epoch 2 | Step 1028500 | Avg Loss: 0.0159 | Grad Norm: 0.01114776\n",
      "Epoch 2 | Step 1028600 | Avg Loss: 0.0157 | Grad Norm: 0.00838553\n",
      "Epoch 2 | Step 1028700 | Avg Loss: 0.0153 | Grad Norm: 0.00808115\n",
      "Epoch 2 | Step 1028800 | Avg Loss: 0.0157 | Grad Norm: 0.01474803\n",
      "Epoch 2 | Step 1028900 | Avg Loss: 0.0156 | Grad Norm: 0.00783133\n",
      "Epoch 2 | Step 1029000 | Avg Loss: 0.0160 | Grad Norm: 0.00915171\n",
      "Epoch 2 | Step 1029100 | Avg Loss: 0.0162 | Grad Norm: 0.00791695\n",
      "Epoch 2 | Step 1029200 | Avg Loss: 0.0160 | Grad Norm: 0.00892099\n",
      "Epoch 2 | Step 1029300 | Avg Loss: 0.0162 | Grad Norm: 0.00871392\n",
      "Epoch 2 | Step 1029400 | Avg Loss: 0.0162 | Grad Norm: 0.00788554\n",
      "Epoch 2 | Step 1029500 | Avg Loss: 0.0161 | Grad Norm: 0.00877651\n",
      "Epoch 2 | Step 1029600 | Avg Loss: 0.0157 | Grad Norm: 0.00938927\n",
      "Epoch 2 | Step 1029700 | Avg Loss: 0.0154 | Grad Norm: 0.00892943\n",
      "Epoch 2 | Step 1029800 | Avg Loss: 0.0155 | Grad Norm: 0.00781321\n",
      "Epoch 2 | Step 1029900 | Avg Loss: 0.0155 | Grad Norm: 0.00886655\n",
      "Epoch 2 | Step 1030000 | Avg Loss: 0.0158 | Grad Norm: 0.01265399\n",
      "Epoch 2 | Step 1030100 | Avg Loss: 0.0160 | Grad Norm: 0.00909135\n",
      "Epoch 2 | Step 1030200 | Avg Loss: 0.0157 | Grad Norm: 0.00941097\n",
      "Epoch 2 | Step 1030300 | Avg Loss: 0.0156 | Grad Norm: 0.00840039\n",
      "Epoch 2 | Step 1030400 | Avg Loss: 0.0156 | Grad Norm: 0.00740882\n",
      "Epoch 2 | Step 1030500 | Avg Loss: 0.0155 | Grad Norm: 0.00782423\n",
      "Epoch 2 | Step 1030600 | Avg Loss: 0.0152 | Grad Norm: 0.00755615\n",
      "Epoch 2 | Step 1030700 | Avg Loss: 0.0157 | Grad Norm: 0.00815451\n",
      "Epoch 2 | Step 1030800 | Avg Loss: 0.0153 | Grad Norm: 0.00884294\n",
      "Epoch 2 | Step 1030900 | Avg Loss: 0.0158 | Grad Norm: 0.00932543\n",
      "Epoch 2 | Step 1031000 | Avg Loss: 0.0157 | Grad Norm: 0.00825916\n",
      "Epoch 2 | Step 1031100 | Avg Loss: 0.0155 | Grad Norm: 0.00906373\n",
      "Epoch 2 | Step 1031200 | Avg Loss: 0.0157 | Grad Norm: 0.00939254\n",
      "Epoch 2 | Step 1031300 | Avg Loss: 0.0152 | Grad Norm: 0.01168243\n",
      "Epoch 2 | Step 1031400 | Avg Loss: 0.0154 | Grad Norm: 0.00832158\n",
      "Epoch 2 | Step 1031500 | Avg Loss: 0.0152 | Grad Norm: 0.00948126\n",
      "Epoch 2 | Step 1031600 | Avg Loss: 0.0149 | Grad Norm: 0.00943663\n",
      "Epoch 2 | Step 1031700 | Avg Loss: 0.0146 | Grad Norm: 0.00858590\n",
      "Epoch 2 | Step 1031800 | Avg Loss: 0.0148 | Grad Norm: 0.01006739\n",
      "Epoch 2 | Step 1031900 | Avg Loss: 0.0150 | Grad Norm: 0.00831244\n",
      "Epoch 2 | Step 1032000 | Avg Loss: 0.0150 | Grad Norm: 0.00840903\n",
      "Epoch 2 | Step 1032100 | Avg Loss: 0.0149 | Grad Norm: 0.00749261\n",
      "Epoch 2 | Step 1032200 | Avg Loss: 0.0150 | Grad Norm: 0.00830980\n",
      "Epoch 2 | Step 1032300 | Avg Loss: 0.0145 | Grad Norm: 0.00850790\n",
      "Epoch 2 | Step 1032400 | Avg Loss: 0.0141 | Grad Norm: 0.00805753\n",
      "Epoch 2 | Step 1032500 | Avg Loss: 0.0146 | Grad Norm: 0.01133039\n",
      "Epoch 2 | Step 1032600 | Avg Loss: 0.0149 | Grad Norm: 0.00875752\n",
      "Epoch 2 | Step 1032700 | Avg Loss: 0.0148 | Grad Norm: 0.00745817\n",
      "Epoch 2 | Step 1032800 | Avg Loss: 0.0148 | Grad Norm: 0.00839374\n",
      "Epoch 2 | Step 1032900 | Avg Loss: 0.0154 | Grad Norm: 0.00800564\n",
      "Epoch 2 | Step 1033000 | Avg Loss: 0.0154 | Grad Norm: 0.00687408\n",
      "Epoch 2 | Step 1033100 | Avg Loss: 0.0155 | Grad Norm: 0.00835955\n",
      "Epoch 2 | Step 1033200 | Avg Loss: 0.0157 | Grad Norm: 0.01061060\n",
      "Epoch 2 | Step 1033300 | Avg Loss: 0.0157 | Grad Norm: 0.00822248\n",
      "Epoch 2 | Step 1033400 | Avg Loss: 0.0157 | Grad Norm: 0.00955598\n",
      "Epoch 2 | Step 1033500 | Avg Loss: 0.0154 | Grad Norm: 0.00862136\n",
      "Epoch 2 | Step 1033600 | Avg Loss: 0.0158 | Grad Norm: 0.00898242\n",
      "Epoch 2 | Step 1033700 | Avg Loss: 0.0160 | Grad Norm: 0.00883122\n",
      "Epoch 2 | Step 1033800 | Avg Loss: 0.0159 | Grad Norm: 0.00951640\n",
      "Epoch 2 | Step 1033900 | Avg Loss: 0.0157 | Grad Norm: 0.00936429\n",
      "Epoch 2 | Step 1034000 | Avg Loss: 0.0157 | Grad Norm: 0.00866848\n",
      "Epoch 2 | Step 1034100 | Avg Loss: 0.0155 | Grad Norm: 0.00916372\n",
      "Epoch 2 | Step 1034200 | Avg Loss: 0.0157 | Grad Norm: 0.00798793\n",
      "Epoch 2 | Step 1034300 | Avg Loss: 0.0154 | Grad Norm: 0.00911369\n",
      "Epoch 2 | Step 1034400 | Avg Loss: 0.0156 | Grad Norm: 0.00789433\n",
      "Epoch 2 | Step 1034500 | Avg Loss: 0.0154 | Grad Norm: 0.00958331\n",
      "Epoch 2 | Step 1034600 | Avg Loss: 0.0156 | Grad Norm: 0.00987860\n",
      "Epoch 2 | Step 1034700 | Avg Loss: 0.0155 | Grad Norm: 0.00943486\n",
      "Epoch 2 | Step 1034800 | Avg Loss: 0.0157 | Grad Norm: 0.00955038\n",
      "Epoch 2 | Step 1034900 | Avg Loss: 0.0154 | Grad Norm: 0.00852471\n",
      "Epoch 2 | Step 1035000 | Avg Loss: 0.0151 | Grad Norm: 0.00800086\n",
      "Epoch 2 | Step 1035100 | Avg Loss: 0.0150 | Grad Norm: 0.00963799\n",
      "Epoch 2 | Step 1035200 | Avg Loss: 0.0152 | Grad Norm: 0.01062664\n",
      "Epoch 2 | Step 1035300 | Avg Loss: 0.0153 | Grad Norm: 0.00848096\n",
      "Epoch 2 | Step 1035400 | Avg Loss: 0.0151 | Grad Norm: 0.00760140\n",
      "Epoch 2 | Step 1035500 | Avg Loss: 0.0154 | Grad Norm: 0.01048965\n",
      "Epoch 2 | Step 1035600 | Avg Loss: 0.0160 | Grad Norm: 0.01066914\n",
      "Epoch 2 | Step 1035700 | Avg Loss: 0.0155 | Grad Norm: 0.00936237\n",
      "Epoch 2 | Step 1035800 | Avg Loss: 0.0159 | Grad Norm: 0.01025390\n",
      "Epoch 2 | Step 1035900 | Avg Loss: 0.0158 | Grad Norm: 0.00928674\n",
      "Epoch 2 | Step 1036000 | Avg Loss: 0.0155 | Grad Norm: 0.00974235\n",
      "Epoch 2 | Step 1036100 | Avg Loss: 0.0157 | Grad Norm: 0.00968014\n",
      "Epoch 2 | Step 1036200 | Avg Loss: 0.0157 | Grad Norm: 0.01121596\n",
      "Epoch 2 | Step 1036300 | Avg Loss: 0.0154 | Grad Norm: 0.00809150\n",
      "Epoch 2 | Step 1036400 | Avg Loss: 0.0155 | Grad Norm: 0.00859563\n",
      "Epoch 2 | Step 1036500 | Avg Loss: 0.0152 | Grad Norm: 0.00983288\n",
      "Epoch 2 | Step 1036600 | Avg Loss: 0.0155 | Grad Norm: 0.01147376\n",
      "Epoch 2 | Step 1036700 | Avg Loss: 0.0154 | Grad Norm: 0.00751896\n",
      "Epoch 2 | Step 1036800 | Avg Loss: 0.0157 | Grad Norm: 0.01144360\n",
      "Epoch 2 | Step 1036900 | Avg Loss: 0.0156 | Grad Norm: 0.00838266\n",
      "Epoch 2 | Step 1037000 | Avg Loss: 0.0158 | Grad Norm: 0.01199458\n",
      "Epoch 2 | Step 1037100 | Avg Loss: 0.0156 | Grad Norm: 0.00818154\n",
      "Epoch 2 | Step 1037200 | Avg Loss: 0.0155 | Grad Norm: 0.00844478\n",
      "Epoch 2 | Step 1037300 | Avg Loss: 0.0154 | Grad Norm: 0.00888868\n",
      "Epoch 2 | Step 1037400 | Avg Loss: 0.0154 | Grad Norm: 0.00867175\n",
      "Epoch 2 | Step 1037500 | Avg Loss: 0.0155 | Grad Norm: 0.00850788\n",
      "Epoch 2 | Step 1037600 | Avg Loss: 0.0157 | Grad Norm: 0.00785898\n",
      "Epoch 2 | Step 1037700 | Avg Loss: 0.0156 | Grad Norm: 0.00887901\n",
      "Epoch 2 | Step 1037800 | Avg Loss: 0.0153 | Grad Norm: 0.00834290\n",
      "Epoch 2 | Step 1037900 | Avg Loss: 0.0154 | Grad Norm: 0.00935036\n",
      "Epoch 2 | Step 1038000 | Avg Loss: 0.0153 | Grad Norm: 0.00857116\n",
      "Epoch 2 | Step 1038100 | Avg Loss: 0.0151 | Grad Norm: 0.00810427\n",
      "Epoch 2 | Step 1038200 | Avg Loss: 0.0153 | Grad Norm: 0.00806810\n",
      "Epoch 2 | Step 1038300 | Avg Loss: 0.0156 | Grad Norm: 0.00788087\n",
      "Epoch 2 | Step 1038400 | Avg Loss: 0.0156 | Grad Norm: 0.00842363\n",
      "Epoch 2 | Step 1038500 | Avg Loss: 0.0159 | Grad Norm: 0.00940500\n",
      "Epoch 2 | Step 1038600 | Avg Loss: 0.0154 | Grad Norm: 0.00734009\n",
      "Epoch 2 | Step 1038700 | Avg Loss: 0.0156 | Grad Norm: 0.00987898\n",
      "Epoch 2 | Step 1038800 | Avg Loss: 0.0157 | Grad Norm: 0.00866885\n",
      "Epoch 2 | Step 1038900 | Avg Loss: 0.0156 | Grad Norm: 0.01064011\n",
      "Epoch 2 | Step 1039000 | Avg Loss: 0.0159 | Grad Norm: 0.00848241\n",
      "Epoch 2 | Step 1039100 | Avg Loss: 0.0163 | Grad Norm: 0.00856695\n",
      "Epoch 2 | Step 1039200 | Avg Loss: 0.0158 | Grad Norm: 0.00966652\n",
      "Epoch 2 | Step 1039300 | Avg Loss: 0.0157 | Grad Norm: 0.00934671\n",
      "Epoch 2 | Step 1039400 | Avg Loss: 0.0157 | Grad Norm: 0.00925349\n",
      "Epoch 2 | Step 1039500 | Avg Loss: 0.0154 | Grad Norm: 0.00853237\n",
      "Epoch 2 | Step 1039600 | Avg Loss: 0.0151 | Grad Norm: 0.00803066\n",
      "Epoch 2 | Step 1039700 | Avg Loss: 0.0150 | Grad Norm: 0.00968265\n",
      "Epoch 2 | Step 1039800 | Avg Loss: 0.0153 | Grad Norm: 0.00892963\n",
      "Epoch 2 | Step 1039900 | Avg Loss: 0.0157 | Grad Norm: 0.00929163\n",
      "Epoch 2 | Step 1040000 | Avg Loss: 0.0159 | Grad Norm: 0.01000987\n",
      "Epoch 2 | Step 1040100 | Avg Loss: 0.0161 | Grad Norm: 0.00948439\n",
      "Epoch 2 | Step 1040200 | Avg Loss: 0.0157 | Grad Norm: 0.00825449\n",
      "Epoch 2 | Step 1040300 | Avg Loss: 0.0159 | Grad Norm: 0.00892288\n",
      "Epoch 2 | Step 1040400 | Avg Loss: 0.0158 | Grad Norm: 0.01022286\n",
      "Epoch 2 | Step 1040500 | Avg Loss: 0.0155 | Grad Norm: 0.00867257\n",
      "Epoch 2 | Step 1040600 | Avg Loss: 0.0153 | Grad Norm: 0.00889176\n",
      "Epoch 2 | Step 1040700 | Avg Loss: 0.0153 | Grad Norm: 0.00731916\n",
      "Epoch 2 | Step 1040800 | Avg Loss: 0.0151 | Grad Norm: 0.00915494\n",
      "Epoch 2 | Step 1040900 | Avg Loss: 0.0150 | Grad Norm: 0.00957030\n",
      "Epoch 2 | Step 1041000 | Avg Loss: 0.0154 | Grad Norm: 0.00874102\n",
      "Epoch 2 | Step 1041100 | Avg Loss: 0.0154 | Grad Norm: 0.00989799\n",
      "Epoch 2 | Step 1041200 | Avg Loss: 0.0152 | Grad Norm: 0.00839565\n",
      "Epoch 2 | Step 1041300 | Avg Loss: 0.0154 | Grad Norm: 0.00900620\n",
      "Epoch 2 | Step 1041400 | Avg Loss: 0.0155 | Grad Norm: 0.00960817\n",
      "Epoch 2 | Step 1041500 | Avg Loss: 0.0153 | Grad Norm: 0.00785650\n",
      "Epoch 2 | Step 1041600 | Avg Loss: 0.0151 | Grad Norm: 0.00905716\n",
      "Epoch 2 | Step 1041700 | Avg Loss: 0.0154 | Grad Norm: 0.01020358\n",
      "Epoch 2 | Step 1041800 | Avg Loss: 0.0155 | Grad Norm: 0.00972453\n",
      "Epoch 2 | Step 1041900 | Avg Loss: 0.0157 | Grad Norm: 0.00959273\n",
      "Epoch 2 | Step 1042000 | Avg Loss: 0.0161 | Grad Norm: 0.00833464\n",
      "Epoch 2 | Step 1042100 | Avg Loss: 0.0160 | Grad Norm: 0.01108246\n",
      "Epoch 2 | Step 1042200 | Avg Loss: 0.0158 | Grad Norm: 0.00996070\n",
      "Epoch 2 | Step 1042300 | Avg Loss: 0.0155 | Grad Norm: 0.00921000\n",
      "Epoch 2 | Step 1042400 | Avg Loss: 0.0154 | Grad Norm: 0.00886247\n",
      "Epoch 2 | Step 1042500 | Avg Loss: 0.0157 | Grad Norm: 0.00847844\n",
      "Epoch 2 | Step 1042600 | Avg Loss: 0.0156 | Grad Norm: 0.00888513\n",
      "Epoch 2 | Step 1042700 | Avg Loss: 0.0158 | Grad Norm: 0.01110320\n",
      "Epoch 2 | Step 1042800 | Avg Loss: 0.0156 | Grad Norm: 0.00816926\n",
      "Epoch 2 | Step 1042900 | Avg Loss: 0.0156 | Grad Norm: 0.00891949\n",
      "Epoch 2 | Step 1043000 | Avg Loss: 0.0156 | Grad Norm: 0.00976181\n",
      "Epoch 2 | Step 1043100 | Avg Loss: 0.0154 | Grad Norm: 0.00759513\n",
      "Epoch 2 | Step 1043200 | Avg Loss: 0.0157 | Grad Norm: 0.00798404\n",
      "Epoch 2 | Step 1043300 | Avg Loss: 0.0159 | Grad Norm: 0.00885011\n",
      "Epoch 2 | Step 1043400 | Avg Loss: 0.0162 | Grad Norm: 0.00872923\n",
      "Epoch 2 | Step 1043500 | Avg Loss: 0.0158 | Grad Norm: 0.00901321\n",
      "Epoch 2 | Step 1043600 | Avg Loss: 0.0156 | Grad Norm: 0.00858542\n",
      "Epoch 2 | Step 1043700 | Avg Loss: 0.0156 | Grad Norm: 0.00942777\n",
      "Epoch 2 | Step 1043800 | Avg Loss: 0.0160 | Grad Norm: 0.00837739\n",
      "Epoch 2 | Step 1043900 | Avg Loss: 0.0153 | Grad Norm: 0.01006659\n",
      "Epoch 2 | Step 1044000 | Avg Loss: 0.0155 | Grad Norm: 0.00852332\n",
      "Epoch 2 | Step 1044100 | Avg Loss: 0.0157 | Grad Norm: 0.01010768\n",
      "Epoch 2 | Step 1044200 | Avg Loss: 0.0159 | Grad Norm: 0.00921126\n",
      "Epoch 2 | Step 1044300 | Avg Loss: 0.0156 | Grad Norm: 0.00875973\n",
      "Epoch 2 | Step 1044400 | Avg Loss: 0.0158 | Grad Norm: 0.00962067\n",
      "Epoch 2 | Step 1044500 | Avg Loss: 0.0155 | Grad Norm: 0.00871878\n",
      "Epoch 2 | Step 1044600 | Avg Loss: 0.0154 | Grad Norm: 0.00850973\n",
      "Epoch 2 | Step 1044700 | Avg Loss: 0.0157 | Grad Norm: 0.00836508\n",
      "Epoch 2 | Step 1044800 | Avg Loss: 0.0158 | Grad Norm: 0.00922930\n",
      "Epoch 2 | Step 1044900 | Avg Loss: 0.0160 | Grad Norm: 0.00823603\n",
      "Epoch 2 | Step 1045000 | Avg Loss: 0.0161 | Grad Norm: 0.00977276\n",
      "Epoch 2 | Step 1045100 | Avg Loss: 0.0160 | Grad Norm: 0.00941312\n",
      "Epoch 2 | Step 1045200 | Avg Loss: 0.0161 | Grad Norm: 0.00823729\n",
      "Epoch 2 | Step 1045300 | Avg Loss: 0.0159 | Grad Norm: 0.01017724\n",
      "Epoch 2 | Step 1045400 | Avg Loss: 0.0164 | Grad Norm: 0.00897152\n",
      "Epoch 2 | Step 1045500 | Avg Loss: 0.0163 | Grad Norm: 0.00992975\n",
      "Epoch 2 | Step 1045600 | Avg Loss: 0.0162 | Grad Norm: 0.00871773\n",
      "Epoch 2 | Step 1045700 | Avg Loss: 0.0161 | Grad Norm: 0.00853474\n",
      "Epoch 2 | Step 1045800 | Avg Loss: 0.0158 | Grad Norm: 0.00862287\n",
      "Epoch 2 | Step 1045900 | Avg Loss: 0.0161 | Grad Norm: 0.01061719\n",
      "Epoch 2 | Step 1046000 | Avg Loss: 0.0159 | Grad Norm: 0.00870001\n",
      "Epoch 2 | Step 1046100 | Avg Loss: 0.0159 | Grad Norm: 0.01054581\n",
      "Epoch 2 | Step 1046200 | Avg Loss: 0.0160 | Grad Norm: 0.00852337\n",
      "Epoch 2 | Step 1046300 | Avg Loss: 0.0162 | Grad Norm: 0.00913863\n",
      "Epoch 2 | Step 1046400 | Avg Loss: 0.0158 | Grad Norm: 0.00847431\n",
      "Epoch 2 | Step 1046500 | Avg Loss: 0.0158 | Grad Norm: 0.00824980\n",
      "Epoch 2 | Step 1046600 | Avg Loss: 0.0154 | Grad Norm: 0.00990295\n",
      "Epoch 2 | Step 1046700 | Avg Loss: 0.0158 | Grad Norm: 0.00822185\n",
      "Epoch 2 | Step 1046800 | Avg Loss: 0.0163 | Grad Norm: 0.00894943\n",
      "Epoch 2 | Step 1046900 | Avg Loss: 0.0159 | Grad Norm: 0.00891235\n",
      "Epoch 2 | Step 1047000 | Avg Loss: 0.0159 | Grad Norm: 0.00871149\n",
      "Epoch 2 | Step 1047100 | Avg Loss: 0.0159 | Grad Norm: 0.00787112\n",
      "Epoch 2 | Step 1047200 | Avg Loss: 0.0157 | Grad Norm: 0.00900604\n",
      "Epoch 2 | Step 1047300 | Avg Loss: 0.0157 | Grad Norm: 0.00896583\n",
      "Epoch 2 | Step 1047400 | Avg Loss: 0.0156 | Grad Norm: 0.00815025\n",
      "Epoch 2 | Step 1047500 | Avg Loss: 0.0158 | Grad Norm: 0.00739667\n",
      "Epoch 2 | Step 1047600 | Avg Loss: 0.0158 | Grad Norm: 0.00907955\n",
      "Epoch 2 | Step 1047700 | Avg Loss: 0.0161 | Grad Norm: 0.00925584\n",
      "Epoch 2 | Step 1047800 | Avg Loss: 0.0164 | Grad Norm: 0.00865673\n",
      "Epoch 2 | Step 1047900 | Avg Loss: 0.0157 | Grad Norm: 0.00884949\n",
      "Epoch 2 | Step 1048000 | Avg Loss: 0.0158 | Grad Norm: 0.00903593\n",
      "Epoch 2 | Step 1048100 | Avg Loss: 0.0158 | Grad Norm: 0.01133429\n",
      "Epoch 2 | Step 1048200 | Avg Loss: 0.0155 | Grad Norm: 0.00949062\n",
      "Epoch 2 | Step 1048300 | Avg Loss: 0.0159 | Grad Norm: 0.00890253\n",
      "Epoch 2 | Step 1048400 | Avg Loss: 0.0156 | Grad Norm: 0.00866821\n",
      "Epoch 2 | Step 1048500 | Avg Loss: 0.0155 | Grad Norm: 0.00760630\n",
      "Epoch 2 | Step 1048600 | Avg Loss: 0.0151 | Grad Norm: 0.00867147\n",
      "Epoch 2 | Step 1048700 | Avg Loss: 0.0151 | Grad Norm: 0.00823438\n",
      "Epoch 2 | Step 1048800 | Avg Loss: 0.0152 | Grad Norm: 0.00811114\n",
      "Epoch 2 | Step 1048900 | Avg Loss: 0.0154 | Grad Norm: 0.00820504\n",
      "Epoch 2 | Step 1049000 | Avg Loss: 0.0152 | Grad Norm: 0.00979012\n",
      "Epoch 2 | Step 1049100 | Avg Loss: 0.0155 | Grad Norm: 0.00905167\n",
      "Epoch 2 | Step 1049200 | Avg Loss: 0.0156 | Grad Norm: 0.00985433\n",
      "Epoch 2 | Step 1049300 | Avg Loss: 0.0156 | Grad Norm: 0.00958104\n",
      "Epoch 2 | Step 1049400 | Avg Loss: 0.0152 | Grad Norm: 0.00943147\n",
      "Epoch 2 | Step 1049500 | Avg Loss: 0.0155 | Grad Norm: 0.00778188\n",
      "Epoch 2 | Step 1049600 | Avg Loss: 0.0157 | Grad Norm: 0.00821827\n",
      "Epoch 2 | Step 1049700 | Avg Loss: 0.0157 | Grad Norm: 0.00897346\n",
      "Epoch 2 | Step 1049800 | Avg Loss: 0.0159 | Grad Norm: 0.00951320\n",
      "Epoch 2 | Step 1049900 | Avg Loss: 0.0159 | Grad Norm: 0.01022885\n",
      "Epoch 2 | Step 1050000 | Avg Loss: 0.0157 | Grad Norm: 0.00882090\n",
      "Epoch 2 | Step 1050100 | Avg Loss: 0.0159 | Grad Norm: 0.00948249\n",
      "Epoch 2 | Step 1050200 | Avg Loss: 0.0158 | Grad Norm: 0.00937147\n",
      "Epoch 2 | Step 1050300 | Avg Loss: 0.0159 | Grad Norm: 0.01016873\n",
      "Epoch 2 | Step 1050400 | Avg Loss: 0.0158 | Grad Norm: 0.00916104\n",
      "Epoch 2 | Step 1050500 | Avg Loss: 0.0160 | Grad Norm: 0.00834667\n",
      "Epoch 2 | Step 1050600 | Avg Loss: 0.0158 | Grad Norm: 0.00767423\n",
      "Epoch 2 | Step 1050700 | Avg Loss: 0.0160 | Grad Norm: 0.01024611\n",
      "Epoch 2 | Step 1050800 | Avg Loss: 0.0160 | Grad Norm: 0.00864004\n",
      "Epoch 2 | Step 1050900 | Avg Loss: 0.0157 | Grad Norm: 0.00860981\n",
      "Epoch 2 | Step 1051000 | Avg Loss: 0.0156 | Grad Norm: 0.00839905\n",
      "Epoch 2 | Step 1051100 | Avg Loss: 0.0154 | Grad Norm: 0.01027812\n",
      "Epoch 2 | Step 1051200 | Avg Loss: 0.0155 | Grad Norm: 0.00972741\n",
      "Epoch 2 | Step 1051300 | Avg Loss: 0.0158 | Grad Norm: 0.01087680\n",
      "Epoch 2 | Step 1051400 | Avg Loss: 0.0158 | Grad Norm: 0.00969484\n",
      "Epoch 2 | Step 1051500 | Avg Loss: 0.0161 | Grad Norm: 0.01016555\n",
      "Epoch 2 | Step 1051600 | Avg Loss: 0.0157 | Grad Norm: 0.01082546\n",
      "Epoch 2 | Step 1051700 | Avg Loss: 0.0154 | Grad Norm: 0.00891469\n",
      "Epoch 2 | Step 1051800 | Avg Loss: 0.0156 | Grad Norm: 0.00912702\n",
      "Epoch 2 | Step 1051900 | Avg Loss: 0.0156 | Grad Norm: 0.00912536\n",
      "Epoch 2 | Step 1052000 | Avg Loss: 0.0153 | Grad Norm: 0.00932237\n",
      "Epoch 2 | Step 1052100 | Avg Loss: 0.0153 | Grad Norm: 0.00867933\n",
      "Epoch 2 | Step 1052200 | Avg Loss: 0.0149 | Grad Norm: 0.01758101\n",
      "Epoch 2 | Step 1052300 | Avg Loss: 0.0150 | Grad Norm: 0.01025999\n",
      "Epoch 2 | Step 1052400 | Avg Loss: 0.0151 | Grad Norm: 0.01065782\n",
      "Epoch 2 | Step 1052500 | Avg Loss: 0.0153 | Grad Norm: 0.01150203\n",
      "Epoch 2 | Step 1052600 | Avg Loss: 0.0152 | Grad Norm: 0.00941641\n",
      "Epoch 2 | Step 1052700 | Avg Loss: 0.0155 | Grad Norm: 0.00909433\n",
      "Epoch 2 | Step 1052800 | Avg Loss: 0.0156 | Grad Norm: 0.00880432\n",
      "Epoch 2 | Step 1052900 | Avg Loss: 0.0155 | Grad Norm: 0.00847828\n",
      "Epoch 2 | Step 1053000 | Avg Loss: 0.0152 | Grad Norm: 0.00760973\n",
      "Epoch 2 | Step 1053100 | Avg Loss: 0.0154 | Grad Norm: 0.00935660\n",
      "Epoch 2 | Step 1053200 | Avg Loss: 0.0155 | Grad Norm: 0.00783241\n",
      "Epoch 2 | Step 1053300 | Avg Loss: 0.0153 | Grad Norm: 0.00898983\n",
      "Epoch 2 | Step 1053400 | Avg Loss: 0.0155 | Grad Norm: 0.00800948\n",
      "Epoch 2 | Step 1053500 | Avg Loss: 0.0159 | Grad Norm: 0.00825286\n",
      "Epoch 2 | Step 1053600 | Avg Loss: 0.0159 | Grad Norm: 0.00885802\n",
      "Epoch 2 | Step 1053700 | Avg Loss: 0.0156 | Grad Norm: 0.00856834\n",
      "Epoch 2 | Step 1053800 | Avg Loss: 0.0156 | Grad Norm: 0.00869676\n",
      "Epoch 2 | Step 1053900 | Avg Loss: 0.0159 | Grad Norm: 0.00830201\n",
      "Epoch 2 | Step 1054000 | Avg Loss: 0.0160 | Grad Norm: 0.00829156\n",
      "Epoch 2 | Step 1054100 | Avg Loss: 0.0159 | Grad Norm: 0.01037024\n",
      "Epoch 2 | Step 1054200 | Avg Loss: 0.0152 | Grad Norm: 0.00866085\n",
      "Epoch 2 | Step 1054300 | Avg Loss: 0.0152 | Grad Norm: 0.00912574\n",
      "Epoch 2 | Step 1054400 | Avg Loss: 0.0151 | Grad Norm: 0.00784159\n",
      "Epoch 2 | Step 1054500 | Avg Loss: 0.0154 | Grad Norm: 0.00814248\n",
      "Epoch 2 | Step 1054600 | Avg Loss: 0.0152 | Grad Norm: 0.00823384\n",
      "Epoch 2 | Step 1054700 | Avg Loss: 0.0153 | Grad Norm: 0.00962367\n",
      "Epoch 2 | Step 1054800 | Avg Loss: 0.0150 | Grad Norm: 0.00772468\n",
      "Epoch 2 | Step 1054900 | Avg Loss: 0.0152 | Grad Norm: 0.01083162\n",
      "Epoch 2 | Step 1055000 | Avg Loss: 0.0153 | Grad Norm: 0.00950729\n",
      "Epoch 2 | Step 1055100 | Avg Loss: 0.0151 | Grad Norm: 0.00879794\n",
      "Epoch 2 | Step 1055200 | Avg Loss: 0.0155 | Grad Norm: 0.00905664\n",
      "Epoch 2 | Step 1055300 | Avg Loss: 0.0157 | Grad Norm: 0.00928626\n",
      "Epoch 2 | Step 1055400 | Avg Loss: 0.0155 | Grad Norm: 0.00853092\n",
      "Epoch 2 | Step 1055500 | Avg Loss: 0.0153 | Grad Norm: 0.00855288\n",
      "Epoch 2 | Step 1055600 | Avg Loss: 0.0154 | Grad Norm: 0.00868390\n",
      "Epoch 2 | Step 1055700 | Avg Loss: 0.0151 | Grad Norm: 0.00874984\n",
      "Epoch 2 | Step 1055800 | Avg Loss: 0.0150 | Grad Norm: 0.01120103\n",
      "Epoch 2 | Step 1055900 | Avg Loss: 0.0151 | Grad Norm: 0.00921900\n",
      "Epoch 2 | Step 1056000 | Avg Loss: 0.0151 | Grad Norm: 0.00757667\n",
      "Epoch 2 | Step 1056100 | Avg Loss: 0.0156 | Grad Norm: 0.00980891\n",
      "Epoch 2 | Step 1056200 | Avg Loss: 0.0158 | Grad Norm: 0.00752844\n",
      "Epoch 2 | Step 1056300 | Avg Loss: 0.0159 | Grad Norm: 0.00833865\n",
      "Epoch 2 | Step 1056400 | Avg Loss: 0.0158 | Grad Norm: 0.00798064\n",
      "Epoch 2 | Step 1056500 | Avg Loss: 0.0158 | Grad Norm: 0.00884239\n",
      "Epoch 2 | Step 1056600 | Avg Loss: 0.0156 | Grad Norm: 0.00957781\n",
      "Epoch 2 | Step 1056700 | Avg Loss: 0.0155 | Grad Norm: 0.00937972\n",
      "Epoch 2 | Step 1056800 | Avg Loss: 0.0153 | Grad Norm: 0.00900657\n",
      "Epoch 2 | Step 1056900 | Avg Loss: 0.0155 | Grad Norm: 0.00824265\n",
      "Epoch 2 | Step 1057000 | Avg Loss: 0.0152 | Grad Norm: 0.00807392\n",
      "Epoch 2 | Step 1057100 | Avg Loss: 0.0155 | Grad Norm: 0.01012201\n",
      "Epoch 2 | Step 1057200 | Avg Loss: 0.0158 | Grad Norm: 0.00987784\n",
      "Epoch 2 | Step 1057300 | Avg Loss: 0.0161 | Grad Norm: 0.00871132\n",
      "Epoch 2 | Step 1057400 | Avg Loss: 0.0158 | Grad Norm: 0.00811294\n",
      "Epoch 2 | Step 1057500 | Avg Loss: 0.0156 | Grad Norm: 0.00838463\n",
      "Epoch 2 | Step 1057600 | Avg Loss: 0.0158 | Grad Norm: 0.00835033\n",
      "Epoch 2 | Step 1057700 | Avg Loss: 0.0162 | Grad Norm: 0.00926342\n",
      "Epoch 2 | Step 1057800 | Avg Loss: 0.0158 | Grad Norm: 0.00809842\n",
      "Epoch 2 | Step 1057900 | Avg Loss: 0.0151 | Grad Norm: 0.00898567\n",
      "Epoch 2 | Step 1058000 | Avg Loss: 0.0153 | Grad Norm: 0.00970334\n",
      "Epoch 2 | Step 1058100 | Avg Loss: 0.0157 | Grad Norm: 0.00768388\n",
      "Epoch 2 | Step 1058200 | Avg Loss: 0.0157 | Grad Norm: 0.00795546\n",
      "Epoch 2 | Step 1058300 | Avg Loss: 0.0157 | Grad Norm: 0.01000555\n",
      "Epoch 2 | Step 1058400 | Avg Loss: 0.0152 | Grad Norm: 0.00869307\n",
      "Epoch 2 | Step 1058500 | Avg Loss: 0.0153 | Grad Norm: 0.00864533\n",
      "Epoch 2 | Step 1058600 | Avg Loss: 0.0154 | Grad Norm: 0.00823743\n",
      "Epoch 2 | Step 1058700 | Avg Loss: 0.0157 | Grad Norm: 0.00993868\n",
      "Epoch 2 | Step 1058800 | Avg Loss: 0.0154 | Grad Norm: 0.00946117\n",
      "Epoch 2 | Step 1058900 | Avg Loss: 0.0151 | Grad Norm: 0.01007676\n",
      "Epoch 2 | Step 1059000 | Avg Loss: 0.0153 | Grad Norm: 0.00923775\n",
      "Epoch 2 | Step 1059100 | Avg Loss: 0.0154 | Grad Norm: 0.00967447\n",
      "Epoch 2 | Step 1059200 | Avg Loss: 0.0152 | Grad Norm: 0.01050172\n",
      "Epoch 2 | Step 1059300 | Avg Loss: 0.0155 | Grad Norm: 0.00808117\n",
      "Epoch 2 | Step 1059400 | Avg Loss: 0.0155 | Grad Norm: 0.00865375\n",
      "Epoch 2 | Step 1059500 | Avg Loss: 0.0155 | Grad Norm: 0.01122776\n",
      "Epoch 2 | Step 1059600 | Avg Loss: 0.0157 | Grad Norm: 0.00924917\n",
      "Epoch 2 | Step 1059700 | Avg Loss: 0.0153 | Grad Norm: 0.00742874\n",
      "Epoch 2 | Step 1059800 | Avg Loss: 0.0150 | Grad Norm: 0.00951828\n",
      "Epoch 2 | Step 1059900 | Avg Loss: 0.0154 | Grad Norm: 0.00798194\n",
      "Epoch 2 | Step 1060000 | Avg Loss: 0.0155 | Grad Norm: 0.00944982\n",
      "Epoch 2 | Step 1060100 | Avg Loss: 0.0156 | Grad Norm: 0.00873735\n",
      "Epoch 2 | Step 1060200 | Avg Loss: 0.0156 | Grad Norm: 0.01004474\n",
      "Epoch 2 | Step 1060300 | Avg Loss: 0.0158 | Grad Norm: 0.00775666\n",
      "Epoch 2 | Step 1060400 | Avg Loss: 0.0156 | Grad Norm: 0.00849776\n",
      "Epoch 2 | Step 1060500 | Avg Loss: 0.0157 | Grad Norm: 0.01043543\n",
      "Epoch 2 | Step 1060600 | Avg Loss: 0.0157 | Grad Norm: 0.00924414\n",
      "Epoch 2 | Step 1060700 | Avg Loss: 0.0160 | Grad Norm: 0.00869814\n",
      "Epoch 2 | Step 1060800 | Avg Loss: 0.0162 | Grad Norm: 0.00855380\n",
      "Epoch 2 | Step 1060900 | Avg Loss: 0.0161 | Grad Norm: 0.00846226\n",
      "Epoch 2 | Step 1061000 | Avg Loss: 0.0160 | Grad Norm: 0.00900837\n",
      "Epoch 2 | Step 1061100 | Avg Loss: 0.0159 | Grad Norm: 0.00897547\n",
      "Epoch 2 | Step 1061200 | Avg Loss: 0.0163 | Grad Norm: 0.00970430\n",
      "Epoch 2 | Step 1061300 | Avg Loss: 0.0160 | Grad Norm: 0.01019783\n",
      "Epoch 2 | Step 1061400 | Avg Loss: 0.0158 | Grad Norm: 0.00980689\n",
      "Epoch 2 | Step 1061500 | Avg Loss: 0.0154 | Grad Norm: 0.00953913\n",
      "Epoch 2 | Step 1061600 | Avg Loss: 0.0156 | Grad Norm: 0.01050743\n",
      "Epoch 2 | Step 1061700 | Avg Loss: 0.0157 | Grad Norm: 0.00937352\n",
      "Epoch 2 | Step 1061800 | Avg Loss: 0.0159 | Grad Norm: 0.01020424\n",
      "Epoch 2 | Step 1061900 | Avg Loss: 0.0158 | Grad Norm: 0.00844112\n",
      "Epoch 2 | Step 1062000 | Avg Loss: 0.0154 | Grad Norm: 0.00785527\n",
      "Epoch 2 | Step 1062100 | Avg Loss: 0.0155 | Grad Norm: 0.00877702\n",
      "Epoch 2 | Step 1062200 | Avg Loss: 0.0157 | Grad Norm: 0.00893219\n",
      "Epoch 2 | Step 1062300 | Avg Loss: 0.0156 | Grad Norm: 0.00946846\n",
      "Epoch 2 | Step 1062400 | Avg Loss: 0.0159 | Grad Norm: 0.00956059\n",
      "Epoch 2 | Step 1062500 | Avg Loss: 0.0160 | Grad Norm: 0.00913256\n",
      "Epoch 2 | Step 1062600 | Avg Loss: 0.0160 | Grad Norm: 0.01000591\n",
      "Epoch 2 | Step 1062700 | Avg Loss: 0.0162 | Grad Norm: 0.01041035\n",
      "Epoch 2 | Step 1062800 | Avg Loss: 0.0161 | Grad Norm: 0.00847725\n",
      "Epoch 2 | Step 1062900 | Avg Loss: 0.0158 | Grad Norm: 0.00910810\n",
      "Epoch 2 | Step 1063000 | Avg Loss: 0.0156 | Grad Norm: 0.00792058\n",
      "Epoch 2 | Step 1063100 | Avg Loss: 0.0157 | Grad Norm: 0.00806207\n",
      "Epoch 2 | Step 1063200 | Avg Loss: 0.0159 | Grad Norm: 0.00800105\n",
      "Epoch 2 | Step 1063300 | Avg Loss: 0.0159 | Grad Norm: 0.00824424\n",
      "Epoch 2 | Step 1063400 | Avg Loss: 0.0155 | Grad Norm: 0.00770329\n",
      "Epoch 2 | Step 1063500 | Avg Loss: 0.0159 | Grad Norm: 0.00957089\n",
      "Epoch 2 | Step 1063600 | Avg Loss: 0.0157 | Grad Norm: 0.00886029\n",
      "Epoch 2 | Step 1063700 | Avg Loss: 0.0155 | Grad Norm: 0.00823783\n",
      "Epoch 2 | Step 1063800 | Avg Loss: 0.0155 | Grad Norm: 0.00830715\n",
      "Epoch 2 | Step 1063900 | Avg Loss: 0.0154 | Grad Norm: 0.00857813\n",
      "Epoch 2 | Step 1064000 | Avg Loss: 0.0155 | Grad Norm: 0.00805226\n",
      "Epoch 2 | Step 1064100 | Avg Loss: 0.0153 | Grad Norm: 0.00741842\n",
      "Epoch 2 | Step 1064200 | Avg Loss: 0.0159 | Grad Norm: 0.00862934\n",
      "Epoch 2 | Step 1064300 | Avg Loss: 0.0157 | Grad Norm: 0.00913705\n",
      "Epoch 2 | Step 1064400 | Avg Loss: 0.0156 | Grad Norm: 0.00920119\n",
      "Epoch 2 | Step 1064500 | Avg Loss: 0.0156 | Grad Norm: 0.00938410\n",
      "Epoch 2 | Step 1064600 | Avg Loss: 0.0158 | Grad Norm: 0.01028164\n",
      "Epoch 2 | Step 1064700 | Avg Loss: 0.0156 | Grad Norm: 0.00763396\n",
      "Epoch 2 | Step 1064800 | Avg Loss: 0.0157 | Grad Norm: 0.00978611\n",
      "Epoch 2 | Step 1064900 | Avg Loss: 0.0154 | Grad Norm: 0.00913193\n",
      "Epoch 2 | Step 1065000 | Avg Loss: 0.0155 | Grad Norm: 0.00907491\n",
      "Epoch 2 | Step 1065100 | Avg Loss: 0.0156 | Grad Norm: 0.00960064\n",
      "Epoch 2 | Step 1065200 | Avg Loss: 0.0155 | Grad Norm: 0.00828518\n",
      "Epoch 2 | Step 1065300 | Avg Loss: 0.0153 | Grad Norm: 0.00949048\n",
      "Epoch 2 | Step 1065400 | Avg Loss: 0.0155 | Grad Norm: 0.00915845\n",
      "Epoch 2 | Step 1065500 | Avg Loss: 0.0154 | Grad Norm: 0.00904257\n",
      "Epoch 2 | Step 1065600 | Avg Loss: 0.0153 | Grad Norm: 0.00695141\n",
      "Epoch 2 | Step 1065700 | Avg Loss: 0.0154 | Grad Norm: 0.00878713\n",
      "Epoch 2 | Step 1065800 | Avg Loss: 0.0156 | Grad Norm: 0.00828079\n",
      "Epoch 2 | Step 1065900 | Avg Loss: 0.0160 | Grad Norm: 0.00899265\n",
      "Epoch 2 | Step 1066000 | Avg Loss: 0.0160 | Grad Norm: 0.00929561\n",
      "Epoch 2 | Step 1066100 | Avg Loss: 0.0158 | Grad Norm: 0.00953263\n",
      "Epoch 2 | Step 1066200 | Avg Loss: 0.0162 | Grad Norm: 0.00934341\n",
      "Epoch 2 | Step 1066300 | Avg Loss: 0.0163 | Grad Norm: 0.00801357\n",
      "Epoch 2 | Step 1066400 | Avg Loss: 0.0161 | Grad Norm: 0.00970778\n",
      "Epoch 2 | Step 1066500 | Avg Loss: 0.0159 | Grad Norm: 0.00992480\n",
      "Epoch 2 | Step 1066600 | Avg Loss: 0.0155 | Grad Norm: 0.00937203\n",
      "Epoch 2 | Step 1066700 | Avg Loss: 0.0155 | Grad Norm: 0.01004423\n",
      "Epoch 2 | Step 1066800 | Avg Loss: 0.0151 | Grad Norm: 0.00959317\n",
      "Epoch 2 | Step 1066900 | Avg Loss: 0.0152 | Grad Norm: 0.00853201\n",
      "Epoch 2 | Step 1067000 | Avg Loss: 0.0149 | Grad Norm: 0.00865737\n",
      "Epoch 2 | Step 1067100 | Avg Loss: 0.0152 | Grad Norm: 0.00819014\n",
      "Epoch 2 | Step 1067200 | Avg Loss: 0.0148 | Grad Norm: 0.00977555\n",
      "Epoch 2 | Step 1067300 | Avg Loss: 0.0153 | Grad Norm: 0.00985524\n",
      "Epoch 2 | Step 1067400 | Avg Loss: 0.0155 | Grad Norm: 0.00795416\n",
      "Epoch 2 | Step 1067500 | Avg Loss: 0.0153 | Grad Norm: 0.01025242\n",
      "Epoch 2 | Step 1067600 | Avg Loss: 0.0152 | Grad Norm: 0.00845543\n",
      "Epoch 2 | Step 1067700 | Avg Loss: 0.0154 | Grad Norm: 0.00893272\n",
      "Epoch 2 | Step 1067800 | Avg Loss: 0.0154 | Grad Norm: 0.00949384\n",
      "Epoch 2 | Step 1067900 | Avg Loss: 0.0153 | Grad Norm: 0.00862624\n",
      "Epoch 2 | Step 1068000 | Avg Loss: 0.0156 | Grad Norm: 0.00929460\n",
      "Epoch 2 | Step 1068100 | Avg Loss: 0.0159 | Grad Norm: 0.00931814\n",
      "Epoch 2 | Step 1068200 | Avg Loss: 0.0163 | Grad Norm: 0.00944167\n",
      "Epoch 2 | Step 1068300 | Avg Loss: 0.0160 | Grad Norm: 0.00802663\n",
      "Epoch 2 | Step 1068400 | Avg Loss: 0.0161 | Grad Norm: 0.00809386\n",
      "Epoch 2 | Step 1068500 | Avg Loss: 0.0165 | Grad Norm: 0.01023952\n",
      "Epoch 2 | Step 1068600 | Avg Loss: 0.0160 | Grad Norm: 0.00857074\n",
      "Epoch 2 | Step 1068700 | Avg Loss: 0.0160 | Grad Norm: 0.00859338\n",
      "Epoch 2 | Step 1068800 | Avg Loss: 0.0163 | Grad Norm: 0.00865324\n",
      "Epoch 2 | Step 1068900 | Avg Loss: 0.0162 | Grad Norm: 0.00834461\n",
      "Epoch 2 | Step 1069000 | Avg Loss: 0.0159 | Grad Norm: 0.00955076\n",
      "Epoch 2 | Step 1069100 | Avg Loss: 0.0158 | Grad Norm: 0.00855314\n",
      "Epoch 2 | Step 1069200 | Avg Loss: 0.0158 | Grad Norm: 0.00814398\n",
      "Epoch 2 | Step 1069300 | Avg Loss: 0.0153 | Grad Norm: 0.00853800\n",
      "Epoch 2 | Step 1069400 | Avg Loss: 0.0153 | Grad Norm: 0.00919895\n",
      "Epoch 2 | Step 1069500 | Avg Loss: 0.0153 | Grad Norm: 0.01158293\n",
      "Epoch 2 | Step 1069600 | Avg Loss: 0.0154 | Grad Norm: 0.00887523\n",
      "Epoch 2 | Step 1069700 | Avg Loss: 0.0155 | Grad Norm: 0.00847237\n",
      "Epoch 2 | Step 1069800 | Avg Loss: 0.0155 | Grad Norm: 0.00844900\n",
      "Epoch 2 | Step 1069900 | Avg Loss: 0.0157 | Grad Norm: 0.00789632\n",
      "Epoch 2 | Step 1070000 | Avg Loss: 0.0160 | Grad Norm: 0.00733154\n",
      "Epoch 2 | Step 1070100 | Avg Loss: 0.0156 | Grad Norm: 0.00956717\n",
      "Epoch 2 | Step 1070200 | Avg Loss: 0.0157 | Grad Norm: 0.00867938\n",
      "Epoch 2 | Step 1070300 | Avg Loss: 0.0161 | Grad Norm: 0.00856326\n",
      "Epoch 2 | Step 1070400 | Avg Loss: 0.0164 | Grad Norm: 0.00994262\n",
      "Epoch 2 | Step 1070500 | Avg Loss: 0.0161 | Grad Norm: 0.00954845\n",
      "Epoch 2 | Step 1070600 | Avg Loss: 0.0156 | Grad Norm: 0.00832046\n",
      "Epoch 2 | Step 1070700 | Avg Loss: 0.0154 | Grad Norm: 0.00837186\n",
      "Epoch 2 | Step 1070800 | Avg Loss: 0.0154 | Grad Norm: 0.00812237\n",
      "Epoch 2 | Step 1070900 | Avg Loss: 0.0155 | Grad Norm: 0.00798863\n",
      "Epoch 2 | Step 1071000 | Avg Loss: 0.0157 | Grad Norm: 0.01021413\n",
      "Epoch 2 | Step 1071100 | Avg Loss: 0.0158 | Grad Norm: 0.00937130\n",
      "Epoch 2 | Step 1071200 | Avg Loss: 0.0158 | Grad Norm: 0.00879308\n",
      "Epoch 2 | Step 1071300 | Avg Loss: 0.0158 | Grad Norm: 0.00893727\n",
      "Epoch 2 | Step 1071400 | Avg Loss: 0.0161 | Grad Norm: 0.00966719\n",
      "Epoch 2 | Step 1071500 | Avg Loss: 0.0157 | Grad Norm: 0.00841468\n",
      "Epoch 2 | Step 1071600 | Avg Loss: 0.0153 | Grad Norm: 0.00964369\n",
      "Epoch 2 | Step 1071700 | Avg Loss: 0.0151 | Grad Norm: 0.01100516\n",
      "Epoch 2 | Step 1071800 | Avg Loss: 0.0152 | Grad Norm: 0.00965744\n",
      "Epoch 2 | Step 1071900 | Avg Loss: 0.0152 | Grad Norm: 0.00929716\n",
      "Epoch 2 | Step 1072000 | Avg Loss: 0.0152 | Grad Norm: 0.00811012\n",
      "Epoch 2 | Step 1072100 | Avg Loss: 0.0152 | Grad Norm: 0.00782867\n",
      "Epoch 2 | Step 1072200 | Avg Loss: 0.0154 | Grad Norm: 0.00874107\n",
      "Epoch 2 | Step 1072300 | Avg Loss: 0.0155 | Grad Norm: 0.00847563\n",
      "Epoch 2 | Step 1072400 | Avg Loss: 0.0155 | Grad Norm: 0.00891673\n",
      "Epoch 2 | Step 1072500 | Avg Loss: 0.0160 | Grad Norm: 0.00805119\n",
      "Epoch 2 | Step 1072600 | Avg Loss: 0.0159 | Grad Norm: 0.01144423\n",
      "Epoch 2 | Step 1072700 | Avg Loss: 0.0160 | Grad Norm: 0.00943348\n",
      "Epoch 2 | Step 1072800 | Avg Loss: 0.0157 | Grad Norm: 0.00880201\n",
      "Epoch 2 | Step 1072900 | Avg Loss: 0.0156 | Grad Norm: 0.00829640\n",
      "Epoch 2 | Step 1073000 | Avg Loss: 0.0161 | Grad Norm: 0.00862328\n",
      "Epoch 2 | Step 1073100 | Avg Loss: 0.0161 | Grad Norm: 0.00974512\n",
      "Epoch 2 | Step 1073200 | Avg Loss: 0.0161 | Grad Norm: 0.00886256\n",
      "Epoch 2 | Step 1073300 | Avg Loss: 0.0162 | Grad Norm: 0.00932331\n",
      "Epoch 2 | Step 1073400 | Avg Loss: 0.0161 | Grad Norm: 0.01237455\n",
      "Epoch 2 | Step 1073500 | Avg Loss: 0.0157 | Grad Norm: 0.00914500\n",
      "Epoch 2 | Step 1073600 | Avg Loss: 0.0155 | Grad Norm: 0.00911459\n",
      "Epoch 2 | Step 1073700 | Avg Loss: 0.0154 | Grad Norm: 0.00805934\n",
      "Epoch 2 | Step 1073800 | Avg Loss: 0.0153 | Grad Norm: 0.01356580\n",
      "Epoch 2 | Step 1073900 | Avg Loss: 0.0152 | Grad Norm: 0.00965056\n",
      "Epoch 2 | Step 1074000 | Avg Loss: 0.0155 | Grad Norm: 0.01022262\n",
      "Epoch 2 | Step 1074100 | Avg Loss: 0.0159 | Grad Norm: 0.01050458\n",
      "Epoch 2 | Step 1074200 | Avg Loss: 0.0160 | Grad Norm: 0.01033704\n",
      "Epoch 2 | Step 1074300 | Avg Loss: 0.0159 | Grad Norm: 0.00888250\n",
      "Epoch 2 | Step 1074400 | Avg Loss: 0.0162 | Grad Norm: 0.00793428\n",
      "Epoch 2 | Step 1074500 | Avg Loss: 0.0161 | Grad Norm: 0.00808441\n",
      "Epoch 2 | Step 1074600 | Avg Loss: 0.0159 | Grad Norm: 0.00940654\n",
      "Epoch 2 | Step 1074700 | Avg Loss: 0.0158 | Grad Norm: 0.01102000\n",
      "Epoch 2 | Step 1074800 | Avg Loss: 0.0158 | Grad Norm: 0.00758432\n",
      "Epoch 2 | Step 1074900 | Avg Loss: 0.0154 | Grad Norm: 0.00824535\n",
      "Epoch 2 | Step 1075000 | Avg Loss: 0.0156 | Grad Norm: 0.00886420\n",
      "Epoch 2 | Step 1075100 | Avg Loss: 0.0156 | Grad Norm: 0.00872639\n",
      "Epoch 2 | Step 1075200 | Avg Loss: 0.0153 | Grad Norm: 0.00913462\n",
      "Epoch 2 | Step 1075300 | Avg Loss: 0.0151 | Grad Norm: 0.00847991\n",
      "Epoch 2 | Step 1075400 | Avg Loss: 0.0152 | Grad Norm: 0.00989701\n",
      "Epoch 2 | Step 1075500 | Avg Loss: 0.0156 | Grad Norm: 0.00873838\n",
      "Epoch 2 | Step 1075600 | Avg Loss: 0.0155 | Grad Norm: 0.00808918\n",
      "Epoch 2 | Step 1075700 | Avg Loss: 0.0155 | Grad Norm: 0.00920174\n",
      "Epoch 2 | Step 1075800 | Avg Loss: 0.0158 | Grad Norm: 0.00851888\n",
      "Epoch 2 | Step 1075900 | Avg Loss: 0.0162 | Grad Norm: 0.01048160\n",
      "Epoch 2 | Step 1076000 | Avg Loss: 0.0163 | Grad Norm: 0.01213687\n",
      "Epoch 2 | Step 1076100 | Avg Loss: 0.0163 | Grad Norm: 0.00906818\n",
      "Epoch 2 | Step 1076200 | Avg Loss: 0.0160 | Grad Norm: 0.00991458\n",
      "Epoch 2 | Step 1076300 | Avg Loss: 0.0157 | Grad Norm: 0.00799367\n",
      "Epoch 2 | Step 1076400 | Avg Loss: 0.0158 | Grad Norm: 0.00819907\n",
      "Epoch 2 | Step 1076500 | Avg Loss: 0.0154 | Grad Norm: 0.01017595\n",
      "Epoch 2 | Step 1076600 | Avg Loss: 0.0158 | Grad Norm: 0.01201626\n",
      "Epoch 2 | Step 1076700 | Avg Loss: 0.0157 | Grad Norm: 0.00787059\n",
      "Epoch 2 | Step 1076800 | Avg Loss: 0.0157 | Grad Norm: 0.00894288\n",
      "Epoch 2 | Step 1076900 | Avg Loss: 0.0156 | Grad Norm: 0.00736242\n",
      "Epoch 2 | Step 1077000 | Avg Loss: 0.0155 | Grad Norm: 0.01089174\n",
      "Epoch 2 | Step 1077100 | Avg Loss: 0.0156 | Grad Norm: 0.00808076\n",
      "Epoch 2 | Step 1077200 | Avg Loss: 0.0159 | Grad Norm: 0.00807095\n",
      "Epoch 2 | Step 1077300 | Avg Loss: 0.0159 | Grad Norm: 0.01279686\n",
      "Epoch 2 | Step 1077400 | Avg Loss: 0.0161 | Grad Norm: 0.01034796\n",
      "Epoch 2 | Step 1077500 | Avg Loss: 0.0163 | Grad Norm: 0.00925664\n",
      "Epoch 2 | Step 1077600 | Avg Loss: 0.0166 | Grad Norm: 0.01398240\n",
      "Epoch 2 | Step 1077700 | Avg Loss: 0.0157 | Grad Norm: 0.00841726\n",
      "Epoch 2 | Step 1077800 | Avg Loss: 0.0156 | Grad Norm: 0.00953293\n",
      "Epoch 2 | Step 1077900 | Avg Loss: 0.0157 | Grad Norm: 0.00974120\n",
      "Epoch 2 | Step 1078000 | Avg Loss: 0.0156 | Grad Norm: 0.00838477\n",
      "Epoch 2 | Step 1078100 | Avg Loss: 0.0156 | Grad Norm: 0.00883597\n",
      "Epoch 2 | Step 1078200 | Avg Loss: 0.0155 | Grad Norm: 0.00953331\n",
      "Epoch 2 | Step 1078300 | Avg Loss: 0.0162 | Grad Norm: 0.01010054\n",
      "Epoch 2 | Step 1078400 | Avg Loss: 0.0161 | Grad Norm: 0.00879537\n",
      "Epoch 2 | Step 1078500 | Avg Loss: 0.0159 | Grad Norm: 0.00811101\n",
      "Epoch 2 | Step 1078600 | Avg Loss: 0.0159 | Grad Norm: 0.00892472\n",
      "Epoch 2 | Step 1078700 | Avg Loss: 0.0163 | Grad Norm: 0.00924322\n",
      "Epoch 2 | Step 1078800 | Avg Loss: 0.0163 | Grad Norm: 0.00961153\n",
      "Epoch 2 | Step 1078900 | Avg Loss: 0.0165 | Grad Norm: 0.01286704\n",
      "Epoch 2 | Step 1079000 | Avg Loss: 0.0166 | Grad Norm: 0.00993829\n",
      "Epoch 2 | Step 1079100 | Avg Loss: 0.0168 | Grad Norm: 0.00938207\n",
      "Epoch 2 | Step 1079200 | Avg Loss: 0.0165 | Grad Norm: 0.00888189\n",
      "Epoch 2 | Step 1079300 | Avg Loss: 0.0161 | Grad Norm: 0.00901426\n",
      "Epoch 2 | Step 1079400 | Avg Loss: 0.0159 | Grad Norm: 0.00780382\n",
      "Epoch 2 | Step 1079500 | Avg Loss: 0.0156 | Grad Norm: 0.00897883\n",
      "Epoch 2 | Step 1079600 | Avg Loss: 0.0159 | Grad Norm: 0.00853278\n",
      "Epoch 2 | Step 1079700 | Avg Loss: 0.0156 | Grad Norm: 0.00892902\n",
      "Epoch 2 | Step 1079800 | Avg Loss: 0.0156 | Grad Norm: 0.01265601\n",
      "Epoch 2 | Step 1079900 | Avg Loss: 0.0158 | Grad Norm: 0.00969940\n",
      "Epoch 2 | Step 1080000 | Avg Loss: 0.0156 | Grad Norm: 0.00792565\n",
      "Epoch 2 | Step 1080100 | Avg Loss: 0.0160 | Grad Norm: 0.00819726\n",
      "Epoch 2 | Step 1080200 | Avg Loss: 0.0153 | Grad Norm: 0.00807721\n",
      "Epoch 2 | Step 1080300 | Avg Loss: 0.0151 | Grad Norm: 0.01009914\n",
      "Epoch 2 | Step 1080400 | Avg Loss: 0.0155 | Grad Norm: 0.00940181\n",
      "Epoch 2 | Step 1080500 | Avg Loss: 0.0153 | Grad Norm: 0.01009382\n",
      "Epoch 2 | Step 1080600 | Avg Loss: 0.0154 | Grad Norm: 0.00815010\n",
      "Epoch 2 | Step 1080700 | Avg Loss: 0.0156 | Grad Norm: 0.01122960\n",
      "Epoch 2 | Step 1080800 | Avg Loss: 0.0155 | Grad Norm: 0.00812627\n",
      "Epoch 2 | Step 1080900 | Avg Loss: 0.0154 | Grad Norm: 0.00949680\n",
      "Epoch 2 | Step 1081000 | Avg Loss: 0.0154 | Grad Norm: 0.00855824\n",
      "Epoch 2 | Step 1081100 | Avg Loss: 0.0155 | Grad Norm: 0.00844182\n",
      "Epoch 2 | Step 1081200 | Avg Loss: 0.0151 | Grad Norm: 0.00852248\n",
      "Epoch 2 | Step 1081300 | Avg Loss: 0.0152 | Grad Norm: 0.00939901\n",
      "Epoch 2 | Step 1081400 | Avg Loss: 0.0155 | Grad Norm: 0.00933571\n",
      "Epoch 2 | Step 1081500 | Avg Loss: 0.0155 | Grad Norm: 0.00927657\n",
      "Epoch 2 | Step 1081600 | Avg Loss: 0.0149 | Grad Norm: 0.00693733\n",
      "Epoch 2 | Step 1081700 | Avg Loss: 0.0149 | Grad Norm: 0.01011984\n",
      "Epoch 2 | Step 1081800 | Avg Loss: 0.0148 | Grad Norm: 0.00919088\n",
      "Epoch 2 | Step 1081900 | Avg Loss: 0.0149 | Grad Norm: 0.00937969\n",
      "Epoch 2 | Step 1082000 | Avg Loss: 0.0149 | Grad Norm: 0.00919038\n",
      "Epoch 2 | Step 1082100 | Avg Loss: 0.0150 | Grad Norm: 0.00827993\n",
      "Epoch 2 | Step 1082200 | Avg Loss: 0.0151 | Grad Norm: 0.00902173\n",
      "Epoch 2 | Step 1082300 | Avg Loss: 0.0152 | Grad Norm: 0.00853873\n",
      "Epoch 2 | Step 1082400 | Avg Loss: 0.0151 | Grad Norm: 0.00926929\n",
      "Epoch 2 | Step 1082500 | Avg Loss: 0.0155 | Grad Norm: 0.00904293\n",
      "Epoch 2 | Step 1082600 | Avg Loss: 0.0154 | Grad Norm: 0.00879846\n",
      "Epoch 2 | Step 1082700 | Avg Loss: 0.0155 | Grad Norm: 0.00856907\n",
      "Epoch 2 | Step 1082800 | Avg Loss: 0.0154 | Grad Norm: 0.01002582\n",
      "Epoch 2 | Step 1082900 | Avg Loss: 0.0155 | Grad Norm: 0.00823930\n",
      "Epoch 2 | Step 1083000 | Avg Loss: 0.0158 | Grad Norm: 0.00911655\n",
      "Epoch 2 | Step 1083100 | Avg Loss: 0.0156 | Grad Norm: 0.00833122\n",
      "Epoch 2 | Step 1083200 | Avg Loss: 0.0156 | Grad Norm: 0.01080451\n",
      "Epoch 2 | Step 1083300 | Avg Loss: 0.0153 | Grad Norm: 0.00915728\n",
      "Epoch 2 | Step 1083400 | Avg Loss: 0.0153 | Grad Norm: 0.00852695\n",
      "Epoch 2 | Step 1083500 | Avg Loss: 0.0151 | Grad Norm: 0.00838564\n",
      "Epoch 2 | Step 1083600 | Avg Loss: 0.0152 | Grad Norm: 0.00827221\n",
      "Epoch 2 | Step 1083700 | Avg Loss: 0.0150 | Grad Norm: 0.00835532\n",
      "Epoch 2 | Step 1083800 | Avg Loss: 0.0150 | Grad Norm: 0.00803756\n",
      "Epoch 2 | Step 1083900 | Avg Loss: 0.0152 | Grad Norm: 0.00857903\n",
      "Epoch 2 | Step 1084000 | Avg Loss: 0.0153 | Grad Norm: 0.00876037\n",
      "Epoch 2 | Step 1084100 | Avg Loss: 0.0153 | Grad Norm: 0.00940238\n",
      "Epoch 2 | Step 1084200 | Avg Loss: 0.0154 | Grad Norm: 0.00881285\n",
      "Epoch 2 | Step 1084300 | Avg Loss: 0.0155 | Grad Norm: 0.00976559\n",
      "Epoch 2 | Step 1084400 | Avg Loss: 0.0156 | Grad Norm: 0.00938638\n",
      "Epoch 2 | Step 1084500 | Avg Loss: 0.0156 | Grad Norm: 0.00966259\n",
      "Epoch 2 | Step 1084600 | Avg Loss: 0.0156 | Grad Norm: 0.00803394\n",
      "Epoch 2 | Step 1084700 | Avg Loss: 0.0154 | Grad Norm: 0.00821528\n",
      "Epoch 2 | Step 1084800 | Avg Loss: 0.0154 | Grad Norm: 0.00988761\n",
      "Epoch 2 | Step 1084900 | Avg Loss: 0.0153 | Grad Norm: 0.00874056\n",
      "Epoch 2 | Step 1085000 | Avg Loss: 0.0154 | Grad Norm: 0.00872080\n",
      "Epoch 2 | Step 1085100 | Avg Loss: 0.0153 | Grad Norm: 0.00784281\n",
      "Epoch 2 | Step 1085200 | Avg Loss: 0.0153 | Grad Norm: 0.01139172\n",
      "Epoch 2 | Step 1085300 | Avg Loss: 0.0157 | Grad Norm: 0.01085513\n",
      "Epoch 2 | Step 1085400 | Avg Loss: 0.0157 | Grad Norm: 0.01226780\n",
      "Epoch 2 | Step 1085500 | Avg Loss: 0.0156 | Grad Norm: 0.01169565\n",
      "Epoch 2 | Step 1085600 | Avg Loss: 0.0154 | Grad Norm: 0.00950969\n",
      "Epoch 2 | Step 1085700 | Avg Loss: 0.0154 | Grad Norm: 0.00899552\n",
      "Epoch 2 | Step 1085800 | Avg Loss: 0.0155 | Grad Norm: 0.01006488\n",
      "Epoch 2 | Step 1085900 | Avg Loss: 0.0155 | Grad Norm: 0.00780679\n",
      "Epoch 2 | Step 1086000 | Avg Loss: 0.0155 | Grad Norm: 0.00832050\n",
      "Epoch 2 | Step 1086100 | Avg Loss: 0.0152 | Grad Norm: 0.00795670\n",
      "Epoch 2 | Step 1086200 | Avg Loss: 0.0154 | Grad Norm: 0.00997315\n",
      "Epoch 2 | Step 1086300 | Avg Loss: 0.0153 | Grad Norm: 0.00964908\n",
      "Epoch 2 | Step 1086400 | Avg Loss: 0.0155 | Grad Norm: 0.00903255\n",
      "Epoch 2 | Step 1086500 | Avg Loss: 0.0154 | Grad Norm: 0.01000326\n",
      "Epoch 2 | Step 1086600 | Avg Loss: 0.0155 | Grad Norm: 0.00920385\n",
      "Epoch 2 | Step 1086700 | Avg Loss: 0.0152 | Grad Norm: 0.00900497\n",
      "Epoch 2 | Step 1086800 | Avg Loss: 0.0155 | Grad Norm: 0.00955403\n",
      "Epoch 2 | Step 1086900 | Avg Loss: 0.0154 | Grad Norm: 0.00909516\n",
      "Epoch 2 | Step 1087000 | Avg Loss: 0.0155 | Grad Norm: 0.00959207\n",
      "Epoch 2 | Step 1087100 | Avg Loss: 0.0156 | Grad Norm: 0.00908785\n",
      "Epoch 2 | Step 1087200 | Avg Loss: 0.0157 | Grad Norm: 0.00831126\n",
      "Epoch 2 | Step 1087300 | Avg Loss: 0.0155 | Grad Norm: 0.00916224\n",
      "Epoch 2 | Step 1087400 | Avg Loss: 0.0151 | Grad Norm: 0.00861655\n",
      "Epoch 2 | Step 1087500 | Avg Loss: 0.0151 | Grad Norm: 0.01164543\n",
      "Epoch 2 | Step 1087600 | Avg Loss: 0.0153 | Grad Norm: 0.00943497\n",
      "Epoch 2 | Step 1087700 | Avg Loss: 0.0156 | Grad Norm: 0.00943090\n",
      "Epoch 2 | Step 1087800 | Avg Loss: 0.0156 | Grad Norm: 0.00941623\n",
      "Epoch 2 | Step 1087900 | Avg Loss: 0.0157 | Grad Norm: 0.00930164\n",
      "Epoch 2 | Step 1088000 | Avg Loss: 0.0155 | Grad Norm: 0.00846891\n",
      "Epoch 2 | Step 1088100 | Avg Loss: 0.0153 | Grad Norm: 0.00957578\n",
      "Epoch 2 | Step 1088200 | Avg Loss: 0.0157 | Grad Norm: 0.00788254\n",
      "Epoch 2 | Step 1088300 | Avg Loss: 0.0157 | Grad Norm: 0.00931621\n",
      "Epoch 2 | Step 1088400 | Avg Loss: 0.0159 | Grad Norm: 0.00868391\n",
      "Epoch 2 | Step 1088500 | Avg Loss: 0.0154 | Grad Norm: 0.00817989\n",
      "Epoch 2 | Step 1088600 | Avg Loss: 0.0156 | Grad Norm: 0.00766500\n",
      "Epoch 2 | Step 1088700 | Avg Loss: 0.0159 | Grad Norm: 0.00819977\n",
      "Epoch 2 | Step 1088800 | Avg Loss: 0.0157 | Grad Norm: 0.00863145\n",
      "Epoch 2 | Step 1088900 | Avg Loss: 0.0154 | Grad Norm: 0.00887897\n",
      "Epoch 2 | Step 1089000 | Avg Loss: 0.0156 | Grad Norm: 0.00832215\n",
      "Epoch 2 | Step 1089100 | Avg Loss: 0.0155 | Grad Norm: 0.00921815\n",
      "Epoch 2 | Step 1089200 | Avg Loss: 0.0156 | Grad Norm: 0.00986250\n",
      "Epoch 2 | Step 1089300 | Avg Loss: 0.0162 | Grad Norm: 0.00978477\n",
      "Epoch 2 | Step 1089400 | Avg Loss: 0.0156 | Grad Norm: 0.00777558\n",
      "Epoch 2 | Step 1089500 | Avg Loss: 0.0153 | Grad Norm: 0.01243791\n",
      "Epoch 2 | Step 1089600 | Avg Loss: 0.0156 | Grad Norm: 0.00826989\n",
      "Epoch 2 | Step 1089700 | Avg Loss: 0.0158 | Grad Norm: 0.00834326\n",
      "Epoch 2 | Step 1089800 | Avg Loss: 0.0156 | Grad Norm: 0.01024879\n",
      "Epoch 2 | Step 1089900 | Avg Loss: 0.0152 | Grad Norm: 0.00920297\n",
      "Epoch 2 | Step 1090000 | Avg Loss: 0.0152 | Grad Norm: 0.01005888\n",
      "Epoch 2 | Step 1090100 | Avg Loss: 0.0155 | Grad Norm: 0.00864379\n",
      "Epoch 2 | Step 1090200 | Avg Loss: 0.0156 | Grad Norm: 0.00766055\n",
      "Epoch 2 | Step 1090300 | Avg Loss: 0.0155 | Grad Norm: 0.00940407\n",
      "Epoch 2 | Step 1090400 | Avg Loss: 0.0152 | Grad Norm: 0.00845874\n",
      "Epoch 2 | Step 1090500 | Avg Loss: 0.0153 | Grad Norm: 0.00857083\n",
      "Epoch 2 | Step 1090600 | Avg Loss: 0.0155 | Grad Norm: 0.00958490\n",
      "Epoch 2 | Step 1090700 | Avg Loss: 0.0152 | Grad Norm: 0.00873024\n",
      "Epoch 2 | Step 1090800 | Avg Loss: 0.0150 | Grad Norm: 0.00887604\n",
      "Epoch 2 | Step 1090900 | Avg Loss: 0.0152 | Grad Norm: 0.00897128\n",
      "Epoch 2 | Step 1091000 | Avg Loss: 0.0154 | Grad Norm: 0.00991954\n",
      "Epoch 2 | Step 1091100 | Avg Loss: 0.0150 | Grad Norm: 0.00998027\n",
      "Epoch 2 | Step 1091200 | Avg Loss: 0.0157 | Grad Norm: 0.01136854\n",
      "Epoch 2 | Step 1091300 | Avg Loss: 0.0155 | Grad Norm: 0.00861524\n",
      "Epoch 2 | Step 1091400 | Avg Loss: 0.0156 | Grad Norm: 0.00801197\n",
      "Epoch 2 | Step 1091500 | Avg Loss: 0.0160 | Grad Norm: 0.00901571\n",
      "Epoch 2 | Step 1091600 | Avg Loss: 0.0156 | Grad Norm: 0.00787702\n",
      "Epoch 2 | Step 1091700 | Avg Loss: 0.0155 | Grad Norm: 0.00976725\n",
      "Epoch 2 | Step 1091800 | Avg Loss: 0.0153 | Grad Norm: 0.00821197\n",
      "Epoch 2 | Step 1091900 | Avg Loss: 0.0153 | Grad Norm: 0.00956671\n",
      "Epoch 2 | Step 1092000 | Avg Loss: 0.0153 | Grad Norm: 0.00882392\n",
      "Epoch 2 | Step 1092100 | Avg Loss: 0.0155 | Grad Norm: 0.00797272\n",
      "Epoch 2 | Step 1092200 | Avg Loss: 0.0154 | Grad Norm: 0.00870327\n",
      "Epoch 2 | Step 1092300 | Avg Loss: 0.0155 | Grad Norm: 0.01374140\n",
      "Epoch 2 | Step 1092400 | Avg Loss: 0.0155 | Grad Norm: 0.00967356\n",
      "Epoch 2 | Step 1092500 | Avg Loss: 0.0160 | Grad Norm: 0.01042611\n",
      "Epoch 2 | Step 1092600 | Avg Loss: 0.0161 | Grad Norm: 0.00884226\n",
      "Epoch 2 | Step 1092700 | Avg Loss: 0.0156 | Grad Norm: 0.00985383\n",
      "Epoch 2 | Step 1092800 | Avg Loss: 0.0156 | Grad Norm: 0.00774012\n",
      "Epoch 2 | Step 1092900 | Avg Loss: 0.0157 | Grad Norm: 0.00951914\n",
      "Epoch 2 | Step 1093000 | Avg Loss: 0.0154 | Grad Norm: 0.00782460\n",
      "Epoch 2 | Step 1093100 | Avg Loss: 0.0155 | Grad Norm: 0.00954873\n",
      "Epoch 2 | Step 1093200 | Avg Loss: 0.0155 | Grad Norm: 0.00883587\n",
      "Epoch 2 | Step 1093300 | Avg Loss: 0.0155 | Grad Norm: 0.01032189\n",
      "Epoch 2 | Step 1093400 | Avg Loss: 0.0160 | Grad Norm: 0.00888151\n",
      "Epoch 2 | Step 1093500 | Avg Loss: 0.0157 | Grad Norm: 0.00783630\n",
      "Epoch 2 | Step 1093600 | Avg Loss: 0.0158 | Grad Norm: 0.00970069\n",
      "Epoch 2 | Step 1093700 | Avg Loss: 0.0158 | Grad Norm: 0.00942263\n",
      "Epoch 2 | Step 1093800 | Avg Loss: 0.0160 | Grad Norm: 0.00852676\n",
      "Epoch 2 | Step 1093900 | Avg Loss: 0.0161 | Grad Norm: 0.00954081\n",
      "Epoch 2 | Step 1094000 | Avg Loss: 0.0156 | Grad Norm: 0.00865783\n",
      "Epoch 2 | Step 1094100 | Avg Loss: 0.0158 | Grad Norm: 0.00756694\n",
      "Epoch 2 | Step 1094200 | Avg Loss: 0.0159 | Grad Norm: 0.00789976\n",
      "Epoch 2 | Step 1094300 | Avg Loss: 0.0156 | Grad Norm: 0.01117013\n",
      "Epoch 2 | Step 1094400 | Avg Loss: 0.0153 | Grad Norm: 0.00853067\n",
      "Epoch 2 | Step 1094500 | Avg Loss: 0.0153 | Grad Norm: 0.00944699\n",
      "Epoch 2 | Step 1094600 | Avg Loss: 0.0154 | Grad Norm: 0.00802301\n",
      "Epoch 2 | Step 1094700 | Avg Loss: 0.0151 | Grad Norm: 0.00864838\n",
      "Epoch 2 | Step 1094800 | Avg Loss: 0.0153 | Grad Norm: 0.00831159\n",
      "Epoch 2 | Step 1094900 | Avg Loss: 0.0149 | Grad Norm: 0.00970682\n",
      "Epoch 2 | Step 1095000 | Avg Loss: 0.0153 | Grad Norm: 0.00868115\n",
      "Epoch 2 | Step 1095100 | Avg Loss: 0.0153 | Grad Norm: 0.00810287\n",
      "Epoch 2 | Step 1095200 | Avg Loss: 0.0149 | Grad Norm: 0.00835907\n",
      "Epoch 2 | Step 1095300 | Avg Loss: 0.0153 | Grad Norm: 0.00904875\n",
      "Epoch 2 | Step 1095400 | Avg Loss: 0.0151 | Grad Norm: 0.00877049\n",
      "Epoch 2 | Step 1095500 | Avg Loss: 0.0148 | Grad Norm: 0.00955193\n",
      "Epoch 2 | Step 1095600 | Avg Loss: 0.0150 | Grad Norm: 0.00827476\n",
      "Epoch 2 | Step 1095700 | Avg Loss: 0.0149 | Grad Norm: 0.00756267\n",
      "Epoch 2 | Step 1095800 | Avg Loss: 0.0154 | Grad Norm: 0.00914661\n",
      "Epoch 2 | Step 1095900 | Avg Loss: 0.0154 | Grad Norm: 0.00788943\n",
      "Epoch 2 | Step 1096000 | Avg Loss: 0.0158 | Grad Norm: 0.00925756\n",
      "Epoch 2 | Step 1096100 | Avg Loss: 0.0159 | Grad Norm: 0.00933616\n",
      "Epoch 2 | Step 1096200 | Avg Loss: 0.0160 | Grad Norm: 0.00793465\n",
      "Epoch 2 | Step 1096300 | Avg Loss: 0.0162 | Grad Norm: 0.01052293\n",
      "Epoch 2 | Step 1096400 | Avg Loss: 0.0160 | Grad Norm: 0.00847571\n",
      "Epoch 2 | Step 1096500 | Avg Loss: 0.0159 | Grad Norm: 0.00810280\n",
      "Epoch 2 | Step 1096600 | Avg Loss: 0.0157 | Grad Norm: 0.00776084\n",
      "Epoch 2 | Step 1096700 | Avg Loss: 0.0159 | Grad Norm: 0.00908411\n",
      "Epoch 2 | Step 1096800 | Avg Loss: 0.0158 | Grad Norm: 0.00817091\n",
      "Epoch 2 | Step 1096900 | Avg Loss: 0.0156 | Grad Norm: 0.00882469\n",
      "Epoch 2 | Step 1097000 | Avg Loss: 0.0159 | Grad Norm: 0.00796959\n",
      "Epoch 2 | Step 1097100 | Avg Loss: 0.0163 | Grad Norm: 0.00876500\n",
      "Epoch 2 | Step 1097200 | Avg Loss: 0.0159 | Grad Norm: 0.00764808\n",
      "Epoch 2 | Step 1097300 | Avg Loss: 0.0159 | Grad Norm: 0.00890666\n",
      "Epoch 2 | Step 1097400 | Avg Loss: 0.0159 | Grad Norm: 0.00898734\n",
      "Epoch 2 | Step 1097500 | Avg Loss: 0.0156 | Grad Norm: 0.00975515\n",
      "Epoch 2 | Step 1097600 | Avg Loss: 0.0158 | Grad Norm: 0.00729815\n",
      "Epoch 2 | Step 1097700 | Avg Loss: 0.0153 | Grad Norm: 0.00875766\n",
      "Epoch 2 | Step 1097800 | Avg Loss: 0.0154 | Grad Norm: 0.00840549\n",
      "Epoch 2 | Step 1097900 | Avg Loss: 0.0158 | Grad Norm: 0.00841711\n",
      "Epoch 2 | Step 1098000 | Avg Loss: 0.0158 | Grad Norm: 0.00991158\n",
      "Epoch 2 | Step 1098100 | Avg Loss: 0.0158 | Grad Norm: 0.00931784\n",
      "Epoch 2 | Step 1098200 | Avg Loss: 0.0156 | Grad Norm: 0.00889000\n",
      "Epoch 2 | Step 1098300 | Avg Loss: 0.0159 | Grad Norm: 0.01156643\n",
      "Epoch 2 | Step 1098400 | Avg Loss: 0.0164 | Grad Norm: 0.00841052\n",
      "Epoch 2 | Step 1098500 | Avg Loss: 0.0163 | Grad Norm: 0.00904413\n",
      "Epoch 2 | Step 1098600 | Avg Loss: 0.0165 | Grad Norm: 0.00838223\n",
      "Epoch 2 | Step 1098700 | Avg Loss: 0.0162 | Grad Norm: 0.00884154\n",
      "Epoch 2 | Step 1098800 | Avg Loss: 0.0160 | Grad Norm: 0.00846016\n",
      "Epoch 2 | Step 1098900 | Avg Loss: 0.0159 | Grad Norm: 0.00950274\n",
      "Epoch 2 | Step 1099000 | Avg Loss: 0.0158 | Grad Norm: 0.01047819\n",
      "Epoch 2 | Step 1099100 | Avg Loss: 0.0156 | Grad Norm: 0.00973792\n",
      "Epoch 2 | Step 1099200 | Avg Loss: 0.0154 | Grad Norm: 0.01181744\n",
      "Epoch 2 | Step 1099300 | Avg Loss: 0.0156 | Grad Norm: 0.00928276\n",
      "Epoch 2 | Step 1099400 | Avg Loss: 0.0156 | Grad Norm: 0.00923400\n",
      "Epoch 2 | Step 1099500 | Avg Loss: 0.0156 | Grad Norm: 0.00900973\n",
      "Epoch 2 | Step 1099600 | Avg Loss: 0.0152 | Grad Norm: 0.00864520\n",
      "Epoch 2 | Step 1099700 | Avg Loss: 0.0153 | Grad Norm: 0.00992274\n",
      "Epoch 2 | Step 1099800 | Avg Loss: 0.0152 | Grad Norm: 0.00855902\n",
      "Epoch 2 | Step 1099900 | Avg Loss: 0.0157 | Grad Norm: 0.00909497\n",
      "Epoch 2 | Step 1100000 | Avg Loss: 0.0157 | Grad Norm: 0.00794087\n",
      "Saving model at step1100000\n",
      "Epoch 2 | Step 1100100 | Avg Loss: 0.0158 | Grad Norm: 0.00868857\n",
      "Epoch 2 | Step 1100200 | Avg Loss: 0.0161 | Grad Norm: 0.00757744\n",
      "Epoch 2 | Step 1100300 | Avg Loss: 0.0158 | Grad Norm: 0.00946667\n",
      "Epoch 2 | Step 1100400 | Avg Loss: 0.0160 | Grad Norm: 0.00812715\n",
      "Epoch 2 | Step 1100500 | Avg Loss: 0.0157 | Grad Norm: 0.01018815\n",
      "Epoch 2 | Step 1100600 | Avg Loss: 0.0158 | Grad Norm: 0.00842717\n",
      "Epoch 2 | Step 1100700 | Avg Loss: 0.0158 | Grad Norm: 0.00817221\n",
      "Epoch 2 | Step 1100800 | Avg Loss: 0.0160 | Grad Norm: 0.00816563\n",
      "Epoch 2 | Step 1100900 | Avg Loss: 0.0163 | Grad Norm: 0.01042633\n",
      "Epoch 2 | Step 1101000 | Avg Loss: 0.0166 | Grad Norm: 0.00920587\n",
      "Epoch 2 | Step 1101100 | Avg Loss: 0.0166 | Grad Norm: 0.00979804\n",
      "Epoch 2 | Step 1101200 | Avg Loss: 0.0162 | Grad Norm: 0.01092218\n",
      "Epoch 2 | Step 1101300 | Avg Loss: 0.0163 | Grad Norm: 0.00838949\n",
      "Epoch 2 | Step 1101400 | Avg Loss: 0.0157 | Grad Norm: 0.00890398\n",
      "Epoch 2 | Step 1101500 | Avg Loss: 0.0158 | Grad Norm: 0.00870293\n",
      "Epoch 2 | Step 1101600 | Avg Loss: 0.0159 | Grad Norm: 0.00852227\n",
      "Epoch 2 | Step 1101700 | Avg Loss: 0.0157 | Grad Norm: 0.00781297\n",
      "Epoch 2 | Step 1101800 | Avg Loss: 0.0162 | Grad Norm: 0.00819309\n",
      "Epoch 2 | Step 1101900 | Avg Loss: 0.0160 | Grad Norm: 0.00970631\n",
      "Epoch 2 | Step 1102000 | Avg Loss: 0.0158 | Grad Norm: 0.00819093\n",
      "Epoch 2 | Step 1102100 | Avg Loss: 0.0156 | Grad Norm: 0.00803117\n",
      "Epoch 2 | Step 1102200 | Avg Loss: 0.0157 | Grad Norm: 0.00917726\n",
      "Epoch 2 | Step 1102300 | Avg Loss: 0.0159 | Grad Norm: 0.01026451\n",
      "Epoch 2 | Step 1102400 | Avg Loss: 0.0160 | Grad Norm: 0.00846777\n",
      "Epoch 2 | Step 1102500 | Avg Loss: 0.0156 | Grad Norm: 0.00856506\n",
      "Epoch 2 | Step 1102600 | Avg Loss: 0.0156 | Grad Norm: 0.00860735\n",
      "Epoch 2 | Step 1102700 | Avg Loss: 0.0157 | Grad Norm: 0.00895888\n",
      "Epoch 2 | Step 1102800 | Avg Loss: 0.0155 | Grad Norm: 0.00886399\n",
      "Epoch 2 | Step 1102900 | Avg Loss: 0.0155 | Grad Norm: 0.00795133\n",
      "Epoch 2 | Step 1103000 | Avg Loss: 0.0158 | Grad Norm: 0.01020768\n",
      "Epoch 2 | Step 1103100 | Avg Loss: 0.0158 | Grad Norm: 0.00964207\n",
      "Epoch 2 | Step 1103200 | Avg Loss: 0.0160 | Grad Norm: 0.01111869\n",
      "Epoch 2 | Step 1103300 | Avg Loss: 0.0158 | Grad Norm: 0.00849122\n",
      "Epoch 2 | Step 1103400 | Avg Loss: 0.0155 | Grad Norm: 0.00935681\n",
      "Epoch 2 | Step 1103500 | Avg Loss: 0.0156 | Grad Norm: 0.01021956\n",
      "Epoch 2 | Step 1103600 | Avg Loss: 0.0155 | Grad Norm: 0.00976954\n",
      "Epoch 2 | Step 1103700 | Avg Loss: 0.0157 | Grad Norm: 0.00942160\n",
      "Epoch 2 | Step 1103800 | Avg Loss: 0.0156 | Grad Norm: 0.01211512\n",
      "Epoch 2 | Step 1103900 | Avg Loss: 0.0155 | Grad Norm: 0.00926464\n",
      "Epoch 2 | Step 1104000 | Avg Loss: 0.0154 | Grad Norm: 0.00839699\n",
      "Epoch 2 | Step 1104100 | Avg Loss: 0.0154 | Grad Norm: 0.00866421\n",
      "Epoch 2 | Step 1104200 | Avg Loss: 0.0156 | Grad Norm: 0.00951277\n",
      "Epoch 2 | Step 1104300 | Avg Loss: 0.0154 | Grad Norm: 0.01018010\n",
      "Epoch 2 | Step 1104400 | Avg Loss: 0.0157 | Grad Norm: 0.00972418\n",
      "Epoch 2 | Step 1104500 | Avg Loss: 0.0156 | Grad Norm: 0.00969332\n",
      "Epoch 2 | Step 1104600 | Avg Loss: 0.0156 | Grad Norm: 0.00844309\n",
      "Epoch 2 | Step 1104700 | Avg Loss: 0.0158 | Grad Norm: 0.01031420\n",
      "Epoch 2 | Step 1104800 | Avg Loss: 0.0161 | Grad Norm: 0.00908890\n",
      "Epoch 2 | Step 1104900 | Avg Loss: 0.0163 | Grad Norm: 0.00951018\n",
      "Epoch 2 | Step 1105000 | Avg Loss: 0.0158 | Grad Norm: 0.00837842\n",
      "Epoch 2 | Step 1105100 | Avg Loss: 0.0154 | Grad Norm: 0.00915918\n",
      "Epoch 2 | Step 1105200 | Avg Loss: 0.0152 | Grad Norm: 0.00867898\n",
      "Epoch 2 | Step 1105300 | Avg Loss: 0.0152 | Grad Norm: 0.00882843\n",
      "Epoch 2 | Step 1105400 | Avg Loss: 0.0154 | Grad Norm: 0.00908633\n",
      "Epoch 2 | Step 1105500 | Avg Loss: 0.0154 | Grad Norm: 0.00830813\n",
      "Epoch 2 | Step 1105600 | Avg Loss: 0.0154 | Grad Norm: 0.00856969\n",
      "Epoch 2 | Step 1105700 | Avg Loss: 0.0154 | Grad Norm: 0.00894522\n",
      "Epoch 2 | Step 1105800 | Avg Loss: 0.0156 | Grad Norm: 0.00885402\n",
      "Epoch 2 | Step 1105900 | Avg Loss: 0.0156 | Grad Norm: 0.00867514\n",
      "Epoch 2 | Step 1106000 | Avg Loss: 0.0156 | Grad Norm: 0.00859664\n",
      "Epoch 2 | Step 1106100 | Avg Loss: 0.0159 | Grad Norm: 0.00854164\n",
      "Epoch 2 | Step 1106200 | Avg Loss: 0.0164 | Grad Norm: 0.01026892\n",
      "Epoch 2 | Step 1106300 | Avg Loss: 0.0161 | Grad Norm: 0.00957852\n",
      "Epoch 2 | Step 1106400 | Avg Loss: 0.0158 | Grad Norm: 0.00807892\n",
      "Epoch 2 | Step 1106500 | Avg Loss: 0.0153 | Grad Norm: 0.00814079\n",
      "Epoch 2 | Step 1106600 | Avg Loss: 0.0152 | Grad Norm: 0.00915167\n",
      "Epoch 2 | Step 1106700 | Avg Loss: 0.0153 | Grad Norm: 0.00756827\n",
      "Epoch 2 | Step 1106800 | Avg Loss: 0.0156 | Grad Norm: 0.00885286\n",
      "Epoch 2 | Step 1106900 | Avg Loss: 0.0156 | Grad Norm: 0.00960357\n",
      "Epoch 2 | Step 1107000 | Avg Loss: 0.0155 | Grad Norm: 0.00928946\n",
      "Epoch 2 | Step 1107100 | Avg Loss: 0.0156 | Grad Norm: 0.00980740\n",
      "Epoch 2 | Step 1107200 | Avg Loss: 0.0156 | Grad Norm: 0.00902504\n",
      "Epoch 2 | Step 1107300 | Avg Loss: 0.0153 | Grad Norm: 0.00839582\n",
      "Epoch 2 | Step 1107400 | Avg Loss: 0.0152 | Grad Norm: 0.00797777\n",
      "Epoch 2 | Step 1107500 | Avg Loss: 0.0147 | Grad Norm: 0.00749312\n",
      "Epoch 2 | Step 1107600 | Avg Loss: 0.0146 | Grad Norm: 0.00806686\n",
      "Epoch 2 | Step 1107700 | Avg Loss: 0.0146 | Grad Norm: 0.00920969\n",
      "Epoch 2 | Step 1107800 | Avg Loss: 0.0154 | Grad Norm: 0.00901639\n",
      "Epoch 2 | Step 1107900 | Avg Loss: 0.0156 | Grad Norm: 0.00793318\n",
      "Epoch 2 | Step 1108000 | Avg Loss: 0.0152 | Grad Norm: 0.00873202\n",
      "Epoch 2 | Step 1108100 | Avg Loss: 0.0155 | Grad Norm: 0.00792338\n",
      "Epoch 2 | Step 1108200 | Avg Loss: 0.0156 | Grad Norm: 0.00847907\n",
      "Epoch 2 | Step 1108300 | Avg Loss: 0.0158 | Grad Norm: 0.00915324\n",
      "Epoch 2 | Step 1108400 | Avg Loss: 0.0160 | Grad Norm: 0.00764857\n",
      "Epoch 2 | Step 1108500 | Avg Loss: 0.0161 | Grad Norm: 0.01224843\n",
      "Epoch 2 | Step 1108600 | Avg Loss: 0.0157 | Grad Norm: 0.00921455\n",
      "Epoch 2 | Step 1108700 | Avg Loss: 0.0159 | Grad Norm: 0.00779527\n",
      "Epoch 2 | Step 1108800 | Avg Loss: 0.0157 | Grad Norm: 0.00890431\n",
      "Epoch 2 | Step 1108900 | Avg Loss: 0.0154 | Grad Norm: 0.00834361\n",
      "Epoch 2 | Step 1109000 | Avg Loss: 0.0154 | Grad Norm: 0.00886290\n",
      "Epoch 2 | Step 1109100 | Avg Loss: 0.0149 | Grad Norm: 0.00785252\n",
      "Epoch 2 | Step 1109200 | Avg Loss: 0.0149 | Grad Norm: 0.01021396\n",
      "Epoch 2 | Step 1109300 | Avg Loss: 0.0150 | Grad Norm: 0.00881206\n",
      "Epoch 2 | Step 1109400 | Avg Loss: 0.0150 | Grad Norm: 0.00832162\n",
      "Epoch 2 | Step 1109500 | Avg Loss: 0.0152 | Grad Norm: 0.00858253\n",
      "Epoch 2 | Step 1109600 | Avg Loss: 0.0151 | Grad Norm: 0.00927128\n",
      "Epoch 2 | Step 1109700 | Avg Loss: 0.0151 | Grad Norm: 0.00947283\n",
      "Epoch 2 | Step 1109800 | Avg Loss: 0.0153 | Grad Norm: 0.00744873\n",
      "Epoch 2 | Step 1109900 | Avg Loss: 0.0156 | Grad Norm: 0.00785475\n",
      "Epoch 2 | Step 1110000 | Avg Loss: 0.0156 | Grad Norm: 0.00908604\n",
      "Epoch 2 | Step 1110100 | Avg Loss: 0.0157 | Grad Norm: 0.00885311\n",
      "Epoch 2 | Step 1110200 | Avg Loss: 0.0159 | Grad Norm: 0.00932625\n",
      "Epoch 2 | Step 1110300 | Avg Loss: 0.0157 | Grad Norm: 0.00931838\n",
      "Epoch 2 | Step 1110400 | Avg Loss: 0.0155 | Grad Norm: 0.00804659\n",
      "Epoch 2 | Step 1110500 | Avg Loss: 0.0153 | Grad Norm: 0.00842652\n",
      "Epoch 2 | Step 1110600 | Avg Loss: 0.0156 | Grad Norm: 0.00885683\n",
      "Epoch 2 | Step 1110700 | Avg Loss: 0.0152 | Grad Norm: 0.00795337\n",
      "Epoch 2 | Step 1110800 | Avg Loss: 0.0150 | Grad Norm: 0.00924765\n",
      "Epoch 2 | Step 1110900 | Avg Loss: 0.0154 | Grad Norm: 0.00872072\n",
      "Epoch 2 | Step 1111000 | Avg Loss: 0.0153 | Grad Norm: 0.01260324\n",
      "Epoch 2 | Step 1111100 | Avg Loss: 0.0157 | Grad Norm: 0.00938297\n",
      "Epoch 2 | Step 1111200 | Avg Loss: 0.0155 | Grad Norm: 0.00832298\n",
      "Epoch 2 | Step 1111300 | Avg Loss: 0.0154 | Grad Norm: 0.00889504\n",
      "Epoch 2 | Step 1111400 | Avg Loss: 0.0150 | Grad Norm: 0.00826786\n",
      "Epoch 2 | Step 1111500 | Avg Loss: 0.0158 | Grad Norm: 0.00921360\n",
      "Epoch 2 | Step 1111600 | Avg Loss: 0.0155 | Grad Norm: 0.01067652\n",
      "Epoch 2 | Step 1111700 | Avg Loss: 0.0158 | Grad Norm: 0.00894586\n",
      "Epoch 2 | Step 1111800 | Avg Loss: 0.0158 | Grad Norm: 0.00938249\n",
      "Epoch 2 | Step 1111900 | Avg Loss: 0.0159 | Grad Norm: 0.00851053\n",
      "Epoch 2 | Step 1112000 | Avg Loss: 0.0163 | Grad Norm: 0.00896060\n",
      "Epoch 2 | Step 1112100 | Avg Loss: 0.0161 | Grad Norm: 0.00924313\n",
      "Epoch 2 | Step 1112200 | Avg Loss: 0.0163 | Grad Norm: 0.00951317\n",
      "Epoch 2 | Step 1112300 | Avg Loss: 0.0161 | Grad Norm: 0.00896367\n",
      "Epoch 2 | Step 1112400 | Avg Loss: 0.0164 | Grad Norm: 0.01058138\n",
      "Epoch 2 | Step 1112500 | Avg Loss: 0.0160 | Grad Norm: 0.01130858\n",
      "Epoch 2 | Step 1112600 | Avg Loss: 0.0157 | Grad Norm: 0.00919648\n",
      "Epoch 2 | Step 1112700 | Avg Loss: 0.0159 | Grad Norm: 0.00977944\n",
      "Epoch 2 | Step 1112800 | Avg Loss: 0.0167 | Grad Norm: 0.00861178\n",
      "Epoch 2 | Step 1112900 | Avg Loss: 0.0165 | Grad Norm: 0.00962963\n",
      "Epoch 2 | Step 1113000 | Avg Loss: 0.0163 | Grad Norm: 0.01027666\n",
      "Epoch 2 | Step 1113100 | Avg Loss: 0.0159 | Grad Norm: 0.00890165\n",
      "Epoch 2 | Step 1113200 | Avg Loss: 0.0158 | Grad Norm: 0.00836160\n",
      "Epoch 2 | Step 1113300 | Avg Loss: 0.0158 | Grad Norm: 0.00975433\n",
      "Epoch 2 | Step 1113400 | Avg Loss: 0.0160 | Grad Norm: 0.00885532\n",
      "Epoch 2 | Step 1113500 | Avg Loss: 0.0160 | Grad Norm: 0.00947910\n",
      "Epoch 2 | Step 1113600 | Avg Loss: 0.0158 | Grad Norm: 0.00873469\n",
      "Epoch 2 | Step 1113700 | Avg Loss: 0.0156 | Grad Norm: 0.00922961\n",
      "Epoch 2 | Step 1113800 | Avg Loss: 0.0155 | Grad Norm: 0.00829852\n",
      "Epoch 2 | Step 1113900 | Avg Loss: 0.0159 | Grad Norm: 0.00976555\n",
      "Epoch 2 | Step 1114000 | Avg Loss: 0.0157 | Grad Norm: 0.01129346\n",
      "Epoch 2 | Step 1114100 | Avg Loss: 0.0158 | Grad Norm: 0.00923271\n",
      "Epoch 2 | Step 1114200 | Avg Loss: 0.0161 | Grad Norm: 0.00880533\n",
      "Epoch 2 | Step 1114300 | Avg Loss: 0.0159 | Grad Norm: 0.00927737\n",
      "Epoch 2 | Step 1114400 | Avg Loss: 0.0158 | Grad Norm: 0.00900896\n",
      "Epoch 2 | Step 1114500 | Avg Loss: 0.0156 | Grad Norm: 0.00849005\n",
      "Epoch 2 | Step 1114600 | Avg Loss: 0.0161 | Grad Norm: 0.00894670\n",
      "Epoch 2 | Step 1114700 | Avg Loss: 0.0160 | Grad Norm: 0.00899382\n",
      "Epoch 2 | Step 1114800 | Avg Loss: 0.0161 | Grad Norm: 0.00980782\n",
      "Epoch 2 | Step 1114900 | Avg Loss: 0.0157 | Grad Norm: 0.00917119\n",
      "Epoch 2 | Step 1115000 | Avg Loss: 0.0155 | Grad Norm: 0.00974918\n",
      "Epoch 2 | Step 1115100 | Avg Loss: 0.0156 | Grad Norm: 0.00891572\n",
      "Epoch 2 | Step 1115200 | Avg Loss: 0.0156 | Grad Norm: 0.00846653\n",
      "Epoch 2 | Step 1115300 | Avg Loss: 0.0157 | Grad Norm: 0.00965359\n",
      "Epoch 2 | Step 1115400 | Avg Loss: 0.0161 | Grad Norm: 0.00873079\n",
      "Epoch 2 | Step 1115500 | Avg Loss: 0.0163 | Grad Norm: 0.00926472\n",
      "Epoch 2 | Step 1115600 | Avg Loss: 0.0157 | Grad Norm: 0.00846246\n",
      "Epoch 2 | Step 1115700 | Avg Loss: 0.0155 | Grad Norm: 0.01031526\n",
      "Epoch 2 | Step 1115800 | Avg Loss: 0.0154 | Grad Norm: 0.00976043\n",
      "Epoch 2 | Step 1115900 | Avg Loss: 0.0157 | Grad Norm: 0.01026317\n",
      "Epoch 2 | Step 1116000 | Avg Loss: 0.0157 | Grad Norm: 0.00761381\n",
      "Epoch 2 | Step 1116100 | Avg Loss: 0.0156 | Grad Norm: 0.00854775\n",
      "Epoch 2 | Step 1116200 | Avg Loss: 0.0157 | Grad Norm: 0.00933544\n",
      "Epoch 2 | Step 1116300 | Avg Loss: 0.0160 | Grad Norm: 0.01110358\n",
      "Epoch 2 | Step 1116400 | Avg Loss: 0.0160 | Grad Norm: 0.00901749\n",
      "Epoch 2 | Step 1116500 | Avg Loss: 0.0158 | Grad Norm: 0.00911264\n",
      "Epoch 2 | Step 1116600 | Avg Loss: 0.0156 | Grad Norm: 0.00992166\n",
      "Epoch 2 | Step 1116700 | Avg Loss: 0.0154 | Grad Norm: 0.00805457\n",
      "Epoch 2 | Step 1116800 | Avg Loss: 0.0158 | Grad Norm: 0.01050618\n",
      "Epoch 2 | Step 1116900 | Avg Loss: 0.0157 | Grad Norm: 0.00864086\n",
      "Epoch 2 | Step 1117000 | Avg Loss: 0.0157 | Grad Norm: 0.00885236\n",
      "Epoch 2 | Step 1117100 | Avg Loss: 0.0157 | Grad Norm: 0.00945405\n",
      "Epoch 2 | Step 1117200 | Avg Loss: 0.0157 | Grad Norm: 0.00804428\n",
      "Epoch 2 | Step 1117300 | Avg Loss: 0.0155 | Grad Norm: 0.01029652\n",
      "Epoch 2 | Step 1117400 | Avg Loss: 0.0156 | Grad Norm: 0.01155861\n",
      "Epoch 2 | Step 1117500 | Avg Loss: 0.0153 | Grad Norm: 0.00815977\n",
      "Epoch 2 | Step 1117600 | Avg Loss: 0.0156 | Grad Norm: 0.00911757\n",
      "Epoch 2 | Step 1117700 | Avg Loss: 0.0155 | Grad Norm: 0.00938973\n",
      "Epoch 2 | Step 1117800 | Avg Loss: 0.0153 | Grad Norm: 0.01066300\n",
      "Epoch 2 | Step 1117900 | Avg Loss: 0.0150 | Grad Norm: 0.00844226\n",
      "Epoch 2 | Step 1118000 | Avg Loss: 0.0156 | Grad Norm: 0.01010636\n",
      "Epoch 2 | Step 1118100 | Avg Loss: 0.0153 | Grad Norm: 0.01032321\n",
      "Epoch 2 | Step 1118200 | Avg Loss: 0.0155 | Grad Norm: 0.00951549\n",
      "Epoch 2 | Step 1118300 | Avg Loss: 0.0156 | Grad Norm: 0.01198783\n",
      "Epoch 2 | Step 1118400 | Avg Loss: 0.0157 | Grad Norm: 0.00906652\n",
      "Epoch 2 | Step 1118500 | Avg Loss: 0.0156 | Grad Norm: 0.00833906\n",
      "Epoch 2 | Step 1118600 | Avg Loss: 0.0156 | Grad Norm: 0.00888033\n",
      "Epoch 2 | Step 1118700 | Avg Loss: 0.0154 | Grad Norm: 0.00940097\n",
      "Epoch 2 | Step 1118800 | Avg Loss: 0.0155 | Grad Norm: 0.00823950\n",
      "Epoch 2 | Step 1118900 | Avg Loss: 0.0155 | Grad Norm: 0.00959509\n",
      "Epoch 2 | Step 1119000 | Avg Loss: 0.0155 | Grad Norm: 0.01043682\n",
      "Epoch 2 | Step 1119100 | Avg Loss: 0.0154 | Grad Norm: 0.00816231\n",
      "Epoch 2 | Step 1119200 | Avg Loss: 0.0154 | Grad Norm: 0.00828593\n",
      "Epoch 2 | Step 1119300 | Avg Loss: 0.0156 | Grad Norm: 0.00827535\n",
      "Epoch 2 | Step 1119400 | Avg Loss: 0.0157 | Grad Norm: 0.00829119\n",
      "Epoch 2 | Step 1119500 | Avg Loss: 0.0155 | Grad Norm: 0.00878032\n",
      "Epoch 2 | Step 1119600 | Avg Loss: 0.0157 | Grad Norm: 0.00934326\n",
      "Epoch 2 | Step 1119700 | Avg Loss: 0.0159 | Grad Norm: 0.00940569\n",
      "Epoch 2 | Step 1119800 | Avg Loss: 0.0157 | Grad Norm: 0.00944485\n",
      "Epoch 2 | Step 1119900 | Avg Loss: 0.0157 | Grad Norm: 0.00844411\n",
      "Epoch 2 | Step 1120000 | Avg Loss: 0.0153 | Grad Norm: 0.00903717\n",
      "Epoch 2 | Step 1120100 | Avg Loss: 0.0152 | Grad Norm: 0.00854878\n",
      "Epoch 2 | Step 1120200 | Avg Loss: 0.0158 | Grad Norm: 0.00874133\n",
      "Epoch 2 | Step 1120300 | Avg Loss: 0.0155 | Grad Norm: 0.00935206\n",
      "Epoch 2 | Step 1120400 | Avg Loss: 0.0154 | Grad Norm: 0.00908375\n",
      "Epoch 2 | Step 1120500 | Avg Loss: 0.0151 | Grad Norm: 0.00842526\n",
      "Epoch 2 | Step 1120600 | Avg Loss: 0.0152 | Grad Norm: 0.00881004\n",
      "Epoch 2 | Step 1120700 | Avg Loss: 0.0150 | Grad Norm: 0.00788403\n",
      "Epoch 2 | Step 1120800 | Avg Loss: 0.0149 | Grad Norm: 0.00917843\n",
      "Epoch 2 | Step 1120900 | Avg Loss: 0.0149 | Grad Norm: 0.00767054\n",
      "Epoch 2 | Step 1121000 | Avg Loss: 0.0148 | Grad Norm: 0.00911648\n",
      "Epoch 2 | Step 1121100 | Avg Loss: 0.0148 | Grad Norm: 0.00906701\n",
      "Epoch 2 | Step 1121200 | Avg Loss: 0.0150 | Grad Norm: 0.00953669\n",
      "Epoch 2 | Step 1121300 | Avg Loss: 0.0155 | Grad Norm: 0.01042675\n",
      "Epoch 2 | Step 1121400 | Avg Loss: 0.0154 | Grad Norm: 0.00798480\n",
      "Epoch 2 | Step 1121500 | Avg Loss: 0.0152 | Grad Norm: 0.00995362\n",
      "Epoch 2 | Step 1121600 | Avg Loss: 0.0150 | Grad Norm: 0.00869336\n",
      "Epoch 2 | Step 1121700 | Avg Loss: 0.0152 | Grad Norm: 0.00774336\n",
      "Epoch 2 | Step 1121800 | Avg Loss: 0.0153 | Grad Norm: 0.00772946\n",
      "Epoch 2 | Step 1121900 | Avg Loss: 0.0155 | Grad Norm: 0.00866815\n",
      "Epoch 2 | Step 1122000 | Avg Loss: 0.0155 | Grad Norm: 0.00856547\n",
      "Epoch 2 | Step 1122100 | Avg Loss: 0.0156 | Grad Norm: 0.01052767\n",
      "Epoch 2 | Step 1122200 | Avg Loss: 0.0155 | Grad Norm: 0.00959055\n",
      "Epoch 2 | Step 1122300 | Avg Loss: 0.0154 | Grad Norm: 0.00862375\n",
      "Epoch 2 | Step 1122400 | Avg Loss: 0.0152 | Grad Norm: 0.00804509\n",
      "Epoch 2 | Step 1122500 | Avg Loss: 0.0148 | Grad Norm: 0.00995755\n",
      "Epoch 2 | Step 1122600 | Avg Loss: 0.0152 | Grad Norm: 0.00841617\n",
      "Epoch 2 | Step 1122700 | Avg Loss: 0.0154 | Grad Norm: 0.00788006\n",
      "Epoch 2 | Step 1122800 | Avg Loss: 0.0154 | Grad Norm: 0.00846850\n",
      "Epoch 2 | Step 1122900 | Avg Loss: 0.0152 | Grad Norm: 0.01057564\n",
      "Epoch 2 | Step 1123000 | Avg Loss: 0.0155 | Grad Norm: 0.00806527\n",
      "Epoch 2 | Step 1123100 | Avg Loss: 0.0155 | Grad Norm: 0.00841374\n",
      "Epoch 2 | Step 1123200 | Avg Loss: 0.0161 | Grad Norm: 0.00889307\n",
      "Epoch 2 | Step 1123300 | Avg Loss: 0.0161 | Grad Norm: 0.00850957\n",
      "Epoch 2 | Step 1123400 | Avg Loss: 0.0161 | Grad Norm: 0.00950493\n",
      "Epoch 2 | Step 1123500 | Avg Loss: 0.0156 | Grad Norm: 0.00967944\n",
      "Epoch 2 | Step 1123600 | Avg Loss: 0.0154 | Grad Norm: 0.00930654\n",
      "Epoch 2 | Step 1123700 | Avg Loss: 0.0151 | Grad Norm: 0.00887820\n",
      "Epoch 2 | Step 1123800 | Avg Loss: 0.0152 | Grad Norm: 0.00846971\n",
      "Epoch 2 | Step 1123900 | Avg Loss: 0.0158 | Grad Norm: 0.00869561\n",
      "Epoch 2 | Step 1124000 | Avg Loss: 0.0158 | Grad Norm: 0.00894165\n",
      "Epoch 2 | Step 1124100 | Avg Loss: 0.0156 | Grad Norm: 0.00987734\n",
      "Epoch 2 | Step 1124200 | Avg Loss: 0.0156 | Grad Norm: 0.00840538\n",
      "Epoch 2 | Step 1124300 | Avg Loss: 0.0155 | Grad Norm: 0.00854358\n",
      "Epoch 2 | Step 1124400 | Avg Loss: 0.0156 | Grad Norm: 0.00871076\n",
      "Epoch 2 | Step 1124500 | Avg Loss: 0.0153 | Grad Norm: 0.00827508\n",
      "Epoch 2 | Step 1124600 | Avg Loss: 0.0155 | Grad Norm: 0.01034190\n",
      "Epoch 2 | Step 1124700 | Avg Loss: 0.0155 | Grad Norm: 0.00825435\n",
      "Epoch 2 | Step 1124800 | Avg Loss: 0.0156 | Grad Norm: 0.00804619\n",
      "Epoch 2 | Step 1124900 | Avg Loss: 0.0158 | Grad Norm: 0.00886360\n",
      "Epoch 2 | Step 1125000 | Avg Loss: 0.0157 | Grad Norm: 0.00769357\n",
      "Epoch 2 | Step 1125100 | Avg Loss: 0.0159 | Grad Norm: 0.00860613\n",
      "Epoch 2 | Step 1125200 | Avg Loss: 0.0160 | Grad Norm: 0.00948648\n",
      "Epoch 2 | Step 1125300 | Avg Loss: 0.0157 | Grad Norm: 0.00802069\n",
      "Epoch 2 | Step 1125400 | Avg Loss: 0.0159 | Grad Norm: 0.01041643\n",
      "Epoch 2 | Step 1125500 | Avg Loss: 0.0161 | Grad Norm: 0.00890061\n",
      "Epoch 2 | Step 1125600 | Avg Loss: 0.0159 | Grad Norm: 0.01012503\n",
      "Epoch 2 | Step 1125700 | Avg Loss: 0.0156 | Grad Norm: 0.01071265\n",
      "Epoch 2 | Step 1125800 | Avg Loss: 0.0159 | Grad Norm: 0.01004985\n",
      "Epoch 2 | Step 1125900 | Avg Loss: 0.0160 | Grad Norm: 0.00845946\n",
      "Epoch 2 | Step 1126000 | Avg Loss: 0.0160 | Grad Norm: 0.00949440\n",
      "Epoch 2 | Step 1126100 | Avg Loss: 0.0157 | Grad Norm: 0.00801491\n",
      "Epoch 2 | Step 1126200 | Avg Loss: 0.0155 | Grad Norm: 0.00927689\n",
      "Epoch 2 | Step 1126300 | Avg Loss: 0.0154 | Grad Norm: 0.01004792\n",
      "Epoch 2 | Step 1126400 | Avg Loss: 0.0156 | Grad Norm: 0.00953888\n",
      "Epoch 2 | Step 1126500 | Avg Loss: 0.0155 | Grad Norm: 0.00953661\n",
      "Epoch 2 | Step 1126600 | Avg Loss: 0.0153 | Grad Norm: 0.00810490\n",
      "Epoch 2 | Step 1126700 | Avg Loss: 0.0151 | Grad Norm: 0.00866881\n",
      "Epoch 2 | Step 1126800 | Avg Loss: 0.0155 | Grad Norm: 0.01107001\n",
      "Epoch 2 | Step 1126900 | Avg Loss: 0.0154 | Grad Norm: 0.01040650\n",
      "Epoch 2 | Step 1127000 | Avg Loss: 0.0154 | Grad Norm: 0.00886131\n",
      "Epoch 2 | Step 1127100 | Avg Loss: 0.0156 | Grad Norm: 0.00995646\n",
      "Epoch 2 | Step 1127200 | Avg Loss: 0.0155 | Grad Norm: 0.00795627\n",
      "Epoch 2 | Step 1127300 | Avg Loss: 0.0155 | Grad Norm: 0.00885917\n",
      "Epoch 2 | Step 1127400 | Avg Loss: 0.0158 | Grad Norm: 0.00885016\n",
      "Epoch 2 | Step 1127500 | Avg Loss: 0.0155 | Grad Norm: 0.00920283\n",
      "Epoch 2 | Step 1127600 | Avg Loss: 0.0155 | Grad Norm: 0.01076997\n",
      "Epoch 2 | Step 1127700 | Avg Loss: 0.0157 | Grad Norm: 0.01041012\n",
      "Epoch 2 | Step 1127800 | Avg Loss: 0.0155 | Grad Norm: 0.00961138\n",
      "Epoch 2 | Step 1127900 | Avg Loss: 0.0155 | Grad Norm: 0.00722559\n",
      "Epoch 2 | Step 1128000 | Avg Loss: 0.0152 | Grad Norm: 0.00857816\n",
      "Epoch 2 | Step 1128100 | Avg Loss: 0.0152 | Grad Norm: 0.00806415\n",
      "Epoch 2 | Step 1128200 | Avg Loss: 0.0153 | Grad Norm: 0.00789215\n",
      "Epoch 2 | Step 1128300 | Avg Loss: 0.0154 | Grad Norm: 0.00934164\n",
      "Epoch 2 | Step 1128400 | Avg Loss: 0.0156 | Grad Norm: 0.00773075\n",
      "Epoch 2 | Step 1128500 | Avg Loss: 0.0150 | Grad Norm: 0.00814208\n",
      "Epoch 2 | Step 1128600 | Avg Loss: 0.0150 | Grad Norm: 0.00865946\n",
      "Epoch 2 | Step 1128700 | Avg Loss: 0.0152 | Grad Norm: 0.00933555\n",
      "Epoch 2 | Step 1128800 | Avg Loss: 0.0152 | Grad Norm: 0.00822073\n",
      "Epoch 2 | Step 1128900 | Avg Loss: 0.0156 | Grad Norm: 0.01110243\n",
      "Epoch 2 | Step 1129000 | Avg Loss: 0.0157 | Grad Norm: 0.01008888\n",
      "Epoch 2 | Step 1129100 | Avg Loss: 0.0157 | Grad Norm: 0.00833613\n",
      "Epoch 2 | Step 1129200 | Avg Loss: 0.0157 | Grad Norm: 0.00917215\n",
      "Epoch 2 | Step 1129300 | Avg Loss: 0.0158 | Grad Norm: 0.00843238\n",
      "Epoch 2 | Step 1129400 | Avg Loss: 0.0157 | Grad Norm: 0.00883597\n",
      "Epoch 2 | Step 1129500 | Avg Loss: 0.0158 | Grad Norm: 0.00803017\n",
      "Epoch 2 | Step 1129600 | Avg Loss: 0.0158 | Grad Norm: 0.00916135\n",
      "Epoch 2 | Step 1129700 | Avg Loss: 0.0155 | Grad Norm: 0.00983152\n",
      "Epoch 2 | Step 1129800 | Avg Loss: 0.0152 | Grad Norm: 0.00852366\n",
      "Epoch 2 | Step 1129900 | Avg Loss: 0.0159 | Grad Norm: 0.01010011\n",
      "Epoch 2 | Step 1130000 | Avg Loss: 0.0154 | Grad Norm: 0.00853558\n",
      "Epoch 2 | Step 1130100 | Avg Loss: 0.0156 | Grad Norm: 0.00844195\n",
      "Epoch 2 | Step 1130200 | Avg Loss: 0.0156 | Grad Norm: 0.00825360\n",
      "Epoch 2 | Step 1130300 | Avg Loss: 0.0154 | Grad Norm: 0.00925559\n",
      "Epoch 2 | Step 1130400 | Avg Loss: 0.0153 | Grad Norm: 0.00876250\n",
      "Epoch 2 | Step 1130500 | Avg Loss: 0.0156 | Grad Norm: 0.01072219\n",
      "Epoch 2 | Step 1130600 | Avg Loss: 0.0157 | Grad Norm: 0.00818441\n",
      "Epoch 2 | Step 1130700 | Avg Loss: 0.0158 | Grad Norm: 0.00853044\n",
      "Epoch 2 | Step 1130800 | Avg Loss: 0.0157 | Grad Norm: 0.00996894\n",
      "Epoch 2 | Step 1130900 | Avg Loss: 0.0158 | Grad Norm: 0.00886181\n",
      "Epoch 2 | Step 1131000 | Avg Loss: 0.0157 | Grad Norm: 0.00875915\n",
      "Epoch 2 | Step 1131100 | Avg Loss: 0.0157 | Grad Norm: 0.00803719\n",
      "Epoch 2 | Step 1131200 | Avg Loss: 0.0154 | Grad Norm: 0.00812853\n",
      "Epoch 2 | Step 1131300 | Avg Loss: 0.0155 | Grad Norm: 0.00845427\n",
      "Epoch 2 | Step 1131400 | Avg Loss: 0.0158 | Grad Norm: 0.00838866\n",
      "Epoch 2 | Step 1131500 | Avg Loss: 0.0159 | Grad Norm: 0.00836751\n",
      "Epoch 2 | Step 1131600 | Avg Loss: 0.0159 | Grad Norm: 0.00838733\n",
      "Epoch 2 | Step 1131700 | Avg Loss: 0.0159 | Grad Norm: 0.00905165\n",
      "Epoch 2 | Step 1131800 | Avg Loss: 0.0159 | Grad Norm: 0.01154540\n",
      "Epoch 2 | Step 1131900 | Avg Loss: 0.0158 | Grad Norm: 0.00867750\n",
      "Epoch 2 | Step 1132000 | Avg Loss: 0.0160 | Grad Norm: 0.00859054\n",
      "Epoch 2 | Step 1132100 | Avg Loss: 0.0161 | Grad Norm: 0.00935919\n",
      "Epoch 2 | Step 1132200 | Avg Loss: 0.0159 | Grad Norm: 0.00961307\n",
      "Epoch 2 | Step 1132300 | Avg Loss: 0.0162 | Grad Norm: 0.00971434\n",
      "Epoch 2 | Step 1132400 | Avg Loss: 0.0160 | Grad Norm: 0.01125655\n",
      "Epoch 2 | Step 1132500 | Avg Loss: 0.0156 | Grad Norm: 0.00923331\n",
      "Epoch 2 | Step 1132600 | Avg Loss: 0.0156 | Grad Norm: 0.00910165\n",
      "Epoch 2 | Step 1132700 | Avg Loss: 0.0154 | Grad Norm: 0.00722300\n",
      "Epoch 2 | Step 1132800 | Avg Loss: 0.0153 | Grad Norm: 0.00917717\n",
      "Epoch 2 | Step 1132900 | Avg Loss: 0.0157 | Grad Norm: 0.00891575\n",
      "Epoch 2 | Step 1133000 | Avg Loss: 0.0158 | Grad Norm: 0.00765949\n",
      "Epoch 2 | Step 1133100 | Avg Loss: 0.0161 | Grad Norm: 0.00959204\n",
      "Epoch 2 | Step 1133200 | Avg Loss: 0.0157 | Grad Norm: 0.00823588\n",
      "Epoch 2 | Step 1133300 | Avg Loss: 0.0159 | Grad Norm: 0.00909696\n",
      "Epoch 2 | Step 1133400 | Avg Loss: 0.0160 | Grad Norm: 0.00877001\n",
      "Epoch 2 | Step 1133500 | Avg Loss: 0.0159 | Grad Norm: 0.00875209\n",
      "Epoch 2 | Step 1133600 | Avg Loss: 0.0158 | Grad Norm: 0.00874463\n",
      "Epoch 2 | Step 1133700 | Avg Loss: 0.0158 | Grad Norm: 0.00885824\n",
      "Epoch 2 | Step 1133800 | Avg Loss: 0.0158 | Grad Norm: 0.00895577\n",
      "Epoch 2 | Step 1133900 | Avg Loss: 0.0159 | Grad Norm: 0.00987782\n",
      "Epoch 2 | Step 1134000 | Avg Loss: 0.0157 | Grad Norm: 0.00817045\n",
      "Epoch 2 | Step 1134100 | Avg Loss: 0.0156 | Grad Norm: 0.00878399\n",
      "Epoch 2 | Step 1134200 | Avg Loss: 0.0152 | Grad Norm: 0.00934446\n",
      "Epoch 2 | Step 1134300 | Avg Loss: 0.0154 | Grad Norm: 0.00901038\n",
      "Epoch 2 | Step 1134400 | Avg Loss: 0.0155 | Grad Norm: 0.00908840\n",
      "Epoch 2 | Step 1134500 | Avg Loss: 0.0158 | Grad Norm: 0.01023205\n",
      "Epoch 2 | Step 1134600 | Avg Loss: 0.0155 | Grad Norm: 0.00849652\n",
      "Epoch 2 | Step 1134700 | Avg Loss: 0.0156 | Grad Norm: 0.00901412\n",
      "Epoch 2 | Step 1134800 | Avg Loss: 0.0149 | Grad Norm: 0.00870766\n",
      "Epoch 2 | Step 1134900 | Avg Loss: 0.0146 | Grad Norm: 0.00942942\n",
      "Epoch 2 | Step 1135000 | Avg Loss: 0.0153 | Grad Norm: 0.00991703\n",
      "Epoch 2 | Step 1135100 | Avg Loss: 0.0155 | Grad Norm: 0.00724851\n",
      "Epoch 2 | Step 1135200 | Avg Loss: 0.0156 | Grad Norm: 0.00812148\n",
      "Epoch 2 | Step 1135300 | Avg Loss: 0.0154 | Grad Norm: 0.00785662\n",
      "Epoch 2 | Step 1135400 | Avg Loss: 0.0152 | Grad Norm: 0.00915998\n",
      "Epoch 2 | Step 1135500 | Avg Loss: 0.0157 | Grad Norm: 0.00874581\n",
      "Epoch 2 | Step 1135600 | Avg Loss: 0.0157 | Grad Norm: 0.01125791\n",
      "Epoch 2 | Step 1135700 | Avg Loss: 0.0156 | Grad Norm: 0.00860745\n",
      "Epoch 2 | Step 1135800 | Avg Loss: 0.0157 | Grad Norm: 0.00888988\n",
      "Epoch 2 | Step 1135900 | Avg Loss: 0.0156 | Grad Norm: 0.00777191\n",
      "Epoch 2 | Step 1136000 | Avg Loss: 0.0155 | Grad Norm: 0.00726361\n",
      "Epoch 2 | Step 1136100 | Avg Loss: 0.0157 | Grad Norm: 0.00913028\n",
      "Epoch 2 | Step 1136200 | Avg Loss: 0.0156 | Grad Norm: 0.01031243\n",
      "Epoch 2 | Step 1136300 | Avg Loss: 0.0154 | Grad Norm: 0.00776107\n",
      "Epoch 2 | Step 1136400 | Avg Loss: 0.0151 | Grad Norm: 0.00802619\n",
      "Epoch 2 | Step 1136500 | Avg Loss: 0.0155 | Grad Norm: 0.00969084\n",
      "Epoch 2 | Step 1136600 | Avg Loss: 0.0153 | Grad Norm: 0.01039618\n",
      "Epoch 2 | Step 1136700 | Avg Loss: 0.0154 | Grad Norm: 0.00803290\n",
      "Epoch 2 | Step 1136800 | Avg Loss: 0.0152 | Grad Norm: 0.00942450\n",
      "Epoch 2 | Step 1136900 | Avg Loss: 0.0153 | Grad Norm: 0.00866009\n",
      "Epoch 2 | Step 1137000 | Avg Loss: 0.0151 | Grad Norm: 0.00838527\n",
      "Epoch 2 | Step 1137100 | Avg Loss: 0.0151 | Grad Norm: 0.00875083\n",
      "Epoch 2 | Step 1137200 | Avg Loss: 0.0153 | Grad Norm: 0.00916096\n",
      "Epoch 2 | Step 1137300 | Avg Loss: 0.0155 | Grad Norm: 0.00891825\n",
      "Epoch 2 | Step 1137400 | Avg Loss: 0.0156 | Grad Norm: 0.00823245\n",
      "Epoch 2 | Step 1137500 | Avg Loss: 0.0154 | Grad Norm: 0.01113930\n",
      "Epoch 2 | Step 1137600 | Avg Loss: 0.0158 | Grad Norm: 0.00712009\n",
      "Epoch 2 | Step 1137700 | Avg Loss: 0.0159 | Grad Norm: 0.00885375\n",
      "Epoch 2 | Step 1137800 | Avg Loss: 0.0155 | Grad Norm: 0.00920312\n",
      "Epoch 2 | Step 1137900 | Avg Loss: 0.0153 | Grad Norm: 0.00839175\n",
      "Epoch 2 | Step 1138000 | Avg Loss: 0.0152 | Grad Norm: 0.00844597\n",
      "Epoch 2 | Step 1138100 | Avg Loss: 0.0150 | Grad Norm: 0.00924909\n",
      "Epoch 2 | Step 1138200 | Avg Loss: 0.0149 | Grad Norm: 0.00896203\n",
      "Epoch 2 | Step 1138300 | Avg Loss: 0.0148 | Grad Norm: 0.00997207\n",
      "Epoch 2 | Step 1138400 | Avg Loss: 0.0151 | Grad Norm: 0.00725949\n",
      "Epoch 2 | Step 1138500 | Avg Loss: 0.0152 | Grad Norm: 0.00810099\n",
      "Epoch 2 | Step 1138600 | Avg Loss: 0.0150 | Grad Norm: 0.00795614\n",
      "Epoch 2 | Step 1138700 | Avg Loss: 0.0149 | Grad Norm: 0.00786080\n",
      "Epoch 2 | Step 1138800 | Avg Loss: 0.0148 | Grad Norm: 0.00831410\n",
      "Epoch 2 | Step 1138900 | Avg Loss: 0.0154 | Grad Norm: 0.00816304\n",
      "Epoch 2 | Step 1139000 | Avg Loss: 0.0151 | Grad Norm: 0.00968231\n",
      "Epoch 2 | Step 1139100 | Avg Loss: 0.0154 | Grad Norm: 0.00886879\n",
      "Epoch 2 | Step 1139200 | Avg Loss: 0.0152 | Grad Norm: 0.00940491\n",
      "Epoch 2 | Step 1139300 | Avg Loss: 0.0151 | Grad Norm: 0.00835272\n",
      "Epoch 2 | Step 1139400 | Avg Loss: 0.0153 | Grad Norm: 0.00774631\n",
      "Epoch 2 | Step 1139500 | Avg Loss: 0.0147 | Grad Norm: 0.00774065\n",
      "Epoch 2 | Step 1139600 | Avg Loss: 0.0150 | Grad Norm: 0.00919440\n",
      "Epoch 2 | Step 1139700 | Avg Loss: 0.0154 | Grad Norm: 0.00821997\n",
      "Epoch 2 | Step 1139800 | Avg Loss: 0.0156 | Grad Norm: 0.00939492\n",
      "Epoch 2 | Step 1139900 | Avg Loss: 0.0159 | Grad Norm: 0.00966571\n",
      "Epoch 2 | Step 1140000 | Avg Loss: 0.0160 | Grad Norm: 0.00850213\n",
      "Epoch 2 | Step 1140100 | Avg Loss: 0.0159 | Grad Norm: 0.00868828\n",
      "Epoch 2 | Step 1140200 | Avg Loss: 0.0157 | Grad Norm: 0.00924691\n",
      "Epoch 2 | Step 1140300 | Avg Loss: 0.0157 | Grad Norm: 0.00837884\n",
      "Epoch 2 | Step 1140400 | Avg Loss: 0.0155 | Grad Norm: 0.00879040\n",
      "Epoch 2 | Step 1140500 | Avg Loss: 0.0154 | Grad Norm: 0.00880692\n",
      "Epoch 2 | Step 1140600 | Avg Loss: 0.0153 | Grad Norm: 0.00834106\n",
      "Epoch 2 | Step 1140700 | Avg Loss: 0.0153 | Grad Norm: 0.00829727\n",
      "Epoch 2 | Step 1140800 | Avg Loss: 0.0153 | Grad Norm: 0.00947869\n",
      "Epoch 2 | Step 1140900 | Avg Loss: 0.0156 | Grad Norm: 0.00782134\n",
      "Epoch 2 | Step 1141000 | Avg Loss: 0.0155 | Grad Norm: 0.00898136\n",
      "Epoch 2 | Step 1141100 | Avg Loss: 0.0157 | Grad Norm: 0.00852744\n",
      "Epoch 2 | Step 1141200 | Avg Loss: 0.0159 | Grad Norm: 0.00880322\n",
      "Epoch 2 | Step 1141300 | Avg Loss: 0.0157 | Grad Norm: 0.00876304\n",
      "Epoch 2 | Step 1141400 | Avg Loss: 0.0157 | Grad Norm: 0.00732930\n",
      "Epoch 2 | Step 1141500 | Avg Loss: 0.0157 | Grad Norm: 0.00964548\n",
      "Epoch 2 | Step 1141600 | Avg Loss: 0.0151 | Grad Norm: 0.00855440\n",
      "Epoch 2 | Step 1141700 | Avg Loss: 0.0153 | Grad Norm: 0.00915902\n",
      "Epoch 2 | Step 1141800 | Avg Loss: 0.0155 | Grad Norm: 0.00905417\n",
      "Epoch 2 | Step 1141900 | Avg Loss: 0.0157 | Grad Norm: 0.00859389\n",
      "Epoch 2 | Step 1142000 | Avg Loss: 0.0159 | Grad Norm: 0.00895804\n",
      "Epoch 2 | Step 1142100 | Avg Loss: 0.0157 | Grad Norm: 0.00822932\n",
      "Epoch 2 | Step 1142200 | Avg Loss: 0.0154 | Grad Norm: 0.00837844\n",
      "Epoch 2 | Step 1142300 | Avg Loss: 0.0151 | Grad Norm: 0.00951104\n",
      "Epoch 2 | Step 1142400 | Avg Loss: 0.0156 | Grad Norm: 0.00906939\n",
      "Epoch 2 | Step 1142500 | Avg Loss: 0.0153 | Grad Norm: 0.00818446\n",
      "Epoch 2 | Step 1142600 | Avg Loss: 0.0157 | Grad Norm: 0.00871694\n",
      "Epoch 2 | Step 1142700 | Avg Loss: 0.0156 | Grad Norm: 0.00933902\n",
      "Epoch 2 | Step 1142800 | Avg Loss: 0.0153 | Grad Norm: 0.00959963\n",
      "Epoch 2 | Step 1142900 | Avg Loss: 0.0152 | Grad Norm: 0.00923733\n",
      "Epoch 2 | Step 1143000 | Avg Loss: 0.0155 | Grad Norm: 0.01060975\n",
      "Epoch 2 | Step 1143100 | Avg Loss: 0.0155 | Grad Norm: 0.00802215\n",
      "Epoch 2 | Step 1143200 | Avg Loss: 0.0158 | Grad Norm: 0.00942303\n",
      "Epoch 2 | Step 1143300 | Avg Loss: 0.0156 | Grad Norm: 0.00856599\n",
      "Epoch 2 | Step 1143400 | Avg Loss: 0.0155 | Grad Norm: 0.00876608\n",
      "Epoch 2 | Step 1143500 | Avg Loss: 0.0152 | Grad Norm: 0.00831433\n",
      "Epoch 2 | Step 1143600 | Avg Loss: 0.0151 | Grad Norm: 0.00766516\n",
      "Epoch 2 | Step 1143700 | Avg Loss: 0.0153 | Grad Norm: 0.00831033\n",
      "Epoch 2 | Step 1143800 | Avg Loss: 0.0154 | Grad Norm: 0.00845895\n",
      "Epoch 2 | Step 1143900 | Avg Loss: 0.0156 | Grad Norm: 0.00844713\n",
      "Epoch 2 | Step 1144000 | Avg Loss: 0.0154 | Grad Norm: 0.00950774\n",
      "Epoch 2 | Step 1144100 | Avg Loss: 0.0152 | Grad Norm: 0.00895617\n",
      "Epoch 2 | Step 1144200 | Avg Loss: 0.0150 | Grad Norm: 0.01000153\n",
      "Epoch 2 | Step 1144300 | Avg Loss: 0.0154 | Grad Norm: 0.00831129\n",
      "Epoch 2 | Step 1144400 | Avg Loss: 0.0157 | Grad Norm: 0.00850154\n",
      "Epoch 2 | Step 1144500 | Avg Loss: 0.0157 | Grad Norm: 0.00848937\n",
      "Epoch 2 | Step 1144600 | Avg Loss: 0.0159 | Grad Norm: 0.01034490\n",
      "Epoch 2 | Step 1144700 | Avg Loss: 0.0156 | Grad Norm: 0.00864990\n",
      "Epoch 2 | Step 1144800 | Avg Loss: 0.0154 | Grad Norm: 0.00925745\n",
      "Epoch 2 | Step 1144900 | Avg Loss: 0.0156 | Grad Norm: 0.00926512\n",
      "Epoch 2 | Step 1145000 | Avg Loss: 0.0157 | Grad Norm: 0.00839609\n",
      "Epoch 2 | Step 1145100 | Avg Loss: 0.0153 | Grad Norm: 0.00859726\n",
      "Epoch 2 | Step 1145200 | Avg Loss: 0.0156 | Grad Norm: 0.00838929\n",
      "Epoch 2 | Step 1145300 | Avg Loss: 0.0157 | Grad Norm: 0.00888770\n",
      "Epoch 2 | Step 1145400 | Avg Loss: 0.0154 | Grad Norm: 0.00943593\n",
      "Epoch 2 | Step 1145500 | Avg Loss: 0.0153 | Grad Norm: 0.00943140\n",
      "Epoch 2 | Step 1145600 | Avg Loss: 0.0156 | Grad Norm: 0.00899592\n",
      "Epoch 2 | Step 1145700 | Avg Loss: 0.0159 | Grad Norm: 0.01043982\n",
      "Epoch 2 | Step 1145800 | Avg Loss: 0.0158 | Grad Norm: 0.00923304\n",
      "Epoch 2 | Step 1145900 | Avg Loss: 0.0158 | Grad Norm: 0.00957682\n",
      "Epoch 2 | Step 1146000 | Avg Loss: 0.0158 | Grad Norm: 0.00895480\n",
      "Epoch 2 | Step 1146100 | Avg Loss: 0.0160 | Grad Norm: 0.01029155\n",
      "Epoch 2 | Step 1146200 | Avg Loss: 0.0158 | Grad Norm: 0.01008014\n",
      "Epoch 2 | Step 1146300 | Avg Loss: 0.0156 | Grad Norm: 0.00925681\n",
      "Epoch 2 | Step 1146400 | Avg Loss: 0.0158 | Grad Norm: 0.00842524\n",
      "Epoch 2 | Step 1146500 | Avg Loss: 0.0156 | Grad Norm: 0.00785233\n",
      "Epoch 2 | Step 1146600 | Avg Loss: 0.0156 | Grad Norm: 0.00839469\n",
      "Epoch 2 | Step 1146700 | Avg Loss: 0.0155 | Grad Norm: 0.01113441\n",
      "Epoch 2 | Step 1146800 | Avg Loss: 0.0151 | Grad Norm: 0.00963027\n",
      "Epoch 2 | Step 1146900 | Avg Loss: 0.0152 | Grad Norm: 0.01021052\n",
      "Epoch 2 | Step 1147000 | Avg Loss: 0.0154 | Grad Norm: 0.00771708\n",
      "Epoch 2 | Step 1147100 | Avg Loss: 0.0157 | Grad Norm: 0.00955810\n",
      "Epoch 2 | Step 1147200 | Avg Loss: 0.0155 | Grad Norm: 0.00820489\n",
      "Epoch 2 | Step 1147300 | Avg Loss: 0.0156 | Grad Norm: 0.01196956\n",
      "Epoch 2 | Step 1147400 | Avg Loss: 0.0154 | Grad Norm: 0.00877654\n",
      "Epoch 2 | Step 1147500 | Avg Loss: 0.0156 | Grad Norm: 0.00858058\n",
      "Epoch 2 | Step 1147600 | Avg Loss: 0.0155 | Grad Norm: 0.00970952\n",
      "Epoch 2 | Step 1147700 | Avg Loss: 0.0156 | Grad Norm: 0.00872561\n",
      "Epoch 2 | Step 1147800 | Avg Loss: 0.0155 | Grad Norm: 0.00981626\n",
      "Epoch 2 | Step 1147900 | Avg Loss: 0.0155 | Grad Norm: 0.01072916\n",
      "Epoch 2 | Step 1148000 | Avg Loss: 0.0152 | Grad Norm: 0.00995431\n",
      "Epoch 2 | Step 1148100 | Avg Loss: 0.0151 | Grad Norm: 0.00895060\n",
      "Epoch 2 | Step 1148200 | Avg Loss: 0.0153 | Grad Norm: 0.01106410\n",
      "Epoch 2 | Step 1148300 | Avg Loss: 0.0153 | Grad Norm: 0.00797023\n",
      "Epoch 2 | Step 1148400 | Avg Loss: 0.0156 | Grad Norm: 0.00905851\n",
      "Epoch 2 | Step 1148500 | Avg Loss: 0.0156 | Grad Norm: 0.01095734\n",
      "Epoch 2 | Step 1148600 | Avg Loss: 0.0153 | Grad Norm: 0.00928295\n",
      "Epoch 2 | Step 1148700 | Avg Loss: 0.0153 | Grad Norm: 0.00949257\n",
      "Epoch 2 | Step 1148800 | Avg Loss: 0.0156 | Grad Norm: 0.00770623\n",
      "Epoch 2 | Step 1148900 | Avg Loss: 0.0157 | Grad Norm: 0.00861373\n",
      "Epoch 2 | Step 1149000 | Avg Loss: 0.0153 | Grad Norm: 0.00809119\n",
      "Epoch 2 | Step 1149100 | Avg Loss: 0.0154 | Grad Norm: 0.00823873\n",
      "Epoch 2 | Step 1149200 | Avg Loss: 0.0156 | Grad Norm: 0.00862994\n",
      "Epoch 2 | Step 1149300 | Avg Loss: 0.0157 | Grad Norm: 0.00813776\n",
      "Epoch 2 | Step 1149400 | Avg Loss: 0.0157 | Grad Norm: 0.00754587\n",
      "Epoch 2 | Step 1149500 | Avg Loss: 0.0158 | Grad Norm: 0.00853867\n",
      "Epoch 2 | Step 1149600 | Avg Loss: 0.0158 | Grad Norm: 0.00934704\n",
      "Epoch 2 | Step 1149700 | Avg Loss: 0.0157 | Grad Norm: 0.00828141\n",
      "Epoch 2 | Step 1149800 | Avg Loss: 0.0160 | Grad Norm: 0.01027289\n",
      "Epoch 2 | Step 1149900 | Avg Loss: 0.0155 | Grad Norm: 0.00810124\n",
      "Epoch 2 | Step 1150000 | Avg Loss: 0.0156 | Grad Norm: 0.00860192\n",
      "Epoch 2 | Step 1150100 | Avg Loss: 0.0153 | Grad Norm: 0.00818365\n",
      "Epoch 2 | Step 1150200 | Avg Loss: 0.0153 | Grad Norm: 0.00853549\n",
      "Epoch 2 | Step 1150300 | Avg Loss: 0.0152 | Grad Norm: 0.00845002\n",
      "Epoch 2 | Step 1150400 | Avg Loss: 0.0152 | Grad Norm: 0.00897443\n",
      "Epoch 2 | Step 1150500 | Avg Loss: 0.0153 | Grad Norm: 0.00925556\n",
      "Epoch 2 | Step 1150600 | Avg Loss: 0.0156 | Grad Norm: 0.00812934\n",
      "Epoch 2 | Step 1150700 | Avg Loss: 0.0158 | Grad Norm: 0.01063801\n",
      "Epoch 2 | Step 1150800 | Avg Loss: 0.0159 | Grad Norm: 0.00728754\n",
      "Epoch 2 | Step 1150900 | Avg Loss: 0.0159 | Grad Norm: 0.00986497\n",
      "Epoch 2 | Step 1151000 | Avg Loss: 0.0158 | Grad Norm: 0.00987152\n",
      "Epoch 2 | Step 1151100 | Avg Loss: 0.0154 | Grad Norm: 0.00806829\n",
      "Epoch 2 | Step 1151200 | Avg Loss: 0.0158 | Grad Norm: 0.01032788\n",
      "Epoch 2 | Step 1151300 | Avg Loss: 0.0154 | Grad Norm: 0.00825186\n",
      "Epoch 2 | Step 1151400 | Avg Loss: 0.0150 | Grad Norm: 0.00886017\n",
      "Epoch 2 | Step 1151500 | Avg Loss: 0.0154 | Grad Norm: 0.00868053\n",
      "Epoch 2 | Step 1151600 | Avg Loss: 0.0152 | Grad Norm: 0.00771992\n",
      "Epoch 2 | Step 1151700 | Avg Loss: 0.0148 | Grad Norm: 0.00964182\n",
      "Epoch 2 | Step 1151800 | Avg Loss: 0.0145 | Grad Norm: 0.00927241\n",
      "Epoch 2 | Step 1151900 | Avg Loss: 0.0146 | Grad Norm: 0.00925819\n",
      "Epoch 2 | Step 1152000 | Avg Loss: 0.0148 | Grad Norm: 0.00859956\n",
      "Epoch 2 | Step 1152100 | Avg Loss: 0.0151 | Grad Norm: 0.00892360\n",
      "Epoch 2 | Step 1152200 | Avg Loss: 0.0151 | Grad Norm: 0.00778817\n",
      "Epoch 2 | Step 1152300 | Avg Loss: 0.0153 | Grad Norm: 0.00976069\n",
      "Epoch 2 | Step 1152400 | Avg Loss: 0.0154 | Grad Norm: 0.00824875\n",
      "Epoch 2 | Step 1152500 | Avg Loss: 0.0154 | Grad Norm: 0.00750696\n",
      "Epoch 2 | Step 1152600 | Avg Loss: 0.0155 | Grad Norm: 0.01505175\n",
      "Epoch 2 | Step 1152700 | Avg Loss: 0.0155 | Grad Norm: 0.00841032\n",
      "Epoch 2 | Step 1152800 | Avg Loss: 0.0153 | Grad Norm: 0.00828567\n",
      "Epoch 2 | Step 1152900 | Avg Loss: 0.0154 | Grad Norm: 0.00827021\n",
      "Epoch 2 | Step 1153000 | Avg Loss: 0.0155 | Grad Norm: 0.00875328\n",
      "Epoch 2 | Step 1153100 | Avg Loss: 0.0154 | Grad Norm: 0.00860607\n",
      "Epoch 2 | Step 1153200 | Avg Loss: 0.0148 | Grad Norm: 0.00919281\n",
      "Epoch 2 | Step 1153300 | Avg Loss: 0.0147 | Grad Norm: 0.00812724\n",
      "Epoch 2 | Step 1153400 | Avg Loss: 0.0151 | Grad Norm: 0.00908895\n",
      "Epoch 2 | Step 1153500 | Avg Loss: 0.0153 | Grad Norm: 0.01190872\n",
      "Epoch 2 | Step 1153600 | Avg Loss: 0.0155 | Grad Norm: 0.00849748\n",
      "Epoch 2 | Step 1153700 | Avg Loss: 0.0158 | Grad Norm: 0.00868677\n",
      "Epoch 2 | Step 1153800 | Avg Loss: 0.0156 | Grad Norm: 0.00777923\n",
      "Epoch 2 | Step 1153900 | Avg Loss: 0.0154 | Grad Norm: 0.00883411\n",
      "Epoch 2 | Step 1154000 | Avg Loss: 0.0153 | Grad Norm: 0.00764939\n",
      "Epoch 2 | Step 1154100 | Avg Loss: 0.0149 | Grad Norm: 0.00912099\n",
      "Epoch 2 | Step 1154200 | Avg Loss: 0.0151 | Grad Norm: 0.00837509\n",
      "Epoch 2 | Step 1154300 | Avg Loss: 0.0153 | Grad Norm: 0.00920027\n",
      "Epoch 2 | Step 1154400 | Avg Loss: 0.0157 | Grad Norm: 0.01016999\n",
      "Epoch 2 | Step 1154500 | Avg Loss: 0.0156 | Grad Norm: 0.00756751\n",
      "Epoch 2 | Step 1154600 | Avg Loss: 0.0155 | Grad Norm: 0.00804572\n",
      "Epoch 2 | Step 1154700 | Avg Loss: 0.0157 | Grad Norm: 0.00875502\n",
      "Epoch 2 | Step 1154800 | Avg Loss: 0.0156 | Grad Norm: 0.01175042\n",
      "Epoch 2 | Step 1154900 | Avg Loss: 0.0154 | Grad Norm: 0.00811313\n",
      "Epoch 2 | Step 1155000 | Avg Loss: 0.0156 | Grad Norm: 0.00859172\n",
      "Epoch 2 | Step 1155100 | Avg Loss: 0.0154 | Grad Norm: 0.00764215\n",
      "Epoch 2 | Step 1155200 | Avg Loss: 0.0158 | Grad Norm: 0.00857852\n",
      "Epoch 2 | Step 1155300 | Avg Loss: 0.0155 | Grad Norm: 0.00851571\n",
      "Epoch 2 | Step 1155400 | Avg Loss: 0.0152 | Grad Norm: 0.01031541\n",
      "Epoch 2 | Step 1155500 | Avg Loss: 0.0149 | Grad Norm: 0.01009729\n",
      "Epoch 2 | Step 1155600 | Avg Loss: 0.0153 | Grad Norm: 0.00967647\n",
      "Epoch 2 | Step 1155700 | Avg Loss: 0.0151 | Grad Norm: 0.00828935\n",
      "Epoch 2 | Step 1155800 | Avg Loss: 0.0153 | Grad Norm: 0.00835140\n",
      "Epoch 2 | Step 1155900 | Avg Loss: 0.0156 | Grad Norm: 0.00740275\n",
      "Epoch 2 | Step 1156000 | Avg Loss: 0.0157 | Grad Norm: 0.00784998\n",
      "Epoch 2 | Step 1156100 | Avg Loss: 0.0155 | Grad Norm: 0.00829582\n",
      "Epoch 2 | Step 1156200 | Avg Loss: 0.0153 | Grad Norm: 0.00965703\n",
      "Epoch 2 | Step 1156300 | Avg Loss: 0.0152 | Grad Norm: 0.00852777\n",
      "Epoch 2 | Step 1156400 | Avg Loss: 0.0155 | Grad Norm: 0.00839805\n",
      "Epoch 2 | Step 1156500 | Avg Loss: 0.0152 | Grad Norm: 0.00814368\n",
      "Epoch 2 | Step 1156600 | Avg Loss: 0.0155 | Grad Norm: 0.00799477\n",
      "Epoch 2 | Step 1156700 | Avg Loss: 0.0150 | Grad Norm: 0.00814094\n",
      "Epoch 2 | Step 1156800 | Avg Loss: 0.0152 | Grad Norm: 0.00847960\n",
      "Epoch 2 | Step 1156900 | Avg Loss: 0.0154 | Grad Norm: 0.00881397\n",
      "Epoch 2 | Step 1157000 | Avg Loss: 0.0157 | Grad Norm: 0.00848087\n",
      "Epoch 2 | Step 1157100 | Avg Loss: 0.0159 | Grad Norm: 0.00883402\n",
      "Epoch 2 | Step 1157200 | Avg Loss: 0.0158 | Grad Norm: 0.00926216\n",
      "Epoch 2 | Step 1157300 | Avg Loss: 0.0158 | Grad Norm: 0.00985268\n",
      "Epoch 2 | Step 1157400 | Avg Loss: 0.0161 | Grad Norm: 0.00881059\n",
      "Epoch 2 | Step 1157500 | Avg Loss: 0.0167 | Grad Norm: 0.01035473\n",
      "Epoch 2 | Step 1157600 | Avg Loss: 0.0167 | Grad Norm: 0.01093842\n",
      "Epoch 2 | Step 1157700 | Avg Loss: 0.0165 | Grad Norm: 0.00836273\n",
      "Epoch 2 | Step 1157800 | Avg Loss: 0.0165 | Grad Norm: 0.00868838\n",
      "Epoch 2 | Step 1157900 | Avg Loss: 0.0163 | Grad Norm: 0.00849531\n",
      "Epoch 2 | Step 1158000 | Avg Loss: 0.0164 | Grad Norm: 0.00917251\n",
      "Epoch 2 | Step 1158100 | Avg Loss: 0.0164 | Grad Norm: 0.00977185\n",
      "Epoch 2 | Step 1158200 | Avg Loss: 0.0164 | Grad Norm: 0.00889160\n",
      "Epoch 2 | Step 1158300 | Avg Loss: 0.0164 | Grad Norm: 0.01011570\n",
      "Epoch 2 | Step 1158400 | Avg Loss: 0.0160 | Grad Norm: 0.00910437\n",
      "Epoch 2 | Step 1158500 | Avg Loss: 0.0156 | Grad Norm: 0.00950996\n",
      "Epoch 2 | Step 1158600 | Avg Loss: 0.0155 | Grad Norm: 0.00818213\n",
      "Epoch 2 | Step 1158700 | Avg Loss: 0.0154 | Grad Norm: 0.00783880\n",
      "Epoch 2 | Step 1158800 | Avg Loss: 0.0156 | Grad Norm: 0.00760338\n",
      "Epoch 2 | Step 1158900 | Avg Loss: 0.0156 | Grad Norm: 0.00926213\n",
      "Epoch 2 | Step 1159000 | Avg Loss: 0.0155 | Grad Norm: 0.00727950\n",
      "Epoch 2 | Step 1159100 | Avg Loss: 0.0157 | Grad Norm: 0.00928256\n",
      "Epoch 2 | Step 1159200 | Avg Loss: 0.0157 | Grad Norm: 0.00790364\n",
      "Epoch 2 | Step 1159300 | Avg Loss: 0.0155 | Grad Norm: 0.00814167\n",
      "Epoch 2 | Step 1159400 | Avg Loss: 0.0155 | Grad Norm: 0.01067053\n",
      "Epoch 2 | Step 1159500 | Avg Loss: 0.0156 | Grad Norm: 0.01008272\n",
      "Epoch 2 | Step 1159600 | Avg Loss: 0.0155 | Grad Norm: 0.00824214\n",
      "Epoch 2 | Step 1159700 | Avg Loss: 0.0153 | Grad Norm: 0.00891290\n",
      "Epoch 2 | Step 1159800 | Avg Loss: 0.0154 | Grad Norm: 0.00783287\n",
      "Epoch 2 | Step 1159900 | Avg Loss: 0.0158 | Grad Norm: 0.01195067\n",
      "Epoch 2 | Step 1160000 | Avg Loss: 0.0160 | Grad Norm: 0.00990538\n",
      "Epoch 2 | Step 1160100 | Avg Loss: 0.0157 | Grad Norm: 0.00886751\n",
      "Epoch 2 | Step 1160200 | Avg Loss: 0.0156 | Grad Norm: 0.01111507\n",
      "Epoch 2 | Step 1160300 | Avg Loss: 0.0159 | Grad Norm: 0.00881161\n",
      "Epoch 2 | Step 1160400 | Avg Loss: 0.0161 | Grad Norm: 0.00900028\n",
      "Epoch 2 | Step 1160500 | Avg Loss: 0.0158 | Grad Norm: 0.00970075\n",
      "Epoch 2 | Step 1160600 | Avg Loss: 0.0158 | Grad Norm: 0.00951985\n",
      "Epoch 2 | Step 1160700 | Avg Loss: 0.0158 | Grad Norm: 0.00936225\n",
      "Epoch 2 | Step 1160800 | Avg Loss: 0.0159 | Grad Norm: 0.00947404\n",
      "Epoch 2 | Step 1160900 | Avg Loss: 0.0157 | Grad Norm: 0.00839835\n",
      "Epoch 2 | Step 1161000 | Avg Loss: 0.0156 | Grad Norm: 0.01163436\n",
      "Epoch 2 | Step 1161100 | Avg Loss: 0.0157 | Grad Norm: 0.00873133\n",
      "Epoch 2 | Step 1161200 | Avg Loss: 0.0155 | Grad Norm: 0.00891316\n",
      "Epoch 2 | Step 1161300 | Avg Loss: 0.0154 | Grad Norm: 0.00891408\n",
      "Epoch 2 | Step 1161400 | Avg Loss: 0.0155 | Grad Norm: 0.00953672\n",
      "Epoch 2 | Step 1161500 | Avg Loss: 0.0157 | Grad Norm: 0.00986354\n",
      "Epoch 2 | Step 1161600 | Avg Loss: 0.0157 | Grad Norm: 0.00951843\n",
      "Epoch 2 | Step 1161700 | Avg Loss: 0.0156 | Grad Norm: 0.00820142\n",
      "Epoch 2 | Step 1161800 | Avg Loss: 0.0152 | Grad Norm: 0.01129616\n",
      "Epoch 2 | Step 1161900 | Avg Loss: 0.0156 | Grad Norm: 0.00795006\n",
      "Epoch 2 | Step 1162000 | Avg Loss: 0.0159 | Grad Norm: 0.00765000\n",
      "Epoch 2 | Step 1162100 | Avg Loss: 0.0164 | Grad Norm: 0.00994354\n",
      "Epoch 2 | Step 1162200 | Avg Loss: 0.0161 | Grad Norm: 0.00927980\n",
      "Epoch 2 | Step 1162300 | Avg Loss: 0.0161 | Grad Norm: 0.00932538\n",
      "Epoch 2 | Step 1162400 | Avg Loss: 0.0159 | Grad Norm: 0.00872193\n",
      "Epoch 2 | Step 1162500 | Avg Loss: 0.0156 | Grad Norm: 0.00893402\n",
      "Epoch 2 | Step 1162600 | Avg Loss: 0.0155 | Grad Norm: 0.00900339\n",
      "Epoch 2 | Step 1162700 | Avg Loss: 0.0154 | Grad Norm: 0.00798142\n",
      "Epoch 2 | Step 1162800 | Avg Loss: 0.0157 | Grad Norm: 0.00893656\n",
      "Epoch 2 | Step 1162900 | Avg Loss: 0.0157 | Grad Norm: 0.00986283\n",
      "Epoch 2 | Step 1163000 | Avg Loss: 0.0156 | Grad Norm: 0.00931996\n",
      "Epoch 2 | Step 1163100 | Avg Loss: 0.0155 | Grad Norm: 0.00831183\n",
      "Epoch 2 | Step 1163200 | Avg Loss: 0.0154 | Grad Norm: 0.00859828\n",
      "Epoch 2 | Step 1163300 | Avg Loss: 0.0152 | Grad Norm: 0.00725432\n",
      "Epoch 2 | Step 1163400 | Avg Loss: 0.0158 | Grad Norm: 0.00725681\n",
      "Epoch 2 | Step 1163500 | Avg Loss: 0.0161 | Grad Norm: 0.01044872\n",
      "Epoch 2 | Step 1163600 | Avg Loss: 0.0158 | Grad Norm: 0.00956471\n",
      "Epoch 2 | Step 1163700 | Avg Loss: 0.0158 | Grad Norm: 0.00912120\n",
      "Epoch 2 | Step 1163800 | Avg Loss: 0.0157 | Grad Norm: 0.00916284\n",
      "Epoch 2 | Step 1163900 | Avg Loss: 0.0155 | Grad Norm: 0.00980930\n",
      "Epoch 2 | Step 1164000 | Avg Loss: 0.0155 | Grad Norm: 0.00909613\n",
      "Epoch 2 | Step 1164100 | Avg Loss: 0.0153 | Grad Norm: 0.00808697\n",
      "Epoch 2 | Step 1164200 | Avg Loss: 0.0155 | Grad Norm: 0.00822320\n",
      "Epoch 2 | Step 1164300 | Avg Loss: 0.0157 | Grad Norm: 0.00887991\n",
      "Epoch 2 | Step 1164400 | Avg Loss: 0.0156 | Grad Norm: 0.00854661\n",
      "Epoch 2 | Step 1164500 | Avg Loss: 0.0154 | Grad Norm: 0.01040228\n",
      "Epoch 2 | Step 1164600 | Avg Loss: 0.0154 | Grad Norm: 0.00918870\n",
      "Epoch 2 | Step 1164700 | Avg Loss: 0.0152 | Grad Norm: 0.00954789\n",
      "Epoch 2 | Step 1164800 | Avg Loss: 0.0154 | Grad Norm: 0.00963229\n",
      "Epoch 2 | Step 1164900 | Avg Loss: 0.0153 | Grad Norm: 0.00787271\n",
      "Epoch 2 | Step 1165000 | Avg Loss: 0.0152 | Grad Norm: 0.00753349\n",
      "Epoch 2 | Step 1165100 | Avg Loss: 0.0149 | Grad Norm: 0.00886628\n",
      "Epoch 2 | Step 1165200 | Avg Loss: 0.0151 | Grad Norm: 0.01061958\n",
      "Epoch 2 | Step 1165300 | Avg Loss: 0.0151 | Grad Norm: 0.00935444\n",
      "Epoch 2 | Step 1165400 | Avg Loss: 0.0153 | Grad Norm: 0.00801115\n",
      "Epoch 2 | Step 1165500 | Avg Loss: 0.0153 | Grad Norm: 0.00869238\n",
      "Epoch 2 | Step 1165600 | Avg Loss: 0.0154 | Grad Norm: 0.00846153\n",
      "Epoch 2 | Step 1165700 | Avg Loss: 0.0156 | Grad Norm: 0.00945068\n",
      "Epoch 2 | Step 1165800 | Avg Loss: 0.0159 | Grad Norm: 0.00886746\n",
      "Epoch 2 | Step 1165900 | Avg Loss: 0.0158 | Grad Norm: 0.00918130\n",
      "Epoch 2 | Step 1166000 | Avg Loss: 0.0160 | Grad Norm: 0.00849301\n",
      "Epoch 2 | Step 1166100 | Avg Loss: 0.0159 | Grad Norm: 0.01170370\n",
      "Epoch 2 | Step 1166200 | Avg Loss: 0.0157 | Grad Norm: 0.00760456\n",
      "Epoch 2 | Step 1166300 | Avg Loss: 0.0153 | Grad Norm: 0.00847579\n",
      "Epoch 2 | Step 1166400 | Avg Loss: 0.0155 | Grad Norm: 0.00842101\n",
      "Epoch 2 | Step 1166500 | Avg Loss: 0.0154 | Grad Norm: 0.00960879\n",
      "Epoch 2 | Step 1166600 | Avg Loss: 0.0156 | Grad Norm: 0.00893698\n",
      "Epoch 2 | Step 1166700 | Avg Loss: 0.0156 | Grad Norm: 0.00871964\n",
      "Epoch 2 | Step 1166800 | Avg Loss: 0.0156 | Grad Norm: 0.00922089\n",
      "Epoch 2 | Step 1166900 | Avg Loss: 0.0157 | Grad Norm: 0.00887464\n",
      "Epoch 2 | Step 1167000 | Avg Loss: 0.0155 | Grad Norm: 0.00829617\n",
      "Epoch 2 | Step 1167100 | Avg Loss: 0.0154 | Grad Norm: 0.01103156\n",
      "Epoch 2 | Step 1167200 | Avg Loss: 0.0158 | Grad Norm: 0.01154366\n",
      "Epoch 2 | Step 1167300 | Avg Loss: 0.0153 | Grad Norm: 0.00889548\n",
      "Epoch 2 | Step 1167400 | Avg Loss: 0.0151 | Grad Norm: 0.01095822\n",
      "Epoch 2 | Step 1167500 | Avg Loss: 0.0151 | Grad Norm: 0.00882812\n",
      "Epoch 2 | Step 1167600 | Avg Loss: 0.0152 | Grad Norm: 0.00767240\n",
      "Epoch 2 | Step 1167700 | Avg Loss: 0.0151 | Grad Norm: 0.00879333\n",
      "Epoch 2 | Step 1167800 | Avg Loss: 0.0151 | Grad Norm: 0.00871990\n",
      "Epoch 2 | Step 1167900 | Avg Loss: 0.0150 | Grad Norm: 0.00833028\n",
      "Epoch 2 | Step 1168000 | Avg Loss: 0.0150 | Grad Norm: 0.01001318\n",
      "Epoch 2 | Step 1168100 | Avg Loss: 0.0149 | Grad Norm: 0.00877852\n",
      "Epoch 2 | Step 1168200 | Avg Loss: 0.0154 | Grad Norm: 0.00850140\n",
      "Epoch 2 | Step 1168300 | Avg Loss: 0.0150 | Grad Norm: 0.00903640\n",
      "Epoch 2 | Step 1168400 | Avg Loss: 0.0150 | Grad Norm: 0.01035639\n",
      "Epoch 2 | Step 1168500 | Avg Loss: 0.0150 | Grad Norm: 0.00911455\n",
      "Epoch 2 | Step 1168600 | Avg Loss: 0.0151 | Grad Norm: 0.00882605\n",
      "Epoch 2 | Step 1168700 | Avg Loss: 0.0153 | Grad Norm: 0.00908446\n",
      "Epoch 2 | Step 1168800 | Avg Loss: 0.0156 | Grad Norm: 0.01346374\n",
      "Epoch 2 | Step 1168900 | Avg Loss: 0.0160 | Grad Norm: 0.01009501\n",
      "Epoch 2 | Step 1169000 | Avg Loss: 0.0159 | Grad Norm: 0.00885454\n",
      "Epoch 2 | Step 1169100 | Avg Loss: 0.0157 | Grad Norm: 0.00802340\n",
      "Epoch 2 | Step 1169200 | Avg Loss: 0.0157 | Grad Norm: 0.01042190\n",
      "Epoch 2 | Step 1169300 | Avg Loss: 0.0159 | Grad Norm: 0.00950300\n",
      "Epoch 2 | Step 1169400 | Avg Loss: 0.0159 | Grad Norm: 0.00902209\n",
      "Epoch 2 | Step 1169500 | Avg Loss: 0.0157 | Grad Norm: 0.00799201\n",
      "Epoch 2 | Step 1169600 | Avg Loss: 0.0161 | Grad Norm: 0.00934223\n",
      "Epoch 2 | Step 1169700 | Avg Loss: 0.0160 | Grad Norm: 0.00962559\n",
      "Epoch 2 | Step 1169800 | Avg Loss: 0.0157 | Grad Norm: 0.00789869\n",
      "Epoch 2 | Step 1169900 | Avg Loss: 0.0161 | Grad Norm: 0.00849869\n",
      "Epoch 2 | Step 1170000 | Avg Loss: 0.0162 | Grad Norm: 0.00940358\n",
      "Epoch 2 | Step 1170100 | Avg Loss: 0.0162 | Grad Norm: 0.00928798\n",
      "Epoch 2 | Step 1170200 | Avg Loss: 0.0159 | Grad Norm: 0.00961790\n",
      "Epoch 2 | Step 1170300 | Avg Loss: 0.0156 | Grad Norm: 0.00886966\n",
      "Epoch 2 | Step 1170400 | Avg Loss: 0.0158 | Grad Norm: 0.00893279\n",
      "Epoch 2 | Step 1170500 | Avg Loss: 0.0158 | Grad Norm: 0.00800949\n",
      "Epoch 2 | Step 1170600 | Avg Loss: 0.0158 | Grad Norm: 0.00812389\n",
      "Epoch 2 | Step 1170700 | Avg Loss: 0.0161 | Grad Norm: 0.01302164\n",
      "Epoch 2 | Step 1170800 | Avg Loss: 0.0159 | Grad Norm: 0.00940901\n",
      "Epoch 2 | Step 1170900 | Avg Loss: 0.0159 | Grad Norm: 0.00854213\n",
      "Epoch 2 | Step 1171000 | Avg Loss: 0.0159 | Grad Norm: 0.01071511\n",
      "Epoch 2 | Step 1171100 | Avg Loss: 0.0156 | Grad Norm: 0.00765815\n",
      "Epoch 2 | Step 1171200 | Avg Loss: 0.0156 | Grad Norm: 0.00962708\n",
      "Epoch 2 | Step 1171300 | Avg Loss: 0.0157 | Grad Norm: 0.00840858\n",
      "Epoch 2 | Step 1171400 | Avg Loss: 0.0163 | Grad Norm: 0.01194766\n",
      "Epoch 2 | Step 1171500 | Avg Loss: 0.0163 | Grad Norm: 0.00821316\n",
      "Epoch 2 | Step 1171600 | Avg Loss: 0.0163 | Grad Norm: 0.00791776\n",
      "Epoch 2 | Step 1171700 | Avg Loss: 0.0166 | Grad Norm: 0.00830217\n",
      "Epoch 2 | Step 1171800 | Avg Loss: 0.0165 | Grad Norm: 0.00906363\n",
      "Epoch 2 | Step 1171900 | Avg Loss: 0.0164 | Grad Norm: 0.00838024\n",
      "Epoch 2 | Step 1172000 | Avg Loss: 0.0158 | Grad Norm: 0.01051009\n",
      "Epoch 2 | Step 1172100 | Avg Loss: 0.0160 | Grad Norm: 0.00824670\n",
      "Epoch 2 | Step 1172200 | Avg Loss: 0.0160 | Grad Norm: 0.00761988\n",
      "Epoch 2 | Step 1172300 | Avg Loss: 0.0160 | Grad Norm: 0.01007064\n",
      "Epoch 2 | Step 1172400 | Avg Loss: 0.0161 | Grad Norm: 0.00976869\n",
      "Epoch 2 | Step 1172500 | Avg Loss: 0.0160 | Grad Norm: 0.00868277\n",
      "Epoch 2 | Step 1172600 | Avg Loss: 0.0162 | Grad Norm: 0.00866851\n",
      "Epoch 2 | Step 1172700 | Avg Loss: 0.0162 | Grad Norm: 0.00922292\n",
      "Epoch 2 | Step 1172800 | Avg Loss: 0.0161 | Grad Norm: 0.00875779\n",
      "Epoch 2 | Step 1172900 | Avg Loss: 0.0159 | Grad Norm: 0.00830471\n",
      "Epoch 2 | Step 1173000 | Avg Loss: 0.0160 | Grad Norm: 0.01007465\n",
      "Epoch 2 | Step 1173100 | Avg Loss: 0.0161 | Grad Norm: 0.00750974\n",
      "Epoch 2 | Step 1173200 | Avg Loss: 0.0156 | Grad Norm: 0.00935289\n",
      "Epoch 2 | Step 1173300 | Avg Loss: 0.0158 | Grad Norm: 0.00835297\n",
      "Epoch 2 | Step 1173400 | Avg Loss: 0.0159 | Grad Norm: 0.00845305\n",
      "Epoch 2 | Step 1173500 | Avg Loss: 0.0156 | Grad Norm: 0.00935795\n",
      "Epoch 2 | Step 1173600 | Avg Loss: 0.0157 | Grad Norm: 0.00831401\n",
      "Epoch 2 | Step 1173700 | Avg Loss: 0.0156 | Grad Norm: 0.00890735\n",
      "Epoch 2 | Step 1173800 | Avg Loss: 0.0156 | Grad Norm: 0.00988324\n",
      "Epoch 2 | Step 1173900 | Avg Loss: 0.0157 | Grad Norm: 0.00865988\n",
      "Epoch 2 | Step 1174000 | Avg Loss: 0.0156 | Grad Norm: 0.00855732\n",
      "Epoch 2 | Step 1174100 | Avg Loss: 0.0156 | Grad Norm: 0.00916063\n",
      "Epoch 2 | Step 1174200 | Avg Loss: 0.0156 | Grad Norm: 0.00779644\n",
      "Epoch 2 | Step 1174300 | Avg Loss: 0.0160 | Grad Norm: 0.00818907\n",
      "Epoch 2 | Step 1174400 | Avg Loss: 0.0159 | Grad Norm: 0.00964231\n",
      "Epoch 2 | Step 1174500 | Avg Loss: 0.0158 | Grad Norm: 0.00895610\n",
      "Epoch 2 | Step 1174600 | Avg Loss: 0.0157 | Grad Norm: 0.00887577\n",
      "Epoch 2 | Step 1174700 | Avg Loss: 0.0159 | Grad Norm: 0.00950597\n",
      "Epoch 2 | Step 1174800 | Avg Loss: 0.0160 | Grad Norm: 0.00845070\n",
      "Epoch 2 | Step 1174900 | Avg Loss: 0.0158 | Grad Norm: 0.00861282\n",
      "Epoch 2 | Step 1175000 | Avg Loss: 0.0160 | Grad Norm: 0.00768799\n",
      "Epoch 2 | Step 1175100 | Avg Loss: 0.0163 | Grad Norm: 0.00895666\n",
      "Epoch 2 | Step 1175200 | Avg Loss: 0.0162 | Grad Norm: 0.01032164\n",
      "Epoch 2 | Step 1175300 | Avg Loss: 0.0162 | Grad Norm: 0.00909512\n",
      "Epoch 2 | Step 1175400 | Avg Loss: 0.0162 | Grad Norm: 0.00929851\n",
      "Epoch 2 | Step 1175500 | Avg Loss: 0.0165 | Grad Norm: 0.01159072\n",
      "Epoch 2 | Step 1175600 | Avg Loss: 0.0162 | Grad Norm: 0.00939610\n",
      "Epoch 2 | Step 1175700 | Avg Loss: 0.0161 | Grad Norm: 0.00859530\n",
      "Epoch 2 | Step 1175800 | Avg Loss: 0.0160 | Grad Norm: 0.01222961\n",
      "Epoch 2 | Step 1175900 | Avg Loss: 0.0158 | Grad Norm: 0.00913079\n",
      "Epoch 2 | Step 1176000 | Avg Loss: 0.0159 | Grad Norm: 0.00918215\n",
      "Epoch 2 | Step 1176100 | Avg Loss: 0.0153 | Grad Norm: 0.01018308\n",
      "Epoch 2 | Step 1176200 | Avg Loss: 0.0155 | Grad Norm: 0.01588367\n",
      "Epoch 2 | Step 1176300 | Avg Loss: 0.0153 | Grad Norm: 0.00808251\n",
      "Epoch 2 | Step 1176400 | Avg Loss: 0.0154 | Grad Norm: 0.00833810\n",
      "Epoch 2 | Step 1176500 | Avg Loss: 0.0156 | Grad Norm: 0.00843076\n",
      "Epoch 2 | Step 1176600 | Avg Loss: 0.0157 | Grad Norm: 0.00891637\n",
      "Epoch 2 | Step 1176700 | Avg Loss: 0.0157 | Grad Norm: 0.00816361\n",
      "Epoch 2 | Step 1176800 | Avg Loss: 0.0158 | Grad Norm: 0.00940247\n",
      "Epoch 2 | Step 1176900 | Avg Loss: 0.0155 | Grad Norm: 0.00842837\n",
      "Epoch 2 | Step 1177000 | Avg Loss: 0.0154 | Grad Norm: 0.00857838\n",
      "Epoch 2 | Step 1177100 | Avg Loss: 0.0158 | Grad Norm: 0.00962609\n",
      "Epoch 2 | Step 1177200 | Avg Loss: 0.0156 | Grad Norm: 0.00921095\n",
      "Epoch 2 | Step 1177300 | Avg Loss: 0.0153 | Grad Norm: 0.00928125\n",
      "Epoch 2 | Step 1177400 | Avg Loss: 0.0154 | Grad Norm: 0.00789300\n",
      "Epoch 2 | Step 1177500 | Avg Loss: 0.0157 | Grad Norm: 0.00834724\n",
      "Epoch 2 | Step 1177600 | Avg Loss: 0.0155 | Grad Norm: 0.00936405\n",
      "Epoch 2 | Step 1177700 | Avg Loss: 0.0154 | Grad Norm: 0.00773593\n",
      "Epoch 2 | Step 1177800 | Avg Loss: 0.0150 | Grad Norm: 0.00890444\n",
      "Epoch 2 | Step 1177900 | Avg Loss: 0.0152 | Grad Norm: 0.00883563\n",
      "Epoch 2 | Step 1178000 | Avg Loss: 0.0154 | Grad Norm: 0.00896870\n",
      "Epoch 2 | Step 1178100 | Avg Loss: 0.0155 | Grad Norm: 0.00881176\n",
      "Epoch 2 | Step 1178200 | Avg Loss: 0.0158 | Grad Norm: 0.00929923\n",
      "Epoch 2 | Step 1178300 | Avg Loss: 0.0158 | Grad Norm: 0.00916469\n",
      "Epoch 2 | Step 1178400 | Avg Loss: 0.0156 | Grad Norm: 0.00934753\n",
      "Epoch 2 | Step 1178500 | Avg Loss: 0.0157 | Grad Norm: 0.00945785\n",
      "Epoch 2 | Step 1178600 | Avg Loss: 0.0157 | Grad Norm: 0.00964091\n",
      "Epoch 2 | Step 1178700 | Avg Loss: 0.0161 | Grad Norm: 0.00831190\n",
      "Epoch 2 | Step 1178800 | Avg Loss: 0.0158 | Grad Norm: 0.00822401\n",
      "Epoch 2 | Step 1178900 | Avg Loss: 0.0155 | Grad Norm: 0.00763624\n",
      "Epoch 2 | Step 1179000 | Avg Loss: 0.0153 | Grad Norm: 0.00842360\n",
      "Epoch 2 | Step 1179100 | Avg Loss: 0.0154 | Grad Norm: 0.00838925\n",
      "Epoch 2 | Step 1179200 | Avg Loss: 0.0154 | Grad Norm: 0.01086282\n",
      "Epoch 2 | Step 1179300 | Avg Loss: 0.0155 | Grad Norm: 0.00921928\n",
      "Epoch 2 | Step 1179400 | Avg Loss: 0.0154 | Grad Norm: 0.00797096\n",
      "Epoch 2 | Step 1179500 | Avg Loss: 0.0154 | Grad Norm: 0.00918972\n",
      "Epoch 2 | Step 1179600 | Avg Loss: 0.0156 | Grad Norm: 0.00968747\n",
      "Epoch 2 | Step 1179700 | Avg Loss: 0.0160 | Grad Norm: 0.00881440\n",
      "Epoch 2 | Step 1179800 | Avg Loss: 0.0156 | Grad Norm: 0.00767080\n",
      "Epoch 2 | Step 1179900 | Avg Loss: 0.0157 | Grad Norm: 0.00882351\n",
      "Epoch 2 | Step 1180000 | Avg Loss: 0.0157 | Grad Norm: 0.00971867\n",
      "Epoch 2 | Step 1180100 | Avg Loss: 0.0157 | Grad Norm: 0.00794128\n",
      "Epoch 2 | Step 1180200 | Avg Loss: 0.0160 | Grad Norm: 0.00914472\n",
      "Epoch 2 | Step 1180300 | Avg Loss: 0.0157 | Grad Norm: 0.00961368\n",
      "Epoch 2 | Step 1180400 | Avg Loss: 0.0156 | Grad Norm: 0.00870347\n",
      "Epoch 2 | Step 1180500 | Avg Loss: 0.0156 | Grad Norm: 0.01006922\n",
      "Epoch 2 | Step 1180600 | Avg Loss: 0.0157 | Grad Norm: 0.01069295\n",
      "Epoch 2 | Step 1180700 | Avg Loss: 0.0160 | Grad Norm: 0.00835532\n",
      "Epoch 2 | Step 1180800 | Avg Loss: 0.0166 | Grad Norm: 0.01084785\n",
      "Epoch 2 | Step 1180900 | Avg Loss: 0.0169 | Grad Norm: 0.00895888\n",
      "Epoch 2 | Step 1181000 | Avg Loss: 0.0163 | Grad Norm: 0.00868052\n",
      "Epoch 2 | Step 1181100 | Avg Loss: 0.0165 | Grad Norm: 0.00969968\n",
      "Epoch 2 | Step 1181200 | Avg Loss: 0.0165 | Grad Norm: 0.00960948\n",
      "Epoch 2 | Step 1181300 | Avg Loss: 0.0160 | Grad Norm: 0.00952021\n",
      "Epoch 2 | Step 1181400 | Avg Loss: 0.0159 | Grad Norm: 0.00946721\n",
      "Epoch 2 | Step 1181500 | Avg Loss: 0.0157 | Grad Norm: 0.00919842\n",
      "Epoch 2 | Step 1181600 | Avg Loss: 0.0160 | Grad Norm: 0.00851660\n",
      "Epoch 2 | Step 1181700 | Avg Loss: 0.0157 | Grad Norm: 0.00858253\n",
      "Epoch 2 | Step 1181800 | Avg Loss: 0.0153 | Grad Norm: 0.00843702\n",
      "Epoch 2 | Step 1181900 | Avg Loss: 0.0153 | Grad Norm: 0.00957808\n",
      "Epoch 2 | Step 1182000 | Avg Loss: 0.0154 | Grad Norm: 0.00865947\n",
      "Epoch 2 | Step 1182100 | Avg Loss: 0.0152 | Grad Norm: 0.00929964\n",
      "Epoch 2 | Step 1182200 | Avg Loss: 0.0148 | Grad Norm: 0.00850212\n",
      "Epoch 2 | Step 1182300 | Avg Loss: 0.0152 | Grad Norm: 0.00771327\n",
      "Epoch 2 | Step 1182400 | Avg Loss: 0.0150 | Grad Norm: 0.01483611\n",
      "Epoch 2 | Step 1182500 | Avg Loss: 0.0153 | Grad Norm: 0.00762544\n",
      "Epoch 2 | Step 1182600 | Avg Loss: 0.0153 | Grad Norm: 0.00930145\n",
      "Epoch 2 | Step 1182700 | Avg Loss: 0.0154 | Grad Norm: 0.00909526\n",
      "Epoch 2 | Step 1182800 | Avg Loss: 0.0157 | Grad Norm: 0.00852189\n",
      "Epoch 2 | Step 1182900 | Avg Loss: 0.0155 | Grad Norm: 0.00915632\n",
      "Epoch 2 | Step 1183000 | Avg Loss: 0.0152 | Grad Norm: 0.00983717\n",
      "Epoch 2 | Step 1183100 | Avg Loss: 0.0150 | Grad Norm: 0.00844296\n",
      "Epoch 2 | Step 1183200 | Avg Loss: 0.0150 | Grad Norm: 0.00854440\n",
      "Epoch 2 | Step 1183300 | Avg Loss: 0.0153 | Grad Norm: 0.00954710\n",
      "Epoch 2 | Step 1183400 | Avg Loss: 0.0156 | Grad Norm: 0.00789426\n",
      "Epoch 2 | Step 1183500 | Avg Loss: 0.0159 | Grad Norm: 0.01261168\n",
      "Epoch 2 | Step 1183600 | Avg Loss: 0.0152 | Grad Norm: 0.00881599\n",
      "Epoch 2 | Step 1183700 | Avg Loss: 0.0152 | Grad Norm: 0.00976067\n",
      "Epoch 2 | Step 1183800 | Avg Loss: 0.0154 | Grad Norm: 0.00928815\n",
      "Epoch 2 | Step 1183900 | Avg Loss: 0.0154 | Grad Norm: 0.01098354\n",
      "Epoch 2 | Step 1184000 | Avg Loss: 0.0154 | Grad Norm: 0.00992724\n",
      "Epoch 2 | Step 1184100 | Avg Loss: 0.0151 | Grad Norm: 0.00739826\n",
      "Epoch 2 | Step 1184200 | Avg Loss: 0.0153 | Grad Norm: 0.00933479\n",
      "Epoch 2 | Step 1184300 | Avg Loss: 0.0153 | Grad Norm: 0.00902625\n",
      "Epoch 2 | Step 1184400 | Avg Loss: 0.0153 | Grad Norm: 0.00851117\n",
      "Epoch 2 | Step 1184500 | Avg Loss: 0.0152 | Grad Norm: 0.00808257\n",
      "Epoch 2 | Step 1184600 | Avg Loss: 0.0153 | Grad Norm: 0.01118165\n",
      "Epoch 2 | Step 1184700 | Avg Loss: 0.0153 | Grad Norm: 0.00847931\n",
      "Epoch 2 | Step 1184800 | Avg Loss: 0.0156 | Grad Norm: 0.00800535\n",
      "Epoch 2 | Step 1184900 | Avg Loss: 0.0153 | Grad Norm: 0.00806954\n",
      "Epoch 2 | Step 1185000 | Avg Loss: 0.0156 | Grad Norm: 0.00965737\n",
      "Epoch 2 | Step 1185100 | Avg Loss: 0.0156 | Grad Norm: 0.00963416\n",
      "Epoch 2 | Step 1185200 | Avg Loss: 0.0159 | Grad Norm: 0.00849750\n",
      "Epoch 2 | Step 1185300 | Avg Loss: 0.0161 | Grad Norm: 0.00866982\n",
      "Epoch 2 | Step 1185400 | Avg Loss: 0.0156 | Grad Norm: 0.00971070\n",
      "Epoch 2 | Step 1185500 | Avg Loss: 0.0157 | Grad Norm: 0.00851355\n",
      "Epoch 2 | Step 1185600 | Avg Loss: 0.0156 | Grad Norm: 0.00957090\n",
      "Epoch 2 | Step 1185700 | Avg Loss: 0.0159 | Grad Norm: 0.01132476\n",
      "Epoch 2 | Step 1185800 | Avg Loss: 0.0162 | Grad Norm: 0.01007973\n",
      "Epoch 2 | Step 1185900 | Avg Loss: 0.0166 | Grad Norm: 0.00963526\n",
      "Epoch 2 | Step 1186000 | Avg Loss: 0.0164 | Grad Norm: 0.00924140\n",
      "Epoch 2 | Step 1186100 | Avg Loss: 0.0163 | Grad Norm: 0.00893217\n",
      "Epoch 2 | Step 1186200 | Avg Loss: 0.0158 | Grad Norm: 0.00881794\n",
      "Epoch 2 | Step 1186300 | Avg Loss: 0.0157 | Grad Norm: 0.00897492\n",
      "Epoch 2 | Step 1186400 | Avg Loss: 0.0158 | Grad Norm: 0.00851949\n",
      "Epoch 2 | Step 1186500 | Avg Loss: 0.0158 | Grad Norm: 0.00855544\n",
      "Epoch 2 | Step 1186600 | Avg Loss: 0.0160 | Grad Norm: 0.00959834\n",
      "Epoch 2 | Step 1186700 | Avg Loss: 0.0157 | Grad Norm: 0.00897902\n",
      "Epoch 2 | Step 1186800 | Avg Loss: 0.0158 | Grad Norm: 0.00911863\n",
      "Epoch 2 | Step 1186900 | Avg Loss: 0.0158 | Grad Norm: 0.00823625\n",
      "Epoch 2 | Step 1187000 | Avg Loss: 0.0159 | Grad Norm: 0.00875147\n",
      "Epoch 2 | Step 1187100 | Avg Loss: 0.0162 | Grad Norm: 0.00902397\n",
      "Epoch 2 | Step 1187200 | Avg Loss: 0.0157 | Grad Norm: 0.00998220\n",
      "Epoch 2 | Step 1187300 | Avg Loss: 0.0157 | Grad Norm: 0.00953809\n",
      "Epoch 2 | Step 1187400 | Avg Loss: 0.0157 | Grad Norm: 0.00846235\n",
      "Epoch 2 | Step 1187500 | Avg Loss: 0.0154 | Grad Norm: 0.00905466\n",
      "Epoch 2 | Step 1187600 | Avg Loss: 0.0157 | Grad Norm: 0.00803243\n",
      "Epoch 2 | Step 1187700 | Avg Loss: 0.0158 | Grad Norm: 0.00835496\n",
      "Epoch 2 | Step 1187800 | Avg Loss: 0.0157 | Grad Norm: 0.00909030\n",
      "Epoch 2 | Step 1187900 | Avg Loss: 0.0157 | Grad Norm: 0.00978723\n",
      "Epoch 2 | Step 1188000 | Avg Loss: 0.0161 | Grad Norm: 0.00954993\n",
      "Epoch 2 | Step 1188100 | Avg Loss: 0.0160 | Grad Norm: 0.01260790\n",
      "Epoch 2 | Step 1188200 | Avg Loss: 0.0157 | Grad Norm: 0.00889913\n",
      "Epoch 2 | Step 1188300 | Avg Loss: 0.0163 | Grad Norm: 0.00960781\n",
      "Epoch 2 | Step 1188400 | Avg Loss: 0.0161 | Grad Norm: 0.00873024\n",
      "Epoch 2 | Step 1188500 | Avg Loss: 0.0159 | Grad Norm: 0.01114721\n",
      "Epoch 2 | Step 1188600 | Avg Loss: 0.0157 | Grad Norm: 0.00908451\n",
      "Epoch 2 | Step 1188700 | Avg Loss: 0.0157 | Grad Norm: 0.00989698\n",
      "Epoch 2 | Step 1188800 | Avg Loss: 0.0159 | Grad Norm: 0.00987155\n",
      "Epoch 2 | Step 1188900 | Avg Loss: 0.0159 | Grad Norm: 0.00828522\n",
      "Epoch 2 | Step 1189000 | Avg Loss: 0.0158 | Grad Norm: 0.00852776\n",
      "Epoch 2 | Step 1189100 | Avg Loss: 0.0156 | Grad Norm: 0.00820196\n",
      "Epoch 2 | Step 1189200 | Avg Loss: 0.0158 | Grad Norm: 0.00844643\n",
      "Epoch 2 | Step 1189300 | Avg Loss: 0.0155 | Grad Norm: 0.00929991\n",
      "Epoch 2 | Step 1189400 | Avg Loss: 0.0156 | Grad Norm: 0.00892933\n",
      "Epoch 2 | Step 1189500 | Avg Loss: 0.0156 | Grad Norm: 0.00872280\n",
      "Epoch 2 | Step 1189600 | Avg Loss: 0.0155 | Grad Norm: 0.00880748\n",
      "Epoch 2 | Step 1189700 | Avg Loss: 0.0154 | Grad Norm: 0.00893307\n",
      "Epoch 2 | Step 1189800 | Avg Loss: 0.0153 | Grad Norm: 0.00859730\n",
      "Epoch 2 | Step 1189900 | Avg Loss: 0.0152 | Grad Norm: 0.00914555\n",
      "Epoch 2 | Step 1190000 | Avg Loss: 0.0156 | Grad Norm: 0.00845439\n",
      "Epoch 2 | Step 1190100 | Avg Loss: 0.0160 | Grad Norm: 0.00796541\n",
      "Epoch 2 | Step 1190200 | Avg Loss: 0.0160 | Grad Norm: 0.00818006\n",
      "Epoch 2 | Step 1190300 | Avg Loss: 0.0156 | Grad Norm: 0.00912406\n",
      "Epoch 2 | Step 1190400 | Avg Loss: 0.0157 | Grad Norm: 0.00822562\n",
      "Epoch 2 | Step 1190500 | Avg Loss: 0.0159 | Grad Norm: 0.00811391\n",
      "Epoch 2 | Step 1190600 | Avg Loss: 0.0163 | Grad Norm: 0.00928364\n",
      "Epoch 2 | Step 1190700 | Avg Loss: 0.0163 | Grad Norm: 0.00798706\n",
      "Epoch 2 | Step 1190800 | Avg Loss: 0.0161 | Grad Norm: 0.00916044\n",
      "Epoch 2 | Step 1190900 | Avg Loss: 0.0163 | Grad Norm: 0.00783705\n",
      "Epoch 2 | Step 1191000 | Avg Loss: 0.0158 | Grad Norm: 0.00910678\n",
      "Epoch 2 | Step 1191100 | Avg Loss: 0.0154 | Grad Norm: 0.00968891\n",
      "Epoch 2 | Step 1191200 | Avg Loss: 0.0153 | Grad Norm: 0.00830051\n",
      "Epoch 2 | Step 1191300 | Avg Loss: 0.0153 | Grad Norm: 0.00862433\n",
      "Epoch 2 | Step 1191400 | Avg Loss: 0.0152 | Grad Norm: 0.00985732\n",
      "Epoch 2 | Step 1191500 | Avg Loss: 0.0152 | Grad Norm: 0.00830936\n",
      "Epoch 2 | Step 1191600 | Avg Loss: 0.0153 | Grad Norm: 0.00748441\n",
      "Epoch 2 | Step 1191700 | Avg Loss: 0.0154 | Grad Norm: 0.00807699\n",
      "Epoch 2 | Step 1191800 | Avg Loss: 0.0157 | Grad Norm: 0.00856443\n",
      "Epoch 2 | Step 1191900 | Avg Loss: 0.0154 | Grad Norm: 0.01052852\n",
      "Epoch 2 | Step 1192000 | Avg Loss: 0.0151 | Grad Norm: 0.00821131\n",
      "Epoch 2 | Step 1192100 | Avg Loss: 0.0154 | Grad Norm: 0.00901673\n",
      "Epoch 2 | Step 1192200 | Avg Loss: 0.0153 | Grad Norm: 0.01123122\n",
      "Epoch 2 | Step 1192300 | Avg Loss: 0.0154 | Grad Norm: 0.00844700\n",
      "Epoch 2 | Step 1192400 | Avg Loss: 0.0156 | Grad Norm: 0.00948938\n",
      "Epoch 2 | Step 1192500 | Avg Loss: 0.0158 | Grad Norm: 0.00901319\n",
      "Epoch 2 | Step 1192600 | Avg Loss: 0.0162 | Grad Norm: 0.01013848\n",
      "Epoch 2 | Step 1192700 | Avg Loss: 0.0162 | Grad Norm: 0.00939783\n",
      "Epoch 2 | Step 1192800 | Avg Loss: 0.0161 | Grad Norm: 0.01232559\n",
      "Epoch 2 | Step 1192900 | Avg Loss: 0.0158 | Grad Norm: 0.00878176\n",
      "Epoch 2 | Step 1193000 | Avg Loss: 0.0157 | Grad Norm: 0.01013669\n",
      "Epoch 2 | Step 1193100 | Avg Loss: 0.0157 | Grad Norm: 0.01139027\n",
      "Epoch 2 | Step 1193200 | Avg Loss: 0.0158 | Grad Norm: 0.00941304\n",
      "Epoch 2 | Step 1193300 | Avg Loss: 0.0156 | Grad Norm: 0.00890996\n",
      "Epoch 2 | Step 1193400 | Avg Loss: 0.0155 | Grad Norm: 0.00928941\n",
      "Epoch 2 | Step 1193500 | Avg Loss: 0.0155 | Grad Norm: 0.00919042\n",
      "Epoch 2 | Step 1193600 | Avg Loss: 0.0159 | Grad Norm: 0.00802454\n",
      "Epoch 2 | Step 1193700 | Avg Loss: 0.0157 | Grad Norm: 0.00874021\n",
      "Epoch 2 | Step 1193800 | Avg Loss: 0.0160 | Grad Norm: 0.00874966\n",
      "Epoch 2 | Step 1193900 | Avg Loss: 0.0155 | Grad Norm: 0.00928214\n",
      "Epoch 2 | Step 1194000 | Avg Loss: 0.0155 | Grad Norm: 0.00827937\n",
      "Epoch 2 | Step 1194100 | Avg Loss: 0.0157 | Grad Norm: 0.00842613\n",
      "Epoch 2 | Step 1194200 | Avg Loss: 0.0156 | Grad Norm: 0.00844647\n",
      "Epoch 2 | Step 1194300 | Avg Loss: 0.0156 | Grad Norm: 0.00925185\n",
      "Epoch 2 | Step 1194400 | Avg Loss: 0.0154 | Grad Norm: 0.00910124\n",
      "Epoch 2 | Step 1194500 | Avg Loss: 0.0154 | Grad Norm: 0.00796798\n",
      "Epoch 2 | Step 1194600 | Avg Loss: 0.0153 | Grad Norm: 0.00887502\n",
      "Epoch 2 | Step 1194700 | Avg Loss: 0.0155 | Grad Norm: 0.00870545\n",
      "Epoch 2 | Step 1194800 | Avg Loss: 0.0156 | Grad Norm: 0.00961213\n",
      "Epoch 2 | Step 1194900 | Avg Loss: 0.0157 | Grad Norm: 0.00879289\n",
      "Epoch 2 | Step 1195000 | Avg Loss: 0.0155 | Grad Norm: 0.00754597\n",
      "Epoch 2 | Step 1195100 | Avg Loss: 0.0156 | Grad Norm: 0.00817440\n",
      "Epoch 2 | Step 1195200 | Avg Loss: 0.0159 | Grad Norm: 0.00912752\n",
      "Epoch 2 | Step 1195300 | Avg Loss: 0.0157 | Grad Norm: 0.00741799\n",
      "Epoch 2 | Step 1195400 | Avg Loss: 0.0157 | Grad Norm: 0.00967717\n",
      "Epoch 2 | Step 1195500 | Avg Loss: 0.0159 | Grad Norm: 0.00834631\n",
      "Epoch 2 | Step 1195600 | Avg Loss: 0.0160 | Grad Norm: 0.00934431\n",
      "Epoch 2 | Step 1195700 | Avg Loss: 0.0163 | Grad Norm: 0.01364785\n",
      "Epoch 2 | Step 1195800 | Avg Loss: 0.0163 | Grad Norm: 0.01009611\n",
      "Epoch 2 | Step 1195900 | Avg Loss: 0.0160 | Grad Norm: 0.00854122\n",
      "Epoch 2 | Step 1196000 | Avg Loss: 0.0162 | Grad Norm: 0.00896338\n",
      "Epoch 2 | Step 1196100 | Avg Loss: 0.0158 | Grad Norm: 0.00843002\n",
      "Epoch 2 | Step 1196200 | Avg Loss: 0.0160 | Grad Norm: 0.00913718\n",
      "Epoch 2 | Step 1196300 | Avg Loss: 0.0159 | Grad Norm: 0.00886446\n",
      "Epoch 2 | Step 1196400 | Avg Loss: 0.0154 | Grad Norm: 0.00812640\n",
      "Epoch 2 | Step 1196500 | Avg Loss: 0.0152 | Grad Norm: 0.00879391\n",
      "Epoch 2 | Step 1196600 | Avg Loss: 0.0156 | Grad Norm: 0.01241819\n",
      "Epoch 2 | Step 1196700 | Avg Loss: 0.0156 | Grad Norm: 0.00869736\n",
      "Epoch 2 | Step 1196800 | Avg Loss: 0.0155 | Grad Norm: 0.00864386\n",
      "Epoch 2 | Step 1196900 | Avg Loss: 0.0157 | Grad Norm: 0.01081188\n",
      "Epoch 2 | Step 1197000 | Avg Loss: 0.0155 | Grad Norm: 0.00848309\n",
      "Epoch 2 | Step 1197100 | Avg Loss: 0.0158 | Grad Norm: 0.00911645\n",
      "Epoch 2 | Step 1197200 | Avg Loss: 0.0159 | Grad Norm: 0.00926939\n",
      "Epoch 2 | Step 1197300 | Avg Loss: 0.0161 | Grad Norm: 0.01029457\n",
      "Epoch 2 | Step 1197400 | Avg Loss: 0.0160 | Grad Norm: 0.00930631\n",
      "Epoch 2 | Step 1197500 | Avg Loss: 0.0155 | Grad Norm: 0.00769437\n",
      "Epoch 2 | Step 1197600 | Avg Loss: 0.0157 | Grad Norm: 0.00840698\n",
      "Epoch 2 | Step 1197700 | Avg Loss: 0.0159 | Grad Norm: 0.00998524\n",
      "Epoch 2 | Step 1197800 | Avg Loss: 0.0153 | Grad Norm: 0.00949669\n",
      "Epoch 2 | Step 1197900 | Avg Loss: 0.0152 | Grad Norm: 0.00971177\n",
      "Epoch 2 | Step 1198000 | Avg Loss: 0.0156 | Grad Norm: 0.00887442\n",
      "Epoch 2 | Step 1198100 | Avg Loss: 0.0158 | Grad Norm: 0.00936620\n",
      "Epoch 2 | Step 1198200 | Avg Loss: 0.0160 | Grad Norm: 0.00987719\n",
      "Epoch 2 | Step 1198300 | Avg Loss: 0.0158 | Grad Norm: 0.01045840\n",
      "Epoch 2 | Step 1198400 | Avg Loss: 0.0160 | Grad Norm: 0.00892839\n",
      "Epoch 2 | Step 1198500 | Avg Loss: 0.0161 | Grad Norm: 0.00899910\n",
      "Epoch 2 | Step 1198600 | Avg Loss: 0.0162 | Grad Norm: 0.00903975\n",
      "Epoch 2 | Step 1198700 | Avg Loss: 0.0165 | Grad Norm: 0.00916630\n",
      "Epoch 2 | Step 1198800 | Avg Loss: 0.0160 | Grad Norm: 0.01017730\n",
      "Epoch 2 | Step 1198900 | Avg Loss: 0.0162 | Grad Norm: 0.01023913\n",
      "Epoch 2 | Step 1199000 | Avg Loss: 0.0164 | Grad Norm: 0.00835904\n",
      "Epoch 2 | Step 1199100 | Avg Loss: 0.0158 | Grad Norm: 0.00883331\n",
      "Epoch 2 | Step 1199200 | Avg Loss: 0.0160 | Grad Norm: 0.00963403\n",
      "Epoch 2 | Step 1199300 | Avg Loss: 0.0158 | Grad Norm: 0.00835116\n",
      "Epoch 2 | Step 1199400 | Avg Loss: 0.0158 | Grad Norm: 0.00867999\n",
      "Epoch 2 | Step 1199500 | Avg Loss: 0.0160 | Grad Norm: 0.00942095\n",
      "Epoch 2 | Step 1199600 | Avg Loss: 0.0159 | Grad Norm: 0.00910870\n",
      "Epoch 2 | Step 1199700 | Avg Loss: 0.0158 | Grad Norm: 0.00737565\n",
      "Epoch 2 | Step 1199800 | Avg Loss: 0.0156 | Grad Norm: 0.00802878\n",
      "Epoch 2 | Step 1199900 | Avg Loss: 0.0161 | Grad Norm: 0.00935861\n",
      "Epoch 2 | Step 1200000 | Avg Loss: 0.0164 | Grad Norm: 0.01171904\n",
      "Saving model at step1200000\n",
      "Epoch 2 | Step 1200100 | Avg Loss: 0.0163 | Grad Norm: 0.01001269\n",
      "Epoch 2 | Step 1200200 | Avg Loss: 0.0161 | Grad Norm: 0.00993871\n",
      "Epoch 2 | Step 1200300 | Avg Loss: 0.0161 | Grad Norm: 0.00849265\n",
      "Epoch 2 | Step 1200400 | Avg Loss: 0.0160 | Grad Norm: 0.00953209\n",
      "Epoch 2 | Step 1200500 | Avg Loss: 0.0160 | Grad Norm: 0.00918459\n",
      "Epoch 2 | Step 1200600 | Avg Loss: 0.0160 | Grad Norm: 0.01138843\n",
      "Epoch 2 | Step 1200700 | Avg Loss: 0.0163 | Grad Norm: 0.01080672\n",
      "Epoch 2 | Step 1200800 | Avg Loss: 0.0159 | Grad Norm: 0.00866598\n",
      "Epoch 2 | Step 1200900 | Avg Loss: 0.0159 | Grad Norm: 0.00850135\n",
      "Epoch 2 | Step 1201000 | Avg Loss: 0.0156 | Grad Norm: 0.00941984\n",
      "Epoch 2 | Step 1201100 | Avg Loss: 0.0151 | Grad Norm: 0.00886612\n",
      "Epoch 2 | Step 1201200 | Avg Loss: 0.0149 | Grad Norm: 0.00790559\n",
      "Epoch 2 | Step 1201300 | Avg Loss: 0.0148 | Grad Norm: 0.00833403\n",
      "Epoch 2 | Step 1201400 | Avg Loss: 0.0151 | Grad Norm: 0.00913343\n",
      "Epoch 2 | Step 1201500 | Avg Loss: 0.0152 | Grad Norm: 0.00772512\n",
      "Epoch 2 | Step 1201600 | Avg Loss: 0.0155 | Grad Norm: 0.00811476\n",
      "Epoch 2 | Step 1201700 | Avg Loss: 0.0158 | Grad Norm: 0.00985967\n",
      "Epoch 2 | Step 1201800 | Avg Loss: 0.0158 | Grad Norm: 0.00932419\n",
      "Epoch 2 | Step 1201900 | Avg Loss: 0.0155 | Grad Norm: 0.00989468\n",
      "Epoch 2 | Step 1202000 | Avg Loss: 0.0158 | Grad Norm: 0.00936561\n",
      "Epoch 2 | Step 1202100 | Avg Loss: 0.0162 | Grad Norm: 0.00979702\n",
      "Epoch 2 | Step 1202200 | Avg Loss: 0.0159 | Grad Norm: 0.00914026\n",
      "Epoch 2 | Step 1202300 | Avg Loss: 0.0159 | Grad Norm: 0.00974494\n",
      "Epoch 2 | Step 1202400 | Avg Loss: 0.0157 | Grad Norm: 0.00902174\n",
      "Epoch 2 | Step 1202500 | Avg Loss: 0.0157 | Grad Norm: 0.00928458\n",
      "Epoch 2 | Step 1202600 | Avg Loss: 0.0154 | Grad Norm: 0.00852803\n",
      "Epoch 2 | Step 1202700 | Avg Loss: 0.0153 | Grad Norm: 0.00838601\n",
      "Epoch 2 | Step 1202800 | Avg Loss: 0.0154 | Grad Norm: 0.00786654\n",
      "Epoch 2 | Step 1202900 | Avg Loss: 0.0154 | Grad Norm: 0.00802647\n",
      "Epoch 2 | Step 1203000 | Avg Loss: 0.0155 | Grad Norm: 0.01110611\n",
      "Epoch 2 | Step 1203100 | Avg Loss: 0.0159 | Grad Norm: 0.00881578\n",
      "Epoch 2 | Step 1203200 | Avg Loss: 0.0159 | Grad Norm: 0.00844082\n",
      "Epoch 2 | Step 1203300 | Avg Loss: 0.0156 | Grad Norm: 0.00898483\n",
      "Epoch 2 | Step 1203400 | Avg Loss: 0.0155 | Grad Norm: 0.01095587\n",
      "Epoch 2 | Step 1203500 | Avg Loss: 0.0154 | Grad Norm: 0.00836303\n",
      "Epoch 2 | Step 1203600 | Avg Loss: 0.0153 | Grad Norm: 0.00845000\n",
      "Epoch 2 | Step 1203700 | Avg Loss: 0.0155 | Grad Norm: 0.01102112\n",
      "Epoch 2 | Step 1203800 | Avg Loss: 0.0156 | Grad Norm: 0.00916487\n",
      "Epoch 2 | Step 1203900 | Avg Loss: 0.0155 | Grad Norm: 0.00944817\n",
      "Epoch 2 | Step 1204000 | Avg Loss: 0.0157 | Grad Norm: 0.00976042\n",
      "Epoch 2 | Step 1204100 | Avg Loss: 0.0156 | Grad Norm: 0.00871227\n",
      "Epoch 2 | Step 1204200 | Avg Loss: 0.0160 | Grad Norm: 0.00975501\n",
      "Epoch 2 | Step 1204300 | Avg Loss: 0.0159 | Grad Norm: 0.00821245\n",
      "Epoch 2 | Step 1204400 | Avg Loss: 0.0158 | Grad Norm: 0.00867451\n",
      "Epoch 2 | Step 1204500 | Avg Loss: 0.0158 | Grad Norm: 0.01004132\n",
      "Epoch 2 | Step 1204600 | Avg Loss: 0.0157 | Grad Norm: 0.00804373\n",
      "Epoch 2 | Step 1204700 | Avg Loss: 0.0156 | Grad Norm: 0.00910020\n",
      "Epoch 2 | Step 1204800 | Avg Loss: 0.0157 | Grad Norm: 0.00801844\n",
      "Epoch 2 | Step 1204900 | Avg Loss: 0.0157 | Grad Norm: 0.00842941\n",
      "Epoch 2 | Step 1205000 | Avg Loss: 0.0156 | Grad Norm: 0.00880259\n",
      "Epoch 2 | Step 1205100 | Avg Loss: 0.0152 | Grad Norm: 0.00812360\n",
      "Epoch 2 | Step 1205200 | Avg Loss: 0.0153 | Grad Norm: 0.00926770\n",
      "Epoch 2 | Step 1205300 | Avg Loss: 0.0156 | Grad Norm: 0.00895552\n",
      "Epoch 2 | Step 1205400 | Avg Loss: 0.0153 | Grad Norm: 0.01068104\n",
      "Epoch 2 | Step 1205500 | Avg Loss: 0.0156 | Grad Norm: 0.00954607\n",
      "Epoch 2 | Step 1205600 | Avg Loss: 0.0155 | Grad Norm: 0.00940704\n",
      "Epoch 2 | Step 1205700 | Avg Loss: 0.0155 | Grad Norm: 0.00872352\n",
      "Epoch 2 | Step 1205800 | Avg Loss: 0.0155 | Grad Norm: 0.00879328\n",
      "Epoch 2 | Step 1205900 | Avg Loss: 0.0157 | Grad Norm: 0.00980593\n",
      "Epoch 2 | Step 1206000 | Avg Loss: 0.0156 | Grad Norm: 0.00964304\n",
      "Epoch 2 | Step 1206100 | Avg Loss: 0.0156 | Grad Norm: 0.00945176\n",
      "Epoch 2 | Step 1206200 | Avg Loss: 0.0157 | Grad Norm: 0.00968793\n",
      "Epoch 2 | Step 1206300 | Avg Loss: 0.0158 | Grad Norm: 0.01083672\n",
      "Epoch 2 | Step 1206400 | Avg Loss: 0.0164 | Grad Norm: 0.01063513\n",
      "Epoch 2 | Step 1206500 | Avg Loss: 0.0158 | Grad Norm: 0.00800838\n",
      "Epoch 2 | Step 1206600 | Avg Loss: 0.0158 | Grad Norm: 0.00837484\n",
      "Epoch 2 | Step 1206700 | Avg Loss: 0.0159 | Grad Norm: 0.00847426\n",
      "Epoch 2 | Step 1206800 | Avg Loss: 0.0158 | Grad Norm: 0.00783580\n",
      "Epoch 2 | Step 1206900 | Avg Loss: 0.0155 | Grad Norm: 0.01109775\n",
      "Epoch 2 | Step 1207000 | Avg Loss: 0.0156 | Grad Norm: 0.00825206\n",
      "Epoch 2 | Step 1207100 | Avg Loss: 0.0155 | Grad Norm: 0.00896960\n",
      "Epoch 2 | Step 1207200 | Avg Loss: 0.0153 | Grad Norm: 0.00900927\n",
      "Epoch 2 | Step 1207300 | Avg Loss: 0.0155 | Grad Norm: 0.00969440\n",
      "Epoch 2 | Step 1207400 | Avg Loss: 0.0153 | Grad Norm: 0.00897094\n",
      "Epoch 2 | Step 1207500 | Avg Loss: 0.0154 | Grad Norm: 0.00876575\n",
      "Epoch 2 | Step 1207600 | Avg Loss: 0.0155 | Grad Norm: 0.00818973\n",
      "Epoch 2 | Step 1207700 | Avg Loss: 0.0151 | Grad Norm: 0.00834446\n",
      "Epoch 2 | Step 1207800 | Avg Loss: 0.0151 | Grad Norm: 0.00914549\n",
      "Epoch 2 | Step 1207900 | Avg Loss: 0.0150 | Grad Norm: 0.00938254\n",
      "Epoch 2 | Step 1208000 | Avg Loss: 0.0152 | Grad Norm: 0.00831752\n",
      "Epoch 2 | Step 1208100 | Avg Loss: 0.0157 | Grad Norm: 0.01046657\n",
      "Epoch 2 | Step 1208200 | Avg Loss: 0.0157 | Grad Norm: 0.00939524\n",
      "Epoch 2 | Step 1208300 | Avg Loss: 0.0154 | Grad Norm: 0.00869243\n",
      "Epoch 2 | Step 1208400 | Avg Loss: 0.0156 | Grad Norm: 0.01037623\n",
      "Epoch 2 | Step 1208500 | Avg Loss: 0.0155 | Grad Norm: 0.00897677\n",
      "Epoch 2 | Step 1208600 | Avg Loss: 0.0158 | Grad Norm: 0.00763749\n",
      "Epoch 2 | Step 1208700 | Avg Loss: 0.0158 | Grad Norm: 0.01095302\n",
      "Epoch 2 | Step 1208800 | Avg Loss: 0.0159 | Grad Norm: 0.00861540\n",
      "Epoch 2 | Step 1208900 | Avg Loss: 0.0161 | Grad Norm: 0.00883157\n",
      "Epoch 2 | Step 1209000 | Avg Loss: 0.0157 | Grad Norm: 0.00787408\n",
      "Epoch 2 | Step 1209100 | Avg Loss: 0.0157 | Grad Norm: 0.00816519\n",
      "Epoch 2 | Step 1209200 | Avg Loss: 0.0156 | Grad Norm: 0.00880075\n",
      "Epoch 2 | Step 1209300 | Avg Loss: 0.0157 | Grad Norm: 0.00870037\n",
      "Epoch 2 | Step 1209400 | Avg Loss: 0.0156 | Grad Norm: 0.01050899\n",
      "Epoch 2 | Step 1209500 | Avg Loss: 0.0157 | Grad Norm: 0.00932264\n",
      "Epoch 2 | Step 1209600 | Avg Loss: 0.0158 | Grad Norm: 0.00862792\n",
      "Epoch 2 | Step 1209700 | Avg Loss: 0.0160 | Grad Norm: 0.00761828\n",
      "Epoch 2 | Step 1209800 | Avg Loss: 0.0153 | Grad Norm: 0.00946068\n",
      "Epoch 2 | Step 1209900 | Avg Loss: 0.0154 | Grad Norm: 0.00946556\n",
      "Epoch 2 | Step 1210000 | Avg Loss: 0.0157 | Grad Norm: 0.00841870\n",
      "Epoch 2 | Step 1210100 | Avg Loss: 0.0155 | Grad Norm: 0.01093589\n",
      "Epoch 2 | Step 1210200 | Avg Loss: 0.0158 | Grad Norm: 0.00826582\n",
      "Epoch 2 | Step 1210300 | Avg Loss: 0.0160 | Grad Norm: 0.00879426\n",
      "Epoch 2 | Step 1210400 | Avg Loss: 0.0160 | Grad Norm: 0.00899542\n",
      "Epoch 2 | Step 1210500 | Avg Loss: 0.0157 | Grad Norm: 0.00853171\n",
      "Epoch 2 | Step 1210600 | Avg Loss: 0.0156 | Grad Norm: 0.01000371\n",
      "Epoch 2 | Step 1210700 | Avg Loss: 0.0159 | Grad Norm: 0.00861290\n",
      "Epoch 2 | Step 1210800 | Avg Loss: 0.0159 | Grad Norm: 0.00889609\n",
      "Epoch 2 | Step 1210900 | Avg Loss: 0.0162 | Grad Norm: 0.01008610\n",
      "Epoch 2 | Step 1211000 | Avg Loss: 0.0158 | Grad Norm: 0.00771706\n",
      "Epoch 2 | Step 1211100 | Avg Loss: 0.0160 | Grad Norm: 0.00896487\n",
      "Epoch 2 | Step 1211200 | Avg Loss: 0.0160 | Grad Norm: 0.00846548\n",
      "Epoch 2 | Step 1211300 | Avg Loss: 0.0160 | Grad Norm: 0.00975066\n",
      "Epoch 2 | Step 1211400 | Avg Loss: 0.0158 | Grad Norm: 0.00910013\n",
      "Epoch 2 | Step 1211500 | Avg Loss: 0.0156 | Grad Norm: 0.00992826\n",
      "Epoch 2 | Step 1211600 | Avg Loss: 0.0160 | Grad Norm: 0.00861871\n",
      "Epoch 2 | Step 1211700 | Avg Loss: 0.0154 | Grad Norm: 0.00838767\n",
      "Epoch 2 | Step 1211800 | Avg Loss: 0.0154 | Grad Norm: 0.00986276\n",
      "Epoch 2 | Step 1211900 | Avg Loss: 0.0153 | Grad Norm: 0.00869406\n",
      "Epoch 2 | Step 1212000 | Avg Loss: 0.0153 | Grad Norm: 0.00981442\n",
      "Epoch 2 | Step 1212100 | Avg Loss: 0.0155 | Grad Norm: 0.00907900\n",
      "Epoch 2 | Step 1212200 | Avg Loss: 0.0153 | Grad Norm: 0.00793982\n",
      "Epoch 2 | Step 1212300 | Avg Loss: 0.0155 | Grad Norm: 0.00883458\n",
      "Epoch 2 | Step 1212400 | Avg Loss: 0.0157 | Grad Norm: 0.00929363\n",
      "Epoch 2 | Step 1212500 | Avg Loss: 0.0158 | Grad Norm: 0.00921950\n",
      "Epoch 2 | Step 1212600 | Avg Loss: 0.0155 | Grad Norm: 0.00918861\n",
      "Epoch 2 | Step 1212700 | Avg Loss: 0.0153 | Grad Norm: 0.00920720\n",
      "Epoch 2 | Step 1212800 | Avg Loss: 0.0157 | Grad Norm: 0.01078690\n",
      "Epoch 2 | Step 1212900 | Avg Loss: 0.0155 | Grad Norm: 0.01071177\n",
      "Epoch 2 | Step 1213000 | Avg Loss: 0.0154 | Grad Norm: 0.00913137\n",
      "Epoch 2 | Step 1213100 | Avg Loss: 0.0158 | Grad Norm: 0.00997928\n",
      "Epoch 2 | Step 1213200 | Avg Loss: 0.0161 | Grad Norm: 0.00867145\n",
      "Epoch 2 | Step 1213300 | Avg Loss: 0.0158 | Grad Norm: 0.00920694\n",
      "Epoch 2 | Step 1213400 | Avg Loss: 0.0157 | Grad Norm: 0.00941016\n",
      "Epoch 2 | Step 1213500 | Avg Loss: 0.0158 | Grad Norm: 0.00887389\n",
      "Epoch 2 | Step 1213600 | Avg Loss: 0.0162 | Grad Norm: 0.00760136\n",
      "Epoch 2 | Step 1213700 | Avg Loss: 0.0157 | Grad Norm: 0.01058811\n",
      "Epoch 2 | Step 1213800 | Avg Loss: 0.0162 | Grad Norm: 0.00949733\n",
      "Epoch 2 | Step 1213900 | Avg Loss: 0.0162 | Grad Norm: 0.01017618\n",
      "Epoch 2 | Step 1214000 | Avg Loss: 0.0161 | Grad Norm: 0.00970671\n",
      "Epoch 2 | Step 1214100 | Avg Loss: 0.0163 | Grad Norm: 0.00971506\n",
      "Epoch 2 | Step 1214200 | Avg Loss: 0.0162 | Grad Norm: 0.00947550\n",
      "Epoch 2 | Step 1214300 | Avg Loss: 0.0157 | Grad Norm: 0.00744470\n",
      "Epoch 2 | Step 1214400 | Avg Loss: 0.0154 | Grad Norm: 0.00898265\n",
      "Epoch 2 | Step 1214500 | Avg Loss: 0.0159 | Grad Norm: 0.00978845\n",
      "Epoch 2 | Step 1214600 | Avg Loss: 0.0160 | Grad Norm: 0.00902974\n",
      "Epoch 2 | Step 1214700 | Avg Loss: 0.0155 | Grad Norm: 0.00793783\n",
      "Epoch 2 | Step 1214800 | Avg Loss: 0.0157 | Grad Norm: 0.00843272\n",
      "Epoch 2 | Step 1214900 | Avg Loss: 0.0155 | Grad Norm: 0.01012556\n",
      "Epoch 2 | Step 1215000 | Avg Loss: 0.0154 | Grad Norm: 0.00848812\n",
      "Epoch 2 | Step 1215100 | Avg Loss: 0.0154 | Grad Norm: 0.00808272\n",
      "Epoch 2 | Step 1215200 | Avg Loss: 0.0155 | Grad Norm: 0.00843309\n",
      "Epoch 2 | Step 1215300 | Avg Loss: 0.0159 | Grad Norm: 0.00826735\n",
      "Epoch 2 | Step 1215400 | Avg Loss: 0.0159 | Grad Norm: 0.00952796\n",
      "Epoch 2 | Step 1215500 | Avg Loss: 0.0157 | Grad Norm: 0.00819843\n",
      "Epoch 2 | Step 1215600 | Avg Loss: 0.0158 | Grad Norm: 0.00883748\n",
      "Epoch 2 | Step 1215700 | Avg Loss: 0.0161 | Grad Norm: 0.00863062\n",
      "Epoch 2 | Step 1215800 | Avg Loss: 0.0157 | Grad Norm: 0.00896579\n",
      "Epoch 2 | Step 1215900 | Avg Loss: 0.0154 | Grad Norm: 0.00925468\n",
      "Epoch 2 | Step 1216000 | Avg Loss: 0.0154 | Grad Norm: 0.00874569\n",
      "Epoch 2 | Step 1216100 | Avg Loss: 0.0156 | Grad Norm: 0.00938229\n",
      "Epoch 2 | Step 1216200 | Avg Loss: 0.0153 | Grad Norm: 0.00779638\n",
      "Epoch 2 | Step 1216300 | Avg Loss: 0.0154 | Grad Norm: 0.00778114\n",
      "Epoch 2 | Step 1216400 | Avg Loss: 0.0154 | Grad Norm: 0.00826108\n",
      "Epoch 2 | Step 1216500 | Avg Loss: 0.0152 | Grad Norm: 0.00876538\n",
      "Epoch 2 | Step 1216600 | Avg Loss: 0.0152 | Grad Norm: 0.00845758\n",
      "Epoch 2 | Step 1216700 | Avg Loss: 0.0156 | Grad Norm: 0.00846999\n",
      "Epoch 2 | Step 1216800 | Avg Loss: 0.0154 | Grad Norm: 0.00860368\n",
      "Epoch 2 | Step 1216900 | Avg Loss: 0.0155 | Grad Norm: 0.00869500\n",
      "Epoch 2 | Step 1217000 | Avg Loss: 0.0157 | Grad Norm: 0.00823978\n",
      "Epoch 2 | Step 1217100 | Avg Loss: 0.0156 | Grad Norm: 0.00785237\n",
      "Epoch 2 | Step 1217200 | Avg Loss: 0.0153 | Grad Norm: 0.00976836\n",
      "Epoch 2 | Step 1217300 | Avg Loss: 0.0153 | Grad Norm: 0.00772074\n",
      "Epoch 2 | Step 1217400 | Avg Loss: 0.0155 | Grad Norm: 0.00909564\n",
      "Epoch 2 | Step 1217500 | Avg Loss: 0.0156 | Grad Norm: 0.00988058\n",
      "Epoch 2 | Step 1217600 | Avg Loss: 0.0158 | Grad Norm: 0.00991490\n",
      "Epoch 2 | Step 1217700 | Avg Loss: 0.0159 | Grad Norm: 0.00806318\n",
      "Epoch 2 | Step 1217800 | Avg Loss: 0.0158 | Grad Norm: 0.00933676\n",
      "Epoch 2 | Step 1217900 | Avg Loss: 0.0157 | Grad Norm: 0.00919359\n",
      "Epoch 2 | Step 1218000 | Avg Loss: 0.0158 | Grad Norm: 0.00899294\n",
      "Epoch 2 | Step 1218100 | Avg Loss: 0.0161 | Grad Norm: 0.01045217\n",
      "Epoch 2 | Step 1218200 | Avg Loss: 0.0156 | Grad Norm: 0.00877393\n",
      "Epoch 2 | Step 1218300 | Avg Loss: 0.0153 | Grad Norm: 0.01024957\n",
      "Epoch 2 | Step 1218400 | Avg Loss: 0.0153 | Grad Norm: 0.00885370\n",
      "Epoch 2 | Step 1218500 | Avg Loss: 0.0158 | Grad Norm: 0.01001076\n",
      "Epoch 2 | Step 1218600 | Avg Loss: 0.0159 | Grad Norm: 0.00913918\n",
      "Epoch 2 | Step 1218700 | Avg Loss: 0.0165 | Grad Norm: 0.00969739\n",
      "Epoch 2 | Step 1218800 | Avg Loss: 0.0159 | Grad Norm: 0.01033214\n",
      "Epoch 2 | Step 1218900 | Avg Loss: 0.0159 | Grad Norm: 0.00832090\n",
      "Epoch 2 | Step 1219000 | Avg Loss: 0.0159 | Grad Norm: 0.00947553\n",
      "Epoch 2 | Step 1219100 | Avg Loss: 0.0159 | Grad Norm: 0.00798440\n",
      "Epoch 2 | Step 1219200 | Avg Loss: 0.0152 | Grad Norm: 0.00976922\n",
      "Epoch 2 | Step 1219300 | Avg Loss: 0.0155 | Grad Norm: 0.00834631\n",
      "Epoch 2 | Step 1219400 | Avg Loss: 0.0154 | Grad Norm: 0.00779774\n",
      "Epoch 2 | Step 1219500 | Avg Loss: 0.0157 | Grad Norm: 0.00791441\n",
      "Epoch 2 | Step 1219600 | Avg Loss: 0.0157 | Grad Norm: 0.00905996\n",
      "Epoch 2 | Step 1219700 | Avg Loss: 0.0156 | Grad Norm: 0.00846465\n",
      "Epoch 2 | Step 1219800 | Avg Loss: 0.0153 | Grad Norm: 0.00983583\n",
      "Epoch 2 | Step 1219900 | Avg Loss: 0.0151 | Grad Norm: 0.00913845\n",
      "Epoch 2 | Step 1220000 | Avg Loss: 0.0150 | Grad Norm: 0.00856613\n",
      "Epoch 2 | Step 1220100 | Avg Loss: 0.0151 | Grad Norm: 0.01001115\n",
      "Epoch 2 | Step 1220200 | Avg Loss: 0.0147 | Grad Norm: 0.00696971\n",
      "Epoch 2 | Step 1220300 | Avg Loss: 0.0149 | Grad Norm: 0.00834091\n",
      "Epoch 2 | Step 1220400 | Avg Loss: 0.0153 | Grad Norm: 0.00844403\n",
      "Epoch 2 | Step 1220500 | Avg Loss: 0.0155 | Grad Norm: 0.00830273\n",
      "Epoch 2 | Step 1220600 | Avg Loss: 0.0154 | Grad Norm: 0.01177012\n",
      "Epoch 2 | Step 1220700 | Avg Loss: 0.0158 | Grad Norm: 0.00878827\n",
      "Epoch 2 | Step 1220800 | Avg Loss: 0.0156 | Grad Norm: 0.00765633\n",
      "Epoch 2 | Step 1220900 | Avg Loss: 0.0160 | Grad Norm: 0.00981014\n",
      "Epoch 2 | Step 1221000 | Avg Loss: 0.0161 | Grad Norm: 0.00960102\n",
      "Epoch 2 | Step 1221100 | Avg Loss: 0.0163 | Grad Norm: 0.00877409\n",
      "Epoch 2 | Step 1221200 | Avg Loss: 0.0164 | Grad Norm: 0.00837509\n",
      "Epoch 2 | Step 1221300 | Avg Loss: 0.0163 | Grad Norm: 0.00886839\n",
      "Epoch 2 | Step 1221400 | Avg Loss: 0.0160 | Grad Norm: 0.01283190\n",
      "Epoch 2 | Step 1221500 | Avg Loss: 0.0157 | Grad Norm: 0.00851079\n",
      "Epoch 2 | Step 1221600 | Avg Loss: 0.0151 | Grad Norm: 0.00799940\n",
      "Epoch 2 | Step 1221700 | Avg Loss: 0.0150 | Grad Norm: 0.00876249\n",
      "Epoch 2 | Step 1221800 | Avg Loss: 0.0148 | Grad Norm: 0.00930399\n",
      "Epoch 2 | Step 1221900 | Avg Loss: 0.0153 | Grad Norm: 0.00949158\n",
      "Epoch 2 | Step 1222000 | Avg Loss: 0.0150 | Grad Norm: 0.00954935\n",
      "Epoch 2 | Step 1222100 | Avg Loss: 0.0147 | Grad Norm: 0.00806578\n",
      "Epoch 2 | Step 1222200 | Avg Loss: 0.0148 | Grad Norm: 0.00776377\n",
      "Epoch 2 | Step 1222300 | Avg Loss: 0.0153 | Grad Norm: 0.00804898\n",
      "Epoch 2 | Step 1222400 | Avg Loss: 0.0154 | Grad Norm: 0.00813148\n",
      "Epoch 2 | Step 1222500 | Avg Loss: 0.0154 | Grad Norm: 0.00831406\n",
      "Epoch 2 | Step 1222600 | Avg Loss: 0.0158 | Grad Norm: 0.00900769\n",
      "Epoch 2 | Step 1222700 | Avg Loss: 0.0161 | Grad Norm: 0.00911067\n",
      "Epoch 2 | Step 1222800 | Avg Loss: 0.0163 | Grad Norm: 0.01004078\n",
      "Epoch 2 | Step 1222900 | Avg Loss: 0.0160 | Grad Norm: 0.00833348\n",
      "Epoch 2 | Step 1223000 | Avg Loss: 0.0155 | Grad Norm: 0.00870308\n",
      "Epoch 2 | Step 1223100 | Avg Loss: 0.0160 | Grad Norm: 0.00914163\n",
      "Epoch 2 | Step 1223200 | Avg Loss: 0.0155 | Grad Norm: 0.00869993\n",
      "Epoch 2 | Step 1223300 | Avg Loss: 0.0153 | Grad Norm: 0.00795887\n",
      "Epoch 2 | Step 1223400 | Avg Loss: 0.0149 | Grad Norm: 0.00947071\n",
      "Epoch 2 | Step 1223500 | Avg Loss: 0.0150 | Grad Norm: 0.00800946\n",
      "Epoch 2 | Step 1223600 | Avg Loss: 0.0152 | Grad Norm: 0.00814203\n",
      "Epoch 2 | Step 1223700 | Avg Loss: 0.0154 | Grad Norm: 0.00777001\n",
      "Epoch 2 | Step 1223800 | Avg Loss: 0.0155 | Grad Norm: 0.00862453\n",
      "Epoch 2 | Step 1223900 | Avg Loss: 0.0157 | Grad Norm: 0.00909082\n",
      "Epoch 2 | Step 1224000 | Avg Loss: 0.0159 | Grad Norm: 0.00835943\n",
      "Epoch 2 | Step 1224100 | Avg Loss: 0.0155 | Grad Norm: 0.00822787\n",
      "Epoch 2 | Step 1224200 | Avg Loss: 0.0153 | Grad Norm: 0.00991329\n",
      "Epoch 2 | Step 1224300 | Avg Loss: 0.0150 | Grad Norm: 0.01117609\n",
      "Epoch 2 | Step 1224400 | Avg Loss: 0.0151 | Grad Norm: 0.00870210\n",
      "Epoch 2 | Step 1224500 | Avg Loss: 0.0156 | Grad Norm: 0.00873395\n",
      "Epoch 2 | Step 1224600 | Avg Loss: 0.0161 | Grad Norm: 0.00976774\n",
      "Epoch 2 | Step 1224700 | Avg Loss: 0.0160 | Grad Norm: 0.00965845\n",
      "Epoch 2 | Step 1224800 | Avg Loss: 0.0157 | Grad Norm: 0.00832902\n",
      "Epoch 2 | Step 1224900 | Avg Loss: 0.0158 | Grad Norm: 0.00854041\n",
      "Epoch 2 | Step 1225000 | Avg Loss: 0.0158 | Grad Norm: 0.00849340\n",
      "Epoch 2 | Step 1225100 | Avg Loss: 0.0158 | Grad Norm: 0.00902248\n",
      "Epoch 2 | Step 1225200 | Avg Loss: 0.0161 | Grad Norm: 0.00838010\n",
      "Epoch 2 | Step 1225300 | Avg Loss: 0.0162 | Grad Norm: 0.00896169\n",
      "Epoch 2 | Step 1225400 | Avg Loss: 0.0161 | Grad Norm: 0.01060747\n",
      "Epoch 2 | Step 1225500 | Avg Loss: 0.0161 | Grad Norm: 0.00933776\n",
      "Epoch 2 | Step 1225600 | Avg Loss: 0.0162 | Grad Norm: 0.00869755\n",
      "Epoch 2 | Step 1225700 | Avg Loss: 0.0163 | Grad Norm: 0.00873249\n",
      "Epoch 2 | Step 1225800 | Avg Loss: 0.0158 | Grad Norm: 0.00899562\n",
      "Epoch 2 | Step 1225900 | Avg Loss: 0.0156 | Grad Norm: 0.00834274\n",
      "Epoch 2 | Step 1226000 | Avg Loss: 0.0161 | Grad Norm: 0.00928329\n",
      "Epoch 2 | Step 1226100 | Avg Loss: 0.0164 | Grad Norm: 0.00856485\n",
      "Epoch 2 | Step 1226200 | Avg Loss: 0.0165 | Grad Norm: 0.01195122\n",
      "Epoch 2 | Step 1226300 | Avg Loss: 0.0158 | Grad Norm: 0.00868412\n",
      "Epoch 2 | Step 1226400 | Avg Loss: 0.0159 | Grad Norm: 0.00778215\n",
      "Epoch 2 | Step 1226500 | Avg Loss: 0.0158 | Grad Norm: 0.00853830\n",
      "Epoch 2 | Step 1226600 | Avg Loss: 0.0159 | Grad Norm: 0.00938586\n",
      "Epoch 2 | Step 1226700 | Avg Loss: 0.0161 | Grad Norm: 0.00872020\n",
      "Epoch 2 | Step 1226800 | Avg Loss: 0.0161 | Grad Norm: 0.00809775\n",
      "Epoch 2 | Step 1226900 | Avg Loss: 0.0160 | Grad Norm: 0.00846836\n",
      "Epoch 2 | Step 1227000 | Avg Loss: 0.0159 | Grad Norm: 0.00781602\n",
      "Epoch 2 | Step 1227100 | Avg Loss: 0.0156 | Grad Norm: 0.01182997\n",
      "Epoch 2 | Step 1227200 | Avg Loss: 0.0157 | Grad Norm: 0.00903582\n",
      "Epoch 2 | Step 1227300 | Avg Loss: 0.0159 | Grad Norm: 0.00943915\n",
      "Epoch 2 | Step 1227400 | Avg Loss: 0.0155 | Grad Norm: 0.00893677\n",
      "Epoch 2 | Step 1227500 | Avg Loss: 0.0153 | Grad Norm: 0.00763802\n",
      "Epoch 2 | Step 1227600 | Avg Loss: 0.0157 | Grad Norm: 0.00769989\n",
      "Epoch 2 | Step 1227700 | Avg Loss: 0.0156 | Grad Norm: 0.00862162\n",
      "Epoch 2 | Step 1227800 | Avg Loss: 0.0155 | Grad Norm: 0.00806650\n",
      "Epoch 2 | Step 1227900 | Avg Loss: 0.0158 | Grad Norm: 0.00794848\n",
      "Epoch 2 | Step 1228000 | Avg Loss: 0.0158 | Grad Norm: 0.00734903\n",
      "Epoch 2 | Step 1228100 | Avg Loss: 0.0157 | Grad Norm: 0.00871892\n",
      "Epoch 2 | Step 1228200 | Avg Loss: 0.0161 | Grad Norm: 0.00880624\n",
      "Epoch 2 | Step 1228300 | Avg Loss: 0.0162 | Grad Norm: 0.00825357\n",
      "Epoch 2 | Step 1228400 | Avg Loss: 0.0162 | Grad Norm: 0.00927417\n",
      "Epoch 2 | Step 1228500 | Avg Loss: 0.0161 | Grad Norm: 0.00989061\n",
      "Epoch 2 | Step 1228600 | Avg Loss: 0.0163 | Grad Norm: 0.01011184\n",
      "Epoch 2 | Step 1228700 | Avg Loss: 0.0158 | Grad Norm: 0.00898727\n",
      "Epoch 2 | Step 1228800 | Avg Loss: 0.0156 | Grad Norm: 0.00883219\n",
      "Epoch 2 | Step 1228900 | Avg Loss: 0.0155 | Grad Norm: 0.00787102\n",
      "Epoch 2 | Step 1229000 | Avg Loss: 0.0158 | Grad Norm: 0.00929992\n",
      "Epoch 2 | Step 1229100 | Avg Loss: 0.0162 | Grad Norm: 0.00877315\n",
      "Epoch 2 | Step 1229200 | Avg Loss: 0.0163 | Grad Norm: 0.00966033\n",
      "Epoch 2 | Step 1229300 | Avg Loss: 0.0163 | Grad Norm: 0.00931002\n",
      "Epoch 2 | Step 1229400 | Avg Loss: 0.0165 | Grad Norm: 0.00885538\n",
      "Epoch 2 | Step 1229500 | Avg Loss: 0.0163 | Grad Norm: 0.00865091\n",
      "Epoch 2 | Step 1229600 | Avg Loss: 0.0158 | Grad Norm: 0.01107124\n",
      "Epoch 2 | Step 1229700 | Avg Loss: 0.0158 | Grad Norm: 0.00861718\n",
      "Epoch 2 | Step 1229800 | Avg Loss: 0.0157 | Grad Norm: 0.00895908\n",
      "Epoch 2 | Step 1229900 | Avg Loss: 0.0155 | Grad Norm: 0.01046342\n",
      "Epoch 2 | Step 1230000 | Avg Loss: 0.0157 | Grad Norm: 0.00925620\n",
      "Epoch 2 | Step 1230100 | Avg Loss: 0.0160 | Grad Norm: 0.00873549\n",
      "Epoch 2 | Step 1230200 | Avg Loss: 0.0156 | Grad Norm: 0.00890276\n",
      "Epoch 2 | Step 1230300 | Avg Loss: 0.0155 | Grad Norm: 0.01107725\n",
      "Epoch 2 | Step 1230400 | Avg Loss: 0.0157 | Grad Norm: 0.00987643\n",
      "Epoch 2 | Step 1230500 | Avg Loss: 0.0160 | Grad Norm: 0.00920506\n",
      "Epoch 2 | Step 1230600 | Avg Loss: 0.0157 | Grad Norm: 0.00925058\n",
      "Epoch 2 | Step 1230700 | Avg Loss: 0.0156 | Grad Norm: 0.00791975\n",
      "Epoch 2 | Step 1230800 | Avg Loss: 0.0158 | Grad Norm: 0.01010796\n",
      "Epoch 2 | Step 1230900 | Avg Loss: 0.0158 | Grad Norm: 0.00922341\n",
      "Epoch 2 | Step 1231000 | Avg Loss: 0.0158 | Grad Norm: 0.00930656\n",
      "Epoch 2 | Step 1231100 | Avg Loss: 0.0157 | Grad Norm: 0.00897127\n",
      "Epoch 2 | Step 1231200 | Avg Loss: 0.0157 | Grad Norm: 0.00984134\n",
      "Epoch 2 | Step 1231300 | Avg Loss: 0.0155 | Grad Norm: 0.00807449\n",
      "Epoch 2 | Step 1231400 | Avg Loss: 0.0154 | Grad Norm: 0.00872895\n",
      "Epoch 2 | Step 1231500 | Avg Loss: 0.0152 | Grad Norm: 0.00811746\n",
      "Epoch 2 | Step 1231600 | Avg Loss: 0.0152 | Grad Norm: 0.00918583\n",
      "Epoch 2 | Step 1231700 | Avg Loss: 0.0154 | Grad Norm: 0.00916331\n",
      "Epoch 2 | Step 1231800 | Avg Loss: 0.0152 | Grad Norm: 0.00890326\n",
      "Epoch 2 | Step 1231900 | Avg Loss: 0.0153 | Grad Norm: 0.00934657\n",
      "Epoch 2 | Step 1232000 | Avg Loss: 0.0152 | Grad Norm: 0.00989885\n",
      "Epoch 2 | Step 1232100 | Avg Loss: 0.0153 | Grad Norm: 0.00892534\n",
      "Epoch 2 | Step 1232200 | Avg Loss: 0.0156 | Grad Norm: 0.00833014\n",
      "Epoch 2 | Step 1232300 | Avg Loss: 0.0156 | Grad Norm: 0.00955655\n",
      "Epoch 2 | Step 1232400 | Avg Loss: 0.0157 | Grad Norm: 0.00824056\n",
      "Epoch 2 | Step 1232500 | Avg Loss: 0.0159 | Grad Norm: 0.00887437\n",
      "Epoch 2 | Step 1232600 | Avg Loss: 0.0161 | Grad Norm: 0.00778100\n",
      "Epoch 2 | Step 1232700 | Avg Loss: 0.0157 | Grad Norm: 0.00966499\n",
      "Epoch 2 | Step 1232800 | Avg Loss: 0.0158 | Grad Norm: 0.00847782\n",
      "Epoch 2 | Step 1232900 | Avg Loss: 0.0157 | Grad Norm: 0.00950901\n",
      "Epoch 2 | Step 1233000 | Avg Loss: 0.0159 | Grad Norm: 0.00950011\n",
      "Epoch 2 | Step 1233100 | Avg Loss: 0.0159 | Grad Norm: 0.00897320\n",
      "Epoch 2 | Step 1233200 | Avg Loss: 0.0163 | Grad Norm: 0.01017132\n",
      "Epoch 2 | Step 1233300 | Avg Loss: 0.0160 | Grad Norm: 0.01013988\n",
      "Epoch 2 | Step 1233400 | Avg Loss: 0.0165 | Grad Norm: 0.00951972\n",
      "Epoch 2 | Step 1233500 | Avg Loss: 0.0165 | Grad Norm: 0.00950258\n",
      "Epoch 2 | Step 1233600 | Avg Loss: 0.0163 | Grad Norm: 0.00892958\n",
      "Epoch 2 | Step 1233700 | Avg Loss: 0.0162 | Grad Norm: 0.00904869\n",
      "Epoch 2 | Step 1233800 | Avg Loss: 0.0159 | Grad Norm: 0.00808211\n",
      "Epoch 2 | Step 1233900 | Avg Loss: 0.0161 | Grad Norm: 0.00879595\n",
      "Epoch 2 | Step 1234000 | Avg Loss: 0.0160 | Grad Norm: 0.00962201\n",
      "Epoch 2 | Step 1234100 | Avg Loss: 0.0156 | Grad Norm: 0.00824379\n",
      "Epoch 2 | Step 1234200 | Avg Loss: 0.0159 | Grad Norm: 0.01033110\n",
      "Epoch 2 | Step 1234300 | Avg Loss: 0.0154 | Grad Norm: 0.00995741\n",
      "Epoch 2 | Step 1234400 | Avg Loss: 0.0158 | Grad Norm: 0.00902645\n",
      "Epoch 2 | Step 1234500 | Avg Loss: 0.0160 | Grad Norm: 0.01012962\n",
      "Epoch 2 | Step 1234600 | Avg Loss: 0.0157 | Grad Norm: 0.00846746\n",
      "Epoch 2 | Step 1234700 | Avg Loss: 0.0157 | Grad Norm: 0.00884542\n",
      "Epoch 2 | Step 1234800 | Avg Loss: 0.0156 | Grad Norm: 0.00886608\n",
      "Epoch 2 | Step 1234900 | Avg Loss: 0.0156 | Grad Norm: 0.01321234\n",
      "Epoch 2 | Step 1235000 | Avg Loss: 0.0157 | Grad Norm: 0.00980356\n",
      "Epoch 2 | Step 1235100 | Avg Loss: 0.0155 | Grad Norm: 0.00858243\n",
      "Epoch 2 | Step 1235200 | Avg Loss: 0.0156 | Grad Norm: 0.00877365\n",
      "Epoch 2 | Step 1235300 | Avg Loss: 0.0157 | Grad Norm: 0.00959670\n",
      "Epoch 2 | Step 1235400 | Avg Loss: 0.0156 | Grad Norm: 0.00957523\n",
      "Epoch 2 | Step 1235500 | Avg Loss: 0.0159 | Grad Norm: 0.00831238\n",
      "Epoch 2 | Step 1235600 | Avg Loss: 0.0163 | Grad Norm: 0.00955917\n",
      "Epoch 2 | Step 1235700 | Avg Loss: 0.0164 | Grad Norm: 0.00849124\n",
      "Epoch 2 | Step 1235800 | Avg Loss: 0.0162 | Grad Norm: 0.00865611\n",
      "Epoch 2 | Step 1235900 | Avg Loss: 0.0165 | Grad Norm: 0.00886362\n",
      "Epoch 2 | Step 1236000 | Avg Loss: 0.0163 | Grad Norm: 0.00905273\n",
      "Epoch 2 | Step 1236100 | Avg Loss: 0.0162 | Grad Norm: 0.00786071\n",
      "Epoch 2 | Step 1236200 | Avg Loss: 0.0162 | Grad Norm: 0.00885179\n",
      "Epoch 2 | Step 1236300 | Avg Loss: 0.0167 | Grad Norm: 0.00860633\n",
      "Epoch 2 | Step 1236400 | Avg Loss: 0.0166 | Grad Norm: 0.00952596\n",
      "Epoch 2 | Step 1236500 | Avg Loss: 0.0160 | Grad Norm: 0.00896422\n",
      "Epoch 2 | Step 1236600 | Avg Loss: 0.0157 | Grad Norm: 0.00861376\n",
      "Epoch 2 | Step 1236700 | Avg Loss: 0.0162 | Grad Norm: 0.01186796\n",
      "Epoch 2 | Step 1236800 | Avg Loss: 0.0159 | Grad Norm: 0.00876547\n",
      "Epoch 2 | Step 1236900 | Avg Loss: 0.0155 | Grad Norm: 0.01112819\n",
      "Epoch 2 | Step 1237000 | Avg Loss: 0.0155 | Grad Norm: 0.00924652\n",
      "Epoch 2 | Step 1237100 | Avg Loss: 0.0152 | Grad Norm: 0.00857458\n",
      "Epoch 2 | Step 1237200 | Avg Loss: 0.0148 | Grad Norm: 0.00869649\n",
      "Epoch 2 | Step 1237300 | Avg Loss: 0.0148 | Grad Norm: 0.00794960\n",
      "Epoch 2 | Step 1237400 | Avg Loss: 0.0149 | Grad Norm: 0.00766218\n",
      "Epoch 2 | Step 1237500 | Avg Loss: 0.0152 | Grad Norm: 0.00779570\n",
      "Epoch 2 | Step 1237600 | Avg Loss: 0.0150 | Grad Norm: 0.00743218\n",
      "Epoch 2 | Step 1237700 | Avg Loss: 0.0151 | Grad Norm: 0.00794503\n",
      "Epoch 2 | Step 1237800 | Avg Loss: 0.0152 | Grad Norm: 0.00842165\n",
      "Epoch 2 | Step 1237900 | Avg Loss: 0.0154 | Grad Norm: 0.00815771\n",
      "Epoch 2 | Step 1238000 | Avg Loss: 0.0154 | Grad Norm: 0.00955494\n",
      "Epoch 2 | Step 1238100 | Avg Loss: 0.0155 | Grad Norm: 0.00899604\n",
      "Epoch 2 | Step 1238200 | Avg Loss: 0.0154 | Grad Norm: 0.00858554\n",
      "Epoch 2 | Step 1238300 | Avg Loss: 0.0153 | Grad Norm: 0.00741628\n",
      "Epoch 2 | Step 1238400 | Avg Loss: 0.0154 | Grad Norm: 0.00851105\n",
      "Epoch 2 | Step 1238500 | Avg Loss: 0.0147 | Grad Norm: 0.01249828\n",
      "Epoch 2 | Step 1238600 | Avg Loss: 0.0151 | Grad Norm: 0.00820858\n",
      "Epoch 2 | Step 1238700 | Avg Loss: 0.0153 | Grad Norm: 0.01008154\n",
      "Epoch 2 | Step 1238800 | Avg Loss: 0.0160 | Grad Norm: 0.00750608\n",
      "Epoch 2 | Step 1238900 | Avg Loss: 0.0157 | Grad Norm: 0.00954176\n",
      "Epoch 2 | Step 1239000 | Avg Loss: 0.0155 | Grad Norm: 0.00793004\n",
      "Epoch 2 | Step 1239100 | Avg Loss: 0.0155 | Grad Norm: 0.00799114\n",
      "Epoch 2 | Step 1239200 | Avg Loss: 0.0151 | Grad Norm: 0.00861095\n",
      "Epoch 2 | Step 1239300 | Avg Loss: 0.0157 | Grad Norm: 0.00880800\n",
      "Epoch 2 | Step 1239400 | Avg Loss: 0.0154 | Grad Norm: 0.00850539\n",
      "Epoch 2 | Step 1239500 | Avg Loss: 0.0157 | Grad Norm: 0.01013881\n",
      "Epoch 2 | Step 1239600 | Avg Loss: 0.0155 | Grad Norm: 0.00811206\n",
      "Epoch 2 | Step 1239700 | Avg Loss: 0.0154 | Grad Norm: 0.00764977\n",
      "Epoch 2 | Step 1239800 | Avg Loss: 0.0156 | Grad Norm: 0.00881763\n",
      "Epoch 2 | Step 1239900 | Avg Loss: 0.0158 | Grad Norm: 0.00868655\n",
      "Epoch 2 | Step 1240000 | Avg Loss: 0.0162 | Grad Norm: 0.00738286\n",
      "Epoch 2 | Step 1240100 | Avg Loss: 0.0162 | Grad Norm: 0.01027571\n",
      "Epoch 2 | Step 1240200 | Avg Loss: 0.0160 | Grad Norm: 0.00757196\n",
      "Epoch 2 | Step 1240300 | Avg Loss: 0.0158 | Grad Norm: 0.01020887\n",
      "Epoch 2 | Step 1240400 | Avg Loss: 0.0157 | Grad Norm: 0.00841344\n",
      "Epoch 2 | Step 1240500 | Avg Loss: 0.0155 | Grad Norm: 0.01003170\n",
      "Epoch 2 | Step 1240600 | Avg Loss: 0.0155 | Grad Norm: 0.00881206\n",
      "Epoch 2 | Step 1240700 | Avg Loss: 0.0153 | Grad Norm: 0.00850385\n",
      "Epoch 2 | Step 1240800 | Avg Loss: 0.0154 | Grad Norm: 0.00816886\n",
      "Epoch 2 | Step 1240900 | Avg Loss: 0.0154 | Grad Norm: 0.00869471\n",
      "Epoch 2 | Step 1241000 | Avg Loss: 0.0154 | Grad Norm: 0.00814875\n",
      "Epoch 2 | Step 1241100 | Avg Loss: 0.0152 | Grad Norm: 0.00954701\n",
      "Epoch 2 | Step 1241200 | Avg Loss: 0.0152 | Grad Norm: 0.00803471\n",
      "Epoch 2 | Step 1241300 | Avg Loss: 0.0152 | Grad Norm: 0.00751358\n",
      "Epoch 2 | Step 1241400 | Avg Loss: 0.0153 | Grad Norm: 0.00759583\n",
      "Epoch 2 | Step 1241500 | Avg Loss: 0.0154 | Grad Norm: 0.00823636\n",
      "Epoch 2 | Step 1241600 | Avg Loss: 0.0153 | Grad Norm: 0.00845726\n",
      "Epoch 2 | Step 1241700 | Avg Loss: 0.0154 | Grad Norm: 0.00829347\n",
      "Epoch 2 | Step 1241800 | Avg Loss: 0.0151 | Grad Norm: 0.00955637\n",
      "Epoch 2 | Step 1241900 | Avg Loss: 0.0152 | Grad Norm: 0.00896320\n",
      "Epoch 2 | Step 1242000 | Avg Loss: 0.0149 | Grad Norm: 0.00856626\n",
      "Epoch 2 | Step 1242100 | Avg Loss: 0.0148 | Grad Norm: 0.01022435\n",
      "Epoch 2 | Step 1242200 | Avg Loss: 0.0153 | Grad Norm: 0.01534550\n",
      "Epoch 2 | Step 1242300 | Avg Loss: 0.0153 | Grad Norm: 0.00887433\n",
      "Epoch 2 | Step 1242400 | Avg Loss: 0.0152 | Grad Norm: 0.00864405\n",
      "Epoch 2 | Step 1242500 | Avg Loss: 0.0150 | Grad Norm: 0.01163572\n",
      "Epoch 2 | Step 1242600 | Avg Loss: 0.0152 | Grad Norm: 0.00941293\n",
      "Epoch 2 | Step 1242700 | Avg Loss: 0.0150 | Grad Norm: 0.00859619\n",
      "Epoch 2 | Step 1242800 | Avg Loss: 0.0147 | Grad Norm: 0.00915686\n",
      "Epoch 2 | Step 1242900 | Avg Loss: 0.0151 | Grad Norm: 0.00903575\n",
      "Epoch 2 | Step 1243000 | Avg Loss: 0.0153 | Grad Norm: 0.01027364\n",
      "Epoch 2 | Step 1243100 | Avg Loss: 0.0151 | Grad Norm: 0.00865695\n",
      "Epoch 2 | Step 1243200 | Avg Loss: 0.0148 | Grad Norm: 0.00751330\n",
      "Epoch 2 | Step 1243300 | Avg Loss: 0.0148 | Grad Norm: 0.00896729\n",
      "Epoch 2 | Step 1243400 | Avg Loss: 0.0153 | Grad Norm: 0.00917558\n",
      "Epoch 2 | Step 1243500 | Avg Loss: 0.0153 | Grad Norm: 0.00968445\n",
      "Epoch 2 | Step 1243600 | Avg Loss: 0.0149 | Grad Norm: 0.00858772\n",
      "Epoch 2 | Step 1243700 | Avg Loss: 0.0153 | Grad Norm: 0.00898752\n",
      "Epoch 2 | Step 1243800 | Avg Loss: 0.0153 | Grad Norm: 0.01086106\n",
      "Epoch 2 | Step 1243900 | Avg Loss: 0.0155 | Grad Norm: 0.00877408\n",
      "Epoch 2 | Step 1244000 | Avg Loss: 0.0153 | Grad Norm: 0.00764923\n",
      "Epoch 2 | Step 1244100 | Avg Loss: 0.0154 | Grad Norm: 0.01084802\n",
      "Epoch 2 | Step 1244200 | Avg Loss: 0.0157 | Grad Norm: 0.00830721\n",
      "Epoch 2 | Step 1244300 | Avg Loss: 0.0158 | Grad Norm: 0.00886810\n",
      "Epoch 2 | Step 1244400 | Avg Loss: 0.0161 | Grad Norm: 0.01035855\n",
      "Epoch 2 | Step 1244500 | Avg Loss: 0.0158 | Grad Norm: 0.00829763\n",
      "Epoch 2 | Step 1244600 | Avg Loss: 0.0159 | Grad Norm: 0.00774772\n",
      "Epoch 2 | Step 1244700 | Avg Loss: 0.0159 | Grad Norm: 0.00935875\n",
      "Epoch 2 | Step 1244800 | Avg Loss: 0.0159 | Grad Norm: 0.00885427\n",
      "Epoch 2 | Step 1244900 | Avg Loss: 0.0155 | Grad Norm: 0.00864086\n",
      "Epoch 2 | Step 1245000 | Avg Loss: 0.0156 | Grad Norm: 0.01088373\n",
      "Epoch 2 | Step 1245100 | Avg Loss: 0.0155 | Grad Norm: 0.00899805\n",
      "Epoch 2 | Step 1245200 | Avg Loss: 0.0151 | Grad Norm: 0.00973289\n",
      "Epoch 2 | Step 1245300 | Avg Loss: 0.0154 | Grad Norm: 0.00826194\n",
      "Epoch 2 | Step 1245400 | Avg Loss: 0.0156 | Grad Norm: 0.00914916\n",
      "Epoch 2 | Step 1245500 | Avg Loss: 0.0158 | Grad Norm: 0.01084672\n",
      "Epoch 2 | Step 1245600 | Avg Loss: 0.0161 | Grad Norm: 0.00937641\n",
      "Epoch 2 | Step 1245700 | Avg Loss: 0.0158 | Grad Norm: 0.00829218\n",
      "Epoch 2 | Step 1245800 | Avg Loss: 0.0154 | Grad Norm: 0.00861049\n",
      "Epoch 2 | Step 1245900 | Avg Loss: 0.0155 | Grad Norm: 0.00925240\n",
      "Epoch 2 | Step 1246000 | Avg Loss: 0.0156 | Grad Norm: 0.00844560\n",
      "Epoch 2 | Step 1246100 | Avg Loss: 0.0155 | Grad Norm: 0.01059899\n",
      "Epoch 2 | Step 1246200 | Avg Loss: 0.0159 | Grad Norm: 0.00811728\n",
      "Epoch 2 | Step 1246300 | Avg Loss: 0.0157 | Grad Norm: 0.00832936\n",
      "Epoch 2 | Step 1246400 | Avg Loss: 0.0159 | Grad Norm: 0.00923955\n",
      "Epoch 2 | Step 1246500 | Avg Loss: 0.0158 | Grad Norm: 0.00881988\n",
      "Epoch 2 | Step 1246600 | Avg Loss: 0.0158 | Grad Norm: 0.00856999\n",
      "Epoch 2 | Step 1246700 | Avg Loss: 0.0157 | Grad Norm: 0.01137099\n",
      "Epoch 2 | Step 1246800 | Avg Loss: 0.0159 | Grad Norm: 0.00840642\n",
      "Epoch 2 | Step 1246900 | Avg Loss: 0.0156 | Grad Norm: 0.00814975\n",
      "Epoch 2 | Step 1247000 | Avg Loss: 0.0156 | Grad Norm: 0.00866555\n",
      "Epoch 2 | Step 1247100 | Avg Loss: 0.0160 | Grad Norm: 0.00803536\n",
      "Epoch 2 | Step 1247200 | Avg Loss: 0.0159 | Grad Norm: 0.00895632\n",
      "Epoch 2 | Step 1247300 | Avg Loss: 0.0160 | Grad Norm: 0.00905523\n",
      "Epoch 2 | Step 1247400 | Avg Loss: 0.0162 | Grad Norm: 0.01019240\n",
      "Epoch 2 | Step 1247500 | Avg Loss: 0.0159 | Grad Norm: 0.00912065\n",
      "Epoch 2 | Step 1247600 | Avg Loss: 0.0159 | Grad Norm: 0.01263163\n",
      "Epoch 2 | Step 1247700 | Avg Loss: 0.0158 | Grad Norm: 0.00795873\n",
      "Epoch 2 | Step 1247800 | Avg Loss: 0.0156 | Grad Norm: 0.00990202\n",
      "Epoch 2 | Step 1247900 | Avg Loss: 0.0155 | Grad Norm: 0.00941103\n",
      "Epoch 2 | Step 1248000 | Avg Loss: 0.0153 | Grad Norm: 0.00865951\n",
      "Epoch 2 | Step 1248100 | Avg Loss: 0.0155 | Grad Norm: 0.00784877\n",
      "Epoch 2 | Step 1248200 | Avg Loss: 0.0158 | Grad Norm: 0.00850194\n",
      "Epoch 2 | Step 1248300 | Avg Loss: 0.0154 | Grad Norm: 0.00850025\n",
      "Epoch 2 | Step 1248400 | Avg Loss: 0.0156 | Grad Norm: 0.00795050\n",
      "Epoch 2 | Step 1248500 | Avg Loss: 0.0158 | Grad Norm: 0.00902948\n",
      "Epoch 2 | Step 1248600 | Avg Loss: 0.0156 | Grad Norm: 0.01094632\n",
      "Epoch 2 | Step 1248700 | Avg Loss: 0.0156 | Grad Norm: 0.00820621\n",
      "Epoch 2 | Step 1248800 | Avg Loss: 0.0156 | Grad Norm: 0.01075247\n",
      "Epoch 2 | Step 1248900 | Avg Loss: 0.0158 | Grad Norm: 0.00974545\n",
      "Epoch 2 | Step 1249000 | Avg Loss: 0.0157 | Grad Norm: 0.00900678\n",
      "Epoch 2 | Step 1249100 | Avg Loss: 0.0158 | Grad Norm: 0.01000384\n",
      "Epoch 2 | Step 1249200 | Avg Loss: 0.0157 | Grad Norm: 0.00945588\n",
      "Epoch 2 | Step 1249300 | Avg Loss: 0.0152 | Grad Norm: 0.00869547\n",
      "Epoch 2 | Step 1249400 | Avg Loss: 0.0154 | Grad Norm: 0.00852745\n",
      "Epoch 2 | Step 1249500 | Avg Loss: 0.0155 | Grad Norm: 0.01130342\n",
      "Epoch 2 | Step 1249600 | Avg Loss: 0.0157 | Grad Norm: 0.00835936\n",
      "Epoch 2 | Step 1249700 | Avg Loss: 0.0158 | Grad Norm: 0.00827959\n",
      "Epoch 2 | Step 1249800 | Avg Loss: 0.0160 | Grad Norm: 0.00851232\n",
      "Epoch 2 | Step 1249900 | Avg Loss: 0.0158 | Grad Norm: 0.00826294\n",
      "Epoch 2 | Step 1250000 | Avg Loss: 0.0157 | Grad Norm: 0.00973481\n",
      "Epoch 2 | Step 1250100 | Avg Loss: 0.0155 | Grad Norm: 0.00863693\n",
      "Epoch 2 | Step 1250200 | Avg Loss: 0.0154 | Grad Norm: 0.01744831\n",
      "Epoch 2 | Step 1250300 | Avg Loss: 0.0155 | Grad Norm: 0.00839709\n",
      "Epoch 2 | Step 1250400 | Avg Loss: 0.0157 | Grad Norm: 0.01059252\n",
      "Epoch 2 | Step 1250500 | Avg Loss: 0.0157 | Grad Norm: 0.01030392\n",
      "Epoch 2 | Step 1250600 | Avg Loss: 0.0158 | Grad Norm: 0.00978004\n",
      "Epoch 2 | Step 1250700 | Avg Loss: 0.0156 | Grad Norm: 0.01003002\n",
      "Epoch 2 | Step 1250800 | Avg Loss: 0.0156 | Grad Norm: 0.00981821\n",
      "Epoch 2 | Step 1250900 | Avg Loss: 0.0156 | Grad Norm: 0.00778046\n",
      "Epoch 2 | Step 1251000 | Avg Loss: 0.0157 | Grad Norm: 0.01013049\n",
      "Epoch 2 | Step 1251100 | Avg Loss: 0.0157 | Grad Norm: 0.00852198\n",
      "Epoch 2 | Step 1251200 | Avg Loss: 0.0156 | Grad Norm: 0.00857341\n",
      "Epoch 2 | Step 1251300 | Avg Loss: 0.0157 | Grad Norm: 0.00896726\n",
      "Epoch 2 | Step 1251400 | Avg Loss: 0.0157 | Grad Norm: 0.00952677\n",
      "Epoch 2 | Step 1251500 | Avg Loss: 0.0157 | Grad Norm: 0.00755257\n",
      "Epoch 2 | Step 1251600 | Avg Loss: 0.0157 | Grad Norm: 0.00792087\n",
      "Epoch 2 | Step 1251700 | Avg Loss: 0.0155 | Grad Norm: 0.00786161\n",
      "Epoch 2 | Step 1251800 | Avg Loss: 0.0155 | Grad Norm: 0.00867390\n",
      "Epoch 2 | Step 1251900 | Avg Loss: 0.0154 | Grad Norm: 0.00820492\n",
      "Epoch 2 | Step 1252000 | Avg Loss: 0.0154 | Grad Norm: 0.00765188\n",
      "Epoch 2 | Step 1252100 | Avg Loss: 0.0152 | Grad Norm: 0.00864367\n",
      "Epoch 2 | Step 1252200 | Avg Loss: 0.0151 | Grad Norm: 0.00922401\n",
      "Epoch 2 | Step 1252300 | Avg Loss: 0.0153 | Grad Norm: 0.00798580\n",
      "Epoch 2 | Step 1252400 | Avg Loss: 0.0156 | Grad Norm: 0.00861840\n",
      "Epoch 2 | Step 1252500 | Avg Loss: 0.0155 | Grad Norm: 0.00932260\n",
      "Epoch 2 | Step 1252600 | Avg Loss: 0.0155 | Grad Norm: 0.00931209\n",
      "Epoch 2 | Step 1252700 | Avg Loss: 0.0156 | Grad Norm: 0.00939097\n",
      "Epoch 2 | Step 1252800 | Avg Loss: 0.0154 | Grad Norm: 0.00885812\n",
      "Epoch 2 | Step 1252900 | Avg Loss: 0.0156 | Grad Norm: 0.00992484\n",
      "Epoch 2 | Step 1253000 | Avg Loss: 0.0158 | Grad Norm: 0.01145485\n",
      "Epoch 2 | Step 1253100 | Avg Loss: 0.0157 | Grad Norm: 0.00855549\n",
      "Epoch 2 | Step 1253200 | Avg Loss: 0.0158 | Grad Norm: 0.00972193\n",
      "Epoch 2 | Step 1253300 | Avg Loss: 0.0155 | Grad Norm: 0.00865770\n",
      "Epoch 2 | Step 1253400 | Avg Loss: 0.0152 | Grad Norm: 0.00824272\n",
      "Epoch 2 | Step 1253500 | Avg Loss: 0.0154 | Grad Norm: 0.00838484\n",
      "Epoch 2 | Step 1253600 | Avg Loss: 0.0151 | Grad Norm: 0.00859624\n",
      "Epoch 2 | Step 1253700 | Avg Loss: 0.0152 | Grad Norm: 0.00917538\n",
      "Epoch 2 | Step 1253800 | Avg Loss: 0.0155 | Grad Norm: 0.00872608\n",
      "Epoch 2 | Step 1253900 | Avg Loss: 0.0155 | Grad Norm: 0.00939875\n",
      "Epoch 2 | Step 1254000 | Avg Loss: 0.0156 | Grad Norm: 0.00915683\n",
      "Epoch 2 | Step 1254100 | Avg Loss: 0.0157 | Grad Norm: 0.00881405\n",
      "Epoch 2 | Step 1254200 | Avg Loss: 0.0157 | Grad Norm: 0.00807892\n",
      "Epoch 2 | Step 1254300 | Avg Loss: 0.0157 | Grad Norm: 0.00766372\n",
      "Epoch 2 | Step 1254400 | Avg Loss: 0.0153 | Grad Norm: 0.00878265\n",
      "Epoch 2 | Step 1254500 | Avg Loss: 0.0154 | Grad Norm: 0.00948646\n",
      "Epoch 2 | Step 1254600 | Avg Loss: 0.0153 | Grad Norm: 0.00959610\n",
      "Epoch 2 | Step 1254700 | Avg Loss: 0.0157 | Grad Norm: 0.01001984\n",
      "Epoch 2 | Step 1254800 | Avg Loss: 0.0154 | Grad Norm: 0.00784531\n",
      "Epoch 2 | Step 1254900 | Avg Loss: 0.0156 | Grad Norm: 0.00970975\n",
      "Epoch 2 | Step 1255000 | Avg Loss: 0.0156 | Grad Norm: 0.00980427\n",
      "Epoch 2 | Step 1255100 | Avg Loss: 0.0158 | Grad Norm: 0.01064125\n",
      "Epoch 2 | Step 1255200 | Avg Loss: 0.0158 | Grad Norm: 0.00904545\n",
      "Epoch 2 | Step 1255300 | Avg Loss: 0.0157 | Grad Norm: 0.00989405\n",
      "Epoch 2 | Step 1255400 | Avg Loss: 0.0160 | Grad Norm: 0.01028783\n",
      "Epoch 2 | Step 1255500 | Avg Loss: 0.0157 | Grad Norm: 0.00946845\n",
      "Epoch 2 | Step 1255600 | Avg Loss: 0.0158 | Grad Norm: 0.00838474\n",
      "Epoch 2 | Step 1255700 | Avg Loss: 0.0162 | Grad Norm: 0.00889265\n",
      "Epoch 2 | Step 1255800 | Avg Loss: 0.0163 | Grad Norm: 0.00857247\n",
      "Epoch 2 | Step 1255900 | Avg Loss: 0.0162 | Grad Norm: 0.00862835\n",
      "Epoch 2 | Step 1256000 | Avg Loss: 0.0164 | Grad Norm: 0.00868520\n",
      "Epoch 2 | Step 1256100 | Avg Loss: 0.0161 | Grad Norm: 0.00822224\n",
      "Epoch 2 | Step 1256200 | Avg Loss: 0.0161 | Grad Norm: 0.00802338\n",
      "Epoch 2 | Step 1256300 | Avg Loss: 0.0160 | Grad Norm: 0.00904186\n",
      "Epoch 2 | Step 1256400 | Avg Loss: 0.0161 | Grad Norm: 0.00924008\n",
      "Epoch 2 | Step 1256500 | Avg Loss: 0.0159 | Grad Norm: 0.00888894\n",
      "Epoch 2 | Step 1256600 | Avg Loss: 0.0155 | Grad Norm: 0.00810170\n",
      "Epoch 2 | Step 1256700 | Avg Loss: 0.0154 | Grad Norm: 0.00859927\n",
      "Epoch 2 | Step 1256800 | Avg Loss: 0.0151 | Grad Norm: 0.00843370\n",
      "Epoch 2 | Step 1256900 | Avg Loss: 0.0151 | Grad Norm: 0.00882004\n",
      "Epoch 2 | Step 1257000 | Avg Loss: 0.0151 | Grad Norm: 0.00981259\n",
      "Epoch 2 | Step 1257100 | Avg Loss: 0.0151 | Grad Norm: 0.00905745\n",
      "Epoch 2 | Step 1257200 | Avg Loss: 0.0151 | Grad Norm: 0.00841617\n",
      "Epoch 2 | Step 1257300 | Avg Loss: 0.0149 | Grad Norm: 0.00777102\n",
      "Epoch 2 | Step 1257400 | Avg Loss: 0.0154 | Grad Norm: 0.00892070\n",
      "Epoch 2 | Step 1257500 | Avg Loss: 0.0154 | Grad Norm: 0.00780281\n",
      "Epoch 2 | Step 1257600 | Avg Loss: 0.0155 | Grad Norm: 0.00883085\n",
      "Epoch 2 | Step 1257700 | Avg Loss: 0.0157 | Grad Norm: 0.01084909\n",
      "Epoch 2 | Step 1257800 | Avg Loss: 0.0158 | Grad Norm: 0.00882913\n",
      "Epoch 2 | Step 1257900 | Avg Loss: 0.0159 | Grad Norm: 0.00863999\n",
      "Epoch 2 | Step 1258000 | Avg Loss: 0.0158 | Grad Norm: 0.00902420\n",
      "Epoch 2 | Step 1258100 | Avg Loss: 0.0158 | Grad Norm: 0.00968871\n",
      "Epoch 2 | Step 1258200 | Avg Loss: 0.0160 | Grad Norm: 0.01069515\n",
      "Epoch 2 | Step 1258300 | Avg Loss: 0.0158 | Grad Norm: 0.00827865\n",
      "Epoch 2 | Step 1258400 | Avg Loss: 0.0153 | Grad Norm: 0.00797354\n",
      "Epoch 2 | Step 1258500 | Avg Loss: 0.0155 | Grad Norm: 0.00937147\n",
      "Epoch 2 | Step 1258600 | Avg Loss: 0.0156 | Grad Norm: 0.01046455\n",
      "Epoch 2 | Step 1258700 | Avg Loss: 0.0158 | Grad Norm: 0.00877599\n",
      "Epoch 2 | Step 1258800 | Avg Loss: 0.0160 | Grad Norm: 0.00865254\n",
      "Epoch 2 | Step 1258900 | Avg Loss: 0.0159 | Grad Norm: 0.00974708\n",
      "Epoch 2 | Step 1259000 | Avg Loss: 0.0159 | Grad Norm: 0.00912749\n",
      "Epoch 2 | Step 1259100 | Avg Loss: 0.0158 | Grad Norm: 0.00766309\n",
      "Epoch 2 | Step 1259200 | Avg Loss: 0.0159 | Grad Norm: 0.00916540\n",
      "Epoch 2 | Step 1259300 | Avg Loss: 0.0163 | Grad Norm: 0.00863037\n",
      "Epoch 2 | Step 1259400 | Avg Loss: 0.0161 | Grad Norm: 0.00800093\n",
      "Epoch 2 | Step 1259500 | Avg Loss: 0.0162 | Grad Norm: 0.00803638\n",
      "Epoch 2 | Step 1259600 | Avg Loss: 0.0159 | Grad Norm: 0.00848843\n",
      "Epoch 2 | Step 1259700 | Avg Loss: 0.0161 | Grad Norm: 0.01166492\n",
      "Epoch 2 | Step 1259800 | Avg Loss: 0.0159 | Grad Norm: 0.00823903\n",
      "Epoch 2 | Step 1259900 | Avg Loss: 0.0160 | Grad Norm: 0.00979124\n",
      "Epoch 2 | Step 1260000 | Avg Loss: 0.0160 | Grad Norm: 0.00853318\n",
      "Epoch 2 | Step 1260100 | Avg Loss: 0.0155 | Grad Norm: 0.01049077\n",
      "Epoch 2 | Step 1260200 | Avg Loss: 0.0158 | Grad Norm: 0.00884975\n",
      "Epoch 2 | Step 1260300 | Avg Loss: 0.0156 | Grad Norm: 0.00960020\n",
      "Epoch 2 | Step 1260400 | Avg Loss: 0.0156 | Grad Norm: 0.00866622\n",
      "Epoch 2 | Step 1260500 | Avg Loss: 0.0153 | Grad Norm: 0.00941834\n",
      "Epoch 2 | Step 1260600 | Avg Loss: 0.0152 | Grad Norm: 0.00823318\n",
      "Epoch 2 | Step 1260700 | Avg Loss: 0.0158 | Grad Norm: 0.00886938\n",
      "Epoch 2 | Step 1260800 | Avg Loss: 0.0162 | Grad Norm: 0.00812058\n",
      "Epoch 2 | Step 1260900 | Avg Loss: 0.0161 | Grad Norm: 0.00889615\n",
      "Epoch 2 | Step 1261000 | Avg Loss: 0.0160 | Grad Norm: 0.00783618\n",
      "Epoch 2 | Step 1261100 | Avg Loss: 0.0160 | Grad Norm: 0.00989313\n",
      "Epoch 2 | Step 1261200 | Avg Loss: 0.0164 | Grad Norm: 0.00769355\n",
      "Epoch 2 | Step 1261300 | Avg Loss: 0.0167 | Grad Norm: 0.00939180\n",
      "Epoch 2 | Step 1261400 | Avg Loss: 0.0162 | Grad Norm: 0.00958590\n",
      "Epoch 2 | Step 1261500 | Avg Loss: 0.0161 | Grad Norm: 0.00890478\n",
      "Epoch 2 | Step 1261600 | Avg Loss: 0.0161 | Grad Norm: 0.00900493\n",
      "Epoch 2 | Step 1261700 | Avg Loss: 0.0157 | Grad Norm: 0.00907797\n",
      "Epoch 2 | Step 1261800 | Avg Loss: 0.0156 | Grad Norm: 0.00901843\n",
      "Epoch 2 | Step 1261900 | Avg Loss: 0.0153 | Grad Norm: 0.00847473\n",
      "Epoch 2 | Step 1262000 | Avg Loss: 0.0154 | Grad Norm: 0.00922254\n",
      "Epoch 2 | Step 1262100 | Avg Loss: 0.0156 | Grad Norm: 0.00744817\n",
      "Epoch 2 | Step 1262200 | Avg Loss: 0.0157 | Grad Norm: 0.00962398\n",
      "Epoch 2 | Step 1262300 | Avg Loss: 0.0159 | Grad Norm: 0.00965123\n",
      "Epoch 2 | Step 1262400 | Avg Loss: 0.0162 | Grad Norm: 0.00819122\n",
      "Epoch 2 | Step 1262500 | Avg Loss: 0.0162 | Grad Norm: 0.00903766\n",
      "Epoch 2 | Step 1262600 | Avg Loss: 0.0162 | Grad Norm: 0.00930746\n",
      "Epoch 2 | Step 1262700 | Avg Loss: 0.0160 | Grad Norm: 0.00892346\n",
      "Epoch 2 | Step 1262800 | Avg Loss: 0.0161 | Grad Norm: 0.00876032\n",
      "Epoch 2 | Step 1262900 | Avg Loss: 0.0161 | Grad Norm: 0.01030458\n",
      "Epoch 2 | Step 1263000 | Avg Loss: 0.0158 | Grad Norm: 0.00853977\n",
      "Epoch 2 | Step 1263100 | Avg Loss: 0.0157 | Grad Norm: 0.00947327\n",
      "Epoch 2 | Step 1263200 | Avg Loss: 0.0152 | Grad Norm: 0.00828784\n",
      "Epoch 2 | Step 1263300 | Avg Loss: 0.0154 | Grad Norm: 0.00970698\n",
      "Epoch 2 | Step 1263400 | Avg Loss: 0.0156 | Grad Norm: 0.00874734\n",
      "Epoch 2 | Step 1263500 | Avg Loss: 0.0153 | Grad Norm: 0.00740916\n",
      "Epoch 2 | Step 1263600 | Avg Loss: 0.0153 | Grad Norm: 0.00774785\n",
      "Epoch 2 | Step 1263700 | Avg Loss: 0.0157 | Grad Norm: 0.00894066\n",
      "Epoch 2 | Step 1263800 | Avg Loss: 0.0160 | Grad Norm: 0.00949634\n",
      "Epoch 2 | Step 1263900 | Avg Loss: 0.0161 | Grad Norm: 0.00890078\n",
      "Epoch 2 | Step 1264000 | Avg Loss: 0.0160 | Grad Norm: 0.00911742\n",
      "Epoch 2 | Step 1264100 | Avg Loss: 0.0161 | Grad Norm: 0.00937498\n",
      "Epoch 2 | Step 1264200 | Avg Loss: 0.0160 | Grad Norm: 0.00863659\n",
      "Epoch 2 | Step 1264300 | Avg Loss: 0.0158 | Grad Norm: 0.00892763\n",
      "Epoch 2 | Step 1264400 | Avg Loss: 0.0158 | Grad Norm: 0.01066170\n",
      "Epoch 2 | Step 1264500 | Avg Loss: 0.0156 | Grad Norm: 0.00837075\n",
      "Epoch 2 | Step 1264600 | Avg Loss: 0.0156 | Grad Norm: 0.01064449\n",
      "Epoch 2 | Step 1264700 | Avg Loss: 0.0156 | Grad Norm: 0.00817877\n",
      "Epoch 2 | Step 1264800 | Avg Loss: 0.0156 | Grad Norm: 0.00896695\n",
      "Epoch 2 | Step 1264900 | Avg Loss: 0.0154 | Grad Norm: 0.00966508\n",
      "Epoch 2 | Step 1265000 | Avg Loss: 0.0152 | Grad Norm: 0.00813268\n",
      "Epoch 2 | Step 1265100 | Avg Loss: 0.0155 | Grad Norm: 0.00858640\n",
      "Epoch 2 | Step 1265200 | Avg Loss: 0.0154 | Grad Norm: 0.00945904\n",
      "Epoch 2 | Step 1265300 | Avg Loss: 0.0156 | Grad Norm: 0.00896777\n",
      "Epoch 2 | Step 1265400 | Avg Loss: 0.0154 | Grad Norm: 0.00783727\n",
      "Epoch 2 | Step 1265500 | Avg Loss: 0.0157 | Grad Norm: 0.00885275\n",
      "Epoch 2 | Step 1265600 | Avg Loss: 0.0155 | Grad Norm: 0.00882850\n",
      "Epoch 2 | Step 1265700 | Avg Loss: 0.0158 | Grad Norm: 0.00975510\n",
      "Epoch 2 | Step 1265800 | Avg Loss: 0.0157 | Grad Norm: 0.01117804\n",
      "Epoch 2 | Step 1265900 | Avg Loss: 0.0156 | Grad Norm: 0.00949234\n",
      "Epoch 2 | Step 1266000 | Avg Loss: 0.0156 | Grad Norm: 0.00839735\n",
      "Epoch 2 | Step 1266100 | Avg Loss: 0.0157 | Grad Norm: 0.01316868\n",
      "Epoch 2 | Step 1266200 | Avg Loss: 0.0158 | Grad Norm: 0.00833555\n",
      "Epoch 2 | Step 1266300 | Avg Loss: 0.0158 | Grad Norm: 0.00786334\n",
      "Epoch 2 | Step 1266400 | Avg Loss: 0.0154 | Grad Norm: 0.00777053\n",
      "Epoch 2 | Step 1266500 | Avg Loss: 0.0150 | Grad Norm: 0.00781769\n",
      "Epoch 2 | Step 1266600 | Avg Loss: 0.0150 | Grad Norm: 0.00792320\n",
      "Epoch 2 | Step 1266700 | Avg Loss: 0.0154 | Grad Norm: 0.00775011\n",
      "Epoch 2 | Step 1266800 | Avg Loss: 0.0154 | Grad Norm: 0.00769179\n",
      "Epoch 2 | Step 1266900 | Avg Loss: 0.0153 | Grad Norm: 0.00926378\n",
      "Epoch 2 | Step 1267000 | Avg Loss: 0.0154 | Grad Norm: 0.00819604\n",
      "Epoch 2 | Step 1267100 | Avg Loss: 0.0154 | Grad Norm: 0.00764410\n",
      "Epoch 2 | Step 1267200 | Avg Loss: 0.0153 | Grad Norm: 0.00760863\n",
      "Epoch 2 | Step 1267300 | Avg Loss: 0.0156 | Grad Norm: 0.00729113\n",
      "Epoch 2 | Step 1267400 | Avg Loss: 0.0156 | Grad Norm: 0.00914306\n",
      "Epoch 2 | Step 1267500 | Avg Loss: 0.0156 | Grad Norm: 0.00907472\n",
      "Epoch 2 | Step 1267600 | Avg Loss: 0.0159 | Grad Norm: 0.01325115\n",
      "Epoch 2 | Step 1267700 | Avg Loss: 0.0161 | Grad Norm: 0.00841903\n",
      "Epoch 2 | Step 1267800 | Avg Loss: 0.0158 | Grad Norm: 0.00838327\n",
      "Epoch 2 | Step 1267900 | Avg Loss: 0.0156 | Grad Norm: 0.00749848\n",
      "Epoch 2 | Step 1268000 | Avg Loss: 0.0155 | Grad Norm: 0.01072337\n",
      "Epoch 2 | Step 1268100 | Avg Loss: 0.0155 | Grad Norm: 0.00843711\n",
      "Epoch 2 | Step 1268200 | Avg Loss: 0.0156 | Grad Norm: 0.00840661\n",
      "Epoch 2 | Step 1268300 | Avg Loss: 0.0157 | Grad Norm: 0.00936126\n",
      "Epoch 2 | Step 1268400 | Avg Loss: 0.0161 | Grad Norm: 0.00818859\n",
      "Epoch 2 | Step 1268500 | Avg Loss: 0.0159 | Grad Norm: 0.00935393\n",
      "Epoch 2 | Step 1268600 | Avg Loss: 0.0156 | Grad Norm: 0.00832525\n",
      "Epoch 2 | Step 1268700 | Avg Loss: 0.0152 | Grad Norm: 0.00850736\n",
      "Epoch 2 | Step 1268800 | Avg Loss: 0.0151 | Grad Norm: 0.00832937\n",
      "Epoch 2 | Step 1268900 | Avg Loss: 0.0148 | Grad Norm: 0.01029235\n",
      "Epoch 2 | Step 1269000 | Avg Loss: 0.0149 | Grad Norm: 0.00865453\n",
      "Epoch 2 | Step 1269100 | Avg Loss: 0.0152 | Grad Norm: 0.00930043\n",
      "Epoch 2 | Step 1269200 | Avg Loss: 0.0152 | Grad Norm: 0.00918158\n",
      "Epoch 2 | Step 1269300 | Avg Loss: 0.0155 | Grad Norm: 0.00929357\n",
      "Epoch 2 | Step 1269400 | Avg Loss: 0.0153 | Grad Norm: 0.00820519\n",
      "Epoch 2 | Step 1269500 | Avg Loss: 0.0149 | Grad Norm: 0.00941923\n",
      "Epoch 2 | Step 1269600 | Avg Loss: 0.0155 | Grad Norm: 0.00937747\n",
      "Epoch 2 | Step 1269700 | Avg Loss: 0.0157 | Grad Norm: 0.00814700\n",
      "Epoch 2 | Step 1269800 | Avg Loss: 0.0154 | Grad Norm: 0.00977014\n",
      "Epoch 2 | Step 1269900 | Avg Loss: 0.0157 | Grad Norm: 0.00914812\n",
      "Epoch 2 | Step 1270000 | Avg Loss: 0.0160 | Grad Norm: 0.00940134\n",
      "Epoch 2 | Step 1270100 | Avg Loss: 0.0161 | Grad Norm: 0.00816187\n",
      "Epoch 2 | Step 1270200 | Avg Loss: 0.0163 | Grad Norm: 0.00960705\n",
      "Epoch 2 | Step 1270300 | Avg Loss: 0.0160 | Grad Norm: 0.00849538\n",
      "Epoch 2 | Step 1270400 | Avg Loss: 0.0157 | Grad Norm: 0.00931517\n",
      "Epoch 2 | Step 1270500 | Avg Loss: 0.0156 | Grad Norm: 0.01030821\n",
      "Epoch 2 | Step 1270600 | Avg Loss: 0.0158 | Grad Norm: 0.00969128\n",
      "Epoch 2 | Step 1270700 | Avg Loss: 0.0157 | Grad Norm: 0.00916478\n",
      "Epoch 2 | Step 1270800 | Avg Loss: 0.0154 | Grad Norm: 0.00999922\n",
      "Epoch 2 | Step 1270900 | Avg Loss: 0.0158 | Grad Norm: 0.01024231\n",
      "Epoch 2 | Step 1271000 | Avg Loss: 0.0158 | Grad Norm: 0.00810289\n",
      "Epoch 2 | Step 1271100 | Avg Loss: 0.0160 | Grad Norm: 0.01152899\n",
      "Epoch 2 | Step 1271200 | Avg Loss: 0.0160 | Grad Norm: 0.00863854\n",
      "Epoch 2 | Step 1271300 | Avg Loss: 0.0161 | Grad Norm: 0.01018724\n",
      "Epoch 2 | Step 1271400 | Avg Loss: 0.0163 | Grad Norm: 0.01062985\n",
      "Epoch 2 | Step 1271500 | Avg Loss: 0.0165 | Grad Norm: 0.01208873\n",
      "Epoch 2 | Step 1271600 | Avg Loss: 0.0164 | Grad Norm: 0.00903627\n",
      "Epoch 2 | Step 1271700 | Avg Loss: 0.0162 | Grad Norm: 0.00862946\n",
      "Epoch 2 | Step 1271800 | Avg Loss: 0.0162 | Grad Norm: 0.01033283\n",
      "Epoch 2 | Step 1271900 | Avg Loss: 0.0161 | Grad Norm: 0.00861970\n",
      "Epoch 2 | Step 1272000 | Avg Loss: 0.0159 | Grad Norm: 0.01116413\n",
      "Epoch 2 | Step 1272100 | Avg Loss: 0.0155 | Grad Norm: 0.00856507\n",
      "Epoch 2 | Step 1272200 | Avg Loss: 0.0161 | Grad Norm: 0.00935004\n",
      "Epoch 2 | Step 1272300 | Avg Loss: 0.0158 | Grad Norm: 0.00850240\n",
      "Epoch 2 | Step 1272400 | Avg Loss: 0.0157 | Grad Norm: 0.00992765\n",
      "Epoch 2 | Step 1272500 | Avg Loss: 0.0154 | Grad Norm: 0.00936911\n",
      "Epoch 2 | Step 1272600 | Avg Loss: 0.0153 | Grad Norm: 0.00943482\n",
      "Epoch 2 | Step 1272700 | Avg Loss: 0.0154 | Grad Norm: 0.01073102\n",
      "Epoch 2 | Step 1272800 | Avg Loss: 0.0157 | Grad Norm: 0.00905911\n",
      "Epoch 2 | Step 1272900 | Avg Loss: 0.0155 | Grad Norm: 0.01040900\n",
      "Epoch 2 | Step 1273000 | Avg Loss: 0.0158 | Grad Norm: 0.00777034\n",
      "Epoch 2 | Step 1273100 | Avg Loss: 0.0154 | Grad Norm: 0.00862564\n",
      "Epoch 2 | Step 1273200 | Avg Loss: 0.0153 | Grad Norm: 0.01128945\n",
      "Epoch 2 | Step 1273300 | Avg Loss: 0.0153 | Grad Norm: 0.00869551\n",
      "Epoch 2 | Step 1273400 | Avg Loss: 0.0153 | Grad Norm: 0.00857528\n",
      "Epoch 2 | Step 1273500 | Avg Loss: 0.0150 | Grad Norm: 0.00849193\n",
      "Epoch 2 | Step 1273600 | Avg Loss: 0.0152 | Grad Norm: 0.01102443\n",
      "Epoch 2 | Step 1273700 | Avg Loss: 0.0156 | Grad Norm: 0.00820591\n",
      "Epoch 2 | Step 1273800 | Avg Loss: 0.0158 | Grad Norm: 0.00834258\n",
      "Epoch 2 | Step 1273900 | Avg Loss: 0.0159 | Grad Norm: 0.00956524\n",
      "Epoch 2 | Step 1274000 | Avg Loss: 0.0157 | Grad Norm: 0.01203078\n",
      "Epoch 2 | Step 1274100 | Avg Loss: 0.0155 | Grad Norm: 0.00904077\n",
      "Epoch 2 | Step 1274200 | Avg Loss: 0.0156 | Grad Norm: 0.00804626\n",
      "Epoch 2 | Step 1274300 | Avg Loss: 0.0156 | Grad Norm: 0.00968611\n",
      "Epoch 2 | Step 1274400 | Avg Loss: 0.0159 | Grad Norm: 0.00844637\n",
      "Epoch 2 | Step 1274500 | Avg Loss: 0.0160 | Grad Norm: 0.00966472\n",
      "Epoch 2 | Step 1274600 | Avg Loss: 0.0157 | Grad Norm: 0.00889110\n",
      "Epoch 2 | Step 1274700 | Avg Loss: 0.0161 | Grad Norm: 0.00863529\n",
      "Epoch 2 | Step 1274800 | Avg Loss: 0.0160 | Grad Norm: 0.00865692\n",
      "Epoch 2 | Step 1274900 | Avg Loss: 0.0159 | Grad Norm: 0.00984805\n",
      "Epoch 2 | Step 1275000 | Avg Loss: 0.0160 | Grad Norm: 0.00789144\n",
      "Epoch 2 | Step 1275100 | Avg Loss: 0.0162 | Grad Norm: 0.00980136\n",
      "Epoch 2 | Step 1275200 | Avg Loss: 0.0158 | Grad Norm: 0.00864889\n",
      "Epoch 2 | Step 1275300 | Avg Loss: 0.0155 | Grad Norm: 0.00817952\n",
      "Epoch 2 | Step 1275400 | Avg Loss: 0.0156 | Grad Norm: 0.00859718\n",
      "Epoch 2 | Step 1275500 | Avg Loss: 0.0158 | Grad Norm: 0.00894324\n",
      "Epoch 2 | Step 1275600 | Avg Loss: 0.0154 | Grad Norm: 0.01179632\n",
      "Epoch 2 | Step 1275700 | Avg Loss: 0.0153 | Grad Norm: 0.01217326\n",
      "Epoch 2 | Step 1275800 | Avg Loss: 0.0153 | Grad Norm: 0.00893851\n",
      "Epoch 2 | Step 1275900 | Avg Loss: 0.0154 | Grad Norm: 0.01211842\n",
      "Epoch 2 | Step 1276000 | Avg Loss: 0.0152 | Grad Norm: 0.00908611\n",
      "Epoch 2 | Step 1276100 | Avg Loss: 0.0158 | Grad Norm: 0.01007733\n",
      "Epoch 2 | Step 1276200 | Avg Loss: 0.0159 | Grad Norm: 0.00992097\n",
      "Epoch 2 | Step 1276300 | Avg Loss: 0.0159 | Grad Norm: 0.00867920\n",
      "Epoch 2 | Step 1276400 | Avg Loss: 0.0161 | Grad Norm: 0.01043241\n",
      "Epoch 2 | Step 1276500 | Avg Loss: 0.0161 | Grad Norm: 0.01002718\n",
      "Epoch 2 | Step 1276600 | Avg Loss: 0.0159 | Grad Norm: 0.00847245\n",
      "Epoch 2 | Step 1276700 | Avg Loss: 0.0160 | Grad Norm: 0.01085277\n",
      "Epoch 2 | Step 1276800 | Avg Loss: 0.0159 | Grad Norm: 0.00935906\n",
      "Epoch 2 | Step 1276900 | Avg Loss: 0.0158 | Grad Norm: 0.01065450\n",
      "Epoch 2 | Step 1277000 | Avg Loss: 0.0157 | Grad Norm: 0.00974850\n",
      "Epoch 2 | Step 1277100 | Avg Loss: 0.0155 | Grad Norm: 0.00888949\n",
      "Epoch 2 | Step 1277200 | Avg Loss: 0.0155 | Grad Norm: 0.01149728\n",
      "Epoch 2 | Step 1277300 | Avg Loss: 0.0153 | Grad Norm: 0.00832424\n",
      "Epoch 2 | Step 1277400 | Avg Loss: 0.0154 | Grad Norm: 0.00907321\n",
      "Epoch 2 | Step 1277500 | Avg Loss: 0.0156 | Grad Norm: 0.00979354\n",
      "Epoch 2 | Step 1277600 | Avg Loss: 0.0154 | Grad Norm: 0.00991555\n",
      "Epoch 2 | Step 1277700 | Avg Loss: 0.0151 | Grad Norm: 0.00908525\n",
      "Epoch 2 | Step 1277800 | Avg Loss: 0.0155 | Grad Norm: 0.00921166\n",
      "Epoch 2 | Step 1277900 | Avg Loss: 0.0157 | Grad Norm: 0.00974888\n",
      "Epoch 2 | Step 1278000 | Avg Loss: 0.0160 | Grad Norm: 0.00902820\n",
      "Epoch 2 | Step 1278100 | Avg Loss: 0.0163 | Grad Norm: 0.00933028\n",
      "Epoch 2 | Step 1278200 | Avg Loss: 0.0158 | Grad Norm: 0.00833175\n",
      "Epoch 2 | Step 1278300 | Avg Loss: 0.0154 | Grad Norm: 0.00803596\n",
      "Epoch 2 | Step 1278400 | Avg Loss: 0.0155 | Grad Norm: 0.00861980\n",
      "Epoch 2 | Step 1278500 | Avg Loss: 0.0156 | Grad Norm: 0.00842030\n",
      "Epoch 2 | Step 1278600 | Avg Loss: 0.0154 | Grad Norm: 0.00806240\n",
      "Epoch 2 | Step 1278700 | Avg Loss: 0.0154 | Grad Norm: 0.00775815\n",
      "Epoch 2 | Step 1278800 | Avg Loss: 0.0154 | Grad Norm: 0.00824151\n",
      "Epoch 2 | Step 1278900 | Avg Loss: 0.0156 | Grad Norm: 0.00834137\n",
      "Epoch 2 | Step 1279000 | Avg Loss: 0.0150 | Grad Norm: 0.00833767\n",
      "Epoch 2 | Step 1279100 | Avg Loss: 0.0150 | Grad Norm: 0.00916972\n",
      "Epoch 2 | Step 1279200 | Avg Loss: 0.0151 | Grad Norm: 0.00805884\n",
      "Epoch 2 | Step 1279300 | Avg Loss: 0.0150 | Grad Norm: 0.00805589\n",
      "Epoch 2 | Step 1279400 | Avg Loss: 0.0151 | Grad Norm: 0.00948607\n",
      "Epoch 2 | Step 1279500 | Avg Loss: 0.0150 | Grad Norm: 0.00934418\n",
      "Epoch 2 | Step 1279600 | Avg Loss: 0.0149 | Grad Norm: 0.00838894\n",
      "Epoch 2 | Step 1279700 | Avg Loss: 0.0151 | Grad Norm: 0.01029010\n",
      "Epoch 2 | Step 1279800 | Avg Loss: 0.0154 | Grad Norm: 0.00815735\n",
      "Epoch 2 | Step 1279900 | Avg Loss: 0.0157 | Grad Norm: 0.00853082\n",
      "Epoch 2 | Step 1280000 | Avg Loss: 0.0159 | Grad Norm: 0.00911905\n",
      "Epoch 2 | Step 1280100 | Avg Loss: 0.0159 | Grad Norm: 0.01300366\n",
      "Epoch 2 | Step 1280200 | Avg Loss: 0.0156 | Grad Norm: 0.00875160\n",
      "Epoch 2 | Step 1280300 | Avg Loss: 0.0160 | Grad Norm: 0.00804818\n",
      "Epoch 2 | Step 1280400 | Avg Loss: 0.0162 | Grad Norm: 0.00926070\n",
      "Epoch 2 | Step 1280500 | Avg Loss: 0.0157 | Grad Norm: 0.00827564\n",
      "Epoch 2 | Step 1280600 | Avg Loss: 0.0158 | Grad Norm: 0.00972844\n",
      "Epoch 2 | Step 1280700 | Avg Loss: 0.0156 | Grad Norm: 0.00820354\n",
      "Epoch 2 | Step 1280800 | Avg Loss: 0.0160 | Grad Norm: 0.00791548\n",
      "Epoch 2 | Step 1280900 | Avg Loss: 0.0160 | Grad Norm: 0.00838554\n",
      "Epoch 2 | Step 1281000 | Avg Loss: 0.0160 | Grad Norm: 0.00861239\n",
      "Epoch 2 | Step 1281100 | Avg Loss: 0.0161 | Grad Norm: 0.00903998\n",
      "Epoch 2 | Step 1281200 | Avg Loss: 0.0164 | Grad Norm: 0.01075219\n",
      "Epoch 2 | Step 1281300 | Avg Loss: 0.0164 | Grad Norm: 0.00923663\n",
      "Epoch 2 | Step 1281400 | Avg Loss: 0.0162 | Grad Norm: 0.01189149\n",
      "Epoch 2 | Step 1281500 | Avg Loss: 0.0158 | Grad Norm: 0.00960554\n",
      "Epoch 2 | Step 1281600 | Avg Loss: 0.0160 | Grad Norm: 0.00847157\n",
      "Epoch 2 | Step 1281700 | Avg Loss: 0.0159 | Grad Norm: 0.00801736\n",
      "Epoch 2 | Step 1281800 | Avg Loss: 0.0157 | Grad Norm: 0.01131676\n",
      "Epoch 2 | Step 1281900 | Avg Loss: 0.0158 | Grad Norm: 0.01180254\n",
      "Epoch 2 | Step 1282000 | Avg Loss: 0.0155 | Grad Norm: 0.00824767\n",
      "Epoch 2 | Step 1282100 | Avg Loss: 0.0155 | Grad Norm: 0.00797619\n",
      "Epoch 2 | Step 1282200 | Avg Loss: 0.0154 | Grad Norm: 0.01238927\n",
      "Epoch 2 | Step 1282300 | Avg Loss: 0.0158 | Grad Norm: 0.00924666\n",
      "Epoch 2 | Step 1282400 | Avg Loss: 0.0157 | Grad Norm: 0.00906188\n",
      "Epoch 2 | Step 1282500 | Avg Loss: 0.0155 | Grad Norm: 0.00862166\n",
      "Epoch 2 | Step 1282600 | Avg Loss: 0.0154 | Grad Norm: 0.00780527\n",
      "Epoch 2 | Step 1282700 | Avg Loss: 0.0155 | Grad Norm: 0.00915201\n",
      "Epoch 2 | Step 1282800 | Avg Loss: 0.0153 | Grad Norm: 0.01184401\n",
      "Epoch 2 | Step 1282900 | Avg Loss: 0.0152 | Grad Norm: 0.00955327\n",
      "Epoch 2 | Step 1283000 | Avg Loss: 0.0148 | Grad Norm: 0.00844213\n",
      "Epoch 2 | Step 1283100 | Avg Loss: 0.0144 | Grad Norm: 0.00889047\n",
      "Epoch 2 | Step 1283200 | Avg Loss: 0.0144 | Grad Norm: 0.00828577\n",
      "Epoch 2 | Step 1283300 | Avg Loss: 0.0146 | Grad Norm: 0.00966528\n",
      "Epoch 2 | Step 1283400 | Avg Loss: 0.0147 | Grad Norm: 0.00785721\n",
      "Epoch 2 | Step 1283500 | Avg Loss: 0.0147 | Grad Norm: 0.00808204\n",
      "Epoch 2 | Step 1283600 | Avg Loss: 0.0152 | Grad Norm: 0.00864878\n",
      "Epoch 2 | Step 1283700 | Avg Loss: 0.0153 | Grad Norm: 0.00751258\n",
      "Epoch 2 | Step 1283800 | Avg Loss: 0.0152 | Grad Norm: 0.00952059\n",
      "Epoch 2 | Step 1283900 | Avg Loss: 0.0148 | Grad Norm: 0.00916657\n",
      "Epoch 2 | Step 1284000 | Avg Loss: 0.0145 | Grad Norm: 0.00722873\n",
      "Epoch 2 | Step 1284100 | Avg Loss: 0.0146 | Grad Norm: 0.00820217\n",
      "Epoch 2 | Step 1284200 | Avg Loss: 0.0148 | Grad Norm: 0.00870964\n",
      "Epoch 2 | Step 1284300 | Avg Loss: 0.0149 | Grad Norm: 0.00865451\n",
      "Epoch 2 | Step 1284400 | Avg Loss: 0.0151 | Grad Norm: 0.00863248\n",
      "Epoch 2 | Step 1284500 | Avg Loss: 0.0156 | Grad Norm: 0.00861004\n",
      "Epoch 2 | Step 1284600 | Avg Loss: 0.0153 | Grad Norm: 0.00866277\n",
      "Epoch 2 | Step 1284700 | Avg Loss: 0.0150 | Grad Norm: 0.00850530\n",
      "Epoch 2 | Step 1284800 | Avg Loss: 0.0157 | Grad Norm: 0.00939318\n",
      "Epoch 2 | Step 1284900 | Avg Loss: 0.0159 | Grad Norm: 0.00792856\n",
      "Epoch 2 | Step 1285000 | Avg Loss: 0.0159 | Grad Norm: 0.00821350\n",
      "Epoch 2 | Step 1285100 | Avg Loss: 0.0157 | Grad Norm: 0.00930090\n",
      "Epoch 2 | Step 1285200 | Avg Loss: 0.0158 | Grad Norm: 0.00904905\n",
      "Epoch 2 | Step 1285300 | Avg Loss: 0.0159 | Grad Norm: 0.00811079\n",
      "Epoch 2 | Step 1285400 | Avg Loss: 0.0158 | Grad Norm: 0.00917168\n",
      "Epoch 2 | Step 1285500 | Avg Loss: 0.0159 | Grad Norm: 0.00876053\n",
      "Epoch 2 | Step 1285600 | Avg Loss: 0.0157 | Grad Norm: 0.00875750\n",
      "Epoch 2 | Step 1285700 | Avg Loss: 0.0158 | Grad Norm: 0.00996367\n",
      "Epoch 2 | Step 1285800 | Avg Loss: 0.0158 | Grad Norm: 0.00889811\n",
      "Epoch 2 | Step 1285900 | Avg Loss: 0.0159 | Grad Norm: 0.00818766\n",
      "Epoch 2 | Step 1286000 | Avg Loss: 0.0162 | Grad Norm: 0.00870736\n",
      "Epoch 2 | Step 1286100 | Avg Loss: 0.0164 | Grad Norm: 0.00837161\n",
      "Epoch 2 | Step 1286200 | Avg Loss: 0.0159 | Grad Norm: 0.00857630\n",
      "Epoch 2 | Step 1286300 | Avg Loss: 0.0154 | Grad Norm: 0.00942776\n",
      "Epoch 2 | Step 1286400 | Avg Loss: 0.0153 | Grad Norm: 0.00860664\n",
      "Epoch 2 | Step 1286500 | Avg Loss: 0.0156 | Grad Norm: 0.00905211\n",
      "Epoch 2 | Step 1286600 | Avg Loss: 0.0155 | Grad Norm: 0.00846167\n",
      "Epoch 2 | Step 1286700 | Avg Loss: 0.0159 | Grad Norm: 0.00949266\n",
      "Epoch 2 | Step 1286800 | Avg Loss: 0.0157 | Grad Norm: 0.00871946\n",
      "Epoch 2 | Step 1286900 | Avg Loss: 0.0163 | Grad Norm: 0.00806772\n",
      "Epoch 2 | Step 1287000 | Avg Loss: 0.0165 | Grad Norm: 0.00984767\n",
      "Epoch 2 | Step 1287100 | Avg Loss: 0.0166 | Grad Norm: 0.00878634\n",
      "Epoch 2 | Step 1287200 | Avg Loss: 0.0164 | Grad Norm: 0.01069756\n",
      "Epoch 2 | Step 1287300 | Avg Loss: 0.0164 | Grad Norm: 0.00827769\n",
      "Epoch 2 | Step 1287400 | Avg Loss: 0.0164 | Grad Norm: 0.01017600\n",
      "Epoch 2 | Step 1287500 | Avg Loss: 0.0160 | Grad Norm: 0.00939353\n",
      "Epoch 2 | Step 1287600 | Avg Loss: 0.0159 | Grad Norm: 0.00937673\n",
      "Epoch 2 | Step 1287700 | Avg Loss: 0.0159 | Grad Norm: 0.00943304\n",
      "Epoch 2 | Step 1287800 | Avg Loss: 0.0154 | Grad Norm: 0.00969971\n",
      "Epoch 2 | Step 1287900 | Avg Loss: 0.0157 | Grad Norm: 0.01260080\n",
      "Epoch 2 | Step 1288000 | Avg Loss: 0.0160 | Grad Norm: 0.00927493\n",
      "Epoch 2 | Step 1288100 | Avg Loss: 0.0160 | Grad Norm: 0.00807030\n",
      "Epoch 2 | Step 1288200 | Avg Loss: 0.0163 | Grad Norm: 0.00942354\n",
      "Epoch 2 | Step 1288300 | Avg Loss: 0.0164 | Grad Norm: 0.01139670\n",
      "Epoch 2 | Step 1288400 | Avg Loss: 0.0158 | Grad Norm: 0.00844528\n",
      "Epoch 2 | Step 1288500 | Avg Loss: 0.0156 | Grad Norm: 0.00890817\n",
      "Epoch 2 | Step 1288600 | Avg Loss: 0.0153 | Grad Norm: 0.00967033\n",
      "Epoch 2 | Step 1288700 | Avg Loss: 0.0153 | Grad Norm: 0.00787528\n",
      "Epoch 2 | Step 1288800 | Avg Loss: 0.0160 | Grad Norm: 0.00994580\n",
      "Epoch 2 | Step 1288900 | Avg Loss: 0.0159 | Grad Norm: 0.00841978\n",
      "Epoch 2 | Step 1289000 | Avg Loss: 0.0160 | Grad Norm: 0.00816676\n",
      "Epoch 2 | Step 1289100 | Avg Loss: 0.0162 | Grad Norm: 0.00834035\n",
      "Epoch 2 | Step 1289200 | Avg Loss: 0.0162 | Grad Norm: 0.00870938\n",
      "Epoch 2 | Step 1289300 | Avg Loss: 0.0162 | Grad Norm: 0.00944532\n",
      "Epoch 2 | Step 1289400 | Avg Loss: 0.0156 | Grad Norm: 0.00981964\n",
      "Epoch 2 | Step 1289500 | Avg Loss: 0.0156 | Grad Norm: 0.00846417\n",
      "Epoch 2 | Step 1289600 | Avg Loss: 0.0154 | Grad Norm: 0.00794223\n",
      "Epoch 2 | Step 1289700 | Avg Loss: 0.0158 | Grad Norm: 0.00880780\n",
      "Epoch 2 | Step 1289800 | Avg Loss: 0.0160 | Grad Norm: 0.00840899\n",
      "Epoch 2 | Step 1289900 | Avg Loss: 0.0157 | Grad Norm: 0.00878011\n",
      "Epoch 2 | Step 1290000 | Avg Loss: 0.0157 | Grad Norm: 0.01015644\n",
      "Epoch 2 | Step 1290100 | Avg Loss: 0.0158 | Grad Norm: 0.00822764\n",
      "Epoch 2 | Step 1290200 | Avg Loss: 0.0154 | Grad Norm: 0.00783068\n",
      "Epoch 2 | Step 1290300 | Avg Loss: 0.0152 | Grad Norm: 0.01053818\n",
      "Epoch 2 | Step 1290400 | Avg Loss: 0.0150 | Grad Norm: 0.00760445\n",
      "Epoch 2 | Step 1290500 | Avg Loss: 0.0152 | Grad Norm: 0.00876318\n",
      "Epoch 2 | Step 1290600 | Avg Loss: 0.0152 | Grad Norm: 0.00846097\n",
      "Epoch 2 | Step 1290700 | Avg Loss: 0.0152 | Grad Norm: 0.00794922\n",
      "Epoch 2 | Step 1290800 | Avg Loss: 0.0149 | Grad Norm: 0.00798155\n",
      "Epoch 2 | Step 1290900 | Avg Loss: 0.0146 | Grad Norm: 0.00851690\n",
      "Epoch 2 | Step 1291000 | Avg Loss: 0.0147 | Grad Norm: 0.00808173\n",
      "Epoch 2 | Step 1291100 | Avg Loss: 0.0147 | Grad Norm: 0.00857149\n",
      "Epoch 2 | Step 1291200 | Avg Loss: 0.0144 | Grad Norm: 0.00791126\n",
      "Epoch 2 | Step 1291300 | Avg Loss: 0.0144 | Grad Norm: 0.00945904\n",
      "Epoch 2 | Step 1291400 | Avg Loss: 0.0148 | Grad Norm: 0.00776923\n",
      "Epoch 2 | Step 1291500 | Avg Loss: 0.0150 | Grad Norm: 0.00749923\n",
      "Epoch 2 | Step 1291600 | Avg Loss: 0.0154 | Grad Norm: 0.00891475\n",
      "Epoch 2 | Step 1291700 | Avg Loss: 0.0152 | Grad Norm: 0.00882013\n",
      "Epoch 2 | Step 1291800 | Avg Loss: 0.0156 | Grad Norm: 0.00891140\n",
      "Epoch 2 | Step 1291900 | Avg Loss: 0.0163 | Grad Norm: 0.00916536\n",
      "Epoch 2 | Step 1292000 | Avg Loss: 0.0160 | Grad Norm: 0.00966527\n",
      "Epoch 2 | Step 1292100 | Avg Loss: 0.0157 | Grad Norm: 0.00803133\n",
      "Epoch 2 | Step 1292200 | Avg Loss: 0.0159 | Grad Norm: 0.00853126\n",
      "Epoch 2 | Step 1292300 | Avg Loss: 0.0159 | Grad Norm: 0.00753674\n",
      "Epoch 2 | Step 1292400 | Avg Loss: 0.0158 | Grad Norm: 0.00983587\n",
      "Epoch 2 | Step 1292500 | Avg Loss: 0.0162 | Grad Norm: 0.00861465\n",
      "Epoch 2 | Step 1292600 | Avg Loss: 0.0159 | Grad Norm: 0.00972413\n",
      "Epoch 2 | Step 1292700 | Avg Loss: 0.0160 | Grad Norm: 0.00895149\n",
      "Epoch 2 | Step 1292800 | Avg Loss: 0.0161 | Grad Norm: 0.00971625\n",
      "Epoch 2 | Step 1292900 | Avg Loss: 0.0161 | Grad Norm: 0.00903160\n",
      "Epoch 2 | Step 1293000 | Avg Loss: 0.0157 | Grad Norm: 0.01134278\n",
      "Epoch 2 | Step 1293100 | Avg Loss: 0.0156 | Grad Norm: 0.01078322\n",
      "Epoch 2 | Step 1293200 | Avg Loss: 0.0161 | Grad Norm: 0.00863048\n",
      "Epoch 2 | Step 1293300 | Avg Loss: 0.0158 | Grad Norm: 0.00857637\n",
      "Epoch 2 | Step 1293400 | Avg Loss: 0.0158 | Grad Norm: 0.00843224\n",
      "Epoch 2 | Step 1293500 | Avg Loss: 0.0156 | Grad Norm: 0.00928783\n",
      "Epoch 2 | Step 1293600 | Avg Loss: 0.0162 | Grad Norm: 0.00933442\n",
      "Epoch 2 | Step 1293700 | Avg Loss: 0.0163 | Grad Norm: 0.00806898\n",
      "Epoch 2 | Step 1293800 | Avg Loss: 0.0161 | Grad Norm: 0.00897177\n",
      "Epoch 2 | Step 1293900 | Avg Loss: 0.0157 | Grad Norm: 0.00838564\n",
      "Epoch 2 | Step 1294000 | Avg Loss: 0.0156 | Grad Norm: 0.00865431\n",
      "Epoch 2 | Step 1294100 | Avg Loss: 0.0152 | Grad Norm: 0.00960131\n",
      "Epoch 2 | Step 1294200 | Avg Loss: 0.0158 | Grad Norm: 0.00917699\n",
      "Epoch 2 | Step 1294300 | Avg Loss: 0.0159 | Grad Norm: 0.00972148\n",
      "Epoch 2 | Step 1294400 | Avg Loss: 0.0158 | Grad Norm: 0.00975987\n",
      "Epoch 2 | Step 1294500 | Avg Loss: 0.0157 | Grad Norm: 0.00899611\n",
      "Epoch 2 | Step 1294600 | Avg Loss: 0.0153 | Grad Norm: 0.00869161\n",
      "Epoch 2 | Step 1294700 | Avg Loss: 0.0154 | Grad Norm: 0.01030836\n",
      "Epoch 2 | Step 1294800 | Avg Loss: 0.0150 | Grad Norm: 0.00911580\n",
      "Epoch 2 | Step 1294900 | Avg Loss: 0.0150 | Grad Norm: 0.01065553\n",
      "Epoch 2 | Step 1295000 | Avg Loss: 0.0154 | Grad Norm: 0.00860879\n",
      "Epoch 2 | Step 1295100 | Avg Loss: 0.0160 | Grad Norm: 0.00900131\n",
      "Epoch 2 | Step 1295200 | Avg Loss: 0.0159 | Grad Norm: 0.00816812\n",
      "Epoch 2 | Step 1295300 | Avg Loss: 0.0157 | Grad Norm: 0.00868640\n",
      "Epoch 2 | Step 1295400 | Avg Loss: 0.0156 | Grad Norm: 0.00718211\n",
      "Epoch 2 | Step 1295500 | Avg Loss: 0.0159 | Grad Norm: 0.00829844\n",
      "Epoch 2 | Step 1295600 | Avg Loss: 0.0162 | Grad Norm: 0.00827727\n",
      "Epoch 2 | Step 1295700 | Avg Loss: 0.0162 | Grad Norm: 0.00852196\n",
      "Epoch 2 | Step 1295800 | Avg Loss: 0.0159 | Grad Norm: 0.01072682\n",
      "Epoch 2 | Step 1295900 | Avg Loss: 0.0160 | Grad Norm: 0.00876828\n",
      "Epoch 2 | Step 1296000 | Avg Loss: 0.0162 | Grad Norm: 0.00901774\n",
      "Epoch 2 | Step 1296100 | Avg Loss: 0.0164 | Grad Norm: 0.00821566\n",
      "Epoch 2 | Step 1296200 | Avg Loss: 0.0162 | Grad Norm: 0.00881858\n",
      "Epoch 2 | Step 1296300 | Avg Loss: 0.0161 | Grad Norm: 0.00888351\n",
      "Epoch 2 | Step 1296400 | Avg Loss: 0.0163 | Grad Norm: 0.00918672\n",
      "Epoch 2 | Step 1296500 | Avg Loss: 0.0160 | Grad Norm: 0.00862235\n",
      "Epoch 2 | Step 1296600 | Avg Loss: 0.0159 | Grad Norm: 0.00805192\n",
      "Epoch 2 | Step 1296700 | Avg Loss: 0.0159 | Grad Norm: 0.00896735\n",
      "Epoch 2 | Step 1296800 | Avg Loss: 0.0162 | Grad Norm: 0.00876079\n",
      "Epoch 2 | Step 1296900 | Avg Loss: 0.0161 | Grad Norm: 0.00898547\n",
      "Epoch 2 | Step 1297000 | Avg Loss: 0.0162 | Grad Norm: 0.00835166\n",
      "Epoch 2 | Step 1297100 | Avg Loss: 0.0163 | Grad Norm: 0.00931747\n",
      "Epoch 2 | Step 1297200 | Avg Loss: 0.0162 | Grad Norm: 0.00891555\n",
      "Epoch 2 | Step 1297300 | Avg Loss: 0.0157 | Grad Norm: 0.01117371\n",
      "Epoch 2 | Step 1297400 | Avg Loss: 0.0157 | Grad Norm: 0.01109880\n",
      "Epoch 2 | Step 1297500 | Avg Loss: 0.0157 | Grad Norm: 0.00867496\n",
      "Epoch 2 | Step 1297600 | Avg Loss: 0.0158 | Grad Norm: 0.01030419\n",
      "Epoch 2 | Step 1297700 | Avg Loss: 0.0160 | Grad Norm: 0.00936653\n",
      "Epoch 2 | Step 1297800 | Avg Loss: 0.0157 | Grad Norm: 0.00885155\n",
      "Epoch 2 | Step 1297900 | Avg Loss: 0.0161 | Grad Norm: 0.00844641\n",
      "Epoch 2 | Step 1298000 | Avg Loss: 0.0161 | Grad Norm: 0.00792696\n",
      "Epoch 2 | Step 1298100 | Avg Loss: 0.0161 | Grad Norm: 0.00968674\n",
      "Epoch 2 | Step 1298200 | Avg Loss: 0.0158 | Grad Norm: 0.00911400\n",
      "Epoch 2 | Step 1298300 | Avg Loss: 0.0157 | Grad Norm: 0.00793985\n",
      "Epoch 2 | Step 1298400 | Avg Loss: 0.0154 | Grad Norm: 0.00832863\n",
      "Epoch 2 | Step 1298500 | Avg Loss: 0.0155 | Grad Norm: 0.00822457\n",
      "Epoch 2 | Step 1298600 | Avg Loss: 0.0155 | Grad Norm: 0.00871355\n",
      "Epoch 2 | Step 1298700 | Avg Loss: 0.0152 | Grad Norm: 0.00945312\n",
      "Epoch 2 | Step 1298800 | Avg Loss: 0.0153 | Grad Norm: 0.00908776\n",
      "Epoch 2 | Step 1298900 | Avg Loss: 0.0154 | Grad Norm: 0.01082458\n",
      "Epoch 2 | Step 1299000 | Avg Loss: 0.0155 | Grad Norm: 0.00838696\n",
      "Epoch 2 | Step 1299100 | Avg Loss: 0.0150 | Grad Norm: 0.00857131\n",
      "Epoch 2 | Step 1299200 | Avg Loss: 0.0152 | Grad Norm: 0.01004247\n",
      "Epoch 2 | Step 1299300 | Avg Loss: 0.0152 | Grad Norm: 0.00925207\n",
      "Epoch 2 | Step 1299400 | Avg Loss: 0.0156 | Grad Norm: 0.00873238\n",
      "Epoch 2 | Step 1299500 | Avg Loss: 0.0157 | Grad Norm: 0.00741395\n",
      "Epoch 2 | Step 1299600 | Avg Loss: 0.0162 | Grad Norm: 0.00921253\n",
      "Epoch 2 | Step 1299700 | Avg Loss: 0.0161 | Grad Norm: 0.01105417\n",
      "Epoch 2 | Step 1299800 | Avg Loss: 0.0156 | Grad Norm: 0.00856572\n",
      "Epoch 2 | Step 1299900 | Avg Loss: 0.0156 | Grad Norm: 0.01241843\n",
      "Epoch 2 | Step 1300000 | Avg Loss: 0.0154 | Grad Norm: 0.00833427\n",
      "Saving model at step1300000\n",
      "Epoch 2 | Step 1300100 | Avg Loss: 0.0151 | Grad Norm: 0.00804896\n",
      "Epoch 2 | Step 1300200 | Avg Loss: 0.0154 | Grad Norm: 0.00784202\n",
      "Epoch 2 | Step 1300300 | Avg Loss: 0.0157 | Grad Norm: 0.00850309\n",
      "Epoch 2 | Step 1300400 | Avg Loss: 0.0155 | Grad Norm: 0.00828473\n",
      "Epoch 2 | Step 1300500 | Avg Loss: 0.0153 | Grad Norm: 0.00822412\n",
      "Epoch 2 | Step 1300600 | Avg Loss: 0.0154 | Grad Norm: 0.00849637\n",
      "Epoch 2 | Step 1300700 | Avg Loss: 0.0154 | Grad Norm: 0.00948094\n",
      "Epoch 2 | Step 1300800 | Avg Loss: 0.0151 | Grad Norm: 0.01031689\n",
      "Epoch 2 | Step 1300900 | Avg Loss: 0.0148 | Grad Norm: 0.00764480\n",
      "Epoch 2 | Step 1301000 | Avg Loss: 0.0148 | Grad Norm: 0.00922985\n",
      "Epoch 2 | Step 1301100 | Avg Loss: 0.0154 | Grad Norm: 0.00900874\n",
      "Epoch 2 | Step 1301200 | Avg Loss: 0.0156 | Grad Norm: 0.00882685\n",
      "Epoch 2 | Step 1301300 | Avg Loss: 0.0158 | Grad Norm: 0.00819604\n",
      "Epoch 2 | Step 1301400 | Avg Loss: 0.0156 | Grad Norm: 0.00848336\n",
      "Epoch 2 | Step 1301500 | Avg Loss: 0.0158 | Grad Norm: 0.00888710\n",
      "Epoch 2 | Step 1301600 | Avg Loss: 0.0158 | Grad Norm: 0.01087940\n",
      "Epoch 2 | Step 1301700 | Avg Loss: 0.0157 | Grad Norm: 0.00956544\n",
      "Epoch 2 | Step 1301800 | Avg Loss: 0.0155 | Grad Norm: 0.00899635\n",
      "Epoch 2 | Step 1301900 | Avg Loss: 0.0159 | Grad Norm: 0.00932484\n",
      "Epoch 2 | Step 1302000 | Avg Loss: 0.0159 | Grad Norm: 0.00844885\n",
      "Epoch 2 | Step 1302100 | Avg Loss: 0.0161 | Grad Norm: 0.00915155\n",
      "Epoch 2 | Step 1302200 | Avg Loss: 0.0156 | Grad Norm: 0.00851891\n",
      "Epoch 2 | Step 1302300 | Avg Loss: 0.0152 | Grad Norm: 0.00904949\n",
      "Epoch 2 | Step 1302400 | Avg Loss: 0.0150 | Grad Norm: 0.00961870\n",
      "Epoch 2 | Step 1302500 | Avg Loss: 0.0153 | Grad Norm: 0.00891641\n",
      "Epoch 2 | Step 1302600 | Avg Loss: 0.0154 | Grad Norm: 0.00848828\n",
      "Epoch 2 | Step 1302700 | Avg Loss: 0.0150 | Grad Norm: 0.00964131\n",
      "Epoch 2 | Step 1302800 | Avg Loss: 0.0153 | Grad Norm: 0.00815176\n",
      "Epoch 2 | Step 1302900 | Avg Loss: 0.0155 | Grad Norm: 0.00853976\n",
      "Epoch 2 | Step 1303000 | Avg Loss: 0.0160 | Grad Norm: 0.00897530\n",
      "Epoch 2 | Step 1303100 | Avg Loss: 0.0157 | Grad Norm: 0.00988801\n",
      "Epoch 2 | Step 1303200 | Avg Loss: 0.0158 | Grad Norm: 0.00875074\n",
      "Epoch 2 | Step 1303300 | Avg Loss: 0.0160 | Grad Norm: 0.00934263\n",
      "Epoch 2 | Step 1303400 | Avg Loss: 0.0156 | Grad Norm: 0.00899912\n",
      "Epoch 2 | Step 1303500 | Avg Loss: 0.0153 | Grad Norm: 0.00840385\n",
      "Epoch 2 | Step 1303600 | Avg Loss: 0.0152 | Grad Norm: 0.00906872\n",
      "Epoch 2 | Step 1303700 | Avg Loss: 0.0155 | Grad Norm: 0.01506680\n",
      "Epoch 2 | Step 1303800 | Avg Loss: 0.0155 | Grad Norm: 0.00903664\n",
      "Epoch 2 | Step 1303900 | Avg Loss: 0.0156 | Grad Norm: 0.00851240\n",
      "Epoch 2 | Step 1304000 | Avg Loss: 0.0153 | Grad Norm: 0.00866573\n",
      "Epoch 2 | Step 1304100 | Avg Loss: 0.0155 | Grad Norm: 0.00948168\n",
      "Epoch 2 | Step 1304200 | Avg Loss: 0.0157 | Grad Norm: 0.00880076\n",
      "Epoch 2 | Step 1304300 | Avg Loss: 0.0157 | Grad Norm: 0.00863040\n",
      "Epoch 2 | Step 1304400 | Avg Loss: 0.0157 | Grad Norm: 0.01040142\n",
      "Epoch 2 | Step 1304500 | Avg Loss: 0.0159 | Grad Norm: 0.00847024\n",
      "Epoch 2 | Step 1304600 | Avg Loss: 0.0157 | Grad Norm: 0.01341510\n",
      "Epoch 2 | Step 1304700 | Avg Loss: 0.0159 | Grad Norm: 0.00922517\n",
      "Epoch 2 | Step 1304800 | Avg Loss: 0.0161 | Grad Norm: 0.00836296\n",
      "Epoch 2 | Step 1304900 | Avg Loss: 0.0158 | Grad Norm: 0.00895759\n",
      "Epoch 2 | Step 1305000 | Avg Loss: 0.0155 | Grad Norm: 0.00973517\n",
      "Epoch 2 | Step 1305100 | Avg Loss: 0.0153 | Grad Norm: 0.00904165\n",
      "Epoch 2 | Step 1305200 | Avg Loss: 0.0152 | Grad Norm: 0.00814269\n",
      "Epoch 2 | Step 1305300 | Avg Loss: 0.0151 | Grad Norm: 0.00857079\n",
      "Epoch 2 | Step 1305400 | Avg Loss: 0.0150 | Grad Norm: 0.00947845\n",
      "Epoch 2 | Step 1305500 | Avg Loss: 0.0152 | Grad Norm: 0.01172499\n",
      "Epoch 2 | Step 1305600 | Avg Loss: 0.0153 | Grad Norm: 0.00863118\n",
      "Epoch 2 | Step 1305700 | Avg Loss: 0.0154 | Grad Norm: 0.00878748\n",
      "Epoch 2 | Step 1305800 | Avg Loss: 0.0153 | Grad Norm: 0.00969760\n",
      "Epoch 2 | Step 1305900 | Avg Loss: 0.0154 | Grad Norm: 0.01034576\n",
      "Epoch 2 | Step 1306000 | Avg Loss: 0.0153 | Grad Norm: 0.00992964\n",
      "Epoch 2 | Step 1306100 | Avg Loss: 0.0155 | Grad Norm: 0.00790661\n",
      "Epoch 2 | Step 1306200 | Avg Loss: 0.0155 | Grad Norm: 0.00840835\n",
      "Epoch 2 | Step 1306300 | Avg Loss: 0.0159 | Grad Norm: 0.00850722\n",
      "Epoch 2 | Step 1306400 | Avg Loss: 0.0158 | Grad Norm: 0.00859179\n",
      "Epoch 2 | Step 1306500 | Avg Loss: 0.0159 | Grad Norm: 0.00955460\n",
      "Epoch 2 | Step 1306600 | Avg Loss: 0.0158 | Grad Norm: 0.00817680\n",
      "Epoch 2 | Step 1306700 | Avg Loss: 0.0158 | Grad Norm: 0.00891735\n",
      "Epoch 2 | Step 1306800 | Avg Loss: 0.0160 | Grad Norm: 0.00839947\n",
      "Epoch 2 | Step 1306900 | Avg Loss: 0.0160 | Grad Norm: 0.00821740\n",
      "Epoch 2 | Step 1307000 | Avg Loss: 0.0159 | Grad Norm: 0.00912676\n",
      "Epoch 2 | Step 1307100 | Avg Loss: 0.0157 | Grad Norm: 0.00804564\n",
      "Epoch 2 | Step 1307200 | Avg Loss: 0.0153 | Grad Norm: 0.00835635\n",
      "Epoch 2 | Step 1307300 | Avg Loss: 0.0153 | Grad Norm: 0.01097953\n",
      "Epoch 2 | Step 1307400 | Avg Loss: 0.0155 | Grad Norm: 0.00767484\n",
      "Epoch 2 | Step 1307500 | Avg Loss: 0.0153 | Grad Norm: 0.00842823\n",
      "Epoch 2 | Step 1307600 | Avg Loss: 0.0149 | Grad Norm: 0.00890558\n",
      "Epoch 2 | Step 1307700 | Avg Loss: 0.0152 | Grad Norm: 0.00825087\n",
      "Epoch 2 | Step 1307800 | Avg Loss: 0.0151 | Grad Norm: 0.00878135\n",
      "Epoch 2 | Step 1307900 | Avg Loss: 0.0151 | Grad Norm: 0.00891090\n",
      "Epoch 2 | Step 1308000 | Avg Loss: 0.0154 | Grad Norm: 0.00958346\n",
      "Epoch 2 | Step 1308100 | Avg Loss: 0.0154 | Grad Norm: 0.00872299\n",
      "Epoch 2 | Step 1308200 | Avg Loss: 0.0157 | Grad Norm: 0.00817858\n",
      "Epoch 2 | Step 1308300 | Avg Loss: 0.0156 | Grad Norm: 0.00924749\n",
      "Epoch 2 | Step 1308400 | Avg Loss: 0.0155 | Grad Norm: 0.00847112\n",
      "Epoch 2 | Step 1308500 | Avg Loss: 0.0159 | Grad Norm: 0.00941457\n",
      "Epoch 2 | Step 1308600 | Avg Loss: 0.0162 | Grad Norm: 0.00827218\n",
      "Epoch 2 | Step 1308700 | Avg Loss: 0.0162 | Grad Norm: 0.00995308\n",
      "Epoch 2 | Step 1308800 | Avg Loss: 0.0161 | Grad Norm: 0.01050372\n",
      "Epoch 2 | Step 1308900 | Avg Loss: 0.0161 | Grad Norm: 0.00820648\n",
      "Epoch 2 | Step 1309000 | Avg Loss: 0.0161 | Grad Norm: 0.00847278\n",
      "Epoch 2 | Step 1309100 | Avg Loss: 0.0161 | Grad Norm: 0.00952109\n",
      "Epoch 2 | Step 1309200 | Avg Loss: 0.0156 | Grad Norm: 0.00967899\n",
      "Epoch 2 | Step 1309300 | Avg Loss: 0.0157 | Grad Norm: 0.00885659\n",
      "Epoch 2 | Step 1309400 | Avg Loss: 0.0152 | Grad Norm: 0.00903495\n",
      "Epoch 2 | Step 1309500 | Avg Loss: 0.0148 | Grad Norm: 0.00811594\n",
      "Epoch 2 | Step 1309600 | Avg Loss: 0.0151 | Grad Norm: 0.00901052\n",
      "Epoch 2 | Step 1309700 | Avg Loss: 0.0150 | Grad Norm: 0.01037062\n",
      "Epoch 2 | Step 1309800 | Avg Loss: 0.0154 | Grad Norm: 0.00989822\n",
      "Epoch 2 | Step 1309900 | Avg Loss: 0.0152 | Grad Norm: 0.00916787\n",
      "Epoch 2 | Step 1310000 | Avg Loss: 0.0153 | Grad Norm: 0.01079635\n",
      "Epoch 2 | Step 1310100 | Avg Loss: 0.0156 | Grad Norm: 0.00813863\n",
      "Epoch 2 | Step 1310200 | Avg Loss: 0.0155 | Grad Norm: 0.00749193\n",
      "Epoch 2 | Step 1310300 | Avg Loss: 0.0157 | Grad Norm: 0.00860573\n",
      "Epoch 2 | Step 1310400 | Avg Loss: 0.0159 | Grad Norm: 0.00828946\n",
      "Epoch 2 | Step 1310500 | Avg Loss: 0.0157 | Grad Norm: 0.00966084\n",
      "Epoch 2 | Step 1310600 | Avg Loss: 0.0159 | Grad Norm: 0.00830409\n",
      "Epoch 2 | Step 1310700 | Avg Loss: 0.0158 | Grad Norm: 0.01161583\n",
      "Epoch 2 | Step 1310800 | Avg Loss: 0.0157 | Grad Norm: 0.00860133\n",
      "Epoch 2 | Step 1310900 | Avg Loss: 0.0157 | Grad Norm: 0.00855461\n",
      "Epoch 2 | Step 1311000 | Avg Loss: 0.0157 | Grad Norm: 0.00997146\n",
      "Epoch 2 | Step 1311100 | Avg Loss: 0.0160 | Grad Norm: 0.01110398\n",
      "Epoch 2 | Step 1311200 | Avg Loss: 0.0156 | Grad Norm: 0.00915832\n",
      "Epoch 2 | Step 1311300 | Avg Loss: 0.0157 | Grad Norm: 0.00917643\n",
      "Epoch 2 | Step 1311400 | Avg Loss: 0.0158 | Grad Norm: 0.01038801\n",
      "Epoch 2 | Step 1311500 | Avg Loss: 0.0159 | Grad Norm: 0.00916026\n",
      "Epoch 2 | Step 1311600 | Avg Loss: 0.0158 | Grad Norm: 0.00948186\n",
      "Epoch 2 | Step 1311700 | Avg Loss: 0.0155 | Grad Norm: 0.00816892\n",
      "Epoch 2 | Step 1311800 | Avg Loss: 0.0153 | Grad Norm: 0.00811014\n",
      "Epoch 2 | Step 1311900 | Avg Loss: 0.0156 | Grad Norm: 0.01190930\n",
      "Epoch 2 | Step 1312000 | Avg Loss: 0.0154 | Grad Norm: 0.00881678\n",
      "Epoch 2 | Step 1312100 | Avg Loss: 0.0155 | Grad Norm: 0.00871309\n",
      "Epoch 2 | Step 1312200 | Avg Loss: 0.0152 | Grad Norm: 0.00973815\n",
      "Epoch 2 | Step 1312300 | Avg Loss: 0.0151 | Grad Norm: 0.00820872\n",
      "Epoch 2 | Step 1312400 | Avg Loss: 0.0150 | Grad Norm: 0.00842675\n",
      "Epoch 2 | Step 1312500 | Avg Loss: 0.0149 | Grad Norm: 0.00829901\n",
      "Epoch 2 | Step 1312600 | Avg Loss: 0.0153 | Grad Norm: 0.00979104\n",
      "Epoch 2 | Step 1312700 | Avg Loss: 0.0151 | Grad Norm: 0.00756040\n",
      "Epoch 2 | Step 1312800 | Avg Loss: 0.0153 | Grad Norm: 0.00841499\n",
      "Epoch 2 | Step 1312900 | Avg Loss: 0.0150 | Grad Norm: 0.00893426\n",
      "Epoch 2 | Step 1313000 | Avg Loss: 0.0154 | Grad Norm: 0.01018708\n",
      "Epoch 2 | Step 1313100 | Avg Loss: 0.0154 | Grad Norm: 0.00902699\n",
      "Epoch 2 | Step 1313200 | Avg Loss: 0.0156 | Grad Norm: 0.01090343\n",
      "Epoch 2 | Step 1313300 | Avg Loss: 0.0161 | Grad Norm: 0.01104802\n",
      "Epoch 2 | Step 1313400 | Avg Loss: 0.0158 | Grad Norm: 0.00989737\n",
      "Epoch 2 | Step 1313500 | Avg Loss: 0.0159 | Grad Norm: 0.01115510\n",
      "Epoch 2 | Step 1313600 | Avg Loss: 0.0157 | Grad Norm: 0.01036523\n",
      "Epoch 2 | Step 1313700 | Avg Loss: 0.0156 | Grad Norm: 0.00900383\n",
      "Epoch 2 | Step 1313800 | Avg Loss: 0.0154 | Grad Norm: 0.00747156\n",
      "Epoch 2 | Step 1313900 | Avg Loss: 0.0153 | Grad Norm: 0.00853217\n",
      "Epoch 2 | Step 1314000 | Avg Loss: 0.0154 | Grad Norm: 0.00854657\n",
      "Epoch 2 | Step 1314100 | Avg Loss: 0.0152 | Grad Norm: 0.00724564\n",
      "Epoch 2 | Step 1314200 | Avg Loss: 0.0148 | Grad Norm: 0.00725160\n",
      "Epoch 2 | Step 1314300 | Avg Loss: 0.0150 | Grad Norm: 0.00762486\n",
      "Epoch 2 | Step 1314400 | Avg Loss: 0.0151 | Grad Norm: 0.00836457\n",
      "Epoch 2 | Step 1314500 | Avg Loss: 0.0152 | Grad Norm: 0.00810568\n",
      "Epoch 2 | Step 1314600 | Avg Loss: 0.0157 | Grad Norm: 0.00786949\n",
      "Epoch 2 | Step 1314700 | Avg Loss: 0.0156 | Grad Norm: 0.00872295\n",
      "Epoch 2 | Step 1314800 | Avg Loss: 0.0156 | Grad Norm: 0.00960334\n",
      "Epoch 2 | Step 1314900 | Avg Loss: 0.0151 | Grad Norm: 0.00836938\n",
      "Epoch 2 | Step 1315000 | Avg Loss: 0.0154 | Grad Norm: 0.00853892\n",
      "Epoch 2 | Step 1315100 | Avg Loss: 0.0154 | Grad Norm: 0.00757920\n",
      "Epoch 2 | Step 1315200 | Avg Loss: 0.0155 | Grad Norm: 0.00804927\n",
      "Epoch 2 | Step 1315300 | Avg Loss: 0.0152 | Grad Norm: 0.00782359\n",
      "Epoch 2 | Step 1315400 | Avg Loss: 0.0152 | Grad Norm: 0.01147792\n",
      "Epoch 2 | Step 1315500 | Avg Loss: 0.0155 | Grad Norm: 0.00784131\n",
      "Epoch 2 | Step 1315600 | Avg Loss: 0.0153 | Grad Norm: 0.00931765\n",
      "Epoch 2 | Step 1315700 | Avg Loss: 0.0153 | Grad Norm: 0.00799157\n",
      "Epoch 2 | Step 1315800 | Avg Loss: 0.0153 | Grad Norm: 0.00873121\n",
      "Epoch 2 | Step 1315900 | Avg Loss: 0.0153 | Grad Norm: 0.00851102\n",
      "Epoch 2 | Step 1316000 | Avg Loss: 0.0153 | Grad Norm: 0.00960403\n",
      "Epoch 2 | Step 1316100 | Avg Loss: 0.0151 | Grad Norm: 0.00963863\n",
      "Epoch 2 | Step 1316200 | Avg Loss: 0.0155 | Grad Norm: 0.00827055\n",
      "Epoch 2 | Step 1316300 | Avg Loss: 0.0153 | Grad Norm: 0.00937251\n",
      "Epoch 2 | Step 1316400 | Avg Loss: 0.0153 | Grad Norm: 0.00872965\n",
      "Epoch 2 | Step 1316500 | Avg Loss: 0.0155 | Grad Norm: 0.00746317\n",
      "Epoch 2 | Step 1316600 | Avg Loss: 0.0148 | Grad Norm: 0.00748354\n",
      "Epoch 2 | Step 1316700 | Avg Loss: 0.0149 | Grad Norm: 0.00874614\n",
      "Epoch 2 | Step 1316800 | Avg Loss: 0.0151 | Grad Norm: 0.01009978\n",
      "Epoch 2 | Step 1316900 | Avg Loss: 0.0147 | Grad Norm: 0.00807409\n",
      "Epoch 2 | Step 1317000 | Avg Loss: 0.0152 | Grad Norm: 0.00957113\n",
      "Epoch 2 | Step 1317100 | Avg Loss: 0.0154 | Grad Norm: 0.00955020\n",
      "Epoch 2 | Step 1317200 | Avg Loss: 0.0153 | Grad Norm: 0.00867820\n",
      "Epoch 2 | Step 1317300 | Avg Loss: 0.0150 | Grad Norm: 0.00831387\n",
      "Epoch 2 | Step 1317400 | Avg Loss: 0.0150 | Grad Norm: 0.00986526\n",
      "Epoch 2 | Step 1317500 | Avg Loss: 0.0150 | Grad Norm: 0.00873048\n",
      "Epoch 2 | Step 1317600 | Avg Loss: 0.0152 | Grad Norm: 0.00758404\n",
      "Epoch 2 | Step 1317700 | Avg Loss: 0.0151 | Grad Norm: 0.00919696\n",
      "Epoch 2 | Step 1317800 | Avg Loss: 0.0157 | Grad Norm: 0.00952525\n",
      "Epoch 2 | Step 1317900 | Avg Loss: 0.0151 | Grad Norm: 0.00972306\n",
      "Epoch 2 | Step 1318000 | Avg Loss: 0.0149 | Grad Norm: 0.00827612\n",
      "Epoch 2 | Step 1318100 | Avg Loss: 0.0148 | Grad Norm: 0.00856213\n",
      "Epoch 2 | Step 1318200 | Avg Loss: 0.0147 | Grad Norm: 0.00895676\n",
      "Epoch 2 | Step 1318300 | Avg Loss: 0.0148 | Grad Norm: 0.00870906\n",
      "Epoch 2 | Step 1318400 | Avg Loss: 0.0149 | Grad Norm: 0.00914706\n",
      "Epoch 2 | Step 1318500 | Avg Loss: 0.0151 | Grad Norm: 0.00880054\n",
      "Epoch 2 | Step 1318600 | Avg Loss: 0.0150 | Grad Norm: 0.01041667\n",
      "Epoch 2 | Step 1318700 | Avg Loss: 0.0152 | Grad Norm: 0.00878846\n",
      "Epoch 2 | Step 1318800 | Avg Loss: 0.0151 | Grad Norm: 0.00872846\n",
      "Epoch 2 | Step 1318900 | Avg Loss: 0.0153 | Grad Norm: 0.00949843\n",
      "Epoch 2 | Step 1319000 | Avg Loss: 0.0155 | Grad Norm: 0.00848412\n",
      "Epoch 2 | Step 1319100 | Avg Loss: 0.0155 | Grad Norm: 0.00858378\n",
      "Epoch 2 | Step 1319200 | Avg Loss: 0.0156 | Grad Norm: 0.00891651\n",
      "Epoch 2 | Step 1319300 | Avg Loss: 0.0160 | Grad Norm: 0.00921011\n",
      "Epoch 2 | Step 1319400 | Avg Loss: 0.0158 | Grad Norm: 0.01238986\n",
      "Epoch 2 | Step 1319500 | Avg Loss: 0.0159 | Grad Norm: 0.00894594\n",
      "Epoch 2 | Step 1319600 | Avg Loss: 0.0159 | Grad Norm: 0.00962895\n",
      "Epoch 2 | Step 1319700 | Avg Loss: 0.0157 | Grad Norm: 0.00958071\n",
      "Epoch 2 | Step 1319800 | Avg Loss: 0.0160 | Grad Norm: 0.00961236\n",
      "Epoch 2 | Step 1319900 | Avg Loss: 0.0160 | Grad Norm: 0.00922484\n",
      "Epoch 2 | Step 1320000 | Avg Loss: 0.0162 | Grad Norm: 0.00854199\n",
      "Epoch 2 | Step 1320100 | Avg Loss: 0.0160 | Grad Norm: 0.00956528\n",
      "Epoch 2 | Step 1320200 | Avg Loss: 0.0162 | Grad Norm: 0.00837096\n",
      "Epoch 2 | Step 1320300 | Avg Loss: 0.0162 | Grad Norm: 0.00981641\n",
      "Epoch 2 | Step 1320400 | Avg Loss: 0.0160 | Grad Norm: 0.00962827\n",
      "Epoch 2 | Step 1320500 | Avg Loss: 0.0163 | Grad Norm: 0.01013835\n",
      "Epoch 2 | Step 1320600 | Avg Loss: 0.0161 | Grad Norm: 0.00839156\n",
      "Epoch 2 | Step 1320700 | Avg Loss: 0.0159 | Grad Norm: 0.00797323\n",
      "Epoch 2 | Step 1320800 | Avg Loss: 0.0156 | Grad Norm: 0.00807025\n",
      "Epoch 2 | Step 1320900 | Avg Loss: 0.0156 | Grad Norm: 0.00920818\n",
      "Epoch 2 | Step 1321000 | Avg Loss: 0.0157 | Grad Norm: 0.01005354\n",
      "Epoch 2 | Step 1321100 | Avg Loss: 0.0153 | Grad Norm: 0.01086253\n",
      "Epoch 2 | Step 1321200 | Avg Loss: 0.0157 | Grad Norm: 0.00914310\n",
      "Epoch 2 | Step 1321300 | Avg Loss: 0.0157 | Grad Norm: 0.00849913\n",
      "Epoch 2 | Step 1321400 | Avg Loss: 0.0162 | Grad Norm: 0.01141160\n",
      "Epoch 2 | Step 1321500 | Avg Loss: 0.0163 | Grad Norm: 0.00740528\n",
      "Epoch 2 | Step 1321600 | Avg Loss: 0.0162 | Grad Norm: 0.00917820\n",
      "Epoch 2 | Step 1321700 | Avg Loss: 0.0163 | Grad Norm: 0.00929518\n",
      "Epoch 2 | Step 1321800 | Avg Loss: 0.0163 | Grad Norm: 0.00941568\n",
      "Epoch 2 | Step 1321900 | Avg Loss: 0.0163 | Grad Norm: 0.00946457\n",
      "Epoch 2 | Step 1322000 | Avg Loss: 0.0166 | Grad Norm: 0.01022357\n",
      "Epoch 2 | Step 1322100 | Avg Loss: 0.0165 | Grad Norm: 0.00903386\n",
      "Epoch 2 | Step 1322200 | Avg Loss: 0.0165 | Grad Norm: 0.00961171\n",
      "Epoch 2 | Step 1322300 | Avg Loss: 0.0162 | Grad Norm: 0.00994769\n",
      "Epoch 2 | Step 1322400 | Avg Loss: 0.0158 | Grad Norm: 0.00921971\n",
      "Epoch 2 | Step 1322500 | Avg Loss: 0.0163 | Grad Norm: 0.00858763\n",
      "Epoch 2 | Step 1322600 | Avg Loss: 0.0165 | Grad Norm: 0.00875905\n",
      "Epoch 2 | Step 1322700 | Avg Loss: 0.0160 | Grad Norm: 0.00837912\n",
      "Epoch 2 | Step 1322800 | Avg Loss: 0.0158 | Grad Norm: 0.00867958\n",
      "Epoch 2 | Step 1322900 | Avg Loss: 0.0160 | Grad Norm: 0.00995589\n",
      "Epoch 2 | Step 1323000 | Avg Loss: 0.0156 | Grad Norm: 0.00930945\n",
      "Epoch 2 | Step 1323100 | Avg Loss: 0.0159 | Grad Norm: 0.00864672\n",
      "Epoch 2 | Step 1323200 | Avg Loss: 0.0156 | Grad Norm: 0.00873476\n",
      "Epoch 2 | Step 1323300 | Avg Loss: 0.0158 | Grad Norm: 0.00938440\n",
      "Epoch 2 | Step 1323400 | Avg Loss: 0.0160 | Grad Norm: 0.00853515\n",
      "Epoch 2 | Step 1323500 | Avg Loss: 0.0160 | Grad Norm: 0.00844920\n",
      "Epoch 2 | Step 1323600 | Avg Loss: 0.0160 | Grad Norm: 0.00922378\n",
      "Epoch 2 | Step 1323700 | Avg Loss: 0.0159 | Grad Norm: 0.00756331\n",
      "Epoch 2 | Step 1323800 | Avg Loss: 0.0159 | Grad Norm: 0.00782044\n",
      "Epoch 2 | Step 1323900 | Avg Loss: 0.0161 | Grad Norm: 0.00885915\n",
      "Epoch 2 | Step 1324000 | Avg Loss: 0.0159 | Grad Norm: 0.00801737\n",
      "Epoch 2 | Step 1324100 | Avg Loss: 0.0159 | Grad Norm: 0.00905095\n",
      "Epoch 2 | Step 1324200 | Avg Loss: 0.0160 | Grad Norm: 0.01062253\n",
      "Epoch 2 | Step 1324300 | Avg Loss: 0.0158 | Grad Norm: 0.00936311\n",
      "Epoch 2 | Step 1324400 | Avg Loss: 0.0162 | Grad Norm: 0.00970790\n",
      "Epoch 2 | Step 1324500 | Avg Loss: 0.0162 | Grad Norm: 0.00805241\n",
      "Epoch 2 | Step 1324600 | Avg Loss: 0.0163 | Grad Norm: 0.00925175\n",
      "Epoch 2 | Step 1324700 | Avg Loss: 0.0160 | Grad Norm: 0.00897046\n",
      "Epoch 2 | Step 1324800 | Avg Loss: 0.0159 | Grad Norm: 0.01017639\n",
      "Epoch 2 | Step 1324900 | Avg Loss: 0.0156 | Grad Norm: 0.00848736\n",
      "Epoch 2 | Step 1325000 | Avg Loss: 0.0155 | Grad Norm: 0.00834351\n",
      "Epoch 2 | Step 1325100 | Avg Loss: 0.0157 | Grad Norm: 0.00795601\n",
      "Epoch 2 | Step 1325200 | Avg Loss: 0.0156 | Grad Norm: 0.01002957\n",
      "Epoch 2 | Step 1325300 | Avg Loss: 0.0157 | Grad Norm: 0.00909211\n",
      "Epoch 2 | Step 1325400 | Avg Loss: 0.0156 | Grad Norm: 0.01010461\n",
      "Epoch 2 | Step 1325500 | Avg Loss: 0.0159 | Grad Norm: 0.00906528\n",
      "Epoch 2 | Step 1325600 | Avg Loss: 0.0161 | Grad Norm: 0.00942107\n",
      "Epoch 2 | Step 1325700 | Avg Loss: 0.0158 | Grad Norm: 0.00949503\n",
      "Epoch 2 | Step 1325800 | Avg Loss: 0.0156 | Grad Norm: 0.00966520\n",
      "Epoch 2 | Step 1325900 | Avg Loss: 0.0157 | Grad Norm: 0.01105942\n",
      "Epoch 2 | Step 1326000 | Avg Loss: 0.0157 | Grad Norm: 0.01029321\n",
      "Epoch 2 | Step 1326100 | Avg Loss: 0.0156 | Grad Norm: 0.01021539\n",
      "Epoch 2 | Step 1326200 | Avg Loss: 0.0153 | Grad Norm: 0.00875299\n",
      "Epoch 2 | Step 1326300 | Avg Loss: 0.0156 | Grad Norm: 0.00992987\n",
      "Epoch 2 | Step 1326400 | Avg Loss: 0.0159 | Grad Norm: 0.00809920\n",
      "Epoch 2 | Step 1326500 | Avg Loss: 0.0159 | Grad Norm: 0.00802145\n",
      "Epoch 2 | Step 1326600 | Avg Loss: 0.0159 | Grad Norm: 0.00936374\n",
      "Epoch 2 | Step 1326700 | Avg Loss: 0.0160 | Grad Norm: 0.00953535\n",
      "Epoch 2 | Step 1326800 | Avg Loss: 0.0154 | Grad Norm: 0.00883296\n",
      "Epoch 2 | Step 1326900 | Avg Loss: 0.0154 | Grad Norm: 0.01016094\n",
      "Epoch 2 | Step 1327000 | Avg Loss: 0.0148 | Grad Norm: 0.01023554\n",
      "Epoch 2 | Step 1327100 | Avg Loss: 0.0149 | Grad Norm: 0.00784496\n",
      "Epoch 2 | Step 1327200 | Avg Loss: 0.0153 | Grad Norm: 0.00828994\n",
      "Epoch 2 | Step 1327300 | Avg Loss: 0.0154 | Grad Norm: 0.00836063\n",
      "Epoch 2 | Step 1327400 | Avg Loss: 0.0156 | Grad Norm: 0.00915931\n",
      "Epoch 2 | Step 1327500 | Avg Loss: 0.0154 | Grad Norm: 0.00819052\n",
      "Epoch 2 | Step 1327600 | Avg Loss: 0.0152 | Grad Norm: 0.00765737\n",
      "Epoch 2 | Step 1327700 | Avg Loss: 0.0149 | Grad Norm: 0.00873093\n",
      "Epoch 2 | Step 1327800 | Avg Loss: 0.0147 | Grad Norm: 0.00956737\n",
      "Epoch 2 | Step 1327900 | Avg Loss: 0.0153 | Grad Norm: 0.00933285\n",
      "Epoch 2 | Step 1328000 | Avg Loss: 0.0154 | Grad Norm: 0.01266050\n",
      "Epoch 2 | Step 1328100 | Avg Loss: 0.0152 | Grad Norm: 0.00917529\n",
      "Epoch 2 | Step 1328200 | Avg Loss: 0.0153 | Grad Norm: 0.00870469\n",
      "Epoch 2 | Step 1328300 | Avg Loss: 0.0154 | Grad Norm: 0.00772609\n",
      "Epoch 2 | Step 1328400 | Avg Loss: 0.0155 | Grad Norm: 0.00784071\n",
      "Epoch 2 | Step 1328500 | Avg Loss: 0.0154 | Grad Norm: 0.00792243\n",
      "Epoch 2 | Step 1328600 | Avg Loss: 0.0157 | Grad Norm: 0.00864715\n",
      "Epoch 2 | Step 1328700 | Avg Loss: 0.0154 | Grad Norm: 0.00793405\n",
      "Epoch 2 | Step 1328800 | Avg Loss: 0.0154 | Grad Norm: 0.00906615\n",
      "Epoch 2 | Step 1328900 | Avg Loss: 0.0154 | Grad Norm: 0.01136330\n",
      "Epoch 2 | Step 1329000 | Avg Loss: 0.0152 | Grad Norm: 0.01087869\n",
      "Epoch 2 | Step 1329100 | Avg Loss: 0.0155 | Grad Norm: 0.01040843\n",
      "Epoch 2 | Step 1329200 | Avg Loss: 0.0157 | Grad Norm: 0.00855743\n",
      "Epoch 2 | Step 1329300 | Avg Loss: 0.0157 | Grad Norm: 0.00828168\n",
      "Epoch 2 | Step 1329400 | Avg Loss: 0.0159 | Grad Norm: 0.00839145\n",
      "Epoch 2 | Step 1329500 | Avg Loss: 0.0154 | Grad Norm: 0.00903778\n",
      "Epoch 2 | Step 1329600 | Avg Loss: 0.0155 | Grad Norm: 0.01068941\n",
      "Epoch 2 | Step 1329700 | Avg Loss: 0.0158 | Grad Norm: 0.00854076\n",
      "Epoch 2 | Step 1329800 | Avg Loss: 0.0155 | Grad Norm: 0.00927653\n",
      "Epoch 2 | Step 1329900 | Avg Loss: 0.0156 | Grad Norm: 0.00966674\n",
      "Epoch 2 | Step 1330000 | Avg Loss: 0.0160 | Grad Norm: 0.00922223\n",
      "Epoch 2 | Step 1330100 | Avg Loss: 0.0162 | Grad Norm: 0.00869496\n",
      "Epoch 2 | Step 1330200 | Avg Loss: 0.0159 | Grad Norm: 0.00932803\n",
      "Epoch 2 | Step 1330300 | Avg Loss: 0.0162 | Grad Norm: 0.00929188\n",
      "Epoch 2 | Step 1330400 | Avg Loss: 0.0160 | Grad Norm: 0.00935860\n",
      "Epoch 2 | Step 1330500 | Avg Loss: 0.0160 | Grad Norm: 0.00880973\n",
      "Epoch 2 | Step 1330600 | Avg Loss: 0.0159 | Grad Norm: 0.00833747\n",
      "Epoch 2 | Step 1330700 | Avg Loss: 0.0160 | Grad Norm: 0.00899735\n",
      "Epoch 2 | Step 1330800 | Avg Loss: 0.0159 | Grad Norm: 0.00799578\n",
      "Epoch 2 | Step 1330900 | Avg Loss: 0.0156 | Grad Norm: 0.01011400\n",
      "Epoch 2 | Step 1331000 | Avg Loss: 0.0155 | Grad Norm: 0.00837330\n",
      "Epoch 2 | Step 1331100 | Avg Loss: 0.0157 | Grad Norm: 0.00916270\n",
      "Epoch 2 | Step 1331200 | Avg Loss: 0.0156 | Grad Norm: 0.01077415\n",
      "Epoch 2 | Step 1331300 | Avg Loss: 0.0153 | Grad Norm: 0.01067886\n",
      "Epoch 2 | Step 1331400 | Avg Loss: 0.0154 | Grad Norm: 0.00937010\n",
      "Epoch 2 | Step 1331500 | Avg Loss: 0.0153 | Grad Norm: 0.00876299\n",
      "Epoch 2 | Step 1331600 | Avg Loss: 0.0151 | Grad Norm: 0.00813306\n",
      "Epoch 2 | Step 1331700 | Avg Loss: 0.0156 | Grad Norm: 0.00931439\n",
      "Epoch 2 | Step 1331800 | Avg Loss: 0.0155 | Grad Norm: 0.00935445\n",
      "Epoch 2 | Step 1331900 | Avg Loss: 0.0160 | Grad Norm: 0.00978553\n",
      "Epoch 2 | Step 1332000 | Avg Loss: 0.0157 | Grad Norm: 0.00886589\n",
      "Epoch 2 | Step 1332100 | Avg Loss: 0.0158 | Grad Norm: 0.00810693\n",
      "Epoch 2 | Step 1332200 | Avg Loss: 0.0156 | Grad Norm: 0.01025714\n",
      "Epoch 2 | Step 1332300 | Avg Loss: 0.0157 | Grad Norm: 0.00877345\n",
      "Epoch 2 | Step 1332400 | Avg Loss: 0.0156 | Grad Norm: 0.00826422\n",
      "Epoch 2 | Step 1332500 | Avg Loss: 0.0154 | Grad Norm: 0.00941532\n",
      "Epoch 2 | Step 1332600 | Avg Loss: 0.0156 | Grad Norm: 0.00881812\n",
      "Epoch 2 | Step 1332700 | Avg Loss: 0.0154 | Grad Norm: 0.00831363\n",
      "Epoch 2 | Step 1332800 | Avg Loss: 0.0153 | Grad Norm: 0.00873320\n",
      "Epoch 2 | Step 1332900 | Avg Loss: 0.0159 | Grad Norm: 0.00813645\n",
      "Epoch 2 | Step 1333000 | Avg Loss: 0.0159 | Grad Norm: 0.01052763\n",
      "Epoch 2 | Step 1333100 | Avg Loss: 0.0158 | Grad Norm: 0.00938495\n",
      "Epoch 2 | Step 1333200 | Avg Loss: 0.0157 | Grad Norm: 0.01015366\n",
      "Epoch 2 | Step 1333300 | Avg Loss: 0.0159 | Grad Norm: 0.00802819\n",
      "Epoch 2 | Step 1333400 | Avg Loss: 0.0157 | Grad Norm: 0.01017231\n",
      "Epoch 2 | Step 1333500 | Avg Loss: 0.0156 | Grad Norm: 0.00861492\n",
      "Epoch 2 | Step 1333600 | Avg Loss: 0.0155 | Grad Norm: 0.00812298\n",
      "Epoch 2 | Step 1333700 | Avg Loss: 0.0153 | Grad Norm: 0.00889592\n",
      "Epoch 2 | Step 1333800 | Avg Loss: 0.0156 | Grad Norm: 0.00814256\n",
      "Epoch 2 | Step 1333900 | Avg Loss: 0.0157 | Grad Norm: 0.00910397\n",
      "Epoch 2 | Step 1334000 | Avg Loss: 0.0161 | Grad Norm: 0.01050630\n",
      "Epoch 2 | Step 1334100 | Avg Loss: 0.0159 | Grad Norm: 0.01925807\n",
      "Epoch 2 | Step 1334200 | Avg Loss: 0.0155 | Grad Norm: 0.00900672\n",
      "Epoch 2 | Step 1334300 | Avg Loss: 0.0153 | Grad Norm: 0.00798136\n",
      "Epoch 2 | Step 1334400 | Avg Loss: 0.0153 | Grad Norm: 0.00853904\n",
      "Epoch 2 | Step 1334500 | Avg Loss: 0.0156 | Grad Norm: 0.00973328\n",
      "Epoch 2 | Step 1334600 | Avg Loss: 0.0161 | Grad Norm: 0.00957435\n",
      "Epoch 2 | Step 1334700 | Avg Loss: 0.0160 | Grad Norm: 0.00901606\n",
      "Epoch 2 | Step 1334800 | Avg Loss: 0.0158 | Grad Norm: 0.00932509\n",
      "Epoch 2 | Step 1334900 | Avg Loss: 0.0160 | Grad Norm: 0.00921547\n",
      "Epoch 2 | Step 1335000 | Avg Loss: 0.0156 | Grad Norm: 0.00862362\n",
      "Epoch 2 | Step 1335100 | Avg Loss: 0.0157 | Grad Norm: 0.00842507\n",
      "Epoch 2 | Step 1335200 | Avg Loss: 0.0154 | Grad Norm: 0.00905165\n",
      "Epoch 2 | Step 1335300 | Avg Loss: 0.0154 | Grad Norm: 0.01019050\n",
      "Epoch 2 | Step 1335400 | Avg Loss: 0.0155 | Grad Norm: 0.00994471\n",
      "Epoch 2 | Step 1335500 | Avg Loss: 0.0155 | Grad Norm: 0.00873195\n",
      "Epoch 2 | Step 1335600 | Avg Loss: 0.0154 | Grad Norm: 0.00955567\n",
      "Epoch 2 | Step 1335700 | Avg Loss: 0.0158 | Grad Norm: 0.00807285\n",
      "Epoch 2 | Step 1335800 | Avg Loss: 0.0159 | Grad Norm: 0.00911893\n",
      "Epoch 2 | Step 1335900 | Avg Loss: 0.0158 | Grad Norm: 0.00887149\n",
      "Epoch 2 | Step 1336000 | Avg Loss: 0.0154 | Grad Norm: 0.00999250\n",
      "Epoch 2 | Step 1336100 | Avg Loss: 0.0155 | Grad Norm: 0.00960298\n",
      "Epoch 2 | Step 1336200 | Avg Loss: 0.0158 | Grad Norm: 0.00773407\n",
      "Epoch 2 | Step 1336300 | Avg Loss: 0.0157 | Grad Norm: 0.00982125\n",
      "Epoch 2 | Step 1336400 | Avg Loss: 0.0157 | Grad Norm: 0.01017334\n",
      "Epoch 2 | Step 1336500 | Avg Loss: 0.0160 | Grad Norm: 0.01006911\n",
      "Epoch 2 | Step 1336600 | Avg Loss: 0.0156 | Grad Norm: 0.00815224\n",
      "Epoch 2 | Step 1336700 | Avg Loss: 0.0157 | Grad Norm: 0.00943713\n",
      "Epoch 2 | Step 1336800 | Avg Loss: 0.0161 | Grad Norm: 0.00826312\n",
      "Epoch 2 | Step 1336900 | Avg Loss: 0.0159 | Grad Norm: 0.00860775\n",
      "Epoch 2 | Step 1337000 | Avg Loss: 0.0158 | Grad Norm: 0.00862864\n",
      "Epoch 2 | Step 1337100 | Avg Loss: 0.0157 | Grad Norm: 0.00817188\n",
      "Epoch 2 | Step 1337200 | Avg Loss: 0.0162 | Grad Norm: 0.01091020\n",
      "Epoch 2 | Step 1337300 | Avg Loss: 0.0161 | Grad Norm: 0.00840591\n",
      "Epoch 2 | Step 1337400 | Avg Loss: 0.0160 | Grad Norm: 0.00919128\n",
      "Epoch 2 | Step 1337500 | Avg Loss: 0.0157 | Grad Norm: 0.00909694\n",
      "Epoch 2 | Step 1337600 | Avg Loss: 0.0157 | Grad Norm: 0.00944414\n",
      "Epoch 2 | Step 1337700 | Avg Loss: 0.0157 | Grad Norm: 0.01010822\n",
      "Epoch 2 | Step 1337800 | Avg Loss: 0.0162 | Grad Norm: 0.00885271\n",
      "Epoch 2 | Step 1337900 | Avg Loss: 0.0158 | Grad Norm: 0.00938013\n",
      "Epoch 2 | Step 1338000 | Avg Loss: 0.0157 | Grad Norm: 0.00918431\n",
      "Epoch 2 | Step 1338100 | Avg Loss: 0.0158 | Grad Norm: 0.00934200\n",
      "Epoch 2 | Step 1338200 | Avg Loss: 0.0159 | Grad Norm: 0.00999293\n",
      "Epoch 2 | Step 1338300 | Avg Loss: 0.0156 | Grad Norm: 0.00903994\n",
      "Epoch 2 | Step 1338400 | Avg Loss: 0.0158 | Grad Norm: 0.00816376\n",
      "Epoch 2 | Step 1338500 | Avg Loss: 0.0161 | Grad Norm: 0.00829394\n",
      "Epoch 2 | Step 1338600 | Avg Loss: 0.0164 | Grad Norm: 0.00874148\n",
      "Epoch 2 | Step 1338700 | Avg Loss: 0.0163 | Grad Norm: 0.00995593\n",
      "Epoch 2 | Step 1338800 | Avg Loss: 0.0166 | Grad Norm: 0.00848550\n",
      "Epoch 2 | Step 1338900 | Avg Loss: 0.0166 | Grad Norm: 0.00873823\n",
      "Epoch 2 | Step 1339000 | Avg Loss: 0.0161 | Grad Norm: 0.00856363\n",
      "Epoch 2 | Step 1339100 | Avg Loss: 0.0161 | Grad Norm: 0.00896676\n",
      "Epoch 2 | Step 1339200 | Avg Loss: 0.0159 | Grad Norm: 0.01013176\n",
      "Epoch 2 | Step 1339300 | Avg Loss: 0.0157 | Grad Norm: 0.00833755\n",
      "Epoch 2 | Step 1339400 | Avg Loss: 0.0157 | Grad Norm: 0.00984581\n",
      "Epoch 2 | Step 1339500 | Avg Loss: 0.0156 | Grad Norm: 0.00833309\n",
      "Epoch 2 | Step 1339600 | Avg Loss: 0.0161 | Grad Norm: 0.00811888\n",
      "Epoch 2 | Step 1339700 | Avg Loss: 0.0158 | Grad Norm: 0.00766537\n",
      "Epoch 2 | Step 1339800 | Avg Loss: 0.0156 | Grad Norm: 0.00764726\n",
      "Epoch 2 | Step 1339900 | Avg Loss: 0.0156 | Grad Norm: 0.00893945\n",
      "Epoch 2 | Step 1340000 | Avg Loss: 0.0157 | Grad Norm: 0.00915393\n",
      "Epoch 2 | Step 1340100 | Avg Loss: 0.0160 | Grad Norm: 0.01113854\n",
      "Epoch 2 | Step 1340200 | Avg Loss: 0.0160 | Grad Norm: 0.00794792\n",
      "Epoch 2 | Step 1340300 | Avg Loss: 0.0160 | Grad Norm: 0.00874831\n",
      "Epoch 2 | Step 1340400 | Avg Loss: 0.0160 | Grad Norm: 0.00826516\n",
      "Epoch 2 | Step 1340500 | Avg Loss: 0.0161 | Grad Norm: 0.00805690\n",
      "Epoch 2 | Step 1340600 | Avg Loss: 0.0160 | Grad Norm: 0.00880951\n",
      "Epoch 2 | Step 1340700 | Avg Loss: 0.0159 | Grad Norm: 0.00882103\n",
      "Epoch 2 | Step 1340800 | Avg Loss: 0.0159 | Grad Norm: 0.00921076\n",
      "Epoch 2 | Step 1340900 | Avg Loss: 0.0162 | Grad Norm: 0.00848499\n",
      "Epoch 2 | Step 1341000 | Avg Loss: 0.0161 | Grad Norm: 0.00958189\n",
      "Epoch 2 | Step 1341100 | Avg Loss: 0.0158 | Grad Norm: 0.00972663\n",
      "Epoch 2 | Step 1341200 | Avg Loss: 0.0160 | Grad Norm: 0.00929003\n",
      "Epoch 2 | Step 1341300 | Avg Loss: 0.0159 | Grad Norm: 0.00983107\n",
      "Epoch 2 | Step 1341400 | Avg Loss: 0.0159 | Grad Norm: 0.00923850\n",
      "Epoch 2 | Step 1341500 | Avg Loss: 0.0158 | Grad Norm: 0.00822092\n",
      "Epoch 2 | Step 1341600 | Avg Loss: 0.0155 | Grad Norm: 0.00976152\n",
      "Epoch 2 | Step 1341700 | Avg Loss: 0.0155 | Grad Norm: 0.00898545\n",
      "Epoch 2 | Step 1341800 | Avg Loss: 0.0154 | Grad Norm: 0.00980621\n",
      "Epoch 2 | Step 1341900 | Avg Loss: 0.0154 | Grad Norm: 0.01108079\n",
      "Epoch 2 | Step 1342000 | Avg Loss: 0.0153 | Grad Norm: 0.00838772\n",
      "Epoch 2 | Step 1342100 | Avg Loss: 0.0151 | Grad Norm: 0.00854294\n",
      "Epoch 2 | Step 1342200 | Avg Loss: 0.0151 | Grad Norm: 0.00947797\n",
      "Epoch 2 | Step 1342300 | Avg Loss: 0.0152 | Grad Norm: 0.00908598\n",
      "Epoch 2 | Step 1342400 | Avg Loss: 0.0158 | Grad Norm: 0.00714076\n",
      "Epoch 2 | Step 1342500 | Avg Loss: 0.0159 | Grad Norm: 0.00919288\n",
      "Epoch 2 | Step 1342600 | Avg Loss: 0.0156 | Grad Norm: 0.00899552\n",
      "Epoch 2 | Step 1342700 | Avg Loss: 0.0158 | Grad Norm: 0.00791294\n",
      "Epoch 2 | Step 1342800 | Avg Loss: 0.0156 | Grad Norm: 0.00911010\n",
      "Epoch 2 | Step 1342900 | Avg Loss: 0.0155 | Grad Norm: 0.00735577\n",
      "Epoch 2 | Step 1343000 | Avg Loss: 0.0156 | Grad Norm: 0.00985886\n",
      "Epoch 2 | Step 1343100 | Avg Loss: 0.0158 | Grad Norm: 0.00799967\n",
      "Epoch 2 | Step 1343200 | Avg Loss: 0.0158 | Grad Norm: 0.00807255\n",
      "Epoch 2 | Step 1343300 | Avg Loss: 0.0159 | Grad Norm: 0.01020528\n",
      "Epoch 2 | Step 1343400 | Avg Loss: 0.0157 | Grad Norm: 0.00815642\n",
      "Epoch 2 | Step 1343500 | Avg Loss: 0.0157 | Grad Norm: 0.00873091\n",
      "Epoch 2 | Step 1343600 | Avg Loss: 0.0159 | Grad Norm: 0.00865012\n",
      "Epoch 2 | Step 1343700 | Avg Loss: 0.0157 | Grad Norm: 0.00800076\n",
      "Epoch 2 | Step 1343800 | Avg Loss: 0.0155 | Grad Norm: 0.00809535\n",
      "Epoch 2 | Step 1343900 | Avg Loss: 0.0154 | Grad Norm: 0.00825450\n",
      "Epoch 2 | Step 1344000 | Avg Loss: 0.0158 | Grad Norm: 0.00890749\n",
      "Epoch 2 | Step 1344100 | Avg Loss: 0.0161 | Grad Norm: 0.00850937\n",
      "Epoch 2 | Step 1344200 | Avg Loss: 0.0162 | Grad Norm: 0.00873050\n",
      "Epoch 2 | Step 1344300 | Avg Loss: 0.0161 | Grad Norm: 0.00894060\n",
      "Epoch 2 | Step 1344400 | Avg Loss: 0.0157 | Grad Norm: 0.00839824\n",
      "Epoch 2 | Step 1344500 | Avg Loss: 0.0157 | Grad Norm: 0.00801417\n",
      "Epoch 2 | Step 1344600 | Avg Loss: 0.0152 | Grad Norm: 0.00934888\n",
      "Epoch 2 | Step 1344700 | Avg Loss: 0.0153 | Grad Norm: 0.00883235\n",
      "Epoch 2 | Step 1344800 | Avg Loss: 0.0150 | Grad Norm: 0.00885789\n",
      "Epoch 2 | Step 1344900 | Avg Loss: 0.0152 | Grad Norm: 0.00858729\n",
      "Epoch 2 | Step 1345000 | Avg Loss: 0.0152 | Grad Norm: 0.00871933\n",
      "Epoch 2 | Step 1345100 | Avg Loss: 0.0156 | Grad Norm: 0.00873719\n",
      "Epoch 2 | Step 1345200 | Avg Loss: 0.0154 | Grad Norm: 0.01024376\n",
      "Epoch 2 | Step 1345300 | Avg Loss: 0.0154 | Grad Norm: 0.00979557\n",
      "Epoch 2 | Step 1345400 | Avg Loss: 0.0155 | Grad Norm: 0.00857920\n",
      "Epoch 2 | Step 1345500 | Avg Loss: 0.0154 | Grad Norm: 0.00778947\n",
      "Epoch 2 | Step 1345600 | Avg Loss: 0.0155 | Grad Norm: 0.00869104\n",
      "Epoch 2 | Step 1345700 | Avg Loss: 0.0155 | Grad Norm: 0.00771028\n",
      "Epoch 2 | Step 1345800 | Avg Loss: 0.0155 | Grad Norm: 0.01054409\n",
      "Epoch 2 | Step 1345900 | Avg Loss: 0.0155 | Grad Norm: 0.00774557\n",
      "Epoch 2 | Step 1346000 | Avg Loss: 0.0155 | Grad Norm: 0.00803626\n",
      "Epoch 2 | Step 1346100 | Avg Loss: 0.0153 | Grad Norm: 0.00945205\n",
      "Epoch 2 | Step 1346200 | Avg Loss: 0.0155 | Grad Norm: 0.00927124\n",
      "Epoch 2 | Step 1346300 | Avg Loss: 0.0157 | Grad Norm: 0.00965088\n",
      "Epoch 2 | Step 1346400 | Avg Loss: 0.0160 | Grad Norm: 0.00843236\n",
      "Epoch 2 | Step 1346500 | Avg Loss: 0.0162 | Grad Norm: 0.00838453\n",
      "Epoch 2 | Step 1346600 | Avg Loss: 0.0157 | Grad Norm: 0.00984395\n",
      "Epoch 2 | Step 1346700 | Avg Loss: 0.0157 | Grad Norm: 0.00813147\n",
      "Epoch 2 | Step 1346800 | Avg Loss: 0.0154 | Grad Norm: 0.00770699\n",
      "Epoch 2 | Step 1346900 | Avg Loss: 0.0155 | Grad Norm: 0.00829845\n",
      "Epoch 2 | Step 1347000 | Avg Loss: 0.0155 | Grad Norm: 0.01141728\n",
      "Epoch 2 | Step 1347100 | Avg Loss: 0.0155 | Grad Norm: 0.01076029\n",
      "Epoch 2 | Step 1347200 | Avg Loss: 0.0155 | Grad Norm: 0.00883672\n",
      "Epoch 2 | Step 1347300 | Avg Loss: 0.0154 | Grad Norm: 0.00792594\n",
      "Epoch 2 | Step 1347400 | Avg Loss: 0.0154 | Grad Norm: 0.00801578\n",
      "Epoch 2 | Step 1347500 | Avg Loss: 0.0156 | Grad Norm: 0.00749617\n",
      "Epoch 2 | Step 1347600 | Avg Loss: 0.0155 | Grad Norm: 0.00863214\n",
      "Epoch 2 | Step 1347700 | Avg Loss: 0.0158 | Grad Norm: 0.00872932\n",
      "Epoch 2 | Step 1347800 | Avg Loss: 0.0157 | Grad Norm: 0.00849245\n",
      "Epoch 2 | Step 1347900 | Avg Loss: 0.0156 | Grad Norm: 0.00918657\n",
      "Epoch 2 | Step 1348000 | Avg Loss: 0.0154 | Grad Norm: 0.00843021\n",
      "Epoch 2 | Step 1348100 | Avg Loss: 0.0152 | Grad Norm: 0.00830610\n",
      "Epoch 2 | Step 1348200 | Avg Loss: 0.0154 | Grad Norm: 0.00786700\n",
      "Epoch 2 | Step 1348300 | Avg Loss: 0.0154 | Grad Norm: 0.00855129\n",
      "Epoch 2 | Step 1348400 | Avg Loss: 0.0154 | Grad Norm: 0.00854287\n",
      "Epoch 2 | Step 1348500 | Avg Loss: 0.0159 | Grad Norm: 0.00783143\n",
      "Epoch 2 | Step 1348600 | Avg Loss: 0.0157 | Grad Norm: 0.01096275\n",
      "Epoch 2 | Step 1348700 | Avg Loss: 0.0159 | Grad Norm: 0.01226679\n",
      "Epoch 2 | Step 1348800 | Avg Loss: 0.0161 | Grad Norm: 0.00922441\n",
      "Epoch 2 | Step 1348900 | Avg Loss: 0.0158 | Grad Norm: 0.00870894\n",
      "Epoch 2 | Step 1349000 | Avg Loss: 0.0158 | Grad Norm: 0.00873996\n",
      "Epoch 2 | Step 1349100 | Avg Loss: 0.0155 | Grad Norm: 0.00908985\n",
      "Epoch 2 | Step 1349200 | Avg Loss: 0.0158 | Grad Norm: 0.00863369\n",
      "Epoch 2 | Step 1349300 | Avg Loss: 0.0162 | Grad Norm: 0.00924654\n",
      "Epoch 2 | Step 1349400 | Avg Loss: 0.0160 | Grad Norm: 0.00888701\n",
      "Epoch 2 | Step 1349500 | Avg Loss: 0.0160 | Grad Norm: 0.00810802\n",
      "Epoch 2 | Step 1349600 | Avg Loss: 0.0160 | Grad Norm: 0.00798249\n",
      "Epoch 2 | Step 1349700 | Avg Loss: 0.0160 | Grad Norm: 0.00918962\n",
      "Epoch 2 | Step 1349800 | Avg Loss: 0.0163 | Grad Norm: 0.00907017\n",
      "Epoch 2 | Step 1349900 | Avg Loss: 0.0163 | Grad Norm: 0.00914818\n",
      "Epoch 2 | Step 1350000 | Avg Loss: 0.0156 | Grad Norm: 0.00884410\n",
      "Epoch 2 | Step 1350100 | Avg Loss: 0.0154 | Grad Norm: 0.00871394\n",
      "Epoch 2 | Step 1350200 | Avg Loss: 0.0154 | Grad Norm: 0.00915607\n",
      "Epoch 2 | Step 1350300 | Avg Loss: 0.0153 | Grad Norm: 0.00803669\n",
      "Epoch 2 | Step 1350400 | Avg Loss: 0.0156 | Grad Norm: 0.00919345\n",
      "Epoch 2 | Step 1350500 | Avg Loss: 0.0152 | Grad Norm: 0.00814631\n",
      "Epoch 2 | Step 1350600 | Avg Loss: 0.0158 | Grad Norm: 0.00827968\n",
      "Epoch 2 | Step 1350700 | Avg Loss: 0.0157 | Grad Norm: 0.00865857\n",
      "Epoch 2 | Step 1350800 | Avg Loss: 0.0156 | Grad Norm: 0.01004100\n",
      "Epoch 2 | Step 1350900 | Avg Loss: 0.0159 | Grad Norm: 0.01038139\n",
      "Epoch 2 | Step 1351000 | Avg Loss: 0.0157 | Grad Norm: 0.00816751\n",
      "Epoch 2 | Step 1351100 | Avg Loss: 0.0156 | Grad Norm: 0.00855248\n",
      "Epoch 2 | Step 1351200 | Avg Loss: 0.0154 | Grad Norm: 0.00878530\n",
      "Epoch 2 | Step 1351300 | Avg Loss: 0.0154 | Grad Norm: 0.00836521\n",
      "Epoch 2 | Step 1351400 | Avg Loss: 0.0155 | Grad Norm: 0.00923748\n",
      "Epoch 2 | Step 1351500 | Avg Loss: 0.0158 | Grad Norm: 0.01020963\n",
      "Epoch 2 | Step 1351600 | Avg Loss: 0.0160 | Grad Norm: 0.00873787\n",
      "Epoch 2 | Step 1351700 | Avg Loss: 0.0162 | Grad Norm: 0.00877084\n",
      "Epoch 2 | Step 1351800 | Avg Loss: 0.0161 | Grad Norm: 0.00969364\n",
      "Epoch 2 | Step 1351900 | Avg Loss: 0.0157 | Grad Norm: 0.00913230\n",
      "Epoch 2 | Step 1352000 | Avg Loss: 0.0157 | Grad Norm: 0.00819412\n",
      "Epoch 2 | Step 1352100 | Avg Loss: 0.0158 | Grad Norm: 0.00954189\n",
      "Epoch 2 | Step 1352200 | Avg Loss: 0.0157 | Grad Norm: 0.00816462\n",
      "Epoch 2 | Step 1352300 | Avg Loss: 0.0161 | Grad Norm: 0.00841991\n",
      "Epoch 2 | Step 1352400 | Avg Loss: 0.0158 | Grad Norm: 0.00919661\n",
      "Epoch 2 | Step 1352500 | Avg Loss: 0.0156 | Grad Norm: 0.00875909\n",
      "Epoch 2 | Step 1352600 | Avg Loss: 0.0162 | Grad Norm: 0.00918521\n",
      "Epoch 2 | Step 1352700 | Avg Loss: 0.0160 | Grad Norm: 0.00945194\n",
      "Epoch 2 | Step 1352800 | Avg Loss: 0.0159 | Grad Norm: 0.00885366\n",
      "Epoch 2 | Step 1352900 | Avg Loss: 0.0160 | Grad Norm: 0.00910744\n",
      "Epoch 2 | Step 1353000 | Avg Loss: 0.0154 | Grad Norm: 0.00873752\n",
      "Epoch 2 | Step 1353100 | Avg Loss: 0.0153 | Grad Norm: 0.00800135\n",
      "Epoch 2 | Step 1353200 | Avg Loss: 0.0156 | Grad Norm: 0.00894355\n",
      "Epoch 2 | Step 1353300 | Avg Loss: 0.0155 | Grad Norm: 0.00852242\n",
      "Epoch 2 | Step 1353400 | Avg Loss: 0.0158 | Grad Norm: 0.00855984\n",
      "Epoch 2 | Step 1353500 | Avg Loss: 0.0155 | Grad Norm: 0.01029218\n",
      "Epoch 2 | Step 1353600 | Avg Loss: 0.0158 | Grad Norm: 0.00868002\n",
      "Epoch 2 | Step 1353700 | Avg Loss: 0.0157 | Grad Norm: 0.00929248\n",
      "Epoch 2 | Step 1353800 | Avg Loss: 0.0159 | Grad Norm: 0.00984912\n",
      "Epoch 2 | Step 1353900 | Avg Loss: 0.0159 | Grad Norm: 0.00869673\n",
      "Epoch 2 | Step 1354000 | Avg Loss: 0.0156 | Grad Norm: 0.00955641\n",
      "Epoch 2 | Step 1354100 | Avg Loss: 0.0156 | Grad Norm: 0.00904847\n",
      "Epoch 2 | Step 1354200 | Avg Loss: 0.0153 | Grad Norm: 0.00886812\n",
      "Epoch 2 | Step 1354300 | Avg Loss: 0.0154 | Grad Norm: 0.00876578\n",
      "Epoch 2 | Step 1354400 | Avg Loss: 0.0152 | Grad Norm: 0.00887999\n",
      "Epoch 2 | Step 1354500 | Avg Loss: 0.0151 | Grad Norm: 0.00903546\n",
      "Epoch 2 | Step 1354600 | Avg Loss: 0.0153 | Grad Norm: 0.01031690\n",
      "Epoch 2 | Step 1354700 | Avg Loss: 0.0153 | Grad Norm: 0.00868929\n",
      "Epoch 2 | Step 1354800 | Avg Loss: 0.0154 | Grad Norm: 0.00759729\n",
      "Epoch 2 | Step 1354900 | Avg Loss: 0.0153 | Grad Norm: 0.00866684\n",
      "Epoch 2 | Step 1355000 | Avg Loss: 0.0155 | Grad Norm: 0.00909025\n",
      "Epoch 2 | Step 1355100 | Avg Loss: 0.0157 | Grad Norm: 0.01006361\n",
      "Epoch 2 | Step 1355200 | Avg Loss: 0.0156 | Grad Norm: 0.00883141\n",
      "Epoch 2 | Step 1355300 | Avg Loss: 0.0155 | Grad Norm: 0.00906924\n",
      "Epoch 2 | Step 1355400 | Avg Loss: 0.0155 | Grad Norm: 0.01160390\n",
      "Epoch 2 | Step 1355500 | Avg Loss: 0.0152 | Grad Norm: 0.00836630\n",
      "Epoch 2 | Step 1355600 | Avg Loss: 0.0153 | Grad Norm: 0.00955411\n",
      "Epoch 2 | Step 1355700 | Avg Loss: 0.0155 | Grad Norm: 0.00763096\n",
      "Epoch 2 | Step 1355800 | Avg Loss: 0.0154 | Grad Norm: 0.00855669\n",
      "Epoch 2 | Step 1355900 | Avg Loss: 0.0155 | Grad Norm: 0.00762128\n",
      "Epoch 2 | Step 1356000 | Avg Loss: 0.0156 | Grad Norm: 0.00858413\n",
      "Epoch 2 | Step 1356100 | Avg Loss: 0.0159 | Grad Norm: 0.00993060\n",
      "Epoch 2 | Step 1356200 | Avg Loss: 0.0157 | Grad Norm: 0.00959245\n",
      "Epoch 2 | Step 1356300 | Avg Loss: 0.0156 | Grad Norm: 0.01084557\n",
      "Epoch 2 | Step 1356400 | Avg Loss: 0.0155 | Grad Norm: 0.00837999\n",
      "Epoch 2 | Step 1356500 | Avg Loss: 0.0157 | Grad Norm: 0.01148176\n",
      "Epoch 2 | Step 1356600 | Avg Loss: 0.0159 | Grad Norm: 0.01129847\n",
      "Epoch 2 | Step 1356700 | Avg Loss: 0.0157 | Grad Norm: 0.00971407\n",
      "Epoch 2 | Step 1356800 | Avg Loss: 0.0159 | Grad Norm: 0.00910741\n",
      "Epoch 2 | Step 1356900 | Avg Loss: 0.0158 | Grad Norm: 0.00866770\n",
      "Epoch 2 | Step 1357000 | Avg Loss: 0.0156 | Grad Norm: 0.01000621\n",
      "Epoch 2 | Step 1357100 | Avg Loss: 0.0155 | Grad Norm: 0.01023045\n",
      "Epoch 2 | Step 1357200 | Avg Loss: 0.0156 | Grad Norm: 0.00782640\n",
      "Epoch 2 | Step 1357300 | Avg Loss: 0.0154 | Grad Norm: 0.00918642\n",
      "Epoch 2 | Step 1357400 | Avg Loss: 0.0157 | Grad Norm: 0.01000890\n",
      "Epoch 2 | Step 1357500 | Avg Loss: 0.0153 | Grad Norm: 0.00882200\n",
      "Epoch 2 | Step 1357600 | Avg Loss: 0.0150 | Grad Norm: 0.00769947\n",
      "Epoch 2 | Step 1357700 | Avg Loss: 0.0151 | Grad Norm: 0.00855146\n",
      "Epoch 2 | Step 1357800 | Avg Loss: 0.0153 | Grad Norm: 0.00814159\n",
      "Epoch 2 | Step 1357900 | Avg Loss: 0.0154 | Grad Norm: 0.00826528\n",
      "Epoch 2 | Step 1358000 | Avg Loss: 0.0155 | Grad Norm: 0.00774575\n",
      "Epoch 2 | Step 1358100 | Avg Loss: 0.0154 | Grad Norm: 0.01235139\n",
      "Epoch 2 | Step 1358200 | Avg Loss: 0.0153 | Grad Norm: 0.01001118\n",
      "Epoch 2 | Step 1358300 | Avg Loss: 0.0150 | Grad Norm: 0.00922705\n",
      "Epoch 2 | Step 1358400 | Avg Loss: 0.0152 | Grad Norm: 0.00850601\n",
      "Epoch 2 | Step 1358500 | Avg Loss: 0.0149 | Grad Norm: 0.00765626\n",
      "Epoch 2 | Step 1358600 | Avg Loss: 0.0152 | Grad Norm: 0.00975399\n",
      "Epoch 2 | Step 1358700 | Avg Loss: 0.0156 | Grad Norm: 0.01031421\n",
      "Epoch 2 | Step 1358800 | Avg Loss: 0.0153 | Grad Norm: 0.00923069\n",
      "Epoch 2 | Step 1358900 | Avg Loss: 0.0152 | Grad Norm: 0.00908888\n",
      "Epoch 2 | Step 1359000 | Avg Loss: 0.0150 | Grad Norm: 0.00971319\n",
      "Epoch 2 | Step 1359100 | Avg Loss: 0.0155 | Grad Norm: 0.01205567\n",
      "Epoch 2 | Step 1359200 | Avg Loss: 0.0153 | Grad Norm: 0.00906835\n",
      "Epoch 2 | Step 1359300 | Avg Loss: 0.0154 | Grad Norm: 0.01277865\n",
      "Epoch 2 | Step 1359400 | Avg Loss: 0.0156 | Grad Norm: 0.00902242\n",
      "Epoch 2 | Step 1359500 | Avg Loss: 0.0153 | Grad Norm: 0.00877363\n",
      "Epoch 2 | Step 1359600 | Avg Loss: 0.0151 | Grad Norm: 0.00803106\n",
      "Epoch 2 | Step 1359700 | Avg Loss: 0.0150 | Grad Norm: 0.00801623\n",
      "Epoch 2 | Step 1359800 | Avg Loss: 0.0150 | Grad Norm: 0.00878429\n",
      "Epoch 2 | Step 1359900 | Avg Loss: 0.0154 | Grad Norm: 0.00843346\n",
      "Epoch 2 | Step 1360000 | Avg Loss: 0.0157 | Grad Norm: 0.00811514\n",
      "Epoch 2 | Step 1360100 | Avg Loss: 0.0156 | Grad Norm: 0.00964035\n",
      "Epoch 2 | Step 1360200 | Avg Loss: 0.0153 | Grad Norm: 0.00887025\n",
      "Epoch 2 | Step 1360300 | Avg Loss: 0.0154 | Grad Norm: 0.01022255\n",
      "Epoch 2 | Step 1360400 | Avg Loss: 0.0155 | Grad Norm: 0.00927727\n",
      "Epoch 2 | Step 1360500 | Avg Loss: 0.0154 | Grad Norm: 0.00933462\n",
      "Epoch 2 | Step 1360600 | Avg Loss: 0.0152 | Grad Norm: 0.00995810\n",
      "Epoch 2 | Step 1360700 | Avg Loss: 0.0155 | Grad Norm: 0.01050601\n",
      "Epoch 2 | Step 1360800 | Avg Loss: 0.0157 | Grad Norm: 0.00769726\n",
      "Epoch 2 | Step 1360900 | Avg Loss: 0.0155 | Grad Norm: 0.00877301\n",
      "Epoch 2 | Step 1361000 | Avg Loss: 0.0152 | Grad Norm: 0.01017939\n",
      "Epoch 2 | Step 1361100 | Avg Loss: 0.0153 | Grad Norm: 0.00964650\n",
      "Epoch 2 | Step 1361200 | Avg Loss: 0.0154 | Grad Norm: 0.00874016\n",
      "Epoch 2 | Step 1361300 | Avg Loss: 0.0156 | Grad Norm: 0.00800770\n",
      "Epoch 2 | Step 1361400 | Avg Loss: 0.0154 | Grad Norm: 0.00885422\n",
      "Epoch 2 | Step 1361500 | Avg Loss: 0.0151 | Grad Norm: 0.00918428\n",
      "Epoch 2 | Step 1361600 | Avg Loss: 0.0151 | Grad Norm: 0.00870809\n",
      "Epoch 2 | Step 1361700 | Avg Loss: 0.0153 | Grad Norm: 0.01009269\n",
      "Epoch 2 | Step 1361800 | Avg Loss: 0.0156 | Grad Norm: 0.00978268\n",
      "Epoch 2 | Step 1361900 | Avg Loss: 0.0156 | Grad Norm: 0.00718777\n",
      "Epoch 2 | Step 1362000 | Avg Loss: 0.0154 | Grad Norm: 0.01141561\n",
      "Epoch 2 | Step 1362100 | Avg Loss: 0.0155 | Grad Norm: 0.00805623\n",
      "Epoch 2 | Step 1362200 | Avg Loss: 0.0156 | Grad Norm: 0.00808455\n",
      "Epoch 2 | Step 1362300 | Avg Loss: 0.0154 | Grad Norm: 0.00862565\n",
      "Epoch 2 | Step 1362400 | Avg Loss: 0.0152 | Grad Norm: 0.01166507\n",
      "Epoch 2 | Step 1362500 | Avg Loss: 0.0155 | Grad Norm: 0.00852741\n",
      "Epoch 2 | Step 1362600 | Avg Loss: 0.0158 | Grad Norm: 0.00889953\n",
      "Epoch 2 | Step 1362700 | Avg Loss: 0.0156 | Grad Norm: 0.00782431\n",
      "Epoch 2 | Step 1362800 | Avg Loss: 0.0153 | Grad Norm: 0.00770902\n",
      "Epoch 2 | Step 1362900 | Avg Loss: 0.0153 | Grad Norm: 0.00837377\n",
      "Epoch 2 | Step 1363000 | Avg Loss: 0.0154 | Grad Norm: 0.00809306\n",
      "Epoch 2 | Step 1363100 | Avg Loss: 0.0151 | Grad Norm: 0.00822178\n",
      "Epoch 2 | Step 1363200 | Avg Loss: 0.0150 | Grad Norm: 0.00851636\n",
      "Epoch 2 | Step 1363300 | Avg Loss: 0.0152 | Grad Norm: 0.00804138\n",
      "Epoch 2 | Step 1363400 | Avg Loss: 0.0154 | Grad Norm: 0.00863205\n",
      "Epoch 2 | Step 1363500 | Avg Loss: 0.0156 | Grad Norm: 0.00860274\n",
      "Epoch 2 | Step 1363600 | Avg Loss: 0.0155 | Grad Norm: 0.00970822\n",
      "Epoch 2 | Step 1363700 | Avg Loss: 0.0155 | Grad Norm: 0.00887994\n",
      "Epoch 2 | Step 1363800 | Avg Loss: 0.0157 | Grad Norm: 0.00768179\n",
      "Epoch 2 | Step 1363900 | Avg Loss: 0.0153 | Grad Norm: 0.00803380\n",
      "Epoch 2 | Step 1364000 | Avg Loss: 0.0154 | Grad Norm: 0.00813048\n",
      "Epoch 2 | Step 1364100 | Avg Loss: 0.0156 | Grad Norm: 0.00971178\n",
      "Epoch 2 | Step 1364200 | Avg Loss: 0.0153 | Grad Norm: 0.00800497\n",
      "Epoch 2 | Step 1364300 | Avg Loss: 0.0152 | Grad Norm: 0.00836254\n",
      "Epoch 2 | Step 1364400 | Avg Loss: 0.0152 | Grad Norm: 0.00835516\n",
      "Epoch 2 | Step 1364500 | Avg Loss: 0.0156 | Grad Norm: 0.00848445\n",
      "Epoch 2 | Step 1364600 | Avg Loss: 0.0154 | Grad Norm: 0.00842308\n",
      "Epoch 2 | Step 1364700 | Avg Loss: 0.0158 | Grad Norm: 0.00950243\n",
      "Epoch 2 | Step 1364800 | Avg Loss: 0.0160 | Grad Norm: 0.00967043\n",
      "Epoch 2 | Step 1364900 | Avg Loss: 0.0160 | Grad Norm: 0.00777401\n",
      "Epoch 2 | Step 1365000 | Avg Loss: 0.0161 | Grad Norm: 0.01208738\n",
      "Epoch 2 | Step 1365100 | Avg Loss: 0.0161 | Grad Norm: 0.01062069\n",
      "Epoch 2 | Step 1365200 | Avg Loss: 0.0159 | Grad Norm: 0.00913310\n",
      "Epoch 2 | Step 1365300 | Avg Loss: 0.0157 | Grad Norm: 0.00896373\n",
      "Epoch 2 | Step 1365400 | Avg Loss: 0.0156 | Grad Norm: 0.00888646\n",
      "Epoch 2 | Step 1365500 | Avg Loss: 0.0158 | Grad Norm: 0.00834285\n",
      "Epoch 2 | Step 1365600 | Avg Loss: 0.0158 | Grad Norm: 0.00895353\n",
      "Epoch 2 | Step 1365700 | Avg Loss: 0.0158 | Grad Norm: 0.00868556\n",
      "Epoch 2 | Step 1365800 | Avg Loss: 0.0161 | Grad Norm: 0.00888530\n",
      "Epoch 2 | Step 1365900 | Avg Loss: 0.0163 | Grad Norm: 0.00924458\n",
      "Epoch 2 | Step 1366000 | Avg Loss: 0.0161 | Grad Norm: 0.00915434\n",
      "Epoch 2 | Step 1366100 | Avg Loss: 0.0163 | Grad Norm: 0.00965708\n",
      "Epoch 2 | Step 1366200 | Avg Loss: 0.0164 | Grad Norm: 0.00841017\n",
      "Epoch 2 | Step 1366300 | Avg Loss: 0.0163 | Grad Norm: 0.00914722\n",
      "Epoch 2 | Step 1366400 | Avg Loss: 0.0165 | Grad Norm: 0.00866395\n",
      "Epoch 2 | Step 1366500 | Avg Loss: 0.0166 | Grad Norm: 0.00940918\n",
      "Epoch 2 | Step 1366600 | Avg Loss: 0.0166 | Grad Norm: 0.01035031\n",
      "Epoch 2 | Step 1366700 | Avg Loss: 0.0163 | Grad Norm: 0.00928726\n",
      "Epoch 2 | Step 1366800 | Avg Loss: 0.0163 | Grad Norm: 0.00865266\n",
      "Epoch 2 | Step 1366900 | Avg Loss: 0.0161 | Grad Norm: 0.00863942\n",
      "Epoch 2 | Step 1367000 | Avg Loss: 0.0161 | Grad Norm: 0.00855613\n",
      "Epoch 2 | Step 1367100 | Avg Loss: 0.0162 | Grad Norm: 0.00899043\n",
      "Epoch 2 | Step 1367200 | Avg Loss: 0.0162 | Grad Norm: 0.00922679\n",
      "Epoch 2 | Step 1367300 | Avg Loss: 0.0157 | Grad Norm: 0.00815280\n",
      "Epoch 2 | Step 1367400 | Avg Loss: 0.0155 | Grad Norm: 0.01059394\n",
      "Epoch 2 | Step 1367500 | Avg Loss: 0.0154 | Grad Norm: 0.00854721\n",
      "Epoch 2 | Step 1367600 | Avg Loss: 0.0154 | Grad Norm: 0.01008287\n",
      "Epoch 2 | Step 1367700 | Avg Loss: 0.0155 | Grad Norm: 0.01013540\n",
      "Epoch 2 | Step 1367800 | Avg Loss: 0.0156 | Grad Norm: 0.00962604\n",
      "Epoch 2 | Step 1367900 | Avg Loss: 0.0161 | Grad Norm: 0.00976516\n",
      "Epoch 2 | Step 1368000 | Avg Loss: 0.0158 | Grad Norm: 0.00810338\n",
      "Epoch 2 | Step 1368100 | Avg Loss: 0.0159 | Grad Norm: 0.01070357\n",
      "Epoch 2 | Step 1368200 | Avg Loss: 0.0160 | Grad Norm: 0.00928982\n",
      "Epoch 2 | Step 1368300 | Avg Loss: 0.0157 | Grad Norm: 0.00956419\n",
      "Epoch 2 | Step 1368400 | Avg Loss: 0.0159 | Grad Norm: 0.00866530\n",
      "Epoch 2 | Step 1368500 | Avg Loss: 0.0159 | Grad Norm: 0.00942189\n",
      "Epoch 2 | Step 1368600 | Avg Loss: 0.0160 | Grad Norm: 0.00921117\n",
      "Epoch 2 | Step 1368700 | Avg Loss: 0.0161 | Grad Norm: 0.00992156\n",
      "Epoch 2 | Step 1368800 | Avg Loss: 0.0161 | Grad Norm: 0.00948511\n",
      "Epoch 2 | Step 1368900 | Avg Loss: 0.0161 | Grad Norm: 0.00867463\n",
      "Epoch 2 | Step 1369000 | Avg Loss: 0.0163 | Grad Norm: 0.00844691\n",
      "Epoch 2 | Step 1369100 | Avg Loss: 0.0160 | Grad Norm: 0.00819273\n",
      "Epoch 2 | Step 1369200 | Avg Loss: 0.0159 | Grad Norm: 0.00851229\n",
      "Epoch 2 | Step 1369300 | Avg Loss: 0.0162 | Grad Norm: 0.01032593\n",
      "Epoch 2 | Step 1369400 | Avg Loss: 0.0160 | Grad Norm: 0.00856072\n",
      "Epoch 2 | Step 1369500 | Avg Loss: 0.0157 | Grad Norm: 0.00827017\n",
      "Epoch 2 | Step 1369600 | Avg Loss: 0.0153 | Grad Norm: 0.01000648\n",
      "Epoch 2 | Step 1369700 | Avg Loss: 0.0153 | Grad Norm: 0.00788617\n",
      "Epoch 2 | Step 1369800 | Avg Loss: 0.0158 | Grad Norm: 0.00802787\n",
      "Epoch 2 | Step 1369900 | Avg Loss: 0.0155 | Grad Norm: 0.01021883\n",
      "Epoch 2 | Step 1370000 | Avg Loss: 0.0155 | Grad Norm: 0.00905049\n",
      "Epoch 2 | Step 1370100 | Avg Loss: 0.0157 | Grad Norm: 0.01098333\n",
      "Epoch 2 | Step 1370200 | Avg Loss: 0.0156 | Grad Norm: 0.00861919\n",
      "Epoch 2 | Step 1370300 | Avg Loss: 0.0155 | Grad Norm: 0.01129971\n",
      "Epoch 2 | Step 1370400 | Avg Loss: 0.0154 | Grad Norm: 0.00974397\n",
      "Epoch 2 | Step 1370500 | Avg Loss: 0.0153 | Grad Norm: 0.00847976\n",
      "Epoch 2 | Step 1370600 | Avg Loss: 0.0156 | Grad Norm: 0.00849698\n",
      "Epoch 2 | Step 1370700 | Avg Loss: 0.0154 | Grad Norm: 0.01005797\n",
      "Epoch 2 | Step 1370800 | Avg Loss: 0.0153 | Grad Norm: 0.00849098\n",
      "Epoch 2 | Step 1370900 | Avg Loss: 0.0155 | Grad Norm: 0.00974453\n",
      "Epoch 2 | Step 1371000 | Avg Loss: 0.0157 | Grad Norm: 0.00832494\n",
      "Epoch 2 | Step 1371100 | Avg Loss: 0.0156 | Grad Norm: 0.00786055\n",
      "Epoch 2 | Step 1371200 | Avg Loss: 0.0157 | Grad Norm: 0.00920134\n",
      "Epoch 2 | Step 1371300 | Avg Loss: 0.0155 | Grad Norm: 0.00942616\n",
      "Epoch 2 | Step 1371400 | Avg Loss: 0.0152 | Grad Norm: 0.01010344\n",
      "Epoch 2 | Step 1371500 | Avg Loss: 0.0152 | Grad Norm: 0.00920240\n",
      "Epoch 2 | Step 1371600 | Avg Loss: 0.0154 | Grad Norm: 0.00795712\n",
      "Epoch 2 | Step 1371700 | Avg Loss: 0.0155 | Grad Norm: 0.00828043\n",
      "Epoch 2 | Step 1371800 | Avg Loss: 0.0154 | Grad Norm: 0.00705127\n",
      "Epoch 2 | Step 1371900 | Avg Loss: 0.0152 | Grad Norm: 0.00959995\n",
      "Epoch 2 | Step 1372000 | Avg Loss: 0.0153 | Grad Norm: 0.01024547\n",
      "Epoch 2 | Step 1372100 | Avg Loss: 0.0156 | Grad Norm: 0.01060157\n",
      "Epoch 2 | Step 1372200 | Avg Loss: 0.0153 | Grad Norm: 0.00838848\n",
      "Epoch 2 | Step 1372300 | Avg Loss: 0.0153 | Grad Norm: 0.00959049\n",
      "Epoch 2 | Step 1372400 | Avg Loss: 0.0153 | Grad Norm: 0.00762349\n",
      "Epoch 2 | Step 1372500 | Avg Loss: 0.0155 | Grad Norm: 0.00823873\n",
      "Epoch 2 | Step 1372600 | Avg Loss: 0.0156 | Grad Norm: 0.00895148\n",
      "Epoch 2 | Step 1372700 | Avg Loss: 0.0158 | Grad Norm: 0.00969146\n",
      "Epoch 2 | Step 1372800 | Avg Loss: 0.0159 | Grad Norm: 0.00875191\n",
      "Epoch 2 | Step 1372900 | Avg Loss: 0.0159 | Grad Norm: 0.00806106\n",
      "Epoch 2 | Step 1373000 | Avg Loss: 0.0162 | Grad Norm: 0.00789506\n",
      "Epoch 2 | Step 1373100 | Avg Loss: 0.0161 | Grad Norm: 0.00964197\n",
      "Epoch 2 | Step 1373200 | Avg Loss: 0.0164 | Grad Norm: 0.00879651\n",
      "Epoch 2 | Step 1373300 | Avg Loss: 0.0164 | Grad Norm: 0.00853663\n",
      "Epoch 2 | Step 1373400 | Avg Loss: 0.0161 | Grad Norm: 0.01037377\n",
      "Epoch 2 | Step 1373500 | Avg Loss: 0.0158 | Grad Norm: 0.00948694\n",
      "Epoch 2 | Step 1373600 | Avg Loss: 0.0160 | Grad Norm: 0.00893061\n",
      "Epoch 2 | Step 1373700 | Avg Loss: 0.0155 | Grad Norm: 0.00840398\n",
      "Epoch 2 | Step 1373800 | Avg Loss: 0.0155 | Grad Norm: 0.00811560\n",
      "Epoch 2 | Step 1373900 | Avg Loss: 0.0155 | Grad Norm: 0.00900649\n",
      "Epoch 2 | Step 1374000 | Avg Loss: 0.0152 | Grad Norm: 0.00972018\n",
      "Epoch 2 | Step 1374100 | Avg Loss: 0.0153 | Grad Norm: 0.00883237\n",
      "Epoch 2 | Step 1374200 | Avg Loss: 0.0153 | Grad Norm: 0.00775089\n",
      "Epoch 2 | Step 1374300 | Avg Loss: 0.0157 | Grad Norm: 0.00822549\n",
      "Epoch 2 | Step 1374400 | Avg Loss: 0.0154 | Grad Norm: 0.00883809\n",
      "Epoch 2 | Step 1374500 | Avg Loss: 0.0156 | Grad Norm: 0.00868553\n",
      "Epoch 2 | Step 1374600 | Avg Loss: 0.0155 | Grad Norm: 0.00954605\n",
      "Epoch 2 | Step 1374700 | Avg Loss: 0.0155 | Grad Norm: 0.00762582\n",
      "Epoch 2 | Step 1374800 | Avg Loss: 0.0153 | Grad Norm: 0.00825685\n",
      "Epoch 2 | Step 1374900 | Avg Loss: 0.0157 | Grad Norm: 0.00879797\n",
      "Epoch 2 | Step 1375000 | Avg Loss: 0.0159 | Grad Norm: 0.00893154\n",
      "Epoch 2 | Step 1375100 | Avg Loss: 0.0159 | Grad Norm: 0.00810674\n",
      "Epoch 2 | Step 1375200 | Avg Loss: 0.0156 | Grad Norm: 0.00876283\n",
      "Epoch 2 | Step 1375300 | Avg Loss: 0.0156 | Grad Norm: 0.00911908\n",
      "Epoch 2 | Step 1375400 | Avg Loss: 0.0156 | Grad Norm: 0.00999244\n",
      "Epoch 2 | Step 1375500 | Avg Loss: 0.0154 | Grad Norm: 0.00822990\n",
      "Epoch 2 | Step 1375600 | Avg Loss: 0.0158 | Grad Norm: 0.00958695\n",
      "Epoch 2 | Step 1375700 | Avg Loss: 0.0156 | Grad Norm: 0.01020233\n",
      "Epoch 2 | Step 1375800 | Avg Loss: 0.0156 | Grad Norm: 0.00870674\n",
      "Epoch 2 | Step 1375900 | Avg Loss: 0.0155 | Grad Norm: 0.00922412\n",
      "Epoch 2 | Step 1376000 | Avg Loss: 0.0154 | Grad Norm: 0.00835104\n",
      "Epoch 2 | Step 1376100 | Avg Loss: 0.0153 | Grad Norm: 0.00832289\n",
      "Epoch 2 | Step 1376200 | Avg Loss: 0.0156 | Grad Norm: 0.00824560\n",
      "Epoch 2 | Step 1376300 | Avg Loss: 0.0157 | Grad Norm: 0.00780061\n",
      "Epoch 2 | Step 1376400 | Avg Loss: 0.0159 | Grad Norm: 0.01039990\n",
      "Epoch 2 | Step 1376500 | Avg Loss: 0.0154 | Grad Norm: 0.00838969\n",
      "Epoch 2 | Step 1376600 | Avg Loss: 0.0157 | Grad Norm: 0.01482147\n",
      "Epoch 2 | Step 1376700 | Avg Loss: 0.0159 | Grad Norm: 0.00817264\n",
      "Epoch 2 | Step 1376800 | Avg Loss: 0.0158 | Grad Norm: 0.00812155\n",
      "Epoch 2 | Step 1376900 | Avg Loss: 0.0159 | Grad Norm: 0.00944985\n",
      "Epoch 2 | Step 1377000 | Avg Loss: 0.0160 | Grad Norm: 0.00926480\n",
      "Epoch 2 | Step 1377100 | Avg Loss: 0.0158 | Grad Norm: 0.01072241\n",
      "Epoch 2 | Step 1377200 | Avg Loss: 0.0159 | Grad Norm: 0.01002131\n",
      "Epoch 2 | Step 1377300 | Avg Loss: 0.0160 | Grad Norm: 0.00818923\n",
      "Epoch 2 | Step 1377400 | Avg Loss: 0.0163 | Grad Norm: 0.01140714\n",
      "Epoch 2 | Step 1377500 | Avg Loss: 0.0161 | Grad Norm: 0.00996558\n",
      "Epoch 2 | Step 1377600 | Avg Loss: 0.0159 | Grad Norm: 0.00866873\n",
      "Epoch 2 | Step 1377700 | Avg Loss: 0.0161 | Grad Norm: 0.00839170\n",
      "Epoch 2 | Step 1377800 | Avg Loss: 0.0156 | Grad Norm: 0.00812865\n",
      "Epoch 2 | Step 1377900 | Avg Loss: 0.0158 | Grad Norm: 0.01034116\n",
      "Epoch 2 | Step 1378000 | Avg Loss: 0.0157 | Grad Norm: 0.00851980\n",
      "Epoch 2 | Step 1378100 | Avg Loss: 0.0157 | Grad Norm: 0.00849833\n",
      "Epoch 2 | Step 1378200 | Avg Loss: 0.0159 | Grad Norm: 0.00967795\n",
      "Epoch 2 | Step 1378300 | Avg Loss: 0.0160 | Grad Norm: 0.00967456\n",
      "Epoch 2 | Step 1378400 | Avg Loss: 0.0157 | Grad Norm: 0.01077323\n",
      "Epoch 2 | Step 1378500 | Avg Loss: 0.0159 | Grad Norm: 0.00918709\n",
      "Epoch 2 | Step 1378600 | Avg Loss: 0.0158 | Grad Norm: 0.00862226\n",
      "Epoch 2 | Step 1378700 | Avg Loss: 0.0154 | Grad Norm: 0.00860696\n",
      "Epoch 2 | Step 1378800 | Avg Loss: 0.0156 | Grad Norm: 0.00824437\n",
      "Epoch 2 | Step 1378900 | Avg Loss: 0.0155 | Grad Norm: 0.00910731\n",
      "Epoch 2 | Step 1379000 | Avg Loss: 0.0153 | Grad Norm: 0.00803220\n",
      "Epoch 2 | Step 1379100 | Avg Loss: 0.0156 | Grad Norm: 0.00904360\n",
      "Epoch 2 | Step 1379200 | Avg Loss: 0.0156 | Grad Norm: 0.00767022\n",
      "Epoch 2 | Step 1379300 | Avg Loss: 0.0154 | Grad Norm: 0.01010377\n",
      "Epoch 2 | Step 1379400 | Avg Loss: 0.0158 | Grad Norm: 0.00828112\n",
      "Epoch 2 | Step 1379500 | Avg Loss: 0.0161 | Grad Norm: 0.00932604\n",
      "Epoch 2 | Step 1379600 | Avg Loss: 0.0157 | Grad Norm: 0.00780412\n",
      "Epoch 2 | Step 1379700 | Avg Loss: 0.0157 | Grad Norm: 0.00886094\n",
      "Epoch 2 | Step 1379800 | Avg Loss: 0.0152 | Grad Norm: 0.01006772\n",
      "Epoch 2 | Step 1379900 | Avg Loss: 0.0151 | Grad Norm: 0.00889282\n",
      "Epoch 2 | Step 1380000 | Avg Loss: 0.0151 | Grad Norm: 0.00812511\n",
      "Epoch 2 | Step 1380100 | Avg Loss: 0.0152 | Grad Norm: 0.00856953\n",
      "Epoch 2 | Step 1380200 | Avg Loss: 0.0156 | Grad Norm: 0.00805635\n",
      "Epoch 2 | Step 1380300 | Avg Loss: 0.0156 | Grad Norm: 0.00919997\n",
      "Epoch 2 | Step 1380400 | Avg Loss: 0.0154 | Grad Norm: 0.00774491\n",
      "Epoch 2 | Step 1380500 | Avg Loss: 0.0157 | Grad Norm: 0.00951045\n",
      "Epoch 2 | Step 1380600 | Avg Loss: 0.0156 | Grad Norm: 0.00834863\n",
      "Epoch 2 | Step 1380700 | Avg Loss: 0.0160 | Grad Norm: 0.00832736\n",
      "Epoch 2 | Step 1380800 | Avg Loss: 0.0154 | Grad Norm: 0.00850866\n",
      "Epoch 2 | Step 1380900 | Avg Loss: 0.0153 | Grad Norm: 0.00832094\n",
      "Epoch 2 | Step 1381000 | Avg Loss: 0.0153 | Grad Norm: 0.00940157\n",
      "Epoch 2 | Step 1381100 | Avg Loss: 0.0153 | Grad Norm: 0.00935526\n",
      "Epoch 2 | Step 1381200 | Avg Loss: 0.0155 | Grad Norm: 0.00812620\n",
      "Epoch 2 | Step 1381300 | Avg Loss: 0.0153 | Grad Norm: 0.00871002\n",
      "Epoch 2 | Step 1381400 | Avg Loss: 0.0153 | Grad Norm: 0.00844180\n",
      "Epoch 2 | Step 1381500 | Avg Loss: 0.0154 | Grad Norm: 0.00808354\n",
      "Epoch 2 | Step 1381600 | Avg Loss: 0.0152 | Grad Norm: 0.00721931\n",
      "Epoch 2 | Step 1381700 | Avg Loss: 0.0153 | Grad Norm: 0.00920017\n",
      "Epoch 2 | Step 1381800 | Avg Loss: 0.0153 | Grad Norm: 0.00876110\n",
      "Epoch 2 | Step 1381900 | Avg Loss: 0.0158 | Grad Norm: 0.00979793\n",
      "Epoch 2 | Step 1382000 | Avg Loss: 0.0158 | Grad Norm: 0.01326288\n",
      "Epoch 2 | Step 1382100 | Avg Loss: 0.0158 | Grad Norm: 0.01063678\n",
      "Epoch 2 | Step 1382200 | Avg Loss: 0.0159 | Grad Norm: 0.00897967\n",
      "Epoch 2 | Step 1382300 | Avg Loss: 0.0160 | Grad Norm: 0.00954900\n",
      "Epoch 2 | Step 1382400 | Avg Loss: 0.0158 | Grad Norm: 0.00884341\n",
      "Epoch 2 | Step 1382500 | Avg Loss: 0.0157 | Grad Norm: 0.00818124\n",
      "Epoch 2 | Step 1382600 | Avg Loss: 0.0158 | Grad Norm: 0.00850938\n",
      "Epoch 2 | Step 1382700 | Avg Loss: 0.0154 | Grad Norm: 0.00951928\n",
      "Epoch 2 | Step 1382800 | Avg Loss: 0.0155 | Grad Norm: 0.00742674\n",
      "Epoch 2 | Step 1382900 | Avg Loss: 0.0155 | Grad Norm: 0.00963555\n",
      "Epoch 2 | Step 1383000 | Avg Loss: 0.0158 | Grad Norm: 0.00870761\n",
      "Epoch 2 | Step 1383100 | Avg Loss: 0.0160 | Grad Norm: 0.00786719\n",
      "Epoch 2 | Step 1383200 | Avg Loss: 0.0154 | Grad Norm: 0.00906027\n",
      "Epoch 2 | Step 1383300 | Avg Loss: 0.0155 | Grad Norm: 0.01111384\n",
      "Epoch 2 | Step 1383400 | Avg Loss: 0.0157 | Grad Norm: 0.00983383\n",
      "Epoch 2 | Step 1383500 | Avg Loss: 0.0152 | Grad Norm: 0.00960893\n",
      "Epoch 2 | Step 1383600 | Avg Loss: 0.0153 | Grad Norm: 0.00833176\n",
      "Epoch 2 | Step 1383700 | Avg Loss: 0.0156 | Grad Norm: 0.00845047\n",
      "Epoch 2 | Step 1383800 | Avg Loss: 0.0156 | Grad Norm: 0.00826296\n",
      "Epoch 2 | Step 1383900 | Avg Loss: 0.0158 | Grad Norm: 0.00770430\n",
      "Epoch 2 | Step 1384000 | Avg Loss: 0.0156 | Grad Norm: 0.00860371\n",
      "Epoch 2 | Step 1384100 | Avg Loss: 0.0155 | Grad Norm: 0.00846069\n",
      "Epoch 2 | Step 1384200 | Avg Loss: 0.0154 | Grad Norm: 0.00850533\n",
      "Epoch 2 | Step 1384300 | Avg Loss: 0.0155 | Grad Norm: 0.00838196\n",
      "Epoch 2 | Step 1384400 | Avg Loss: 0.0156 | Grad Norm: 0.01049067\n",
      "Epoch 2 | Step 1384500 | Avg Loss: 0.0154 | Grad Norm: 0.00803005\n",
      "Epoch 2 | Step 1384600 | Avg Loss: 0.0154 | Grad Norm: 0.00782256\n",
      "Epoch 2 | Step 1384700 | Avg Loss: 0.0155 | Grad Norm: 0.01087705\n",
      "Epoch 2 | Step 1384800 | Avg Loss: 0.0157 | Grad Norm: 0.00920160\n",
      "Epoch 2 | Step 1384900 | Avg Loss: 0.0152 | Grad Norm: 0.00860063\n",
      "Epoch 2 | Step 1385000 | Avg Loss: 0.0157 | Grad Norm: 0.01016638\n",
      "Epoch 2 | Step 1385100 | Avg Loss: 0.0154 | Grad Norm: 0.00995014\n",
      "Epoch 2 | Step 1385200 | Avg Loss: 0.0156 | Grad Norm: 0.00827685\n",
      "Epoch 2 | Step 1385300 | Avg Loss: 0.0158 | Grad Norm: 0.01219042\n",
      "Epoch 2 | Step 1385400 | Avg Loss: 0.0156 | Grad Norm: 0.00914445\n",
      "Epoch 2 | Step 1385500 | Avg Loss: 0.0156 | Grad Norm: 0.00868505\n",
      "Epoch 2 | Step 1385600 | Avg Loss: 0.0156 | Grad Norm: 0.00837221\n",
      "Epoch 2 | Step 1385700 | Avg Loss: 0.0155 | Grad Norm: 0.00784768\n",
      "Epoch 2 | Step 1385800 | Avg Loss: 0.0158 | Grad Norm: 0.00889964\n",
      "Epoch 2 | Step 1385900 | Avg Loss: 0.0163 | Grad Norm: 0.01044514\n",
      "Epoch 2 | Step 1386000 | Avg Loss: 0.0159 | Grad Norm: 0.00855821\n",
      "Epoch 2 | Step 1386100 | Avg Loss: 0.0156 | Grad Norm: 0.00892776\n",
      "Epoch 2 | Step 1386200 | Avg Loss: 0.0158 | Grad Norm: 0.01390148\n",
      "Epoch 2 | Step 1386300 | Avg Loss: 0.0162 | Grad Norm: 0.00896969\n",
      "Epoch 2 | Step 1386400 | Avg Loss: 0.0162 | Grad Norm: 0.00998296\n",
      "Epoch 2 | Step 1386500 | Avg Loss: 0.0157 | Grad Norm: 0.00932110\n",
      "Epoch 2 | Step 1386600 | Avg Loss: 0.0152 | Grad Norm: 0.00869483\n",
      "Epoch 2 | Step 1386700 | Avg Loss: 0.0158 | Grad Norm: 0.00752636\n",
      "Epoch 2 | Step 1386800 | Avg Loss: 0.0156 | Grad Norm: 0.00897256\n",
      "Epoch 2 | Step 1386900 | Avg Loss: 0.0158 | Grad Norm: 0.00924248\n",
      "Epoch 2 | Step 1387000 | Avg Loss: 0.0155 | Grad Norm: 0.00810968\n",
      "Epoch 2 | Step 1387100 | Avg Loss: 0.0151 | Grad Norm: 0.00974963\n",
      "Epoch 2 | Step 1387200 | Avg Loss: 0.0148 | Grad Norm: 0.00781783\n",
      "Epoch 2 | Step 1387300 | Avg Loss: 0.0149 | Grad Norm: 0.00844014\n",
      "Epoch 2 | Step 1387400 | Avg Loss: 0.0152 | Grad Norm: 0.00839166\n",
      "Epoch 2 | Step 1387500 | Avg Loss: 0.0155 | Grad Norm: 0.00796370\n",
      "Epoch 2 | Step 1387600 | Avg Loss: 0.0160 | Grad Norm: 0.00929166\n",
      "Epoch 2 | Step 1387700 | Avg Loss: 0.0158 | Grad Norm: 0.00841311\n",
      "Epoch 2 | Step 1387800 | Avg Loss: 0.0159 | Grad Norm: 0.00838144\n",
      "Epoch 2 | Step 1387900 | Avg Loss: 0.0158 | Grad Norm: 0.00809712\n",
      "Epoch 2 | Step 1388000 | Avg Loss: 0.0157 | Grad Norm: 0.00967888\n",
      "Epoch 2 | Step 1388100 | Avg Loss: 0.0159 | Grad Norm: 0.00933070\n",
      "Epoch 2 | Step 1388200 | Avg Loss: 0.0156 | Grad Norm: 0.00972887\n",
      "Epoch 2 | Step 1388300 | Avg Loss: 0.0157 | Grad Norm: 0.00752648\n",
      "Epoch 2 | Step 1388400 | Avg Loss: 0.0157 | Grad Norm: 0.00935647\n",
      "Epoch 2 | Step 1388500 | Avg Loss: 0.0155 | Grad Norm: 0.00814936\n",
      "Epoch 2 | Step 1388600 | Avg Loss: 0.0154 | Grad Norm: 0.00844272\n",
      "Epoch 2 | Step 1388700 | Avg Loss: 0.0154 | Grad Norm: 0.00890257\n",
      "Epoch 2 | Step 1388800 | Avg Loss: 0.0155 | Grad Norm: 0.01130642\n",
      "Epoch 2 | Step 1388900 | Avg Loss: 0.0158 | Grad Norm: 0.00869269\n",
      "Epoch 2 | Step 1389000 | Avg Loss: 0.0155 | Grad Norm: 0.00765993\n",
      "Epoch 2 | Step 1389100 | Avg Loss: 0.0156 | Grad Norm: 0.00986489\n",
      "Epoch 2 | Step 1389200 | Avg Loss: 0.0155 | Grad Norm: 0.00846227\n",
      "Epoch 2 | Step 1389300 | Avg Loss: 0.0159 | Grad Norm: 0.01375391\n",
      "Epoch 2 | Step 1389400 | Avg Loss: 0.0157 | Grad Norm: 0.00862726\n",
      "Epoch 2 | Step 1389500 | Avg Loss: 0.0155 | Grad Norm: 0.01067173\n",
      "Epoch 2 | Step 1389600 | Avg Loss: 0.0155 | Grad Norm: 0.00977034\n",
      "Epoch 2 | Step 1389700 | Avg Loss: 0.0156 | Grad Norm: 0.00928344\n",
      "Epoch 2 | Step 1389800 | Avg Loss: 0.0156 | Grad Norm: 0.01080277\n",
      "Epoch 2 | Step 1389900 | Avg Loss: 0.0158 | Grad Norm: 0.00872637\n",
      "Epoch 2 | Step 1390000 | Avg Loss: 0.0159 | Grad Norm: 0.00967759\n",
      "Epoch 2 | Step 1390100 | Avg Loss: 0.0158 | Grad Norm: 0.01024159\n",
      "Epoch 2 | Step 1390200 | Avg Loss: 0.0158 | Grad Norm: 0.00930972\n",
      "Epoch 2 | Step 1390300 | Avg Loss: 0.0156 | Grad Norm: 0.01177963\n",
      "Epoch 2 | Step 1390400 | Avg Loss: 0.0155 | Grad Norm: 0.00772101\n",
      "Epoch 2 | Step 1390500 | Avg Loss: 0.0152 | Grad Norm: 0.00859806\n",
      "Epoch 2 | Step 1390600 | Avg Loss: 0.0157 | Grad Norm: 0.00843272\n",
      "Epoch 2 | Step 1390700 | Avg Loss: 0.0154 | Grad Norm: 0.00981765\n",
      "Epoch 2 | Step 1390800 | Avg Loss: 0.0153 | Grad Norm: 0.00834136\n",
      "Epoch 2 | Step 1390900 | Avg Loss: 0.0157 | Grad Norm: 0.00866449\n",
      "Epoch 2 | Step 1391000 | Avg Loss: 0.0156 | Grad Norm: 0.00823500\n",
      "Epoch 2 | Step 1391100 | Avg Loss: 0.0154 | Grad Norm: 0.00844633\n",
      "Epoch 2 | Step 1391200 | Avg Loss: 0.0156 | Grad Norm: 0.01062062\n",
      "Epoch 2 | Step 1391300 | Avg Loss: 0.0159 | Grad Norm: 0.00866970\n",
      "Epoch 2 | Step 1391400 | Avg Loss: 0.0161 | Grad Norm: 0.00881627\n",
      "Epoch 2 | Step 1391500 | Avg Loss: 0.0161 | Grad Norm: 0.00778668\n",
      "Epoch 2 | Step 1391600 | Avg Loss: 0.0158 | Grad Norm: 0.00860427\n",
      "Epoch 2 | Step 1391700 | Avg Loss: 0.0154 | Grad Norm: 0.00852116\n",
      "Epoch 2 | Step 1391800 | Avg Loss: 0.0155 | Grad Norm: 0.00795543\n",
      "Epoch 2 | Step 1391900 | Avg Loss: 0.0155 | Grad Norm: 0.01018607\n",
      "Epoch 2 | Step 1392000 | Avg Loss: 0.0157 | Grad Norm: 0.00728298\n",
      "Epoch 2 | Step 1392100 | Avg Loss: 0.0158 | Grad Norm: 0.00860765\n",
      "Epoch 2 | Step 1392200 | Avg Loss: 0.0157 | Grad Norm: 0.00800434\n",
      "Epoch 2 | Step 1392300 | Avg Loss: 0.0160 | Grad Norm: 0.00940558\n",
      "Epoch 2 | Step 1392400 | Avg Loss: 0.0156 | Grad Norm: 0.01242860\n",
      "Epoch 2 | Step 1392500 | Avg Loss: 0.0154 | Grad Norm: 0.00845786\n",
      "Epoch 2 | Step 1392600 | Avg Loss: 0.0154 | Grad Norm: 0.00868852\n",
      "Epoch 2 | Step 1392700 | Avg Loss: 0.0156 | Grad Norm: 0.01086983\n",
      "Epoch 2 | Step 1392800 | Avg Loss: 0.0157 | Grad Norm: 0.01057629\n",
      "Epoch 2 | Step 1392900 | Avg Loss: 0.0159 | Grad Norm: 0.00817516\n",
      "Epoch 2 | Step 1393000 | Avg Loss: 0.0157 | Grad Norm: 0.00970889\n",
      "Epoch 2 | Step 1393100 | Avg Loss: 0.0156 | Grad Norm: 0.00866193\n",
      "Epoch 2 | Step 1393200 | Avg Loss: 0.0157 | Grad Norm: 0.01022907\n",
      "Epoch 2 | Step 1393300 | Avg Loss: 0.0159 | Grad Norm: 0.01054553\n",
      "Epoch 2 | Step 1393400 | Avg Loss: 0.0158 | Grad Norm: 0.00883667\n",
      "Epoch 2 | Step 1393500 | Avg Loss: 0.0157 | Grad Norm: 0.00892083\n",
      "Epoch 2 | Step 1393600 | Avg Loss: 0.0157 | Grad Norm: 0.01260383\n",
      "Epoch 2 | Step 1393700 | Avg Loss: 0.0155 | Grad Norm: 0.00830352\n",
      "Epoch 2 | Step 1393800 | Avg Loss: 0.0156 | Grad Norm: 0.00849535\n",
      "Epoch 2 | Step 1393900 | Avg Loss: 0.0155 | Grad Norm: 0.00792813\n",
      "Epoch 2 | Step 1394000 | Avg Loss: 0.0156 | Grad Norm: 0.00867002\n",
      "Epoch 2 | Step 1394100 | Avg Loss: 0.0155 | Grad Norm: 0.00969409\n",
      "Epoch 2 | Step 1394200 | Avg Loss: 0.0157 | Grad Norm: 0.00871292\n",
      "Epoch 2 | Step 1394300 | Avg Loss: 0.0159 | Grad Norm: 0.00875113\n",
      "Epoch 2 | Step 1394400 | Avg Loss: 0.0156 | Grad Norm: 0.00840224\n",
      "Epoch 2 | Step 1394500 | Avg Loss: 0.0158 | Grad Norm: 0.00949989\n",
      "Epoch 2 | Step 1394600 | Avg Loss: 0.0160 | Grad Norm: 0.00846899\n",
      "Epoch 2 | Step 1394700 | Avg Loss: 0.0158 | Grad Norm: 0.01036423\n",
      "Epoch 2 | Step 1394800 | Avg Loss: 0.0155 | Grad Norm: 0.00831389\n",
      "Epoch 2 | Step 1394900 | Avg Loss: 0.0161 | Grad Norm: 0.00938941\n",
      "Epoch 2 | Step 1395000 | Avg Loss: 0.0161 | Grad Norm: 0.01054975\n",
      "Epoch 2 | Step 1395100 | Avg Loss: 0.0161 | Grad Norm: 0.01011597\n",
      "Epoch 2 | Step 1395200 | Avg Loss: 0.0158 | Grad Norm: 0.01007369\n",
      "Epoch 2 | Step 1395300 | Avg Loss: 0.0159 | Grad Norm: 0.01150082\n",
      "Epoch 2 | Step 1395400 | Avg Loss: 0.0156 | Grad Norm: 0.00850679\n",
      "Epoch 2 | Step 1395500 | Avg Loss: 0.0161 | Grad Norm: 0.00937578\n",
      "Epoch 2 | Step 1395600 | Avg Loss: 0.0159 | Grad Norm: 0.00830045\n",
      "Epoch 2 | Step 1395700 | Avg Loss: 0.0156 | Grad Norm: 0.00833787\n",
      "Epoch 2 | Step 1395800 | Avg Loss: 0.0156 | Grad Norm: 0.00900754\n",
      "Epoch 2 | Step 1395900 | Avg Loss: 0.0156 | Grad Norm: 0.00959394\n",
      "Epoch 2 | Step 1396000 | Avg Loss: 0.0154 | Grad Norm: 0.00799060\n",
      "Epoch 2 | Step 1396100 | Avg Loss: 0.0158 | Grad Norm: 0.00916130\n",
      "Epoch 2 | Step 1396200 | Avg Loss: 0.0159 | Grad Norm: 0.00930927\n",
      "Epoch 2 | Step 1396300 | Avg Loss: 0.0162 | Grad Norm: 0.00995275\n",
      "Epoch 2 | Step 1396400 | Avg Loss: 0.0156 | Grad Norm: 0.00963723\n",
      "Epoch 2 | Step 1396500 | Avg Loss: 0.0156 | Grad Norm: 0.00921636\n",
      "Epoch 2 | Step 1396600 | Avg Loss: 0.0157 | Grad Norm: 0.00948425\n",
      "Epoch 2 | Step 1396700 | Avg Loss: 0.0159 | Grad Norm: 0.00984018\n",
      "Epoch 2 | Step 1396800 | Avg Loss: 0.0157 | Grad Norm: 0.00880742\n",
      "Epoch 2 | Step 1396900 | Avg Loss: 0.0157 | Grad Norm: 0.00799841\n",
      "Epoch 2 | Step 1397000 | Avg Loss: 0.0158 | Grad Norm: 0.01042886\n",
      "Epoch 2 | Step 1397100 | Avg Loss: 0.0158 | Grad Norm: 0.00774265\n",
      "Epoch 2 | Step 1397200 | Avg Loss: 0.0159 | Grad Norm: 0.00832408\n",
      "Epoch 2 | Step 1397300 | Avg Loss: 0.0159 | Grad Norm: 0.00931605\n",
      "Epoch 2 | Step 1397400 | Avg Loss: 0.0158 | Grad Norm: 0.00865687\n",
      "Epoch 2 | Step 1397500 | Avg Loss: 0.0156 | Grad Norm: 0.00842673\n",
      "Epoch 2 | Step 1397600 | Avg Loss: 0.0160 | Grad Norm: 0.00807024\n",
      "Epoch 2 | Step 1397700 | Avg Loss: 0.0158 | Grad Norm: 0.00827222\n",
      "Epoch 2 | Step 1397800 | Avg Loss: 0.0161 | Grad Norm: 0.00804905\n",
      "Epoch 2 | Step 1397900 | Avg Loss: 0.0160 | Grad Norm: 0.00928773\n",
      "Epoch 2 | Step 1398000 | Avg Loss: 0.0161 | Grad Norm: 0.00922040\n",
      "Epoch 2 | Step 1398100 | Avg Loss: 0.0159 | Grad Norm: 0.00834626\n",
      "Epoch 2 | Step 1398200 | Avg Loss: 0.0164 | Grad Norm: 0.00865870\n",
      "Epoch 2 | Step 1398300 | Avg Loss: 0.0160 | Grad Norm: 0.00838273\n",
      "Epoch 2 | Step 1398400 | Avg Loss: 0.0157 | Grad Norm: 0.01116763\n",
      "Epoch 2 | Step 1398500 | Avg Loss: 0.0154 | Grad Norm: 0.00918090\n",
      "Epoch 2 | Step 1398600 | Avg Loss: 0.0157 | Grad Norm: 0.00817961\n",
      "Epoch 2 | Step 1398700 | Avg Loss: 0.0159 | Grad Norm: 0.00869512\n",
      "Epoch 2 | Step 1398800 | Avg Loss: 0.0160 | Grad Norm: 0.00866225\n",
      "Epoch 2 | Step 1398900 | Avg Loss: 0.0162 | Grad Norm: 0.00845885\n",
      "Epoch 2 | Step 1399000 | Avg Loss: 0.0159 | Grad Norm: 0.00798700\n",
      "Epoch 2 | Step 1399100 | Avg Loss: 0.0160 | Grad Norm: 0.00970934\n",
      "Epoch 2 | Step 1399200 | Avg Loss: 0.0160 | Grad Norm: 0.00991620\n",
      "Epoch 2 | Step 1399300 | Avg Loss: 0.0158 | Grad Norm: 0.00942704\n",
      "Epoch 2 | Step 1399400 | Avg Loss: 0.0159 | Grad Norm: 0.00844694\n",
      "Epoch 2 | Step 1399500 | Avg Loss: 0.0154 | Grad Norm: 0.00719869\n",
      "Epoch 2 | Step 1399600 | Avg Loss: 0.0153 | Grad Norm: 0.00865204\n",
      "Epoch 2 | Step 1399700 | Avg Loss: 0.0155 | Grad Norm: 0.00878821\n",
      "Epoch 2 | Step 1399800 | Avg Loss: 0.0153 | Grad Norm: 0.00796408\n",
      "Epoch 2 | Step 1399900 | Avg Loss: 0.0154 | Grad Norm: 0.00902054\n",
      "Epoch 2 | Step 1400000 | Avg Loss: 0.0153 | Grad Norm: 0.00972175\n",
      "Saving model at step1400000\n",
      "Epoch 2 | Step 1400100 | Avg Loss: 0.0155 | Grad Norm: 0.00971899\n",
      "Epoch 2 | Step 1400200 | Avg Loss: 0.0155 | Grad Norm: 0.00842057\n",
      "Epoch 2 | Step 1400300 | Avg Loss: 0.0157 | Grad Norm: 0.00959921\n",
      "Epoch 2 | Step 1400400 | Avg Loss: 0.0154 | Grad Norm: 0.00898309\n",
      "Epoch 2 | Step 1400500 | Avg Loss: 0.0156 | Grad Norm: 0.00966220\n",
      "Epoch 2 | Step 1400600 | Avg Loss: 0.0157 | Grad Norm: 0.00890786\n",
      "Epoch 2 | Step 1400700 | Avg Loss: 0.0159 | Grad Norm: 0.00832185\n",
      "Epoch 2 | Step 1400800 | Avg Loss: 0.0158 | Grad Norm: 0.01024209\n",
      "Epoch 2 | Step 1400900 | Avg Loss: 0.0162 | Grad Norm: 0.00766449\n",
      "Epoch 2 | Step 1401000 | Avg Loss: 0.0158 | Grad Norm: 0.00808034\n",
      "Epoch 2 | Step 1401100 | Avg Loss: 0.0160 | Grad Norm: 0.00903811\n",
      "Epoch 2 | Step 1401200 | Avg Loss: 0.0159 | Grad Norm: 0.01173526\n",
      "Epoch 2 | Step 1401300 | Avg Loss: 0.0158 | Grad Norm: 0.00923497\n",
      "Epoch 2 | Step 1401400 | Avg Loss: 0.0159 | Grad Norm: 0.00827568\n",
      "Epoch 2 | Step 1401500 | Avg Loss: 0.0156 | Grad Norm: 0.00849731\n",
      "Epoch 2 | Step 1401600 | Avg Loss: 0.0158 | Grad Norm: 0.00814306\n",
      "Epoch 2 | Step 1401700 | Avg Loss: 0.0159 | Grad Norm: 0.00841858\n",
      "Epoch 2 | Step 1401800 | Avg Loss: 0.0156 | Grad Norm: 0.00829763\n",
      "Epoch 2 | Step 1401900 | Avg Loss: 0.0156 | Grad Norm: 0.00915466\n",
      "Epoch 2 | Step 1402000 | Avg Loss: 0.0158 | Grad Norm: 0.01020931\n",
      "Epoch 2 | Step 1402100 | Avg Loss: 0.0157 | Grad Norm: 0.00895022\n",
      "Epoch 2 | Step 1402200 | Avg Loss: 0.0158 | Grad Norm: 0.00830123\n",
      "Epoch 2 | Step 1402300 | Avg Loss: 0.0157 | Grad Norm: 0.00844771\n",
      "Epoch 2 | Step 1402400 | Avg Loss: 0.0156 | Grad Norm: 0.00799026\n",
      "Epoch 2 | Step 1402500 | Avg Loss: 0.0153 | Grad Norm: 0.00900553\n",
      "Epoch 2 | Step 1402600 | Avg Loss: 0.0153 | Grad Norm: 0.00723376\n",
      "Epoch 2 | Step 1402700 | Avg Loss: 0.0152 | Grad Norm: 0.00925308\n",
      "Epoch 2 | Step 1402800 | Avg Loss: 0.0154 | Grad Norm: 0.00940146\n",
      "Epoch 2 | Step 1402900 | Avg Loss: 0.0155 | Grad Norm: 0.00799813\n",
      "Epoch 2 | Step 1403000 | Avg Loss: 0.0155 | Grad Norm: 0.00810350\n",
      "Epoch 2 | Step 1403100 | Avg Loss: 0.0156 | Grad Norm: 0.00887550\n",
      "Epoch 2 | Step 1403200 | Avg Loss: 0.0157 | Grad Norm: 0.00966768\n",
      "Epoch 2 | Step 1403300 | Avg Loss: 0.0156 | Grad Norm: 0.01014956\n",
      "Epoch 2 | Step 1403400 | Avg Loss: 0.0155 | Grad Norm: 0.00990086\n",
      "Epoch 2 | Step 1403500 | Avg Loss: 0.0155 | Grad Norm: 0.00887745\n",
      "Epoch 2 | Step 1403600 | Avg Loss: 0.0155 | Grad Norm: 0.00939762\n",
      "Epoch 2 | Step 1403700 | Avg Loss: 0.0154 | Grad Norm: 0.00898488\n",
      "Epoch 2 | Step 1403800 | Avg Loss: 0.0157 | Grad Norm: 0.00812317\n",
      "Epoch 2 | Step 1403900 | Avg Loss: 0.0155 | Grad Norm: 0.00970055\n",
      "Epoch 2 | Step 1404000 | Avg Loss: 0.0151 | Grad Norm: 0.00852651\n",
      "Epoch 2 | Step 1404100 | Avg Loss: 0.0152 | Grad Norm: 0.00971090\n",
      "Epoch 2 | Step 1404200 | Avg Loss: 0.0152 | Grad Norm: 0.00840218\n",
      "Epoch 2 | Step 1404300 | Avg Loss: 0.0153 | Grad Norm: 0.00907179\n",
      "Epoch 2 | Step 1404400 | Avg Loss: 0.0150 | Grad Norm: 0.00898521\n",
      "Epoch 2 | Step 1404500 | Avg Loss: 0.0151 | Grad Norm: 0.00997798\n",
      "Epoch 2 | Step 1404600 | Avg Loss: 0.0151 | Grad Norm: 0.00834657\n",
      "Epoch 2 | Step 1404700 | Avg Loss: 0.0151 | Grad Norm: 0.00959310\n",
      "Epoch 2 | Step 1404800 | Avg Loss: 0.0148 | Grad Norm: 0.00928415\n",
      "Epoch 2 | Step 1404900 | Avg Loss: 0.0150 | Grad Norm: 0.01009876\n",
      "Epoch 2 | Step 1405000 | Avg Loss: 0.0153 | Grad Norm: 0.00854262\n",
      "Epoch 2 | Step 1405100 | Avg Loss: 0.0157 | Grad Norm: 0.00786130\n",
      "Epoch 2 | Step 1405200 | Avg Loss: 0.0156 | Grad Norm: 0.00952006\n",
      "Epoch 2 | Step 1405300 | Avg Loss: 0.0153 | Grad Norm: 0.00862884\n",
      "Epoch 2 | Step 1405400 | Avg Loss: 0.0154 | Grad Norm: 0.00993840\n",
      "Epoch 2 | Step 1405500 | Avg Loss: 0.0155 | Grad Norm: 0.00950307\n",
      "Epoch 2 | Step 1405600 | Avg Loss: 0.0156 | Grad Norm: 0.00719425\n",
      "Epoch 2 | Step 1405700 | Avg Loss: 0.0155 | Grad Norm: 0.00980727\n",
      "Epoch 2 | Step 1405800 | Avg Loss: 0.0159 | Grad Norm: 0.00874084\n",
      "Epoch 2 | Step 1405900 | Avg Loss: 0.0155 | Grad Norm: 0.00970118\n",
      "Epoch 2 | Step 1406000 | Avg Loss: 0.0155 | Grad Norm: 0.00945697\n",
      "Epoch 2 | Step 1406100 | Avg Loss: 0.0157 | Grad Norm: 0.00890796\n",
      "Epoch 2 | Step 1406200 | Avg Loss: 0.0156 | Grad Norm: 0.00802089\n",
      "Epoch 2 | Step 1406300 | Avg Loss: 0.0152 | Grad Norm: 0.00876102\n",
      "Epoch 2 | Step 1406400 | Avg Loss: 0.0151 | Grad Norm: 0.01053258\n",
      "Epoch 2 | Step 1406500 | Avg Loss: 0.0148 | Grad Norm: 0.00822393\n",
      "Epoch 2 | Step 1406600 | Avg Loss: 0.0148 | Grad Norm: 0.01237798\n",
      "Epoch 2 | Step 1406700 | Avg Loss: 0.0150 | Grad Norm: 0.00816067\n",
      "Epoch 2 | Step 1406800 | Avg Loss: 0.0153 | Grad Norm: 0.00903280\n",
      "Epoch 2 | Step 1406900 | Avg Loss: 0.0153 | Grad Norm: 0.00928272\n",
      "Epoch 2 | Step 1407000 | Avg Loss: 0.0155 | Grad Norm: 0.00880306\n",
      "Epoch 2 | Step 1407100 | Avg Loss: 0.0151 | Grad Norm: 0.00840139\n",
      "Epoch 2 | Step 1407200 | Avg Loss: 0.0149 | Grad Norm: 0.00954882\n",
      "Epoch 2 | Step 1407300 | Avg Loss: 0.0153 | Grad Norm: 0.00926758\n",
      "Epoch 2 | Step 1407400 | Avg Loss: 0.0154 | Grad Norm: 0.00918546\n",
      "Epoch 2 | Step 1407500 | Avg Loss: 0.0151 | Grad Norm: 0.00900632\n",
      "Epoch 2 | Step 1407600 | Avg Loss: 0.0155 | Grad Norm: 0.00863895\n",
      "Epoch 2 | Step 1407700 | Avg Loss: 0.0157 | Grad Norm: 0.00894877\n",
      "Epoch 2 | Step 1407800 | Avg Loss: 0.0157 | Grad Norm: 0.01008844\n",
      "Epoch 2 | Step 1407900 | Avg Loss: 0.0158 | Grad Norm: 0.00909318\n",
      "Epoch 2 | Step 1408000 | Avg Loss: 0.0158 | Grad Norm: 0.00898764\n",
      "Epoch 2 | Step 1408100 | Avg Loss: 0.0158 | Grad Norm: 0.00964850\n",
      "Epoch 2 | Step 1408200 | Avg Loss: 0.0158 | Grad Norm: 0.00762172\n",
      "Epoch 2 | Step 1408300 | Avg Loss: 0.0156 | Grad Norm: 0.00771645\n",
      "Epoch 2 | Step 1408400 | Avg Loss: 0.0155 | Grad Norm: 0.01052742\n",
      "Epoch 2 | Step 1408500 | Avg Loss: 0.0155 | Grad Norm: 0.00894386\n",
      "Epoch 2 | Step 1408600 | Avg Loss: 0.0154 | Grad Norm: 0.00807762\n",
      "Epoch 2 | Step 1408700 | Avg Loss: 0.0155 | Grad Norm: 0.01096287\n",
      "Epoch 2 | Step 1408800 | Avg Loss: 0.0153 | Grad Norm: 0.00860423\n",
      "Epoch 2 | Step 1408900 | Avg Loss: 0.0154 | Grad Norm: 0.00808878\n",
      "Epoch 2 | Step 1409000 | Avg Loss: 0.0156 | Grad Norm: 0.00960265\n",
      "Epoch 2 | Step 1409100 | Avg Loss: 0.0156 | Grad Norm: 0.01039419\n",
      "Epoch 2 | Step 1409200 | Avg Loss: 0.0159 | Grad Norm: 0.00903820\n",
      "Epoch 2 | Step 1409300 | Avg Loss: 0.0152 | Grad Norm: 0.00854645\n",
      "Epoch 2 | Step 1409400 | Avg Loss: 0.0155 | Grad Norm: 0.00793233\n",
      "Epoch 2 | Step 1409500 | Avg Loss: 0.0151 | Grad Norm: 0.00792538\n",
      "Epoch 2 | Step 1409600 | Avg Loss: 0.0151 | Grad Norm: 0.00753531\n",
      "Epoch 2 | Step 1409700 | Avg Loss: 0.0150 | Grad Norm: 0.00901727\n",
      "Epoch 2 | Step 1409800 | Avg Loss: 0.0157 | Grad Norm: 0.01018788\n",
      "Epoch 2 | Step 1409900 | Avg Loss: 0.0157 | Grad Norm: 0.00845502\n",
      "Epoch 2 | Step 1410000 | Avg Loss: 0.0157 | Grad Norm: 0.00901886\n",
      "Epoch 2 | Step 1410100 | Avg Loss: 0.0153 | Grad Norm: 0.00798478\n",
      "Epoch 2 | Step 1410200 | Avg Loss: 0.0153 | Grad Norm: 0.00972259\n",
      "Epoch 2 | Step 1410300 | Avg Loss: 0.0154 | Grad Norm: 0.01028950\n",
      "Epoch 2 | Step 1410400 | Avg Loss: 0.0154 | Grad Norm: 0.00805048\n",
      "Epoch 2 | Step 1410500 | Avg Loss: 0.0155 | Grad Norm: 0.00876604\n",
      "Epoch 2 | Step 1410600 | Avg Loss: 0.0154 | Grad Norm: 0.00901061\n",
      "Epoch 2 | Step 1410700 | Avg Loss: 0.0154 | Grad Norm: 0.00771668\n",
      "Epoch 2 | Step 1410800 | Avg Loss: 0.0157 | Grad Norm: 0.00779416\n",
      "Epoch 2 | Step 1410900 | Avg Loss: 0.0159 | Grad Norm: 0.00936518\n",
      "Epoch 2 | Step 1411000 | Avg Loss: 0.0162 | Grad Norm: 0.00923064\n",
      "Epoch 2 | Step 1411100 | Avg Loss: 0.0162 | Grad Norm: 0.00860456\n",
      "Epoch 2 | Step 1411200 | Avg Loss: 0.0158 | Grad Norm: 0.00821999\n",
      "Epoch 2 | Step 1411300 | Avg Loss: 0.0156 | Grad Norm: 0.01105850\n",
      "Epoch 2 | Step 1411400 | Avg Loss: 0.0155 | Grad Norm: 0.00812315\n",
      "Epoch 2 | Step 1411500 | Avg Loss: 0.0156 | Grad Norm: 0.01026067\n",
      "Epoch 2 | Step 1411600 | Avg Loss: 0.0160 | Grad Norm: 0.00835132\n",
      "Epoch 2 | Step 1411700 | Avg Loss: 0.0158 | Grad Norm: 0.00790439\n",
      "Epoch 2 | Step 1411800 | Avg Loss: 0.0157 | Grad Norm: 0.00896504\n",
      "Epoch 2 | Step 1411900 | Avg Loss: 0.0156 | Grad Norm: 0.00763230\n",
      "Epoch 2 | Step 1412000 | Avg Loss: 0.0158 | Grad Norm: 0.00773447\n",
      "Epoch 2 | Step 1412100 | Avg Loss: 0.0159 | Grad Norm: 0.00821064\n",
      "Epoch 2 | Step 1412200 | Avg Loss: 0.0161 | Grad Norm: 0.01030481\n",
      "Epoch 2 | Step 1412300 | Avg Loss: 0.0160 | Grad Norm: 0.00795325\n",
      "Epoch 2 | Step 1412400 | Avg Loss: 0.0161 | Grad Norm: 0.00888561\n",
      "Epoch 2 | Step 1412500 | Avg Loss: 0.0159 | Grad Norm: 0.00947452\n",
      "Epoch 2 | Step 1412600 | Avg Loss: 0.0158 | Grad Norm: 0.00905775\n",
      "Epoch 2 | Step 1412700 | Avg Loss: 0.0161 | Grad Norm: 0.00946609\n",
      "Epoch 2 | Step 1412800 | Avg Loss: 0.0163 | Grad Norm: 0.00937167\n",
      "Epoch 2 | Step 1412900 | Avg Loss: 0.0162 | Grad Norm: 0.00976204\n",
      "Epoch 2 | Step 1413000 | Avg Loss: 0.0158 | Grad Norm: 0.00758429\n",
      "Epoch 2 | Step 1413100 | Avg Loss: 0.0158 | Grad Norm: 0.00852293\n",
      "Epoch 2 | Step 1413200 | Avg Loss: 0.0157 | Grad Norm: 0.00927193\n",
      "Epoch 2 | Step 1413300 | Avg Loss: 0.0156 | Grad Norm: 0.00845252\n",
      "Epoch 2 | Step 1413400 | Avg Loss: 0.0156 | Grad Norm: 0.00919253\n",
      "Epoch 2 | Step 1413500 | Avg Loss: 0.0153 | Grad Norm: 0.00782253\n",
      "Epoch 2 | Step 1413600 | Avg Loss: 0.0156 | Grad Norm: 0.00826048\n",
      "Epoch 2 | Step 1413700 | Avg Loss: 0.0152 | Grad Norm: 0.00952376\n",
      "Epoch 2 | Step 1413800 | Avg Loss: 0.0154 | Grad Norm: 0.00863409\n",
      "Epoch 2 | Step 1413900 | Avg Loss: 0.0157 | Grad Norm: 0.00784130\n",
      "Epoch 2 | Step 1414000 | Avg Loss: 0.0156 | Grad Norm: 0.00844223\n",
      "Epoch 2 | Step 1414100 | Avg Loss: 0.0157 | Grad Norm: 0.00866397\n",
      "Epoch 2 | Step 1414200 | Avg Loss: 0.0157 | Grad Norm: 0.00914033\n",
      "Epoch 2 | Step 1414300 | Avg Loss: 0.0156 | Grad Norm: 0.00819267\n",
      "Epoch 2 | Step 1414400 | Avg Loss: 0.0152 | Grad Norm: 0.00798707\n",
      "Epoch 2 | Step 1414500 | Avg Loss: 0.0152 | Grad Norm: 0.00928671\n",
      "Epoch 2 | Step 1414600 | Avg Loss: 0.0155 | Grad Norm: 0.00811226\n",
      "Epoch 2 | Step 1414700 | Avg Loss: 0.0156 | Grad Norm: 0.00894274\n",
      "Epoch 2 | Step 1414800 | Avg Loss: 0.0157 | Grad Norm: 0.00897831\n",
      "Epoch 2 | Step 1414900 | Avg Loss: 0.0153 | Grad Norm: 0.00899785\n",
      "Epoch 2 | Step 1415000 | Avg Loss: 0.0154 | Grad Norm: 0.00890593\n",
      "Epoch 2 | Step 1415100 | Avg Loss: 0.0151 | Grad Norm: 0.00934730\n",
      "Epoch 2 | Step 1415200 | Avg Loss: 0.0152 | Grad Norm: 0.00842998\n",
      "Epoch 2 | Step 1415300 | Avg Loss: 0.0154 | Grad Norm: 0.00864865\n",
      "Epoch 2 | Step 1415400 | Avg Loss: 0.0153 | Grad Norm: 0.00962616\n",
      "Epoch 2 | Step 1415500 | Avg Loss: 0.0152 | Grad Norm: 0.00811691\n",
      "Epoch 2 | Step 1415600 | Avg Loss: 0.0154 | Grad Norm: 0.00906199\n",
      "Epoch 2 | Step 1415700 | Avg Loss: 0.0157 | Grad Norm: 0.00940928\n",
      "Epoch 2 | Step 1415800 | Avg Loss: 0.0157 | Grad Norm: 0.00887316\n",
      "Epoch 2 | Step 1415900 | Avg Loss: 0.0156 | Grad Norm: 0.01034365\n",
      "Epoch 2 | Step 1416000 | Avg Loss: 0.0157 | Grad Norm: 0.00803787\n",
      "Epoch 2 | Step 1416100 | Avg Loss: 0.0160 | Grad Norm: 0.00917339\n",
      "Epoch 2 | Step 1416200 | Avg Loss: 0.0161 | Grad Norm: 0.00974936\n",
      "Epoch 2 | Step 1416300 | Avg Loss: 0.0158 | Grad Norm: 0.00841040\n",
      "Epoch 2 | Step 1416400 | Avg Loss: 0.0160 | Grad Norm: 0.00858312\n",
      "Epoch 2 | Step 1416500 | Avg Loss: 0.0160 | Grad Norm: 0.00829813\n",
      "Epoch 2 | Step 1416600 | Avg Loss: 0.0160 | Grad Norm: 0.01045242\n",
      "Epoch 2 | Step 1416700 | Avg Loss: 0.0155 | Grad Norm: 0.00849758\n",
      "Epoch 2 | Step 1416800 | Avg Loss: 0.0160 | Grad Norm: 0.01135396\n",
      "Epoch 2 | Step 1416900 | Avg Loss: 0.0162 | Grad Norm: 0.01121073\n",
      "Epoch 2 | Step 1417000 | Avg Loss: 0.0160 | Grad Norm: 0.00964675\n",
      "Epoch 2 | Step 1417100 | Avg Loss: 0.0159 | Grad Norm: 0.00845162\n",
      "Epoch 2 | Step 1417200 | Avg Loss: 0.0162 | Grad Norm: 0.00911341\n",
      "Epoch 2 | Step 1417300 | Avg Loss: 0.0163 | Grad Norm: 0.00961777\n",
      "Epoch 2 | Step 1417400 | Avg Loss: 0.0159 | Grad Norm: 0.00943792\n",
      "Epoch 2 | Step 1417500 | Avg Loss: 0.0159 | Grad Norm: 0.00933019\n",
      "Epoch 2 | Step 1417600 | Avg Loss: 0.0159 | Grad Norm: 0.00899394\n",
      "Epoch 2 | Step 1417700 | Avg Loss: 0.0159 | Grad Norm: 0.00894249\n",
      "Epoch 2 | Step 1417800 | Avg Loss: 0.0157 | Grad Norm: 0.00967797\n",
      "Epoch 2 | Step 1417900 | Avg Loss: 0.0153 | Grad Norm: 0.00928368\n",
      "Epoch 2 | Step 1418000 | Avg Loss: 0.0154 | Grad Norm: 0.00864858\n",
      "Epoch 2 | Step 1418100 | Avg Loss: 0.0155 | Grad Norm: 0.00953169\n",
      "Epoch 2 | Step 1418200 | Avg Loss: 0.0154 | Grad Norm: 0.00813917\n",
      "Epoch 2 | Step 1418300 | Avg Loss: 0.0157 | Grad Norm: 0.01073955\n",
      "Epoch 2 | Step 1418400 | Avg Loss: 0.0154 | Grad Norm: 0.00933229\n",
      "Epoch 2 | Step 1418500 | Avg Loss: 0.0155 | Grad Norm: 0.00879450\n",
      "Epoch 2 | Step 1418600 | Avg Loss: 0.0155 | Grad Norm: 0.00820772\n",
      "Epoch 2 | Step 1418700 | Avg Loss: 0.0155 | Grad Norm: 0.00869478\n",
      "Epoch 2 | Step 1418800 | Avg Loss: 0.0155 | Grad Norm: 0.00900078\n",
      "Epoch 2 | Step 1418900 | Avg Loss: 0.0158 | Grad Norm: 0.01113443\n",
      "Epoch 2 | Step 1419000 | Avg Loss: 0.0159 | Grad Norm: 0.00953169\n",
      "Epoch 2 | Step 1419100 | Avg Loss: 0.0158 | Grad Norm: 0.00851554\n",
      "Epoch 2 | Step 1419200 | Avg Loss: 0.0159 | Grad Norm: 0.00936186\n",
      "Epoch 2 | Step 1419300 | Avg Loss: 0.0159 | Grad Norm: 0.00896053\n",
      "Epoch 2 | Step 1419400 | Avg Loss: 0.0158 | Grad Norm: 0.01027761\n",
      "Epoch 2 | Step 1419500 | Avg Loss: 0.0156 | Grad Norm: 0.01050025\n",
      "Epoch 2 | Step 1419600 | Avg Loss: 0.0157 | Grad Norm: 0.00921582\n",
      "Epoch 2 | Step 1419700 | Avg Loss: 0.0156 | Grad Norm: 0.01018849\n",
      "Epoch 2 | Step 1419800 | Avg Loss: 0.0159 | Grad Norm: 0.00973815\n",
      "Epoch 2 | Step 1419900 | Avg Loss: 0.0154 | Grad Norm: 0.00855017\n",
      "Epoch 2 | Step 1420000 | Avg Loss: 0.0154 | Grad Norm: 0.00822147\n",
      "Epoch 2 | Step 1420100 | Avg Loss: 0.0155 | Grad Norm: 0.01162627\n",
      "Epoch 2 | Step 1420200 | Avg Loss: 0.0156 | Grad Norm: 0.00890728\n",
      "Epoch 2 | Step 1420300 | Avg Loss: 0.0155 | Grad Norm: 0.00808374\n",
      "Epoch 2 | Step 1420400 | Avg Loss: 0.0155 | Grad Norm: 0.00924385\n",
      "Epoch 2 | Step 1420500 | Avg Loss: 0.0157 | Grad Norm: 0.00878882\n",
      "Epoch 2 | Step 1420600 | Avg Loss: 0.0156 | Grad Norm: 0.00906342\n",
      "Epoch 2 | Step 1420700 | Avg Loss: 0.0161 | Grad Norm: 0.00916011\n",
      "Epoch 2 | Step 1420800 | Avg Loss: 0.0160 | Grad Norm: 0.00875669\n",
      "Epoch 2 | Step 1420900 | Avg Loss: 0.0157 | Grad Norm: 0.00856511\n",
      "Epoch 2 | Step 1421000 | Avg Loss: 0.0158 | Grad Norm: 0.00957719\n",
      "Epoch 2 | Step 1421100 | Avg Loss: 0.0161 | Grad Norm: 0.00801362\n",
      "Epoch 2 | Step 1421200 | Avg Loss: 0.0158 | Grad Norm: 0.00876109\n",
      "Epoch 2 | Step 1421300 | Avg Loss: 0.0160 | Grad Norm: 0.00864933\n",
      "Epoch 2 | Step 1421400 | Avg Loss: 0.0157 | Grad Norm: 0.00849714\n",
      "Epoch 2 | Step 1421500 | Avg Loss: 0.0155 | Grad Norm: 0.00829746\n",
      "Epoch 2 | Step 1421600 | Avg Loss: 0.0153 | Grad Norm: 0.00973101\n",
      "Epoch 2 | Step 1421700 | Avg Loss: 0.0154 | Grad Norm: 0.00847408\n",
      "Epoch 2 | Step 1421800 | Avg Loss: 0.0156 | Grad Norm: 0.00872904\n",
      "Epoch 2 | Step 1421900 | Avg Loss: 0.0151 | Grad Norm: 0.00942920\n",
      "Epoch 2 | Step 1422000 | Avg Loss: 0.0153 | Grad Norm: 0.00806531\n",
      "Epoch 2 | Step 1422100 | Avg Loss: 0.0150 | Grad Norm: 0.00867201\n",
      "Epoch 2 | Step 1422200 | Avg Loss: 0.0152 | Grad Norm: 0.01015998\n",
      "Epoch 2 | Step 1422300 | Avg Loss: 0.0154 | Grad Norm: 0.00825166\n",
      "Epoch 2 | Step 1422400 | Avg Loss: 0.0155 | Grad Norm: 0.00846659\n",
      "Epoch 2 | Step 1422500 | Avg Loss: 0.0156 | Grad Norm: 0.00898787\n",
      "Epoch 2 | Step 1422600 | Avg Loss: 0.0156 | Grad Norm: 0.00801188\n",
      "Epoch 2 | Step 1422700 | Avg Loss: 0.0158 | Grad Norm: 0.00949827\n",
      "Epoch 2 | Step 1422800 | Avg Loss: 0.0156 | Grad Norm: 0.00819641\n",
      "Epoch 2 | Step 1422900 | Avg Loss: 0.0152 | Grad Norm: 0.00832290\n",
      "Epoch 2 | Step 1423000 | Avg Loss: 0.0150 | Grad Norm: 0.00893792\n",
      "Epoch 2 | Step 1423100 | Avg Loss: 0.0153 | Grad Norm: 0.00979190\n",
      "Epoch 2 | Step 1423200 | Avg Loss: 0.0152 | Grad Norm: 0.00819753\n",
      "Epoch 2 | Step 1423300 | Avg Loss: 0.0157 | Grad Norm: 0.00849059\n",
      "Epoch 2 | Step 1423400 | Avg Loss: 0.0155 | Grad Norm: 0.00835995\n",
      "Epoch 2 | Step 1423500 | Avg Loss: 0.0156 | Grad Norm: 0.00800441\n",
      "Epoch 2 | Step 1423600 | Avg Loss: 0.0154 | Grad Norm: 0.00936837\n",
      "Epoch 2 | Step 1423700 | Avg Loss: 0.0156 | Grad Norm: 0.00893302\n",
      "Epoch 2 | Step 1423800 | Avg Loss: 0.0157 | Grad Norm: 0.00855055\n",
      "Epoch 2 | Step 1423900 | Avg Loss: 0.0155 | Grad Norm: 0.00863375\n",
      "Epoch 2 | Step 1424000 | Avg Loss: 0.0154 | Grad Norm: 0.00911711\n",
      "Epoch 2 | Step 1424100 | Avg Loss: 0.0155 | Grad Norm: 0.00904163\n",
      "Epoch 2 | Step 1424200 | Avg Loss: 0.0154 | Grad Norm: 0.00849472\n",
      "Epoch 2 | Step 1424300 | Avg Loss: 0.0157 | Grad Norm: 0.00824033\n",
      "Epoch 2 | Step 1424400 | Avg Loss: 0.0154 | Grad Norm: 0.00774938\n",
      "Epoch 2 | Step 1424500 | Avg Loss: 0.0156 | Grad Norm: 0.00884575\n",
      "Epoch 2 | Step 1424600 | Avg Loss: 0.0154 | Grad Norm: 0.00858494\n",
      "Epoch 2 | Step 1424700 | Avg Loss: 0.0157 | Grad Norm: 0.00927017\n",
      "Epoch 2 | Step 1424800 | Avg Loss: 0.0156 | Grad Norm: 0.00869486\n",
      "Epoch 2 | Step 1424900 | Avg Loss: 0.0156 | Grad Norm: 0.00985172\n",
      "Epoch 2 | Step 1425000 | Avg Loss: 0.0157 | Grad Norm: 0.00918132\n",
      "Epoch 2 | Step 1425100 | Avg Loss: 0.0156 | Grad Norm: 0.01079297\n",
      "Epoch 2 | Step 1425200 | Avg Loss: 0.0157 | Grad Norm: 0.01087352\n",
      "Epoch 2 | Step 1425300 | Avg Loss: 0.0156 | Grad Norm: 0.01046286\n",
      "Epoch 2 | Step 1425400 | Avg Loss: 0.0156 | Grad Norm: 0.00767360\n",
      "Epoch 2 | Step 1425500 | Avg Loss: 0.0155 | Grad Norm: 0.01023672\n",
      "Epoch 2 | Step 1425600 | Avg Loss: 0.0158 | Grad Norm: 0.00794359\n",
      "Epoch 2 | Step 1425700 | Avg Loss: 0.0158 | Grad Norm: 0.00969055\n",
      "Epoch 2 | Step 1425800 | Avg Loss: 0.0157 | Grad Norm: 0.00859409\n",
      "Epoch 2 | Step 1425900 | Avg Loss: 0.0157 | Grad Norm: 0.00850633\n",
      "Epoch 2 | Step 1426000 | Avg Loss: 0.0158 | Grad Norm: 0.01026809\n",
      "Epoch 2 | Step 1426100 | Avg Loss: 0.0156 | Grad Norm: 0.00883336\n",
      "Epoch 2 | Step 1426200 | Avg Loss: 0.0157 | Grad Norm: 0.00952707\n",
      "Epoch 2 | Step 1426300 | Avg Loss: 0.0156 | Grad Norm: 0.01004499\n",
      "Epoch 2 | Step 1426400 | Avg Loss: 0.0154 | Grad Norm: 0.00846404\n",
      "Epoch 2 | Step 1426500 | Avg Loss: 0.0152 | Grad Norm: 0.00849280\n",
      "Epoch 2 | Step 1426600 | Avg Loss: 0.0153 | Grad Norm: 0.00921511\n",
      "Epoch 2 | Step 1426700 | Avg Loss: 0.0154 | Grad Norm: 0.00989963\n",
      "Epoch 2 | Step 1426800 | Avg Loss: 0.0152 | Grad Norm: 0.00954873\n",
      "Epoch 2 | Step 1426900 | Avg Loss: 0.0152 | Grad Norm: 0.00914239\n",
      "Epoch 2 | Step 1427000 | Avg Loss: 0.0154 | Grad Norm: 0.01052731\n",
      "Epoch 2 | Step 1427100 | Avg Loss: 0.0152 | Grad Norm: 0.00845161\n",
      "Epoch 2 | Step 1427200 | Avg Loss: 0.0152 | Grad Norm: 0.00866195\n",
      "Epoch 2 | Step 1427300 | Avg Loss: 0.0153 | Grad Norm: 0.00921677\n",
      "Epoch 2 | Step 1427400 | Avg Loss: 0.0154 | Grad Norm: 0.00810078\n",
      "Epoch 2 | Step 1427500 | Avg Loss: 0.0153 | Grad Norm: 0.00878167\n",
      "Epoch 2 | Step 1427600 | Avg Loss: 0.0151 | Grad Norm: 0.00969890\n",
      "Epoch 2 | Step 1427700 | Avg Loss: 0.0153 | Grad Norm: 0.00799016\n",
      "Epoch 2 | Step 1427800 | Avg Loss: 0.0155 | Grad Norm: 0.00877088\n",
      "Epoch 2 | Step 1427900 | Avg Loss: 0.0157 | Grad Norm: 0.00888149\n",
      "Epoch 2 | Step 1428000 | Avg Loss: 0.0153 | Grad Norm: 0.00918612\n",
      "Epoch 2 | Step 1428100 | Avg Loss: 0.0152 | Grad Norm: 0.00971636\n",
      "Epoch 2 | Step 1428200 | Avg Loss: 0.0153 | Grad Norm: 0.00822359\n",
      "Epoch 2 | Step 1428300 | Avg Loss: 0.0154 | Grad Norm: 0.00945541\n",
      "Epoch 2 | Step 1428400 | Avg Loss: 0.0155 | Grad Norm: 0.01046683\n",
      "Epoch 2 | Step 1428500 | Avg Loss: 0.0157 | Grad Norm: 0.01282148\n",
      "Epoch 2 | Step 1428600 | Avg Loss: 0.0155 | Grad Norm: 0.00970797\n",
      "Epoch 2 | Step 1428700 | Avg Loss: 0.0158 | Grad Norm: 0.00841787\n",
      "Epoch 2 | Step 1428800 | Avg Loss: 0.0152 | Grad Norm: 0.01078474\n",
      "Epoch 2 | Step 1428900 | Avg Loss: 0.0153 | Grad Norm: 0.00895863\n",
      "Epoch 2 | Step 1429000 | Avg Loss: 0.0150 | Grad Norm: 0.00911317\n",
      "Epoch 2 | Step 1429100 | Avg Loss: 0.0153 | Grad Norm: 0.00881428\n",
      "Epoch 2 | Step 1429200 | Avg Loss: 0.0155 | Grad Norm: 0.00872754\n",
      "Epoch 2 | Step 1429300 | Avg Loss: 0.0156 | Grad Norm: 0.01237937\n",
      "Epoch 2 | Step 1429400 | Avg Loss: 0.0157 | Grad Norm: 0.00810262\n",
      "Epoch 2 | Step 1429500 | Avg Loss: 0.0156 | Grad Norm: 0.00876370\n",
      "Epoch 2 | Step 1429600 | Avg Loss: 0.0157 | Grad Norm: 0.00917706\n",
      "Epoch 2 | Step 1429700 | Avg Loss: 0.0158 | Grad Norm: 0.01003812\n",
      "Epoch 2 | Step 1429800 | Avg Loss: 0.0159 | Grad Norm: 0.00933386\n",
      "Epoch 2 | Step 1429900 | Avg Loss: 0.0154 | Grad Norm: 0.00955333\n",
      "Epoch 2 | Step 1430000 | Avg Loss: 0.0155 | Grad Norm: 0.00886391\n",
      "Epoch 2 | Step 1430100 | Avg Loss: 0.0154 | Grad Norm: 0.00863995\n",
      "Epoch 2 | Step 1430200 | Avg Loss: 0.0155 | Grad Norm: 0.00732985\n",
      "Epoch 2 | Step 1430300 | Avg Loss: 0.0163 | Grad Norm: 0.00946317\n",
      "Epoch 2 | Step 1430400 | Avg Loss: 0.0159 | Grad Norm: 0.00835058\n",
      "Epoch 2 | Step 1430500 | Avg Loss: 0.0160 | Grad Norm: 0.00849122\n",
      "Epoch 2 | Step 1430600 | Avg Loss: 0.0160 | Grad Norm: 0.00930813\n",
      "Epoch 2 | Step 1430700 | Avg Loss: 0.0161 | Grad Norm: 0.01078040\n",
      "Epoch 2 | Step 1430800 | Avg Loss: 0.0162 | Grad Norm: 0.00906436\n",
      "Epoch 2 | Step 1430900 | Avg Loss: 0.0161 | Grad Norm: 0.00913947\n",
      "Epoch 2 | Step 1431000 | Avg Loss: 0.0159 | Grad Norm: 0.00892509\n",
      "Epoch 2 | Step 1431100 | Avg Loss: 0.0157 | Grad Norm: 0.00780194\n",
      "Epoch 2 | Step 1431200 | Avg Loss: 0.0158 | Grad Norm: 0.00848711\n",
      "Epoch 2 | Step 1431300 | Avg Loss: 0.0158 | Grad Norm: 0.00913493\n",
      "Epoch 2 | Step 1431400 | Avg Loss: 0.0153 | Grad Norm: 0.00818781\n",
      "Epoch 2 | Step 1431500 | Avg Loss: 0.0152 | Grad Norm: 0.00815369\n",
      "Epoch 2 | Step 1431600 | Avg Loss: 0.0157 | Grad Norm: 0.00903196\n",
      "Epoch 2 | Step 1431700 | Avg Loss: 0.0160 | Grad Norm: 0.01038902\n",
      "Epoch 2 | Step 1431800 | Avg Loss: 0.0162 | Grad Norm: 0.00834687\n",
      "Epoch 2 | Step 1431900 | Avg Loss: 0.0164 | Grad Norm: 0.01000848\n",
      "Epoch 2 | Step 1432000 | Avg Loss: 0.0161 | Grad Norm: 0.00908138\n",
      "Epoch 2 | Step 1432100 | Avg Loss: 0.0162 | Grad Norm: 0.00833543\n",
      "Epoch 2 | Step 1432200 | Avg Loss: 0.0162 | Grad Norm: 0.00951541\n",
      "Epoch 2 | Step 1432300 | Avg Loss: 0.0164 | Grad Norm: 0.00866879\n",
      "Epoch 2 | Step 1432400 | Avg Loss: 0.0165 | Grad Norm: 0.00822006\n",
      "Epoch 2 | Step 1432500 | Avg Loss: 0.0161 | Grad Norm: 0.00921143\n",
      "Epoch 2 | Step 1432600 | Avg Loss: 0.0159 | Grad Norm: 0.00959309\n",
      "Epoch 2 | Step 1432700 | Avg Loss: 0.0156 | Grad Norm: 0.01062096\n",
      "Epoch 2 | Step 1432800 | Avg Loss: 0.0156 | Grad Norm: 0.00892290\n",
      "Epoch 2 | Step 1432900 | Avg Loss: 0.0156 | Grad Norm: 0.00990974\n",
      "Epoch 2 | Step 1433000 | Avg Loss: 0.0158 | Grad Norm: 0.00731936\n",
      "Epoch 2 | Step 1433100 | Avg Loss: 0.0154 | Grad Norm: 0.00761670\n",
      "Epoch 2 | Step 1433200 | Avg Loss: 0.0151 | Grad Norm: 0.00929976\n",
      "Epoch 2 | Step 1433300 | Avg Loss: 0.0149 | Grad Norm: 0.00801829\n",
      "Epoch 2 | Step 1433400 | Avg Loss: 0.0151 | Grad Norm: 0.00788897\n",
      "Epoch 2 | Step 1433500 | Avg Loss: 0.0153 | Grad Norm: 0.00930641\n",
      "Epoch 2 | Step 1433600 | Avg Loss: 0.0152 | Grad Norm: 0.00925342\n",
      "Epoch 2 | Step 1433700 | Avg Loss: 0.0159 | Grad Norm: 0.00880961\n",
      "Epoch 2 | Step 1433800 | Avg Loss: 0.0157 | Grad Norm: 0.00782684\n",
      "Epoch 2 | Step 1433900 | Avg Loss: 0.0158 | Grad Norm: 0.00847935\n",
      "Epoch 2 | Step 1434000 | Avg Loss: 0.0156 | Grad Norm: 0.01057173\n",
      "Epoch 2 | Step 1434100 | Avg Loss: 0.0155 | Grad Norm: 0.00746127\n",
      "Epoch 2 | Step 1434200 | Avg Loss: 0.0156 | Grad Norm: 0.00796084\n",
      "Epoch 2 | Step 1434300 | Avg Loss: 0.0156 | Grad Norm: 0.01003137\n",
      "Epoch 2 | Step 1434400 | Avg Loss: 0.0157 | Grad Norm: 0.00886782\n",
      "Epoch 2 | Step 1434500 | Avg Loss: 0.0160 | Grad Norm: 0.00876375\n",
      "Epoch 2 | Step 1434600 | Avg Loss: 0.0158 | Grad Norm: 0.00886011\n",
      "Epoch 2 | Step 1434700 | Avg Loss: 0.0159 | Grad Norm: 0.00913503\n",
      "Epoch 2 | Step 1434800 | Avg Loss: 0.0155 | Grad Norm: 0.00895517\n",
      "Epoch 2 | Step 1434900 | Avg Loss: 0.0155 | Grad Norm: 0.00950038\n",
      "Epoch 2 | Step 1435000 | Avg Loss: 0.0158 | Grad Norm: 0.00848917\n",
      "Epoch 2 | Step 1435100 | Avg Loss: 0.0156 | Grad Norm: 0.00844522\n",
      "Epoch 2 | Step 1435200 | Avg Loss: 0.0152 | Grad Norm: 0.00919710\n",
      "Epoch 2 | Step 1435300 | Avg Loss: 0.0152 | Grad Norm: 0.00814863\n",
      "Epoch 2 | Step 1435400 | Avg Loss: 0.0154 | Grad Norm: 0.00919140\n",
      "Epoch 2 | Step 1435500 | Avg Loss: 0.0153 | Grad Norm: 0.00889303\n",
      "Epoch 2 | Step 1435600 | Avg Loss: 0.0154 | Grad Norm: 0.00931406\n",
      "Epoch 2 | Step 1435700 | Avg Loss: 0.0156 | Grad Norm: 0.00777886\n",
      "Epoch 2 | Step 1435800 | Avg Loss: 0.0157 | Grad Norm: 0.00878294\n",
      "Epoch 2 | Step 1435900 | Avg Loss: 0.0159 | Grad Norm: 0.00884768\n",
      "Epoch 2 | Step 1436000 | Avg Loss: 0.0159 | Grad Norm: 0.00889975\n",
      "Epoch 2 | Step 1436100 | Avg Loss: 0.0161 | Grad Norm: 0.00810439\n",
      "Epoch 2 | Step 1436200 | Avg Loss: 0.0158 | Grad Norm: 0.01025951\n",
      "Epoch 2 | Step 1436300 | Avg Loss: 0.0156 | Grad Norm: 0.00805210\n",
      "Epoch 2 | Step 1436400 | Avg Loss: 0.0157 | Grad Norm: 0.01197716\n",
      "Epoch 2 | Step 1436500 | Avg Loss: 0.0157 | Grad Norm: 0.00938464\n",
      "Epoch 2 | Step 1436600 | Avg Loss: 0.0160 | Grad Norm: 0.01009680\n",
      "Epoch 2 | Step 1436700 | Avg Loss: 0.0163 | Grad Norm: 0.01072349\n",
      "Epoch 2 | Step 1436800 | Avg Loss: 0.0158 | Grad Norm: 0.00803336\n",
      "Epoch 2 | Step 1436900 | Avg Loss: 0.0155 | Grad Norm: 0.00900269\n",
      "Epoch 2 | Step 1437000 | Avg Loss: 0.0157 | Grad Norm: 0.00877863\n",
      "Epoch 2 | Step 1437100 | Avg Loss: 0.0158 | Grad Norm: 0.01004729\n",
      "Epoch 2 | Step 1437200 | Avg Loss: 0.0156 | Grad Norm: 0.00989710\n",
      "Epoch 2 | Step 1437300 | Avg Loss: 0.0159 | Grad Norm: 0.00899927\n",
      "Epoch 2 | Step 1437400 | Avg Loss: 0.0158 | Grad Norm: 0.00979705\n",
      "Epoch 2 | Step 1437500 | Avg Loss: 0.0155 | Grad Norm: 0.00888214\n",
      "Epoch 2 | Step 1437600 | Avg Loss: 0.0156 | Grad Norm: 0.00937532\n",
      "Epoch 2 | Step 1437700 | Avg Loss: 0.0151 | Grad Norm: 0.00796936\n",
      "Epoch 2 | Step 1437800 | Avg Loss: 0.0151 | Grad Norm: 0.00861333\n",
      "Epoch 2 | Step 1437900 | Avg Loss: 0.0155 | Grad Norm: 0.00887045\n",
      "Epoch 2 | Step 1438000 | Avg Loss: 0.0163 | Grad Norm: 0.00871807\n",
      "Epoch 2 | Step 1438100 | Avg Loss: 0.0162 | Grad Norm: 0.00817515\n",
      "Epoch 2 | Step 1438200 | Avg Loss: 0.0159 | Grad Norm: 0.00958919\n",
      "Epoch 2 | Step 1438300 | Avg Loss: 0.0157 | Grad Norm: 0.00916633\n",
      "Epoch 2 | Step 1438400 | Avg Loss: 0.0157 | Grad Norm: 0.00930227\n",
      "Epoch 2 | Step 1438500 | Avg Loss: 0.0158 | Grad Norm: 0.00940116\n",
      "Epoch 2 | Step 1438600 | Avg Loss: 0.0157 | Grad Norm: 0.00790442\n",
      "Epoch 2 | Step 1438700 | Avg Loss: 0.0156 | Grad Norm: 0.00869832\n",
      "Epoch 2 | Step 1438800 | Avg Loss: 0.0158 | Grad Norm: 0.00886271\n",
      "Epoch 2 | Step 1438900 | Avg Loss: 0.0156 | Grad Norm: 0.00787382\n",
      "Epoch 2 | Step 1439000 | Avg Loss: 0.0159 | Grad Norm: 0.00756096\n",
      "Epoch 2 | Step 1439100 | Avg Loss: 0.0157 | Grad Norm: 0.00864010\n",
      "Epoch 2 | Step 1439200 | Avg Loss: 0.0154 | Grad Norm: 0.00832069\n",
      "Epoch 2 | Step 1439300 | Avg Loss: 0.0155 | Grad Norm: 0.00780126\n",
      "Epoch 2 | Step 1439400 | Avg Loss: 0.0154 | Grad Norm: 0.00807813\n",
      "Epoch 2 | Step 1439500 | Avg Loss: 0.0154 | Grad Norm: 0.00794553\n",
      "Epoch 2 | Step 1439600 | Avg Loss: 0.0149 | Grad Norm: 0.01079864\n",
      "Epoch 2 | Step 1439700 | Avg Loss: 0.0148 | Grad Norm: 0.01006765\n",
      "Epoch 2 | Step 1439800 | Avg Loss: 0.0151 | Grad Norm: 0.00828735\n",
      "Epoch 2 | Step 1439900 | Avg Loss: 0.0149 | Grad Norm: 0.00968188\n",
      "Epoch 2 | Step 1440000 | Avg Loss: 0.0149 | Grad Norm: 0.00901842\n",
      "Epoch 2 | Step 1440100 | Avg Loss: 0.0152 | Grad Norm: 0.00895218\n",
      "Epoch 2 | Step 1440200 | Avg Loss: 0.0153 | Grad Norm: 0.01006004\n",
      "Epoch 2 | Step 1440300 | Avg Loss: 0.0154 | Grad Norm: 0.00998797\n",
      "Epoch 2 | Step 1440400 | Avg Loss: 0.0157 | Grad Norm: 0.00940443\n",
      "Epoch 2 | Step 1440500 | Avg Loss: 0.0160 | Grad Norm: 0.01106076\n",
      "Epoch 2 | Step 1440600 | Avg Loss: 0.0166 | Grad Norm: 0.00962822\n",
      "Epoch 2 | Step 1440700 | Avg Loss: 0.0161 | Grad Norm: 0.00871351\n",
      "Epoch 2 | Step 1440800 | Avg Loss: 0.0157 | Grad Norm: 0.00932858\n",
      "Epoch 2 | Step 1440900 | Avg Loss: 0.0160 | Grad Norm: 0.00876103\n",
      "Epoch 2 | Step 1441000 | Avg Loss: 0.0158 | Grad Norm: 0.00976311\n",
      "Epoch 2 | Step 1441100 | Avg Loss: 0.0161 | Grad Norm: 0.00914386\n",
      "Epoch 2 | Step 1441200 | Avg Loss: 0.0162 | Grad Norm: 0.00842731\n",
      "Epoch 2 | Step 1441300 | Avg Loss: 0.0162 | Grad Norm: 0.00799366\n",
      "Epoch 2 | Step 1441400 | Avg Loss: 0.0163 | Grad Norm: 0.00911749\n",
      "Epoch 2 | Step 1441500 | Avg Loss: 0.0156 | Grad Norm: 0.00889050\n",
      "Epoch 2 | Step 1441600 | Avg Loss: 0.0159 | Grad Norm: 0.00787917\n",
      "Epoch 2 | Step 1441700 | Avg Loss: 0.0159 | Grad Norm: 0.01242187\n",
      "Epoch 2 | Step 1441800 | Avg Loss: 0.0160 | Grad Norm: 0.00934236\n",
      "Epoch 2 | Step 1441900 | Avg Loss: 0.0158 | Grad Norm: 0.00968417\n",
      "Epoch 2 | Step 1442000 | Avg Loss: 0.0158 | Grad Norm: 0.00872344\n",
      "Epoch 2 | Step 1442100 | Avg Loss: 0.0155 | Grad Norm: 0.00766291\n",
      "Epoch 2 | Step 1442200 | Avg Loss: 0.0158 | Grad Norm: 0.01067306\n",
      "Epoch 2 | Step 1442300 | Avg Loss: 0.0160 | Grad Norm: 0.00945523\n",
      "Epoch 2 | Step 1442400 | Avg Loss: 0.0161 | Grad Norm: 0.00989150\n",
      "Epoch 2 | Step 1442500 | Avg Loss: 0.0161 | Grad Norm: 0.01094102\n",
      "Epoch 2 | Step 1442600 | Avg Loss: 0.0158 | Grad Norm: 0.01066461\n",
      "Epoch 2 | Step 1442700 | Avg Loss: 0.0160 | Grad Norm: 0.00837038\n",
      "Epoch 2 | Step 1442800 | Avg Loss: 0.0156 | Grad Norm: 0.00877955\n",
      "Epoch 2 | Step 1442900 | Avg Loss: 0.0154 | Grad Norm: 0.01167118\n",
      "Epoch 2 | Step 1443000 | Avg Loss: 0.0155 | Grad Norm: 0.00846426\n",
      "Epoch 2 | Step 1443100 | Avg Loss: 0.0151 | Grad Norm: 0.00786241\n",
      "Epoch 2 | Step 1443200 | Avg Loss: 0.0150 | Grad Norm: 0.00820529\n",
      "Epoch 2 | Step 1443300 | Avg Loss: 0.0153 | Grad Norm: 0.01053988\n",
      "Epoch 2 | Step 1443400 | Avg Loss: 0.0156 | Grad Norm: 0.00811791\n",
      "Epoch 2 | Step 1443500 | Avg Loss: 0.0154 | Grad Norm: 0.00797623\n",
      "Epoch 2 | Step 1443600 | Avg Loss: 0.0153 | Grad Norm: 0.00847301\n",
      "Epoch 2 | Step 1443700 | Avg Loss: 0.0153 | Grad Norm: 0.00841380\n",
      "Epoch 2 | Step 1443800 | Avg Loss: 0.0153 | Grad Norm: 0.00860645\n",
      "Epoch 2 | Step 1443900 | Avg Loss: 0.0150 | Grad Norm: 0.00923924\n",
      "Epoch 2 | Step 1444000 | Avg Loss: 0.0152 | Grad Norm: 0.00919925\n",
      "Epoch 2 | Step 1444100 | Avg Loss: 0.0152 | Grad Norm: 0.00866002\n",
      "Epoch 2 | Step 1444200 | Avg Loss: 0.0155 | Grad Norm: 0.00836391\n",
      "Epoch 2 | Step 1444300 | Avg Loss: 0.0153 | Grad Norm: 0.00858485\n",
      "Epoch 2 | Step 1444400 | Avg Loss: 0.0154 | Grad Norm: 0.00804892\n",
      "Epoch 2 | Step 1444500 | Avg Loss: 0.0157 | Grad Norm: 0.01112488\n",
      "Epoch 2 | Step 1444600 | Avg Loss: 0.0159 | Grad Norm: 0.01073844\n",
      "Epoch 2 | Step 1444700 | Avg Loss: 0.0160 | Grad Norm: 0.00836084\n",
      "Epoch 2 | Step 1444800 | Avg Loss: 0.0160 | Grad Norm: 0.00857458\n",
      "Epoch 2 | Step 1444900 | Avg Loss: 0.0162 | Grad Norm: 0.01053195\n",
      "Epoch 2 | Step 1445000 | Avg Loss: 0.0164 | Grad Norm: 0.01069347\n",
      "Epoch 2 | Step 1445100 | Avg Loss: 0.0164 | Grad Norm: 0.00921125\n",
      "Epoch 2 | Step 1445200 | Avg Loss: 0.0163 | Grad Norm: 0.00868394\n",
      "Epoch 2 | Step 1445300 | Avg Loss: 0.0163 | Grad Norm: 0.01024953\n",
      "Epoch 2 | Step 1445400 | Avg Loss: 0.0158 | Grad Norm: 0.00936481\n",
      "Epoch 2 | Step 1445500 | Avg Loss: 0.0152 | Grad Norm: 0.00888936\n",
      "Epoch 2 | Step 1445600 | Avg Loss: 0.0154 | Grad Norm: 0.01124075\n",
      "Epoch 2 | Step 1445700 | Avg Loss: 0.0147 | Grad Norm: 0.00825589\n",
      "Epoch 2 | Step 1445800 | Avg Loss: 0.0145 | Grad Norm: 0.01028305\n",
      "Epoch 2 | Step 1445900 | Avg Loss: 0.0147 | Grad Norm: 0.00877568\n",
      "Epoch 2 | Step 1446000 | Avg Loss: 0.0149 | Grad Norm: 0.00771425\n",
      "Epoch 2 | Step 1446100 | Avg Loss: 0.0150 | Grad Norm: 0.00806894\n",
      "Epoch 2 | Step 1446200 | Avg Loss: 0.0148 | Grad Norm: 0.00824742\n",
      "Epoch 2 | Step 1446300 | Avg Loss: 0.0148 | Grad Norm: 0.00922798\n",
      "Epoch 2 | Step 1446400 | Avg Loss: 0.0149 | Grad Norm: 0.00821239\n",
      "Epoch 2 | Step 1446500 | Avg Loss: 0.0154 | Grad Norm: 0.00836841\n",
      "Epoch 2 | Step 1446600 | Avg Loss: 0.0157 | Grad Norm: 0.00937391\n",
      "Epoch 2 | Step 1446700 | Avg Loss: 0.0152 | Grad Norm: 0.00835974\n",
      "Epoch 2 | Step 1446800 | Avg Loss: 0.0156 | Grad Norm: 0.00849570\n",
      "Epoch 2 | Step 1446900 | Avg Loss: 0.0154 | Grad Norm: 0.00923512\n",
      "Epoch 2 | Step 1447000 | Avg Loss: 0.0157 | Grad Norm: 0.00876789\n",
      "Epoch 2 | Step 1447100 | Avg Loss: 0.0158 | Grad Norm: 0.00812980\n",
      "Epoch 2 | Step 1447200 | Avg Loss: 0.0159 | Grad Norm: 0.00814776\n",
      "Epoch 2 | Step 1447300 | Avg Loss: 0.0157 | Grad Norm: 0.00952689\n",
      "Epoch 2 | Step 1447400 | Avg Loss: 0.0155 | Grad Norm: 0.00856014\n",
      "Epoch 2 | Step 1447500 | Avg Loss: 0.0156 | Grad Norm: 0.00946300\n",
      "Epoch 2 | Step 1447600 | Avg Loss: 0.0158 | Grad Norm: 0.00973079\n",
      "Epoch 2 | Step 1447700 | Avg Loss: 0.0160 | Grad Norm: 0.00980542\n",
      "Epoch 2 | Step 1447800 | Avg Loss: 0.0157 | Grad Norm: 0.00724471\n",
      "Epoch 2 | Step 1447900 | Avg Loss: 0.0155 | Grad Norm: 0.00855803\n",
      "Epoch 2 | Step 1448000 | Avg Loss: 0.0158 | Grad Norm: 0.00860043\n",
      "Epoch 2 | Step 1448100 | Avg Loss: 0.0158 | Grad Norm: 0.00935765\n",
      "Epoch 2 | Step 1448200 | Avg Loss: 0.0161 | Grad Norm: 0.00881946\n",
      "Epoch 2 | Step 1448300 | Avg Loss: 0.0159 | Grad Norm: 0.00995436\n",
      "Epoch 2 | Step 1448400 | Avg Loss: 0.0159 | Grad Norm: 0.00909517\n",
      "Epoch 2 | Step 1448500 | Avg Loss: 0.0161 | Grad Norm: 0.00819447\n",
      "Epoch 2 | Step 1448600 | Avg Loss: 0.0158 | Grad Norm: 0.00806639\n",
      "Epoch 2 | Step 1448700 | Avg Loss: 0.0155 | Grad Norm: 0.00916553\n",
      "Epoch 2 | Step 1448800 | Avg Loss: 0.0157 | Grad Norm: 0.00884027\n",
      "Epoch 2 | Step 1448900 | Avg Loss: 0.0155 | Grad Norm: 0.00960290\n",
      "Epoch 2 | Step 1449000 | Avg Loss: 0.0157 | Grad Norm: 0.00883242\n",
      "Epoch 2 | Step 1449100 | Avg Loss: 0.0159 | Grad Norm: 0.00856183\n",
      "Epoch 2 | Step 1449200 | Avg Loss: 0.0153 | Grad Norm: 0.00726165\n",
      "Epoch 2 | Step 1449300 | Avg Loss: 0.0152 | Grad Norm: 0.00918568\n",
      "Epoch 2 | Step 1449400 | Avg Loss: 0.0151 | Grad Norm: 0.00883352\n",
      "Epoch 2 | Step 1449500 | Avg Loss: 0.0150 | Grad Norm: 0.00831598\n",
      "Epoch 2 | Step 1449600 | Avg Loss: 0.0154 | Grad Norm: 0.00811995\n",
      "Epoch 2 | Step 1449700 | Avg Loss: 0.0156 | Grad Norm: 0.01068104\n",
      "Epoch 2 | Step 1449800 | Avg Loss: 0.0157 | Grad Norm: 0.00821384\n",
      "Epoch 2 | Step 1449900 | Avg Loss: 0.0158 | Grad Norm: 0.00968328\n",
      "Epoch 2 | Step 1450000 | Avg Loss: 0.0159 | Grad Norm: 0.01965256\n",
      "Epoch 2 | Step 1450100 | Avg Loss: 0.0158 | Grad Norm: 0.00875766\n",
      "Epoch 2 | Step 1450200 | Avg Loss: 0.0159 | Grad Norm: 0.00925468\n",
      "Epoch 2 | Step 1450300 | Avg Loss: 0.0159 | Grad Norm: 0.01094591\n",
      "Epoch 2 | Step 1450400 | Avg Loss: 0.0156 | Grad Norm: 0.00759717\n",
      "Epoch 2 | Step 1450500 | Avg Loss: 0.0158 | Grad Norm: 0.00768999\n",
      "Epoch 2 | Step 1450600 | Avg Loss: 0.0159 | Grad Norm: 0.00840918\n",
      "Epoch 2 | Step 1450700 | Avg Loss: 0.0157 | Grad Norm: 0.01137130\n",
      "Epoch 2 | Step 1450800 | Avg Loss: 0.0158 | Grad Norm: 0.01084419\n",
      "Epoch 2 | Step 1450900 | Avg Loss: 0.0161 | Grad Norm: 0.01076080\n",
      "Epoch 2 | Step 1451000 | Avg Loss: 0.0159 | Grad Norm: 0.01482683\n",
      "Epoch 2 | Step 1451100 | Avg Loss: 0.0160 | Grad Norm: 0.00951220\n",
      "Epoch 2 | Step 1451200 | Avg Loss: 0.0159 | Grad Norm: 0.01026385\n",
      "Epoch 2 | Step 1451300 | Avg Loss: 0.0159 | Grad Norm: 0.01093548\n",
      "Epoch 2 | Step 1451400 | Avg Loss: 0.0157 | Grad Norm: 0.00863054\n",
      "Epoch 2 | Step 1451500 | Avg Loss: 0.0159 | Grad Norm: 0.00893285\n",
      "Epoch 2 | Step 1451600 | Avg Loss: 0.0159 | Grad Norm: 0.00927544\n",
      "Epoch 2 | Step 1451700 | Avg Loss: 0.0160 | Grad Norm: 0.00748428\n",
      "Epoch 2 | Step 1451800 | Avg Loss: 0.0159 | Grad Norm: 0.00817148\n",
      "Epoch 2 | Step 1451900 | Avg Loss: 0.0157 | Grad Norm: 0.00884907\n",
      "Epoch 2 | Step 1452000 | Avg Loss: 0.0159 | Grad Norm: 0.01088393\n",
      "Epoch 2 | Step 1452100 | Avg Loss: 0.0157 | Grad Norm: 0.00901777\n",
      "Epoch 2 | Step 1452200 | Avg Loss: 0.0153 | Grad Norm: 0.01063322\n",
      "Epoch 2 | Step 1452300 | Avg Loss: 0.0154 | Grad Norm: 0.00817440\n",
      "Epoch 2 | Step 1452400 | Avg Loss: 0.0153 | Grad Norm: 0.00843175\n",
      "Epoch 2 | Step 1452500 | Avg Loss: 0.0151 | Grad Norm: 0.00792655\n",
      "Epoch 2 | Step 1452600 | Avg Loss: 0.0149 | Grad Norm: 0.00742744\n",
      "Epoch 2 | Step 1452700 | Avg Loss: 0.0151 | Grad Norm: 0.00810477\n",
      "Epoch 2 | Step 1452800 | Avg Loss: 0.0152 | Grad Norm: 0.00867325\n",
      "Epoch 2 | Step 1452900 | Avg Loss: 0.0155 | Grad Norm: 0.00880232\n",
      "Epoch 2 | Step 1453000 | Avg Loss: 0.0154 | Grad Norm: 0.00949635\n",
      "Epoch 2 | Step 1453100 | Avg Loss: 0.0155 | Grad Norm: 0.00909155\n",
      "Epoch 2 | Step 1453200 | Avg Loss: 0.0159 | Grad Norm: 0.00903937\n",
      "Epoch 2 | Step 1453300 | Avg Loss: 0.0154 | Grad Norm: 0.01011230\n",
      "Epoch 2 | Step 1453400 | Avg Loss: 0.0157 | Grad Norm: 0.00799633\n",
      "Epoch 2 | Step 1453500 | Avg Loss: 0.0162 | Grad Norm: 0.00964199\n",
      "Epoch 2 | Step 1453600 | Avg Loss: 0.0160 | Grad Norm: 0.00892749\n",
      "Epoch 2 | Step 1453700 | Avg Loss: 0.0158 | Grad Norm: 0.00865324\n",
      "Epoch 2 | Step 1453800 | Avg Loss: 0.0156 | Grad Norm: 0.00727809\n",
      "Epoch 2 | Step 1453900 | Avg Loss: 0.0160 | Grad Norm: 0.00993030\n",
      "Epoch 2 | Step 1454000 | Avg Loss: 0.0157 | Grad Norm: 0.00912814\n",
      "Epoch 2 | Step 1454100 | Avg Loss: 0.0161 | Grad Norm: 0.00880662\n",
      "Epoch 2 | Step 1454200 | Avg Loss: 0.0162 | Grad Norm: 0.00817913\n",
      "Epoch 2 | Step 1454300 | Avg Loss: 0.0157 | Grad Norm: 0.01017135\n",
      "Epoch 2 | Step 1454400 | Avg Loss: 0.0156 | Grad Norm: 0.01097149\n",
      "Epoch 2 | Step 1454500 | Avg Loss: 0.0159 | Grad Norm: 0.00833591\n",
      "Epoch 2 | Step 1454600 | Avg Loss: 0.0154 | Grad Norm: 0.00770672\n",
      "Epoch 2 | Step 1454700 | Avg Loss: 0.0155 | Grad Norm: 0.00840953\n",
      "Epoch 2 | Step 1454800 | Avg Loss: 0.0156 | Grad Norm: 0.00917557\n",
      "Epoch 2 | Step 1454900 | Avg Loss: 0.0155 | Grad Norm: 0.00946062\n",
      "Epoch 2 | Step 1455000 | Avg Loss: 0.0157 | Grad Norm: 0.00920496\n",
      "Epoch 2 | Step 1455100 | Avg Loss: 0.0157 | Grad Norm: 0.00878642\n",
      "Epoch 2 | Step 1455200 | Avg Loss: 0.0154 | Grad Norm: 0.00851177\n",
      "Epoch 2 | Step 1455300 | Avg Loss: 0.0157 | Grad Norm: 0.00857804\n",
      "Epoch 2 | Step 1455400 | Avg Loss: 0.0155 | Grad Norm: 0.00861789\n",
      "Epoch 2 | Step 1455500 | Avg Loss: 0.0157 | Grad Norm: 0.00841639\n",
      "Epoch 2 | Step 1455600 | Avg Loss: 0.0160 | Grad Norm: 0.00899369\n",
      "Epoch 2 | Step 1455700 | Avg Loss: 0.0155 | Grad Norm: 0.01009413\n",
      "Epoch 2 | Step 1455800 | Avg Loss: 0.0152 | Grad Norm: 0.00915100\n",
      "Epoch 2 | Step 1455900 | Avg Loss: 0.0154 | Grad Norm: 0.00870753\n",
      "Epoch 2 | Step 1456000 | Avg Loss: 0.0155 | Grad Norm: 0.00837946\n",
      "Epoch 2 | Step 1456100 | Avg Loss: 0.0156 | Grad Norm: 0.00833668\n",
      "Epoch 2 | Step 1456200 | Avg Loss: 0.0156 | Grad Norm: 0.00806584\n",
      "Epoch 2 | Step 1456300 | Avg Loss: 0.0151 | Grad Norm: 0.00935589\n",
      "Epoch 2 | Step 1456400 | Avg Loss: 0.0148 | Grad Norm: 0.00856998\n",
      "Epoch 2 | Step 1456500 | Avg Loss: 0.0149 | Grad Norm: 0.00801409\n",
      "Epoch 2 | Step 1456600 | Avg Loss: 0.0149 | Grad Norm: 0.00904709\n",
      "Epoch 2 | Step 1456700 | Avg Loss: 0.0149 | Grad Norm: 0.00781071\n",
      "Epoch 2 | Step 1456800 | Avg Loss: 0.0148 | Grad Norm: 0.00804271\n",
      "Epoch 2 | Step 1456900 | Avg Loss: 0.0150 | Grad Norm: 0.00804382\n",
      "Epoch 2 | Step 1457000 | Avg Loss: 0.0152 | Grad Norm: 0.00865411\n",
      "Epoch 2 | Step 1457100 | Avg Loss: 0.0151 | Grad Norm: 0.00874024\n",
      "Epoch 2 | Step 1457200 | Avg Loss: 0.0152 | Grad Norm: 0.00890017\n",
      "Epoch 2 | Step 1457300 | Avg Loss: 0.0152 | Grad Norm: 0.00819330\n",
      "Epoch 2 | Step 1457400 | Avg Loss: 0.0153 | Grad Norm: 0.01111303\n",
      "Epoch 2 | Step 1457500 | Avg Loss: 0.0157 | Grad Norm: 0.00909958\n",
      "Epoch 2 | Step 1457600 | Avg Loss: 0.0153 | Grad Norm: 0.00956413\n",
      "Epoch 2 | Step 1457700 | Avg Loss: 0.0150 | Grad Norm: 0.00832815\n",
      "Epoch 2 | Step 1457800 | Avg Loss: 0.0148 | Grad Norm: 0.00815363\n",
      "Epoch 2 | Step 1457900 | Avg Loss: 0.0152 | Grad Norm: 0.00908926\n",
      "Epoch 2 | Step 1458000 | Avg Loss: 0.0149 | Grad Norm: 0.01026466\n",
      "Epoch 2 | Step 1458100 | Avg Loss: 0.0148 | Grad Norm: 0.00769371\n",
      "Epoch 2 | Step 1458200 | Avg Loss: 0.0153 | Grad Norm: 0.00850514\n",
      "Epoch 2 | Step 1458300 | Avg Loss: 0.0154 | Grad Norm: 0.00899967\n",
      "Epoch 2 | Step 1458400 | Avg Loss: 0.0158 | Grad Norm: 0.00785207\n",
      "Epoch 2 | Step 1458500 | Avg Loss: 0.0158 | Grad Norm: 0.01218369\n",
      "Epoch 2 | Step 1458600 | Avg Loss: 0.0161 | Grad Norm: 0.00960726\n",
      "Epoch 2 | Step 1458700 | Avg Loss: 0.0159 | Grad Norm: 0.00896843\n",
      "Epoch 2 | Step 1458800 | Avg Loss: 0.0157 | Grad Norm: 0.00899425\n",
      "Epoch 2 | Step 1458900 | Avg Loss: 0.0157 | Grad Norm: 0.00914448\n",
      "Epoch 2 | Step 1459000 | Avg Loss: 0.0154 | Grad Norm: 0.00780680\n",
      "Epoch 2 | Step 1459100 | Avg Loss: 0.0152 | Grad Norm: 0.00767556\n",
      "Epoch 2 | Step 1459200 | Avg Loss: 0.0155 | Grad Norm: 0.00881694\n",
      "Epoch 2 | Step 1459300 | Avg Loss: 0.0154 | Grad Norm: 0.00894406\n",
      "Epoch 2 | Step 1459400 | Avg Loss: 0.0155 | Grad Norm: 0.00806650\n",
      "Epoch 2 | Step 1459500 | Avg Loss: 0.0154 | Grad Norm: 0.00843784\n",
      "Epoch 2 | Step 1459600 | Avg Loss: 0.0151 | Grad Norm: 0.00931100\n",
      "Epoch 2 | Step 1459700 | Avg Loss: 0.0153 | Grad Norm: 0.00805302\n",
      "Epoch 2 | Step 1459800 | Avg Loss: 0.0158 | Grad Norm: 0.01086278\n",
      "Epoch 2 | Step 1459900 | Avg Loss: 0.0161 | Grad Norm: 0.00957553\n",
      "Epoch 2 | Step 1460000 | Avg Loss: 0.0160 | Grad Norm: 0.00828232\n",
      "Epoch 2 | Step 1460100 | Avg Loss: 0.0163 | Grad Norm: 0.00958533\n",
      "Epoch 2 | Step 1460200 | Avg Loss: 0.0161 | Grad Norm: 0.00971727\n",
      "Epoch 2 | Step 1460300 | Avg Loss: 0.0160 | Grad Norm: 0.00922037\n",
      "Epoch 2 | Step 1460400 | Avg Loss: 0.0159 | Grad Norm: 0.00871591\n",
      "Epoch 2 | Step 1460500 | Avg Loss: 0.0165 | Grad Norm: 0.00810430\n",
      "Epoch 2 | Step 1460600 | Avg Loss: 0.0162 | Grad Norm: 0.00784338\n",
      "Epoch 2 | Step 1460700 | Avg Loss: 0.0159 | Grad Norm: 0.00763199\n",
      "Epoch 2 | Step 1460800 | Avg Loss: 0.0162 | Grad Norm: 0.00872519\n",
      "Epoch 2 | Step 1460900 | Avg Loss: 0.0159 | Grad Norm: 0.00842265\n",
      "Epoch 2 | Step 1461000 | Avg Loss: 0.0158 | Grad Norm: 0.00906330\n",
      "Epoch 2 | Step 1461100 | Avg Loss: 0.0161 | Grad Norm: 0.00918848\n",
      "Epoch 2 | Step 1461200 | Avg Loss: 0.0159 | Grad Norm: 0.00991137\n",
      "Epoch 2 | Step 1461300 | Avg Loss: 0.0162 | Grad Norm: 0.00969814\n",
      "Epoch 2 | Step 1461400 | Avg Loss: 0.0162 | Grad Norm: 0.00947930\n",
      "Epoch 2 | Step 1461500 | Avg Loss: 0.0165 | Grad Norm: 0.01075226\n",
      "Epoch 2 | Step 1461600 | Avg Loss: 0.0163 | Grad Norm: 0.00852356\n",
      "Epoch 2 | Step 1461700 | Avg Loss: 0.0164 | Grad Norm: 0.00863035\n",
      "Epoch 2 | Step 1461800 | Avg Loss: 0.0161 | Grad Norm: 0.00839737\n",
      "Epoch 2 | Step 1461900 | Avg Loss: 0.0161 | Grad Norm: 0.00887053\n",
      "Epoch 2 | Step 1462000 | Avg Loss: 0.0161 | Grad Norm: 0.00847376\n",
      "Epoch 2 | Step 1462100 | Avg Loss: 0.0158 | Grad Norm: 0.00850989\n",
      "Epoch 2 | Step 1462200 | Avg Loss: 0.0158 | Grad Norm: 0.00955853\n",
      "Epoch 2 | Step 1462300 | Avg Loss: 0.0155 | Grad Norm: 0.00838402\n",
      "Epoch 2 | Step 1462400 | Avg Loss: 0.0157 | Grad Norm: 0.01007478\n",
      "Epoch 2 | Step 1462500 | Avg Loss: 0.0157 | Grad Norm: 0.01098750\n",
      "Epoch 2 | Step 1462600 | Avg Loss: 0.0159 | Grad Norm: 0.00803339\n",
      "Epoch 2 | Step 1462700 | Avg Loss: 0.0158 | Grad Norm: 0.00930632\n",
      "Epoch 2 | Step 1462800 | Avg Loss: 0.0154 | Grad Norm: 0.00919756\n",
      "Epoch 2 | Step 1462900 | Avg Loss: 0.0157 | Grad Norm: 0.00832862\n",
      "Epoch 2 | Step 1463000 | Avg Loss: 0.0155 | Grad Norm: 0.00928910\n",
      "Epoch 2 | Step 1463100 | Avg Loss: 0.0155 | Grad Norm: 0.01008139\n",
      "Epoch 2 | Step 1463200 | Avg Loss: 0.0153 | Grad Norm: 0.00806713\n",
      "Epoch 2 | Step 1463300 | Avg Loss: 0.0156 | Grad Norm: 0.00947423\n",
      "Epoch 2 | Step 1463400 | Avg Loss: 0.0157 | Grad Norm: 0.00959975\n",
      "Epoch 2 | Step 1463500 | Avg Loss: 0.0157 | Grad Norm: 0.00867147\n",
      "Epoch 2 | Step 1463600 | Avg Loss: 0.0154 | Grad Norm: 0.01052352\n",
      "Epoch 2 | Step 1463700 | Avg Loss: 0.0149 | Grad Norm: 0.00853207\n",
      "Epoch 2 | Step 1463800 | Avg Loss: 0.0151 | Grad Norm: 0.00830196\n",
      "Epoch 2 | Step 1463900 | Avg Loss: 0.0155 | Grad Norm: 0.00871640\n",
      "Epoch 2 | Step 1464000 | Avg Loss: 0.0154 | Grad Norm: 0.00851467\n",
      "Epoch 2 | Step 1464100 | Avg Loss: 0.0153 | Grad Norm: 0.00899524\n",
      "Epoch 2 | Step 1464200 | Avg Loss: 0.0157 | Grad Norm: 0.00929915\n",
      "Epoch 2 | Step 1464300 | Avg Loss: 0.0160 | Grad Norm: 0.00878341\n",
      "Epoch 2 | Step 1464400 | Avg Loss: 0.0156 | Grad Norm: 0.00836944\n",
      "Epoch 2 | Step 1464500 | Avg Loss: 0.0155 | Grad Norm: 0.00847237\n",
      "Epoch 2 | Step 1464600 | Avg Loss: 0.0152 | Grad Norm: 0.00934150\n",
      "Epoch 2 | Step 1464700 | Avg Loss: 0.0149 | Grad Norm: 0.00812240\n",
      "Epoch 2 | Step 1464800 | Avg Loss: 0.0148 | Grad Norm: 0.00894938\n",
      "Epoch 2 | Step 1464900 | Avg Loss: 0.0149 | Grad Norm: 0.00927868\n",
      "Epoch 2 | Step 1465000 | Avg Loss: 0.0150 | Grad Norm: 0.00931046\n",
      "Epoch 2 | Step 1465100 | Avg Loss: 0.0151 | Grad Norm: 0.00893081\n",
      "Epoch 2 | Step 1465200 | Avg Loss: 0.0154 | Grad Norm: 0.00887612\n",
      "Epoch 2 | Step 1465300 | Avg Loss: 0.0152 | Grad Norm: 0.00818844\n",
      "Epoch 2 | Step 1465400 | Avg Loss: 0.0151 | Grad Norm: 0.00844308\n",
      "Epoch 2 | Step 1465500 | Avg Loss: 0.0153 | Grad Norm: 0.00984852\n",
      "Epoch 2 | Step 1465600 | Avg Loss: 0.0156 | Grad Norm: 0.00857390\n",
      "Epoch 2 | Step 1465700 | Avg Loss: 0.0158 | Grad Norm: 0.00813548\n",
      "Epoch 2 | Step 1465800 | Avg Loss: 0.0159 | Grad Norm: 0.00820273\n",
      "Epoch 2 | Step 1465900 | Avg Loss: 0.0157 | Grad Norm: 0.00838598\n",
      "Epoch 2 | Step 1466000 | Avg Loss: 0.0160 | Grad Norm: 0.00871861\n",
      "Epoch 2 | Step 1466100 | Avg Loss: 0.0161 | Grad Norm: 0.00895281\n",
      "Epoch 2 | Step 1466200 | Avg Loss: 0.0160 | Grad Norm: 0.00920697\n",
      "Epoch 2 | Step 1466300 | Avg Loss: 0.0155 | Grad Norm: 0.00785834\n",
      "Epoch 2 | Step 1466400 | Avg Loss: 0.0159 | Grad Norm: 0.00836096\n",
      "Epoch 2 | Step 1466500 | Avg Loss: 0.0160 | Grad Norm: 0.00903712\n",
      "Epoch 2 | Step 1466600 | Avg Loss: 0.0157 | Grad Norm: 0.00846226\n",
      "Epoch 2 | Step 1466700 | Avg Loss: 0.0160 | Grad Norm: 0.00894053\n",
      "Epoch 2 | Step 1466800 | Avg Loss: 0.0160 | Grad Norm: 0.00844231\n",
      "Epoch 2 | Step 1466900 | Avg Loss: 0.0159 | Grad Norm: 0.00859537\n",
      "Epoch 2 | Step 1467000 | Avg Loss: 0.0158 | Grad Norm: 0.00900914\n",
      "Epoch 2 | Step 1467100 | Avg Loss: 0.0156 | Grad Norm: 0.00957713\n",
      "Epoch 2 | Step 1467200 | Avg Loss: 0.0153 | Grad Norm: 0.01033748\n",
      "Epoch 2 | Step 1467300 | Avg Loss: 0.0152 | Grad Norm: 0.00977540\n",
      "Epoch 2 | Step 1467400 | Avg Loss: 0.0154 | Grad Norm: 0.00893786\n",
      "Epoch 2 | Step 1467500 | Avg Loss: 0.0152 | Grad Norm: 0.00882231\n",
      "Epoch 2 | Step 1467600 | Avg Loss: 0.0154 | Grad Norm: 0.00989949\n",
      "Epoch 2 | Step 1467700 | Avg Loss: 0.0158 | Grad Norm: 0.01056016\n",
      "Epoch 2 | Step 1467800 | Avg Loss: 0.0158 | Grad Norm: 0.00745539\n",
      "Epoch 2 | Step 1467900 | Avg Loss: 0.0157 | Grad Norm: 0.00944757\n",
      "Epoch 2 | Step 1468000 | Avg Loss: 0.0160 | Grad Norm: 0.00943720\n",
      "Epoch 2 | Step 1468100 | Avg Loss: 0.0160 | Grad Norm: 0.00892659\n",
      "Epoch 2 | Step 1468200 | Avg Loss: 0.0163 | Grad Norm: 0.01172232\n",
      "Epoch 2 | Step 1468300 | Avg Loss: 0.0161 | Grad Norm: 0.00904139\n",
      "Epoch 2 | Step 1468400 | Avg Loss: 0.0160 | Grad Norm: 0.00979723\n",
      "Epoch 2 | Step 1468500 | Avg Loss: 0.0159 | Grad Norm: 0.01208261\n",
      "Epoch 2 | Step 1468600 | Avg Loss: 0.0159 | Grad Norm: 0.00880102\n",
      "Epoch 2 | Step 1468700 | Avg Loss: 0.0160 | Grad Norm: 0.00851455\n",
      "Epoch 2 | Step 1468800 | Avg Loss: 0.0162 | Grad Norm: 0.00904382\n",
      "Epoch 2 | Step 1468900 | Avg Loss: 0.0162 | Grad Norm: 0.00834670\n",
      "Epoch 2 | Step 1469000 | Avg Loss: 0.0161 | Grad Norm: 0.00920197\n",
      "Epoch 2 | Step 1469100 | Avg Loss: 0.0165 | Grad Norm: 0.00848872\n",
      "Epoch 2 | Step 1469200 | Avg Loss: 0.0161 | Grad Norm: 0.01009860\n",
      "Epoch 2 | Step 1469300 | Avg Loss: 0.0157 | Grad Norm: 0.00896823\n",
      "Epoch 2 | Step 1469400 | Avg Loss: 0.0158 | Grad Norm: 0.00986304\n",
      "Epoch 2 | Step 1469500 | Avg Loss: 0.0161 | Grad Norm: 0.01001362\n",
      "Epoch 2 | Step 1469600 | Avg Loss: 0.0161 | Grad Norm: 0.00946603\n",
      "Epoch 2 | Step 1469700 | Avg Loss: 0.0158 | Grad Norm: 0.00920408\n",
      "Epoch 2 | Step 1469800 | Avg Loss: 0.0161 | Grad Norm: 0.00977671\n",
      "Epoch 2 | Step 1469900 | Avg Loss: 0.0157 | Grad Norm: 0.00803479\n",
      "Epoch 2 | Step 1470000 | Avg Loss: 0.0158 | Grad Norm: 0.01149049\n",
      "Epoch 2 | Step 1470100 | Avg Loss: 0.0156 | Grad Norm: 0.00814044\n",
      "Epoch 2 | Step 1470200 | Avg Loss: 0.0156 | Grad Norm: 0.00891515\n",
      "Epoch 2 | Step 1470300 | Avg Loss: 0.0151 | Grad Norm: 0.00949568\n",
      "Epoch 2 | Step 1470400 | Avg Loss: 0.0153 | Grad Norm: 0.00926874\n",
      "Epoch 2 | Step 1470500 | Avg Loss: 0.0154 | Grad Norm: 0.00965595\n",
      "Epoch 2 | Step 1470600 | Avg Loss: 0.0158 | Grad Norm: 0.00866396\n",
      "Epoch 2 | Step 1470700 | Avg Loss: 0.0156 | Grad Norm: 0.01208868\n",
      "Epoch 2 | Step 1470800 | Avg Loss: 0.0155 | Grad Norm: 0.00957966\n",
      "Epoch 2 | Step 1470900 | Avg Loss: 0.0156 | Grad Norm: 0.00870778\n",
      "Epoch 2 | Step 1471000 | Avg Loss: 0.0155 | Grad Norm: 0.00802505\n",
      "Epoch 2 | Step 1471100 | Avg Loss: 0.0151 | Grad Norm: 0.00883837\n",
      "Epoch 2 | Step 1471200 | Avg Loss: 0.0151 | Grad Norm: 0.00912736\n",
      "Epoch 2 | Step 1471300 | Avg Loss: 0.0149 | Grad Norm: 0.00732086\n",
      "Epoch 2 | Step 1471400 | Avg Loss: 0.0149 | Grad Norm: 0.01087015\n",
      "Epoch 2 | Step 1471500 | Avg Loss: 0.0149 | Grad Norm: 0.00949141\n",
      "Epoch 2 | Step 1471600 | Avg Loss: 0.0152 | Grad Norm: 0.00801403\n",
      "Epoch 2 | Step 1471700 | Avg Loss: 0.0156 | Grad Norm: 0.01059972\n",
      "Epoch 2 | Step 1471800 | Avg Loss: 0.0154 | Grad Norm: 0.00833679\n",
      "Epoch 2 | Step 1471900 | Avg Loss: 0.0158 | Grad Norm: 0.00873465\n",
      "Epoch 2 | Step 1472000 | Avg Loss: 0.0155 | Grad Norm: 0.00789961\n",
      "Epoch 2 | Step 1472100 | Avg Loss: 0.0155 | Grad Norm: 0.00804148\n",
      "Epoch 2 | Step 1472200 | Avg Loss: 0.0157 | Grad Norm: 0.00866270\n",
      "Epoch 2 | Step 1472300 | Avg Loss: 0.0154 | Grad Norm: 0.00871618\n",
      "Epoch 2 | Step 1472400 | Avg Loss: 0.0151 | Grad Norm: 0.00851675\n",
      "Epoch 2 | Step 1472500 | Avg Loss: 0.0154 | Grad Norm: 0.00813235\n",
      "Epoch 2 | Step 1472600 | Avg Loss: 0.0156 | Grad Norm: 0.00807992\n",
      "Epoch 2 | Step 1472700 | Avg Loss: 0.0152 | Grad Norm: 0.00772601\n",
      "Epoch 2 | Step 1472800 | Avg Loss: 0.0155 | Grad Norm: 0.00864335\n",
      "Epoch 2 | Step 1472900 | Avg Loss: 0.0153 | Grad Norm: 0.00773438\n",
      "Epoch 2 | Step 1473000 | Avg Loss: 0.0156 | Grad Norm: 0.00936587\n",
      "Epoch 2 | Step 1473100 | Avg Loss: 0.0156 | Grad Norm: 0.00895057\n",
      "Epoch 2 | Step 1473200 | Avg Loss: 0.0153 | Grad Norm: 0.00859676\n",
      "Epoch 2 | Step 1473300 | Avg Loss: 0.0154 | Grad Norm: 0.01025621\n",
      "Epoch 2 | Step 1473400 | Avg Loss: 0.0157 | Grad Norm: 0.00888840\n",
      "Epoch 2 | Step 1473500 | Avg Loss: 0.0157 | Grad Norm: 0.00867313\n",
      "Epoch 2 | Step 1473600 | Avg Loss: 0.0160 | Grad Norm: 0.00922906\n",
      "Epoch 2 | Step 1473700 | Avg Loss: 0.0159 | Grad Norm: 0.00856432\n",
      "Epoch 2 | Step 1473800 | Avg Loss: 0.0159 | Grad Norm: 0.00857175\n",
      "Epoch 2 | Step 1473900 | Avg Loss: 0.0156 | Grad Norm: 0.00949546\n",
      "Epoch 2 | Step 1474000 | Avg Loss: 0.0158 | Grad Norm: 0.00821549\n",
      "Epoch 2 | Step 1474100 | Avg Loss: 0.0162 | Grad Norm: 0.00815590\n",
      "Epoch 2 | Step 1474200 | Avg Loss: 0.0160 | Grad Norm: 0.00911302\n",
      "Epoch 2 | Step 1474300 | Avg Loss: 0.0160 | Grad Norm: 0.01177686\n",
      "Epoch 2 | Step 1474400 | Avg Loss: 0.0160 | Grad Norm: 0.00827111\n",
      "Epoch 2 | Step 1474500 | Avg Loss: 0.0161 | Grad Norm: 0.01013102\n",
      "Epoch 2 | Step 1474600 | Avg Loss: 0.0159 | Grad Norm: 0.00855395\n",
      "Epoch 2 | Step 1474700 | Avg Loss: 0.0158 | Grad Norm: 0.00796693\n",
      "Epoch 2 | Step 1474800 | Avg Loss: 0.0155 | Grad Norm: 0.00938071\n",
      "Epoch 2 | Step 1474900 | Avg Loss: 0.0156 | Grad Norm: 0.00992448\n",
      "Epoch 2 | Step 1475000 | Avg Loss: 0.0158 | Grad Norm: 0.00843677\n",
      "Epoch 2 | Step 1475100 | Avg Loss: 0.0160 | Grad Norm: 0.00857185\n",
      "Epoch 2 | Step 1475200 | Avg Loss: 0.0158 | Grad Norm: 0.00844583\n",
      "Epoch 2 | Step 1475300 | Avg Loss: 0.0160 | Grad Norm: 0.00953495\n",
      "Epoch 2 | Step 1475400 | Avg Loss: 0.0160 | Grad Norm: 0.00799858\n",
      "Epoch 2 | Step 1475500 | Avg Loss: 0.0165 | Grad Norm: 0.00880063\n",
      "Epoch 2 | Step 1475600 | Avg Loss: 0.0164 | Grad Norm: 0.00859237\n",
      "Epoch 2 | Step 1475700 | Avg Loss: 0.0161 | Grad Norm: 0.00972162\n",
      "Epoch 2 | Step 1475800 | Avg Loss: 0.0162 | Grad Norm: 0.00911728\n",
      "Epoch 2 | Step 1475900 | Avg Loss: 0.0162 | Grad Norm: 0.00837389\n",
      "Epoch 2 | Step 1476000 | Avg Loss: 0.0160 | Grad Norm: 0.00847578\n",
      "Epoch 2 | Step 1476100 | Avg Loss: 0.0161 | Grad Norm: 0.00834481\n",
      "Epoch 2 | Step 1476200 | Avg Loss: 0.0157 | Grad Norm: 0.00924258\n",
      "Epoch 2 | Step 1476300 | Avg Loss: 0.0160 | Grad Norm: 0.00790873\n",
      "Epoch 2 | Step 1476400 | Avg Loss: 0.0159 | Grad Norm: 0.00819347\n",
      "Epoch 2 | Step 1476500 | Avg Loss: 0.0160 | Grad Norm: 0.00885173\n",
      "Epoch 2 | Step 1476600 | Avg Loss: 0.0155 | Grad Norm: 0.01066677\n",
      "Epoch 2 | Step 1476700 | Avg Loss: 0.0156 | Grad Norm: 0.01018483\n",
      "Epoch 2 | Step 1476800 | Avg Loss: 0.0156 | Grad Norm: 0.00991341\n",
      "Epoch 2 | Step 1476900 | Avg Loss: 0.0159 | Grad Norm: 0.00902480\n",
      "Epoch 2 | Step 1477000 | Avg Loss: 0.0158 | Grad Norm: 0.01090132\n",
      "Epoch 2 | Step 1477100 | Avg Loss: 0.0155 | Grad Norm: 0.00879505\n",
      "Epoch 2 | Step 1477200 | Avg Loss: 0.0158 | Grad Norm: 0.00982114\n",
      "Epoch 2 | Step 1477300 | Avg Loss: 0.0158 | Grad Norm: 0.00885628\n",
      "Epoch 2 | Step 1477400 | Avg Loss: 0.0155 | Grad Norm: 0.00772038\n",
      "Epoch 2 | Step 1477500 | Avg Loss: 0.0158 | Grad Norm: 0.00717619\n",
      "Epoch 2 | Step 1477600 | Avg Loss: 0.0157 | Grad Norm: 0.01020355\n",
      "Epoch 2 | Step 1477700 | Avg Loss: 0.0156 | Grad Norm: 0.00830320\n",
      "Epoch 2 | Step 1477800 | Avg Loss: 0.0154 | Grad Norm: 0.00873199\n",
      "Epoch 2 | Step 1477900 | Avg Loss: 0.0158 | Grad Norm: 0.00775016\n",
      "Epoch 2 | Step 1478000 | Avg Loss: 0.0154 | Grad Norm: 0.00999867\n",
      "Epoch 2 | Step 1478100 | Avg Loss: 0.0157 | Grad Norm: 0.00731114\n",
      "Epoch 2 | Step 1478200 | Avg Loss: 0.0156 | Grad Norm: 0.00867582\n",
      "Epoch 2 | Step 1478300 | Avg Loss: 0.0156 | Grad Norm: 0.00980191\n",
      "Epoch 2 | Step 1478400 | Avg Loss: 0.0157 | Grad Norm: 0.00940020\n",
      "Epoch 2 | Step 1478500 | Avg Loss: 0.0160 | Grad Norm: 0.00889141\n",
      "Epoch 2 | Step 1478600 | Avg Loss: 0.0161 | Grad Norm: 0.00908289\n",
      "Epoch 2 | Step 1478700 | Avg Loss: 0.0162 | Grad Norm: 0.00949311\n",
      "Epoch 2 | Step 1478800 | Avg Loss: 0.0162 | Grad Norm: 0.00919580\n",
      "Epoch 2 | Step 1478900 | Avg Loss: 0.0161 | Grad Norm: 0.00909571\n",
      "Epoch 2 | Step 1479000 | Avg Loss: 0.0163 | Grad Norm: 0.01061403\n",
      "Epoch 2 | Step 1479100 | Avg Loss: 0.0163 | Grad Norm: 0.00953950\n",
      "Epoch 2 | Step 1479200 | Avg Loss: 0.0161 | Grad Norm: 0.00774888\n",
      "Epoch 2 | Step 1479300 | Avg Loss: 0.0157 | Grad Norm: 0.00898242\n",
      "Epoch 2 | Step 1479400 | Avg Loss: 0.0161 | Grad Norm: 0.01051402\n",
      "Epoch 2 | Step 1479500 | Avg Loss: 0.0163 | Grad Norm: 0.00950819\n",
      "Epoch 2 | Step 1479600 | Avg Loss: 0.0161 | Grad Norm: 0.00886850\n",
      "Epoch 2 | Step 1479700 | Avg Loss: 0.0164 | Grad Norm: 0.00932915\n",
      "Epoch 2 | Step 1479800 | Avg Loss: 0.0160 | Grad Norm: 0.01061844\n",
      "Epoch 2 | Step 1479900 | Avg Loss: 0.0161 | Grad Norm: 0.00943213\n",
      "Epoch 2 | Step 1480000 | Avg Loss: 0.0163 | Grad Norm: 0.00988381\n",
      "Epoch 2 | Step 1480100 | Avg Loss: 0.0158 | Grad Norm: 0.00894218\n",
      "Epoch 2 | Step 1480200 | Avg Loss: 0.0153 | Grad Norm: 0.01093227\n",
      "Epoch 2 | Step 1480300 | Avg Loss: 0.0153 | Grad Norm: 0.00770820\n",
      "Epoch 2 | Step 1480400 | Avg Loss: 0.0155 | Grad Norm: 0.00839844\n",
      "Epoch 2 | Step 1480500 | Avg Loss: 0.0159 | Grad Norm: 0.00940411\n",
      "Epoch 2 | Step 1480600 | Avg Loss: 0.0161 | Grad Norm: 0.00936335\n",
      "Epoch 2 | Step 1480700 | Avg Loss: 0.0157 | Grad Norm: 0.00869281\n",
      "Epoch 2 | Step 1480800 | Avg Loss: 0.0158 | Grad Norm: 0.00858284\n",
      "Epoch 2 | Step 1480900 | Avg Loss: 0.0157 | Grad Norm: 0.00816002\n",
      "Epoch 2 | Step 1481000 | Avg Loss: 0.0156 | Grad Norm: 0.00938883\n",
      "Epoch 2 | Step 1481100 | Avg Loss: 0.0157 | Grad Norm: 0.00839656\n",
      "Epoch 2 | Step 1481200 | Avg Loss: 0.0155 | Grad Norm: 0.00873283\n",
      "Epoch 2 | Step 1481300 | Avg Loss: 0.0154 | Grad Norm: 0.00759447\n",
      "Epoch 2 | Step 1481400 | Avg Loss: 0.0157 | Grad Norm: 0.01074978\n",
      "Epoch 2 | Step 1481500 | Avg Loss: 0.0157 | Grad Norm: 0.00991015\n",
      "Epoch 2 | Step 1481600 | Avg Loss: 0.0161 | Grad Norm: 0.00823269\n",
      "Epoch 2 | Step 1481700 | Avg Loss: 0.0160 | Grad Norm: 0.00961236\n",
      "Epoch 2 | Step 1481800 | Avg Loss: 0.0159 | Grad Norm: 0.00931113\n",
      "Epoch 2 | Step 1481900 | Avg Loss: 0.0157 | Grad Norm: 0.00922743\n",
      "Epoch 2 | Step 1482000 | Avg Loss: 0.0152 | Grad Norm: 0.00800400\n",
      "Epoch 2 | Step 1482100 | Avg Loss: 0.0154 | Grad Norm: 0.00864472\n",
      "Epoch 2 | Step 1482200 | Avg Loss: 0.0153 | Grad Norm: 0.00966097\n",
      "Epoch 2 | Step 1482300 | Avg Loss: 0.0154 | Grad Norm: 0.01008911\n",
      "Epoch 2 | Step 1482400 | Avg Loss: 0.0158 | Grad Norm: 0.00890618\n",
      "Epoch 2 | Step 1482500 | Avg Loss: 0.0157 | Grad Norm: 0.01016741\n",
      "Epoch 2 | Step 1482600 | Avg Loss: 0.0153 | Grad Norm: 0.00872075\n",
      "Epoch 2 | Step 1482700 | Avg Loss: 0.0157 | Grad Norm: 0.00806609\n",
      "Epoch 2 | Step 1482800 | Avg Loss: 0.0154 | Grad Norm: 0.00816959\n",
      "Epoch 2 | Step 1482900 | Avg Loss: 0.0157 | Grad Norm: 0.00893795\n",
      "Epoch 2 | Step 1483000 | Avg Loss: 0.0157 | Grad Norm: 0.00815811\n",
      "Epoch 2 | Step 1483100 | Avg Loss: 0.0157 | Grad Norm: 0.00753928\n",
      "Epoch 2 | Step 1483200 | Avg Loss: 0.0156 | Grad Norm: 0.00885227\n",
      "Epoch 2 | Step 1483300 | Avg Loss: 0.0158 | Grad Norm: 0.00990355\n",
      "Epoch 2 | Step 1483400 | Avg Loss: 0.0158 | Grad Norm: 0.00855603\n",
      "Epoch 2 | Step 1483500 | Avg Loss: 0.0157 | Grad Norm: 0.00893650\n",
      "Epoch 2 | Step 1483600 | Avg Loss: 0.0154 | Grad Norm: 0.00788410\n",
      "Epoch 2 | Step 1483700 | Avg Loss: 0.0156 | Grad Norm: 0.00783128\n",
      "Epoch 2 | Step 1483800 | Avg Loss: 0.0154 | Grad Norm: 0.00834732\n",
      "Epoch 2 | Step 1483900 | Avg Loss: 0.0156 | Grad Norm: 0.01169239\n",
      "Epoch 2 | Step 1484000 | Avg Loss: 0.0156 | Grad Norm: 0.00881605\n",
      "Epoch 2 | Step 1484100 | Avg Loss: 0.0156 | Grad Norm: 0.00827883\n",
      "Epoch 2 | Step 1484200 | Avg Loss: 0.0152 | Grad Norm: 0.00861524\n",
      "Epoch 2 | Step 1484300 | Avg Loss: 0.0155 | Grad Norm: 0.00976217\n",
      "Epoch 2 | Step 1484400 | Avg Loss: 0.0156 | Grad Norm: 0.01056517\n",
      "Epoch 2 | Step 1484500 | Avg Loss: 0.0153 | Grad Norm: 0.00875702\n",
      "Epoch 2 | Step 1484600 | Avg Loss: 0.0157 | Grad Norm: 0.00839825\n",
      "Epoch 2 | Step 1484700 | Avg Loss: 0.0156 | Grad Norm: 0.00939294\n",
      "Epoch 2 | Step 1484800 | Avg Loss: 0.0158 | Grad Norm: 0.00919902\n",
      "Epoch 2 | Step 1484900 | Avg Loss: 0.0155 | Grad Norm: 0.00954947\n",
      "Epoch 2 | Step 1485000 | Avg Loss: 0.0157 | Grad Norm: 0.01206444\n",
      "Epoch 2 | Step 1485100 | Avg Loss: 0.0155 | Grad Norm: 0.00804300\n",
      "Epoch 2 | Step 1485200 | Avg Loss: 0.0157 | Grad Norm: 0.00973669\n",
      "Epoch 2 | Step 1485300 | Avg Loss: 0.0153 | Grad Norm: 0.00824367\n",
      "Epoch 2 | Step 1485400 | Avg Loss: 0.0154 | Grad Norm: 0.00907395\n",
      "Epoch 2 | Step 1485500 | Avg Loss: 0.0154 | Grad Norm: 0.00870019\n",
      "Epoch 2 | Step 1485600 | Avg Loss: 0.0153 | Grad Norm: 0.00854755\n",
      "Epoch 2 | Step 1485700 | Avg Loss: 0.0153 | Grad Norm: 0.01169975\n",
      "Epoch 2 | Step 1485800 | Avg Loss: 0.0154 | Grad Norm: 0.00943541\n",
      "Epoch 2 | Step 1485900 | Avg Loss: 0.0153 | Grad Norm: 0.00957110\n",
      "Epoch 2 | Step 1486000 | Avg Loss: 0.0154 | Grad Norm: 0.00923083\n",
      "Epoch 2 | Step 1486100 | Avg Loss: 0.0154 | Grad Norm: 0.00875495\n",
      "Epoch 2 | Step 1486200 | Avg Loss: 0.0156 | Grad Norm: 0.00913616\n",
      "Epoch 2 | Step 1486300 | Avg Loss: 0.0154 | Grad Norm: 0.00848480\n",
      "Epoch 2 | Step 1486400 | Avg Loss: 0.0154 | Grad Norm: 0.00956682\n",
      "Epoch 2 | Step 1486500 | Avg Loss: 0.0156 | Grad Norm: 0.00975658\n",
      "Epoch 2 | Step 1486600 | Avg Loss: 0.0155 | Grad Norm: 0.00933845\n",
      "Epoch 2 | Step 1486700 | Avg Loss: 0.0153 | Grad Norm: 0.01084663\n",
      "Epoch 2 | Step 1486800 | Avg Loss: 0.0154 | Grad Norm: 0.00985053\n",
      "Epoch 2 | Step 1486900 | Avg Loss: 0.0156 | Grad Norm: 0.00831032\n",
      "Epoch 2 | Step 1487000 | Avg Loss: 0.0155 | Grad Norm: 0.00958040\n",
      "Epoch 2 | Step 1487100 | Avg Loss: 0.0157 | Grad Norm: 0.00792941\n",
      "Epoch 2 | Step 1487200 | Avg Loss: 0.0155 | Grad Norm: 0.00859298\n",
      "Epoch 2 | Step 1487300 | Avg Loss: 0.0154 | Grad Norm: 0.00816150\n",
      "Epoch 2 | Step 1487400 | Avg Loss: 0.0153 | Grad Norm: 0.01000378\n",
      "Epoch 2 | Step 1487500 | Avg Loss: 0.0156 | Grad Norm: 0.00753602\n",
      "Epoch 2 | Step 1487600 | Avg Loss: 0.0155 | Grad Norm: 0.00813097\n",
      "Epoch 2 | Step 1487700 | Avg Loss: 0.0154 | Grad Norm: 0.00939120\n",
      "Epoch 2 | Step 1487800 | Avg Loss: 0.0161 | Grad Norm: 0.00901523\n",
      "Epoch 2 | Step 1487900 | Avg Loss: 0.0163 | Grad Norm: 0.00850417\n",
      "Epoch 2 | Step 1488000 | Avg Loss: 0.0160 | Grad Norm: 0.00991204\n",
      "Epoch 2 | Step 1488100 | Avg Loss: 0.0158 | Grad Norm: 0.00906105\n",
      "Epoch 2 | Step 1488200 | Avg Loss: 0.0159 | Grad Norm: 0.00879480\n",
      "Epoch 2 | Step 1488300 | Avg Loss: 0.0159 | Grad Norm: 0.00765884\n",
      "Epoch 2 | Step 1488400 | Avg Loss: 0.0157 | Grad Norm: 0.00800375\n",
      "Epoch 2 | Step 1488500 | Avg Loss: 0.0156 | Grad Norm: 0.00815172\n",
      "Epoch 2 | Step 1488600 | Avg Loss: 0.0157 | Grad Norm: 0.00815211\n",
      "Epoch 2 | Step 1488700 | Avg Loss: 0.0157 | Grad Norm: 0.00804559\n",
      "Epoch 2 | Step 1488800 | Avg Loss: 0.0156 | Grad Norm: 0.00967016\n",
      "Epoch 2 | Step 1488900 | Avg Loss: 0.0157 | Grad Norm: 0.00839166\n",
      "Epoch 2 | Step 1489000 | Avg Loss: 0.0160 | Grad Norm: 0.01021958\n",
      "Epoch 2 | Step 1489100 | Avg Loss: 0.0162 | Grad Norm: 0.00906872\n",
      "Epoch 2 | Step 1489200 | Avg Loss: 0.0158 | Grad Norm: 0.00926399\n",
      "Epoch 2 | Step 1489300 | Avg Loss: 0.0156 | Grad Norm: 0.00901553\n",
      "Epoch 2 | Step 1489400 | Avg Loss: 0.0155 | Grad Norm: 0.00955171\n",
      "Epoch 2 | Step 1489500 | Avg Loss: 0.0160 | Grad Norm: 0.00969175\n",
      "Epoch 2 | Step 1489600 | Avg Loss: 0.0159 | Grad Norm: 0.00805416\n",
      "Epoch 2 | Step 1489700 | Avg Loss: 0.0161 | Grad Norm: 0.00876922\n",
      "Epoch 2 | Step 1489800 | Avg Loss: 0.0162 | Grad Norm: 0.01044843\n",
      "Epoch 2 | Step 1489900 | Avg Loss: 0.0160 | Grad Norm: 0.00883471\n",
      "Epoch 2 | Step 1490000 | Avg Loss: 0.0157 | Grad Norm: 0.00903758\n",
      "Epoch 2 | Step 1490100 | Avg Loss: 0.0155 | Grad Norm: 0.01008453\n",
      "Epoch 2 | Step 1490200 | Avg Loss: 0.0155 | Grad Norm: 0.01068917\n",
      "Epoch 2 | Step 1490300 | Avg Loss: 0.0156 | Grad Norm: 0.01014268\n",
      "Epoch 2 | Step 1490400 | Avg Loss: 0.0157 | Grad Norm: 0.00840821\n",
      "Epoch 2 | Step 1490500 | Avg Loss: 0.0156 | Grad Norm: 0.00956610\n",
      "Epoch 2 | Step 1490600 | Avg Loss: 0.0155 | Grad Norm: 0.00919023\n",
      "Epoch 2 | Step 1490700 | Avg Loss: 0.0157 | Grad Norm: 0.00836523\n",
      "Epoch 2 | Step 1490800 | Avg Loss: 0.0155 | Grad Norm: 0.00859714\n",
      "Epoch 2 | Step 1490900 | Avg Loss: 0.0155 | Grad Norm: 0.00954552\n",
      "Epoch 2 | Step 1491000 | Avg Loss: 0.0151 | Grad Norm: 0.00984703\n",
      "Epoch 2 | Step 1491100 | Avg Loss: 0.0154 | Grad Norm: 0.00847870\n",
      "Epoch 2 | Step 1491200 | Avg Loss: 0.0156 | Grad Norm: 0.00836628\n",
      "Epoch 2 | Step 1491300 | Avg Loss: 0.0159 | Grad Norm: 0.00933059\n",
      "Epoch 2 | Step 1491400 | Avg Loss: 0.0157 | Grad Norm: 0.01098692\n",
      "Epoch 2 | Step 1491500 | Avg Loss: 0.0158 | Grad Norm: 0.00789306\n",
      "Epoch 2 | Step 1491600 | Avg Loss: 0.0156 | Grad Norm: 0.01053665\n",
      "Epoch 2 | Step 1491700 | Avg Loss: 0.0157 | Grad Norm: 0.00870544\n",
      "Epoch 2 | Step 1491800 | Avg Loss: 0.0157 | Grad Norm: 0.00805478\n",
      "Epoch 2 | Step 1491900 | Avg Loss: 0.0156 | Grad Norm: 0.01022993\n",
      "Epoch 2 | Step 1492000 | Avg Loss: 0.0161 | Grad Norm: 0.00959314\n",
      "Epoch 2 | Step 1492100 | Avg Loss: 0.0160 | Grad Norm: 0.00966295\n",
      "Epoch 2 | Step 1492200 | Avg Loss: 0.0157 | Grad Norm: 0.00859481\n",
      "Epoch 2 | Step 1492300 | Avg Loss: 0.0156 | Grad Norm: 0.01011583\n",
      "Epoch 2 | Step 1492400 | Avg Loss: 0.0156 | Grad Norm: 0.00971785\n",
      "Epoch 2 | Step 1492500 | Avg Loss: 0.0152 | Grad Norm: 0.01049328\n",
      "Epoch 2 | Step 1492600 | Avg Loss: 0.0152 | Grad Norm: 0.00897257\n",
      "Epoch 2 | Step 1492700 | Avg Loss: 0.0151 | Grad Norm: 0.00814898\n",
      "Epoch 2 | Step 1492800 | Avg Loss: 0.0152 | Grad Norm: 0.00872287\n",
      "Epoch 2 | Step 1492900 | Avg Loss: 0.0154 | Grad Norm: 0.00872339\n",
      "Epoch 2 | Step 1493000 | Avg Loss: 0.0152 | Grad Norm: 0.00937402\n",
      "Epoch 2 | Step 1493100 | Avg Loss: 0.0154 | Grad Norm: 0.00925297\n",
      "Epoch 2 | Step 1493200 | Avg Loss: 0.0155 | Grad Norm: 0.00774818\n",
      "Epoch 2 | Step 1493300 | Avg Loss: 0.0161 | Grad Norm: 0.00934891\n",
      "Epoch 2 | Step 1493400 | Avg Loss: 0.0159 | Grad Norm: 0.00859249\n",
      "Epoch 2 | Step 1493500 | Avg Loss: 0.0157 | Grad Norm: 0.00881034\n",
      "Epoch 2 | Step 1493600 | Avg Loss: 0.0157 | Grad Norm: 0.00834928\n",
      "Epoch 2 | Step 1493700 | Avg Loss: 0.0158 | Grad Norm: 0.00871532\n",
      "Epoch 2 | Step 1493800 | Avg Loss: 0.0159 | Grad Norm: 0.00936375\n",
      "Epoch 2 | Step 1493900 | Avg Loss: 0.0157 | Grad Norm: 0.01006948\n",
      "Epoch 2 | Step 1494000 | Avg Loss: 0.0157 | Grad Norm: 0.00785061\n",
      "Epoch 2 | Step 1494100 | Avg Loss: 0.0155 | Grad Norm: 0.00923374\n",
      "Epoch 2 | Step 1494200 | Avg Loss: 0.0155 | Grad Norm: 0.00850698\n",
      "Epoch 2 | Step 1494300 | Avg Loss: 0.0154 | Grad Norm: 0.00854560\n",
      "Epoch 2 | Step 1494400 | Avg Loss: 0.0155 | Grad Norm: 0.00839832\n",
      "Epoch 2 | Step 1494500 | Avg Loss: 0.0158 | Grad Norm: 0.00979427\n",
      "Epoch 2 | Step 1494600 | Avg Loss: 0.0159 | Grad Norm: 0.01085974\n",
      "Epoch 2 | Step 1494700 | Avg Loss: 0.0160 | Grad Norm: 0.00843296\n",
      "Epoch 2 | Step 1494800 | Avg Loss: 0.0161 | Grad Norm: 0.00890761\n",
      "Epoch 2 | Step 1494900 | Avg Loss: 0.0159 | Grad Norm: 0.00789910\n",
      "Epoch 2 | Step 1495000 | Avg Loss: 0.0159 | Grad Norm: 0.01259227\n",
      "Epoch 2 | Step 1495100 | Avg Loss: 0.0161 | Grad Norm: 0.00926182\n",
      "Epoch 2 | Step 1495200 | Avg Loss: 0.0159 | Grad Norm: 0.01176248\n",
      "Epoch 2 | Step 1495300 | Avg Loss: 0.0157 | Grad Norm: 0.00912085\n",
      "Epoch 2 | Step 1495400 | Avg Loss: 0.0157 | Grad Norm: 0.00906393\n",
      "Epoch 2 | Step 1495500 | Avg Loss: 0.0156 | Grad Norm: 0.01018579\n",
      "Epoch 2 | Step 1495600 | Avg Loss: 0.0153 | Grad Norm: 0.01047616\n",
      "Epoch 2 | Step 1495700 | Avg Loss: 0.0155 | Grad Norm: 0.00934041\n",
      "Epoch 2 | Step 1495800 | Avg Loss: 0.0158 | Grad Norm: 0.00903354\n",
      "Epoch 2 | Step 1495900 | Avg Loss: 0.0157 | Grad Norm: 0.01048946\n",
      "Epoch 2 | Step 1496000 | Avg Loss: 0.0158 | Grad Norm: 0.00844586\n",
      "Epoch 2 | Step 1496100 | Avg Loss: 0.0153 | Grad Norm: 0.00836568\n",
      "Epoch 2 | Step 1496200 | Avg Loss: 0.0154 | Grad Norm: 0.01016143\n",
      "Epoch 2 | Step 1496300 | Avg Loss: 0.0155 | Grad Norm: 0.00852815\n",
      "Epoch 2 | Step 1496400 | Avg Loss: 0.0157 | Grad Norm: 0.00793488\n",
      "Epoch 2 | Step 1496500 | Avg Loss: 0.0156 | Grad Norm: 0.00818732\n",
      "Epoch 2 | Step 1496600 | Avg Loss: 0.0154 | Grad Norm: 0.00846602\n",
      "Epoch 2 | Step 1496700 | Avg Loss: 0.0155 | Grad Norm: 0.00848081\n",
      "Epoch 2 | Step 1496800 | Avg Loss: 0.0158 | Grad Norm: 0.00829574\n",
      "Epoch 2 | Step 1496900 | Avg Loss: 0.0158 | Grad Norm: 0.00965809\n",
      "Epoch 2 | Step 1497000 | Avg Loss: 0.0162 | Grad Norm: 0.00886703\n",
      "Epoch 2 | Step 1497100 | Avg Loss: 0.0162 | Grad Norm: 0.00803968\n",
      "Epoch 2 | Step 1497200 | Avg Loss: 0.0160 | Grad Norm: 0.00919094\n",
      "Epoch 2 | Step 1497300 | Avg Loss: 0.0161 | Grad Norm: 0.00951381\n",
      "Epoch 2 | Step 1497400 | Avg Loss: 0.0161 | Grad Norm: 0.00865781\n",
      "Epoch 2 | Step 1497500 | Avg Loss: 0.0164 | Grad Norm: 0.00778255\n",
      "Epoch 2 | Step 1497600 | Avg Loss: 0.0163 | Grad Norm: 0.00871555\n",
      "Epoch 2 | Step 1497700 | Avg Loss: 0.0162 | Grad Norm: 0.00923150\n",
      "Epoch 2 | Step 1497800 | Avg Loss: 0.0159 | Grad Norm: 0.01023439\n",
      "Epoch 2 | Step 1497900 | Avg Loss: 0.0155 | Grad Norm: 0.00896927\n",
      "Epoch 2 | Step 1498000 | Avg Loss: 0.0155 | Grad Norm: 0.00806528\n",
      "Epoch 2 | Step 1498100 | Avg Loss: 0.0160 | Grad Norm: 0.01058572\n",
      "Epoch 2 | Step 1498200 | Avg Loss: 0.0161 | Grad Norm: 0.00885743\n",
      "Epoch 2 | Step 1498300 | Avg Loss: 0.0159 | Grad Norm: 0.00775629\n",
      "Epoch 2 | Step 1498400 | Avg Loss: 0.0157 | Grad Norm: 0.00894806\n",
      "Epoch 2 | Step 1498500 | Avg Loss: 0.0157 | Grad Norm: 0.00905682\n",
      "Epoch 2 | Step 1498600 | Avg Loss: 0.0158 | Grad Norm: 0.00873224\n",
      "Epoch 2 | Step 1498700 | Avg Loss: 0.0161 | Grad Norm: 0.00795934\n",
      "Epoch 2 | Step 1498800 | Avg Loss: 0.0160 | Grad Norm: 0.00897174\n",
      "Epoch 2 | Step 1498900 | Avg Loss: 0.0161 | Grad Norm: 0.01056396\n",
      "Epoch 2 | Step 1499000 | Avg Loss: 0.0162 | Grad Norm: 0.01223744\n",
      "Epoch 2 | Step 1499100 | Avg Loss: 0.0164 | Grad Norm: 0.00889097\n",
      "Epoch 2 | Step 1499200 | Avg Loss: 0.0161 | Grad Norm: 0.00882529\n",
      "Epoch 2 | Step 1499300 | Avg Loss: 0.0160 | Grad Norm: 0.00856587\n",
      "Epoch 2 | Step 1499400 | Avg Loss: 0.0159 | Grad Norm: 0.00868188\n",
      "Epoch 2 | Step 1499500 | Avg Loss: 0.0163 | Grad Norm: 0.01037874\n",
      "Epoch 2 | Step 1499600 | Avg Loss: 0.0162 | Grad Norm: 0.01155185\n",
      "Epoch 2 | Step 1499700 | Avg Loss: 0.0162 | Grad Norm: 0.01163200\n",
      "Epoch 2 | Step 1499800 | Avg Loss: 0.0162 | Grad Norm: 0.01001038\n",
      "Epoch 2 | Step 1499900 | Avg Loss: 0.0159 | Grad Norm: 0.00860871\n",
      "Epoch 2 | Step 1500000 | Avg Loss: 0.0157 | Grad Norm: 0.01116695\n",
      "Saving model at step1500000\n",
      "Epoch 2 | Step 1500100 | Avg Loss: 0.0158 | Grad Norm: 0.00883662\n",
      "Epoch 2 | Step 1500200 | Avg Loss: 0.0156 | Grad Norm: 0.00921870\n",
      "Epoch 2 | Step 1500300 | Avg Loss: 0.0154 | Grad Norm: 0.01034198\n",
      "Epoch 2 | Step 1500400 | Avg Loss: 0.0155 | Grad Norm: 0.00896644\n",
      "Epoch 2 | Step 1500500 | Avg Loss: 0.0156 | Grad Norm: 0.01064345\n",
      "Epoch 2 | Step 1500600 | Avg Loss: 0.0154 | Grad Norm: 0.00910741\n",
      "Epoch 2 | Step 1500700 | Avg Loss: 0.0155 | Grad Norm: 0.00797184\n",
      "Epoch 2 | Step 1500800 | Avg Loss: 0.0155 | Grad Norm: 0.00804036\n",
      "Epoch 2 | Step 1500900 | Avg Loss: 0.0153 | Grad Norm: 0.00888251\n",
      "Epoch 2 | Step 1501000 | Avg Loss: 0.0151 | Grad Norm: 0.00896614\n",
      "Epoch 2 | Step 1501100 | Avg Loss: 0.0154 | Grad Norm: 0.00973933\n",
      "Epoch 2 | Step 1501200 | Avg Loss: 0.0155 | Grad Norm: 0.00990991\n",
      "Epoch 2 | Step 1501300 | Avg Loss: 0.0155 | Grad Norm: 0.00773740\n",
      "Epoch 2 | Step 1501400 | Avg Loss: 0.0156 | Grad Norm: 0.00805529\n",
      "Epoch 2 | Step 1501500 | Avg Loss: 0.0156 | Grad Norm: 0.00922854\n",
      "Epoch 2 | Step 1501600 | Avg Loss: 0.0151 | Grad Norm: 0.00902031\n",
      "Epoch 2 | Step 1501700 | Avg Loss: 0.0155 | Grad Norm: 0.00869582\n",
      "Epoch 2 | Step 1501800 | Avg Loss: 0.0153 | Grad Norm: 0.00823944\n",
      "Epoch 2 | Step 1501900 | Avg Loss: 0.0151 | Grad Norm: 0.00872490\n",
      "Epoch 2 | Step 1502000 | Avg Loss: 0.0148 | Grad Norm: 0.00834674\n",
      "Epoch 2 | Step 1502100 | Avg Loss: 0.0150 | Grad Norm: 0.00769468\n",
      "Epoch 2 | Step 1502200 | Avg Loss: 0.0152 | Grad Norm: 0.00841315\n",
      "Epoch 2 | Step 1502300 | Avg Loss: 0.0149 | Grad Norm: 0.00794881\n",
      "Epoch 2 | Step 1502400 | Avg Loss: 0.0152 | Grad Norm: 0.00880432\n",
      "Epoch 2 | Step 1502500 | Avg Loss: 0.0152 | Grad Norm: 0.00948488\n",
      "Epoch 2 | Step 1502600 | Avg Loss: 0.0155 | Grad Norm: 0.00867443\n",
      "Epoch 2 | Step 1502700 | Avg Loss: 0.0156 | Grad Norm: 0.01146339\n",
      "Epoch 2 | Step 1502800 | Avg Loss: 0.0157 | Grad Norm: 0.00817604\n",
      "Epoch 2 | Step 1502900 | Avg Loss: 0.0157 | Grad Norm: 0.01013687\n",
      "Epoch 2 | Step 1503000 | Avg Loss: 0.0155 | Grad Norm: 0.00970718\n",
      "Epoch 2 | Step 1503100 | Avg Loss: 0.0155 | Grad Norm: 0.00849593\n",
      "Epoch 2 | Step 1503200 | Avg Loss: 0.0151 | Grad Norm: 0.00891992\n",
      "Epoch 2 | Step 1503300 | Avg Loss: 0.0154 | Grad Norm: 0.01069786\n",
      "Epoch 2 | Step 1503400 | Avg Loss: 0.0153 | Grad Norm: 0.01194786\n",
      "Epoch 2 | Step 1503500 | Avg Loss: 0.0155 | Grad Norm: 0.00876160\n",
      "Epoch 2 | Step 1503600 | Avg Loss: 0.0154 | Grad Norm: 0.00874713\n",
      "Epoch 2 | Step 1503700 | Avg Loss: 0.0157 | Grad Norm: 0.00802755\n",
      "Epoch 2 | Step 1503800 | Avg Loss: 0.0160 | Grad Norm: 0.00866258\n",
      "Epoch 2 | Step 1503900 | Avg Loss: 0.0157 | Grad Norm: 0.00872858\n",
      "Epoch 2 | Step 1504000 | Avg Loss: 0.0159 | Grad Norm: 0.00891309\n",
      "Epoch 2 | Step 1504100 | Avg Loss: 0.0157 | Grad Norm: 0.00931029\n",
      "Epoch 2 | Step 1504200 | Avg Loss: 0.0156 | Grad Norm: 0.00894544\n",
      "Epoch 2 | Step 1504300 | Avg Loss: 0.0156 | Grad Norm: 0.00889156\n",
      "Epoch 2 | Step 1504400 | Avg Loss: 0.0156 | Grad Norm: 0.00799512\n",
      "Epoch 2 | Step 1504500 | Avg Loss: 0.0156 | Grad Norm: 0.00965790\n",
      "Epoch 2 | Step 1504600 | Avg Loss: 0.0155 | Grad Norm: 0.00873451\n",
      "Epoch 2 | Step 1504700 | Avg Loss: 0.0155 | Grad Norm: 0.00883595\n",
      "Epoch 2 | Step 1504800 | Avg Loss: 0.0152 | Grad Norm: 0.00843159\n",
      "Epoch 2 | Step 1504900 | Avg Loss: 0.0151 | Grad Norm: 0.00879458\n",
      "Epoch 2 | Step 1505000 | Avg Loss: 0.0152 | Grad Norm: 0.00856434\n",
      "Epoch 2 | Step 1505100 | Avg Loss: 0.0155 | Grad Norm: 0.00802337\n",
      "Epoch 2 | Step 1505200 | Avg Loss: 0.0156 | Grad Norm: 0.00790194\n",
      "Epoch 2 | Step 1505300 | Avg Loss: 0.0158 | Grad Norm: 0.00939260\n",
      "Epoch 2 | Step 1505400 | Avg Loss: 0.0158 | Grad Norm: 0.00808941\n",
      "Epoch 2 | Step 1505500 | Avg Loss: 0.0158 | Grad Norm: 0.00968752\n",
      "Epoch 2 | Step 1505600 | Avg Loss: 0.0160 | Grad Norm: 0.00917986\n",
      "Epoch 2 | Step 1505700 | Avg Loss: 0.0161 | Grad Norm: 0.00851383\n",
      "Epoch 2 | Step 1505800 | Avg Loss: 0.0155 | Grad Norm: 0.00809725\n",
      "Epoch 2 | Step 1505900 | Avg Loss: 0.0158 | Grad Norm: 0.00858836\n",
      "Epoch 2 | Step 1506000 | Avg Loss: 0.0160 | Grad Norm: 0.00817105\n",
      "Epoch 2 | Step 1506100 | Avg Loss: 0.0155 | Grad Norm: 0.00868533\n",
      "Epoch 2 | Step 1506200 | Avg Loss: 0.0154 | Grad Norm: 0.00875714\n",
      "Epoch 2 | Step 1506300 | Avg Loss: 0.0156 | Grad Norm: 0.00891132\n",
      "Epoch 2 | Step 1506400 | Avg Loss: 0.0155 | Grad Norm: 0.00907686\n",
      "Epoch 2 | Step 1506500 | Avg Loss: 0.0154 | Grad Norm: 0.01046882\n",
      "Epoch 2 | Step 1506600 | Avg Loss: 0.0155 | Grad Norm: 0.00964761\n",
      "Epoch 2 | Step 1506700 | Avg Loss: 0.0153 | Grad Norm: 0.00904261\n",
      "Epoch 2 | Step 1506800 | Avg Loss: 0.0148 | Grad Norm: 0.00879838\n",
      "Epoch 2 | Step 1506900 | Avg Loss: 0.0150 | Grad Norm: 0.00967930\n",
      "Epoch 2 | Step 1507000 | Avg Loss: 0.0148 | Grad Norm: 0.01591432\n",
      "Epoch 2 | Step 1507100 | Avg Loss: 0.0148 | Grad Norm: 0.00881162\n",
      "Epoch 2 | Step 1507200 | Avg Loss: 0.0148 | Grad Norm: 0.00846081\n",
      "Epoch 2 | Step 1507300 | Avg Loss: 0.0151 | Grad Norm: 0.00916387\n",
      "Epoch 2 | Step 1507400 | Avg Loss: 0.0155 | Grad Norm: 0.00903957\n",
      "Epoch 2 | Step 1507500 | Avg Loss: 0.0157 | Grad Norm: 0.00969085\n",
      "Epoch 2 | Step 1507600 | Avg Loss: 0.0152 | Grad Norm: 0.00822436\n",
      "Epoch 2 | Step 1507700 | Avg Loss: 0.0152 | Grad Norm: 0.01064975\n",
      "Epoch 2 | Step 1507800 | Avg Loss: 0.0152 | Grad Norm: 0.00857961\n",
      "Epoch 2 | Step 1507900 | Avg Loss: 0.0154 | Grad Norm: 0.00859365\n",
      "Epoch 2 | Step 1508000 | Avg Loss: 0.0154 | Grad Norm: 0.01030835\n",
      "Epoch 2 | Step 1508100 | Avg Loss: 0.0153 | Grad Norm: 0.00853964\n",
      "Epoch 2 | Step 1508200 | Avg Loss: 0.0157 | Grad Norm: 0.01100336\n",
      "Epoch 2 | Step 1508300 | Avg Loss: 0.0157 | Grad Norm: 0.01090184\n",
      "Epoch 2 | Step 1508400 | Avg Loss: 0.0159 | Grad Norm: 0.00996245\n",
      "Epoch 2 | Step 1508500 | Avg Loss: 0.0156 | Grad Norm: 0.00943895\n",
      "Epoch 2 | Step 1508600 | Avg Loss: 0.0157 | Grad Norm: 0.00950190\n",
      "Epoch 2 | Step 1508700 | Avg Loss: 0.0155 | Grad Norm: 0.00742656\n",
      "Epoch 2 | Step 1508800 | Avg Loss: 0.0153 | Grad Norm: 0.01133062\n",
      "Epoch 2 | Step 1508900 | Avg Loss: 0.0155 | Grad Norm: 0.00932506\n",
      "Epoch 2 | Step 1509000 | Avg Loss: 0.0156 | Grad Norm: 0.00784593\n",
      "Epoch 2 | Step 1509100 | Avg Loss: 0.0157 | Grad Norm: 0.00910234\n",
      "Epoch 2 | Step 1509200 | Avg Loss: 0.0159 | Grad Norm: 0.00860628\n",
      "Epoch 2 | Step 1509300 | Avg Loss: 0.0160 | Grad Norm: 0.01049870\n",
      "Epoch 2 | Step 1509400 | Avg Loss: 0.0154 | Grad Norm: 0.00757806\n",
      "Epoch 2 | Step 1509500 | Avg Loss: 0.0150 | Grad Norm: 0.01081844\n",
      "Epoch 2 | Step 1509600 | Avg Loss: 0.0152 | Grad Norm: 0.00933901\n",
      "Epoch 2 | Step 1509700 | Avg Loss: 0.0153 | Grad Norm: 0.00991532\n",
      "Epoch 2 | Step 1509800 | Avg Loss: 0.0155 | Grad Norm: 0.00848156\n",
      "Epoch 2 | Step 1509900 | Avg Loss: 0.0155 | Grad Norm: 0.01214499\n",
      "Epoch 2 | Step 1510000 | Avg Loss: 0.0156 | Grad Norm: 0.00856597\n",
      "Epoch 2 | Step 1510100 | Avg Loss: 0.0157 | Grad Norm: 0.00917035\n",
      "Epoch 2 | Step 1510200 | Avg Loss: 0.0159 | Grad Norm: 0.00948338\n",
      "Epoch 2 | Step 1510300 | Avg Loss: 0.0158 | Grad Norm: 0.01041205\n",
      "Epoch 2 | Step 1510400 | Avg Loss: 0.0159 | Grad Norm: 0.00933156\n",
      "Epoch 2 | Step 1510500 | Avg Loss: 0.0157 | Grad Norm: 0.00916572\n",
      "Epoch 2 | Step 1510600 | Avg Loss: 0.0157 | Grad Norm: 0.00851034\n",
      "Epoch 2 | Step 1510700 | Avg Loss: 0.0158 | Grad Norm: 0.01036837\n",
      "Epoch 2 | Step 1510800 | Avg Loss: 0.0160 | Grad Norm: 0.00801537\n",
      "Epoch 2 | Step 1510900 | Avg Loss: 0.0159 | Grad Norm: 0.00915122\n",
      "Epoch 2 | Step 1511000 | Avg Loss: 0.0160 | Grad Norm: 0.00838280\n",
      "Epoch 2 | Step 1511100 | Avg Loss: 0.0162 | Grad Norm: 0.00828873\n",
      "Epoch 2 | Step 1511200 | Avg Loss: 0.0158 | Grad Norm: 0.00905559\n",
      "Epoch 2 | Step 1511300 | Avg Loss: 0.0158 | Grad Norm: 0.00931922\n",
      "Epoch 2 | Step 1511400 | Avg Loss: 0.0158 | Grad Norm: 0.00986301\n",
      "Epoch 2 | Step 1511500 | Avg Loss: 0.0156 | Grad Norm: 0.00953023\n",
      "Epoch 2 | Step 1511600 | Avg Loss: 0.0161 | Grad Norm: 0.00977902\n",
      "Epoch 2 | Step 1511700 | Avg Loss: 0.0164 | Grad Norm: 0.00800675\n",
      "Epoch 2 | Step 1511800 | Avg Loss: 0.0162 | Grad Norm: 0.00919214\n",
      "Epoch 2 | Step 1511900 | Avg Loss: 0.0159 | Grad Norm: 0.00926214\n",
      "Epoch 2 | Step 1512000 | Avg Loss: 0.0159 | Grad Norm: 0.01000920\n",
      "Epoch 2 | Step 1512100 | Avg Loss: 0.0158 | Grad Norm: 0.00833084\n",
      "Epoch 2 | Step 1512200 | Avg Loss: 0.0157 | Grad Norm: 0.00997673\n",
      "Epoch 2 | Step 1512300 | Avg Loss: 0.0154 | Grad Norm: 0.00993356\n",
      "Epoch 2 | Step 1512400 | Avg Loss: 0.0155 | Grad Norm: 0.00770035\n",
      "Epoch 2 | Step 1512500 | Avg Loss: 0.0153 | Grad Norm: 0.00930788\n",
      "Epoch 2 | Step 1512600 | Avg Loss: 0.0155 | Grad Norm: 0.00944554\n",
      "Epoch 2 | Step 1512700 | Avg Loss: 0.0157 | Grad Norm: 0.00885472\n",
      "Epoch 2 | Step 1512800 | Avg Loss: 0.0157 | Grad Norm: 0.00885380\n",
      "Epoch 2 | Step 1512900 | Avg Loss: 0.0157 | Grad Norm: 0.00929628\n",
      "Epoch 2 | Step 1513000 | Avg Loss: 0.0158 | Grad Norm: 0.01320449\n",
      "Epoch 2 | Step 1513100 | Avg Loss: 0.0155 | Grad Norm: 0.00804820\n",
      "Epoch 2 | Step 1513200 | Avg Loss: 0.0156 | Grad Norm: 0.00891828\n",
      "Epoch 2 | Step 1513300 | Avg Loss: 0.0156 | Grad Norm: 0.00959611\n",
      "Epoch 2 | Step 1513400 | Avg Loss: 0.0153 | Grad Norm: 0.01286401\n",
      "Epoch 2 | Step 1513500 | Avg Loss: 0.0158 | Grad Norm: 0.00864129\n",
      "Epoch 2 | Step 1513600 | Avg Loss: 0.0157 | Grad Norm: 0.00818168\n",
      "Epoch 2 | Step 1513700 | Avg Loss: 0.0154 | Grad Norm: 0.00840094\n",
      "Epoch 2 | Step 1513800 | Avg Loss: 0.0149 | Grad Norm: 0.00816705\n",
      "Epoch 2 | Step 1513900 | Avg Loss: 0.0150 | Grad Norm: 0.00922128\n",
      "Epoch 2 | Step 1514000 | Avg Loss: 0.0152 | Grad Norm: 0.00815004\n",
      "Epoch 2 | Step 1514100 | Avg Loss: 0.0149 | Grad Norm: 0.00900358\n",
      "Epoch 2 | Step 1514200 | Avg Loss: 0.0155 | Grad Norm: 0.01029435\n",
      "Epoch 2 | Step 1514300 | Avg Loss: 0.0153 | Grad Norm: 0.00919711\n",
      "Epoch 2 | Step 1514400 | Avg Loss: 0.0153 | Grad Norm: 0.00848208\n",
      "Epoch 2 | Step 1514500 | Avg Loss: 0.0152 | Grad Norm: 0.00997513\n",
      "Epoch 2 | Step 1514600 | Avg Loss: 0.0152 | Grad Norm: 0.00955541\n",
      "Epoch 2 | Step 1514700 | Avg Loss: 0.0152 | Grad Norm: 0.00868412\n",
      "Epoch 2 | Step 1514800 | Avg Loss: 0.0153 | Grad Norm: 0.00912622\n",
      "Epoch 2 | Step 1514900 | Avg Loss: 0.0156 | Grad Norm: 0.00926982\n",
      "Epoch 2 | Step 1515000 | Avg Loss: 0.0155 | Grad Norm: 0.01085439\n",
      "Epoch 2 | Step 1515100 | Avg Loss: 0.0158 | Grad Norm: 0.00952451\n",
      "Epoch 2 | Step 1515200 | Avg Loss: 0.0161 | Grad Norm: 0.00951525\n",
      "Epoch 2 | Step 1515300 | Avg Loss: 0.0161 | Grad Norm: 0.00946843\n",
      "Epoch 2 | Step 1515400 | Avg Loss: 0.0159 | Grad Norm: 0.01016516\n",
      "Epoch 2 | Step 1515500 | Avg Loss: 0.0159 | Grad Norm: 0.01107637\n",
      "Epoch 2 | Step 1515600 | Avg Loss: 0.0159 | Grad Norm: 0.00926413\n",
      "Epoch 2 | Step 1515700 | Avg Loss: 0.0155 | Grad Norm: 0.00941909\n",
      "Epoch 2 | Step 1515800 | Avg Loss: 0.0159 | Grad Norm: 0.00859873\n",
      "Epoch 2 | Step 1515900 | Avg Loss: 0.0156 | Grad Norm: 0.00810784\n",
      "Epoch 2 | Step 1516000 | Avg Loss: 0.0160 | Grad Norm: 0.00902302\n",
      "Epoch 2 | Step 1516100 | Avg Loss: 0.0162 | Grad Norm: 0.00927981\n",
      "Epoch 2 | Step 1516200 | Avg Loss: 0.0162 | Grad Norm: 0.00863864\n",
      "Epoch 2 | Step 1516300 | Avg Loss: 0.0159 | Grad Norm: 0.00856547\n",
      "Epoch 2 | Step 1516400 | Avg Loss: 0.0159 | Grad Norm: 0.00980992\n",
      "Epoch 2 | Step 1516500 | Avg Loss: 0.0159 | Grad Norm: 0.00959010\n",
      "Epoch 2 | Step 1516600 | Avg Loss: 0.0157 | Grad Norm: 0.00876785\n",
      "Epoch 2 | Step 1516700 | Avg Loss: 0.0156 | Grad Norm: 0.01123220\n",
      "Epoch 2 | Step 1516800 | Avg Loss: 0.0151 | Grad Norm: 0.00913596\n",
      "Epoch 2 | Step 1516900 | Avg Loss: 0.0153 | Grad Norm: 0.00780340\n",
      "Epoch 2 | Step 1517000 | Avg Loss: 0.0155 | Grad Norm: 0.00982669\n",
      "Epoch 2 | Step 1517100 | Avg Loss: 0.0154 | Grad Norm: 0.00920076\n",
      "Epoch 2 | Step 1517200 | Avg Loss: 0.0155 | Grad Norm: 0.01040114\n",
      "Epoch 2 | Step 1517300 | Avg Loss: 0.0154 | Grad Norm: 0.00857957\n",
      "Epoch 2 | Step 1517400 | Avg Loss: 0.0152 | Grad Norm: 0.00757825\n",
      "Epoch 2 | Step 1517500 | Avg Loss: 0.0152 | Grad Norm: 0.00820758\n",
      "Epoch 2 | Step 1517600 | Avg Loss: 0.0155 | Grad Norm: 0.00909836\n",
      "Epoch 2 | Step 1517700 | Avg Loss: 0.0155 | Grad Norm: 0.00834565\n",
      "Epoch 2 | Step 1517800 | Avg Loss: 0.0154 | Grad Norm: 0.00997445\n",
      "Epoch 2 | Step 1517900 | Avg Loss: 0.0157 | Grad Norm: 0.00782834\n",
      "Epoch 2 | Step 1518000 | Avg Loss: 0.0156 | Grad Norm: 0.00938880\n",
      "Epoch 2 | Step 1518100 | Avg Loss: 0.0159 | Grad Norm: 0.00802373\n",
      "Epoch 2 | Step 1518200 | Avg Loss: 0.0157 | Grad Norm: 0.00799282\n",
      "Epoch 2 | Step 1518300 | Avg Loss: 0.0158 | Grad Norm: 0.01093626\n",
      "Epoch 2 | Step 1518400 | Avg Loss: 0.0157 | Grad Norm: 0.01072031\n",
      "Epoch 2 | Step 1518500 | Avg Loss: 0.0162 | Grad Norm: 0.00890185\n",
      "Epoch 2 | Step 1518600 | Avg Loss: 0.0164 | Grad Norm: 0.00959487\n",
      "Epoch 2 | Step 1518700 | Avg Loss: 0.0163 | Grad Norm: 0.00888377\n",
      "Epoch 2 | Step 1518800 | Avg Loss: 0.0163 | Grad Norm: 0.00896427\n",
      "Epoch 2 | Step 1518900 | Avg Loss: 0.0163 | Grad Norm: 0.00936994\n",
      "Epoch 2 | Step 1519000 | Avg Loss: 0.0163 | Grad Norm: 0.00951382\n",
      "Epoch 2 | Step 1519100 | Avg Loss: 0.0160 | Grad Norm: 0.00911484\n",
      "Epoch 2 | Step 1519200 | Avg Loss: 0.0154 | Grad Norm: 0.00898401\n",
      "Epoch 2 | Step 1519300 | Avg Loss: 0.0154 | Grad Norm: 0.00854436\n",
      "Epoch 2 | Step 1519400 | Avg Loss: 0.0157 | Grad Norm: 0.00949962\n",
      "Epoch 2 | Step 1519500 | Avg Loss: 0.0159 | Grad Norm: 0.00890015\n",
      "Epoch 2 | Step 1519600 | Avg Loss: 0.0158 | Grad Norm: 0.01059912\n",
      "Epoch 2 | Step 1519700 | Avg Loss: 0.0155 | Grad Norm: 0.01410634\n",
      "Epoch 2 | Step 1519800 | Avg Loss: 0.0153 | Grad Norm: 0.00960902\n",
      "Epoch 2 | Step 1519900 | Avg Loss: 0.0154 | Grad Norm: 0.00783541\n",
      "Epoch 2 | Step 1520000 | Avg Loss: 0.0154 | Grad Norm: 0.00868553\n",
      "Epoch 2 | Step 1520100 | Avg Loss: 0.0158 | Grad Norm: 0.00890527\n",
      "Epoch 2 | Step 1520200 | Avg Loss: 0.0158 | Grad Norm: 0.00807759\n",
      "Epoch 2 | Step 1520300 | Avg Loss: 0.0162 | Grad Norm: 0.01003297\n",
      "Epoch 2 | Step 1520400 | Avg Loss: 0.0165 | Grad Norm: 0.00891723\n",
      "Epoch 2 | Step 1520500 | Avg Loss: 0.0165 | Grad Norm: 0.00895820\n",
      "Epoch 2 | Step 1520600 | Avg Loss: 0.0162 | Grad Norm: 0.00827858\n",
      "Epoch 2 | Step 1520700 | Avg Loss: 0.0163 | Grad Norm: 0.00896989\n",
      "Epoch 2 | Step 1520800 | Avg Loss: 0.0163 | Grad Norm: 0.00856873\n",
      "Epoch 2 | Step 1520900 | Avg Loss: 0.0162 | Grad Norm: 0.00836566\n",
      "Epoch 2 | Step 1521000 | Avg Loss: 0.0159 | Grad Norm: 0.00933503\n",
      "Epoch 2 | Step 1521100 | Avg Loss: 0.0158 | Grad Norm: 0.01026155\n",
      "Epoch 2 | Step 1521200 | Avg Loss: 0.0156 | Grad Norm: 0.00816067\n",
      "Epoch 2 | Step 1521300 | Avg Loss: 0.0155 | Grad Norm: 0.01147305\n",
      "Epoch 2 | Step 1521400 | Avg Loss: 0.0159 | Grad Norm: 0.00954420\n",
      "Epoch 2 | Step 1521500 | Avg Loss: 0.0161 | Grad Norm: 0.00887320\n",
      "Epoch 2 | Step 1521600 | Avg Loss: 0.0162 | Grad Norm: 0.00977475\n",
      "Epoch 2 | Step 1521700 | Avg Loss: 0.0162 | Grad Norm: 0.00961097\n",
      "Epoch 2 | Step 1521800 | Avg Loss: 0.0163 | Grad Norm: 0.00903406\n",
      "Epoch 2 | Step 1521900 | Avg Loss: 0.0164 | Grad Norm: 0.00994162\n",
      "Epoch 2 | Step 1522000 | Avg Loss: 0.0160 | Grad Norm: 0.00880724\n",
      "Epoch 2 | Step 1522100 | Avg Loss: 0.0158 | Grad Norm: 0.00957600\n",
      "Epoch 2 | Step 1522200 | Avg Loss: 0.0158 | Grad Norm: 0.00896268\n",
      "Epoch 2 | Step 1522300 | Avg Loss: 0.0159 | Grad Norm: 0.00982245\n",
      "Epoch 2 | Step 1522400 | Avg Loss: 0.0162 | Grad Norm: 0.00804727\n",
      "Epoch 2 | Step 1522500 | Avg Loss: 0.0161 | Grad Norm: 0.00991143\n",
      "Epoch 2 | Step 1522600 | Avg Loss: 0.0161 | Grad Norm: 0.00973752\n",
      "Epoch 2 | Step 1522700 | Avg Loss: 0.0159 | Grad Norm: 0.00834269\n",
      "Epoch 2 | Step 1522800 | Avg Loss: 0.0160 | Grad Norm: 0.01228405\n",
      "Epoch 2 | Step 1522900 | Avg Loss: 0.0159 | Grad Norm: 0.00984837\n",
      "Epoch 2 | Step 1523000 | Avg Loss: 0.0160 | Grad Norm: 0.00995484\n",
      "Epoch 2 | Step 1523100 | Avg Loss: 0.0161 | Grad Norm: 0.00794496\n",
      "Epoch 2 | Step 1523200 | Avg Loss: 0.0159 | Grad Norm: 0.00899837\n",
      "Epoch 2 | Step 1523300 | Avg Loss: 0.0159 | Grad Norm: 0.01008783\n",
      "Epoch 2 | Step 1523400 | Avg Loss: 0.0156 | Grad Norm: 0.01224162\n",
      "Epoch 2 | Step 1523500 | Avg Loss: 0.0157 | Grad Norm: 0.00923852\n",
      "Epoch 2 | Step 1523600 | Avg Loss: 0.0156 | Grad Norm: 0.00811164\n",
      "Epoch 2 | Step 1523700 | Avg Loss: 0.0155 | Grad Norm: 0.00891912\n",
      "Epoch 2 | Step 1523800 | Avg Loss: 0.0157 | Grad Norm: 0.00920399\n",
      "Epoch 2 | Step 1523900 | Avg Loss: 0.0158 | Grad Norm: 0.00784867\n",
      "Epoch 2 | Step 1524000 | Avg Loss: 0.0159 | Grad Norm: 0.01024527\n",
      "Epoch 2 | Step 1524100 | Avg Loss: 0.0159 | Grad Norm: 0.00836541\n",
      "Epoch 2 | Step 1524200 | Avg Loss: 0.0157 | Grad Norm: 0.00830403\n",
      "Epoch 2 | Step 1524300 | Avg Loss: 0.0159 | Grad Norm: 0.01613446\n",
      "Epoch 2 | Step 1524400 | Avg Loss: 0.0163 | Grad Norm: 0.00925444\n",
      "Epoch 2 | Step 1524500 | Avg Loss: 0.0161 | Grad Norm: 0.01133458\n",
      "Epoch 2 | Step 1524600 | Avg Loss: 0.0162 | Grad Norm: 0.00861948\n",
      "Epoch 2 | Step 1524700 | Avg Loss: 0.0160 | Grad Norm: 0.00890692\n",
      "Epoch 2 | Step 1524800 | Avg Loss: 0.0160 | Grad Norm: 0.01031245\n",
      "Epoch 2 | Step 1524900 | Avg Loss: 0.0160 | Grad Norm: 0.00928410\n",
      "Epoch 2 | Step 1525000 | Avg Loss: 0.0159 | Grad Norm: 0.00996873\n",
      "Epoch 2 | Step 1525100 | Avg Loss: 0.0158 | Grad Norm: 0.00817394\n",
      "Epoch 2 | Step 1525200 | Avg Loss: 0.0155 | Grad Norm: 0.00865588\n",
      "Epoch 2 | Step 1525300 | Avg Loss: 0.0155 | Grad Norm: 0.00779889\n",
      "Epoch 2 | Step 1525400 | Avg Loss: 0.0157 | Grad Norm: 0.00938681\n",
      "Epoch 2 | Step 1525500 | Avg Loss: 0.0160 | Grad Norm: 0.00899635\n",
      "Epoch 2 | Step 1525600 | Avg Loss: 0.0166 | Grad Norm: 0.00872610\n",
      "Epoch 2 | Step 1525700 | Avg Loss: 0.0161 | Grad Norm: 0.01056499\n",
      "Epoch 2 | Step 1525800 | Avg Loss: 0.0161 | Grad Norm: 0.00856378\n",
      "Epoch 2 | Step 1525900 | Avg Loss: 0.0159 | Grad Norm: 0.00855758\n",
      "Epoch 2 | Step 1526000 | Avg Loss: 0.0161 | Grad Norm: 0.00887877\n",
      "Epoch 2 | Step 1526100 | Avg Loss: 0.0160 | Grad Norm: 0.00834349\n",
      "Epoch 2 | Step 1526200 | Avg Loss: 0.0161 | Grad Norm: 0.00961065\n",
      "Epoch 2 | Step 1526300 | Avg Loss: 0.0159 | Grad Norm: 0.00928515\n",
      "Epoch 2 | Step 1526400 | Avg Loss: 0.0156 | Grad Norm: 0.00835824\n",
      "Epoch 2 | Step 1526500 | Avg Loss: 0.0158 | Grad Norm: 0.00859010\n",
      "Epoch 2 | Step 1526600 | Avg Loss: 0.0155 | Grad Norm: 0.00981793\n",
      "Epoch 2 | Step 1526700 | Avg Loss: 0.0153 | Grad Norm: 0.00845175\n",
      "Epoch 2 | Step 1526800 | Avg Loss: 0.0151 | Grad Norm: 0.00880191\n",
      "Epoch 2 | Step 1526900 | Avg Loss: 0.0150 | Grad Norm: 0.00815445\n",
      "Epoch 2 | Step 1527000 | Avg Loss: 0.0148 | Grad Norm: 0.00937174\n",
      "Epoch 2 | Step 1527100 | Avg Loss: 0.0149 | Grad Norm: 0.01052225\n",
      "Epoch 2 | Step 1527200 | Avg Loss: 0.0154 | Grad Norm: 0.00881096\n",
      "Epoch 2 | Step 1527300 | Avg Loss: 0.0155 | Grad Norm: 0.00910576\n",
      "Epoch 2 | Step 1527400 | Avg Loss: 0.0157 | Grad Norm: 0.00815695\n",
      "Epoch 2 | Step 1527500 | Avg Loss: 0.0159 | Grad Norm: 0.01075115\n",
      "Epoch 2 | Step 1527600 | Avg Loss: 0.0160 | Grad Norm: 0.00768019\n",
      "Epoch 2 | Step 1527700 | Avg Loss: 0.0160 | Grad Norm: 0.00909909\n",
      "Epoch 2 | Step 1527800 | Avg Loss: 0.0158 | Grad Norm: 0.00841584\n",
      "Epoch 2 | Step 1527900 | Avg Loss: 0.0156 | Grad Norm: 0.01009543\n",
      "Epoch 2 | Step 1528000 | Avg Loss: 0.0158 | Grad Norm: 0.01030669\n",
      "Epoch 2 | Step 1528100 | Avg Loss: 0.0160 | Grad Norm: 0.00825845\n",
      "Epoch 2 | Step 1528200 | Avg Loss: 0.0158 | Grad Norm: 0.00949196\n",
      "Epoch 2 | Step 1528300 | Avg Loss: 0.0158 | Grad Norm: 0.00836198\n",
      "Epoch 2 | Step 1528400 | Avg Loss: 0.0159 | Grad Norm: 0.00828666\n",
      "Epoch 2 | Step 1528500 | Avg Loss: 0.0159 | Grad Norm: 0.00742720\n",
      "Epoch 2 | Step 1528600 | Avg Loss: 0.0158 | Grad Norm: 0.00962703\n",
      "Epoch 2 | Step 1528700 | Avg Loss: 0.0156 | Grad Norm: 0.00853663\n",
      "Epoch 2 | Step 1528800 | Avg Loss: 0.0159 | Grad Norm: 0.00853630\n",
      "Epoch 2 | Step 1528900 | Avg Loss: 0.0159 | Grad Norm: 0.00879236\n",
      "Epoch 2 | Step 1529000 | Avg Loss: 0.0159 | Grad Norm: 0.00964240\n",
      "Epoch 2 | Step 1529100 | Avg Loss: 0.0158 | Grad Norm: 0.00889481\n",
      "Epoch 2 | Step 1529200 | Avg Loss: 0.0157 | Grad Norm: 0.00827721\n",
      "Epoch 2 | Step 1529300 | Avg Loss: 0.0162 | Grad Norm: 0.00962548\n",
      "Epoch 2 | Step 1529400 | Avg Loss: 0.0162 | Grad Norm: 0.00888493\n",
      "Epoch 2 | Step 1529500 | Avg Loss: 0.0162 | Grad Norm: 0.01037515\n",
      "Epoch 2 | Step 1529600 | Avg Loss: 0.0158 | Grad Norm: 0.00874375\n",
      "Epoch 2 | Step 1529700 | Avg Loss: 0.0156 | Grad Norm: 0.00910018\n",
      "Epoch 2 | Step 1529800 | Avg Loss: 0.0156 | Grad Norm: 0.00891772\n",
      "Epoch 2 | Step 1529900 | Avg Loss: 0.0157 | Grad Norm: 0.00871171\n",
      "Epoch 2 | Step 1530000 | Avg Loss: 0.0158 | Grad Norm: 0.00896017\n",
      "Epoch 2 | Step 1530100 | Avg Loss: 0.0161 | Grad Norm: 0.00927902\n",
      "Epoch 2 | Step 1530200 | Avg Loss: 0.0157 | Grad Norm: 0.00882107\n",
      "Epoch 2 | Step 1530300 | Avg Loss: 0.0155 | Grad Norm: 0.00922191\n",
      "Epoch 2 | Step 1530400 | Avg Loss: 0.0155 | Grad Norm: 0.01004647\n",
      "Epoch 2 | Step 1530500 | Avg Loss: 0.0157 | Grad Norm: 0.00893768\n",
      "Epoch 2 | Step 1530600 | Avg Loss: 0.0156 | Grad Norm: 0.00969504\n",
      "Epoch 2 | Step 1530700 | Avg Loss: 0.0159 | Grad Norm: 0.00789387\n",
      "Epoch 2 | Step 1530800 | Avg Loss: 0.0162 | Grad Norm: 0.01042416\n",
      "Epoch 2 | Step 1530900 | Avg Loss: 0.0161 | Grad Norm: 0.00997026\n",
      "Epoch 2 | Step 1531000 | Avg Loss: 0.0161 | Grad Norm: 0.00795559\n",
      "Epoch 2 | Step 1531100 | Avg Loss: 0.0156 | Grad Norm: 0.00848858\n",
      "Epoch 2 | Step 1531200 | Avg Loss: 0.0155 | Grad Norm: 0.00880306\n",
      "Epoch 2 | Step 1531300 | Avg Loss: 0.0153 | Grad Norm: 0.01031008\n",
      "Epoch 2 | Step 1531400 | Avg Loss: 0.0158 | Grad Norm: 0.00929938\n",
      "Epoch 2 | Step 1531500 | Avg Loss: 0.0154 | Grad Norm: 0.00989841\n",
      "Epoch 2 | Step 1531600 | Avg Loss: 0.0154 | Grad Norm: 0.00857230\n",
      "Epoch 2 | Step 1531700 | Avg Loss: 0.0159 | Grad Norm: 0.01180254\n",
      "Epoch 2 | Step 1531800 | Avg Loss: 0.0162 | Grad Norm: 0.01088201\n",
      "Epoch 2 | Step 1531900 | Avg Loss: 0.0161 | Grad Norm: 0.00887281\n",
      "Epoch 2 | Step 1532000 | Avg Loss: 0.0158 | Grad Norm: 0.00930737\n",
      "Epoch 2 | Step 1532100 | Avg Loss: 0.0158 | Grad Norm: 0.00852538\n",
      "Epoch 2 | Step 1532200 | Avg Loss: 0.0162 | Grad Norm: 0.00994356\n",
      "Epoch 2 | Step 1532300 | Avg Loss: 0.0159 | Grad Norm: 0.00906968\n",
      "Epoch 2 | Step 1532400 | Avg Loss: 0.0156 | Grad Norm: 0.01022078\n",
      "Epoch 2 | Step 1532500 | Avg Loss: 0.0155 | Grad Norm: 0.00815208\n",
      "Epoch 2 | Step 1532600 | Avg Loss: 0.0156 | Grad Norm: 0.00931272\n",
      "Epoch 2 | Step 1532700 | Avg Loss: 0.0156 | Grad Norm: 0.00876692\n",
      "Epoch 2 | Step 1532800 | Avg Loss: 0.0156 | Grad Norm: 0.00823529\n",
      "Epoch 2 | Step 1532900 | Avg Loss: 0.0158 | Grad Norm: 0.00793926\n",
      "Epoch 2 | Step 1533000 | Avg Loss: 0.0160 | Grad Norm: 0.00878185\n",
      "Epoch 2 | Step 1533100 | Avg Loss: 0.0164 | Grad Norm: 0.00845362\n",
      "Epoch 2 | Step 1533200 | Avg Loss: 0.0166 | Grad Norm: 0.00941650\n",
      "Epoch 2 | Step 1533300 | Avg Loss: 0.0167 | Grad Norm: 0.00906469\n",
      "Epoch 2 | Step 1533400 | Avg Loss: 0.0167 | Grad Norm: 0.00809717\n",
      "Epoch 2 | Step 1533500 | Avg Loss: 0.0167 | Grad Norm: 0.00915194\n",
      "Epoch 2 | Step 1533600 | Avg Loss: 0.0162 | Grad Norm: 0.00828914\n",
      "Epoch 2 | Step 1533700 | Avg Loss: 0.0163 | Grad Norm: 0.00922185\n",
      "Epoch 2 | Step 1533800 | Avg Loss: 0.0166 | Grad Norm: 0.01003896\n",
      "Epoch 2 | Step 1533900 | Avg Loss: 0.0162 | Grad Norm: 0.00827858\n",
      "Epoch 2 | Step 1534000 | Avg Loss: 0.0162 | Grad Norm: 0.00820299\n",
      "Epoch 2 | Step 1534100 | Avg Loss: 0.0159 | Grad Norm: 0.00917116\n",
      "Epoch 2 | Step 1534200 | Avg Loss: 0.0157 | Grad Norm: 0.00815178\n",
      "Epoch 2 | Step 1534300 | Avg Loss: 0.0156 | Grad Norm: 0.00906391\n",
      "Epoch 2 | Step 1534400 | Avg Loss: 0.0159 | Grad Norm: 0.00908913\n",
      "Epoch 2 | Step 1534500 | Avg Loss: 0.0162 | Grad Norm: 0.01062764\n",
      "Epoch 2 | Step 1534600 | Avg Loss: 0.0158 | Grad Norm: 0.00888512\n",
      "Epoch 2 | Step 1534700 | Avg Loss: 0.0161 | Grad Norm: 0.00887146\n",
      "Epoch 2 | Step 1534800 | Avg Loss: 0.0159 | Grad Norm: 0.00841524\n",
      "Epoch 2 | Step 1534900 | Avg Loss: 0.0160 | Grad Norm: 0.00870998\n",
      "Epoch 2 | Step 1535000 | Avg Loss: 0.0155 | Grad Norm: 0.00813829\n",
      "Epoch 2 | Step 1535100 | Avg Loss: 0.0154 | Grad Norm: 0.02236998\n",
      "Epoch 2 | Step 1535200 | Avg Loss: 0.0156 | Grad Norm: 0.00871804\n",
      "Epoch 2 | Step 1535300 | Avg Loss: 0.0157 | Grad Norm: 0.00912882\n",
      "Epoch 2 | Step 1535400 | Avg Loss: 0.0156 | Grad Norm: 0.00905842\n",
      "Epoch 2 | Step 1535500 | Avg Loss: 0.0162 | Grad Norm: 0.00892866\n",
      "Epoch 2 | Step 1535600 | Avg Loss: 0.0160 | Grad Norm: 0.00829961\n",
      "Epoch 2 | Step 1535700 | Avg Loss: 0.0158 | Grad Norm: 0.00899943\n",
      "Epoch 2 | Step 1535800 | Avg Loss: 0.0162 | Grad Norm: 0.00977912\n",
      "Epoch 2 | Step 1535900 | Avg Loss: 0.0158 | Grad Norm: 0.00956707\n",
      "Epoch 2 | Step 1536000 | Avg Loss: 0.0160 | Grad Norm: 0.01066395\n",
      "Epoch 2 | Step 1536100 | Avg Loss: 0.0159 | Grad Norm: 0.00768061\n",
      "Epoch 2 | Step 1536200 | Avg Loss: 0.0159 | Grad Norm: 0.00817058\n",
      "Epoch 2 | Step 1536300 | Avg Loss: 0.0161 | Grad Norm: 0.00892646\n",
      "Epoch 2 | Step 1536400 | Avg Loss: 0.0161 | Grad Norm: 0.00799644\n",
      "Epoch 2 | Step 1536500 | Avg Loss: 0.0160 | Grad Norm: 0.00961899\n",
      "Epoch 2 | Step 1536600 | Avg Loss: 0.0157 | Grad Norm: 0.00819452\n",
      "Epoch 2 | Step 1536700 | Avg Loss: 0.0155 | Grad Norm: 0.00976467\n",
      "Epoch 2 | Step 1536800 | Avg Loss: 0.0153 | Grad Norm: 0.00836290\n",
      "Epoch 2 | Step 1536900 | Avg Loss: 0.0154 | Grad Norm: 0.01109866\n",
      "Epoch 2 | Step 1537000 | Avg Loss: 0.0157 | Grad Norm: 0.00993533\n",
      "Epoch 2 | Step 1537100 | Avg Loss: 0.0158 | Grad Norm: 0.01022905\n",
      "Epoch 2 | Step 1537200 | Avg Loss: 0.0159 | Grad Norm: 0.00846946\n",
      "Epoch 2 | Step 1537300 | Avg Loss: 0.0155 | Grad Norm: 0.00926645\n",
      "Epoch 2 | Step 1537400 | Avg Loss: 0.0161 | Grad Norm: 0.00830158\n",
      "Epoch 2 | Step 1537500 | Avg Loss: 0.0161 | Grad Norm: 0.00996695\n",
      "Epoch 2 | Step 1537600 | Avg Loss: 0.0161 | Grad Norm: 0.01080643\n",
      "Epoch 2 | Step 1537700 | Avg Loss: 0.0160 | Grad Norm: 0.00956193\n",
      "Epoch 2 | Step 1537800 | Avg Loss: 0.0160 | Grad Norm: 0.00888724\n",
      "Epoch 2 | Step 1537900 | Avg Loss: 0.0162 | Grad Norm: 0.00849465\n",
      "Epoch 2 | Step 1538000 | Avg Loss: 0.0160 | Grad Norm: 0.01020135\n",
      "Epoch 2 | Step 1538100 | Avg Loss: 0.0157 | Grad Norm: 0.01026141\n",
      "Epoch 2 | Step 1538200 | Avg Loss: 0.0159 | Grad Norm: 0.00847159\n",
      "Epoch 2 | Step 1538300 | Avg Loss: 0.0160 | Grad Norm: 0.01002116\n",
      "Epoch 2 | Step 1538400 | Avg Loss: 0.0157 | Grad Norm: 0.00895300\n",
      "Epoch 2 | Step 1538500 | Avg Loss: 0.0158 | Grad Norm: 0.00836861\n",
      "Epoch 2 | Step 1538600 | Avg Loss: 0.0156 | Grad Norm: 0.01107761\n",
      "Epoch 2 | Step 1538700 | Avg Loss: 0.0153 | Grad Norm: 0.00845050\n",
      "Epoch 2 | Step 1538800 | Avg Loss: 0.0155 | Grad Norm: 0.00767806\n",
      "Epoch 2 | Step 1538900 | Avg Loss: 0.0156 | Grad Norm: 0.00956670\n",
      "Epoch 2 | Step 1539000 | Avg Loss: 0.0154 | Grad Norm: 0.00872965\n",
      "Epoch 2 | Step 1539100 | Avg Loss: 0.0155 | Grad Norm: 0.00769028\n",
      "Epoch 2 | Step 1539200 | Avg Loss: 0.0155 | Grad Norm: 0.01160995\n",
      "Epoch 2 | Step 1539300 | Avg Loss: 0.0153 | Grad Norm: 0.00931839\n",
      "Epoch 2 | Step 1539400 | Avg Loss: 0.0153 | Grad Norm: 0.00802261\n",
      "Epoch 2 | Step 1539500 | Avg Loss: 0.0150 | Grad Norm: 0.00892919\n",
      "Epoch 2 | Step 1539600 | Avg Loss: 0.0155 | Grad Norm: 0.00818562\n",
      "Epoch 2 | Step 1539700 | Avg Loss: 0.0157 | Grad Norm: 0.00814306\n",
      "Epoch 2 | Step 1539800 | Avg Loss: 0.0156 | Grad Norm: 0.01088127\n",
      "Epoch 2 | Step 1539900 | Avg Loss: 0.0157 | Grad Norm: 0.01097313\n",
      "Epoch 2 | Step 1540000 | Avg Loss: 0.0160 | Grad Norm: 0.00928159\n",
      "Epoch 2 | Step 1540100 | Avg Loss: 0.0160 | Grad Norm: 0.00994136\n",
      "Epoch 2 | Step 1540200 | Avg Loss: 0.0160 | Grad Norm: 0.00907096\n",
      "Epoch 2 | Step 1540300 | Avg Loss: 0.0159 | Grad Norm: 0.00970096\n",
      "Epoch 2 | Step 1540400 | Avg Loss: 0.0155 | Grad Norm: 0.01212635\n",
      "Epoch 2 | Step 1540500 | Avg Loss: 0.0156 | Grad Norm: 0.00921312\n",
      "Epoch 2 | Step 1540600 | Avg Loss: 0.0157 | Grad Norm: 0.00897554\n",
      "Epoch 2 | Step 1540700 | Avg Loss: 0.0160 | Grad Norm: 0.00884705\n",
      "Epoch 2 | Step 1540800 | Avg Loss: 0.0159 | Grad Norm: 0.01090279\n",
      "Epoch 2 | Step 1540900 | Avg Loss: 0.0156 | Grad Norm: 0.00979469\n",
      "Epoch 2 | Step 1541000 | Avg Loss: 0.0154 | Grad Norm: 0.00803209\n",
      "Epoch 2 | Step 1541100 | Avg Loss: 0.0153 | Grad Norm: 0.00843111\n",
      "Epoch 2 | Step 1541200 | Avg Loss: 0.0154 | Grad Norm: 0.00858921\n",
      "Epoch 2 | Step 1541300 | Avg Loss: 0.0153 | Grad Norm: 0.00853216\n",
      "Epoch 2 | Step 1541400 | Avg Loss: 0.0153 | Grad Norm: 0.00851057\n",
      "Epoch 2 | Step 1541500 | Avg Loss: 0.0152 | Grad Norm: 0.00880653\n",
      "Epoch 2 | Step 1541600 | Avg Loss: 0.0154 | Grad Norm: 0.00945227\n",
      "Epoch 2 | Step 1541700 | Avg Loss: 0.0157 | Grad Norm: 0.00923668\n",
      "Epoch 2 | Step 1541800 | Avg Loss: 0.0158 | Grad Norm: 0.01151115\n",
      "Epoch 2 | Step 1541900 | Avg Loss: 0.0159 | Grad Norm: 0.00995977\n",
      "Epoch 2 | Step 1542000 | Avg Loss: 0.0160 | Grad Norm: 0.00923579\n",
      "Epoch 2 | Step 1542100 | Avg Loss: 0.0156 | Grad Norm: 0.00844342\n",
      "Epoch 2 | Step 1542200 | Avg Loss: 0.0154 | Grad Norm: 0.00868033\n",
      "Epoch 2 | Step 1542300 | Avg Loss: 0.0154 | Grad Norm: 0.00911080\n",
      "Epoch 2 | Step 1542400 | Avg Loss: 0.0157 | Grad Norm: 0.00797669\n",
      "Epoch 2 | Step 1542500 | Avg Loss: 0.0160 | Grad Norm: 0.00826566\n",
      "Epoch 2 | Step 1542600 | Avg Loss: 0.0159 | Grad Norm: 0.00859357\n",
      "Epoch 2 | Step 1542700 | Avg Loss: 0.0160 | Grad Norm: 0.00887339\n",
      "Epoch 2 | Step 1542800 | Avg Loss: 0.0157 | Grad Norm: 0.00853869\n",
      "Epoch 2 | Step 1542900 | Avg Loss: 0.0160 | Grad Norm: 0.01033867\n",
      "Epoch 2 | Step 1543000 | Avg Loss: 0.0160 | Grad Norm: 0.00797307\n",
      "Epoch 2 | Step 1543100 | Avg Loss: 0.0159 | Grad Norm: 0.00842870\n",
      "Epoch 2 | Step 1543200 | Avg Loss: 0.0160 | Grad Norm: 0.00885248\n",
      "Epoch 2 | Step 1543300 | Avg Loss: 0.0158 | Grad Norm: 0.00866059\n",
      "Epoch 2 | Step 1543400 | Avg Loss: 0.0158 | Grad Norm: 0.01108520\n",
      "Epoch 2 | Step 1543500 | Avg Loss: 0.0159 | Grad Norm: 0.00842264\n",
      "Epoch 2 | Step 1543600 | Avg Loss: 0.0157 | Grad Norm: 0.00826989\n",
      "Epoch 2 | Step 1543700 | Avg Loss: 0.0159 | Grad Norm: 0.00822029\n",
      "Epoch 2 | Step 1543800 | Avg Loss: 0.0160 | Grad Norm: 0.00891203\n",
      "Epoch 2 | Step 1543900 | Avg Loss: 0.0159 | Grad Norm: 0.01046462\n",
      "Epoch 2 | Step 1544000 | Avg Loss: 0.0161 | Grad Norm: 0.00828479\n",
      "Epoch 2 | Step 1544100 | Avg Loss: 0.0163 | Grad Norm: 0.00877345\n",
      "Epoch 2 | Step 1544200 | Avg Loss: 0.0162 | Grad Norm: 0.00905973\n",
      "Epoch 2 | Step 1544300 | Avg Loss: 0.0159 | Grad Norm: 0.00808067\n",
      "Epoch 2 | Step 1544400 | Avg Loss: 0.0161 | Grad Norm: 0.00820153\n",
      "Epoch 2 | Step 1544500 | Avg Loss: 0.0161 | Grad Norm: 0.01005994\n",
      "Epoch 2 | Step 1544600 | Avg Loss: 0.0162 | Grad Norm: 0.00856138\n",
      "Epoch 2 | Step 1544700 | Avg Loss: 0.0161 | Grad Norm: 0.01031036\n",
      "Epoch 2 | Step 1544800 | Avg Loss: 0.0160 | Grad Norm: 0.00793009\n",
      "Epoch 2 | Step 1544900 | Avg Loss: 0.0157 | Grad Norm: 0.00813540\n",
      "Epoch 2 | Step 1545000 | Avg Loss: 0.0157 | Grad Norm: 0.00806963\n",
      "Epoch 2 | Step 1545100 | Avg Loss: 0.0156 | Grad Norm: 0.00940201\n",
      "Epoch 2 | Step 1545200 | Avg Loss: 0.0157 | Grad Norm: 0.00928186\n",
      "Epoch 2 | Step 1545300 | Avg Loss: 0.0152 | Grad Norm: 0.00880659\n",
      "Epoch 2 | Step 1545400 | Avg Loss: 0.0151 | Grad Norm: 0.00699056\n",
      "Epoch 2 | Step 1545500 | Avg Loss: 0.0151 | Grad Norm: 0.00949399\n",
      "Epoch 2 | Step 1545600 | Avg Loss: 0.0150 | Grad Norm: 0.00828383\n",
      "Epoch 2 | Step 1545700 | Avg Loss: 0.0152 | Grad Norm: 0.00951596\n",
      "Epoch 2 | Step 1545800 | Avg Loss: 0.0155 | Grad Norm: 0.00803861\n",
      "Epoch 2 | Step 1545900 | Avg Loss: 0.0154 | Grad Norm: 0.00953349\n",
      "Epoch 2 | Step 1546000 | Avg Loss: 0.0155 | Grad Norm: 0.00903207\n",
      "Epoch 2 | Step 1546100 | Avg Loss: 0.0151 | Grad Norm: 0.00803060\n",
      "Epoch 2 | Step 1546200 | Avg Loss: 0.0154 | Grad Norm: 0.00803869\n",
      "Epoch 2 | Step 1546300 | Avg Loss: 0.0155 | Grad Norm: 0.00827233\n",
      "Epoch 2 | Step 1546400 | Avg Loss: 0.0154 | Grad Norm: 0.00991483\n",
      "Epoch 2 | Step 1546500 | Avg Loss: 0.0154 | Grad Norm: 0.00959025\n",
      "Epoch 2 | Step 1546600 | Avg Loss: 0.0154 | Grad Norm: 0.00859051\n",
      "Epoch 2 | Step 1546700 | Avg Loss: 0.0158 | Grad Norm: 0.00800151\n",
      "Epoch 2 | Step 1546800 | Avg Loss: 0.0157 | Grad Norm: 0.00929010\n",
      "Epoch 2 | Step 1546900 | Avg Loss: 0.0154 | Grad Norm: 0.00793531\n",
      "Epoch 2 | Step 1547000 | Avg Loss: 0.0154 | Grad Norm: 0.00822852\n",
      "Epoch 2 | Step 1547100 | Avg Loss: 0.0151 | Grad Norm: 0.00825329\n",
      "Epoch 2 | Step 1547200 | Avg Loss: 0.0147 | Grad Norm: 0.01026323\n",
      "Epoch 2 | Step 1547300 | Avg Loss: 0.0151 | Grad Norm: 0.00881585\n",
      "Epoch 2 | Step 1547400 | Avg Loss: 0.0156 | Grad Norm: 0.00911451\n",
      "Epoch 2 | Step 1547500 | Avg Loss: 0.0159 | Grad Norm: 0.00846538\n",
      "Epoch 2 | Step 1547600 | Avg Loss: 0.0159 | Grad Norm: 0.00781528\n",
      "Epoch 2 | Step 1547700 | Avg Loss: 0.0159 | Grad Norm: 0.00914056\n",
      "Epoch 2 | Step 1547800 | Avg Loss: 0.0160 | Grad Norm: 0.00866847\n",
      "Epoch 2 | Step 1547900 | Avg Loss: 0.0161 | Grad Norm: 0.00795020\n",
      "Epoch 2 | Step 1548000 | Avg Loss: 0.0154 | Grad Norm: 0.01038206\n",
      "Epoch 2 | Step 1548100 | Avg Loss: 0.0156 | Grad Norm: 0.00891619\n",
      "Epoch 2 | Step 1548200 | Avg Loss: 0.0157 | Grad Norm: 0.00822690\n",
      "Epoch 2 | Step 1548300 | Avg Loss: 0.0155 | Grad Norm: 0.00840720\n",
      "Epoch 2 | Step 1548400 | Avg Loss: 0.0155 | Grad Norm: 0.00896516\n",
      "Epoch 2 | Step 1548500 | Avg Loss: 0.0154 | Grad Norm: 0.00934313\n",
      "Epoch 2 | Step 1548600 | Avg Loss: 0.0156 | Grad Norm: 0.00880042\n",
      "Epoch 2 | Step 1548700 | Avg Loss: 0.0155 | Grad Norm: 0.00975354\n",
      "Epoch 2 | Step 1548800 | Avg Loss: 0.0157 | Grad Norm: 0.00961545\n",
      "Epoch 2 | Step 1548900 | Avg Loss: 0.0159 | Grad Norm: 0.00869298\n",
      "Epoch 2 | Step 1549000 | Avg Loss: 0.0155 | Grad Norm: 0.00990688\n",
      "Epoch 2 | Step 1549100 | Avg Loss: 0.0155 | Grad Norm: 0.01060857\n",
      "Epoch 2 | Step 1549200 | Avg Loss: 0.0151 | Grad Norm: 0.00855089\n",
      "Epoch 2 | Step 1549300 | Avg Loss: 0.0151 | Grad Norm: 0.00778752\n",
      "Epoch 2 | Step 1549400 | Avg Loss: 0.0151 | Grad Norm: 0.00846787\n",
      "Epoch 2 | Step 1549500 | Avg Loss: 0.0157 | Grad Norm: 0.00868472\n",
      "Epoch 2 | Step 1549600 | Avg Loss: 0.0157 | Grad Norm: 0.00851509\n",
      "Epoch 2 | Step 1549700 | Avg Loss: 0.0160 | Grad Norm: 0.00969788\n",
      "Epoch 2 | Step 1549800 | Avg Loss: 0.0157 | Grad Norm: 0.00925048\n",
      "Epoch 2 | Step 1549900 | Avg Loss: 0.0161 | Grad Norm: 0.00972830\n",
      "Epoch 2 | Step 1550000 | Avg Loss: 0.0163 | Grad Norm: 0.00905170\n",
      "Epoch 2 | Step 1550100 | Avg Loss: 0.0160 | Grad Norm: 0.00936808\n",
      "Epoch 2 | Step 1550200 | Avg Loss: 0.0161 | Grad Norm: 0.00865047\n",
      "Epoch 2 | Step 1550300 | Avg Loss: 0.0160 | Grad Norm: 0.00836910\n",
      "Epoch 2 | Step 1550400 | Avg Loss: 0.0156 | Grad Norm: 0.00931872\n",
      "Epoch 2 | Step 1550500 | Avg Loss: 0.0156 | Grad Norm: 0.00908615\n",
      "Epoch 2 | Step 1550600 | Avg Loss: 0.0155 | Grad Norm: 0.00890797\n",
      "Epoch 2 | Step 1550700 | Avg Loss: 0.0156 | Grad Norm: 0.01082521\n",
      "Epoch 2 | Step 1550800 | Avg Loss: 0.0155 | Grad Norm: 0.00866226\n",
      "Epoch 2 | Step 1550900 | Avg Loss: 0.0155 | Grad Norm: 0.00873144\n",
      "Epoch 2 | Step 1551000 | Avg Loss: 0.0156 | Grad Norm: 0.00795490\n",
      "Epoch 2 | Step 1551100 | Avg Loss: 0.0157 | Grad Norm: 0.00832381\n",
      "Epoch 2 | Step 1551200 | Avg Loss: 0.0158 | Grad Norm: 0.00903821\n",
      "Epoch 2 | Step 1551300 | Avg Loss: 0.0158 | Grad Norm: 0.01142094\n",
      "Epoch 2 | Step 1551400 | Avg Loss: 0.0157 | Grad Norm: 0.00800172\n",
      "Epoch 2 | Step 1551500 | Avg Loss: 0.0158 | Grad Norm: 0.00807351\n",
      "Epoch 2 | Step 1551600 | Avg Loss: 0.0159 | Grad Norm: 0.01034786\n",
      "Epoch 2 | Step 1551700 | Avg Loss: 0.0159 | Grad Norm: 0.00888791\n",
      "Epoch 2 | Step 1551800 | Avg Loss: 0.0159 | Grad Norm: 0.00861097\n",
      "Epoch 2 | Step 1551900 | Avg Loss: 0.0159 | Grad Norm: 0.01061393\n",
      "Epoch 2 | Step 1552000 | Avg Loss: 0.0158 | Grad Norm: 0.00830608\n",
      "Epoch 2 | Step 1552100 | Avg Loss: 0.0159 | Grad Norm: 0.00918307\n",
      "Epoch 2 | Step 1552200 | Avg Loss: 0.0157 | Grad Norm: 0.00878826\n",
      "Epoch 2 | Step 1552300 | Avg Loss: 0.0155 | Grad Norm: 0.01025847\n",
      "Epoch 2 | Step 1552400 | Avg Loss: 0.0154 | Grad Norm: 0.00944082\n",
      "Epoch 2 | Step 1552500 | Avg Loss: 0.0155 | Grad Norm: 0.01037003\n",
      "Epoch 2 | Step 1552600 | Avg Loss: 0.0157 | Grad Norm: 0.01225928\n",
      "Epoch 2 | Step 1552700 | Avg Loss: 0.0155 | Grad Norm: 0.00939757\n",
      "Epoch 2 | Step 1552800 | Avg Loss: 0.0155 | Grad Norm: 0.00808955\n",
      "Epoch 2 | Step 1552900 | Avg Loss: 0.0152 | Grad Norm: 0.00939473\n",
      "Epoch 2 | Step 1553000 | Avg Loss: 0.0151 | Grad Norm: 0.01135831\n",
      "Epoch 2 | Step 1553100 | Avg Loss: 0.0154 | Grad Norm: 0.00991018\n",
      "Epoch 2 | Step 1553200 | Avg Loss: 0.0150 | Grad Norm: 0.00801738\n",
      "Epoch 2 | Step 1553300 | Avg Loss: 0.0149 | Grad Norm: 0.00852272\n",
      "Epoch 2 | Step 1553400 | Avg Loss: 0.0153 | Grad Norm: 0.00764778\n",
      "Epoch 2 | Step 1553500 | Avg Loss: 0.0155 | Grad Norm: 0.00792405\n",
      "Epoch 2 | Step 1553600 | Avg Loss: 0.0154 | Grad Norm: 0.00797789\n",
      "Epoch 2 | Step 1553700 | Avg Loss: 0.0156 | Grad Norm: 0.00802754\n",
      "Epoch 2 | Step 1553800 | Avg Loss: 0.0156 | Grad Norm: 0.00992463\n",
      "Epoch 2 | Step 1553900 | Avg Loss: 0.0156 | Grad Norm: 0.00843181\n",
      "Epoch 2 | Step 1554000 | Avg Loss: 0.0156 | Grad Norm: 0.01112098\n",
      "Epoch 2 | Step 1554100 | Avg Loss: 0.0156 | Grad Norm: 0.00890930\n",
      "Epoch 2 | Step 1554200 | Avg Loss: 0.0160 | Grad Norm: 0.00928964\n",
      "Epoch 2 | Step 1554300 | Avg Loss: 0.0158 | Grad Norm: 0.00788719\n",
      "Epoch 2 | Step 1554400 | Avg Loss: 0.0159 | Grad Norm: 0.00924652\n",
      "Epoch 2 | Step 1554500 | Avg Loss: 0.0158 | Grad Norm: 0.01294293\n",
      "Epoch 2 | Step 1554600 | Avg Loss: 0.0159 | Grad Norm: 0.00943917\n",
      "Epoch 2 | Step 1554700 | Avg Loss: 0.0159 | Grad Norm: 0.00852168\n",
      "Epoch 2 | Step 1554800 | Avg Loss: 0.0159 | Grad Norm: 0.00924949\n",
      "Epoch 2 | Step 1554900 | Avg Loss: 0.0162 | Grad Norm: 0.00851909\n",
      "Epoch 2 | Step 1555000 | Avg Loss: 0.0161 | Grad Norm: 0.00880153\n",
      "Epoch 2 | Step 1555100 | Avg Loss: 0.0158 | Grad Norm: 0.00942141\n",
      "Epoch 2 | Step 1555200 | Avg Loss: 0.0154 | Grad Norm: 0.00975503\n",
      "Epoch 2 | Step 1555300 | Avg Loss: 0.0159 | Grad Norm: 0.00943776\n",
      "Epoch 2 | Step 1555400 | Avg Loss: 0.0158 | Grad Norm: 0.01044236\n",
      "Epoch 2 | Step 1555500 | Avg Loss: 0.0158 | Grad Norm: 0.00888613\n",
      "Epoch 2 | Step 1555600 | Avg Loss: 0.0159 | Grad Norm: 0.00927099\n",
      "Epoch 2 | Step 1555700 | Avg Loss: 0.0160 | Grad Norm: 0.00924332\n",
      "Epoch 2 | Step 1555800 | Avg Loss: 0.0157 | Grad Norm: 0.00923555\n",
      "Epoch 2 | Step 1555900 | Avg Loss: 0.0155 | Grad Norm: 0.00949656\n",
      "Epoch 2 | Step 1556000 | Avg Loss: 0.0155 | Grad Norm: 0.00861347\n",
      "Epoch 2 | Step 1556100 | Avg Loss: 0.0158 | Grad Norm: 0.01056685\n",
      "Epoch 2 | Step 1556200 | Avg Loss: 0.0160 | Grad Norm: 0.00902955\n",
      "Epoch 2 | Step 1556300 | Avg Loss: 0.0159 | Grad Norm: 0.00955606\n",
      "Epoch 2 | Step 1556400 | Avg Loss: 0.0161 | Grad Norm: 0.00829771\n",
      "Epoch 2 | Step 1556500 | Avg Loss: 0.0159 | Grad Norm: 0.00806227\n",
      "Epoch 2 | Step 1556600 | Avg Loss: 0.0158 | Grad Norm: 0.00838302\n",
      "Epoch 2 | Step 1556700 | Avg Loss: 0.0158 | Grad Norm: 0.00834038\n",
      "Epoch 2 | Step 1556800 | Avg Loss: 0.0161 | Grad Norm: 0.00914598\n",
      "Epoch 2 | Step 1556900 | Avg Loss: 0.0162 | Grad Norm: 0.00913721\n",
      "Epoch 2 | Step 1557000 | Avg Loss: 0.0162 | Grad Norm: 0.00985611\n",
      "Epoch 2 | Step 1557100 | Avg Loss: 0.0157 | Grad Norm: 0.00865110\n",
      "Epoch 2 | Step 1557200 | Avg Loss: 0.0159 | Grad Norm: 0.01131820\n",
      "Epoch 2 | Step 1557300 | Avg Loss: 0.0158 | Grad Norm: 0.01174953\n",
      "Epoch 2 | Step 1557400 | Avg Loss: 0.0158 | Grad Norm: 0.01278130\n",
      "Epoch 2 | Step 1557500 | Avg Loss: 0.0154 | Grad Norm: 0.00887959\n",
      "Epoch 2 | Step 1557600 | Avg Loss: 0.0155 | Grad Norm: 0.00931024\n",
      "Epoch 2 | Step 1557700 | Avg Loss: 0.0159 | Grad Norm: 0.00916059\n",
      "Epoch 2 | Step 1557800 | Avg Loss: 0.0160 | Grad Norm: 0.00920396\n",
      "Epoch 2 | Step 1557900 | Avg Loss: 0.0157 | Grad Norm: 0.00974169\n",
      "Epoch 2 | Step 1558000 | Avg Loss: 0.0157 | Grad Norm: 0.00906350\n",
      "Epoch 2 | Step 1558100 | Avg Loss: 0.0156 | Grad Norm: 0.00998229\n",
      "Epoch 2 | Step 1558200 | Avg Loss: 0.0157 | Grad Norm: 0.01045824\n",
      "Epoch 2 | Step 1558300 | Avg Loss: 0.0158 | Grad Norm: 0.00992695\n",
      "Epoch 2 | Step 1558400 | Avg Loss: 0.0157 | Grad Norm: 0.00785621\n",
      "Epoch 2 | Step 1558500 | Avg Loss: 0.0156 | Grad Norm: 0.00884765\n",
      "Epoch 2 | Step 1558600 | Avg Loss: 0.0157 | Grad Norm: 0.00960988\n",
      "Epoch 2 | Step 1558700 | Avg Loss: 0.0154 | Grad Norm: 0.00801679\n",
      "Epoch 2 | Step 1558800 | Avg Loss: 0.0159 | Grad Norm: 0.00955217\n",
      "Epoch 2 | Step 1558900 | Avg Loss: 0.0156 | Grad Norm: 0.00828015\n",
      "Epoch 2 | Step 1559000 | Avg Loss: 0.0158 | Grad Norm: 0.00853369\n",
      "Epoch 2 | Step 1559100 | Avg Loss: 0.0153 | Grad Norm: 0.00860926\n",
      "Epoch 2 | Step 1559200 | Avg Loss: 0.0155 | Grad Norm: 0.00972567\n",
      "Epoch 2 | Step 1559300 | Avg Loss: 0.0152 | Grad Norm: 0.00920895\n",
      "Epoch 2 | Step 1559400 | Avg Loss: 0.0151 | Grad Norm: 0.00818147\n",
      "Epoch 2 | Step 1559500 | Avg Loss: 0.0153 | Grad Norm: 0.00793942\n",
      "Epoch 2 | Step 1559600 | Avg Loss: 0.0154 | Grad Norm: 0.00852870\n",
      "Epoch 2 | Step 1559700 | Avg Loss: 0.0154 | Grad Norm: 0.00906752\n",
      "Epoch 2 | Step 1559800 | Avg Loss: 0.0155 | Grad Norm: 0.00874213\n",
      "Epoch 2 | Step 1559900 | Avg Loss: 0.0151 | Grad Norm: 0.00830419\n",
      "Epoch 2 | Step 1560000 | Avg Loss: 0.0149 | Grad Norm: 0.00899558\n",
      "Epoch 2 | Step 1560100 | Avg Loss: 0.0153 | Grad Norm: 0.00850989\n",
      "Epoch 2 | Step 1560200 | Avg Loss: 0.0150 | Grad Norm: 0.00784056\n",
      "Epoch 2 | Step 1560300 | Avg Loss: 0.0153 | Grad Norm: 0.00779783\n",
      "Epoch 2 | Step 1560400 | Avg Loss: 0.0157 | Grad Norm: 0.00867521\n",
      "Epoch 2 | Step 1560500 | Avg Loss: 0.0155 | Grad Norm: 0.00868673\n",
      "Epoch 2 | Step 1560600 | Avg Loss: 0.0154 | Grad Norm: 0.00816227\n",
      "Epoch 2 | Step 1560700 | Avg Loss: 0.0156 | Grad Norm: 0.01034643\n",
      "Epoch 2 | Step 1560800 | Avg Loss: 0.0157 | Grad Norm: 0.00995828\n",
      "Epoch 2 | Step 1560900 | Avg Loss: 0.0156 | Grad Norm: 0.00920135\n",
      "Epoch 2 | Step 1561000 | Avg Loss: 0.0156 | Grad Norm: 0.00846944\n",
      "Epoch 2 | Step 1561100 | Avg Loss: 0.0155 | Grad Norm: 0.00855075\n",
      "Epoch 2 | Step 1561200 | Avg Loss: 0.0155 | Grad Norm: 0.00922413\n",
      "Epoch 2 | Step 1561300 | Avg Loss: 0.0154 | Grad Norm: 0.00818002\n",
      "Epoch 2 | Step 1561400 | Avg Loss: 0.0152 | Grad Norm: 0.00869237\n",
      "Epoch 2 | Step 1561500 | Avg Loss: 0.0154 | Grad Norm: 0.00810796\n",
      "Epoch 2 | Step 1561600 | Avg Loss: 0.0155 | Grad Norm: 0.00746048\n",
      "Epoch 2 | Step 1561700 | Avg Loss: 0.0154 | Grad Norm: 0.00837313\n",
      "Epoch 2 | Step 1561800 | Avg Loss: 0.0156 | Grad Norm: 0.00948163\n",
      "Epoch 2 | Step 1561900 | Avg Loss: 0.0153 | Grad Norm: 0.01129282\n",
      "Epoch 2 | Step 1562000 | Avg Loss: 0.0151 | Grad Norm: 0.00915724\n",
      "Epoch 2 | Step 1562100 | Avg Loss: 0.0149 | Grad Norm: 0.01040596\n",
      "Epoch 2 | Step 1562200 | Avg Loss: 0.0151 | Grad Norm: 0.00783580\n",
      "Epoch 2 | Step 1562300 | Avg Loss: 0.0150 | Grad Norm: 0.00746663\n",
      "Epoch 2 | Step 1562400 | Avg Loss: 0.0148 | Grad Norm: 0.00855716\n",
      "Epoch 2 | Step 1562500 | Avg Loss: 0.0146 | Grad Norm: 0.00832274\n",
      "Epoch 2, Loss: 0.0151\n",
      "Epoch 3 | Step 1562600 | Avg Loss: 0.0156 | Grad Norm: 0.00835219\n",
      "Epoch 3 | Step 1562700 | Avg Loss: 0.0157 | Grad Norm: 0.00966712\n",
      "Epoch 3 | Step 1562800 | Avg Loss: 0.0154 | Grad Norm: 0.00883094\n",
      "Epoch 3 | Step 1562900 | Avg Loss: 0.0151 | Grad Norm: 0.00924139\n",
      "Epoch 3 | Step 1563000 | Avg Loss: 0.0153 | Grad Norm: 0.00936912\n",
      "Epoch 3 | Step 1563100 | Avg Loss: 0.0155 | Grad Norm: 0.00889914\n",
      "Epoch 3 | Step 1563200 | Avg Loss: 0.0161 | Grad Norm: 0.00882480\n",
      "Epoch 3 | Step 1563300 | Avg Loss: 0.0161 | Grad Norm: 0.01156359\n",
      "Epoch 3 | Step 1563400 | Avg Loss: 0.0164 | Grad Norm: 0.00952891\n",
      "Epoch 3 | Step 1563500 | Avg Loss: 0.0160 | Grad Norm: 0.00968189\n",
      "Epoch 3 | Step 1563600 | Avg Loss: 0.0157 | Grad Norm: 0.00847996\n",
      "Epoch 3 | Step 1563700 | Avg Loss: 0.0156 | Grad Norm: 0.01054244\n",
      "Epoch 3 | Step 1563800 | Avg Loss: 0.0155 | Grad Norm: 0.00952537\n",
      "Epoch 3 | Step 1563900 | Avg Loss: 0.0153 | Grad Norm: 0.00996297\n",
      "Epoch 3 | Step 1564000 | Avg Loss: 0.0152 | Grad Norm: 0.01030229\n",
      "Epoch 3 | Step 1564100 | Avg Loss: 0.0150 | Grad Norm: 0.01083772\n",
      "Epoch 3 | Step 1564200 | Avg Loss: 0.0153 | Grad Norm: 0.00898352\n",
      "Epoch 3 | Step 1564300 | Avg Loss: 0.0152 | Grad Norm: 0.00830543\n",
      "Epoch 3 | Step 1564400 | Avg Loss: 0.0151 | Grad Norm: 0.00950820\n",
      "Epoch 3 | Step 1564500 | Avg Loss: 0.0154 | Grad Norm: 0.00901991\n",
      "Epoch 3 | Step 1564600 | Avg Loss: 0.0153 | Grad Norm: 0.01039087\n",
      "Epoch 3 | Step 1564700 | Avg Loss: 0.0153 | Grad Norm: 0.00821834\n",
      "Epoch 3 | Step 1564800 | Avg Loss: 0.0155 | Grad Norm: 0.00938393\n",
      "Epoch 3 | Step 1564900 | Avg Loss: 0.0156 | Grad Norm: 0.00891208\n",
      "Epoch 3 | Step 1565000 | Avg Loss: 0.0157 | Grad Norm: 0.01018219\n",
      "Epoch 3 | Step 1565100 | Avg Loss: 0.0159 | Grad Norm: 0.00983604\n",
      "Epoch 3 | Step 1565200 | Avg Loss: 0.0161 | Grad Norm: 0.00966694\n",
      "Epoch 3 | Step 1565300 | Avg Loss: 0.0156 | Grad Norm: 0.00948536\n",
      "Epoch 3 | Step 1565400 | Avg Loss: 0.0158 | Grad Norm: 0.00932334\n",
      "Epoch 3 | Step 1565500 | Avg Loss: 0.0159 | Grad Norm: 0.01109386\n",
      "Epoch 3 | Step 1565600 | Avg Loss: 0.0154 | Grad Norm: 0.00917168\n",
      "Epoch 3 | Step 1565700 | Avg Loss: 0.0152 | Grad Norm: 0.00755845\n",
      "Epoch 3 | Step 1565800 | Avg Loss: 0.0153 | Grad Norm: 0.00779701\n",
      "Epoch 3 | Step 1565900 | Avg Loss: 0.0156 | Grad Norm: 0.00772765\n",
      "Epoch 3 | Step 1566000 | Avg Loss: 0.0152 | Grad Norm: 0.00970401\n",
      "Epoch 3 | Step 1566100 | Avg Loss: 0.0152 | Grad Norm: 0.00877533\n",
      "Epoch 3 | Step 1566200 | Avg Loss: 0.0156 | Grad Norm: 0.00916291\n",
      "Epoch 3 | Step 1566300 | Avg Loss: 0.0159 | Grad Norm: 0.00856543\n",
      "Epoch 3 | Step 1566400 | Avg Loss: 0.0158 | Grad Norm: 0.00949123\n",
      "Epoch 3 | Step 1566500 | Avg Loss: 0.0156 | Grad Norm: 0.00832891\n",
      "Epoch 3 | Step 1566600 | Avg Loss: 0.0152 | Grad Norm: 0.00937818\n",
      "Epoch 3 | Step 1566700 | Avg Loss: 0.0151 | Grad Norm: 0.01184488\n",
      "Epoch 3 | Step 1566800 | Avg Loss: 0.0151 | Grad Norm: 0.00904590\n",
      "Epoch 3 | Step 1566900 | Avg Loss: 0.0154 | Grad Norm: 0.00811874\n",
      "Epoch 3 | Step 1567000 | Avg Loss: 0.0152 | Grad Norm: 0.00905500\n",
      "Epoch 3 | Step 1567100 | Avg Loss: 0.0150 | Grad Norm: 0.00945176\n",
      "Epoch 3 | Step 1567200 | Avg Loss: 0.0150 | Grad Norm: 0.00900277\n",
      "Epoch 3 | Step 1567300 | Avg Loss: 0.0150 | Grad Norm: 0.00820858\n",
      "Epoch 3 | Step 1567400 | Avg Loss: 0.0154 | Grad Norm: 0.00888964\n",
      "Epoch 3 | Step 1567500 | Avg Loss: 0.0153 | Grad Norm: 0.00887429\n",
      "Epoch 3 | Step 1567600 | Avg Loss: 0.0152 | Grad Norm: 0.00862979\n",
      "Epoch 3 | Step 1567700 | Avg Loss: 0.0152 | Grad Norm: 0.00848945\n",
      "Epoch 3 | Step 1567800 | Avg Loss: 0.0155 | Grad Norm: 0.00898437\n",
      "Epoch 3 | Step 1567900 | Avg Loss: 0.0158 | Grad Norm: 0.00962941\n",
      "Epoch 3 | Step 1568000 | Avg Loss: 0.0159 | Grad Norm: 0.00942732\n",
      "Epoch 3 | Step 1568100 | Avg Loss: 0.0156 | Grad Norm: 0.00784000\n",
      "Epoch 3 | Step 1568200 | Avg Loss: 0.0155 | Grad Norm: 0.01095577\n",
      "Epoch 3 | Step 1568300 | Avg Loss: 0.0157 | Grad Norm: 0.00892177\n",
      "Epoch 3 | Step 1568400 | Avg Loss: 0.0155 | Grad Norm: 0.00894042\n",
      "Epoch 3 | Step 1568500 | Avg Loss: 0.0158 | Grad Norm: 0.00862617\n",
      "Epoch 3 | Step 1568600 | Avg Loss: 0.0153 | Grad Norm: 0.00796255\n",
      "Epoch 3 | Step 1568700 | Avg Loss: 0.0156 | Grad Norm: 0.00878787\n",
      "Epoch 3 | Step 1568800 | Avg Loss: 0.0158 | Grad Norm: 0.00811505\n",
      "Epoch 3 | Step 1568900 | Avg Loss: 0.0157 | Grad Norm: 0.00876339\n",
      "Epoch 3 | Step 1569000 | Avg Loss: 0.0156 | Grad Norm: 0.00892392\n",
      "Epoch 3 | Step 1569100 | Avg Loss: 0.0159 | Grad Norm: 0.00891387\n",
      "Epoch 3 | Step 1569200 | Avg Loss: 0.0157 | Grad Norm: 0.00904646\n",
      "Epoch 3 | Step 1569300 | Avg Loss: 0.0157 | Grad Norm: 0.01014156\n",
      "Epoch 3 | Step 1569400 | Avg Loss: 0.0157 | Grad Norm: 0.00930941\n",
      "Epoch 3 | Step 1569500 | Avg Loss: 0.0156 | Grad Norm: 0.00913215\n",
      "Epoch 3 | Step 1569600 | Avg Loss: 0.0154 | Grad Norm: 0.00811673\n",
      "Epoch 3 | Step 1569700 | Avg Loss: 0.0151 | Grad Norm: 0.00802425\n",
      "Epoch 3 | Step 1569800 | Avg Loss: 0.0151 | Grad Norm: 0.01741480\n",
      "Epoch 3 | Step 1569900 | Avg Loss: 0.0153 | Grad Norm: 0.01049614\n",
      "Epoch 3 | Step 1570000 | Avg Loss: 0.0154 | Grad Norm: 0.00824033\n",
      "Epoch 3 | Step 1570100 | Avg Loss: 0.0156 | Grad Norm: 0.00924394\n",
      "Epoch 3 | Step 1570200 | Avg Loss: 0.0153 | Grad Norm: 0.00840267\n",
      "Epoch 3 | Step 1570300 | Avg Loss: 0.0155 | Grad Norm: 0.00943475\n",
      "Epoch 3 | Step 1570400 | Avg Loss: 0.0155 | Grad Norm: 0.00932471\n",
      "Epoch 3 | Step 1570500 | Avg Loss: 0.0159 | Grad Norm: 0.00805399\n",
      "Epoch 3 | Step 1570600 | Avg Loss: 0.0157 | Grad Norm: 0.01036985\n",
      "Epoch 3 | Step 1570700 | Avg Loss: 0.0156 | Grad Norm: 0.00825705\n",
      "Epoch 3 | Step 1570800 | Avg Loss: 0.0158 | Grad Norm: 0.00932691\n",
      "Epoch 3 | Step 1570900 | Avg Loss: 0.0155 | Grad Norm: 0.00986067\n",
      "Epoch 3 | Step 1571000 | Avg Loss: 0.0155 | Grad Norm: 0.00850161\n",
      "Epoch 3 | Step 1571100 | Avg Loss: 0.0158 | Grad Norm: 0.01000663\n",
      "Epoch 3 | Step 1571200 | Avg Loss: 0.0161 | Grad Norm: 0.00997404\n",
      "Epoch 3 | Step 1571300 | Avg Loss: 0.0160 | Grad Norm: 0.01280367\n",
      "Epoch 3 | Step 1571400 | Avg Loss: 0.0159 | Grad Norm: 0.00878578\n",
      "Epoch 3 | Step 1571500 | Avg Loss: 0.0157 | Grad Norm: 0.00931901\n",
      "Epoch 3 | Step 1571600 | Avg Loss: 0.0156 | Grad Norm: 0.01055691\n",
      "Epoch 3 | Step 1571700 | Avg Loss: 0.0156 | Grad Norm: 0.01091711\n",
      "Epoch 3 | Step 1571800 | Avg Loss: 0.0155 | Grad Norm: 0.00890934\n",
      "Epoch 3 | Step 1571900 | Avg Loss: 0.0153 | Grad Norm: 0.01124977\n",
      "Epoch 3 | Step 1572000 | Avg Loss: 0.0154 | Grad Norm: 0.00885096\n",
      "Epoch 3 | Step 1572100 | Avg Loss: 0.0154 | Grad Norm: 0.00875497\n",
      "Epoch 3 | Step 1572200 | Avg Loss: 0.0156 | Grad Norm: 0.00986561\n",
      "Epoch 3 | Step 1572300 | Avg Loss: 0.0157 | Grad Norm: 0.00858243\n",
      "Epoch 3 | Step 1572400 | Avg Loss: 0.0157 | Grad Norm: 0.00806883\n",
      "Epoch 3 | Step 1572500 | Avg Loss: 0.0157 | Grad Norm: 0.00965208\n",
      "Epoch 3 | Step 1572600 | Avg Loss: 0.0159 | Grad Norm: 0.00937824\n",
      "Epoch 3 | Step 1572700 | Avg Loss: 0.0160 | Grad Norm: 0.01021104\n",
      "Epoch 3 | Step 1572800 | Avg Loss: 0.0156 | Grad Norm: 0.00786383\n",
      "Epoch 3 | Step 1572900 | Avg Loss: 0.0158 | Grad Norm: 0.00923377\n",
      "Epoch 3 | Step 1573000 | Avg Loss: 0.0162 | Grad Norm: 0.00893223\n",
      "Epoch 3 | Step 1573100 | Avg Loss: 0.0163 | Grad Norm: 0.00789076\n",
      "Epoch 3 | Step 1573200 | Avg Loss: 0.0163 | Grad Norm: 0.00830880\n",
      "Epoch 3 | Step 1573300 | Avg Loss: 0.0164 | Grad Norm: 0.01052691\n",
      "Epoch 3 | Step 1573400 | Avg Loss: 0.0158 | Grad Norm: 0.00877146\n",
      "Epoch 3 | Step 1573500 | Avg Loss: 0.0159 | Grad Norm: 0.00938131\n",
      "Epoch 3 | Step 1573600 | Avg Loss: 0.0163 | Grad Norm: 0.00917365\n",
      "Epoch 3 | Step 1573700 | Avg Loss: 0.0159 | Grad Norm: 0.00818054\n",
      "Epoch 3 | Step 1573800 | Avg Loss: 0.0161 | Grad Norm: 0.00841421\n",
      "Epoch 3 | Step 1573900 | Avg Loss: 0.0158 | Grad Norm: 0.00795192\n",
      "Epoch 3 | Step 1574000 | Avg Loss: 0.0161 | Grad Norm: 0.00915849\n",
      "Epoch 3 | Step 1574100 | Avg Loss: 0.0158 | Grad Norm: 0.00830596\n",
      "Epoch 3 | Step 1574200 | Avg Loss: 0.0158 | Grad Norm: 0.01096234\n",
      "Epoch 3 | Step 1574300 | Avg Loss: 0.0156 | Grad Norm: 0.00806403\n",
      "Epoch 3 | Step 1574400 | Avg Loss: 0.0152 | Grad Norm: 0.00845787\n",
      "Epoch 3 | Step 1574500 | Avg Loss: 0.0152 | Grad Norm: 0.00842776\n",
      "Epoch 3 | Step 1574600 | Avg Loss: 0.0153 | Grad Norm: 0.00758059\n",
      "Epoch 3 | Step 1574700 | Avg Loss: 0.0151 | Grad Norm: 0.00863650\n",
      "Epoch 3 | Step 1574800 | Avg Loss: 0.0154 | Grad Norm: 0.00903690\n",
      "Epoch 3 | Step 1574900 | Avg Loss: 0.0158 | Grad Norm: 0.01157730\n",
      "Epoch 3 | Step 1575000 | Avg Loss: 0.0159 | Grad Norm: 0.00764639\n",
      "Epoch 3 | Step 1575100 | Avg Loss: 0.0157 | Grad Norm: 0.00823407\n",
      "Epoch 3 | Step 1575200 | Avg Loss: 0.0156 | Grad Norm: 0.00779154\n",
      "Epoch 3 | Step 1575300 | Avg Loss: 0.0156 | Grad Norm: 0.00775448\n",
      "Epoch 3 | Step 1575400 | Avg Loss: 0.0158 | Grad Norm: 0.00900121\n",
      "Epoch 3 | Step 1575500 | Avg Loss: 0.0156 | Grad Norm: 0.00828320\n",
      "Epoch 3 | Step 1575600 | Avg Loss: 0.0157 | Grad Norm: 0.00873600\n",
      "Epoch 3 | Step 1575700 | Avg Loss: 0.0157 | Grad Norm: 0.00928145\n",
      "Epoch 3 | Step 1575800 | Avg Loss: 0.0155 | Grad Norm: 0.00829135\n",
      "Epoch 3 | Step 1575900 | Avg Loss: 0.0157 | Grad Norm: 0.00905556\n",
      "Epoch 3 | Step 1576000 | Avg Loss: 0.0156 | Grad Norm: 0.00862208\n",
      "Epoch 3 | Step 1576100 | Avg Loss: 0.0156 | Grad Norm: 0.00842675\n",
      "Epoch 3 | Step 1576200 | Avg Loss: 0.0152 | Grad Norm: 0.00849730\n",
      "Epoch 3 | Step 1576300 | Avg Loss: 0.0154 | Grad Norm: 0.00858023\n",
      "Epoch 3 | Step 1576400 | Avg Loss: 0.0158 | Grad Norm: 0.00877118\n",
      "Epoch 3 | Step 1576500 | Avg Loss: 0.0158 | Grad Norm: 0.00944023\n",
      "Epoch 3 | Step 1576600 | Avg Loss: 0.0155 | Grad Norm: 0.00825494\n",
      "Epoch 3 | Step 1576700 | Avg Loss: 0.0156 | Grad Norm: 0.00967423\n",
      "Epoch 3 | Step 1576800 | Avg Loss: 0.0154 | Grad Norm: 0.01143466\n",
      "Epoch 3 | Step 1576900 | Avg Loss: 0.0153 | Grad Norm: 0.00981832\n",
      "Epoch 3 | Step 1577000 | Avg Loss: 0.0156 | Grad Norm: 0.01051553\n",
      "Epoch 3 | Step 1577100 | Avg Loss: 0.0158 | Grad Norm: 0.00872719\n",
      "Epoch 3 | Step 1577200 | Avg Loss: 0.0158 | Grad Norm: 0.01234233\n",
      "Epoch 3 | Step 1577300 | Avg Loss: 0.0156 | Grad Norm: 0.00852676\n",
      "Epoch 3 | Step 1577400 | Avg Loss: 0.0157 | Grad Norm: 0.00995295\n",
      "Epoch 3 | Step 1577500 | Avg Loss: 0.0157 | Grad Norm: 0.00858433\n",
      "Epoch 3 | Step 1577600 | Avg Loss: 0.0157 | Grad Norm: 0.00851203\n",
      "Epoch 3 | Step 1577700 | Avg Loss: 0.0155 | Grad Norm: 0.01054130\n",
      "Epoch 3 | Step 1577800 | Avg Loss: 0.0155 | Grad Norm: 0.00939564\n",
      "Epoch 3 | Step 1577900 | Avg Loss: 0.0153 | Grad Norm: 0.00903306\n",
      "Epoch 3 | Step 1578000 | Avg Loss: 0.0157 | Grad Norm: 0.00877354\n",
      "Epoch 3 | Step 1578100 | Avg Loss: 0.0160 | Grad Norm: 0.01755992\n",
      "Epoch 3 | Step 1578200 | Avg Loss: 0.0161 | Grad Norm: 0.00967568\n",
      "Epoch 3 | Step 1578300 | Avg Loss: 0.0165 | Grad Norm: 0.01265428\n",
      "Epoch 3 | Step 1578400 | Avg Loss: 0.0165 | Grad Norm: 0.00828234\n",
      "Epoch 3 | Step 1578500 | Avg Loss: 0.0164 | Grad Norm: 0.00854304\n",
      "Epoch 3 | Step 1578600 | Avg Loss: 0.0165 | Grad Norm: 0.00917959\n",
      "Epoch 3 | Step 1578700 | Avg Loss: 0.0165 | Grad Norm: 0.00878291\n",
      "Epoch 3 | Step 1578800 | Avg Loss: 0.0165 | Grad Norm: 0.01113239\n",
      "Epoch 3 | Step 1578900 | Avg Loss: 0.0161 | Grad Norm: 0.01076703\n",
      "Epoch 3 | Step 1579000 | Avg Loss: 0.0165 | Grad Norm: 0.00967385\n",
      "Epoch 3 | Step 1579100 | Avg Loss: 0.0159 | Grad Norm: 0.00856059\n",
      "Epoch 3 | Step 1579200 | Avg Loss: 0.0155 | Grad Norm: 0.00911536\n",
      "Epoch 3 | Step 1579300 | Avg Loss: 0.0156 | Grad Norm: 0.01096794\n",
      "Epoch 3 | Step 1579400 | Avg Loss: 0.0158 | Grad Norm: 0.00823357\n",
      "Epoch 3 | Step 1579500 | Avg Loss: 0.0158 | Grad Norm: 0.00774050\n",
      "Epoch 3 | Step 1579600 | Avg Loss: 0.0158 | Grad Norm: 0.00859893\n",
      "Epoch 3 | Step 1579700 | Avg Loss: 0.0156 | Grad Norm: 0.00826345\n",
      "Epoch 3 | Step 1579800 | Avg Loss: 0.0156 | Grad Norm: 0.01116140\n",
      "Epoch 3 | Step 1579900 | Avg Loss: 0.0157 | Grad Norm: 0.00969526\n",
      "Epoch 3 | Step 1580000 | Avg Loss: 0.0156 | Grad Norm: 0.00848249\n",
      "Epoch 3 | Step 1580100 | Avg Loss: 0.0156 | Grad Norm: 0.00777431\n",
      "Epoch 3 | Step 1580200 | Avg Loss: 0.0154 | Grad Norm: 0.01007027\n",
      "Epoch 3 | Step 1580300 | Avg Loss: 0.0155 | Grad Norm: 0.00879546\n",
      "Epoch 3 | Step 1580400 | Avg Loss: 0.0155 | Grad Norm: 0.00893317\n",
      "Epoch 3 | Step 1580500 | Avg Loss: 0.0154 | Grad Norm: 0.00890404\n",
      "Epoch 3 | Step 1580600 | Avg Loss: 0.0150 | Grad Norm: 0.01015769\n",
      "Epoch 3 | Step 1580700 | Avg Loss: 0.0154 | Grad Norm: 0.00830037\n",
      "Epoch 3 | Step 1580800 | Avg Loss: 0.0154 | Grad Norm: 0.01052450\n",
      "Epoch 3 | Step 1580900 | Avg Loss: 0.0152 | Grad Norm: 0.00869738\n",
      "Epoch 3 | Step 1581000 | Avg Loss: 0.0157 | Grad Norm: 0.00945981\n",
      "Epoch 3 | Step 1581100 | Avg Loss: 0.0159 | Grad Norm: 0.00948130\n",
      "Epoch 3 | Step 1581200 | Avg Loss: 0.0159 | Grad Norm: 0.00794579\n",
      "Epoch 3 | Step 1581300 | Avg Loss: 0.0162 | Grad Norm: 0.00947951\n",
      "Epoch 3 | Step 1581400 | Avg Loss: 0.0163 | Grad Norm: 0.00861956\n",
      "Epoch 3 | Step 1581500 | Avg Loss: 0.0165 | Grad Norm: 0.00823967\n",
      "Epoch 3 | Step 1581600 | Avg Loss: 0.0164 | Grad Norm: 0.00871674\n",
      "Epoch 3 | Step 1581700 | Avg Loss: 0.0163 | Grad Norm: 0.01022603\n",
      "Epoch 3 | Step 1581800 | Avg Loss: 0.0163 | Grad Norm: 0.00904499\n",
      "Epoch 3 | Step 1581900 | Avg Loss: 0.0157 | Grad Norm: 0.00825082\n",
      "Epoch 3 | Step 1582000 | Avg Loss: 0.0155 | Grad Norm: 0.00930703\n",
      "Epoch 3 | Step 1582100 | Avg Loss: 0.0153 | Grad Norm: 0.00992843\n",
      "Epoch 3 | Step 1582200 | Avg Loss: 0.0155 | Grad Norm: 0.00836819\n",
      "Epoch 3 | Step 1582300 | Avg Loss: 0.0155 | Grad Norm: 0.00826807\n",
      "Epoch 3 | Step 1582400 | Avg Loss: 0.0152 | Grad Norm: 0.00896350\n",
      "Epoch 3 | Step 1582500 | Avg Loss: 0.0154 | Grad Norm: 0.01031109\n",
      "Epoch 3 | Step 1582600 | Avg Loss: 0.0153 | Grad Norm: 0.01034922\n",
      "Epoch 3 | Step 1582700 | Avg Loss: 0.0151 | Grad Norm: 0.00736362\n",
      "Epoch 3 | Step 1582800 | Avg Loss: 0.0156 | Grad Norm: 0.00907139\n",
      "Epoch 3 | Step 1582900 | Avg Loss: 0.0158 | Grad Norm: 0.00828985\n",
      "Epoch 3 | Step 1583000 | Avg Loss: 0.0161 | Grad Norm: 0.00876982\n",
      "Epoch 3 | Step 1583100 | Avg Loss: 0.0160 | Grad Norm: 0.00873252\n",
      "Epoch 3 | Step 1583200 | Avg Loss: 0.0158 | Grad Norm: 0.00935422\n",
      "Epoch 3 | Step 1583300 | Avg Loss: 0.0156 | Grad Norm: 0.01014689\n",
      "Epoch 3 | Step 1583400 | Avg Loss: 0.0154 | Grad Norm: 0.00842517\n",
      "Epoch 3 | Step 1583500 | Avg Loss: 0.0156 | Grad Norm: 0.00805293\n",
      "Epoch 3 | Step 1583600 | Avg Loss: 0.0154 | Grad Norm: 0.00859482\n",
      "Epoch 3 | Step 1583700 | Avg Loss: 0.0156 | Grad Norm: 0.00777580\n",
      "Epoch 3 | Step 1583800 | Avg Loss: 0.0155 | Grad Norm: 0.00817192\n",
      "Epoch 3 | Step 1583900 | Avg Loss: 0.0157 | Grad Norm: 0.00815095\n",
      "Epoch 3 | Step 1584000 | Avg Loss: 0.0159 | Grad Norm: 0.00898299\n",
      "Epoch 3 | Step 1584100 | Avg Loss: 0.0159 | Grad Norm: 0.00803167\n",
      "Epoch 3 | Step 1584200 | Avg Loss: 0.0160 | Grad Norm: 0.00893621\n",
      "Epoch 3 | Step 1584300 | Avg Loss: 0.0162 | Grad Norm: 0.00779246\n",
      "Epoch 3 | Step 1584400 | Avg Loss: 0.0160 | Grad Norm: 0.00870182\n",
      "Epoch 3 | Step 1584500 | Avg Loss: 0.0161 | Grad Norm: 0.00833808\n",
      "Epoch 3 | Step 1584600 | Avg Loss: 0.0158 | Grad Norm: 0.01004029\n",
      "Epoch 3 | Step 1584700 | Avg Loss: 0.0154 | Grad Norm: 0.00786346\n",
      "Epoch 3 | Step 1584800 | Avg Loss: 0.0157 | Grad Norm: 0.00815069\n",
      "Epoch 3 | Step 1584900 | Avg Loss: 0.0159 | Grad Norm: 0.00955796\n",
      "Epoch 3 | Step 1585000 | Avg Loss: 0.0157 | Grad Norm: 0.00925801\n",
      "Epoch 3 | Step 1585100 | Avg Loss: 0.0156 | Grad Norm: 0.00768390\n",
      "Epoch 3 | Step 1585200 | Avg Loss: 0.0158 | Grad Norm: 0.00939472\n",
      "Epoch 3 | Step 1585300 | Avg Loss: 0.0160 | Grad Norm: 0.00958630\n",
      "Epoch 3 | Step 1585400 | Avg Loss: 0.0160 | Grad Norm: 0.00855759\n",
      "Epoch 3 | Step 1585500 | Avg Loss: 0.0157 | Grad Norm: 0.00873106\n",
      "Epoch 3 | Step 1585600 | Avg Loss: 0.0154 | Grad Norm: 0.00933336\n",
      "Epoch 3 | Step 1585700 | Avg Loss: 0.0153 | Grad Norm: 0.00898700\n",
      "Epoch 3 | Step 1585800 | Avg Loss: 0.0157 | Grad Norm: 0.00923834\n",
      "Epoch 3 | Step 1585900 | Avg Loss: 0.0156 | Grad Norm: 0.00872254\n",
      "Epoch 3 | Step 1586000 | Avg Loss: 0.0158 | Grad Norm: 0.00888413\n",
      "Epoch 3 | Step 1586100 | Avg Loss: 0.0162 | Grad Norm: 0.00871692\n",
      "Epoch 3 | Step 1586200 | Avg Loss: 0.0162 | Grad Norm: 0.00879921\n",
      "Epoch 3 | Step 1586300 | Avg Loss: 0.0162 | Grad Norm: 0.00827425\n",
      "Epoch 3 | Step 1586400 | Avg Loss: 0.0160 | Grad Norm: 0.00877260\n",
      "Epoch 3 | Step 1586500 | Avg Loss: 0.0161 | Grad Norm: 0.00885596\n",
      "Epoch 3 | Step 1586600 | Avg Loss: 0.0159 | Grad Norm: 0.00964444\n",
      "Epoch 3 | Step 1586700 | Avg Loss: 0.0160 | Grad Norm: 0.01083406\n",
      "Epoch 3 | Step 1586800 | Avg Loss: 0.0158 | Grad Norm: 0.00793194\n",
      "Epoch 3 | Step 1586900 | Avg Loss: 0.0156 | Grad Norm: 0.00878090\n",
      "Epoch 3 | Step 1587000 | Avg Loss: 0.0158 | Grad Norm: 0.01063448\n",
      "Epoch 3 | Step 1587100 | Avg Loss: 0.0157 | Grad Norm: 0.00872747\n",
      "Epoch 3 | Step 1587200 | Avg Loss: 0.0163 | Grad Norm: 0.00871442\n",
      "Epoch 3 | Step 1587300 | Avg Loss: 0.0160 | Grad Norm: 0.00863625\n",
      "Epoch 3 | Step 1587400 | Avg Loss: 0.0158 | Grad Norm: 0.01022953\n",
      "Epoch 3 | Step 1587500 | Avg Loss: 0.0158 | Grad Norm: 0.00860082\n",
      "Epoch 3 | Step 1587600 | Avg Loss: 0.0160 | Grad Norm: 0.00856933\n",
      "Epoch 3 | Step 1587700 | Avg Loss: 0.0162 | Grad Norm: 0.00969338\n",
      "Epoch 3 | Step 1587800 | Avg Loss: 0.0166 | Grad Norm: 0.00980648\n",
      "Epoch 3 | Step 1587900 | Avg Loss: 0.0163 | Grad Norm: 0.01058504\n",
      "Epoch 3 | Step 1588000 | Avg Loss: 0.0161 | Grad Norm: 0.00870987\n",
      "Epoch 3 | Step 1588100 | Avg Loss: 0.0161 | Grad Norm: 0.00733425\n",
      "Epoch 3 | Step 1588200 | Avg Loss: 0.0160 | Grad Norm: 0.00818162\n",
      "Epoch 3 | Step 1588300 | Avg Loss: 0.0162 | Grad Norm: 0.00863107\n",
      "Epoch 3 | Step 1588400 | Avg Loss: 0.0163 | Grad Norm: 0.00820965\n",
      "Epoch 3 | Step 1588500 | Avg Loss: 0.0158 | Grad Norm: 0.00900498\n",
      "Epoch 3 | Step 1588600 | Avg Loss: 0.0156 | Grad Norm: 0.00963923\n",
      "Epoch 3 | Step 1588700 | Avg Loss: 0.0154 | Grad Norm: 0.00846109\n",
      "Epoch 3 | Step 1588800 | Avg Loss: 0.0157 | Grad Norm: 0.01020695\n",
      "Epoch 3 | Step 1588900 | Avg Loss: 0.0158 | Grad Norm: 0.00839911\n",
      "Epoch 3 | Step 1589000 | Avg Loss: 0.0155 | Grad Norm: 0.00949989\n",
      "Epoch 3 | Step 1589100 | Avg Loss: 0.0154 | Grad Norm: 0.00925677\n",
      "Epoch 3 | Step 1589200 | Avg Loss: 0.0154 | Grad Norm: 0.00803336\n",
      "Epoch 3 | Step 1589300 | Avg Loss: 0.0158 | Grad Norm: 0.01005364\n",
      "Epoch 3 | Step 1589400 | Avg Loss: 0.0158 | Grad Norm: 0.00857215\n",
      "Epoch 3 | Step 1589500 | Avg Loss: 0.0160 | Grad Norm: 0.01149649\n",
      "Epoch 3 | Step 1589600 | Avg Loss: 0.0158 | Grad Norm: 0.00844431\n",
      "Epoch 3 | Step 1589700 | Avg Loss: 0.0157 | Grad Norm: 0.00794731\n",
      "Epoch 3 | Step 1589800 | Avg Loss: 0.0155 | Grad Norm: 0.01056822\n",
      "Epoch 3 | Step 1589900 | Avg Loss: 0.0158 | Grad Norm: 0.00951465\n",
      "Epoch 3 | Step 1590000 | Avg Loss: 0.0161 | Grad Norm: 0.00884262\n",
      "Epoch 3 | Step 1590100 | Avg Loss: 0.0158 | Grad Norm: 0.00861008\n",
      "Epoch 3 | Step 1590200 | Avg Loss: 0.0158 | Grad Norm: 0.00857296\n",
      "Epoch 3 | Step 1590300 | Avg Loss: 0.0162 | Grad Norm: 0.00883685\n",
      "Epoch 3 | Step 1590400 | Avg Loss: 0.0161 | Grad Norm: 0.01010755\n",
      "Epoch 3 | Step 1590500 | Avg Loss: 0.0159 | Grad Norm: 0.00843787\n",
      "Epoch 3 | Step 1590600 | Avg Loss: 0.0160 | Grad Norm: 0.01283038\n",
      "Epoch 3 | Step 1590700 | Avg Loss: 0.0160 | Grad Norm: 0.00863243\n",
      "Epoch 3 | Step 1590800 | Avg Loss: 0.0154 | Grad Norm: 0.00926959\n",
      "Epoch 3 | Step 1590900 | Avg Loss: 0.0155 | Grad Norm: 0.00915466\n",
      "Epoch 3 | Step 1591000 | Avg Loss: 0.0157 | Grad Norm: 0.00984276\n",
      "Epoch 3 | Step 1591100 | Avg Loss: 0.0158 | Grad Norm: 0.00854492\n",
      "Epoch 3 | Step 1591200 | Avg Loss: 0.0156 | Grad Norm: 0.00768179\n",
      "Epoch 3 | Step 1591300 | Avg Loss: 0.0155 | Grad Norm: 0.00922674\n",
      "Epoch 3 | Step 1591400 | Avg Loss: 0.0156 | Grad Norm: 0.01001778\n",
      "Epoch 3 | Step 1591500 | Avg Loss: 0.0153 | Grad Norm: 0.01277613\n",
      "Epoch 3 | Step 1591600 | Avg Loss: 0.0156 | Grad Norm: 0.00925912\n",
      "Epoch 3 | Step 1591700 | Avg Loss: 0.0157 | Grad Norm: 0.00868236\n",
      "Epoch 3 | Step 1591800 | Avg Loss: 0.0153 | Grad Norm: 0.00865622\n",
      "Epoch 3 | Step 1591900 | Avg Loss: 0.0152 | Grad Norm: 0.00808677\n",
      "Epoch 3 | Step 1592000 | Avg Loss: 0.0158 | Grad Norm: 0.00838747\n",
      "Epoch 3 | Step 1592100 | Avg Loss: 0.0156 | Grad Norm: 0.01112619\n",
      "Epoch 3 | Step 1592200 | Avg Loss: 0.0157 | Grad Norm: 0.00901666\n",
      "Epoch 3 | Step 1592300 | Avg Loss: 0.0158 | Grad Norm: 0.00907506\n",
      "Epoch 3 | Step 1592400 | Avg Loss: 0.0160 | Grad Norm: 0.00849669\n",
      "Epoch 3 | Step 1592500 | Avg Loss: 0.0158 | Grad Norm: 0.00917531\n",
      "Epoch 3 | Step 1592600 | Avg Loss: 0.0157 | Grad Norm: 0.00905641\n",
      "Epoch 3 | Step 1592700 | Avg Loss: 0.0159 | Grad Norm: 0.00915917\n",
      "Epoch 3 | Step 1592800 | Avg Loss: 0.0160 | Grad Norm: 0.00945748\n",
      "Epoch 3 | Step 1592900 | Avg Loss: 0.0160 | Grad Norm: 0.00836838\n",
      "Epoch 3 | Step 1593000 | Avg Loss: 0.0155 | Grad Norm: 0.00779004\n",
      "Epoch 3 | Step 1593100 | Avg Loss: 0.0155 | Grad Norm: 0.00847272\n",
      "Epoch 3 | Step 1593200 | Avg Loss: 0.0156 | Grad Norm: 0.00882137\n",
      "Epoch 3 | Step 1593300 | Avg Loss: 0.0158 | Grad Norm: 0.00966498\n",
      "Epoch 3 | Step 1593400 | Avg Loss: 0.0158 | Grad Norm: 0.00792708\n",
      "Epoch 3 | Step 1593500 | Avg Loss: 0.0158 | Grad Norm: 0.00789724\n",
      "Epoch 3 | Step 1593600 | Avg Loss: 0.0154 | Grad Norm: 0.00834060\n",
      "Epoch 3 | Step 1593700 | Avg Loss: 0.0156 | Grad Norm: 0.00834816\n",
      "Epoch 3 | Step 1593800 | Avg Loss: 0.0154 | Grad Norm: 0.01105554\n",
      "Epoch 3 | Step 1593900 | Avg Loss: 0.0156 | Grad Norm: 0.00877780\n",
      "Epoch 3 | Step 1594000 | Avg Loss: 0.0154 | Grad Norm: 0.00876326\n",
      "Epoch 3 | Step 1594100 | Avg Loss: 0.0153 | Grad Norm: 0.00779617\n",
      "Epoch 3 | Step 1594200 | Avg Loss: 0.0153 | Grad Norm: 0.01093347\n",
      "Epoch 3 | Step 1594300 | Avg Loss: 0.0157 | Grad Norm: 0.00973483\n",
      "Epoch 3 | Step 1594400 | Avg Loss: 0.0159 | Grad Norm: 0.00763443\n",
      "Epoch 3 | Step 1594500 | Avg Loss: 0.0160 | Grad Norm: 0.00880640\n",
      "Epoch 3 | Step 1594600 | Avg Loss: 0.0155 | Grad Norm: 0.00907710\n",
      "Epoch 3 | Step 1594700 | Avg Loss: 0.0158 | Grad Norm: 0.00866022\n",
      "Epoch 3 | Step 1594800 | Avg Loss: 0.0159 | Grad Norm: 0.00983882\n",
      "Epoch 3 | Step 1594900 | Avg Loss: 0.0161 | Grad Norm: 0.00895616\n",
      "Epoch 3 | Step 1595000 | Avg Loss: 0.0162 | Grad Norm: 0.00919805\n",
      "Epoch 3 | Step 1595100 | Avg Loss: 0.0162 | Grad Norm: 0.01050949\n",
      "Epoch 3 | Step 1595200 | Avg Loss: 0.0160 | Grad Norm: 0.00801321\n",
      "Epoch 3 | Step 1595300 | Avg Loss: 0.0160 | Grad Norm: 0.00918141\n",
      "Epoch 3 | Step 1595400 | Avg Loss: 0.0157 | Grad Norm: 0.00935662\n",
      "Epoch 3 | Step 1595500 | Avg Loss: 0.0158 | Grad Norm: 0.00939917\n",
      "Epoch 3 | Step 1595600 | Avg Loss: 0.0154 | Grad Norm: 0.00899147\n",
      "Epoch 3 | Step 1595700 | Avg Loss: 0.0154 | Grad Norm: 0.00797997\n",
      "Epoch 3 | Step 1595800 | Avg Loss: 0.0154 | Grad Norm: 0.01059570\n",
      "Epoch 3 | Step 1595900 | Avg Loss: 0.0157 | Grad Norm: 0.00917761\n",
      "Epoch 3 | Step 1596000 | Avg Loss: 0.0155 | Grad Norm: 0.00838548\n",
      "Epoch 3 | Step 1596100 | Avg Loss: 0.0157 | Grad Norm: 0.00923048\n",
      "Epoch 3 | Step 1596200 | Avg Loss: 0.0158 | Grad Norm: 0.00877988\n",
      "Epoch 3 | Step 1596300 | Avg Loss: 0.0157 | Grad Norm: 0.00953059\n",
      "Epoch 3 | Step 1596400 | Avg Loss: 0.0155 | Grad Norm: 0.00963586\n",
      "Epoch 3 | Step 1596500 | Avg Loss: 0.0153 | Grad Norm: 0.00954752\n",
      "Epoch 3 | Step 1596600 | Avg Loss: 0.0148 | Grad Norm: 0.00924448\n",
      "Epoch 3 | Step 1596700 | Avg Loss: 0.0145 | Grad Norm: 0.00831483\n",
      "Epoch 3 | Step 1596800 | Avg Loss: 0.0149 | Grad Norm: 0.00765167\n",
      "Epoch 3 | Step 1596900 | Avg Loss: 0.0152 | Grad Norm: 0.01001306\n",
      "Epoch 3 | Step 1597000 | Avg Loss: 0.0156 | Grad Norm: 0.01074679\n",
      "Epoch 3 | Step 1597100 | Avg Loss: 0.0157 | Grad Norm: 0.00986499\n",
      "Epoch 3 | Step 1597200 | Avg Loss: 0.0152 | Grad Norm: 0.01001429\n",
      "Epoch 3 | Step 1597300 | Avg Loss: 0.0153 | Grad Norm: 0.00817926\n",
      "Epoch 3 | Step 1597400 | Avg Loss: 0.0159 | Grad Norm: 0.00928920\n",
      "Epoch 3 | Step 1597500 | Avg Loss: 0.0156 | Grad Norm: 0.00778617\n",
      "Epoch 3 | Step 1597600 | Avg Loss: 0.0156 | Grad Norm: 0.01088437\n",
      "Epoch 3 | Step 1597700 | Avg Loss: 0.0159 | Grad Norm: 0.00927916\n",
      "Epoch 3 | Step 1597800 | Avg Loss: 0.0160 | Grad Norm: 0.00857882\n",
      "Epoch 3 | Step 1597900 | Avg Loss: 0.0157 | Grad Norm: 0.01171690\n",
      "Epoch 3 | Step 1598000 | Avg Loss: 0.0160 | Grad Norm: 0.01007257\n",
      "Epoch 3 | Step 1598100 | Avg Loss: 0.0163 | Grad Norm: 0.00749571\n",
      "Epoch 3 | Step 1598200 | Avg Loss: 0.0159 | Grad Norm: 0.00927940\n",
      "Epoch 3 | Step 1598300 | Avg Loss: 0.0157 | Grad Norm: 0.00971845\n",
      "Epoch 3 | Step 1598400 | Avg Loss: 0.0156 | Grad Norm: 0.00902351\n",
      "Epoch 3 | Step 1598500 | Avg Loss: 0.0151 | Grad Norm: 0.00873374\n",
      "Epoch 3 | Step 1598600 | Avg Loss: 0.0153 | Grad Norm: 0.00994170\n",
      "Epoch 3 | Step 1598700 | Avg Loss: 0.0156 | Grad Norm: 0.00999826\n",
      "Epoch 3 | Step 1598800 | Avg Loss: 0.0160 | Grad Norm: 0.00941777\n",
      "Epoch 3 | Step 1598900 | Avg Loss: 0.0162 | Grad Norm: 0.01000780\n",
      "Epoch 3 | Step 1599000 | Avg Loss: 0.0159 | Grad Norm: 0.00927551\n",
      "Epoch 3 | Step 1599100 | Avg Loss: 0.0161 | Grad Norm: 0.00823945\n",
      "Epoch 3 | Step 1599200 | Avg Loss: 0.0160 | Grad Norm: 0.00834718\n",
      "Epoch 3 | Step 1599300 | Avg Loss: 0.0160 | Grad Norm: 0.00872915\n",
      "Epoch 3 | Step 1599400 | Avg Loss: 0.0161 | Grad Norm: 0.00914371\n",
      "Epoch 3 | Step 1599500 | Avg Loss: 0.0163 | Grad Norm: 0.00892726\n",
      "Epoch 3 | Step 1599600 | Avg Loss: 0.0161 | Grad Norm: 0.00753891\n",
      "Epoch 3 | Step 1599700 | Avg Loss: 0.0160 | Grad Norm: 0.00800717\n",
      "Epoch 3 | Step 1599800 | Avg Loss: 0.0157 | Grad Norm: 0.00908950\n",
      "Epoch 3 | Step 1599900 | Avg Loss: 0.0161 | Grad Norm: 0.00902118\n",
      "Epoch 3 | Step 1600000 | Avg Loss: 0.0159 | Grad Norm: 0.00852428\n",
      "Saving model at step1600000\n",
      "Epoch 3 | Step 1600100 | Avg Loss: 0.0159 | Grad Norm: 0.00927266\n",
      "Epoch 3 | Step 1600200 | Avg Loss: 0.0159 | Grad Norm: 0.00882554\n",
      "Epoch 3 | Step 1600300 | Avg Loss: 0.0160 | Grad Norm: 0.00884053\n",
      "Epoch 3 | Step 1600400 | Avg Loss: 0.0158 | Grad Norm: 0.00871345\n",
      "Epoch 3 | Step 1600500 | Avg Loss: 0.0160 | Grad Norm: 0.00865779\n",
      "Epoch 3 | Step 1600600 | Avg Loss: 0.0162 | Grad Norm: 0.00966147\n",
      "Epoch 3 | Step 1600700 | Avg Loss: 0.0164 | Grad Norm: 0.00825897\n",
      "Epoch 3 | Step 1600800 | Avg Loss: 0.0165 | Grad Norm: 0.00934406\n",
      "Epoch 3 | Step 1600900 | Avg Loss: 0.0164 | Grad Norm: 0.00880440\n",
      "Epoch 3 | Step 1601000 | Avg Loss: 0.0162 | Grad Norm: 0.00894214\n",
      "Epoch 3 | Step 1601100 | Avg Loss: 0.0162 | Grad Norm: 0.00918439\n",
      "Epoch 3 | Step 1601200 | Avg Loss: 0.0162 | Grad Norm: 0.00885441\n",
      "Epoch 3 | Step 1601300 | Avg Loss: 0.0161 | Grad Norm: 0.00816481\n",
      "Epoch 3 | Step 1601400 | Avg Loss: 0.0159 | Grad Norm: 0.00991507\n",
      "Epoch 3 | Step 1601500 | Avg Loss: 0.0155 | Grad Norm: 0.00708162\n",
      "Epoch 3 | Step 1601600 | Avg Loss: 0.0158 | Grad Norm: 0.00780733\n",
      "Epoch 3 | Step 1601700 | Avg Loss: 0.0158 | Grad Norm: 0.01125258\n",
      "Epoch 3 | Step 1601800 | Avg Loss: 0.0156 | Grad Norm: 0.00956725\n",
      "Epoch 3 | Step 1601900 | Avg Loss: 0.0158 | Grad Norm: 0.00834647\n",
      "Epoch 3 | Step 1602000 | Avg Loss: 0.0157 | Grad Norm: 0.00842076\n",
      "Epoch 3 | Step 1602100 | Avg Loss: 0.0155 | Grad Norm: 0.00981595\n",
      "Epoch 3 | Step 1602200 | Avg Loss: 0.0159 | Grad Norm: 0.00920926\n",
      "Epoch 3 | Step 1602300 | Avg Loss: 0.0158 | Grad Norm: 0.00834899\n",
      "Epoch 3 | Step 1602400 | Avg Loss: 0.0155 | Grad Norm: 0.00847525\n",
      "Epoch 3 | Step 1602500 | Avg Loss: 0.0159 | Grad Norm: 0.00938897\n",
      "Epoch 3 | Step 1602600 | Avg Loss: 0.0154 | Grad Norm: 0.00959276\n",
      "Epoch 3 | Step 1602700 | Avg Loss: 0.0156 | Grad Norm: 0.00887908\n",
      "Epoch 3 | Step 1602800 | Avg Loss: 0.0156 | Grad Norm: 0.00853625\n",
      "Epoch 3 | Step 1602900 | Avg Loss: 0.0156 | Grad Norm: 0.00975417\n",
      "Epoch 3 | Step 1603000 | Avg Loss: 0.0155 | Grad Norm: 0.00914083\n",
      "Epoch 3 | Step 1603100 | Avg Loss: 0.0154 | Grad Norm: 0.00969611\n",
      "Epoch 3 | Step 1603200 | Avg Loss: 0.0152 | Grad Norm: 0.00832109\n",
      "Epoch 3 | Step 1603300 | Avg Loss: 0.0151 | Grad Norm: 0.00751186\n",
      "Epoch 3 | Step 1603400 | Avg Loss: 0.0152 | Grad Norm: 0.00904858\n",
      "Epoch 3 | Step 1603500 | Avg Loss: 0.0153 | Grad Norm: 0.00876886\n",
      "Epoch 3 | Step 1603600 | Avg Loss: 0.0152 | Grad Norm: 0.00989692\n",
      "Epoch 3 | Step 1603700 | Avg Loss: 0.0156 | Grad Norm: 0.00956387\n",
      "Epoch 3 | Step 1603800 | Avg Loss: 0.0157 | Grad Norm: 0.00870125\n",
      "Epoch 3 | Step 1603900 | Avg Loss: 0.0155 | Grad Norm: 0.00922251\n",
      "Epoch 3 | Step 1604000 | Avg Loss: 0.0154 | Grad Norm: 0.00732609\n",
      "Epoch 3 | Step 1604100 | Avg Loss: 0.0156 | Grad Norm: 0.01023884\n",
      "Epoch 3 | Step 1604200 | Avg Loss: 0.0158 | Grad Norm: 0.00935587\n",
      "Epoch 3 | Step 1604300 | Avg Loss: 0.0155 | Grad Norm: 0.00797524\n",
      "Epoch 3 | Step 1604400 | Avg Loss: 0.0158 | Grad Norm: 0.00861853\n",
      "Epoch 3 | Step 1604500 | Avg Loss: 0.0161 | Grad Norm: 0.00962871\n",
      "Epoch 3 | Step 1604600 | Avg Loss: 0.0160 | Grad Norm: 0.00891811\n",
      "Epoch 3 | Step 1604700 | Avg Loss: 0.0162 | Grad Norm: 0.00905577\n",
      "Epoch 3 | Step 1604800 | Avg Loss: 0.0163 | Grad Norm: 0.01054904\n",
      "Epoch 3 | Step 1604900 | Avg Loss: 0.0167 | Grad Norm: 0.00938993\n",
      "Epoch 3 | Step 1605000 | Avg Loss: 0.0163 | Grad Norm: 0.00981211\n",
      "Epoch 3 | Step 1605100 | Avg Loss: 0.0159 | Grad Norm: 0.00941040\n",
      "Epoch 3 | Step 1605200 | Avg Loss: 0.0155 | Grad Norm: 0.01160137\n",
      "Epoch 3 | Step 1605300 | Avg Loss: 0.0156 | Grad Norm: 0.00909547\n",
      "Epoch 3 | Step 1605400 | Avg Loss: 0.0153 | Grad Norm: 0.01313952\n",
      "Epoch 3 | Step 1605500 | Avg Loss: 0.0152 | Grad Norm: 0.00771923\n",
      "Epoch 3 | Step 1605600 | Avg Loss: 0.0156 | Grad Norm: 0.00779738\n",
      "Epoch 3 | Step 1605700 | Avg Loss: 0.0158 | Grad Norm: 0.00838874\n",
      "Epoch 3 | Step 1605800 | Avg Loss: 0.0157 | Grad Norm: 0.00872882\n",
      "Epoch 3 | Step 1605900 | Avg Loss: 0.0159 | Grad Norm: 0.00738323\n",
      "Epoch 3 | Step 1606000 | Avg Loss: 0.0158 | Grad Norm: 0.01010254\n",
      "Epoch 3 | Step 1606100 | Avg Loss: 0.0157 | Grad Norm: 0.01001132\n",
      "Epoch 3 | Step 1606200 | Avg Loss: 0.0153 | Grad Norm: 0.00843424\n",
      "Epoch 3 | Step 1606300 | Avg Loss: 0.0155 | Grad Norm: 0.00822949\n",
      "Epoch 3 | Step 1606400 | Avg Loss: 0.0157 | Grad Norm: 0.00948796\n",
      "Epoch 3 | Step 1606500 | Avg Loss: 0.0153 | Grad Norm: 0.00865334\n",
      "Epoch 3 | Step 1606600 | Avg Loss: 0.0155 | Grad Norm: 0.00851363\n",
      "Epoch 3 | Step 1606700 | Avg Loss: 0.0156 | Grad Norm: 0.01049198\n",
      "Epoch 3 | Step 1606800 | Avg Loss: 0.0161 | Grad Norm: 0.00882543\n",
      "Epoch 3 | Step 1606900 | Avg Loss: 0.0160 | Grad Norm: 0.01095769\n",
      "Epoch 3 | Step 1607000 | Avg Loss: 0.0159 | Grad Norm: 0.00859300\n",
      "Epoch 3 | Step 1607100 | Avg Loss: 0.0158 | Grad Norm: 0.00966324\n",
      "Epoch 3 | Step 1607200 | Avg Loss: 0.0162 | Grad Norm: 0.01045599\n",
      "Epoch 3 | Step 1607300 | Avg Loss: 0.0163 | Grad Norm: 0.00865966\n",
      "Epoch 3 | Step 1607400 | Avg Loss: 0.0160 | Grad Norm: 0.00989980\n",
      "Epoch 3 | Step 1607500 | Avg Loss: 0.0155 | Grad Norm: 0.00888864\n",
      "Epoch 3 | Step 1607600 | Avg Loss: 0.0155 | Grad Norm: 0.00881129\n",
      "Epoch 3 | Step 1607700 | Avg Loss: 0.0154 | Grad Norm: 0.00893488\n",
      "Epoch 3 | Step 1607800 | Avg Loss: 0.0154 | Grad Norm: 0.00759499\n",
      "Epoch 3 | Step 1607900 | Avg Loss: 0.0154 | Grad Norm: 0.01093692\n",
      "Epoch 3 | Step 1608000 | Avg Loss: 0.0154 | Grad Norm: 0.00966734\n",
      "Epoch 3 | Step 1608100 | Avg Loss: 0.0155 | Grad Norm: 0.00959519\n",
      "Epoch 3 | Step 1608200 | Avg Loss: 0.0157 | Grad Norm: 0.01099360\n",
      "Epoch 3 | Step 1608300 | Avg Loss: 0.0155 | Grad Norm: 0.00892749\n",
      "Epoch 3 | Step 1608400 | Avg Loss: 0.0152 | Grad Norm: 0.00815237\n",
      "Epoch 3 | Step 1608500 | Avg Loss: 0.0154 | Grad Norm: 0.00843686\n",
      "Epoch 3 | Step 1608600 | Avg Loss: 0.0154 | Grad Norm: 0.00961132\n",
      "Epoch 3 | Step 1608700 | Avg Loss: 0.0150 | Grad Norm: 0.00780233\n",
      "Epoch 3 | Step 1608800 | Avg Loss: 0.0151 | Grad Norm: 0.00978582\n",
      "Epoch 3 | Step 1608900 | Avg Loss: 0.0155 | Grad Norm: 0.00866221\n",
      "Epoch 3 | Step 1609000 | Avg Loss: 0.0151 | Grad Norm: 0.00811007\n",
      "Epoch 3 | Step 1609100 | Avg Loss: 0.0150 | Grad Norm: 0.00982846\n",
      "Epoch 3 | Step 1609200 | Avg Loss: 0.0154 | Grad Norm: 0.00902265\n",
      "Epoch 3 | Step 1609300 | Avg Loss: 0.0151 | Grad Norm: 0.01199834\n",
      "Epoch 3 | Step 1609400 | Avg Loss: 0.0152 | Grad Norm: 0.01090974\n",
      "Epoch 3 | Step 1609500 | Avg Loss: 0.0153 | Grad Norm: 0.00755102\n",
      "Epoch 3 | Step 1609600 | Avg Loss: 0.0156 | Grad Norm: 0.01067861\n",
      "Epoch 3 | Step 1609700 | Avg Loss: 0.0154 | Grad Norm: 0.00993839\n",
      "Epoch 3 | Step 1609800 | Avg Loss: 0.0155 | Grad Norm: 0.00871079\n",
      "Epoch 3 | Step 1609900 | Avg Loss: 0.0158 | Grad Norm: 0.00968059\n",
      "Epoch 3 | Step 1610000 | Avg Loss: 0.0159 | Grad Norm: 0.00871767\n",
      "Epoch 3 | Step 1610100 | Avg Loss: 0.0159 | Grad Norm: 0.01000976\n",
      "Epoch 3 | Step 1610200 | Avg Loss: 0.0163 | Grad Norm: 0.00754416\n",
      "Epoch 3 | Step 1610300 | Avg Loss: 0.0164 | Grad Norm: 0.00855628\n",
      "Epoch 3 | Step 1610400 | Avg Loss: 0.0160 | Grad Norm: 0.00822645\n",
      "Epoch 3 | Step 1610500 | Avg Loss: 0.0159 | Grad Norm: 0.00813465\n",
      "Epoch 3 | Step 1610600 | Avg Loss: 0.0157 | Grad Norm: 0.00766221\n",
      "Epoch 3 | Step 1610700 | Avg Loss: 0.0154 | Grad Norm: 0.00905906\n",
      "Epoch 3 | Step 1610800 | Avg Loss: 0.0154 | Grad Norm: 0.00812136\n",
      "Epoch 3 | Step 1610900 | Avg Loss: 0.0156 | Grad Norm: 0.00914686\n",
      "Epoch 3 | Step 1611000 | Avg Loss: 0.0158 | Grad Norm: 0.00886524\n",
      "Epoch 3 | Step 1611100 | Avg Loss: 0.0154 | Grad Norm: 0.01047763\n",
      "Epoch 3 | Step 1611200 | Avg Loss: 0.0156 | Grad Norm: 0.01039239\n",
      "Epoch 3 | Step 1611300 | Avg Loss: 0.0154 | Grad Norm: 0.01276461\n",
      "Epoch 3 | Step 1611400 | Avg Loss: 0.0155 | Grad Norm: 0.00924995\n",
      "Epoch 3 | Step 1611500 | Avg Loss: 0.0157 | Grad Norm: 0.00839156\n",
      "Epoch 3 | Step 1611600 | Avg Loss: 0.0154 | Grad Norm: 0.00855336\n",
      "Epoch 3 | Step 1611700 | Avg Loss: 0.0154 | Grad Norm: 0.01216628\n",
      "Epoch 3 | Step 1611800 | Avg Loss: 0.0158 | Grad Norm: 0.00866988\n",
      "Epoch 3 | Step 1611900 | Avg Loss: 0.0163 | Grad Norm: 0.00809950\n",
      "Epoch 3 | Step 1612000 | Avg Loss: 0.0159 | Grad Norm: 0.01035239\n",
      "Epoch 3 | Step 1612100 | Avg Loss: 0.0158 | Grad Norm: 0.00857882\n",
      "Epoch 3 | Step 1612200 | Avg Loss: 0.0160 | Grad Norm: 0.00886208\n",
      "Epoch 3 | Step 1612300 | Avg Loss: 0.0157 | Grad Norm: 0.00919692\n",
      "Epoch 3 | Step 1612400 | Avg Loss: 0.0156 | Grad Norm: 0.01097516\n",
      "Epoch 3 | Step 1612500 | Avg Loss: 0.0157 | Grad Norm: 0.01021049\n",
      "Epoch 3 | Step 1612600 | Avg Loss: 0.0156 | Grad Norm: 0.01077177\n",
      "Epoch 3 | Step 1612700 | Avg Loss: 0.0156 | Grad Norm: 0.00894504\n",
      "Epoch 3 | Step 1612800 | Avg Loss: 0.0157 | Grad Norm: 0.01020045\n",
      "Epoch 3 | Step 1612900 | Avg Loss: 0.0156 | Grad Norm: 0.00937498\n",
      "Epoch 3 | Step 1613000 | Avg Loss: 0.0157 | Grad Norm: 0.00918035\n",
      "Epoch 3 | Step 1613100 | Avg Loss: 0.0157 | Grad Norm: 0.00910397\n",
      "Epoch 3 | Step 1613200 | Avg Loss: 0.0156 | Grad Norm: 0.00915030\n",
      "Epoch 3 | Step 1613300 | Avg Loss: 0.0152 | Grad Norm: 0.00834650\n",
      "Epoch 3 | Step 1613400 | Avg Loss: 0.0156 | Grad Norm: 0.00981347\n",
      "Epoch 3 | Step 1613500 | Avg Loss: 0.0157 | Grad Norm: 0.00864563\n",
      "Epoch 3 | Step 1613600 | Avg Loss: 0.0158 | Grad Norm: 0.01130706\n",
      "Epoch 3 | Step 1613700 | Avg Loss: 0.0159 | Grad Norm: 0.00945104\n",
      "Epoch 3 | Step 1613800 | Avg Loss: 0.0156 | Grad Norm: 0.00886556\n",
      "Epoch 3 | Step 1613900 | Avg Loss: 0.0155 | Grad Norm: 0.00998623\n",
      "Epoch 3 | Step 1614000 | Avg Loss: 0.0155 | Grad Norm: 0.00983819\n",
      "Epoch 3 | Step 1614100 | Avg Loss: 0.0159 | Grad Norm: 0.00819905\n",
      "Epoch 3 | Step 1614200 | Avg Loss: 0.0156 | Grad Norm: 0.01111827\n",
      "Epoch 3 | Step 1614300 | Avg Loss: 0.0157 | Grad Norm: 0.00953046\n",
      "Epoch 3 | Step 1614400 | Avg Loss: 0.0159 | Grad Norm: 0.00807178\n",
      "Epoch 3 | Step 1614500 | Avg Loss: 0.0157 | Grad Norm: 0.00937121\n",
      "Epoch 3 | Step 1614600 | Avg Loss: 0.0160 | Grad Norm: 0.00800040\n",
      "Epoch 3 | Step 1614700 | Avg Loss: 0.0159 | Grad Norm: 0.00842223\n",
      "Epoch 3 | Step 1614800 | Avg Loss: 0.0159 | Grad Norm: 0.01352248\n",
      "Epoch 3 | Step 1614900 | Avg Loss: 0.0155 | Grad Norm: 0.00890269\n",
      "Epoch 3 | Step 1615000 | Avg Loss: 0.0153 | Grad Norm: 0.00857323\n",
      "Epoch 3 | Step 1615100 | Avg Loss: 0.0154 | Grad Norm: 0.01135429\n",
      "Epoch 3 | Step 1615200 | Avg Loss: 0.0154 | Grad Norm: 0.00824607\n",
      "Epoch 3 | Step 1615300 | Avg Loss: 0.0154 | Grad Norm: 0.00837123\n",
      "Epoch 3 | Step 1615400 | Avg Loss: 0.0152 | Grad Norm: 0.00875595\n",
      "Epoch 3 | Step 1615500 | Avg Loss: 0.0151 | Grad Norm: 0.00744326\n",
      "Epoch 3 | Step 1615600 | Avg Loss: 0.0152 | Grad Norm: 0.00866898\n",
      "Epoch 3 | Step 1615700 | Avg Loss: 0.0153 | Grad Norm: 0.01046866\n",
      "Epoch 3 | Step 1615800 | Avg Loss: 0.0152 | Grad Norm: 0.00802895\n",
      "Epoch 3 | Step 1615900 | Avg Loss: 0.0154 | Grad Norm: 0.00900257\n",
      "Epoch 3 | Step 1616000 | Avg Loss: 0.0156 | Grad Norm: 0.01222633\n",
      "Epoch 3 | Step 1616100 | Avg Loss: 0.0155 | Grad Norm: 0.00911398\n",
      "Epoch 3 | Step 1616200 | Avg Loss: 0.0155 | Grad Norm: 0.00890271\n",
      "Epoch 3 | Step 1616300 | Avg Loss: 0.0156 | Grad Norm: 0.00803560\n",
      "Epoch 3 | Step 1616400 | Avg Loss: 0.0158 | Grad Norm: 0.00809710\n",
      "Epoch 3 | Step 1616500 | Avg Loss: 0.0160 | Grad Norm: 0.01020550\n",
      "Epoch 3 | Step 1616600 | Avg Loss: 0.0162 | Grad Norm: 0.01016316\n",
      "Epoch 3 | Step 1616700 | Avg Loss: 0.0160 | Grad Norm: 0.00917667\n",
      "Epoch 3 | Step 1616800 | Avg Loss: 0.0163 | Grad Norm: 0.00969498\n",
      "Epoch 3 | Step 1616900 | Avg Loss: 0.0163 | Grad Norm: 0.01026226\n",
      "Epoch 3 | Step 1617000 | Avg Loss: 0.0164 | Grad Norm: 0.00979503\n",
      "Epoch 3 | Step 1617100 | Avg Loss: 0.0162 | Grad Norm: 0.00967342\n",
      "Epoch 3 | Step 1617200 | Avg Loss: 0.0161 | Grad Norm: 0.00792498\n",
      "Epoch 3 | Step 1617300 | Avg Loss: 0.0162 | Grad Norm: 0.00877102\n",
      "Epoch 3 | Step 1617400 | Avg Loss: 0.0159 | Grad Norm: 0.00880101\n",
      "Epoch 3 | Step 1617500 | Avg Loss: 0.0162 | Grad Norm: 0.00862829\n",
      "Epoch 3 | Step 1617600 | Avg Loss: 0.0164 | Grad Norm: 0.01212153\n",
      "Epoch 3 | Step 1617700 | Avg Loss: 0.0160 | Grad Norm: 0.00696973\n",
      "Epoch 3 | Step 1617800 | Avg Loss: 0.0159 | Grad Norm: 0.00916676\n",
      "Epoch 3 | Step 1617900 | Avg Loss: 0.0156 | Grad Norm: 0.00810553\n",
      "Epoch 3 | Step 1618000 | Avg Loss: 0.0160 | Grad Norm: 0.00757823\n",
      "Epoch 3 | Step 1618100 | Avg Loss: 0.0157 | Grad Norm: 0.00916734\n",
      "Epoch 3 | Step 1618200 | Avg Loss: 0.0158 | Grad Norm: 0.01068915\n",
      "Epoch 3 | Step 1618300 | Avg Loss: 0.0158 | Grad Norm: 0.00939936\n",
      "Epoch 3 | Step 1618400 | Avg Loss: 0.0159 | Grad Norm: 0.00811197\n",
      "Epoch 3 | Step 1618500 | Avg Loss: 0.0158 | Grad Norm: 0.00822977\n",
      "Epoch 3 | Step 1618600 | Avg Loss: 0.0159 | Grad Norm: 0.00985670\n",
      "Epoch 3 | Step 1618700 | Avg Loss: 0.0162 | Grad Norm: 0.00814538\n",
      "Epoch 3 | Step 1618800 | Avg Loss: 0.0161 | Grad Norm: 0.00921970\n",
      "Epoch 3 | Step 1618900 | Avg Loss: 0.0161 | Grad Norm: 0.00786897\n",
      "Epoch 3 | Step 1619000 | Avg Loss: 0.0160 | Grad Norm: 0.00944856\n",
      "Epoch 3 | Step 1619100 | Avg Loss: 0.0160 | Grad Norm: 0.01285024\n",
      "Epoch 3 | Step 1619200 | Avg Loss: 0.0163 | Grad Norm: 0.01010926\n",
      "Epoch 3 | Step 1619300 | Avg Loss: 0.0166 | Grad Norm: 0.00907003\n",
      "Epoch 3 | Step 1619400 | Avg Loss: 0.0168 | Grad Norm: 0.00808477\n",
      "Epoch 3 | Step 1619500 | Avg Loss: 0.0162 | Grad Norm: 0.00912364\n",
      "Epoch 3 | Step 1619600 | Avg Loss: 0.0158 | Grad Norm: 0.00849801\n",
      "Epoch 3 | Step 1619700 | Avg Loss: 0.0158 | Grad Norm: 0.00969570\n",
      "Epoch 3 | Step 1619800 | Avg Loss: 0.0156 | Grad Norm: 0.00855034\n",
      "Epoch 3 | Step 1619900 | Avg Loss: 0.0156 | Grad Norm: 0.01006854\n",
      "Epoch 3 | Step 1620000 | Avg Loss: 0.0156 | Grad Norm: 0.00857414\n",
      "Epoch 3 | Step 1620100 | Avg Loss: 0.0151 | Grad Norm: 0.00945756\n",
      "Epoch 3 | Step 1620200 | Avg Loss: 0.0156 | Grad Norm: 0.01420610\n",
      "Epoch 3 | Step 1620300 | Avg Loss: 0.0152 | Grad Norm: 0.00882367\n",
      "Epoch 3 | Step 1620400 | Avg Loss: 0.0152 | Grad Norm: 0.00772962\n",
      "Epoch 3 | Step 1620500 | Avg Loss: 0.0153 | Grad Norm: 0.00854663\n",
      "Epoch 3 | Step 1620600 | Avg Loss: 0.0154 | Grad Norm: 0.01002968\n",
      "Epoch 3 | Step 1620700 | Avg Loss: 0.0157 | Grad Norm: 0.00887307\n",
      "Epoch 3 | Step 1620800 | Avg Loss: 0.0156 | Grad Norm: 0.01198257\n",
      "Epoch 3 | Step 1620900 | Avg Loss: 0.0157 | Grad Norm: 0.01120293\n",
      "Epoch 3 | Step 1621000 | Avg Loss: 0.0159 | Grad Norm: 0.01096275\n",
      "Epoch 3 | Step 1621100 | Avg Loss: 0.0155 | Grad Norm: 0.00841404\n",
      "Epoch 3 | Step 1621200 | Avg Loss: 0.0156 | Grad Norm: 0.00899994\n",
      "Epoch 3 | Step 1621300 | Avg Loss: 0.0155 | Grad Norm: 0.00850575\n",
      "Epoch 3 | Step 1621400 | Avg Loss: 0.0156 | Grad Norm: 0.00959649\n",
      "Epoch 3 | Step 1621500 | Avg Loss: 0.0156 | Grad Norm: 0.00966840\n",
      "Epoch 3 | Step 1621600 | Avg Loss: 0.0157 | Grad Norm: 0.00963679\n",
      "Epoch 3 | Step 1621700 | Avg Loss: 0.0158 | Grad Norm: 0.00906307\n",
      "Epoch 3 | Step 1621800 | Avg Loss: 0.0159 | Grad Norm: 0.01047966\n",
      "Epoch 3 | Step 1621900 | Avg Loss: 0.0155 | Grad Norm: 0.00884024\n",
      "Epoch 3 | Step 1622000 | Avg Loss: 0.0154 | Grad Norm: 0.01178775\n",
      "Epoch 3 | Step 1622100 | Avg Loss: 0.0156 | Grad Norm: 0.00820831\n",
      "Epoch 3 | Step 1622200 | Avg Loss: 0.0152 | Grad Norm: 0.01044326\n",
      "Epoch 3 | Step 1622300 | Avg Loss: 0.0151 | Grad Norm: 0.00883593\n",
      "Epoch 3 | Step 1622400 | Avg Loss: 0.0153 | Grad Norm: 0.01104530\n",
      "Epoch 3 | Step 1622500 | Avg Loss: 0.0155 | Grad Norm: 0.00888382\n",
      "Epoch 3 | Step 1622600 | Avg Loss: 0.0159 | Grad Norm: 0.00917023\n",
      "Epoch 3 | Step 1622700 | Avg Loss: 0.0156 | Grad Norm: 0.00826112\n",
      "Epoch 3 | Step 1622800 | Avg Loss: 0.0155 | Grad Norm: 0.01004717\n",
      "Epoch 3 | Step 1622900 | Avg Loss: 0.0156 | Grad Norm: 0.00887101\n",
      "Epoch 3 | Step 1623000 | Avg Loss: 0.0157 | Grad Norm: 0.00823140\n",
      "Epoch 3 | Step 1623100 | Avg Loss: 0.0152 | Grad Norm: 0.00914097\n",
      "Epoch 3 | Step 1623200 | Avg Loss: 0.0153 | Grad Norm: 0.00790810\n",
      "Epoch 3 | Step 1623300 | Avg Loss: 0.0153 | Grad Norm: 0.01097318\n",
      "Epoch 3 | Step 1623400 | Avg Loss: 0.0155 | Grad Norm: 0.00912516\n",
      "Epoch 3 | Step 1623500 | Avg Loss: 0.0156 | Grad Norm: 0.01006810\n",
      "Epoch 3 | Step 1623600 | Avg Loss: 0.0155 | Grad Norm: 0.00880225\n",
      "Epoch 3 | Step 1623700 | Avg Loss: 0.0154 | Grad Norm: 0.00855989\n",
      "Epoch 3 | Step 1623800 | Avg Loss: 0.0156 | Grad Norm: 0.00901457\n",
      "Epoch 3 | Step 1623900 | Avg Loss: 0.0154 | Grad Norm: 0.01026162\n",
      "Epoch 3 | Step 1624000 | Avg Loss: 0.0157 | Grad Norm: 0.00804174\n",
      "Epoch 3 | Step 1624100 | Avg Loss: 0.0156 | Grad Norm: 0.00863411\n",
      "Epoch 3 | Step 1624200 | Avg Loss: 0.0154 | Grad Norm: 0.00924061\n",
      "Epoch 3 | Step 1624300 | Avg Loss: 0.0156 | Grad Norm: 0.00835357\n",
      "Epoch 3 | Step 1624400 | Avg Loss: 0.0156 | Grad Norm: 0.00854772\n",
      "Epoch 3 | Step 1624500 | Avg Loss: 0.0160 | Grad Norm: 0.00880363\n",
      "Epoch 3 | Step 1624600 | Avg Loss: 0.0164 | Grad Norm: 0.01146173\n",
      "Epoch 3 | Step 1624700 | Avg Loss: 0.0161 | Grad Norm: 0.01005972\n",
      "Epoch 3 | Step 1624800 | Avg Loss: 0.0157 | Grad Norm: 0.00922012\n",
      "Epoch 3 | Step 1624900 | Avg Loss: 0.0150 | Grad Norm: 0.00927485\n",
      "Epoch 3 | Step 1625000 | Avg Loss: 0.0151 | Grad Norm: 0.01006859\n",
      "Epoch 3 | Step 1625100 | Avg Loss: 0.0154 | Grad Norm: 0.01133545\n",
      "Epoch 3 | Step 1625200 | Avg Loss: 0.0154 | Grad Norm: 0.00817938\n",
      "Epoch 3 | Step 1625300 | Avg Loss: 0.0157 | Grad Norm: 0.00823729\n",
      "Epoch 3 | Step 1625400 | Avg Loss: 0.0158 | Grad Norm: 0.00900331\n",
      "Epoch 3 | Step 1625500 | Avg Loss: 0.0161 | Grad Norm: 0.00991498\n",
      "Epoch 3 | Step 1625600 | Avg Loss: 0.0160 | Grad Norm: 0.00915783\n",
      "Epoch 3 | Step 1625700 | Avg Loss: 0.0157 | Grad Norm: 0.00861830\n",
      "Epoch 3 | Step 1625800 | Avg Loss: 0.0159 | Grad Norm: 0.00812062\n",
      "Epoch 3 | Step 1625900 | Avg Loss: 0.0156 | Grad Norm: 0.00874374\n",
      "Epoch 3 | Step 1626000 | Avg Loss: 0.0155 | Grad Norm: 0.00790476\n",
      "Epoch 3 | Step 1626100 | Avg Loss: 0.0149 | Grad Norm: 0.00911508\n",
      "Epoch 3 | Step 1626200 | Avg Loss: 0.0149 | Grad Norm: 0.00828839\n",
      "Epoch 3 | Step 1626300 | Avg Loss: 0.0149 | Grad Norm: 0.01062660\n",
      "Epoch 3 | Step 1626400 | Avg Loss: 0.0149 | Grad Norm: 0.01002007\n",
      "Epoch 3 | Step 1626500 | Avg Loss: 0.0151 | Grad Norm: 0.00891821\n",
      "Epoch 3 | Step 1626600 | Avg Loss: 0.0151 | Grad Norm: 0.00923241\n",
      "Epoch 3 | Step 1626700 | Avg Loss: 0.0152 | Grad Norm: 0.00825582\n",
      "Epoch 3 | Step 1626800 | Avg Loss: 0.0153 | Grad Norm: 0.01218076\n",
      "Epoch 3 | Step 1626900 | Avg Loss: 0.0155 | Grad Norm: 0.00994928\n",
      "Epoch 3 | Step 1627000 | Avg Loss: 0.0160 | Grad Norm: 0.00928846\n",
      "Epoch 3 | Step 1627100 | Avg Loss: 0.0159 | Grad Norm: 0.00909900\n",
      "Epoch 3 | Step 1627200 | Avg Loss: 0.0159 | Grad Norm: 0.00867895\n",
      "Epoch 3 | Step 1627300 | Avg Loss: 0.0156 | Grad Norm: 0.00967374\n",
      "Epoch 3 | Step 1627400 | Avg Loss: 0.0154 | Grad Norm: 0.00959334\n",
      "Epoch 3 | Step 1627500 | Avg Loss: 0.0154 | Grad Norm: 0.00867030\n",
      "Epoch 3 | Step 1627600 | Avg Loss: 0.0154 | Grad Norm: 0.00971195\n",
      "Epoch 3 | Step 1627700 | Avg Loss: 0.0152 | Grad Norm: 0.00856043\n",
      "Epoch 3 | Step 1627800 | Avg Loss: 0.0150 | Grad Norm: 0.00905817\n",
      "Epoch 3 | Step 1627900 | Avg Loss: 0.0152 | Grad Norm: 0.00842355\n",
      "Epoch 3 | Step 1628000 | Avg Loss: 0.0148 | Grad Norm: 0.01086501\n",
      "Epoch 3 | Step 1628100 | Avg Loss: 0.0152 | Grad Norm: 0.00850020\n",
      "Epoch 3 | Step 1628200 | Avg Loss: 0.0150 | Grad Norm: 0.00760754\n",
      "Epoch 3 | Step 1628300 | Avg Loss: 0.0151 | Grad Norm: 0.00963315\n",
      "Epoch 3 | Step 1628400 | Avg Loss: 0.0150 | Grad Norm: 0.00904276\n",
      "Epoch 3 | Step 1628500 | Avg Loss: 0.0152 | Grad Norm: 0.00948324\n",
      "Epoch 3 | Step 1628600 | Avg Loss: 0.0152 | Grad Norm: 0.00905331\n",
      "Epoch 3 | Step 1628700 | Avg Loss: 0.0153 | Grad Norm: 0.01080864\n",
      "Epoch 3 | Step 1628800 | Avg Loss: 0.0153 | Grad Norm: 0.00997058\n",
      "Epoch 3 | Step 1628900 | Avg Loss: 0.0156 | Grad Norm: 0.00994110\n",
      "Epoch 3 | Step 1629000 | Avg Loss: 0.0160 | Grad Norm: 0.00922540\n",
      "Epoch 3 | Step 1629100 | Avg Loss: 0.0163 | Grad Norm: 0.00930984\n",
      "Epoch 3 | Step 1629200 | Avg Loss: 0.0156 | Grad Norm: 0.00879406\n",
      "Epoch 3 | Step 1629300 | Avg Loss: 0.0150 | Grad Norm: 0.00830152\n",
      "Epoch 3 | Step 1629400 | Avg Loss: 0.0151 | Grad Norm: 0.00828837\n",
      "Epoch 3 | Step 1629500 | Avg Loss: 0.0155 | Grad Norm: 0.00741862\n",
      "Epoch 3 | Step 1629600 | Avg Loss: 0.0153 | Grad Norm: 0.00810845\n",
      "Epoch 3 | Step 1629700 | Avg Loss: 0.0155 | Grad Norm: 0.00839866\n",
      "Epoch 3 | Step 1629800 | Avg Loss: 0.0153 | Grad Norm: 0.01002917\n",
      "Epoch 3 | Step 1629900 | Avg Loss: 0.0154 | Grad Norm: 0.00848940\n",
      "Epoch 3 | Step 1630000 | Avg Loss: 0.0158 | Grad Norm: 0.00818841\n",
      "Epoch 3 | Step 1630100 | Avg Loss: 0.0160 | Grad Norm: 0.00858160\n",
      "Epoch 3 | Step 1630200 | Avg Loss: 0.0162 | Grad Norm: 0.00933695\n",
      "Epoch 3 | Step 1630300 | Avg Loss: 0.0159 | Grad Norm: 0.00885597\n",
      "Epoch 3 | Step 1630400 | Avg Loss: 0.0155 | Grad Norm: 0.00888928\n",
      "Epoch 3 | Step 1630500 | Avg Loss: 0.0152 | Grad Norm: 0.00813004\n",
      "Epoch 3 | Step 1630600 | Avg Loss: 0.0156 | Grad Norm: 0.00971165\n",
      "Epoch 3 | Step 1630700 | Avg Loss: 0.0155 | Grad Norm: 0.00922841\n",
      "Epoch 3 | Step 1630800 | Avg Loss: 0.0154 | Grad Norm: 0.00781700\n",
      "Epoch 3 | Step 1630900 | Avg Loss: 0.0158 | Grad Norm: 0.01193225\n",
      "Epoch 3 | Step 1631000 | Avg Loss: 0.0156 | Grad Norm: 0.01038178\n",
      "Epoch 3 | Step 1631100 | Avg Loss: 0.0156 | Grad Norm: 0.00860973\n",
      "Epoch 3 | Step 1631200 | Avg Loss: 0.0155 | Grad Norm: 0.00844757\n",
      "Epoch 3 | Step 1631300 | Avg Loss: 0.0157 | Grad Norm: 0.00802797\n",
      "Epoch 3 | Step 1631400 | Avg Loss: 0.0159 | Grad Norm: 0.00920648\n",
      "Epoch 3 | Step 1631500 | Avg Loss: 0.0162 | Grad Norm: 0.00795191\n",
      "Epoch 3 | Step 1631600 | Avg Loss: 0.0159 | Grad Norm: 0.00877221\n",
      "Epoch 3 | Step 1631700 | Avg Loss: 0.0158 | Grad Norm: 0.00997278\n",
      "Epoch 3 | Step 1631800 | Avg Loss: 0.0157 | Grad Norm: 0.00903882\n",
      "Epoch 3 | Step 1631900 | Avg Loss: 0.0153 | Grad Norm: 0.00769312\n",
      "Epoch 3 | Step 1632000 | Avg Loss: 0.0157 | Grad Norm: 0.00817631\n",
      "Epoch 3 | Step 1632100 | Avg Loss: 0.0155 | Grad Norm: 0.01071931\n",
      "Epoch 3 | Step 1632200 | Avg Loss: 0.0156 | Grad Norm: 0.00981616\n",
      "Epoch 3 | Step 1632300 | Avg Loss: 0.0152 | Grad Norm: 0.00991275\n",
      "Epoch 3 | Step 1632400 | Avg Loss: 0.0155 | Grad Norm: 0.00882065\n",
      "Epoch 3 | Step 1632500 | Avg Loss: 0.0160 | Grad Norm: 0.00920379\n",
      "Epoch 3 | Step 1632600 | Avg Loss: 0.0159 | Grad Norm: 0.00925325\n",
      "Epoch 3 | Step 1632700 | Avg Loss: 0.0157 | Grad Norm: 0.00823687\n",
      "Epoch 3 | Step 1632800 | Avg Loss: 0.0157 | Grad Norm: 0.00950623\n",
      "Epoch 3 | Step 1632900 | Avg Loss: 0.0155 | Grad Norm: 0.01078662\n",
      "Epoch 3 | Step 1633000 | Avg Loss: 0.0158 | Grad Norm: 0.01171905\n",
      "Epoch 3 | Step 1633100 | Avg Loss: 0.0156 | Grad Norm: 0.00848109\n",
      "Epoch 3 | Step 1633200 | Avg Loss: 0.0157 | Grad Norm: 0.00917967\n",
      "Epoch 3 | Step 1633300 | Avg Loss: 0.0152 | Grad Norm: 0.00985058\n",
      "Epoch 3 | Step 1633400 | Avg Loss: 0.0152 | Grad Norm: 0.00916211\n",
      "Epoch 3 | Step 1633500 | Avg Loss: 0.0157 | Grad Norm: 0.00823776\n",
      "Epoch 3 | Step 1633600 | Avg Loss: 0.0155 | Grad Norm: 0.00996534\n",
      "Epoch 3 | Step 1633700 | Avg Loss: 0.0160 | Grad Norm: 0.00849337\n",
      "Epoch 3 | Step 1633800 | Avg Loss: 0.0158 | Grad Norm: 0.00973434\n",
      "Epoch 3 | Step 1633900 | Avg Loss: 0.0155 | Grad Norm: 0.01043581\n",
      "Epoch 3 | Step 1634000 | Avg Loss: 0.0153 | Grad Norm: 0.00796902\n",
      "Epoch 3 | Step 1634100 | Avg Loss: 0.0158 | Grad Norm: 0.00887212\n",
      "Epoch 3 | Step 1634200 | Avg Loss: 0.0156 | Grad Norm: 0.00839608\n",
      "Epoch 3 | Step 1634300 | Avg Loss: 0.0155 | Grad Norm: 0.00817453\n",
      "Epoch 3 | Step 1634400 | Avg Loss: 0.0154 | Grad Norm: 0.00852681\n",
      "Epoch 3 | Step 1634500 | Avg Loss: 0.0160 | Grad Norm: 0.00937221\n",
      "Epoch 3 | Step 1634600 | Avg Loss: 0.0157 | Grad Norm: 0.01052616\n",
      "Epoch 3 | Step 1634700 | Avg Loss: 0.0155 | Grad Norm: 0.00988192\n",
      "Epoch 3 | Step 1634800 | Avg Loss: 0.0154 | Grad Norm: 0.00812843\n",
      "Epoch 3 | Step 1634900 | Avg Loss: 0.0156 | Grad Norm: 0.00980346\n",
      "Epoch 3 | Step 1635000 | Avg Loss: 0.0157 | Grad Norm: 0.01065177\n",
      "Epoch 3 | Step 1635100 | Avg Loss: 0.0155 | Grad Norm: 0.00889092\n",
      "Epoch 3 | Step 1635200 | Avg Loss: 0.0156 | Grad Norm: 0.00899930\n",
      "Epoch 3 | Step 1635300 | Avg Loss: 0.0155 | Grad Norm: 0.00815988\n",
      "Epoch 3 | Step 1635400 | Avg Loss: 0.0158 | Grad Norm: 0.00873592\n",
      "Epoch 3 | Step 1635500 | Avg Loss: 0.0162 | Grad Norm: 0.00817976\n",
      "Epoch 3 | Step 1635600 | Avg Loss: 0.0161 | Grad Norm: 0.00929433\n",
      "Epoch 3 | Step 1635700 | Avg Loss: 0.0160 | Grad Norm: 0.00887079\n",
      "Epoch 3 | Step 1635800 | Avg Loss: 0.0160 | Grad Norm: 0.00999292\n",
      "Epoch 3 | Step 1635900 | Avg Loss: 0.0156 | Grad Norm: 0.00760828\n",
      "Epoch 3 | Step 1636000 | Avg Loss: 0.0152 | Grad Norm: 0.00843453\n",
      "Epoch 3 | Step 1636100 | Avg Loss: 0.0154 | Grad Norm: 0.00898588\n",
      "Epoch 3 | Step 1636200 | Avg Loss: 0.0154 | Grad Norm: 0.01037292\n",
      "Epoch 3 | Step 1636300 | Avg Loss: 0.0155 | Grad Norm: 0.01029050\n",
      "Epoch 3 | Step 1636400 | Avg Loss: 0.0150 | Grad Norm: 0.00991030\n",
      "Epoch 3 | Step 1636500 | Avg Loss: 0.0151 | Grad Norm: 0.00937583\n",
      "Epoch 3 | Step 1636600 | Avg Loss: 0.0155 | Grad Norm: 0.00949048\n",
      "Epoch 3 | Step 1636700 | Avg Loss: 0.0154 | Grad Norm: 0.00889281\n",
      "Epoch 3 | Step 1636800 | Avg Loss: 0.0152 | Grad Norm: 0.00818123\n",
      "Epoch 3 | Step 1636900 | Avg Loss: 0.0156 | Grad Norm: 0.00848479\n",
      "Epoch 3 | Step 1637000 | Avg Loss: 0.0157 | Grad Norm: 0.01202334\n",
      "Epoch 3 | Step 1637100 | Avg Loss: 0.0158 | Grad Norm: 0.00769247\n",
      "Epoch 3 | Step 1637200 | Avg Loss: 0.0157 | Grad Norm: 0.00799946\n",
      "Epoch 3 | Step 1637300 | Avg Loss: 0.0155 | Grad Norm: 0.00918943\n",
      "Epoch 3 | Step 1637400 | Avg Loss: 0.0155 | Grad Norm: 0.00768095\n",
      "Epoch 3 | Step 1637500 | Avg Loss: 0.0160 | Grad Norm: 0.01150336\n",
      "Epoch 3 | Step 1637600 | Avg Loss: 0.0159 | Grad Norm: 0.00935727\n",
      "Epoch 3 | Step 1637700 | Avg Loss: 0.0159 | Grad Norm: 0.00800906\n",
      "Epoch 3 | Step 1637800 | Avg Loss: 0.0154 | Grad Norm: 0.00896458\n",
      "Epoch 3 | Step 1637900 | Avg Loss: 0.0154 | Grad Norm: 0.00878819\n",
      "Epoch 3 | Step 1638000 | Avg Loss: 0.0154 | Grad Norm: 0.01236025\n",
      "Epoch 3 | Step 1638100 | Avg Loss: 0.0153 | Grad Norm: 0.00819219\n",
      "Epoch 3 | Step 1638200 | Avg Loss: 0.0154 | Grad Norm: 0.00852450\n",
      "Epoch 3 | Step 1638300 | Avg Loss: 0.0159 | Grad Norm: 0.01048610\n",
      "Epoch 3 | Step 1638400 | Avg Loss: 0.0158 | Grad Norm: 0.01007873\n",
      "Epoch 3 | Step 1638500 | Avg Loss: 0.0157 | Grad Norm: 0.00849060\n",
      "Epoch 3 | Step 1638600 | Avg Loss: 0.0156 | Grad Norm: 0.00894524\n",
      "Epoch 3 | Step 1638700 | Avg Loss: 0.0157 | Grad Norm: 0.00939320\n",
      "Epoch 3 | Step 1638800 | Avg Loss: 0.0160 | Grad Norm: 0.01037772\n",
      "Epoch 3 | Step 1638900 | Avg Loss: 0.0158 | Grad Norm: 0.00768528\n",
      "Epoch 3 | Step 1639000 | Avg Loss: 0.0157 | Grad Norm: 0.00959991\n",
      "Epoch 3 | Step 1639100 | Avg Loss: 0.0152 | Grad Norm: 0.00981702\n",
      "Epoch 3 | Step 1639200 | Avg Loss: 0.0157 | Grad Norm: 0.01052341\n",
      "Epoch 3 | Step 1639300 | Avg Loss: 0.0156 | Grad Norm: 0.01020344\n",
      "Epoch 3 | Step 1639400 | Avg Loss: 0.0156 | Grad Norm: 0.00855863\n",
      "Epoch 3 | Step 1639500 | Avg Loss: 0.0157 | Grad Norm: 0.00940084\n",
      "Epoch 3 | Step 1639600 | Avg Loss: 0.0157 | Grad Norm: 0.00951147\n",
      "Epoch 3 | Step 1639700 | Avg Loss: 0.0156 | Grad Norm: 0.00766288\n",
      "Epoch 3 | Step 1639800 | Avg Loss: 0.0159 | Grad Norm: 0.00927538\n",
      "Epoch 3 | Step 1639900 | Avg Loss: 0.0159 | Grad Norm: 0.01030339\n",
      "Epoch 3 | Step 1640000 | Avg Loss: 0.0159 | Grad Norm: 0.00906399\n",
      "Epoch 3 | Step 1640100 | Avg Loss: 0.0162 | Grad Norm: 0.00848110\n",
      "Epoch 3 | Step 1640200 | Avg Loss: 0.0164 | Grad Norm: 0.00938241\n",
      "Epoch 3 | Step 1640300 | Avg Loss: 0.0165 | Grad Norm: 0.00905877\n",
      "Epoch 3 | Step 1640400 | Avg Loss: 0.0164 | Grad Norm: 0.00778201\n",
      "Epoch 3 | Step 1640500 | Avg Loss: 0.0160 | Grad Norm: 0.01108298\n",
      "Epoch 3 | Step 1640600 | Avg Loss: 0.0159 | Grad Norm: 0.00781320\n",
      "Epoch 3 | Step 1640700 | Avg Loss: 0.0162 | Grad Norm: 0.00827558\n",
      "Epoch 3 | Step 1640800 | Avg Loss: 0.0163 | Grad Norm: 0.00976974\n",
      "Epoch 3 | Step 1640900 | Avg Loss: 0.0162 | Grad Norm: 0.00882386\n",
      "Epoch 3 | Step 1641000 | Avg Loss: 0.0162 | Grad Norm: 0.01062570\n",
      "Epoch 3 | Step 1641100 | Avg Loss: 0.0161 | Grad Norm: 0.00902102\n",
      "Epoch 3 | Step 1641200 | Avg Loss: 0.0160 | Grad Norm: 0.00940306\n",
      "Epoch 3 | Step 1641300 | Avg Loss: 0.0163 | Grad Norm: 0.00906764\n",
      "Epoch 3 | Step 1641400 | Avg Loss: 0.0162 | Grad Norm: 0.00944794\n",
      "Epoch 3 | Step 1641500 | Avg Loss: 0.0163 | Grad Norm: 0.00850621\n",
      "Epoch 3 | Step 1641600 | Avg Loss: 0.0158 | Grad Norm: 0.00873584\n",
      "Epoch 3 | Step 1641700 | Avg Loss: 0.0158 | Grad Norm: 0.00913979\n",
      "Epoch 3 | Step 1641800 | Avg Loss: 0.0156 | Grad Norm: 0.01244126\n",
      "Epoch 3 | Step 1641900 | Avg Loss: 0.0156 | Grad Norm: 0.00841896\n",
      "Epoch 3 | Step 1642000 | Avg Loss: 0.0157 | Grad Norm: 0.01059387\n",
      "Epoch 3 | Step 1642100 | Avg Loss: 0.0156 | Grad Norm: 0.00873192\n",
      "Epoch 3 | Step 1642200 | Avg Loss: 0.0160 | Grad Norm: 0.00751604\n",
      "Epoch 3 | Step 1642300 | Avg Loss: 0.0162 | Grad Norm: 0.01019551\n",
      "Epoch 3 | Step 1642400 | Avg Loss: 0.0163 | Grad Norm: 0.01018385\n",
      "Epoch 3 | Step 1642500 | Avg Loss: 0.0164 | Grad Norm: 0.00821610\n",
      "Epoch 3 | Step 1642600 | Avg Loss: 0.0160 | Grad Norm: 0.00962434\n",
      "Epoch 3 | Step 1642700 | Avg Loss: 0.0158 | Grad Norm: 0.00800096\n",
      "Epoch 3 | Step 1642800 | Avg Loss: 0.0156 | Grad Norm: 0.00782314\n",
      "Epoch 3 | Step 1642900 | Avg Loss: 0.0158 | Grad Norm: 0.00955313\n",
      "Epoch 3 | Step 1643000 | Avg Loss: 0.0157 | Grad Norm: 0.00982370\n",
      "Epoch 3 | Step 1643100 | Avg Loss: 0.0159 | Grad Norm: 0.00820592\n",
      "Epoch 3 | Step 1643200 | Avg Loss: 0.0158 | Grad Norm: 0.00991096\n",
      "Epoch 3 | Step 1643300 | Avg Loss: 0.0156 | Grad Norm: 0.00804416\n",
      "Epoch 3 | Step 1643400 | Avg Loss: 0.0153 | Grad Norm: 0.01082982\n",
      "Epoch 3 | Step 1643500 | Avg Loss: 0.0153 | Grad Norm: 0.01136599\n",
      "Epoch 3 | Step 1643600 | Avg Loss: 0.0154 | Grad Norm: 0.00927363\n",
      "Epoch 3 | Step 1643700 | Avg Loss: 0.0152 | Grad Norm: 0.00861485\n",
      "Epoch 3 | Step 1643800 | Avg Loss: 0.0149 | Grad Norm: 0.00811307\n",
      "Epoch 3 | Step 1643900 | Avg Loss: 0.0154 | Grad Norm: 0.01022485\n",
      "Epoch 3 | Step 1644000 | Avg Loss: 0.0153 | Grad Norm: 0.00865221\n",
      "Epoch 3 | Step 1644100 | Avg Loss: 0.0153 | Grad Norm: 0.01023201\n",
      "Epoch 3 | Step 1644200 | Avg Loss: 0.0154 | Grad Norm: 0.00921206\n",
      "Epoch 3 | Step 1644300 | Avg Loss: 0.0158 | Grad Norm: 0.00878309\n",
      "Epoch 3 | Step 1644400 | Avg Loss: 0.0156 | Grad Norm: 0.01011314\n",
      "Epoch 3 | Step 1644500 | Avg Loss: 0.0156 | Grad Norm: 0.00829786\n",
      "Epoch 3 | Step 1644600 | Avg Loss: 0.0155 | Grad Norm: 0.00879164\n",
      "Epoch 3 | Step 1644700 | Avg Loss: 0.0152 | Grad Norm: 0.01010337\n",
      "Epoch 3 | Step 1644800 | Avg Loss: 0.0154 | Grad Norm: 0.00859354\n",
      "Epoch 3 | Step 1644900 | Avg Loss: 0.0155 | Grad Norm: 0.00835750\n",
      "Epoch 3 | Step 1645000 | Avg Loss: 0.0154 | Grad Norm: 0.00827445\n",
      "Epoch 3 | Step 1645100 | Avg Loss: 0.0154 | Grad Norm: 0.00949086\n",
      "Epoch 3 | Step 1645200 | Avg Loss: 0.0152 | Grad Norm: 0.00849458\n",
      "Epoch 3 | Step 1645300 | Avg Loss: 0.0153 | Grad Norm: 0.00974645\n",
      "Epoch 3 | Step 1645400 | Avg Loss: 0.0154 | Grad Norm: 0.00937019\n",
      "Epoch 3 | Step 1645500 | Avg Loss: 0.0157 | Grad Norm: 0.00910423\n",
      "Epoch 3 | Step 1645600 | Avg Loss: 0.0157 | Grad Norm: 0.00850889\n",
      "Epoch 3 | Step 1645700 | Avg Loss: 0.0152 | Grad Norm: 0.00924354\n",
      "Epoch 3 | Step 1645800 | Avg Loss: 0.0157 | Grad Norm: 0.00943226\n",
      "Epoch 3 | Step 1645900 | Avg Loss: 0.0156 | Grad Norm: 0.00855892\n",
      "Epoch 3 | Step 1646000 | Avg Loss: 0.0159 | Grad Norm: 0.00846610\n",
      "Epoch 3 | Step 1646100 | Avg Loss: 0.0158 | Grad Norm: 0.00863228\n",
      "Epoch 3 | Step 1646200 | Avg Loss: 0.0155 | Grad Norm: 0.00834153\n",
      "Epoch 3 | Step 1646300 | Avg Loss: 0.0162 | Grad Norm: 0.00849535\n",
      "Epoch 3 | Step 1646400 | Avg Loss: 0.0160 | Grad Norm: 0.00794321\n",
      "Epoch 3 | Step 1646500 | Avg Loss: 0.0160 | Grad Norm: 0.00843977\n",
      "Epoch 3 | Step 1646600 | Avg Loss: 0.0155 | Grad Norm: 0.00928856\n",
      "Epoch 3 | Step 1646700 | Avg Loss: 0.0154 | Grad Norm: 0.00978210\n",
      "Epoch 3 | Step 1646800 | Avg Loss: 0.0158 | Grad Norm: 0.00932354\n",
      "Epoch 3 | Step 1646900 | Avg Loss: 0.0162 | Grad Norm: 0.00774676\n",
      "Epoch 3 | Step 1647000 | Avg Loss: 0.0160 | Grad Norm: 0.00875166\n",
      "Epoch 3 | Step 1647100 | Avg Loss: 0.0156 | Grad Norm: 0.00863506\n",
      "Epoch 3 | Step 1647200 | Avg Loss: 0.0156 | Grad Norm: 0.01339127\n",
      "Epoch 3 | Step 1647300 | Avg Loss: 0.0158 | Grad Norm: 0.00888968\n",
      "Epoch 3 | Step 1647400 | Avg Loss: 0.0157 | Grad Norm: 0.00947025\n",
      "Epoch 3 | Step 1647500 | Avg Loss: 0.0156 | Grad Norm: 0.00992093\n",
      "Epoch 3 | Step 1647600 | Avg Loss: 0.0154 | Grad Norm: 0.00986169\n",
      "Epoch 3 | Step 1647700 | Avg Loss: 0.0153 | Grad Norm: 0.01130124\n",
      "Epoch 3 | Step 1647800 | Avg Loss: 0.0155 | Grad Norm: 0.00735651\n",
      "Epoch 3 | Step 1647900 | Avg Loss: 0.0152 | Grad Norm: 0.01023025\n",
      "Epoch 3 | Step 1648000 | Avg Loss: 0.0149 | Grad Norm: 0.00828337\n",
      "Epoch 3 | Step 1648100 | Avg Loss: 0.0149 | Grad Norm: 0.00966392\n",
      "Epoch 3 | Step 1648200 | Avg Loss: 0.0148 | Grad Norm: 0.00861736\n",
      "Epoch 3 | Step 1648300 | Avg Loss: 0.0153 | Grad Norm: 0.00975038\n",
      "Epoch 3 | Step 1648400 | Avg Loss: 0.0153 | Grad Norm: 0.01021597\n",
      "Epoch 3 | Step 1648500 | Avg Loss: 0.0155 | Grad Norm: 0.00965929\n",
      "Epoch 3 | Step 1648600 | Avg Loss: 0.0154 | Grad Norm: 0.00962495\n",
      "Epoch 3 | Step 1648700 | Avg Loss: 0.0156 | Grad Norm: 0.00956882\n",
      "Epoch 3 | Step 1648800 | Avg Loss: 0.0158 | Grad Norm: 0.00852486\n",
      "Epoch 3 | Step 1648900 | Avg Loss: 0.0156 | Grad Norm: 0.00902147\n",
      "Epoch 3 | Step 1649000 | Avg Loss: 0.0155 | Grad Norm: 0.00976054\n",
      "Epoch 3 | Step 1649100 | Avg Loss: 0.0156 | Grad Norm: 0.00866619\n",
      "Epoch 3 | Step 1649200 | Avg Loss: 0.0156 | Grad Norm: 0.01086964\n",
      "Epoch 3 | Step 1649300 | Avg Loss: 0.0155 | Grad Norm: 0.00917979\n",
      "Epoch 3 | Step 1649400 | Avg Loss: 0.0155 | Grad Norm: 0.00959843\n",
      "Epoch 3 | Step 1649500 | Avg Loss: 0.0160 | Grad Norm: 0.00861248\n",
      "Epoch 3 | Step 1649600 | Avg Loss: 0.0158 | Grad Norm: 0.01085894\n",
      "Epoch 3 | Step 1649700 | Avg Loss: 0.0159 | Grad Norm: 0.00917186\n",
      "Epoch 3 | Step 1649800 | Avg Loss: 0.0161 | Grad Norm: 0.00975794\n",
      "Epoch 3 | Step 1649900 | Avg Loss: 0.0163 | Grad Norm: 0.01025092\n",
      "Epoch 3 | Step 1650000 | Avg Loss: 0.0159 | Grad Norm: 0.01039299\n",
      "Epoch 3 | Step 1650100 | Avg Loss: 0.0160 | Grad Norm: 0.01035772\n",
      "Epoch 3 | Step 1650200 | Avg Loss: 0.0163 | Grad Norm: 0.00922921\n",
      "Epoch 3 | Step 1650300 | Avg Loss: 0.0163 | Grad Norm: 0.01068114\n",
      "Epoch 3 | Step 1650400 | Avg Loss: 0.0158 | Grad Norm: 0.00939333\n",
      "Epoch 3 | Step 1650500 | Avg Loss: 0.0159 | Grad Norm: 0.00988734\n",
      "Epoch 3 | Step 1650600 | Avg Loss: 0.0159 | Grad Norm: 0.00807595\n",
      "Epoch 3 | Step 1650700 | Avg Loss: 0.0160 | Grad Norm: 0.01025223\n",
      "Epoch 3 | Step 1650800 | Avg Loss: 0.0157 | Grad Norm: 0.00877226\n",
      "Epoch 3 | Step 1650900 | Avg Loss: 0.0158 | Grad Norm: 0.00892059\n",
      "Epoch 3 | Step 1651000 | Avg Loss: 0.0154 | Grad Norm: 0.00761753\n",
      "Epoch 3 | Step 1651100 | Avg Loss: 0.0152 | Grad Norm: 0.00819824\n",
      "Epoch 3 | Step 1651200 | Avg Loss: 0.0154 | Grad Norm: 0.00928954\n",
      "Epoch 3 | Step 1651300 | Avg Loss: 0.0154 | Grad Norm: 0.01206971\n",
      "Epoch 3 | Step 1651400 | Avg Loss: 0.0153 | Grad Norm: 0.00787968\n",
      "Epoch 3 | Step 1651500 | Avg Loss: 0.0155 | Grad Norm: 0.00841296\n",
      "Epoch 3 | Step 1651600 | Avg Loss: 0.0157 | Grad Norm: 0.00965133\n",
      "Epoch 3 | Step 1651700 | Avg Loss: 0.0158 | Grad Norm: 0.00897933\n",
      "Epoch 3 | Step 1651800 | Avg Loss: 0.0159 | Grad Norm: 0.01092755\n",
      "Epoch 3 | Step 1651900 | Avg Loss: 0.0159 | Grad Norm: 0.00927428\n",
      "Epoch 3 | Step 1652000 | Avg Loss: 0.0162 | Grad Norm: 0.00811787\n",
      "Epoch 3 | Step 1652100 | Avg Loss: 0.0158 | Grad Norm: 0.00917448\n",
      "Epoch 3 | Step 1652200 | Avg Loss: 0.0152 | Grad Norm: 0.00860704\n",
      "Epoch 3 | Step 1652300 | Avg Loss: 0.0149 | Grad Norm: 0.00831288\n",
      "Epoch 3 | Step 1652400 | Avg Loss: 0.0155 | Grad Norm: 0.00789422\n",
      "Epoch 3 | Step 1652500 | Avg Loss: 0.0158 | Grad Norm: 0.00952142\n",
      "Epoch 3 | Step 1652600 | Avg Loss: 0.0157 | Grad Norm: 0.00919945\n",
      "Epoch 3 | Step 1652700 | Avg Loss: 0.0158 | Grad Norm: 0.00848598\n",
      "Epoch 3 | Step 1652800 | Avg Loss: 0.0157 | Grad Norm: 0.00858876\n",
      "Epoch 3 | Step 1652900 | Avg Loss: 0.0160 | Grad Norm: 0.01153167\n",
      "Epoch 3 | Step 1653000 | Avg Loss: 0.0159 | Grad Norm: 0.00882990\n",
      "Epoch 3 | Step 1653100 | Avg Loss: 0.0159 | Grad Norm: 0.00977237\n",
      "Epoch 3 | Step 1653200 | Avg Loss: 0.0161 | Grad Norm: 0.00805517\n",
      "Epoch 3 | Step 1653300 | Avg Loss: 0.0154 | Grad Norm: 0.00888328\n",
      "Epoch 3 | Step 1653400 | Avg Loss: 0.0152 | Grad Norm: 0.00845724\n",
      "Epoch 3 | Step 1653500 | Avg Loss: 0.0150 | Grad Norm: 0.00955164\n",
      "Epoch 3 | Step 1653600 | Avg Loss: 0.0149 | Grad Norm: 0.00826852\n",
      "Epoch 3 | Step 1653700 | Avg Loss: 0.0155 | Grad Norm: 0.00920350\n",
      "Epoch 3 | Step 1653800 | Avg Loss: 0.0159 | Grad Norm: 0.00843022\n",
      "Epoch 3 | Step 1653900 | Avg Loss: 0.0159 | Grad Norm: 0.00924350\n",
      "Epoch 3 | Step 1654000 | Avg Loss: 0.0152 | Grad Norm: 0.00925016\n",
      "Epoch 3 | Step 1654100 | Avg Loss: 0.0156 | Grad Norm: 0.00899217\n",
      "Epoch 3 | Step 1654200 | Avg Loss: 0.0154 | Grad Norm: 0.00897952\n",
      "Epoch 3 | Step 1654300 | Avg Loss: 0.0150 | Grad Norm: 0.00902752\n",
      "Epoch 3 | Step 1654400 | Avg Loss: 0.0157 | Grad Norm: 0.00945678\n",
      "Epoch 3 | Step 1654500 | Avg Loss: 0.0158 | Grad Norm: 0.01101213\n",
      "Epoch 3 | Step 1654600 | Avg Loss: 0.0157 | Grad Norm: 0.00873346\n",
      "Epoch 3 | Step 1654700 | Avg Loss: 0.0155 | Grad Norm: 0.00976116\n",
      "Epoch 3 | Step 1654800 | Avg Loss: 0.0159 | Grad Norm: 0.00993594\n",
      "Epoch 3 | Step 1654900 | Avg Loss: 0.0158 | Grad Norm: 0.01146260\n",
      "Epoch 3 | Step 1655000 | Avg Loss: 0.0157 | Grad Norm: 0.00848026\n",
      "Epoch 3 | Step 1655100 | Avg Loss: 0.0155 | Grad Norm: 0.00863322\n",
      "Epoch 3 | Step 1655200 | Avg Loss: 0.0157 | Grad Norm: 0.00978705\n",
      "Epoch 3 | Step 1655300 | Avg Loss: 0.0153 | Grad Norm: 0.00971929\n",
      "Epoch 3 | Step 1655400 | Avg Loss: 0.0151 | Grad Norm: 0.01037996\n",
      "Epoch 3 | Step 1655500 | Avg Loss: 0.0152 | Grad Norm: 0.00773812\n",
      "Epoch 3 | Step 1655600 | Avg Loss: 0.0155 | Grad Norm: 0.00802178\n",
      "Epoch 3 | Step 1655700 | Avg Loss: 0.0153 | Grad Norm: 0.00837646\n",
      "Epoch 3 | Step 1655800 | Avg Loss: 0.0150 | Grad Norm: 0.00812546\n",
      "Epoch 3 | Step 1655900 | Avg Loss: 0.0149 | Grad Norm: 0.00867922\n",
      "Epoch 3 | Step 1656000 | Avg Loss: 0.0152 | Grad Norm: 0.00963820\n",
      "Epoch 3 | Step 1656100 | Avg Loss: 0.0152 | Grad Norm: 0.00762975\n",
      "Epoch 3 | Step 1656200 | Avg Loss: 0.0152 | Grad Norm: 0.00895415\n",
      "Epoch 3 | Step 1656300 | Avg Loss: 0.0157 | Grad Norm: 0.01067584\n",
      "Epoch 3 | Step 1656400 | Avg Loss: 0.0159 | Grad Norm: 0.00976431\n",
      "Epoch 3 | Step 1656500 | Avg Loss: 0.0157 | Grad Norm: 0.00849346\n",
      "Epoch 3 | Step 1656600 | Avg Loss: 0.0158 | Grad Norm: 0.00917795\n",
      "Epoch 3 | Step 1656700 | Avg Loss: 0.0156 | Grad Norm: 0.00875870\n",
      "Epoch 3 | Step 1656800 | Avg Loss: 0.0154 | Grad Norm: 0.01051700\n",
      "Epoch 3 | Step 1656900 | Avg Loss: 0.0156 | Grad Norm: 0.00864751\n",
      "Epoch 3 | Step 1657000 | Avg Loss: 0.0155 | Grad Norm: 0.00872599\n",
      "Epoch 3 | Step 1657100 | Avg Loss: 0.0153 | Grad Norm: 0.00941364\n",
      "Epoch 3 | Step 1657200 | Avg Loss: 0.0151 | Grad Norm: 0.00936859\n",
      "Epoch 3 | Step 1657300 | Avg Loss: 0.0154 | Grad Norm: 0.00838350\n",
      "Epoch 3 | Step 1657400 | Avg Loss: 0.0152 | Grad Norm: 0.00923553\n",
      "Epoch 3 | Step 1657500 | Avg Loss: 0.0155 | Grad Norm: 0.00938162\n",
      "Epoch 3 | Step 1657600 | Avg Loss: 0.0156 | Grad Norm: 0.01060341\n",
      "Epoch 3 | Step 1657700 | Avg Loss: 0.0156 | Grad Norm: 0.00819409\n",
      "Epoch 3 | Step 1657800 | Avg Loss: 0.0155 | Grad Norm: 0.00934011\n",
      "Epoch 3 | Step 1657900 | Avg Loss: 0.0157 | Grad Norm: 0.00934791\n",
      "Epoch 3 | Step 1658000 | Avg Loss: 0.0156 | Grad Norm: 0.00882414\n",
      "Epoch 3 | Step 1658100 | Avg Loss: 0.0153 | Grad Norm: 0.00908545\n",
      "Epoch 3 | Step 1658200 | Avg Loss: 0.0154 | Grad Norm: 0.00908097\n",
      "Epoch 3 | Step 1658300 | Avg Loss: 0.0150 | Grad Norm: 0.01009988\n",
      "Epoch 3 | Step 1658400 | Avg Loss: 0.0150 | Grad Norm: 0.00857291\n",
      "Epoch 3 | Step 1658500 | Avg Loss: 0.0150 | Grad Norm: 0.00831321\n",
      "Epoch 3 | Step 1658600 | Avg Loss: 0.0147 | Grad Norm: 0.00787008\n",
      "Epoch 3 | Step 1658700 | Avg Loss: 0.0147 | Grad Norm: 0.00831627\n",
      "Epoch 3 | Step 1658800 | Avg Loss: 0.0148 | Grad Norm: 0.00788006\n",
      "Epoch 3 | Step 1658900 | Avg Loss: 0.0149 | Grad Norm: 0.00866154\n",
      "Epoch 3 | Step 1659000 | Avg Loss: 0.0155 | Grad Norm: 0.00897804\n",
      "Epoch 3 | Step 1659100 | Avg Loss: 0.0155 | Grad Norm: 0.00847710\n",
      "Epoch 3 | Step 1659200 | Avg Loss: 0.0157 | Grad Norm: 0.00873234\n",
      "Epoch 3 | Step 1659300 | Avg Loss: 0.0156 | Grad Norm: 0.00833732\n",
      "Epoch 3 | Step 1659400 | Avg Loss: 0.0157 | Grad Norm: 0.01031943\n",
      "Epoch 3 | Step 1659500 | Avg Loss: 0.0156 | Grad Norm: 0.00845628\n",
      "Epoch 3 | Step 1659600 | Avg Loss: 0.0154 | Grad Norm: 0.01040872\n",
      "Epoch 3 | Step 1659700 | Avg Loss: 0.0160 | Grad Norm: 0.01046343\n",
      "Epoch 3 | Step 1659800 | Avg Loss: 0.0159 | Grad Norm: 0.00939480\n",
      "Epoch 3 | Step 1659900 | Avg Loss: 0.0160 | Grad Norm: 0.00910555\n",
      "Epoch 3 | Step 1660000 | Avg Loss: 0.0162 | Grad Norm: 0.01027319\n",
      "Epoch 3 | Step 1660100 | Avg Loss: 0.0159 | Grad Norm: 0.00947280\n",
      "Epoch 3 | Step 1660200 | Avg Loss: 0.0159 | Grad Norm: 0.00969300\n",
      "Epoch 3 | Step 1660300 | Avg Loss: 0.0163 | Grad Norm: 0.00985205\n",
      "Epoch 3 | Step 1660400 | Avg Loss: 0.0166 | Grad Norm: 0.00873472\n",
      "Epoch 3 | Step 1660500 | Avg Loss: 0.0163 | Grad Norm: 0.01004995\n",
      "Epoch 3 | Step 1660600 | Avg Loss: 0.0158 | Grad Norm: 0.00941719\n",
      "Epoch 3 | Step 1660700 | Avg Loss: 0.0159 | Grad Norm: 0.00919209\n",
      "Epoch 3 | Step 1660800 | Avg Loss: 0.0161 | Grad Norm: 0.00844826\n",
      "Epoch 3 | Step 1660900 | Avg Loss: 0.0159 | Grad Norm: 0.01417442\n",
      "Epoch 3 | Step 1661000 | Avg Loss: 0.0158 | Grad Norm: 0.00956830\n",
      "Epoch 3 | Step 1661100 | Avg Loss: 0.0155 | Grad Norm: 0.01149835\n",
      "Epoch 3 | Step 1661200 | Avg Loss: 0.0157 | Grad Norm: 0.00913300\n",
      "Epoch 3 | Step 1661300 | Avg Loss: 0.0157 | Grad Norm: 0.00847198\n",
      "Epoch 3 | Step 1661400 | Avg Loss: 0.0157 | Grad Norm: 0.00786251\n",
      "Epoch 3 | Step 1661500 | Avg Loss: 0.0157 | Grad Norm: 0.00904324\n",
      "Epoch 3 | Step 1661600 | Avg Loss: 0.0158 | Grad Norm: 0.00820132\n",
      "Epoch 3 | Step 1661700 | Avg Loss: 0.0156 | Grad Norm: 0.00998670\n",
      "Epoch 3 | Step 1661800 | Avg Loss: 0.0154 | Grad Norm: 0.00729236\n",
      "Epoch 3 | Step 1661900 | Avg Loss: 0.0149 | Grad Norm: 0.00821600\n",
      "Epoch 3 | Step 1662000 | Avg Loss: 0.0149 | Grad Norm: 0.00791606\n",
      "Epoch 3 | Step 1662100 | Avg Loss: 0.0152 | Grad Norm: 0.00879956\n",
      "Epoch 3 | Step 1662200 | Avg Loss: 0.0150 | Grad Norm: 0.00948706\n",
      "Epoch 3 | Step 1662300 | Avg Loss: 0.0147 | Grad Norm: 0.00903538\n",
      "Epoch 3 | Step 1662400 | Avg Loss: 0.0151 | Grad Norm: 0.00965983\n",
      "Epoch 3 | Step 1662500 | Avg Loss: 0.0152 | Grad Norm: 0.00873530\n",
      "Epoch 3 | Step 1662600 | Avg Loss: 0.0153 | Grad Norm: 0.00990516\n",
      "Epoch 3 | Step 1662700 | Avg Loss: 0.0151 | Grad Norm: 0.00839146\n",
      "Epoch 3 | Step 1662800 | Avg Loss: 0.0152 | Grad Norm: 0.00986419\n",
      "Epoch 3 | Step 1662900 | Avg Loss: 0.0154 | Grad Norm: 0.00775053\n",
      "Epoch 3 | Step 1663000 | Avg Loss: 0.0155 | Grad Norm: 0.00915271\n",
      "Epoch 3 | Step 1663100 | Avg Loss: 0.0160 | Grad Norm: 0.00926861\n",
      "Epoch 3 | Step 1663200 | Avg Loss: 0.0161 | Grad Norm: 0.00937460\n",
      "Epoch 3 | Step 1663300 | Avg Loss: 0.0160 | Grad Norm: 0.00897007\n",
      "Epoch 3 | Step 1663400 | Avg Loss: 0.0163 | Grad Norm: 0.01002634\n",
      "Epoch 3 | Step 1663500 | Avg Loss: 0.0161 | Grad Norm: 0.01060237\n",
      "Epoch 3 | Step 1663600 | Avg Loss: 0.0163 | Grad Norm: 0.00886053\n",
      "Epoch 3 | Step 1663700 | Avg Loss: 0.0156 | Grad Norm: 0.00842663\n",
      "Epoch 3 | Step 1663800 | Avg Loss: 0.0157 | Grad Norm: 0.01008443\n",
      "Epoch 3 | Step 1663900 | Avg Loss: 0.0156 | Grad Norm: 0.00788397\n",
      "Epoch 3 | Step 1664000 | Avg Loss: 0.0154 | Grad Norm: 0.00763304\n",
      "Epoch 3 | Step 1664100 | Avg Loss: 0.0148 | Grad Norm: 0.00865667\n",
      "Epoch 3 | Step 1664200 | Avg Loss: 0.0151 | Grad Norm: 0.00926852\n",
      "Epoch 3 | Step 1664300 | Avg Loss: 0.0146 | Grad Norm: 0.00824729\n",
      "Epoch 3 | Step 1664400 | Avg Loss: 0.0150 | Grad Norm: 0.00903642\n",
      "Epoch 3 | Step 1664500 | Avg Loss: 0.0152 | Grad Norm: 0.00931501\n",
      "Epoch 3 | Step 1664600 | Avg Loss: 0.0151 | Grad Norm: 0.00821349\n",
      "Epoch 3 | Step 1664700 | Avg Loss: 0.0150 | Grad Norm: 0.00856812\n",
      "Epoch 3 | Step 1664800 | Avg Loss: 0.0149 | Grad Norm: 0.00754262\n",
      "Epoch 3 | Step 1664900 | Avg Loss: 0.0151 | Grad Norm: 0.00812307\n",
      "Epoch 3 | Step 1665000 | Avg Loss: 0.0153 | Grad Norm: 0.00793232\n",
      "Epoch 3 | Step 1665100 | Avg Loss: 0.0159 | Grad Norm: 0.00848147\n",
      "Epoch 3 | Step 1665200 | Avg Loss: 0.0158 | Grad Norm: 0.00869790\n",
      "Epoch 3 | Step 1665300 | Avg Loss: 0.0152 | Grad Norm: 0.00830289\n",
      "Epoch 3 | Step 1665400 | Avg Loss: 0.0154 | Grad Norm: 0.00893958\n",
      "Epoch 3 | Step 1665500 | Avg Loss: 0.0156 | Grad Norm: 0.00967427\n",
      "Epoch 3 | Step 1665600 | Avg Loss: 0.0157 | Grad Norm: 0.00891482\n",
      "Epoch 3 | Step 1665700 | Avg Loss: 0.0153 | Grad Norm: 0.00821249\n",
      "Epoch 3 | Step 1665800 | Avg Loss: 0.0150 | Grad Norm: 0.00826419\n",
      "Epoch 3 | Step 1665900 | Avg Loss: 0.0152 | Grad Norm: 0.00883558\n",
      "Epoch 3 | Step 1666000 | Avg Loss: 0.0157 | Grad Norm: 0.00853196\n",
      "Epoch 3 | Step 1666100 | Avg Loss: 0.0157 | Grad Norm: 0.00982453\n",
      "Epoch 3 | Step 1666200 | Avg Loss: 0.0155 | Grad Norm: 0.00852515\n",
      "Epoch 3 | Step 1666300 | Avg Loss: 0.0155 | Grad Norm: 0.00778190\n",
      "Epoch 3 | Step 1666400 | Avg Loss: 0.0154 | Grad Norm: 0.01187748\n",
      "Epoch 3 | Step 1666500 | Avg Loss: 0.0155 | Grad Norm: 0.00984144\n",
      "Epoch 3 | Step 1666600 | Avg Loss: 0.0153 | Grad Norm: 0.00988031\n",
      "Epoch 3 | Step 1666700 | Avg Loss: 0.0154 | Grad Norm: 0.00946729\n",
      "Epoch 3 | Step 1666800 | Avg Loss: 0.0152 | Grad Norm: 0.00937484\n",
      "Epoch 3 | Step 1666900 | Avg Loss: 0.0154 | Grad Norm: 0.00875779\n",
      "Epoch 3 | Step 1667000 | Avg Loss: 0.0157 | Grad Norm: 0.00916532\n",
      "Epoch 3 | Step 1667100 | Avg Loss: 0.0158 | Grad Norm: 0.00836810\n",
      "Epoch 3 | Step 1667200 | Avg Loss: 0.0153 | Grad Norm: 0.00959337\n",
      "Epoch 3 | Step 1667300 | Avg Loss: 0.0153 | Grad Norm: 0.00826312\n",
      "Epoch 3 | Step 1667400 | Avg Loss: 0.0155 | Grad Norm: 0.00953434\n",
      "Epoch 3 | Step 1667500 | Avg Loss: 0.0155 | Grad Norm: 0.00926372\n",
      "Epoch 3 | Step 1667600 | Avg Loss: 0.0156 | Grad Norm: 0.00961133\n",
      "Epoch 3 | Step 1667700 | Avg Loss: 0.0159 | Grad Norm: 0.00938800\n",
      "Epoch 3 | Step 1667800 | Avg Loss: 0.0159 | Grad Norm: 0.00883860\n",
      "Epoch 3 | Step 1667900 | Avg Loss: 0.0159 | Grad Norm: 0.00914798\n",
      "Epoch 3 | Step 1668000 | Avg Loss: 0.0152 | Grad Norm: 0.00873639\n",
      "Epoch 3 | Step 1668100 | Avg Loss: 0.0152 | Grad Norm: 0.00879177\n",
      "Epoch 3 | Step 1668200 | Avg Loss: 0.0154 | Grad Norm: 0.00882571\n",
      "Epoch 3 | Step 1668300 | Avg Loss: 0.0154 | Grad Norm: 0.00910438\n",
      "Epoch 3 | Step 1668400 | Avg Loss: 0.0154 | Grad Norm: 0.00820655\n",
      "Epoch 3 | Step 1668500 | Avg Loss: 0.0159 | Grad Norm: 0.01118350\n",
      "Epoch 3 | Step 1668600 | Avg Loss: 0.0159 | Grad Norm: 0.01041801\n",
      "Epoch 3 | Step 1668700 | Avg Loss: 0.0156 | Grad Norm: 0.00965345\n",
      "Epoch 3 | Step 1668800 | Avg Loss: 0.0157 | Grad Norm: 0.00985079\n",
      "Epoch 3 | Step 1668900 | Avg Loss: 0.0154 | Grad Norm: 0.00880162\n",
      "Epoch 3 | Step 1669000 | Avg Loss: 0.0155 | Grad Norm: 0.01013984\n",
      "Epoch 3 | Step 1669100 | Avg Loss: 0.0148 | Grad Norm: 0.00936752\n",
      "Epoch 3 | Step 1669200 | Avg Loss: 0.0149 | Grad Norm: 0.00927082\n",
      "Epoch 3 | Step 1669300 | Avg Loss: 0.0151 | Grad Norm: 0.01009300\n",
      "Epoch 3 | Step 1669400 | Avg Loss: 0.0153 | Grad Norm: 0.00932756\n",
      "Epoch 3 | Step 1669500 | Avg Loss: 0.0152 | Grad Norm: 0.00948331\n",
      "Epoch 3 | Step 1669600 | Avg Loss: 0.0154 | Grad Norm: 0.00895113\n",
      "Epoch 3 | Step 1669700 | Avg Loss: 0.0160 | Grad Norm: 0.01038278\n",
      "Epoch 3 | Step 1669800 | Avg Loss: 0.0165 | Grad Norm: 0.01151210\n",
      "Epoch 3 | Step 1669900 | Avg Loss: 0.0165 | Grad Norm: 0.00984213\n",
      "Epoch 3 | Step 1670000 | Avg Loss: 0.0163 | Grad Norm: 0.00841948\n",
      "Epoch 3 | Step 1670100 | Avg Loss: 0.0162 | Grad Norm: 0.00826373\n",
      "Epoch 3 | Step 1670200 | Avg Loss: 0.0159 | Grad Norm: 0.00814919\n",
      "Epoch 3 | Step 1670300 | Avg Loss: 0.0159 | Grad Norm: 0.00875936\n",
      "Epoch 3 | Step 1670400 | Avg Loss: 0.0160 | Grad Norm: 0.00957485\n",
      "Epoch 3 | Step 1670500 | Avg Loss: 0.0162 | Grad Norm: 0.00923778\n",
      "Epoch 3 | Step 1670600 | Avg Loss: 0.0163 | Grad Norm: 0.00928032\n",
      "Epoch 3 | Step 1670700 | Avg Loss: 0.0160 | Grad Norm: 0.00842715\n",
      "Epoch 3 | Step 1670800 | Avg Loss: 0.0156 | Grad Norm: 0.00792556\n",
      "Epoch 3 | Step 1670900 | Avg Loss: 0.0161 | Grad Norm: 0.00881283\n",
      "Epoch 3 | Step 1671000 | Avg Loss: 0.0154 | Grad Norm: 0.00975379\n",
      "Epoch 3 | Step 1671100 | Avg Loss: 0.0153 | Grad Norm: 0.01006955\n",
      "Epoch 3 | Step 1671200 | Avg Loss: 0.0153 | Grad Norm: 0.00786382\n",
      "Epoch 3 | Step 1671300 | Avg Loss: 0.0156 | Grad Norm: 0.00822841\n",
      "Epoch 3 | Step 1671400 | Avg Loss: 0.0156 | Grad Norm: 0.01257530\n",
      "Epoch 3 | Step 1671500 | Avg Loss: 0.0159 | Grad Norm: 0.01195780\n",
      "Epoch 3 | Step 1671600 | Avg Loss: 0.0155 | Grad Norm: 0.00748139\n",
      "Epoch 3 | Step 1671700 | Avg Loss: 0.0155 | Grad Norm: 0.00908973\n",
      "Epoch 3 | Step 1671800 | Avg Loss: 0.0158 | Grad Norm: 0.01041709\n",
      "Epoch 3 | Step 1671900 | Avg Loss: 0.0158 | Grad Norm: 0.00991433\n",
      "Epoch 3 | Step 1672000 | Avg Loss: 0.0157 | Grad Norm: 0.00933346\n",
      "Epoch 3 | Step 1672100 | Avg Loss: 0.0157 | Grad Norm: 0.00907335\n",
      "Epoch 3 | Step 1672200 | Avg Loss: 0.0155 | Grad Norm: 0.00816715\n",
      "Epoch 3 | Step 1672300 | Avg Loss: 0.0156 | Grad Norm: 0.00942075\n",
      "Epoch 3 | Step 1672400 | Avg Loss: 0.0155 | Grad Norm: 0.00929458\n",
      "Epoch 3 | Step 1672500 | Avg Loss: 0.0153 | Grad Norm: 0.00749266\n",
      "Epoch 3 | Step 1672600 | Avg Loss: 0.0155 | Grad Norm: 0.00850955\n",
      "Epoch 3 | Step 1672700 | Avg Loss: 0.0151 | Grad Norm: 0.00822168\n",
      "Epoch 3 | Step 1672800 | Avg Loss: 0.0151 | Grad Norm: 0.00897972\n",
      "Epoch 3 | Step 1672900 | Avg Loss: 0.0154 | Grad Norm: 0.00806101\n",
      "Epoch 3 | Step 1673000 | Avg Loss: 0.0152 | Grad Norm: 0.00762844\n",
      "Epoch 3 | Step 1673100 | Avg Loss: 0.0155 | Grad Norm: 0.00895371\n",
      "Epoch 3 | Step 1673200 | Avg Loss: 0.0158 | Grad Norm: 0.00832983\n",
      "Epoch 3 | Step 1673300 | Avg Loss: 0.0156 | Grad Norm: 0.00874183\n",
      "Epoch 3 | Step 1673400 | Avg Loss: 0.0157 | Grad Norm: 0.01112381\n",
      "Epoch 3 | Step 1673500 | Avg Loss: 0.0158 | Grad Norm: 0.00810738\n",
      "Epoch 3 | Step 1673600 | Avg Loss: 0.0158 | Grad Norm: 0.00841536\n",
      "Epoch 3 | Step 1673700 | Avg Loss: 0.0157 | Grad Norm: 0.00999214\n",
      "Epoch 3 | Step 1673800 | Avg Loss: 0.0157 | Grad Norm: 0.01085347\n",
      "Epoch 3 | Step 1673900 | Avg Loss: 0.0159 | Grad Norm: 0.00878629\n",
      "Epoch 3 | Step 1674000 | Avg Loss: 0.0156 | Grad Norm: 0.01019160\n",
      "Epoch 3 | Step 1674100 | Avg Loss: 0.0156 | Grad Norm: 0.00821926\n",
      "Epoch 3 | Step 1674200 | Avg Loss: 0.0159 | Grad Norm: 0.00942203\n",
      "Epoch 3 | Step 1674300 | Avg Loss: 0.0153 | Grad Norm: 0.00908189\n",
      "Epoch 3 | Step 1674400 | Avg Loss: 0.0156 | Grad Norm: 0.00814039\n",
      "Epoch 3 | Step 1674500 | Avg Loss: 0.0153 | Grad Norm: 0.00839065\n",
      "Epoch 3 | Step 1674600 | Avg Loss: 0.0155 | Grad Norm: 0.00928749\n",
      "Epoch 3 | Step 1674700 | Avg Loss: 0.0157 | Grad Norm: 0.00916748\n",
      "Epoch 3 | Step 1674800 | Avg Loss: 0.0156 | Grad Norm: 0.00801303\n",
      "Epoch 3 | Step 1674900 | Avg Loss: 0.0154 | Grad Norm: 0.00884190\n",
      "Epoch 3 | Step 1675000 | Avg Loss: 0.0153 | Grad Norm: 0.00908609\n",
      "Epoch 3 | Step 1675100 | Avg Loss: 0.0152 | Grad Norm: 0.00916206\n",
      "Epoch 3 | Step 1675200 | Avg Loss: 0.0157 | Grad Norm: 0.00827526\n",
      "Epoch 3 | Step 1675300 | Avg Loss: 0.0156 | Grad Norm: 0.00889666\n",
      "Epoch 3 | Step 1675400 | Avg Loss: 0.0157 | Grad Norm: 0.00756540\n",
      "Epoch 3 | Step 1675500 | Avg Loss: 0.0160 | Grad Norm: 0.00905724\n",
      "Epoch 3 | Step 1675600 | Avg Loss: 0.0164 | Grad Norm: 0.00942025\n",
      "Epoch 3 | Step 1675700 | Avg Loss: 0.0161 | Grad Norm: 0.00844247\n",
      "Epoch 3 | Step 1675800 | Avg Loss: 0.0161 | Grad Norm: 0.00824946\n",
      "Epoch 3 | Step 1675900 | Avg Loss: 0.0158 | Grad Norm: 0.00769898\n",
      "Epoch 3 | Step 1676000 | Avg Loss: 0.0153 | Grad Norm: 0.01003639\n",
      "Epoch 3 | Step 1676100 | Avg Loss: 0.0152 | Grad Norm: 0.00856612\n",
      "Epoch 3 | Step 1676200 | Avg Loss: 0.0155 | Grad Norm: 0.00871555\n",
      "Epoch 3 | Step 1676300 | Avg Loss: 0.0153 | Grad Norm: 0.00887509\n",
      "Epoch 3 | Step 1676400 | Avg Loss: 0.0156 | Grad Norm: 0.00899594\n",
      "Epoch 3 | Step 1676500 | Avg Loss: 0.0155 | Grad Norm: 0.01016065\n",
      "Epoch 3 | Step 1676600 | Avg Loss: 0.0153 | Grad Norm: 0.00826514\n",
      "Epoch 3 | Step 1676700 | Avg Loss: 0.0158 | Grad Norm: 0.00979987\n",
      "Epoch 3 | Step 1676800 | Avg Loss: 0.0158 | Grad Norm: 0.01019142\n",
      "Epoch 3 | Step 1676900 | Avg Loss: 0.0159 | Grad Norm: 0.01061106\n",
      "Epoch 3 | Step 1677000 | Avg Loss: 0.0158 | Grad Norm: 0.01030362\n",
      "Epoch 3 | Step 1677100 | Avg Loss: 0.0156 | Grad Norm: 0.00964225\n",
      "Epoch 3 | Step 1677200 | Avg Loss: 0.0155 | Grad Norm: 0.00937103\n",
      "Epoch 3 | Step 1677300 | Avg Loss: 0.0152 | Grad Norm: 0.01159255\n",
      "Epoch 3 | Step 1677400 | Avg Loss: 0.0156 | Grad Norm: 0.00916167\n",
      "Epoch 3 | Step 1677500 | Avg Loss: 0.0157 | Grad Norm: 0.00818066\n",
      "Epoch 3 | Step 1677600 | Avg Loss: 0.0160 | Grad Norm: 0.00844130\n",
      "Epoch 3 | Step 1677700 | Avg Loss: 0.0157 | Grad Norm: 0.00858415\n",
      "Epoch 3 | Step 1677800 | Avg Loss: 0.0157 | Grad Norm: 0.00858766\n",
      "Epoch 3 | Step 1677900 | Avg Loss: 0.0152 | Grad Norm: 0.00836572\n",
      "Epoch 3 | Step 1678000 | Avg Loss: 0.0153 | Grad Norm: 0.00927722\n",
      "Epoch 3 | Step 1678100 | Avg Loss: 0.0153 | Grad Norm: 0.00796083\n",
      "Epoch 3 | Step 1678200 | Avg Loss: 0.0152 | Grad Norm: 0.00817228\n",
      "Epoch 3 | Step 1678300 | Avg Loss: 0.0155 | Grad Norm: 0.01140798\n",
      "Epoch 3 | Step 1678400 | Avg Loss: 0.0155 | Grad Norm: 0.00889204\n",
      "Epoch 3 | Step 1678500 | Avg Loss: 0.0154 | Grad Norm: 0.00895470\n",
      "Epoch 3 | Step 1678600 | Avg Loss: 0.0156 | Grad Norm: 0.00945748\n",
      "Epoch 3 | Step 1678700 | Avg Loss: 0.0156 | Grad Norm: 0.00841880\n",
      "Epoch 3 | Step 1678800 | Avg Loss: 0.0162 | Grad Norm: 0.00909817\n",
      "Epoch 3 | Step 1678900 | Avg Loss: 0.0164 | Grad Norm: 0.01273321\n",
      "Epoch 3 | Step 1679000 | Avg Loss: 0.0156 | Grad Norm: 0.00821072\n",
      "Epoch 3 | Step 1679100 | Avg Loss: 0.0158 | Grad Norm: 0.01015367\n",
      "Epoch 3 | Step 1679200 | Avg Loss: 0.0157 | Grad Norm: 0.00897209\n",
      "Epoch 3 | Step 1679300 | Avg Loss: 0.0160 | Grad Norm: 0.01038768\n",
      "Epoch 3 | Step 1679400 | Avg Loss: 0.0158 | Grad Norm: 0.00918931\n",
      "Epoch 3 | Step 1679500 | Avg Loss: 0.0156 | Grad Norm: 0.01000482\n",
      "Epoch 3 | Step 1679600 | Avg Loss: 0.0155 | Grad Norm: 0.01008800\n",
      "Epoch 3 | Step 1679700 | Avg Loss: 0.0154 | Grad Norm: 0.00910071\n",
      "Epoch 3 | Step 1679800 | Avg Loss: 0.0157 | Grad Norm: 0.01065787\n",
      "Epoch 3 | Step 1679900 | Avg Loss: 0.0157 | Grad Norm: 0.00911155\n",
      "Epoch 3 | Step 1680000 | Avg Loss: 0.0154 | Grad Norm: 0.00880249\n",
      "Epoch 3 | Step 1680100 | Avg Loss: 0.0159 | Grad Norm: 0.00902769\n",
      "Epoch 3 | Step 1680200 | Avg Loss: 0.0162 | Grad Norm: 0.00861020\n",
      "Epoch 3 | Step 1680300 | Avg Loss: 0.0159 | Grad Norm: 0.00838832\n",
      "Epoch 3 | Step 1680400 | Avg Loss: 0.0158 | Grad Norm: 0.00843839\n",
      "Epoch 3 | Step 1680500 | Avg Loss: 0.0159 | Grad Norm: 0.00861294\n",
      "Epoch 3 | Step 1680600 | Avg Loss: 0.0160 | Grad Norm: 0.00988860\n",
      "Epoch 3 | Step 1680700 | Avg Loss: 0.0161 | Grad Norm: 0.00879651\n",
      "Epoch 3 | Step 1680800 | Avg Loss: 0.0157 | Grad Norm: 0.00945925\n",
      "Epoch 3 | Step 1680900 | Avg Loss: 0.0158 | Grad Norm: 0.00851234\n",
      "Epoch 3 | Step 1681000 | Avg Loss: 0.0158 | Grad Norm: 0.00876564\n",
      "Epoch 3 | Step 1681100 | Avg Loss: 0.0158 | Grad Norm: 0.00891917\n",
      "Epoch 3 | Step 1681200 | Avg Loss: 0.0158 | Grad Norm: 0.00759252\n",
      "Epoch 3 | Step 1681300 | Avg Loss: 0.0155 | Grad Norm: 0.00899274\n",
      "Epoch 3 | Step 1681400 | Avg Loss: 0.0154 | Grad Norm: 0.00893247\n",
      "Epoch 3 | Step 1681500 | Avg Loss: 0.0152 | Grad Norm: 0.00893561\n",
      "Epoch 3 | Step 1681600 | Avg Loss: 0.0153 | Grad Norm: 0.00834964\n",
      "Epoch 3 | Step 1681700 | Avg Loss: 0.0153 | Grad Norm: 0.00992832\n",
      "Epoch 3 | Step 1681800 | Avg Loss: 0.0153 | Grad Norm: 0.00995184\n",
      "Epoch 3 | Step 1681900 | Avg Loss: 0.0151 | Grad Norm: 0.00866425\n",
      "Epoch 3 | Step 1682000 | Avg Loss: 0.0150 | Grad Norm: 0.00951613\n",
      "Epoch 3 | Step 1682100 | Avg Loss: 0.0150 | Grad Norm: 0.00848996\n",
      "Epoch 3 | Step 1682200 | Avg Loss: 0.0151 | Grad Norm: 0.01198996\n",
      "Epoch 3 | Step 1682300 | Avg Loss: 0.0153 | Grad Norm: 0.00848996\n",
      "Epoch 3 | Step 1682400 | Avg Loss: 0.0155 | Grad Norm: 0.00912986\n",
      "Epoch 3 | Step 1682500 | Avg Loss: 0.0159 | Grad Norm: 0.00881412\n",
      "Epoch 3 | Step 1682600 | Avg Loss: 0.0157 | Grad Norm: 0.00819559\n",
      "Epoch 3 | Step 1682700 | Avg Loss: 0.0157 | Grad Norm: 0.00774852\n",
      "Epoch 3 | Step 1682800 | Avg Loss: 0.0153 | Grad Norm: 0.01175558\n",
      "Epoch 3 | Step 1682900 | Avg Loss: 0.0155 | Grad Norm: 0.00753574\n",
      "Epoch 3 | Step 1683000 | Avg Loss: 0.0155 | Grad Norm: 0.01004515\n",
      "Epoch 3 | Step 1683100 | Avg Loss: 0.0154 | Grad Norm: 0.00944605\n",
      "Epoch 3 | Step 1683200 | Avg Loss: 0.0153 | Grad Norm: 0.00878856\n",
      "Epoch 3 | Step 1683300 | Avg Loss: 0.0151 | Grad Norm: 0.00935599\n",
      "Epoch 3 | Step 1683400 | Avg Loss: 0.0153 | Grad Norm: 0.00780777\n",
      "Epoch 3 | Step 1683500 | Avg Loss: 0.0153 | Grad Norm: 0.00813305\n",
      "Epoch 3 | Step 1683600 | Avg Loss: 0.0151 | Grad Norm: 0.00873558\n",
      "Epoch 3 | Step 1683700 | Avg Loss: 0.0151 | Grad Norm: 0.00789134\n",
      "Epoch 3 | Step 1683800 | Avg Loss: 0.0152 | Grad Norm: 0.00905597\n",
      "Epoch 3 | Step 1683900 | Avg Loss: 0.0152 | Grad Norm: 0.00875977\n",
      "Epoch 3 | Step 1684000 | Avg Loss: 0.0158 | Grad Norm: 0.00964002\n",
      "Epoch 3 | Step 1684100 | Avg Loss: 0.0156 | Grad Norm: 0.00842061\n",
      "Epoch 3 | Step 1684200 | Avg Loss: 0.0156 | Grad Norm: 0.00926760\n",
      "Epoch 3 | Step 1684300 | Avg Loss: 0.0154 | Grad Norm: 0.00868514\n",
      "Epoch 3 | Step 1684400 | Avg Loss: 0.0155 | Grad Norm: 0.00757168\n",
      "Epoch 3 | Step 1684500 | Avg Loss: 0.0155 | Grad Norm: 0.01027860\n",
      "Epoch 3 | Step 1684600 | Avg Loss: 0.0155 | Grad Norm: 0.00820718\n",
      "Epoch 3 | Step 1684700 | Avg Loss: 0.0154 | Grad Norm: 0.00827995\n",
      "Epoch 3 | Step 1684800 | Avg Loss: 0.0154 | Grad Norm: 0.00951506\n",
      "Epoch 3 | Step 1684900 | Avg Loss: 0.0154 | Grad Norm: 0.00804587\n",
      "Epoch 3 | Step 1685000 | Avg Loss: 0.0154 | Grad Norm: 0.00820917\n",
      "Epoch 3 | Step 1685100 | Avg Loss: 0.0155 | Grad Norm: 0.01025572\n",
      "Epoch 3 | Step 1685200 | Avg Loss: 0.0150 | Grad Norm: 0.00896436\n",
      "Epoch 3 | Step 1685300 | Avg Loss: 0.0155 | Grad Norm: 0.01055585\n",
      "Epoch 3 | Step 1685400 | Avg Loss: 0.0154 | Grad Norm: 0.00952044\n",
      "Epoch 3 | Step 1685500 | Avg Loss: 0.0151 | Grad Norm: 0.00955598\n",
      "Epoch 3 | Step 1685600 | Avg Loss: 0.0154 | Grad Norm: 0.00810349\n",
      "Epoch 3 | Step 1685700 | Avg Loss: 0.0150 | Grad Norm: 0.00825959\n",
      "Epoch 3 | Step 1685800 | Avg Loss: 0.0152 | Grad Norm: 0.00938691\n",
      "Epoch 3 | Step 1685900 | Avg Loss: 0.0151 | Grad Norm: 0.00876834\n",
      "Epoch 3 | Step 1686000 | Avg Loss: 0.0151 | Grad Norm: 0.00809676\n",
      "Epoch 3 | Step 1686100 | Avg Loss: 0.0154 | Grad Norm: 0.00875548\n",
      "Epoch 3 | Step 1686200 | Avg Loss: 0.0154 | Grad Norm: 0.01012804\n",
      "Epoch 3 | Step 1686300 | Avg Loss: 0.0152 | Grad Norm: 0.00933428\n",
      "Epoch 3 | Step 1686400 | Avg Loss: 0.0154 | Grad Norm: 0.00998346\n",
      "Epoch 3 | Step 1686500 | Avg Loss: 0.0158 | Grad Norm: 0.00881975\n",
      "Epoch 3 | Step 1686600 | Avg Loss: 0.0156 | Grad Norm: 0.01075191\n",
      "Epoch 3 | Step 1686700 | Avg Loss: 0.0156 | Grad Norm: 0.00961329\n",
      "Epoch 3 | Step 1686800 | Avg Loss: 0.0156 | Grad Norm: 0.01173278\n",
      "Epoch 3 | Step 1686900 | Avg Loss: 0.0156 | Grad Norm: 0.01001474\n",
      "Epoch 3 | Step 1687000 | Avg Loss: 0.0157 | Grad Norm: 0.00858673\n",
      "Epoch 3 | Step 1687100 | Avg Loss: 0.0155 | Grad Norm: 0.00798008\n",
      "Epoch 3 | Step 1687200 | Avg Loss: 0.0152 | Grad Norm: 0.01337755\n",
      "Epoch 3 | Step 1687300 | Avg Loss: 0.0152 | Grad Norm: 0.00803991\n",
      "Epoch 3 | Step 1687400 | Avg Loss: 0.0156 | Grad Norm: 0.00979746\n",
      "Epoch 3 | Step 1687500 | Avg Loss: 0.0155 | Grad Norm: 0.00860861\n",
      "Epoch 3 | Step 1687600 | Avg Loss: 0.0156 | Grad Norm: 0.00892233\n",
      "Epoch 3 | Step 1687700 | Avg Loss: 0.0156 | Grad Norm: 0.01138746\n",
      "Epoch 3 | Step 1687800 | Avg Loss: 0.0157 | Grad Norm: 0.00953918\n",
      "Epoch 3 | Step 1687900 | Avg Loss: 0.0157 | Grad Norm: 0.00945802\n",
      "Epoch 3 | Step 1688000 | Avg Loss: 0.0157 | Grad Norm: 0.00890867\n",
      "Epoch 3 | Step 1688100 | Avg Loss: 0.0159 | Grad Norm: 0.00961222\n",
      "Epoch 3 | Step 1688200 | Avg Loss: 0.0156 | Grad Norm: 0.01014890\n",
      "Epoch 3 | Step 1688300 | Avg Loss: 0.0155 | Grad Norm: 0.00878937\n",
      "Epoch 3 | Step 1688400 | Avg Loss: 0.0153 | Grad Norm: 0.01014534\n",
      "Epoch 3 | Step 1688500 | Avg Loss: 0.0156 | Grad Norm: 0.00803954\n",
      "Epoch 3 | Step 1688600 | Avg Loss: 0.0160 | Grad Norm: 0.01034366\n",
      "Epoch 3 | Step 1688700 | Avg Loss: 0.0157 | Grad Norm: 0.00889069\n",
      "Epoch 3 | Step 1688800 | Avg Loss: 0.0160 | Grad Norm: 0.00866519\n",
      "Epoch 3 | Step 1688900 | Avg Loss: 0.0159 | Grad Norm: 0.00874100\n",
      "Epoch 3 | Step 1689000 | Avg Loss: 0.0157 | Grad Norm: 0.01090847\n",
      "Epoch 3 | Step 1689100 | Avg Loss: 0.0154 | Grad Norm: 0.00858295\n",
      "Epoch 3 | Step 1689200 | Avg Loss: 0.0155 | Grad Norm: 0.00845046\n",
      "Epoch 3 | Step 1689300 | Avg Loss: 0.0154 | Grad Norm: 0.00856769\n",
      "Epoch 3 | Step 1689400 | Avg Loss: 0.0151 | Grad Norm: 0.00894504\n",
      "Epoch 3 | Step 1689500 | Avg Loss: 0.0155 | Grad Norm: 0.00810738\n",
      "Epoch 3 | Step 1689600 | Avg Loss: 0.0155 | Grad Norm: 0.01004430\n",
      "Epoch 3 | Step 1689700 | Avg Loss: 0.0158 | Grad Norm: 0.00907971\n",
      "Epoch 3 | Step 1689800 | Avg Loss: 0.0157 | Grad Norm: 0.00823795\n",
      "Epoch 3 | Step 1689900 | Avg Loss: 0.0153 | Grad Norm: 0.00862815\n",
      "Epoch 3 | Step 1690000 | Avg Loss: 0.0151 | Grad Norm: 0.01388014\n",
      "Epoch 3 | Step 1690100 | Avg Loss: 0.0155 | Grad Norm: 0.00915211\n",
      "Epoch 3 | Step 1690200 | Avg Loss: 0.0155 | Grad Norm: 0.00958027\n",
      "Epoch 3 | Step 1690300 | Avg Loss: 0.0159 | Grad Norm: 0.00887991\n",
      "Epoch 3 | Step 1690400 | Avg Loss: 0.0158 | Grad Norm: 0.00859026\n",
      "Epoch 3 | Step 1690500 | Avg Loss: 0.0157 | Grad Norm: 0.00927052\n",
      "Epoch 3 | Step 1690600 | Avg Loss: 0.0155 | Grad Norm: 0.00900110\n",
      "Epoch 3 | Step 1690700 | Avg Loss: 0.0155 | Grad Norm: 0.00931550\n",
      "Epoch 3 | Step 1690800 | Avg Loss: 0.0155 | Grad Norm: 0.00856010\n",
      "Epoch 3 | Step 1690900 | Avg Loss: 0.0154 | Grad Norm: 0.00758316\n",
      "Epoch 3 | Step 1691000 | Avg Loss: 0.0152 | Grad Norm: 0.00805060\n",
      "Epoch 3 | Step 1691100 | Avg Loss: 0.0154 | Grad Norm: 0.01038429\n",
      "Epoch 3 | Step 1691200 | Avg Loss: 0.0153 | Grad Norm: 0.00923992\n",
      "Epoch 3 | Step 1691300 | Avg Loss: 0.0156 | Grad Norm: 0.01127633\n",
      "Epoch 3 | Step 1691400 | Avg Loss: 0.0157 | Grad Norm: 0.00910109\n",
      "Epoch 3 | Step 1691500 | Avg Loss: 0.0159 | Grad Norm: 0.00953043\n",
      "Epoch 3 | Step 1691600 | Avg Loss: 0.0158 | Grad Norm: 0.00890396\n",
      "Epoch 3 | Step 1691700 | Avg Loss: 0.0161 | Grad Norm: 0.00868318\n",
      "Epoch 3 | Step 1691800 | Avg Loss: 0.0156 | Grad Norm: 0.00884424\n",
      "Epoch 3 | Step 1691900 | Avg Loss: 0.0157 | Grad Norm: 0.00924308\n",
      "Epoch 3 | Step 1692000 | Avg Loss: 0.0157 | Grad Norm: 0.01051432\n",
      "Epoch 3 | Step 1692100 | Avg Loss: 0.0159 | Grad Norm: 0.00890747\n",
      "Epoch 3 | Step 1692200 | Avg Loss: 0.0158 | Grad Norm: 0.00897592\n",
      "Epoch 3 | Step 1692300 | Avg Loss: 0.0159 | Grad Norm: 0.00823300\n",
      "Epoch 3 | Step 1692400 | Avg Loss: 0.0162 | Grad Norm: 0.00926670\n",
      "Epoch 3 | Step 1692500 | Avg Loss: 0.0157 | Grad Norm: 0.00849498\n",
      "Epoch 3 | Step 1692600 | Avg Loss: 0.0155 | Grad Norm: 0.01080603\n",
      "Epoch 3 | Step 1692700 | Avg Loss: 0.0155 | Grad Norm: 0.00988156\n",
      "Epoch 3 | Step 1692800 | Avg Loss: 0.0156 | Grad Norm: 0.00918446\n",
      "Epoch 3 | Step 1692900 | Avg Loss: 0.0154 | Grad Norm: 0.00981311\n",
      "Epoch 3 | Step 1693000 | Avg Loss: 0.0155 | Grad Norm: 0.00914530\n",
      "Epoch 3 | Step 1693100 | Avg Loss: 0.0160 | Grad Norm: 0.00897535\n",
      "Epoch 3 | Step 1693200 | Avg Loss: 0.0161 | Grad Norm: 0.00883959\n",
      "Epoch 3 | Step 1693300 | Avg Loss: 0.0158 | Grad Norm: 0.01045533\n",
      "Epoch 3 | Step 1693400 | Avg Loss: 0.0158 | Grad Norm: 0.00983923\n",
      "Epoch 3 | Step 1693500 | Avg Loss: 0.0158 | Grad Norm: 0.00854576\n",
      "Epoch 3 | Step 1693600 | Avg Loss: 0.0157 | Grad Norm: 0.00846679\n",
      "Epoch 3 | Step 1693700 | Avg Loss: 0.0156 | Grad Norm: 0.00947726\n",
      "Epoch 3 | Step 1693800 | Avg Loss: 0.0160 | Grad Norm: 0.01011315\n",
      "Epoch 3 | Step 1693900 | Avg Loss: 0.0163 | Grad Norm: 0.01045288\n",
      "Epoch 3 | Step 1694000 | Avg Loss: 0.0160 | Grad Norm: 0.00940967\n",
      "Epoch 3 | Step 1694100 | Avg Loss: 0.0159 | Grad Norm: 0.01002438\n",
      "Epoch 3 | Step 1694200 | Avg Loss: 0.0160 | Grad Norm: 0.01050301\n",
      "Epoch 3 | Step 1694300 | Avg Loss: 0.0158 | Grad Norm: 0.00891666\n",
      "Epoch 3 | Step 1694400 | Avg Loss: 0.0156 | Grad Norm: 0.00918748\n",
      "Epoch 3 | Step 1694500 | Avg Loss: 0.0153 | Grad Norm: 0.01005764\n",
      "Epoch 3 | Step 1694600 | Avg Loss: 0.0155 | Grad Norm: 0.00817244\n",
      "Epoch 3 | Step 1694700 | Avg Loss: 0.0158 | Grad Norm: 0.00938330\n",
      "Epoch 3 | Step 1694800 | Avg Loss: 0.0161 | Grad Norm: 0.00940160\n",
      "Epoch 3 | Step 1694900 | Avg Loss: 0.0160 | Grad Norm: 0.00941087\n",
      "Epoch 3 | Step 1695000 | Avg Loss: 0.0160 | Grad Norm: 0.00832196\n",
      "Epoch 3 | Step 1695100 | Avg Loss: 0.0158 | Grad Norm: 0.00951436\n",
      "Epoch 3 | Step 1695200 | Avg Loss: 0.0161 | Grad Norm: 0.00964525\n",
      "Epoch 3 | Step 1695300 | Avg Loss: 0.0159 | Grad Norm: 0.00813196\n",
      "Epoch 3 | Step 1695400 | Avg Loss: 0.0160 | Grad Norm: 0.00841267\n",
      "Epoch 3 | Step 1695500 | Avg Loss: 0.0161 | Grad Norm: 0.01030457\n",
      "Epoch 3 | Step 1695600 | Avg Loss: 0.0157 | Grad Norm: 0.00992846\n",
      "Epoch 3 | Step 1695700 | Avg Loss: 0.0158 | Grad Norm: 0.00874145\n",
      "Epoch 3 | Step 1695800 | Avg Loss: 0.0158 | Grad Norm: 0.00851953\n",
      "Epoch 3 | Step 1695900 | Avg Loss: 0.0160 | Grad Norm: 0.01048942\n",
      "Epoch 3 | Step 1696000 | Avg Loss: 0.0156 | Grad Norm: 0.00952660\n",
      "Epoch 3 | Step 1696100 | Avg Loss: 0.0157 | Grad Norm: 0.00920625\n",
      "Epoch 3 | Step 1696200 | Avg Loss: 0.0154 | Grad Norm: 0.00835566\n",
      "Epoch 3 | Step 1696300 | Avg Loss: 0.0153 | Grad Norm: 0.01001227\n",
      "Epoch 3 | Step 1696400 | Avg Loss: 0.0151 | Grad Norm: 0.00829113\n",
      "Epoch 3 | Step 1696500 | Avg Loss: 0.0150 | Grad Norm: 0.00817718\n",
      "Epoch 3 | Step 1696600 | Avg Loss: 0.0151 | Grad Norm: 0.00821957\n",
      "Epoch 3 | Step 1696700 | Avg Loss: 0.0152 | Grad Norm: 0.00956595\n",
      "Epoch 3 | Step 1696800 | Avg Loss: 0.0153 | Grad Norm: 0.00952148\n",
      "Epoch 3 | Step 1696900 | Avg Loss: 0.0156 | Grad Norm: 0.00778888\n",
      "Epoch 3 | Step 1697000 | Avg Loss: 0.0155 | Grad Norm: 0.01012049\n",
      "Epoch 3 | Step 1697100 | Avg Loss: 0.0155 | Grad Norm: 0.00836036\n",
      "Epoch 3 | Step 1697200 | Avg Loss: 0.0155 | Grad Norm: 0.00824315\n",
      "Epoch 3 | Step 1697300 | Avg Loss: 0.0152 | Grad Norm: 0.00914795\n",
      "Epoch 3 | Step 1697400 | Avg Loss: 0.0157 | Grad Norm: 0.00803368\n",
      "Epoch 3 | Step 1697500 | Avg Loss: 0.0155 | Grad Norm: 0.00765254\n",
      "Epoch 3 | Step 1697600 | Avg Loss: 0.0156 | Grad Norm: 0.00781198\n",
      "Epoch 3 | Step 1697700 | Avg Loss: 0.0162 | Grad Norm: 0.00860974\n",
      "Epoch 3 | Step 1697800 | Avg Loss: 0.0159 | Grad Norm: 0.01054682\n",
      "Epoch 3 | Step 1697900 | Avg Loss: 0.0160 | Grad Norm: 0.00985685\n",
      "Epoch 3 | Step 1698000 | Avg Loss: 0.0162 | Grad Norm: 0.00900971\n",
      "Epoch 3 | Step 1698100 | Avg Loss: 0.0157 | Grad Norm: 0.00889112\n",
      "Epoch 3 | Step 1698200 | Avg Loss: 0.0157 | Grad Norm: 0.00837392\n",
      "Epoch 3 | Step 1698300 | Avg Loss: 0.0158 | Grad Norm: 0.00900392\n",
      "Epoch 3 | Step 1698400 | Avg Loss: 0.0160 | Grad Norm: 0.00804867\n",
      "Epoch 3 | Step 1698500 | Avg Loss: 0.0154 | Grad Norm: 0.00851454\n",
      "Epoch 3 | Step 1698600 | Avg Loss: 0.0159 | Grad Norm: 0.00915627\n",
      "Epoch 3 | Step 1698700 | Avg Loss: 0.0154 | Grad Norm: 0.00822143\n",
      "Epoch 3 | Step 1698800 | Avg Loss: 0.0152 | Grad Norm: 0.00775359\n",
      "Epoch 3 | Step 1698900 | Avg Loss: 0.0150 | Grad Norm: 0.00753006\n",
      "Epoch 3 | Step 1699000 | Avg Loss: 0.0153 | Grad Norm: 0.00851951\n",
      "Epoch 3 | Step 1699100 | Avg Loss: 0.0149 | Grad Norm: 0.00822857\n",
      "Epoch 3 | Step 1699200 | Avg Loss: 0.0148 | Grad Norm: 0.00897485\n",
      "Epoch 3 | Step 1699300 | Avg Loss: 0.0151 | Grad Norm: 0.00728724\n",
      "Epoch 3 | Step 1699400 | Avg Loss: 0.0152 | Grad Norm: 0.00857653\n",
      "Epoch 3 | Step 1699500 | Avg Loss: 0.0155 | Grad Norm: 0.00766377\n",
      "Epoch 3 | Step 1699600 | Avg Loss: 0.0154 | Grad Norm: 0.01012107\n",
      "Epoch 3 | Step 1699700 | Avg Loss: 0.0154 | Grad Norm: 0.01100810\n",
      "Epoch 3 | Step 1699800 | Avg Loss: 0.0152 | Grad Norm: 0.00905897\n",
      "Epoch 3 | Step 1699900 | Avg Loss: 0.0156 | Grad Norm: 0.01091813\n",
      "Epoch 3 | Step 1700000 | Avg Loss: 0.0154 | Grad Norm: 0.00854509\n",
      "Saving model at step1700000\n",
      "Epoch 3 | Step 1700100 | Avg Loss: 0.0156 | Grad Norm: 0.00868413\n",
      "Epoch 3 | Step 1700200 | Avg Loss: 0.0157 | Grad Norm: 0.00904677\n",
      "Epoch 3 | Step 1700300 | Avg Loss: 0.0155 | Grad Norm: 0.00890101\n",
      "Epoch 3 | Step 1700400 | Avg Loss: 0.0156 | Grad Norm: 0.01001262\n",
      "Epoch 3 | Step 1700500 | Avg Loss: 0.0155 | Grad Norm: 0.01047246\n",
      "Epoch 3 | Step 1700600 | Avg Loss: 0.0157 | Grad Norm: 0.00832258\n",
      "Epoch 3 | Step 1700700 | Avg Loss: 0.0157 | Grad Norm: 0.00835920\n",
      "Epoch 3 | Step 1700800 | Avg Loss: 0.0163 | Grad Norm: 0.01046994\n",
      "Epoch 3 | Step 1700900 | Avg Loss: 0.0158 | Grad Norm: 0.00773274\n",
      "Epoch 3 | Step 1701000 | Avg Loss: 0.0153 | Grad Norm: 0.00886780\n",
      "Epoch 3 | Step 1701100 | Avg Loss: 0.0156 | Grad Norm: 0.00969996\n",
      "Epoch 3 | Step 1701200 | Avg Loss: 0.0155 | Grad Norm: 0.00755141\n",
      "Epoch 3 | Step 1701300 | Avg Loss: 0.0154 | Grad Norm: 0.00809452\n",
      "Epoch 3 | Step 1701400 | Avg Loss: 0.0153 | Grad Norm: 0.00704738\n",
      "Epoch 3 | Step 1701500 | Avg Loss: 0.0150 | Grad Norm: 0.00814676\n",
      "Epoch 3 | Step 1701600 | Avg Loss: 0.0155 | Grad Norm: 0.00865790\n",
      "Epoch 3 | Step 1701700 | Avg Loss: 0.0159 | Grad Norm: 0.00796498\n",
      "Epoch 3 | Step 1701800 | Avg Loss: 0.0160 | Grad Norm: 0.00987060\n",
      "Epoch 3 | Step 1701900 | Avg Loss: 0.0160 | Grad Norm: 0.01057693\n",
      "Epoch 3 | Step 1702000 | Avg Loss: 0.0158 | Grad Norm: 0.00800704\n",
      "Epoch 3 | Step 1702100 | Avg Loss: 0.0157 | Grad Norm: 0.00890076\n",
      "Epoch 3 | Step 1702200 | Avg Loss: 0.0161 | Grad Norm: 0.01025011\n",
      "Epoch 3 | Step 1702300 | Avg Loss: 0.0162 | Grad Norm: 0.01080316\n",
      "Epoch 3 | Step 1702400 | Avg Loss: 0.0156 | Grad Norm: 0.00802008\n",
      "Epoch 3 | Step 1702500 | Avg Loss: 0.0154 | Grad Norm: 0.00792239\n",
      "Epoch 3 | Step 1702600 | Avg Loss: 0.0152 | Grad Norm: 0.00867494\n",
      "Epoch 3 | Step 1702700 | Avg Loss: 0.0151 | Grad Norm: 0.00848874\n",
      "Epoch 3 | Step 1702800 | Avg Loss: 0.0152 | Grad Norm: 0.00711102\n",
      "Epoch 3 | Step 1702900 | Avg Loss: 0.0152 | Grad Norm: 0.01048202\n",
      "Epoch 3 | Step 1703000 | Avg Loss: 0.0158 | Grad Norm: 0.00820565\n",
      "Epoch 3 | Step 1703100 | Avg Loss: 0.0160 | Grad Norm: 0.00964370\n",
      "Epoch 3 | Step 1703200 | Avg Loss: 0.0160 | Grad Norm: 0.00958136\n",
      "Epoch 3 | Step 1703300 | Avg Loss: 0.0159 | Grad Norm: 0.00834248\n",
      "Epoch 3 | Step 1703400 | Avg Loss: 0.0158 | Grad Norm: 0.00980597\n",
      "Epoch 3 | Step 1703500 | Avg Loss: 0.0160 | Grad Norm: 0.00946117\n",
      "Epoch 3 | Step 1703600 | Avg Loss: 0.0161 | Grad Norm: 0.01195315\n",
      "Epoch 3 | Step 1703700 | Avg Loss: 0.0159 | Grad Norm: 0.01408050\n",
      "Epoch 3 | Step 1703800 | Avg Loss: 0.0157 | Grad Norm: 0.01013006\n",
      "Epoch 3 | Step 1703900 | Avg Loss: 0.0156 | Grad Norm: 0.00933442\n",
      "Epoch 3 | Step 1704000 | Avg Loss: 0.0153 | Grad Norm: 0.00979018\n",
      "Epoch 3 | Step 1704100 | Avg Loss: 0.0155 | Grad Norm: 0.00936978\n",
      "Epoch 3 | Step 1704200 | Avg Loss: 0.0150 | Grad Norm: 0.00976232\n",
      "Epoch 3 | Step 1704300 | Avg Loss: 0.0152 | Grad Norm: 0.00961197\n",
      "Epoch 3 | Step 1704400 | Avg Loss: 0.0151 | Grad Norm: 0.00858174\n",
      "Epoch 3 | Step 1704500 | Avg Loss: 0.0153 | Grad Norm: 0.00788292\n",
      "Epoch 3 | Step 1704600 | Avg Loss: 0.0153 | Grad Norm: 0.00930790\n",
      "Epoch 3 | Step 1704700 | Avg Loss: 0.0157 | Grad Norm: 0.00859414\n",
      "Epoch 3 | Step 1704800 | Avg Loss: 0.0158 | Grad Norm: 0.00857940\n",
      "Epoch 3 | Step 1704900 | Avg Loss: 0.0159 | Grad Norm: 0.00847969\n",
      "Epoch 3 | Step 1705000 | Avg Loss: 0.0156 | Grad Norm: 0.00944174\n",
      "Epoch 3 | Step 1705100 | Avg Loss: 0.0150 | Grad Norm: 0.00731997\n",
      "Epoch 3 | Step 1705200 | Avg Loss: 0.0151 | Grad Norm: 0.00870829\n",
      "Epoch 3 | Step 1705300 | Avg Loss: 0.0148 | Grad Norm: 0.00822029\n",
      "Epoch 3 | Step 1705400 | Avg Loss: 0.0148 | Grad Norm: 0.00944053\n",
      "Epoch 3 | Step 1705500 | Avg Loss: 0.0152 | Grad Norm: 0.00828094\n",
      "Epoch 3 | Step 1705600 | Avg Loss: 0.0153 | Grad Norm: 0.00889114\n",
      "Epoch 3 | Step 1705700 | Avg Loss: 0.0157 | Grad Norm: 0.00870305\n",
      "Epoch 3 | Step 1705800 | Avg Loss: 0.0154 | Grad Norm: 0.00803948\n",
      "Epoch 3 | Step 1705900 | Avg Loss: 0.0153 | Grad Norm: 0.00804743\n",
      "Epoch 3 | Step 1706000 | Avg Loss: 0.0155 | Grad Norm: 0.00880185\n",
      "Epoch 3 | Step 1706100 | Avg Loss: 0.0154 | Grad Norm: 0.01035911\n",
      "Epoch 3 | Step 1706200 | Avg Loss: 0.0149 | Grad Norm: 0.00772541\n",
      "Epoch 3 | Step 1706300 | Avg Loss: 0.0152 | Grad Norm: 0.00826537\n",
      "Epoch 3 | Step 1706400 | Avg Loss: 0.0155 | Grad Norm: 0.00899663\n",
      "Epoch 3 | Step 1706500 | Avg Loss: 0.0152 | Grad Norm: 0.00861708\n",
      "Epoch 3 | Step 1706600 | Avg Loss: 0.0153 | Grad Norm: 0.00726373\n",
      "Epoch 3 | Step 1706700 | Avg Loss: 0.0155 | Grad Norm: 0.00845936\n",
      "Epoch 3 | Step 1706800 | Avg Loss: 0.0156 | Grad Norm: 0.00935015\n",
      "Epoch 3 | Step 1706900 | Avg Loss: 0.0160 | Grad Norm: 0.00888446\n",
      "Epoch 3 | Step 1707000 | Avg Loss: 0.0159 | Grad Norm: 0.00956557\n",
      "Epoch 3 | Step 1707100 | Avg Loss: 0.0158 | Grad Norm: 0.00780301\n",
      "Epoch 3 | Step 1707200 | Avg Loss: 0.0162 | Grad Norm: 0.00939465\n",
      "Epoch 3 | Step 1707300 | Avg Loss: 0.0161 | Grad Norm: 0.01027344\n",
      "Epoch 3 | Step 1707400 | Avg Loss: 0.0161 | Grad Norm: 0.00853099\n",
      "Epoch 3 | Step 1707500 | Avg Loss: 0.0159 | Grad Norm: 0.00919457\n",
      "Epoch 3 | Step 1707600 | Avg Loss: 0.0154 | Grad Norm: 0.00815055\n",
      "Epoch 3 | Step 1707700 | Avg Loss: 0.0155 | Grad Norm: 0.00916198\n",
      "Epoch 3 | Step 1707800 | Avg Loss: 0.0154 | Grad Norm: 0.00866963\n",
      "Epoch 3 | Step 1707900 | Avg Loss: 0.0157 | Grad Norm: 0.00816548\n",
      "Epoch 3 | Step 1708000 | Avg Loss: 0.0158 | Grad Norm: 0.00825216\n",
      "Epoch 3 | Step 1708100 | Avg Loss: 0.0159 | Grad Norm: 0.00794876\n",
      "Epoch 3 | Step 1708200 | Avg Loss: 0.0158 | Grad Norm: 0.00892214\n",
      "Epoch 3 | Step 1708300 | Avg Loss: 0.0155 | Grad Norm: 0.00823112\n",
      "Epoch 3 | Step 1708400 | Avg Loss: 0.0156 | Grad Norm: 0.00826761\n",
      "Epoch 3 | Step 1708500 | Avg Loss: 0.0156 | Grad Norm: 0.00850132\n",
      "Epoch 3 | Step 1708600 | Avg Loss: 0.0156 | Grad Norm: 0.00852464\n",
      "Epoch 3 | Step 1708700 | Avg Loss: 0.0156 | Grad Norm: 0.00885734\n",
      "Epoch 3 | Step 1708800 | Avg Loss: 0.0158 | Grad Norm: 0.00855576\n",
      "Epoch 3 | Step 1708900 | Avg Loss: 0.0156 | Grad Norm: 0.00860021\n",
      "Epoch 3 | Step 1709000 | Avg Loss: 0.0156 | Grad Norm: 0.00885617\n",
      "Epoch 3 | Step 1709100 | Avg Loss: 0.0151 | Grad Norm: 0.01041171\n",
      "Epoch 3 | Step 1709200 | Avg Loss: 0.0149 | Grad Norm: 0.00880597\n",
      "Epoch 3 | Step 1709300 | Avg Loss: 0.0151 | Grad Norm: 0.00756429\n",
      "Epoch 3 | Step 1709400 | Avg Loss: 0.0155 | Grad Norm: 0.00983874\n",
      "Epoch 3 | Step 1709500 | Avg Loss: 0.0159 | Grad Norm: 0.00940398\n",
      "Epoch 3 | Step 1709600 | Avg Loss: 0.0163 | Grad Norm: 0.00997120\n",
      "Epoch 3 | Step 1709700 | Avg Loss: 0.0163 | Grad Norm: 0.00924515\n",
      "Epoch 3 | Step 1709800 | Avg Loss: 0.0160 | Grad Norm: 0.00969091\n",
      "Epoch 3 | Step 1709900 | Avg Loss: 0.0160 | Grad Norm: 0.00846082\n",
      "Epoch 3 | Step 1710000 | Avg Loss: 0.0158 | Grad Norm: 0.00929366\n",
      "Epoch 3 | Step 1710100 | Avg Loss: 0.0155 | Grad Norm: 0.01016816\n",
      "Epoch 3 | Step 1710200 | Avg Loss: 0.0154 | Grad Norm: 0.00813905\n",
      "Epoch 3 | Step 1710300 | Avg Loss: 0.0155 | Grad Norm: 0.00861918\n",
      "Epoch 3 | Step 1710400 | Avg Loss: 0.0153 | Grad Norm: 0.00835611\n",
      "Epoch 3 | Step 1710500 | Avg Loss: 0.0154 | Grad Norm: 0.00844076\n",
      "Epoch 3 | Step 1710600 | Avg Loss: 0.0154 | Grad Norm: 0.00905010\n",
      "Epoch 3 | Step 1710700 | Avg Loss: 0.0152 | Grad Norm: 0.00869625\n",
      "Epoch 3 | Step 1710800 | Avg Loss: 0.0154 | Grad Norm: 0.00893188\n",
      "Epoch 3 | Step 1710900 | Avg Loss: 0.0154 | Grad Norm: 0.00892553\n",
      "Epoch 3 | Step 1711000 | Avg Loss: 0.0155 | Grad Norm: 0.00889118\n",
      "Epoch 3 | Step 1711100 | Avg Loss: 0.0159 | Grad Norm: 0.00868288\n",
      "Epoch 3 | Step 1711200 | Avg Loss: 0.0161 | Grad Norm: 0.00808583\n",
      "Epoch 3 | Step 1711300 | Avg Loss: 0.0161 | Grad Norm: 0.00934815\n",
      "Epoch 3 | Step 1711400 | Avg Loss: 0.0157 | Grad Norm: 0.00934793\n",
      "Epoch 3 | Step 1711500 | Avg Loss: 0.0154 | Grad Norm: 0.00823995\n",
      "Epoch 3 | Step 1711600 | Avg Loss: 0.0156 | Grad Norm: 0.00933412\n",
      "Epoch 3 | Step 1711700 | Avg Loss: 0.0157 | Grad Norm: 0.00938937\n",
      "Epoch 3 | Step 1711800 | Avg Loss: 0.0155 | Grad Norm: 0.00889983\n",
      "Epoch 3 | Step 1711900 | Avg Loss: 0.0155 | Grad Norm: 0.00979458\n",
      "Epoch 3 | Step 1712000 | Avg Loss: 0.0157 | Grad Norm: 0.01031522\n",
      "Epoch 3 | Step 1712100 | Avg Loss: 0.0153 | Grad Norm: 0.00929608\n",
      "Epoch 3 | Step 1712200 | Avg Loss: 0.0154 | Grad Norm: 0.01064802\n",
      "Epoch 3 | Step 1712300 | Avg Loss: 0.0154 | Grad Norm: 0.00963458\n",
      "Epoch 3 | Step 1712400 | Avg Loss: 0.0149 | Grad Norm: 0.00989717\n",
      "Epoch 3 | Step 1712500 | Avg Loss: 0.0152 | Grad Norm: 0.00815493\n",
      "Epoch 3 | Step 1712600 | Avg Loss: 0.0152 | Grad Norm: 0.00945944\n",
      "Epoch 3 | Step 1712700 | Avg Loss: 0.0151 | Grad Norm: 0.00837845\n",
      "Epoch 3 | Step 1712800 | Avg Loss: 0.0154 | Grad Norm: 0.00875612\n",
      "Epoch 3 | Step 1712900 | Avg Loss: 0.0155 | Grad Norm: 0.00994183\n",
      "Epoch 3 | Step 1713000 | Avg Loss: 0.0155 | Grad Norm: 0.00844813\n",
      "Epoch 3 | Step 1713100 | Avg Loss: 0.0156 | Grad Norm: 0.00819360\n",
      "Epoch 3 | Step 1713200 | Avg Loss: 0.0153 | Grad Norm: 0.00999403\n",
      "Epoch 3 | Step 1713300 | Avg Loss: 0.0150 | Grad Norm: 0.00863261\n",
      "Epoch 3 | Step 1713400 | Avg Loss: 0.0149 | Grad Norm: 0.01040336\n",
      "Epoch 3 | Step 1713500 | Avg Loss: 0.0150 | Grad Norm: 0.00873852\n",
      "Epoch 3 | Step 1713600 | Avg Loss: 0.0151 | Grad Norm: 0.00904467\n",
      "Epoch 3 | Step 1713700 | Avg Loss: 0.0148 | Grad Norm: 0.00859986\n",
      "Epoch 3 | Step 1713800 | Avg Loss: 0.0147 | Grad Norm: 0.00816184\n",
      "Epoch 3 | Step 1713900 | Avg Loss: 0.0148 | Grad Norm: 0.00912571\n",
      "Epoch 3 | Step 1714000 | Avg Loss: 0.0149 | Grad Norm: 0.01052608\n",
      "Epoch 3 | Step 1714100 | Avg Loss: 0.0147 | Grad Norm: 0.00796289\n",
      "Epoch 3 | Step 1714200 | Avg Loss: 0.0150 | Grad Norm: 0.00692910\n",
      "Epoch 3 | Step 1714300 | Avg Loss: 0.0149 | Grad Norm: 0.00765583\n",
      "Epoch 3 | Step 1714400 | Avg Loss: 0.0153 | Grad Norm: 0.00967720\n",
      "Epoch 3 | Step 1714500 | Avg Loss: 0.0150 | Grad Norm: 0.00838814\n",
      "Epoch 3 | Step 1714600 | Avg Loss: 0.0150 | Grad Norm: 0.00838427\n",
      "Epoch 3 | Step 1714700 | Avg Loss: 0.0152 | Grad Norm: 0.00903680\n",
      "Epoch 3 | Step 1714800 | Avg Loss: 0.0155 | Grad Norm: 0.00879354\n",
      "Epoch 3 | Step 1714900 | Avg Loss: 0.0156 | Grad Norm: 0.01434093\n",
      "Epoch 3 | Step 1715000 | Avg Loss: 0.0154 | Grad Norm: 0.00808352\n",
      "Epoch 3 | Step 1715100 | Avg Loss: 0.0156 | Grad Norm: 0.00894393\n",
      "Epoch 3 | Step 1715200 | Avg Loss: 0.0155 | Grad Norm: 0.00967741\n",
      "Epoch 3 | Step 1715300 | Avg Loss: 0.0155 | Grad Norm: 0.00832449\n",
      "Epoch 3 | Step 1715400 | Avg Loss: 0.0155 | Grad Norm: 0.00989220\n",
      "Epoch 3 | Step 1715500 | Avg Loss: 0.0154 | Grad Norm: 0.00838304\n",
      "Epoch 3 | Step 1715600 | Avg Loss: 0.0152 | Grad Norm: 0.01055392\n",
      "Epoch 3 | Step 1715700 | Avg Loss: 0.0150 | Grad Norm: 0.00944157\n",
      "Epoch 3 | Step 1715800 | Avg Loss: 0.0151 | Grad Norm: 0.00856684\n",
      "Epoch 3 | Step 1715900 | Avg Loss: 0.0156 | Grad Norm: 0.01009748\n",
      "Epoch 3 | Step 1716000 | Avg Loss: 0.0156 | Grad Norm: 0.00900534\n",
      "Epoch 3 | Step 1716100 | Avg Loss: 0.0152 | Grad Norm: 0.00766206\n",
      "Epoch 3 | Step 1716200 | Avg Loss: 0.0149 | Grad Norm: 0.00896482\n",
      "Epoch 3 | Step 1716300 | Avg Loss: 0.0153 | Grad Norm: 0.00800798\n",
      "Epoch 3 | Step 1716400 | Avg Loss: 0.0156 | Grad Norm: 0.00944096\n",
      "Epoch 3 | Step 1716500 | Avg Loss: 0.0159 | Grad Norm: 0.00808845\n",
      "Epoch 3 | Step 1716600 | Avg Loss: 0.0160 | Grad Norm: 0.00826194\n",
      "Epoch 3 | Step 1716700 | Avg Loss: 0.0157 | Grad Norm: 0.00904428\n",
      "Epoch 3 | Step 1716800 | Avg Loss: 0.0154 | Grad Norm: 0.00894968\n",
      "Epoch 3 | Step 1716900 | Avg Loss: 0.0153 | Grad Norm: 0.00784498\n",
      "Epoch 3 | Step 1717000 | Avg Loss: 0.0148 | Grad Norm: 0.00873258\n",
      "Epoch 3 | Step 1717100 | Avg Loss: 0.0150 | Grad Norm: 0.00888469\n",
      "Epoch 3 | Step 1717200 | Avg Loss: 0.0150 | Grad Norm: 0.00968245\n",
      "Epoch 3 | Step 1717300 | Avg Loss: 0.0157 | Grad Norm: 0.00903029\n",
      "Epoch 3 | Step 1717400 | Avg Loss: 0.0158 | Grad Norm: 0.00934780\n",
      "Epoch 3 | Step 1717500 | Avg Loss: 0.0156 | Grad Norm: 0.00914286\n",
      "Epoch 3 | Step 1717600 | Avg Loss: 0.0156 | Grad Norm: 0.00818822\n",
      "Epoch 3 | Step 1717700 | Avg Loss: 0.0154 | Grad Norm: 0.00971713\n",
      "Epoch 3 | Step 1717800 | Avg Loss: 0.0154 | Grad Norm: 0.00859856\n",
      "Epoch 3 | Step 1717900 | Avg Loss: 0.0156 | Grad Norm: 0.00922131\n",
      "Epoch 3 | Step 1718000 | Avg Loss: 0.0150 | Grad Norm: 0.00867463\n",
      "Epoch 3 | Step 1718100 | Avg Loss: 0.0151 | Grad Norm: 0.00855563\n",
      "Epoch 3 | Step 1718200 | Avg Loss: 0.0148 | Grad Norm: 0.00926035\n",
      "Epoch 3 | Step 1718300 | Avg Loss: 0.0144 | Grad Norm: 0.00813892\n",
      "Epoch 3 | Step 1718400 | Avg Loss: 0.0145 | Grad Norm: 0.00832355\n",
      "Epoch 3 | Step 1718500 | Avg Loss: 0.0147 | Grad Norm: 0.00883691\n",
      "Epoch 3 | Step 1718600 | Avg Loss: 0.0151 | Grad Norm: 0.00785029\n",
      "Epoch 3 | Step 1718700 | Avg Loss: 0.0155 | Grad Norm: 0.00789519\n",
      "Epoch 3 | Step 1718800 | Avg Loss: 0.0157 | Grad Norm: 0.00811829\n",
      "Epoch 3 | Step 1718900 | Avg Loss: 0.0159 | Grad Norm: 0.00798071\n",
      "Epoch 3 | Step 1719000 | Avg Loss: 0.0158 | Grad Norm: 0.00808673\n",
      "Epoch 3 | Step 1719100 | Avg Loss: 0.0156 | Grad Norm: 0.00787879\n",
      "Epoch 3 | Step 1719200 | Avg Loss: 0.0154 | Grad Norm: 0.00928183\n",
      "Epoch 3 | Step 1719300 | Avg Loss: 0.0151 | Grad Norm: 0.00769255\n",
      "Epoch 3 | Step 1719400 | Avg Loss: 0.0152 | Grad Norm: 0.00788796\n",
      "Epoch 3 | Step 1719500 | Avg Loss: 0.0154 | Grad Norm: 0.01075736\n",
      "Epoch 3 | Step 1719600 | Avg Loss: 0.0152 | Grad Norm: 0.00799008\n",
      "Epoch 3 | Step 1719700 | Avg Loss: 0.0153 | Grad Norm: 0.00736451\n",
      "Epoch 3 | Step 1719800 | Avg Loss: 0.0154 | Grad Norm: 0.00890483\n",
      "Epoch 3 | Step 1719900 | Avg Loss: 0.0155 | Grad Norm: 0.00798362\n",
      "Epoch 3 | Step 1720000 | Avg Loss: 0.0155 | Grad Norm: 0.00940075\n",
      "Epoch 3 | Step 1720100 | Avg Loss: 0.0153 | Grad Norm: 0.00785081\n",
      "Epoch 3 | Step 1720200 | Avg Loss: 0.0153 | Grad Norm: 0.00897887\n",
      "Epoch 3 | Step 1720300 | Avg Loss: 0.0153 | Grad Norm: 0.00962417\n",
      "Epoch 3 | Step 1720400 | Avg Loss: 0.0153 | Grad Norm: 0.00830054\n",
      "Epoch 3 | Step 1720500 | Avg Loss: 0.0152 | Grad Norm: 0.01046790\n",
      "Epoch 3 | Step 1720600 | Avg Loss: 0.0155 | Grad Norm: 0.00806611\n",
      "Epoch 3 | Step 1720700 | Avg Loss: 0.0154 | Grad Norm: 0.00849479\n",
      "Epoch 3 | Step 1720800 | Avg Loss: 0.0155 | Grad Norm: 0.00931105\n",
      "Epoch 3 | Step 1720900 | Avg Loss: 0.0156 | Grad Norm: 0.00917703\n",
      "Epoch 3 | Step 1721000 | Avg Loss: 0.0155 | Grad Norm: 0.00771064\n",
      "Epoch 3 | Step 1721100 | Avg Loss: 0.0156 | Grad Norm: 0.01457383\n",
      "Epoch 3 | Step 1721200 | Avg Loss: 0.0157 | Grad Norm: 0.01058987\n",
      "Epoch 3 | Step 1721300 | Avg Loss: 0.0155 | Grad Norm: 0.00799812\n",
      "Epoch 3 | Step 1721400 | Avg Loss: 0.0156 | Grad Norm: 0.00783437\n",
      "Epoch 3 | Step 1721500 | Avg Loss: 0.0157 | Grad Norm: 0.00839558\n",
      "Epoch 3 | Step 1721600 | Avg Loss: 0.0156 | Grad Norm: 0.01020735\n",
      "Epoch 3 | Step 1721700 | Avg Loss: 0.0155 | Grad Norm: 0.00930943\n",
      "Epoch 3 | Step 1721800 | Avg Loss: 0.0158 | Grad Norm: 0.00892401\n",
      "Epoch 3 | Step 1721900 | Avg Loss: 0.0157 | Grad Norm: 0.00925855\n",
      "Epoch 3 | Step 1722000 | Avg Loss: 0.0160 | Grad Norm: 0.00899235\n",
      "Epoch 3 | Step 1722100 | Avg Loss: 0.0158 | Grad Norm: 0.00825287\n",
      "Epoch 3 | Step 1722200 | Avg Loss: 0.0157 | Grad Norm: 0.00883179\n",
      "Epoch 3 | Step 1722300 | Avg Loss: 0.0160 | Grad Norm: 0.01079030\n",
      "Epoch 3 | Step 1722400 | Avg Loss: 0.0158 | Grad Norm: 0.00827135\n",
      "Epoch 3 | Step 1722500 | Avg Loss: 0.0158 | Grad Norm: 0.00946078\n",
      "Epoch 3 | Step 1722600 | Avg Loss: 0.0157 | Grad Norm: 0.00895283\n",
      "Epoch 3 | Step 1722700 | Avg Loss: 0.0157 | Grad Norm: 0.00889780\n",
      "Epoch 3 | Step 1722800 | Avg Loss: 0.0154 | Grad Norm: 0.00976945\n",
      "Epoch 3 | Step 1722900 | Avg Loss: 0.0155 | Grad Norm: 0.00995028\n",
      "Epoch 3 | Step 1723000 | Avg Loss: 0.0155 | Grad Norm: 0.00792810\n",
      "Epoch 3 | Step 1723100 | Avg Loss: 0.0153 | Grad Norm: 0.00879510\n",
      "Epoch 3 | Step 1723200 | Avg Loss: 0.0154 | Grad Norm: 0.00969583\n",
      "Epoch 3 | Step 1723300 | Avg Loss: 0.0151 | Grad Norm: 0.00788136\n",
      "Epoch 3 | Step 1723400 | Avg Loss: 0.0151 | Grad Norm: 0.00970294\n",
      "Epoch 3 | Step 1723500 | Avg Loss: 0.0152 | Grad Norm: 0.01092007\n",
      "Epoch 3 | Step 1723600 | Avg Loss: 0.0154 | Grad Norm: 0.00883467\n",
      "Epoch 3 | Step 1723700 | Avg Loss: 0.0153 | Grad Norm: 0.00994207\n",
      "Epoch 3 | Step 1723800 | Avg Loss: 0.0154 | Grad Norm: 0.00762387\n",
      "Epoch 3 | Step 1723900 | Avg Loss: 0.0152 | Grad Norm: 0.00880154\n",
      "Epoch 3 | Step 1724000 | Avg Loss: 0.0154 | Grad Norm: 0.00911983\n",
      "Epoch 3 | Step 1724100 | Avg Loss: 0.0152 | Grad Norm: 0.00918068\n",
      "Epoch 3 | Step 1724200 | Avg Loss: 0.0156 | Grad Norm: 0.00860939\n",
      "Epoch 3 | Step 1724300 | Avg Loss: 0.0153 | Grad Norm: 0.00895211\n",
      "Epoch 3 | Step 1724400 | Avg Loss: 0.0149 | Grad Norm: 0.00932846\n",
      "Epoch 3 | Step 1724500 | Avg Loss: 0.0148 | Grad Norm: 0.00881563\n",
      "Epoch 3 | Step 1724600 | Avg Loss: 0.0152 | Grad Norm: 0.00985865\n",
      "Epoch 3 | Step 1724700 | Avg Loss: 0.0156 | Grad Norm: 0.01031379\n",
      "Epoch 3 | Step 1724800 | Avg Loss: 0.0154 | Grad Norm: 0.00927019\n",
      "Epoch 3 | Step 1724900 | Avg Loss: 0.0153 | Grad Norm: 0.00805598\n",
      "Epoch 3 | Step 1725000 | Avg Loss: 0.0150 | Grad Norm: 0.00813312\n",
      "Epoch 3 | Step 1725100 | Avg Loss: 0.0153 | Grad Norm: 0.00937049\n",
      "Epoch 3 | Step 1725200 | Avg Loss: 0.0154 | Grad Norm: 0.00854570\n",
      "Epoch 3 | Step 1725300 | Avg Loss: 0.0157 | Grad Norm: 0.00868685\n",
      "Epoch 3 | Step 1725400 | Avg Loss: 0.0157 | Grad Norm: 0.00864875\n",
      "Epoch 3 | Step 1725500 | Avg Loss: 0.0156 | Grad Norm: 0.01135008\n",
      "Epoch 3 | Step 1725600 | Avg Loss: 0.0159 | Grad Norm: 0.00741503\n",
      "Epoch 3 | Step 1725700 | Avg Loss: 0.0159 | Grad Norm: 0.00838530\n",
      "Epoch 3 | Step 1725800 | Avg Loss: 0.0157 | Grad Norm: 0.01311500\n",
      "Epoch 3 | Step 1725900 | Avg Loss: 0.0158 | Grad Norm: 0.00950760\n",
      "Epoch 3 | Step 1726000 | Avg Loss: 0.0159 | Grad Norm: 0.01190683\n",
      "Epoch 3 | Step 1726100 | Avg Loss: 0.0159 | Grad Norm: 0.00784448\n",
      "Epoch 3 | Step 1726200 | Avg Loss: 0.0154 | Grad Norm: 0.00918280\n",
      "Epoch 3 | Step 1726300 | Avg Loss: 0.0155 | Grad Norm: 0.01125193\n",
      "Epoch 3 | Step 1726400 | Avg Loss: 0.0157 | Grad Norm: 0.00880988\n",
      "Epoch 3 | Step 1726500 | Avg Loss: 0.0156 | Grad Norm: 0.00873847\n",
      "Epoch 3 | Step 1726600 | Avg Loss: 0.0161 | Grad Norm: 0.00919597\n",
      "Epoch 3 | Step 1726700 | Avg Loss: 0.0161 | Grad Norm: 0.00863827\n",
      "Epoch 3 | Step 1726800 | Avg Loss: 0.0161 | Grad Norm: 0.00922297\n",
      "Epoch 3 | Step 1726900 | Avg Loss: 0.0159 | Grad Norm: 0.00798272\n",
      "Epoch 3 | Step 1727000 | Avg Loss: 0.0154 | Grad Norm: 0.00836957\n",
      "Epoch 3 | Step 1727100 | Avg Loss: 0.0154 | Grad Norm: 0.00905370\n",
      "Epoch 3 | Step 1727200 | Avg Loss: 0.0155 | Grad Norm: 0.00790242\n",
      "Epoch 3 | Step 1727300 | Avg Loss: 0.0153 | Grad Norm: 0.01096575\n",
      "Epoch 3 | Step 1727400 | Avg Loss: 0.0155 | Grad Norm: 0.00859264\n",
      "Epoch 3 | Step 1727500 | Avg Loss: 0.0154 | Grad Norm: 0.00943102\n",
      "Epoch 3 | Step 1727600 | Avg Loss: 0.0153 | Grad Norm: 0.00786401\n",
      "Epoch 3 | Step 1727700 | Avg Loss: 0.0152 | Grad Norm: 0.00861970\n",
      "Epoch 3 | Step 1727800 | Avg Loss: 0.0156 | Grad Norm: 0.00933932\n",
      "Epoch 3 | Step 1727900 | Avg Loss: 0.0156 | Grad Norm: 0.00885794\n",
      "Epoch 3 | Step 1728000 | Avg Loss: 0.0158 | Grad Norm: 0.00947471\n",
      "Epoch 3 | Step 1728100 | Avg Loss: 0.0156 | Grad Norm: 0.00902617\n",
      "Epoch 3 | Step 1728200 | Avg Loss: 0.0156 | Grad Norm: 0.00903253\n",
      "Epoch 3 | Step 1728300 | Avg Loss: 0.0155 | Grad Norm: 0.00870572\n",
      "Epoch 3 | Step 1728400 | Avg Loss: 0.0154 | Grad Norm: 0.00774190\n",
      "Epoch 3 | Step 1728500 | Avg Loss: 0.0152 | Grad Norm: 0.00922708\n",
      "Epoch 3 | Step 1728600 | Avg Loss: 0.0151 | Grad Norm: 0.00873901\n",
      "Epoch 3 | Step 1728700 | Avg Loss: 0.0154 | Grad Norm: 0.00864194\n",
      "Epoch 3 | Step 1728800 | Avg Loss: 0.0159 | Grad Norm: 0.00875617\n",
      "Epoch 3 | Step 1728900 | Avg Loss: 0.0163 | Grad Norm: 0.01772188\n",
      "Epoch 3 | Step 1729000 | Avg Loss: 0.0160 | Grad Norm: 0.01285064\n",
      "Epoch 3 | Step 1729100 | Avg Loss: 0.0159 | Grad Norm: 0.00862820\n",
      "Epoch 3 | Step 1729200 | Avg Loss: 0.0158 | Grad Norm: 0.01024279\n",
      "Epoch 3 | Step 1729300 | Avg Loss: 0.0160 | Grad Norm: 0.00907882\n",
      "Epoch 3 | Step 1729400 | Avg Loss: 0.0158 | Grad Norm: 0.01039214\n",
      "Epoch 3 | Step 1729500 | Avg Loss: 0.0157 | Grad Norm: 0.00797484\n",
      "Epoch 3 | Step 1729600 | Avg Loss: 0.0162 | Grad Norm: 0.01078250\n",
      "Epoch 3 | Step 1729700 | Avg Loss: 0.0161 | Grad Norm: 0.00910170\n",
      "Epoch 3 | Step 1729800 | Avg Loss: 0.0163 | Grad Norm: 0.00899537\n",
      "Epoch 3 | Step 1729900 | Avg Loss: 0.0161 | Grad Norm: 0.00943862\n",
      "Epoch 3 | Step 1730000 | Avg Loss: 0.0161 | Grad Norm: 0.00954188\n",
      "Epoch 3 | Step 1730100 | Avg Loss: 0.0162 | Grad Norm: 0.00870382\n",
      "Epoch 3 | Step 1730200 | Avg Loss: 0.0164 | Grad Norm: 0.00928188\n",
      "Epoch 3 | Step 1730300 | Avg Loss: 0.0161 | Grad Norm: 0.00882365\n",
      "Epoch 3 | Step 1730400 | Avg Loss: 0.0158 | Grad Norm: 0.00970150\n",
      "Epoch 3 | Step 1730500 | Avg Loss: 0.0156 | Grad Norm: 0.01020091\n",
      "Epoch 3 | Step 1730600 | Avg Loss: 0.0158 | Grad Norm: 0.00895303\n",
      "Epoch 3 | Step 1730700 | Avg Loss: 0.0163 | Grad Norm: 0.00862586\n",
      "Epoch 3 | Step 1730800 | Avg Loss: 0.0160 | Grad Norm: 0.00850059\n",
      "Epoch 3 | Step 1730900 | Avg Loss: 0.0157 | Grad Norm: 0.00858720\n",
      "Epoch 3 | Step 1731000 | Avg Loss: 0.0157 | Grad Norm: 0.00848847\n",
      "Epoch 3 | Step 1731100 | Avg Loss: 0.0159 | Grad Norm: 0.00896447\n",
      "Epoch 3 | Step 1731200 | Avg Loss: 0.0157 | Grad Norm: 0.00873870\n",
      "Epoch 3 | Step 1731300 | Avg Loss: 0.0155 | Grad Norm: 0.00815510\n",
      "Epoch 3 | Step 1731400 | Avg Loss: 0.0154 | Grad Norm: 0.00889085\n",
      "Epoch 3 | Step 1731500 | Avg Loss: 0.0151 | Grad Norm: 0.00828378\n",
      "Epoch 3 | Step 1731600 | Avg Loss: 0.0152 | Grad Norm: 0.00802615\n",
      "Epoch 3 | Step 1731700 | Avg Loss: 0.0153 | Grad Norm: 0.00865136\n",
      "Epoch 3 | Step 1731800 | Avg Loss: 0.0152 | Grad Norm: 0.00896082\n",
      "Epoch 3 | Step 1731900 | Avg Loss: 0.0150 | Grad Norm: 0.00807504\n",
      "Epoch 3 | Step 1732000 | Avg Loss: 0.0151 | Grad Norm: 0.00946354\n",
      "Epoch 3 | Step 1732100 | Avg Loss: 0.0152 | Grad Norm: 0.00906550\n",
      "Epoch 3 | Step 1732200 | Avg Loss: 0.0154 | Grad Norm: 0.00945992\n",
      "Epoch 3 | Step 1732300 | Avg Loss: 0.0151 | Grad Norm: 0.00847992\n",
      "Epoch 3 | Step 1732400 | Avg Loss: 0.0151 | Grad Norm: 0.00778774\n",
      "Epoch 3 | Step 1732500 | Avg Loss: 0.0151 | Grad Norm: 0.00846307\n",
      "Epoch 3 | Step 1732600 | Avg Loss: 0.0150 | Grad Norm: 0.00784326\n",
      "Epoch 3 | Step 1732700 | Avg Loss: 0.0150 | Grad Norm: 0.00892804\n",
      "Epoch 3 | Step 1732800 | Avg Loss: 0.0156 | Grad Norm: 0.00910805\n",
      "Epoch 3 | Step 1732900 | Avg Loss: 0.0154 | Grad Norm: 0.00831789\n",
      "Epoch 3 | Step 1733000 | Avg Loss: 0.0155 | Grad Norm: 0.00970263\n",
      "Epoch 3 | Step 1733100 | Avg Loss: 0.0157 | Grad Norm: 0.00915828\n",
      "Epoch 3 | Step 1733200 | Avg Loss: 0.0157 | Grad Norm: 0.00996629\n",
      "Epoch 3 | Step 1733300 | Avg Loss: 0.0156 | Grad Norm: 0.00901643\n",
      "Epoch 3 | Step 1733400 | Avg Loss: 0.0157 | Grad Norm: 0.01212126\n",
      "Epoch 3 | Step 1733500 | Avg Loss: 0.0153 | Grad Norm: 0.00920427\n",
      "Epoch 3 | Step 1733600 | Avg Loss: 0.0151 | Grad Norm: 0.00968350\n",
      "Epoch 3 | Step 1733700 | Avg Loss: 0.0153 | Grad Norm: 0.00941293\n",
      "Epoch 3 | Step 1733800 | Avg Loss: 0.0157 | Grad Norm: 0.00911306\n",
      "Epoch 3 | Step 1733900 | Avg Loss: 0.0157 | Grad Norm: 0.01039122\n",
      "Epoch 3 | Step 1734000 | Avg Loss: 0.0160 | Grad Norm: 0.00890701\n",
      "Epoch 3 | Step 1734100 | Avg Loss: 0.0155 | Grad Norm: 0.00805193\n",
      "Epoch 3 | Step 1734200 | Avg Loss: 0.0157 | Grad Norm: 0.00890350\n",
      "Epoch 3 | Step 1734300 | Avg Loss: 0.0153 | Grad Norm: 0.00797594\n",
      "Epoch 3 | Step 1734400 | Avg Loss: 0.0158 | Grad Norm: 0.01083905\n",
      "Epoch 3 | Step 1734500 | Avg Loss: 0.0156 | Grad Norm: 0.00791849\n",
      "Epoch 3 | Step 1734600 | Avg Loss: 0.0156 | Grad Norm: 0.00794476\n",
      "Epoch 3 | Step 1734700 | Avg Loss: 0.0156 | Grad Norm: 0.00870043\n",
      "Epoch 3 | Step 1734800 | Avg Loss: 0.0152 | Grad Norm: 0.00872817\n",
      "Epoch 3 | Step 1734900 | Avg Loss: 0.0152 | Grad Norm: 0.00870836\n",
      "Epoch 3 | Step 1735000 | Avg Loss: 0.0151 | Grad Norm: 0.00882738\n",
      "Epoch 3 | Step 1735100 | Avg Loss: 0.0150 | Grad Norm: 0.00835331\n",
      "Epoch 3 | Step 1735200 | Avg Loss: 0.0148 | Grad Norm: 0.00820893\n",
      "Epoch 3 | Step 1735300 | Avg Loss: 0.0150 | Grad Norm: 0.01049338\n",
      "Epoch 3 | Step 1735400 | Avg Loss: 0.0149 | Grad Norm: 0.00818126\n",
      "Epoch 3 | Step 1735500 | Avg Loss: 0.0148 | Grad Norm: 0.00840603\n",
      "Epoch 3 | Step 1735600 | Avg Loss: 0.0153 | Grad Norm: 0.00829490\n",
      "Epoch 3 | Step 1735700 | Avg Loss: 0.0157 | Grad Norm: 0.00762744\n",
      "Epoch 3 | Step 1735800 | Avg Loss: 0.0159 | Grad Norm: 0.00958199\n",
      "Epoch 3 | Step 1735900 | Avg Loss: 0.0158 | Grad Norm: 0.00909861\n",
      "Epoch 3 | Step 1736000 | Avg Loss: 0.0159 | Grad Norm: 0.01140571\n",
      "Epoch 3 | Step 1736100 | Avg Loss: 0.0155 | Grad Norm: 0.00876736\n",
      "Epoch 3 | Step 1736200 | Avg Loss: 0.0156 | Grad Norm: 0.00768974\n",
      "Epoch 3 | Step 1736300 | Avg Loss: 0.0158 | Grad Norm: 0.00960774\n",
      "Epoch 3 | Step 1736400 | Avg Loss: 0.0160 | Grad Norm: 0.00920363\n",
      "Epoch 3 | Step 1736500 | Avg Loss: 0.0162 | Grad Norm: 0.00786043\n",
      "Epoch 3 | Step 1736600 | Avg Loss: 0.0161 | Grad Norm: 0.00894201\n",
      "Epoch 3 | Step 1736700 | Avg Loss: 0.0158 | Grad Norm: 0.00813014\n",
      "Epoch 3 | Step 1736800 | Avg Loss: 0.0158 | Grad Norm: 0.01095741\n",
      "Epoch 3 | Step 1736900 | Avg Loss: 0.0159 | Grad Norm: 0.00832235\n",
      "Epoch 3 | Step 1737000 | Avg Loss: 0.0158 | Grad Norm: 0.00890452\n",
      "Epoch 3 | Step 1737100 | Avg Loss: 0.0158 | Grad Norm: 0.00808128\n",
      "Epoch 3 | Step 1737200 | Avg Loss: 0.0158 | Grad Norm: 0.01133816\n",
      "Epoch 3 | Step 1737300 | Avg Loss: 0.0161 | Grad Norm: 0.00876245\n",
      "Epoch 3 | Step 1737400 | Avg Loss: 0.0160 | Grad Norm: 0.00912767\n",
      "Epoch 3 | Step 1737500 | Avg Loss: 0.0159 | Grad Norm: 0.01408982\n",
      "Epoch 3 | Step 1737600 | Avg Loss: 0.0158 | Grad Norm: 0.00884274\n",
      "Epoch 3 | Step 1737700 | Avg Loss: 0.0154 | Grad Norm: 0.00767372\n",
      "Epoch 3 | Step 1737800 | Avg Loss: 0.0154 | Grad Norm: 0.00820393\n",
      "Epoch 3 | Step 1737900 | Avg Loss: 0.0154 | Grad Norm: 0.00775939\n",
      "Epoch 3 | Step 1738000 | Avg Loss: 0.0153 | Grad Norm: 0.00885812\n",
      "Epoch 3 | Step 1738100 | Avg Loss: 0.0153 | Grad Norm: 0.00967220\n",
      "Epoch 3 | Step 1738200 | Avg Loss: 0.0158 | Grad Norm: 0.01146269\n",
      "Epoch 3 | Step 1738300 | Avg Loss: 0.0158 | Grad Norm: 0.00773910\n",
      "Epoch 3 | Step 1738400 | Avg Loss: 0.0157 | Grad Norm: 0.00839756\n",
      "Epoch 3 | Step 1738500 | Avg Loss: 0.0155 | Grad Norm: 0.00889022\n",
      "Epoch 3 | Step 1738600 | Avg Loss: 0.0155 | Grad Norm: 0.00870039\n",
      "Epoch 3 | Step 1738700 | Avg Loss: 0.0155 | Grad Norm: 0.00956624\n",
      "Epoch 3 | Step 1738800 | Avg Loss: 0.0154 | Grad Norm: 0.00866193\n",
      "Epoch 3 | Step 1738900 | Avg Loss: 0.0154 | Grad Norm: 0.00778731\n",
      "Epoch 3 | Step 1739000 | Avg Loss: 0.0155 | Grad Norm: 0.00793658\n",
      "Epoch 3 | Step 1739100 | Avg Loss: 0.0155 | Grad Norm: 0.00890512\n",
      "Epoch 3 | Step 1739200 | Avg Loss: 0.0157 | Grad Norm: 0.00909086\n",
      "Epoch 3 | Step 1739300 | Avg Loss: 0.0161 | Grad Norm: 0.00810837\n",
      "Epoch 3 | Step 1739400 | Avg Loss: 0.0158 | Grad Norm: 0.00862587\n",
      "Epoch 3 | Step 1739500 | Avg Loss: 0.0161 | Grad Norm: 0.00804966\n",
      "Epoch 3 | Step 1739600 | Avg Loss: 0.0155 | Grad Norm: 0.00837370\n",
      "Epoch 3 | Step 1739700 | Avg Loss: 0.0158 | Grad Norm: 0.01029200\n",
      "Epoch 3 | Step 1739800 | Avg Loss: 0.0160 | Grad Norm: 0.00863292\n",
      "Epoch 3 | Step 1739900 | Avg Loss: 0.0157 | Grad Norm: 0.00919744\n",
      "Epoch 3 | Step 1740000 | Avg Loss: 0.0158 | Grad Norm: 0.00881943\n",
      "Epoch 3 | Step 1740100 | Avg Loss: 0.0155 | Grad Norm: 0.00929257\n",
      "Epoch 3 | Step 1740200 | Avg Loss: 0.0154 | Grad Norm: 0.00804522\n",
      "Epoch 3 | Step 1740300 | Avg Loss: 0.0153 | Grad Norm: 0.00953105\n",
      "Epoch 3 | Step 1740400 | Avg Loss: 0.0156 | Grad Norm: 0.00935480\n",
      "Epoch 3 | Step 1740500 | Avg Loss: 0.0158 | Grad Norm: 0.00946774\n",
      "Epoch 3 | Step 1740600 | Avg Loss: 0.0159 | Grad Norm: 0.00888534\n",
      "Epoch 3 | Step 1740700 | Avg Loss: 0.0157 | Grad Norm: 0.00884578\n",
      "Epoch 3 | Step 1740800 | Avg Loss: 0.0156 | Grad Norm: 0.01043317\n",
      "Epoch 3 | Step 1740900 | Avg Loss: 0.0156 | Grad Norm: 0.00849624\n",
      "Epoch 3 | Step 1741000 | Avg Loss: 0.0155 | Grad Norm: 0.00858485\n",
      "Epoch 3 | Step 1741100 | Avg Loss: 0.0151 | Grad Norm: 0.00926048\n",
      "Epoch 3 | Step 1741200 | Avg Loss: 0.0154 | Grad Norm: 0.00835474\n",
      "Epoch 3 | Step 1741300 | Avg Loss: 0.0156 | Grad Norm: 0.00922304\n",
      "Epoch 3 | Step 1741400 | Avg Loss: 0.0159 | Grad Norm: 0.01017151\n",
      "Epoch 3 | Step 1741500 | Avg Loss: 0.0158 | Grad Norm: 0.00935416\n",
      "Epoch 3 | Step 1741600 | Avg Loss: 0.0157 | Grad Norm: 0.00871426\n",
      "Epoch 3 | Step 1741700 | Avg Loss: 0.0162 | Grad Norm: 0.00806801\n",
      "Epoch 3 | Step 1741800 | Avg Loss: 0.0161 | Grad Norm: 0.00973755\n",
      "Epoch 3 | Step 1741900 | Avg Loss: 0.0159 | Grad Norm: 0.00968212\n",
      "Epoch 3 | Step 1742000 | Avg Loss: 0.0161 | Grad Norm: 0.01179479\n",
      "Epoch 3 | Step 1742100 | Avg Loss: 0.0163 | Grad Norm: 0.00837204\n",
      "Epoch 3 | Step 1742200 | Avg Loss: 0.0162 | Grad Norm: 0.00954983\n",
      "Epoch 3 | Step 1742300 | Avg Loss: 0.0161 | Grad Norm: 0.00821636\n",
      "Epoch 3 | Step 1742400 | Avg Loss: 0.0161 | Grad Norm: 0.00959424\n",
      "Epoch 3 | Step 1742500 | Avg Loss: 0.0161 | Grad Norm: 0.01108360\n",
      "Epoch 3 | Step 1742600 | Avg Loss: 0.0160 | Grad Norm: 0.00975651\n",
      "Epoch 3 | Step 1742700 | Avg Loss: 0.0161 | Grad Norm: 0.00924418\n",
      "Epoch 3 | Step 1742800 | Avg Loss: 0.0160 | Grad Norm: 0.00928733\n",
      "Epoch 3 | Step 1742900 | Avg Loss: 0.0162 | Grad Norm: 0.00893410\n",
      "Epoch 3 | Step 1743000 | Avg Loss: 0.0161 | Grad Norm: 0.01008813\n",
      "Epoch 3 | Step 1743100 | Avg Loss: 0.0158 | Grad Norm: 0.00849549\n",
      "Epoch 3 | Step 1743200 | Avg Loss: 0.0158 | Grad Norm: 0.00860916\n",
      "Epoch 3 | Step 1743300 | Avg Loss: 0.0156 | Grad Norm: 0.00852729\n",
      "Epoch 3 | Step 1743400 | Avg Loss: 0.0156 | Grad Norm: 0.00877567\n",
      "Epoch 3 | Step 1743500 | Avg Loss: 0.0157 | Grad Norm: 0.00844739\n",
      "Epoch 3 | Step 1743600 | Avg Loss: 0.0158 | Grad Norm: 0.00859304\n",
      "Epoch 3 | Step 1743700 | Avg Loss: 0.0162 | Grad Norm: 0.00927842\n",
      "Epoch 3 | Step 1743800 | Avg Loss: 0.0162 | Grad Norm: 0.01007971\n",
      "Epoch 3 | Step 1743900 | Avg Loss: 0.0164 | Grad Norm: 0.00925686\n",
      "Epoch 3 | Step 1744000 | Avg Loss: 0.0158 | Grad Norm: 0.00946579\n",
      "Epoch 3 | Step 1744100 | Avg Loss: 0.0158 | Grad Norm: 0.00868293\n",
      "Epoch 3 | Step 1744200 | Avg Loss: 0.0158 | Grad Norm: 0.01039732\n",
      "Epoch 3 | Step 1744300 | Avg Loss: 0.0156 | Grad Norm: 0.00876768\n",
      "Epoch 3 | Step 1744400 | Avg Loss: 0.0152 | Grad Norm: 0.00839736\n",
      "Epoch 3 | Step 1744500 | Avg Loss: 0.0155 | Grad Norm: 0.01056502\n",
      "Epoch 3 | Step 1744600 | Avg Loss: 0.0156 | Grad Norm: 0.00832353\n",
      "Epoch 3 | Step 1744700 | Avg Loss: 0.0155 | Grad Norm: 0.01099170\n",
      "Epoch 3 | Step 1744800 | Avg Loss: 0.0155 | Grad Norm: 0.00892460\n",
      "Epoch 3 | Step 1744900 | Avg Loss: 0.0158 | Grad Norm: 0.00870643\n",
      "Epoch 3 | Step 1745000 | Avg Loss: 0.0155 | Grad Norm: 0.00851191\n",
      "Epoch 3 | Step 1745100 | Avg Loss: 0.0153 | Grad Norm: 0.00930764\n",
      "Epoch 3 | Step 1745200 | Avg Loss: 0.0149 | Grad Norm: 0.00857953\n",
      "Epoch 3 | Step 1745300 | Avg Loss: 0.0151 | Grad Norm: 0.01066127\n",
      "Epoch 3 | Step 1745400 | Avg Loss: 0.0149 | Grad Norm: 0.00935990\n",
      "Epoch 3 | Step 1745500 | Avg Loss: 0.0151 | Grad Norm: 0.00964348\n",
      "Epoch 3 | Step 1745600 | Avg Loss: 0.0156 | Grad Norm: 0.00886723\n",
      "Epoch 3 | Step 1745700 | Avg Loss: 0.0153 | Grad Norm: 0.00877584\n",
      "Epoch 3 | Step 1745800 | Avg Loss: 0.0153 | Grad Norm: 0.00826368\n",
      "Epoch 3 | Step 1745900 | Avg Loss: 0.0155 | Grad Norm: 0.00909172\n",
      "Epoch 3 | Step 1746000 | Avg Loss: 0.0155 | Grad Norm: 0.00966970\n",
      "Epoch 3 | Step 1746100 | Avg Loss: 0.0156 | Grad Norm: 0.01015794\n",
      "Epoch 3 | Step 1746200 | Avg Loss: 0.0155 | Grad Norm: 0.00885737\n",
      "Epoch 3 | Step 1746300 | Avg Loss: 0.0156 | Grad Norm: 0.00846127\n",
      "Epoch 3 | Step 1746400 | Avg Loss: 0.0154 | Grad Norm: 0.00915231\n",
      "Epoch 3 | Step 1746500 | Avg Loss: 0.0153 | Grad Norm: 0.00962849\n",
      "Epoch 3 | Step 1746600 | Avg Loss: 0.0155 | Grad Norm: 0.00912198\n",
      "Epoch 3 | Step 1746700 | Avg Loss: 0.0158 | Grad Norm: 0.01014301\n",
      "Epoch 3 | Step 1746800 | Avg Loss: 0.0156 | Grad Norm: 0.00854748\n",
      "Epoch 3 | Step 1746900 | Avg Loss: 0.0156 | Grad Norm: 0.00987439\n",
      "Epoch 3 | Step 1747000 | Avg Loss: 0.0154 | Grad Norm: 0.00847507\n",
      "Epoch 3 | Step 1747100 | Avg Loss: 0.0154 | Grad Norm: 0.00820954\n",
      "Epoch 3 | Step 1747200 | Avg Loss: 0.0154 | Grad Norm: 0.00934037\n",
      "Epoch 3 | Step 1747300 | Avg Loss: 0.0155 | Grad Norm: 0.00830382\n",
      "Epoch 3 | Step 1747400 | Avg Loss: 0.0155 | Grad Norm: 0.01004233\n",
      "Epoch 3 | Step 1747500 | Avg Loss: 0.0154 | Grad Norm: 0.00966084\n",
      "Epoch 3 | Step 1747600 | Avg Loss: 0.0153 | Grad Norm: 0.00861945\n",
      "Epoch 3 | Step 1747700 | Avg Loss: 0.0153 | Grad Norm: 0.00910126\n",
      "Epoch 3 | Step 1747800 | Avg Loss: 0.0151 | Grad Norm: 0.00978386\n",
      "Epoch 3 | Step 1747900 | Avg Loss: 0.0151 | Grad Norm: 0.00838173\n",
      "Epoch 3 | Step 1748000 | Avg Loss: 0.0155 | Grad Norm: 0.00958255\n",
      "Epoch 3 | Step 1748100 | Avg Loss: 0.0154 | Grad Norm: 0.00836449\n",
      "Epoch 3 | Step 1748200 | Avg Loss: 0.0152 | Grad Norm: 0.00912285\n",
      "Epoch 3 | Step 1748300 | Avg Loss: 0.0152 | Grad Norm: 0.00888357\n",
      "Epoch 3 | Step 1748400 | Avg Loss: 0.0152 | Grad Norm: 0.00795287\n",
      "Epoch 3 | Step 1748500 | Avg Loss: 0.0151 | Grad Norm: 0.00897662\n",
      "Epoch 3 | Step 1748600 | Avg Loss: 0.0152 | Grad Norm: 0.00806760\n",
      "Epoch 3 | Step 1748700 | Avg Loss: 0.0152 | Grad Norm: 0.00769328\n",
      "Epoch 3 | Step 1748800 | Avg Loss: 0.0152 | Grad Norm: 0.01244967\n",
      "Epoch 3 | Step 1748900 | Avg Loss: 0.0148 | Grad Norm: 0.00825871\n",
      "Epoch 3 | Step 1749000 | Avg Loss: 0.0149 | Grad Norm: 0.01139384\n",
      "Epoch 3 | Step 1749100 | Avg Loss: 0.0151 | Grad Norm: 0.00901575\n",
      "Epoch 3 | Step 1749200 | Avg Loss: 0.0154 | Grad Norm: 0.00962311\n",
      "Epoch 3 | Step 1749300 | Avg Loss: 0.0154 | Grad Norm: 0.00942955\n",
      "Epoch 3 | Step 1749400 | Avg Loss: 0.0159 | Grad Norm: 0.00893024\n",
      "Epoch 3 | Step 1749500 | Avg Loss: 0.0157 | Grad Norm: 0.00954005\n",
      "Epoch 3 | Step 1749600 | Avg Loss: 0.0154 | Grad Norm: 0.00839741\n",
      "Epoch 3 | Step 1749700 | Avg Loss: 0.0159 | Grad Norm: 0.01263589\n",
      "Epoch 3 | Step 1749800 | Avg Loss: 0.0160 | Grad Norm: 0.00945929\n",
      "Epoch 3 | Step 1749900 | Avg Loss: 0.0161 | Grad Norm: 0.01061514\n",
      "Epoch 3 | Step 1750000 | Avg Loss: 0.0161 | Grad Norm: 0.00943409\n",
      "Epoch 3 | Step 1750100 | Avg Loss: 0.0160 | Grad Norm: 0.00982656\n",
      "Epoch 3 | Step 1750200 | Avg Loss: 0.0160 | Grad Norm: 0.01007551\n",
      "Epoch 3 | Step 1750300 | Avg Loss: 0.0163 | Grad Norm: 0.00795509\n",
      "Epoch 3 | Step 1750400 | Avg Loss: 0.0164 | Grad Norm: 0.00988608\n",
      "Epoch 3 | Step 1750500 | Avg Loss: 0.0165 | Grad Norm: 0.00829224\n",
      "Epoch 3 | Step 1750600 | Avg Loss: 0.0162 | Grad Norm: 0.00914455\n",
      "Epoch 3 | Step 1750700 | Avg Loss: 0.0161 | Grad Norm: 0.00861309\n",
      "Epoch 3 | Step 1750800 | Avg Loss: 0.0159 | Grad Norm: 0.00932343\n",
      "Epoch 3 | Step 1750900 | Avg Loss: 0.0157 | Grad Norm: 0.00934929\n",
      "Epoch 3 | Step 1751000 | Avg Loss: 0.0160 | Grad Norm: 0.00839366\n",
      "Epoch 3 | Step 1751100 | Avg Loss: 0.0160 | Grad Norm: 0.01043443\n",
      "Epoch 3 | Step 1751200 | Avg Loss: 0.0160 | Grad Norm: 0.00950834\n",
      "Epoch 3 | Step 1751300 | Avg Loss: 0.0155 | Grad Norm: 0.00877667\n",
      "Epoch 3 | Step 1751400 | Avg Loss: 0.0155 | Grad Norm: 0.01015322\n",
      "Epoch 3 | Step 1751500 | Avg Loss: 0.0156 | Grad Norm: 0.00883936\n",
      "Epoch 3 | Step 1751600 | Avg Loss: 0.0159 | Grad Norm: 0.00880266\n",
      "Epoch 3 | Step 1751700 | Avg Loss: 0.0162 | Grad Norm: 0.00910522\n",
      "Epoch 3 | Step 1751800 | Avg Loss: 0.0160 | Grad Norm: 0.00909245\n",
      "Epoch 3 | Step 1751900 | Avg Loss: 0.0157 | Grad Norm: 0.00805439\n",
      "Epoch 3 | Step 1752000 | Avg Loss: 0.0155 | Grad Norm: 0.01088134\n",
      "Epoch 3 | Step 1752100 | Avg Loss: 0.0154 | Grad Norm: 0.00766552\n",
      "Epoch 3 | Step 1752200 | Avg Loss: 0.0154 | Grad Norm: 0.00936059\n",
      "Epoch 3 | Step 1752300 | Avg Loss: 0.0152 | Grad Norm: 0.00859548\n",
      "Epoch 3 | Step 1752400 | Avg Loss: 0.0150 | Grad Norm: 0.00922332\n",
      "Epoch 3 | Step 1752500 | Avg Loss: 0.0152 | Grad Norm: 0.00950232\n",
      "Epoch 3 | Step 1752600 | Avg Loss: 0.0153 | Grad Norm: 0.00914888\n",
      "Epoch 3 | Step 1752700 | Avg Loss: 0.0152 | Grad Norm: 0.00835308\n",
      "Epoch 3 | Step 1752800 | Avg Loss: 0.0157 | Grad Norm: 0.00937020\n",
      "Epoch 3 | Step 1752900 | Avg Loss: 0.0159 | Grad Norm: 0.01025337\n",
      "Epoch 3 | Step 1753000 | Avg Loss: 0.0161 | Grad Norm: 0.00906221\n",
      "Epoch 3 | Step 1753100 | Avg Loss: 0.0158 | Grad Norm: 0.00860392\n",
      "Epoch 3 | Step 1753200 | Avg Loss: 0.0157 | Grad Norm: 0.00778307\n",
      "Epoch 3 | Step 1753300 | Avg Loss: 0.0153 | Grad Norm: 0.01138693\n",
      "Epoch 3 | Step 1753400 | Avg Loss: 0.0155 | Grad Norm: 0.00817521\n",
      "Epoch 3 | Step 1753500 | Avg Loss: 0.0154 | Grad Norm: 0.00870267\n",
      "Epoch 3 | Step 1753600 | Avg Loss: 0.0156 | Grad Norm: 0.01169239\n",
      "Epoch 3 | Step 1753700 | Avg Loss: 0.0153 | Grad Norm: 0.00860375\n",
      "Epoch 3 | Step 1753800 | Avg Loss: 0.0151 | Grad Norm: 0.00790094\n",
      "Epoch 3 | Step 1753900 | Avg Loss: 0.0152 | Grad Norm: 0.00875743\n",
      "Epoch 3 | Step 1754000 | Avg Loss: 0.0155 | Grad Norm: 0.00910263\n",
      "Epoch 3 | Step 1754100 | Avg Loss: 0.0153 | Grad Norm: 0.00995249\n",
      "Epoch 3 | Step 1754200 | Avg Loss: 0.0155 | Grad Norm: 0.00892035\n",
      "Epoch 3 | Step 1754300 | Avg Loss: 0.0154 | Grad Norm: 0.00814766\n",
      "Epoch 3 | Step 1754400 | Avg Loss: 0.0156 | Grad Norm: 0.00862896\n",
      "Epoch 3 | Step 1754500 | Avg Loss: 0.0156 | Grad Norm: 0.00904397\n",
      "Epoch 3 | Step 1754600 | Avg Loss: 0.0157 | Grad Norm: 0.00952339\n",
      "Epoch 3 | Step 1754700 | Avg Loss: 0.0155 | Grad Norm: 0.01014629\n",
      "Epoch 3 | Step 1754800 | Avg Loss: 0.0155 | Grad Norm: 0.01162326\n",
      "Epoch 3 | Step 1754900 | Avg Loss: 0.0157 | Grad Norm: 0.00882331\n",
      "Epoch 3 | Step 1755000 | Avg Loss: 0.0160 | Grad Norm: 0.00925741\n",
      "Epoch 3 | Step 1755100 | Avg Loss: 0.0158 | Grad Norm: 0.00793969\n",
      "Epoch 3 | Step 1755200 | Avg Loss: 0.0154 | Grad Norm: 0.00943380\n",
      "Epoch 3 | Step 1755300 | Avg Loss: 0.0155 | Grad Norm: 0.00835931\n",
      "Epoch 3 | Step 1755400 | Avg Loss: 0.0154 | Grad Norm: 0.00977412\n",
      "Epoch 3 | Step 1755500 | Avg Loss: 0.0159 | Grad Norm: 0.00964490\n",
      "Epoch 3 | Step 1755600 | Avg Loss: 0.0158 | Grad Norm: 0.00914784\n",
      "Epoch 3 | Step 1755700 | Avg Loss: 0.0159 | Grad Norm: 0.00795717\n",
      "Epoch 3 | Step 1755800 | Avg Loss: 0.0161 | Grad Norm: 0.00864005\n",
      "Epoch 3 | Step 1755900 | Avg Loss: 0.0157 | Grad Norm: 0.00842071\n",
      "Epoch 3 | Step 1756000 | Avg Loss: 0.0159 | Grad Norm: 0.00969120\n",
      "Epoch 3 | Step 1756100 | Avg Loss: 0.0160 | Grad Norm: 0.00948563\n",
      "Epoch 3 | Step 1756200 | Avg Loss: 0.0159 | Grad Norm: 0.00873792\n",
      "Epoch 3 | Step 1756300 | Avg Loss: 0.0157 | Grad Norm: 0.00855521\n",
      "Epoch 3 | Step 1756400 | Avg Loss: 0.0156 | Grad Norm: 0.00804965\n",
      "Epoch 3 | Step 1756500 | Avg Loss: 0.0161 | Grad Norm: 0.00998161\n",
      "Epoch 3 | Step 1756600 | Avg Loss: 0.0159 | Grad Norm: 0.00935964\n",
      "Epoch 3 | Step 1756700 | Avg Loss: 0.0159 | Grad Norm: 0.00823439\n",
      "Epoch 3 | Step 1756800 | Avg Loss: 0.0156 | Grad Norm: 0.01068352\n",
      "Epoch 3 | Step 1756900 | Avg Loss: 0.0153 | Grad Norm: 0.00948091\n",
      "Epoch 3 | Step 1757000 | Avg Loss: 0.0152 | Grad Norm: 0.00977637\n",
      "Epoch 3 | Step 1757100 | Avg Loss: 0.0155 | Grad Norm: 0.00885772\n",
      "Epoch 3 | Step 1757200 | Avg Loss: 0.0156 | Grad Norm: 0.00966474\n",
      "Epoch 3 | Step 1757300 | Avg Loss: 0.0153 | Grad Norm: 0.00890443\n",
      "Epoch 3 | Step 1757400 | Avg Loss: 0.0156 | Grad Norm: 0.00905469\n",
      "Epoch 3 | Step 1757500 | Avg Loss: 0.0153 | Grad Norm: 0.00989344\n",
      "Epoch 3 | Step 1757600 | Avg Loss: 0.0158 | Grad Norm: 0.00914762\n",
      "Epoch 3 | Step 1757700 | Avg Loss: 0.0157 | Grad Norm: 0.00839792\n",
      "Epoch 3 | Step 1757800 | Avg Loss: 0.0158 | Grad Norm: 0.01014288\n",
      "Epoch 3 | Step 1757900 | Avg Loss: 0.0162 | Grad Norm: 0.00886881\n",
      "Epoch 3 | Step 1758000 | Avg Loss: 0.0161 | Grad Norm: 0.01054915\n",
      "Epoch 3 | Step 1758100 | Avg Loss: 0.0160 | Grad Norm: 0.00902911\n",
      "Epoch 3 | Step 1758200 | Avg Loss: 0.0158 | Grad Norm: 0.00861822\n",
      "Epoch 3 | Step 1758300 | Avg Loss: 0.0158 | Grad Norm: 0.01070449\n",
      "Epoch 3 | Step 1758400 | Avg Loss: 0.0158 | Grad Norm: 0.01065592\n",
      "Epoch 3 | Step 1758500 | Avg Loss: 0.0155 | Grad Norm: 0.00927084\n",
      "Epoch 3 | Step 1758600 | Avg Loss: 0.0157 | Grad Norm: 0.00963935\n",
      "Epoch 3 | Step 1758700 | Avg Loss: 0.0159 | Grad Norm: 0.00899099\n",
      "Epoch 3 | Step 1758800 | Avg Loss: 0.0156 | Grad Norm: 0.01076252\n",
      "Epoch 3 | Step 1758900 | Avg Loss: 0.0154 | Grad Norm: 0.00784511\n",
      "Epoch 3 | Step 1759000 | Avg Loss: 0.0154 | Grad Norm: 0.00916459\n",
      "Epoch 3 | Step 1759100 | Avg Loss: 0.0152 | Grad Norm: 0.00764549\n",
      "Epoch 3 | Step 1759200 | Avg Loss: 0.0153 | Grad Norm: 0.00901850\n",
      "Epoch 3 | Step 1759300 | Avg Loss: 0.0153 | Grad Norm: 0.00970632\n",
      "Epoch 3 | Step 1759400 | Avg Loss: 0.0155 | Grad Norm: 0.00859537\n",
      "Epoch 3 | Step 1759500 | Avg Loss: 0.0155 | Grad Norm: 0.00958789\n",
      "Epoch 3 | Step 1759600 | Avg Loss: 0.0157 | Grad Norm: 0.00856479\n",
      "Epoch 3 | Step 1759700 | Avg Loss: 0.0154 | Grad Norm: 0.00854740\n",
      "Epoch 3 | Step 1759800 | Avg Loss: 0.0156 | Grad Norm: 0.00803284\n",
      "Epoch 3 | Step 1759900 | Avg Loss: 0.0153 | Grad Norm: 0.00838459\n",
      "Epoch 3 | Step 1760000 | Avg Loss: 0.0153 | Grad Norm: 0.00939307\n",
      "Epoch 3 | Step 1760100 | Avg Loss: 0.0154 | Grad Norm: 0.00808950\n",
      "Epoch 3 | Step 1760200 | Avg Loss: 0.0155 | Grad Norm: 0.00973268\n",
      "Epoch 3 | Step 1760300 | Avg Loss: 0.0157 | Grad Norm: 0.00905529\n",
      "Epoch 3 | Step 1760400 | Avg Loss: 0.0161 | Grad Norm: 0.01083557\n",
      "Epoch 3 | Step 1760500 | Avg Loss: 0.0162 | Grad Norm: 0.00954620\n",
      "Epoch 3 | Step 1760600 | Avg Loss: 0.0160 | Grad Norm: 0.00975058\n",
      "Epoch 3 | Step 1760700 | Avg Loss: 0.0155 | Grad Norm: 0.00898886\n",
      "Epoch 3 | Step 1760800 | Avg Loss: 0.0153 | Grad Norm: 0.00960304\n",
      "Epoch 3 | Step 1760900 | Avg Loss: 0.0155 | Grad Norm: 0.00880738\n",
      "Epoch 3 | Step 1761000 | Avg Loss: 0.0155 | Grad Norm: 0.01007558\n",
      "Epoch 3 | Step 1761100 | Avg Loss: 0.0157 | Grad Norm: 0.00910398\n",
      "Epoch 3 | Step 1761200 | Avg Loss: 0.0155 | Grad Norm: 0.00871760\n",
      "Epoch 3 | Step 1761300 | Avg Loss: 0.0154 | Grad Norm: 0.00875413\n",
      "Epoch 3 | Step 1761400 | Avg Loss: 0.0157 | Grad Norm: 0.01050256\n",
      "Epoch 3 | Step 1761500 | Avg Loss: 0.0157 | Grad Norm: 0.00800921\n",
      "Epoch 3 | Step 1761600 | Avg Loss: 0.0156 | Grad Norm: 0.00900299\n",
      "Epoch 3 | Step 1761700 | Avg Loss: 0.0158 | Grad Norm: 0.01002598\n",
      "Epoch 3 | Step 1761800 | Avg Loss: 0.0157 | Grad Norm: 0.00922408\n",
      "Epoch 3 | Step 1761900 | Avg Loss: 0.0162 | Grad Norm: 0.00873941\n",
      "Epoch 3 | Step 1762000 | Avg Loss: 0.0162 | Grad Norm: 0.00937060\n",
      "Epoch 3 | Step 1762100 | Avg Loss: 0.0163 | Grad Norm: 0.01042512\n",
      "Epoch 3 | Step 1762200 | Avg Loss: 0.0161 | Grad Norm: 0.00870363\n",
      "Epoch 3 | Step 1762300 | Avg Loss: 0.0158 | Grad Norm: 0.00955221\n",
      "Epoch 3 | Step 1762400 | Avg Loss: 0.0154 | Grad Norm: 0.00824856\n",
      "Epoch 3 | Step 1762500 | Avg Loss: 0.0156 | Grad Norm: 0.00901953\n",
      "Epoch 3 | Step 1762600 | Avg Loss: 0.0153 | Grad Norm: 0.00911279\n",
      "Epoch 3 | Step 1762700 | Avg Loss: 0.0154 | Grad Norm: 0.00877761\n",
      "Epoch 3 | Step 1762800 | Avg Loss: 0.0152 | Grad Norm: 0.00779347\n",
      "Epoch 3 | Step 1762900 | Avg Loss: 0.0154 | Grad Norm: 0.00848078\n",
      "Epoch 3 | Step 1763000 | Avg Loss: 0.0156 | Grad Norm: 0.01397552\n",
      "Epoch 3 | Step 1763100 | Avg Loss: 0.0156 | Grad Norm: 0.00916441\n",
      "Epoch 3 | Step 1763200 | Avg Loss: 0.0156 | Grad Norm: 0.01044310\n",
      "Epoch 3 | Step 1763300 | Avg Loss: 0.0158 | Grad Norm: 0.00919282\n",
      "Epoch 3 | Step 1763400 | Avg Loss: 0.0158 | Grad Norm: 0.00846797\n",
      "Epoch 3 | Step 1763500 | Avg Loss: 0.0158 | Grad Norm: 0.00943598\n",
      "Epoch 3 | Step 1763600 | Avg Loss: 0.0156 | Grad Norm: 0.00715149\n",
      "Epoch 3 | Step 1763700 | Avg Loss: 0.0159 | Grad Norm: 0.00919251\n",
      "Epoch 3 | Step 1763800 | Avg Loss: 0.0158 | Grad Norm: 0.01145324\n",
      "Epoch 3 | Step 1763900 | Avg Loss: 0.0155 | Grad Norm: 0.00870491\n",
      "Epoch 3 | Step 1764000 | Avg Loss: 0.0156 | Grad Norm: 0.00738128\n",
      "Epoch 3 | Step 1764100 | Avg Loss: 0.0157 | Grad Norm: 0.00963569\n",
      "Epoch 3 | Step 1764200 | Avg Loss: 0.0155 | Grad Norm: 0.00969954\n",
      "Epoch 3 | Step 1764300 | Avg Loss: 0.0152 | Grad Norm: 0.00910260\n",
      "Epoch 3 | Step 1764400 | Avg Loss: 0.0150 | Grad Norm: 0.00803899\n",
      "Epoch 3 | Step 1764500 | Avg Loss: 0.0155 | Grad Norm: 0.00974830\n",
      "Epoch 3 | Step 1764600 | Avg Loss: 0.0156 | Grad Norm: 0.00929157\n",
      "Epoch 3 | Step 1764700 | Avg Loss: 0.0155 | Grad Norm: 0.00876430\n",
      "Epoch 3 | Step 1764800 | Avg Loss: 0.0152 | Grad Norm: 0.00830273\n",
      "Epoch 3 | Step 1764900 | Avg Loss: 0.0154 | Grad Norm: 0.00880066\n",
      "Epoch 3 | Step 1765000 | Avg Loss: 0.0158 | Grad Norm: 0.00942269\n",
      "Epoch 3 | Step 1765100 | Avg Loss: 0.0157 | Grad Norm: 0.00811765\n",
      "Epoch 3 | Step 1765200 | Avg Loss: 0.0159 | Grad Norm: 0.00988803\n",
      "Epoch 3 | Step 1765300 | Avg Loss: 0.0163 | Grad Norm: 0.00823399\n",
      "Epoch 3 | Step 1765400 | Avg Loss: 0.0164 | Grad Norm: 0.00926450\n",
      "Epoch 3 | Step 1765500 | Avg Loss: 0.0164 | Grad Norm: 0.00997003\n",
      "Epoch 3 | Step 1765600 | Avg Loss: 0.0162 | Grad Norm: 0.01065496\n",
      "Epoch 3 | Step 1765700 | Avg Loss: 0.0163 | Grad Norm: 0.00964381\n",
      "Epoch 3 | Step 1765800 | Avg Loss: 0.0156 | Grad Norm: 0.00811131\n",
      "Epoch 3 | Step 1765900 | Avg Loss: 0.0158 | Grad Norm: 0.00844330\n",
      "Epoch 3 | Step 1766000 | Avg Loss: 0.0158 | Grad Norm: 0.00837539\n",
      "Epoch 3 | Step 1766100 | Avg Loss: 0.0156 | Grad Norm: 0.00838005\n",
      "Epoch 3 | Step 1766200 | Avg Loss: 0.0161 | Grad Norm: 0.00984426\n",
      "Epoch 3 | Step 1766300 | Avg Loss: 0.0163 | Grad Norm: 0.00878407\n",
      "Epoch 3 | Step 1766400 | Avg Loss: 0.0159 | Grad Norm: 0.01164630\n",
      "Epoch 3 | Step 1766500 | Avg Loss: 0.0157 | Grad Norm: 0.00792909\n",
      "Epoch 3 | Step 1766600 | Avg Loss: 0.0159 | Grad Norm: 0.01017085\n",
      "Epoch 3 | Step 1766700 | Avg Loss: 0.0159 | Grad Norm: 0.00931106\n",
      "Epoch 3 | Step 1766800 | Avg Loss: 0.0158 | Grad Norm: 0.00945081\n",
      "Epoch 3 | Step 1766900 | Avg Loss: 0.0158 | Grad Norm: 0.01002588\n",
      "Epoch 3 | Step 1767000 | Avg Loss: 0.0156 | Grad Norm: 0.00866718\n",
      "Epoch 3 | Step 1767100 | Avg Loss: 0.0156 | Grad Norm: 0.00973854\n",
      "Epoch 3 | Step 1767200 | Avg Loss: 0.0157 | Grad Norm: 0.00856686\n",
      "Epoch 3 | Step 1767300 | Avg Loss: 0.0155 | Grad Norm: 0.00796904\n",
      "Epoch 3 | Step 1767400 | Avg Loss: 0.0153 | Grad Norm: 0.00727410\n",
      "Epoch 3 | Step 1767500 | Avg Loss: 0.0154 | Grad Norm: 0.01064019\n",
      "Epoch 3 | Step 1767600 | Avg Loss: 0.0153 | Grad Norm: 0.00950393\n",
      "Epoch 3 | Step 1767700 | Avg Loss: 0.0152 | Grad Norm: 0.01080512\n",
      "Epoch 3 | Step 1767800 | Avg Loss: 0.0155 | Grad Norm: 0.00846193\n",
      "Epoch 3 | Step 1767900 | Avg Loss: 0.0156 | Grad Norm: 0.01200501\n",
      "Epoch 3 | Step 1768000 | Avg Loss: 0.0156 | Grad Norm: 0.00956528\n",
      "Epoch 3 | Step 1768100 | Avg Loss: 0.0152 | Grad Norm: 0.00810708\n",
      "Epoch 3 | Step 1768200 | Avg Loss: 0.0154 | Grad Norm: 0.00905341\n",
      "Epoch 3 | Step 1768300 | Avg Loss: 0.0158 | Grad Norm: 0.00964014\n",
      "Epoch 3 | Step 1768400 | Avg Loss: 0.0159 | Grad Norm: 0.00963435\n",
      "Epoch 3 | Step 1768500 | Avg Loss: 0.0159 | Grad Norm: 0.00844395\n",
      "Epoch 3 | Step 1768600 | Avg Loss: 0.0160 | Grad Norm: 0.00957586\n",
      "Epoch 3 | Step 1768700 | Avg Loss: 0.0159 | Grad Norm: 0.00795651\n",
      "Epoch 3 | Step 1768800 | Avg Loss: 0.0157 | Grad Norm: 0.00977125\n",
      "Epoch 3 | Step 1768900 | Avg Loss: 0.0156 | Grad Norm: 0.00959297\n",
      "Epoch 3 | Step 1769000 | Avg Loss: 0.0154 | Grad Norm: 0.01027877\n",
      "Epoch 3 | Step 1769100 | Avg Loss: 0.0157 | Grad Norm: 0.00991089\n",
      "Epoch 3 | Step 1769200 | Avg Loss: 0.0161 | Grad Norm: 0.00962677\n",
      "Epoch 3 | Step 1769300 | Avg Loss: 0.0159 | Grad Norm: 0.00899115\n",
      "Epoch 3 | Step 1769400 | Avg Loss: 0.0159 | Grad Norm: 0.00806086\n",
      "Epoch 3 | Step 1769500 | Avg Loss: 0.0158 | Grad Norm: 0.00842337\n",
      "Epoch 3 | Step 1769600 | Avg Loss: 0.0159 | Grad Norm: 0.01031605\n",
      "Epoch 3 | Step 1769700 | Avg Loss: 0.0158 | Grad Norm: 0.00795252\n",
      "Epoch 3 | Step 1769800 | Avg Loss: 0.0157 | Grad Norm: 0.00748022\n",
      "Epoch 3 | Step 1769900 | Avg Loss: 0.0157 | Grad Norm: 0.00981322\n",
      "Epoch 3 | Step 1770000 | Avg Loss: 0.0157 | Grad Norm: 0.00836362\n",
      "Epoch 3 | Step 1770100 | Avg Loss: 0.0156 | Grad Norm: 0.00894648\n",
      "Epoch 3 | Step 1770200 | Avg Loss: 0.0155 | Grad Norm: 0.00924932\n",
      "Epoch 3 | Step 1770300 | Avg Loss: 0.0156 | Grad Norm: 0.00944546\n",
      "Epoch 3 | Step 1770400 | Avg Loss: 0.0156 | Grad Norm: 0.01027121\n",
      "Epoch 3 | Step 1770500 | Avg Loss: 0.0157 | Grad Norm: 0.00916702\n",
      "Epoch 3 | Step 1770600 | Avg Loss: 0.0160 | Grad Norm: 0.01260880\n",
      "Epoch 3 | Step 1770700 | Avg Loss: 0.0160 | Grad Norm: 0.01016190\n",
      "Epoch 3 | Step 1770800 | Avg Loss: 0.0160 | Grad Norm: 0.01024086\n",
      "Epoch 3 | Step 1770900 | Avg Loss: 0.0160 | Grad Norm: 0.01162913\n",
      "Epoch 3 | Step 1771000 | Avg Loss: 0.0160 | Grad Norm: 0.00963530\n",
      "Epoch 3 | Step 1771100 | Avg Loss: 0.0162 | Grad Norm: 0.01007236\n",
      "Epoch 3 | Step 1771200 | Avg Loss: 0.0160 | Grad Norm: 0.00915237\n",
      "Epoch 3 | Step 1771300 | Avg Loss: 0.0157 | Grad Norm: 0.00983164\n",
      "Epoch 3 | Step 1771400 | Avg Loss: 0.0161 | Grad Norm: 0.00882269\n",
      "Epoch 3 | Step 1771500 | Avg Loss: 0.0158 | Grad Norm: 0.00826554\n",
      "Epoch 3 | Step 1771600 | Avg Loss: 0.0161 | Grad Norm: 0.01018879\n",
      "Epoch 3 | Step 1771700 | Avg Loss: 0.0160 | Grad Norm: 0.01283823\n",
      "Epoch 3 | Step 1771800 | Avg Loss: 0.0163 | Grad Norm: 0.00847501\n",
      "Epoch 3 | Step 1771900 | Avg Loss: 0.0162 | Grad Norm: 0.01050914\n",
      "Epoch 3 | Step 1772000 | Avg Loss: 0.0160 | Grad Norm: 0.00977323\n",
      "Epoch 3 | Step 1772100 | Avg Loss: 0.0157 | Grad Norm: 0.00887056\n",
      "Epoch 3 | Step 1772200 | Avg Loss: 0.0155 | Grad Norm: 0.00910081\n",
      "Epoch 3 | Step 1772300 | Avg Loss: 0.0155 | Grad Norm: 0.00877974\n",
      "Epoch 3 | Step 1772400 | Avg Loss: 0.0158 | Grad Norm: 0.01007135\n",
      "Epoch 3 | Step 1772500 | Avg Loss: 0.0159 | Grad Norm: 0.00863353\n",
      "Epoch 3 | Step 1772600 | Avg Loss: 0.0156 | Grad Norm: 0.00874051\n",
      "Epoch 3 | Step 1772700 | Avg Loss: 0.0162 | Grad Norm: 0.00986053\n",
      "Epoch 3 | Step 1772800 | Avg Loss: 0.0159 | Grad Norm: 0.00979367\n",
      "Epoch 3 | Step 1772900 | Avg Loss: 0.0157 | Grad Norm: 0.00917810\n",
      "Epoch 3 | Step 1773000 | Avg Loss: 0.0155 | Grad Norm: 0.00873286\n",
      "Epoch 3 | Step 1773100 | Avg Loss: 0.0156 | Grad Norm: 0.00896872\n",
      "Epoch 3 | Step 1773200 | Avg Loss: 0.0158 | Grad Norm: 0.00990265\n",
      "Epoch 3 | Step 1773300 | Avg Loss: 0.0157 | Grad Norm: 0.00784769\n",
      "Epoch 3 | Step 1773400 | Avg Loss: 0.0157 | Grad Norm: 0.00911938\n",
      "Epoch 3 | Step 1773500 | Avg Loss: 0.0151 | Grad Norm: 0.00845394\n",
      "Epoch 3 | Step 1773600 | Avg Loss: 0.0149 | Grad Norm: 0.00827303\n",
      "Epoch 3 | Step 1773700 | Avg Loss: 0.0151 | Grad Norm: 0.00938011\n",
      "Epoch 3 | Step 1773800 | Avg Loss: 0.0150 | Grad Norm: 0.00890548\n",
      "Epoch 3 | Step 1773900 | Avg Loss: 0.0154 | Grad Norm: 0.00888360\n",
      "Epoch 3 | Step 1774000 | Avg Loss: 0.0152 | Grad Norm: 0.00821254\n",
      "Epoch 3 | Step 1774100 | Avg Loss: 0.0150 | Grad Norm: 0.00788465\n",
      "Epoch 3 | Step 1774200 | Avg Loss: 0.0154 | Grad Norm: 0.00795360\n",
      "Epoch 3 | Step 1774300 | Avg Loss: 0.0152 | Grad Norm: 0.00885483\n",
      "Epoch 3 | Step 1774400 | Avg Loss: 0.0155 | Grad Norm: 0.00894737\n",
      "Epoch 3 | Step 1774500 | Avg Loss: 0.0159 | Grad Norm: 0.00975174\n",
      "Epoch 3 | Step 1774600 | Avg Loss: 0.0161 | Grad Norm: 0.01017942\n",
      "Epoch 3 | Step 1774700 | Avg Loss: 0.0162 | Grad Norm: 0.00844676\n",
      "Epoch 3 | Step 1774800 | Avg Loss: 0.0160 | Grad Norm: 0.01037670\n",
      "Epoch 3 | Step 1774900 | Avg Loss: 0.0161 | Grad Norm: 0.00932961\n",
      "Epoch 3 | Step 1775000 | Avg Loss: 0.0161 | Grad Norm: 0.00911299\n",
      "Epoch 3 | Step 1775100 | Avg Loss: 0.0160 | Grad Norm: 0.00871246\n",
      "Epoch 3 | Step 1775200 | Avg Loss: 0.0158 | Grad Norm: 0.00897503\n",
      "Epoch 3 | Step 1775300 | Avg Loss: 0.0156 | Grad Norm: 0.00774987\n",
      "Epoch 3 | Step 1775400 | Avg Loss: 0.0155 | Grad Norm: 0.00988915\n",
      "Epoch 3 | Step 1775500 | Avg Loss: 0.0156 | Grad Norm: 0.00988602\n",
      "Epoch 3 | Step 1775600 | Avg Loss: 0.0152 | Grad Norm: 0.00847534\n",
      "Epoch 3 | Step 1775700 | Avg Loss: 0.0154 | Grad Norm: 0.00924277\n",
      "Epoch 3 | Step 1775800 | Avg Loss: 0.0152 | Grad Norm: 0.00844326\n",
      "Epoch 3 | Step 1775900 | Avg Loss: 0.0149 | Grad Norm: 0.00894219\n",
      "Epoch 3 | Step 1776000 | Avg Loss: 0.0154 | Grad Norm: 0.00853412\n",
      "Epoch 3 | Step 1776100 | Avg Loss: 0.0155 | Grad Norm: 0.00990766\n",
      "Epoch 3 | Step 1776200 | Avg Loss: 0.0155 | Grad Norm: 0.00897702\n",
      "Epoch 3 | Step 1776300 | Avg Loss: 0.0158 | Grad Norm: 0.00866992\n",
      "Epoch 3 | Step 1776400 | Avg Loss: 0.0155 | Grad Norm: 0.01124880\n",
      "Epoch 3 | Step 1776500 | Avg Loss: 0.0155 | Grad Norm: 0.00884202\n",
      "Epoch 3 | Step 1776600 | Avg Loss: 0.0158 | Grad Norm: 0.00865716\n",
      "Epoch 3 | Step 1776700 | Avg Loss: 0.0159 | Grad Norm: 0.00705237\n",
      "Epoch 3 | Step 1776800 | Avg Loss: 0.0159 | Grad Norm: 0.01081828\n",
      "Epoch 3 | Step 1776900 | Avg Loss: 0.0158 | Grad Norm: 0.00949771\n",
      "Epoch 3 | Step 1777000 | Avg Loss: 0.0156 | Grad Norm: 0.00755318\n",
      "Epoch 3 | Step 1777100 | Avg Loss: 0.0151 | Grad Norm: 0.00846896\n",
      "Epoch 3 | Step 1777200 | Avg Loss: 0.0151 | Grad Norm: 0.00856296\n",
      "Epoch 3 | Step 1777300 | Avg Loss: 0.0149 | Grad Norm: 0.00874506\n",
      "Epoch 3 | Step 1777400 | Avg Loss: 0.0154 | Grad Norm: 0.00859340\n",
      "Epoch 3 | Step 1777500 | Avg Loss: 0.0155 | Grad Norm: 0.01012155\n",
      "Epoch 3 | Step 1777600 | Avg Loss: 0.0156 | Grad Norm: 0.00844476\n",
      "Epoch 3 | Step 1777700 | Avg Loss: 0.0154 | Grad Norm: 0.00928821\n",
      "Epoch 3 | Step 1777800 | Avg Loss: 0.0157 | Grad Norm: 0.00983634\n",
      "Epoch 3 | Step 1777900 | Avg Loss: 0.0151 | Grad Norm: 0.00967348\n",
      "Epoch 3 | Step 1778000 | Avg Loss: 0.0155 | Grad Norm: 0.00938515\n",
      "Epoch 3 | Step 1778100 | Avg Loss: 0.0154 | Grad Norm: 0.00901899\n",
      "Epoch 3 | Step 1778200 | Avg Loss: 0.0156 | Grad Norm: 0.00819385\n",
      "Epoch 3 | Step 1778300 | Avg Loss: 0.0151 | Grad Norm: 0.00826895\n",
      "Epoch 3 | Step 1778400 | Avg Loss: 0.0151 | Grad Norm: 0.00796270\n",
      "Epoch 3 | Step 1778500 | Avg Loss: 0.0151 | Grad Norm: 0.00798415\n",
      "Epoch 3 | Step 1778600 | Avg Loss: 0.0152 | Grad Norm: 0.00873038\n",
      "Epoch 3 | Step 1778700 | Avg Loss: 0.0154 | Grad Norm: 0.00950183\n",
      "Epoch 3 | Step 1778800 | Avg Loss: 0.0156 | Grad Norm: 0.00891982\n",
      "Epoch 3 | Step 1778900 | Avg Loss: 0.0156 | Grad Norm: 0.00846019\n",
      "Epoch 3 | Step 1779000 | Avg Loss: 0.0153 | Grad Norm: 0.00842152\n",
      "Epoch 3 | Step 1779100 | Avg Loss: 0.0154 | Grad Norm: 0.00932480\n",
      "Epoch 3 | Step 1779200 | Avg Loss: 0.0153 | Grad Norm: 0.00820047\n",
      "Epoch 3 | Step 1779300 | Avg Loss: 0.0154 | Grad Norm: 0.00908839\n",
      "Epoch 3 | Step 1779400 | Avg Loss: 0.0154 | Grad Norm: 0.00940420\n",
      "Epoch 3 | Step 1779500 | Avg Loss: 0.0156 | Grad Norm: 0.00949818\n",
      "Epoch 3 | Step 1779600 | Avg Loss: 0.0156 | Grad Norm: 0.00909138\n",
      "Epoch 3 | Step 1779700 | Avg Loss: 0.0161 | Grad Norm: 0.01058497\n",
      "Epoch 3 | Step 1779800 | Avg Loss: 0.0160 | Grad Norm: 0.00902078\n",
      "Epoch 3 | Step 1779900 | Avg Loss: 0.0161 | Grad Norm: 0.00933021\n",
      "Epoch 3 | Step 1780000 | Avg Loss: 0.0159 | Grad Norm: 0.00881668\n",
      "Epoch 3 | Step 1780100 | Avg Loss: 0.0159 | Grad Norm: 0.00780530\n",
      "Epoch 3 | Step 1780200 | Avg Loss: 0.0160 | Grad Norm: 0.00732100\n",
      "Epoch 3 | Step 1780300 | Avg Loss: 0.0161 | Grad Norm: 0.00922183\n",
      "Epoch 3 | Step 1780400 | Avg Loss: 0.0164 | Grad Norm: 0.01057105\n",
      "Epoch 3 | Step 1780500 | Avg Loss: 0.0160 | Grad Norm: 0.00838472\n",
      "Epoch 3 | Step 1780600 | Avg Loss: 0.0157 | Grad Norm: 0.00850037\n",
      "Epoch 3 | Step 1780700 | Avg Loss: 0.0157 | Grad Norm: 0.00978783\n",
      "Epoch 3 | Step 1780800 | Avg Loss: 0.0159 | Grad Norm: 0.00855397\n",
      "Epoch 3 | Step 1780900 | Avg Loss: 0.0158 | Grad Norm: 0.00944598\n",
      "Epoch 3 | Step 1781000 | Avg Loss: 0.0157 | Grad Norm: 0.00971055\n",
      "Epoch 3 | Step 1781100 | Avg Loss: 0.0154 | Grad Norm: 0.00960866\n",
      "Epoch 3 | Step 1781200 | Avg Loss: 0.0151 | Grad Norm: 0.00828979\n",
      "Epoch 3 | Step 1781300 | Avg Loss: 0.0154 | Grad Norm: 0.01543648\n",
      "Epoch 3 | Step 1781400 | Avg Loss: 0.0155 | Grad Norm: 0.00961327\n",
      "Epoch 3 | Step 1781500 | Avg Loss: 0.0157 | Grad Norm: 0.00939146\n",
      "Epoch 3 | Step 1781600 | Avg Loss: 0.0161 | Grad Norm: 0.00839032\n",
      "Epoch 3 | Step 1781700 | Avg Loss: 0.0161 | Grad Norm: 0.00997039\n",
      "Epoch 3 | Step 1781800 | Avg Loss: 0.0157 | Grad Norm: 0.00872150\n",
      "Epoch 3 | Step 1781900 | Avg Loss: 0.0155 | Grad Norm: 0.01000265\n",
      "Epoch 3 | Step 1782000 | Avg Loss: 0.0158 | Grad Norm: 0.00974423\n",
      "Epoch 3 | Step 1782100 | Avg Loss: 0.0156 | Grad Norm: 0.00821463\n",
      "Epoch 3 | Step 1782200 | Avg Loss: 0.0155 | Grad Norm: 0.01014576\n",
      "Epoch 3 | Step 1782300 | Avg Loss: 0.0156 | Grad Norm: 0.00965886\n",
      "Epoch 3 | Step 1782400 | Avg Loss: 0.0156 | Grad Norm: 0.00856515\n",
      "Epoch 3 | Step 1782500 | Avg Loss: 0.0155 | Grad Norm: 0.01057548\n",
      "Epoch 3 | Step 1782600 | Avg Loss: 0.0155 | Grad Norm: 0.01412130\n",
      "Epoch 3 | Step 1782700 | Avg Loss: 0.0152 | Grad Norm: 0.00735060\n",
      "Epoch 3 | Step 1782800 | Avg Loss: 0.0152 | Grad Norm: 0.00934674\n",
      "Epoch 3 | Step 1782900 | Avg Loss: 0.0155 | Grad Norm: 0.00902649\n",
      "Epoch 3 | Step 1783000 | Avg Loss: 0.0154 | Grad Norm: 0.00925355\n",
      "Epoch 3 | Step 1783100 | Avg Loss: 0.0149 | Grad Norm: 0.00778230\n",
      "Epoch 3 | Step 1783200 | Avg Loss: 0.0155 | Grad Norm: 0.00998554\n",
      "Epoch 3 | Step 1783300 | Avg Loss: 0.0151 | Grad Norm: 0.00900014\n",
      "Epoch 3 | Step 1783400 | Avg Loss: 0.0153 | Grad Norm: 0.00911197\n",
      "Epoch 3 | Step 1783500 | Avg Loss: 0.0154 | Grad Norm: 0.00945878\n",
      "Epoch 3 | Step 1783600 | Avg Loss: 0.0153 | Grad Norm: 0.00935010\n",
      "Epoch 3 | Step 1783700 | Avg Loss: 0.0155 | Grad Norm: 0.00793224\n",
      "Epoch 3 | Step 1783800 | Avg Loss: 0.0156 | Grad Norm: 0.00913389\n",
      "Epoch 3 | Step 1783900 | Avg Loss: 0.0158 | Grad Norm: 0.00955161\n",
      "Epoch 3 | Step 1784000 | Avg Loss: 0.0157 | Grad Norm: 0.00863127\n",
      "Epoch 3 | Step 1784100 | Avg Loss: 0.0159 | Grad Norm: 0.00990369\n",
      "Epoch 3 | Step 1784200 | Avg Loss: 0.0160 | Grad Norm: 0.00872898\n",
      "Epoch 3 | Step 1784300 | Avg Loss: 0.0161 | Grad Norm: 0.01094648\n",
      "Epoch 3 | Step 1784400 | Avg Loss: 0.0156 | Grad Norm: 0.00851158\n",
      "Epoch 3 | Step 1784500 | Avg Loss: 0.0159 | Grad Norm: 0.00888969\n",
      "Epoch 3 | Step 1784600 | Avg Loss: 0.0158 | Grad Norm: 0.00835295\n",
      "Epoch 3 | Step 1784700 | Avg Loss: 0.0156 | Grad Norm: 0.00884016\n",
      "Epoch 3 | Step 1784800 | Avg Loss: 0.0154 | Grad Norm: 0.00833676\n",
      "Epoch 3 | Step 1784900 | Avg Loss: 0.0156 | Grad Norm: 0.00958668\n",
      "Epoch 3 | Step 1785000 | Avg Loss: 0.0155 | Grad Norm: 0.00762650\n",
      "Epoch 3 | Step 1785100 | Avg Loss: 0.0153 | Grad Norm: 0.01054142\n",
      "Epoch 3 | Step 1785200 | Avg Loss: 0.0155 | Grad Norm: 0.01153056\n",
      "Epoch 3 | Step 1785300 | Avg Loss: 0.0156 | Grad Norm: 0.00808922\n",
      "Epoch 3 | Step 1785400 | Avg Loss: 0.0158 | Grad Norm: 0.00887633\n",
      "Epoch 3 | Step 1785500 | Avg Loss: 0.0159 | Grad Norm: 0.00944092\n",
      "Epoch 3 | Step 1785600 | Avg Loss: 0.0159 | Grad Norm: 0.00967949\n",
      "Epoch 3 | Step 1785700 | Avg Loss: 0.0158 | Grad Norm: 0.00922382\n",
      "Epoch 3 | Step 1785800 | Avg Loss: 0.0154 | Grad Norm: 0.00895129\n",
      "Epoch 3 | Step 1785900 | Avg Loss: 0.0155 | Grad Norm: 0.01130513\n",
      "Epoch 3 | Step 1786000 | Avg Loss: 0.0155 | Grad Norm: 0.00800878\n",
      "Epoch 3 | Step 1786100 | Avg Loss: 0.0153 | Grad Norm: 0.00945640\n",
      "Epoch 3 | Step 1786200 | Avg Loss: 0.0155 | Grad Norm: 0.00777041\n",
      "Epoch 3 | Step 1786300 | Avg Loss: 0.0153 | Grad Norm: 0.00767675\n",
      "Epoch 3 | Step 1786400 | Avg Loss: 0.0151 | Grad Norm: 0.00968396\n",
      "Epoch 3 | Step 1786500 | Avg Loss: 0.0151 | Grad Norm: 0.00933004\n",
      "Epoch 3 | Step 1786600 | Avg Loss: 0.0153 | Grad Norm: 0.00818957\n",
      "Epoch 3 | Step 1786700 | Avg Loss: 0.0154 | Grad Norm: 0.00866278\n",
      "Epoch 3 | Step 1786800 | Avg Loss: 0.0153 | Grad Norm: 0.00822280\n",
      "Epoch 3 | Step 1786900 | Avg Loss: 0.0155 | Grad Norm: 0.00676648\n",
      "Epoch 3 | Step 1787000 | Avg Loss: 0.0155 | Grad Norm: 0.00820341\n",
      "Epoch 3 | Step 1787100 | Avg Loss: 0.0158 | Grad Norm: 0.00865810\n",
      "Epoch 3 | Step 1787200 | Avg Loss: 0.0160 | Grad Norm: 0.00933238\n",
      "Epoch 3 | Step 1787300 | Avg Loss: 0.0162 | Grad Norm: 0.00842434\n",
      "Epoch 3 | Step 1787400 | Avg Loss: 0.0161 | Grad Norm: 0.00960257\n",
      "Epoch 3 | Step 1787500 | Avg Loss: 0.0164 | Grad Norm: 0.01020288\n",
      "Epoch 3 | Step 1787600 | Avg Loss: 0.0160 | Grad Norm: 0.00991989\n",
      "Epoch 3 | Step 1787700 | Avg Loss: 0.0153 | Grad Norm: 0.00889193\n",
      "Epoch 3 | Step 1787800 | Avg Loss: 0.0154 | Grad Norm: 0.00977508\n",
      "Epoch 3 | Step 1787900 | Avg Loss: 0.0158 | Grad Norm: 0.00844623\n",
      "Epoch 3 | Step 1788000 | Avg Loss: 0.0157 | Grad Norm: 0.00794579\n",
      "Epoch 3 | Step 1788100 | Avg Loss: 0.0158 | Grad Norm: 0.01036095\n",
      "Epoch 3 | Step 1788200 | Avg Loss: 0.0158 | Grad Norm: 0.01100990\n",
      "Epoch 3 | Step 1788300 | Avg Loss: 0.0159 | Grad Norm: 0.00751635\n",
      "Epoch 3 | Step 1788400 | Avg Loss: 0.0161 | Grad Norm: 0.01193747\n",
      "Epoch 3 | Step 1788500 | Avg Loss: 0.0161 | Grad Norm: 0.00896684\n",
      "Epoch 3 | Step 1788600 | Avg Loss: 0.0158 | Grad Norm: 0.00930003\n",
      "Epoch 3 | Step 1788700 | Avg Loss: 0.0156 | Grad Norm: 0.00836953\n",
      "Epoch 3 | Step 1788800 | Avg Loss: 0.0155 | Grad Norm: 0.00923659\n",
      "Epoch 3 | Step 1788900 | Avg Loss: 0.0155 | Grad Norm: 0.00933849\n",
      "Epoch 3 | Step 1789000 | Avg Loss: 0.0155 | Grad Norm: 0.00767745\n",
      "Epoch 3 | Step 1789100 | Avg Loss: 0.0158 | Grad Norm: 0.00837733\n",
      "Epoch 3 | Step 1789200 | Avg Loss: 0.0155 | Grad Norm: 0.00927087\n",
      "Epoch 3 | Step 1789300 | Avg Loss: 0.0154 | Grad Norm: 0.00772875\n",
      "Epoch 3 | Step 1789400 | Avg Loss: 0.0149 | Grad Norm: 0.01007311\n",
      "Epoch 3 | Step 1789500 | Avg Loss: 0.0147 | Grad Norm: 0.01054275\n",
      "Epoch 3 | Step 1789600 | Avg Loss: 0.0151 | Grad Norm: 0.00812857\n",
      "Epoch 3 | Step 1789700 | Avg Loss: 0.0152 | Grad Norm: 0.00809299\n",
      "Epoch 3 | Step 1789800 | Avg Loss: 0.0153 | Grad Norm: 0.00837034\n",
      "Epoch 3 | Step 1789900 | Avg Loss: 0.0157 | Grad Norm: 0.00905115\n",
      "Epoch 3 | Step 1790000 | Avg Loss: 0.0161 | Grad Norm: 0.00926233\n",
      "Epoch 3 | Step 1790100 | Avg Loss: 0.0159 | Grad Norm: 0.00814581\n",
      "Epoch 3 | Step 1790200 | Avg Loss: 0.0160 | Grad Norm: 0.00953242\n",
      "Epoch 3 | Step 1790300 | Avg Loss: 0.0160 | Grad Norm: 0.00969459\n",
      "Epoch 3 | Step 1790400 | Avg Loss: 0.0157 | Grad Norm: 0.00911617\n",
      "Epoch 3 | Step 1790500 | Avg Loss: 0.0156 | Grad Norm: 0.00984706\n",
      "Epoch 3 | Step 1790600 | Avg Loss: 0.0159 | Grad Norm: 0.00939773\n",
      "Epoch 3 | Step 1790700 | Avg Loss: 0.0156 | Grad Norm: 0.00983506\n",
      "Epoch 3 | Step 1790800 | Avg Loss: 0.0159 | Grad Norm: 0.00903147\n",
      "Epoch 3 | Step 1790900 | Avg Loss: 0.0160 | Grad Norm: 0.00835657\n",
      "Epoch 3 | Step 1791000 | Avg Loss: 0.0160 | Grad Norm: 0.00862106\n",
      "Epoch 3 | Step 1791100 | Avg Loss: 0.0158 | Grad Norm: 0.00955228\n",
      "Epoch 3 | Step 1791200 | Avg Loss: 0.0157 | Grad Norm: 0.01039335\n",
      "Epoch 3 | Step 1791300 | Avg Loss: 0.0155 | Grad Norm: 0.00870244\n",
      "Epoch 3 | Step 1791400 | Avg Loss: 0.0155 | Grad Norm: 0.00859930\n",
      "Epoch 3 | Step 1791500 | Avg Loss: 0.0152 | Grad Norm: 0.00900271\n",
      "Epoch 3 | Step 1791600 | Avg Loss: 0.0156 | Grad Norm: 0.00859967\n",
      "Epoch 3 | Step 1791700 | Avg Loss: 0.0158 | Grad Norm: 0.00894364\n",
      "Epoch 3 | Step 1791800 | Avg Loss: 0.0157 | Grad Norm: 0.00939879\n",
      "Epoch 3 | Step 1791900 | Avg Loss: 0.0150 | Grad Norm: 0.00826998\n",
      "Epoch 3 | Step 1792000 | Avg Loss: 0.0152 | Grad Norm: 0.00892591\n",
      "Epoch 3 | Step 1792100 | Avg Loss: 0.0154 | Grad Norm: 0.00910990\n",
      "Epoch 3 | Step 1792200 | Avg Loss: 0.0155 | Grad Norm: 0.00822405\n",
      "Epoch 3 | Step 1792300 | Avg Loss: 0.0153 | Grad Norm: 0.01201375\n",
      "Epoch 3 | Step 1792400 | Avg Loss: 0.0153 | Grad Norm: 0.00728534\n",
      "Epoch 3 | Step 1792500 | Avg Loss: 0.0152 | Grad Norm: 0.00945080\n",
      "Epoch 3 | Step 1792600 | Avg Loss: 0.0153 | Grad Norm: 0.00849793\n",
      "Epoch 3 | Step 1792700 | Avg Loss: 0.0149 | Grad Norm: 0.01006434\n",
      "Epoch 3 | Step 1792800 | Avg Loss: 0.0148 | Grad Norm: 0.00775347\n",
      "Epoch 3 | Step 1792900 | Avg Loss: 0.0150 | Grad Norm: 0.00871788\n",
      "Epoch 3 | Step 1793000 | Avg Loss: 0.0146 | Grad Norm: 0.00969477\n",
      "Epoch 3 | Step 1793100 | Avg Loss: 0.0154 | Grad Norm: 0.01067984\n",
      "Epoch 3 | Step 1793200 | Avg Loss: 0.0153 | Grad Norm: 0.00801080\n",
      "Epoch 3 | Step 1793300 | Avg Loss: 0.0154 | Grad Norm: 0.00916785\n",
      "Epoch 3 | Step 1793400 | Avg Loss: 0.0155 | Grad Norm: 0.00999167\n",
      "Epoch 3 | Step 1793500 | Avg Loss: 0.0152 | Grad Norm: 0.00972375\n",
      "Epoch 3 | Step 1793600 | Avg Loss: 0.0153 | Grad Norm: 0.00790381\n",
      "Epoch 3 | Step 1793700 | Avg Loss: 0.0153 | Grad Norm: 0.00847543\n",
      "Epoch 3 | Step 1793800 | Avg Loss: 0.0157 | Grad Norm: 0.00832441\n",
      "Epoch 3 | Step 1793900 | Avg Loss: 0.0156 | Grad Norm: 0.00862847\n",
      "Epoch 3 | Step 1794000 | Avg Loss: 0.0159 | Grad Norm: 0.00838996\n",
      "Epoch 3 | Step 1794100 | Avg Loss: 0.0154 | Grad Norm: 0.00768263\n",
      "Epoch 3 | Step 1794200 | Avg Loss: 0.0157 | Grad Norm: 0.00879569\n",
      "Epoch 3 | Step 1794300 | Avg Loss: 0.0157 | Grad Norm: 0.00849198\n",
      "Epoch 3 | Step 1794400 | Avg Loss: 0.0156 | Grad Norm: 0.00887001\n",
      "Epoch 3 | Step 1794500 | Avg Loss: 0.0159 | Grad Norm: 0.01004336\n",
      "Epoch 3 | Step 1794600 | Avg Loss: 0.0159 | Grad Norm: 0.00928997\n",
      "Epoch 3 | Step 1794700 | Avg Loss: 0.0159 | Grad Norm: 0.00933293\n",
      "Epoch 3 | Step 1794800 | Avg Loss: 0.0155 | Grad Norm: 0.01041604\n",
      "Epoch 3 | Step 1794900 | Avg Loss: 0.0154 | Grad Norm: 0.00975376\n",
      "Epoch 3 | Step 1795000 | Avg Loss: 0.0152 | Grad Norm: 0.00887510\n",
      "Epoch 3 | Step 1795100 | Avg Loss: 0.0154 | Grad Norm: 0.00996611\n",
      "Epoch 3 | Step 1795200 | Avg Loss: 0.0157 | Grad Norm: 0.01152314\n",
      "Epoch 3 | Step 1795300 | Avg Loss: 0.0157 | Grad Norm: 0.00881840\n",
      "Epoch 3 | Step 1795400 | Avg Loss: 0.0157 | Grad Norm: 0.00839168\n",
      "Epoch 3 | Step 1795500 | Avg Loss: 0.0158 | Grad Norm: 0.00867762\n",
      "Epoch 3 | Step 1795600 | Avg Loss: 0.0159 | Grad Norm: 0.01053567\n",
      "Epoch 3 | Step 1795700 | Avg Loss: 0.0159 | Grad Norm: 0.00910544\n",
      "Epoch 3 | Step 1795800 | Avg Loss: 0.0156 | Grad Norm: 0.00884106\n",
      "Epoch 3 | Step 1795900 | Avg Loss: 0.0152 | Grad Norm: 0.00895537\n",
      "Epoch 3 | Step 1796000 | Avg Loss: 0.0160 | Grad Norm: 0.00886894\n",
      "Epoch 3 | Step 1796100 | Avg Loss: 0.0159 | Grad Norm: 0.00925103\n",
      "Epoch 3 | Step 1796200 | Avg Loss: 0.0155 | Grad Norm: 0.00980413\n",
      "Epoch 3 | Step 1796300 | Avg Loss: 0.0153 | Grad Norm: 0.00858703\n",
      "Epoch 3 | Step 1796400 | Avg Loss: 0.0153 | Grad Norm: 0.00807152\n",
      "Epoch 3 | Step 1796500 | Avg Loss: 0.0154 | Grad Norm: 0.00905659\n",
      "Epoch 3 | Step 1796600 | Avg Loss: 0.0156 | Grad Norm: 0.00863066\n",
      "Epoch 3 | Step 1796700 | Avg Loss: 0.0159 | Grad Norm: 0.01000770\n",
      "Epoch 3 | Step 1796800 | Avg Loss: 0.0156 | Grad Norm: 0.00772042\n",
      "Epoch 3 | Step 1796900 | Avg Loss: 0.0154 | Grad Norm: 0.00904986\n",
      "Epoch 3 | Step 1797000 | Avg Loss: 0.0155 | Grad Norm: 0.00915069\n",
      "Epoch 3 | Step 1797100 | Avg Loss: 0.0154 | Grad Norm: 0.00743267\n",
      "Epoch 3 | Step 1797200 | Avg Loss: 0.0157 | Grad Norm: 0.00998980\n",
      "Epoch 3 | Step 1797300 | Avg Loss: 0.0157 | Grad Norm: 0.00902732\n",
      "Epoch 3 | Step 1797400 | Avg Loss: 0.0158 | Grad Norm: 0.01006258\n",
      "Epoch 3 | Step 1797500 | Avg Loss: 0.0156 | Grad Norm: 0.00842240\n",
      "Epoch 3 | Step 1797600 | Avg Loss: 0.0155 | Grad Norm: 0.00924324\n",
      "Epoch 3 | Step 1797700 | Avg Loss: 0.0152 | Grad Norm: 0.00890810\n",
      "Epoch 3 | Step 1797800 | Avg Loss: 0.0152 | Grad Norm: 0.00859360\n",
      "Epoch 3 | Step 1797900 | Avg Loss: 0.0156 | Grad Norm: 0.00962191\n",
      "Epoch 3 | Step 1798000 | Avg Loss: 0.0152 | Grad Norm: 0.00802786\n",
      "Epoch 3 | Step 1798100 | Avg Loss: 0.0155 | Grad Norm: 0.00804378\n",
      "Epoch 3 | Step 1798200 | Avg Loss: 0.0156 | Grad Norm: 0.01103732\n",
      "Epoch 3 | Step 1798300 | Avg Loss: 0.0154 | Grad Norm: 0.00787958\n",
      "Epoch 3 | Step 1798400 | Avg Loss: 0.0155 | Grad Norm: 0.00869340\n",
      "Epoch 3 | Step 1798500 | Avg Loss: 0.0155 | Grad Norm: 0.00839553\n",
      "Epoch 3 | Step 1798600 | Avg Loss: 0.0155 | Grad Norm: 0.00856421\n",
      "Epoch 3 | Step 1798700 | Avg Loss: 0.0153 | Grad Norm: 0.01012646\n",
      "Epoch 3 | Step 1798800 | Avg Loss: 0.0154 | Grad Norm: 0.00942546\n",
      "Epoch 3 | Step 1798900 | Avg Loss: 0.0156 | Grad Norm: 0.01256072\n",
      "Epoch 3 | Step 1799000 | Avg Loss: 0.0152 | Grad Norm: 0.01016981\n",
      "Epoch 3 | Step 1799100 | Avg Loss: 0.0151 | Grad Norm: 0.00886283\n",
      "Epoch 3 | Step 1799200 | Avg Loss: 0.0151 | Grad Norm: 0.00872577\n",
      "Epoch 3 | Step 1799300 | Avg Loss: 0.0154 | Grad Norm: 0.00851593\n",
      "Epoch 3 | Step 1799400 | Avg Loss: 0.0157 | Grad Norm: 0.00827886\n",
      "Epoch 3 | Step 1799500 | Avg Loss: 0.0156 | Grad Norm: 0.00899693\n",
      "Epoch 3 | Step 1799600 | Avg Loss: 0.0158 | Grad Norm: 0.00796098\n",
      "Epoch 3 | Step 1799700 | Avg Loss: 0.0158 | Grad Norm: 0.00831603\n",
      "Epoch 3 | Step 1799800 | Avg Loss: 0.0161 | Grad Norm: 0.00908948\n",
      "Epoch 3 | Step 1799900 | Avg Loss: 0.0158 | Grad Norm: 0.00833036\n",
      "Epoch 3 | Step 1800000 | Avg Loss: 0.0160 | Grad Norm: 0.00832245\n",
      "Saving model at step1800000\n",
      "Epoch 3 | Step 1800100 | Avg Loss: 0.0158 | Grad Norm: 0.00954673\n",
      "Epoch 3 | Step 1800200 | Avg Loss: 0.0157 | Grad Norm: 0.00962372\n",
      "Epoch 3 | Step 1800300 | Avg Loss: 0.0157 | Grad Norm: 0.00897540\n",
      "Epoch 3 | Step 1800400 | Avg Loss: 0.0153 | Grad Norm: 0.00896071\n",
      "Epoch 3 | Step 1800500 | Avg Loss: 0.0155 | Grad Norm: 0.00796062\n",
      "Epoch 3 | Step 1800600 | Avg Loss: 0.0159 | Grad Norm: 0.00879951\n",
      "Epoch 3 | Step 1800700 | Avg Loss: 0.0160 | Grad Norm: 0.00975985\n",
      "Epoch 3 | Step 1800800 | Avg Loss: 0.0153 | Grad Norm: 0.00935770\n",
      "Epoch 3 | Step 1800900 | Avg Loss: 0.0155 | Grad Norm: 0.00782173\n",
      "Epoch 3 | Step 1801000 | Avg Loss: 0.0153 | Grad Norm: 0.00828431\n",
      "Epoch 3 | Step 1801100 | Avg Loss: 0.0154 | Grad Norm: 0.00787964\n",
      "Epoch 3 | Step 1801200 | Avg Loss: 0.0157 | Grad Norm: 0.00893699\n",
      "Epoch 3 | Step 1801300 | Avg Loss: 0.0160 | Grad Norm: 0.00846107\n",
      "Epoch 3 | Step 1801400 | Avg Loss: 0.0157 | Grad Norm: 0.00898138\n",
      "Epoch 3 | Step 1801500 | Avg Loss: 0.0156 | Grad Norm: 0.00841798\n",
      "Epoch 3 | Step 1801600 | Avg Loss: 0.0152 | Grad Norm: 0.00805351\n",
      "Epoch 3 | Step 1801700 | Avg Loss: 0.0150 | Grad Norm: 0.00855801\n",
      "Epoch 3 | Step 1801800 | Avg Loss: 0.0153 | Grad Norm: 0.00949092\n",
      "Epoch 3 | Step 1801900 | Avg Loss: 0.0152 | Grad Norm: 0.00886596\n",
      "Epoch 3 | Step 1802000 | Avg Loss: 0.0157 | Grad Norm: 0.00842104\n",
      "Epoch 3 | Step 1802100 | Avg Loss: 0.0157 | Grad Norm: 0.00956151\n",
      "Epoch 3 | Step 1802200 | Avg Loss: 0.0156 | Grad Norm: 0.00829345\n",
      "Epoch 3 | Step 1802300 | Avg Loss: 0.0156 | Grad Norm: 0.00986379\n",
      "Epoch 3 | Step 1802400 | Avg Loss: 0.0154 | Grad Norm: 0.00961997\n",
      "Epoch 3 | Step 1802500 | Avg Loss: 0.0156 | Grad Norm: 0.01195741\n",
      "Epoch 3 | Step 1802600 | Avg Loss: 0.0155 | Grad Norm: 0.01046301\n",
      "Epoch 3 | Step 1802700 | Avg Loss: 0.0153 | Grad Norm: 0.00970049\n",
      "Epoch 3 | Step 1802800 | Avg Loss: 0.0158 | Grad Norm: 0.00851183\n",
      "Epoch 3 | Step 1802900 | Avg Loss: 0.0159 | Grad Norm: 0.00855716\n",
      "Epoch 3 | Step 1803000 | Avg Loss: 0.0157 | Grad Norm: 0.00994609\n",
      "Epoch 3 | Step 1803100 | Avg Loss: 0.0159 | Grad Norm: 0.00999989\n",
      "Epoch 3 | Step 1803200 | Avg Loss: 0.0158 | Grad Norm: 0.01001580\n",
      "Epoch 3 | Step 1803300 | Avg Loss: 0.0157 | Grad Norm: 0.00896580\n",
      "Epoch 3 | Step 1803400 | Avg Loss: 0.0159 | Grad Norm: 0.00910284\n",
      "Epoch 3 | Step 1803500 | Avg Loss: 0.0157 | Grad Norm: 0.00802656\n",
      "Epoch 3 | Step 1803600 | Avg Loss: 0.0156 | Grad Norm: 0.00908985\n",
      "Epoch 3 | Step 1803700 | Avg Loss: 0.0159 | Grad Norm: 0.00965404\n",
      "Epoch 3 | Step 1803800 | Avg Loss: 0.0159 | Grad Norm: 0.00970632\n",
      "Epoch 3 | Step 1803900 | Avg Loss: 0.0160 | Grad Norm: 0.00878808\n",
      "Epoch 3 | Step 1804000 | Avg Loss: 0.0157 | Grad Norm: 0.00876805\n",
      "Epoch 3 | Step 1804100 | Avg Loss: 0.0155 | Grad Norm: 0.00785956\n",
      "Epoch 3 | Step 1804200 | Avg Loss: 0.0159 | Grad Norm: 0.00941327\n",
      "Epoch 3 | Step 1804300 | Avg Loss: 0.0156 | Grad Norm: 0.00819234\n",
      "Epoch 3 | Step 1804400 | Avg Loss: 0.0163 | Grad Norm: 0.00929904\n",
      "Epoch 3 | Step 1804500 | Avg Loss: 0.0161 | Grad Norm: 0.00932222\n",
      "Epoch 3 | Step 1804600 | Avg Loss: 0.0157 | Grad Norm: 0.01001094\n",
      "Epoch 3 | Step 1804700 | Avg Loss: 0.0158 | Grad Norm: 0.01081386\n",
      "Epoch 3 | Step 1804800 | Avg Loss: 0.0155 | Grad Norm: 0.00872645\n",
      "Epoch 3 | Step 1804900 | Avg Loss: 0.0153 | Grad Norm: 0.00737871\n",
      "Epoch 3 | Step 1805000 | Avg Loss: 0.0154 | Grad Norm: 0.00774431\n",
      "Epoch 3 | Step 1805100 | Avg Loss: 0.0154 | Grad Norm: 0.00888222\n",
      "Epoch 3 | Step 1805200 | Avg Loss: 0.0156 | Grad Norm: 0.01026070\n",
      "Epoch 3 | Step 1805300 | Avg Loss: 0.0155 | Grad Norm: 0.00948365\n",
      "Epoch 3 | Step 1805400 | Avg Loss: 0.0151 | Grad Norm: 0.00972897\n",
      "Epoch 3 | Step 1805500 | Avg Loss: 0.0149 | Grad Norm: 0.00872849\n",
      "Epoch 3 | Step 1805600 | Avg Loss: 0.0149 | Grad Norm: 0.00898790\n",
      "Epoch 3 | Step 1805700 | Avg Loss: 0.0149 | Grad Norm: 0.00913561\n",
      "Epoch 3 | Step 1805800 | Avg Loss: 0.0151 | Grad Norm: 0.00750760\n",
      "Epoch 3 | Step 1805900 | Avg Loss: 0.0154 | Grad Norm: 0.00828185\n",
      "Epoch 3 | Step 1806000 | Avg Loss: 0.0157 | Grad Norm: 0.00872388\n",
      "Epoch 3 | Step 1806100 | Avg Loss: 0.0158 | Grad Norm: 0.00827145\n",
      "Epoch 3 | Step 1806200 | Avg Loss: 0.0156 | Grad Norm: 0.00877012\n",
      "Epoch 3 | Step 1806300 | Avg Loss: 0.0151 | Grad Norm: 0.00751331\n",
      "Epoch 3 | Step 1806400 | Avg Loss: 0.0154 | Grad Norm: 0.00864912\n",
      "Epoch 3 | Step 1806500 | Avg Loss: 0.0157 | Grad Norm: 0.00831146\n",
      "Epoch 3 | Step 1806600 | Avg Loss: 0.0156 | Grad Norm: 0.00926939\n",
      "Epoch 3 | Step 1806700 | Avg Loss: 0.0155 | Grad Norm: 0.00872731\n",
      "Epoch 3 | Step 1806800 | Avg Loss: 0.0156 | Grad Norm: 0.00808682\n",
      "Epoch 3 | Step 1806900 | Avg Loss: 0.0154 | Grad Norm: 0.01030339\n",
      "Epoch 3 | Step 1807000 | Avg Loss: 0.0152 | Grad Norm: 0.00784774\n",
      "Epoch 3 | Step 1807100 | Avg Loss: 0.0150 | Grad Norm: 0.00805607\n",
      "Epoch 3 | Step 1807200 | Avg Loss: 0.0149 | Grad Norm: 0.00856572\n",
      "Epoch 3 | Step 1807300 | Avg Loss: 0.0152 | Grad Norm: 0.00892875\n",
      "Epoch 3 | Step 1807400 | Avg Loss: 0.0150 | Grad Norm: 0.00829668\n",
      "Epoch 3 | Step 1807500 | Avg Loss: 0.0152 | Grad Norm: 0.00890866\n",
      "Epoch 3 | Step 1807600 | Avg Loss: 0.0153 | Grad Norm: 0.00815103\n",
      "Epoch 3 | Step 1807700 | Avg Loss: 0.0154 | Grad Norm: 0.00768218\n",
      "Epoch 3 | Step 1807800 | Avg Loss: 0.0151 | Grad Norm: 0.00892913\n",
      "Epoch 3 | Step 1807900 | Avg Loss: 0.0153 | Grad Norm: 0.00864046\n",
      "Epoch 3 | Step 1808000 | Avg Loss: 0.0154 | Grad Norm: 0.00818420\n",
      "Epoch 3 | Step 1808100 | Avg Loss: 0.0154 | Grad Norm: 0.00893966\n",
      "Epoch 3 | Step 1808200 | Avg Loss: 0.0152 | Grad Norm: 0.00853920\n",
      "Epoch 3 | Step 1808300 | Avg Loss: 0.0160 | Grad Norm: 0.00789312\n",
      "Epoch 3 | Step 1808400 | Avg Loss: 0.0158 | Grad Norm: 0.00935988\n",
      "Epoch 3 | Step 1808500 | Avg Loss: 0.0160 | Grad Norm: 0.00926255\n",
      "Epoch 3 | Step 1808600 | Avg Loss: 0.0158 | Grad Norm: 0.01061491\n",
      "Epoch 3 | Step 1808700 | Avg Loss: 0.0156 | Grad Norm: 0.00923964\n",
      "Epoch 3 | Step 1808800 | Avg Loss: 0.0157 | Grad Norm: 0.00950671\n",
      "Epoch 3 | Step 1808900 | Avg Loss: 0.0157 | Grad Norm: 0.01070523\n",
      "Epoch 3 | Step 1809000 | Avg Loss: 0.0157 | Grad Norm: 0.00818341\n",
      "Epoch 3 | Step 1809100 | Avg Loss: 0.0155 | Grad Norm: 0.00918111\n",
      "Epoch 3 | Step 1809200 | Avg Loss: 0.0157 | Grad Norm: 0.00965765\n",
      "Epoch 3 | Step 1809300 | Avg Loss: 0.0157 | Grad Norm: 0.00884022\n",
      "Epoch 3 | Step 1809400 | Avg Loss: 0.0153 | Grad Norm: 0.00936091\n",
      "Epoch 3 | Step 1809500 | Avg Loss: 0.0156 | Grad Norm: 0.00960330\n",
      "Epoch 3 | Step 1809600 | Avg Loss: 0.0157 | Grad Norm: 0.01092132\n",
      "Epoch 3 | Step 1809700 | Avg Loss: 0.0158 | Grad Norm: 0.00857019\n",
      "Epoch 3 | Step 1809800 | Avg Loss: 0.0155 | Grad Norm: 0.00882317\n",
      "Epoch 3 | Step 1809900 | Avg Loss: 0.0155 | Grad Norm: 0.00939146\n",
      "Epoch 3 | Step 1810000 | Avg Loss: 0.0152 | Grad Norm: 0.01382521\n",
      "Epoch 3 | Step 1810100 | Avg Loss: 0.0159 | Grad Norm: 0.01250985\n",
      "Epoch 3 | Step 1810200 | Avg Loss: 0.0157 | Grad Norm: 0.00811243\n",
      "Epoch 3 | Step 1810300 | Avg Loss: 0.0162 | Grad Norm: 0.00836855\n",
      "Epoch 3 | Step 1810400 | Avg Loss: 0.0160 | Grad Norm: 0.00848884\n",
      "Epoch 3 | Step 1810500 | Avg Loss: 0.0160 | Grad Norm: 0.00868125\n",
      "Epoch 3 | Step 1810600 | Avg Loss: 0.0160 | Grad Norm: 0.00900673\n",
      "Epoch 3 | Step 1810700 | Avg Loss: 0.0162 | Grad Norm: 0.00981518\n",
      "Epoch 3 | Step 1810800 | Avg Loss: 0.0159 | Grad Norm: 0.00798778\n",
      "Epoch 3 | Step 1810900 | Avg Loss: 0.0154 | Grad Norm: 0.00784327\n",
      "Epoch 3 | Step 1811000 | Avg Loss: 0.0150 | Grad Norm: 0.00848214\n",
      "Epoch 3 | Step 1811100 | Avg Loss: 0.0153 | Grad Norm: 0.00995896\n",
      "Epoch 3 | Step 1811200 | Avg Loss: 0.0157 | Grad Norm: 0.01067562\n",
      "Epoch 3 | Step 1811300 | Avg Loss: 0.0160 | Grad Norm: 0.00828735\n",
      "Epoch 3 | Step 1811400 | Avg Loss: 0.0160 | Grad Norm: 0.00896946\n",
      "Epoch 3 | Step 1811500 | Avg Loss: 0.0157 | Grad Norm: 0.00871855\n",
      "Epoch 3 | Step 1811600 | Avg Loss: 0.0158 | Grad Norm: 0.00885225\n",
      "Epoch 3 | Step 1811700 | Avg Loss: 0.0157 | Grad Norm: 0.00929216\n",
      "Epoch 3 | Step 1811800 | Avg Loss: 0.0153 | Grad Norm: 0.00864317\n",
      "Epoch 3 | Step 1811900 | Avg Loss: 0.0152 | Grad Norm: 0.01096560\n",
      "Epoch 3 | Step 1812000 | Avg Loss: 0.0153 | Grad Norm: 0.00897193\n",
      "Epoch 3 | Step 1812100 | Avg Loss: 0.0155 | Grad Norm: 0.00964632\n",
      "Epoch 3 | Step 1812200 | Avg Loss: 0.0155 | Grad Norm: 0.00830933\n",
      "Epoch 3 | Step 1812300 | Avg Loss: 0.0157 | Grad Norm: 0.00889054\n",
      "Epoch 3 | Step 1812400 | Avg Loss: 0.0154 | Grad Norm: 0.01005379\n",
      "Epoch 3 | Step 1812500 | Avg Loss: 0.0154 | Grad Norm: 0.00907218\n",
      "Epoch 3 | Step 1812600 | Avg Loss: 0.0154 | Grad Norm: 0.01053899\n",
      "Epoch 3 | Step 1812700 | Avg Loss: 0.0151 | Grad Norm: 0.00894962\n",
      "Epoch 3 | Step 1812800 | Avg Loss: 0.0149 | Grad Norm: 0.00894111\n",
      "Epoch 3 | Step 1812900 | Avg Loss: 0.0147 | Grad Norm: 0.00870752\n",
      "Epoch 3 | Step 1813000 | Avg Loss: 0.0145 | Grad Norm: 0.00762320\n",
      "Epoch 3 | Step 1813100 | Avg Loss: 0.0149 | Grad Norm: 0.00915532\n",
      "Epoch 3 | Step 1813200 | Avg Loss: 0.0149 | Grad Norm: 0.00907180\n",
      "Epoch 3 | Step 1813300 | Avg Loss: 0.0150 | Grad Norm: 0.00948695\n",
      "Epoch 3 | Step 1813400 | Avg Loss: 0.0149 | Grad Norm: 0.00906173\n",
      "Epoch 3 | Step 1813500 | Avg Loss: 0.0146 | Grad Norm: 0.00854038\n",
      "Epoch 3 | Step 1813600 | Avg Loss: 0.0144 | Grad Norm: 0.00770882\n",
      "Epoch 3 | Step 1813700 | Avg Loss: 0.0144 | Grad Norm: 0.00875799\n",
      "Epoch 3 | Step 1813800 | Avg Loss: 0.0146 | Grad Norm: 0.00809807\n",
      "Epoch 3 | Step 1813900 | Avg Loss: 0.0148 | Grad Norm: 0.01098970\n",
      "Epoch 3 | Step 1814000 | Avg Loss: 0.0148 | Grad Norm: 0.01025183\n",
      "Epoch 3 | Step 1814100 | Avg Loss: 0.0150 | Grad Norm: 0.00721544\n",
      "Epoch 3 | Step 1814200 | Avg Loss: 0.0155 | Grad Norm: 0.01078945\n",
      "Epoch 3 | Step 1814300 | Avg Loss: 0.0154 | Grad Norm: 0.01112242\n",
      "Epoch 3 | Step 1814400 | Avg Loss: 0.0159 | Grad Norm: 0.00984645\n",
      "Epoch 3 | Step 1814500 | Avg Loss: 0.0156 | Grad Norm: 0.00828796\n",
      "Epoch 3 | Step 1814600 | Avg Loss: 0.0157 | Grad Norm: 0.00890222\n",
      "Epoch 3 | Step 1814700 | Avg Loss: 0.0153 | Grad Norm: 0.00768466\n",
      "Epoch 3 | Step 1814800 | Avg Loss: 0.0155 | Grad Norm: 0.01082572\n",
      "Epoch 3 | Step 1814900 | Avg Loss: 0.0159 | Grad Norm: 0.01065234\n",
      "Epoch 3 | Step 1815000 | Avg Loss: 0.0159 | Grad Norm: 0.00888874\n",
      "Epoch 3 | Step 1815100 | Avg Loss: 0.0157 | Grad Norm: 0.01081222\n",
      "Epoch 3 | Step 1815200 | Avg Loss: 0.0158 | Grad Norm: 0.00966078\n",
      "Epoch 3 | Step 1815300 | Avg Loss: 0.0155 | Grad Norm: 0.01088263\n",
      "Epoch 3 | Step 1815400 | Avg Loss: 0.0156 | Grad Norm: 0.01145991\n",
      "Epoch 3 | Step 1815500 | Avg Loss: 0.0156 | Grad Norm: 0.01061812\n",
      "Epoch 3 | Step 1815600 | Avg Loss: 0.0153 | Grad Norm: 0.00817898\n",
      "Epoch 3 | Step 1815700 | Avg Loss: 0.0154 | Grad Norm: 0.00903353\n",
      "Epoch 3 | Step 1815800 | Avg Loss: 0.0153 | Grad Norm: 0.00966000\n",
      "Epoch 3 | Step 1815900 | Avg Loss: 0.0155 | Grad Norm: 0.00890067\n",
      "Epoch 3 | Step 1816000 | Avg Loss: 0.0155 | Grad Norm: 0.00976882\n",
      "Epoch 3 | Step 1816100 | Avg Loss: 0.0152 | Grad Norm: 0.00883728\n",
      "Epoch 3 | Step 1816200 | Avg Loss: 0.0154 | Grad Norm: 0.00878655\n",
      "Epoch 3 | Step 1816300 | Avg Loss: 0.0150 | Grad Norm: 0.00969255\n",
      "Epoch 3 | Step 1816400 | Avg Loss: 0.0151 | Grad Norm: 0.00852210\n",
      "Epoch 3 | Step 1816500 | Avg Loss: 0.0152 | Grad Norm: 0.00893613\n",
      "Epoch 3 | Step 1816600 | Avg Loss: 0.0149 | Grad Norm: 0.01135202\n",
      "Epoch 3 | Step 1816700 | Avg Loss: 0.0153 | Grad Norm: 0.00825253\n",
      "Epoch 3 | Step 1816800 | Avg Loss: 0.0157 | Grad Norm: 0.00847016\n",
      "Epoch 3 | Step 1816900 | Avg Loss: 0.0156 | Grad Norm: 0.01130803\n",
      "Epoch 3 | Step 1817000 | Avg Loss: 0.0155 | Grad Norm: 0.00871814\n",
      "Epoch 3 | Step 1817100 | Avg Loss: 0.0156 | Grad Norm: 0.00918832\n",
      "Epoch 3 | Step 1817200 | Avg Loss: 0.0156 | Grad Norm: 0.00863942\n",
      "Epoch 3 | Step 1817300 | Avg Loss: 0.0158 | Grad Norm: 0.00836867\n",
      "Epoch 3 | Step 1817400 | Avg Loss: 0.0157 | Grad Norm: 0.00891959\n",
      "Epoch 3 | Step 1817500 | Avg Loss: 0.0153 | Grad Norm: 0.00817689\n",
      "Epoch 3 | Step 1817600 | Avg Loss: 0.0152 | Grad Norm: 0.00932876\n",
      "Epoch 3 | Step 1817700 | Avg Loss: 0.0153 | Grad Norm: 0.00878996\n",
      "Epoch 3 | Step 1817800 | Avg Loss: 0.0152 | Grad Norm: 0.01029901\n",
      "Epoch 3 | Step 1817900 | Avg Loss: 0.0154 | Grad Norm: 0.01126673\n",
      "Epoch 3 | Step 1818000 | Avg Loss: 0.0155 | Grad Norm: 0.01243195\n",
      "Epoch 3 | Step 1818100 | Avg Loss: 0.0157 | Grad Norm: 0.00861314\n",
      "Epoch 3 | Step 1818200 | Avg Loss: 0.0157 | Grad Norm: 0.00963093\n",
      "Epoch 3 | Step 1818300 | Avg Loss: 0.0155 | Grad Norm: 0.01154350\n",
      "Epoch 3 | Step 1818400 | Avg Loss: 0.0156 | Grad Norm: 0.00978956\n",
      "Epoch 3 | Step 1818500 | Avg Loss: 0.0156 | Grad Norm: 0.00875250\n",
      "Epoch 3 | Step 1818600 | Avg Loss: 0.0153 | Grad Norm: 0.00905057\n",
      "Epoch 3 | Step 1818700 | Avg Loss: 0.0153 | Grad Norm: 0.00827415\n",
      "Epoch 3 | Step 1818800 | Avg Loss: 0.0155 | Grad Norm: 0.00885792\n",
      "Epoch 3 | Step 1818900 | Avg Loss: 0.0156 | Grad Norm: 0.00871375\n",
      "Epoch 3 | Step 1819000 | Avg Loss: 0.0155 | Grad Norm: 0.00802222\n",
      "Epoch 3 | Step 1819100 | Avg Loss: 0.0152 | Grad Norm: 0.00962592\n",
      "Epoch 3 | Step 1819200 | Avg Loss: 0.0153 | Grad Norm: 0.00856709\n",
      "Epoch 3 | Step 1819300 | Avg Loss: 0.0150 | Grad Norm: 0.00840731\n",
      "Epoch 3 | Step 1819400 | Avg Loss: 0.0153 | Grad Norm: 0.01097068\n",
      "Epoch 3 | Step 1819500 | Avg Loss: 0.0153 | Grad Norm: 0.00778048\n",
      "Epoch 3 | Step 1819600 | Avg Loss: 0.0154 | Grad Norm: 0.00847705\n",
      "Epoch 3 | Step 1819700 | Avg Loss: 0.0156 | Grad Norm: 0.00974337\n",
      "Epoch 3 | Step 1819800 | Avg Loss: 0.0153 | Grad Norm: 0.01134202\n",
      "Epoch 3 | Step 1819900 | Avg Loss: 0.0154 | Grad Norm: 0.00791642\n",
      "Epoch 3 | Step 1820000 | Avg Loss: 0.0157 | Grad Norm: 0.00885334\n",
      "Epoch 3 | Step 1820100 | Avg Loss: 0.0157 | Grad Norm: 0.01088863\n",
      "Epoch 3 | Step 1820200 | Avg Loss: 0.0155 | Grad Norm: 0.00900121\n",
      "Epoch 3 | Step 1820300 | Avg Loss: 0.0159 | Grad Norm: 0.00972576\n",
      "Epoch 3 | Step 1820400 | Avg Loss: 0.0161 | Grad Norm: 0.00863602\n",
      "Epoch 3 | Step 1820500 | Avg Loss: 0.0158 | Grad Norm: 0.00837018\n",
      "Epoch 3 | Step 1820600 | Avg Loss: 0.0158 | Grad Norm: 0.00954955\n",
      "Epoch 3 | Step 1820700 | Avg Loss: 0.0154 | Grad Norm: 0.00945436\n",
      "Epoch 3 | Step 1820800 | Avg Loss: 0.0153 | Grad Norm: 0.00883069\n",
      "Epoch 3 | Step 1820900 | Avg Loss: 0.0149 | Grad Norm: 0.00936729\n",
      "Epoch 3 | Step 1821000 | Avg Loss: 0.0151 | Grad Norm: 0.00969646\n",
      "Epoch 3 | Step 1821100 | Avg Loss: 0.0154 | Grad Norm: 0.00846691\n",
      "Epoch 3 | Step 1821200 | Avg Loss: 0.0160 | Grad Norm: 0.00984089\n",
      "Epoch 3 | Step 1821300 | Avg Loss: 0.0159 | Grad Norm: 0.00807381\n",
      "Epoch 3 | Step 1821400 | Avg Loss: 0.0159 | Grad Norm: 0.01026774\n",
      "Epoch 3 | Step 1821500 | Avg Loss: 0.0159 | Grad Norm: 0.00821364\n",
      "Epoch 3 | Step 1821600 | Avg Loss: 0.0157 | Grad Norm: 0.00944849\n",
      "Epoch 3 | Step 1821700 | Avg Loss: 0.0154 | Grad Norm: 0.00800244\n",
      "Epoch 3 | Step 1821800 | Avg Loss: 0.0152 | Grad Norm: 0.00877150\n",
      "Epoch 3 | Step 1821900 | Avg Loss: 0.0152 | Grad Norm: 0.00894365\n",
      "Epoch 3 | Step 1822000 | Avg Loss: 0.0153 | Grad Norm: 0.00884425\n",
      "Epoch 3 | Step 1822100 | Avg Loss: 0.0151 | Grad Norm: 0.00924910\n",
      "Epoch 3 | Step 1822200 | Avg Loss: 0.0152 | Grad Norm: 0.00900776\n",
      "Epoch 3 | Step 1822300 | Avg Loss: 0.0154 | Grad Norm: 0.01000821\n",
      "Epoch 3 | Step 1822400 | Avg Loss: 0.0153 | Grad Norm: 0.01062401\n",
      "Epoch 3 | Step 1822500 | Avg Loss: 0.0151 | Grad Norm: 0.00840140\n",
      "Epoch 3 | Step 1822600 | Avg Loss: 0.0152 | Grad Norm: 0.01110312\n",
      "Epoch 3 | Step 1822700 | Avg Loss: 0.0154 | Grad Norm: 0.01136534\n",
      "Epoch 3 | Step 1822800 | Avg Loss: 0.0151 | Grad Norm: 0.00874362\n",
      "Epoch 3 | Step 1822900 | Avg Loss: 0.0154 | Grad Norm: 0.00979628\n",
      "Epoch 3 | Step 1823000 | Avg Loss: 0.0155 | Grad Norm: 0.00920904\n",
      "Epoch 3 | Step 1823100 | Avg Loss: 0.0155 | Grad Norm: 0.01073368\n",
      "Epoch 3 | Step 1823200 | Avg Loss: 0.0157 | Grad Norm: 0.01103794\n",
      "Epoch 3 | Step 1823300 | Avg Loss: 0.0161 | Grad Norm: 0.00896319\n",
      "Epoch 3 | Step 1823400 | Avg Loss: 0.0158 | Grad Norm: 0.01014147\n",
      "Epoch 3 | Step 1823500 | Avg Loss: 0.0156 | Grad Norm: 0.01064123\n",
      "Epoch 3 | Step 1823600 | Avg Loss: 0.0154 | Grad Norm: 0.00830669\n",
      "Epoch 3 | Step 1823700 | Avg Loss: 0.0156 | Grad Norm: 0.00988961\n",
      "Epoch 3 | Step 1823800 | Avg Loss: 0.0154 | Grad Norm: 0.01014665\n",
      "Epoch 3 | Step 1823900 | Avg Loss: 0.0155 | Grad Norm: 0.00827135\n",
      "Epoch 3 | Step 1824000 | Avg Loss: 0.0157 | Grad Norm: 0.01061688\n",
      "Epoch 3 | Step 1824100 | Avg Loss: 0.0155 | Grad Norm: 0.00923238\n",
      "Epoch 3 | Step 1824200 | Avg Loss: 0.0155 | Grad Norm: 0.00791486\n",
      "Epoch 3 | Step 1824300 | Avg Loss: 0.0155 | Grad Norm: 0.00938671\n",
      "Epoch 3 | Step 1824400 | Avg Loss: 0.0156 | Grad Norm: 0.01107496\n",
      "Epoch 3 | Step 1824500 | Avg Loss: 0.0156 | Grad Norm: 0.00877891\n",
      "Epoch 3 | Step 1824600 | Avg Loss: 0.0159 | Grad Norm: 0.00974176\n",
      "Epoch 3 | Step 1824700 | Avg Loss: 0.0159 | Grad Norm: 0.01009550\n",
      "Epoch 3 | Step 1824800 | Avg Loss: 0.0157 | Grad Norm: 0.00914481\n",
      "Epoch 3 | Step 1824900 | Avg Loss: 0.0152 | Grad Norm: 0.00956767\n",
      "Epoch 3 | Step 1825000 | Avg Loss: 0.0157 | Grad Norm: 0.00950277\n",
      "Epoch 3 | Step 1825100 | Avg Loss: 0.0159 | Grad Norm: 0.00864533\n",
      "Epoch 3 | Step 1825200 | Avg Loss: 0.0156 | Grad Norm: 0.00842614\n",
      "Epoch 3 | Step 1825300 | Avg Loss: 0.0157 | Grad Norm: 0.00834441\n",
      "Epoch 3 | Step 1825400 | Avg Loss: 0.0156 | Grad Norm: 0.01012921\n",
      "Epoch 3 | Step 1825500 | Avg Loss: 0.0159 | Grad Norm: 0.00869490\n",
      "Epoch 3 | Step 1825600 | Avg Loss: 0.0155 | Grad Norm: 0.00933220\n",
      "Epoch 3 | Step 1825700 | Avg Loss: 0.0155 | Grad Norm: 0.00849380\n",
      "Epoch 3 | Step 1825800 | Avg Loss: 0.0156 | Grad Norm: 0.00906240\n",
      "Epoch 3 | Step 1825900 | Avg Loss: 0.0155 | Grad Norm: 0.00805659\n",
      "Epoch 3 | Step 1826000 | Avg Loss: 0.0159 | Grad Norm: 0.00848973\n",
      "Epoch 3 | Step 1826100 | Avg Loss: 0.0159 | Grad Norm: 0.00763176\n",
      "Epoch 3 | Step 1826200 | Avg Loss: 0.0159 | Grad Norm: 0.00823797\n",
      "Epoch 3 | Step 1826300 | Avg Loss: 0.0161 | Grad Norm: 0.00908168\n",
      "Epoch 3 | Step 1826400 | Avg Loss: 0.0158 | Grad Norm: 0.00796495\n",
      "Epoch 3 | Step 1826500 | Avg Loss: 0.0159 | Grad Norm: 0.00911806\n",
      "Epoch 3 | Step 1826600 | Avg Loss: 0.0162 | Grad Norm: 0.01719271\n",
      "Epoch 3 | Step 1826700 | Avg Loss: 0.0163 | Grad Norm: 0.00790426\n",
      "Epoch 3 | Step 1826800 | Avg Loss: 0.0162 | Grad Norm: 0.00918224\n",
      "Epoch 3 | Step 1826900 | Avg Loss: 0.0160 | Grad Norm: 0.01008522\n",
      "Epoch 3 | Step 1827000 | Avg Loss: 0.0160 | Grad Norm: 0.00902575\n",
      "Epoch 3 | Step 1827100 | Avg Loss: 0.0157 | Grad Norm: 0.00995148\n",
      "Epoch 3 | Step 1827200 | Avg Loss: 0.0157 | Grad Norm: 0.00936731\n",
      "Epoch 3 | Step 1827300 | Avg Loss: 0.0159 | Grad Norm: 0.00939971\n",
      "Epoch 3 | Step 1827400 | Avg Loss: 0.0162 | Grad Norm: 0.01020301\n",
      "Epoch 3 | Step 1827500 | Avg Loss: 0.0164 | Grad Norm: 0.00980227\n",
      "Epoch 3 | Step 1827600 | Avg Loss: 0.0160 | Grad Norm: 0.00905638\n",
      "Epoch 3 | Step 1827700 | Avg Loss: 0.0159 | Grad Norm: 0.00849560\n",
      "Epoch 3 | Step 1827800 | Avg Loss: 0.0152 | Grad Norm: 0.00892271\n",
      "Epoch 3 | Step 1827900 | Avg Loss: 0.0153 | Grad Norm: 0.00872639\n",
      "Epoch 3 | Step 1828000 | Avg Loss: 0.0161 | Grad Norm: 0.00926807\n",
      "Epoch 3 | Step 1828100 | Avg Loss: 0.0161 | Grad Norm: 0.01042107\n",
      "Epoch 3 | Step 1828200 | Avg Loss: 0.0161 | Grad Norm: 0.01258163\n",
      "Epoch 3 | Step 1828300 | Avg Loss: 0.0159 | Grad Norm: 0.00911499\n",
      "Epoch 3 | Step 1828400 | Avg Loss: 0.0156 | Grad Norm: 0.01050840\n",
      "Epoch 3 | Step 1828500 | Avg Loss: 0.0154 | Grad Norm: 0.01053836\n",
      "Epoch 3 | Step 1828600 | Avg Loss: 0.0155 | Grad Norm: 0.00754151\n",
      "Epoch 3 | Step 1828700 | Avg Loss: 0.0159 | Grad Norm: 0.00920294\n",
      "Epoch 3 | Step 1828800 | Avg Loss: 0.0158 | Grad Norm: 0.00917436\n",
      "Epoch 3 | Step 1828900 | Avg Loss: 0.0159 | Grad Norm: 0.00809203\n",
      "Epoch 3 | Step 1829000 | Avg Loss: 0.0161 | Grad Norm: 0.01020053\n",
      "Epoch 3 | Step 1829100 | Avg Loss: 0.0161 | Grad Norm: 0.00995284\n",
      "Epoch 3 | Step 1829200 | Avg Loss: 0.0157 | Grad Norm: 0.00971140\n",
      "Epoch 3 | Step 1829300 | Avg Loss: 0.0159 | Grad Norm: 0.00875591\n",
      "Epoch 3 | Step 1829400 | Avg Loss: 0.0157 | Grad Norm: 0.00835228\n",
      "Epoch 3 | Step 1829500 | Avg Loss: 0.0157 | Grad Norm: 0.01023727\n",
      "Epoch 3 | Step 1829600 | Avg Loss: 0.0155 | Grad Norm: 0.00828548\n",
      "Epoch 3 | Step 1829700 | Avg Loss: 0.0156 | Grad Norm: 0.00761976\n",
      "Epoch 3 | Step 1829800 | Avg Loss: 0.0151 | Grad Norm: 0.01119723\n",
      "Epoch 3 | Step 1829900 | Avg Loss: 0.0150 | Grad Norm: 0.00809012\n",
      "Epoch 3 | Step 1830000 | Avg Loss: 0.0149 | Grad Norm: 0.00913179\n",
      "Epoch 3 | Step 1830100 | Avg Loss: 0.0154 | Grad Norm: 0.00888151\n",
      "Epoch 3 | Step 1830200 | Avg Loss: 0.0154 | Grad Norm: 0.00838520\n",
      "Epoch 3 | Step 1830300 | Avg Loss: 0.0152 | Grad Norm: 0.01161160\n",
      "Epoch 3 | Step 1830400 | Avg Loss: 0.0155 | Grad Norm: 0.00842067\n",
      "Epoch 3 | Step 1830500 | Avg Loss: 0.0155 | Grad Norm: 0.01035147\n",
      "Epoch 3 | Step 1830600 | Avg Loss: 0.0156 | Grad Norm: 0.00850167\n",
      "Epoch 3 | Step 1830700 | Avg Loss: 0.0153 | Grad Norm: 0.00919608\n",
      "Epoch 3 | Step 1830800 | Avg Loss: 0.0154 | Grad Norm: 0.01060531\n",
      "Epoch 3 | Step 1830900 | Avg Loss: 0.0156 | Grad Norm: 0.01056635\n",
      "Epoch 3 | Step 1831000 | Avg Loss: 0.0156 | Grad Norm: 0.00871382\n",
      "Epoch 3 | Step 1831100 | Avg Loss: 0.0158 | Grad Norm: 0.00845054\n",
      "Epoch 3 | Step 1831200 | Avg Loss: 0.0157 | Grad Norm: 0.00855067\n",
      "Epoch 3 | Step 1831300 | Avg Loss: 0.0159 | Grad Norm: 0.00892803\n",
      "Epoch 3 | Step 1831400 | Avg Loss: 0.0157 | Grad Norm: 0.00910057\n",
      "Epoch 3 | Step 1831500 | Avg Loss: 0.0158 | Grad Norm: 0.00903961\n",
      "Epoch 3 | Step 1831600 | Avg Loss: 0.0158 | Grad Norm: 0.00862694\n",
      "Epoch 3 | Step 1831700 | Avg Loss: 0.0157 | Grad Norm: 0.00972541\n",
      "Epoch 3 | Step 1831800 | Avg Loss: 0.0158 | Grad Norm: 0.00998513\n",
      "Epoch 3 | Step 1831900 | Avg Loss: 0.0156 | Grad Norm: 0.01020665\n",
      "Epoch 3 | Step 1832000 | Avg Loss: 0.0161 | Grad Norm: 0.00866879\n",
      "Epoch 3 | Step 1832100 | Avg Loss: 0.0159 | Grad Norm: 0.01068452\n",
      "Epoch 3 | Step 1832200 | Avg Loss: 0.0155 | Grad Norm: 0.00858242\n",
      "Epoch 3 | Step 1832300 | Avg Loss: 0.0157 | Grad Norm: 0.00951680\n",
      "Epoch 3 | Step 1832400 | Avg Loss: 0.0154 | Grad Norm: 0.00834417\n",
      "Epoch 3 | Step 1832500 | Avg Loss: 0.0155 | Grad Norm: 0.00889687\n",
      "Epoch 3 | Step 1832600 | Avg Loss: 0.0159 | Grad Norm: 0.00954931\n",
      "Epoch 3 | Step 1832700 | Avg Loss: 0.0155 | Grad Norm: 0.00937034\n",
      "Epoch 3 | Step 1832800 | Avg Loss: 0.0158 | Grad Norm: 0.00996808\n",
      "Epoch 3 | Step 1832900 | Avg Loss: 0.0154 | Grad Norm: 0.00849788\n",
      "Epoch 3 | Step 1833000 | Avg Loss: 0.0155 | Grad Norm: 0.00988680\n",
      "Epoch 3 | Step 1833100 | Avg Loss: 0.0155 | Grad Norm: 0.01069364\n",
      "Epoch 3 | Step 1833200 | Avg Loss: 0.0156 | Grad Norm: 0.01226997\n",
      "Epoch 3 | Step 1833300 | Avg Loss: 0.0154 | Grad Norm: 0.00880329\n",
      "Epoch 3 | Step 1833400 | Avg Loss: 0.0148 | Grad Norm: 0.00961638\n",
      "Epoch 3 | Step 1833500 | Avg Loss: 0.0148 | Grad Norm: 0.00760358\n",
      "Epoch 3 | Step 1833600 | Avg Loss: 0.0150 | Grad Norm: 0.00834810\n",
      "Epoch 3 | Step 1833700 | Avg Loss: 0.0151 | Grad Norm: 0.00935692\n",
      "Epoch 3 | Step 1833800 | Avg Loss: 0.0152 | Grad Norm: 0.01220355\n",
      "Epoch 3 | Step 1833900 | Avg Loss: 0.0151 | Grad Norm: 0.01158840\n",
      "Epoch 3 | Step 1834000 | Avg Loss: 0.0155 | Grad Norm: 0.01004511\n",
      "Epoch 3 | Step 1834100 | Avg Loss: 0.0154 | Grad Norm: 0.00999303\n",
      "Epoch 3 | Step 1834200 | Avg Loss: 0.0152 | Grad Norm: 0.00800930\n",
      "Epoch 3 | Step 1834300 | Avg Loss: 0.0152 | Grad Norm: 0.00818706\n",
      "Epoch 3 | Step 1834400 | Avg Loss: 0.0153 | Grad Norm: 0.00958539\n",
      "Epoch 3 | Step 1834500 | Avg Loss: 0.0153 | Grad Norm: 0.00859071\n",
      "Epoch 3 | Step 1834600 | Avg Loss: 0.0152 | Grad Norm: 0.00934754\n",
      "Epoch 3 | Step 1834700 | Avg Loss: 0.0155 | Grad Norm: 0.01052461\n",
      "Epoch 3 | Step 1834800 | Avg Loss: 0.0161 | Grad Norm: 0.00863900\n",
      "Epoch 3 | Step 1834900 | Avg Loss: 0.0156 | Grad Norm: 0.00829962\n",
      "Epoch 3 | Step 1835000 | Avg Loss: 0.0155 | Grad Norm: 0.00886055\n",
      "Epoch 3 | Step 1835100 | Avg Loss: 0.0157 | Grad Norm: 0.00830906\n",
      "Epoch 3 | Step 1835200 | Avg Loss: 0.0160 | Grad Norm: 0.00973028\n",
      "Epoch 3 | Step 1835300 | Avg Loss: 0.0161 | Grad Norm: 0.00875324\n",
      "Epoch 3 | Step 1835400 | Avg Loss: 0.0155 | Grad Norm: 0.00817023\n",
      "Epoch 3 | Step 1835500 | Avg Loss: 0.0151 | Grad Norm: 0.00905401\n",
      "Epoch 3 | Step 1835600 | Avg Loss: 0.0149 | Grad Norm: 0.00816357\n",
      "Epoch 3 | Step 1835700 | Avg Loss: 0.0154 | Grad Norm: 0.00806492\n",
      "Epoch 3 | Step 1835800 | Avg Loss: 0.0154 | Grad Norm: 0.00788371\n",
      "Epoch 3 | Step 1835900 | Avg Loss: 0.0151 | Grad Norm: 0.00988586\n",
      "Epoch 3 | Step 1836000 | Avg Loss: 0.0152 | Grad Norm: 0.00914444\n",
      "Epoch 3 | Step 1836100 | Avg Loss: 0.0151 | Grad Norm: 0.00874275\n",
      "Epoch 3 | Step 1836200 | Avg Loss: 0.0154 | Grad Norm: 0.00771440\n",
      "Epoch 3 | Step 1836300 | Avg Loss: 0.0154 | Grad Norm: 0.00809479\n",
      "Epoch 3 | Step 1836400 | Avg Loss: 0.0155 | Grad Norm: 0.01047113\n",
      "Epoch 3 | Step 1836500 | Avg Loss: 0.0155 | Grad Norm: 0.00909343\n",
      "Epoch 3 | Step 1836600 | Avg Loss: 0.0154 | Grad Norm: 0.00973159\n",
      "Epoch 3 | Step 1836700 | Avg Loss: 0.0153 | Grad Norm: 0.00988654\n",
      "Epoch 3 | Step 1836800 | Avg Loss: 0.0153 | Grad Norm: 0.00797168\n",
      "Epoch 3 | Step 1836900 | Avg Loss: 0.0152 | Grad Norm: 0.00834610\n",
      "Epoch 3 | Step 1837000 | Avg Loss: 0.0150 | Grad Norm: 0.00866547\n",
      "Epoch 3 | Step 1837100 | Avg Loss: 0.0151 | Grad Norm: 0.00884376\n",
      "Epoch 3 | Step 1837200 | Avg Loss: 0.0148 | Grad Norm: 0.00950094\n",
      "Epoch 3 | Step 1837300 | Avg Loss: 0.0151 | Grad Norm: 0.00793043\n",
      "Epoch 3 | Step 1837400 | Avg Loss: 0.0156 | Grad Norm: 0.00928846\n",
      "Epoch 3 | Step 1837500 | Avg Loss: 0.0155 | Grad Norm: 0.01009004\n",
      "Epoch 3 | Step 1837600 | Avg Loss: 0.0159 | Grad Norm: 0.00898000\n",
      "Epoch 3 | Step 1837700 | Avg Loss: 0.0158 | Grad Norm: 0.00962746\n",
      "Epoch 3 | Step 1837800 | Avg Loss: 0.0157 | Grad Norm: 0.00829877\n",
      "Epoch 3 | Step 1837900 | Avg Loss: 0.0155 | Grad Norm: 0.00823441\n",
      "Epoch 3 | Step 1838000 | Avg Loss: 0.0152 | Grad Norm: 0.00896880\n",
      "Epoch 3 | Step 1838100 | Avg Loss: 0.0153 | Grad Norm: 0.01032135\n",
      "Epoch 3 | Step 1838200 | Avg Loss: 0.0153 | Grad Norm: 0.00851393\n",
      "Epoch 3 | Step 1838300 | Avg Loss: 0.0151 | Grad Norm: 0.00906573\n",
      "Epoch 3 | Step 1838400 | Avg Loss: 0.0156 | Grad Norm: 0.01011074\n",
      "Epoch 3 | Step 1838500 | Avg Loss: 0.0157 | Grad Norm: 0.00897279\n",
      "Epoch 3 | Step 1838600 | Avg Loss: 0.0159 | Grad Norm: 0.00944338\n",
      "Epoch 3 | Step 1838700 | Avg Loss: 0.0157 | Grad Norm: 0.00856315\n",
      "Epoch 3 | Step 1838800 | Avg Loss: 0.0158 | Grad Norm: 0.00811700\n",
      "Epoch 3 | Step 1838900 | Avg Loss: 0.0162 | Grad Norm: 0.00869933\n",
      "Epoch 3 | Step 1839000 | Avg Loss: 0.0159 | Grad Norm: 0.00902555\n",
      "Epoch 3 | Step 1839100 | Avg Loss: 0.0153 | Grad Norm: 0.00909486\n",
      "Epoch 3 | Step 1839200 | Avg Loss: 0.0150 | Grad Norm: 0.00861108\n",
      "Epoch 3 | Step 1839300 | Avg Loss: 0.0152 | Grad Norm: 0.00888292\n",
      "Epoch 3 | Step 1839400 | Avg Loss: 0.0157 | Grad Norm: 0.01269792\n",
      "Epoch 3 | Step 1839500 | Avg Loss: 0.0156 | Grad Norm: 0.00937653\n",
      "Epoch 3 | Step 1839600 | Avg Loss: 0.0153 | Grad Norm: 0.00932800\n",
      "Epoch 3 | Step 1839700 | Avg Loss: 0.0151 | Grad Norm: 0.00755499\n",
      "Epoch 3 | Step 1839800 | Avg Loss: 0.0151 | Grad Norm: 0.00921974\n",
      "Epoch 3 | Step 1839900 | Avg Loss: 0.0155 | Grad Norm: 0.00825582\n",
      "Epoch 3 | Step 1840000 | Avg Loss: 0.0155 | Grad Norm: 0.00800542\n",
      "Epoch 3 | Step 1840100 | Avg Loss: 0.0154 | Grad Norm: 0.00756414\n",
      "Epoch 3 | Step 1840200 | Avg Loss: 0.0150 | Grad Norm: 0.00804281\n",
      "Epoch 3 | Step 1840300 | Avg Loss: 0.0152 | Grad Norm: 0.00837699\n",
      "Epoch 3 | Step 1840400 | Avg Loss: 0.0152 | Grad Norm: 0.00844971\n",
      "Epoch 3 | Step 1840500 | Avg Loss: 0.0154 | Grad Norm: 0.00838073\n",
      "Epoch 3 | Step 1840600 | Avg Loss: 0.0154 | Grad Norm: 0.00870909\n",
      "Epoch 3 | Step 1840700 | Avg Loss: 0.0155 | Grad Norm: 0.00911502\n",
      "Epoch 3 | Step 1840800 | Avg Loss: 0.0155 | Grad Norm: 0.00910194\n",
      "Epoch 3 | Step 1840900 | Avg Loss: 0.0155 | Grad Norm: 0.00865835\n",
      "Epoch 3 | Step 1841000 | Avg Loss: 0.0152 | Grad Norm: 0.00877107\n",
      "Epoch 3 | Step 1841100 | Avg Loss: 0.0152 | Grad Norm: 0.00793236\n",
      "Epoch 3 | Step 1841200 | Avg Loss: 0.0152 | Grad Norm: 0.00960526\n",
      "Epoch 3 | Step 1841300 | Avg Loss: 0.0153 | Grad Norm: 0.00992568\n",
      "Epoch 3 | Step 1841400 | Avg Loss: 0.0157 | Grad Norm: 0.00881915\n",
      "Epoch 3 | Step 1841500 | Avg Loss: 0.0156 | Grad Norm: 0.00933669\n",
      "Epoch 3 | Step 1841600 | Avg Loss: 0.0157 | Grad Norm: 0.00731944\n",
      "Epoch 3 | Step 1841700 | Avg Loss: 0.0155 | Grad Norm: 0.00762714\n",
      "Epoch 3 | Step 1841800 | Avg Loss: 0.0156 | Grad Norm: 0.00862089\n",
      "Epoch 3 | Step 1841900 | Avg Loss: 0.0156 | Grad Norm: 0.00888363\n",
      "Epoch 3 | Step 1842000 | Avg Loss: 0.0160 | Grad Norm: 0.00920705\n",
      "Epoch 3 | Step 1842100 | Avg Loss: 0.0161 | Grad Norm: 0.00798374\n",
      "Epoch 3 | Step 1842200 | Avg Loss: 0.0160 | Grad Norm: 0.01060021\n",
      "Epoch 3 | Step 1842300 | Avg Loss: 0.0157 | Grad Norm: 0.00822092\n",
      "Epoch 3 | Step 1842400 | Avg Loss: 0.0160 | Grad Norm: 0.00903175\n",
      "Epoch 3 | Step 1842500 | Avg Loss: 0.0162 | Grad Norm: 0.00909420\n",
      "Epoch 3 | Step 1842600 | Avg Loss: 0.0157 | Grad Norm: 0.01019572\n",
      "Epoch 3 | Step 1842700 | Avg Loss: 0.0155 | Grad Norm: 0.00995872\n",
      "Epoch 3 | Step 1842800 | Avg Loss: 0.0157 | Grad Norm: 0.00797287\n",
      "Epoch 3 | Step 1842900 | Avg Loss: 0.0159 | Grad Norm: 0.01016734\n",
      "Epoch 3 | Step 1843000 | Avg Loss: 0.0156 | Grad Norm: 0.00905120\n",
      "Epoch 3 | Step 1843100 | Avg Loss: 0.0157 | Grad Norm: 0.00872738\n",
      "Epoch 3 | Step 1843200 | Avg Loss: 0.0158 | Grad Norm: 0.00922898\n",
      "Epoch 3 | Step 1843300 | Avg Loss: 0.0155 | Grad Norm: 0.01006968\n",
      "Epoch 3 | Step 1843400 | Avg Loss: 0.0154 | Grad Norm: 0.00823621\n",
      "Epoch 3 | Step 1843500 | Avg Loss: 0.0157 | Grad Norm: 0.00869576\n",
      "Epoch 3 | Step 1843600 | Avg Loss: 0.0158 | Grad Norm: 0.00851166\n",
      "Epoch 3 | Step 1843700 | Avg Loss: 0.0158 | Grad Norm: 0.00927438\n",
      "Epoch 3 | Step 1843800 | Avg Loss: 0.0158 | Grad Norm: 0.00935879\n",
      "Epoch 3 | Step 1843900 | Avg Loss: 0.0160 | Grad Norm: 0.00958815\n",
      "Epoch 3 | Step 1844000 | Avg Loss: 0.0160 | Grad Norm: 0.00966278\n",
      "Epoch 3 | Step 1844100 | Avg Loss: 0.0159 | Grad Norm: 0.00919522\n",
      "Epoch 3 | Step 1844200 | Avg Loss: 0.0157 | Grad Norm: 0.00917535\n",
      "Epoch 3 | Step 1844300 | Avg Loss: 0.0157 | Grad Norm: 0.00777096\n",
      "Epoch 3 | Step 1844400 | Avg Loss: 0.0158 | Grad Norm: 0.01055926\n",
      "Epoch 3 | Step 1844500 | Avg Loss: 0.0158 | Grad Norm: 0.00820951\n",
      "Epoch 3 | Step 1844600 | Avg Loss: 0.0154 | Grad Norm: 0.01018435\n",
      "Epoch 3 | Step 1844700 | Avg Loss: 0.0153 | Grad Norm: 0.00925537\n",
      "Epoch 3 | Step 1844800 | Avg Loss: 0.0157 | Grad Norm: 0.00821706\n",
      "Epoch 3 | Step 1844900 | Avg Loss: 0.0156 | Grad Norm: 0.00881969\n",
      "Epoch 3 | Step 1845000 | Avg Loss: 0.0154 | Grad Norm: 0.00972747\n",
      "Epoch 3 | Step 1845100 | Avg Loss: 0.0154 | Grad Norm: 0.00888789\n",
      "Epoch 3 | Step 1845200 | Avg Loss: 0.0154 | Grad Norm: 0.01002903\n",
      "Epoch 3 | Step 1845300 | Avg Loss: 0.0152 | Grad Norm: 0.00815984\n",
      "Epoch 3 | Step 1845400 | Avg Loss: 0.0155 | Grad Norm: 0.00836019\n",
      "Epoch 3 | Step 1845500 | Avg Loss: 0.0159 | Grad Norm: 0.00839589\n",
      "Epoch 3 | Step 1845600 | Avg Loss: 0.0157 | Grad Norm: 0.00876394\n",
      "Epoch 3 | Step 1845700 | Avg Loss: 0.0155 | Grad Norm: 0.01047209\n",
      "Epoch 3 | Step 1845800 | Avg Loss: 0.0159 | Grad Norm: 0.00930993\n",
      "Epoch 3 | Step 1845900 | Avg Loss: 0.0156 | Grad Norm: 0.00905322\n",
      "Epoch 3 | Step 1846000 | Avg Loss: 0.0156 | Grad Norm: 0.00840106\n",
      "Epoch 3 | Step 1846100 | Avg Loss: 0.0156 | Grad Norm: 0.00973425\n",
      "Epoch 3 | Step 1846200 | Avg Loss: 0.0154 | Grad Norm: 0.00834535\n",
      "Epoch 3 | Step 1846300 | Avg Loss: 0.0155 | Grad Norm: 0.00866477\n",
      "Epoch 3 | Step 1846400 | Avg Loss: 0.0153 | Grad Norm: 0.01084133\n",
      "Epoch 3 | Step 1846500 | Avg Loss: 0.0152 | Grad Norm: 0.00854251\n",
      "Epoch 3 | Step 1846600 | Avg Loss: 0.0153 | Grad Norm: 0.00727705\n",
      "Epoch 3 | Step 1846700 | Avg Loss: 0.0153 | Grad Norm: 0.00954695\n",
      "Epoch 3 | Step 1846800 | Avg Loss: 0.0155 | Grad Norm: 0.00824544\n",
      "Epoch 3 | Step 1846900 | Avg Loss: 0.0152 | Grad Norm: 0.00989309\n",
      "Epoch 3 | Step 1847000 | Avg Loss: 0.0153 | Grad Norm: 0.00982390\n",
      "Epoch 3 | Step 1847100 | Avg Loss: 0.0159 | Grad Norm: 0.00878185\n",
      "Epoch 3 | Step 1847200 | Avg Loss: 0.0161 | Grad Norm: 0.01038763\n",
      "Epoch 3 | Step 1847300 | Avg Loss: 0.0160 | Grad Norm: 0.00970162\n",
      "Epoch 3 | Step 1847400 | Avg Loss: 0.0158 | Grad Norm: 0.01099750\n",
      "Epoch 3 | Step 1847500 | Avg Loss: 0.0163 | Grad Norm: 0.01010157\n",
      "Epoch 3 | Step 1847600 | Avg Loss: 0.0161 | Grad Norm: 0.00990654\n",
      "Epoch 3 | Step 1847700 | Avg Loss: 0.0160 | Grad Norm: 0.00973882\n",
      "Epoch 3 | Step 1847800 | Avg Loss: 0.0153 | Grad Norm: 0.00862481\n",
      "Epoch 3 | Step 1847900 | Avg Loss: 0.0154 | Grad Norm: 0.00819446\n",
      "Epoch 3 | Step 1848000 | Avg Loss: 0.0152 | Grad Norm: 0.00856504\n",
      "Epoch 3 | Step 1848100 | Avg Loss: 0.0153 | Grad Norm: 0.00946987\n",
      "Epoch 3 | Step 1848200 | Avg Loss: 0.0150 | Grad Norm: 0.00811979\n",
      "Epoch 3 | Step 1848300 | Avg Loss: 0.0149 | Grad Norm: 0.00798564\n",
      "Epoch 3 | Step 1848400 | Avg Loss: 0.0150 | Grad Norm: 0.01146162\n",
      "Epoch 3 | Step 1848500 | Avg Loss: 0.0151 | Grad Norm: 0.00920178\n",
      "Epoch 3 | Step 1848600 | Avg Loss: 0.0151 | Grad Norm: 0.00869096\n",
      "Epoch 3 | Step 1848700 | Avg Loss: 0.0153 | Grad Norm: 0.00842943\n",
      "Epoch 3 | Step 1848800 | Avg Loss: 0.0152 | Grad Norm: 0.00855475\n",
      "Epoch 3 | Step 1848900 | Avg Loss: 0.0154 | Grad Norm: 0.00950563\n",
      "Epoch 3 | Step 1849000 | Avg Loss: 0.0152 | Grad Norm: 0.00832897\n",
      "Epoch 3 | Step 1849100 | Avg Loss: 0.0151 | Grad Norm: 0.00920807\n",
      "Epoch 3 | Step 1849200 | Avg Loss: 0.0153 | Grad Norm: 0.00879253\n",
      "Epoch 3 | Step 1849300 | Avg Loss: 0.0157 | Grad Norm: 0.00931999\n",
      "Epoch 3 | Step 1849400 | Avg Loss: 0.0163 | Grad Norm: 0.00870024\n",
      "Epoch 3 | Step 1849500 | Avg Loss: 0.0161 | Grad Norm: 0.00911443\n",
      "Epoch 3 | Step 1849600 | Avg Loss: 0.0160 | Grad Norm: 0.00946850\n",
      "Epoch 3 | Step 1849700 | Avg Loss: 0.0166 | Grad Norm: 0.01020388\n",
      "Epoch 3 | Step 1849800 | Avg Loss: 0.0162 | Grad Norm: 0.00954412\n",
      "Epoch 3 | Step 1849900 | Avg Loss: 0.0156 | Grad Norm: 0.00854990\n",
      "Epoch 3 | Step 1850000 | Avg Loss: 0.0161 | Grad Norm: 0.00856844\n",
      "Epoch 3 | Step 1850100 | Avg Loss: 0.0163 | Grad Norm: 0.01016547\n",
      "Epoch 3 | Step 1850200 | Avg Loss: 0.0159 | Grad Norm: 0.00846764\n",
      "Epoch 3 | Step 1850300 | Avg Loss: 0.0158 | Grad Norm: 0.00738106\n",
      "Epoch 3 | Step 1850400 | Avg Loss: 0.0158 | Grad Norm: 0.00955842\n",
      "Epoch 3 | Step 1850500 | Avg Loss: 0.0156 | Grad Norm: 0.01117848\n",
      "Epoch 3 | Step 1850600 | Avg Loss: 0.0152 | Grad Norm: 0.00860318\n",
      "Epoch 3 | Step 1850700 | Avg Loss: 0.0152 | Grad Norm: 0.01224672\n",
      "Epoch 3 | Step 1850800 | Avg Loss: 0.0151 | Grad Norm: 0.00892018\n",
      "Epoch 3 | Step 1850900 | Avg Loss: 0.0155 | Grad Norm: 0.00943081\n",
      "Epoch 3 | Step 1851000 | Avg Loss: 0.0153 | Grad Norm: 0.00970135\n",
      "Epoch 3 | Step 1851100 | Avg Loss: 0.0157 | Grad Norm: 0.00865434\n",
      "Epoch 3 | Step 1851200 | Avg Loss: 0.0158 | Grad Norm: 0.00817308\n",
      "Epoch 3 | Step 1851300 | Avg Loss: 0.0157 | Grad Norm: 0.01202355\n",
      "Epoch 3 | Step 1851400 | Avg Loss: 0.0155 | Grad Norm: 0.01016772\n",
      "Epoch 3 | Step 1851500 | Avg Loss: 0.0157 | Grad Norm: 0.00895756\n",
      "Epoch 3 | Step 1851600 | Avg Loss: 0.0162 | Grad Norm: 0.00948666\n",
      "Epoch 3 | Step 1851700 | Avg Loss: 0.0163 | Grad Norm: 0.01037839\n",
      "Epoch 3 | Step 1851800 | Avg Loss: 0.0160 | Grad Norm: 0.00943778\n",
      "Epoch 3 | Step 1851900 | Avg Loss: 0.0155 | Grad Norm: 0.00896898\n",
      "Epoch 3 | Step 1852000 | Avg Loss: 0.0152 | Grad Norm: 0.00890936\n",
      "Epoch 3 | Step 1852100 | Avg Loss: 0.0155 | Grad Norm: 0.00745107\n",
      "Epoch 3 | Step 1852200 | Avg Loss: 0.0155 | Grad Norm: 0.00895897\n",
      "Epoch 3 | Step 1852300 | Avg Loss: 0.0159 | Grad Norm: 0.00881422\n",
      "Epoch 3 | Step 1852400 | Avg Loss: 0.0157 | Grad Norm: 0.00898935\n",
      "Epoch 3 | Step 1852500 | Avg Loss: 0.0159 | Grad Norm: 0.00914853\n",
      "Epoch 3 | Step 1852600 | Avg Loss: 0.0160 | Grad Norm: 0.00796065\n",
      "Epoch 3 | Step 1852700 | Avg Loss: 0.0157 | Grad Norm: 0.00934772\n",
      "Epoch 3 | Step 1852800 | Avg Loss: 0.0155 | Grad Norm: 0.00947195\n",
      "Epoch 3 | Step 1852900 | Avg Loss: 0.0153 | Grad Norm: 0.01062557\n",
      "Epoch 3 | Step 1853000 | Avg Loss: 0.0152 | Grad Norm: 0.01399855\n",
      "Epoch 3 | Step 1853100 | Avg Loss: 0.0151 | Grad Norm: 0.01014295\n",
      "Epoch 3 | Step 1853200 | Avg Loss: 0.0150 | Grad Norm: 0.00845346\n",
      "Epoch 3 | Step 1853300 | Avg Loss: 0.0152 | Grad Norm: 0.00859337\n",
      "Epoch 3 | Step 1853400 | Avg Loss: 0.0151 | Grad Norm: 0.00893202\n",
      "Epoch 3 | Step 1853500 | Avg Loss: 0.0157 | Grad Norm: 0.00808690\n",
      "Epoch 3 | Step 1853600 | Avg Loss: 0.0153 | Grad Norm: 0.00972314\n",
      "Epoch 3 | Step 1853700 | Avg Loss: 0.0157 | Grad Norm: 0.00801348\n",
      "Epoch 3 | Step 1853800 | Avg Loss: 0.0158 | Grad Norm: 0.00802534\n",
      "Epoch 3 | Step 1853900 | Avg Loss: 0.0160 | Grad Norm: 0.01124554\n",
      "Epoch 3 | Step 1854000 | Avg Loss: 0.0158 | Grad Norm: 0.00838157\n",
      "Epoch 3 | Step 1854100 | Avg Loss: 0.0154 | Grad Norm: 0.01517127\n",
      "Epoch 3 | Step 1854200 | Avg Loss: 0.0158 | Grad Norm: 0.00854500\n",
      "Epoch 3 | Step 1854300 | Avg Loss: 0.0160 | Grad Norm: 0.00733664\n",
      "Epoch 3 | Step 1854400 | Avg Loss: 0.0161 | Grad Norm: 0.01155738\n",
      "Epoch 3 | Step 1854500 | Avg Loss: 0.0161 | Grad Norm: 0.00788507\n",
      "Epoch 3 | Step 1854600 | Avg Loss: 0.0162 | Grad Norm: 0.01017352\n",
      "Epoch 3 | Step 1854700 | Avg Loss: 0.0158 | Grad Norm: 0.00944724\n",
      "Epoch 3 | Step 1854800 | Avg Loss: 0.0156 | Grad Norm: 0.01110146\n",
      "Epoch 3 | Step 1854900 | Avg Loss: 0.0154 | Grad Norm: 0.00799464\n",
      "Epoch 3 | Step 1855000 | Avg Loss: 0.0154 | Grad Norm: 0.00851077\n",
      "Epoch 3 | Step 1855100 | Avg Loss: 0.0153 | Grad Norm: 0.00930902\n",
      "Epoch 3 | Step 1855200 | Avg Loss: 0.0151 | Grad Norm: 0.00932689\n",
      "Epoch 3 | Step 1855300 | Avg Loss: 0.0156 | Grad Norm: 0.01014580\n",
      "Epoch 3 | Step 1855400 | Avg Loss: 0.0160 | Grad Norm: 0.00851590\n",
      "Epoch 3 | Step 1855500 | Avg Loss: 0.0161 | Grad Norm: 0.01028178\n",
      "Epoch 3 | Step 1855600 | Avg Loss: 0.0159 | Grad Norm: 0.00958449\n",
      "Epoch 3 | Step 1855700 | Avg Loss: 0.0161 | Grad Norm: 0.00804930\n",
      "Epoch 3 | Step 1855800 | Avg Loss: 0.0157 | Grad Norm: 0.00907879\n",
      "Epoch 3 | Step 1855900 | Avg Loss: 0.0159 | Grad Norm: 0.01039769\n",
      "Epoch 3 | Step 1856000 | Avg Loss: 0.0157 | Grad Norm: 0.00976001\n",
      "Epoch 3 | Step 1856100 | Avg Loss: 0.0156 | Grad Norm: 0.00892256\n",
      "Epoch 3 | Step 1856200 | Avg Loss: 0.0158 | Grad Norm: 0.01012067\n",
      "Epoch 3 | Step 1856300 | Avg Loss: 0.0154 | Grad Norm: 0.00957709\n",
      "Epoch 3 | Step 1856400 | Avg Loss: 0.0155 | Grad Norm: 0.00874819\n",
      "Epoch 3 | Step 1856500 | Avg Loss: 0.0150 | Grad Norm: 0.00758745\n",
      "Epoch 3 | Step 1856600 | Avg Loss: 0.0150 | Grad Norm: 0.00842202\n",
      "Epoch 3 | Step 1856700 | Avg Loss: 0.0153 | Grad Norm: 0.00844760\n",
      "Epoch 3 | Step 1856800 | Avg Loss: 0.0157 | Grad Norm: 0.01034362\n",
      "Epoch 3 | Step 1856900 | Avg Loss: 0.0155 | Grad Norm: 0.00950741\n",
      "Epoch 3 | Step 1857000 | Avg Loss: 0.0158 | Grad Norm: 0.00873689\n",
      "Epoch 3 | Step 1857100 | Avg Loss: 0.0159 | Grad Norm: 0.00814510\n",
      "Epoch 3 | Step 1857200 | Avg Loss: 0.0162 | Grad Norm: 0.00907568\n",
      "Epoch 3 | Step 1857300 | Avg Loss: 0.0161 | Grad Norm: 0.00779656\n",
      "Epoch 3 | Step 1857400 | Avg Loss: 0.0161 | Grad Norm: 0.00958374\n",
      "Epoch 3 | Step 1857500 | Avg Loss: 0.0159 | Grad Norm: 0.00865754\n",
      "Epoch 3 | Step 1857600 | Avg Loss: 0.0158 | Grad Norm: 0.01093452\n",
      "Epoch 3 | Step 1857700 | Avg Loss: 0.0156 | Grad Norm: 0.01073348\n",
      "Epoch 3 | Step 1857800 | Avg Loss: 0.0153 | Grad Norm: 0.00834830\n",
      "Epoch 3 | Step 1857900 | Avg Loss: 0.0156 | Grad Norm: 0.00901282\n",
      "Epoch 3 | Step 1858000 | Avg Loss: 0.0155 | Grad Norm: 0.01055171\n",
      "Epoch 3 | Step 1858100 | Avg Loss: 0.0159 | Grad Norm: 0.00888643\n",
      "Epoch 3 | Step 1858200 | Avg Loss: 0.0155 | Grad Norm: 0.01116148\n",
      "Epoch 3 | Step 1858300 | Avg Loss: 0.0153 | Grad Norm: 0.00781813\n",
      "Epoch 3 | Step 1858400 | Avg Loss: 0.0158 | Grad Norm: 0.00866158\n",
      "Epoch 3 | Step 1858500 | Avg Loss: 0.0160 | Grad Norm: 0.00983050\n",
      "Epoch 3 | Step 1858600 | Avg Loss: 0.0159 | Grad Norm: 0.01019393\n",
      "Epoch 3 | Step 1858700 | Avg Loss: 0.0161 | Grad Norm: 0.01098067\n",
      "Epoch 3 | Step 1858800 | Avg Loss: 0.0163 | Grad Norm: 0.01041947\n",
      "Epoch 3 | Step 1858900 | Avg Loss: 0.0160 | Grad Norm: 0.00924018\n",
      "Epoch 3 | Step 1859000 | Avg Loss: 0.0156 | Grad Norm: 0.00864758\n",
      "Epoch 3 | Step 1859100 | Avg Loss: 0.0156 | Grad Norm: 0.00908475\n",
      "Epoch 3 | Step 1859200 | Avg Loss: 0.0157 | Grad Norm: 0.00922708\n",
      "Epoch 3 | Step 1859300 | Avg Loss: 0.0155 | Grad Norm: 0.00803821\n",
      "Epoch 3 | Step 1859400 | Avg Loss: 0.0154 | Grad Norm: 0.00922291\n",
      "Epoch 3 | Step 1859500 | Avg Loss: 0.0157 | Grad Norm: 0.01075124\n",
      "Epoch 3 | Step 1859600 | Avg Loss: 0.0161 | Grad Norm: 0.01162371\n",
      "Epoch 3 | Step 1859700 | Avg Loss: 0.0159 | Grad Norm: 0.01522970\n",
      "Epoch 3 | Step 1859800 | Avg Loss: 0.0158 | Grad Norm: 0.00799845\n",
      "Epoch 3 | Step 1859900 | Avg Loss: 0.0158 | Grad Norm: 0.00856346\n",
      "Epoch 3 | Step 1860000 | Avg Loss: 0.0162 | Grad Norm: 0.00887337\n",
      "Epoch 3 | Step 1860100 | Avg Loss: 0.0164 | Grad Norm: 0.01268770\n",
      "Epoch 3 | Step 1860200 | Avg Loss: 0.0164 | Grad Norm: 0.00960981\n",
      "Epoch 3 | Step 1860300 | Avg Loss: 0.0165 | Grad Norm: 0.00970362\n",
      "Epoch 3 | Step 1860400 | Avg Loss: 0.0165 | Grad Norm: 0.00900517\n",
      "Epoch 3 | Step 1860500 | Avg Loss: 0.0164 | Grad Norm: 0.01014797\n",
      "Epoch 3 | Step 1860600 | Avg Loss: 0.0161 | Grad Norm: 0.00875069\n",
      "Epoch 3 | Step 1860700 | Avg Loss: 0.0154 | Grad Norm: 0.00963014\n",
      "Epoch 3 | Step 1860800 | Avg Loss: 0.0158 | Grad Norm: 0.01106146\n",
      "Epoch 3 | Step 1860900 | Avg Loss: 0.0156 | Grad Norm: 0.00819530\n",
      "Epoch 3 | Step 1861000 | Avg Loss: 0.0154 | Grad Norm: 0.00806945\n",
      "Epoch 3 | Step 1861100 | Avg Loss: 0.0157 | Grad Norm: 0.01025931\n",
      "Epoch 3 | Step 1861200 | Avg Loss: 0.0157 | Grad Norm: 0.00813479\n",
      "Epoch 3 | Step 1861300 | Avg Loss: 0.0157 | Grad Norm: 0.00871938\n",
      "Epoch 3 | Step 1861400 | Avg Loss: 0.0155 | Grad Norm: 0.00944424\n",
      "Epoch 3 | Step 1861500 | Avg Loss: 0.0152 | Grad Norm: 0.01033718\n",
      "Epoch 3 | Step 1861600 | Avg Loss: 0.0154 | Grad Norm: 0.00892860\n",
      "Epoch 3 | Step 1861700 | Avg Loss: 0.0152 | Grad Norm: 0.00911984\n",
      "Epoch 3 | Step 1861800 | Avg Loss: 0.0153 | Grad Norm: 0.00976718\n",
      "Epoch 3 | Step 1861900 | Avg Loss: 0.0156 | Grad Norm: 0.00999925\n",
      "Epoch 3 | Step 1862000 | Avg Loss: 0.0154 | Grad Norm: 0.00894548\n",
      "Epoch 3 | Step 1862100 | Avg Loss: 0.0154 | Grad Norm: 0.00898700\n",
      "Epoch 3 | Step 1862200 | Avg Loss: 0.0152 | Grad Norm: 0.00951474\n",
      "Epoch 3 | Step 1862300 | Avg Loss: 0.0152 | Grad Norm: 0.00800926\n",
      "Epoch 3 | Step 1862400 | Avg Loss: 0.0152 | Grad Norm: 0.01173101\n",
      "Epoch 3 | Step 1862500 | Avg Loss: 0.0150 | Grad Norm: 0.00854903\n",
      "Epoch 3 | Step 1862600 | Avg Loss: 0.0151 | Grad Norm: 0.00759213\n",
      "Epoch 3 | Step 1862700 | Avg Loss: 0.0153 | Grad Norm: 0.00749334\n",
      "Epoch 3 | Step 1862800 | Avg Loss: 0.0151 | Grad Norm: 0.00859382\n",
      "Epoch 3 | Step 1862900 | Avg Loss: 0.0148 | Grad Norm: 0.00785160\n",
      "Epoch 3 | Step 1863000 | Avg Loss: 0.0148 | Grad Norm: 0.00855877\n",
      "Epoch 3 | Step 1863100 | Avg Loss: 0.0149 | Grad Norm: 0.00895123\n",
      "Epoch 3 | Step 1863200 | Avg Loss: 0.0148 | Grad Norm: 0.00910005\n",
      "Epoch 3 | Step 1863300 | Avg Loss: 0.0150 | Grad Norm: 0.00838265\n",
      "Epoch 3 | Step 1863400 | Avg Loss: 0.0151 | Grad Norm: 0.00878343\n",
      "Epoch 3 | Step 1863500 | Avg Loss: 0.0151 | Grad Norm: 0.00906826\n",
      "Epoch 3 | Step 1863600 | Avg Loss: 0.0151 | Grad Norm: 0.00869787\n",
      "Epoch 3 | Step 1863700 | Avg Loss: 0.0152 | Grad Norm: 0.00893248\n",
      "Epoch 3 | Step 1863800 | Avg Loss: 0.0153 | Grad Norm: 0.00932895\n",
      "Epoch 3 | Step 1863900 | Avg Loss: 0.0154 | Grad Norm: 0.00849079\n",
      "Epoch 3 | Step 1864000 | Avg Loss: 0.0156 | Grad Norm: 0.00879058\n",
      "Epoch 3 | Step 1864100 | Avg Loss: 0.0154 | Grad Norm: 0.00836262\n",
      "Epoch 3 | Step 1864200 | Avg Loss: 0.0157 | Grad Norm: 0.00987544\n",
      "Epoch 3 | Step 1864300 | Avg Loss: 0.0157 | Grad Norm: 0.00885837\n",
      "Epoch 3 | Step 1864400 | Avg Loss: 0.0153 | Grad Norm: 0.00872955\n",
      "Epoch 3 | Step 1864500 | Avg Loss: 0.0154 | Grad Norm: 0.00878019\n",
      "Epoch 3 | Step 1864600 | Avg Loss: 0.0150 | Grad Norm: 0.01022481\n",
      "Epoch 3 | Step 1864700 | Avg Loss: 0.0153 | Grad Norm: 0.01072177\n",
      "Epoch 3 | Step 1864800 | Avg Loss: 0.0150 | Grad Norm: 0.00793428\n",
      "Epoch 3 | Step 1864900 | Avg Loss: 0.0151 | Grad Norm: 0.00858442\n",
      "Epoch 3 | Step 1865000 | Avg Loss: 0.0150 | Grad Norm: 0.00986691\n",
      "Epoch 3 | Step 1865100 | Avg Loss: 0.0151 | Grad Norm: 0.00810075\n",
      "Epoch 3 | Step 1865200 | Avg Loss: 0.0151 | Grad Norm: 0.00843548\n",
      "Epoch 3 | Step 1865300 | Avg Loss: 0.0152 | Grad Norm: 0.00841085\n",
      "Epoch 3 | Step 1865400 | Avg Loss: 0.0153 | Grad Norm: 0.00924783\n",
      "Epoch 3 | Step 1865500 | Avg Loss: 0.0152 | Grad Norm: 0.00992939\n",
      "Epoch 3 | Step 1865600 | Avg Loss: 0.0158 | Grad Norm: 0.00875172\n",
      "Epoch 3 | Step 1865700 | Avg Loss: 0.0157 | Grad Norm: 0.00831724\n",
      "Epoch 3 | Step 1865800 | Avg Loss: 0.0157 | Grad Norm: 0.00789547\n",
      "Epoch 3 | Step 1865900 | Avg Loss: 0.0155 | Grad Norm: 0.00869882\n",
      "Epoch 3 | Step 1866000 | Avg Loss: 0.0152 | Grad Norm: 0.00864046\n",
      "Epoch 3 | Step 1866100 | Avg Loss: 0.0153 | Grad Norm: 0.00997217\n",
      "Epoch 3 | Step 1866200 | Avg Loss: 0.0154 | Grad Norm: 0.00903703\n",
      "Epoch 3 | Step 1866300 | Avg Loss: 0.0152 | Grad Norm: 0.00846111\n",
      "Epoch 3 | Step 1866400 | Avg Loss: 0.0153 | Grad Norm: 0.00965792\n",
      "Epoch 3 | Step 1866500 | Avg Loss: 0.0156 | Grad Norm: 0.00858587\n",
      "Epoch 3 | Step 1866600 | Avg Loss: 0.0155 | Grad Norm: 0.00773751\n",
      "Epoch 3 | Step 1866700 | Avg Loss: 0.0155 | Grad Norm: 0.00886158\n",
      "Epoch 3 | Step 1866800 | Avg Loss: 0.0156 | Grad Norm: 0.00869148\n",
      "Epoch 3 | Step 1866900 | Avg Loss: 0.0153 | Grad Norm: 0.00883014\n",
      "Epoch 3 | Step 1867000 | Avg Loss: 0.0151 | Grad Norm: 0.00833993\n",
      "Epoch 3 | Step 1867100 | Avg Loss: 0.0154 | Grad Norm: 0.00872560\n",
      "Epoch 3 | Step 1867200 | Avg Loss: 0.0153 | Grad Norm: 0.00972701\n",
      "Epoch 3 | Step 1867300 | Avg Loss: 0.0153 | Grad Norm: 0.00921499\n",
      "Epoch 3 | Step 1867400 | Avg Loss: 0.0150 | Grad Norm: 0.00905978\n",
      "Epoch 3 | Step 1867500 | Avg Loss: 0.0155 | Grad Norm: 0.00884851\n",
      "Epoch 3 | Step 1867600 | Avg Loss: 0.0153 | Grad Norm: 0.00884857\n",
      "Epoch 3 | Step 1867700 | Avg Loss: 0.0156 | Grad Norm: 0.00963379\n",
      "Epoch 3 | Step 1867800 | Avg Loss: 0.0152 | Grad Norm: 0.00857289\n",
      "Epoch 3 | Step 1867900 | Avg Loss: 0.0151 | Grad Norm: 0.00848605\n",
      "Epoch 3 | Step 1868000 | Avg Loss: 0.0155 | Grad Norm: 0.00748536\n",
      "Epoch 3 | Step 1868100 | Avg Loss: 0.0159 | Grad Norm: 0.01019713\n",
      "Epoch 3 | Step 1868200 | Avg Loss: 0.0154 | Grad Norm: 0.00940997\n",
      "Epoch 3 | Step 1868300 | Avg Loss: 0.0156 | Grad Norm: 0.00955451\n",
      "Epoch 3 | Step 1868400 | Avg Loss: 0.0154 | Grad Norm: 0.00946102\n",
      "Epoch 3 | Step 1868500 | Avg Loss: 0.0155 | Grad Norm: 0.00851018\n",
      "Epoch 3 | Step 1868600 | Avg Loss: 0.0156 | Grad Norm: 0.00843810\n",
      "Epoch 3 | Step 1868700 | Avg Loss: 0.0152 | Grad Norm: 0.00847200\n",
      "Epoch 3 | Step 1868800 | Avg Loss: 0.0150 | Grad Norm: 0.00927411\n",
      "Epoch 3 | Step 1868900 | Avg Loss: 0.0155 | Grad Norm: 0.00807537\n",
      "Epoch 3 | Step 1869000 | Avg Loss: 0.0154 | Grad Norm: 0.00922392\n",
      "Epoch 3 | Step 1869100 | Avg Loss: 0.0154 | Grad Norm: 0.00858851\n",
      "Epoch 3 | Step 1869200 | Avg Loss: 0.0156 | Grad Norm: 0.00822866\n",
      "Epoch 3 | Step 1869300 | Avg Loss: 0.0152 | Grad Norm: 0.00837075\n",
      "Epoch 3 | Step 1869400 | Avg Loss: 0.0154 | Grad Norm: 0.00860389\n",
      "Epoch 3 | Step 1869500 | Avg Loss: 0.0157 | Grad Norm: 0.00856273\n",
      "Epoch 3 | Step 1869600 | Avg Loss: 0.0157 | Grad Norm: 0.00881183\n",
      "Epoch 3 | Step 1869700 | Avg Loss: 0.0157 | Grad Norm: 0.01040546\n",
      "Epoch 3 | Step 1869800 | Avg Loss: 0.0154 | Grad Norm: 0.01186445\n",
      "Epoch 3 | Step 1869900 | Avg Loss: 0.0156 | Grad Norm: 0.00927138\n",
      "Epoch 3 | Step 1870000 | Avg Loss: 0.0158 | Grad Norm: 0.01071185\n",
      "Epoch 3 | Step 1870100 | Avg Loss: 0.0156 | Grad Norm: 0.00844303\n",
      "Epoch 3 | Step 1870200 | Avg Loss: 0.0153 | Grad Norm: 0.00958506\n",
      "Epoch 3 | Step 1870300 | Avg Loss: 0.0155 | Grad Norm: 0.00892724\n",
      "Epoch 3 | Step 1870400 | Avg Loss: 0.0155 | Grad Norm: 0.00846817\n",
      "Epoch 3 | Step 1870500 | Avg Loss: 0.0158 | Grad Norm: 0.00926924\n",
      "Epoch 3 | Step 1870600 | Avg Loss: 0.0160 | Grad Norm: 0.00841884\n",
      "Epoch 3 | Step 1870700 | Avg Loss: 0.0153 | Grad Norm: 0.00876430\n",
      "Epoch 3 | Step 1870800 | Avg Loss: 0.0154 | Grad Norm: 0.00815924\n",
      "Epoch 3 | Step 1870900 | Avg Loss: 0.0156 | Grad Norm: 0.00866434\n",
      "Epoch 3 | Step 1871000 | Avg Loss: 0.0158 | Grad Norm: 0.00866749\n",
      "Epoch 3 | Step 1871100 | Avg Loss: 0.0151 | Grad Norm: 0.00826923\n",
      "Epoch 3 | Step 1871200 | Avg Loss: 0.0153 | Grad Norm: 0.00781816\n",
      "Epoch 3 | Step 1871300 | Avg Loss: 0.0153 | Grad Norm: 0.00849646\n",
      "Epoch 3 | Step 1871400 | Avg Loss: 0.0154 | Grad Norm: 0.00902162\n",
      "Epoch 3 | Step 1871500 | Avg Loss: 0.0154 | Grad Norm: 0.00876478\n",
      "Epoch 3 | Step 1871600 | Avg Loss: 0.0151 | Grad Norm: 0.01064409\n",
      "Epoch 3 | Step 1871700 | Avg Loss: 0.0153 | Grad Norm: 0.00928694\n",
      "Epoch 3 | Step 1871800 | Avg Loss: 0.0154 | Grad Norm: 0.00835147\n",
      "Epoch 3 | Step 1871900 | Avg Loss: 0.0154 | Grad Norm: 0.00739833\n",
      "Epoch 3 | Step 1872000 | Avg Loss: 0.0150 | Grad Norm: 0.00837059\n",
      "Epoch 3 | Step 1872100 | Avg Loss: 0.0150 | Grad Norm: 0.00782814\n",
      "Epoch 3 | Step 1872200 | Avg Loss: 0.0151 | Grad Norm: 0.00910102\n",
      "Epoch 3 | Step 1872300 | Avg Loss: 0.0150 | Grad Norm: 0.00866262\n",
      "Epoch 3 | Step 1872400 | Avg Loss: 0.0153 | Grad Norm: 0.00871622\n",
      "Epoch 3 | Step 1872500 | Avg Loss: 0.0158 | Grad Norm: 0.00843872\n",
      "Epoch 3 | Step 1872600 | Avg Loss: 0.0154 | Grad Norm: 0.00891209\n",
      "Epoch 3 | Step 1872700 | Avg Loss: 0.0156 | Grad Norm: 0.00864471\n",
      "Epoch 3 | Step 1872800 | Avg Loss: 0.0157 | Grad Norm: 0.00781986\n",
      "Epoch 3 | Step 1872900 | Avg Loss: 0.0154 | Grad Norm: 0.00844199\n",
      "Epoch 3 | Step 1873000 | Avg Loss: 0.0154 | Grad Norm: 0.00836902\n",
      "Epoch 3 | Step 1873100 | Avg Loss: 0.0151 | Grad Norm: 0.00863727\n",
      "Epoch 3 | Step 1873200 | Avg Loss: 0.0152 | Grad Norm: 0.00867035\n",
      "Epoch 3 | Step 1873300 | Avg Loss: 0.0153 | Grad Norm: 0.00873271\n",
      "Epoch 3 | Step 1873400 | Avg Loss: 0.0155 | Grad Norm: 0.00908862\n",
      "Epoch 3 | Step 1873500 | Avg Loss: 0.0154 | Grad Norm: 0.00916267\n",
      "Epoch 3 | Step 1873600 | Avg Loss: 0.0156 | Grad Norm: 0.00880294\n",
      "Epoch 3 | Step 1873700 | Avg Loss: 0.0158 | Grad Norm: 0.00856312\n",
      "Epoch 3 | Step 1873800 | Avg Loss: 0.0159 | Grad Norm: 0.01000833\n",
      "Epoch 3 | Step 1873900 | Avg Loss: 0.0158 | Grad Norm: 0.00821101\n",
      "Epoch 3 | Step 1874000 | Avg Loss: 0.0153 | Grad Norm: 0.00845535\n",
      "Epoch 3 | Step 1874100 | Avg Loss: 0.0154 | Grad Norm: 0.00853060\n",
      "Epoch 3 | Step 1874200 | Avg Loss: 0.0156 | Grad Norm: 0.00952433\n",
      "Epoch 3 | Step 1874300 | Avg Loss: 0.0151 | Grad Norm: 0.00806550\n",
      "Epoch 3 | Step 1874400 | Avg Loss: 0.0153 | Grad Norm: 0.00827001\n",
      "Epoch 3 | Step 1874500 | Avg Loss: 0.0152 | Grad Norm: 0.00945406\n",
      "Epoch 3 | Step 1874600 | Avg Loss: 0.0156 | Grad Norm: 0.00936065\n",
      "Epoch 3 | Step 1874700 | Avg Loss: 0.0158 | Grad Norm: 0.00964990\n",
      "Epoch 3 | Step 1874800 | Avg Loss: 0.0158 | Grad Norm: 0.00844265\n",
      "Epoch 3 | Step 1874900 | Avg Loss: 0.0160 | Grad Norm: 0.00804672\n",
      "Epoch 3 | Step 1875000 | Avg Loss: 0.0157 | Grad Norm: 0.00829205\n",
      "Epoch 3 | Step 1875100 | Avg Loss: 0.0159 | Grad Norm: 0.00884349\n",
      "Epoch 3 | Step 1875200 | Avg Loss: 0.0158 | Grad Norm: 0.00803194\n",
      "Epoch 3 | Step 1875300 | Avg Loss: 0.0158 | Grad Norm: 0.00901228\n",
      "Epoch 3 | Step 1875400 | Avg Loss: 0.0158 | Grad Norm: 0.00969422\n",
      "Epoch 3 | Step 1875500 | Avg Loss: 0.0156 | Grad Norm: 0.00904997\n",
      "Epoch 3 | Step 1875600 | Avg Loss: 0.0155 | Grad Norm: 0.00948064\n",
      "Epoch 3 | Step 1875700 | Avg Loss: 0.0152 | Grad Norm: 0.01084564\n",
      "Epoch 3 | Step 1875800 | Avg Loss: 0.0153 | Grad Norm: 0.00748893\n",
      "Epoch 3 | Step 1875900 | Avg Loss: 0.0152 | Grad Norm: 0.00726725\n",
      "Epoch 3 | Step 1876000 | Avg Loss: 0.0151 | Grad Norm: 0.00883908\n",
      "Epoch 3 | Step 1876100 | Avg Loss: 0.0151 | Grad Norm: 0.00912744\n",
      "Epoch 3 | Step 1876200 | Avg Loss: 0.0149 | Grad Norm: 0.00977861\n",
      "Epoch 3 | Step 1876300 | Avg Loss: 0.0153 | Grad Norm: 0.00907293\n",
      "Epoch 3 | Step 1876400 | Avg Loss: 0.0151 | Grad Norm: 0.00911329\n",
      "Epoch 3 | Step 1876500 | Avg Loss: 0.0151 | Grad Norm: 0.00752661\n",
      "Epoch 3 | Step 1876600 | Avg Loss: 0.0151 | Grad Norm: 0.01057505\n",
      "Epoch 3 | Step 1876700 | Avg Loss: 0.0150 | Grad Norm: 0.00756794\n",
      "Epoch 3 | Step 1876800 | Avg Loss: 0.0147 | Grad Norm: 0.00773483\n",
      "Epoch 3 | Step 1876900 | Avg Loss: 0.0149 | Grad Norm: 0.00881587\n",
      "Epoch 3 | Step 1877000 | Avg Loss: 0.0151 | Grad Norm: 0.00970853\n",
      "Epoch 3 | Step 1877100 | Avg Loss: 0.0152 | Grad Norm: 0.00896790\n",
      "Epoch 3 | Step 1877200 | Avg Loss: 0.0155 | Grad Norm: 0.01343652\n",
      "Epoch 3 | Step 1877300 | Avg Loss: 0.0158 | Grad Norm: 0.00890143\n",
      "Epoch 3 | Step 1877400 | Avg Loss: 0.0161 | Grad Norm: 0.00975395\n",
      "Epoch 3 | Step 1877500 | Avg Loss: 0.0160 | Grad Norm: 0.00815670\n",
      "Epoch 3 | Step 1877600 | Avg Loss: 0.0160 | Grad Norm: 0.00830928\n",
      "Epoch 3 | Step 1877700 | Avg Loss: 0.0161 | Grad Norm: 0.01016977\n",
      "Epoch 3 | Step 1877800 | Avg Loss: 0.0157 | Grad Norm: 0.01011182\n",
      "Epoch 3 | Step 1877900 | Avg Loss: 0.0158 | Grad Norm: 0.00914255\n",
      "Epoch 3 | Step 1878000 | Avg Loss: 0.0158 | Grad Norm: 0.00993047\n",
      "Epoch 3 | Step 1878100 | Avg Loss: 0.0156 | Grad Norm: 0.00955155\n",
      "Epoch 3 | Step 1878200 | Avg Loss: 0.0159 | Grad Norm: 0.00984546\n",
      "Epoch 3 | Step 1878300 | Avg Loss: 0.0161 | Grad Norm: 0.00817859\n",
      "Epoch 3 | Step 1878400 | Avg Loss: 0.0161 | Grad Norm: 0.00939286\n",
      "Epoch 3 | Step 1878500 | Avg Loss: 0.0158 | Grad Norm: 0.00884279\n",
      "Epoch 3 | Step 1878600 | Avg Loss: 0.0158 | Grad Norm: 0.00959778\n",
      "Epoch 3 | Step 1878700 | Avg Loss: 0.0156 | Grad Norm: 0.00915240\n",
      "Epoch 3 | Step 1878800 | Avg Loss: 0.0155 | Grad Norm: 0.00909466\n",
      "Epoch 3 | Step 1878900 | Avg Loss: 0.0155 | Grad Norm: 0.01032076\n",
      "Epoch 3 | Step 1879000 | Avg Loss: 0.0152 | Grad Norm: 0.00813949\n",
      "Epoch 3 | Step 1879100 | Avg Loss: 0.0159 | Grad Norm: 0.00814856\n",
      "Epoch 3 | Step 1879200 | Avg Loss: 0.0157 | Grad Norm: 0.00893520\n",
      "Epoch 3 | Step 1879300 | Avg Loss: 0.0158 | Grad Norm: 0.00938916\n",
      "Epoch 3 | Step 1879400 | Avg Loss: 0.0155 | Grad Norm: 0.00781384\n",
      "Epoch 3 | Step 1879500 | Avg Loss: 0.0158 | Grad Norm: 0.00921750\n",
      "Epoch 3 | Step 1879600 | Avg Loss: 0.0162 | Grad Norm: 0.00926356\n",
      "Epoch 3 | Step 1879700 | Avg Loss: 0.0165 | Grad Norm: 0.01088239\n",
      "Epoch 3 | Step 1879800 | Avg Loss: 0.0164 | Grad Norm: 0.00935439\n",
      "Epoch 3 | Step 1879900 | Avg Loss: 0.0162 | Grad Norm: 0.00935085\n",
      "Epoch 3 | Step 1880000 | Avg Loss: 0.0160 | Grad Norm: 0.00940168\n",
      "Epoch 3 | Step 1880100 | Avg Loss: 0.0157 | Grad Norm: 0.00887489\n",
      "Epoch 3 | Step 1880200 | Avg Loss: 0.0156 | Grad Norm: 0.00828451\n",
      "Epoch 3 | Step 1880300 | Avg Loss: 0.0155 | Grad Norm: 0.00848116\n",
      "Epoch 3 | Step 1880400 | Avg Loss: 0.0155 | Grad Norm: 0.00800482\n",
      "Epoch 3 | Step 1880500 | Avg Loss: 0.0154 | Grad Norm: 0.01012736\n",
      "Epoch 3 | Step 1880600 | Avg Loss: 0.0157 | Grad Norm: 0.00918701\n",
      "Epoch 3 | Step 1880700 | Avg Loss: 0.0157 | Grad Norm: 0.00962798\n",
      "Epoch 3 | Step 1880800 | Avg Loss: 0.0153 | Grad Norm: 0.00973747\n",
      "Epoch 3 | Step 1880900 | Avg Loss: 0.0151 | Grad Norm: 0.00852948\n",
      "Epoch 3 | Step 1881000 | Avg Loss: 0.0151 | Grad Norm: 0.00856624\n",
      "Epoch 3 | Step 1881100 | Avg Loss: 0.0154 | Grad Norm: 0.00794580\n",
      "Epoch 3 | Step 1881200 | Avg Loss: 0.0155 | Grad Norm: 0.01094889\n",
      "Epoch 3 | Step 1881300 | Avg Loss: 0.0158 | Grad Norm: 0.00905687\n",
      "Epoch 3 | Step 1881400 | Avg Loss: 0.0160 | Grad Norm: 0.00919553\n",
      "Epoch 3 | Step 1881500 | Avg Loss: 0.0160 | Grad Norm: 0.00932451\n",
      "Epoch 3 | Step 1881600 | Avg Loss: 0.0159 | Grad Norm: 0.00867958\n",
      "Epoch 3 | Step 1881700 | Avg Loss: 0.0159 | Grad Norm: 0.00880654\n",
      "Epoch 3 | Step 1881800 | Avg Loss: 0.0156 | Grad Norm: 0.01051131\n",
      "Epoch 3 | Step 1881900 | Avg Loss: 0.0157 | Grad Norm: 0.00866090\n",
      "Epoch 3 | Step 1882000 | Avg Loss: 0.0155 | Grad Norm: 0.01002436\n",
      "Epoch 3 | Step 1882100 | Avg Loss: 0.0160 | Grad Norm: 0.01072581\n",
      "Epoch 3 | Step 1882200 | Avg Loss: 0.0162 | Grad Norm: 0.00878403\n",
      "Epoch 3 | Step 1882300 | Avg Loss: 0.0164 | Grad Norm: 0.00935375\n",
      "Epoch 3 | Step 1882400 | Avg Loss: 0.0163 | Grad Norm: 0.00793710\n",
      "Epoch 3 | Step 1882500 | Avg Loss: 0.0163 | Grad Norm: 0.00866707\n",
      "Epoch 3 | Step 1882600 | Avg Loss: 0.0160 | Grad Norm: 0.00870709\n",
      "Epoch 3 | Step 1882700 | Avg Loss: 0.0155 | Grad Norm: 0.00829832\n",
      "Epoch 3 | Step 1882800 | Avg Loss: 0.0161 | Grad Norm: 0.00919740\n",
      "Epoch 3 | Step 1882900 | Avg Loss: 0.0158 | Grad Norm: 0.00949953\n",
      "Epoch 3 | Step 1883000 | Avg Loss: 0.0161 | Grad Norm: 0.00862684\n",
      "Epoch 3 | Step 1883100 | Avg Loss: 0.0162 | Grad Norm: 0.00892375\n",
      "Epoch 3 | Step 1883200 | Avg Loss: 0.0160 | Grad Norm: 0.00858084\n",
      "Epoch 3 | Step 1883300 | Avg Loss: 0.0158 | Grad Norm: 0.01097705\n",
      "Epoch 3 | Step 1883400 | Avg Loss: 0.0156 | Grad Norm: 0.01096571\n",
      "Epoch 3 | Step 1883500 | Avg Loss: 0.0156 | Grad Norm: 0.00836742\n",
      "Epoch 3 | Step 1883600 | Avg Loss: 0.0158 | Grad Norm: 0.01606835\n",
      "Epoch 3 | Step 1883700 | Avg Loss: 0.0159 | Grad Norm: 0.00828217\n",
      "Epoch 3 | Step 1883800 | Avg Loss: 0.0154 | Grad Norm: 0.00972451\n",
      "Epoch 3 | Step 1883900 | Avg Loss: 0.0156 | Grad Norm: 0.00875253\n",
      "Epoch 3 | Step 1884000 | Avg Loss: 0.0156 | Grad Norm: 0.00811851\n",
      "Epoch 3 | Step 1884100 | Avg Loss: 0.0154 | Grad Norm: 0.00931411\n",
      "Epoch 3 | Step 1884200 | Avg Loss: 0.0154 | Grad Norm: 0.00936256\n",
      "Epoch 3 | Step 1884300 | Avg Loss: 0.0157 | Grad Norm: 0.01135815\n",
      "Epoch 3 | Step 1884400 | Avg Loss: 0.0160 | Grad Norm: 0.01015236\n",
      "Epoch 3 | Step 1884500 | Avg Loss: 0.0160 | Grad Norm: 0.01298709\n",
      "Epoch 3 | Step 1884600 | Avg Loss: 0.0155 | Grad Norm: 0.00964109\n",
      "Epoch 3 | Step 1884700 | Avg Loss: 0.0153 | Grad Norm: 0.00887608\n",
      "Epoch 3 | Step 1884800 | Avg Loss: 0.0156 | Grad Norm: 0.00886132\n",
      "Epoch 3 | Step 1884900 | Avg Loss: 0.0156 | Grad Norm: 0.00863025\n",
      "Epoch 3 | Step 1885000 | Avg Loss: 0.0155 | Grad Norm: 0.00996096\n",
      "Epoch 3 | Step 1885100 | Avg Loss: 0.0154 | Grad Norm: 0.01009968\n",
      "Epoch 3 | Step 1885200 | Avg Loss: 0.0153 | Grad Norm: 0.00987363\n",
      "Epoch 3 | Step 1885300 | Avg Loss: 0.0153 | Grad Norm: 0.00818826\n",
      "Epoch 3 | Step 1885400 | Avg Loss: 0.0157 | Grad Norm: 0.00836944\n",
      "Epoch 3 | Step 1885500 | Avg Loss: 0.0154 | Grad Norm: 0.00858087\n",
      "Epoch 3 | Step 1885600 | Avg Loss: 0.0156 | Grad Norm: 0.00937739\n",
      "Epoch 3 | Step 1885700 | Avg Loss: 0.0156 | Grad Norm: 0.00846939\n",
      "Epoch 3 | Step 1885800 | Avg Loss: 0.0155 | Grad Norm: 0.00888387\n",
      "Epoch 3 | Step 1885900 | Avg Loss: 0.0154 | Grad Norm: 0.01031849\n",
      "Epoch 3 | Step 1886000 | Avg Loss: 0.0157 | Grad Norm: 0.00887982\n",
      "Epoch 3 | Step 1886100 | Avg Loss: 0.0162 | Grad Norm: 0.00931192\n",
      "Epoch 3 | Step 1886200 | Avg Loss: 0.0161 | Grad Norm: 0.00844531\n",
      "Epoch 3 | Step 1886300 | Avg Loss: 0.0156 | Grad Norm: 0.00830426\n",
      "Epoch 3 | Step 1886400 | Avg Loss: 0.0154 | Grad Norm: 0.00850563\n",
      "Epoch 3 | Step 1886500 | Avg Loss: 0.0151 | Grad Norm: 0.00786527\n",
      "Epoch 3 | Step 1886600 | Avg Loss: 0.0152 | Grad Norm: 0.00757810\n",
      "Epoch 3 | Step 1886700 | Avg Loss: 0.0154 | Grad Norm: 0.00821870\n",
      "Epoch 3 | Step 1886800 | Avg Loss: 0.0154 | Grad Norm: 0.00904782\n",
      "Epoch 3 | Step 1886900 | Avg Loss: 0.0152 | Grad Norm: 0.00884908\n",
      "Epoch 3 | Step 1887000 | Avg Loss: 0.0153 | Grad Norm: 0.00934468\n",
      "Epoch 3 | Step 1887100 | Avg Loss: 0.0155 | Grad Norm: 0.00820152\n",
      "Epoch 3 | Step 1887200 | Avg Loss: 0.0155 | Grad Norm: 0.00811395\n",
      "Epoch 3 | Step 1887300 | Avg Loss: 0.0157 | Grad Norm: 0.00897731\n",
      "Epoch 3 | Step 1887400 | Avg Loss: 0.0162 | Grad Norm: 0.00923429\n",
      "Epoch 3 | Step 1887500 | Avg Loss: 0.0160 | Grad Norm: 0.00911615\n",
      "Epoch 3 | Step 1887600 | Avg Loss: 0.0159 | Grad Norm: 0.00966344\n",
      "Epoch 3 | Step 1887700 | Avg Loss: 0.0154 | Grad Norm: 0.00914960\n",
      "Epoch 3 | Step 1887800 | Avg Loss: 0.0153 | Grad Norm: 0.00853290\n",
      "Epoch 3 | Step 1887900 | Avg Loss: 0.0152 | Grad Norm: 0.00870333\n",
      "Epoch 3 | Step 1888000 | Avg Loss: 0.0153 | Grad Norm: 0.00874458\n",
      "Epoch 3 | Step 1888100 | Avg Loss: 0.0155 | Grad Norm: 0.00912205\n",
      "Epoch 3 | Step 1888200 | Avg Loss: 0.0157 | Grad Norm: 0.01264877\n",
      "Epoch 3 | Step 1888300 | Avg Loss: 0.0154 | Grad Norm: 0.00842967\n",
      "Epoch 3 | Step 1888400 | Avg Loss: 0.0155 | Grad Norm: 0.00810644\n",
      "Epoch 3 | Step 1888500 | Avg Loss: 0.0153 | Grad Norm: 0.00834906\n",
      "Epoch 3 | Step 1888600 | Avg Loss: 0.0154 | Grad Norm: 0.00912913\n",
      "Epoch 3 | Step 1888700 | Avg Loss: 0.0149 | Grad Norm: 0.00887998\n",
      "Epoch 3 | Step 1888800 | Avg Loss: 0.0145 | Grad Norm: 0.00830988\n",
      "Epoch 3 | Step 1888900 | Avg Loss: 0.0147 | Grad Norm: 0.00992256\n",
      "Epoch 3 | Step 1889000 | Avg Loss: 0.0148 | Grad Norm: 0.00844049\n",
      "Epoch 3 | Step 1889100 | Avg Loss: 0.0152 | Grad Norm: 0.00902308\n",
      "Epoch 3 | Step 1889200 | Avg Loss: 0.0152 | Grad Norm: 0.00930019\n",
      "Epoch 3 | Step 1889300 | Avg Loss: 0.0152 | Grad Norm: 0.00709069\n",
      "Epoch 3 | Step 1889400 | Avg Loss: 0.0155 | Grad Norm: 0.00826623\n",
      "Epoch 3 | Step 1889500 | Avg Loss: 0.0157 | Grad Norm: 0.00849375\n",
      "Epoch 3 | Step 1889600 | Avg Loss: 0.0157 | Grad Norm: 0.00959482\n",
      "Epoch 3 | Step 1889700 | Avg Loss: 0.0159 | Grad Norm: 0.00866909\n",
      "Epoch 3 | Step 1889800 | Avg Loss: 0.0158 | Grad Norm: 0.01152948\n",
      "Epoch 3 | Step 1889900 | Avg Loss: 0.0157 | Grad Norm: 0.00879176\n",
      "Epoch 3 | Step 1890000 | Avg Loss: 0.0159 | Grad Norm: 0.00888045\n",
      "Epoch 3 | Step 1890100 | Avg Loss: 0.0155 | Grad Norm: 0.00878447\n",
      "Epoch 3 | Step 1890200 | Avg Loss: 0.0153 | Grad Norm: 0.00882221\n",
      "Epoch 3 | Step 1890300 | Avg Loss: 0.0151 | Grad Norm: 0.00922173\n",
      "Epoch 3 | Step 1890400 | Avg Loss: 0.0152 | Grad Norm: 0.00846263\n",
      "Epoch 3 | Step 1890500 | Avg Loss: 0.0149 | Grad Norm: 0.01205408\n",
      "Epoch 3 | Step 1890600 | Avg Loss: 0.0146 | Grad Norm: 0.00872809\n",
      "Epoch 3 | Step 1890700 | Avg Loss: 0.0151 | Grad Norm: 0.00958851\n",
      "Epoch 3 | Step 1890800 | Avg Loss: 0.0151 | Grad Norm: 0.00902064\n",
      "Epoch 3 | Step 1890900 | Avg Loss: 0.0150 | Grad Norm: 0.00823544\n",
      "Epoch 3 | Step 1891000 | Avg Loss: 0.0151 | Grad Norm: 0.00783589\n",
      "Epoch 3 | Step 1891100 | Avg Loss: 0.0152 | Grad Norm: 0.00812256\n",
      "Epoch 3 | Step 1891200 | Avg Loss: 0.0155 | Grad Norm: 0.00956547\n",
      "Epoch 3 | Step 1891300 | Avg Loss: 0.0157 | Grad Norm: 0.00847255\n",
      "Epoch 3 | Step 1891400 | Avg Loss: 0.0160 | Grad Norm: 0.00940961\n",
      "Epoch 3 | Step 1891500 | Avg Loss: 0.0157 | Grad Norm: 0.00886246\n",
      "Epoch 3 | Step 1891600 | Avg Loss: 0.0156 | Grad Norm: 0.00984768\n",
      "Epoch 3 | Step 1891700 | Avg Loss: 0.0155 | Grad Norm: 0.01035841\n",
      "Epoch 3 | Step 1891800 | Avg Loss: 0.0154 | Grad Norm: 0.01006354\n",
      "Epoch 3 | Step 1891900 | Avg Loss: 0.0154 | Grad Norm: 0.00829888\n",
      "Epoch 3 | Step 1892000 | Avg Loss: 0.0151 | Grad Norm: 0.00930580\n",
      "Epoch 3 | Step 1892100 | Avg Loss: 0.0151 | Grad Norm: 0.00929773\n",
      "Epoch 3 | Step 1892200 | Avg Loss: 0.0153 | Grad Norm: 0.01016224\n",
      "Epoch 3 | Step 1892300 | Avg Loss: 0.0152 | Grad Norm: 0.00798609\n",
      "Epoch 3 | Step 1892400 | Avg Loss: 0.0155 | Grad Norm: 0.00850432\n",
      "Epoch 3 | Step 1892500 | Avg Loss: 0.0152 | Grad Norm: 0.00743340\n",
      "Epoch 3 | Step 1892600 | Avg Loss: 0.0151 | Grad Norm: 0.00805556\n",
      "Epoch 3 | Step 1892700 | Avg Loss: 0.0151 | Grad Norm: 0.00865024\n",
      "Epoch 3 | Step 1892800 | Avg Loss: 0.0155 | Grad Norm: 0.00909670\n",
      "Epoch 3 | Step 1892900 | Avg Loss: 0.0160 | Grad Norm: 0.00890469\n",
      "Epoch 3 | Step 1893000 | Avg Loss: 0.0158 | Grad Norm: 0.00836877\n",
      "Epoch 3 | Step 1893100 | Avg Loss: 0.0158 | Grad Norm: 0.00998396\n",
      "Epoch 3 | Step 1893200 | Avg Loss: 0.0161 | Grad Norm: 0.00814812\n",
      "Epoch 3 | Step 1893300 | Avg Loss: 0.0163 | Grad Norm: 0.00915335\n",
      "Epoch 3 | Step 1893400 | Avg Loss: 0.0161 | Grad Norm: 0.00975492\n",
      "Epoch 3 | Step 1893500 | Avg Loss: 0.0160 | Grad Norm: 0.00992181\n",
      "Epoch 3 | Step 1893600 | Avg Loss: 0.0161 | Grad Norm: 0.00793192\n",
      "Epoch 3 | Step 1893700 | Avg Loss: 0.0162 | Grad Norm: 0.01003390\n",
      "Epoch 3 | Step 1893800 | Avg Loss: 0.0158 | Grad Norm: 0.01027523\n",
      "Epoch 3 | Step 1893900 | Avg Loss: 0.0156 | Grad Norm: 0.01102105\n",
      "Epoch 3 | Step 1894000 | Avg Loss: 0.0164 | Grad Norm: 0.01024565\n",
      "Epoch 3 | Step 1894100 | Avg Loss: 0.0166 | Grad Norm: 0.00908209\n",
      "Epoch 3 | Step 1894200 | Avg Loss: 0.0162 | Grad Norm: 0.01013416\n",
      "Epoch 3 | Step 1894300 | Avg Loss: 0.0163 | Grad Norm: 0.00823340\n",
      "Epoch 3 | Step 1894400 | Avg Loss: 0.0159 | Grad Norm: 0.00899304\n",
      "Epoch 3 | Step 1894500 | Avg Loss: 0.0157 | Grad Norm: 0.00982718\n",
      "Epoch 3 | Step 1894600 | Avg Loss: 0.0160 | Grad Norm: 0.00996367\n",
      "Epoch 3 | Step 1894700 | Avg Loss: 0.0160 | Grad Norm: 0.00903770\n",
      "Epoch 3 | Step 1894800 | Avg Loss: 0.0157 | Grad Norm: 0.00888642\n",
      "Epoch 3 | Step 1894900 | Avg Loss: 0.0154 | Grad Norm: 0.00856393\n",
      "Epoch 3 | Step 1895000 | Avg Loss: 0.0156 | Grad Norm: 0.00921125\n",
      "Epoch 3 | Step 1895100 | Avg Loss: 0.0158 | Grad Norm: 0.01079134\n",
      "Epoch 3 | Step 1895200 | Avg Loss: 0.0156 | Grad Norm: 0.00864662\n",
      "Epoch 3 | Step 1895300 | Avg Loss: 0.0155 | Grad Norm: 0.01748761\n",
      "Epoch 3 | Step 1895400 | Avg Loss: 0.0162 | Grad Norm: 0.00970294\n",
      "Epoch 3 | Step 1895500 | Avg Loss: 0.0161 | Grad Norm: 0.00962204\n",
      "Epoch 3 | Step 1895600 | Avg Loss: 0.0157 | Grad Norm: 0.00837447\n",
      "Epoch 3 | Step 1895700 | Avg Loss: 0.0158 | Grad Norm: 0.00939639\n",
      "Epoch 3 | Step 1895800 | Avg Loss: 0.0160 | Grad Norm: 0.00869385\n",
      "Epoch 3 | Step 1895900 | Avg Loss: 0.0158 | Grad Norm: 0.01160646\n",
      "Epoch 3 | Step 1896000 | Avg Loss: 0.0161 | Grad Norm: 0.00920613\n",
      "Epoch 3 | Step 1896100 | Avg Loss: 0.0157 | Grad Norm: 0.00829532\n",
      "Epoch 3 | Step 1896200 | Avg Loss: 0.0155 | Grad Norm: 0.00926423\n",
      "Epoch 3 | Step 1896300 | Avg Loss: 0.0155 | Grad Norm: 0.00872506\n",
      "Epoch 3 | Step 1896400 | Avg Loss: 0.0154 | Grad Norm: 0.00733043\n",
      "Epoch 3 | Step 1896500 | Avg Loss: 0.0154 | Grad Norm: 0.00945346\n",
      "Epoch 3 | Step 1896600 | Avg Loss: 0.0159 | Grad Norm: 0.00888451\n",
      "Epoch 3 | Step 1896700 | Avg Loss: 0.0160 | Grad Norm: 0.00910612\n",
      "Epoch 3 | Step 1896800 | Avg Loss: 0.0159 | Grad Norm: 0.00841383\n",
      "Epoch 3 | Step 1896900 | Avg Loss: 0.0157 | Grad Norm: 0.00831118\n",
      "Epoch 3 | Step 1897000 | Avg Loss: 0.0155 | Grad Norm: 0.00953584\n",
      "Epoch 3 | Step 1897100 | Avg Loss: 0.0156 | Grad Norm: 0.00863387\n",
      "Epoch 3 | Step 1897200 | Avg Loss: 0.0155 | Grad Norm: 0.00879519\n",
      "Epoch 3 | Step 1897300 | Avg Loss: 0.0156 | Grad Norm: 0.00854197\n",
      "Epoch 3 | Step 1897400 | Avg Loss: 0.0158 | Grad Norm: 0.00817659\n",
      "Epoch 3 | Step 1897500 | Avg Loss: 0.0157 | Grad Norm: 0.00901748\n",
      "Epoch 3 | Step 1897600 | Avg Loss: 0.0160 | Grad Norm: 0.01007288\n",
      "Epoch 3 | Step 1897700 | Avg Loss: 0.0157 | Grad Norm: 0.00727452\n",
      "Epoch 3 | Step 1897800 | Avg Loss: 0.0157 | Grad Norm: 0.00957875\n",
      "Epoch 3 | Step 1897900 | Avg Loss: 0.0155 | Grad Norm: 0.01093599\n",
      "Epoch 3 | Step 1898000 | Avg Loss: 0.0154 | Grad Norm: 0.00917439\n",
      "Epoch 3 | Step 1898100 | Avg Loss: 0.0157 | Grad Norm: 0.00799067\n",
      "Epoch 3 | Step 1898200 | Avg Loss: 0.0154 | Grad Norm: 0.00944973\n",
      "Epoch 3 | Step 1898300 | Avg Loss: 0.0156 | Grad Norm: 0.00901732\n",
      "Epoch 3 | Step 1898400 | Avg Loss: 0.0157 | Grad Norm: 0.00793588\n",
      "Epoch 3 | Step 1898500 | Avg Loss: 0.0158 | Grad Norm: 0.01036769\n",
      "Epoch 3 | Step 1898600 | Avg Loss: 0.0155 | Grad Norm: 0.00909628\n",
      "Epoch 3 | Step 1898700 | Avg Loss: 0.0155 | Grad Norm: 0.01054305\n",
      "Epoch 3 | Step 1898800 | Avg Loss: 0.0153 | Grad Norm: 0.00966451\n",
      "Epoch 3 | Step 1898900 | Avg Loss: 0.0156 | Grad Norm: 0.00852361\n",
      "Epoch 3 | Step 1899000 | Avg Loss: 0.0153 | Grad Norm: 0.00996142\n",
      "Epoch 3 | Step 1899100 | Avg Loss: 0.0151 | Grad Norm: 0.00911079\n",
      "Epoch 3 | Step 1899200 | Avg Loss: 0.0153 | Grad Norm: 0.00820540\n",
      "Epoch 3 | Step 1899300 | Avg Loss: 0.0154 | Grad Norm: 0.00838067\n",
      "Epoch 3 | Step 1899400 | Avg Loss: 0.0154 | Grad Norm: 0.00904724\n",
      "Epoch 3 | Step 1899500 | Avg Loss: 0.0155 | Grad Norm: 0.00758262\n",
      "Epoch 3 | Step 1899600 | Avg Loss: 0.0155 | Grad Norm: 0.00975282\n",
      "Epoch 3 | Step 1899700 | Avg Loss: 0.0155 | Grad Norm: 0.00924807\n",
      "Epoch 3 | Step 1899800 | Avg Loss: 0.0155 | Grad Norm: 0.00977126\n",
      "Epoch 3 | Step 1899900 | Avg Loss: 0.0155 | Grad Norm: 0.00972003\n",
      "Epoch 3 | Step 1900000 | Avg Loss: 0.0155 | Grad Norm: 0.00945996\n",
      "Saving model at step1900000\n",
      "Epoch 3 | Step 1900100 | Avg Loss: 0.0154 | Grad Norm: 0.00963280\n",
      "Epoch 3 | Step 1900200 | Avg Loss: 0.0154 | Grad Norm: 0.00895607\n",
      "Epoch 3 | Step 1900300 | Avg Loss: 0.0152 | Grad Norm: 0.00801948\n",
      "Epoch 3 | Step 1900400 | Avg Loss: 0.0152 | Grad Norm: 0.00893576\n",
      "Epoch 3 | Step 1900500 | Avg Loss: 0.0156 | Grad Norm: 0.00886053\n",
      "Epoch 3 | Step 1900600 | Avg Loss: 0.0155 | Grad Norm: 0.00841485\n",
      "Epoch 3 | Step 1900700 | Avg Loss: 0.0158 | Grad Norm: 0.00903900\n",
      "Epoch 3 | Step 1900800 | Avg Loss: 0.0156 | Grad Norm: 0.00860633\n",
      "Epoch 3 | Step 1900900 | Avg Loss: 0.0157 | Grad Norm: 0.00852246\n",
      "Epoch 3 | Step 1901000 | Avg Loss: 0.0159 | Grad Norm: 0.00869268\n",
      "Epoch 3 | Step 1901100 | Avg Loss: 0.0158 | Grad Norm: 0.00920425\n",
      "Epoch 3 | Step 1901200 | Avg Loss: 0.0154 | Grad Norm: 0.00786450\n",
      "Epoch 3 | Step 1901300 | Avg Loss: 0.0151 | Grad Norm: 0.00842981\n",
      "Epoch 3 | Step 1901400 | Avg Loss: 0.0153 | Grad Norm: 0.00930755\n",
      "Epoch 3 | Step 1901500 | Avg Loss: 0.0157 | Grad Norm: 0.01063886\n",
      "Epoch 3 | Step 1901600 | Avg Loss: 0.0154 | Grad Norm: 0.01053853\n",
      "Epoch 3 | Step 1901700 | Avg Loss: 0.0151 | Grad Norm: 0.01017828\n",
      "Epoch 3 | Step 1901800 | Avg Loss: 0.0150 | Grad Norm: 0.00913040\n",
      "Epoch 3 | Step 1901900 | Avg Loss: 0.0151 | Grad Norm: 0.00853053\n",
      "Epoch 3 | Step 1902000 | Avg Loss: 0.0149 | Grad Norm: 0.00983518\n",
      "Epoch 3 | Step 1902100 | Avg Loss: 0.0150 | Grad Norm: 0.01016888\n",
      "Epoch 3 | Step 1902200 | Avg Loss: 0.0146 | Grad Norm: 0.00903998\n",
      "Epoch 3 | Step 1902300 | Avg Loss: 0.0148 | Grad Norm: 0.00856913\n",
      "Epoch 3 | Step 1902400 | Avg Loss: 0.0149 | Grad Norm: 0.00847735\n",
      "Epoch 3 | Step 1902500 | Avg Loss: 0.0152 | Grad Norm: 0.00892514\n",
      "Epoch 3 | Step 1902600 | Avg Loss: 0.0154 | Grad Norm: 0.00835794\n",
      "Epoch 3 | Step 1902700 | Avg Loss: 0.0155 | Grad Norm: 0.00881908\n",
      "Epoch 3 | Step 1902800 | Avg Loss: 0.0151 | Grad Norm: 0.00812337\n",
      "Epoch 3 | Step 1902900 | Avg Loss: 0.0148 | Grad Norm: 0.01247449\n",
      "Epoch 3 | Step 1903000 | Avg Loss: 0.0152 | Grad Norm: 0.00776941\n",
      "Epoch 3 | Step 1903100 | Avg Loss: 0.0154 | Grad Norm: 0.00911910\n",
      "Epoch 3 | Step 1903200 | Avg Loss: 0.0155 | Grad Norm: 0.00809404\n",
      "Epoch 3 | Step 1903300 | Avg Loss: 0.0154 | Grad Norm: 0.00861590\n",
      "Epoch 3 | Step 1903400 | Avg Loss: 0.0154 | Grad Norm: 0.00799561\n",
      "Epoch 3 | Step 1903500 | Avg Loss: 0.0152 | Grad Norm: 0.00912077\n",
      "Epoch 3 | Step 1903600 | Avg Loss: 0.0154 | Grad Norm: 0.00847884\n",
      "Epoch 3 | Step 1903700 | Avg Loss: 0.0151 | Grad Norm: 0.00807310\n",
      "Epoch 3 | Step 1903800 | Avg Loss: 0.0152 | Grad Norm: 0.00893792\n",
      "Epoch 3 | Step 1903900 | Avg Loss: 0.0152 | Grad Norm: 0.00886707\n",
      "Epoch 3 | Step 1904000 | Avg Loss: 0.0155 | Grad Norm: 0.00824164\n",
      "Epoch 3 | Step 1904100 | Avg Loss: 0.0153 | Grad Norm: 0.00862258\n",
      "Epoch 3 | Step 1904200 | Avg Loss: 0.0153 | Grad Norm: 0.01362400\n",
      "Epoch 3 | Step 1904300 | Avg Loss: 0.0155 | Grad Norm: 0.00955644\n",
      "Epoch 3 | Step 1904400 | Avg Loss: 0.0158 | Grad Norm: 0.00976453\n",
      "Epoch 3 | Step 1904500 | Avg Loss: 0.0159 | Grad Norm: 0.00914693\n",
      "Epoch 3 | Step 1904600 | Avg Loss: 0.0161 | Grad Norm: 0.00895030\n",
      "Epoch 3 | Step 1904700 | Avg Loss: 0.0158 | Grad Norm: 0.00836643\n",
      "Epoch 3 | Step 1904800 | Avg Loss: 0.0154 | Grad Norm: 0.00787790\n",
      "Epoch 3 | Step 1904900 | Avg Loss: 0.0151 | Grad Norm: 0.00859876\n",
      "Epoch 3 | Step 1905000 | Avg Loss: 0.0154 | Grad Norm: 0.00859167\n",
      "Epoch 3 | Step 1905100 | Avg Loss: 0.0153 | Grad Norm: 0.00780975\n",
      "Epoch 3 | Step 1905200 | Avg Loss: 0.0155 | Grad Norm: 0.00907881\n",
      "Epoch 3 | Step 1905300 | Avg Loss: 0.0156 | Grad Norm: 0.00920548\n",
      "Epoch 3 | Step 1905400 | Avg Loss: 0.0155 | Grad Norm: 0.00824732\n",
      "Epoch 3 | Step 1905500 | Avg Loss: 0.0154 | Grad Norm: 0.00868204\n",
      "Epoch 3 | Step 1905600 | Avg Loss: 0.0155 | Grad Norm: 0.01093627\n",
      "Epoch 3 | Step 1905700 | Avg Loss: 0.0152 | Grad Norm: 0.00994379\n",
      "Epoch 3 | Step 1905800 | Avg Loss: 0.0154 | Grad Norm: 0.00861138\n",
      "Epoch 3 | Step 1905900 | Avg Loss: 0.0155 | Grad Norm: 0.00845432\n",
      "Epoch 3 | Step 1906000 | Avg Loss: 0.0156 | Grad Norm: 0.00946252\n",
      "Epoch 3 | Step 1906100 | Avg Loss: 0.0154 | Grad Norm: 0.00891179\n",
      "Epoch 3 | Step 1906200 | Avg Loss: 0.0154 | Grad Norm: 0.00995863\n",
      "Epoch 3 | Step 1906300 | Avg Loss: 0.0158 | Grad Norm: 0.00900677\n",
      "Epoch 3 | Step 1906400 | Avg Loss: 0.0161 | Grad Norm: 0.00784573\n",
      "Epoch 3 | Step 1906500 | Avg Loss: 0.0156 | Grad Norm: 0.00968208\n",
      "Epoch 3 | Step 1906600 | Avg Loss: 0.0157 | Grad Norm: 0.00998856\n",
      "Epoch 3 | Step 1906700 | Avg Loss: 0.0159 | Grad Norm: 0.00856720\n",
      "Epoch 3 | Step 1906800 | Avg Loss: 0.0159 | Grad Norm: 0.00954984\n",
      "Epoch 3 | Step 1906900 | Avg Loss: 0.0158 | Grad Norm: 0.01106087\n",
      "Epoch 3 | Step 1907000 | Avg Loss: 0.0158 | Grad Norm: 0.00832841\n",
      "Epoch 3 | Step 1907100 | Avg Loss: 0.0159 | Grad Norm: 0.00996005\n",
      "Epoch 3 | Step 1907200 | Avg Loss: 0.0160 | Grad Norm: 0.00908241\n",
      "Epoch 3 | Step 1907300 | Avg Loss: 0.0155 | Grad Norm: 0.00887051\n",
      "Epoch 3 | Step 1907400 | Avg Loss: 0.0157 | Grad Norm: 0.00827987\n",
      "Epoch 3 | Step 1907500 | Avg Loss: 0.0155 | Grad Norm: 0.00955837\n",
      "Epoch 3 | Step 1907600 | Avg Loss: 0.0154 | Grad Norm: 0.00894262\n",
      "Epoch 3 | Step 1907700 | Avg Loss: 0.0154 | Grad Norm: 0.00834351\n",
      "Epoch 3 | Step 1907800 | Avg Loss: 0.0152 | Grad Norm: 0.00857060\n",
      "Epoch 3 | Step 1907900 | Avg Loss: 0.0152 | Grad Norm: 0.00904145\n",
      "Epoch 3 | Step 1908000 | Avg Loss: 0.0154 | Grad Norm: 0.00873014\n",
      "Epoch 3 | Step 1908100 | Avg Loss: 0.0156 | Grad Norm: 0.00871696\n",
      "Epoch 3 | Step 1908200 | Avg Loss: 0.0154 | Grad Norm: 0.00911417\n",
      "Epoch 3 | Step 1908300 | Avg Loss: 0.0154 | Grad Norm: 0.00901802\n",
      "Epoch 3 | Step 1908400 | Avg Loss: 0.0153 | Grad Norm: 0.00993662\n",
      "Epoch 3 | Step 1908500 | Avg Loss: 0.0154 | Grad Norm: 0.00925917\n",
      "Epoch 3 | Step 1908600 | Avg Loss: 0.0155 | Grad Norm: 0.00759008\n",
      "Epoch 3 | Step 1908700 | Avg Loss: 0.0155 | Grad Norm: 0.00894313\n",
      "Epoch 3 | Step 1908800 | Avg Loss: 0.0154 | Grad Norm: 0.00891920\n",
      "Epoch 3 | Step 1908900 | Avg Loss: 0.0155 | Grad Norm: 0.00824138\n",
      "Epoch 3 | Step 1909000 | Avg Loss: 0.0155 | Grad Norm: 0.00952710\n",
      "Epoch 3 | Step 1909100 | Avg Loss: 0.0156 | Grad Norm: 0.00831712\n",
      "Epoch 3 | Step 1909200 | Avg Loss: 0.0153 | Grad Norm: 0.00968296\n",
      "Epoch 3 | Step 1909300 | Avg Loss: 0.0151 | Grad Norm: 0.01366372\n",
      "Epoch 3 | Step 1909400 | Avg Loss: 0.0153 | Grad Norm: 0.00884319\n",
      "Epoch 3 | Step 1909500 | Avg Loss: 0.0153 | Grad Norm: 0.01061689\n",
      "Epoch 3 | Step 1909600 | Avg Loss: 0.0153 | Grad Norm: 0.00861278\n",
      "Epoch 3 | Step 1909700 | Avg Loss: 0.0150 | Grad Norm: 0.00996893\n",
      "Epoch 3 | Step 1909800 | Avg Loss: 0.0151 | Grad Norm: 0.00898572\n",
      "Epoch 3 | Step 1909900 | Avg Loss: 0.0151 | Grad Norm: 0.00809537\n",
      "Epoch 3 | Step 1910000 | Avg Loss: 0.0151 | Grad Norm: 0.00762805\n",
      "Epoch 3 | Step 1910100 | Avg Loss: 0.0152 | Grad Norm: 0.00884629\n",
      "Epoch 3 | Step 1910200 | Avg Loss: 0.0158 | Grad Norm: 0.00911028\n",
      "Epoch 3 | Step 1910300 | Avg Loss: 0.0157 | Grad Norm: 0.00913541\n",
      "Epoch 3 | Step 1910400 | Avg Loss: 0.0154 | Grad Norm: 0.00838327\n",
      "Epoch 3 | Step 1910500 | Avg Loss: 0.0158 | Grad Norm: 0.00988045\n",
      "Epoch 3 | Step 1910600 | Avg Loss: 0.0156 | Grad Norm: 0.00811154\n",
      "Epoch 3 | Step 1910700 | Avg Loss: 0.0155 | Grad Norm: 0.00886930\n",
      "Epoch 3 | Step 1910800 | Avg Loss: 0.0158 | Grad Norm: 0.00802435\n",
      "Epoch 3 | Step 1910900 | Avg Loss: 0.0155 | Grad Norm: 0.00982938\n",
      "Epoch 3 | Step 1911000 | Avg Loss: 0.0152 | Grad Norm: 0.00860132\n",
      "Epoch 3 | Step 1911100 | Avg Loss: 0.0154 | Grad Norm: 0.01361670\n",
      "Epoch 3 | Step 1911200 | Avg Loss: 0.0157 | Grad Norm: 0.00861328\n",
      "Epoch 3 | Step 1911300 | Avg Loss: 0.0153 | Grad Norm: 0.00897001\n",
      "Epoch 3 | Step 1911400 | Avg Loss: 0.0157 | Grad Norm: 0.00863974\n",
      "Epoch 3 | Step 1911500 | Avg Loss: 0.0155 | Grad Norm: 0.00915453\n",
      "Epoch 3 | Step 1911600 | Avg Loss: 0.0154 | Grad Norm: 0.00903181\n",
      "Epoch 3 | Step 1911700 | Avg Loss: 0.0153 | Grad Norm: 0.01103795\n",
      "Epoch 3 | Step 1911800 | Avg Loss: 0.0155 | Grad Norm: 0.00968414\n",
      "Epoch 3 | Step 1911900 | Avg Loss: 0.0158 | Grad Norm: 0.00855937\n",
      "Epoch 3 | Step 1912000 | Avg Loss: 0.0156 | Grad Norm: 0.00890360\n",
      "Epoch 3 | Step 1912100 | Avg Loss: 0.0158 | Grad Norm: 0.00840468\n",
      "Epoch 3 | Step 1912200 | Avg Loss: 0.0155 | Grad Norm: 0.00849497\n",
      "Epoch 3 | Step 1912300 | Avg Loss: 0.0158 | Grad Norm: 0.00818367\n",
      "Epoch 3 | Step 1912400 | Avg Loss: 0.0156 | Grad Norm: 0.00902384\n",
      "Epoch 3 | Step 1912500 | Avg Loss: 0.0155 | Grad Norm: 0.00926147\n",
      "Epoch 3 | Step 1912600 | Avg Loss: 0.0156 | Grad Norm: 0.00800724\n",
      "Epoch 3 | Step 1912700 | Avg Loss: 0.0157 | Grad Norm: 0.00826657\n",
      "Epoch 3 | Step 1912800 | Avg Loss: 0.0156 | Grad Norm: 0.00946816\n",
      "Epoch 3 | Step 1912900 | Avg Loss: 0.0157 | Grad Norm: 0.00998551\n",
      "Epoch 3 | Step 1913000 | Avg Loss: 0.0159 | Grad Norm: 0.00904320\n",
      "Epoch 3 | Step 1913100 | Avg Loss: 0.0158 | Grad Norm: 0.00942970\n",
      "Epoch 3 | Step 1913200 | Avg Loss: 0.0161 | Grad Norm: 0.01074084\n",
      "Epoch 3 | Step 1913300 | Avg Loss: 0.0161 | Grad Norm: 0.00929401\n",
      "Epoch 3 | Step 1913400 | Avg Loss: 0.0160 | Grad Norm: 0.00785032\n",
      "Epoch 3 | Step 1913500 | Avg Loss: 0.0160 | Grad Norm: 0.00810587\n",
      "Epoch 3 | Step 1913600 | Avg Loss: 0.0161 | Grad Norm: 0.00901084\n",
      "Epoch 3 | Step 1913700 | Avg Loss: 0.0157 | Grad Norm: 0.00906481\n",
      "Epoch 3 | Step 1913800 | Avg Loss: 0.0154 | Grad Norm: 0.00915949\n",
      "Epoch 3 | Step 1913900 | Avg Loss: 0.0155 | Grad Norm: 0.00867118\n",
      "Epoch 3 | Step 1914000 | Avg Loss: 0.0151 | Grad Norm: 0.00915053\n",
      "Epoch 3 | Step 1914100 | Avg Loss: 0.0155 | Grad Norm: 0.00892429\n",
      "Epoch 3 | Step 1914200 | Avg Loss: 0.0157 | Grad Norm: 0.00787265\n",
      "Epoch 3 | Step 1914300 | Avg Loss: 0.0161 | Grad Norm: 0.00773301\n",
      "Epoch 3 | Step 1914400 | Avg Loss: 0.0160 | Grad Norm: 0.00804838\n",
      "Epoch 3 | Step 1914500 | Avg Loss: 0.0160 | Grad Norm: 0.01013335\n",
      "Epoch 3 | Step 1914600 | Avg Loss: 0.0161 | Grad Norm: 0.00826909\n",
      "Epoch 3 | Step 1914700 | Avg Loss: 0.0157 | Grad Norm: 0.01073429\n",
      "Epoch 3 | Step 1914800 | Avg Loss: 0.0157 | Grad Norm: 0.00878569\n",
      "Epoch 3 | Step 1914900 | Avg Loss: 0.0155 | Grad Norm: 0.00830932\n",
      "Epoch 3 | Step 1915000 | Avg Loss: 0.0156 | Grad Norm: 0.00856535\n",
      "Epoch 3 | Step 1915100 | Avg Loss: 0.0159 | Grad Norm: 0.01066810\n",
      "Epoch 3 | Step 1915200 | Avg Loss: 0.0158 | Grad Norm: 0.00959098\n",
      "Epoch 3 | Step 1915300 | Avg Loss: 0.0155 | Grad Norm: 0.00871333\n",
      "Epoch 3 | Step 1915400 | Avg Loss: 0.0153 | Grad Norm: 0.00894299\n",
      "Epoch 3 | Step 1915500 | Avg Loss: 0.0153 | Grad Norm: 0.00917790\n",
      "Epoch 3 | Step 1915600 | Avg Loss: 0.0152 | Grad Norm: 0.00891558\n",
      "Epoch 3 | Step 1915700 | Avg Loss: 0.0154 | Grad Norm: 0.00862927\n",
      "Epoch 3 | Step 1915800 | Avg Loss: 0.0155 | Grad Norm: 0.00901778\n",
      "Epoch 3 | Step 1915900 | Avg Loss: 0.0155 | Grad Norm: 0.00906116\n",
      "Epoch 3 | Step 1916000 | Avg Loss: 0.0152 | Grad Norm: 0.00707685\n",
      "Epoch 3 | Step 1916100 | Avg Loss: 0.0149 | Grad Norm: 0.00806799\n",
      "Epoch 3 | Step 1916200 | Avg Loss: 0.0151 | Grad Norm: 0.00972376\n",
      "Epoch 3 | Step 1916300 | Avg Loss: 0.0156 | Grad Norm: 0.00948606\n",
      "Epoch 3 | Step 1916400 | Avg Loss: 0.0155 | Grad Norm: 0.00904135\n",
      "Epoch 3 | Step 1916500 | Avg Loss: 0.0156 | Grad Norm: 0.00941203\n",
      "Epoch 3 | Step 1916600 | Avg Loss: 0.0152 | Grad Norm: 0.00795114\n",
      "Epoch 3 | Step 1916700 | Avg Loss: 0.0155 | Grad Norm: 0.00919610\n",
      "Epoch 3 | Step 1916800 | Avg Loss: 0.0155 | Grad Norm: 0.00855420\n",
      "Epoch 3 | Step 1916900 | Avg Loss: 0.0158 | Grad Norm: 0.00879671\n",
      "Epoch 3 | Step 1917000 | Avg Loss: 0.0155 | Grad Norm: 0.00895257\n",
      "Epoch 3 | Step 1917100 | Avg Loss: 0.0155 | Grad Norm: 0.01022434\n",
      "Epoch 3 | Step 1917200 | Avg Loss: 0.0153 | Grad Norm: 0.00903805\n",
      "Epoch 3 | Step 1917300 | Avg Loss: 0.0153 | Grad Norm: 0.00810284\n",
      "Epoch 3 | Step 1917400 | Avg Loss: 0.0156 | Grad Norm: 0.00901216\n",
      "Epoch 3 | Step 1917500 | Avg Loss: 0.0156 | Grad Norm: 0.00905060\n",
      "Epoch 3 | Step 1917600 | Avg Loss: 0.0151 | Grad Norm: 0.00849926\n",
      "Epoch 3 | Step 1917700 | Avg Loss: 0.0153 | Grad Norm: 0.00827315\n",
      "Epoch 3 | Step 1917800 | Avg Loss: 0.0155 | Grad Norm: 0.00940512\n",
      "Epoch 3 | Step 1917900 | Avg Loss: 0.0154 | Grad Norm: 0.00921836\n",
      "Epoch 3 | Step 1918000 | Avg Loss: 0.0152 | Grad Norm: 0.00910714\n",
      "Epoch 3 | Step 1918100 | Avg Loss: 0.0152 | Grad Norm: 0.00996360\n",
      "Epoch 3 | Step 1918200 | Avg Loss: 0.0151 | Grad Norm: 0.00710621\n",
      "Epoch 3 | Step 1918300 | Avg Loss: 0.0150 | Grad Norm: 0.00808398\n",
      "Epoch 3 | Step 1918400 | Avg Loss: 0.0152 | Grad Norm: 0.01081365\n",
      "Epoch 3 | Step 1918500 | Avg Loss: 0.0154 | Grad Norm: 0.00836095\n",
      "Epoch 3 | Step 1918600 | Avg Loss: 0.0156 | Grad Norm: 0.00885712\n",
      "Epoch 3 | Step 1918700 | Avg Loss: 0.0154 | Grad Norm: 0.00951398\n",
      "Epoch 3 | Step 1918800 | Avg Loss: 0.0155 | Grad Norm: 0.00901163\n",
      "Epoch 3 | Step 1918900 | Avg Loss: 0.0158 | Grad Norm: 0.00950069\n",
      "Epoch 3 | Step 1919000 | Avg Loss: 0.0157 | Grad Norm: 0.00791164\n",
      "Epoch 3 | Step 1919100 | Avg Loss: 0.0151 | Grad Norm: 0.00836955\n",
      "Epoch 3 | Step 1919200 | Avg Loss: 0.0154 | Grad Norm: 0.01355756\n",
      "Epoch 3 | Step 1919300 | Avg Loss: 0.0150 | Grad Norm: 0.00846265\n",
      "Epoch 3 | Step 1919400 | Avg Loss: 0.0149 | Grad Norm: 0.01098480\n",
      "Epoch 3 | Step 1919500 | Avg Loss: 0.0149 | Grad Norm: 0.00852717\n",
      "Epoch 3 | Step 1919600 | Avg Loss: 0.0150 | Grad Norm: 0.00733997\n",
      "Epoch 3 | Step 1919700 | Avg Loss: 0.0150 | Grad Norm: 0.00985743\n",
      "Epoch 3 | Step 1919800 | Avg Loss: 0.0151 | Grad Norm: 0.00745656\n",
      "Epoch 3 | Step 1919900 | Avg Loss: 0.0150 | Grad Norm: 0.00815356\n",
      "Epoch 3 | Step 1920000 | Avg Loss: 0.0145 | Grad Norm: 0.00793226\n",
      "Epoch 3 | Step 1920100 | Avg Loss: 0.0151 | Grad Norm: 0.01118974\n",
      "Epoch 3 | Step 1920200 | Avg Loss: 0.0154 | Grad Norm: 0.01073054\n",
      "Epoch 3 | Step 1920300 | Avg Loss: 0.0152 | Grad Norm: 0.00931569\n",
      "Epoch 3 | Step 1920400 | Avg Loss: 0.0152 | Grad Norm: 0.00830339\n",
      "Epoch 3 | Step 1920500 | Avg Loss: 0.0152 | Grad Norm: 0.00812125\n",
      "Epoch 3 | Step 1920600 | Avg Loss: 0.0152 | Grad Norm: 0.00942016\n",
      "Epoch 3 | Step 1920700 | Avg Loss: 0.0150 | Grad Norm: 0.00799129\n",
      "Epoch 3 | Step 1920800 | Avg Loss: 0.0145 | Grad Norm: 0.01053960\n",
      "Epoch 3 | Step 1920900 | Avg Loss: 0.0151 | Grad Norm: 0.00770162\n",
      "Epoch 3 | Step 1921000 | Avg Loss: 0.0154 | Grad Norm: 0.00871577\n",
      "Epoch 3 | Step 1921100 | Avg Loss: 0.0157 | Grad Norm: 0.00989891\n",
      "Epoch 3 | Step 1921200 | Avg Loss: 0.0160 | Grad Norm: 0.00806990\n",
      "Epoch 3 | Step 1921300 | Avg Loss: 0.0157 | Grad Norm: 0.00929053\n",
      "Epoch 3 | Step 1921400 | Avg Loss: 0.0159 | Grad Norm: 0.00879195\n",
      "Epoch 3 | Step 1921500 | Avg Loss: 0.0155 | Grad Norm: 0.00757605\n",
      "Epoch 3 | Step 1921600 | Avg Loss: 0.0157 | Grad Norm: 0.00833946\n",
      "Epoch 3 | Step 1921700 | Avg Loss: 0.0155 | Grad Norm: 0.01064645\n",
      "Epoch 3 | Step 1921800 | Avg Loss: 0.0154 | Grad Norm: 0.01042119\n",
      "Epoch 3 | Step 1921900 | Avg Loss: 0.0153 | Grad Norm: 0.00844801\n",
      "Epoch 3 | Step 1922000 | Avg Loss: 0.0153 | Grad Norm: 0.00793478\n",
      "Epoch 3 | Step 1922100 | Avg Loss: 0.0153 | Grad Norm: 0.00893253\n",
      "Epoch 3 | Step 1922200 | Avg Loss: 0.0153 | Grad Norm: 0.00845433\n",
      "Epoch 3 | Step 1922300 | Avg Loss: 0.0156 | Grad Norm: 0.00804426\n",
      "Epoch 3 | Step 1922400 | Avg Loss: 0.0159 | Grad Norm: 0.01078421\n",
      "Epoch 3 | Step 1922500 | Avg Loss: 0.0160 | Grad Norm: 0.00863931\n",
      "Epoch 3 | Step 1922600 | Avg Loss: 0.0156 | Grad Norm: 0.00972797\n",
      "Epoch 3 | Step 1922700 | Avg Loss: 0.0156 | Grad Norm: 0.00799333\n",
      "Epoch 3 | Step 1922800 | Avg Loss: 0.0152 | Grad Norm: 0.00869087\n",
      "Epoch 3 | Step 1922900 | Avg Loss: 0.0151 | Grad Norm: 0.00891996\n",
      "Epoch 3 | Step 1923000 | Avg Loss: 0.0154 | Grad Norm: 0.01100608\n",
      "Epoch 3 | Step 1923100 | Avg Loss: 0.0156 | Grad Norm: 0.00873833\n",
      "Epoch 3 | Step 1923200 | Avg Loss: 0.0158 | Grad Norm: 0.00813391\n",
      "Epoch 3 | Step 1923300 | Avg Loss: 0.0159 | Grad Norm: 0.00809813\n",
      "Epoch 3 | Step 1923400 | Avg Loss: 0.0155 | Grad Norm: 0.00946588\n",
      "Epoch 3 | Step 1923500 | Avg Loss: 0.0151 | Grad Norm: 0.00965632\n",
      "Epoch 3 | Step 1923600 | Avg Loss: 0.0153 | Grad Norm: 0.00816739\n",
      "Epoch 3 | Step 1923700 | Avg Loss: 0.0153 | Grad Norm: 0.00899323\n",
      "Epoch 3 | Step 1923800 | Avg Loss: 0.0153 | Grad Norm: 0.00958534\n",
      "Epoch 3 | Step 1923900 | Avg Loss: 0.0157 | Grad Norm: 0.00911534\n",
      "Epoch 3 | Step 1924000 | Avg Loss: 0.0155 | Grad Norm: 0.00922041\n",
      "Epoch 3 | Step 1924100 | Avg Loss: 0.0154 | Grad Norm: 0.00805665\n",
      "Epoch 3 | Step 1924200 | Avg Loss: 0.0152 | Grad Norm: 0.00931730\n",
      "Epoch 3 | Step 1924300 | Avg Loss: 0.0155 | Grad Norm: 0.00783704\n",
      "Epoch 3 | Step 1924400 | Avg Loss: 0.0156 | Grad Norm: 0.01019659\n",
      "Epoch 3 | Step 1924500 | Avg Loss: 0.0156 | Grad Norm: 0.00943528\n",
      "Epoch 3 | Step 1924600 | Avg Loss: 0.0155 | Grad Norm: 0.00821528\n",
      "Epoch 3 | Step 1924700 | Avg Loss: 0.0154 | Grad Norm: 0.00862747\n",
      "Epoch 3 | Step 1924800 | Avg Loss: 0.0151 | Grad Norm: 0.00868209\n",
      "Epoch 3 | Step 1924900 | Avg Loss: 0.0150 | Grad Norm: 0.00801196\n",
      "Epoch 3 | Step 1925000 | Avg Loss: 0.0154 | Grad Norm: 0.00820624\n",
      "Epoch 3 | Step 1925100 | Avg Loss: 0.0157 | Grad Norm: 0.00770040\n",
      "Epoch 3 | Step 1925200 | Avg Loss: 0.0153 | Grad Norm: 0.00903239\n",
      "Epoch 3 | Step 1925300 | Avg Loss: 0.0152 | Grad Norm: 0.00827608\n",
      "Epoch 3 | Step 1925400 | Avg Loss: 0.0150 | Grad Norm: 0.01084623\n",
      "Epoch 3 | Step 1925500 | Avg Loss: 0.0152 | Grad Norm: 0.00929164\n",
      "Epoch 3 | Step 1925600 | Avg Loss: 0.0155 | Grad Norm: 0.00909674\n",
      "Epoch 3 | Step 1925700 | Avg Loss: 0.0157 | Grad Norm: 0.00798019\n",
      "Epoch 3 | Step 1925800 | Avg Loss: 0.0157 | Grad Norm: 0.00959752\n",
      "Epoch 3 | Step 1925900 | Avg Loss: 0.0159 | Grad Norm: 0.00844174\n",
      "Epoch 3 | Step 1926000 | Avg Loss: 0.0153 | Grad Norm: 0.01095695\n",
      "Epoch 3 | Step 1926100 | Avg Loss: 0.0155 | Grad Norm: 0.00940168\n",
      "Epoch 3 | Step 1926200 | Avg Loss: 0.0155 | Grad Norm: 0.00876966\n",
      "Epoch 3 | Step 1926300 | Avg Loss: 0.0154 | Grad Norm: 0.00866185\n",
      "Epoch 3 | Step 1926400 | Avg Loss: 0.0151 | Grad Norm: 0.00963934\n",
      "Epoch 3 | Step 1926500 | Avg Loss: 0.0156 | Grad Norm: 0.00937863\n",
      "Epoch 3 | Step 1926600 | Avg Loss: 0.0156 | Grad Norm: 0.00891570\n",
      "Epoch 3 | Step 1926700 | Avg Loss: 0.0153 | Grad Norm: 0.00895148\n",
      "Epoch 3 | Step 1926800 | Avg Loss: 0.0155 | Grad Norm: 0.00954019\n",
      "Epoch 3 | Step 1926900 | Avg Loss: 0.0154 | Grad Norm: 0.00840251\n",
      "Epoch 3 | Step 1927000 | Avg Loss: 0.0157 | Grad Norm: 0.01080778\n",
      "Epoch 3 | Step 1927100 | Avg Loss: 0.0158 | Grad Norm: 0.00833914\n",
      "Epoch 3 | Step 1927200 | Avg Loss: 0.0161 | Grad Norm: 0.00849557\n",
      "Epoch 3 | Step 1927300 | Avg Loss: 0.0159 | Grad Norm: 0.00865750\n",
      "Epoch 3 | Step 1927400 | Avg Loss: 0.0159 | Grad Norm: 0.00918695\n",
      "Epoch 3 | Step 1927500 | Avg Loss: 0.0157 | Grad Norm: 0.00955233\n",
      "Epoch 3 | Step 1927600 | Avg Loss: 0.0155 | Grad Norm: 0.00906471\n",
      "Epoch 3 | Step 1927700 | Avg Loss: 0.0155 | Grad Norm: 0.00844656\n",
      "Epoch 3 | Step 1927800 | Avg Loss: 0.0155 | Grad Norm: 0.00895766\n",
      "Epoch 3 | Step 1927900 | Avg Loss: 0.0155 | Grad Norm: 0.01000936\n",
      "Epoch 3 | Step 1928000 | Avg Loss: 0.0153 | Grad Norm: 0.00866110\n",
      "Epoch 3 | Step 1928100 | Avg Loss: 0.0152 | Grad Norm: 0.00974365\n",
      "Epoch 3 | Step 1928200 | Avg Loss: 0.0152 | Grad Norm: 0.00968774\n",
      "Epoch 3 | Step 1928300 | Avg Loss: 0.0155 | Grad Norm: 0.00894683\n",
      "Epoch 3 | Step 1928400 | Avg Loss: 0.0156 | Grad Norm: 0.00992946\n",
      "Epoch 3 | Step 1928500 | Avg Loss: 0.0156 | Grad Norm: 0.00707161\n",
      "Epoch 3 | Step 1928600 | Avg Loss: 0.0155 | Grad Norm: 0.00989984\n",
      "Epoch 3 | Step 1928700 | Avg Loss: 0.0155 | Grad Norm: 0.00906626\n",
      "Epoch 3 | Step 1928800 | Avg Loss: 0.0155 | Grad Norm: 0.00858883\n",
      "Epoch 3 | Step 1928900 | Avg Loss: 0.0155 | Grad Norm: 0.00804561\n",
      "Epoch 3 | Step 1929000 | Avg Loss: 0.0155 | Grad Norm: 0.01176420\n",
      "Epoch 3 | Step 1929100 | Avg Loss: 0.0153 | Grad Norm: 0.00851966\n",
      "Epoch 3 | Step 1929200 | Avg Loss: 0.0152 | Grad Norm: 0.00866648\n",
      "Epoch 3 | Step 1929300 | Avg Loss: 0.0150 | Grad Norm: 0.01228842\n",
      "Epoch 3 | Step 1929400 | Avg Loss: 0.0151 | Grad Norm: 0.01941639\n",
      "Epoch 3 | Step 1929500 | Avg Loss: 0.0154 | Grad Norm: 0.00844599\n",
      "Epoch 3 | Step 1929600 | Avg Loss: 0.0155 | Grad Norm: 0.00926199\n",
      "Epoch 3 | Step 1929700 | Avg Loss: 0.0157 | Grad Norm: 0.00987662\n",
      "Epoch 3 | Step 1929800 | Avg Loss: 0.0154 | Grad Norm: 0.00859789\n",
      "Epoch 3 | Step 1929900 | Avg Loss: 0.0150 | Grad Norm: 0.00780138\n",
      "Epoch 3 | Step 1930000 | Avg Loss: 0.0156 | Grad Norm: 0.00774896\n",
      "Epoch 3 | Step 1930100 | Avg Loss: 0.0155 | Grad Norm: 0.01057821\n",
      "Epoch 3 | Step 1930200 | Avg Loss: 0.0155 | Grad Norm: 0.00938115\n",
      "Epoch 3 | Step 1930300 | Avg Loss: 0.0153 | Grad Norm: 0.00794036\n",
      "Epoch 3 | Step 1930400 | Avg Loss: 0.0154 | Grad Norm: 0.00857897\n",
      "Epoch 3 | Step 1930500 | Avg Loss: 0.0156 | Grad Norm: 0.00909695\n",
      "Epoch 3 | Step 1930600 | Avg Loss: 0.0156 | Grad Norm: 0.00821035\n",
      "Epoch 3 | Step 1930700 | Avg Loss: 0.0154 | Grad Norm: 0.00768886\n",
      "Epoch 3 | Step 1930800 | Avg Loss: 0.0157 | Grad Norm: 0.00854082\n",
      "Epoch 3 | Step 1930900 | Avg Loss: 0.0157 | Grad Norm: 0.00899201\n",
      "Epoch 3 | Step 1931000 | Avg Loss: 0.0160 | Grad Norm: 0.00879824\n",
      "Epoch 3 | Step 1931100 | Avg Loss: 0.0157 | Grad Norm: 0.00828296\n",
      "Epoch 3 | Step 1931200 | Avg Loss: 0.0155 | Grad Norm: 0.00889499\n",
      "Epoch 3 | Step 1931300 | Avg Loss: 0.0155 | Grad Norm: 0.00931185\n",
      "Epoch 3 | Step 1931400 | Avg Loss: 0.0151 | Grad Norm: 0.00886477\n",
      "Epoch 3 | Step 1931500 | Avg Loss: 0.0153 | Grad Norm: 0.00906892\n",
      "Epoch 3 | Step 1931600 | Avg Loss: 0.0154 | Grad Norm: 0.00759067\n",
      "Epoch 3 | Step 1931700 | Avg Loss: 0.0150 | Grad Norm: 0.00965231\n",
      "Epoch 3 | Step 1931800 | Avg Loss: 0.0153 | Grad Norm: 0.00804141\n",
      "Epoch 3 | Step 1931900 | Avg Loss: 0.0157 | Grad Norm: 0.00872106\n",
      "Epoch 3 | Step 1932000 | Avg Loss: 0.0158 | Grad Norm: 0.00855313\n",
      "Epoch 3 | Step 1932100 | Avg Loss: 0.0157 | Grad Norm: 0.01015091\n",
      "Epoch 3 | Step 1932200 | Avg Loss: 0.0158 | Grad Norm: 0.00763412\n",
      "Epoch 3 | Step 1932300 | Avg Loss: 0.0154 | Grad Norm: 0.01192792\n",
      "Epoch 3 | Step 1932400 | Avg Loss: 0.0155 | Grad Norm: 0.01074267\n",
      "Epoch 3 | Step 1932500 | Avg Loss: 0.0156 | Grad Norm: 0.00860198\n",
      "Epoch 3 | Step 1932600 | Avg Loss: 0.0151 | Grad Norm: 0.00834235\n",
      "Epoch 3 | Step 1932700 | Avg Loss: 0.0152 | Grad Norm: 0.01253850\n",
      "Epoch 3 | Step 1932800 | Avg Loss: 0.0152 | Grad Norm: 0.00871622\n",
      "Epoch 3 | Step 1932900 | Avg Loss: 0.0149 | Grad Norm: 0.00899342\n",
      "Epoch 3 | Step 1933000 | Avg Loss: 0.0146 | Grad Norm: 0.00887143\n",
      "Epoch 3 | Step 1933100 | Avg Loss: 0.0144 | Grad Norm: 0.00978603\n",
      "Epoch 3 | Step 1933200 | Avg Loss: 0.0147 | Grad Norm: 0.00877982\n",
      "Epoch 3 | Step 1933300 | Avg Loss: 0.0150 | Grad Norm: 0.00831974\n",
      "Epoch 3 | Step 1933400 | Avg Loss: 0.0148 | Grad Norm: 0.00768834\n",
      "Epoch 3 | Step 1933500 | Avg Loss: 0.0152 | Grad Norm: 0.00887922\n",
      "Epoch 3 | Step 1933600 | Avg Loss: 0.0153 | Grad Norm: 0.00715028\n",
      "Epoch 3 | Step 1933700 | Avg Loss: 0.0153 | Grad Norm: 0.00827867\n",
      "Epoch 3 | Step 1933800 | Avg Loss: 0.0155 | Grad Norm: 0.00993611\n",
      "Epoch 3 | Step 1933900 | Avg Loss: 0.0152 | Grad Norm: 0.00859078\n",
      "Epoch 3 | Step 1934000 | Avg Loss: 0.0153 | Grad Norm: 0.00840958\n",
      "Epoch 3 | Step 1934100 | Avg Loss: 0.0153 | Grad Norm: 0.00891572\n",
      "Epoch 3 | Step 1934200 | Avg Loss: 0.0152 | Grad Norm: 0.00865202\n",
      "Epoch 3 | Step 1934300 | Avg Loss: 0.0152 | Grad Norm: 0.00861647\n",
      "Epoch 3 | Step 1934400 | Avg Loss: 0.0149 | Grad Norm: 0.00865726\n",
      "Epoch 3 | Step 1934500 | Avg Loss: 0.0148 | Grad Norm: 0.00895045\n",
      "Epoch 3 | Step 1934600 | Avg Loss: 0.0147 | Grad Norm: 0.01106067\n",
      "Epoch 3 | Step 1934700 | Avg Loss: 0.0153 | Grad Norm: 0.00882264\n",
      "Epoch 3 | Step 1934800 | Avg Loss: 0.0154 | Grad Norm: 0.00798536\n",
      "Epoch 3 | Step 1934900 | Avg Loss: 0.0157 | Grad Norm: 0.00820170\n",
      "Epoch 3 | Step 1935000 | Avg Loss: 0.0156 | Grad Norm: 0.00892552\n",
      "Epoch 3 | Step 1935100 | Avg Loss: 0.0155 | Grad Norm: 0.00779305\n",
      "Epoch 3 | Step 1935200 | Avg Loss: 0.0151 | Grad Norm: 0.00801593\n",
      "Epoch 3 | Step 1935300 | Avg Loss: 0.0152 | Grad Norm: 0.00865379\n",
      "Epoch 3 | Step 1935400 | Avg Loss: 0.0151 | Grad Norm: 0.00779962\n",
      "Epoch 3 | Step 1935500 | Avg Loss: 0.0149 | Grad Norm: 0.00769591\n",
      "Epoch 3 | Step 1935600 | Avg Loss: 0.0154 | Grad Norm: 0.00800105\n",
      "Epoch 3 | Step 1935700 | Avg Loss: 0.0156 | Grad Norm: 0.00981890\n",
      "Epoch 3 | Step 1935800 | Avg Loss: 0.0155 | Grad Norm: 0.00879172\n",
      "Epoch 3 | Step 1935900 | Avg Loss: 0.0154 | Grad Norm: 0.00892747\n",
      "Epoch 3 | Step 1936000 | Avg Loss: 0.0155 | Grad Norm: 0.00864719\n",
      "Epoch 3 | Step 1936100 | Avg Loss: 0.0155 | Grad Norm: 0.00905264\n",
      "Epoch 3 | Step 1936200 | Avg Loss: 0.0154 | Grad Norm: 0.00784565\n",
      "Epoch 3 | Step 1936300 | Avg Loss: 0.0156 | Grad Norm: 0.00921898\n",
      "Epoch 3 | Step 1936400 | Avg Loss: 0.0153 | Grad Norm: 0.00966040\n",
      "Epoch 3 | Step 1936500 | Avg Loss: 0.0155 | Grad Norm: 0.00903650\n",
      "Epoch 3 | Step 1936600 | Avg Loss: 0.0151 | Grad Norm: 0.00908537\n",
      "Epoch 3 | Step 1936700 | Avg Loss: 0.0151 | Grad Norm: 0.00922258\n",
      "Epoch 3 | Step 1936800 | Avg Loss: 0.0151 | Grad Norm: 0.00911062\n",
      "Epoch 3 | Step 1936900 | Avg Loss: 0.0152 | Grad Norm: 0.00733550\n",
      "Epoch 3 | Step 1937000 | Avg Loss: 0.0152 | Grad Norm: 0.00858835\n",
      "Epoch 3 | Step 1937100 | Avg Loss: 0.0153 | Grad Norm: 0.01004243\n",
      "Epoch 3 | Step 1937200 | Avg Loss: 0.0156 | Grad Norm: 0.00895695\n",
      "Epoch 3 | Step 1937300 | Avg Loss: 0.0156 | Grad Norm: 0.00933808\n",
      "Epoch 3 | Step 1937400 | Avg Loss: 0.0151 | Grad Norm: 0.00805311\n",
      "Epoch 3 | Step 1937500 | Avg Loss: 0.0154 | Grad Norm: 0.00775531\n",
      "Epoch 3 | Step 1937600 | Avg Loss: 0.0153 | Grad Norm: 0.00984273\n",
      "Epoch 3 | Step 1937700 | Avg Loss: 0.0154 | Grad Norm: 0.01036463\n",
      "Epoch 3 | Step 1937800 | Avg Loss: 0.0154 | Grad Norm: 0.01016623\n",
      "Epoch 3 | Step 1937900 | Avg Loss: 0.0152 | Grad Norm: 0.00920807\n",
      "Epoch 3 | Step 1938000 | Avg Loss: 0.0151 | Grad Norm: 0.00925075\n",
      "Epoch 3 | Step 1938100 | Avg Loss: 0.0154 | Grad Norm: 0.00721953\n",
      "Epoch 3 | Step 1938200 | Avg Loss: 0.0154 | Grad Norm: 0.00914646\n",
      "Epoch 3 | Step 1938300 | Avg Loss: 0.0157 | Grad Norm: 0.01153468\n",
      "Epoch 3 | Step 1938400 | Avg Loss: 0.0159 | Grad Norm: 0.01026792\n",
      "Epoch 3 | Step 1938500 | Avg Loss: 0.0156 | Grad Norm: 0.00888996\n",
      "Epoch 3 | Step 1938600 | Avg Loss: 0.0159 | Grad Norm: 0.00964251\n",
      "Epoch 3 | Step 1938700 | Avg Loss: 0.0164 | Grad Norm: 0.00878747\n",
      "Epoch 3 | Step 1938800 | Avg Loss: 0.0163 | Grad Norm: 0.01005967\n",
      "Epoch 3 | Step 1938900 | Avg Loss: 0.0167 | Grad Norm: 0.00935563\n",
      "Epoch 3 | Step 1939000 | Avg Loss: 0.0167 | Grad Norm: 0.00919032\n",
      "Epoch 3 | Step 1939100 | Avg Loss: 0.0161 | Grad Norm: 0.00808664\n",
      "Epoch 3 | Step 1939200 | Avg Loss: 0.0163 | Grad Norm: 0.01007287\n",
      "Epoch 3 | Step 1939300 | Avg Loss: 0.0165 | Grad Norm: 0.00864199\n",
      "Epoch 3 | Step 1939400 | Avg Loss: 0.0165 | Grad Norm: 0.01004481\n",
      "Epoch 3 | Step 1939500 | Avg Loss: 0.0165 | Grad Norm: 0.01410048\n",
      "Epoch 3 | Step 1939600 | Avg Loss: 0.0161 | Grad Norm: 0.00920700\n",
      "Epoch 3 | Step 1939700 | Avg Loss: 0.0158 | Grad Norm: 0.00952094\n",
      "Epoch 3 | Step 1939800 | Avg Loss: 0.0154 | Grad Norm: 0.00992804\n",
      "Epoch 3 | Step 1939900 | Avg Loss: 0.0155 | Grad Norm: 0.01083885\n",
      "Epoch 3 | Step 1940000 | Avg Loss: 0.0154 | Grad Norm: 0.01057135\n",
      "Epoch 3 | Step 1940100 | Avg Loss: 0.0156 | Grad Norm: 0.00845853\n",
      "Epoch 3 | Step 1940200 | Avg Loss: 0.0153 | Grad Norm: 0.00800444\n",
      "Epoch 3 | Step 1940300 | Avg Loss: 0.0151 | Grad Norm: 0.01087104\n",
      "Epoch 3 | Step 1940400 | Avg Loss: 0.0157 | Grad Norm: 0.00807714\n",
      "Epoch 3 | Step 1940500 | Avg Loss: 0.0155 | Grad Norm: 0.00896261\n",
      "Epoch 3 | Step 1940600 | Avg Loss: 0.0155 | Grad Norm: 0.00952637\n",
      "Epoch 3 | Step 1940700 | Avg Loss: 0.0154 | Grad Norm: 0.00855857\n",
      "Epoch 3 | Step 1940800 | Avg Loss: 0.0156 | Grad Norm: 0.00912329\n",
      "Epoch 3 | Step 1940900 | Avg Loss: 0.0152 | Grad Norm: 0.00788530\n",
      "Epoch 3 | Step 1941000 | Avg Loss: 0.0155 | Grad Norm: 0.00873815\n",
      "Epoch 3 | Step 1941100 | Avg Loss: 0.0156 | Grad Norm: 0.01149771\n",
      "Epoch 3 | Step 1941200 | Avg Loss: 0.0158 | Grad Norm: 0.00890140\n",
      "Epoch 3 | Step 1941300 | Avg Loss: 0.0159 | Grad Norm: 0.00883463\n",
      "Epoch 3 | Step 1941400 | Avg Loss: 0.0158 | Grad Norm: 0.00904834\n",
      "Epoch 3 | Step 1941500 | Avg Loss: 0.0157 | Grad Norm: 0.00980858\n",
      "Epoch 3 | Step 1941600 | Avg Loss: 0.0162 | Grad Norm: 0.00880433\n",
      "Epoch 3 | Step 1941700 | Avg Loss: 0.0156 | Grad Norm: 0.00925901\n",
      "Epoch 3 | Step 1941800 | Avg Loss: 0.0157 | Grad Norm: 0.00892282\n",
      "Epoch 3 | Step 1941900 | Avg Loss: 0.0158 | Grad Norm: 0.00975169\n",
      "Epoch 3 | Step 1942000 | Avg Loss: 0.0157 | Grad Norm: 0.00911569\n",
      "Epoch 3 | Step 1942100 | Avg Loss: 0.0157 | Grad Norm: 0.00930182\n",
      "Epoch 3 | Step 1942200 | Avg Loss: 0.0156 | Grad Norm: 0.00904298\n",
      "Epoch 3 | Step 1942300 | Avg Loss: 0.0155 | Grad Norm: 0.00803558\n",
      "Epoch 3 | Step 1942400 | Avg Loss: 0.0155 | Grad Norm: 0.00884152\n",
      "Epoch 3 | Step 1942500 | Avg Loss: 0.0154 | Grad Norm: 0.00914934\n",
      "Epoch 3 | Step 1942600 | Avg Loss: 0.0153 | Grad Norm: 0.00909317\n",
      "Epoch 3 | Step 1942700 | Avg Loss: 0.0157 | Grad Norm: 0.00929478\n",
      "Epoch 3 | Step 1942800 | Avg Loss: 0.0157 | Grad Norm: 0.00868943\n",
      "Epoch 3 | Step 1942900 | Avg Loss: 0.0154 | Grad Norm: 0.00993909\n",
      "Epoch 3 | Step 1943000 | Avg Loss: 0.0155 | Grad Norm: 0.00879287\n",
      "Epoch 3 | Step 1943100 | Avg Loss: 0.0155 | Grad Norm: 0.00892441\n",
      "Epoch 3 | Step 1943200 | Avg Loss: 0.0156 | Grad Norm: 0.00820843\n",
      "Epoch 3 | Step 1943300 | Avg Loss: 0.0159 | Grad Norm: 0.01012375\n",
      "Epoch 3 | Step 1943400 | Avg Loss: 0.0163 | Grad Norm: 0.01047530\n",
      "Epoch 3 | Step 1943500 | Avg Loss: 0.0165 | Grad Norm: 0.00980766\n",
      "Epoch 3 | Step 1943600 | Avg Loss: 0.0159 | Grad Norm: 0.00965198\n",
      "Epoch 3 | Step 1943700 | Avg Loss: 0.0159 | Grad Norm: 0.00942520\n",
      "Epoch 3 | Step 1943800 | Avg Loss: 0.0152 | Grad Norm: 0.00895686\n",
      "Epoch 3 | Step 1943900 | Avg Loss: 0.0153 | Grad Norm: 0.00727245\n",
      "Epoch 3 | Step 1944000 | Avg Loss: 0.0154 | Grad Norm: 0.00902404\n",
      "Epoch 3 | Step 1944100 | Avg Loss: 0.0160 | Grad Norm: 0.00815928\n",
      "Epoch 3 | Step 1944200 | Avg Loss: 0.0156 | Grad Norm: 0.00936921\n",
      "Epoch 3 | Step 1944300 | Avg Loss: 0.0154 | Grad Norm: 0.00886546\n",
      "Epoch 3 | Step 1944400 | Avg Loss: 0.0153 | Grad Norm: 0.00815933\n",
      "Epoch 3 | Step 1944500 | Avg Loss: 0.0152 | Grad Norm: 0.00844113\n",
      "Epoch 3 | Step 1944600 | Avg Loss: 0.0153 | Grad Norm: 0.00885665\n",
      "Epoch 3 | Step 1944700 | Avg Loss: 0.0158 | Grad Norm: 0.00822381\n",
      "Epoch 3 | Step 1944800 | Avg Loss: 0.0159 | Grad Norm: 0.00888972\n",
      "Epoch 3 | Step 1944900 | Avg Loss: 0.0156 | Grad Norm: 0.01054547\n",
      "Epoch 3 | Step 1945000 | Avg Loss: 0.0158 | Grad Norm: 0.00885772\n",
      "Epoch 3 | Step 1945100 | Avg Loss: 0.0157 | Grad Norm: 0.00780681\n",
      "Epoch 3 | Step 1945200 | Avg Loss: 0.0154 | Grad Norm: 0.01123699\n",
      "Epoch 3 | Step 1945300 | Avg Loss: 0.0154 | Grad Norm: 0.00871689\n",
      "Epoch 3 | Step 1945400 | Avg Loss: 0.0153 | Grad Norm: 0.01007915\n",
      "Epoch 3 | Step 1945500 | Avg Loss: 0.0156 | Grad Norm: 0.00795165\n",
      "Epoch 3 | Step 1945600 | Avg Loss: 0.0155 | Grad Norm: 0.01019660\n",
      "Epoch 3 | Step 1945700 | Avg Loss: 0.0154 | Grad Norm: 0.00798628\n",
      "Epoch 3 | Step 1945800 | Avg Loss: 0.0156 | Grad Norm: 0.00894278\n",
      "Epoch 3 | Step 1945900 | Avg Loss: 0.0153 | Grad Norm: 0.00984167\n",
      "Epoch 3 | Step 1946000 | Avg Loss: 0.0150 | Grad Norm: 0.00887320\n",
      "Epoch 3 | Step 1946100 | Avg Loss: 0.0154 | Grad Norm: 0.00895887\n",
      "Epoch 3 | Step 1946200 | Avg Loss: 0.0152 | Grad Norm: 0.00890834\n",
      "Epoch 3 | Step 1946300 | Avg Loss: 0.0151 | Grad Norm: 0.00811119\n",
      "Epoch 3 | Step 1946400 | Avg Loss: 0.0149 | Grad Norm: 0.00797005\n",
      "Epoch 3 | Step 1946500 | Avg Loss: 0.0149 | Grad Norm: 0.00950344\n",
      "Epoch 3 | Step 1946600 | Avg Loss: 0.0153 | Grad Norm: 0.00899472\n",
      "Epoch 3 | Step 1946700 | Avg Loss: 0.0153 | Grad Norm: 0.00862673\n",
      "Epoch 3 | Step 1946800 | Avg Loss: 0.0153 | Grad Norm: 0.00890966\n",
      "Epoch 3 | Step 1946900 | Avg Loss: 0.0151 | Grad Norm: 0.00890903\n",
      "Epoch 3 | Step 1947000 | Avg Loss: 0.0158 | Grad Norm: 0.01119261\n",
      "Epoch 3 | Step 1947100 | Avg Loss: 0.0158 | Grad Norm: 0.00969733\n",
      "Epoch 3 | Step 1947200 | Avg Loss: 0.0156 | Grad Norm: 0.00933590\n",
      "Epoch 3 | Step 1947300 | Avg Loss: 0.0158 | Grad Norm: 0.00911617\n",
      "Epoch 3 | Step 1947400 | Avg Loss: 0.0157 | Grad Norm: 0.00967973\n",
      "Epoch 3 | Step 1947500 | Avg Loss: 0.0156 | Grad Norm: 0.00837324\n",
      "Epoch 3 | Step 1947600 | Avg Loss: 0.0153 | Grad Norm: 0.00967339\n",
      "Epoch 3 | Step 1947700 | Avg Loss: 0.0153 | Grad Norm: 0.00889970\n",
      "Epoch 3 | Step 1947800 | Avg Loss: 0.0153 | Grad Norm: 0.00882081\n",
      "Epoch 3 | Step 1947900 | Avg Loss: 0.0155 | Grad Norm: 0.00815888\n",
      "Epoch 3 | Step 1948000 | Avg Loss: 0.0155 | Grad Norm: 0.00911940\n",
      "Epoch 3 | Step 1948100 | Avg Loss: 0.0156 | Grad Norm: 0.00789353\n",
      "Epoch 3 | Step 1948200 | Avg Loss: 0.0158 | Grad Norm: 0.00908909\n",
      "Epoch 3 | Step 1948300 | Avg Loss: 0.0156 | Grad Norm: 0.00970809\n",
      "Epoch 3 | Step 1948400 | Avg Loss: 0.0155 | Grad Norm: 0.00854402\n",
      "Epoch 3 | Step 1948500 | Avg Loss: 0.0152 | Grad Norm: 0.00796190\n",
      "Epoch 3 | Step 1948600 | Avg Loss: 0.0151 | Grad Norm: 0.01051625\n",
      "Epoch 3 | Step 1948700 | Avg Loss: 0.0151 | Grad Norm: 0.00795759\n",
      "Epoch 3 | Step 1948800 | Avg Loss: 0.0150 | Grad Norm: 0.00760577\n",
      "Epoch 3 | Step 1948900 | Avg Loss: 0.0152 | Grad Norm: 0.00988565\n",
      "Epoch 3 | Step 1949000 | Avg Loss: 0.0149 | Grad Norm: 0.00870669\n",
      "Epoch 3 | Step 1949100 | Avg Loss: 0.0152 | Grad Norm: 0.00907982\n",
      "Epoch 3 | Step 1949200 | Avg Loss: 0.0149 | Grad Norm: 0.00803437\n",
      "Epoch 3 | Step 1949300 | Avg Loss: 0.0149 | Grad Norm: 0.00895843\n",
      "Epoch 3 | Step 1949400 | Avg Loss: 0.0151 | Grad Norm: 0.00912572\n",
      "Epoch 3 | Step 1949500 | Avg Loss: 0.0152 | Grad Norm: 0.00746612\n",
      "Epoch 3 | Step 1949600 | Avg Loss: 0.0151 | Grad Norm: 0.00803893\n",
      "Epoch 3 | Step 1949700 | Avg Loss: 0.0149 | Grad Norm: 0.00792648\n",
      "Epoch 3 | Step 1949800 | Avg Loss: 0.0149 | Grad Norm: 0.00956163\n",
      "Epoch 3 | Step 1949900 | Avg Loss: 0.0151 | Grad Norm: 0.00903645\n",
      "Epoch 3 | Step 1950000 | Avg Loss: 0.0153 | Grad Norm: 0.00925770\n",
      "Epoch 3 | Step 1950100 | Avg Loss: 0.0159 | Grad Norm: 0.00840755\n",
      "Epoch 3 | Step 1950200 | Avg Loss: 0.0159 | Grad Norm: 0.00946730\n",
      "Epoch 3 | Step 1950300 | Avg Loss: 0.0157 | Grad Norm: 0.00910007\n",
      "Epoch 3 | Step 1950400 | Avg Loss: 0.0157 | Grad Norm: 0.00958223\n",
      "Epoch 3 | Step 1950500 | Avg Loss: 0.0158 | Grad Norm: 0.00797308\n",
      "Epoch 3 | Step 1950600 | Avg Loss: 0.0156 | Grad Norm: 0.00794211\n",
      "Epoch 3 | Step 1950700 | Avg Loss: 0.0158 | Grad Norm: 0.00808827\n",
      "Epoch 3 | Step 1950800 | Avg Loss: 0.0159 | Grad Norm: 0.01199574\n",
      "Epoch 3 | Step 1950900 | Avg Loss: 0.0159 | Grad Norm: 0.00856444\n",
      "Epoch 3 | Step 1951000 | Avg Loss: 0.0158 | Grad Norm: 0.00767307\n",
      "Epoch 3 | Step 1951100 | Avg Loss: 0.0160 | Grad Norm: 0.00882182\n",
      "Epoch 3 | Step 1951200 | Avg Loss: 0.0160 | Grad Norm: 0.01022434\n",
      "Epoch 3 | Step 1951300 | Avg Loss: 0.0162 | Grad Norm: 0.00971420\n",
      "Epoch 3 | Step 1951400 | Avg Loss: 0.0159 | Grad Norm: 0.00935422\n",
      "Epoch 3 | Step 1951500 | Avg Loss: 0.0160 | Grad Norm: 0.01018210\n",
      "Epoch 3 | Step 1951600 | Avg Loss: 0.0157 | Grad Norm: 0.00956255\n",
      "Epoch 3 | Step 1951700 | Avg Loss: 0.0157 | Grad Norm: 0.00902933\n",
      "Epoch 3 | Step 1951800 | Avg Loss: 0.0158 | Grad Norm: 0.00915307\n",
      "Epoch 3 | Step 1951900 | Avg Loss: 0.0158 | Grad Norm: 0.00831911\n",
      "Epoch 3 | Step 1952000 | Avg Loss: 0.0159 | Grad Norm: 0.00866195\n",
      "Epoch 3 | Step 1952100 | Avg Loss: 0.0159 | Grad Norm: 0.00955083\n",
      "Epoch 3 | Step 1952200 | Avg Loss: 0.0159 | Grad Norm: 0.00785817\n",
      "Epoch 3 | Step 1952300 | Avg Loss: 0.0158 | Grad Norm: 0.00976631\n",
      "Epoch 3 | Step 1952400 | Avg Loss: 0.0157 | Grad Norm: 0.00925147\n",
      "Epoch 3 | Step 1952500 | Avg Loss: 0.0155 | Grad Norm: 0.00907200\n",
      "Epoch 3 | Step 1952600 | Avg Loss: 0.0159 | Grad Norm: 0.00863943\n",
      "Epoch 3 | Step 1952700 | Avg Loss: 0.0160 | Grad Norm: 0.00888486\n",
      "Epoch 3 | Step 1952800 | Avg Loss: 0.0161 | Grad Norm: 0.01014541\n",
      "Epoch 3 | Step 1952900 | Avg Loss: 0.0164 | Grad Norm: 0.00911714\n",
      "Epoch 3 | Step 1953000 | Avg Loss: 0.0163 | Grad Norm: 0.00870237\n",
      "Epoch 3 | Step 1953100 | Avg Loss: 0.0164 | Grad Norm: 0.00881189\n",
      "Epoch 3 | Step 1953200 | Avg Loss: 0.0160 | Grad Norm: 0.00791978\n",
      "Epoch 3 | Step 1953300 | Avg Loss: 0.0162 | Grad Norm: 0.01179764\n",
      "Epoch 3 | Step 1953400 | Avg Loss: 0.0159 | Grad Norm: 0.00976040\n",
      "Epoch 3 | Step 1953500 | Avg Loss: 0.0158 | Grad Norm: 0.00875678\n",
      "Epoch 3 | Step 1953600 | Avg Loss: 0.0161 | Grad Norm: 0.00794084\n",
      "Epoch 3 | Step 1953700 | Avg Loss: 0.0161 | Grad Norm: 0.01090095\n",
      "Epoch 3 | Step 1953800 | Avg Loss: 0.0160 | Grad Norm: 0.00901495\n",
      "Epoch 3 | Step 1953900 | Avg Loss: 0.0160 | Grad Norm: 0.00886499\n",
      "Epoch 3 | Step 1954000 | Avg Loss: 0.0161 | Grad Norm: 0.00919445\n",
      "Epoch 3 | Step 1954100 | Avg Loss: 0.0160 | Grad Norm: 0.00888151\n",
      "Epoch 3 | Step 1954200 | Avg Loss: 0.0159 | Grad Norm: 0.00931069\n",
      "Epoch 3 | Step 1954300 | Avg Loss: 0.0158 | Grad Norm: 0.00968466\n",
      "Epoch 3 | Step 1954400 | Avg Loss: 0.0157 | Grad Norm: 0.00949368\n",
      "Epoch 3 | Step 1954500 | Avg Loss: 0.0159 | Grad Norm: 0.01029336\n",
      "Epoch 3 | Step 1954600 | Avg Loss: 0.0159 | Grad Norm: 0.00919594\n",
      "Epoch 3 | Step 1954700 | Avg Loss: 0.0158 | Grad Norm: 0.00959952\n",
      "Epoch 3 | Step 1954800 | Avg Loss: 0.0157 | Grad Norm: 0.00912043\n",
      "Epoch 3 | Step 1954900 | Avg Loss: 0.0157 | Grad Norm: 0.00874783\n",
      "Epoch 3 | Step 1955000 | Avg Loss: 0.0155 | Grad Norm: 0.01032569\n",
      "Epoch 3 | Step 1955100 | Avg Loss: 0.0156 | Grad Norm: 0.00880223\n",
      "Epoch 3 | Step 1955200 | Avg Loss: 0.0157 | Grad Norm: 0.00931037\n",
      "Epoch 3 | Step 1955300 | Avg Loss: 0.0158 | Grad Norm: 0.01304936\n",
      "Epoch 3 | Step 1955400 | Avg Loss: 0.0154 | Grad Norm: 0.00857439\n",
      "Epoch 3 | Step 1955500 | Avg Loss: 0.0156 | Grad Norm: 0.01230116\n",
      "Epoch 3 | Step 1955600 | Avg Loss: 0.0158 | Grad Norm: 0.00868849\n",
      "Epoch 3 | Step 1955700 | Avg Loss: 0.0154 | Grad Norm: 0.00847124\n",
      "Epoch 3 | Step 1955800 | Avg Loss: 0.0157 | Grad Norm: 0.01013766\n",
      "Epoch 3 | Step 1955900 | Avg Loss: 0.0157 | Grad Norm: 0.01091170\n",
      "Epoch 3 | Step 1956000 | Avg Loss: 0.0159 | Grad Norm: 0.00893675\n",
      "Epoch 3 | Step 1956100 | Avg Loss: 0.0157 | Grad Norm: 0.00845141\n",
      "Epoch 3 | Step 1956200 | Avg Loss: 0.0160 | Grad Norm: 0.00919349\n",
      "Epoch 3 | Step 1956300 | Avg Loss: 0.0162 | Grad Norm: 0.01043331\n",
      "Epoch 3 | Step 1956400 | Avg Loss: 0.0163 | Grad Norm: 0.00896831\n",
      "Epoch 3 | Step 1956500 | Avg Loss: 0.0164 | Grad Norm: 0.00989763\n",
      "Epoch 3 | Step 1956600 | Avg Loss: 0.0159 | Grad Norm: 0.00838891\n",
      "Epoch 3 | Step 1956700 | Avg Loss: 0.0161 | Grad Norm: 0.00907197\n",
      "Epoch 3 | Step 1956800 | Avg Loss: 0.0163 | Grad Norm: 0.01035971\n",
      "Epoch 3 | Step 1956900 | Avg Loss: 0.0161 | Grad Norm: 0.00920486\n",
      "Epoch 3 | Step 1957000 | Avg Loss: 0.0161 | Grad Norm: 0.00979529\n",
      "Epoch 3 | Step 1957100 | Avg Loss: 0.0159 | Grad Norm: 0.00836435\n",
      "Epoch 3 | Step 1957200 | Avg Loss: 0.0158 | Grad Norm: 0.01217325\n",
      "Epoch 3 | Step 1957300 | Avg Loss: 0.0157 | Grad Norm: 0.00867234\n",
      "Epoch 3 | Step 1957400 | Avg Loss: 0.0157 | Grad Norm: 0.00972050\n",
      "Epoch 3 | Step 1957500 | Avg Loss: 0.0154 | Grad Norm: 0.00933848\n",
      "Epoch 3 | Step 1957600 | Avg Loss: 0.0152 | Grad Norm: 0.00966498\n",
      "Epoch 3 | Step 1957700 | Avg Loss: 0.0156 | Grad Norm: 0.00819700\n",
      "Epoch 3 | Step 1957800 | Avg Loss: 0.0155 | Grad Norm: 0.00877607\n",
      "Epoch 3 | Step 1957900 | Avg Loss: 0.0157 | Grad Norm: 0.00801144\n",
      "Epoch 3 | Step 1958000 | Avg Loss: 0.0153 | Grad Norm: 0.00815284\n",
      "Epoch 3 | Step 1958100 | Avg Loss: 0.0155 | Grad Norm: 0.00932393\n",
      "Epoch 3 | Step 1958200 | Avg Loss: 0.0154 | Grad Norm: 0.00875134\n",
      "Epoch 3 | Step 1958300 | Avg Loss: 0.0155 | Grad Norm: 0.00784131\n",
      "Epoch 3 | Step 1958400 | Avg Loss: 0.0156 | Grad Norm: 0.00883558\n",
      "Epoch 3 | Step 1958500 | Avg Loss: 0.0157 | Grad Norm: 0.00812672\n",
      "Epoch 3 | Step 1958600 | Avg Loss: 0.0151 | Grad Norm: 0.00844123\n",
      "Epoch 3 | Step 1958700 | Avg Loss: 0.0151 | Grad Norm: 0.00887364\n",
      "Epoch 3 | Step 1958800 | Avg Loss: 0.0156 | Grad Norm: 0.00795520\n",
      "Epoch 3 | Step 1958900 | Avg Loss: 0.0154 | Grad Norm: 0.00941613\n",
      "Epoch 3 | Step 1959000 | Avg Loss: 0.0152 | Grad Norm: 0.01043303\n",
      "Epoch 3 | Step 1959100 | Avg Loss: 0.0153 | Grad Norm: 0.00906002\n",
      "Epoch 3 | Step 1959200 | Avg Loss: 0.0151 | Grad Norm: 0.00898489\n",
      "Epoch 3 | Step 1959300 | Avg Loss: 0.0153 | Grad Norm: 0.00905193\n",
      "Epoch 3 | Step 1959400 | Avg Loss: 0.0156 | Grad Norm: 0.00939604\n",
      "Epoch 3 | Step 1959500 | Avg Loss: 0.0159 | Grad Norm: 0.01154846\n",
      "Epoch 3 | Step 1959600 | Avg Loss: 0.0155 | Grad Norm: 0.00912320\n",
      "Epoch 3 | Step 1959700 | Avg Loss: 0.0157 | Grad Norm: 0.01028055\n",
      "Epoch 3 | Step 1959800 | Avg Loss: 0.0157 | Grad Norm: 0.00963243\n",
      "Epoch 3 | Step 1959900 | Avg Loss: 0.0159 | Grad Norm: 0.01037787\n",
      "Epoch 3 | Step 1960000 | Avg Loss: 0.0159 | Grad Norm: 0.00967062\n",
      "Epoch 3 | Step 1960100 | Avg Loss: 0.0155 | Grad Norm: 0.00819820\n",
      "Epoch 3 | Step 1960200 | Avg Loss: 0.0152 | Grad Norm: 0.00841717\n",
      "Epoch 3 | Step 1960300 | Avg Loss: 0.0151 | Grad Norm: 0.00779113\n",
      "Epoch 3 | Step 1960400 | Avg Loss: 0.0154 | Grad Norm: 0.00767472\n",
      "Epoch 3 | Step 1960500 | Avg Loss: 0.0154 | Grad Norm: 0.01007886\n",
      "Epoch 3 | Step 1960600 | Avg Loss: 0.0152 | Grad Norm: 0.00803620\n",
      "Epoch 3 | Step 1960700 | Avg Loss: 0.0152 | Grad Norm: 0.00869361\n",
      "Epoch 3 | Step 1960800 | Avg Loss: 0.0154 | Grad Norm: 0.00836959\n",
      "Epoch 3 | Step 1960900 | Avg Loss: 0.0157 | Grad Norm: 0.00936481\n",
      "Epoch 3 | Step 1961000 | Avg Loss: 0.0159 | Grad Norm: 0.00802218\n",
      "Epoch 3 | Step 1961100 | Avg Loss: 0.0156 | Grad Norm: 0.00811619\n",
      "Epoch 3 | Step 1961200 | Avg Loss: 0.0157 | Grad Norm: 0.00897799\n",
      "Epoch 3 | Step 1961300 | Avg Loss: 0.0155 | Grad Norm: 0.00799367\n",
      "Epoch 3 | Step 1961400 | Avg Loss: 0.0157 | Grad Norm: 0.00929941\n",
      "Epoch 3 | Step 1961500 | Avg Loss: 0.0159 | Grad Norm: 0.00904892\n",
      "Epoch 3 | Step 1961600 | Avg Loss: 0.0153 | Grad Norm: 0.01016520\n",
      "Epoch 3 | Step 1961700 | Avg Loss: 0.0156 | Grad Norm: 0.00987136\n",
      "Epoch 3 | Step 1961800 | Avg Loss: 0.0156 | Grad Norm: 0.01081638\n",
      "Epoch 3 | Step 1961900 | Avg Loss: 0.0159 | Grad Norm: 0.00868332\n",
      "Epoch 3 | Step 1962000 | Avg Loss: 0.0164 | Grad Norm: 0.00817798\n",
      "Epoch 3 | Step 1962100 | Avg Loss: 0.0166 | Grad Norm: 0.00902047\n",
      "Epoch 3 | Step 1962200 | Avg Loss: 0.0166 | Grad Norm: 0.00878128\n",
      "Epoch 3 | Step 1962300 | Avg Loss: 0.0164 | Grad Norm: 0.00838152\n",
      "Epoch 3 | Step 1962400 | Avg Loss: 0.0164 | Grad Norm: 0.01039110\n",
      "Epoch 3 | Step 1962500 | Avg Loss: 0.0161 | Grad Norm: 0.00912073\n",
      "Epoch 3 | Step 1962600 | Avg Loss: 0.0161 | Grad Norm: 0.00889057\n",
      "Epoch 3 | Step 1962700 | Avg Loss: 0.0155 | Grad Norm: 0.00944492\n",
      "Epoch 3 | Step 1962800 | Avg Loss: 0.0159 | Grad Norm: 0.00996069\n",
      "Epoch 3 | Step 1962900 | Avg Loss: 0.0158 | Grad Norm: 0.01040742\n",
      "Epoch 3 | Step 1963000 | Avg Loss: 0.0155 | Grad Norm: 0.00895160\n",
      "Epoch 3 | Step 1963100 | Avg Loss: 0.0153 | Grad Norm: 0.00774635\n",
      "Epoch 3 | Step 1963200 | Avg Loss: 0.0153 | Grad Norm: 0.00808898\n",
      "Epoch 3 | Step 1963300 | Avg Loss: 0.0152 | Grad Norm: 0.00845508\n",
      "Epoch 3 | Step 1963400 | Avg Loss: 0.0148 | Grad Norm: 0.00717923\n",
      "Epoch 3 | Step 1963500 | Avg Loss: 0.0151 | Grad Norm: 0.00857447\n",
      "Epoch 3 | Step 1963600 | Avg Loss: 0.0151 | Grad Norm: 0.00904040\n",
      "Epoch 3 | Step 1963700 | Avg Loss: 0.0150 | Grad Norm: 0.00756082\n",
      "Epoch 3 | Step 1963800 | Avg Loss: 0.0151 | Grad Norm: 0.00847545\n",
      "Epoch 3 | Step 1963900 | Avg Loss: 0.0155 | Grad Norm: 0.00830775\n",
      "Epoch 3 | Step 1964000 | Avg Loss: 0.0152 | Grad Norm: 0.01348828\n",
      "Epoch 3 | Step 1964100 | Avg Loss: 0.0154 | Grad Norm: 0.00835229\n",
      "Epoch 3 | Step 1964200 | Avg Loss: 0.0153 | Grad Norm: 0.00792301\n",
      "Epoch 3 | Step 1964300 | Avg Loss: 0.0150 | Grad Norm: 0.00838677\n",
      "Epoch 3 | Step 1964400 | Avg Loss: 0.0150 | Grad Norm: 0.00866741\n",
      "Epoch 3 | Step 1964500 | Avg Loss: 0.0150 | Grad Norm: 0.00812334\n",
      "Epoch 3 | Step 1964600 | Avg Loss: 0.0154 | Grad Norm: 0.00859703\n",
      "Epoch 3 | Step 1964700 | Avg Loss: 0.0157 | Grad Norm: 0.00912825\n",
      "Epoch 3 | Step 1964800 | Avg Loss: 0.0156 | Grad Norm: 0.00941004\n",
      "Epoch 3 | Step 1964900 | Avg Loss: 0.0153 | Grad Norm: 0.00929623\n",
      "Epoch 3 | Step 1965000 | Avg Loss: 0.0152 | Grad Norm: 0.00841533\n",
      "Epoch 3 | Step 1965100 | Avg Loss: 0.0151 | Grad Norm: 0.00862536\n",
      "Epoch 3 | Step 1965200 | Avg Loss: 0.0153 | Grad Norm: 0.01084153\n",
      "Epoch 3 | Step 1965300 | Avg Loss: 0.0152 | Grad Norm: 0.00786975\n",
      "Epoch 3 | Step 1965400 | Avg Loss: 0.0149 | Grad Norm: 0.01213432\n",
      "Epoch 3 | Step 1965500 | Avg Loss: 0.0153 | Grad Norm: 0.00828557\n",
      "Epoch 3 | Step 1965600 | Avg Loss: 0.0153 | Grad Norm: 0.00798550\n",
      "Epoch 3 | Step 1965700 | Avg Loss: 0.0154 | Grad Norm: 0.00850685\n",
      "Epoch 3 | Step 1965800 | Avg Loss: 0.0152 | Grad Norm: 0.00819942\n",
      "Epoch 3 | Step 1965900 | Avg Loss: 0.0152 | Grad Norm: 0.00941644\n",
      "Epoch 3 | Step 1966000 | Avg Loss: 0.0155 | Grad Norm: 0.00928847\n",
      "Epoch 3 | Step 1966100 | Avg Loss: 0.0154 | Grad Norm: 0.00782749\n",
      "Epoch 3 | Step 1966200 | Avg Loss: 0.0156 | Grad Norm: 0.00927513\n",
      "Epoch 3 | Step 1966300 | Avg Loss: 0.0156 | Grad Norm: 0.00877113\n",
      "Epoch 3 | Step 1966400 | Avg Loss: 0.0157 | Grad Norm: 0.00888545\n",
      "Epoch 3 | Step 1966500 | Avg Loss: 0.0157 | Grad Norm: 0.00865477\n",
      "Epoch 3 | Step 1966600 | Avg Loss: 0.0158 | Grad Norm: 0.01075625\n",
      "Epoch 3 | Step 1966700 | Avg Loss: 0.0157 | Grad Norm: 0.00845457\n",
      "Epoch 3 | Step 1966800 | Avg Loss: 0.0156 | Grad Norm: 0.01107716\n",
      "Epoch 3 | Step 1966900 | Avg Loss: 0.0156 | Grad Norm: 0.00853145\n",
      "Epoch 3 | Step 1967000 | Avg Loss: 0.0160 | Grad Norm: 0.00947979\n",
      "Epoch 3 | Step 1967100 | Avg Loss: 0.0166 | Grad Norm: 0.00919478\n",
      "Epoch 3 | Step 1967200 | Avg Loss: 0.0161 | Grad Norm: 0.00928737\n",
      "Epoch 3 | Step 1967300 | Avg Loss: 0.0165 | Grad Norm: 0.00865573\n",
      "Epoch 3 | Step 1967400 | Avg Loss: 0.0159 | Grad Norm: 0.00897952\n",
      "Epoch 3 | Step 1967500 | Avg Loss: 0.0157 | Grad Norm: 0.00852309\n",
      "Epoch 3 | Step 1967600 | Avg Loss: 0.0157 | Grad Norm: 0.01030503\n",
      "Epoch 3 | Step 1967700 | Avg Loss: 0.0157 | Grad Norm: 0.00825173\n",
      "Epoch 3 | Step 1967800 | Avg Loss: 0.0158 | Grad Norm: 0.00868899\n",
      "Epoch 3 | Step 1967900 | Avg Loss: 0.0159 | Grad Norm: 0.00756524\n",
      "Epoch 3 | Step 1968000 | Avg Loss: 0.0156 | Grad Norm: 0.00982461\n",
      "Epoch 3 | Step 1968100 | Avg Loss: 0.0159 | Grad Norm: 0.01040899\n",
      "Epoch 3 | Step 1968200 | Avg Loss: 0.0158 | Grad Norm: 0.00820436\n",
      "Epoch 3 | Step 1968300 | Avg Loss: 0.0159 | Grad Norm: 0.00870687\n",
      "Epoch 3 | Step 1968400 | Avg Loss: 0.0160 | Grad Norm: 0.00828335\n",
      "Epoch 3 | Step 1968500 | Avg Loss: 0.0156 | Grad Norm: 0.00991180\n",
      "Epoch 3 | Step 1968600 | Avg Loss: 0.0157 | Grad Norm: 0.01003220\n",
      "Epoch 3 | Step 1968700 | Avg Loss: 0.0154 | Grad Norm: 0.00836601\n",
      "Epoch 3 | Step 1968800 | Avg Loss: 0.0153 | Grad Norm: 0.00929806\n",
      "Epoch 3 | Step 1968900 | Avg Loss: 0.0158 | Grad Norm: 0.00969734\n",
      "Epoch 3 | Step 1969000 | Avg Loss: 0.0158 | Grad Norm: 0.00842200\n",
      "Epoch 3 | Step 1969100 | Avg Loss: 0.0156 | Grad Norm: 0.00886370\n",
      "Epoch 3 | Step 1969200 | Avg Loss: 0.0158 | Grad Norm: 0.00697181\n",
      "Epoch 3 | Step 1969300 | Avg Loss: 0.0162 | Grad Norm: 0.01100872\n",
      "Epoch 3 | Step 1969400 | Avg Loss: 0.0157 | Grad Norm: 0.00870228\n",
      "Epoch 3 | Step 1969500 | Avg Loss: 0.0161 | Grad Norm: 0.01019843\n",
      "Epoch 3 | Step 1969600 | Avg Loss: 0.0159 | Grad Norm: 0.00909857\n",
      "Epoch 3 | Step 1969700 | Avg Loss: 0.0159 | Grad Norm: 0.00850863\n",
      "Epoch 3 | Step 1969800 | Avg Loss: 0.0159 | Grad Norm: 0.01045471\n",
      "Epoch 3 | Step 1969900 | Avg Loss: 0.0157 | Grad Norm: 0.01033597\n",
      "Epoch 3 | Step 1970000 | Avg Loss: 0.0158 | Grad Norm: 0.00858396\n",
      "Epoch 3 | Step 1970100 | Avg Loss: 0.0159 | Grad Norm: 0.01051684\n",
      "Epoch 3 | Step 1970200 | Avg Loss: 0.0160 | Grad Norm: 0.00811403\n",
      "Epoch 3 | Step 1970300 | Avg Loss: 0.0153 | Grad Norm: 0.00909840\n",
      "Epoch 3 | Step 1970400 | Avg Loss: 0.0157 | Grad Norm: 0.00875405\n",
      "Epoch 3 | Step 1970500 | Avg Loss: 0.0154 | Grad Norm: 0.00818201\n",
      "Epoch 3 | Step 1970600 | Avg Loss: 0.0154 | Grad Norm: 0.00947325\n",
      "Epoch 3 | Step 1970700 | Avg Loss: 0.0155 | Grad Norm: 0.00792297\n",
      "Epoch 3 | Step 1970800 | Avg Loss: 0.0156 | Grad Norm: 0.00902740\n",
      "Epoch 3 | Step 1970900 | Avg Loss: 0.0156 | Grad Norm: 0.00895552\n",
      "Epoch 3 | Step 1971000 | Avg Loss: 0.0154 | Grad Norm: 0.00977184\n",
      "Epoch 3 | Step 1971100 | Avg Loss: 0.0151 | Grad Norm: 0.00897062\n",
      "Epoch 3 | Step 1971200 | Avg Loss: 0.0154 | Grad Norm: 0.00996854\n",
      "Epoch 3 | Step 1971300 | Avg Loss: 0.0158 | Grad Norm: 0.00987150\n",
      "Epoch 3 | Step 1971400 | Avg Loss: 0.0159 | Grad Norm: 0.00963317\n",
      "Epoch 3 | Step 1971500 | Avg Loss: 0.0158 | Grad Norm: 0.00921890\n",
      "Epoch 3 | Step 1971600 | Avg Loss: 0.0155 | Grad Norm: 0.00976393\n",
      "Epoch 3 | Step 1971700 | Avg Loss: 0.0157 | Grad Norm: 0.01028798\n",
      "Epoch 3 | Step 1971800 | Avg Loss: 0.0160 | Grad Norm: 0.00955353\n",
      "Epoch 3 | Step 1971900 | Avg Loss: 0.0162 | Grad Norm: 0.00846986\n",
      "Epoch 3 | Step 1972000 | Avg Loss: 0.0162 | Grad Norm: 0.00864673\n",
      "Epoch 3 | Step 1972100 | Avg Loss: 0.0163 | Grad Norm: 0.01002661\n",
      "Epoch 3 | Step 1972200 | Avg Loss: 0.0158 | Grad Norm: 0.00871972\n",
      "Epoch 3 | Step 1972300 | Avg Loss: 0.0154 | Grad Norm: 0.00907295\n",
      "Epoch 3 | Step 1972400 | Avg Loss: 0.0153 | Grad Norm: 0.00868815\n",
      "Epoch 3 | Step 1972500 | Avg Loss: 0.0154 | Grad Norm: 0.00941721\n",
      "Epoch 3 | Step 1972600 | Avg Loss: 0.0153 | Grad Norm: 0.00825241\n",
      "Epoch 3 | Step 1972700 | Avg Loss: 0.0151 | Grad Norm: 0.00857083\n",
      "Epoch 3 | Step 1972800 | Avg Loss: 0.0151 | Grad Norm: 0.01010804\n",
      "Epoch 3 | Step 1972900 | Avg Loss: 0.0154 | Grad Norm: 0.00835832\n",
      "Epoch 3 | Step 1973000 | Avg Loss: 0.0153 | Grad Norm: 0.00962126\n",
      "Epoch 3 | Step 1973100 | Avg Loss: 0.0154 | Grad Norm: 0.00945053\n",
      "Epoch 3 | Step 1973200 | Avg Loss: 0.0151 | Grad Norm: 0.00947194\n",
      "Epoch 3 | Step 1973300 | Avg Loss: 0.0153 | Grad Norm: 0.01065756\n",
      "Epoch 3 | Step 1973400 | Avg Loss: 0.0153 | Grad Norm: 0.00944205\n",
      "Epoch 3 | Step 1973500 | Avg Loss: 0.0153 | Grad Norm: 0.00836990\n",
      "Epoch 3 | Step 1973600 | Avg Loss: 0.0154 | Grad Norm: 0.00909750\n",
      "Epoch 3 | Step 1973700 | Avg Loss: 0.0158 | Grad Norm: 0.00910467\n",
      "Epoch 3 | Step 1973800 | Avg Loss: 0.0159 | Grad Norm: 0.00893886\n",
      "Epoch 3 | Step 1973900 | Avg Loss: 0.0161 | Grad Norm: 0.00958898\n",
      "Epoch 3 | Step 1974000 | Avg Loss: 0.0161 | Grad Norm: 0.01132582\n",
      "Epoch 3 | Step 1974100 | Avg Loss: 0.0159 | Grad Norm: 0.00803899\n",
      "Epoch 3 | Step 1974200 | Avg Loss: 0.0158 | Grad Norm: 0.00827218\n",
      "Epoch 3 | Step 1974300 | Avg Loss: 0.0156 | Grad Norm: 0.00997253\n",
      "Epoch 3 | Step 1974400 | Avg Loss: 0.0154 | Grad Norm: 0.00752914\n",
      "Epoch 3 | Step 1974500 | Avg Loss: 0.0157 | Grad Norm: 0.01082381\n",
      "Epoch 3 | Step 1974600 | Avg Loss: 0.0156 | Grad Norm: 0.00848463\n",
      "Epoch 3 | Step 1974700 | Avg Loss: 0.0154 | Grad Norm: 0.00772800\n",
      "Epoch 3 | Step 1974800 | Avg Loss: 0.0157 | Grad Norm: 0.00884016\n",
      "Epoch 3 | Step 1974900 | Avg Loss: 0.0159 | Grad Norm: 0.00966369\n",
      "Epoch 3 | Step 1975000 | Avg Loss: 0.0155 | Grad Norm: 0.01004550\n",
      "Epoch 3 | Step 1975100 | Avg Loss: 0.0158 | Grad Norm: 0.00946991\n",
      "Epoch 3 | Step 1975200 | Avg Loss: 0.0156 | Grad Norm: 0.00924457\n",
      "Epoch 3 | Step 1975300 | Avg Loss: 0.0156 | Grad Norm: 0.00856325\n",
      "Epoch 3 | Step 1975400 | Avg Loss: 0.0154 | Grad Norm: 0.00936433\n",
      "Epoch 3 | Step 1975500 | Avg Loss: 0.0155 | Grad Norm: 0.00953315\n",
      "Epoch 3 | Step 1975600 | Avg Loss: 0.0155 | Grad Norm: 0.00894945\n",
      "Epoch 3 | Step 1975700 | Avg Loss: 0.0155 | Grad Norm: 0.00931680\n",
      "Epoch 3 | Step 1975800 | Avg Loss: 0.0156 | Grad Norm: 0.00869835\n",
      "Epoch 3 | Step 1975900 | Avg Loss: 0.0153 | Grad Norm: 0.00830726\n",
      "Epoch 3 | Step 1976000 | Avg Loss: 0.0157 | Grad Norm: 0.00895430\n",
      "Epoch 3 | Step 1976100 | Avg Loss: 0.0157 | Grad Norm: 0.00821895\n",
      "Epoch 3 | Step 1976200 | Avg Loss: 0.0156 | Grad Norm: 0.00974326\n",
      "Epoch 3 | Step 1976300 | Avg Loss: 0.0154 | Grad Norm: 0.00938466\n",
      "Epoch 3 | Step 1976400 | Avg Loss: 0.0157 | Grad Norm: 0.00888009\n",
      "Epoch 3 | Step 1976500 | Avg Loss: 0.0158 | Grad Norm: 0.00881405\n",
      "Epoch 3 | Step 1976600 | Avg Loss: 0.0156 | Grad Norm: 0.00919605\n",
      "Epoch 3 | Step 1976700 | Avg Loss: 0.0157 | Grad Norm: 0.00968152\n",
      "Epoch 3 | Step 1976800 | Avg Loss: 0.0159 | Grad Norm: 0.00938238\n",
      "Epoch 3 | Step 1976900 | Avg Loss: 0.0161 | Grad Norm: 0.00821518\n",
      "Epoch 3 | Step 1977000 | Avg Loss: 0.0161 | Grad Norm: 0.01099183\n",
      "Epoch 3 | Step 1977100 | Avg Loss: 0.0160 | Grad Norm: 0.00849556\n",
      "Epoch 3 | Step 1977200 | Avg Loss: 0.0157 | Grad Norm: 0.00821179\n",
      "Epoch 3 | Step 1977300 | Avg Loss: 0.0160 | Grad Norm: 0.00845523\n",
      "Epoch 3 | Step 1977400 | Avg Loss: 0.0158 | Grad Norm: 0.00902834\n",
      "Epoch 3 | Step 1977500 | Avg Loss: 0.0159 | Grad Norm: 0.00981411\n",
      "Epoch 3 | Step 1977600 | Avg Loss: 0.0158 | Grad Norm: 0.00782683\n",
      "Epoch 3 | Step 1977700 | Avg Loss: 0.0151 | Grad Norm: 0.00813840\n",
      "Epoch 3 | Step 1977800 | Avg Loss: 0.0153 | Grad Norm: 0.00805064\n",
      "Epoch 3 | Step 1977900 | Avg Loss: 0.0155 | Grad Norm: 0.00934168\n",
      "Epoch 3 | Step 1978000 | Avg Loss: 0.0155 | Grad Norm: 0.00758639\n",
      "Epoch 3 | Step 1978100 | Avg Loss: 0.0155 | Grad Norm: 0.00882714\n",
      "Epoch 3 | Step 1978200 | Avg Loss: 0.0154 | Grad Norm: 0.00824362\n",
      "Epoch 3 | Step 1978300 | Avg Loss: 0.0156 | Grad Norm: 0.00890002\n",
      "Epoch 3 | Step 1978400 | Avg Loss: 0.0160 | Grad Norm: 0.00980926\n",
      "Epoch 3 | Step 1978500 | Avg Loss: 0.0160 | Grad Norm: 0.00955671\n",
      "Epoch 3 | Step 1978600 | Avg Loss: 0.0159 | Grad Norm: 0.00876374\n",
      "Epoch 3 | Step 1978700 | Avg Loss: 0.0155 | Grad Norm: 0.00809144\n",
      "Epoch 3 | Step 1978800 | Avg Loss: 0.0157 | Grad Norm: 0.00777011\n",
      "Epoch 3 | Step 1978900 | Avg Loss: 0.0157 | Grad Norm: 0.00937798\n",
      "Epoch 3 | Step 1979000 | Avg Loss: 0.0155 | Grad Norm: 0.01057419\n",
      "Epoch 3 | Step 1979100 | Avg Loss: 0.0152 | Grad Norm: 0.00852609\n",
      "Epoch 3 | Step 1979200 | Avg Loss: 0.0156 | Grad Norm: 0.00859411\n",
      "Epoch 3 | Step 1979300 | Avg Loss: 0.0157 | Grad Norm: 0.00924515\n",
      "Epoch 3 | Step 1979400 | Avg Loss: 0.0158 | Grad Norm: 0.00902783\n",
      "Epoch 3 | Step 1979500 | Avg Loss: 0.0160 | Grad Norm: 0.00854299\n",
      "Epoch 3 | Step 1979600 | Avg Loss: 0.0157 | Grad Norm: 0.00788230\n",
      "Epoch 3 | Step 1979700 | Avg Loss: 0.0161 | Grad Norm: 0.01067794\n",
      "Epoch 3 | Step 1979800 | Avg Loss: 0.0160 | Grad Norm: 0.00965113\n",
      "Epoch 3 | Step 1979900 | Avg Loss: 0.0161 | Grad Norm: 0.00882543\n",
      "Epoch 3 | Step 1980000 | Avg Loss: 0.0160 | Grad Norm: 0.00977591\n",
      "Epoch 3 | Step 1980100 | Avg Loss: 0.0161 | Grad Norm: 0.00814693\n",
      "Epoch 3 | Step 1980200 | Avg Loss: 0.0161 | Grad Norm: 0.00951048\n",
      "Epoch 3 | Step 1980300 | Avg Loss: 0.0162 | Grad Norm: 0.00793813\n",
      "Epoch 3 | Step 1980400 | Avg Loss: 0.0158 | Grad Norm: 0.00860806\n",
      "Epoch 3 | Step 1980500 | Avg Loss: 0.0159 | Grad Norm: 0.00846731\n",
      "Epoch 3 | Step 1980600 | Avg Loss: 0.0157 | Grad Norm: 0.00819784\n",
      "Epoch 3 | Step 1980700 | Avg Loss: 0.0157 | Grad Norm: 0.00914973\n",
      "Epoch 3 | Step 1980800 | Avg Loss: 0.0158 | Grad Norm: 0.00837875\n",
      "Epoch 3 | Step 1980900 | Avg Loss: 0.0157 | Grad Norm: 0.01008470\n",
      "Epoch 3 | Step 1981000 | Avg Loss: 0.0157 | Grad Norm: 0.01102129\n",
      "Epoch 3 | Step 1981100 | Avg Loss: 0.0157 | Grad Norm: 0.00775637\n",
      "Epoch 3 | Step 1981200 | Avg Loss: 0.0163 | Grad Norm: 0.00928339\n",
      "Epoch 3 | Step 1981300 | Avg Loss: 0.0162 | Grad Norm: 0.01026738\n",
      "Epoch 3 | Step 1981400 | Avg Loss: 0.0163 | Grad Norm: 0.00880836\n",
      "Epoch 3 | Step 1981500 | Avg Loss: 0.0160 | Grad Norm: 0.00903608\n",
      "Epoch 3 | Step 1981600 | Avg Loss: 0.0158 | Grad Norm: 0.00907519\n",
      "Epoch 3 | Step 1981700 | Avg Loss: 0.0162 | Grad Norm: 0.00837020\n",
      "Epoch 3 | Step 1981800 | Avg Loss: 0.0159 | Grad Norm: 0.01039057\n",
      "Epoch 3 | Step 1981900 | Avg Loss: 0.0161 | Grad Norm: 0.00890116\n",
      "Epoch 3 | Step 1982000 | Avg Loss: 0.0160 | Grad Norm: 0.00896479\n",
      "Epoch 3 | Step 1982100 | Avg Loss: 0.0158 | Grad Norm: 0.00820489\n",
      "Epoch 3 | Step 1982200 | Avg Loss: 0.0160 | Grad Norm: 0.01051776\n",
      "Epoch 3 | Step 1982300 | Avg Loss: 0.0153 | Grad Norm: 0.00936820\n",
      "Epoch 3 | Step 1982400 | Avg Loss: 0.0148 | Grad Norm: 0.00848191\n",
      "Epoch 3 | Step 1982500 | Avg Loss: 0.0147 | Grad Norm: 0.00880609\n",
      "Epoch 3 | Step 1982600 | Avg Loss: 0.0150 | Grad Norm: 0.00865992\n",
      "Epoch 3 | Step 1982700 | Avg Loss: 0.0152 | Grad Norm: 0.00944973\n",
      "Epoch 3 | Step 1982800 | Avg Loss: 0.0153 | Grad Norm: 0.00879410\n",
      "Epoch 3 | Step 1982900 | Avg Loss: 0.0154 | Grad Norm: 0.00751941\n",
      "Epoch 3 | Step 1983000 | Avg Loss: 0.0155 | Grad Norm: 0.00930587\n",
      "Epoch 3 | Step 1983100 | Avg Loss: 0.0156 | Grad Norm: 0.00845318\n",
      "Epoch 3 | Step 1983200 | Avg Loss: 0.0157 | Grad Norm: 0.00762032\n",
      "Epoch 3 | Step 1983300 | Avg Loss: 0.0160 | Grad Norm: 0.00906451\n",
      "Epoch 3 | Step 1983400 | Avg Loss: 0.0158 | Grad Norm: 0.00896294\n",
      "Epoch 3 | Step 1983500 | Avg Loss: 0.0159 | Grad Norm: 0.00849074\n",
      "Epoch 3 | Step 1983600 | Avg Loss: 0.0158 | Grad Norm: 0.00906287\n",
      "Epoch 3 | Step 1983700 | Avg Loss: 0.0157 | Grad Norm: 0.01211292\n",
      "Epoch 3 | Step 1983800 | Avg Loss: 0.0154 | Grad Norm: 0.00835658\n",
      "Epoch 3 | Step 1983900 | Avg Loss: 0.0154 | Grad Norm: 0.01094420\n",
      "Epoch 3 | Step 1984000 | Avg Loss: 0.0152 | Grad Norm: 0.00901922\n",
      "Epoch 3 | Step 1984100 | Avg Loss: 0.0153 | Grad Norm: 0.00891320\n",
      "Epoch 3 | Step 1984200 | Avg Loss: 0.0157 | Grad Norm: 0.00808713\n",
      "Epoch 3 | Step 1984300 | Avg Loss: 0.0157 | Grad Norm: 0.00840197\n",
      "Epoch 3 | Step 1984400 | Avg Loss: 0.0159 | Grad Norm: 0.00982141\n",
      "Epoch 3 | Step 1984500 | Avg Loss: 0.0156 | Grad Norm: 0.00989219\n",
      "Epoch 3 | Step 1984600 | Avg Loss: 0.0154 | Grad Norm: 0.00868201\n",
      "Epoch 3 | Step 1984700 | Avg Loss: 0.0152 | Grad Norm: 0.00909750\n",
      "Epoch 3 | Step 1984800 | Avg Loss: 0.0153 | Grad Norm: 0.00882076\n",
      "Epoch 3 | Step 1984900 | Avg Loss: 0.0154 | Grad Norm: 0.00857664\n",
      "Epoch 3 | Step 1985000 | Avg Loss: 0.0156 | Grad Norm: 0.00969944\n",
      "Epoch 3 | Step 1985100 | Avg Loss: 0.0155 | Grad Norm: 0.00925216\n",
      "Epoch 3 | Step 1985200 | Avg Loss: 0.0155 | Grad Norm: 0.00900906\n",
      "Epoch 3 | Step 1985300 | Avg Loss: 0.0156 | Grad Norm: 0.01062308\n",
      "Epoch 3 | Step 1985400 | Avg Loss: 0.0156 | Grad Norm: 0.01040990\n",
      "Epoch 3 | Step 1985500 | Avg Loss: 0.0160 | Grad Norm: 0.01041978\n",
      "Epoch 3 | Step 1985600 | Avg Loss: 0.0155 | Grad Norm: 0.00961086\n",
      "Epoch 3 | Step 1985700 | Avg Loss: 0.0159 | Grad Norm: 0.00948801\n",
      "Epoch 3 | Step 1985800 | Avg Loss: 0.0157 | Grad Norm: 0.00852908\n",
      "Epoch 3 | Step 1985900 | Avg Loss: 0.0156 | Grad Norm: 0.00922096\n",
      "Epoch 3 | Step 1986000 | Avg Loss: 0.0156 | Grad Norm: 0.00979990\n",
      "Epoch 3 | Step 1986100 | Avg Loss: 0.0157 | Grad Norm: 0.00875926\n",
      "Epoch 3 | Step 1986200 | Avg Loss: 0.0158 | Grad Norm: 0.00867986\n",
      "Epoch 3 | Step 1986300 | Avg Loss: 0.0153 | Grad Norm: 0.00894896\n",
      "Epoch 3 | Step 1986400 | Avg Loss: 0.0150 | Grad Norm: 0.01010727\n",
      "Epoch 3 | Step 1986500 | Avg Loss: 0.0156 | Grad Norm: 0.00889925\n",
      "Epoch 3 | Step 1986600 | Avg Loss: 0.0154 | Grad Norm: 0.00918571\n",
      "Epoch 3 | Step 1986700 | Avg Loss: 0.0154 | Grad Norm: 0.00886339\n",
      "Epoch 3 | Step 1986800 | Avg Loss: 0.0154 | Grad Norm: 0.00929947\n",
      "Epoch 3 | Step 1986900 | Avg Loss: 0.0154 | Grad Norm: 0.00787882\n",
      "Epoch 3 | Step 1987000 | Avg Loss: 0.0155 | Grad Norm: 0.00818416\n",
      "Epoch 3 | Step 1987100 | Avg Loss: 0.0155 | Grad Norm: 0.00823708\n",
      "Epoch 3 | Step 1987200 | Avg Loss: 0.0156 | Grad Norm: 0.00810379\n",
      "Epoch 3 | Step 1987300 | Avg Loss: 0.0155 | Grad Norm: 0.00952056\n",
      "Epoch 3 | Step 1987400 | Avg Loss: 0.0159 | Grad Norm: 0.00911731\n",
      "Epoch 3 | Step 1987500 | Avg Loss: 0.0156 | Grad Norm: 0.00900216\n",
      "Epoch 3 | Step 1987600 | Avg Loss: 0.0161 | Grad Norm: 0.00969908\n",
      "Epoch 3 | Step 1987700 | Avg Loss: 0.0161 | Grad Norm: 0.00742020\n",
      "Epoch 3 | Step 1987800 | Avg Loss: 0.0157 | Grad Norm: 0.00935590\n",
      "Epoch 3 | Step 1987900 | Avg Loss: 0.0157 | Grad Norm: 0.00827719\n",
      "Epoch 3 | Step 1988000 | Avg Loss: 0.0156 | Grad Norm: 0.00949481\n",
      "Epoch 3 | Step 1988100 | Avg Loss: 0.0157 | Grad Norm: 0.00836908\n",
      "Epoch 3 | Step 1988200 | Avg Loss: 0.0156 | Grad Norm: 0.00942747\n",
      "Epoch 3 | Step 1988300 | Avg Loss: 0.0155 | Grad Norm: 0.00820496\n",
      "Epoch 3 | Step 1988400 | Avg Loss: 0.0153 | Grad Norm: 0.00883948\n",
      "Epoch 3 | Step 1988500 | Avg Loss: 0.0154 | Grad Norm: 0.00866326\n",
      "Epoch 3 | Step 1988600 | Avg Loss: 0.0153 | Grad Norm: 0.00951210\n",
      "Epoch 3 | Step 1988700 | Avg Loss: 0.0154 | Grad Norm: 0.00978965\n",
      "Epoch 3 | Step 1988800 | Avg Loss: 0.0151 | Grad Norm: 0.01021803\n",
      "Epoch 3 | Step 1988900 | Avg Loss: 0.0153 | Grad Norm: 0.00899716\n",
      "Epoch 3 | Step 1989000 | Avg Loss: 0.0152 | Grad Norm: 0.00842371\n",
      "Epoch 3 | Step 1989100 | Avg Loss: 0.0150 | Grad Norm: 0.00968828\n",
      "Epoch 3 | Step 1989200 | Avg Loss: 0.0150 | Grad Norm: 0.00856291\n",
      "Epoch 3 | Step 1989300 | Avg Loss: 0.0152 | Grad Norm: 0.00926467\n",
      "Epoch 3 | Step 1989400 | Avg Loss: 0.0155 | Grad Norm: 0.00960011\n",
      "Epoch 3 | Step 1989500 | Avg Loss: 0.0157 | Grad Norm: 0.00943988\n",
      "Epoch 3 | Step 1989600 | Avg Loss: 0.0152 | Grad Norm: 0.01276578\n",
      "Epoch 3 | Step 1989700 | Avg Loss: 0.0156 | Grad Norm: 0.00874758\n",
      "Epoch 3 | Step 1989800 | Avg Loss: 0.0157 | Grad Norm: 0.00927513\n",
      "Epoch 3 | Step 1989900 | Avg Loss: 0.0157 | Grad Norm: 0.00948715\n",
      "Epoch 3 | Step 1990000 | Avg Loss: 0.0157 | Grad Norm: 0.00925218\n",
      "Epoch 3 | Step 1990100 | Avg Loss: 0.0159 | Grad Norm: 0.00886911\n",
      "Epoch 3 | Step 1990200 | Avg Loss: 0.0159 | Grad Norm: 0.00988813\n",
      "Epoch 3 | Step 1990300 | Avg Loss: 0.0159 | Grad Norm: 0.01018087\n",
      "Epoch 3 | Step 1990400 | Avg Loss: 0.0157 | Grad Norm: 0.00860535\n",
      "Epoch 3 | Step 1990500 | Avg Loss: 0.0156 | Grad Norm: 0.00866701\n",
      "Epoch 3 | Step 1990600 | Avg Loss: 0.0156 | Grad Norm: 0.00893032\n",
      "Epoch 3 | Step 1990700 | Avg Loss: 0.0156 | Grad Norm: 0.00975539\n",
      "Epoch 3 | Step 1990800 | Avg Loss: 0.0157 | Grad Norm: 0.00876359\n",
      "Epoch 3 | Step 1990900 | Avg Loss: 0.0159 | Grad Norm: 0.01173806\n",
      "Epoch 3 | Step 1991000 | Avg Loss: 0.0153 | Grad Norm: 0.00849238\n",
      "Epoch 3 | Step 1991100 | Avg Loss: 0.0154 | Grad Norm: 0.00920857\n",
      "Epoch 3 | Step 1991200 | Avg Loss: 0.0154 | Grad Norm: 0.00965338\n",
      "Epoch 3 | Step 1991300 | Avg Loss: 0.0156 | Grad Norm: 0.00926842\n",
      "Epoch 3 | Step 1991400 | Avg Loss: 0.0154 | Grad Norm: 0.01037390\n",
      "Epoch 3 | Step 1991500 | Avg Loss: 0.0158 | Grad Norm: 0.01062318\n",
      "Epoch 3 | Step 1991600 | Avg Loss: 0.0158 | Grad Norm: 0.00950057\n",
      "Epoch 3 | Step 1991700 | Avg Loss: 0.0156 | Grad Norm: 0.00931111\n",
      "Epoch 3 | Step 1991800 | Avg Loss: 0.0157 | Grad Norm: 0.01132452\n",
      "Epoch 3 | Step 1991900 | Avg Loss: 0.0158 | Grad Norm: 0.00887289\n",
      "Epoch 3 | Step 1992000 | Avg Loss: 0.0159 | Grad Norm: 0.00897501\n",
      "Epoch 3 | Step 1992100 | Avg Loss: 0.0159 | Grad Norm: 0.00900193\n",
      "Epoch 3 | Step 1992200 | Avg Loss: 0.0160 | Grad Norm: 0.00929697\n",
      "Epoch 3 | Step 1992300 | Avg Loss: 0.0159 | Grad Norm: 0.00888339\n",
      "Epoch 3 | Step 1992400 | Avg Loss: 0.0160 | Grad Norm: 0.00881915\n",
      "Epoch 3 | Step 1992500 | Avg Loss: 0.0158 | Grad Norm: 0.00829691\n",
      "Epoch 3 | Step 1992600 | Avg Loss: 0.0160 | Grad Norm: 0.00940983\n",
      "Epoch 3 | Step 1992700 | Avg Loss: 0.0156 | Grad Norm: 0.01122570\n",
      "Epoch 3 | Step 1992800 | Avg Loss: 0.0158 | Grad Norm: 0.00850816\n",
      "Epoch 3 | Step 1992900 | Avg Loss: 0.0153 | Grad Norm: 0.00975232\n",
      "Epoch 3 | Step 1993000 | Avg Loss: 0.0153 | Grad Norm: 0.00859603\n",
      "Epoch 3 | Step 1993100 | Avg Loss: 0.0153 | Grad Norm: 0.01058731\n",
      "Epoch 3 | Step 1993200 | Avg Loss: 0.0151 | Grad Norm: 0.00883226\n",
      "Epoch 3 | Step 1993300 | Avg Loss: 0.0153 | Grad Norm: 0.00836374\n",
      "Epoch 3 | Step 1993400 | Avg Loss: 0.0154 | Grad Norm: 0.01067151\n",
      "Epoch 3 | Step 1993500 | Avg Loss: 0.0154 | Grad Norm: 0.00952518\n",
      "Epoch 3 | Step 1993600 | Avg Loss: 0.0156 | Grad Norm: 0.00751410\n",
      "Epoch 3 | Step 1993700 | Avg Loss: 0.0156 | Grad Norm: 0.00944383\n",
      "Epoch 3 | Step 1993800 | Avg Loss: 0.0157 | Grad Norm: 0.00983318\n",
      "Epoch 3 | Step 1993900 | Avg Loss: 0.0155 | Grad Norm: 0.00946348\n",
      "Epoch 3 | Step 1994000 | Avg Loss: 0.0154 | Grad Norm: 0.01092056\n",
      "Epoch 3 | Step 1994100 | Avg Loss: 0.0156 | Grad Norm: 0.00849889\n",
      "Epoch 3 | Step 1994200 | Avg Loss: 0.0155 | Grad Norm: 0.01054374\n",
      "Epoch 3 | Step 1994300 | Avg Loss: 0.0155 | Grad Norm: 0.00924256\n",
      "Epoch 3 | Step 1994400 | Avg Loss: 0.0158 | Grad Norm: 0.00936694\n",
      "Epoch 3 | Step 1994500 | Avg Loss: 0.0159 | Grad Norm: 0.00904986\n",
      "Epoch 3 | Step 1994600 | Avg Loss: 0.0157 | Grad Norm: 0.00875541\n",
      "Epoch 3 | Step 1994700 | Avg Loss: 0.0154 | Grad Norm: 0.00860609\n",
      "Epoch 3 | Step 1994800 | Avg Loss: 0.0160 | Grad Norm: 0.00830007\n",
      "Epoch 3 | Step 1994900 | Avg Loss: 0.0161 | Grad Norm: 0.00889031\n",
      "Epoch 3 | Step 1995000 | Avg Loss: 0.0157 | Grad Norm: 0.00954255\n",
      "Epoch 3 | Step 1995100 | Avg Loss: 0.0162 | Grad Norm: 0.00855209\n",
      "Epoch 3 | Step 1995200 | Avg Loss: 0.0162 | Grad Norm: 0.00902744\n",
      "Epoch 3 | Step 1995300 | Avg Loss: 0.0162 | Grad Norm: 0.00808115\n",
      "Epoch 3 | Step 1995400 | Avg Loss: 0.0162 | Grad Norm: 0.00869007\n",
      "Epoch 3 | Step 1995500 | Avg Loss: 0.0159 | Grad Norm: 0.00913325\n",
      "Epoch 3 | Step 1995600 | Avg Loss: 0.0157 | Grad Norm: 0.00850547\n",
      "Epoch 3 | Step 1995700 | Avg Loss: 0.0156 | Grad Norm: 0.00869976\n",
      "Epoch 3 | Step 1995800 | Avg Loss: 0.0157 | Grad Norm: 0.00835525\n",
      "Epoch 3 | Step 1995900 | Avg Loss: 0.0159 | Grad Norm: 0.00814612\n",
      "Epoch 3 | Step 1996000 | Avg Loss: 0.0154 | Grad Norm: 0.01140645\n",
      "Epoch 3 | Step 1996100 | Avg Loss: 0.0156 | Grad Norm: 0.00903579\n",
      "Epoch 3 | Step 1996200 | Avg Loss: 0.0156 | Grad Norm: 0.00911412\n",
      "Epoch 3 | Step 1996300 | Avg Loss: 0.0152 | Grad Norm: 0.00894423\n",
      "Epoch 3 | Step 1996400 | Avg Loss: 0.0154 | Grad Norm: 0.00845874\n",
      "Epoch 3 | Step 1996500 | Avg Loss: 0.0158 | Grad Norm: 0.00908474\n",
      "Epoch 3 | Step 1996600 | Avg Loss: 0.0158 | Grad Norm: 0.00791866\n",
      "Epoch 3 | Step 1996700 | Avg Loss: 0.0157 | Grad Norm: 0.00859118\n",
      "Epoch 3 | Step 1996800 | Avg Loss: 0.0156 | Grad Norm: 0.00787100\n",
      "Epoch 3 | Step 1996900 | Avg Loss: 0.0158 | Grad Norm: 0.00957979\n",
      "Epoch 3 | Step 1997000 | Avg Loss: 0.0158 | Grad Norm: 0.00850184\n",
      "Epoch 3 | Step 1997100 | Avg Loss: 0.0155 | Grad Norm: 0.00876689\n",
      "Epoch 3 | Step 1997200 | Avg Loss: 0.0153 | Grad Norm: 0.00876879\n",
      "Epoch 3 | Step 1997300 | Avg Loss: 0.0154 | Grad Norm: 0.00931128\n",
      "Epoch 3 | Step 1997400 | Avg Loss: 0.0155 | Grad Norm: 0.00833199\n",
      "Epoch 3 | Step 1997500 | Avg Loss: 0.0154 | Grad Norm: 0.00847212\n",
      "Epoch 3 | Step 1997600 | Avg Loss: 0.0154 | Grad Norm: 0.01006902\n",
      "Epoch 3 | Step 1997700 | Avg Loss: 0.0152 | Grad Norm: 0.00918945\n",
      "Epoch 3 | Step 1997800 | Avg Loss: 0.0152 | Grad Norm: 0.01113078\n",
      "Epoch 3 | Step 1997900 | Avg Loss: 0.0154 | Grad Norm: 0.00902601\n",
      "Epoch 3 | Step 1998000 | Avg Loss: 0.0152 | Grad Norm: 0.00989802\n",
      "Epoch 3 | Step 1998100 | Avg Loss: 0.0153 | Grad Norm: 0.00910742\n",
      "Epoch 3 | Step 1998200 | Avg Loss: 0.0154 | Grad Norm: 0.00967060\n",
      "Epoch 3 | Step 1998300 | Avg Loss: 0.0155 | Grad Norm: 0.01046428\n",
      "Epoch 3 | Step 1998400 | Avg Loss: 0.0156 | Grad Norm: 0.00873531\n",
      "Epoch 3 | Step 1998500 | Avg Loss: 0.0155 | Grad Norm: 0.01053658\n",
      "Epoch 3 | Step 1998600 | Avg Loss: 0.0154 | Grad Norm: 0.00849300\n",
      "Epoch 3 | Step 1998700 | Avg Loss: 0.0155 | Grad Norm: 0.00778603\n",
      "Epoch 3 | Step 1998800 | Avg Loss: 0.0157 | Grad Norm: 0.00875976\n",
      "Epoch 3 | Step 1998900 | Avg Loss: 0.0159 | Grad Norm: 0.00820744\n",
      "Epoch 3 | Step 1999000 | Avg Loss: 0.0158 | Grad Norm: 0.00829829\n",
      "Epoch 3 | Step 1999100 | Avg Loss: 0.0157 | Grad Norm: 0.01071063\n",
      "Epoch 3 | Step 1999200 | Avg Loss: 0.0156 | Grad Norm: 0.00944782\n",
      "Epoch 3 | Step 1999300 | Avg Loss: 0.0159 | Grad Norm: 0.00906500\n",
      "Epoch 3 | Step 1999400 | Avg Loss: 0.0156 | Grad Norm: 0.00848022\n",
      "Epoch 3 | Step 1999500 | Avg Loss: 0.0153 | Grad Norm: 0.00823292\n",
      "Epoch 3 | Step 1999600 | Avg Loss: 0.0153 | Grad Norm: 0.00844274\n",
      "Epoch 3 | Step 1999700 | Avg Loss: 0.0155 | Grad Norm: 0.01110916\n",
      "Epoch 3 | Step 1999800 | Avg Loss: 0.0158 | Grad Norm: 0.00837487\n",
      "Epoch 3 | Step 1999900 | Avg Loss: 0.0161 | Grad Norm: 0.00856254\n",
      "Epoch 3 | Step 2000000 | Avg Loss: 0.0161 | Grad Norm: 0.01003660\n",
      "Saving model at step2000000\n",
      "Epoch 3 | Step 2000100 | Avg Loss: 0.0160 | Grad Norm: 0.00839645\n",
      "Epoch 3 | Step 2000200 | Avg Loss: 0.0160 | Grad Norm: 0.00978747\n",
      "Epoch 3 | Step 2000300 | Avg Loss: 0.0157 | Grad Norm: 0.00835598\n",
      "Epoch 3 | Step 2000400 | Avg Loss: 0.0156 | Grad Norm: 0.01195726\n",
      "Epoch 3 | Step 2000500 | Avg Loss: 0.0151 | Grad Norm: 0.00870757\n",
      "Epoch 3 | Step 2000600 | Avg Loss: 0.0153 | Grad Norm: 0.00771498\n",
      "Epoch 3 | Step 2000700 | Avg Loss: 0.0155 | Grad Norm: 0.00819367\n",
      "Epoch 3 | Step 2000800 | Avg Loss: 0.0160 | Grad Norm: 0.01035226\n",
      "Epoch 3 | Step 2000900 | Avg Loss: 0.0155 | Grad Norm: 0.00898332\n",
      "Epoch 3 | Step 2001000 | Avg Loss: 0.0154 | Grad Norm: 0.00879158\n",
      "Epoch 3 | Step 2001100 | Avg Loss: 0.0150 | Grad Norm: 0.00862472\n",
      "Epoch 3 | Step 2001200 | Avg Loss: 0.0149 | Grad Norm: 0.00772627\n",
      "Epoch 3 | Step 2001300 | Avg Loss: 0.0149 | Grad Norm: 0.00820389\n",
      "Epoch 3 | Step 2001400 | Avg Loss: 0.0147 | Grad Norm: 0.00807352\n",
      "Epoch 3 | Step 2001500 | Avg Loss: 0.0146 | Grad Norm: 0.00904743\n",
      "Epoch 3 | Step 2001600 | Avg Loss: 0.0151 | Grad Norm: 0.00741272\n",
      "Epoch 3 | Step 2001700 | Avg Loss: 0.0154 | Grad Norm: 0.00999651\n",
      "Epoch 3 | Step 2001800 | Avg Loss: 0.0153 | Grad Norm: 0.00870366\n",
      "Epoch 3 | Step 2001900 | Avg Loss: 0.0154 | Grad Norm: 0.00860341\n",
      "Epoch 3 | Step 2002000 | Avg Loss: 0.0158 | Grad Norm: 0.00842111\n",
      "Epoch 3 | Step 2002100 | Avg Loss: 0.0157 | Grad Norm: 0.01197155\n",
      "Epoch 3 | Step 2002200 | Avg Loss: 0.0160 | Grad Norm: 0.00856218\n",
      "Epoch 3 | Step 2002300 | Avg Loss: 0.0162 | Grad Norm: 0.00857336\n",
      "Epoch 3 | Step 2002400 | Avg Loss: 0.0163 | Grad Norm: 0.00946399\n",
      "Epoch 3 | Step 2002500 | Avg Loss: 0.0164 | Grad Norm: 0.01503460\n",
      "Epoch 3 | Step 2002600 | Avg Loss: 0.0160 | Grad Norm: 0.00875889\n",
      "Epoch 3 | Step 2002700 | Avg Loss: 0.0157 | Grad Norm: 0.01018644\n",
      "Epoch 3 | Step 2002800 | Avg Loss: 0.0151 | Grad Norm: 0.00813775\n",
      "Epoch 3 | Step 2002900 | Avg Loss: 0.0151 | Grad Norm: 0.00829226\n",
      "Epoch 3 | Step 2003000 | Avg Loss: 0.0146 | Grad Norm: 0.00878657\n",
      "Epoch 3 | Step 2003100 | Avg Loss: 0.0150 | Grad Norm: 0.00829721\n",
      "Epoch 3 | Step 2003200 | Avg Loss: 0.0148 | Grad Norm: 0.00912064\n",
      "Epoch 3 | Step 2003300 | Avg Loss: 0.0148 | Grad Norm: 0.00916520\n",
      "Epoch 3 | Step 2003400 | Avg Loss: 0.0148 | Grad Norm: 0.00792478\n",
      "Epoch 3 | Step 2003500 | Avg Loss: 0.0148 | Grad Norm: 0.00903796\n",
      "Epoch 3 | Step 2003600 | Avg Loss: 0.0155 | Grad Norm: 0.01217701\n",
      "Epoch 3 | Step 2003700 | Avg Loss: 0.0153 | Grad Norm: 0.00882761\n",
      "Epoch 3 | Step 2003800 | Avg Loss: 0.0156 | Grad Norm: 0.00836144\n",
      "Epoch 3 | Step 2003900 | Avg Loss: 0.0160 | Grad Norm: 0.00929578\n",
      "Epoch 3 | Step 2004000 | Avg Loss: 0.0160 | Grad Norm: 0.00883459\n",
      "Epoch 3 | Step 2004100 | Avg Loss: 0.0158 | Grad Norm: 0.00910273\n",
      "Epoch 3 | Step 2004200 | Avg Loss: 0.0159 | Grad Norm: 0.01104767\n",
      "Epoch 3 | Step 2004300 | Avg Loss: 0.0157 | Grad Norm: 0.00985566\n",
      "Epoch 3 | Step 2004400 | Avg Loss: 0.0157 | Grad Norm: 0.00995837\n",
      "Epoch 3 | Step 2004500 | Avg Loss: 0.0154 | Grad Norm: 0.00909845\n",
      "Epoch 3 | Step 2004600 | Avg Loss: 0.0151 | Grad Norm: 0.00777081\n",
      "Epoch 3 | Step 2004700 | Avg Loss: 0.0147 | Grad Norm: 0.01093638\n",
      "Epoch 3 | Step 2004800 | Avg Loss: 0.0152 | Grad Norm: 0.00798512\n",
      "Epoch 3 | Step 2004900 | Avg Loss: 0.0153 | Grad Norm: 0.00834874\n",
      "Epoch 3 | Step 2005000 | Avg Loss: 0.0152 | Grad Norm: 0.00878411\n",
      "Epoch 3 | Step 2005100 | Avg Loss: 0.0157 | Grad Norm: 0.00872126\n",
      "Epoch 3 | Step 2005200 | Avg Loss: 0.0158 | Grad Norm: 0.01009585\n",
      "Epoch 3 | Step 2005300 | Avg Loss: 0.0153 | Grad Norm: 0.00858121\n",
      "Epoch 3 | Step 2005400 | Avg Loss: 0.0154 | Grad Norm: 0.00879463\n",
      "Epoch 3 | Step 2005500 | Avg Loss: 0.0151 | Grad Norm: 0.00915665\n",
      "Epoch 3 | Step 2005600 | Avg Loss: 0.0150 | Grad Norm: 0.00887290\n",
      "Epoch 3 | Step 2005700 | Avg Loss: 0.0155 | Grad Norm: 0.00932964\n",
      "Epoch 3 | Step 2005800 | Avg Loss: 0.0160 | Grad Norm: 0.01124934\n",
      "Epoch 3 | Step 2005900 | Avg Loss: 0.0160 | Grad Norm: 0.01121535\n",
      "Epoch 3 | Step 2006000 | Avg Loss: 0.0157 | Grad Norm: 0.01037364\n",
      "Epoch 3 | Step 2006100 | Avg Loss: 0.0157 | Grad Norm: 0.00927965\n",
      "Epoch 3 | Step 2006200 | Avg Loss: 0.0156 | Grad Norm: 0.00845819\n",
      "Epoch 3 | Step 2006300 | Avg Loss: 0.0159 | Grad Norm: 0.01031216\n",
      "Epoch 3 | Step 2006400 | Avg Loss: 0.0161 | Grad Norm: 0.00711681\n",
      "Epoch 3 | Step 2006500 | Avg Loss: 0.0161 | Grad Norm: 0.00793627\n",
      "Epoch 3 | Step 2006600 | Avg Loss: 0.0161 | Grad Norm: 0.00909852\n",
      "Epoch 3 | Step 2006700 | Avg Loss: 0.0161 | Grad Norm: 0.00873979\n",
      "Epoch 3 | Step 2006800 | Avg Loss: 0.0160 | Grad Norm: 0.00818385\n",
      "Epoch 3 | Step 2006900 | Avg Loss: 0.0160 | Grad Norm: 0.00868576\n",
      "Epoch 3 | Step 2007000 | Avg Loss: 0.0162 | Grad Norm: 0.00840893\n",
      "Epoch 3 | Step 2007100 | Avg Loss: 0.0160 | Grad Norm: 0.00923650\n",
      "Epoch 3 | Step 2007200 | Avg Loss: 0.0155 | Grad Norm: 0.00914060\n",
      "Epoch 3 | Step 2007300 | Avg Loss: 0.0160 | Grad Norm: 0.00991808\n",
      "Epoch 3 | Step 2007400 | Avg Loss: 0.0163 | Grad Norm: 0.00928208\n",
      "Epoch 3 | Step 2007500 | Avg Loss: 0.0160 | Grad Norm: 0.00943242\n",
      "Epoch 3 | Step 2007600 | Avg Loss: 0.0159 | Grad Norm: 0.01015621\n",
      "Epoch 3 | Step 2007700 | Avg Loss: 0.0159 | Grad Norm: 0.00884031\n",
      "Epoch 3 | Step 2007800 | Avg Loss: 0.0156 | Grad Norm: 0.00923261\n",
      "Epoch 3 | Step 2007900 | Avg Loss: 0.0158 | Grad Norm: 0.01031047\n",
      "Epoch 3 | Step 2008000 | Avg Loss: 0.0161 | Grad Norm: 0.00796124\n",
      "Epoch 3 | Step 2008100 | Avg Loss: 0.0158 | Grad Norm: 0.00946282\n",
      "Epoch 3 | Step 2008200 | Avg Loss: 0.0161 | Grad Norm: 0.00966688\n",
      "Epoch 3 | Step 2008300 | Avg Loss: 0.0158 | Grad Norm: 0.00832852\n",
      "Epoch 3 | Step 2008400 | Avg Loss: 0.0153 | Grad Norm: 0.00962511\n",
      "Epoch 3 | Step 2008500 | Avg Loss: 0.0157 | Grad Norm: 0.00920444\n",
      "Epoch 3 | Step 2008600 | Avg Loss: 0.0157 | Grad Norm: 0.00837519\n",
      "Epoch 3 | Step 2008700 | Avg Loss: 0.0154 | Grad Norm: 0.00809160\n",
      "Epoch 3 | Step 2008800 | Avg Loss: 0.0156 | Grad Norm: 0.00882974\n",
      "Epoch 3 | Step 2008900 | Avg Loss: 0.0153 | Grad Norm: 0.00825702\n",
      "Epoch 3 | Step 2009000 | Avg Loss: 0.0154 | Grad Norm: 0.00928096\n",
      "Epoch 3 | Step 2009100 | Avg Loss: 0.0156 | Grad Norm: 0.00836654\n",
      "Epoch 3 | Step 2009200 | Avg Loss: 0.0157 | Grad Norm: 0.00884321\n",
      "Epoch 3 | Step 2009300 | Avg Loss: 0.0158 | Grad Norm: 0.01122445\n",
      "Epoch 3 | Step 2009400 | Avg Loss: 0.0158 | Grad Norm: 0.00910096\n",
      "Epoch 3 | Step 2009500 | Avg Loss: 0.0163 | Grad Norm: 0.00854069\n",
      "Epoch 3 | Step 2009600 | Avg Loss: 0.0163 | Grad Norm: 0.00879124\n",
      "Epoch 3 | Step 2009700 | Avg Loss: 0.0159 | Grad Norm: 0.00932488\n",
      "Epoch 3 | Step 2009800 | Avg Loss: 0.0160 | Grad Norm: 0.00949472\n",
      "Epoch 3 | Step 2009900 | Avg Loss: 0.0161 | Grad Norm: 0.01035658\n",
      "Epoch 3 | Step 2010000 | Avg Loss: 0.0156 | Grad Norm: 0.00884093\n",
      "Epoch 3 | Step 2010100 | Avg Loss: 0.0154 | Grad Norm: 0.00844734\n",
      "Epoch 3 | Step 2010200 | Avg Loss: 0.0157 | Grad Norm: 0.00940438\n",
      "Epoch 3 | Step 2010300 | Avg Loss: 0.0158 | Grad Norm: 0.00786875\n",
      "Epoch 3 | Step 2010400 | Avg Loss: 0.0161 | Grad Norm: 0.01031582\n",
      "Epoch 3 | Step 2010500 | Avg Loss: 0.0163 | Grad Norm: 0.01005068\n",
      "Epoch 3 | Step 2010600 | Avg Loss: 0.0162 | Grad Norm: 0.00876751\n",
      "Epoch 3 | Step 2010700 | Avg Loss: 0.0165 | Grad Norm: 0.01036007\n",
      "Epoch 3 | Step 2010800 | Avg Loss: 0.0161 | Grad Norm: 0.00836604\n",
      "Epoch 3 | Step 2010900 | Avg Loss: 0.0157 | Grad Norm: 0.00914214\n",
      "Epoch 3 | Step 2011000 | Avg Loss: 0.0157 | Grad Norm: 0.01038696\n",
      "Epoch 3 | Step 2011100 | Avg Loss: 0.0156 | Grad Norm: 0.00927573\n",
      "Epoch 3 | Step 2011200 | Avg Loss: 0.0154 | Grad Norm: 0.00892883\n",
      "Epoch 3 | Step 2011300 | Avg Loss: 0.0156 | Grad Norm: 0.00892165\n",
      "Epoch 3 | Step 2011400 | Avg Loss: 0.0155 | Grad Norm: 0.00851424\n",
      "Epoch 3 | Step 2011500 | Avg Loss: 0.0154 | Grad Norm: 0.00867365\n",
      "Epoch 3 | Step 2011600 | Avg Loss: 0.0157 | Grad Norm: 0.01000371\n",
      "Epoch 3 | Step 2011700 | Avg Loss: 0.0157 | Grad Norm: 0.00798737\n",
      "Epoch 3 | Step 2011800 | Avg Loss: 0.0158 | Grad Norm: 0.00817359\n",
      "Epoch 3 | Step 2011900 | Avg Loss: 0.0157 | Grad Norm: 0.00835912\n",
      "Epoch 3 | Step 2012000 | Avg Loss: 0.0154 | Grad Norm: 0.00965267\n",
      "Epoch 3 | Step 2012100 | Avg Loss: 0.0157 | Grad Norm: 0.00907093\n",
      "Epoch 3 | Step 2012200 | Avg Loss: 0.0160 | Grad Norm: 0.00874939\n",
      "Epoch 3 | Step 2012300 | Avg Loss: 0.0158 | Grad Norm: 0.00870676\n",
      "Epoch 3 | Step 2012400 | Avg Loss: 0.0156 | Grad Norm: 0.00801169\n",
      "Epoch 3 | Step 2012500 | Avg Loss: 0.0156 | Grad Norm: 0.00832094\n",
      "Epoch 3 | Step 2012600 | Avg Loss: 0.0153 | Grad Norm: 0.00932124\n",
      "Epoch 3 | Step 2012700 | Avg Loss: 0.0152 | Grad Norm: 0.00905094\n",
      "Epoch 3 | Step 2012800 | Avg Loss: 0.0151 | Grad Norm: 0.00941081\n",
      "Epoch 3 | Step 2012900 | Avg Loss: 0.0152 | Grad Norm: 0.00926980\n",
      "Epoch 3 | Step 2013000 | Avg Loss: 0.0152 | Grad Norm: 0.00883442\n",
      "Epoch 3 | Step 2013100 | Avg Loss: 0.0151 | Grad Norm: 0.00876286\n",
      "Epoch 3 | Step 2013200 | Avg Loss: 0.0154 | Grad Norm: 0.01183137\n",
      "Epoch 3 | Step 2013300 | Avg Loss: 0.0149 | Grad Norm: 0.00866983\n",
      "Epoch 3 | Step 2013400 | Avg Loss: 0.0154 | Grad Norm: 0.00815616\n",
      "Epoch 3 | Step 2013500 | Avg Loss: 0.0155 | Grad Norm: 0.00978596\n",
      "Epoch 3 | Step 2013600 | Avg Loss: 0.0157 | Grad Norm: 0.00925537\n",
      "Epoch 3 | Step 2013700 | Avg Loss: 0.0159 | Grad Norm: 0.00931697\n",
      "Epoch 3 | Step 2013800 | Avg Loss: 0.0157 | Grad Norm: 0.00938129\n",
      "Epoch 3 | Step 2013900 | Avg Loss: 0.0157 | Grad Norm: 0.00821868\n",
      "Epoch 3 | Step 2014000 | Avg Loss: 0.0158 | Grad Norm: 0.00908242\n",
      "Epoch 3 | Step 2014100 | Avg Loss: 0.0155 | Grad Norm: 0.00914371\n",
      "Epoch 3 | Step 2014200 | Avg Loss: 0.0156 | Grad Norm: 0.00926354\n",
      "Epoch 3 | Step 2014300 | Avg Loss: 0.0160 | Grad Norm: 0.00934125\n",
      "Epoch 3 | Step 2014400 | Avg Loss: 0.0159 | Grad Norm: 0.00968366\n",
      "Epoch 3 | Step 2014500 | Avg Loss: 0.0160 | Grad Norm: 0.01073207\n",
      "Epoch 3 | Step 2014600 | Avg Loss: 0.0161 | Grad Norm: 0.00927247\n",
      "Epoch 3 | Step 2014700 | Avg Loss: 0.0164 | Grad Norm: 0.00920797\n",
      "Epoch 3 | Step 2014800 | Avg Loss: 0.0164 | Grad Norm: 0.00908070\n",
      "Epoch 3 | Step 2014900 | Avg Loss: 0.0163 | Grad Norm: 0.01074920\n",
      "Epoch 3 | Step 2015000 | Avg Loss: 0.0158 | Grad Norm: 0.00889334\n",
      "Epoch 3 | Step 2015100 | Avg Loss: 0.0161 | Grad Norm: 0.01083500\n",
      "Epoch 3 | Step 2015200 | Avg Loss: 0.0160 | Grad Norm: 0.00858263\n",
      "Epoch 3 | Step 2015300 | Avg Loss: 0.0159 | Grad Norm: 0.00869228\n",
      "Epoch 3 | Step 2015400 | Avg Loss: 0.0156 | Grad Norm: 0.00850258\n",
      "Epoch 3 | Step 2015500 | Avg Loss: 0.0156 | Grad Norm: 0.00938544\n",
      "Epoch 3 | Step 2015600 | Avg Loss: 0.0157 | Grad Norm: 0.00908846\n",
      "Epoch 3 | Step 2015700 | Avg Loss: 0.0159 | Grad Norm: 0.00892172\n",
      "Epoch 3 | Step 2015800 | Avg Loss: 0.0157 | Grad Norm: 0.01027378\n",
      "Epoch 3 | Step 2015900 | Avg Loss: 0.0155 | Grad Norm: 0.00790841\n",
      "Epoch 3 | Step 2016000 | Avg Loss: 0.0158 | Grad Norm: 0.00852654\n",
      "Epoch 3 | Step 2016100 | Avg Loss: 0.0154 | Grad Norm: 0.00828915\n",
      "Epoch 3 | Step 2016200 | Avg Loss: 0.0156 | Grad Norm: 0.00841752\n",
      "Epoch 3 | Step 2016300 | Avg Loss: 0.0155 | Grad Norm: 0.00885597\n",
      "Epoch 3 | Step 2016400 | Avg Loss: 0.0153 | Grad Norm: 0.00856945\n",
      "Epoch 3 | Step 2016500 | Avg Loss: 0.0156 | Grad Norm: 0.00919440\n",
      "Epoch 3 | Step 2016600 | Avg Loss: 0.0156 | Grad Norm: 0.00894498\n",
      "Epoch 3 | Step 2016700 | Avg Loss: 0.0157 | Grad Norm: 0.00921845\n",
      "Epoch 3 | Step 2016800 | Avg Loss: 0.0161 | Grad Norm: 0.00792397\n",
      "Epoch 3 | Step 2016900 | Avg Loss: 0.0161 | Grad Norm: 0.00816053\n",
      "Epoch 3 | Step 2017000 | Avg Loss: 0.0160 | Grad Norm: 0.00919844\n",
      "Epoch 3 | Step 2017100 | Avg Loss: 0.0164 | Grad Norm: 0.00952182\n",
      "Epoch 3 | Step 2017200 | Avg Loss: 0.0166 | Grad Norm: 0.00932188\n",
      "Epoch 3 | Step 2017300 | Avg Loss: 0.0163 | Grad Norm: 0.01123863\n",
      "Epoch 3 | Step 2017400 | Avg Loss: 0.0162 | Grad Norm: 0.00863887\n",
      "Epoch 3 | Step 2017500 | Avg Loss: 0.0161 | Grad Norm: 0.00871995\n",
      "Epoch 3 | Step 2017600 | Avg Loss: 0.0167 | Grad Norm: 0.00995194\n",
      "Epoch 3 | Step 2017700 | Avg Loss: 0.0158 | Grad Norm: 0.00907077\n",
      "Epoch 3 | Step 2017800 | Avg Loss: 0.0158 | Grad Norm: 0.01139600\n",
      "Epoch 3 | Step 2017900 | Avg Loss: 0.0161 | Grad Norm: 0.00970180\n",
      "Epoch 3 | Step 2018000 | Avg Loss: 0.0160 | Grad Norm: 0.00969951\n",
      "Epoch 3 | Step 2018100 | Avg Loss: 0.0156 | Grad Norm: 0.00909626\n",
      "Epoch 3 | Step 2018200 | Avg Loss: 0.0156 | Grad Norm: 0.00963803\n",
      "Epoch 3 | Step 2018300 | Avg Loss: 0.0152 | Grad Norm: 0.00880333\n",
      "Epoch 3 | Step 2018400 | Avg Loss: 0.0152 | Grad Norm: 0.00896875\n",
      "Epoch 3 | Step 2018500 | Avg Loss: 0.0148 | Grad Norm: 0.00895636\n",
      "Epoch 3 | Step 2018600 | Avg Loss: 0.0149 | Grad Norm: 0.01100265\n",
      "Epoch 3 | Step 2018700 | Avg Loss: 0.0150 | Grad Norm: 0.00934979\n",
      "Epoch 3 | Step 2018800 | Avg Loss: 0.0151 | Grad Norm: 0.00953078\n",
      "Epoch 3 | Step 2018900 | Avg Loss: 0.0150 | Grad Norm: 0.00770328\n",
      "Epoch 3 | Step 2019000 | Avg Loss: 0.0150 | Grad Norm: 0.00869949\n",
      "Epoch 3 | Step 2019100 | Avg Loss: 0.0151 | Grad Norm: 0.00849768\n",
      "Epoch 3 | Step 2019200 | Avg Loss: 0.0154 | Grad Norm: 0.00883170\n",
      "Epoch 3 | Step 2019300 | Avg Loss: 0.0154 | Grad Norm: 0.00741957\n",
      "Epoch 3 | Step 2019400 | Avg Loss: 0.0153 | Grad Norm: 0.00896852\n",
      "Epoch 3 | Step 2019500 | Avg Loss: 0.0152 | Grad Norm: 0.00854133\n",
      "Epoch 3 | Step 2019600 | Avg Loss: 0.0151 | Grad Norm: 0.00848101\n",
      "Epoch 3 | Step 2019700 | Avg Loss: 0.0149 | Grad Norm: 0.00851943\n",
      "Epoch 3 | Step 2019800 | Avg Loss: 0.0150 | Grad Norm: 0.01008178\n",
      "Epoch 3 | Step 2019900 | Avg Loss: 0.0152 | Grad Norm: 0.00842382\n",
      "Epoch 3 | Step 2020000 | Avg Loss: 0.0157 | Grad Norm: 0.00966955\n",
      "Epoch 3 | Step 2020100 | Avg Loss: 0.0157 | Grad Norm: 0.00893285\n",
      "Epoch 3 | Step 2020200 | Avg Loss: 0.0155 | Grad Norm: 0.00760459\n",
      "Epoch 3 | Step 2020300 | Avg Loss: 0.0153 | Grad Norm: 0.01041464\n",
      "Epoch 3 | Step 2020400 | Avg Loss: 0.0154 | Grad Norm: 0.00771021\n",
      "Epoch 3 | Step 2020500 | Avg Loss: 0.0153 | Grad Norm: 0.00850028\n",
      "Epoch 3 | Step 2020600 | Avg Loss: 0.0152 | Grad Norm: 0.00818951\n",
      "Epoch 3 | Step 2020700 | Avg Loss: 0.0154 | Grad Norm: 0.00739335\n",
      "Epoch 3 | Step 2020800 | Avg Loss: 0.0156 | Grad Norm: 0.00797321\n",
      "Epoch 3 | Step 2020900 | Avg Loss: 0.0154 | Grad Norm: 0.00981968\n",
      "Epoch 3 | Step 2021000 | Avg Loss: 0.0155 | Grad Norm: 0.00849388\n",
      "Epoch 3 | Step 2021100 | Avg Loss: 0.0157 | Grad Norm: 0.00806042\n",
      "Epoch 3 | Step 2021200 | Avg Loss: 0.0161 | Grad Norm: 0.01002901\n",
      "Epoch 3 | Step 2021300 | Avg Loss: 0.0163 | Grad Norm: 0.01017158\n",
      "Epoch 3 | Step 2021400 | Avg Loss: 0.0160 | Grad Norm: 0.00927807\n",
      "Epoch 3 | Step 2021500 | Avg Loss: 0.0158 | Grad Norm: 0.00915134\n",
      "Epoch 3 | Step 2021600 | Avg Loss: 0.0155 | Grad Norm: 0.00825087\n",
      "Epoch 3 | Step 2021700 | Avg Loss: 0.0157 | Grad Norm: 0.00939459\n",
      "Epoch 3 | Step 2021800 | Avg Loss: 0.0154 | Grad Norm: 0.01012885\n",
      "Epoch 3 | Step 2021900 | Avg Loss: 0.0153 | Grad Norm: 0.00927221\n",
      "Epoch 3 | Step 2022000 | Avg Loss: 0.0153 | Grad Norm: 0.00830297\n",
      "Epoch 3 | Step 2022100 | Avg Loss: 0.0154 | Grad Norm: 0.00858490\n",
      "Epoch 3 | Step 2022200 | Avg Loss: 0.0155 | Grad Norm: 0.00987282\n",
      "Epoch 3 | Step 2022300 | Avg Loss: 0.0153 | Grad Norm: 0.01134717\n",
      "Epoch 3 | Step 2022400 | Avg Loss: 0.0150 | Grad Norm: 0.01001684\n",
      "Epoch 3 | Step 2022500 | Avg Loss: 0.0152 | Grad Norm: 0.00857961\n",
      "Epoch 3 | Step 2022600 | Avg Loss: 0.0151 | Grad Norm: 0.00784029\n",
      "Epoch 3 | Step 2022700 | Avg Loss: 0.0152 | Grad Norm: 0.00924024\n",
      "Epoch 3 | Step 2022800 | Avg Loss: 0.0153 | Grad Norm: 0.00887136\n",
      "Epoch 3 | Step 2022900 | Avg Loss: 0.0155 | Grad Norm: 0.00790831\n",
      "Epoch 3 | Step 2023000 | Avg Loss: 0.0152 | Grad Norm: 0.00945564\n",
      "Epoch 3 | Step 2023100 | Avg Loss: 0.0151 | Grad Norm: 0.00825807\n",
      "Epoch 3 | Step 2023200 | Avg Loss: 0.0150 | Grad Norm: 0.00812196\n",
      "Epoch 3 | Step 2023300 | Avg Loss: 0.0150 | Grad Norm: 0.00980000\n",
      "Epoch 3 | Step 2023400 | Avg Loss: 0.0148 | Grad Norm: 0.00749087\n",
      "Epoch 3 | Step 2023500 | Avg Loss: 0.0152 | Grad Norm: 0.00860223\n",
      "Epoch 3 | Step 2023600 | Avg Loss: 0.0150 | Grad Norm: 0.00946616\n",
      "Epoch 3 | Step 2023700 | Avg Loss: 0.0151 | Grad Norm: 0.00875361\n",
      "Epoch 3 | Step 2023800 | Avg Loss: 0.0149 | Grad Norm: 0.00728184\n",
      "Epoch 3 | Step 2023900 | Avg Loss: 0.0152 | Grad Norm: 0.00879972\n",
      "Epoch 3 | Step 2024000 | Avg Loss: 0.0149 | Grad Norm: 0.00791594\n",
      "Epoch 3 | Step 2024100 | Avg Loss: 0.0150 | Grad Norm: 0.00843800\n",
      "Epoch 3 | Step 2024200 | Avg Loss: 0.0152 | Grad Norm: 0.00912598\n",
      "Epoch 3 | Step 2024300 | Avg Loss: 0.0154 | Grad Norm: 0.01042760\n",
      "Epoch 3 | Step 2024400 | Avg Loss: 0.0149 | Grad Norm: 0.00851396\n",
      "Epoch 3 | Step 2024500 | Avg Loss: 0.0147 | Grad Norm: 0.00943073\n",
      "Epoch 3 | Step 2024600 | Avg Loss: 0.0149 | Grad Norm: 0.00781211\n",
      "Epoch 3 | Step 2024700 | Avg Loss: 0.0153 | Grad Norm: 0.00861262\n",
      "Epoch 3 | Step 2024800 | Avg Loss: 0.0152 | Grad Norm: 0.00874514\n",
      "Epoch 3 | Step 2024900 | Avg Loss: 0.0150 | Grad Norm: 0.00889535\n",
      "Epoch 3 | Step 2025000 | Avg Loss: 0.0151 | Grad Norm: 0.00900011\n",
      "Epoch 3 | Step 2025100 | Avg Loss: 0.0155 | Grad Norm: 0.00902047\n",
      "Epoch 3 | Step 2025200 | Avg Loss: 0.0154 | Grad Norm: 0.01167859\n",
      "Epoch 3 | Step 2025300 | Avg Loss: 0.0153 | Grad Norm: 0.00932752\n",
      "Epoch 3 | Step 2025400 | Avg Loss: 0.0156 | Grad Norm: 0.00992874\n",
      "Epoch 3 | Step 2025500 | Avg Loss: 0.0157 | Grad Norm: 0.00928311\n",
      "Epoch 3 | Step 2025600 | Avg Loss: 0.0156 | Grad Norm: 0.00970300\n",
      "Epoch 3 | Step 2025700 | Avg Loss: 0.0158 | Grad Norm: 0.00979018\n",
      "Epoch 3 | Step 2025800 | Avg Loss: 0.0157 | Grad Norm: 0.00878292\n",
      "Epoch 3 | Step 2025900 | Avg Loss: 0.0158 | Grad Norm: 0.00883565\n",
      "Epoch 3 | Step 2026000 | Avg Loss: 0.0161 | Grad Norm: 0.00946784\n",
      "Epoch 3 | Step 2026100 | Avg Loss: 0.0158 | Grad Norm: 0.00871854\n",
      "Epoch 3 | Step 2026200 | Avg Loss: 0.0155 | Grad Norm: 0.00813809\n",
      "Epoch 3 | Step 2026300 | Avg Loss: 0.0156 | Grad Norm: 0.01100071\n",
      "Epoch 3 | Step 2026400 | Avg Loss: 0.0154 | Grad Norm: 0.00818862\n",
      "Epoch 3 | Step 2026500 | Avg Loss: 0.0151 | Grad Norm: 0.00769565\n",
      "Epoch 3 | Step 2026600 | Avg Loss: 0.0155 | Grad Norm: 0.00995551\n",
      "Epoch 3 | Step 2026700 | Avg Loss: 0.0155 | Grad Norm: 0.00735823\n",
      "Epoch 3 | Step 2026800 | Avg Loss: 0.0157 | Grad Norm: 0.00956723\n",
      "Epoch 3 | Step 2026900 | Avg Loss: 0.0158 | Grad Norm: 0.00830083\n",
      "Epoch 3 | Step 2027000 | Avg Loss: 0.0155 | Grad Norm: 0.00958344\n",
      "Epoch 3 | Step 2027100 | Avg Loss: 0.0154 | Grad Norm: 0.00862408\n",
      "Epoch 3 | Step 2027200 | Avg Loss: 0.0156 | Grad Norm: 0.00977586\n",
      "Epoch 3 | Step 2027300 | Avg Loss: 0.0155 | Grad Norm: 0.00919977\n",
      "Epoch 3 | Step 2027400 | Avg Loss: 0.0157 | Grad Norm: 0.00838041\n",
      "Epoch 3 | Step 2027500 | Avg Loss: 0.0158 | Grad Norm: 0.00856161\n",
      "Epoch 3 | Step 2027600 | Avg Loss: 0.0159 | Grad Norm: 0.00965782\n",
      "Epoch 3 | Step 2027700 | Avg Loss: 0.0156 | Grad Norm: 0.00853212\n",
      "Epoch 3 | Step 2027800 | Avg Loss: 0.0157 | Grad Norm: 0.00827158\n",
      "Epoch 3 | Step 2027900 | Avg Loss: 0.0157 | Grad Norm: 0.00895203\n",
      "Epoch 3 | Step 2028000 | Avg Loss: 0.0158 | Grad Norm: 0.01091038\n",
      "Epoch 3 | Step 2028100 | Avg Loss: 0.0158 | Grad Norm: 0.00794521\n",
      "Epoch 3 | Step 2028200 | Avg Loss: 0.0156 | Grad Norm: 0.00959866\n",
      "Epoch 3 | Step 2028300 | Avg Loss: 0.0158 | Grad Norm: 0.01156068\n",
      "Epoch 3 | Step 2028400 | Avg Loss: 0.0161 | Grad Norm: 0.00842486\n",
      "Epoch 3 | Step 2028500 | Avg Loss: 0.0161 | Grad Norm: 0.00916862\n",
      "Epoch 3 | Step 2028600 | Avg Loss: 0.0159 | Grad Norm: 0.00848077\n",
      "Epoch 3 | Step 2028700 | Avg Loss: 0.0160 | Grad Norm: 0.00945669\n",
      "Epoch 3 | Step 2028800 | Avg Loss: 0.0159 | Grad Norm: 0.00926993\n",
      "Epoch 3 | Step 2028900 | Avg Loss: 0.0159 | Grad Norm: 0.00966382\n",
      "Epoch 3 | Step 2029000 | Avg Loss: 0.0157 | Grad Norm: 0.00951426\n",
      "Epoch 3 | Step 2029100 | Avg Loss: 0.0156 | Grad Norm: 0.00938684\n",
      "Epoch 3 | Step 2029200 | Avg Loss: 0.0154 | Grad Norm: 0.00831385\n",
      "Epoch 3 | Step 2029300 | Avg Loss: 0.0153 | Grad Norm: 0.00847696\n",
      "Epoch 3 | Step 2029400 | Avg Loss: 0.0156 | Grad Norm: 0.00909646\n",
      "Epoch 3 | Step 2029500 | Avg Loss: 0.0155 | Grad Norm: 0.00834290\n",
      "Epoch 3 | Step 2029600 | Avg Loss: 0.0155 | Grad Norm: 0.00802217\n",
      "Epoch 3 | Step 2029700 | Avg Loss: 0.0156 | Grad Norm: 0.01145792\n",
      "Epoch 3 | Step 2029800 | Avg Loss: 0.0156 | Grad Norm: 0.00898228\n",
      "Epoch 3 | Step 2029900 | Avg Loss: 0.0156 | Grad Norm: 0.00876536\n",
      "Epoch 3 | Step 2030000 | Avg Loss: 0.0155 | Grad Norm: 0.00877466\n",
      "Epoch 3 | Step 2030100 | Avg Loss: 0.0156 | Grad Norm: 0.00842166\n",
      "Epoch 3 | Step 2030200 | Avg Loss: 0.0155 | Grad Norm: 0.01012953\n",
      "Epoch 3 | Step 2030300 | Avg Loss: 0.0157 | Grad Norm: 0.00868713\n",
      "Epoch 3 | Step 2030400 | Avg Loss: 0.0156 | Grad Norm: 0.00992401\n",
      "Epoch 3 | Step 2030500 | Avg Loss: 0.0155 | Grad Norm: 0.00845987\n",
      "Epoch 3 | Step 2030600 | Avg Loss: 0.0153 | Grad Norm: 0.00823337\n",
      "Epoch 3 | Step 2030700 | Avg Loss: 0.0153 | Grad Norm: 0.00837138\n",
      "Epoch 3 | Step 2030800 | Avg Loss: 0.0156 | Grad Norm: 0.00910936\n",
      "Epoch 3 | Step 2030900 | Avg Loss: 0.0158 | Grad Norm: 0.00947920\n",
      "Epoch 3 | Step 2031000 | Avg Loss: 0.0155 | Grad Norm: 0.00880291\n",
      "Epoch 3 | Step 2031100 | Avg Loss: 0.0157 | Grad Norm: 0.00815675\n",
      "Epoch 3 | Step 2031200 | Avg Loss: 0.0157 | Grad Norm: 0.00840762\n",
      "Epoch 3 | Step 2031300 | Avg Loss: 0.0154 | Grad Norm: 0.00954019\n",
      "Epoch 3 | Step 2031400 | Avg Loss: 0.0155 | Grad Norm: 0.00828244\n",
      "Epoch 3 | Step 2031500 | Avg Loss: 0.0152 | Grad Norm: 0.00949508\n",
      "Epoch 3 | Step 2031600 | Avg Loss: 0.0155 | Grad Norm: 0.01006806\n",
      "Epoch 3 | Step 2031700 | Avg Loss: 0.0155 | Grad Norm: 0.00895907\n",
      "Epoch 3 | Step 2031800 | Avg Loss: 0.0156 | Grad Norm: 0.00930397\n",
      "Epoch 3 | Step 2031900 | Avg Loss: 0.0157 | Grad Norm: 0.00882379\n",
      "Epoch 3 | Step 2032000 | Avg Loss: 0.0155 | Grad Norm: 0.00910019\n",
      "Epoch 3 | Step 2032100 | Avg Loss: 0.0157 | Grad Norm: 0.00835137\n",
      "Epoch 3 | Step 2032200 | Avg Loss: 0.0156 | Grad Norm: 0.00897796\n",
      "Epoch 3 | Step 2032300 | Avg Loss: 0.0156 | Grad Norm: 0.01010047\n",
      "Epoch 3 | Step 2032400 | Avg Loss: 0.0157 | Grad Norm: 0.00907201\n",
      "Epoch 3 | Step 2032500 | Avg Loss: 0.0155 | Grad Norm: 0.00981956\n",
      "Epoch 3 | Step 2032600 | Avg Loss: 0.0159 | Grad Norm: 0.00997577\n",
      "Epoch 3 | Step 2032700 | Avg Loss: 0.0155 | Grad Norm: 0.00847808\n",
      "Epoch 3 | Step 2032800 | Avg Loss: 0.0157 | Grad Norm: 0.00868577\n",
      "Epoch 3 | Step 2032900 | Avg Loss: 0.0155 | Grad Norm: 0.00866263\n",
      "Epoch 3 | Step 2033000 | Avg Loss: 0.0154 | Grad Norm: 0.00900051\n",
      "Epoch 3 | Step 2033100 | Avg Loss: 0.0155 | Grad Norm: 0.00842109\n",
      "Epoch 3 | Step 2033200 | Avg Loss: 0.0155 | Grad Norm: 0.01011160\n",
      "Epoch 3 | Step 2033300 | Avg Loss: 0.0152 | Grad Norm: 0.00978977\n",
      "Epoch 3 | Step 2033400 | Avg Loss: 0.0149 | Grad Norm: 0.01279113\n",
      "Epoch 3 | Step 2033500 | Avg Loss: 0.0151 | Grad Norm: 0.00915624\n",
      "Epoch 3 | Step 2033600 | Avg Loss: 0.0153 | Grad Norm: 0.00874242\n",
      "Epoch 3 | Step 2033700 | Avg Loss: 0.0155 | Grad Norm: 0.00861806\n",
      "Epoch 3 | Step 2033800 | Avg Loss: 0.0155 | Grad Norm: 0.01025878\n",
      "Epoch 3 | Step 2033900 | Avg Loss: 0.0156 | Grad Norm: 0.00877012\n",
      "Epoch 3 | Step 2034000 | Avg Loss: 0.0152 | Grad Norm: 0.00842810\n",
      "Epoch 3 | Step 2034100 | Avg Loss: 0.0153 | Grad Norm: 0.00870041\n",
      "Epoch 3 | Step 2034200 | Avg Loss: 0.0156 | Grad Norm: 0.00858467\n",
      "Epoch 3 | Step 2034300 | Avg Loss: 0.0157 | Grad Norm: 0.00920107\n",
      "Epoch 3 | Step 2034400 | Avg Loss: 0.0159 | Grad Norm: 0.00872646\n",
      "Epoch 3 | Step 2034500 | Avg Loss: 0.0156 | Grad Norm: 0.00869306\n",
      "Epoch 3 | Step 2034600 | Avg Loss: 0.0153 | Grad Norm: 0.00918199\n",
      "Epoch 3 | Step 2034700 | Avg Loss: 0.0151 | Grad Norm: 0.00926741\n",
      "Epoch 3 | Step 2034800 | Avg Loss: 0.0153 | Grad Norm: 0.01035073\n",
      "Epoch 3 | Step 2034900 | Avg Loss: 0.0153 | Grad Norm: 0.00772085\n",
      "Epoch 3 | Step 2035000 | Avg Loss: 0.0153 | Grad Norm: 0.00894104\n",
      "Epoch 3 | Step 2035100 | Avg Loss: 0.0157 | Grad Norm: 0.00906128\n",
      "Epoch 3 | Step 2035200 | Avg Loss: 0.0155 | Grad Norm: 0.00997827\n",
      "Epoch 3 | Step 2035300 | Avg Loss: 0.0156 | Grad Norm: 0.00833188\n",
      "Epoch 3 | Step 2035400 | Avg Loss: 0.0157 | Grad Norm: 0.00859949\n",
      "Epoch 3 | Step 2035500 | Avg Loss: 0.0156 | Grad Norm: 0.00848695\n",
      "Epoch 3 | Step 2035600 | Avg Loss: 0.0155 | Grad Norm: 0.00854115\n",
      "Epoch 3 | Step 2035700 | Avg Loss: 0.0153 | Grad Norm: 0.00819951\n",
      "Epoch 3 | Step 2035800 | Avg Loss: 0.0155 | Grad Norm: 0.01019375\n",
      "Epoch 3 | Step 2035900 | Avg Loss: 0.0153 | Grad Norm: 0.00730072\n",
      "Epoch 3 | Step 2036000 | Avg Loss: 0.0154 | Grad Norm: 0.00857301\n",
      "Epoch 3 | Step 2036100 | Avg Loss: 0.0153 | Grad Norm: 0.00741918\n",
      "Epoch 3 | Step 2036200 | Avg Loss: 0.0154 | Grad Norm: 0.01005914\n",
      "Epoch 3 | Step 2036300 | Avg Loss: 0.0156 | Grad Norm: 0.00907021\n",
      "Epoch 3 | Step 2036400 | Avg Loss: 0.0161 | Grad Norm: 0.00954456\n",
      "Epoch 3 | Step 2036500 | Avg Loss: 0.0159 | Grad Norm: 0.01025944\n",
      "Epoch 3 | Step 2036600 | Avg Loss: 0.0158 | Grad Norm: 0.00828779\n",
      "Epoch 3 | Step 2036700 | Avg Loss: 0.0158 | Grad Norm: 0.00979346\n",
      "Epoch 3 | Step 2036800 | Avg Loss: 0.0156 | Grad Norm: 0.00901853\n",
      "Epoch 3 | Step 2036900 | Avg Loss: 0.0161 | Grad Norm: 0.00838608\n",
      "Epoch 3 | Step 2037000 | Avg Loss: 0.0162 | Grad Norm: 0.01039832\n",
      "Epoch 3 | Step 2037100 | Avg Loss: 0.0162 | Grad Norm: 0.00910197\n",
      "Epoch 3 | Step 2037200 | Avg Loss: 0.0160 | Grad Norm: 0.00974474\n",
      "Epoch 3 | Step 2037300 | Avg Loss: 0.0163 | Grad Norm: 0.00944936\n",
      "Epoch 3 | Step 2037400 | Avg Loss: 0.0161 | Grad Norm: 0.00864973\n",
      "Epoch 3 | Step 2037500 | Avg Loss: 0.0159 | Grad Norm: 0.00910610\n",
      "Epoch 3 | Step 2037600 | Avg Loss: 0.0160 | Grad Norm: 0.00886566\n",
      "Epoch 3 | Step 2037700 | Avg Loss: 0.0160 | Grad Norm: 0.00839115\n",
      "Epoch 3 | Step 2037800 | Avg Loss: 0.0158 | Grad Norm: 0.01075315\n",
      "Epoch 3 | Step 2037900 | Avg Loss: 0.0154 | Grad Norm: 0.01415401\n",
      "Epoch 3 | Step 2038000 | Avg Loss: 0.0153 | Grad Norm: 0.00948333\n",
      "Epoch 3 | Step 2038100 | Avg Loss: 0.0148 | Grad Norm: 0.00886976\n",
      "Epoch 3 | Step 2038200 | Avg Loss: 0.0150 | Grad Norm: 0.00856829\n",
      "Epoch 3 | Step 2038300 | Avg Loss: 0.0150 | Grad Norm: 0.00814430\n",
      "Epoch 3 | Step 2038400 | Avg Loss: 0.0150 | Grad Norm: 0.00797164\n",
      "Epoch 3 | Step 2038500 | Avg Loss: 0.0149 | Grad Norm: 0.00847029\n",
      "Epoch 3 | Step 2038600 | Avg Loss: 0.0152 | Grad Norm: 0.00874249\n",
      "Epoch 3 | Step 2038700 | Avg Loss: 0.0150 | Grad Norm: 0.01177918\n",
      "Epoch 3 | Step 2038800 | Avg Loss: 0.0153 | Grad Norm: 0.00827952\n",
      "Epoch 3 | Step 2038900 | Avg Loss: 0.0157 | Grad Norm: 0.00778419\n",
      "Epoch 3 | Step 2039000 | Avg Loss: 0.0158 | Grad Norm: 0.00878146\n",
      "Epoch 3 | Step 2039100 | Avg Loss: 0.0159 | Grad Norm: 0.00883275\n",
      "Epoch 3 | Step 2039200 | Avg Loss: 0.0159 | Grad Norm: 0.00940693\n",
      "Epoch 3 | Step 2039300 | Avg Loss: 0.0157 | Grad Norm: 0.01031476\n",
      "Epoch 3 | Step 2039400 | Avg Loss: 0.0155 | Grad Norm: 0.00960114\n",
      "Epoch 3 | Step 2039500 | Avg Loss: 0.0158 | Grad Norm: 0.01057714\n",
      "Epoch 3 | Step 2039600 | Avg Loss: 0.0155 | Grad Norm: 0.00942522\n",
      "Epoch 3 | Step 2039700 | Avg Loss: 0.0153 | Grad Norm: 0.00922222\n",
      "Epoch 3 | Step 2039800 | Avg Loss: 0.0155 | Grad Norm: 0.00765220\n",
      "Epoch 3 | Step 2039900 | Avg Loss: 0.0157 | Grad Norm: 0.00823082\n",
      "Epoch 3 | Step 2040000 | Avg Loss: 0.0158 | Grad Norm: 0.01079331\n",
      "Epoch 3 | Step 2040100 | Avg Loss: 0.0159 | Grad Norm: 0.00925264\n",
      "Epoch 3 | Step 2040200 | Avg Loss: 0.0155 | Grad Norm: 0.00883673\n",
      "Epoch 3 | Step 2040300 | Avg Loss: 0.0157 | Grad Norm: 0.00888351\n",
      "Epoch 3 | Step 2040400 | Avg Loss: 0.0158 | Grad Norm: 0.00973221\n",
      "Epoch 3 | Step 2040500 | Avg Loss: 0.0163 | Grad Norm: 0.00908429\n",
      "Epoch 3 | Step 2040600 | Avg Loss: 0.0163 | Grad Norm: 0.01242528\n",
      "Epoch 3 | Step 2040700 | Avg Loss: 0.0161 | Grad Norm: 0.00856375\n",
      "Epoch 3 | Step 2040800 | Avg Loss: 0.0160 | Grad Norm: 0.00829849\n",
      "Epoch 3 | Step 2040900 | Avg Loss: 0.0159 | Grad Norm: 0.00964447\n",
      "Epoch 3 | Step 2041000 | Avg Loss: 0.0160 | Grad Norm: 0.00952556\n",
      "Epoch 3 | Step 2041100 | Avg Loss: 0.0161 | Grad Norm: 0.00874488\n",
      "Epoch 3 | Step 2041200 | Avg Loss: 0.0158 | Grad Norm: 0.01039445\n",
      "Epoch 3 | Step 2041300 | Avg Loss: 0.0154 | Grad Norm: 0.00902295\n",
      "Epoch 3 | Step 2041400 | Avg Loss: 0.0154 | Grad Norm: 0.00952371\n",
      "Epoch 3 | Step 2041500 | Avg Loss: 0.0156 | Grad Norm: 0.00909121\n",
      "Epoch 3 | Step 2041600 | Avg Loss: 0.0157 | Grad Norm: 0.00875029\n",
      "Epoch 3 | Step 2041700 | Avg Loss: 0.0153 | Grad Norm: 0.00971954\n",
      "Epoch 3 | Step 2041800 | Avg Loss: 0.0150 | Grad Norm: 0.00981589\n",
      "Epoch 3 | Step 2041900 | Avg Loss: 0.0156 | Grad Norm: 0.00905105\n",
      "Epoch 3 | Step 2042000 | Avg Loss: 0.0160 | Grad Norm: 0.00881304\n",
      "Epoch 3 | Step 2042100 | Avg Loss: 0.0161 | Grad Norm: 0.00935399\n",
      "Epoch 3 | Step 2042200 | Avg Loss: 0.0160 | Grad Norm: 0.01026341\n",
      "Epoch 3 | Step 2042300 | Avg Loss: 0.0158 | Grad Norm: 0.01070344\n",
      "Epoch 3 | Step 2042400 | Avg Loss: 0.0160 | Grad Norm: 0.00844261\n",
      "Epoch 3 | Step 2042500 | Avg Loss: 0.0165 | Grad Norm: 0.00791632\n",
      "Epoch 3 | Step 2042600 | Avg Loss: 0.0162 | Grad Norm: 0.00871204\n",
      "Epoch 3 | Step 2042700 | Avg Loss: 0.0161 | Grad Norm: 0.00946859\n",
      "Epoch 3 | Step 2042800 | Avg Loss: 0.0162 | Grad Norm: 0.00784700\n",
      "Epoch 3 | Step 2042900 | Avg Loss: 0.0157 | Grad Norm: 0.00886220\n",
      "Epoch 3 | Step 2043000 | Avg Loss: 0.0153 | Grad Norm: 0.00713894\n",
      "Epoch 3 | Step 2043100 | Avg Loss: 0.0155 | Grad Norm: 0.00926289\n",
      "Epoch 3 | Step 2043200 | Avg Loss: 0.0152 | Grad Norm: 0.00840606\n",
      "Epoch 3 | Step 2043300 | Avg Loss: 0.0153 | Grad Norm: 0.00839305\n",
      "Epoch 3 | Step 2043400 | Avg Loss: 0.0156 | Grad Norm: 0.01054029\n",
      "Epoch 3 | Step 2043500 | Avg Loss: 0.0155 | Grad Norm: 0.00953589\n",
      "Epoch 3 | Step 2043600 | Avg Loss: 0.0162 | Grad Norm: 0.00933369\n",
      "Epoch 3 | Step 2043700 | Avg Loss: 0.0161 | Grad Norm: 0.01086129\n",
      "Epoch 3 | Step 2043800 | Avg Loss: 0.0160 | Grad Norm: 0.00793138\n",
      "Epoch 3 | Step 2043900 | Avg Loss: 0.0162 | Grad Norm: 0.00956713\n",
      "Epoch 3 | Step 2044000 | Avg Loss: 0.0158 | Grad Norm: 0.00828606\n",
      "Epoch 3 | Step 2044100 | Avg Loss: 0.0159 | Grad Norm: 0.00914733\n",
      "Epoch 3 | Step 2044200 | Avg Loss: 0.0158 | Grad Norm: 0.00948460\n",
      "Epoch 3 | Step 2044300 | Avg Loss: 0.0159 | Grad Norm: 0.00935452\n",
      "Epoch 3 | Step 2044400 | Avg Loss: 0.0154 | Grad Norm: 0.00823783\n",
      "Epoch 3 | Step 2044500 | Avg Loss: 0.0150 | Grad Norm: 0.00934587\n",
      "Epoch 3 | Step 2044600 | Avg Loss: 0.0153 | Grad Norm: 0.00930548\n",
      "Epoch 3 | Step 2044700 | Avg Loss: 0.0155 | Grad Norm: 0.00979465\n",
      "Epoch 3 | Step 2044800 | Avg Loss: 0.0150 | Grad Norm: 0.00816055\n",
      "Epoch 3 | Step 2044900 | Avg Loss: 0.0154 | Grad Norm: 0.00869981\n",
      "Epoch 3 | Step 2045000 | Avg Loss: 0.0159 | Grad Norm: 0.00981621\n",
      "Epoch 3 | Step 2045100 | Avg Loss: 0.0159 | Grad Norm: 0.01012426\n",
      "Epoch 3 | Step 2045200 | Avg Loss: 0.0160 | Grad Norm: 0.00959914\n",
      "Epoch 3 | Step 2045300 | Avg Loss: 0.0161 | Grad Norm: 0.00857647\n",
      "Epoch 3 | Step 2045400 | Avg Loss: 0.0158 | Grad Norm: 0.00931357\n",
      "Epoch 3 | Step 2045500 | Avg Loss: 0.0160 | Grad Norm: 0.00845050\n",
      "Epoch 3 | Step 2045600 | Avg Loss: 0.0159 | Grad Norm: 0.00976961\n",
      "Epoch 3 | Step 2045700 | Avg Loss: 0.0156 | Grad Norm: 0.00950749\n",
      "Epoch 3 | Step 2045800 | Avg Loss: 0.0157 | Grad Norm: 0.00880012\n",
      "Epoch 3 | Step 2045900 | Avg Loss: 0.0157 | Grad Norm: 0.00900098\n",
      "Epoch 3 | Step 2046000 | Avg Loss: 0.0156 | Grad Norm: 0.00992833\n",
      "Epoch 3 | Step 2046100 | Avg Loss: 0.0154 | Grad Norm: 0.00969027\n",
      "Epoch 3 | Step 2046200 | Avg Loss: 0.0151 | Grad Norm: 0.00843417\n",
      "Epoch 3 | Step 2046300 | Avg Loss: 0.0154 | Grad Norm: 0.00836552\n",
      "Epoch 3 | Step 2046400 | Avg Loss: 0.0153 | Grad Norm: 0.00817933\n",
      "Epoch 3 | Step 2046500 | Avg Loss: 0.0155 | Grad Norm: 0.00769058\n",
      "Epoch 3 | Step 2046600 | Avg Loss: 0.0154 | Grad Norm: 0.00888637\n",
      "Epoch 3 | Step 2046700 | Avg Loss: 0.0155 | Grad Norm: 0.00937117\n",
      "Epoch 3 | Step 2046800 | Avg Loss: 0.0156 | Grad Norm: 0.01014635\n",
      "Epoch 3 | Step 2046900 | Avg Loss: 0.0158 | Grad Norm: 0.00874297\n",
      "Epoch 3 | Step 2047000 | Avg Loss: 0.0156 | Grad Norm: 0.00911497\n",
      "Epoch 3 | Step 2047100 | Avg Loss: 0.0154 | Grad Norm: 0.00864439\n",
      "Epoch 3 | Step 2047200 | Avg Loss: 0.0154 | Grad Norm: 0.00839778\n",
      "Epoch 3 | Step 2047300 | Avg Loss: 0.0156 | Grad Norm: 0.00869582\n",
      "Epoch 3 | Step 2047400 | Avg Loss: 0.0158 | Grad Norm: 0.00978494\n",
      "Epoch 3 | Step 2047500 | Avg Loss: 0.0156 | Grad Norm: 0.00831311\n",
      "Epoch 3 | Step 2047600 | Avg Loss: 0.0155 | Grad Norm: 0.01061617\n",
      "Epoch 3 | Step 2047700 | Avg Loss: 0.0153 | Grad Norm: 0.00997511\n",
      "Epoch 3 | Step 2047800 | Avg Loss: 0.0152 | Grad Norm: 0.00799614\n",
      "Epoch 3 | Step 2047900 | Avg Loss: 0.0152 | Grad Norm: 0.00959924\n",
      "Epoch 3 | Step 2048000 | Avg Loss: 0.0153 | Grad Norm: 0.00836371\n",
      "Epoch 3 | Step 2048100 | Avg Loss: 0.0153 | Grad Norm: 0.00841047\n",
      "Epoch 3 | Step 2048200 | Avg Loss: 0.0154 | Grad Norm: 0.00827881\n",
      "Epoch 3 | Step 2048300 | Avg Loss: 0.0153 | Grad Norm: 0.00764222\n",
      "Epoch 3 | Step 2048400 | Avg Loss: 0.0153 | Grad Norm: 0.00924906\n",
      "Epoch 3 | Step 2048500 | Avg Loss: 0.0152 | Grad Norm: 0.00810663\n",
      "Epoch 3 | Step 2048600 | Avg Loss: 0.0153 | Grad Norm: 0.00915682\n",
      "Epoch 3 | Step 2048700 | Avg Loss: 0.0156 | Grad Norm: 0.00902218\n",
      "Epoch 3 | Step 2048800 | Avg Loss: 0.0157 | Grad Norm: 0.00897165\n",
      "Epoch 3 | Step 2048900 | Avg Loss: 0.0161 | Grad Norm: 0.00890681\n",
      "Epoch 3 | Step 2049000 | Avg Loss: 0.0158 | Grad Norm: 0.00935549\n",
      "Epoch 3 | Step 2049100 | Avg Loss: 0.0156 | Grad Norm: 0.01089444\n",
      "Epoch 3 | Step 2049200 | Avg Loss: 0.0155 | Grad Norm: 0.00940709\n",
      "Epoch 3 | Step 2049300 | Avg Loss: 0.0156 | Grad Norm: 0.00942538\n",
      "Epoch 3 | Step 2049400 | Avg Loss: 0.0155 | Grad Norm: 0.00838339\n",
      "Epoch 3 | Step 2049500 | Avg Loss: 0.0156 | Grad Norm: 0.00842169\n",
      "Epoch 3 | Step 2049600 | Avg Loss: 0.0158 | Grad Norm: 0.00872539\n",
      "Epoch 3 | Step 2049700 | Avg Loss: 0.0159 | Grad Norm: 0.00891769\n",
      "Epoch 3 | Step 2049800 | Avg Loss: 0.0157 | Grad Norm: 0.00933650\n",
      "Epoch 3 | Step 2049900 | Avg Loss: 0.0154 | Grad Norm: 0.00909670\n",
      "Epoch 3 | Step 2050000 | Avg Loss: 0.0151 | Grad Norm: 0.00814861\n",
      "Epoch 3 | Step 2050100 | Avg Loss: 0.0148 | Grad Norm: 0.00888635\n",
      "Epoch 3 | Step 2050200 | Avg Loss: 0.0147 | Grad Norm: 0.00787824\n",
      "Epoch 3 | Step 2050300 | Avg Loss: 0.0150 | Grad Norm: 0.00818796\n",
      "Epoch 3 | Step 2050400 | Avg Loss: 0.0152 | Grad Norm: 0.00815008\n",
      "Epoch 3 | Step 2050500 | Avg Loss: 0.0150 | Grad Norm: 0.00853547\n",
      "Epoch 3 | Step 2050600 | Avg Loss: 0.0153 | Grad Norm: 0.00918939\n",
      "Epoch 3 | Step 2050700 | Avg Loss: 0.0151 | Grad Norm: 0.00883870\n",
      "Epoch 3 | Step 2050800 | Avg Loss: 0.0152 | Grad Norm: 0.01099635\n",
      "Epoch 3 | Step 2050900 | Avg Loss: 0.0154 | Grad Norm: 0.00968027\n",
      "Epoch 3 | Step 2051000 | Avg Loss: 0.0156 | Grad Norm: 0.00825106\n",
      "Epoch 3 | Step 2051100 | Avg Loss: 0.0155 | Grad Norm: 0.01434675\n",
      "Epoch 3 | Step 2051200 | Avg Loss: 0.0158 | Grad Norm: 0.00989141\n",
      "Epoch 3 | Step 2051300 | Avg Loss: 0.0160 | Grad Norm: 0.01097100\n",
      "Epoch 3 | Step 2051400 | Avg Loss: 0.0162 | Grad Norm: 0.01576376\n",
      "Epoch 3 | Step 2051500 | Avg Loss: 0.0160 | Grad Norm: 0.00933318\n",
      "Epoch 3 | Step 2051600 | Avg Loss: 0.0158 | Grad Norm: 0.00943481\n",
      "Epoch 3 | Step 2051700 | Avg Loss: 0.0155 | Grad Norm: 0.00889109\n",
      "Epoch 3 | Step 2051800 | Avg Loss: 0.0155 | Grad Norm: 0.00903108\n",
      "Epoch 3 | Step 2051900 | Avg Loss: 0.0157 | Grad Norm: 0.00937467\n",
      "Epoch 3 | Step 2052000 | Avg Loss: 0.0157 | Grad Norm: 0.00833055\n",
      "Epoch 3 | Step 2052100 | Avg Loss: 0.0155 | Grad Norm: 0.01092033\n",
      "Epoch 3 | Step 2052200 | Avg Loss: 0.0156 | Grad Norm: 0.00829916\n",
      "Epoch 3 | Step 2052300 | Avg Loss: 0.0157 | Grad Norm: 0.00856101\n",
      "Epoch 3 | Step 2052400 | Avg Loss: 0.0159 | Grad Norm: 0.00926333\n",
      "Epoch 3 | Step 2052500 | Avg Loss: 0.0161 | Grad Norm: 0.01124357\n",
      "Epoch 3 | Step 2052600 | Avg Loss: 0.0160 | Grad Norm: 0.00848043\n",
      "Epoch 3 | Step 2052700 | Avg Loss: 0.0165 | Grad Norm: 0.01002065\n",
      "Epoch 3 | Step 2052800 | Avg Loss: 0.0164 | Grad Norm: 0.00947308\n",
      "Epoch 3 | Step 2052900 | Avg Loss: 0.0161 | Grad Norm: 0.00910173\n",
      "Epoch 3 | Step 2053000 | Avg Loss: 0.0164 | Grad Norm: 0.00947816\n",
      "Epoch 3 | Step 2053100 | Avg Loss: 0.0163 | Grad Norm: 0.00928412\n",
      "Epoch 3 | Step 2053200 | Avg Loss: 0.0160 | Grad Norm: 0.00868224\n",
      "Epoch 3 | Step 2053300 | Avg Loss: 0.0156 | Grad Norm: 0.00891107\n",
      "Epoch 3 | Step 2053400 | Avg Loss: 0.0155 | Grad Norm: 0.00870629\n",
      "Epoch 3 | Step 2053500 | Avg Loss: 0.0160 | Grad Norm: 0.00775401\n",
      "Epoch 3 | Step 2053600 | Avg Loss: 0.0157 | Grad Norm: 0.00830510\n",
      "Epoch 3 | Step 2053700 | Avg Loss: 0.0156 | Grad Norm: 0.01036455\n",
      "Epoch 3 | Step 2053800 | Avg Loss: 0.0154 | Grad Norm: 0.00913767\n",
      "Epoch 3 | Step 2053900 | Avg Loss: 0.0154 | Grad Norm: 0.00886007\n",
      "Epoch 3 | Step 2054000 | Avg Loss: 0.0153 | Grad Norm: 0.00906814\n",
      "Epoch 3 | Step 2054100 | Avg Loss: 0.0155 | Grad Norm: 0.00841103\n",
      "Epoch 3 | Step 2054200 | Avg Loss: 0.0156 | Grad Norm: 0.00955206\n",
      "Epoch 3 | Step 2054300 | Avg Loss: 0.0157 | Grad Norm: 0.00794653\n",
      "Epoch 3 | Step 2054400 | Avg Loss: 0.0154 | Grad Norm: 0.00912474\n",
      "Epoch 3 | Step 2054500 | Avg Loss: 0.0150 | Grad Norm: 0.01055219\n",
      "Epoch 3 | Step 2054600 | Avg Loss: 0.0152 | Grad Norm: 0.00913860\n",
      "Epoch 3 | Step 2054700 | Avg Loss: 0.0152 | Grad Norm: 0.00889579\n",
      "Epoch 3 | Step 2054800 | Avg Loss: 0.0154 | Grad Norm: 0.00795981\n",
      "Epoch 3 | Step 2054900 | Avg Loss: 0.0150 | Grad Norm: 0.00848987\n",
      "Epoch 3 | Step 2055000 | Avg Loss: 0.0157 | Grad Norm: 0.00815845\n",
      "Epoch 3 | Step 2055100 | Avg Loss: 0.0160 | Grad Norm: 0.00986432\n",
      "Epoch 3 | Step 2055200 | Avg Loss: 0.0158 | Grad Norm: 0.00806328\n",
      "Epoch 3 | Step 2055300 | Avg Loss: 0.0155 | Grad Norm: 0.00869977\n",
      "Epoch 3 | Step 2055400 | Avg Loss: 0.0156 | Grad Norm: 0.00952836\n",
      "Epoch 3 | Step 2055500 | Avg Loss: 0.0158 | Grad Norm: 0.00858543\n",
      "Epoch 3 | Step 2055600 | Avg Loss: 0.0157 | Grad Norm: 0.00867089\n",
      "Epoch 3 | Step 2055700 | Avg Loss: 0.0159 | Grad Norm: 0.01068138\n",
      "Epoch 3 | Step 2055800 | Avg Loss: 0.0157 | Grad Norm: 0.00887470\n",
      "Epoch 3 | Step 2055900 | Avg Loss: 0.0159 | Grad Norm: 0.00964690\n",
      "Epoch 3 | Step 2056000 | Avg Loss: 0.0158 | Grad Norm: 0.00882896\n",
      "Epoch 3 | Step 2056100 | Avg Loss: 0.0158 | Grad Norm: 0.00954795\n",
      "Epoch 3 | Step 2056200 | Avg Loss: 0.0159 | Grad Norm: 0.00971728\n",
      "Epoch 3 | Step 2056300 | Avg Loss: 0.0162 | Grad Norm: 0.01039667\n",
      "Epoch 3 | Step 2056400 | Avg Loss: 0.0156 | Grad Norm: 0.00965534\n",
      "Epoch 3 | Step 2056500 | Avg Loss: 0.0154 | Grad Norm: 0.00990511\n",
      "Epoch 3 | Step 2056600 | Avg Loss: 0.0155 | Grad Norm: 0.00920752\n",
      "Epoch 3 | Step 2056700 | Avg Loss: 0.0157 | Grad Norm: 0.01244887\n",
      "Epoch 3 | Step 2056800 | Avg Loss: 0.0156 | Grad Norm: 0.00935690\n",
      "Epoch 3 | Step 2056900 | Avg Loss: 0.0153 | Grad Norm: 0.01142721\n",
      "Epoch 3 | Step 2057000 | Avg Loss: 0.0154 | Grad Norm: 0.00813807\n",
      "Epoch 3 | Step 2057100 | Avg Loss: 0.0152 | Grad Norm: 0.00864619\n",
      "Epoch 3 | Step 2057200 | Avg Loss: 0.0152 | Grad Norm: 0.01127790\n",
      "Epoch 3 | Step 2057300 | Avg Loss: 0.0154 | Grad Norm: 0.00943188\n",
      "Epoch 3 | Step 2057400 | Avg Loss: 0.0157 | Grad Norm: 0.00869007\n",
      "Epoch 3 | Step 2057500 | Avg Loss: 0.0159 | Grad Norm: 0.00835185\n",
      "Epoch 3 | Step 2057600 | Avg Loss: 0.0160 | Grad Norm: 0.00881310\n",
      "Epoch 3 | Step 2057700 | Avg Loss: 0.0158 | Grad Norm: 0.00844136\n",
      "Epoch 3 | Step 2057800 | Avg Loss: 0.0159 | Grad Norm: 0.00954899\n",
      "Epoch 3 | Step 2057900 | Avg Loss: 0.0160 | Grad Norm: 0.00909468\n",
      "Epoch 3 | Step 2058000 | Avg Loss: 0.0159 | Grad Norm: 0.01042193\n",
      "Epoch 3 | Step 2058100 | Avg Loss: 0.0158 | Grad Norm: 0.00897817\n",
      "Epoch 3 | Step 2058200 | Avg Loss: 0.0157 | Grad Norm: 0.00906988\n",
      "Epoch 3 | Step 2058300 | Avg Loss: 0.0155 | Grad Norm: 0.01113842\n",
      "Epoch 3 | Step 2058400 | Avg Loss: 0.0155 | Grad Norm: 0.00874313\n",
      "Epoch 3 | Step 2058500 | Avg Loss: 0.0154 | Grad Norm: 0.00846274\n",
      "Epoch 3 | Step 2058600 | Avg Loss: 0.0152 | Grad Norm: 0.00939660\n",
      "Epoch 3 | Step 2058700 | Avg Loss: 0.0154 | Grad Norm: 0.00887508\n",
      "Epoch 3 | Step 2058800 | Avg Loss: 0.0156 | Grad Norm: 0.00851955\n",
      "Epoch 3 | Step 2058900 | Avg Loss: 0.0151 | Grad Norm: 0.01305426\n",
      "Epoch 3 | Step 2059000 | Avg Loss: 0.0152 | Grad Norm: 0.00856707\n",
      "Epoch 3 | Step 2059100 | Avg Loss: 0.0156 | Grad Norm: 0.00929608\n",
      "Epoch 3 | Step 2059200 | Avg Loss: 0.0159 | Grad Norm: 0.01096347\n",
      "Epoch 3 | Step 2059300 | Avg Loss: 0.0162 | Grad Norm: 0.00887550\n",
      "Epoch 3 | Step 2059400 | Avg Loss: 0.0159 | Grad Norm: 0.00908516\n",
      "Epoch 3 | Step 2059500 | Avg Loss: 0.0154 | Grad Norm: 0.00856653\n",
      "Epoch 3 | Step 2059600 | Avg Loss: 0.0152 | Grad Norm: 0.01115990\n",
      "Epoch 3 | Step 2059700 | Avg Loss: 0.0156 | Grad Norm: 0.00901772\n",
      "Epoch 3 | Step 2059800 | Avg Loss: 0.0156 | Grad Norm: 0.00922362\n",
      "Epoch 3 | Step 2059900 | Avg Loss: 0.0155 | Grad Norm: 0.00837897\n",
      "Epoch 3 | Step 2060000 | Avg Loss: 0.0155 | Grad Norm: 0.00890536\n",
      "Epoch 3 | Step 2060100 | Avg Loss: 0.0153 | Grad Norm: 0.00814236\n",
      "Epoch 3 | Step 2060200 | Avg Loss: 0.0151 | Grad Norm: 0.00842364\n",
      "Epoch 3 | Step 2060300 | Avg Loss: 0.0150 | Grad Norm: 0.00831624\n",
      "Epoch 3 | Step 2060400 | Avg Loss: 0.0149 | Grad Norm: 0.00857606\n",
      "Epoch 3 | Step 2060500 | Avg Loss: 0.0149 | Grad Norm: 0.00892715\n",
      "Epoch 3 | Step 2060600 | Avg Loss: 0.0150 | Grad Norm: 0.00896896\n",
      "Epoch 3 | Step 2060700 | Avg Loss: 0.0151 | Grad Norm: 0.00878123\n",
      "Epoch 3 | Step 2060800 | Avg Loss: 0.0148 | Grad Norm: 0.00829130\n",
      "Epoch 3 | Step 2060900 | Avg Loss: 0.0149 | Grad Norm: 0.00768876\n",
      "Epoch 3 | Step 2061000 | Avg Loss: 0.0152 | Grad Norm: 0.00945327\n",
      "Epoch 3 | Step 2061100 | Avg Loss: 0.0156 | Grad Norm: 0.00904511\n",
      "Epoch 3 | Step 2061200 | Avg Loss: 0.0158 | Grad Norm: 0.00941996\n",
      "Epoch 3 | Step 2061300 | Avg Loss: 0.0159 | Grad Norm: 0.00888804\n",
      "Epoch 3 | Step 2061400 | Avg Loss: 0.0159 | Grad Norm: 0.00984116\n",
      "Epoch 3 | Step 2061500 | Avg Loss: 0.0154 | Grad Norm: 0.00837930\n",
      "Epoch 3 | Step 2061600 | Avg Loss: 0.0159 | Grad Norm: 0.00798176\n",
      "Epoch 3 | Step 2061700 | Avg Loss: 0.0161 | Grad Norm: 0.00980611\n",
      "Epoch 3 | Step 2061800 | Avg Loss: 0.0157 | Grad Norm: 0.01065123\n",
      "Epoch 3 | Step 2061900 | Avg Loss: 0.0157 | Grad Norm: 0.00846850\n",
      "Epoch 3 | Step 2062000 | Avg Loss: 0.0158 | Grad Norm: 0.00850949\n",
      "Epoch 3 | Step 2062100 | Avg Loss: 0.0159 | Grad Norm: 0.00894329\n",
      "Epoch 3 | Step 2062200 | Avg Loss: 0.0159 | Grad Norm: 0.00859489\n",
      "Epoch 3 | Step 2062300 | Avg Loss: 0.0162 | Grad Norm: 0.00876637\n",
      "Epoch 3 | Step 2062400 | Avg Loss: 0.0161 | Grad Norm: 0.00815706\n",
      "Epoch 3 | Step 2062500 | Avg Loss: 0.0165 | Grad Norm: 0.00841609\n",
      "Epoch 3 | Step 2062600 | Avg Loss: 0.0161 | Grad Norm: 0.00796935\n",
      "Epoch 3 | Step 2062700 | Avg Loss: 0.0158 | Grad Norm: 0.00831186\n",
      "Epoch 3 | Step 2062800 | Avg Loss: 0.0159 | Grad Norm: 0.00864882\n",
      "Epoch 3 | Step 2062900 | Avg Loss: 0.0156 | Grad Norm: 0.01123752\n",
      "Epoch 3 | Step 2063000 | Avg Loss: 0.0157 | Grad Norm: 0.01182693\n",
      "Epoch 3 | Step 2063100 | Avg Loss: 0.0155 | Grad Norm: 0.00910648\n",
      "Epoch 3 | Step 2063200 | Avg Loss: 0.0157 | Grad Norm: 0.00899329\n",
      "Epoch 3 | Step 2063300 | Avg Loss: 0.0155 | Grad Norm: 0.00995106\n",
      "Epoch 3 | Step 2063400 | Avg Loss: 0.0154 | Grad Norm: 0.00830992\n",
      "Epoch 3 | Step 2063500 | Avg Loss: 0.0156 | Grad Norm: 0.00930862\n",
      "Epoch 3 | Step 2063600 | Avg Loss: 0.0157 | Grad Norm: 0.00916189\n",
      "Epoch 3 | Step 2063700 | Avg Loss: 0.0156 | Grad Norm: 0.00884593\n",
      "Epoch 3 | Step 2063800 | Avg Loss: 0.0154 | Grad Norm: 0.00936903\n",
      "Epoch 3 | Step 2063900 | Avg Loss: 0.0156 | Grad Norm: 0.00909918\n",
      "Epoch 3 | Step 2064000 | Avg Loss: 0.0152 | Grad Norm: 0.00829209\n",
      "Epoch 3 | Step 2064100 | Avg Loss: 0.0153 | Grad Norm: 0.00833983\n",
      "Epoch 3 | Step 2064200 | Avg Loss: 0.0151 | Grad Norm: 0.01000992\n",
      "Epoch 3 | Step 2064300 | Avg Loss: 0.0144 | Grad Norm: 0.00909619\n",
      "Epoch 3 | Step 2064400 | Avg Loss: 0.0144 | Grad Norm: 0.00825848\n",
      "Epoch 3 | Step 2064500 | Avg Loss: 0.0144 | Grad Norm: 0.00816822\n",
      "Epoch 3 | Step 2064600 | Avg Loss: 0.0147 | Grad Norm: 0.00955359\n",
      "Epoch 3 | Step 2064700 | Avg Loss: 0.0147 | Grad Norm: 0.01044970\n",
      "Epoch 3 | Step 2064800 | Avg Loss: 0.0148 | Grad Norm: 0.00928370\n",
      "Epoch 3 | Step 2064900 | Avg Loss: 0.0153 | Grad Norm: 0.00879234\n",
      "Epoch 3 | Step 2065000 | Avg Loss: 0.0153 | Grad Norm: 0.00845959\n",
      "Epoch 3 | Step 2065100 | Avg Loss: 0.0148 | Grad Norm: 0.00836460\n",
      "Epoch 3 | Step 2065200 | Avg Loss: 0.0147 | Grad Norm: 0.00847868\n",
      "Epoch 3 | Step 2065300 | Avg Loss: 0.0145 | Grad Norm: 0.00837541\n",
      "Epoch 3 | Step 2065400 | Avg Loss: 0.0148 | Grad Norm: 0.00855394\n",
      "Epoch 3 | Step 2065500 | Avg Loss: 0.0149 | Grad Norm: 0.00877636\n",
      "Epoch 3 | Step 2065600 | Avg Loss: 0.0150 | Grad Norm: 0.00897143\n",
      "Epoch 3 | Step 2065700 | Avg Loss: 0.0151 | Grad Norm: 0.00874822\n",
      "Epoch 3 | Step 2065800 | Avg Loss: 0.0155 | Grad Norm: 0.00789778\n",
      "Epoch 3 | Step 2065900 | Avg Loss: 0.0150 | Grad Norm: 0.00986227\n",
      "Epoch 3 | Step 2066000 | Avg Loss: 0.0153 | Grad Norm: 0.00938363\n",
      "Epoch 3 | Step 2066100 | Avg Loss: 0.0158 | Grad Norm: 0.00968695\n",
      "Epoch 3 | Step 2066200 | Avg Loss: 0.0159 | Grad Norm: 0.01018586\n",
      "Epoch 3 | Step 2066300 | Avg Loss: 0.0159 | Grad Norm: 0.00866131\n",
      "Epoch 3 | Step 2066400 | Avg Loss: 0.0154 | Grad Norm: 0.00836971\n",
      "Epoch 3 | Step 2066500 | Avg Loss: 0.0157 | Grad Norm: 0.00813503\n",
      "Epoch 3 | Step 2066600 | Avg Loss: 0.0157 | Grad Norm: 0.00984054\n",
      "Epoch 3 | Step 2066700 | Avg Loss: 0.0156 | Grad Norm: 0.00905191\n",
      "Epoch 3 | Step 2066800 | Avg Loss: 0.0157 | Grad Norm: 0.00805266\n",
      "Epoch 3 | Step 2066900 | Avg Loss: 0.0155 | Grad Norm: 0.00882326\n",
      "Epoch 3 | Step 2067000 | Avg Loss: 0.0156 | Grad Norm: 0.00967206\n",
      "Epoch 3 | Step 2067100 | Avg Loss: 0.0158 | Grad Norm: 0.00934889\n",
      "Epoch 3 | Step 2067200 | Avg Loss: 0.0158 | Grad Norm: 0.00896875\n",
      "Epoch 3 | Step 2067300 | Avg Loss: 0.0163 | Grad Norm: 0.00915609\n",
      "Epoch 3 | Step 2067400 | Avg Loss: 0.0162 | Grad Norm: 0.01017105\n",
      "Epoch 3 | Step 2067500 | Avg Loss: 0.0157 | Grad Norm: 0.00930624\n",
      "Epoch 3 | Step 2067600 | Avg Loss: 0.0152 | Grad Norm: 0.00875961\n",
      "Epoch 3 | Step 2067700 | Avg Loss: 0.0153 | Grad Norm: 0.00883678\n",
      "Epoch 3 | Step 2067800 | Avg Loss: 0.0156 | Grad Norm: 0.00885859\n",
      "Epoch 3 | Step 2067900 | Avg Loss: 0.0155 | Grad Norm: 0.00902562\n",
      "Epoch 3 | Step 2068000 | Avg Loss: 0.0159 | Grad Norm: 0.00887470\n",
      "Epoch 3 | Step 2068100 | Avg Loss: 0.0159 | Grad Norm: 0.00927213\n",
      "Epoch 3 | Step 2068200 | Avg Loss: 0.0164 | Grad Norm: 0.00880744\n",
      "Epoch 3 | Step 2068300 | Avg Loss: 0.0164 | Grad Norm: 0.01091402\n",
      "Epoch 3 | Step 2068400 | Avg Loss: 0.0163 | Grad Norm: 0.00846149\n",
      "Epoch 3 | Step 2068500 | Avg Loss: 0.0164 | Grad Norm: 0.01019251\n",
      "Epoch 3 | Step 2068600 | Avg Loss: 0.0164 | Grad Norm: 0.01034593\n",
      "Epoch 3 | Step 2068700 | Avg Loss: 0.0162 | Grad Norm: 0.01100856\n",
      "Epoch 3 | Step 2068800 | Avg Loss: 0.0159 | Grad Norm: 0.00899680\n",
      "Epoch 3 | Step 2068900 | Avg Loss: 0.0158 | Grad Norm: 0.01071799\n",
      "Epoch 3 | Step 2069000 | Avg Loss: 0.0156 | Grad Norm: 0.00904194\n",
      "Epoch 3 | Step 2069100 | Avg Loss: 0.0153 | Grad Norm: 0.00931076\n",
      "Epoch 3 | Step 2069200 | Avg Loss: 0.0157 | Grad Norm: 0.00859549\n",
      "Epoch 3 | Step 2069300 | Avg Loss: 0.0160 | Grad Norm: 0.01206867\n",
      "Epoch 3 | Step 2069400 | Avg Loss: 0.0160 | Grad Norm: 0.00916226\n",
      "Epoch 3 | Step 2069500 | Avg Loss: 0.0163 | Grad Norm: 0.00909840\n",
      "Epoch 3 | Step 2069600 | Avg Loss: 0.0162 | Grad Norm: 0.01243577\n",
      "Epoch 3 | Step 2069700 | Avg Loss: 0.0155 | Grad Norm: 0.00732436\n",
      "Epoch 3 | Step 2069800 | Avg Loss: 0.0156 | Grad Norm: 0.00827316\n",
      "Epoch 3 | Step 2069900 | Avg Loss: 0.0152 | Grad Norm: 0.00863058\n",
      "Epoch 3 | Step 2070000 | Avg Loss: 0.0158 | Grad Norm: 0.00922775\n",
      "Epoch 3 | Step 2070100 | Avg Loss: 0.0159 | Grad Norm: 0.00887540\n",
      "Epoch 3 | Step 2070200 | Avg Loss: 0.0158 | Grad Norm: 0.00867936\n",
      "Epoch 3 | Step 2070300 | Avg Loss: 0.0162 | Grad Norm: 0.00861121\n",
      "Epoch 3 | Step 2070400 | Avg Loss: 0.0160 | Grad Norm: 0.00853889\n",
      "Epoch 3 | Step 2070500 | Avg Loss: 0.0160 | Grad Norm: 0.00910023\n",
      "Epoch 3 | Step 2070600 | Avg Loss: 0.0157 | Grad Norm: 0.00868273\n",
      "Epoch 3 | Step 2070700 | Avg Loss: 0.0157 | Grad Norm: 0.00852518\n",
      "Epoch 3 | Step 2070800 | Avg Loss: 0.0156 | Grad Norm: 0.00915889\n",
      "Epoch 3 | Step 2070900 | Avg Loss: 0.0158 | Grad Norm: 0.00877408\n",
      "Epoch 3 | Step 2071000 | Avg Loss: 0.0159 | Grad Norm: 0.00861095\n",
      "Epoch 3 | Step 2071100 | Avg Loss: 0.0156 | Grad Norm: 0.00928528\n",
      "Epoch 3 | Step 2071200 | Avg Loss: 0.0156 | Grad Norm: 0.00956245\n",
      "Epoch 3 | Step 2071300 | Avg Loss: 0.0156 | Grad Norm: 0.00846174\n",
      "Epoch 3 | Step 2071400 | Avg Loss: 0.0155 | Grad Norm: 0.01305274\n",
      "Epoch 3 | Step 2071500 | Avg Loss: 0.0153 | Grad Norm: 0.01027870\n",
      "Epoch 3 | Step 2071600 | Avg Loss: 0.0152 | Grad Norm: 0.01030683\n",
      "Epoch 3 | Step 2071700 | Avg Loss: 0.0150 | Grad Norm: 0.00845095\n",
      "Epoch 3 | Step 2071800 | Avg Loss: 0.0151 | Grad Norm: 0.01000675\n",
      "Epoch 3 | Step 2071900 | Avg Loss: 0.0151 | Grad Norm: 0.00891063\n",
      "Epoch 3 | Step 2072000 | Avg Loss: 0.0149 | Grad Norm: 0.00879866\n",
      "Epoch 3 | Step 2072100 | Avg Loss: 0.0146 | Grad Norm: 0.00838589\n",
      "Epoch 3 | Step 2072200 | Avg Loss: 0.0146 | Grad Norm: 0.00788367\n",
      "Epoch 3 | Step 2072300 | Avg Loss: 0.0147 | Grad Norm: 0.00937157\n",
      "Epoch 3 | Step 2072400 | Avg Loss: 0.0147 | Grad Norm: 0.00944375\n",
      "Epoch 3 | Step 2072500 | Avg Loss: 0.0143 | Grad Norm: 0.00821763\n",
      "Epoch 3 | Step 2072600 | Avg Loss: 0.0142 | Grad Norm: 0.00859871\n",
      "Epoch 3 | Step 2072700 | Avg Loss: 0.0147 | Grad Norm: 0.00961767\n",
      "Epoch 3 | Step 2072800 | Avg Loss: 0.0151 | Grad Norm: 0.00928056\n",
      "Epoch 3 | Step 2072900 | Avg Loss: 0.0154 | Grad Norm: 0.00855986\n",
      "Epoch 3 | Step 2073000 | Avg Loss: 0.0153 | Grad Norm: 0.00913479\n",
      "Epoch 3 | Step 2073100 | Avg Loss: 0.0157 | Grad Norm: 0.00850083\n",
      "Epoch 3 | Step 2073200 | Avg Loss: 0.0158 | Grad Norm: 0.00861072\n",
      "Epoch 3 | Step 2073300 | Avg Loss: 0.0156 | Grad Norm: 0.01172503\n",
      "Epoch 3 | Step 2073400 | Avg Loss: 0.0158 | Grad Norm: 0.00837506\n",
      "Epoch 3 | Step 2073500 | Avg Loss: 0.0158 | Grad Norm: 0.01047723\n",
      "Epoch 3 | Step 2073600 | Avg Loss: 0.0161 | Grad Norm: 0.00868654\n",
      "Epoch 3 | Step 2073700 | Avg Loss: 0.0160 | Grad Norm: 0.00867207\n",
      "Epoch 3 | Step 2073800 | Avg Loss: 0.0159 | Grad Norm: 0.00810869\n",
      "Epoch 3 | Step 2073900 | Avg Loss: 0.0160 | Grad Norm: 0.00840788\n",
      "Epoch 3 | Step 2074000 | Avg Loss: 0.0160 | Grad Norm: 0.00811412\n",
      "Epoch 3 | Step 2074100 | Avg Loss: 0.0159 | Grad Norm: 0.00990056\n",
      "Epoch 3 | Step 2074200 | Avg Loss: 0.0159 | Grad Norm: 0.01038149\n",
      "Epoch 3 | Step 2074300 | Avg Loss: 0.0155 | Grad Norm: 0.00908782\n",
      "Epoch 3 | Step 2074400 | Avg Loss: 0.0157 | Grad Norm: 0.00847011\n",
      "Epoch 3 | Step 2074500 | Avg Loss: 0.0157 | Grad Norm: 0.01000807\n",
      "Epoch 3 | Step 2074600 | Avg Loss: 0.0156 | Grad Norm: 0.00829064\n",
      "Epoch 3 | Step 2074700 | Avg Loss: 0.0155 | Grad Norm: 0.00870141\n",
      "Epoch 3 | Step 2074800 | Avg Loss: 0.0159 | Grad Norm: 0.00996918\n",
      "Epoch 3 | Step 2074900 | Avg Loss: 0.0164 | Grad Norm: 0.01019424\n",
      "Epoch 3 | Step 2075000 | Avg Loss: 0.0161 | Grad Norm: 0.00931571\n",
      "Epoch 3 | Step 2075100 | Avg Loss: 0.0160 | Grad Norm: 0.00866564\n",
      "Epoch 3 | Step 2075200 | Avg Loss: 0.0157 | Grad Norm: 0.00949126\n",
      "Epoch 3 | Step 2075300 | Avg Loss: 0.0154 | Grad Norm: 0.00866092\n",
      "Epoch 3 | Step 2075400 | Avg Loss: 0.0154 | Grad Norm: 0.00853525\n",
      "Epoch 3 | Step 2075500 | Avg Loss: 0.0157 | Grad Norm: 0.00937516\n",
      "Epoch 3 | Step 2075600 | Avg Loss: 0.0158 | Grad Norm: 0.00906140\n",
      "Epoch 3 | Step 2075700 | Avg Loss: 0.0155 | Grad Norm: 0.00832186\n",
      "Epoch 3 | Step 2075800 | Avg Loss: 0.0155 | Grad Norm: 0.00972197\n",
      "Epoch 3 | Step 2075900 | Avg Loss: 0.0150 | Grad Norm: 0.00808368\n",
      "Epoch 3 | Step 2076000 | Avg Loss: 0.0151 | Grad Norm: 0.00846263\n",
      "Epoch 3 | Step 2076100 | Avg Loss: 0.0152 | Grad Norm: 0.00838906\n",
      "Epoch 3 | Step 2076200 | Avg Loss: 0.0153 | Grad Norm: 0.00921235\n",
      "Epoch 3 | Step 2076300 | Avg Loss: 0.0156 | Grad Norm: 0.01416379\n",
      "Epoch 3 | Step 2076400 | Avg Loss: 0.0160 | Grad Norm: 0.01018638\n",
      "Epoch 3 | Step 2076500 | Avg Loss: 0.0159 | Grad Norm: 0.00896265\n",
      "Epoch 3 | Step 2076600 | Avg Loss: 0.0155 | Grad Norm: 0.00904639\n",
      "Epoch 3 | Step 2076700 | Avg Loss: 0.0157 | Grad Norm: 0.00931816\n",
      "Epoch 3 | Step 2076800 | Avg Loss: 0.0158 | Grad Norm: 0.00984921\n",
      "Epoch 3 | Step 2076900 | Avg Loss: 0.0163 | Grad Norm: 0.01018350\n",
      "Epoch 3 | Step 2077000 | Avg Loss: 0.0161 | Grad Norm: 0.00897980\n",
      "Epoch 3 | Step 2077100 | Avg Loss: 0.0160 | Grad Norm: 0.00949456\n",
      "Epoch 3 | Step 2077200 | Avg Loss: 0.0160 | Grad Norm: 0.00946102\n",
      "Epoch 3 | Step 2077300 | Avg Loss: 0.0162 | Grad Norm: 0.01013876\n",
      "Epoch 3 | Step 2077400 | Avg Loss: 0.0162 | Grad Norm: 0.00872247\n",
      "Epoch 3 | Step 2077500 | Avg Loss: 0.0162 | Grad Norm: 0.00922603\n",
      "Epoch 3 | Step 2077600 | Avg Loss: 0.0160 | Grad Norm: 0.01000074\n",
      "Epoch 3 | Step 2077700 | Avg Loss: 0.0161 | Grad Norm: 0.00965632\n",
      "Epoch 3 | Step 2077800 | Avg Loss: 0.0160 | Grad Norm: 0.00956694\n",
      "Epoch 3 | Step 2077900 | Avg Loss: 0.0159 | Grad Norm: 0.00894838\n",
      "Epoch 3 | Step 2078000 | Avg Loss: 0.0159 | Grad Norm: 0.00879645\n",
      "Epoch 3 | Step 2078100 | Avg Loss: 0.0162 | Grad Norm: 0.00992985\n",
      "Epoch 3 | Step 2078200 | Avg Loss: 0.0161 | Grad Norm: 0.00795477\n",
      "Epoch 3 | Step 2078300 | Avg Loss: 0.0159 | Grad Norm: 0.00913933\n",
      "Epoch 3 | Step 2078400 | Avg Loss: 0.0162 | Grad Norm: 0.00871522\n",
      "Epoch 3 | Step 2078500 | Avg Loss: 0.0158 | Grad Norm: 0.00895614\n",
      "Epoch 3 | Step 2078600 | Avg Loss: 0.0157 | Grad Norm: 0.00831107\n",
      "Epoch 3 | Step 2078700 | Avg Loss: 0.0158 | Grad Norm: 0.00808953\n",
      "Epoch 3 | Step 2078800 | Avg Loss: 0.0160 | Grad Norm: 0.00855454\n",
      "Epoch 3 | Step 2078900 | Avg Loss: 0.0158 | Grad Norm: 0.00961903\n",
      "Epoch 3 | Step 2079000 | Avg Loss: 0.0158 | Grad Norm: 0.00925459\n",
      "Epoch 3 | Step 2079100 | Avg Loss: 0.0158 | Grad Norm: 0.00873860\n",
      "Epoch 3 | Step 2079200 | Avg Loss: 0.0162 | Grad Norm: 0.00850671\n",
      "Epoch 3 | Step 2079300 | Avg Loss: 0.0158 | Grad Norm: 0.00864060\n",
      "Epoch 3 | Step 2079400 | Avg Loss: 0.0158 | Grad Norm: 0.00856519\n",
      "Epoch 3 | Step 2079500 | Avg Loss: 0.0157 | Grad Norm: 0.01082773\n",
      "Epoch 3 | Step 2079600 | Avg Loss: 0.0157 | Grad Norm: 0.00899181\n",
      "Epoch 3 | Step 2079700 | Avg Loss: 0.0153 | Grad Norm: 0.00853222\n",
      "Epoch 3 | Step 2079800 | Avg Loss: 0.0154 | Grad Norm: 0.00907156\n",
      "Epoch 3 | Step 2079900 | Avg Loss: 0.0152 | Grad Norm: 0.01030304\n",
      "Epoch 3 | Step 2080000 | Avg Loss: 0.0151 | Grad Norm: 0.00812575\n",
      "Epoch 3 | Step 2080100 | Avg Loss: 0.0154 | Grad Norm: 0.00956648\n",
      "Epoch 3 | Step 2080200 | Avg Loss: 0.0156 | Grad Norm: 0.00978540\n",
      "Epoch 3 | Step 2080300 | Avg Loss: 0.0153 | Grad Norm: 0.00909813\n",
      "Epoch 3 | Step 2080400 | Avg Loss: 0.0151 | Grad Norm: 0.00904969\n",
      "Epoch 3 | Step 2080500 | Avg Loss: 0.0152 | Grad Norm: 0.00853019\n",
      "Epoch 3 | Step 2080600 | Avg Loss: 0.0152 | Grad Norm: 0.00857702\n",
      "Epoch 3 | Step 2080700 | Avg Loss: 0.0156 | Grad Norm: 0.00841164\n",
      "Epoch 3 | Step 2080800 | Avg Loss: 0.0156 | Grad Norm: 0.01049473\n",
      "Epoch 3 | Step 2080900 | Avg Loss: 0.0159 | Grad Norm: 0.01129487\n",
      "Epoch 3 | Step 2081000 | Avg Loss: 0.0158 | Grad Norm: 0.00815730\n",
      "Epoch 3 | Step 2081100 | Avg Loss: 0.0155 | Grad Norm: 0.00914885\n",
      "Epoch 3 | Step 2081200 | Avg Loss: 0.0156 | Grad Norm: 0.00823173\n",
      "Epoch 3 | Step 2081300 | Avg Loss: 0.0153 | Grad Norm: 0.00866605\n",
      "Epoch 3 | Step 2081400 | Avg Loss: 0.0153 | Grad Norm: 0.01179079\n",
      "Epoch 3 | Step 2081500 | Avg Loss: 0.0154 | Grad Norm: 0.00893945\n",
      "Epoch 3 | Step 2081600 | Avg Loss: 0.0155 | Grad Norm: 0.00960801\n",
      "Epoch 3 | Step 2081700 | Avg Loss: 0.0154 | Grad Norm: 0.01034544\n",
      "Epoch 3 | Step 2081800 | Avg Loss: 0.0151 | Grad Norm: 0.00821665\n",
      "Epoch 3 | Step 2081900 | Avg Loss: 0.0151 | Grad Norm: 0.00870405\n",
      "Epoch 3 | Step 2082000 | Avg Loss: 0.0152 | Grad Norm: 0.00972411\n",
      "Epoch 3 | Step 2082100 | Avg Loss: 0.0149 | Grad Norm: 0.00854977\n",
      "Epoch 3 | Step 2082200 | Avg Loss: 0.0149 | Grad Norm: 0.00784683\n",
      "Epoch 3 | Step 2082300 | Avg Loss: 0.0150 | Grad Norm: 0.00830235\n",
      "Epoch 3 | Step 2082400 | Avg Loss: 0.0153 | Grad Norm: 0.00871500\n",
      "Epoch 3 | Step 2082500 | Avg Loss: 0.0156 | Grad Norm: 0.00725919\n",
      "Epoch 3 | Step 2082600 | Avg Loss: 0.0158 | Grad Norm: 0.00934163\n",
      "Epoch 3 | Step 2082700 | Avg Loss: 0.0157 | Grad Norm: 0.01141796\n",
      "Epoch 3 | Step 2082800 | Avg Loss: 0.0157 | Grad Norm: 0.00965819\n",
      "Epoch 3 | Step 2082900 | Avg Loss: 0.0154 | Grad Norm: 0.01127706\n",
      "Epoch 3 | Step 2083000 | Avg Loss: 0.0156 | Grad Norm: 0.00832156\n",
      "Epoch 3 | Step 2083100 | Avg Loss: 0.0157 | Grad Norm: 0.00849126\n",
      "Epoch 3 | Step 2083200 | Avg Loss: 0.0159 | Grad Norm: 0.01004311\n",
      "Epoch 3 | Step 2083300 | Avg Loss: 0.0162 | Grad Norm: 0.01026918\n",
      "Epoch 3 | Step 2083400 | Avg Loss: 0.0157 | Grad Norm: 0.00976583\n",
      "Epoch 3 | Step 2083500 | Avg Loss: 0.0154 | Grad Norm: 0.01006535\n",
      "Epoch 3 | Step 2083600 | Avg Loss: 0.0151 | Grad Norm: 0.00909390\n",
      "Epoch 3 | Step 2083700 | Avg Loss: 0.0148 | Grad Norm: 0.00959741\n",
      "Epoch 3 | Step 2083800 | Avg Loss: 0.0153 | Grad Norm: 0.00952334\n",
      "Epoch 3 | Step 2083900 | Avg Loss: 0.0151 | Grad Norm: 0.00803685\n",
      "Epoch 3 | Step 2084000 | Avg Loss: 0.0152 | Grad Norm: 0.00962967\n",
      "Epoch 3 | Step 2084100 | Avg Loss: 0.0156 | Grad Norm: 0.00781476\n",
      "Epoch 3 | Step 2084200 | Avg Loss: 0.0157 | Grad Norm: 0.00903442\n",
      "Epoch 3 | Step 2084300 | Avg Loss: 0.0157 | Grad Norm: 0.00942921\n",
      "Epoch 3 | Step 2084400 | Avg Loss: 0.0156 | Grad Norm: 0.00834076\n",
      "Epoch 3 | Step 2084500 | Avg Loss: 0.0159 | Grad Norm: 0.01000258\n",
      "Epoch 3 | Step 2084600 | Avg Loss: 0.0157 | Grad Norm: 0.00922972\n",
      "Epoch 3 | Step 2084700 | Avg Loss: 0.0151 | Grad Norm: 0.00708778\n",
      "Epoch 3 | Step 2084800 | Avg Loss: 0.0151 | Grad Norm: 0.00876185\n",
      "Epoch 3 | Step 2084900 | Avg Loss: 0.0152 | Grad Norm: 0.00970290\n",
      "Epoch 3 | Step 2085000 | Avg Loss: 0.0158 | Grad Norm: 0.00963635\n",
      "Epoch 3 | Step 2085100 | Avg Loss: 0.0154 | Grad Norm: 0.00870923\n",
      "Epoch 3 | Step 2085200 | Avg Loss: 0.0154 | Grad Norm: 0.00835456\n",
      "Epoch 3 | Step 2085300 | Avg Loss: 0.0154 | Grad Norm: 0.01038936\n",
      "Epoch 3 | Step 2085400 | Avg Loss: 0.0156 | Grad Norm: 0.01144340\n",
      "Epoch 3 | Step 2085500 | Avg Loss: 0.0158 | Grad Norm: 0.01078866\n",
      "Epoch 3 | Step 2085600 | Avg Loss: 0.0156 | Grad Norm: 0.00985984\n",
      "Epoch 3 | Step 2085700 | Avg Loss: 0.0158 | Grad Norm: 0.01170718\n",
      "Epoch 3 | Step 2085800 | Avg Loss: 0.0156 | Grad Norm: 0.00872418\n",
      "Epoch 3 | Step 2085900 | Avg Loss: 0.0159 | Grad Norm: 0.00876853\n",
      "Epoch 3 | Step 2086000 | Avg Loss: 0.0158 | Grad Norm: 0.00812440\n",
      "Epoch 3 | Step 2086100 | Avg Loss: 0.0159 | Grad Norm: 0.01016997\n",
      "Epoch 3 | Step 2086200 | Avg Loss: 0.0157 | Grad Norm: 0.00858922\n",
      "Epoch 3 | Step 2086300 | Avg Loss: 0.0153 | Grad Norm: 0.01054034\n",
      "Epoch 3 | Step 2086400 | Avg Loss: 0.0151 | Grad Norm: 0.00977032\n",
      "Epoch 3 | Step 2086500 | Avg Loss: 0.0151 | Grad Norm: 0.00837129\n",
      "Epoch 3 | Step 2086600 | Avg Loss: 0.0151 | Grad Norm: 0.01039106\n",
      "Epoch 3 | Step 2086700 | Avg Loss: 0.0152 | Grad Norm: 0.00860822\n",
      "Epoch 3 | Step 2086800 | Avg Loss: 0.0154 | Grad Norm: 0.00793072\n",
      "Epoch 3 | Step 2086900 | Avg Loss: 0.0153 | Grad Norm: 0.00928913\n",
      "Epoch 3 | Step 2087000 | Avg Loss: 0.0152 | Grad Norm: 0.00871804\n",
      "Epoch 3 | Step 2087100 | Avg Loss: 0.0153 | Grad Norm: 0.00840345\n",
      "Epoch 3 | Step 2087200 | Avg Loss: 0.0153 | Grad Norm: 0.01125125\n",
      "Epoch 3 | Step 2087300 | Avg Loss: 0.0155 | Grad Norm: 0.01043365\n",
      "Epoch 3 | Step 2087400 | Avg Loss: 0.0155 | Grad Norm: 0.00870955\n",
      "Epoch 3 | Step 2087500 | Avg Loss: 0.0155 | Grad Norm: 0.00909365\n",
      "Epoch 3 | Step 2087600 | Avg Loss: 0.0156 | Grad Norm: 0.01012485\n",
      "Epoch 3 | Step 2087700 | Avg Loss: 0.0157 | Grad Norm: 0.00901713\n",
      "Epoch 3 | Step 2087800 | Avg Loss: 0.0157 | Grad Norm: 0.00852952\n",
      "Epoch 3 | Step 2087900 | Avg Loss: 0.0158 | Grad Norm: 0.00897251\n",
      "Epoch 3 | Step 2088000 | Avg Loss: 0.0157 | Grad Norm: 0.00872761\n",
      "Epoch 3 | Step 2088100 | Avg Loss: 0.0161 | Grad Norm: 0.00855443\n",
      "Epoch 3 | Step 2088200 | Avg Loss: 0.0158 | Grad Norm: 0.00948767\n",
      "Epoch 3 | Step 2088300 | Avg Loss: 0.0160 | Grad Norm: 0.00776044\n",
      "Epoch 3 | Step 2088400 | Avg Loss: 0.0155 | Grad Norm: 0.00959093\n",
      "Epoch 3 | Step 2088500 | Avg Loss: 0.0154 | Grad Norm: 0.00989316\n",
      "Epoch 3 | Step 2088600 | Avg Loss: 0.0153 | Grad Norm: 0.00768189\n",
      "Epoch 3 | Step 2088700 | Avg Loss: 0.0151 | Grad Norm: 0.00943103\n",
      "Epoch 3 | Step 2088800 | Avg Loss: 0.0150 | Grad Norm: 0.00833442\n",
      "Epoch 3 | Step 2088900 | Avg Loss: 0.0150 | Grad Norm: 0.00892415\n",
      "Epoch 3 | Step 2089000 | Avg Loss: 0.0153 | Grad Norm: 0.00892293\n",
      "Epoch 3 | Step 2089100 | Avg Loss: 0.0153 | Grad Norm: 0.00829026\n",
      "Epoch 3 | Step 2089200 | Avg Loss: 0.0151 | Grad Norm: 0.00817639\n",
      "Epoch 3 | Step 2089300 | Avg Loss: 0.0151 | Grad Norm: 0.00909270\n",
      "Epoch 3 | Step 2089400 | Avg Loss: 0.0152 | Grad Norm: 0.00943116\n",
      "Epoch 3 | Step 2089500 | Avg Loss: 0.0156 | Grad Norm: 0.00869353\n",
      "Epoch 3 | Step 2089600 | Avg Loss: 0.0155 | Grad Norm: 0.00912831\n",
      "Epoch 3 | Step 2089700 | Avg Loss: 0.0157 | Grad Norm: 0.00912716\n",
      "Epoch 3 | Step 2089800 | Avg Loss: 0.0159 | Grad Norm: 0.00969361\n",
      "Epoch 3 | Step 2089900 | Avg Loss: 0.0165 | Grad Norm: 0.00953757\n",
      "Epoch 3 | Step 2090000 | Avg Loss: 0.0162 | Grad Norm: 0.00956479\n",
      "Epoch 3 | Step 2090100 | Avg Loss: 0.0162 | Grad Norm: 0.01154336\n",
      "Epoch 3 | Step 2090200 | Avg Loss: 0.0159 | Grad Norm: 0.00980751\n",
      "Epoch 3 | Step 2090300 | Avg Loss: 0.0161 | Grad Norm: 0.00901235\n",
      "Epoch 3 | Step 2090400 | Avg Loss: 0.0156 | Grad Norm: 0.00854872\n",
      "Epoch 3 | Step 2090500 | Avg Loss: 0.0154 | Grad Norm: 0.00980791\n",
      "Epoch 3 | Step 2090600 | Avg Loss: 0.0155 | Grad Norm: 0.00969188\n",
      "Epoch 3 | Step 2090700 | Avg Loss: 0.0152 | Grad Norm: 0.00903474\n",
      "Epoch 3 | Step 2090800 | Avg Loss: 0.0150 | Grad Norm: 0.00909104\n",
      "Epoch 3 | Step 2090900 | Avg Loss: 0.0152 | Grad Norm: 0.00940051\n",
      "Epoch 3 | Step 2091000 | Avg Loss: 0.0149 | Grad Norm: 0.00830917\n",
      "Epoch 3 | Step 2091100 | Avg Loss: 0.0154 | Grad Norm: 0.00938387\n",
      "Epoch 3 | Step 2091200 | Avg Loss: 0.0150 | Grad Norm: 0.00927042\n",
      "Epoch 3 | Step 2091300 | Avg Loss: 0.0154 | Grad Norm: 0.00915528\n",
      "Epoch 3 | Step 2091400 | Avg Loss: 0.0153 | Grad Norm: 0.00876488\n",
      "Epoch 3 | Step 2091500 | Avg Loss: 0.0156 | Grad Norm: 0.00824491\n",
      "Epoch 3 | Step 2091600 | Avg Loss: 0.0157 | Grad Norm: 0.00853159\n",
      "Epoch 3 | Step 2091700 | Avg Loss: 0.0158 | Grad Norm: 0.00833005\n",
      "Epoch 3 | Step 2091800 | Avg Loss: 0.0158 | Grad Norm: 0.00988866\n",
      "Epoch 3 | Step 2091900 | Avg Loss: 0.0158 | Grad Norm: 0.01042264\n",
      "Epoch 3 | Step 2092000 | Avg Loss: 0.0157 | Grad Norm: 0.00867192\n",
      "Epoch 3 | Step 2092100 | Avg Loss: 0.0156 | Grad Norm: 0.00965206\n",
      "Epoch 3 | Step 2092200 | Avg Loss: 0.0159 | Grad Norm: 0.00926975\n",
      "Epoch 3 | Step 2092300 | Avg Loss: 0.0160 | Grad Norm: 0.00936655\n",
      "Epoch 3 | Step 2092400 | Avg Loss: 0.0153 | Grad Norm: 0.00897926\n",
      "Epoch 3 | Step 2092500 | Avg Loss: 0.0155 | Grad Norm: 0.00855814\n",
      "Epoch 3 | Step 2092600 | Avg Loss: 0.0158 | Grad Norm: 0.00849835\n",
      "Epoch 3 | Step 2092700 | Avg Loss: 0.0160 | Grad Norm: 0.00777897\n",
      "Epoch 3 | Step 2092800 | Avg Loss: 0.0157 | Grad Norm: 0.00908372\n",
      "Epoch 3 | Step 2092900 | Avg Loss: 0.0154 | Grad Norm: 0.00886497\n",
      "Epoch 3 | Step 2093000 | Avg Loss: 0.0151 | Grad Norm: 0.00741409\n",
      "Epoch 3 | Step 2093100 | Avg Loss: 0.0154 | Grad Norm: 0.00966501\n",
      "Epoch 3 | Step 2093200 | Avg Loss: 0.0155 | Grad Norm: 0.00888173\n",
      "Epoch 3 | Step 2093300 | Avg Loss: 0.0155 | Grad Norm: 0.00830948\n",
      "Epoch 3 | Step 2093400 | Avg Loss: 0.0150 | Grad Norm: 0.00828649\n",
      "Epoch 3 | Step 2093500 | Avg Loss: 0.0152 | Grad Norm: 0.00954353\n",
      "Epoch 3 | Step 2093600 | Avg Loss: 0.0149 | Grad Norm: 0.00822332\n",
      "Epoch 3 | Step 2093700 | Avg Loss: 0.0148 | Grad Norm: 0.00917960\n",
      "Epoch 3 | Step 2093800 | Avg Loss: 0.0154 | Grad Norm: 0.00974306\n",
      "Epoch 3 | Step 2093900 | Avg Loss: 0.0152 | Grad Norm: 0.00817207\n",
      "Epoch 3 | Step 2094000 | Avg Loss: 0.0150 | Grad Norm: 0.00842023\n",
      "Epoch 3 | Step 2094100 | Avg Loss: 0.0151 | Grad Norm: 0.00835303\n",
      "Epoch 3 | Step 2094200 | Avg Loss: 0.0152 | Grad Norm: 0.00859161\n",
      "Epoch 3 | Step 2094300 | Avg Loss: 0.0153 | Grad Norm: 0.00970098\n",
      "Epoch 3 | Step 2094400 | Avg Loss: 0.0155 | Grad Norm: 0.01110059\n",
      "Epoch 3 | Step 2094500 | Avg Loss: 0.0156 | Grad Norm: 0.00811760\n",
      "Epoch 3 | Step 2094600 | Avg Loss: 0.0158 | Grad Norm: 0.00878576\n",
      "Epoch 3 | Step 2094700 | Avg Loss: 0.0158 | Grad Norm: 0.00888256\n",
      "Epoch 3 | Step 2094800 | Avg Loss: 0.0155 | Grad Norm: 0.00890180\n",
      "Epoch 3 | Step 2094900 | Avg Loss: 0.0156 | Grad Norm: 0.00870793\n",
      "Epoch 3 | Step 2095000 | Avg Loss: 0.0156 | Grad Norm: 0.01056321\n",
      "Epoch 3 | Step 2095100 | Avg Loss: 0.0154 | Grad Norm: 0.00902428\n",
      "Epoch 3 | Step 2095200 | Avg Loss: 0.0155 | Grad Norm: 0.00863875\n",
      "Epoch 3 | Step 2095300 | Avg Loss: 0.0153 | Grad Norm: 0.00956185\n",
      "Epoch 3 | Step 2095400 | Avg Loss: 0.0150 | Grad Norm: 0.00926783\n",
      "Epoch 3 | Step 2095500 | Avg Loss: 0.0146 | Grad Norm: 0.01041739\n",
      "Epoch 3 | Step 2095600 | Avg Loss: 0.0147 | Grad Norm: 0.00876688\n",
      "Epoch 3 | Step 2095700 | Avg Loss: 0.0150 | Grad Norm: 0.00826936\n",
      "Epoch 3 | Step 2095800 | Avg Loss: 0.0155 | Grad Norm: 0.00790399\n",
      "Epoch 3 | Step 2095900 | Avg Loss: 0.0156 | Grad Norm: 0.00946532\n",
      "Epoch 3 | Step 2096000 | Avg Loss: 0.0154 | Grad Norm: 0.00990128\n",
      "Epoch 3 | Step 2096100 | Avg Loss: 0.0153 | Grad Norm: 0.01057883\n",
      "Epoch 3 | Step 2096200 | Avg Loss: 0.0151 | Grad Norm: 0.01102304\n",
      "Epoch 3 | Step 2096300 | Avg Loss: 0.0154 | Grad Norm: 0.01060803\n",
      "Epoch 3 | Step 2096400 | Avg Loss: 0.0153 | Grad Norm: 0.00839705\n",
      "Epoch 3 | Step 2096500 | Avg Loss: 0.0153 | Grad Norm: 0.00953732\n",
      "Epoch 3 | Step 2096600 | Avg Loss: 0.0154 | Grad Norm: 0.00939180\n",
      "Epoch 3 | Step 2096700 | Avg Loss: 0.0153 | Grad Norm: 0.00890399\n",
      "Epoch 3 | Step 2096800 | Avg Loss: 0.0152 | Grad Norm: 0.00915933\n",
      "Epoch 3 | Step 2096900 | Avg Loss: 0.0153 | Grad Norm: 0.00836106\n",
      "Epoch 3 | Step 2097000 | Avg Loss: 0.0152 | Grad Norm: 0.00795617\n",
      "Epoch 3 | Step 2097100 | Avg Loss: 0.0154 | Grad Norm: 0.00817060\n",
      "Epoch 3 | Step 2097200 | Avg Loss: 0.0152 | Grad Norm: 0.00794121\n",
      "Epoch 3 | Step 2097300 | Avg Loss: 0.0150 | Grad Norm: 0.00800354\n",
      "Epoch 3 | Step 2097400 | Avg Loss: 0.0150 | Grad Norm: 0.01102916\n",
      "Epoch 3 | Step 2097500 | Avg Loss: 0.0153 | Grad Norm: 0.00848232\n",
      "Epoch 3 | Step 2097600 | Avg Loss: 0.0153 | Grad Norm: 0.01007152\n",
      "Epoch 3 | Step 2097700 | Avg Loss: 0.0157 | Grad Norm: 0.00934143\n",
      "Epoch 3 | Step 2097800 | Avg Loss: 0.0151 | Grad Norm: 0.00823109\n",
      "Epoch 3 | Step 2097900 | Avg Loss: 0.0148 | Grad Norm: 0.00990305\n",
      "Epoch 3 | Step 2098000 | Avg Loss: 0.0151 | Grad Norm: 0.00841196\n",
      "Epoch 3 | Step 2098100 | Avg Loss: 0.0149 | Grad Norm: 0.00995574\n",
      "Epoch 3 | Step 2098200 | Avg Loss: 0.0150 | Grad Norm: 0.00817387\n",
      "Epoch 3 | Step 2098300 | Avg Loss: 0.0152 | Grad Norm: 0.00860985\n",
      "Epoch 3 | Step 2098400 | Avg Loss: 0.0153 | Grad Norm: 0.00918887\n",
      "Epoch 3 | Step 2098500 | Avg Loss: 0.0151 | Grad Norm: 0.00934686\n",
      "Epoch 3 | Step 2098600 | Avg Loss: 0.0151 | Grad Norm: 0.00803265\n",
      "Epoch 3 | Step 2098700 | Avg Loss: 0.0150 | Grad Norm: 0.00885389\n",
      "Epoch 3 | Step 2098800 | Avg Loss: 0.0152 | Grad Norm: 0.01024978\n",
      "Epoch 3 | Step 2098900 | Avg Loss: 0.0150 | Grad Norm: 0.00845111\n",
      "Epoch 3 | Step 2099000 | Avg Loss: 0.0153 | Grad Norm: 0.00836883\n",
      "Epoch 3 | Step 2099100 | Avg Loss: 0.0152 | Grad Norm: 0.00939421\n",
      "Epoch 3 | Step 2099200 | Avg Loss: 0.0151 | Grad Norm: 0.00815863\n",
      "Epoch 3 | Step 2099300 | Avg Loss: 0.0148 | Grad Norm: 0.00742102\n",
      "Epoch 3 | Step 2099400 | Avg Loss: 0.0149 | Grad Norm: 0.00731787\n",
      "Epoch 3 | Step 2099500 | Avg Loss: 0.0145 | Grad Norm: 0.00876286\n",
      "Epoch 3 | Step 2099600 | Avg Loss: 0.0146 | Grad Norm: 0.00875674\n",
      "Epoch 3 | Step 2099700 | Avg Loss: 0.0151 | Grad Norm: 0.00803056\n",
      "Epoch 3 | Step 2099800 | Avg Loss: 0.0150 | Grad Norm: 0.00985517\n",
      "Epoch 3 | Step 2099900 | Avg Loss: 0.0150 | Grad Norm: 0.00860568\n",
      "Epoch 3 | Step 2100000 | Avg Loss: 0.0151 | Grad Norm: 0.01022202\n",
      "Saving model at step2100000\n",
      "Epoch 3 | Step 2100100 | Avg Loss: 0.0152 | Grad Norm: 0.00973003\n",
      "Epoch 3 | Step 2100200 | Avg Loss: 0.0150 | Grad Norm: 0.00890837\n",
      "Epoch 3 | Step 2100300 | Avg Loss: 0.0151 | Grad Norm: 0.00897332\n",
      "Epoch 3 | Step 2100400 | Avg Loss: 0.0152 | Grad Norm: 0.01094502\n",
      "Epoch 3 | Step 2100500 | Avg Loss: 0.0157 | Grad Norm: 0.00867007\n",
      "Epoch 3 | Step 2100600 | Avg Loss: 0.0159 | Grad Norm: 0.00945411\n",
      "Epoch 3 | Step 2100700 | Avg Loss: 0.0158 | Grad Norm: 0.00977935\n",
      "Epoch 3 | Step 2100800 | Avg Loss: 0.0159 | Grad Norm: 0.01286124\n",
      "Epoch 3 | Step 2100900 | Avg Loss: 0.0157 | Grad Norm: 0.00882402\n",
      "Epoch 3 | Step 2101000 | Avg Loss: 0.0158 | Grad Norm: 0.01114804\n",
      "Epoch 3 | Step 2101100 | Avg Loss: 0.0160 | Grad Norm: 0.00881544\n",
      "Epoch 3 | Step 2101200 | Avg Loss: 0.0159 | Grad Norm: 0.00936432\n",
      "Epoch 3 | Step 2101300 | Avg Loss: 0.0160 | Grad Norm: 0.00828342\n",
      "Epoch 3 | Step 2101400 | Avg Loss: 0.0161 | Grad Norm: 0.00980971\n",
      "Epoch 3 | Step 2101500 | Avg Loss: 0.0162 | Grad Norm: 0.00813657\n",
      "Epoch 3 | Step 2101600 | Avg Loss: 0.0163 | Grad Norm: 0.00920194\n",
      "Epoch 3 | Step 2101700 | Avg Loss: 0.0161 | Grad Norm: 0.00853521\n",
      "Epoch 3 | Step 2101800 | Avg Loss: 0.0161 | Grad Norm: 0.00924153\n",
      "Epoch 3 | Step 2101900 | Avg Loss: 0.0161 | Grad Norm: 0.00858744\n",
      "Epoch 3 | Step 2102000 | Avg Loss: 0.0155 | Grad Norm: 0.00858104\n",
      "Epoch 3 | Step 2102100 | Avg Loss: 0.0157 | Grad Norm: 0.00956310\n",
      "Epoch 3 | Step 2102200 | Avg Loss: 0.0153 | Grad Norm: 0.00994887\n",
      "Epoch 3 | Step 2102300 | Avg Loss: 0.0153 | Grad Norm: 0.00880118\n",
      "Epoch 3 | Step 2102400 | Avg Loss: 0.0153 | Grad Norm: 0.00957859\n",
      "Epoch 3 | Step 2102500 | Avg Loss: 0.0156 | Grad Norm: 0.00925387\n",
      "Epoch 3 | Step 2102600 | Avg Loss: 0.0160 | Grad Norm: 0.00932224\n",
      "Epoch 3 | Step 2102700 | Avg Loss: 0.0162 | Grad Norm: 0.00981311\n",
      "Epoch 3 | Step 2102800 | Avg Loss: 0.0162 | Grad Norm: 0.00926572\n",
      "Epoch 3 | Step 2102900 | Avg Loss: 0.0162 | Grad Norm: 0.00986764\n",
      "Epoch 3 | Step 2103000 | Avg Loss: 0.0160 | Grad Norm: 0.00942270\n",
      "Epoch 3 | Step 2103100 | Avg Loss: 0.0163 | Grad Norm: 0.01004711\n",
      "Epoch 3 | Step 2103200 | Avg Loss: 0.0164 | Grad Norm: 0.00923186\n",
      "Epoch 3 | Step 2103300 | Avg Loss: 0.0164 | Grad Norm: 0.00904215\n",
      "Epoch 3 | Step 2103400 | Avg Loss: 0.0165 | Grad Norm: 0.00986242\n",
      "Epoch 3 | Step 2103500 | Avg Loss: 0.0163 | Grad Norm: 0.00995645\n",
      "Epoch 3 | Step 2103600 | Avg Loss: 0.0161 | Grad Norm: 0.01095958\n",
      "Epoch 3 | Step 2103700 | Avg Loss: 0.0160 | Grad Norm: 0.00970853\n",
      "Epoch 3 | Step 2103800 | Avg Loss: 0.0161 | Grad Norm: 0.00921965\n",
      "Epoch 3 | Step 2103900 | Avg Loss: 0.0161 | Grad Norm: 0.00918837\n",
      "Epoch 3 | Step 2104000 | Avg Loss: 0.0157 | Grad Norm: 0.00958773\n",
      "Epoch 3 | Step 2104100 | Avg Loss: 0.0159 | Grad Norm: 0.00892898\n",
      "Epoch 3 | Step 2104200 | Avg Loss: 0.0160 | Grad Norm: 0.00912523\n",
      "Epoch 3 | Step 2104300 | Avg Loss: 0.0158 | Grad Norm: 0.01178770\n",
      "Epoch 3 | Step 2104400 | Avg Loss: 0.0156 | Grad Norm: 0.00932903\n",
      "Epoch 3 | Step 2104500 | Avg Loss: 0.0157 | Grad Norm: 0.00874126\n",
      "Epoch 3 | Step 2104600 | Avg Loss: 0.0156 | Grad Norm: 0.00977387\n",
      "Epoch 3 | Step 2104700 | Avg Loss: 0.0158 | Grad Norm: 0.00837199\n",
      "Epoch 3 | Step 2104800 | Avg Loss: 0.0156 | Grad Norm: 0.01031167\n",
      "Epoch 3 | Step 2104900 | Avg Loss: 0.0160 | Grad Norm: 0.00995780\n",
      "Epoch 3 | Step 2105000 | Avg Loss: 0.0159 | Grad Norm: 0.01034584\n",
      "Epoch 3 | Step 2105100 | Avg Loss: 0.0158 | Grad Norm: 0.00914343\n",
      "Epoch 3 | Step 2105200 | Avg Loss: 0.0161 | Grad Norm: 0.00832072\n",
      "Epoch 3 | Step 2105300 | Avg Loss: 0.0159 | Grad Norm: 0.00930901\n",
      "Epoch 3 | Step 2105400 | Avg Loss: 0.0160 | Grad Norm: 0.00771803\n",
      "Epoch 3 | Step 2105500 | Avg Loss: 0.0158 | Grad Norm: 0.00836557\n",
      "Epoch 3 | Step 2105600 | Avg Loss: 0.0159 | Grad Norm: 0.00852648\n",
      "Epoch 3 | Step 2105700 | Avg Loss: 0.0161 | Grad Norm: 0.01222713\n",
      "Epoch 3 | Step 2105800 | Avg Loss: 0.0163 | Grad Norm: 0.00894583\n",
      "Epoch 3 | Step 2105900 | Avg Loss: 0.0162 | Grad Norm: 0.00905044\n",
      "Epoch 3 | Step 2106000 | Avg Loss: 0.0160 | Grad Norm: 0.00908615\n",
      "Epoch 3 | Step 2106100 | Avg Loss: 0.0157 | Grad Norm: 0.00999350\n",
      "Epoch 3 | Step 2106200 | Avg Loss: 0.0154 | Grad Norm: 0.01116724\n",
      "Epoch 3 | Step 2106300 | Avg Loss: 0.0154 | Grad Norm: 0.00904820\n",
      "Epoch 3 | Step 2106400 | Avg Loss: 0.0155 | Grad Norm: 0.00815484\n",
      "Epoch 3 | Step 2106500 | Avg Loss: 0.0157 | Grad Norm: 0.00919751\n",
      "Epoch 3 | Step 2106600 | Avg Loss: 0.0155 | Grad Norm: 0.00869203\n",
      "Epoch 3 | Step 2106700 | Avg Loss: 0.0156 | Grad Norm: 0.00957823\n",
      "Epoch 3 | Step 2106800 | Avg Loss: 0.0159 | Grad Norm: 0.00968946\n",
      "Epoch 3 | Step 2106900 | Avg Loss: 0.0159 | Grad Norm: 0.01119913\n",
      "Epoch 3 | Step 2107000 | Avg Loss: 0.0156 | Grad Norm: 0.00879226\n",
      "Epoch 3 | Step 2107100 | Avg Loss: 0.0156 | Grad Norm: 0.00985943\n",
      "Epoch 3 | Step 2107200 | Avg Loss: 0.0157 | Grad Norm: 0.00854555\n",
      "Epoch 3 | Step 2107300 | Avg Loss: 0.0156 | Grad Norm: 0.00808041\n",
      "Epoch 3 | Step 2107400 | Avg Loss: 0.0154 | Grad Norm: 0.01032954\n",
      "Epoch 3 | Step 2107500 | Avg Loss: 0.0154 | Grad Norm: 0.00820468\n",
      "Epoch 3 | Step 2107600 | Avg Loss: 0.0157 | Grad Norm: 0.00894130\n",
      "Epoch 3 | Step 2107700 | Avg Loss: 0.0158 | Grad Norm: 0.00833845\n",
      "Epoch 3 | Step 2107800 | Avg Loss: 0.0161 | Grad Norm: 0.00882084\n",
      "Epoch 3 | Step 2107900 | Avg Loss: 0.0159 | Grad Norm: 0.01037816\n",
      "Epoch 3 | Step 2108000 | Avg Loss: 0.0158 | Grad Norm: 0.00797130\n",
      "Epoch 3 | Step 2108100 | Avg Loss: 0.0152 | Grad Norm: 0.00988487\n",
      "Epoch 3 | Step 2108200 | Avg Loss: 0.0151 | Grad Norm: 0.00809838\n",
      "Epoch 3 | Step 2108300 | Avg Loss: 0.0147 | Grad Norm: 0.00810940\n",
      "Epoch 3 | Step 2108400 | Avg Loss: 0.0150 | Grad Norm: 0.00782811\n",
      "Epoch 3 | Step 2108500 | Avg Loss: 0.0154 | Grad Norm: 0.00855543\n",
      "Epoch 3 | Step 2108600 | Avg Loss: 0.0152 | Grad Norm: 0.01219975\n",
      "Epoch 3 | Step 2108700 | Avg Loss: 0.0152 | Grad Norm: 0.00870233\n",
      "Epoch 3 | Step 2108800 | Avg Loss: 0.0154 | Grad Norm: 0.00843496\n",
      "Epoch 3 | Step 2108900 | Avg Loss: 0.0152 | Grad Norm: 0.00901666\n",
      "Epoch 3 | Step 2109000 | Avg Loss: 0.0150 | Grad Norm: 0.00915672\n",
      "Epoch 3 | Step 2109100 | Avg Loss: 0.0148 | Grad Norm: 0.00999374\n",
      "Epoch 3 | Step 2109200 | Avg Loss: 0.0153 | Grad Norm: 0.00816097\n",
      "Epoch 3 | Step 2109300 | Avg Loss: 0.0154 | Grad Norm: 0.00814521\n",
      "Epoch 3 | Step 2109400 | Avg Loss: 0.0152 | Grad Norm: 0.00932081\n",
      "Epoch 3 | Step 2109500 | Avg Loss: 0.0152 | Grad Norm: 0.01031146\n",
      "Epoch 3 | Step 2109600 | Avg Loss: 0.0152 | Grad Norm: 0.00848909\n",
      "Epoch 3 | Step 2109700 | Avg Loss: 0.0154 | Grad Norm: 0.00853684\n",
      "Epoch 3 | Step 2109800 | Avg Loss: 0.0154 | Grad Norm: 0.00897778\n",
      "Epoch 3 | Step 2109900 | Avg Loss: 0.0156 | Grad Norm: 0.00848354\n",
      "Epoch 3 | Step 2110000 | Avg Loss: 0.0153 | Grad Norm: 0.00855830\n",
      "Epoch 3 | Step 2110100 | Avg Loss: 0.0155 | Grad Norm: 0.00847075\n",
      "Epoch 3 | Step 2110200 | Avg Loss: 0.0153 | Grad Norm: 0.01078387\n",
      "Epoch 3 | Step 2110300 | Avg Loss: 0.0153 | Grad Norm: 0.00895656\n",
      "Epoch 3 | Step 2110400 | Avg Loss: 0.0157 | Grad Norm: 0.00915885\n",
      "Epoch 3 | Step 2110500 | Avg Loss: 0.0156 | Grad Norm: 0.00911348\n",
      "Epoch 3 | Step 2110600 | Avg Loss: 0.0156 | Grad Norm: 0.00883099\n",
      "Epoch 3 | Step 2110700 | Avg Loss: 0.0158 | Grad Norm: 0.00845503\n",
      "Epoch 3 | Step 2110800 | Avg Loss: 0.0155 | Grad Norm: 0.00911515\n",
      "Epoch 3 | Step 2110900 | Avg Loss: 0.0155 | Grad Norm: 0.00882591\n",
      "Epoch 3 | Step 2111000 | Avg Loss: 0.0156 | Grad Norm: 0.00979953\n",
      "Epoch 3 | Step 2111100 | Avg Loss: 0.0154 | Grad Norm: 0.00850921\n",
      "Epoch 3 | Step 2111200 | Avg Loss: 0.0156 | Grad Norm: 0.00839851\n",
      "Epoch 3 | Step 2111300 | Avg Loss: 0.0161 | Grad Norm: 0.00903977\n",
      "Epoch 3 | Step 2111400 | Avg Loss: 0.0158 | Grad Norm: 0.00776382\n",
      "Epoch 3 | Step 2111500 | Avg Loss: 0.0164 | Grad Norm: 0.00871374\n",
      "Epoch 3 | Step 2111600 | Avg Loss: 0.0161 | Grad Norm: 0.01109166\n",
      "Epoch 3 | Step 2111700 | Avg Loss: 0.0159 | Grad Norm: 0.00820729\n",
      "Epoch 3 | Step 2111800 | Avg Loss: 0.0158 | Grad Norm: 0.00881746\n",
      "Epoch 3 | Step 2111900 | Avg Loss: 0.0158 | Grad Norm: 0.00837104\n",
      "Epoch 3 | Step 2112000 | Avg Loss: 0.0159 | Grad Norm: 0.00941260\n",
      "Epoch 3 | Step 2112100 | Avg Loss: 0.0157 | Grad Norm: 0.00937575\n",
      "Epoch 3 | Step 2112200 | Avg Loss: 0.0157 | Grad Norm: 0.00826226\n",
      "Epoch 3 | Step 2112300 | Avg Loss: 0.0155 | Grad Norm: 0.00799360\n",
      "Epoch 3 | Step 2112400 | Avg Loss: 0.0155 | Grad Norm: 0.00885275\n",
      "Epoch 3 | Step 2112500 | Avg Loss: 0.0153 | Grad Norm: 0.00824652\n",
      "Epoch 3 | Step 2112600 | Avg Loss: 0.0152 | Grad Norm: 0.00860666\n",
      "Epoch 3 | Step 2112700 | Avg Loss: 0.0153 | Grad Norm: 0.00841686\n",
      "Epoch 3 | Step 2112800 | Avg Loss: 0.0150 | Grad Norm: 0.00861940\n",
      "Epoch 3 | Step 2112900 | Avg Loss: 0.0150 | Grad Norm: 0.00848785\n",
      "Epoch 3 | Step 2113000 | Avg Loss: 0.0156 | Grad Norm: 0.01021718\n",
      "Epoch 3 | Step 2113100 | Avg Loss: 0.0158 | Grad Norm: 0.00977590\n",
      "Epoch 3 | Step 2113200 | Avg Loss: 0.0160 | Grad Norm: 0.00835793\n",
      "Epoch 3 | Step 2113300 | Avg Loss: 0.0157 | Grad Norm: 0.00896504\n",
      "Epoch 3 | Step 2113400 | Avg Loss: 0.0157 | Grad Norm: 0.00919225\n",
      "Epoch 3 | Step 2113500 | Avg Loss: 0.0157 | Grad Norm: 0.00893991\n",
      "Epoch 3 | Step 2113600 | Avg Loss: 0.0155 | Grad Norm: 0.00803089\n",
      "Epoch 3 | Step 2113700 | Avg Loss: 0.0155 | Grad Norm: 0.01057759\n",
      "Epoch 3 | Step 2113800 | Avg Loss: 0.0154 | Grad Norm: 0.00897416\n",
      "Epoch 3 | Step 2113900 | Avg Loss: 0.0156 | Grad Norm: 0.00828209\n",
      "Epoch 3 | Step 2114000 | Avg Loss: 0.0153 | Grad Norm: 0.00884675\n",
      "Epoch 3 | Step 2114100 | Avg Loss: 0.0157 | Grad Norm: 0.01190273\n",
      "Epoch 3 | Step 2114200 | Avg Loss: 0.0158 | Grad Norm: 0.00858710\n",
      "Epoch 3 | Step 2114300 | Avg Loss: 0.0157 | Grad Norm: 0.00876643\n",
      "Epoch 3 | Step 2114400 | Avg Loss: 0.0157 | Grad Norm: 0.00909775\n",
      "Epoch 3 | Step 2114500 | Avg Loss: 0.0155 | Grad Norm: 0.00803441\n",
      "Epoch 3 | Step 2114600 | Avg Loss: 0.0155 | Grad Norm: 0.00837156\n",
      "Epoch 3 | Step 2114700 | Avg Loss: 0.0156 | Grad Norm: 0.02221516\n",
      "Epoch 3 | Step 2114800 | Avg Loss: 0.0157 | Grad Norm: 0.01066487\n",
      "Epoch 3 | Step 2114900 | Avg Loss: 0.0155 | Grad Norm: 0.00904464\n",
      "Epoch 3 | Step 2115000 | Avg Loss: 0.0151 | Grad Norm: 0.00840121\n",
      "Epoch 3 | Step 2115100 | Avg Loss: 0.0157 | Grad Norm: 0.00917566\n",
      "Epoch 3 | Step 2115200 | Avg Loss: 0.0157 | Grad Norm: 0.00902237\n",
      "Epoch 3 | Step 2115300 | Avg Loss: 0.0159 | Grad Norm: 0.00903982\n",
      "Epoch 3 | Step 2115400 | Avg Loss: 0.0157 | Grad Norm: 0.00867642\n",
      "Epoch 3 | Step 2115500 | Avg Loss: 0.0155 | Grad Norm: 0.00896310\n",
      "Epoch 3 | Step 2115600 | Avg Loss: 0.0153 | Grad Norm: 0.00934053\n",
      "Epoch 3 | Step 2115700 | Avg Loss: 0.0154 | Grad Norm: 0.00840954\n",
      "Epoch 3 | Step 2115800 | Avg Loss: 0.0158 | Grad Norm: 0.00928264\n",
      "Epoch 3 | Step 2115900 | Avg Loss: 0.0161 | Grad Norm: 0.00953028\n",
      "Epoch 3 | Step 2116000 | Avg Loss: 0.0159 | Grad Norm: 0.00894551\n",
      "Epoch 3 | Step 2116100 | Avg Loss: 0.0155 | Grad Norm: 0.00871415\n",
      "Epoch 3 | Step 2116200 | Avg Loss: 0.0159 | Grad Norm: 0.00824632\n",
      "Epoch 3 | Step 2116300 | Avg Loss: 0.0156 | Grad Norm: 0.00894263\n",
      "Epoch 3 | Step 2116400 | Avg Loss: 0.0156 | Grad Norm: 0.01115879\n",
      "Epoch 3 | Step 2116500 | Avg Loss: 0.0152 | Grad Norm: 0.01031871\n",
      "Epoch 3 | Step 2116600 | Avg Loss: 0.0155 | Grad Norm: 0.00844035\n",
      "Epoch 3 | Step 2116700 | Avg Loss: 0.0154 | Grad Norm: 0.00949408\n",
      "Epoch 3 | Step 2116800 | Avg Loss: 0.0154 | Grad Norm: 0.00918303\n",
      "Epoch 3 | Step 2116900 | Avg Loss: 0.0156 | Grad Norm: 0.00792400\n",
      "Epoch 3 | Step 2117000 | Avg Loss: 0.0157 | Grad Norm: 0.00972223\n",
      "Epoch 3 | Step 2117100 | Avg Loss: 0.0160 | Grad Norm: 0.00954412\n",
      "Epoch 3 | Step 2117200 | Avg Loss: 0.0156 | Grad Norm: 0.00837759\n",
      "Epoch 3 | Step 2117300 | Avg Loss: 0.0155 | Grad Norm: 0.00967400\n",
      "Epoch 3 | Step 2117400 | Avg Loss: 0.0155 | Grad Norm: 0.00855755\n",
      "Epoch 3 | Step 2117500 | Avg Loss: 0.0158 | Grad Norm: 0.00870402\n",
      "Epoch 3 | Step 2117600 | Avg Loss: 0.0157 | Grad Norm: 0.00811987\n",
      "Epoch 3 | Step 2117700 | Avg Loss: 0.0155 | Grad Norm: 0.01144346\n",
      "Epoch 3 | Step 2117800 | Avg Loss: 0.0159 | Grad Norm: 0.00973258\n",
      "Epoch 3 | Step 2117900 | Avg Loss: 0.0157 | Grad Norm: 0.00894521\n",
      "Epoch 3 | Step 2118000 | Avg Loss: 0.0158 | Grad Norm: 0.00851578\n",
      "Epoch 3 | Step 2118100 | Avg Loss: 0.0156 | Grad Norm: 0.00878265\n",
      "Epoch 3 | Step 2118200 | Avg Loss: 0.0155 | Grad Norm: 0.00927688\n",
      "Epoch 3 | Step 2118300 | Avg Loss: 0.0157 | Grad Norm: 0.00957629\n",
      "Epoch 3 | Step 2118400 | Avg Loss: 0.0159 | Grad Norm: 0.00824008\n",
      "Epoch 3 | Step 2118500 | Avg Loss: 0.0159 | Grad Norm: 0.00874231\n",
      "Epoch 3 | Step 2118600 | Avg Loss: 0.0159 | Grad Norm: 0.00920913\n",
      "Epoch 3 | Step 2118700 | Avg Loss: 0.0160 | Grad Norm: 0.01002038\n",
      "Epoch 3 | Step 2118800 | Avg Loss: 0.0157 | Grad Norm: 0.00867983\n",
      "Epoch 3 | Step 2118900 | Avg Loss: 0.0158 | Grad Norm: 0.01102420\n",
      "Epoch 3 | Step 2119000 | Avg Loss: 0.0160 | Grad Norm: 0.01008994\n",
      "Epoch 3 | Step 2119100 | Avg Loss: 0.0159 | Grad Norm: 0.00945113\n",
      "Epoch 3 | Step 2119200 | Avg Loss: 0.0156 | Grad Norm: 0.00832475\n",
      "Epoch 3 | Step 2119300 | Avg Loss: 0.0158 | Grad Norm: 0.00883199\n",
      "Epoch 3 | Step 2119400 | Avg Loss: 0.0159 | Grad Norm: 0.01084560\n",
      "Epoch 3 | Step 2119500 | Avg Loss: 0.0156 | Grad Norm: 0.00968426\n",
      "Epoch 3 | Step 2119600 | Avg Loss: 0.0158 | Grad Norm: 0.00893467\n",
      "Epoch 3 | Step 2119700 | Avg Loss: 0.0157 | Grad Norm: 0.00876746\n",
      "Epoch 3 | Step 2119800 | Avg Loss: 0.0160 | Grad Norm: 0.00859267\n",
      "Epoch 3 | Step 2119900 | Avg Loss: 0.0162 | Grad Norm: 0.01048241\n",
      "Epoch 3 | Step 2120000 | Avg Loss: 0.0162 | Grad Norm: 0.01008914\n",
      "Epoch 3 | Step 2120100 | Avg Loss: 0.0165 | Grad Norm: 0.01005338\n",
      "Epoch 3 | Step 2120200 | Avg Loss: 0.0161 | Grad Norm: 0.00873537\n",
      "Epoch 3 | Step 2120300 | Avg Loss: 0.0160 | Grad Norm: 0.01012793\n",
      "Epoch 3 | Step 2120400 | Avg Loss: 0.0160 | Grad Norm: 0.00890127\n",
      "Epoch 3 | Step 2120500 | Avg Loss: 0.0157 | Grad Norm: 0.00851441\n",
      "Epoch 3 | Step 2120600 | Avg Loss: 0.0159 | Grad Norm: 0.00960882\n",
      "Epoch 3 | Step 2120700 | Avg Loss: 0.0158 | Grad Norm: 0.00799138\n",
      "Epoch 3 | Step 2120800 | Avg Loss: 0.0157 | Grad Norm: 0.00863631\n",
      "Epoch 3 | Step 2120900 | Avg Loss: 0.0160 | Grad Norm: 0.00771292\n",
      "Epoch 3 | Step 2121000 | Avg Loss: 0.0157 | Grad Norm: 0.00883642\n",
      "Epoch 3 | Step 2121100 | Avg Loss: 0.0157 | Grad Norm: 0.00782999\n",
      "Epoch 3 | Step 2121200 | Avg Loss: 0.0157 | Grad Norm: 0.00816921\n",
      "Epoch 3 | Step 2121300 | Avg Loss: 0.0158 | Grad Norm: 0.00828485\n",
      "Epoch 3 | Step 2121400 | Avg Loss: 0.0159 | Grad Norm: 0.00898690\n",
      "Epoch 3 | Step 2121500 | Avg Loss: 0.0160 | Grad Norm: 0.00827189\n",
      "Epoch 3 | Step 2121600 | Avg Loss: 0.0162 | Grad Norm: 0.00961966\n",
      "Epoch 3 | Step 2121700 | Avg Loss: 0.0159 | Grad Norm: 0.00975554\n",
      "Epoch 3 | Step 2121800 | Avg Loss: 0.0158 | Grad Norm: 0.01019991\n",
      "Epoch 3 | Step 2121900 | Avg Loss: 0.0160 | Grad Norm: 0.00926573\n",
      "Epoch 3 | Step 2122000 | Avg Loss: 0.0160 | Grad Norm: 0.00865033\n",
      "Epoch 3 | Step 2122100 | Avg Loss: 0.0161 | Grad Norm: 0.00871924\n",
      "Epoch 3 | Step 2122200 | Avg Loss: 0.0159 | Grad Norm: 0.00782200\n",
      "Epoch 3 | Step 2122300 | Avg Loss: 0.0161 | Grad Norm: 0.00948218\n",
      "Epoch 3 | Step 2122400 | Avg Loss: 0.0158 | Grad Norm: 0.00958113\n",
      "Epoch 3 | Step 2122500 | Avg Loss: 0.0158 | Grad Norm: 0.00873714\n",
      "Epoch 3 | Step 2122600 | Avg Loss: 0.0157 | Grad Norm: 0.00966248\n",
      "Epoch 3 | Step 2122700 | Avg Loss: 0.0158 | Grad Norm: 0.00829718\n",
      "Epoch 3 | Step 2122800 | Avg Loss: 0.0155 | Grad Norm: 0.00890560\n",
      "Epoch 3 | Step 2122900 | Avg Loss: 0.0154 | Grad Norm: 0.00932825\n",
      "Epoch 3 | Step 2123000 | Avg Loss: 0.0154 | Grad Norm: 0.01115301\n",
      "Epoch 3 | Step 2123100 | Avg Loss: 0.0155 | Grad Norm: 0.00891777\n",
      "Epoch 3 | Step 2123200 | Avg Loss: 0.0153 | Grad Norm: 0.00912152\n",
      "Epoch 3 | Step 2123300 | Avg Loss: 0.0151 | Grad Norm: 0.00916406\n",
      "Epoch 3 | Step 2123400 | Avg Loss: 0.0149 | Grad Norm: 0.00924893\n",
      "Epoch 3 | Step 2123500 | Avg Loss: 0.0151 | Grad Norm: 0.01035729\n",
      "Epoch 3 | Step 2123600 | Avg Loss: 0.0154 | Grad Norm: 0.00695792\n",
      "Epoch 3 | Step 2123700 | Avg Loss: 0.0158 | Grad Norm: 0.01015160\n",
      "Epoch 3 | Step 2123800 | Avg Loss: 0.0155 | Grad Norm: 0.00783969\n",
      "Epoch 3 | Step 2123900 | Avg Loss: 0.0154 | Grad Norm: 0.01025132\n",
      "Epoch 3 | Step 2124000 | Avg Loss: 0.0154 | Grad Norm: 0.00865672\n",
      "Epoch 3 | Step 2124100 | Avg Loss: 0.0157 | Grad Norm: 0.00861702\n",
      "Epoch 3 | Step 2124200 | Avg Loss: 0.0158 | Grad Norm: 0.00784021\n",
      "Epoch 3 | Step 2124300 | Avg Loss: 0.0156 | Grad Norm: 0.00767893\n",
      "Epoch 3 | Step 2124400 | Avg Loss: 0.0155 | Grad Norm: 0.00995297\n",
      "Epoch 3 | Step 2124500 | Avg Loss: 0.0160 | Grad Norm: 0.00818902\n",
      "Epoch 3 | Step 2124600 | Avg Loss: 0.0157 | Grad Norm: 0.00903968\n",
      "Epoch 3 | Step 2124700 | Avg Loss: 0.0157 | Grad Norm: 0.00981261\n",
      "Epoch 3 | Step 2124800 | Avg Loss: 0.0158 | Grad Norm: 0.00824109\n",
      "Epoch 3 | Step 2124900 | Avg Loss: 0.0155 | Grad Norm: 0.00861794\n",
      "Epoch 3 | Step 2125000 | Avg Loss: 0.0155 | Grad Norm: 0.00903576\n",
      "Epoch 3 | Step 2125100 | Avg Loss: 0.0152 | Grad Norm: 0.00874718\n",
      "Epoch 3 | Step 2125200 | Avg Loss: 0.0158 | Grad Norm: 0.00928634\n",
      "Epoch 3 | Step 2125300 | Avg Loss: 0.0159 | Grad Norm: 0.00796248\n",
      "Epoch 3 | Step 2125400 | Avg Loss: 0.0163 | Grad Norm: 0.00941516\n",
      "Epoch 3 | Step 2125500 | Avg Loss: 0.0159 | Grad Norm: 0.01004864\n",
      "Epoch 3 | Step 2125600 | Avg Loss: 0.0159 | Grad Norm: 0.00852666\n",
      "Epoch 3 | Step 2125700 | Avg Loss: 0.0156 | Grad Norm: 0.00880499\n",
      "Epoch 3 | Step 2125800 | Avg Loss: 0.0154 | Grad Norm: 0.00920167\n",
      "Epoch 3 | Step 2125900 | Avg Loss: 0.0153 | Grad Norm: 0.00852495\n",
      "Epoch 3 | Step 2126000 | Avg Loss: 0.0151 | Grad Norm: 0.00842944\n",
      "Epoch 3 | Step 2126100 | Avg Loss: 0.0150 | Grad Norm: 0.01050520\n",
      "Epoch 3 | Step 2126200 | Avg Loss: 0.0154 | Grad Norm: 0.00778281\n",
      "Epoch 3 | Step 2126300 | Avg Loss: 0.0154 | Grad Norm: 0.00871140\n",
      "Epoch 3 | Step 2126400 | Avg Loss: 0.0153 | Grad Norm: 0.00880057\n",
      "Epoch 3 | Step 2126500 | Avg Loss: 0.0153 | Grad Norm: 0.00980690\n",
      "Epoch 3 | Step 2126600 | Avg Loss: 0.0155 | Grad Norm: 0.00817492\n",
      "Epoch 3 | Step 2126700 | Avg Loss: 0.0155 | Grad Norm: 0.01008374\n",
      "Epoch 3 | Step 2126800 | Avg Loss: 0.0152 | Grad Norm: 0.00978415\n",
      "Epoch 3 | Step 2126900 | Avg Loss: 0.0153 | Grad Norm: 0.00785711\n",
      "Epoch 3 | Step 2127000 | Avg Loss: 0.0154 | Grad Norm: 0.00964152\n",
      "Epoch 3 | Step 2127100 | Avg Loss: 0.0154 | Grad Norm: 0.01007165\n",
      "Epoch 3 | Step 2127200 | Avg Loss: 0.0156 | Grad Norm: 0.00833499\n",
      "Epoch 3 | Step 2127300 | Avg Loss: 0.0154 | Grad Norm: 0.00902196\n",
      "Epoch 3 | Step 2127400 | Avg Loss: 0.0151 | Grad Norm: 0.00923540\n",
      "Epoch 3 | Step 2127500 | Avg Loss: 0.0154 | Grad Norm: 0.00909875\n",
      "Epoch 3 | Step 2127600 | Avg Loss: 0.0159 | Grad Norm: 0.00887871\n",
      "Epoch 3 | Step 2127700 | Avg Loss: 0.0163 | Grad Norm: 0.00992460\n",
      "Epoch 3 | Step 2127800 | Avg Loss: 0.0162 | Grad Norm: 0.00931310\n",
      "Epoch 3 | Step 2127900 | Avg Loss: 0.0155 | Grad Norm: 0.00943641\n",
      "Epoch 3 | Step 2128000 | Avg Loss: 0.0155 | Grad Norm: 0.00733011\n",
      "Epoch 3 | Step 2128100 | Avg Loss: 0.0156 | Grad Norm: 0.00923176\n",
      "Epoch 3 | Step 2128200 | Avg Loss: 0.0156 | Grad Norm: 0.01071260\n",
      "Epoch 3 | Step 2128300 | Avg Loss: 0.0155 | Grad Norm: 0.00756791\n",
      "Epoch 3 | Step 2128400 | Avg Loss: 0.0154 | Grad Norm: 0.00959300\n",
      "Epoch 3 | Step 2128500 | Avg Loss: 0.0154 | Grad Norm: 0.00846856\n",
      "Epoch 3 | Step 2128600 | Avg Loss: 0.0153 | Grad Norm: 0.01049473\n",
      "Epoch 3 | Step 2128700 | Avg Loss: 0.0157 | Grad Norm: 0.00831310\n",
      "Epoch 3 | Step 2128800 | Avg Loss: 0.0153 | Grad Norm: 0.00902073\n",
      "Epoch 3 | Step 2128900 | Avg Loss: 0.0154 | Grad Norm: 0.00921802\n",
      "Epoch 3 | Step 2129000 | Avg Loss: 0.0159 | Grad Norm: 0.00876900\n",
      "Epoch 3 | Step 2129100 | Avg Loss: 0.0155 | Grad Norm: 0.00937222\n",
      "Epoch 3 | Step 2129200 | Avg Loss: 0.0155 | Grad Norm: 0.00911034\n",
      "Epoch 3 | Step 2129300 | Avg Loss: 0.0152 | Grad Norm: 0.00890842\n",
      "Epoch 3 | Step 2129400 | Avg Loss: 0.0153 | Grad Norm: 0.00961116\n",
      "Epoch 3 | Step 2129500 | Avg Loss: 0.0153 | Grad Norm: 0.00823894\n",
      "Epoch 3 | Step 2129600 | Avg Loss: 0.0153 | Grad Norm: 0.00943896\n",
      "Epoch 3 | Step 2129700 | Avg Loss: 0.0157 | Grad Norm: 0.01871103\n",
      "Epoch 3 | Step 2129800 | Avg Loss: 0.0158 | Grad Norm: 0.00880413\n",
      "Epoch 3 | Step 2129900 | Avg Loss: 0.0157 | Grad Norm: 0.00762099\n",
      "Epoch 3 | Step 2130000 | Avg Loss: 0.0161 | Grad Norm: 0.01056822\n",
      "Epoch 3 | Step 2130100 | Avg Loss: 0.0160 | Grad Norm: 0.00831777\n",
      "Epoch 3 | Step 2130200 | Avg Loss: 0.0157 | Grad Norm: 0.01097213\n",
      "Epoch 3 | Step 2130300 | Avg Loss: 0.0154 | Grad Norm: 0.00837897\n",
      "Epoch 3 | Step 2130400 | Avg Loss: 0.0157 | Grad Norm: 0.00826621\n",
      "Epoch 3 | Step 2130500 | Avg Loss: 0.0157 | Grad Norm: 0.00899345\n",
      "Epoch 3 | Step 2130600 | Avg Loss: 0.0158 | Grad Norm: 0.01164298\n",
      "Epoch 3 | Step 2130700 | Avg Loss: 0.0158 | Grad Norm: 0.00956435\n",
      "Epoch 3 | Step 2130800 | Avg Loss: 0.0160 | Grad Norm: 0.00922767\n",
      "Epoch 3 | Step 2130900 | Avg Loss: 0.0159 | Grad Norm: 0.01042941\n",
      "Epoch 3 | Step 2131000 | Avg Loss: 0.0159 | Grad Norm: 0.00991792\n",
      "Epoch 3 | Step 2131100 | Avg Loss: 0.0163 | Grad Norm: 0.00895851\n",
      "Epoch 3 | Step 2131200 | Avg Loss: 0.0159 | Grad Norm: 0.01016407\n",
      "Epoch 3 | Step 2131300 | Avg Loss: 0.0154 | Grad Norm: 0.00925292\n",
      "Epoch 3 | Step 2131400 | Avg Loss: 0.0152 | Grad Norm: 0.00854933\n",
      "Epoch 3 | Step 2131500 | Avg Loss: 0.0153 | Grad Norm: 0.01131345\n",
      "Epoch 3 | Step 2131600 | Avg Loss: 0.0153 | Grad Norm: 0.01127996\n",
      "Epoch 3 | Step 2131700 | Avg Loss: 0.0154 | Grad Norm: 0.00878330\n",
      "Epoch 3 | Step 2131800 | Avg Loss: 0.0153 | Grad Norm: 0.00813933\n",
      "Epoch 3 | Step 2131900 | Avg Loss: 0.0160 | Grad Norm: 0.00981863\n",
      "Epoch 3 | Step 2132000 | Avg Loss: 0.0155 | Grad Norm: 0.00884942\n",
      "Epoch 3 | Step 2132100 | Avg Loss: 0.0158 | Grad Norm: 0.00858293\n",
      "Epoch 3 | Step 2132200 | Avg Loss: 0.0159 | Grad Norm: 0.00873870\n",
      "Epoch 3 | Step 2132300 | Avg Loss: 0.0157 | Grad Norm: 0.01268504\n",
      "Epoch 3 | Step 2132400 | Avg Loss: 0.0156 | Grad Norm: 0.01018893\n",
      "Epoch 3 | Step 2132500 | Avg Loss: 0.0155 | Grad Norm: 0.01038056\n",
      "Epoch 3 | Step 2132600 | Avg Loss: 0.0152 | Grad Norm: 0.00923401\n",
      "Epoch 3 | Step 2132700 | Avg Loss: 0.0157 | Grad Norm: 0.00856711\n",
      "Epoch 3 | Step 2132800 | Avg Loss: 0.0159 | Grad Norm: 0.00813623\n",
      "Epoch 3 | Step 2132900 | Avg Loss: 0.0160 | Grad Norm: 0.01099754\n",
      "Epoch 3 | Step 2133000 | Avg Loss: 0.0159 | Grad Norm: 0.00920216\n",
      "Epoch 3 | Step 2133100 | Avg Loss: 0.0160 | Grad Norm: 0.00991873\n",
      "Epoch 3 | Step 2133200 | Avg Loss: 0.0156 | Grad Norm: 0.00823945\n",
      "Epoch 3 | Step 2133300 | Avg Loss: 0.0157 | Grad Norm: 0.00931834\n",
      "Epoch 3 | Step 2133400 | Avg Loss: 0.0157 | Grad Norm: 0.00798894\n",
      "Epoch 3 | Step 2133500 | Avg Loss: 0.0156 | Grad Norm: 0.00836897\n",
      "Epoch 3 | Step 2133600 | Avg Loss: 0.0158 | Grad Norm: 0.00786595\n",
      "Epoch 3 | Step 2133700 | Avg Loss: 0.0158 | Grad Norm: 0.00845639\n",
      "Epoch 3 | Step 2133800 | Avg Loss: 0.0157 | Grad Norm: 0.00847541\n",
      "Epoch 3 | Step 2133900 | Avg Loss: 0.0160 | Grad Norm: 0.00927714\n",
      "Epoch 3 | Step 2134000 | Avg Loss: 0.0157 | Grad Norm: 0.01063446\n",
      "Epoch 3 | Step 2134100 | Avg Loss: 0.0162 | Grad Norm: 0.00887241\n",
      "Epoch 3 | Step 2134200 | Avg Loss: 0.0157 | Grad Norm: 0.00865989\n",
      "Epoch 3 | Step 2134300 | Avg Loss: 0.0152 | Grad Norm: 0.01055338\n",
      "Epoch 3 | Step 2134400 | Avg Loss: 0.0156 | Grad Norm: 0.00881921\n",
      "Epoch 3 | Step 2134500 | Avg Loss: 0.0156 | Grad Norm: 0.00877460\n",
      "Epoch 3 | Step 2134600 | Avg Loss: 0.0154 | Grad Norm: 0.00961679\n",
      "Epoch 3 | Step 2134700 | Avg Loss: 0.0157 | Grad Norm: 0.00793304\n",
      "Epoch 3 | Step 2134800 | Avg Loss: 0.0154 | Grad Norm: 0.01052294\n",
      "Epoch 3 | Step 2134900 | Avg Loss: 0.0158 | Grad Norm: 0.00892443\n",
      "Epoch 3 | Step 2135000 | Avg Loss: 0.0155 | Grad Norm: 0.00844932\n",
      "Epoch 3 | Step 2135100 | Avg Loss: 0.0156 | Grad Norm: 0.00828853\n",
      "Epoch 3 | Step 2135200 | Avg Loss: 0.0157 | Grad Norm: 0.00868069\n",
      "Epoch 3 | Step 2135300 | Avg Loss: 0.0155 | Grad Norm: 0.00901827\n",
      "Epoch 3 | Step 2135400 | Avg Loss: 0.0156 | Grad Norm: 0.00901834\n",
      "Epoch 3 | Step 2135500 | Avg Loss: 0.0151 | Grad Norm: 0.00963364\n",
      "Epoch 3 | Step 2135600 | Avg Loss: 0.0153 | Grad Norm: 0.00904448\n",
      "Epoch 3 | Step 2135700 | Avg Loss: 0.0151 | Grad Norm: 0.00826601\n",
      "Epoch 3 | Step 2135800 | Avg Loss: 0.0152 | Grad Norm: 0.00802457\n",
      "Epoch 3 | Step 2135900 | Avg Loss: 0.0150 | Grad Norm: 0.00880917\n",
      "Epoch 3 | Step 2136000 | Avg Loss: 0.0152 | Grad Norm: 0.01231699\n",
      "Epoch 3 | Step 2136100 | Avg Loss: 0.0154 | Grad Norm: 0.00808945\n",
      "Epoch 3 | Step 2136200 | Avg Loss: 0.0153 | Grad Norm: 0.00856484\n",
      "Epoch 3 | Step 2136300 | Avg Loss: 0.0153 | Grad Norm: 0.01093159\n",
      "Epoch 3 | Step 2136400 | Avg Loss: 0.0156 | Grad Norm: 0.00927379\n",
      "Epoch 3 | Step 2136500 | Avg Loss: 0.0158 | Grad Norm: 0.00872916\n",
      "Epoch 3 | Step 2136600 | Avg Loss: 0.0155 | Grad Norm: 0.00872352\n",
      "Epoch 3 | Step 2136700 | Avg Loss: 0.0153 | Grad Norm: 0.00917557\n",
      "Epoch 3 | Step 2136800 | Avg Loss: 0.0154 | Grad Norm: 0.00846054\n",
      "Epoch 3 | Step 2136900 | Avg Loss: 0.0154 | Grad Norm: 0.00874565\n",
      "Epoch 3 | Step 2137000 | Avg Loss: 0.0153 | Grad Norm: 0.00896880\n",
      "Epoch 3 | Step 2137100 | Avg Loss: 0.0152 | Grad Norm: 0.00843648\n",
      "Epoch 3 | Step 2137200 | Avg Loss: 0.0155 | Grad Norm: 0.00943663\n",
      "Epoch 3 | Step 2137300 | Avg Loss: 0.0156 | Grad Norm: 0.00869597\n",
      "Epoch 3 | Step 2137400 | Avg Loss: 0.0155 | Grad Norm: 0.01089594\n",
      "Epoch 3 | Step 2137500 | Avg Loss: 0.0157 | Grad Norm: 0.00953741\n",
      "Epoch 3 | Step 2137600 | Avg Loss: 0.0156 | Grad Norm: 0.00972431\n",
      "Epoch 3 | Step 2137700 | Avg Loss: 0.0157 | Grad Norm: 0.00873600\n",
      "Epoch 3 | Step 2137800 | Avg Loss: 0.0158 | Grad Norm: 0.00800712\n",
      "Epoch 3 | Step 2137900 | Avg Loss: 0.0157 | Grad Norm: 0.00864514\n",
      "Epoch 3 | Step 2138000 | Avg Loss: 0.0155 | Grad Norm: 0.00837428\n",
      "Epoch 3 | Step 2138100 | Avg Loss: 0.0159 | Grad Norm: 0.00899979\n",
      "Epoch 3 | Step 2138200 | Avg Loss: 0.0154 | Grad Norm: 0.00908585\n",
      "Epoch 3 | Step 2138300 | Avg Loss: 0.0156 | Grad Norm: 0.00837374\n",
      "Epoch 3 | Step 2138400 | Avg Loss: 0.0153 | Grad Norm: 0.01069373\n",
      "Epoch 3 | Step 2138500 | Avg Loss: 0.0155 | Grad Norm: 0.00822825\n",
      "Epoch 3 | Step 2138600 | Avg Loss: 0.0152 | Grad Norm: 0.00895834\n",
      "Epoch 3 | Step 2138700 | Avg Loss: 0.0155 | Grad Norm: 0.00969625\n",
      "Epoch 3 | Step 2138800 | Avg Loss: 0.0152 | Grad Norm: 0.01040629\n",
      "Epoch 3 | Step 2138900 | Avg Loss: 0.0150 | Grad Norm: 0.00994902\n",
      "Epoch 3 | Step 2139000 | Avg Loss: 0.0150 | Grad Norm: 0.00926749\n",
      "Epoch 3 | Step 2139100 | Avg Loss: 0.0154 | Grad Norm: 0.00906831\n",
      "Epoch 3 | Step 2139200 | Avg Loss: 0.0155 | Grad Norm: 0.00830852\n",
      "Epoch 3 | Step 2139300 | Avg Loss: 0.0153 | Grad Norm: 0.00910254\n",
      "Epoch 3 | Step 2139400 | Avg Loss: 0.0152 | Grad Norm: 0.00907034\n",
      "Epoch 3 | Step 2139500 | Avg Loss: 0.0151 | Grad Norm: 0.00993513\n",
      "Epoch 3 | Step 2139600 | Avg Loss: 0.0152 | Grad Norm: 0.01108328\n",
      "Epoch 3 | Step 2139700 | Avg Loss: 0.0151 | Grad Norm: 0.00883356\n",
      "Epoch 3 | Step 2139800 | Avg Loss: 0.0150 | Grad Norm: 0.00832609\n",
      "Epoch 3 | Step 2139900 | Avg Loss: 0.0151 | Grad Norm: 0.00864869\n",
      "Epoch 3 | Step 2140000 | Avg Loss: 0.0157 | Grad Norm: 0.01088834\n",
      "Epoch 3 | Step 2140100 | Avg Loss: 0.0151 | Grad Norm: 0.00935874\n",
      "Epoch 3 | Step 2140200 | Avg Loss: 0.0149 | Grad Norm: 0.01009290\n",
      "Epoch 3 | Step 2140300 | Avg Loss: 0.0152 | Grad Norm: 0.00928206\n",
      "Epoch 3 | Step 2140400 | Avg Loss: 0.0153 | Grad Norm: 0.00955378\n",
      "Epoch 3 | Step 2140500 | Avg Loss: 0.0152 | Grad Norm: 0.01274218\n",
      "Epoch 3 | Step 2140600 | Avg Loss: 0.0155 | Grad Norm: 0.00993689\n",
      "Epoch 3 | Step 2140700 | Avg Loss: 0.0153 | Grad Norm: 0.00852526\n",
      "Epoch 3 | Step 2140800 | Avg Loss: 0.0149 | Grad Norm: 0.01157663\n",
      "Epoch 3 | Step 2140900 | Avg Loss: 0.0151 | Grad Norm: 0.00894207\n",
      "Epoch 3 | Step 2141000 | Avg Loss: 0.0147 | Grad Norm: 0.00835275\n",
      "Epoch 3 | Step 2141100 | Avg Loss: 0.0152 | Grad Norm: 0.00826530\n",
      "Epoch 3 | Step 2141200 | Avg Loss: 0.0155 | Grad Norm: 0.00891088\n",
      "Epoch 3 | Step 2141300 | Avg Loss: 0.0154 | Grad Norm: 0.00788774\n",
      "Epoch 3 | Step 2141400 | Avg Loss: 0.0154 | Grad Norm: 0.00865347\n",
      "Epoch 3 | Step 2141500 | Avg Loss: 0.0153 | Grad Norm: 0.00868676\n",
      "Epoch 3 | Step 2141600 | Avg Loss: 0.0154 | Grad Norm: 0.00936226\n",
      "Epoch 3 | Step 2141700 | Avg Loss: 0.0153 | Grad Norm: 0.00967372\n",
      "Epoch 3 | Step 2141800 | Avg Loss: 0.0153 | Grad Norm: 0.00818229\n",
      "Epoch 3 | Step 2141900 | Avg Loss: 0.0154 | Grad Norm: 0.00926850\n",
      "Epoch 3 | Step 2142000 | Avg Loss: 0.0153 | Grad Norm: 0.01004482\n",
      "Epoch 3 | Step 2142100 | Avg Loss: 0.0156 | Grad Norm: 0.00864226\n",
      "Epoch 3 | Step 2142200 | Avg Loss: 0.0156 | Grad Norm: 0.00808148\n",
      "Epoch 3 | Step 2142300 | Avg Loss: 0.0153 | Grad Norm: 0.00803817\n",
      "Epoch 3 | Step 2142400 | Avg Loss: 0.0154 | Grad Norm: 0.00848252\n",
      "Epoch 3 | Step 2142500 | Avg Loss: 0.0155 | Grad Norm: 0.00829201\n",
      "Epoch 3 | Step 2142600 | Avg Loss: 0.0154 | Grad Norm: 0.00921022\n",
      "Epoch 3 | Step 2142700 | Avg Loss: 0.0152 | Grad Norm: 0.00966770\n",
      "Epoch 3 | Step 2142800 | Avg Loss: 0.0152 | Grad Norm: 0.00825871\n",
      "Epoch 3 | Step 2142900 | Avg Loss: 0.0151 | Grad Norm: 0.01019067\n",
      "Epoch 3 | Step 2143000 | Avg Loss: 0.0154 | Grad Norm: 0.00841968\n",
      "Epoch 3 | Step 2143100 | Avg Loss: 0.0154 | Grad Norm: 0.00778829\n",
      "Epoch 3 | Step 2143200 | Avg Loss: 0.0155 | Grad Norm: 0.00853163\n",
      "Epoch 3 | Step 2143300 | Avg Loss: 0.0153 | Grad Norm: 0.00898674\n",
      "Epoch 3 | Step 2143400 | Avg Loss: 0.0156 | Grad Norm: 0.00950990\n",
      "Epoch 3 | Step 2143500 | Avg Loss: 0.0154 | Grad Norm: 0.00961054\n",
      "Epoch 3 | Step 2143600 | Avg Loss: 0.0151 | Grad Norm: 0.00983542\n",
      "Epoch 3 | Step 2143700 | Avg Loss: 0.0154 | Grad Norm: 0.00869958\n",
      "Epoch 3 | Step 2143800 | Avg Loss: 0.0154 | Grad Norm: 0.00939460\n",
      "Epoch 3 | Step 2143900 | Avg Loss: 0.0156 | Grad Norm: 0.00804474\n",
      "Epoch 3 | Step 2144000 | Avg Loss: 0.0153 | Grad Norm: 0.00956208\n",
      "Epoch 3 | Step 2144100 | Avg Loss: 0.0154 | Grad Norm: 0.00813488\n",
      "Epoch 3 | Step 2144200 | Avg Loss: 0.0156 | Grad Norm: 0.00930582\n",
      "Epoch 3 | Step 2144300 | Avg Loss: 0.0150 | Grad Norm: 0.00891047\n",
      "Epoch 3 | Step 2144400 | Avg Loss: 0.0150 | Grad Norm: 0.00902149\n",
      "Epoch 3 | Step 2144500 | Avg Loss: 0.0153 | Grad Norm: 0.00929096\n",
      "Epoch 3 | Step 2144600 | Avg Loss: 0.0153 | Grad Norm: 0.00908236\n",
      "Epoch 3 | Step 2144700 | Avg Loss: 0.0154 | Grad Norm: 0.00884887\n",
      "Epoch 3 | Step 2144800 | Avg Loss: 0.0154 | Grad Norm: 0.00866354\n",
      "Epoch 3 | Step 2144900 | Avg Loss: 0.0154 | Grad Norm: 0.01039471\n",
      "Epoch 3 | Step 2145000 | Avg Loss: 0.0155 | Grad Norm: 0.00904173\n",
      "Epoch 3 | Step 2145100 | Avg Loss: 0.0154 | Grad Norm: 0.00871772\n",
      "Epoch 3 | Step 2145200 | Avg Loss: 0.0153 | Grad Norm: 0.00870214\n",
      "Epoch 3 | Step 2145300 | Avg Loss: 0.0155 | Grad Norm: 0.00778546\n",
      "Epoch 3 | Step 2145400 | Avg Loss: 0.0155 | Grad Norm: 0.00835449\n",
      "Epoch 3 | Step 2145500 | Avg Loss: 0.0152 | Grad Norm: 0.00861818\n",
      "Epoch 3 | Step 2145600 | Avg Loss: 0.0151 | Grad Norm: 0.00876249\n",
      "Epoch 3 | Step 2145700 | Avg Loss: 0.0151 | Grad Norm: 0.00936944\n",
      "Epoch 3 | Step 2145800 | Avg Loss: 0.0153 | Grad Norm: 0.00786437\n",
      "Epoch 3 | Step 2145900 | Avg Loss: 0.0155 | Grad Norm: 0.00890075\n",
      "Epoch 3 | Step 2146000 | Avg Loss: 0.0159 | Grad Norm: 0.00814208\n",
      "Epoch 3 | Step 2146100 | Avg Loss: 0.0160 | Grad Norm: 0.00914583\n",
      "Epoch 3 | Step 2146200 | Avg Loss: 0.0160 | Grad Norm: 0.00891655\n",
      "Epoch 3 | Step 2146300 | Avg Loss: 0.0159 | Grad Norm: 0.00923470\n",
      "Epoch 3 | Step 2146400 | Avg Loss: 0.0160 | Grad Norm: 0.00938565\n",
      "Epoch 3 | Step 2146500 | Avg Loss: 0.0156 | Grad Norm: 0.00863404\n",
      "Epoch 3 | Step 2146600 | Avg Loss: 0.0157 | Grad Norm: 0.00888727\n",
      "Epoch 3 | Step 2146700 | Avg Loss: 0.0156 | Grad Norm: 0.00814591\n",
      "Epoch 3 | Step 2146800 | Avg Loss: 0.0155 | Grad Norm: 0.00877838\n",
      "Epoch 3 | Step 2146900 | Avg Loss: 0.0158 | Grad Norm: 0.00854149\n",
      "Epoch 3 | Step 2147000 | Avg Loss: 0.0158 | Grad Norm: 0.00949103\n",
      "Epoch 3 | Step 2147100 | Avg Loss: 0.0161 | Grad Norm: 0.01086536\n",
      "Epoch 3 | Step 2147200 | Avg Loss: 0.0161 | Grad Norm: 0.00856729\n",
      "Epoch 3 | Step 2147300 | Avg Loss: 0.0163 | Grad Norm: 0.01020995\n",
      "Epoch 3 | Step 2147400 | Avg Loss: 0.0166 | Grad Norm: 0.00948294\n",
      "Epoch 3 | Step 2147500 | Avg Loss: 0.0163 | Grad Norm: 0.00928251\n",
      "Epoch 3 | Step 2147600 | Avg Loss: 0.0162 | Grad Norm: 0.00762749\n",
      "Epoch 3 | Step 2147700 | Avg Loss: 0.0163 | Grad Norm: 0.00936113\n",
      "Epoch 3 | Step 2147800 | Avg Loss: 0.0165 | Grad Norm: 0.00920943\n",
      "Epoch 3 | Step 2147900 | Avg Loss: 0.0163 | Grad Norm: 0.00833318\n",
      "Epoch 3 | Step 2148000 | Avg Loss: 0.0162 | Grad Norm: 0.00897452\n",
      "Epoch 3 | Step 2148100 | Avg Loss: 0.0161 | Grad Norm: 0.00936325\n",
      "Epoch 3 | Step 2148200 | Avg Loss: 0.0161 | Grad Norm: 0.00946763\n",
      "Epoch 3 | Step 2148300 | Avg Loss: 0.0162 | Grad Norm: 0.00880351\n",
      "Epoch 3 | Step 2148400 | Avg Loss: 0.0161 | Grad Norm: 0.00859016\n",
      "Epoch 3 | Step 2148500 | Avg Loss: 0.0160 | Grad Norm: 0.01079838\n",
      "Epoch 3 | Step 2148600 | Avg Loss: 0.0153 | Grad Norm: 0.00792322\n",
      "Epoch 3 | Step 2148700 | Avg Loss: 0.0155 | Grad Norm: 0.00937469\n",
      "Epoch 3 | Step 2148800 | Avg Loss: 0.0153 | Grad Norm: 0.00943525\n",
      "Epoch 3 | Step 2148900 | Avg Loss: 0.0154 | Grad Norm: 0.00827662\n",
      "Epoch 3 | Step 2149000 | Avg Loss: 0.0153 | Grad Norm: 0.00963770\n",
      "Epoch 3 | Step 2149100 | Avg Loss: 0.0159 | Grad Norm: 0.00929377\n",
      "Epoch 3 | Step 2149200 | Avg Loss: 0.0159 | Grad Norm: 0.00891424\n",
      "Epoch 3 | Step 2149300 | Avg Loss: 0.0158 | Grad Norm: 0.00837377\n",
      "Epoch 3 | Step 2149400 | Avg Loss: 0.0159 | Grad Norm: 0.00987363\n",
      "Epoch 3 | Step 2149500 | Avg Loss: 0.0159 | Grad Norm: 0.01005296\n",
      "Epoch 3 | Step 2149600 | Avg Loss: 0.0158 | Grad Norm: 0.00879828\n",
      "Epoch 3 | Step 2149700 | Avg Loss: 0.0157 | Grad Norm: 0.00936020\n",
      "Epoch 3 | Step 2149800 | Avg Loss: 0.0158 | Grad Norm: 0.00865997\n",
      "Epoch 3 | Step 2149900 | Avg Loss: 0.0159 | Grad Norm: 0.00984691\n",
      "Epoch 3 | Step 2150000 | Avg Loss: 0.0160 | Grad Norm: 0.00969766\n",
      "Epoch 3 | Step 2150100 | Avg Loss: 0.0162 | Grad Norm: 0.01132660\n",
      "Epoch 3 | Step 2150200 | Avg Loss: 0.0159 | Grad Norm: 0.00911647\n",
      "Epoch 3 | Step 2150300 | Avg Loss: 0.0160 | Grad Norm: 0.00874050\n",
      "Epoch 3 | Step 2150400 | Avg Loss: 0.0160 | Grad Norm: 0.00792631\n",
      "Epoch 3 | Step 2150500 | Avg Loss: 0.0159 | Grad Norm: 0.00952776\n",
      "Epoch 3 | Step 2150600 | Avg Loss: 0.0160 | Grad Norm: 0.00968399\n",
      "Epoch 3 | Step 2150700 | Avg Loss: 0.0158 | Grad Norm: 0.00816481\n",
      "Epoch 3 | Step 2150800 | Avg Loss: 0.0155 | Grad Norm: 0.00860929\n",
      "Epoch 3 | Step 2150900 | Avg Loss: 0.0154 | Grad Norm: 0.01000907\n",
      "Epoch 3 | Step 2151000 | Avg Loss: 0.0156 | Grad Norm: 0.01141389\n",
      "Epoch 3 | Step 2151100 | Avg Loss: 0.0155 | Grad Norm: 0.00903875\n",
      "Epoch 3 | Step 2151200 | Avg Loss: 0.0156 | Grad Norm: 0.00830658\n",
      "Epoch 3 | Step 2151300 | Avg Loss: 0.0156 | Grad Norm: 0.00832802\n",
      "Epoch 3 | Step 2151400 | Avg Loss: 0.0154 | Grad Norm: 0.01150013\n",
      "Epoch 3 | Step 2151500 | Avg Loss: 0.0154 | Grad Norm: 0.00982095\n",
      "Epoch 3 | Step 2151600 | Avg Loss: 0.0154 | Grad Norm: 0.00904626\n",
      "Epoch 3 | Step 2151700 | Avg Loss: 0.0151 | Grad Norm: 0.00887163\n",
      "Epoch 3 | Step 2151800 | Avg Loss: 0.0153 | Grad Norm: 0.00988382\n",
      "Epoch 3 | Step 2151900 | Avg Loss: 0.0153 | Grad Norm: 0.00794529\n",
      "Epoch 3 | Step 2152000 | Avg Loss: 0.0156 | Grad Norm: 0.00909851\n",
      "Epoch 3 | Step 2152100 | Avg Loss: 0.0152 | Grad Norm: 0.00967280\n",
      "Epoch 3 | Step 2152200 | Avg Loss: 0.0156 | Grad Norm: 0.00859410\n",
      "Epoch 3 | Step 2152300 | Avg Loss: 0.0156 | Grad Norm: 0.00860567\n",
      "Epoch 3 | Step 2152400 | Avg Loss: 0.0154 | Grad Norm: 0.01061544\n",
      "Epoch 3 | Step 2152500 | Avg Loss: 0.0152 | Grad Norm: 0.00908136\n",
      "Epoch 3 | Step 2152600 | Avg Loss: 0.0156 | Grad Norm: 0.00998619\n",
      "Epoch 3 | Step 2152700 | Avg Loss: 0.0153 | Grad Norm: 0.00930550\n",
      "Epoch 3 | Step 2152800 | Avg Loss: 0.0152 | Grad Norm: 0.00829505\n",
      "Epoch 3 | Step 2152900 | Avg Loss: 0.0154 | Grad Norm: 0.00976235\n",
      "Epoch 3 | Step 2153000 | Avg Loss: 0.0155 | Grad Norm: 0.00956464\n",
      "Epoch 3 | Step 2153100 | Avg Loss: 0.0152 | Grad Norm: 0.01193657\n",
      "Epoch 3 | Step 2153200 | Avg Loss: 0.0151 | Grad Norm: 0.00942691\n",
      "Epoch 3 | Step 2153300 | Avg Loss: 0.0152 | Grad Norm: 0.00827245\n",
      "Epoch 3 | Step 2153400 | Avg Loss: 0.0154 | Grad Norm: 0.00978233\n",
      "Epoch 3 | Step 2153500 | Avg Loss: 0.0153 | Grad Norm: 0.00782551\n",
      "Epoch 3 | Step 2153600 | Avg Loss: 0.0151 | Grad Norm: 0.00930856\n",
      "Epoch 3 | Step 2153700 | Avg Loss: 0.0154 | Grad Norm: 0.00941319\n",
      "Epoch 3 | Step 2153800 | Avg Loss: 0.0156 | Grad Norm: 0.00932503\n",
      "Epoch 3 | Step 2153900 | Avg Loss: 0.0157 | Grad Norm: 0.01059355\n",
      "Epoch 3 | Step 2154000 | Avg Loss: 0.0158 | Grad Norm: 0.00912928\n",
      "Epoch 3 | Step 2154100 | Avg Loss: 0.0161 | Grad Norm: 0.00800476\n",
      "Epoch 3 | Step 2154200 | Avg Loss: 0.0158 | Grad Norm: 0.02102026\n",
      "Epoch 3 | Step 2154300 | Avg Loss: 0.0159 | Grad Norm: 0.00993145\n",
      "Epoch 3 | Step 2154400 | Avg Loss: 0.0164 | Grad Norm: 0.00961269\n",
      "Epoch 3 | Step 2154500 | Avg Loss: 0.0162 | Grad Norm: 0.00974616\n",
      "Epoch 3 | Step 2154600 | Avg Loss: 0.0164 | Grad Norm: 0.00899506\n",
      "Epoch 3 | Step 2154700 | Avg Loss: 0.0160 | Grad Norm: 0.00996885\n",
      "Epoch 3 | Step 2154800 | Avg Loss: 0.0157 | Grad Norm: 0.01082028\n",
      "Epoch 3 | Step 2154900 | Avg Loss: 0.0155 | Grad Norm: 0.00788197\n",
      "Epoch 3 | Step 2155000 | Avg Loss: 0.0153 | Grad Norm: 0.00983194\n",
      "Epoch 3 | Step 2155100 | Avg Loss: 0.0155 | Grad Norm: 0.00847446\n",
      "Epoch 3 | Step 2155200 | Avg Loss: 0.0155 | Grad Norm: 0.00875076\n",
      "Epoch 3 | Step 2155300 | Avg Loss: 0.0152 | Grad Norm: 0.00805933\n",
      "Epoch 3 | Step 2155400 | Avg Loss: 0.0152 | Grad Norm: 0.00758204\n",
      "Epoch 3 | Step 2155500 | Avg Loss: 0.0152 | Grad Norm: 0.00796860\n",
      "Epoch 3 | Step 2155600 | Avg Loss: 0.0155 | Grad Norm: 0.00835572\n",
      "Epoch 3 | Step 2155700 | Avg Loss: 0.0153 | Grad Norm: 0.00857076\n",
      "Epoch 3 | Step 2155800 | Avg Loss: 0.0156 | Grad Norm: 0.00886972\n",
      "Epoch 3 | Step 2155900 | Avg Loss: 0.0154 | Grad Norm: 0.01128391\n",
      "Epoch 3 | Step 2156000 | Avg Loss: 0.0156 | Grad Norm: 0.00884882\n",
      "Epoch 3 | Step 2156100 | Avg Loss: 0.0156 | Grad Norm: 0.00834336\n",
      "Epoch 3 | Step 2156200 | Avg Loss: 0.0156 | Grad Norm: 0.00772626\n",
      "Epoch 3 | Step 2156300 | Avg Loss: 0.0160 | Grad Norm: 0.00907388\n",
      "Epoch 3 | Step 2156400 | Avg Loss: 0.0156 | Grad Norm: 0.00849935\n",
      "Epoch 3 | Step 2156500 | Avg Loss: 0.0154 | Grad Norm: 0.00959172\n",
      "Epoch 3 | Step 2156600 | Avg Loss: 0.0156 | Grad Norm: 0.00905964\n",
      "Epoch 3 | Step 2156700 | Avg Loss: 0.0156 | Grad Norm: 0.00867563\n",
      "Epoch 3 | Step 2156800 | Avg Loss: 0.0155 | Grad Norm: 0.00863751\n",
      "Epoch 3 | Step 2156900 | Avg Loss: 0.0156 | Grad Norm: 0.00870259\n",
      "Epoch 3 | Step 2157000 | Avg Loss: 0.0156 | Grad Norm: 0.00936471\n",
      "Epoch 3 | Step 2157100 | Avg Loss: 0.0155 | Grad Norm: 0.01054183\n",
      "Epoch 3 | Step 2157200 | Avg Loss: 0.0153 | Grad Norm: 0.00903890\n",
      "Epoch 3 | Step 2157300 | Avg Loss: 0.0154 | Grad Norm: 0.00889926\n",
      "Epoch 3 | Step 2157400 | Avg Loss: 0.0153 | Grad Norm: 0.00983840\n",
      "Epoch 3 | Step 2157500 | Avg Loss: 0.0153 | Grad Norm: 0.00824244\n",
      "Epoch 3 | Step 2157600 | Avg Loss: 0.0158 | Grad Norm: 0.00906136\n",
      "Epoch 3 | Step 2157700 | Avg Loss: 0.0156 | Grad Norm: 0.01052276\n",
      "Epoch 3 | Step 2157800 | Avg Loss: 0.0156 | Grad Norm: 0.00987535\n",
      "Epoch 3 | Step 2157900 | Avg Loss: 0.0158 | Grad Norm: 0.00815980\n",
      "Epoch 3 | Step 2158000 | Avg Loss: 0.0159 | Grad Norm: 0.00841819\n",
      "Epoch 3 | Step 2158100 | Avg Loss: 0.0157 | Grad Norm: 0.00960914\n",
      "Epoch 3 | Step 2158200 | Avg Loss: 0.0158 | Grad Norm: 0.01025661\n",
      "Epoch 3 | Step 2158300 | Avg Loss: 0.0157 | Grad Norm: 0.00948852\n",
      "Epoch 3 | Step 2158400 | Avg Loss: 0.0157 | Grad Norm: 0.00950991\n",
      "Epoch 3 | Step 2158500 | Avg Loss: 0.0160 | Grad Norm: 0.00852358\n",
      "Epoch 3 | Step 2158600 | Avg Loss: 0.0161 | Grad Norm: 0.01058914\n",
      "Epoch 3 | Step 2158700 | Avg Loss: 0.0161 | Grad Norm: 0.00938999\n",
      "Epoch 3 | Step 2158800 | Avg Loss: 0.0159 | Grad Norm: 0.00808468\n",
      "Epoch 3 | Step 2158900 | Avg Loss: 0.0159 | Grad Norm: 0.00894553\n",
      "Epoch 3 | Step 2159000 | Avg Loss: 0.0158 | Grad Norm: 0.00891269\n",
      "Epoch 3 | Step 2159100 | Avg Loss: 0.0157 | Grad Norm: 0.00804957\n",
      "Epoch 3 | Step 2159200 | Avg Loss: 0.0156 | Grad Norm: 0.00961749\n",
      "Epoch 3 | Step 2159300 | Avg Loss: 0.0157 | Grad Norm: 0.00843109\n",
      "Epoch 3 | Step 2159400 | Avg Loss: 0.0158 | Grad Norm: 0.00936102\n",
      "Epoch 3 | Step 2159500 | Avg Loss: 0.0159 | Grad Norm: 0.00772558\n",
      "Epoch 3 | Step 2159600 | Avg Loss: 0.0158 | Grad Norm: 0.00827328\n",
      "Epoch 3 | Step 2159700 | Avg Loss: 0.0158 | Grad Norm: 0.00944558\n",
      "Epoch 3 | Step 2159800 | Avg Loss: 0.0159 | Grad Norm: 0.00914854\n",
      "Epoch 3 | Step 2159900 | Avg Loss: 0.0154 | Grad Norm: 0.00991202\n",
      "Epoch 3 | Step 2160000 | Avg Loss: 0.0154 | Grad Norm: 0.00880535\n",
      "Epoch 3 | Step 2160100 | Avg Loss: 0.0156 | Grad Norm: 0.00841737\n",
      "Epoch 3 | Step 2160200 | Avg Loss: 0.0154 | Grad Norm: 0.00763359\n",
      "Epoch 3 | Step 2160300 | Avg Loss: 0.0153 | Grad Norm: 0.00825179\n",
      "Epoch 3 | Step 2160400 | Avg Loss: 0.0155 | Grad Norm: 0.00881237\n",
      "Epoch 3 | Step 2160500 | Avg Loss: 0.0154 | Grad Norm: 0.00881161\n",
      "Epoch 3 | Step 2160600 | Avg Loss: 0.0156 | Grad Norm: 0.00927720\n",
      "Epoch 3 | Step 2160700 | Avg Loss: 0.0157 | Grad Norm: 0.00788514\n",
      "Epoch 3 | Step 2160800 | Avg Loss: 0.0158 | Grad Norm: 0.00908809\n",
      "Epoch 3 | Step 2160900 | Avg Loss: 0.0157 | Grad Norm: 0.00962196\n",
      "Epoch 3 | Step 2161000 | Avg Loss: 0.0156 | Grad Norm: 0.00827616\n",
      "Epoch 3 | Step 2161100 | Avg Loss: 0.0151 | Grad Norm: 0.00822643\n",
      "Epoch 3 | Step 2161200 | Avg Loss: 0.0149 | Grad Norm: 0.00737193\n",
      "Epoch 3 | Step 2161300 | Avg Loss: 0.0150 | Grad Norm: 0.00786731\n",
      "Epoch 3 | Step 2161400 | Avg Loss: 0.0152 | Grad Norm: 0.00916716\n",
      "Epoch 3 | Step 2161500 | Avg Loss: 0.0152 | Grad Norm: 0.01339284\n",
      "Epoch 3 | Step 2161600 | Avg Loss: 0.0155 | Grad Norm: 0.00967300\n",
      "Epoch 3 | Step 2161700 | Avg Loss: 0.0155 | Grad Norm: 0.00822617\n",
      "Epoch 3 | Step 2161800 | Avg Loss: 0.0157 | Grad Norm: 0.00931223\n",
      "Epoch 3 | Step 2161900 | Avg Loss: 0.0157 | Grad Norm: 0.00894992\n",
      "Epoch 3 | Step 2162000 | Avg Loss: 0.0155 | Grad Norm: 0.00869972\n",
      "Epoch 3 | Step 2162100 | Avg Loss: 0.0153 | Grad Norm: 0.01293575\n",
      "Epoch 3 | Step 2162200 | Avg Loss: 0.0152 | Grad Norm: 0.00858775\n",
      "Epoch 3 | Step 2162300 | Avg Loss: 0.0152 | Grad Norm: 0.00910901\n",
      "Epoch 3 | Step 2162400 | Avg Loss: 0.0153 | Grad Norm: 0.00915136\n",
      "Epoch 3 | Step 2162500 | Avg Loss: 0.0154 | Grad Norm: 0.00894993\n",
      "Epoch 3 | Step 2162600 | Avg Loss: 0.0151 | Grad Norm: 0.00801406\n",
      "Epoch 3 | Step 2162700 | Avg Loss: 0.0153 | Grad Norm: 0.00857399\n",
      "Epoch 3 | Step 2162800 | Avg Loss: 0.0153 | Grad Norm: 0.00746695\n",
      "Epoch 3 | Step 2162900 | Avg Loss: 0.0151 | Grad Norm: 0.00867475\n",
      "Epoch 3 | Step 2163000 | Avg Loss: 0.0152 | Grad Norm: 0.00907157\n",
      "Epoch 3 | Step 2163100 | Avg Loss: 0.0156 | Grad Norm: 0.00788980\n",
      "Epoch 3 | Step 2163200 | Avg Loss: 0.0158 | Grad Norm: 0.00794566\n",
      "Epoch 3 | Step 2163300 | Avg Loss: 0.0158 | Grad Norm: 0.00871625\n",
      "Epoch 3 | Step 2163400 | Avg Loss: 0.0158 | Grad Norm: 0.00940638\n",
      "Epoch 3 | Step 2163500 | Avg Loss: 0.0159 | Grad Norm: 0.00854497\n",
      "Epoch 3 | Step 2163600 | Avg Loss: 0.0157 | Grad Norm: 0.00988739\n",
      "Epoch 3 | Step 2163700 | Avg Loss: 0.0158 | Grad Norm: 0.00950502\n",
      "Epoch 3 | Step 2163800 | Avg Loss: 0.0157 | Grad Norm: 0.00904278\n",
      "Epoch 3 | Step 2163900 | Avg Loss: 0.0154 | Grad Norm: 0.00896548\n",
      "Epoch 3 | Step 2164000 | Avg Loss: 0.0155 | Grad Norm: 0.00929973\n",
      "Epoch 3 | Step 2164100 | Avg Loss: 0.0155 | Grad Norm: 0.00852184\n",
      "Epoch 3 | Step 2164200 | Avg Loss: 0.0156 | Grad Norm: 0.01022600\n",
      "Epoch 3 | Step 2164300 | Avg Loss: 0.0158 | Grad Norm: 0.01018962\n",
      "Epoch 3 | Step 2164400 | Avg Loss: 0.0156 | Grad Norm: 0.00817619\n",
      "Epoch 3 | Step 2164500 | Avg Loss: 0.0155 | Grad Norm: 0.00895182\n",
      "Epoch 3 | Step 2164600 | Avg Loss: 0.0155 | Grad Norm: 0.00854454\n",
      "Epoch 3 | Step 2164700 | Avg Loss: 0.0155 | Grad Norm: 0.00791979\n",
      "Epoch 3 | Step 2164800 | Avg Loss: 0.0151 | Grad Norm: 0.00946894\n",
      "Epoch 3 | Step 2164900 | Avg Loss: 0.0154 | Grad Norm: 0.00856284\n",
      "Epoch 3 | Step 2165000 | Avg Loss: 0.0156 | Grad Norm: 0.00923564\n",
      "Epoch 3 | Step 2165100 | Avg Loss: 0.0159 | Grad Norm: 0.00964312\n",
      "Epoch 3 | Step 2165200 | Avg Loss: 0.0155 | Grad Norm: 0.00846770\n",
      "Epoch 3 | Step 2165300 | Avg Loss: 0.0153 | Grad Norm: 0.00822913\n",
      "Epoch 3 | Step 2165400 | Avg Loss: 0.0155 | Grad Norm: 0.00985290\n",
      "Epoch 3 | Step 2165500 | Avg Loss: 0.0153 | Grad Norm: 0.00966990\n",
      "Epoch 3 | Step 2165600 | Avg Loss: 0.0153 | Grad Norm: 0.00929222\n",
      "Epoch 3 | Step 2165700 | Avg Loss: 0.0155 | Grad Norm: 0.00888371\n",
      "Epoch 3 | Step 2165800 | Avg Loss: 0.0153 | Grad Norm: 0.00822570\n",
      "Epoch 3 | Step 2165900 | Avg Loss: 0.0152 | Grad Norm: 0.00761011\n",
      "Epoch 3 | Step 2166000 | Avg Loss: 0.0155 | Grad Norm: 0.01150086\n",
      "Epoch 3 | Step 2166100 | Avg Loss: 0.0155 | Grad Norm: 0.00815229\n",
      "Epoch 3 | Step 2166200 | Avg Loss: 0.0154 | Grad Norm: 0.00918566\n",
      "Epoch 3 | Step 2166300 | Avg Loss: 0.0153 | Grad Norm: 0.01102160\n",
      "Epoch 3 | Step 2166400 | Avg Loss: 0.0156 | Grad Norm: 0.00866274\n",
      "Epoch 3 | Step 2166500 | Avg Loss: 0.0157 | Grad Norm: 0.01053899\n",
      "Epoch 3 | Step 2166600 | Avg Loss: 0.0159 | Grad Norm: 0.01041754\n",
      "Epoch 3 | Step 2166700 | Avg Loss: 0.0155 | Grad Norm: 0.00965284\n",
      "Epoch 3 | Step 2166800 | Avg Loss: 0.0157 | Grad Norm: 0.00957481\n",
      "Epoch 3 | Step 2166900 | Avg Loss: 0.0155 | Grad Norm: 0.00945085\n",
      "Epoch 3 | Step 2167000 | Avg Loss: 0.0155 | Grad Norm: 0.00887221\n",
      "Epoch 3 | Step 2167100 | Avg Loss: 0.0158 | Grad Norm: 0.00909211\n",
      "Epoch 3 | Step 2167200 | Avg Loss: 0.0160 | Grad Norm: 0.00933925\n",
      "Epoch 3 | Step 2167300 | Avg Loss: 0.0156 | Grad Norm: 0.00919402\n",
      "Epoch 3 | Step 2167400 | Avg Loss: 0.0156 | Grad Norm: 0.00903279\n",
      "Epoch 3 | Step 2167500 | Avg Loss: 0.0158 | Grad Norm: 0.00982103\n",
      "Epoch 3 | Step 2167600 | Avg Loss: 0.0163 | Grad Norm: 0.00937200\n",
      "Epoch 3 | Step 2167700 | Avg Loss: 0.0159 | Grad Norm: 0.00883923\n",
      "Epoch 3 | Step 2167800 | Avg Loss: 0.0155 | Grad Norm: 0.00870990\n",
      "Epoch 3 | Step 2167900 | Avg Loss: 0.0156 | Grad Norm: 0.00946684\n",
      "Epoch 3 | Step 2168000 | Avg Loss: 0.0157 | Grad Norm: 0.01168131\n",
      "Epoch 3 | Step 2168100 | Avg Loss: 0.0157 | Grad Norm: 0.00826672\n",
      "Epoch 3 | Step 2168200 | Avg Loss: 0.0158 | Grad Norm: 0.00845298\n",
      "Epoch 3 | Step 2168300 | Avg Loss: 0.0155 | Grad Norm: 0.01122056\n",
      "Epoch 3 | Step 2168400 | Avg Loss: 0.0149 | Grad Norm: 0.00908871\n",
      "Epoch 3 | Step 2168500 | Avg Loss: 0.0147 | Grad Norm: 0.00865331\n",
      "Epoch 3 | Step 2168600 | Avg Loss: 0.0151 | Grad Norm: 0.00811823\n",
      "Epoch 3 | Step 2168700 | Avg Loss: 0.0153 | Grad Norm: 0.00872377\n",
      "Epoch 3 | Step 2168800 | Avg Loss: 0.0156 | Grad Norm: 0.00849917\n",
      "Epoch 3 | Step 2168900 | Avg Loss: 0.0157 | Grad Norm: 0.00929335\n",
      "Epoch 3 | Step 2169000 | Avg Loss: 0.0157 | Grad Norm: 0.00873607\n",
      "Epoch 3 | Step 2169100 | Avg Loss: 0.0158 | Grad Norm: 0.00859227\n",
      "Epoch 3 | Step 2169200 | Avg Loss: 0.0159 | Grad Norm: 0.00969806\n",
      "Epoch 3 | Step 2169300 | Avg Loss: 0.0156 | Grad Norm: 0.00990626\n",
      "Epoch 3 | Step 2169400 | Avg Loss: 0.0155 | Grad Norm: 0.00844142\n",
      "Epoch 3 | Step 2169500 | Avg Loss: 0.0155 | Grad Norm: 0.01044998\n",
      "Epoch 3 | Step 2169600 | Avg Loss: 0.0156 | Grad Norm: 0.00838806\n",
      "Epoch 3 | Step 2169700 | Avg Loss: 0.0155 | Grad Norm: 0.00914660\n",
      "Epoch 3 | Step 2169800 | Avg Loss: 0.0151 | Grad Norm: 0.00841284\n",
      "Epoch 3 | Step 2169900 | Avg Loss: 0.0153 | Grad Norm: 0.00951257\n",
      "Epoch 3 | Step 2170000 | Avg Loss: 0.0155 | Grad Norm: 0.00951115\n",
      "Epoch 3 | Step 2170100 | Avg Loss: 0.0157 | Grad Norm: 0.00989995\n",
      "Epoch 3 | Step 2170200 | Avg Loss: 0.0156 | Grad Norm: 0.00997518\n",
      "Epoch 3 | Step 2170300 | Avg Loss: 0.0157 | Grad Norm: 0.00864982\n",
      "Epoch 3 | Step 2170400 | Avg Loss: 0.0154 | Grad Norm: 0.00989187\n",
      "Epoch 3 | Step 2170500 | Avg Loss: 0.0155 | Grad Norm: 0.00940195\n",
      "Epoch 3 | Step 2170600 | Avg Loss: 0.0157 | Grad Norm: 0.00785145\n",
      "Epoch 3 | Step 2170700 | Avg Loss: 0.0155 | Grad Norm: 0.00818224\n",
      "Epoch 3 | Step 2170800 | Avg Loss: 0.0155 | Grad Norm: 0.00917931\n",
      "Epoch 3 | Step 2170900 | Avg Loss: 0.0157 | Grad Norm: 0.00825168\n",
      "Epoch 3 | Step 2171000 | Avg Loss: 0.0154 | Grad Norm: 0.00887266\n",
      "Epoch 3 | Step 2171100 | Avg Loss: 0.0157 | Grad Norm: 0.01327816\n",
      "Epoch 3 | Step 2171200 | Avg Loss: 0.0160 | Grad Norm: 0.01106205\n",
      "Epoch 3 | Step 2171300 | Avg Loss: 0.0158 | Grad Norm: 0.01232425\n",
      "Epoch 3 | Step 2171400 | Avg Loss: 0.0159 | Grad Norm: 0.00872960\n",
      "Epoch 3 | Step 2171500 | Avg Loss: 0.0156 | Grad Norm: 0.00962630\n",
      "Epoch 3 | Step 2171600 | Avg Loss: 0.0154 | Grad Norm: 0.00944571\n",
      "Epoch 3 | Step 2171700 | Avg Loss: 0.0155 | Grad Norm: 0.00868729\n",
      "Epoch 3 | Step 2171800 | Avg Loss: 0.0154 | Grad Norm: 0.01053611\n",
      "Epoch 3 | Step 2171900 | Avg Loss: 0.0153 | Grad Norm: 0.00954084\n",
      "Epoch 3 | Step 2172000 | Avg Loss: 0.0153 | Grad Norm: 0.00879195\n",
      "Epoch 3 | Step 2172100 | Avg Loss: 0.0152 | Grad Norm: 0.01066615\n",
      "Epoch 3 | Step 2172200 | Avg Loss: 0.0155 | Grad Norm: 0.01026413\n",
      "Epoch 3 | Step 2172300 | Avg Loss: 0.0155 | Grad Norm: 0.00897554\n",
      "Epoch 3 | Step 2172400 | Avg Loss: 0.0155 | Grad Norm: 0.00756893\n",
      "Epoch 3 | Step 2172500 | Avg Loss: 0.0158 | Grad Norm: 0.00930560\n",
      "Epoch 3 | Step 2172600 | Avg Loss: 0.0159 | Grad Norm: 0.00867061\n",
      "Epoch 3 | Step 2172700 | Avg Loss: 0.0161 | Grad Norm: 0.00892705\n",
      "Epoch 3 | Step 2172800 | Avg Loss: 0.0158 | Grad Norm: 0.00927427\n",
      "Epoch 3 | Step 2172900 | Avg Loss: 0.0154 | Grad Norm: 0.00968509\n",
      "Epoch 3 | Step 2173000 | Avg Loss: 0.0154 | Grad Norm: 0.00877947\n",
      "Epoch 3 | Step 2173100 | Avg Loss: 0.0155 | Grad Norm: 0.00768144\n",
      "Epoch 3 | Step 2173200 | Avg Loss: 0.0156 | Grad Norm: 0.00823819\n",
      "Epoch 3 | Step 2173300 | Avg Loss: 0.0157 | Grad Norm: 0.00890708\n",
      "Epoch 3 | Step 2173400 | Avg Loss: 0.0156 | Grad Norm: 0.00860875\n",
      "Epoch 3 | Step 2173500 | Avg Loss: 0.0159 | Grad Norm: 0.00814621\n",
      "Epoch 3 | Step 2173600 | Avg Loss: 0.0156 | Grad Norm: 0.00876406\n",
      "Epoch 3 | Step 2173700 | Avg Loss: 0.0156 | Grad Norm: 0.00913709\n",
      "Epoch 3 | Step 2173800 | Avg Loss: 0.0152 | Grad Norm: 0.00826105\n",
      "Epoch 3 | Step 2173900 | Avg Loss: 0.0151 | Grad Norm: 0.00877628\n",
      "Epoch 3 | Step 2174000 | Avg Loss: 0.0156 | Grad Norm: 0.00953124\n",
      "Epoch 3 | Step 2174100 | Avg Loss: 0.0157 | Grad Norm: 0.01191878\n",
      "Epoch 3 | Step 2174200 | Avg Loss: 0.0161 | Grad Norm: 0.00989158\n",
      "Epoch 3 | Step 2174300 | Avg Loss: 0.0156 | Grad Norm: 0.01035980\n",
      "Epoch 3 | Step 2174400 | Avg Loss: 0.0158 | Grad Norm: 0.00823763\n",
      "Epoch 3 | Step 2174500 | Avg Loss: 0.0154 | Grad Norm: 0.00928210\n",
      "Epoch 3 | Step 2174600 | Avg Loss: 0.0159 | Grad Norm: 0.01036951\n",
      "Epoch 3 | Step 2174700 | Avg Loss: 0.0160 | Grad Norm: 0.00961600\n",
      "Epoch 3 | Step 2174800 | Avg Loss: 0.0156 | Grad Norm: 0.00888935\n",
      "Epoch 3 | Step 2174900 | Avg Loss: 0.0155 | Grad Norm: 0.00857625\n",
      "Epoch 3 | Step 2175000 | Avg Loss: 0.0154 | Grad Norm: 0.01010070\n",
      "Epoch 3 | Step 2175100 | Avg Loss: 0.0154 | Grad Norm: 0.00852615\n",
      "Epoch 3 | Step 2175200 | Avg Loss: 0.0156 | Grad Norm: 0.00876200\n",
      "Epoch 3 | Step 2175300 | Avg Loss: 0.0154 | Grad Norm: 0.00894751\n",
      "Epoch 3 | Step 2175400 | Avg Loss: 0.0155 | Grad Norm: 0.00840994\n",
      "Epoch 3 | Step 2175500 | Avg Loss: 0.0154 | Grad Norm: 0.00813980\n",
      "Epoch 3 | Step 2175600 | Avg Loss: 0.0159 | Grad Norm: 0.00802270\n",
      "Epoch 3 | Step 2175700 | Avg Loss: 0.0155 | Grad Norm: 0.00925532\n",
      "Epoch 3 | Step 2175800 | Avg Loss: 0.0160 | Grad Norm: 0.01208331\n",
      "Epoch 3 | Step 2175900 | Avg Loss: 0.0158 | Grad Norm: 0.00800684\n",
      "Epoch 3 | Step 2176000 | Avg Loss: 0.0157 | Grad Norm: 0.00861686\n",
      "Epoch 3 | Step 2176100 | Avg Loss: 0.0160 | Grad Norm: 0.00902275\n",
      "Epoch 3 | Step 2176200 | Avg Loss: 0.0162 | Grad Norm: 0.00819002\n",
      "Epoch 3 | Step 2176300 | Avg Loss: 0.0159 | Grad Norm: 0.01302806\n",
      "Epoch 3 | Step 2176400 | Avg Loss: 0.0160 | Grad Norm: 0.01258358\n",
      "Epoch 3 | Step 2176500 | Avg Loss: 0.0158 | Grad Norm: 0.01131095\n",
      "Epoch 3 | Step 2176600 | Avg Loss: 0.0158 | Grad Norm: 0.01099886\n",
      "Epoch 3 | Step 2176700 | Avg Loss: 0.0157 | Grad Norm: 0.00816065\n",
      "Epoch 3 | Step 2176800 | Avg Loss: 0.0157 | Grad Norm: 0.00865588\n",
      "Epoch 3 | Step 2176900 | Avg Loss: 0.0154 | Grad Norm: 0.01115798\n",
      "Epoch 3 | Step 2177000 | Avg Loss: 0.0157 | Grad Norm: 0.00985349\n",
      "Epoch 3 | Step 2177100 | Avg Loss: 0.0156 | Grad Norm: 0.00973466\n",
      "Epoch 3 | Step 2177200 | Avg Loss: 0.0156 | Grad Norm: 0.01000478\n",
      "Epoch 3 | Step 2177300 | Avg Loss: 0.0155 | Grad Norm: 0.00930185\n",
      "Epoch 3 | Step 2177400 | Avg Loss: 0.0157 | Grad Norm: 0.00936108\n",
      "Epoch 3 | Step 2177500 | Avg Loss: 0.0161 | Grad Norm: 0.00826196\n",
      "Epoch 3 | Step 2177600 | Avg Loss: 0.0158 | Grad Norm: 0.00883665\n",
      "Epoch 3 | Step 2177700 | Avg Loss: 0.0156 | Grad Norm: 0.00876431\n",
      "Epoch 3 | Step 2177800 | Avg Loss: 0.0157 | Grad Norm: 0.00882646\n",
      "Epoch 3 | Step 2177900 | Avg Loss: 0.0157 | Grad Norm: 0.00936273\n",
      "Epoch 3 | Step 2178000 | Avg Loss: 0.0156 | Grad Norm: 0.00816294\n",
      "Epoch 3 | Step 2178100 | Avg Loss: 0.0158 | Grad Norm: 0.00952217\n",
      "Epoch 3 | Step 2178200 | Avg Loss: 0.0157 | Grad Norm: 0.00827677\n",
      "Epoch 3 | Step 2178300 | Avg Loss: 0.0156 | Grad Norm: 0.00862635\n",
      "Epoch 3 | Step 2178400 | Avg Loss: 0.0159 | Grad Norm: 0.00898805\n",
      "Epoch 3 | Step 2178500 | Avg Loss: 0.0160 | Grad Norm: 0.00877430\n",
      "Epoch 3 | Step 2178600 | Avg Loss: 0.0159 | Grad Norm: 0.00863096\n",
      "Epoch 3 | Step 2178700 | Avg Loss: 0.0157 | Grad Norm: 0.00961903\n",
      "Epoch 3 | Step 2178800 | Avg Loss: 0.0159 | Grad Norm: 0.00996748\n",
      "Epoch 3 | Step 2178900 | Avg Loss: 0.0159 | Grad Norm: 0.01026129\n",
      "Epoch 3 | Step 2179000 | Avg Loss: 0.0159 | Grad Norm: 0.00796272\n",
      "Epoch 3 | Step 2179100 | Avg Loss: 0.0160 | Grad Norm: 0.00939438\n",
      "Epoch 3 | Step 2179200 | Avg Loss: 0.0157 | Grad Norm: 0.00845362\n",
      "Epoch 3 | Step 2179300 | Avg Loss: 0.0160 | Grad Norm: 0.00899059\n",
      "Epoch 3 | Step 2179400 | Avg Loss: 0.0159 | Grad Norm: 0.01006441\n",
      "Epoch 3 | Step 2179500 | Avg Loss: 0.0160 | Grad Norm: 0.00959701\n",
      "Epoch 3 | Step 2179600 | Avg Loss: 0.0157 | Grad Norm: 0.00917598\n",
      "Epoch 3 | Step 2179700 | Avg Loss: 0.0156 | Grad Norm: 0.00774805\n",
      "Epoch 3 | Step 2179800 | Avg Loss: 0.0154 | Grad Norm: 0.00992132\n",
      "Epoch 3 | Step 2179900 | Avg Loss: 0.0159 | Grad Norm: 0.00903263\n",
      "Epoch 3 | Step 2180000 | Avg Loss: 0.0157 | Grad Norm: 0.00920951\n",
      "Epoch 3 | Step 2180100 | Avg Loss: 0.0158 | Grad Norm: 0.01194574\n",
      "Epoch 3 | Step 2180200 | Avg Loss: 0.0158 | Grad Norm: 0.00886858\n",
      "Epoch 3 | Step 2180300 | Avg Loss: 0.0158 | Grad Norm: 0.00890679\n",
      "Epoch 3 | Step 2180400 | Avg Loss: 0.0160 | Grad Norm: 0.00929058\n",
      "Epoch 3 | Step 2180500 | Avg Loss: 0.0158 | Grad Norm: 0.00881262\n",
      "Epoch 3 | Step 2180600 | Avg Loss: 0.0158 | Grad Norm: 0.01002661\n",
      "Epoch 3 | Step 2180700 | Avg Loss: 0.0156 | Grad Norm: 0.00912287\n",
      "Epoch 3 | Step 2180800 | Avg Loss: 0.0153 | Grad Norm: 0.00999270\n",
      "Epoch 3 | Step 2180900 | Avg Loss: 0.0154 | Grad Norm: 0.00845436\n",
      "Epoch 3 | Step 2181000 | Avg Loss: 0.0152 | Grad Norm: 0.00936448\n",
      "Epoch 3 | Step 2181100 | Avg Loss: 0.0152 | Grad Norm: 0.00956137\n",
      "Epoch 3 | Step 2181200 | Avg Loss: 0.0154 | Grad Norm: 0.00874995\n",
      "Epoch 3 | Step 2181300 | Avg Loss: 0.0154 | Grad Norm: 0.01006362\n",
      "Epoch 3 | Step 2181400 | Avg Loss: 0.0154 | Grad Norm: 0.00848913\n",
      "Epoch 3 | Step 2181500 | Avg Loss: 0.0154 | Grad Norm: 0.00929335\n",
      "Epoch 3 | Step 2181600 | Avg Loss: 0.0153 | Grad Norm: 0.01176175\n",
      "Epoch 3 | Step 2181700 | Avg Loss: 0.0155 | Grad Norm: 0.00872508\n",
      "Epoch 3 | Step 2181800 | Avg Loss: 0.0157 | Grad Norm: 0.00836875\n",
      "Epoch 3 | Step 2181900 | Avg Loss: 0.0158 | Grad Norm: 0.00867968\n",
      "Epoch 3 | Step 2182000 | Avg Loss: 0.0159 | Grad Norm: 0.00817403\n",
      "Epoch 3 | Step 2182100 | Avg Loss: 0.0160 | Grad Norm: 0.00990188\n",
      "Epoch 3 | Step 2182200 | Avg Loss: 0.0159 | Grad Norm: 0.00843141\n",
      "Epoch 3 | Step 2182300 | Avg Loss: 0.0158 | Grad Norm: 0.00842824\n",
      "Epoch 3 | Step 2182400 | Avg Loss: 0.0157 | Grad Norm: 0.00904754\n",
      "Epoch 3 | Step 2182500 | Avg Loss: 0.0159 | Grad Norm: 0.00870884\n",
      "Epoch 3 | Step 2182600 | Avg Loss: 0.0158 | Grad Norm: 0.00875357\n",
      "Epoch 3 | Step 2182700 | Avg Loss: 0.0156 | Grad Norm: 0.00887479\n",
      "Epoch 3 | Step 2182800 | Avg Loss: 0.0155 | Grad Norm: 0.00808815\n",
      "Epoch 3 | Step 2182900 | Avg Loss: 0.0159 | Grad Norm: 0.00904547\n",
      "Epoch 3 | Step 2183000 | Avg Loss: 0.0158 | Grad Norm: 0.00992948\n",
      "Epoch 3 | Step 2183100 | Avg Loss: 0.0156 | Grad Norm: 0.00884185\n",
      "Epoch 3 | Step 2183200 | Avg Loss: 0.0158 | Grad Norm: 0.00904566\n",
      "Epoch 3 | Step 2183300 | Avg Loss: 0.0158 | Grad Norm: 0.00842842\n",
      "Epoch 3 | Step 2183400 | Avg Loss: 0.0156 | Grad Norm: 0.00829141\n",
      "Epoch 3 | Step 2183500 | Avg Loss: 0.0157 | Grad Norm: 0.00880782\n",
      "Epoch 3 | Step 2183600 | Avg Loss: 0.0158 | Grad Norm: 0.00941183\n",
      "Epoch 3 | Step 2183700 | Avg Loss: 0.0153 | Grad Norm: 0.00850236\n",
      "Epoch 3 | Step 2183800 | Avg Loss: 0.0153 | Grad Norm: 0.00945431\n",
      "Epoch 3 | Step 2183900 | Avg Loss: 0.0150 | Grad Norm: 0.00866109\n",
      "Epoch 3 | Step 2184000 | Avg Loss: 0.0152 | Grad Norm: 0.00810255\n",
      "Epoch 3 | Step 2184100 | Avg Loss: 0.0152 | Grad Norm: 0.00880954\n",
      "Epoch 3 | Step 2184200 | Avg Loss: 0.0156 | Grad Norm: 0.00918147\n",
      "Epoch 3 | Step 2184300 | Avg Loss: 0.0154 | Grad Norm: 0.00794185\n",
      "Epoch 3 | Step 2184400 | Avg Loss: 0.0156 | Grad Norm: 0.00860780\n",
      "Epoch 3 | Step 2184500 | Avg Loss: 0.0154 | Grad Norm: 0.00734807\n",
      "Epoch 3 | Step 2184600 | Avg Loss: 0.0153 | Grad Norm: 0.00820613\n",
      "Epoch 3 | Step 2184700 | Avg Loss: 0.0156 | Grad Norm: 0.00970569\n",
      "Epoch 3 | Step 2184800 | Avg Loss: 0.0155 | Grad Norm: 0.00787767\n",
      "Epoch 3 | Step 2184900 | Avg Loss: 0.0152 | Grad Norm: 0.00782455\n",
      "Epoch 3 | Step 2185000 | Avg Loss: 0.0156 | Grad Norm: 0.01006731\n",
      "Epoch 3 | Step 2185100 | Avg Loss: 0.0157 | Grad Norm: 0.01187474\n",
      "Epoch 3 | Step 2185200 | Avg Loss: 0.0154 | Grad Norm: 0.00797094\n",
      "Epoch 3 | Step 2185300 | Avg Loss: 0.0153 | Grad Norm: 0.00817302\n",
      "Epoch 3 | Step 2185400 | Avg Loss: 0.0152 | Grad Norm: 0.00787865\n",
      "Epoch 3 | Step 2185500 | Avg Loss: 0.0151 | Grad Norm: 0.00762057\n",
      "Epoch 3 | Step 2185600 | Avg Loss: 0.0151 | Grad Norm: 0.00885289\n",
      "Epoch 3 | Step 2185700 | Avg Loss: 0.0149 | Grad Norm: 0.00852867\n",
      "Epoch 3 | Step 2185800 | Avg Loss: 0.0155 | Grad Norm: 0.00926682\n",
      "Epoch 3 | Step 2185900 | Avg Loss: 0.0150 | Grad Norm: 0.00982682\n",
      "Epoch 3 | Step 2186000 | Avg Loss: 0.0149 | Grad Norm: 0.00819590\n",
      "Epoch 3 | Step 2186100 | Avg Loss: 0.0148 | Grad Norm: 0.00858658\n",
      "Epoch 3 | Step 2186200 | Avg Loss: 0.0150 | Grad Norm: 0.00852572\n",
      "Epoch 3 | Step 2186300 | Avg Loss: 0.0155 | Grad Norm: 0.01053224\n",
      "Epoch 3 | Step 2186400 | Avg Loss: 0.0156 | Grad Norm: 0.00852661\n",
      "Epoch 3 | Step 2186500 | Avg Loss: 0.0153 | Grad Norm: 0.01002879\n",
      "Epoch 3 | Step 2186600 | Avg Loss: 0.0150 | Grad Norm: 0.01605642\n",
      "Epoch 3 | Step 2186700 | Avg Loss: 0.0153 | Grad Norm: 0.00913538\n",
      "Epoch 3 | Step 2186800 | Avg Loss: 0.0157 | Grad Norm: 0.00788874\n",
      "Epoch 3 | Step 2186900 | Avg Loss: 0.0155 | Grad Norm: 0.00894370\n",
      "Epoch 3 | Step 2187000 | Avg Loss: 0.0156 | Grad Norm: 0.01048010\n",
      "Epoch 3 | Step 2187100 | Avg Loss: 0.0154 | Grad Norm: 0.00787459\n",
      "Epoch 3 | Step 2187200 | Avg Loss: 0.0155 | Grad Norm: 0.00835106\n",
      "Epoch 3 | Step 2187300 | Avg Loss: 0.0155 | Grad Norm: 0.00893242\n",
      "Epoch 3 | Step 2187400 | Avg Loss: 0.0156 | Grad Norm: 0.00940139\n",
      "Epoch 3 | Step 2187500 | Avg Loss: 0.0155 | Grad Norm: 0.00915559\n",
      "Epoch 3 | Step 2187600 | Avg Loss: 0.0151 | Grad Norm: 0.00808639\n",
      "Epoch 3 | Step 2187700 | Avg Loss: 0.0148 | Grad Norm: 0.00909403\n",
      "Epoch 3 | Step 2187800 | Avg Loss: 0.0146 | Grad Norm: 0.00936903\n",
      "Epoch 3 | Step 2187900 | Avg Loss: 0.0148 | Grad Norm: 0.00839808\n",
      "Epoch 3 | Step 2188000 | Avg Loss: 0.0150 | Grad Norm: 0.00875010\n",
      "Epoch 3 | Step 2188100 | Avg Loss: 0.0156 | Grad Norm: 0.01001749\n",
      "Epoch 3 | Step 2188200 | Avg Loss: 0.0151 | Grad Norm: 0.00885426\n",
      "Epoch 3 | Step 2188300 | Avg Loss: 0.0153 | Grad Norm: 0.00817161\n",
      "Epoch 3 | Step 2188400 | Avg Loss: 0.0148 | Grad Norm: 0.00819314\n",
      "Epoch 3 | Step 2188500 | Avg Loss: 0.0153 | Grad Norm: 0.00920443\n",
      "Epoch 3 | Step 2188600 | Avg Loss: 0.0154 | Grad Norm: 0.00836871\n",
      "Epoch 3 | Step 2188700 | Avg Loss: 0.0151 | Grad Norm: 0.00906082\n",
      "Epoch 3 | Step 2188800 | Avg Loss: 0.0151 | Grad Norm: 0.00950130\n",
      "Epoch 3 | Step 2188900 | Avg Loss: 0.0154 | Grad Norm: 0.00993171\n",
      "Epoch 3 | Step 2189000 | Avg Loss: 0.0157 | Grad Norm: 0.00861996\n",
      "Epoch 3 | Step 2189100 | Avg Loss: 0.0156 | Grad Norm: 0.00814561\n",
      "Epoch 3 | Step 2189200 | Avg Loss: 0.0157 | Grad Norm: 0.00825569\n",
      "Epoch 3 | Step 2189300 | Avg Loss: 0.0157 | Grad Norm: 0.00916622\n",
      "Epoch 3 | Step 2189400 | Avg Loss: 0.0157 | Grad Norm: 0.00829991\n",
      "Epoch 3 | Step 2189500 | Avg Loss: 0.0156 | Grad Norm: 0.00756191\n",
      "Epoch 3 | Step 2189600 | Avg Loss: 0.0154 | Grad Norm: 0.01027160\n",
      "Epoch 3 | Step 2189700 | Avg Loss: 0.0152 | Grad Norm: 0.00884883\n",
      "Epoch 3 | Step 2189800 | Avg Loss: 0.0153 | Grad Norm: 0.00864668\n",
      "Epoch 3 | Step 2189900 | Avg Loss: 0.0155 | Grad Norm: 0.01041516\n",
      "Epoch 3 | Step 2190000 | Avg Loss: 0.0153 | Grad Norm: 0.00893254\n",
      "Epoch 3 | Step 2190100 | Avg Loss: 0.0152 | Grad Norm: 0.01037537\n",
      "Epoch 3 | Step 2190200 | Avg Loss: 0.0154 | Grad Norm: 0.00892252\n",
      "Epoch 3 | Step 2190300 | Avg Loss: 0.0155 | Grad Norm: 0.00935775\n",
      "Epoch 3 | Step 2190400 | Avg Loss: 0.0158 | Grad Norm: 0.00780984\n",
      "Epoch 3 | Step 2190500 | Avg Loss: 0.0158 | Grad Norm: 0.00872391\n",
      "Epoch 3 | Step 2190600 | Avg Loss: 0.0152 | Grad Norm: 0.00791886\n",
      "Epoch 3 | Step 2190700 | Avg Loss: 0.0152 | Grad Norm: 0.00937322\n",
      "Epoch 3 | Step 2190800 | Avg Loss: 0.0151 | Grad Norm: 0.00888896\n",
      "Epoch 3 | Step 2190900 | Avg Loss: 0.0149 | Grad Norm: 0.00875531\n",
      "Epoch 3 | Step 2191000 | Avg Loss: 0.0154 | Grad Norm: 0.01023344\n",
      "Epoch 3 | Step 2191100 | Avg Loss: 0.0156 | Grad Norm: 0.00993857\n",
      "Epoch 3 | Step 2191200 | Avg Loss: 0.0156 | Grad Norm: 0.00949287\n",
      "Epoch 3 | Step 2191300 | Avg Loss: 0.0156 | Grad Norm: 0.00967768\n",
      "Epoch 3 | Step 2191400 | Avg Loss: 0.0154 | Grad Norm: 0.01062624\n",
      "Epoch 3 | Step 2191500 | Avg Loss: 0.0154 | Grad Norm: 0.01209052\n",
      "Epoch 3 | Step 2191600 | Avg Loss: 0.0151 | Grad Norm: 0.01049347\n",
      "Epoch 3 | Step 2191700 | Avg Loss: 0.0155 | Grad Norm: 0.00862927\n",
      "Epoch 3 | Step 2191800 | Avg Loss: 0.0154 | Grad Norm: 0.00821505\n",
      "Epoch 3 | Step 2191900 | Avg Loss: 0.0154 | Grad Norm: 0.00854575\n",
      "Epoch 3 | Step 2192000 | Avg Loss: 0.0157 | Grad Norm: 0.00852342\n",
      "Epoch 3 | Step 2192100 | Avg Loss: 0.0155 | Grad Norm: 0.00913094\n",
      "Epoch 3 | Step 2192200 | Avg Loss: 0.0160 | Grad Norm: 0.00979327\n",
      "Epoch 3 | Step 2192300 | Avg Loss: 0.0161 | Grad Norm: 0.00926777\n",
      "Epoch 3 | Step 2192400 | Avg Loss: 0.0160 | Grad Norm: 0.00896173\n",
      "Epoch 3 | Step 2192500 | Avg Loss: 0.0156 | Grad Norm: 0.00771644\n",
      "Epoch 3 | Step 2192600 | Avg Loss: 0.0156 | Grad Norm: 0.01312648\n",
      "Epoch 3 | Step 2192700 | Avg Loss: 0.0155 | Grad Norm: 0.00819142\n",
      "Epoch 3 | Step 2192800 | Avg Loss: 0.0157 | Grad Norm: 0.01395413\n",
      "Epoch 3 | Step 2192900 | Avg Loss: 0.0157 | Grad Norm: 0.00932325\n",
      "Epoch 3 | Step 2193000 | Avg Loss: 0.0159 | Grad Norm: 0.01013478\n",
      "Epoch 3 | Step 2193100 | Avg Loss: 0.0156 | Grad Norm: 0.00936943\n",
      "Epoch 3 | Step 2193200 | Avg Loss: 0.0158 | Grad Norm: 0.01010553\n",
      "Epoch 3 | Step 2193300 | Avg Loss: 0.0156 | Grad Norm: 0.00848783\n",
      "Epoch 3 | Step 2193400 | Avg Loss: 0.0157 | Grad Norm: 0.01223583\n",
      "Epoch 3 | Step 2193500 | Avg Loss: 0.0160 | Grad Norm: 0.00925496\n",
      "Epoch 3 | Step 2193600 | Avg Loss: 0.0161 | Grad Norm: 0.00839646\n",
      "Epoch 3 | Step 2193700 | Avg Loss: 0.0158 | Grad Norm: 0.00861003\n",
      "Epoch 3 | Step 2193800 | Avg Loss: 0.0159 | Grad Norm: 0.00866526\n",
      "Epoch 3 | Step 2193900 | Avg Loss: 0.0161 | Grad Norm: 0.00933856\n",
      "Epoch 3 | Step 2194000 | Avg Loss: 0.0160 | Grad Norm: 0.00970156\n",
      "Epoch 3 | Step 2194100 | Avg Loss: 0.0161 | Grad Norm: 0.00886253\n",
      "Epoch 3 | Step 2194200 | Avg Loss: 0.0159 | Grad Norm: 0.00985872\n",
      "Epoch 3 | Step 2194300 | Avg Loss: 0.0157 | Grad Norm: 0.00783095\n",
      "Epoch 3 | Step 2194400 | Avg Loss: 0.0159 | Grad Norm: 0.01077065\n",
      "Epoch 3 | Step 2194500 | Avg Loss: 0.0155 | Grad Norm: 0.01178582\n",
      "Epoch 3 | Step 2194600 | Avg Loss: 0.0156 | Grad Norm: 0.00841710\n",
      "Epoch 3 | Step 2194700 | Avg Loss: 0.0154 | Grad Norm: 0.00904302\n",
      "Epoch 3 | Step 2194800 | Avg Loss: 0.0153 | Grad Norm: 0.00808758\n",
      "Epoch 3 | Step 2194900 | Avg Loss: 0.0153 | Grad Norm: 0.00914296\n",
      "Epoch 3 | Step 2195000 | Avg Loss: 0.0150 | Grad Norm: 0.00925004\n",
      "Epoch 3 | Step 2195100 | Avg Loss: 0.0154 | Grad Norm: 0.00825012\n",
      "Epoch 3 | Step 2195200 | Avg Loss: 0.0156 | Grad Norm: 0.01027362\n",
      "Epoch 3 | Step 2195300 | Avg Loss: 0.0157 | Grad Norm: 0.00881584\n",
      "Epoch 3 | Step 2195400 | Avg Loss: 0.0157 | Grad Norm: 0.00989949\n",
      "Epoch 3 | Step 2195500 | Avg Loss: 0.0156 | Grad Norm: 0.01071093\n",
      "Epoch 3 | Step 2195600 | Avg Loss: 0.0153 | Grad Norm: 0.00853325\n",
      "Epoch 3 | Step 2195700 | Avg Loss: 0.0151 | Grad Norm: 0.00816628\n",
      "Epoch 3 | Step 2195800 | Avg Loss: 0.0154 | Grad Norm: 0.00880486\n",
      "Epoch 3 | Step 2195900 | Avg Loss: 0.0151 | Grad Norm: 0.00806631\n",
      "Epoch 3 | Step 2196000 | Avg Loss: 0.0158 | Grad Norm: 0.00973634\n",
      "Epoch 3 | Step 2196100 | Avg Loss: 0.0155 | Grad Norm: 0.00918541\n",
      "Epoch 3 | Step 2196200 | Avg Loss: 0.0151 | Grad Norm: 0.00961088\n",
      "Epoch 3 | Step 2196300 | Avg Loss: 0.0153 | Grad Norm: 0.00932428\n",
      "Epoch 3 | Step 2196400 | Avg Loss: 0.0153 | Grad Norm: 0.01143120\n",
      "Epoch 3 | Step 2196500 | Avg Loss: 0.0153 | Grad Norm: 0.00968202\n",
      "Epoch 3 | Step 2196600 | Avg Loss: 0.0154 | Grad Norm: 0.00949709\n",
      "Epoch 3 | Step 2196700 | Avg Loss: 0.0155 | Grad Norm: 0.00851830\n",
      "Epoch 3 | Step 2196800 | Avg Loss: 0.0152 | Grad Norm: 0.00988984\n",
      "Epoch 3 | Step 2196900 | Avg Loss: 0.0156 | Grad Norm: 0.01456804\n",
      "Epoch 3 | Step 2197000 | Avg Loss: 0.0157 | Grad Norm: 0.00842293\n",
      "Epoch 3 | Step 2197100 | Avg Loss: 0.0155 | Grad Norm: 0.00942338\n",
      "Epoch 3 | Step 2197200 | Avg Loss: 0.0157 | Grad Norm: 0.00921339\n",
      "Epoch 3 | Step 2197300 | Avg Loss: 0.0157 | Grad Norm: 0.01009625\n",
      "Epoch 3 | Step 2197400 | Avg Loss: 0.0160 | Grad Norm: 0.00931747\n",
      "Epoch 3 | Step 2197500 | Avg Loss: 0.0160 | Grad Norm: 0.01142939\n",
      "Epoch 3 | Step 2197600 | Avg Loss: 0.0158 | Grad Norm: 0.00857364\n",
      "Epoch 3 | Step 2197700 | Avg Loss: 0.0160 | Grad Norm: 0.00894583\n",
      "Epoch 3 | Step 2197800 | Avg Loss: 0.0162 | Grad Norm: 0.00900065\n",
      "Epoch 3 | Step 2197900 | Avg Loss: 0.0155 | Grad Norm: 0.00934459\n",
      "Epoch 3 | Step 2198000 | Avg Loss: 0.0157 | Grad Norm: 0.00996459\n",
      "Epoch 3 | Step 2198100 | Avg Loss: 0.0161 | Grad Norm: 0.01082463\n",
      "Epoch 3 | Step 2198200 | Avg Loss: 0.0159 | Grad Norm: 0.01046709\n",
      "Epoch 3 | Step 2198300 | Avg Loss: 0.0159 | Grad Norm: 0.00932236\n",
      "Epoch 3 | Step 2198400 | Avg Loss: 0.0159 | Grad Norm: 0.00959251\n",
      "Epoch 3 | Step 2198500 | Avg Loss: 0.0161 | Grad Norm: 0.01170621\n",
      "Epoch 3 | Step 2198600 | Avg Loss: 0.0159 | Grad Norm: 0.01055962\n",
      "Epoch 3 | Step 2198700 | Avg Loss: 0.0160 | Grad Norm: 0.00905591\n",
      "Epoch 3 | Step 2198800 | Avg Loss: 0.0160 | Grad Norm: 0.00902037\n",
      "Epoch 3 | Step 2198900 | Avg Loss: 0.0159 | Grad Norm: 0.00880652\n",
      "Epoch 3 | Step 2199000 | Avg Loss: 0.0158 | Grad Norm: 0.00901212\n",
      "Epoch 3 | Step 2199100 | Avg Loss: 0.0155 | Grad Norm: 0.01044131\n",
      "Epoch 3 | Step 2199200 | Avg Loss: 0.0153 | Grad Norm: 0.00818376\n",
      "Epoch 3 | Step 2199300 | Avg Loss: 0.0153 | Grad Norm: 0.00829599\n",
      "Epoch 3 | Step 2199400 | Avg Loss: 0.0153 | Grad Norm: 0.01118370\n",
      "Epoch 3 | Step 2199500 | Avg Loss: 0.0153 | Grad Norm: 0.00748534\n",
      "Epoch 3 | Step 2199600 | Avg Loss: 0.0154 | Grad Norm: 0.00851188\n",
      "Epoch 3 | Step 2199700 | Avg Loss: 0.0155 | Grad Norm: 0.00896747\n",
      "Epoch 3 | Step 2199800 | Avg Loss: 0.0154 | Grad Norm: 0.00833991\n",
      "Epoch 3 | Step 2199900 | Avg Loss: 0.0157 | Grad Norm: 0.01054697\n",
      "Epoch 3 | Step 2200000 | Avg Loss: 0.0155 | Grad Norm: 0.00972201\n",
      "Saving model at step2200000\n",
      "Epoch 3 | Step 2200100 | Avg Loss: 0.0154 | Grad Norm: 0.00909446\n",
      "Epoch 3 | Step 2200200 | Avg Loss: 0.0159 | Grad Norm: 0.00827911\n",
      "Epoch 3 | Step 2200300 | Avg Loss: 0.0157 | Grad Norm: 0.00879253\n",
      "Epoch 3 | Step 2200400 | Avg Loss: 0.0159 | Grad Norm: 0.01055842\n",
      "Epoch 3 | Step 2200500 | Avg Loss: 0.0158 | Grad Norm: 0.00882895\n",
      "Epoch 3 | Step 2200600 | Avg Loss: 0.0160 | Grad Norm: 0.00977489\n",
      "Epoch 3 | Step 2200700 | Avg Loss: 0.0156 | Grad Norm: 0.00893366\n",
      "Epoch 3 | Step 2200800 | Avg Loss: 0.0156 | Grad Norm: 0.00825555\n",
      "Epoch 3 | Step 2200900 | Avg Loss: 0.0157 | Grad Norm: 0.00797232\n",
      "Epoch 3 | Step 2201000 | Avg Loss: 0.0157 | Grad Norm: 0.00901772\n",
      "Epoch 3 | Step 2201100 | Avg Loss: 0.0156 | Grad Norm: 0.00954158\n",
      "Epoch 3 | Step 2201200 | Avg Loss: 0.0153 | Grad Norm: 0.00848276\n",
      "Epoch 3 | Step 2201300 | Avg Loss: 0.0153 | Grad Norm: 0.00867756\n",
      "Epoch 3 | Step 2201400 | Avg Loss: 0.0154 | Grad Norm: 0.00965497\n",
      "Epoch 3 | Step 2201500 | Avg Loss: 0.0156 | Grad Norm: 0.00878706\n",
      "Epoch 3 | Step 2201600 | Avg Loss: 0.0153 | Grad Norm: 0.00802831\n",
      "Epoch 3 | Step 2201700 | Avg Loss: 0.0157 | Grad Norm: 0.00909011\n",
      "Epoch 3 | Step 2201800 | Avg Loss: 0.0155 | Grad Norm: 0.00847881\n",
      "Epoch 3 | Step 2201900 | Avg Loss: 0.0158 | Grad Norm: 0.00834310\n",
      "Epoch 3 | Step 2202000 | Avg Loss: 0.0158 | Grad Norm: 0.00775944\n",
      "Epoch 3 | Step 2202100 | Avg Loss: 0.0160 | Grad Norm: 0.00919914\n",
      "Epoch 3 | Step 2202200 | Avg Loss: 0.0155 | Grad Norm: 0.01177317\n",
      "Epoch 3 | Step 2202300 | Avg Loss: 0.0160 | Grad Norm: 0.00925167\n",
      "Epoch 3 | Step 2202400 | Avg Loss: 0.0156 | Grad Norm: 0.00922589\n",
      "Epoch 3 | Step 2202500 | Avg Loss: 0.0161 | Grad Norm: 0.00816188\n",
      "Epoch 3 | Step 2202600 | Avg Loss: 0.0159 | Grad Norm: 0.00767045\n",
      "Epoch 3 | Step 2202700 | Avg Loss: 0.0155 | Grad Norm: 0.00891247\n",
      "Epoch 3 | Step 2202800 | Avg Loss: 0.0154 | Grad Norm: 0.00858279\n",
      "Epoch 3 | Step 2202900 | Avg Loss: 0.0152 | Grad Norm: 0.00982220\n",
      "Epoch 3 | Step 2203000 | Avg Loss: 0.0154 | Grad Norm: 0.00902103\n",
      "Epoch 3 | Step 2203100 | Avg Loss: 0.0155 | Grad Norm: 0.00885611\n",
      "Epoch 3 | Step 2203200 | Avg Loss: 0.0152 | Grad Norm: 0.00951754\n",
      "Epoch 3 | Step 2203300 | Avg Loss: 0.0150 | Grad Norm: 0.00877944\n",
      "Epoch 3 | Step 2203400 | Avg Loss: 0.0149 | Grad Norm: 0.00843210\n",
      "Epoch 3 | Step 2203500 | Avg Loss: 0.0153 | Grad Norm: 0.00912991\n",
      "Epoch 3 | Step 2203600 | Avg Loss: 0.0156 | Grad Norm: 0.00812526\n",
      "Epoch 3 | Step 2203700 | Avg Loss: 0.0153 | Grad Norm: 0.00788381\n",
      "Epoch 3 | Step 2203800 | Avg Loss: 0.0152 | Grad Norm: 0.01127730\n",
      "Epoch 3 | Step 2203900 | Avg Loss: 0.0157 | Grad Norm: 0.00997799\n",
      "Epoch 3 | Step 2204000 | Avg Loss: 0.0157 | Grad Norm: 0.00902966\n",
      "Epoch 3 | Step 2204100 | Avg Loss: 0.0154 | Grad Norm: 0.01062848\n",
      "Epoch 3 | Step 2204200 | Avg Loss: 0.0149 | Grad Norm: 0.00851730\n",
      "Epoch 3 | Step 2204300 | Avg Loss: 0.0150 | Grad Norm: 0.00850882\n",
      "Epoch 3 | Step 2204400 | Avg Loss: 0.0153 | Grad Norm: 0.00900152\n",
      "Epoch 3 | Step 2204500 | Avg Loss: 0.0154 | Grad Norm: 0.00894073\n",
      "Epoch 3 | Step 2204600 | Avg Loss: 0.0157 | Grad Norm: 0.00909668\n",
      "Epoch 3 | Step 2204700 | Avg Loss: 0.0154 | Grad Norm: 0.00771293\n",
      "Epoch 3 | Step 2204800 | Avg Loss: 0.0156 | Grad Norm: 0.00834145\n",
      "Epoch 3 | Step 2204900 | Avg Loss: 0.0155 | Grad Norm: 0.00864248\n",
      "Epoch 3 | Step 2205000 | Avg Loss: 0.0155 | Grad Norm: 0.00917154\n",
      "Epoch 3 | Step 2205100 | Avg Loss: 0.0158 | Grad Norm: 0.00826716\n",
      "Epoch 3 | Step 2205200 | Avg Loss: 0.0155 | Grad Norm: 0.00908444\n",
      "Epoch 3 | Step 2205300 | Avg Loss: 0.0157 | Grad Norm: 0.00781313\n",
      "Epoch 3 | Step 2205400 | Avg Loss: 0.0154 | Grad Norm: 0.00870247\n",
      "Epoch 3 | Step 2205500 | Avg Loss: 0.0155 | Grad Norm: 0.00903586\n",
      "Epoch 3 | Step 2205600 | Avg Loss: 0.0153 | Grad Norm: 0.00749792\n",
      "Epoch 3 | Step 2205700 | Avg Loss: 0.0154 | Grad Norm: 0.00839765\n",
      "Epoch 3 | Step 2205800 | Avg Loss: 0.0153 | Grad Norm: 0.00944553\n",
      "Epoch 3 | Step 2205900 | Avg Loss: 0.0154 | Grad Norm: 0.00903685\n",
      "Epoch 3 | Step 2206000 | Avg Loss: 0.0155 | Grad Norm: 0.00806894\n",
      "Epoch 3 | Step 2206100 | Avg Loss: 0.0158 | Grad Norm: 0.00903510\n",
      "Epoch 3 | Step 2206200 | Avg Loss: 0.0157 | Grad Norm: 0.00901661\n",
      "Epoch 3 | Step 2206300 | Avg Loss: 0.0155 | Grad Norm: 0.00878746\n",
      "Epoch 3 | Step 2206400 | Avg Loss: 0.0157 | Grad Norm: 0.00882251\n",
      "Epoch 3 | Step 2206500 | Avg Loss: 0.0155 | Grad Norm: 0.00930814\n",
      "Epoch 3 | Step 2206600 | Avg Loss: 0.0156 | Grad Norm: 0.00838199\n",
      "Epoch 3 | Step 2206700 | Avg Loss: 0.0156 | Grad Norm: 0.00876981\n",
      "Epoch 3 | Step 2206800 | Avg Loss: 0.0154 | Grad Norm: 0.01037224\n",
      "Epoch 3 | Step 2206900 | Avg Loss: 0.0155 | Grad Norm: 0.00874396\n",
      "Epoch 3 | Step 2207000 | Avg Loss: 0.0156 | Grad Norm: 0.00811111\n",
      "Epoch 3 | Step 2207100 | Avg Loss: 0.0156 | Grad Norm: 0.00939627\n",
      "Epoch 3 | Step 2207200 | Avg Loss: 0.0157 | Grad Norm: 0.00915303\n",
      "Epoch 3 | Step 2207300 | Avg Loss: 0.0156 | Grad Norm: 0.00828589\n",
      "Epoch 3 | Step 2207400 | Avg Loss: 0.0156 | Grad Norm: 0.00992743\n",
      "Epoch 3 | Step 2207500 | Avg Loss: 0.0155 | Grad Norm: 0.00920550\n",
      "Epoch 3 | Step 2207600 | Avg Loss: 0.0155 | Grad Norm: 0.00866795\n",
      "Epoch 3 | Step 2207700 | Avg Loss: 0.0152 | Grad Norm: 0.00919659\n",
      "Epoch 3 | Step 2207800 | Avg Loss: 0.0153 | Grad Norm: 0.01020812\n",
      "Epoch 3 | Step 2207900 | Avg Loss: 0.0155 | Grad Norm: 0.00844426\n",
      "Epoch 3 | Step 2208000 | Avg Loss: 0.0152 | Grad Norm: 0.00954632\n",
      "Epoch 3 | Step 2208100 | Avg Loss: 0.0152 | Grad Norm: 0.00934218\n",
      "Epoch 3 | Step 2208200 | Avg Loss: 0.0153 | Grad Norm: 0.00821471\n",
      "Epoch 3 | Step 2208300 | Avg Loss: 0.0154 | Grad Norm: 0.00892045\n",
      "Epoch 3 | Step 2208400 | Avg Loss: 0.0151 | Grad Norm: 0.01288362\n",
      "Epoch 3 | Step 2208500 | Avg Loss: 0.0151 | Grad Norm: 0.00986104\n",
      "Epoch 3 | Step 2208600 | Avg Loss: 0.0152 | Grad Norm: 0.01063785\n",
      "Epoch 3 | Step 2208700 | Avg Loss: 0.0152 | Grad Norm: 0.00837055\n",
      "Epoch 3 | Step 2208800 | Avg Loss: 0.0150 | Grad Norm: 0.00802058\n",
      "Epoch 3 | Step 2208900 | Avg Loss: 0.0152 | Grad Norm: 0.00913657\n",
      "Epoch 3 | Step 2209000 | Avg Loss: 0.0151 | Grad Norm: 0.01008632\n",
      "Epoch 3 | Step 2209100 | Avg Loss: 0.0155 | Grad Norm: 0.01134729\n",
      "Epoch 3 | Step 2209200 | Avg Loss: 0.0154 | Grad Norm: 0.00799109\n",
      "Epoch 3 | Step 2209300 | Avg Loss: 0.0152 | Grad Norm: 0.01196423\n",
      "Epoch 3 | Step 2209400 | Avg Loss: 0.0148 | Grad Norm: 0.00917376\n",
      "Epoch 3 | Step 2209500 | Avg Loss: 0.0155 | Grad Norm: 0.00913971\n",
      "Epoch 3 | Step 2209600 | Avg Loss: 0.0157 | Grad Norm: 0.00869696\n",
      "Epoch 3 | Step 2209700 | Avg Loss: 0.0156 | Grad Norm: 0.01071974\n",
      "Epoch 3 | Step 2209800 | Avg Loss: 0.0155 | Grad Norm: 0.00824609\n",
      "Epoch 3 | Step 2209900 | Avg Loss: 0.0155 | Grad Norm: 0.00827159\n",
      "Epoch 3 | Step 2210000 | Avg Loss: 0.0154 | Grad Norm: 0.00799895\n",
      "Epoch 3 | Step 2210100 | Avg Loss: 0.0152 | Grad Norm: 0.00885702\n",
      "Epoch 3 | Step 2210200 | Avg Loss: 0.0149 | Grad Norm: 0.00889386\n",
      "Epoch 3 | Step 2210300 | Avg Loss: 0.0153 | Grad Norm: 0.00891422\n",
      "Epoch 3 | Step 2210400 | Avg Loss: 0.0154 | Grad Norm: 0.00894374\n",
      "Epoch 3 | Step 2210500 | Avg Loss: 0.0155 | Grad Norm: 0.00863856\n",
      "Epoch 3 | Step 2210600 | Avg Loss: 0.0155 | Grad Norm: 0.01302986\n",
      "Epoch 3 | Step 2210700 | Avg Loss: 0.0156 | Grad Norm: 0.00988489\n",
      "Epoch 3 | Step 2210800 | Avg Loss: 0.0156 | Grad Norm: 0.00864908\n",
      "Epoch 3 | Step 2210900 | Avg Loss: 0.0155 | Grad Norm: 0.00943219\n",
      "Epoch 3 | Step 2211000 | Avg Loss: 0.0157 | Grad Norm: 0.00854128\n",
      "Epoch 3 | Step 2211100 | Avg Loss: 0.0155 | Grad Norm: 0.00973190\n",
      "Epoch 3 | Step 2211200 | Avg Loss: 0.0157 | Grad Norm: 0.00953064\n",
      "Epoch 3 | Step 2211300 | Avg Loss: 0.0153 | Grad Norm: 0.00831526\n",
      "Epoch 3 | Step 2211400 | Avg Loss: 0.0156 | Grad Norm: 0.00891846\n",
      "Epoch 3 | Step 2211500 | Avg Loss: 0.0157 | Grad Norm: 0.00930604\n",
      "Epoch 3 | Step 2211600 | Avg Loss: 0.0161 | Grad Norm: 0.00861875\n",
      "Epoch 3 | Step 2211700 | Avg Loss: 0.0161 | Grad Norm: 0.00989346\n",
      "Epoch 3 | Step 2211800 | Avg Loss: 0.0158 | Grad Norm: 0.00885035\n",
      "Epoch 3 | Step 2211900 | Avg Loss: 0.0159 | Grad Norm: 0.01002569\n",
      "Epoch 3 | Step 2212000 | Avg Loss: 0.0162 | Grad Norm: 0.00895425\n",
      "Epoch 3 | Step 2212100 | Avg Loss: 0.0160 | Grad Norm: 0.00820527\n",
      "Epoch 3 | Step 2212200 | Avg Loss: 0.0161 | Grad Norm: 0.01462919\n",
      "Epoch 3 | Step 2212300 | Avg Loss: 0.0157 | Grad Norm: 0.00918090\n",
      "Epoch 3 | Step 2212400 | Avg Loss: 0.0153 | Grad Norm: 0.00977884\n",
      "Epoch 3 | Step 2212500 | Avg Loss: 0.0157 | Grad Norm: 0.00991395\n",
      "Epoch 3 | Step 2212600 | Avg Loss: 0.0154 | Grad Norm: 0.00854585\n",
      "Epoch 3 | Step 2212700 | Avg Loss: 0.0152 | Grad Norm: 0.00844911\n",
      "Epoch 3 | Step 2212800 | Avg Loss: 0.0156 | Grad Norm: 0.01087669\n",
      "Epoch 3 | Step 2212900 | Avg Loss: 0.0156 | Grad Norm: 0.01008797\n",
      "Epoch 3 | Step 2213000 | Avg Loss: 0.0160 | Grad Norm: 0.00949682\n",
      "Epoch 3 | Step 2213100 | Avg Loss: 0.0162 | Grad Norm: 0.01028941\n",
      "Epoch 3 | Step 2213200 | Avg Loss: 0.0163 | Grad Norm: 0.01402809\n",
      "Epoch 3 | Step 2213300 | Avg Loss: 0.0162 | Grad Norm: 0.00959697\n",
      "Epoch 3 | Step 2213400 | Avg Loss: 0.0162 | Grad Norm: 0.00949554\n",
      "Epoch 3 | Step 2213500 | Avg Loss: 0.0161 | Grad Norm: 0.00817224\n",
      "Epoch 3 | Step 2213600 | Avg Loss: 0.0164 | Grad Norm: 0.01142070\n",
      "Epoch 3 | Step 2213700 | Avg Loss: 0.0164 | Grad Norm: 0.01199037\n",
      "Epoch 3 | Step 2213800 | Avg Loss: 0.0160 | Grad Norm: 0.01087831\n",
      "Epoch 3 | Step 2213900 | Avg Loss: 0.0157 | Grad Norm: 0.00933891\n",
      "Epoch 3 | Step 2214000 | Avg Loss: 0.0158 | Grad Norm: 0.00924234\n",
      "Epoch 3 | Step 2214100 | Avg Loss: 0.0156 | Grad Norm: 0.00812693\n",
      "Epoch 3 | Step 2214200 | Avg Loss: 0.0155 | Grad Norm: 0.00964938\n",
      "Epoch 3 | Step 2214300 | Avg Loss: 0.0154 | Grad Norm: 0.00877021\n",
      "Epoch 3 | Step 2214400 | Avg Loss: 0.0152 | Grad Norm: 0.00893507\n",
      "Epoch 3 | Step 2214500 | Avg Loss: 0.0148 | Grad Norm: 0.00834087\n",
      "Epoch 3 | Step 2214600 | Avg Loss: 0.0151 | Grad Norm: 0.00776939\n",
      "Epoch 3 | Step 2214700 | Avg Loss: 0.0151 | Grad Norm: 0.00908283\n",
      "Epoch 3 | Step 2214800 | Avg Loss: 0.0152 | Grad Norm: 0.00798398\n",
      "Epoch 3 | Step 2214900 | Avg Loss: 0.0155 | Grad Norm: 0.00992564\n",
      "Epoch 3 | Step 2215000 | Avg Loss: 0.0158 | Grad Norm: 0.00929912\n",
      "Epoch 3 | Step 2215100 | Avg Loss: 0.0154 | Grad Norm: 0.00898294\n",
      "Epoch 3 | Step 2215200 | Avg Loss: 0.0156 | Grad Norm: 0.00939492\n",
      "Epoch 3 | Step 2215300 | Avg Loss: 0.0157 | Grad Norm: 0.00993327\n",
      "Epoch 3 | Step 2215400 | Avg Loss: 0.0155 | Grad Norm: 0.01042138\n",
      "Epoch 3 | Step 2215500 | Avg Loss: 0.0155 | Grad Norm: 0.00797865\n",
      "Epoch 3 | Step 2215600 | Avg Loss: 0.0156 | Grad Norm: 0.00841321\n",
      "Epoch 3 | Step 2215700 | Avg Loss: 0.0159 | Grad Norm: 0.00939245\n",
      "Epoch 3 | Step 2215800 | Avg Loss: 0.0155 | Grad Norm: 0.00834712\n",
      "Epoch 3 | Step 2215900 | Avg Loss: 0.0159 | Grad Norm: 0.00876524\n",
      "Epoch 3 | Step 2216000 | Avg Loss: 0.0156 | Grad Norm: 0.00811724\n",
      "Epoch 3 | Step 2216100 | Avg Loss: 0.0154 | Grad Norm: 0.01031114\n",
      "Epoch 3 | Step 2216200 | Avg Loss: 0.0155 | Grad Norm: 0.01012742\n",
      "Epoch 3 | Step 2216300 | Avg Loss: 0.0157 | Grad Norm: 0.00998235\n",
      "Epoch 3 | Step 2216400 | Avg Loss: 0.0158 | Grad Norm: 0.00952730\n",
      "Epoch 3 | Step 2216500 | Avg Loss: 0.0151 | Grad Norm: 0.00799735\n",
      "Epoch 3 | Step 2216600 | Avg Loss: 0.0152 | Grad Norm: 0.00901908\n",
      "Epoch 3 | Step 2216700 | Avg Loss: 0.0151 | Grad Norm: 0.00805535\n",
      "Epoch 3 | Step 2216800 | Avg Loss: 0.0151 | Grad Norm: 0.01215764\n",
      "Epoch 3 | Step 2216900 | Avg Loss: 0.0154 | Grad Norm: 0.01036913\n",
      "Epoch 3 | Step 2217000 | Avg Loss: 0.0154 | Grad Norm: 0.00885040\n",
      "Epoch 3 | Step 2217100 | Avg Loss: 0.0160 | Grad Norm: 0.00845647\n",
      "Epoch 3 | Step 2217200 | Avg Loss: 0.0159 | Grad Norm: 0.00872301\n",
      "Epoch 3 | Step 2217300 | Avg Loss: 0.0158 | Grad Norm: 0.01111574\n",
      "Epoch 3 | Step 2217400 | Avg Loss: 0.0159 | Grad Norm: 0.00826615\n",
      "Epoch 3 | Step 2217500 | Avg Loss: 0.0156 | Grad Norm: 0.00880634\n",
      "Epoch 3 | Step 2217600 | Avg Loss: 0.0156 | Grad Norm: 0.00974696\n",
      "Epoch 3 | Step 2217700 | Avg Loss: 0.0157 | Grad Norm: 0.00875858\n",
      "Epoch 3 | Step 2217800 | Avg Loss: 0.0157 | Grad Norm: 0.00824714\n",
      "Epoch 3 | Step 2217900 | Avg Loss: 0.0163 | Grad Norm: 0.01123515\n",
      "Epoch 3 | Step 2218000 | Avg Loss: 0.0161 | Grad Norm: 0.00959774\n",
      "Epoch 3 | Step 2218100 | Avg Loss: 0.0156 | Grad Norm: 0.00829847\n",
      "Epoch 3 | Step 2218200 | Avg Loss: 0.0154 | Grad Norm: 0.00843006\n",
      "Epoch 3 | Step 2218300 | Avg Loss: 0.0157 | Grad Norm: 0.00849142\n",
      "Epoch 3 | Step 2218400 | Avg Loss: 0.0158 | Grad Norm: 0.01131690\n",
      "Epoch 3 | Step 2218500 | Avg Loss: 0.0158 | Grad Norm: 0.00854145\n",
      "Epoch 3 | Step 2218600 | Avg Loss: 0.0158 | Grad Norm: 0.00944607\n",
      "Epoch 3 | Step 2218700 | Avg Loss: 0.0155 | Grad Norm: 0.00831192\n",
      "Epoch 3 | Step 2218800 | Avg Loss: 0.0155 | Grad Norm: 0.00920234\n",
      "Epoch 3 | Step 2218900 | Avg Loss: 0.0154 | Grad Norm: 0.00935846\n",
      "Epoch 3 | Step 2219000 | Avg Loss: 0.0149 | Grad Norm: 0.00941185\n",
      "Epoch 3 | Step 2219100 | Avg Loss: 0.0153 | Grad Norm: 0.00797115\n",
      "Epoch 3 | Step 2219200 | Avg Loss: 0.0157 | Grad Norm: 0.00972382\n",
      "Epoch 3 | Step 2219300 | Avg Loss: 0.0161 | Grad Norm: 0.00961114\n",
      "Epoch 3 | Step 2219400 | Avg Loss: 0.0160 | Grad Norm: 0.00959544\n",
      "Epoch 3 | Step 2219500 | Avg Loss: 0.0156 | Grad Norm: 0.00933502\n",
      "Epoch 3 | Step 2219600 | Avg Loss: 0.0156 | Grad Norm: 0.00944975\n",
      "Epoch 3 | Step 2219700 | Avg Loss: 0.0159 | Grad Norm: 0.00902618\n",
      "Epoch 3 | Step 2219800 | Avg Loss: 0.0159 | Grad Norm: 0.00999976\n",
      "Epoch 3 | Step 2219900 | Avg Loss: 0.0155 | Grad Norm: 0.00840809\n",
      "Epoch 3 | Step 2220000 | Avg Loss: 0.0155 | Grad Norm: 0.00993057\n",
      "Epoch 3 | Step 2220100 | Avg Loss: 0.0158 | Grad Norm: 0.00830044\n",
      "Epoch 3 | Step 2220200 | Avg Loss: 0.0155 | Grad Norm: 0.00830715\n",
      "Epoch 3 | Step 2220300 | Avg Loss: 0.0157 | Grad Norm: 0.00956036\n",
      "Epoch 3 | Step 2220400 | Avg Loss: 0.0156 | Grad Norm: 0.00879933\n",
      "Epoch 3 | Step 2220500 | Avg Loss: 0.0152 | Grad Norm: 0.00890288\n",
      "Epoch 3 | Step 2220600 | Avg Loss: 0.0154 | Grad Norm: 0.01041370\n",
      "Epoch 3 | Step 2220700 | Avg Loss: 0.0154 | Grad Norm: 0.00991752\n",
      "Epoch 3 | Step 2220800 | Avg Loss: 0.0149 | Grad Norm: 0.00849428\n",
      "Epoch 3 | Step 2220900 | Avg Loss: 0.0149 | Grad Norm: 0.00829105\n",
      "Epoch 3 | Step 2221000 | Avg Loss: 0.0146 | Grad Norm: 0.00836825\n",
      "Epoch 3 | Step 2221100 | Avg Loss: 0.0150 | Grad Norm: 0.00919424\n",
      "Epoch 3 | Step 2221200 | Avg Loss: 0.0150 | Grad Norm: 0.00910462\n",
      "Epoch 3 | Step 2221300 | Avg Loss: 0.0151 | Grad Norm: 0.01042334\n",
      "Epoch 3 | Step 2221400 | Avg Loss: 0.0150 | Grad Norm: 0.00864671\n",
      "Epoch 3 | Step 2221500 | Avg Loss: 0.0153 | Grad Norm: 0.01029722\n",
      "Epoch 3 | Step 2221600 | Avg Loss: 0.0157 | Grad Norm: 0.00872865\n",
      "Epoch 3 | Step 2221700 | Avg Loss: 0.0157 | Grad Norm: 0.00895529\n",
      "Epoch 3 | Step 2221800 | Avg Loss: 0.0164 | Grad Norm: 0.01336695\n",
      "Epoch 3 | Step 2221900 | Avg Loss: 0.0163 | Grad Norm: 0.00817692\n",
      "Epoch 3 | Step 2222000 | Avg Loss: 0.0159 | Grad Norm: 0.00956527\n",
      "Epoch 3 | Step 2222100 | Avg Loss: 0.0158 | Grad Norm: 0.00833099\n",
      "Epoch 3 | Step 2222200 | Avg Loss: 0.0159 | Grad Norm: 0.01116081\n",
      "Epoch 3 | Step 2222300 | Avg Loss: 0.0158 | Grad Norm: 0.00851891\n",
      "Epoch 3 | Step 2222400 | Avg Loss: 0.0161 | Grad Norm: 0.00912253\n",
      "Epoch 3 | Step 2222500 | Avg Loss: 0.0158 | Grad Norm: 0.00847426\n",
      "Epoch 3 | Step 2222600 | Avg Loss: 0.0163 | Grad Norm: 0.00947247\n",
      "Epoch 3 | Step 2222700 | Avg Loss: 0.0161 | Grad Norm: 0.00982655\n",
      "Epoch 3 | Step 2222800 | Avg Loss: 0.0157 | Grad Norm: 0.00890504\n",
      "Epoch 3 | Step 2222900 | Avg Loss: 0.0154 | Grad Norm: 0.00881669\n",
      "Epoch 3 | Step 2223000 | Avg Loss: 0.0158 | Grad Norm: 0.00811545\n",
      "Epoch 3 | Step 2223100 | Avg Loss: 0.0159 | Grad Norm: 0.00897740\n",
      "Epoch 3 | Step 2223200 | Avg Loss: 0.0160 | Grad Norm: 0.00964134\n",
      "Epoch 3 | Step 2223300 | Avg Loss: 0.0159 | Grad Norm: 0.00981744\n",
      "Epoch 3 | Step 2223400 | Avg Loss: 0.0155 | Grad Norm: 0.00885798\n",
      "Epoch 3 | Step 2223500 | Avg Loss: 0.0159 | Grad Norm: 0.00882980\n",
      "Epoch 3 | Step 2223600 | Avg Loss: 0.0159 | Grad Norm: 0.01450771\n",
      "Epoch 3 | Step 2223700 | Avg Loss: 0.0160 | Grad Norm: 0.01048311\n",
      "Epoch 3 | Step 2223800 | Avg Loss: 0.0158 | Grad Norm: 0.01003683\n",
      "Epoch 3 | Step 2223900 | Avg Loss: 0.0157 | Grad Norm: 0.01090782\n",
      "Epoch 3 | Step 2224000 | Avg Loss: 0.0155 | Grad Norm: 0.00919797\n",
      "Epoch 3 | Step 2224100 | Avg Loss: 0.0154 | Grad Norm: 0.00848786\n",
      "Epoch 3 | Step 2224200 | Avg Loss: 0.0155 | Grad Norm: 0.00915642\n",
      "Epoch 3 | Step 2224300 | Avg Loss: 0.0152 | Grad Norm: 0.00909839\n",
      "Epoch 3 | Step 2224400 | Avg Loss: 0.0148 | Grad Norm: 0.00984259\n",
      "Epoch 3 | Step 2224500 | Avg Loss: 0.0151 | Grad Norm: 0.00939270\n",
      "Epoch 3 | Step 2224600 | Avg Loss: 0.0154 | Grad Norm: 0.00922889\n",
      "Epoch 3 | Step 2224700 | Avg Loss: 0.0155 | Grad Norm: 0.00883826\n",
      "Epoch 3 | Step 2224800 | Avg Loss: 0.0153 | Grad Norm: 0.00830986\n",
      "Epoch 3 | Step 2224900 | Avg Loss: 0.0154 | Grad Norm: 0.00880461\n",
      "Epoch 3 | Step 2225000 | Avg Loss: 0.0152 | Grad Norm: 0.00917574\n",
      "Epoch 3 | Step 2225100 | Avg Loss: 0.0152 | Grad Norm: 0.00932448\n",
      "Epoch 3 | Step 2225200 | Avg Loss: 0.0150 | Grad Norm: 0.00786143\n",
      "Epoch 3 | Step 2225300 | Avg Loss: 0.0150 | Grad Norm: 0.01051348\n",
      "Epoch 3 | Step 2225400 | Avg Loss: 0.0152 | Grad Norm: 0.00781776\n",
      "Epoch 3 | Step 2225500 | Avg Loss: 0.0152 | Grad Norm: 0.01036795\n",
      "Epoch 3 | Step 2225600 | Avg Loss: 0.0153 | Grad Norm: 0.00875047\n",
      "Epoch 3 | Step 2225700 | Avg Loss: 0.0154 | Grad Norm: 0.00874640\n",
      "Epoch 3 | Step 2225800 | Avg Loss: 0.0159 | Grad Norm: 0.00869924\n",
      "Epoch 3 | Step 2225900 | Avg Loss: 0.0156 | Grad Norm: 0.00890676\n",
      "Epoch 3 | Step 2226000 | Avg Loss: 0.0159 | Grad Norm: 0.01138051\n",
      "Epoch 3 | Step 2226100 | Avg Loss: 0.0161 | Grad Norm: 0.00980664\n",
      "Epoch 3 | Step 2226200 | Avg Loss: 0.0165 | Grad Norm: 0.00947498\n",
      "Epoch 3 | Step 2226300 | Avg Loss: 0.0162 | Grad Norm: 0.01149074\n",
      "Epoch 3 | Step 2226400 | Avg Loss: 0.0163 | Grad Norm: 0.00998445\n",
      "Epoch 3 | Step 2226500 | Avg Loss: 0.0164 | Grad Norm: 0.00949615\n",
      "Epoch 3 | Step 2226600 | Avg Loss: 0.0161 | Grad Norm: 0.01390887\n",
      "Epoch 3 | Step 2226700 | Avg Loss: 0.0152 | Grad Norm: 0.00927331\n",
      "Epoch 3 | Step 2226800 | Avg Loss: 0.0152 | Grad Norm: 0.00850650\n",
      "Epoch 3 | Step 2226900 | Avg Loss: 0.0149 | Grad Norm: 0.00886817\n",
      "Epoch 3 | Step 2227000 | Avg Loss: 0.0150 | Grad Norm: 0.00750675\n",
      "Epoch 3 | Step 2227100 | Avg Loss: 0.0147 | Grad Norm: 0.00979738\n",
      "Epoch 3 | Step 2227200 | Avg Loss: 0.0146 | Grad Norm: 0.01115795\n",
      "Epoch 3 | Step 2227300 | Avg Loss: 0.0148 | Grad Norm: 0.00913268\n",
      "Epoch 3 | Step 2227400 | Avg Loss: 0.0147 | Grad Norm: 0.00984207\n",
      "Epoch 3 | Step 2227500 | Avg Loss: 0.0148 | Grad Norm: 0.00813164\n",
      "Epoch 3 | Step 2227600 | Avg Loss: 0.0147 | Grad Norm: 0.00831398\n",
      "Epoch 3 | Step 2227700 | Avg Loss: 0.0150 | Grad Norm: 0.00846170\n",
      "Epoch 3 | Step 2227800 | Avg Loss: 0.0153 | Grad Norm: 0.00916203\n",
      "Epoch 3 | Step 2227900 | Avg Loss: 0.0154 | Grad Norm: 0.00815954\n",
      "Epoch 3 | Step 2228000 | Avg Loss: 0.0155 | Grad Norm: 0.00880563\n",
      "Epoch 3 | Step 2228100 | Avg Loss: 0.0156 | Grad Norm: 0.00812999\n",
      "Epoch 3 | Step 2228200 | Avg Loss: 0.0153 | Grad Norm: 0.00835995\n",
      "Epoch 3 | Step 2228300 | Avg Loss: 0.0159 | Grad Norm: 0.00832522\n",
      "Epoch 3 | Step 2228400 | Avg Loss: 0.0160 | Grad Norm: 0.00840077\n",
      "Epoch 3 | Step 2228500 | Avg Loss: 0.0156 | Grad Norm: 0.00892751\n",
      "Epoch 3 | Step 2228600 | Avg Loss: 0.0157 | Grad Norm: 0.00967608\n",
      "Epoch 3 | Step 2228700 | Avg Loss: 0.0154 | Grad Norm: 0.00910692\n",
      "Epoch 3 | Step 2228800 | Avg Loss: 0.0157 | Grad Norm: 0.01075293\n",
      "Epoch 3 | Step 2228900 | Avg Loss: 0.0158 | Grad Norm: 0.00801400\n",
      "Epoch 3 | Step 2229000 | Avg Loss: 0.0158 | Grad Norm: 0.00828060\n",
      "Epoch 3 | Step 2229100 | Avg Loss: 0.0154 | Grad Norm: 0.00976574\n",
      "Epoch 3 | Step 2229200 | Avg Loss: 0.0157 | Grad Norm: 0.00916540\n",
      "Epoch 3 | Step 2229300 | Avg Loss: 0.0156 | Grad Norm: 0.00924385\n",
      "Epoch 3 | Step 2229400 | Avg Loss: 0.0157 | Grad Norm: 0.00913620\n",
      "Epoch 3 | Step 2229500 | Avg Loss: 0.0160 | Grad Norm: 0.00917625\n",
      "Epoch 3 | Step 2229600 | Avg Loss: 0.0160 | Grad Norm: 0.00878415\n",
      "Epoch 3 | Step 2229700 | Avg Loss: 0.0158 | Grad Norm: 0.01257605\n",
      "Epoch 3 | Step 2229800 | Avg Loss: 0.0157 | Grad Norm: 0.01025497\n",
      "Epoch 3 | Step 2229900 | Avg Loss: 0.0156 | Grad Norm: 0.00876543\n",
      "Epoch 3 | Step 2230000 | Avg Loss: 0.0156 | Grad Norm: 0.00881148\n",
      "Epoch 3 | Step 2230100 | Avg Loss: 0.0156 | Grad Norm: 0.01040047\n",
      "Epoch 3 | Step 2230200 | Avg Loss: 0.0156 | Grad Norm: 0.00899965\n",
      "Epoch 3 | Step 2230300 | Avg Loss: 0.0157 | Grad Norm: 0.00928989\n",
      "Epoch 3 | Step 2230400 | Avg Loss: 0.0155 | Grad Norm: 0.01073234\n",
      "Epoch 3 | Step 2230500 | Avg Loss: 0.0153 | Grad Norm: 0.00891733\n",
      "Epoch 3 | Step 2230600 | Avg Loss: 0.0151 | Grad Norm: 0.00789206\n",
      "Epoch 3 | Step 2230700 | Avg Loss: 0.0148 | Grad Norm: 0.00861462\n",
      "Epoch 3 | Step 2230800 | Avg Loss: 0.0152 | Grad Norm: 0.01104782\n",
      "Epoch 3 | Step 2230900 | Avg Loss: 0.0153 | Grad Norm: 0.00840480\n",
      "Epoch 3 | Step 2231000 | Avg Loss: 0.0157 | Grad Norm: 0.01003401\n",
      "Epoch 3 | Step 2231100 | Avg Loss: 0.0156 | Grad Norm: 0.01167993\n",
      "Epoch 3 | Step 2231200 | Avg Loss: 0.0159 | Grad Norm: 0.00937333\n",
      "Epoch 3 | Step 2231300 | Avg Loss: 0.0157 | Grad Norm: 0.00841751\n",
      "Epoch 3 | Step 2231400 | Avg Loss: 0.0156 | Grad Norm: 0.00804249\n",
      "Epoch 3 | Step 2231500 | Avg Loss: 0.0160 | Grad Norm: 0.00834609\n",
      "Epoch 3 | Step 2231600 | Avg Loss: 0.0156 | Grad Norm: 0.00949811\n",
      "Epoch 3 | Step 2231700 | Avg Loss: 0.0155 | Grad Norm: 0.00813611\n",
      "Epoch 3 | Step 2231800 | Avg Loss: 0.0159 | Grad Norm: 0.00991323\n",
      "Epoch 3 | Step 2231900 | Avg Loss: 0.0155 | Grad Norm: 0.00924238\n",
      "Epoch 3 | Step 2232000 | Avg Loss: 0.0156 | Grad Norm: 0.00827664\n",
      "Epoch 3 | Step 2232100 | Avg Loss: 0.0160 | Grad Norm: 0.00805003\n",
      "Epoch 3 | Step 2232200 | Avg Loss: 0.0161 | Grad Norm: 0.00796667\n",
      "Epoch 3 | Step 2232300 | Avg Loss: 0.0160 | Grad Norm: 0.01040830\n",
      "Epoch 3 | Step 2232400 | Avg Loss: 0.0159 | Grad Norm: 0.01000288\n",
      "Epoch 3 | Step 2232500 | Avg Loss: 0.0159 | Grad Norm: 0.00918482\n",
      "Epoch 3 | Step 2232600 | Avg Loss: 0.0161 | Grad Norm: 0.00790793\n",
      "Epoch 3 | Step 2232700 | Avg Loss: 0.0156 | Grad Norm: 0.00875281\n",
      "Epoch 3 | Step 2232800 | Avg Loss: 0.0158 | Grad Norm: 0.00894202\n",
      "Epoch 3 | Step 2232900 | Avg Loss: 0.0159 | Grad Norm: 0.00927884\n",
      "Epoch 3 | Step 2233000 | Avg Loss: 0.0160 | Grad Norm: 0.00800842\n",
      "Epoch 3 | Step 2233100 | Avg Loss: 0.0158 | Grad Norm: 0.00920353\n",
      "Epoch 3 | Step 2233200 | Avg Loss: 0.0158 | Grad Norm: 0.00800239\n",
      "Epoch 3 | Step 2233300 | Avg Loss: 0.0158 | Grad Norm: 0.00776123\n",
      "Epoch 3 | Step 2233400 | Avg Loss: 0.0155 | Grad Norm: 0.00925438\n",
      "Epoch 3 | Step 2233500 | Avg Loss: 0.0152 | Grad Norm: 0.01081419\n",
      "Epoch 3 | Step 2233600 | Avg Loss: 0.0152 | Grad Norm: 0.00880616\n",
      "Epoch 3 | Step 2233700 | Avg Loss: 0.0152 | Grad Norm: 0.00887917\n",
      "Epoch 3 | Step 2233800 | Avg Loss: 0.0151 | Grad Norm: 0.00993038\n",
      "Epoch 3 | Step 2233900 | Avg Loss: 0.0151 | Grad Norm: 0.00963397\n",
      "Epoch 3 | Step 2234000 | Avg Loss: 0.0151 | Grad Norm: 0.00894869\n",
      "Epoch 3 | Step 2234100 | Avg Loss: 0.0152 | Grad Norm: 0.00878238\n",
      "Epoch 3 | Step 2234200 | Avg Loss: 0.0153 | Grad Norm: 0.00893776\n",
      "Epoch 3 | Step 2234300 | Avg Loss: 0.0154 | Grad Norm: 0.00864691\n",
      "Epoch 3 | Step 2234400 | Avg Loss: 0.0157 | Grad Norm: 0.00997745\n",
      "Epoch 3 | Step 2234500 | Avg Loss: 0.0159 | Grad Norm: 0.01024215\n",
      "Epoch 3 | Step 2234600 | Avg Loss: 0.0155 | Grad Norm: 0.00818592\n",
      "Epoch 3 | Step 2234700 | Avg Loss: 0.0160 | Grad Norm: 0.00984213\n",
      "Epoch 3 | Step 2234800 | Avg Loss: 0.0161 | Grad Norm: 0.00919475\n",
      "Epoch 3 | Step 2234900 | Avg Loss: 0.0157 | Grad Norm: 0.00983901\n",
      "Epoch 3 | Step 2235000 | Avg Loss: 0.0156 | Grad Norm: 0.01160944\n",
      "Epoch 3 | Step 2235100 | Avg Loss: 0.0155 | Grad Norm: 0.01017696\n",
      "Epoch 3 | Step 2235200 | Avg Loss: 0.0160 | Grad Norm: 0.00842027\n",
      "Epoch 3 | Step 2235300 | Avg Loss: 0.0158 | Grad Norm: 0.01157552\n",
      "Epoch 3 | Step 2235400 | Avg Loss: 0.0158 | Grad Norm: 0.00859571\n",
      "Epoch 3 | Step 2235500 | Avg Loss: 0.0160 | Grad Norm: 0.00997557\n",
      "Epoch 3 | Step 2235600 | Avg Loss: 0.0155 | Grad Norm: 0.01027020\n",
      "Epoch 3 | Step 2235700 | Avg Loss: 0.0157 | Grad Norm: 0.01024580\n",
      "Epoch 3 | Step 2235800 | Avg Loss: 0.0157 | Grad Norm: 0.00984996\n",
      "Epoch 3 | Step 2235900 | Avg Loss: 0.0153 | Grad Norm: 0.01174558\n",
      "Epoch 3 | Step 2236000 | Avg Loss: 0.0155 | Grad Norm: 0.00968314\n",
      "Epoch 3 | Step 2236100 | Avg Loss: 0.0153 | Grad Norm: 0.00871770\n",
      "Epoch 3 | Step 2236200 | Avg Loss: 0.0156 | Grad Norm: 0.00962819\n",
      "Epoch 3 | Step 2236300 | Avg Loss: 0.0159 | Grad Norm: 0.00950730\n",
      "Epoch 3 | Step 2236400 | Avg Loss: 0.0154 | Grad Norm: 0.00947535\n",
      "Epoch 3 | Step 2236500 | Avg Loss: 0.0154 | Grad Norm: 0.00965182\n",
      "Epoch 3 | Step 2236600 | Avg Loss: 0.0155 | Grad Norm: 0.01297328\n",
      "Epoch 3 | Step 2236700 | Avg Loss: 0.0157 | Grad Norm: 0.00862167\n",
      "Epoch 3 | Step 2236800 | Avg Loss: 0.0157 | Grad Norm: 0.00906108\n",
      "Epoch 3 | Step 2236900 | Avg Loss: 0.0157 | Grad Norm: 0.00910979\n",
      "Epoch 3 | Step 2237000 | Avg Loss: 0.0153 | Grad Norm: 0.00941915\n",
      "Epoch 3 | Step 2237100 | Avg Loss: 0.0152 | Grad Norm: 0.00923086\n",
      "Epoch 3 | Step 2237200 | Avg Loss: 0.0153 | Grad Norm: 0.00923038\n",
      "Epoch 3 | Step 2237300 | Avg Loss: 0.0158 | Grad Norm: 0.00891984\n",
      "Epoch 3 | Step 2237400 | Avg Loss: 0.0156 | Grad Norm: 0.00843406\n",
      "Epoch 3 | Step 2237500 | Avg Loss: 0.0154 | Grad Norm: 0.00818074\n",
      "Epoch 3 | Step 2237600 | Avg Loss: 0.0149 | Grad Norm: 0.01128960\n",
      "Epoch 3 | Step 2237700 | Avg Loss: 0.0147 | Grad Norm: 0.00883410\n",
      "Epoch 3 | Step 2237800 | Avg Loss: 0.0147 | Grad Norm: 0.00819981\n",
      "Epoch 3 | Step 2237900 | Avg Loss: 0.0149 | Grad Norm: 0.00877435\n",
      "Epoch 3 | Step 2238000 | Avg Loss: 0.0149 | Grad Norm: 0.00723753\n",
      "Epoch 3 | Step 2238100 | Avg Loss: 0.0150 | Grad Norm: 0.00817434\n",
      "Epoch 3 | Step 2238200 | Avg Loss: 0.0150 | Grad Norm: 0.00903126\n",
      "Epoch 3 | Step 2238300 | Avg Loss: 0.0149 | Grad Norm: 0.00838969\n",
      "Epoch 3 | Step 2238400 | Avg Loss: 0.0150 | Grad Norm: 0.00891230\n",
      "Epoch 3 | Step 2238500 | Avg Loss: 0.0150 | Grad Norm: 0.00815971\n",
      "Epoch 3 | Step 2238600 | Avg Loss: 0.0152 | Grad Norm: 0.00861560\n",
      "Epoch 3 | Step 2238700 | Avg Loss: 0.0153 | Grad Norm: 0.00817312\n",
      "Epoch 3 | Step 2238800 | Avg Loss: 0.0154 | Grad Norm: 0.01010287\n",
      "Epoch 3 | Step 2238900 | Avg Loss: 0.0150 | Grad Norm: 0.00873151\n",
      "Epoch 3 | Step 2239000 | Avg Loss: 0.0149 | Grad Norm: 0.00889369\n",
      "Epoch 3 | Step 2239100 | Avg Loss: 0.0151 | Grad Norm: 0.00929027\n",
      "Epoch 3 | Step 2239200 | Avg Loss: 0.0148 | Grad Norm: 0.00876750\n",
      "Epoch 3 | Step 2239300 | Avg Loss: 0.0149 | Grad Norm: 0.00945327\n",
      "Epoch 3 | Step 2239400 | Avg Loss: 0.0152 | Grad Norm: 0.00827079\n",
      "Epoch 3 | Step 2239500 | Avg Loss: 0.0151 | Grad Norm: 0.00737579\n",
      "Epoch 3 | Step 2239600 | Avg Loss: 0.0157 | Grad Norm: 0.00858703\n",
      "Epoch 3 | Step 2239700 | Avg Loss: 0.0157 | Grad Norm: 0.00933740\n",
      "Epoch 3 | Step 2239800 | Avg Loss: 0.0159 | Grad Norm: 0.00773463\n",
      "Epoch 3 | Step 2239900 | Avg Loss: 0.0160 | Grad Norm: 0.00832332\n",
      "Epoch 3 | Step 2240000 | Avg Loss: 0.0155 | Grad Norm: 0.01253459\n",
      "Epoch 3 | Step 2240100 | Avg Loss: 0.0156 | Grad Norm: 0.00851369\n",
      "Epoch 3 | Step 2240200 | Avg Loss: 0.0156 | Grad Norm: 0.01004987\n",
      "Epoch 3 | Step 2240300 | Avg Loss: 0.0153 | Grad Norm: 0.01011093\n",
      "Epoch 3 | Step 2240400 | Avg Loss: 0.0151 | Grad Norm: 0.00871718\n",
      "Epoch 3 | Step 2240500 | Avg Loss: 0.0156 | Grad Norm: 0.00900982\n",
      "Epoch 3 | Step 2240600 | Avg Loss: 0.0154 | Grad Norm: 0.00969451\n",
      "Epoch 3 | Step 2240700 | Avg Loss: 0.0154 | Grad Norm: 0.00949639\n",
      "Epoch 3 | Step 2240800 | Avg Loss: 0.0151 | Grad Norm: 0.00974492\n",
      "Epoch 3 | Step 2240900 | Avg Loss: 0.0151 | Grad Norm: 0.00893058\n",
      "Epoch 3 | Step 2241000 | Avg Loss: 0.0154 | Grad Norm: 0.00941749\n",
      "Epoch 3 | Step 2241100 | Avg Loss: 0.0161 | Grad Norm: 0.00942096\n",
      "Epoch 3 | Step 2241200 | Avg Loss: 0.0162 | Grad Norm: 0.00889393\n",
      "Epoch 3 | Step 2241300 | Avg Loss: 0.0159 | Grad Norm: 0.00890465\n",
      "Epoch 3 | Step 2241400 | Avg Loss: 0.0159 | Grad Norm: 0.00961173\n",
      "Epoch 3 | Step 2241500 | Avg Loss: 0.0159 | Grad Norm: 0.00953132\n",
      "Epoch 3 | Step 2241600 | Avg Loss: 0.0159 | Grad Norm: 0.00928817\n",
      "Epoch 3 | Step 2241700 | Avg Loss: 0.0160 | Grad Norm: 0.00878927\n",
      "Epoch 3 | Step 2241800 | Avg Loss: 0.0163 | Grad Norm: 0.00862378\n",
      "Epoch 3 | Step 2241900 | Avg Loss: 0.0160 | Grad Norm: 0.00879234\n",
      "Epoch 3 | Step 2242000 | Avg Loss: 0.0162 | Grad Norm: 0.00944086\n",
      "Epoch 3 | Step 2242100 | Avg Loss: 0.0160 | Grad Norm: 0.00817305\n",
      "Epoch 3 | Step 2242200 | Avg Loss: 0.0158 | Grad Norm: 0.00992160\n",
      "Epoch 3 | Step 2242300 | Avg Loss: 0.0159 | Grad Norm: 0.01247634\n",
      "Epoch 3 | Step 2242400 | Avg Loss: 0.0159 | Grad Norm: 0.00932747\n",
      "Epoch 3 | Step 2242500 | Avg Loss: 0.0160 | Grad Norm: 0.00980183\n",
      "Epoch 3 | Step 2242600 | Avg Loss: 0.0161 | Grad Norm: 0.01047870\n",
      "Epoch 3 | Step 2242700 | Avg Loss: 0.0162 | Grad Norm: 0.01135552\n",
      "Epoch 3 | Step 2242800 | Avg Loss: 0.0163 | Grad Norm: 0.00897896\n",
      "Epoch 3 | Step 2242900 | Avg Loss: 0.0163 | Grad Norm: 0.00965022\n",
      "Epoch 3 | Step 2243000 | Avg Loss: 0.0161 | Grad Norm: 0.01004880\n",
      "Epoch 3 | Step 2243100 | Avg Loss: 0.0160 | Grad Norm: 0.00796813\n",
      "Epoch 3 | Step 2243200 | Avg Loss: 0.0158 | Grad Norm: 0.00887314\n",
      "Epoch 3 | Step 2243300 | Avg Loss: 0.0160 | Grad Norm: 0.01071633\n",
      "Epoch 3 | Step 2243400 | Avg Loss: 0.0159 | Grad Norm: 0.01160627\n",
      "Epoch 3 | Step 2243500 | Avg Loss: 0.0155 | Grad Norm: 0.00882420\n",
      "Epoch 3 | Step 2243600 | Avg Loss: 0.0155 | Grad Norm: 0.00924144\n",
      "Epoch 3 | Step 2243700 | Avg Loss: 0.0155 | Grad Norm: 0.00909000\n",
      "Epoch 3 | Step 2243800 | Avg Loss: 0.0156 | Grad Norm: 0.00803409\n",
      "Epoch 3 | Step 2243900 | Avg Loss: 0.0157 | Grad Norm: 0.00978970\n",
      "Epoch 3 | Step 2244000 | Avg Loss: 0.0158 | Grad Norm: 0.00935604\n",
      "Epoch 3 | Step 2244100 | Avg Loss: 0.0157 | Grad Norm: 0.00906872\n",
      "Epoch 3 | Step 2244200 | Avg Loss: 0.0156 | Grad Norm: 0.00933616\n",
      "Epoch 3 | Step 2244300 | Avg Loss: 0.0153 | Grad Norm: 0.00920942\n",
      "Epoch 3 | Step 2244400 | Avg Loss: 0.0149 | Grad Norm: 0.00895300\n",
      "Epoch 3 | Step 2244500 | Avg Loss: 0.0155 | Grad Norm: 0.00856377\n",
      "Epoch 3 | Step 2244600 | Avg Loss: 0.0153 | Grad Norm: 0.00910554\n",
      "Epoch 3 | Step 2244700 | Avg Loss: 0.0154 | Grad Norm: 0.00992504\n",
      "Epoch 3 | Step 2244800 | Avg Loss: 0.0156 | Grad Norm: 0.00774051\n",
      "Epoch 3 | Step 2244900 | Avg Loss: 0.0151 | Grad Norm: 0.00733626\n",
      "Epoch 3 | Step 2245000 | Avg Loss: 0.0149 | Grad Norm: 0.01025733\n",
      "Epoch 3 | Step 2245100 | Avg Loss: 0.0150 | Grad Norm: 0.01100299\n",
      "Epoch 3 | Step 2245200 | Avg Loss: 0.0155 | Grad Norm: 0.01164862\n",
      "Epoch 3 | Step 2245300 | Avg Loss: 0.0154 | Grad Norm: 0.00972919\n",
      "Epoch 3 | Step 2245400 | Avg Loss: 0.0155 | Grad Norm: 0.00717046\n",
      "Epoch 3 | Step 2245500 | Avg Loss: 0.0159 | Grad Norm: 0.00872589\n",
      "Epoch 3 | Step 2245600 | Avg Loss: 0.0158 | Grad Norm: 0.00871481\n",
      "Epoch 3 | Step 2245700 | Avg Loss: 0.0152 | Grad Norm: 0.00829248\n",
      "Epoch 3 | Step 2245800 | Avg Loss: 0.0153 | Grad Norm: 0.00775298\n",
      "Epoch 3 | Step 2245900 | Avg Loss: 0.0147 | Grad Norm: 0.01738550\n",
      "Epoch 3 | Step 2246000 | Avg Loss: 0.0147 | Grad Norm: 0.00879273\n",
      "Epoch 3 | Step 2246100 | Avg Loss: 0.0149 | Grad Norm: 0.00815193\n",
      "Epoch 3 | Step 2246200 | Avg Loss: 0.0148 | Grad Norm: 0.00924267\n",
      "Epoch 3 | Step 2246300 | Avg Loss: 0.0149 | Grad Norm: 0.00951416\n",
      "Epoch 3 | Step 2246400 | Avg Loss: 0.0149 | Grad Norm: 0.00895847\n",
      "Epoch 3 | Step 2246500 | Avg Loss: 0.0153 | Grad Norm: 0.00838816\n",
      "Epoch 3 | Step 2246600 | Avg Loss: 0.0149 | Grad Norm: 0.00849984\n",
      "Epoch 3 | Step 2246700 | Avg Loss: 0.0151 | Grad Norm: 0.00767017\n",
      "Epoch 3 | Step 2246800 | Avg Loss: 0.0152 | Grad Norm: 0.00889081\n",
      "Epoch 3 | Step 2246900 | Avg Loss: 0.0156 | Grad Norm: 0.00911878\n",
      "Epoch 3 | Step 2247000 | Avg Loss: 0.0156 | Grad Norm: 0.00780964\n",
      "Epoch 3 | Step 2247100 | Avg Loss: 0.0158 | Grad Norm: 0.00924762\n",
      "Epoch 3 | Step 2247200 | Avg Loss: 0.0157 | Grad Norm: 0.00937177\n",
      "Epoch 3 | Step 2247300 | Avg Loss: 0.0159 | Grad Norm: 0.00939278\n",
      "Epoch 3 | Step 2247400 | Avg Loss: 0.0162 | Grad Norm: 0.00911990\n",
      "Epoch 3 | Step 2247500 | Avg Loss: 0.0160 | Grad Norm: 0.00991498\n",
      "Epoch 3 | Step 2247600 | Avg Loss: 0.0159 | Grad Norm: 0.00814629\n",
      "Epoch 3 | Step 2247700 | Avg Loss: 0.0159 | Grad Norm: 0.00756059\n",
      "Epoch 3 | Step 2247800 | Avg Loss: 0.0158 | Grad Norm: 0.00805938\n",
      "Epoch 3 | Step 2247900 | Avg Loss: 0.0158 | Grad Norm: 0.00849500\n",
      "Epoch 3 | Step 2248000 | Avg Loss: 0.0159 | Grad Norm: 0.00785421\n",
      "Epoch 3 | Step 2248100 | Avg Loss: 0.0158 | Grad Norm: 0.01084919\n",
      "Epoch 3 | Step 2248200 | Avg Loss: 0.0159 | Grad Norm: 0.00982137\n",
      "Epoch 3 | Step 2248300 | Avg Loss: 0.0157 | Grad Norm: 0.00913587\n",
      "Epoch 3 | Step 2248400 | Avg Loss: 0.0152 | Grad Norm: 0.00969406\n",
      "Epoch 3 | Step 2248500 | Avg Loss: 0.0152 | Grad Norm: 0.00886121\n",
      "Epoch 3 | Step 2248600 | Avg Loss: 0.0152 | Grad Norm: 0.00927722\n",
      "Epoch 3 | Step 2248700 | Avg Loss: 0.0152 | Grad Norm: 0.00882639\n",
      "Epoch 3 | Step 2248800 | Avg Loss: 0.0154 | Grad Norm: 0.00854501\n",
      "Epoch 3 | Step 2248900 | Avg Loss: 0.0155 | Grad Norm: 0.00856408\n",
      "Epoch 3 | Step 2249000 | Avg Loss: 0.0158 | Grad Norm: 0.00907939\n",
      "Epoch 3 | Step 2249100 | Avg Loss: 0.0157 | Grad Norm: 0.00920897\n",
      "Epoch 3 | Step 2249200 | Avg Loss: 0.0156 | Grad Norm: 0.00834153\n",
      "Epoch 3 | Step 2249300 | Avg Loss: 0.0160 | Grad Norm: 0.00894958\n",
      "Epoch 3 | Step 2249400 | Avg Loss: 0.0161 | Grad Norm: 0.01024913\n",
      "Epoch 3 | Step 2249500 | Avg Loss: 0.0161 | Grad Norm: 0.01167269\n",
      "Epoch 3 | Step 2249600 | Avg Loss: 0.0163 | Grad Norm: 0.01091769\n",
      "Epoch 3 | Step 2249700 | Avg Loss: 0.0159 | Grad Norm: 0.01008611\n",
      "Epoch 3 | Step 2249800 | Avg Loss: 0.0160 | Grad Norm: 0.01263272\n",
      "Epoch 3 | Step 2249900 | Avg Loss: 0.0156 | Grad Norm: 0.00817483\n",
      "Epoch 3 | Step 2250000 | Avg Loss: 0.0162 | Grad Norm: 0.00965994\n",
      "Epoch 3 | Step 2250100 | Avg Loss: 0.0164 | Grad Norm: 0.00899466\n",
      "Epoch 3 | Step 2250200 | Avg Loss: 0.0161 | Grad Norm: 0.00916710\n",
      "Epoch 3 | Step 2250300 | Avg Loss: 0.0160 | Grad Norm: 0.01625529\n",
      "Epoch 3 | Step 2250400 | Avg Loss: 0.0161 | Grad Norm: 0.00816843\n",
      "Epoch 3 | Step 2250500 | Avg Loss: 0.0158 | Grad Norm: 0.00908489\n",
      "Epoch 3 | Step 2250600 | Avg Loss: 0.0156 | Grad Norm: 0.00800225\n",
      "Epoch 3 | Step 2250700 | Avg Loss: 0.0158 | Grad Norm: 0.00778370\n",
      "Epoch 3 | Step 2250800 | Avg Loss: 0.0164 | Grad Norm: 0.01007711\n",
      "Epoch 3 | Step 2250900 | Avg Loss: 0.0156 | Grad Norm: 0.00860248\n",
      "Epoch 3 | Step 2251000 | Avg Loss: 0.0158 | Grad Norm: 0.00921764\n",
      "Epoch 3 | Step 2251100 | Avg Loss: 0.0159 | Grad Norm: 0.00871747\n",
      "Epoch 3 | Step 2251200 | Avg Loss: 0.0155 | Grad Norm: 0.00915220\n",
      "Epoch 3 | Step 2251300 | Avg Loss: 0.0155 | Grad Norm: 0.00847579\n",
      "Epoch 3 | Step 2251400 | Avg Loss: 0.0157 | Grad Norm: 0.00775218\n",
      "Epoch 3 | Step 2251500 | Avg Loss: 0.0151 | Grad Norm: 0.00834028\n",
      "Epoch 3 | Step 2251600 | Avg Loss: 0.0154 | Grad Norm: 0.00819939\n",
      "Epoch 3 | Step 2251700 | Avg Loss: 0.0152 | Grad Norm: 0.00892142\n",
      "Epoch 3 | Step 2251800 | Avg Loss: 0.0156 | Grad Norm: 0.00912760\n",
      "Epoch 3 | Step 2251900 | Avg Loss: 0.0157 | Grad Norm: 0.00869456\n",
      "Epoch 3 | Step 2252000 | Avg Loss: 0.0155 | Grad Norm: 0.00943175\n",
      "Epoch 3 | Step 2252100 | Avg Loss: 0.0155 | Grad Norm: 0.00850339\n",
      "Epoch 3 | Step 2252200 | Avg Loss: 0.0156 | Grad Norm: 0.00812047\n",
      "Epoch 3 | Step 2252300 | Avg Loss: 0.0151 | Grad Norm: 0.00894426\n",
      "Epoch 3 | Step 2252400 | Avg Loss: 0.0150 | Grad Norm: 0.00913793\n",
      "Epoch 3 | Step 2252500 | Avg Loss: 0.0149 | Grad Norm: 0.00881434\n",
      "Epoch 3 | Step 2252600 | Avg Loss: 0.0147 | Grad Norm: 0.01000726\n",
      "Epoch 3 | Step 2252700 | Avg Loss: 0.0151 | Grad Norm: 0.00894344\n",
      "Epoch 3 | Step 2252800 | Avg Loss: 0.0149 | Grad Norm: 0.00941975\n",
      "Epoch 3 | Step 2252900 | Avg Loss: 0.0153 | Grad Norm: 0.00863660\n",
      "Epoch 3 | Step 2253000 | Avg Loss: 0.0154 | Grad Norm: 0.00894210\n",
      "Epoch 3 | Step 2253100 | Avg Loss: 0.0157 | Grad Norm: 0.00910556\n",
      "Epoch 3 | Step 2253200 | Avg Loss: 0.0155 | Grad Norm: 0.00939793\n",
      "Epoch 3 | Step 2253300 | Avg Loss: 0.0153 | Grad Norm: 0.00795318\n",
      "Epoch 3 | Step 2253400 | Avg Loss: 0.0155 | Grad Norm: 0.00918721\n",
      "Epoch 3 | Step 2253500 | Avg Loss: 0.0156 | Grad Norm: 0.00770093\n",
      "Epoch 3 | Step 2253600 | Avg Loss: 0.0150 | Grad Norm: 0.00885857\n",
      "Epoch 3 | Step 2253700 | Avg Loss: 0.0152 | Grad Norm: 0.00825955\n",
      "Epoch 3 | Step 2253800 | Avg Loss: 0.0153 | Grad Norm: 0.01049881\n",
      "Epoch 3 | Step 2253900 | Avg Loss: 0.0155 | Grad Norm: 0.00878211\n",
      "Epoch 3 | Step 2254000 | Avg Loss: 0.0154 | Grad Norm: 0.00770938\n",
      "Epoch 3 | Step 2254100 | Avg Loss: 0.0152 | Grad Norm: 0.00781788\n",
      "Epoch 3 | Step 2254200 | Avg Loss: 0.0157 | Grad Norm: 0.00796950\n",
      "Epoch 3 | Step 2254300 | Avg Loss: 0.0154 | Grad Norm: 0.00872625\n",
      "Epoch 3 | Step 2254400 | Avg Loss: 0.0155 | Grad Norm: 0.00793423\n",
      "Epoch 3 | Step 2254500 | Avg Loss: 0.0154 | Grad Norm: 0.00995360\n",
      "Epoch 3 | Step 2254600 | Avg Loss: 0.0155 | Grad Norm: 0.00914553\n",
      "Epoch 3 | Step 2254700 | Avg Loss: 0.0158 | Grad Norm: 0.01118862\n",
      "Epoch 3 | Step 2254800 | Avg Loss: 0.0157 | Grad Norm: 0.00954007\n",
      "Epoch 3 | Step 2254900 | Avg Loss: 0.0161 | Grad Norm: 0.00926528\n",
      "Epoch 3 | Step 2255000 | Avg Loss: 0.0159 | Grad Norm: 0.00970557\n",
      "Epoch 3 | Step 2255100 | Avg Loss: 0.0159 | Grad Norm: 0.00850120\n",
      "Epoch 3 | Step 2255200 | Avg Loss: 0.0154 | Grad Norm: 0.00877327\n",
      "Epoch 3 | Step 2255300 | Avg Loss: 0.0158 | Grad Norm: 0.00906255\n",
      "Epoch 3 | Step 2255400 | Avg Loss: 0.0159 | Grad Norm: 0.00751164\n",
      "Epoch 3 | Step 2255500 | Avg Loss: 0.0158 | Grad Norm: 0.00986054\n",
      "Epoch 3 | Step 2255600 | Avg Loss: 0.0159 | Grad Norm: 0.00886491\n",
      "Epoch 3 | Step 2255700 | Avg Loss: 0.0159 | Grad Norm: 0.00922033\n",
      "Epoch 3 | Step 2255800 | Avg Loss: 0.0160 | Grad Norm: 0.00898975\n",
      "Epoch 3 | Step 2255900 | Avg Loss: 0.0157 | Grad Norm: 0.00915574\n",
      "Epoch 3 | Step 2256000 | Avg Loss: 0.0156 | Grad Norm: 0.00890641\n",
      "Epoch 3 | Step 2256100 | Avg Loss: 0.0156 | Grad Norm: 0.00934471\n",
      "Epoch 3 | Step 2256200 | Avg Loss: 0.0156 | Grad Norm: 0.01226190\n",
      "Epoch 3 | Step 2256300 | Avg Loss: 0.0156 | Grad Norm: 0.01034902\n",
      "Epoch 3 | Step 2256400 | Avg Loss: 0.0159 | Grad Norm: 0.00866408\n",
      "Epoch 3 | Step 2256500 | Avg Loss: 0.0159 | Grad Norm: 0.01006728\n",
      "Epoch 3 | Step 2256600 | Avg Loss: 0.0159 | Grad Norm: 0.01133096\n",
      "Epoch 3 | Step 2256700 | Avg Loss: 0.0160 | Grad Norm: 0.00776730\n",
      "Epoch 3 | Step 2256800 | Avg Loss: 0.0166 | Grad Norm: 0.00982602\n",
      "Epoch 3 | Step 2256900 | Avg Loss: 0.0162 | Grad Norm: 0.00870940\n",
      "Epoch 3 | Step 2257000 | Avg Loss: 0.0159 | Grad Norm: 0.01034028\n",
      "Epoch 3 | Step 2257100 | Avg Loss: 0.0160 | Grad Norm: 0.00749782\n",
      "Epoch 3 | Step 2257200 | Avg Loss: 0.0161 | Grad Norm: 0.01053168\n",
      "Epoch 3 | Step 2257300 | Avg Loss: 0.0160 | Grad Norm: 0.00918004\n",
      "Epoch 3 | Step 2257400 | Avg Loss: 0.0159 | Grad Norm: 0.00893805\n",
      "Epoch 3 | Step 2257500 | Avg Loss: 0.0158 | Grad Norm: 0.01180127\n",
      "Epoch 3 | Step 2257600 | Avg Loss: 0.0159 | Grad Norm: 0.00903539\n",
      "Epoch 3 | Step 2257700 | Avg Loss: 0.0159 | Grad Norm: 0.00919663\n",
      "Epoch 3 | Step 2257800 | Avg Loss: 0.0157 | Grad Norm: 0.00861031\n",
      "Epoch 3 | Step 2257900 | Avg Loss: 0.0155 | Grad Norm: 0.00816074\n",
      "Epoch 3 | Step 2258000 | Avg Loss: 0.0154 | Grad Norm: 0.00944097\n",
      "Epoch 3 | Step 2258100 | Avg Loss: 0.0158 | Grad Norm: 0.00981698\n",
      "Epoch 3 | Step 2258200 | Avg Loss: 0.0155 | Grad Norm: 0.01021342\n",
      "Epoch 3 | Step 2258300 | Avg Loss: 0.0155 | Grad Norm: 0.00871148\n",
      "Epoch 3 | Step 2258400 | Avg Loss: 0.0155 | Grad Norm: 0.00991901\n",
      "Epoch 3 | Step 2258500 | Avg Loss: 0.0159 | Grad Norm: 0.00967005\n",
      "Epoch 3 | Step 2258600 | Avg Loss: 0.0157 | Grad Norm: 0.00818863\n",
      "Epoch 3 | Step 2258700 | Avg Loss: 0.0156 | Grad Norm: 0.01032401\n",
      "Epoch 3 | Step 2258800 | Avg Loss: 0.0156 | Grad Norm: 0.00986853\n",
      "Epoch 3 | Step 2258900 | Avg Loss: 0.0156 | Grad Norm: 0.00960524\n",
      "Epoch 3 | Step 2259000 | Avg Loss: 0.0153 | Grad Norm: 0.00946031\n",
      "Epoch 3 | Step 2259100 | Avg Loss: 0.0155 | Grad Norm: 0.00886181\n",
      "Epoch 3 | Step 2259200 | Avg Loss: 0.0155 | Grad Norm: 0.01038944\n",
      "Epoch 3 | Step 2259300 | Avg Loss: 0.0157 | Grad Norm: 0.00877997\n",
      "Epoch 3 | Step 2259400 | Avg Loss: 0.0157 | Grad Norm: 0.00883314\n",
      "Epoch 3 | Step 2259500 | Avg Loss: 0.0154 | Grad Norm: 0.00886618\n",
      "Epoch 3 | Step 2259600 | Avg Loss: 0.0154 | Grad Norm: 0.00874638\n",
      "Epoch 3 | Step 2259700 | Avg Loss: 0.0157 | Grad Norm: 0.00875966\n",
      "Epoch 3 | Step 2259800 | Avg Loss: 0.0161 | Grad Norm: 0.01168623\n",
      "Epoch 3 | Step 2259900 | Avg Loss: 0.0161 | Grad Norm: 0.00866278\n",
      "Epoch 3 | Step 2260000 | Avg Loss: 0.0159 | Grad Norm: 0.00850405\n",
      "Epoch 3 | Step 2260100 | Avg Loss: 0.0163 | Grad Norm: 0.01053154\n",
      "Epoch 3 | Step 2260200 | Avg Loss: 0.0162 | Grad Norm: 0.00903086\n",
      "Epoch 3 | Step 2260300 | Avg Loss: 0.0165 | Grad Norm: 0.00977453\n",
      "Epoch 3 | Step 2260400 | Avg Loss: 0.0160 | Grad Norm: 0.00933513\n",
      "Epoch 3 | Step 2260500 | Avg Loss: 0.0160 | Grad Norm: 0.00977381\n",
      "Epoch 3 | Step 2260600 | Avg Loss: 0.0158 | Grad Norm: 0.00932255\n",
      "Epoch 3 | Step 2260700 | Avg Loss: 0.0160 | Grad Norm: 0.00918117\n",
      "Epoch 3 | Step 2260800 | Avg Loss: 0.0160 | Grad Norm: 0.00967659\n",
      "Epoch 3 | Step 2260900 | Avg Loss: 0.0163 | Grad Norm: 0.00887724\n",
      "Epoch 3 | Step 2261000 | Avg Loss: 0.0159 | Grad Norm: 0.00865187\n",
      "Epoch 3 | Step 2261100 | Avg Loss: 0.0161 | Grad Norm: 0.01019579\n",
      "Epoch 3 | Step 2261200 | Avg Loss: 0.0161 | Grad Norm: 0.00917651\n",
      "Epoch 3 | Step 2261300 | Avg Loss: 0.0161 | Grad Norm: 0.00965020\n",
      "Epoch 3 | Step 2261400 | Avg Loss: 0.0153 | Grad Norm: 0.00837196\n",
      "Epoch 3 | Step 2261500 | Avg Loss: 0.0153 | Grad Norm: 0.00871949\n",
      "Epoch 3 | Step 2261600 | Avg Loss: 0.0153 | Grad Norm: 0.00957548\n",
      "Epoch 3 | Step 2261700 | Avg Loss: 0.0157 | Grad Norm: 0.01047251\n",
      "Epoch 3 | Step 2261800 | Avg Loss: 0.0159 | Grad Norm: 0.00777098\n",
      "Epoch 3 | Step 2261900 | Avg Loss: 0.0157 | Grad Norm: 0.00853859\n",
      "Epoch 3 | Step 2262000 | Avg Loss: 0.0158 | Grad Norm: 0.00904360\n",
      "Epoch 3 | Step 2262100 | Avg Loss: 0.0159 | Grad Norm: 0.00963690\n",
      "Epoch 3 | Step 2262200 | Avg Loss: 0.0155 | Grad Norm: 0.00866553\n",
      "Epoch 3 | Step 2262300 | Avg Loss: 0.0154 | Grad Norm: 0.00838500\n",
      "Epoch 3 | Step 2262400 | Avg Loss: 0.0155 | Grad Norm: 0.00851309\n",
      "Epoch 3 | Step 2262500 | Avg Loss: 0.0156 | Grad Norm: 0.00920796\n",
      "Epoch 3 | Step 2262600 | Avg Loss: 0.0154 | Grad Norm: 0.00969721\n",
      "Epoch 3 | Step 2262700 | Avg Loss: 0.0159 | Grad Norm: 0.01027893\n",
      "Epoch 3 | Step 2262800 | Avg Loss: 0.0157 | Grad Norm: 0.01041817\n",
      "Epoch 3 | Step 2262900 | Avg Loss: 0.0161 | Grad Norm: 0.00945488\n",
      "Epoch 3 | Step 2263000 | Avg Loss: 0.0160 | Grad Norm: 0.00870442\n",
      "Epoch 3 | Step 2263100 | Avg Loss: 0.0158 | Grad Norm: 0.00922021\n",
      "Epoch 3 | Step 2263200 | Avg Loss: 0.0155 | Grad Norm: 0.00872305\n",
      "Epoch 3 | Step 2263300 | Avg Loss: 0.0151 | Grad Norm: 0.00859632\n",
      "Epoch 3 | Step 2263400 | Avg Loss: 0.0153 | Grad Norm: 0.00862228\n",
      "Epoch 3 | Step 2263500 | Avg Loss: 0.0153 | Grad Norm: 0.00964005\n",
      "Epoch 3 | Step 2263600 | Avg Loss: 0.0155 | Grad Norm: 0.00978829\n",
      "Epoch 3 | Step 2263700 | Avg Loss: 0.0157 | Grad Norm: 0.01025882\n",
      "Epoch 3 | Step 2263800 | Avg Loss: 0.0156 | Grad Norm: 0.00969112\n",
      "Epoch 3 | Step 2263900 | Avg Loss: 0.0154 | Grad Norm: 0.00795976\n",
      "Epoch 3 | Step 2264000 | Avg Loss: 0.0156 | Grad Norm: 0.00875398\n",
      "Epoch 3 | Step 2264100 | Avg Loss: 0.0153 | Grad Norm: 0.00802954\n",
      "Epoch 3 | Step 2264200 | Avg Loss: 0.0159 | Grad Norm: 0.00940220\n",
      "Epoch 3 | Step 2264300 | Avg Loss: 0.0155 | Grad Norm: 0.00925485\n",
      "Epoch 3 | Step 2264400 | Avg Loss: 0.0156 | Grad Norm: 0.00758610\n",
      "Epoch 3 | Step 2264500 | Avg Loss: 0.0155 | Grad Norm: 0.00854960\n",
      "Epoch 3 | Step 2264600 | Avg Loss: 0.0157 | Grad Norm: 0.01103776\n",
      "Epoch 3 | Step 2264700 | Avg Loss: 0.0158 | Grad Norm: 0.00868413\n",
      "Epoch 3 | Step 2264800 | Avg Loss: 0.0155 | Grad Norm: 0.01002793\n",
      "Epoch 3 | Step 2264900 | Avg Loss: 0.0153 | Grad Norm: 0.00807280\n",
      "Epoch 3 | Step 2265000 | Avg Loss: 0.0155 | Grad Norm: 0.00929103\n",
      "Epoch 3 | Step 2265100 | Avg Loss: 0.0155 | Grad Norm: 0.01088731\n",
      "Epoch 3 | Step 2265200 | Avg Loss: 0.0155 | Grad Norm: 0.00917858\n",
      "Epoch 3 | Step 2265300 | Avg Loss: 0.0156 | Grad Norm: 0.00952661\n",
      "Epoch 3 | Step 2265400 | Avg Loss: 0.0151 | Grad Norm: 0.00972255\n",
      "Epoch 3 | Step 2265500 | Avg Loss: 0.0153 | Grad Norm: 0.00877957\n",
      "Epoch 3 | Step 2265600 | Avg Loss: 0.0155 | Grad Norm: 0.01108135\n",
      "Epoch 3 | Step 2265700 | Avg Loss: 0.0157 | Grad Norm: 0.00898401\n",
      "Epoch 3 | Step 2265800 | Avg Loss: 0.0155 | Grad Norm: 0.00847872\n",
      "Epoch 3 | Step 2265900 | Avg Loss: 0.0157 | Grad Norm: 0.00845305\n",
      "Epoch 3 | Step 2266000 | Avg Loss: 0.0157 | Grad Norm: 0.00889985\n",
      "Epoch 3 | Step 2266100 | Avg Loss: 0.0155 | Grad Norm: 0.00898717\n",
      "Epoch 3 | Step 2266200 | Avg Loss: 0.0154 | Grad Norm: 0.00918619\n",
      "Epoch 3 | Step 2266300 | Avg Loss: 0.0155 | Grad Norm: 0.00959423\n",
      "Epoch 3 | Step 2266400 | Avg Loss: 0.0154 | Grad Norm: 0.01019205\n",
      "Epoch 3 | Step 2266500 | Avg Loss: 0.0153 | Grad Norm: 0.00849202\n",
      "Epoch 3 | Step 2266600 | Avg Loss: 0.0150 | Grad Norm: 0.00831336\n",
      "Epoch 3 | Step 2266700 | Avg Loss: 0.0154 | Grad Norm: 0.00884613\n",
      "Epoch 3 | Step 2266800 | Avg Loss: 0.0154 | Grad Norm: 0.00838515\n",
      "Epoch 3 | Step 2266900 | Avg Loss: 0.0152 | Grad Norm: 0.00860448\n",
      "Epoch 3 | Step 2267000 | Avg Loss: 0.0153 | Grad Norm: 0.00870777\n",
      "Epoch 3 | Step 2267100 | Avg Loss: 0.0154 | Grad Norm: 0.00991327\n",
      "Epoch 3 | Step 2267200 | Avg Loss: 0.0154 | Grad Norm: 0.00928235\n",
      "Epoch 3 | Step 2267300 | Avg Loss: 0.0153 | Grad Norm: 0.00961281\n",
      "Epoch 3 | Step 2267400 | Avg Loss: 0.0153 | Grad Norm: 0.00966259\n",
      "Epoch 3 | Step 2267500 | Avg Loss: 0.0154 | Grad Norm: 0.00949597\n",
      "Epoch 3 | Step 2267600 | Avg Loss: 0.0154 | Grad Norm: 0.00791528\n",
      "Epoch 3 | Step 2267700 | Avg Loss: 0.0154 | Grad Norm: 0.01046267\n",
      "Epoch 3 | Step 2267800 | Avg Loss: 0.0153 | Grad Norm: 0.00936673\n",
      "Epoch 3 | Step 2267900 | Avg Loss: 0.0154 | Grad Norm: 0.00978124\n",
      "Epoch 3 | Step 2268000 | Avg Loss: 0.0152 | Grad Norm: 0.00862745\n",
      "Epoch 3 | Step 2268100 | Avg Loss: 0.0154 | Grad Norm: 0.00781954\n",
      "Epoch 3 | Step 2268200 | Avg Loss: 0.0156 | Grad Norm: 0.00908637\n",
      "Epoch 3 | Step 2268300 | Avg Loss: 0.0155 | Grad Norm: 0.00881739\n",
      "Epoch 3 | Step 2268400 | Avg Loss: 0.0156 | Grad Norm: 0.01200511\n",
      "Epoch 3 | Step 2268500 | Avg Loss: 0.0155 | Grad Norm: 0.00812218\n",
      "Epoch 3 | Step 2268600 | Avg Loss: 0.0153 | Grad Norm: 0.00981519\n",
      "Epoch 3 | Step 2268700 | Avg Loss: 0.0154 | Grad Norm: 0.00850254\n",
      "Epoch 3 | Step 2268800 | Avg Loss: 0.0156 | Grad Norm: 0.00863685\n",
      "Epoch 3 | Step 2268900 | Avg Loss: 0.0153 | Grad Norm: 0.00896215\n",
      "Epoch 3 | Step 2269000 | Avg Loss: 0.0156 | Grad Norm: 0.01036773\n",
      "Epoch 3 | Step 2269100 | Avg Loss: 0.0159 | Grad Norm: 0.00845142\n",
      "Epoch 3 | Step 2269200 | Avg Loss: 0.0160 | Grad Norm: 0.00989846\n",
      "Epoch 3 | Step 2269300 | Avg Loss: 0.0158 | Grad Norm: 0.00891421\n",
      "Epoch 3 | Step 2269400 | Avg Loss: 0.0160 | Grad Norm: 0.00920169\n",
      "Epoch 3 | Step 2269500 | Avg Loss: 0.0157 | Grad Norm: 0.01143437\n",
      "Epoch 3 | Step 2269600 | Avg Loss: 0.0158 | Grad Norm: 0.00881620\n",
      "Epoch 3 | Step 2269700 | Avg Loss: 0.0157 | Grad Norm: 0.00862195\n",
      "Epoch 3 | Step 2269800 | Avg Loss: 0.0158 | Grad Norm: 0.00850018\n",
      "Epoch 3 | Step 2269900 | Avg Loss: 0.0155 | Grad Norm: 0.00819870\n",
      "Epoch 3 | Step 2270000 | Avg Loss: 0.0158 | Grad Norm: 0.00807051\n",
      "Epoch 3 | Step 2270100 | Avg Loss: 0.0154 | Grad Norm: 0.00902443\n",
      "Epoch 3 | Step 2270200 | Avg Loss: 0.0156 | Grad Norm: 0.00887608\n",
      "Epoch 3 | Step 2270300 | Avg Loss: 0.0162 | Grad Norm: 0.00891154\n",
      "Epoch 3 | Step 2270400 | Avg Loss: 0.0158 | Grad Norm: 0.00851339\n",
      "Epoch 3 | Step 2270500 | Avg Loss: 0.0154 | Grad Norm: 0.00962741\n",
      "Epoch 3 | Step 2270600 | Avg Loss: 0.0155 | Grad Norm: 0.01170395\n",
      "Epoch 3 | Step 2270700 | Avg Loss: 0.0156 | Grad Norm: 0.01060261\n",
      "Epoch 3 | Step 2270800 | Avg Loss: 0.0158 | Grad Norm: 0.00887356\n",
      "Epoch 3 | Step 2270900 | Avg Loss: 0.0161 | Grad Norm: 0.00991341\n",
      "Epoch 3 | Step 2271000 | Avg Loss: 0.0161 | Grad Norm: 0.00893888\n",
      "Epoch 3 | Step 2271100 | Avg Loss: 0.0158 | Grad Norm: 0.00985161\n",
      "Epoch 3 | Step 2271200 | Avg Loss: 0.0159 | Grad Norm: 0.00912736\n",
      "Epoch 3 | Step 2271300 | Avg Loss: 0.0155 | Grad Norm: 0.01051906\n",
      "Epoch 3 | Step 2271400 | Avg Loss: 0.0155 | Grad Norm: 0.00930098\n",
      "Epoch 3 | Step 2271500 | Avg Loss: 0.0158 | Grad Norm: 0.00911194\n",
      "Epoch 3 | Step 2271600 | Avg Loss: 0.0158 | Grad Norm: 0.01059299\n",
      "Epoch 3 | Step 2271700 | Avg Loss: 0.0156 | Grad Norm: 0.00980745\n",
      "Epoch 3 | Step 2271800 | Avg Loss: 0.0153 | Grad Norm: 0.00925727\n",
      "Epoch 3 | Step 2271900 | Avg Loss: 0.0157 | Grad Norm: 0.00860816\n",
      "Epoch 3 | Step 2272000 | Avg Loss: 0.0155 | Grad Norm: 0.00875913\n",
      "Epoch 3 | Step 2272100 | Avg Loss: 0.0153 | Grad Norm: 0.00920019\n",
      "Epoch 3 | Step 2272200 | Avg Loss: 0.0152 | Grad Norm: 0.00848038\n",
      "Epoch 3 | Step 2272300 | Avg Loss: 0.0152 | Grad Norm: 0.00856952\n",
      "Epoch 3 | Step 2272400 | Avg Loss: 0.0153 | Grad Norm: 0.00808762\n",
      "Epoch 3 | Step 2272500 | Avg Loss: 0.0158 | Grad Norm: 0.01015552\n",
      "Epoch 3 | Step 2272600 | Avg Loss: 0.0159 | Grad Norm: 0.01008807\n",
      "Epoch 3 | Step 2272700 | Avg Loss: 0.0159 | Grad Norm: 0.00906120\n",
      "Epoch 3 | Step 2272800 | Avg Loss: 0.0152 | Grad Norm: 0.00874854\n",
      "Epoch 3 | Step 2272900 | Avg Loss: 0.0154 | Grad Norm: 0.00820497\n",
      "Epoch 3 | Step 2273000 | Avg Loss: 0.0156 | Grad Norm: 0.01056506\n",
      "Epoch 3 | Step 2273100 | Avg Loss: 0.0155 | Grad Norm: 0.00925002\n",
      "Epoch 3 | Step 2273200 | Avg Loss: 0.0160 | Grad Norm: 0.01016766\n",
      "Epoch 3 | Step 2273300 | Avg Loss: 0.0159 | Grad Norm: 0.01094719\n",
      "Epoch 3 | Step 2273400 | Avg Loss: 0.0157 | Grad Norm: 0.01317690\n",
      "Epoch 3 | Step 2273500 | Avg Loss: 0.0156 | Grad Norm: 0.00975221\n",
      "Epoch 3 | Step 2273600 | Avg Loss: 0.0156 | Grad Norm: 0.00952971\n",
      "Epoch 3 | Step 2273700 | Avg Loss: 0.0153 | Grad Norm: 0.01012393\n",
      "Epoch 3 | Step 2273800 | Avg Loss: 0.0152 | Grad Norm: 0.00945605\n",
      "Epoch 3 | Step 2273900 | Avg Loss: 0.0151 | Grad Norm: 0.00821731\n",
      "Epoch 3 | Step 2274000 | Avg Loss: 0.0152 | Grad Norm: 0.00843237\n",
      "Epoch 3 | Step 2274100 | Avg Loss: 0.0152 | Grad Norm: 0.01013406\n",
      "Epoch 3 | Step 2274200 | Avg Loss: 0.0151 | Grad Norm: 0.00848142\n",
      "Epoch 3 | Step 2274300 | Avg Loss: 0.0151 | Grad Norm: 0.00861356\n",
      "Epoch 3 | Step 2274400 | Avg Loss: 0.0153 | Grad Norm: 0.00922450\n",
      "Epoch 3 | Step 2274500 | Avg Loss: 0.0158 | Grad Norm: 0.00931916\n",
      "Epoch 3 | Step 2274600 | Avg Loss: 0.0160 | Grad Norm: 0.00883460\n",
      "Epoch 3 | Step 2274700 | Avg Loss: 0.0157 | Grad Norm: 0.00839246\n",
      "Epoch 3 | Step 2274800 | Avg Loss: 0.0158 | Grad Norm: 0.00869231\n",
      "Epoch 3 | Step 2274900 | Avg Loss: 0.0157 | Grad Norm: 0.00828241\n",
      "Epoch 3 | Step 2275000 | Avg Loss: 0.0158 | Grad Norm: 0.01033758\n",
      "Epoch 3 | Step 2275100 | Avg Loss: 0.0156 | Grad Norm: 0.00758353\n",
      "Epoch 3 | Step 2275200 | Avg Loss: 0.0158 | Grad Norm: 0.01140385\n",
      "Epoch 3 | Step 2275300 | Avg Loss: 0.0156 | Grad Norm: 0.00804930\n",
      "Epoch 3 | Step 2275400 | Avg Loss: 0.0156 | Grad Norm: 0.00884669\n",
      "Epoch 3 | Step 2275500 | Avg Loss: 0.0153 | Grad Norm: 0.00833762\n",
      "Epoch 3 | Step 2275600 | Avg Loss: 0.0154 | Grad Norm: 0.00826466\n",
      "Epoch 3 | Step 2275700 | Avg Loss: 0.0156 | Grad Norm: 0.01048920\n",
      "Epoch 3 | Step 2275800 | Avg Loss: 0.0157 | Grad Norm: 0.00892187\n",
      "Epoch 3 | Step 2275900 | Avg Loss: 0.0157 | Grad Norm: 0.00939716\n",
      "Epoch 3 | Step 2276000 | Avg Loss: 0.0160 | Grad Norm: 0.00900850\n",
      "Epoch 3 | Step 2276100 | Avg Loss: 0.0162 | Grad Norm: 0.00876890\n",
      "Epoch 3 | Step 2276200 | Avg Loss: 0.0157 | Grad Norm: 0.00909318\n",
      "Epoch 3 | Step 2276300 | Avg Loss: 0.0160 | Grad Norm: 0.00862604\n",
      "Epoch 3 | Step 2276400 | Avg Loss: 0.0161 | Grad Norm: 0.00920721\n",
      "Epoch 3 | Step 2276500 | Avg Loss: 0.0159 | Grad Norm: 0.01021693\n",
      "Epoch 3 | Step 2276600 | Avg Loss: 0.0155 | Grad Norm: 0.00746473\n",
      "Epoch 3 | Step 2276700 | Avg Loss: 0.0155 | Grad Norm: 0.01022686\n",
      "Epoch 3 | Step 2276800 | Avg Loss: 0.0155 | Grad Norm: 0.00936794\n",
      "Epoch 3 | Step 2276900 | Avg Loss: 0.0155 | Grad Norm: 0.00815861\n",
      "Epoch 3 | Step 2277000 | Avg Loss: 0.0154 | Grad Norm: 0.00801093\n",
      "Epoch 3 | Step 2277100 | Avg Loss: 0.0155 | Grad Norm: 0.00938667\n",
      "Epoch 3 | Step 2277200 | Avg Loss: 0.0154 | Grad Norm: 0.01326882\n",
      "Epoch 3 | Step 2277300 | Avg Loss: 0.0155 | Grad Norm: 0.00895870\n",
      "Epoch 3 | Step 2277400 | Avg Loss: 0.0150 | Grad Norm: 0.00919889\n",
      "Epoch 3 | Step 2277500 | Avg Loss: 0.0152 | Grad Norm: 0.00746453\n",
      "Epoch 3 | Step 2277600 | Avg Loss: 0.0156 | Grad Norm: 0.00938633\n",
      "Epoch 3 | Step 2277700 | Avg Loss: 0.0158 | Grad Norm: 0.00869060\n",
      "Epoch 3 | Step 2277800 | Avg Loss: 0.0155 | Grad Norm: 0.00935820\n",
      "Epoch 3 | Step 2277900 | Avg Loss: 0.0155 | Grad Norm: 0.00845996\n",
      "Epoch 3 | Step 2278000 | Avg Loss: 0.0156 | Grad Norm: 0.01020504\n",
      "Epoch 3 | Step 2278100 | Avg Loss: 0.0157 | Grad Norm: 0.00857062\n",
      "Epoch 3 | Step 2278200 | Avg Loss: 0.0158 | Grad Norm: 0.00935159\n",
      "Epoch 3 | Step 2278300 | Avg Loss: 0.0160 | Grad Norm: 0.01199430\n",
      "Epoch 3 | Step 2278400 | Avg Loss: 0.0161 | Grad Norm: 0.00920350\n",
      "Epoch 3 | Step 2278500 | Avg Loss: 0.0159 | Grad Norm: 0.00994782\n",
      "Epoch 3 | Step 2278600 | Avg Loss: 0.0160 | Grad Norm: 0.01350523\n",
      "Epoch 3 | Step 2278700 | Avg Loss: 0.0164 | Grad Norm: 0.00962638\n",
      "Epoch 3 | Step 2278800 | Avg Loss: 0.0162 | Grad Norm: 0.00942095\n",
      "Epoch 3 | Step 2278900 | Avg Loss: 0.0160 | Grad Norm: 0.00994798\n",
      "Epoch 3 | Step 2279000 | Avg Loss: 0.0159 | Grad Norm: 0.00923021\n",
      "Epoch 3 | Step 2279100 | Avg Loss: 0.0158 | Grad Norm: 0.00811509\n",
      "Epoch 3 | Step 2279200 | Avg Loss: 0.0152 | Grad Norm: 0.00753356\n",
      "Epoch 3 | Step 2279300 | Avg Loss: 0.0158 | Grad Norm: 0.00873763\n",
      "Epoch 3 | Step 2279400 | Avg Loss: 0.0160 | Grad Norm: 0.01057239\n",
      "Epoch 3 | Step 2279500 | Avg Loss: 0.0159 | Grad Norm: 0.00899264\n",
      "Epoch 3 | Step 2279600 | Avg Loss: 0.0158 | Grad Norm: 0.01049784\n",
      "Epoch 3 | Step 2279700 | Avg Loss: 0.0159 | Grad Norm: 0.01200976\n",
      "Epoch 3 | Step 2279800 | Avg Loss: 0.0156 | Grad Norm: 0.00978865\n",
      "Epoch 3 | Step 2279900 | Avg Loss: 0.0159 | Grad Norm: 0.00859539\n",
      "Epoch 3 | Step 2280000 | Avg Loss: 0.0159 | Grad Norm: 0.00913753\n",
      "Epoch 3 | Step 2280100 | Avg Loss: 0.0160 | Grad Norm: 0.00852409\n",
      "Epoch 3 | Step 2280200 | Avg Loss: 0.0161 | Grad Norm: 0.01075697\n",
      "Epoch 3 | Step 2280300 | Avg Loss: 0.0161 | Grad Norm: 0.00802952\n",
      "Epoch 3 | Step 2280400 | Avg Loss: 0.0162 | Grad Norm: 0.00877552\n",
      "Epoch 3 | Step 2280500 | Avg Loss: 0.0159 | Grad Norm: 0.01197203\n",
      "Epoch 3 | Step 2280600 | Avg Loss: 0.0161 | Grad Norm: 0.00908054\n",
      "Epoch 3 | Step 2280700 | Avg Loss: 0.0161 | Grad Norm: 0.00921251\n",
      "Epoch 3 | Step 2280800 | Avg Loss: 0.0161 | Grad Norm: 0.00917033\n",
      "Epoch 3 | Step 2280900 | Avg Loss: 0.0162 | Grad Norm: 0.00836030\n",
      "Epoch 3 | Step 2281000 | Avg Loss: 0.0162 | Grad Norm: 0.00897577\n",
      "Epoch 3 | Step 2281100 | Avg Loss: 0.0159 | Grad Norm: 0.01494164\n",
      "Epoch 3 | Step 2281200 | Avg Loss: 0.0157 | Grad Norm: 0.00923729\n",
      "Epoch 3 | Step 2281300 | Avg Loss: 0.0158 | Grad Norm: 0.00932983\n",
      "Epoch 3 | Step 2281400 | Avg Loss: 0.0158 | Grad Norm: 0.01083282\n",
      "Epoch 3 | Step 2281500 | Avg Loss: 0.0154 | Grad Norm: 0.00951634\n",
      "Epoch 3 | Step 2281600 | Avg Loss: 0.0154 | Grad Norm: 0.00941918\n",
      "Epoch 3 | Step 2281700 | Avg Loss: 0.0156 | Grad Norm: 0.00843562\n",
      "Epoch 3 | Step 2281800 | Avg Loss: 0.0155 | Grad Norm: 0.00962665\n",
      "Epoch 3 | Step 2281900 | Avg Loss: 0.0156 | Grad Norm: 0.00901251\n",
      "Epoch 3 | Step 2282000 | Avg Loss: 0.0152 | Grad Norm: 0.01144624\n",
      "Epoch 3 | Step 2282100 | Avg Loss: 0.0152 | Grad Norm: 0.00936513\n",
      "Epoch 3 | Step 2282200 | Avg Loss: 0.0155 | Grad Norm: 0.01015091\n",
      "Epoch 3 | Step 2282300 | Avg Loss: 0.0152 | Grad Norm: 0.00954647\n",
      "Epoch 3 | Step 2282400 | Avg Loss: 0.0156 | Grad Norm: 0.00911199\n",
      "Epoch 3 | Step 2282500 | Avg Loss: 0.0151 | Grad Norm: 0.00822210\n",
      "Epoch 3 | Step 2282600 | Avg Loss: 0.0153 | Grad Norm: 0.00914853\n",
      "Epoch 3 | Step 2282700 | Avg Loss: 0.0155 | Grad Norm: 0.00939058\n",
      "Epoch 3 | Step 2282800 | Avg Loss: 0.0151 | Grad Norm: 0.00899303\n",
      "Epoch 3 | Step 2282900 | Avg Loss: 0.0152 | Grad Norm: 0.00794264\n",
      "Epoch 3 | Step 2283000 | Avg Loss: 0.0154 | Grad Norm: 0.00944164\n",
      "Epoch 3 | Step 2283100 | Avg Loss: 0.0151 | Grad Norm: 0.00848091\n",
      "Epoch 3 | Step 2283200 | Avg Loss: 0.0151 | Grad Norm: 0.00865738\n",
      "Epoch 3 | Step 2283300 | Avg Loss: 0.0148 | Grad Norm: 0.00872254\n",
      "Epoch 3 | Step 2283400 | Avg Loss: 0.0152 | Grad Norm: 0.00937964\n",
      "Epoch 3 | Step 2283500 | Avg Loss: 0.0147 | Grad Norm: 0.00771134\n",
      "Epoch 3 | Step 2283600 | Avg Loss: 0.0151 | Grad Norm: 0.00906841\n",
      "Epoch 3 | Step 2283700 | Avg Loss: 0.0153 | Grad Norm: 0.00940523\n",
      "Epoch 3 | Step 2283800 | Avg Loss: 0.0152 | Grad Norm: 0.00876880\n",
      "Epoch 3 | Step 2283900 | Avg Loss: 0.0153 | Grad Norm: 0.00785777\n",
      "Epoch 3 | Step 2284000 | Avg Loss: 0.0156 | Grad Norm: 0.00869828\n",
      "Epoch 3 | Step 2284100 | Avg Loss: 0.0157 | Grad Norm: 0.00904442\n",
      "Epoch 3 | Step 2284200 | Avg Loss: 0.0157 | Grad Norm: 0.00955676\n",
      "Epoch 3 | Step 2284300 | Avg Loss: 0.0156 | Grad Norm: 0.00984779\n",
      "Epoch 3 | Step 2284400 | Avg Loss: 0.0155 | Grad Norm: 0.01030059\n",
      "Epoch 3 | Step 2284500 | Avg Loss: 0.0151 | Grad Norm: 0.00909042\n",
      "Epoch 3 | Step 2284600 | Avg Loss: 0.0153 | Grad Norm: 0.00999092\n",
      "Epoch 3 | Step 2284700 | Avg Loss: 0.0151 | Grad Norm: 0.00975154\n",
      "Epoch 3 | Step 2284800 | Avg Loss: 0.0153 | Grad Norm: 0.00867805\n",
      "Epoch 3 | Step 2284900 | Avg Loss: 0.0155 | Grad Norm: 0.00985603\n",
      "Epoch 3 | Step 2285000 | Avg Loss: 0.0160 | Grad Norm: 0.00923121\n",
      "Epoch 3 | Step 2285100 | Avg Loss: 0.0156 | Grad Norm: 0.00876871\n",
      "Epoch 3 | Step 2285200 | Avg Loss: 0.0158 | Grad Norm: 0.00976905\n",
      "Epoch 3 | Step 2285300 | Avg Loss: 0.0157 | Grad Norm: 0.01150252\n",
      "Epoch 3 | Step 2285400 | Avg Loss: 0.0155 | Grad Norm: 0.00926257\n",
      "Epoch 3 | Step 2285500 | Avg Loss: 0.0155 | Grad Norm: 0.00980532\n",
      "Epoch 3 | Step 2285600 | Avg Loss: 0.0154 | Grad Norm: 0.01039348\n",
      "Epoch 3 | Step 2285700 | Avg Loss: 0.0156 | Grad Norm: 0.00869017\n",
      "Epoch 3 | Step 2285800 | Avg Loss: 0.0154 | Grad Norm: 0.00820133\n",
      "Epoch 3 | Step 2285900 | Avg Loss: 0.0152 | Grad Norm: 0.00890205\n",
      "Epoch 3 | Step 2286000 | Avg Loss: 0.0154 | Grad Norm: 0.00747945\n",
      "Epoch 3 | Step 2286100 | Avg Loss: 0.0151 | Grad Norm: 0.00896832\n",
      "Epoch 3 | Step 2286200 | Avg Loss: 0.0152 | Grad Norm: 0.01071747\n",
      "Epoch 3 | Step 2286300 | Avg Loss: 0.0153 | Grad Norm: 0.00825606\n",
      "Epoch 3 | Step 2286400 | Avg Loss: 0.0156 | Grad Norm: 0.00982069\n",
      "Epoch 3 | Step 2286500 | Avg Loss: 0.0157 | Grad Norm: 0.00822580\n",
      "Epoch 3 | Step 2286600 | Avg Loss: 0.0156 | Grad Norm: 0.00973470\n",
      "Epoch 3 | Step 2286700 | Avg Loss: 0.0156 | Grad Norm: 0.00919398\n",
      "Epoch 3 | Step 2286800 | Avg Loss: 0.0158 | Grad Norm: 0.00949841\n",
      "Epoch 3 | Step 2286900 | Avg Loss: 0.0158 | Grad Norm: 0.00951766\n",
      "Epoch 3 | Step 2287000 | Avg Loss: 0.0157 | Grad Norm: 0.00918725\n",
      "Epoch 3 | Step 2287100 | Avg Loss: 0.0156 | Grad Norm: 0.00870040\n",
      "Epoch 3 | Step 2287200 | Avg Loss: 0.0158 | Grad Norm: 0.00811571\n",
      "Epoch 3 | Step 2287300 | Avg Loss: 0.0160 | Grad Norm: 0.00949959\n",
      "Epoch 3 | Step 2287400 | Avg Loss: 0.0154 | Grad Norm: 0.00949150\n",
      "Epoch 3 | Step 2287500 | Avg Loss: 0.0152 | Grad Norm: 0.00878703\n",
      "Epoch 3 | Step 2287600 | Avg Loss: 0.0155 | Grad Norm: 0.00962857\n",
      "Epoch 3 | Step 2287700 | Avg Loss: 0.0152 | Grad Norm: 0.00948590\n",
      "Epoch 3 | Step 2287800 | Avg Loss: 0.0155 | Grad Norm: 0.00914506\n",
      "Epoch 3 | Step 2287900 | Avg Loss: 0.0154 | Grad Norm: 0.00875452\n",
      "Epoch 3 | Step 2288000 | Avg Loss: 0.0149 | Grad Norm: 0.00823085\n",
      "Epoch 3 | Step 2288100 | Avg Loss: 0.0150 | Grad Norm: 0.00832031\n",
      "Epoch 3 | Step 2288200 | Avg Loss: 0.0148 | Grad Norm: 0.00937120\n",
      "Epoch 3 | Step 2288300 | Avg Loss: 0.0145 | Grad Norm: 0.00886195\n",
      "Epoch 3 | Step 2288400 | Avg Loss: 0.0147 | Grad Norm: 0.00990582\n",
      "Epoch 3 | Step 2288500 | Avg Loss: 0.0151 | Grad Norm: 0.00822503\n",
      "Epoch 3 | Step 2288600 | Avg Loss: 0.0152 | Grad Norm: 0.00925753\n",
      "Epoch 3 | Step 2288700 | Avg Loss: 0.0155 | Grad Norm: 0.00983373\n",
      "Epoch 3 | Step 2288800 | Avg Loss: 0.0153 | Grad Norm: 0.00897488\n",
      "Epoch 3 | Step 2288900 | Avg Loss: 0.0150 | Grad Norm: 0.00823011\n",
      "Epoch 3 | Step 2289000 | Avg Loss: 0.0152 | Grad Norm: 0.01019414\n",
      "Epoch 3 | Step 2289100 | Avg Loss: 0.0149 | Grad Norm: 0.00915653\n",
      "Epoch 3 | Step 2289200 | Avg Loss: 0.0155 | Grad Norm: 0.00807210\n",
      "Epoch 3 | Step 2289300 | Avg Loss: 0.0154 | Grad Norm: 0.00870717\n",
      "Epoch 3 | Step 2289400 | Avg Loss: 0.0156 | Grad Norm: 0.00840589\n",
      "Epoch 3 | Step 2289500 | Avg Loss: 0.0155 | Grad Norm: 0.01094551\n",
      "Epoch 3 | Step 2289600 | Avg Loss: 0.0159 | Grad Norm: 0.01287124\n",
      "Epoch 3 | Step 2289700 | Avg Loss: 0.0155 | Grad Norm: 0.00760618\n",
      "Epoch 3 | Step 2289800 | Avg Loss: 0.0153 | Grad Norm: 0.00908553\n",
      "Epoch 3 | Step 2289900 | Avg Loss: 0.0157 | Grad Norm: 0.00943894\n",
      "Epoch 3 | Step 2290000 | Avg Loss: 0.0151 | Grad Norm: 0.00810459\n",
      "Epoch 3 | Step 2290100 | Avg Loss: 0.0153 | Grad Norm: 0.00926235\n",
      "Epoch 3 | Step 2290200 | Avg Loss: 0.0154 | Grad Norm: 0.00982022\n",
      "Epoch 3 | Step 2290300 | Avg Loss: 0.0156 | Grad Norm: 0.00891160\n",
      "Epoch 3 | Step 2290400 | Avg Loss: 0.0156 | Grad Norm: 0.00836620\n",
      "Epoch 3 | Step 2290500 | Avg Loss: 0.0162 | Grad Norm: 0.00926276\n",
      "Epoch 3 | Step 2290600 | Avg Loss: 0.0158 | Grad Norm: 0.00784569\n",
      "Epoch 3 | Step 2290700 | Avg Loss: 0.0151 | Grad Norm: 0.01004346\n",
      "Epoch 3 | Step 2290800 | Avg Loss: 0.0150 | Grad Norm: 0.00910508\n",
      "Epoch 3 | Step 2290900 | Avg Loss: 0.0153 | Grad Norm: 0.00797913\n",
      "Epoch 3 | Step 2291000 | Avg Loss: 0.0154 | Grad Norm: 0.01016025\n",
      "Epoch 3 | Step 2291100 | Avg Loss: 0.0154 | Grad Norm: 0.00874330\n",
      "Epoch 3 | Step 2291200 | Avg Loss: 0.0156 | Grad Norm: 0.00969997\n",
      "Epoch 3 | Step 2291300 | Avg Loss: 0.0154 | Grad Norm: 0.00913740\n",
      "Epoch 3 | Step 2291400 | Avg Loss: 0.0158 | Grad Norm: 0.00932958\n",
      "Epoch 3 | Step 2291500 | Avg Loss: 0.0159 | Grad Norm: 0.00973511\n",
      "Epoch 3 | Step 2291600 | Avg Loss: 0.0157 | Grad Norm: 0.00840258\n",
      "Epoch 3 | Step 2291700 | Avg Loss: 0.0158 | Grad Norm: 0.01093036\n",
      "Epoch 3 | Step 2291800 | Avg Loss: 0.0156 | Grad Norm: 0.00830820\n",
      "Epoch 3 | Step 2291900 | Avg Loss: 0.0156 | Grad Norm: 0.00858757\n",
      "Epoch 3 | Step 2292000 | Avg Loss: 0.0159 | Grad Norm: 0.00851243\n",
      "Epoch 3 | Step 2292100 | Avg Loss: 0.0158 | Grad Norm: 0.00798777\n",
      "Epoch 3 | Step 2292200 | Avg Loss: 0.0159 | Grad Norm: 0.00905163\n",
      "Epoch 3 | Step 2292300 | Avg Loss: 0.0160 | Grad Norm: 0.00993357\n",
      "Epoch 3 | Step 2292400 | Avg Loss: 0.0160 | Grad Norm: 0.00982185\n",
      "Epoch 3 | Step 2292500 | Avg Loss: 0.0155 | Grad Norm: 0.00984196\n",
      "Epoch 3 | Step 2292600 | Avg Loss: 0.0157 | Grad Norm: 0.00780356\n",
      "Epoch 3 | Step 2292700 | Avg Loss: 0.0156 | Grad Norm: 0.01007489\n",
      "Epoch 3 | Step 2292800 | Avg Loss: 0.0157 | Grad Norm: 0.00908338\n",
      "Epoch 3 | Step 2292900 | Avg Loss: 0.0162 | Grad Norm: 0.00849469\n",
      "Epoch 3 | Step 2293000 | Avg Loss: 0.0161 | Grad Norm: 0.00911797\n",
      "Epoch 3 | Step 2293100 | Avg Loss: 0.0160 | Grad Norm: 0.00826668\n",
      "Epoch 3 | Step 2293200 | Avg Loss: 0.0159 | Grad Norm: 0.00906370\n",
      "Epoch 3 | Step 2293300 | Avg Loss: 0.0158 | Grad Norm: 0.00954011\n",
      "Epoch 3 | Step 2293400 | Avg Loss: 0.0156 | Grad Norm: 0.00792452\n",
      "Epoch 3 | Step 2293500 | Avg Loss: 0.0156 | Grad Norm: 0.01002768\n",
      "Epoch 3 | Step 2293600 | Avg Loss: 0.0155 | Grad Norm: 0.00830468\n",
      "Epoch 3 | Step 2293700 | Avg Loss: 0.0154 | Grad Norm: 0.00982601\n",
      "Epoch 3 | Step 2293800 | Avg Loss: 0.0151 | Grad Norm: 0.00786423\n",
      "Epoch 3 | Step 2293900 | Avg Loss: 0.0154 | Grad Norm: 0.00754583\n",
      "Epoch 3 | Step 2294000 | Avg Loss: 0.0156 | Grad Norm: 0.00897955\n",
      "Epoch 3 | Step 2294100 | Avg Loss: 0.0156 | Grad Norm: 0.00791372\n",
      "Epoch 3 | Step 2294200 | Avg Loss: 0.0157 | Grad Norm: 0.01018575\n",
      "Epoch 3 | Step 2294300 | Avg Loss: 0.0158 | Grad Norm: 0.00878496\n",
      "Epoch 3 | Step 2294400 | Avg Loss: 0.0156 | Grad Norm: 0.00913871\n",
      "Epoch 3 | Step 2294500 | Avg Loss: 0.0155 | Grad Norm: 0.00881524\n",
      "Epoch 3 | Step 2294600 | Avg Loss: 0.0153 | Grad Norm: 0.00808335\n",
      "Epoch 3 | Step 2294700 | Avg Loss: 0.0154 | Grad Norm: 0.01053132\n",
      "Epoch 3 | Step 2294800 | Avg Loss: 0.0157 | Grad Norm: 0.00928742\n",
      "Epoch 3 | Step 2294900 | Avg Loss: 0.0155 | Grad Norm: 0.00949057\n",
      "Epoch 3 | Step 2295000 | Avg Loss: 0.0151 | Grad Norm: 0.00896764\n",
      "Epoch 3 | Step 2295100 | Avg Loss: 0.0149 | Grad Norm: 0.00786969\n",
      "Epoch 3 | Step 2295200 | Avg Loss: 0.0151 | Grad Norm: 0.00879587\n",
      "Epoch 3 | Step 2295300 | Avg Loss: 0.0151 | Grad Norm: 0.00838522\n",
      "Epoch 3 | Step 2295400 | Avg Loss: 0.0151 | Grad Norm: 0.00838818\n",
      "Epoch 3 | Step 2295500 | Avg Loss: 0.0153 | Grad Norm: 0.01055251\n",
      "Epoch 3 | Step 2295600 | Avg Loss: 0.0154 | Grad Norm: 0.00828062\n",
      "Epoch 3 | Step 2295700 | Avg Loss: 0.0151 | Grad Norm: 0.00800254\n",
      "Epoch 3 | Step 2295800 | Avg Loss: 0.0151 | Grad Norm: 0.00877402\n",
      "Epoch 3 | Step 2295900 | Avg Loss: 0.0153 | Grad Norm: 0.01024176\n",
      "Epoch 3 | Step 2296000 | Avg Loss: 0.0152 | Grad Norm: 0.00969482\n",
      "Epoch 3 | Step 2296100 | Avg Loss: 0.0151 | Grad Norm: 0.00865193\n",
      "Epoch 3 | Step 2296200 | Avg Loss: 0.0154 | Grad Norm: 0.00923995\n",
      "Epoch 3 | Step 2296300 | Avg Loss: 0.0157 | Grad Norm: 0.01132149\n",
      "Epoch 3 | Step 2296400 | Avg Loss: 0.0158 | Grad Norm: 0.01116572\n",
      "Epoch 3 | Step 2296500 | Avg Loss: 0.0161 | Grad Norm: 0.00903805\n",
      "Epoch 3 | Step 2296600 | Avg Loss: 0.0158 | Grad Norm: 0.00869190\n",
      "Epoch 3 | Step 2296700 | Avg Loss: 0.0159 | Grad Norm: 0.00911899\n",
      "Epoch 3 | Step 2296800 | Avg Loss: 0.0160 | Grad Norm: 0.00862365\n",
      "Epoch 3 | Step 2296900 | Avg Loss: 0.0156 | Grad Norm: 0.00784156\n",
      "Epoch 3 | Step 2297000 | Avg Loss: 0.0158 | Grad Norm: 0.00807961\n",
      "Epoch 3 | Step 2297100 | Avg Loss: 0.0157 | Grad Norm: 0.00866342\n",
      "Epoch 3 | Step 2297200 | Avg Loss: 0.0152 | Grad Norm: 0.00835046\n",
      "Epoch 3 | Step 2297300 | Avg Loss: 0.0159 | Grad Norm: 0.00908889\n",
      "Epoch 3 | Step 2297400 | Avg Loss: 0.0159 | Grad Norm: 0.00868036\n",
      "Epoch 3 | Step 2297500 | Avg Loss: 0.0161 | Grad Norm: 0.00973382\n",
      "Epoch 3 | Step 2297600 | Avg Loss: 0.0158 | Grad Norm: 0.00982775\n",
      "Epoch 3 | Step 2297700 | Avg Loss: 0.0160 | Grad Norm: 0.01347066\n",
      "Epoch 3 | Step 2297800 | Avg Loss: 0.0156 | Grad Norm: 0.01166917\n",
      "Epoch 3 | Step 2297900 | Avg Loss: 0.0158 | Grad Norm: 0.00921163\n",
      "Epoch 3 | Step 2298000 | Avg Loss: 0.0151 | Grad Norm: 0.00907699\n",
      "Epoch 3 | Step 2298100 | Avg Loss: 0.0151 | Grad Norm: 0.00803254\n",
      "Epoch 3 | Step 2298200 | Avg Loss: 0.0153 | Grad Norm: 0.00876173\n",
      "Epoch 3 | Step 2298300 | Avg Loss: 0.0153 | Grad Norm: 0.01041523\n",
      "Epoch 3 | Step 2298400 | Avg Loss: 0.0154 | Grad Norm: 0.00828146\n",
      "Epoch 3 | Step 2298500 | Avg Loss: 0.0154 | Grad Norm: 0.00863789\n",
      "Epoch 3 | Step 2298600 | Avg Loss: 0.0152 | Grad Norm: 0.00984196\n",
      "Epoch 3 | Step 2298700 | Avg Loss: 0.0151 | Grad Norm: 0.00976447\n",
      "Epoch 3 | Step 2298800 | Avg Loss: 0.0152 | Grad Norm: 0.01044745\n",
      "Epoch 3 | Step 2298900 | Avg Loss: 0.0154 | Grad Norm: 0.00805340\n",
      "Epoch 3 | Step 2299000 | Avg Loss: 0.0155 | Grad Norm: 0.01126244\n",
      "Epoch 3 | Step 2299100 | Avg Loss: 0.0154 | Grad Norm: 0.00943534\n",
      "Epoch 3 | Step 2299200 | Avg Loss: 0.0156 | Grad Norm: 0.00956912\n",
      "Epoch 3 | Step 2299300 | Avg Loss: 0.0155 | Grad Norm: 0.00889003\n",
      "Epoch 3 | Step 2299400 | Avg Loss: 0.0156 | Grad Norm: 0.01231695\n",
      "Epoch 3 | Step 2299500 | Avg Loss: 0.0157 | Grad Norm: 0.00809995\n",
      "Epoch 3 | Step 2299600 | Avg Loss: 0.0157 | Grad Norm: 0.00943302\n",
      "Epoch 3 | Step 2299700 | Avg Loss: 0.0160 | Grad Norm: 0.00982199\n",
      "Epoch 3 | Step 2299800 | Avg Loss: 0.0162 | Grad Norm: 0.00970015\n",
      "Epoch 3 | Step 2299900 | Avg Loss: 0.0165 | Grad Norm: 0.01021963\n",
      "Epoch 3 | Step 2300000 | Avg Loss: 0.0163 | Grad Norm: 0.00764805\n",
      "Saving model at step2300000\n",
      "Epoch 3 | Step 2300100 | Avg Loss: 0.0161 | Grad Norm: 0.00979855\n",
      "Epoch 3 | Step 2300200 | Avg Loss: 0.0159 | Grad Norm: 0.00819560\n",
      "Epoch 3 | Step 2300300 | Avg Loss: 0.0158 | Grad Norm: 0.01068464\n",
      "Epoch 3 | Step 2300400 | Avg Loss: 0.0159 | Grad Norm: 0.00803156\n",
      "Epoch 3 | Step 2300500 | Avg Loss: 0.0153 | Grad Norm: 0.00882882\n",
      "Epoch 3 | Step 2300600 | Avg Loss: 0.0156 | Grad Norm: 0.00911930\n",
      "Epoch 3 | Step 2300700 | Avg Loss: 0.0156 | Grad Norm: 0.00910872\n",
      "Epoch 3 | Step 2300800 | Avg Loss: 0.0159 | Grad Norm: 0.00913796\n",
      "Epoch 3 | Step 2300900 | Avg Loss: 0.0155 | Grad Norm: 0.00906497\n",
      "Epoch 3 | Step 2301000 | Avg Loss: 0.0153 | Grad Norm: 0.00916707\n",
      "Epoch 3 | Step 2301100 | Avg Loss: 0.0154 | Grad Norm: 0.00923259\n",
      "Epoch 3 | Step 2301200 | Avg Loss: 0.0153 | Grad Norm: 0.01012448\n",
      "Epoch 3 | Step 2301300 | Avg Loss: 0.0155 | Grad Norm: 0.00970161\n",
      "Epoch 3 | Step 2301400 | Avg Loss: 0.0158 | Grad Norm: 0.00856811\n",
      "Epoch 3 | Step 2301500 | Avg Loss: 0.0158 | Grad Norm: 0.00928201\n",
      "Epoch 3 | Step 2301600 | Avg Loss: 0.0164 | Grad Norm: 0.00930150\n",
      "Epoch 3 | Step 2301700 | Avg Loss: 0.0164 | Grad Norm: 0.00795412\n",
      "Epoch 3 | Step 2301800 | Avg Loss: 0.0164 | Grad Norm: 0.01092171\n",
      "Epoch 3 | Step 2301900 | Avg Loss: 0.0162 | Grad Norm: 0.01281039\n",
      "Epoch 3 | Step 2302000 | Avg Loss: 0.0161 | Grad Norm: 0.00945355\n",
      "Epoch 3 | Step 2302100 | Avg Loss: 0.0161 | Grad Norm: 0.00898222\n",
      "Epoch 3 | Step 2302200 | Avg Loss: 0.0159 | Grad Norm: 0.00946748\n",
      "Epoch 3 | Step 2302300 | Avg Loss: 0.0158 | Grad Norm: 0.00947853\n",
      "Epoch 3 | Step 2302400 | Avg Loss: 0.0155 | Grad Norm: 0.00977728\n",
      "Epoch 3 | Step 2302500 | Avg Loss: 0.0156 | Grad Norm: 0.00915386\n",
      "Epoch 3 | Step 2302600 | Avg Loss: 0.0158 | Grad Norm: 0.01069540\n",
      "Epoch 3 | Step 2302700 | Avg Loss: 0.0160 | Grad Norm: 0.00982053\n",
      "Epoch 3 | Step 2302800 | Avg Loss: 0.0160 | Grad Norm: 0.00907208\n",
      "Epoch 3 | Step 2302900 | Avg Loss: 0.0160 | Grad Norm: 0.00919109\n",
      "Epoch 3 | Step 2303000 | Avg Loss: 0.0162 | Grad Norm: 0.01078521\n",
      "Epoch 3 | Step 2303100 | Avg Loss: 0.0162 | Grad Norm: 0.01006545\n",
      "Epoch 3 | Step 2303200 | Avg Loss: 0.0161 | Grad Norm: 0.00885808\n",
      "Epoch 3 | Step 2303300 | Avg Loss: 0.0160 | Grad Norm: 0.00876310\n",
      "Epoch 3 | Step 2303400 | Avg Loss: 0.0155 | Grad Norm: 0.00872306\n",
      "Epoch 3 | Step 2303500 | Avg Loss: 0.0161 | Grad Norm: 0.00971938\n",
      "Epoch 3 | Step 2303600 | Avg Loss: 0.0161 | Grad Norm: 0.00914676\n",
      "Epoch 3 | Step 2303700 | Avg Loss: 0.0157 | Grad Norm: 0.00935803\n",
      "Epoch 3 | Step 2303800 | Avg Loss: 0.0158 | Grad Norm: 0.00761461\n",
      "Epoch 3 | Step 2303900 | Avg Loss: 0.0161 | Grad Norm: 0.00977146\n",
      "Epoch 3 | Step 2304000 | Avg Loss: 0.0157 | Grad Norm: 0.00958432\n",
      "Epoch 3 | Step 2304100 | Avg Loss: 0.0160 | Grad Norm: 0.00894180\n",
      "Epoch 3 | Step 2304200 | Avg Loss: 0.0158 | Grad Norm: 0.01025251\n",
      "Epoch 3 | Step 2304300 | Avg Loss: 0.0159 | Grad Norm: 0.01213435\n",
      "Epoch 3 | Step 2304400 | Avg Loss: 0.0159 | Grad Norm: 0.00827214\n",
      "Epoch 3 | Step 2304500 | Avg Loss: 0.0160 | Grad Norm: 0.00883208\n",
      "Epoch 3 | Step 2304600 | Avg Loss: 0.0157 | Grad Norm: 0.00943214\n",
      "Epoch 3 | Step 2304700 | Avg Loss: 0.0157 | Grad Norm: 0.00928609\n",
      "Epoch 3 | Step 2304800 | Avg Loss: 0.0154 | Grad Norm: 0.00837499\n",
      "Epoch 3 | Step 2304900 | Avg Loss: 0.0156 | Grad Norm: 0.00883658\n",
      "Epoch 3 | Step 2305000 | Avg Loss: 0.0155 | Grad Norm: 0.00802163\n",
      "Epoch 3 | Step 2305100 | Avg Loss: 0.0154 | Grad Norm: 0.00948857\n",
      "Epoch 3 | Step 2305200 | Avg Loss: 0.0158 | Grad Norm: 0.00927049\n",
      "Epoch 3 | Step 2305300 | Avg Loss: 0.0160 | Grad Norm: 0.00915298\n",
      "Epoch 3 | Step 2305400 | Avg Loss: 0.0159 | Grad Norm: 0.00874365\n",
      "Epoch 3 | Step 2305500 | Avg Loss: 0.0155 | Grad Norm: 0.01008370\n",
      "Epoch 3 | Step 2305600 | Avg Loss: 0.0163 | Grad Norm: 0.00981266\n",
      "Epoch 3 | Step 2305700 | Avg Loss: 0.0162 | Grad Norm: 0.01036633\n",
      "Epoch 3 | Step 2305800 | Avg Loss: 0.0160 | Grad Norm: 0.00992274\n",
      "Epoch 3 | Step 2305900 | Avg Loss: 0.0160 | Grad Norm: 0.00911657\n",
      "Epoch 3 | Step 2306000 | Avg Loss: 0.0157 | Grad Norm: 0.00860626\n",
      "Epoch 3 | Step 2306100 | Avg Loss: 0.0160 | Grad Norm: 0.01005118\n",
      "Epoch 3 | Step 2306200 | Avg Loss: 0.0159 | Grad Norm: 0.00920384\n",
      "Epoch 3 | Step 2306300 | Avg Loss: 0.0156 | Grad Norm: 0.00977793\n",
      "Epoch 3 | Step 2306400 | Avg Loss: 0.0157 | Grad Norm: 0.00868218\n",
      "Epoch 3 | Step 2306500 | Avg Loss: 0.0157 | Grad Norm: 0.00950153\n",
      "Epoch 3 | Step 2306600 | Avg Loss: 0.0155 | Grad Norm: 0.01003885\n",
      "Epoch 3 | Step 2306700 | Avg Loss: 0.0156 | Grad Norm: 0.00847567\n",
      "Epoch 3 | Step 2306800 | Avg Loss: 0.0163 | Grad Norm: 0.00996583\n",
      "Epoch 3 | Step 2306900 | Avg Loss: 0.0162 | Grad Norm: 0.00856308\n",
      "Epoch 3 | Step 2307000 | Avg Loss: 0.0159 | Grad Norm: 0.00909350\n",
      "Epoch 3 | Step 2307100 | Avg Loss: 0.0160 | Grad Norm: 0.01091168\n",
      "Epoch 3 | Step 2307200 | Avg Loss: 0.0161 | Grad Norm: 0.00850448\n",
      "Epoch 3 | Step 2307300 | Avg Loss: 0.0160 | Grad Norm: 0.00981692\n",
      "Epoch 3 | Step 2307400 | Avg Loss: 0.0159 | Grad Norm: 0.00853671\n",
      "Epoch 3 | Step 2307500 | Avg Loss: 0.0158 | Grad Norm: 0.00980304\n",
      "Epoch 3 | Step 2307600 | Avg Loss: 0.0155 | Grad Norm: 0.00891848\n",
      "Epoch 3 | Step 2307700 | Avg Loss: 0.0156 | Grad Norm: 0.00829251\n",
      "Epoch 3 | Step 2307800 | Avg Loss: 0.0156 | Grad Norm: 0.00940237\n",
      "Epoch 3 | Step 2307900 | Avg Loss: 0.0153 | Grad Norm: 0.00987387\n",
      "Epoch 3 | Step 2308000 | Avg Loss: 0.0152 | Grad Norm: 0.00899779\n",
      "Epoch 3 | Step 2308100 | Avg Loss: 0.0148 | Grad Norm: 0.00846664\n",
      "Epoch 3 | Step 2308200 | Avg Loss: 0.0151 | Grad Norm: 0.00809003\n",
      "Epoch 3 | Step 2308300 | Avg Loss: 0.0147 | Grad Norm: 0.00752361\n",
      "Epoch 3 | Step 2308400 | Avg Loss: 0.0149 | Grad Norm: 0.00826049\n",
      "Epoch 3 | Step 2308500 | Avg Loss: 0.0152 | Grad Norm: 0.00805348\n",
      "Epoch 3 | Step 2308600 | Avg Loss: 0.0157 | Grad Norm: 0.00893288\n",
      "Epoch 3 | Step 2308700 | Avg Loss: 0.0157 | Grad Norm: 0.00792091\n",
      "Epoch 3 | Step 2308800 | Avg Loss: 0.0160 | Grad Norm: 0.00986660\n",
      "Epoch 3 | Step 2308900 | Avg Loss: 0.0160 | Grad Norm: 0.01199801\n",
      "Epoch 3 | Step 2309000 | Avg Loss: 0.0156 | Grad Norm: 0.00911674\n",
      "Epoch 3 | Step 2309100 | Avg Loss: 0.0157 | Grad Norm: 0.00955810\n",
      "Epoch 3 | Step 2309200 | Avg Loss: 0.0155 | Grad Norm: 0.00821809\n",
      "Epoch 3 | Step 2309300 | Avg Loss: 0.0160 | Grad Norm: 0.00974914\n",
      "Epoch 3 | Step 2309400 | Avg Loss: 0.0158 | Grad Norm: 0.00935940\n",
      "Epoch 3 | Step 2309500 | Avg Loss: 0.0155 | Grad Norm: 0.01079933\n",
      "Epoch 3 | Step 2309600 | Avg Loss: 0.0156 | Grad Norm: 0.01087314\n",
      "Epoch 3 | Step 2309700 | Avg Loss: 0.0157 | Grad Norm: 0.01132264\n",
      "Epoch 3 | Step 2309800 | Avg Loss: 0.0160 | Grad Norm: 0.00947514\n",
      "Epoch 3 | Step 2309900 | Avg Loss: 0.0157 | Grad Norm: 0.01204229\n",
      "Epoch 3 | Step 2310000 | Avg Loss: 0.0156 | Grad Norm: 0.01154159\n",
      "Epoch 3 | Step 2310100 | Avg Loss: 0.0159 | Grad Norm: 0.01173552\n",
      "Epoch 3 | Step 2310200 | Avg Loss: 0.0158 | Grad Norm: 0.00873160\n",
      "Epoch 3 | Step 2310300 | Avg Loss: 0.0159 | Grad Norm: 0.01069628\n",
      "Epoch 3 | Step 2310400 | Avg Loss: 0.0158 | Grad Norm: 0.01076965\n",
      "Epoch 3 | Step 2310500 | Avg Loss: 0.0158 | Grad Norm: 0.00902947\n",
      "Epoch 3 | Step 2310600 | Avg Loss: 0.0163 | Grad Norm: 0.01040670\n",
      "Epoch 3 | Step 2310700 | Avg Loss: 0.0160 | Grad Norm: 0.01017470\n",
      "Epoch 3 | Step 2310800 | Avg Loss: 0.0159 | Grad Norm: 0.00859487\n",
      "Epoch 3 | Step 2310900 | Avg Loss: 0.0157 | Grad Norm: 0.00847548\n",
      "Epoch 3 | Step 2311000 | Avg Loss: 0.0155 | Grad Norm: 0.01428186\n",
      "Epoch 3 | Step 2311100 | Avg Loss: 0.0158 | Grad Norm: 0.00948535\n",
      "Epoch 3 | Step 2311200 | Avg Loss: 0.0154 | Grad Norm: 0.00877258\n",
      "Epoch 3 | Step 2311300 | Avg Loss: 0.0160 | Grad Norm: 0.00969138\n",
      "Epoch 3 | Step 2311400 | Avg Loss: 0.0158 | Grad Norm: 0.00822733\n",
      "Epoch 3 | Step 2311500 | Avg Loss: 0.0155 | Grad Norm: 0.00775716\n",
      "Epoch 3 | Step 2311600 | Avg Loss: 0.0152 | Grad Norm: 0.00901914\n",
      "Epoch 3 | Step 2311700 | Avg Loss: 0.0157 | Grad Norm: 0.01039183\n",
      "Epoch 3 | Step 2311800 | Avg Loss: 0.0156 | Grad Norm: 0.01076336\n",
      "Epoch 3 | Step 2311900 | Avg Loss: 0.0157 | Grad Norm: 0.00881877\n",
      "Epoch 3 | Step 2312000 | Avg Loss: 0.0159 | Grad Norm: 0.00833974\n",
      "Epoch 3 | Step 2312100 | Avg Loss: 0.0163 | Grad Norm: 0.00887324\n",
      "Epoch 3 | Step 2312200 | Avg Loss: 0.0159 | Grad Norm: 0.01033682\n",
      "Epoch 3 | Step 2312300 | Avg Loss: 0.0159 | Grad Norm: 0.00947063\n",
      "Epoch 3 | Step 2312400 | Avg Loss: 0.0155 | Grad Norm: 0.00919189\n",
      "Epoch 3 | Step 2312500 | Avg Loss: 0.0157 | Grad Norm: 0.00987599\n",
      "Epoch 3 | Step 2312600 | Avg Loss: 0.0155 | Grad Norm: 0.00887731\n",
      "Epoch 3 | Step 2312700 | Avg Loss: 0.0154 | Grad Norm: 0.00977777\n",
      "Epoch 3 | Step 2312800 | Avg Loss: 0.0155 | Grad Norm: 0.00907934\n",
      "Epoch 3 | Step 2312900 | Avg Loss: 0.0156 | Grad Norm: 0.01116003\n",
      "Epoch 3 | Step 2313000 | Avg Loss: 0.0158 | Grad Norm: 0.01004766\n",
      "Epoch 3 | Step 2313100 | Avg Loss: 0.0162 | Grad Norm: 0.00931241\n",
      "Epoch 3 | Step 2313200 | Avg Loss: 0.0159 | Grad Norm: 0.00918810\n",
      "Epoch 3 | Step 2313300 | Avg Loss: 0.0157 | Grad Norm: 0.00980971\n",
      "Epoch 3 | Step 2313400 | Avg Loss: 0.0158 | Grad Norm: 0.01096401\n",
      "Epoch 3 | Step 2313500 | Avg Loss: 0.0160 | Grad Norm: 0.00848539\n",
      "Epoch 3 | Step 2313600 | Avg Loss: 0.0158 | Grad Norm: 0.01079463\n",
      "Epoch 3 | Step 2313700 | Avg Loss: 0.0154 | Grad Norm: 0.00824119\n",
      "Epoch 3 | Step 2313800 | Avg Loss: 0.0153 | Grad Norm: 0.00965635\n",
      "Epoch 3 | Step 2313900 | Avg Loss: 0.0155 | Grad Norm: 0.00925797\n",
      "Epoch 3 | Step 2314000 | Avg Loss: 0.0155 | Grad Norm: 0.00968606\n",
      "Epoch 3 | Step 2314100 | Avg Loss: 0.0156 | Grad Norm: 0.00872887\n",
      "Epoch 3 | Step 2314200 | Avg Loss: 0.0158 | Grad Norm: 0.00986759\n",
      "Epoch 3 | Step 2314300 | Avg Loss: 0.0163 | Grad Norm: 0.00940483\n",
      "Epoch 3 | Step 2314400 | Avg Loss: 0.0165 | Grad Norm: 0.01009506\n",
      "Epoch 3 | Step 2314500 | Avg Loss: 0.0165 | Grad Norm: 0.00986319\n",
      "Epoch 3 | Step 2314600 | Avg Loss: 0.0165 | Grad Norm: 0.00915834\n",
      "Epoch 3 | Step 2314700 | Avg Loss: 0.0167 | Grad Norm: 0.00915384\n",
      "Epoch 3 | Step 2314800 | Avg Loss: 0.0164 | Grad Norm: 0.01179908\n",
      "Epoch 3 | Step 2314900 | Avg Loss: 0.0162 | Grad Norm: 0.00825465\n",
      "Epoch 3 | Step 2315000 | Avg Loss: 0.0161 | Grad Norm: 0.00891760\n",
      "Epoch 3 | Step 2315100 | Avg Loss: 0.0164 | Grad Norm: 0.00926609\n",
      "Epoch 3 | Step 2315200 | Avg Loss: 0.0164 | Grad Norm: 0.00840575\n",
      "Epoch 3 | Step 2315300 | Avg Loss: 0.0160 | Grad Norm: 0.00965035\n",
      "Epoch 3 | Step 2315400 | Avg Loss: 0.0156 | Grad Norm: 0.00918566\n",
      "Epoch 3 | Step 2315500 | Avg Loss: 0.0157 | Grad Norm: 0.00893647\n",
      "Epoch 3 | Step 2315600 | Avg Loss: 0.0156 | Grad Norm: 0.00956801\n",
      "Epoch 3 | Step 2315700 | Avg Loss: 0.0160 | Grad Norm: 0.00850330\n",
      "Epoch 3 | Step 2315800 | Avg Loss: 0.0158 | Grad Norm: 0.00962558\n",
      "Epoch 3 | Step 2315900 | Avg Loss: 0.0157 | Grad Norm: 0.00862416\n",
      "Epoch 3 | Step 2316000 | Avg Loss: 0.0160 | Grad Norm: 0.00937983\n",
      "Epoch 3 | Step 2316100 | Avg Loss: 0.0158 | Grad Norm: 0.00937420\n",
      "Epoch 3 | Step 2316200 | Avg Loss: 0.0156 | Grad Norm: 0.01350709\n",
      "Epoch 3 | Step 2316300 | Avg Loss: 0.0153 | Grad Norm: 0.00905980\n",
      "Epoch 3 | Step 2316400 | Avg Loss: 0.0155 | Grad Norm: 0.01066593\n",
      "Epoch 3 | Step 2316500 | Avg Loss: 0.0156 | Grad Norm: 0.00969948\n",
      "Epoch 3 | Step 2316600 | Avg Loss: 0.0155 | Grad Norm: 0.00905563\n",
      "Epoch 3 | Step 2316700 | Avg Loss: 0.0158 | Grad Norm: 0.01075734\n",
      "Epoch 3 | Step 2316800 | Avg Loss: 0.0162 | Grad Norm: 0.00991802\n",
      "Epoch 3 | Step 2316900 | Avg Loss: 0.0156 | Grad Norm: 0.01076779\n",
      "Epoch 3 | Step 2317000 | Avg Loss: 0.0159 | Grad Norm: 0.00972638\n",
      "Epoch 3 | Step 2317100 | Avg Loss: 0.0161 | Grad Norm: 0.00794919\n",
      "Epoch 3 | Step 2317200 | Avg Loss: 0.0157 | Grad Norm: 0.01014617\n",
      "Epoch 3 | Step 2317300 | Avg Loss: 0.0160 | Grad Norm: 0.00859207\n",
      "Epoch 3 | Step 2317400 | Avg Loss: 0.0157 | Grad Norm: 0.00958520\n",
      "Epoch 3 | Step 2317500 | Avg Loss: 0.0161 | Grad Norm: 0.00825191\n",
      "Epoch 3 | Step 2317600 | Avg Loss: 0.0160 | Grad Norm: 0.01058642\n",
      "Epoch 3 | Step 2317700 | Avg Loss: 0.0159 | Grad Norm: 0.01008549\n",
      "Epoch 3 | Step 2317800 | Avg Loss: 0.0157 | Grad Norm: 0.01017077\n",
      "Epoch 3 | Step 2317900 | Avg Loss: 0.0157 | Grad Norm: 0.00864989\n",
      "Epoch 3 | Step 2318000 | Avg Loss: 0.0153 | Grad Norm: 0.00923332\n",
      "Epoch 3 | Step 2318100 | Avg Loss: 0.0152 | Grad Norm: 0.00925105\n",
      "Epoch 3 | Step 2318200 | Avg Loss: 0.0157 | Grad Norm: 0.01146179\n",
      "Epoch 3 | Step 2318300 | Avg Loss: 0.0160 | Grad Norm: 0.00798305\n",
      "Epoch 3 | Step 2318400 | Avg Loss: 0.0156 | Grad Norm: 0.00767801\n",
      "Epoch 3 | Step 2318500 | Avg Loss: 0.0156 | Grad Norm: 0.00917069\n",
      "Epoch 3 | Step 2318600 | Avg Loss: 0.0156 | Grad Norm: 0.00884553\n",
      "Epoch 3 | Step 2318700 | Avg Loss: 0.0160 | Grad Norm: 0.00921406\n",
      "Epoch 3 | Step 2318800 | Avg Loss: 0.0162 | Grad Norm: 0.01053794\n",
      "Epoch 3 | Step 2318900 | Avg Loss: 0.0160 | Grad Norm: 0.00863584\n",
      "Epoch 3 | Step 2319000 | Avg Loss: 0.0158 | Grad Norm: 0.00936805\n",
      "Epoch 3 | Step 2319100 | Avg Loss: 0.0159 | Grad Norm: 0.01040366\n",
      "Epoch 3 | Step 2319200 | Avg Loss: 0.0158 | Grad Norm: 0.01004749\n",
      "Epoch 3 | Step 2319300 | Avg Loss: 0.0162 | Grad Norm: 0.00863712\n",
      "Epoch 3 | Step 2319400 | Avg Loss: 0.0156 | Grad Norm: 0.01053559\n",
      "Epoch 3 | Step 2319500 | Avg Loss: 0.0160 | Grad Norm: 0.00817832\n",
      "Epoch 3 | Step 2319600 | Avg Loss: 0.0159 | Grad Norm: 0.00874582\n",
      "Epoch 3 | Step 2319700 | Avg Loss: 0.0157 | Grad Norm: 0.00907867\n",
      "Epoch 3 | Step 2319800 | Avg Loss: 0.0157 | Grad Norm: 0.01001819\n",
      "Epoch 3 | Step 2319900 | Avg Loss: 0.0152 | Grad Norm: 0.00839838\n",
      "Epoch 3 | Step 2320000 | Avg Loss: 0.0151 | Grad Norm: 0.00808633\n",
      "Epoch 3 | Step 2320100 | Avg Loss: 0.0156 | Grad Norm: 0.01007130\n",
      "Epoch 3 | Step 2320200 | Avg Loss: 0.0153 | Grad Norm: 0.00855191\n",
      "Epoch 3 | Step 2320300 | Avg Loss: 0.0151 | Grad Norm: 0.00879071\n",
      "Epoch 3 | Step 2320400 | Avg Loss: 0.0154 | Grad Norm: 0.00925946\n",
      "Epoch 3 | Step 2320500 | Avg Loss: 0.0154 | Grad Norm: 0.01135650\n",
      "Epoch 3 | Step 2320600 | Avg Loss: 0.0152 | Grad Norm: 0.00785401\n",
      "Epoch 3 | Step 2320700 | Avg Loss: 0.0151 | Grad Norm: 0.00951158\n",
      "Epoch 3 | Step 2320800 | Avg Loss: 0.0152 | Grad Norm: 0.00847264\n",
      "Epoch 3 | Step 2320900 | Avg Loss: 0.0155 | Grad Norm: 0.00904898\n",
      "Epoch 3 | Step 2321000 | Avg Loss: 0.0157 | Grad Norm: 0.00827925\n",
      "Epoch 3 | Step 2321100 | Avg Loss: 0.0155 | Grad Norm: 0.00891345\n",
      "Epoch 3 | Step 2321200 | Avg Loss: 0.0158 | Grad Norm: 0.00824173\n",
      "Epoch 3 | Step 2321300 | Avg Loss: 0.0161 | Grad Norm: 0.00912141\n",
      "Epoch 3 | Step 2321400 | Avg Loss: 0.0159 | Grad Norm: 0.00899488\n",
      "Epoch 3 | Step 2321500 | Avg Loss: 0.0157 | Grad Norm: 0.00871766\n",
      "Epoch 3 | Step 2321600 | Avg Loss: 0.0161 | Grad Norm: 0.00853177\n",
      "Epoch 3 | Step 2321700 | Avg Loss: 0.0155 | Grad Norm: 0.00796529\n",
      "Epoch 3 | Step 2321800 | Avg Loss: 0.0155 | Grad Norm: 0.00772277\n",
      "Epoch 3 | Step 2321900 | Avg Loss: 0.0159 | Grad Norm: 0.00876024\n",
      "Epoch 3 | Step 2322000 | Avg Loss: 0.0160 | Grad Norm: 0.00926241\n",
      "Epoch 3 | Step 2322100 | Avg Loss: 0.0156 | Grad Norm: 0.00946808\n",
      "Epoch 3 | Step 2322200 | Avg Loss: 0.0154 | Grad Norm: 0.00925881\n",
      "Epoch 3 | Step 2322300 | Avg Loss: 0.0152 | Grad Norm: 0.00961746\n",
      "Epoch 3 | Step 2322400 | Avg Loss: 0.0151 | Grad Norm: 0.00871393\n",
      "Epoch 3 | Step 2322500 | Avg Loss: 0.0152 | Grad Norm: 0.00865006\n",
      "Epoch 3 | Step 2322600 | Avg Loss: 0.0154 | Grad Norm: 0.00906406\n",
      "Epoch 3 | Step 2322700 | Avg Loss: 0.0152 | Grad Norm: 0.01013059\n",
      "Epoch 3 | Step 2322800 | Avg Loss: 0.0152 | Grad Norm: 0.00892894\n",
      "Epoch 3 | Step 2322900 | Avg Loss: 0.0154 | Grad Norm: 0.00843971\n",
      "Epoch 3 | Step 2323000 | Avg Loss: 0.0156 | Grad Norm: 0.01018169\n",
      "Epoch 3 | Step 2323100 | Avg Loss: 0.0157 | Grad Norm: 0.00960140\n",
      "Epoch 3 | Step 2323200 | Avg Loss: 0.0158 | Grad Norm: 0.00833750\n",
      "Epoch 3 | Step 2323300 | Avg Loss: 0.0158 | Grad Norm: 0.00799449\n",
      "Epoch 3 | Step 2323400 | Avg Loss: 0.0155 | Grad Norm: 0.01149779\n",
      "Epoch 3 | Step 2323500 | Avg Loss: 0.0152 | Grad Norm: 0.00877815\n",
      "Epoch 3 | Step 2323600 | Avg Loss: 0.0153 | Grad Norm: 0.00841896\n",
      "Epoch 3 | Step 2323700 | Avg Loss: 0.0158 | Grad Norm: 0.00893852\n",
      "Epoch 3 | Step 2323800 | Avg Loss: 0.0157 | Grad Norm: 0.00892805\n",
      "Epoch 3 | Step 2323900 | Avg Loss: 0.0158 | Grad Norm: 0.00776729\n",
      "Epoch 3 | Step 2324000 | Avg Loss: 0.0159 | Grad Norm: 0.00872317\n",
      "Epoch 3 | Step 2324100 | Avg Loss: 0.0158 | Grad Norm: 0.00926871\n",
      "Epoch 3 | Step 2324200 | Avg Loss: 0.0161 | Grad Norm: 0.01411276\n",
      "Epoch 3 | Step 2324300 | Avg Loss: 0.0157 | Grad Norm: 0.00798497\n",
      "Epoch 3 | Step 2324400 | Avg Loss: 0.0161 | Grad Norm: 0.00934230\n",
      "Epoch 3 | Step 2324500 | Avg Loss: 0.0158 | Grad Norm: 0.00823124\n",
      "Epoch 3 | Step 2324600 | Avg Loss: 0.0159 | Grad Norm: 0.00892171\n",
      "Epoch 3 | Step 2324700 | Avg Loss: 0.0156 | Grad Norm: 0.00986736\n",
      "Epoch 3 | Step 2324800 | Avg Loss: 0.0159 | Grad Norm: 0.01125160\n",
      "Epoch 3 | Step 2324900 | Avg Loss: 0.0159 | Grad Norm: 0.00883788\n",
      "Epoch 3 | Step 2325000 | Avg Loss: 0.0159 | Grad Norm: 0.00962502\n",
      "Epoch 3 | Step 2325100 | Avg Loss: 0.0157 | Grad Norm: 0.00956062\n",
      "Epoch 3 | Step 2325200 | Avg Loss: 0.0160 | Grad Norm: 0.00852406\n",
      "Epoch 3 | Step 2325300 | Avg Loss: 0.0162 | Grad Norm: 0.01057213\n",
      "Epoch 3 | Step 2325400 | Avg Loss: 0.0163 | Grad Norm: 0.00963011\n",
      "Epoch 3 | Step 2325500 | Avg Loss: 0.0160 | Grad Norm: 0.00992597\n",
      "Epoch 3 | Step 2325600 | Avg Loss: 0.0158 | Grad Norm: 0.00908733\n",
      "Epoch 3 | Step 2325700 | Avg Loss: 0.0163 | Grad Norm: 0.00897415\n",
      "Epoch 3 | Step 2325800 | Avg Loss: 0.0163 | Grad Norm: 0.00870435\n",
      "Epoch 3 | Step 2325900 | Avg Loss: 0.0162 | Grad Norm: 0.01022983\n",
      "Epoch 3 | Step 2326000 | Avg Loss: 0.0160 | Grad Norm: 0.00924913\n",
      "Epoch 3 | Step 2326100 | Avg Loss: 0.0158 | Grad Norm: 0.01019558\n",
      "Epoch 3 | Step 2326200 | Avg Loss: 0.0158 | Grad Norm: 0.00869423\n",
      "Epoch 3 | Step 2326300 | Avg Loss: 0.0156 | Grad Norm: 0.00778627\n",
      "Epoch 3 | Step 2326400 | Avg Loss: 0.0155 | Grad Norm: 0.00885339\n",
      "Epoch 3 | Step 2326500 | Avg Loss: 0.0154 | Grad Norm: 0.00899136\n",
      "Epoch 3 | Step 2326600 | Avg Loss: 0.0150 | Grad Norm: 0.00911373\n",
      "Epoch 3 | Step 2326700 | Avg Loss: 0.0151 | Grad Norm: 0.00878182\n",
      "Epoch 3 | Step 2326800 | Avg Loss: 0.0150 | Grad Norm: 0.00814913\n",
      "Epoch 3 | Step 2326900 | Avg Loss: 0.0150 | Grad Norm: 0.00985623\n",
      "Epoch 3 | Step 2327000 | Avg Loss: 0.0151 | Grad Norm: 0.00823174\n",
      "Epoch 3 | Step 2327100 | Avg Loss: 0.0151 | Grad Norm: 0.00912114\n",
      "Epoch 3 | Step 2327200 | Avg Loss: 0.0154 | Grad Norm: 0.00974650\n",
      "Epoch 3 | Step 2327300 | Avg Loss: 0.0155 | Grad Norm: 0.00862334\n",
      "Epoch 3 | Step 2327400 | Avg Loss: 0.0152 | Grad Norm: 0.01471438\n",
      "Epoch 3 | Step 2327500 | Avg Loss: 0.0152 | Grad Norm: 0.00835920\n",
      "Epoch 3 | Step 2327600 | Avg Loss: 0.0155 | Grad Norm: 0.00842395\n",
      "Epoch 3 | Step 2327700 | Avg Loss: 0.0155 | Grad Norm: 0.00897046\n",
      "Epoch 3 | Step 2327800 | Avg Loss: 0.0155 | Grad Norm: 0.00999992\n",
      "Epoch 3 | Step 2327900 | Avg Loss: 0.0156 | Grad Norm: 0.01042220\n",
      "Epoch 3 | Step 2328000 | Avg Loss: 0.0157 | Grad Norm: 0.00812995\n",
      "Epoch 3 | Step 2328100 | Avg Loss: 0.0155 | Grad Norm: 0.01107000\n",
      "Epoch 3 | Step 2328200 | Avg Loss: 0.0152 | Grad Norm: 0.00804043\n",
      "Epoch 3 | Step 2328300 | Avg Loss: 0.0150 | Grad Norm: 0.00875473\n",
      "Epoch 3 | Step 2328400 | Avg Loss: 0.0150 | Grad Norm: 0.01035248\n",
      "Epoch 3 | Step 2328500 | Avg Loss: 0.0150 | Grad Norm: 0.00888886\n",
      "Epoch 3 | Step 2328600 | Avg Loss: 0.0154 | Grad Norm: 0.01047724\n",
      "Epoch 3 | Step 2328700 | Avg Loss: 0.0157 | Grad Norm: 0.00821849\n",
      "Epoch 3 | Step 2328800 | Avg Loss: 0.0161 | Grad Norm: 0.00852291\n",
      "Epoch 3 | Step 2328900 | Avg Loss: 0.0158 | Grad Norm: 0.00881558\n",
      "Epoch 3 | Step 2329000 | Avg Loss: 0.0159 | Grad Norm: 0.00938888\n",
      "Epoch 3 | Step 2329100 | Avg Loss: 0.0158 | Grad Norm: 0.00837365\n",
      "Epoch 3 | Step 2329200 | Avg Loss: 0.0156 | Grad Norm: 0.00891865\n",
      "Epoch 3 | Step 2329300 | Avg Loss: 0.0153 | Grad Norm: 0.00878237\n",
      "Epoch 3 | Step 2329400 | Avg Loss: 0.0157 | Grad Norm: 0.00847199\n",
      "Epoch 3 | Step 2329500 | Avg Loss: 0.0154 | Grad Norm: 0.01014825\n",
      "Epoch 3 | Step 2329600 | Avg Loss: 0.0153 | Grad Norm: 0.00812662\n",
      "Epoch 3 | Step 2329700 | Avg Loss: 0.0156 | Grad Norm: 0.00884621\n",
      "Epoch 3 | Step 2329800 | Avg Loss: 0.0152 | Grad Norm: 0.00903030\n",
      "Epoch 3 | Step 2329900 | Avg Loss: 0.0157 | Grad Norm: 0.00973761\n",
      "Epoch 3 | Step 2330000 | Avg Loss: 0.0154 | Grad Norm: 0.00948974\n",
      "Epoch 3 | Step 2330100 | Avg Loss: 0.0157 | Grad Norm: 0.00876418\n",
      "Epoch 3 | Step 2330200 | Avg Loss: 0.0157 | Grad Norm: 0.00955823\n",
      "Epoch 3 | Step 2330300 | Avg Loss: 0.0155 | Grad Norm: 0.00825186\n",
      "Epoch 3 | Step 2330400 | Avg Loss: 0.0151 | Grad Norm: 0.00893969\n",
      "Epoch 3 | Step 2330500 | Avg Loss: 0.0151 | Grad Norm: 0.00962199\n",
      "Epoch 3 | Step 2330600 | Avg Loss: 0.0152 | Grad Norm: 0.01041107\n",
      "Epoch 3 | Step 2330700 | Avg Loss: 0.0152 | Grad Norm: 0.00860212\n",
      "Epoch 3 | Step 2330800 | Avg Loss: 0.0156 | Grad Norm: 0.00796111\n",
      "Epoch 3 | Step 2330900 | Avg Loss: 0.0158 | Grad Norm: 0.00904902\n",
      "Epoch 3 | Step 2331000 | Avg Loss: 0.0157 | Grad Norm: 0.00890237\n",
      "Epoch 3 | Step 2331100 | Avg Loss: 0.0159 | Grad Norm: 0.00834324\n",
      "Epoch 3 | Step 2331200 | Avg Loss: 0.0161 | Grad Norm: 0.00901253\n",
      "Epoch 3 | Step 2331300 | Avg Loss: 0.0161 | Grad Norm: 0.00883431\n",
      "Epoch 3 | Step 2331400 | Avg Loss: 0.0158 | Grad Norm: 0.00852264\n",
      "Epoch 3 | Step 2331500 | Avg Loss: 0.0159 | Grad Norm: 0.00816660\n",
      "Epoch 3 | Step 2331600 | Avg Loss: 0.0158 | Grad Norm: 0.00924554\n",
      "Epoch 3 | Step 2331700 | Avg Loss: 0.0156 | Grad Norm: 0.00790964\n",
      "Epoch 3 | Step 2331800 | Avg Loss: 0.0153 | Grad Norm: 0.00890712\n",
      "Epoch 3 | Step 2331900 | Avg Loss: 0.0156 | Grad Norm: 0.00892935\n",
      "Epoch 3 | Step 2332000 | Avg Loss: 0.0156 | Grad Norm: 0.01024729\n",
      "Epoch 3 | Step 2332100 | Avg Loss: 0.0154 | Grad Norm: 0.00879389\n",
      "Epoch 3 | Step 2332200 | Avg Loss: 0.0156 | Grad Norm: 0.00857146\n",
      "Epoch 3 | Step 2332300 | Avg Loss: 0.0156 | Grad Norm: 0.00911075\n",
      "Epoch 3 | Step 2332400 | Avg Loss: 0.0155 | Grad Norm: 0.01033316\n",
      "Epoch 3 | Step 2332500 | Avg Loss: 0.0159 | Grad Norm: 0.00839568\n",
      "Epoch 3 | Step 2332600 | Avg Loss: 0.0158 | Grad Norm: 0.00885999\n",
      "Epoch 3 | Step 2332700 | Avg Loss: 0.0156 | Grad Norm: 0.01042239\n",
      "Epoch 3 | Step 2332800 | Avg Loss: 0.0157 | Grad Norm: 0.00994115\n",
      "Epoch 3 | Step 2332900 | Avg Loss: 0.0158 | Grad Norm: 0.00891048\n",
      "Epoch 3 | Step 2333000 | Avg Loss: 0.0159 | Grad Norm: 0.00868257\n",
      "Epoch 3 | Step 2333100 | Avg Loss: 0.0157 | Grad Norm: 0.00781216\n",
      "Epoch 3 | Step 2333200 | Avg Loss: 0.0160 | Grad Norm: 0.00956803\n",
      "Epoch 3 | Step 2333300 | Avg Loss: 0.0156 | Grad Norm: 0.00921987\n",
      "Epoch 3 | Step 2333400 | Avg Loss: 0.0156 | Grad Norm: 0.00892850\n",
      "Epoch 3 | Step 2333500 | Avg Loss: 0.0155 | Grad Norm: 0.00955674\n",
      "Epoch 3 | Step 2333600 | Avg Loss: 0.0155 | Grad Norm: 0.01206846\n",
      "Epoch 3 | Step 2333700 | Avg Loss: 0.0154 | Grad Norm: 0.00988595\n",
      "Epoch 3 | Step 2333800 | Avg Loss: 0.0155 | Grad Norm: 0.00865930\n",
      "Epoch 3 | Step 2333900 | Avg Loss: 0.0157 | Grad Norm: 0.00801847\n",
      "Epoch 3 | Step 2334000 | Avg Loss: 0.0154 | Grad Norm: 0.00921021\n",
      "Epoch 3 | Step 2334100 | Avg Loss: 0.0153 | Grad Norm: 0.00865664\n",
      "Epoch 3 | Step 2334200 | Avg Loss: 0.0151 | Grad Norm: 0.00928095\n",
      "Epoch 3 | Step 2334300 | Avg Loss: 0.0151 | Grad Norm: 0.00815132\n",
      "Epoch 3 | Step 2334400 | Avg Loss: 0.0151 | Grad Norm: 0.00737889\n",
      "Epoch 3 | Step 2334500 | Avg Loss: 0.0148 | Grad Norm: 0.00810411\n",
      "Epoch 3 | Step 2334600 | Avg Loss: 0.0149 | Grad Norm: 0.01052214\n",
      "Epoch 3 | Step 2334700 | Avg Loss: 0.0152 | Grad Norm: 0.00919845\n",
      "Epoch 3 | Step 2334800 | Avg Loss: 0.0154 | Grad Norm: 0.00798216\n",
      "Epoch 3 | Step 2334900 | Avg Loss: 0.0153 | Grad Norm: 0.00756604\n",
      "Epoch 3 | Step 2335000 | Avg Loss: 0.0156 | Grad Norm: 0.00843553\n",
      "Epoch 3 | Step 2335100 | Avg Loss: 0.0156 | Grad Norm: 0.01036716\n",
      "Epoch 3 | Step 2335200 | Avg Loss: 0.0156 | Grad Norm: 0.00911212\n",
      "Epoch 3 | Step 2335300 | Avg Loss: 0.0154 | Grad Norm: 0.00764276\n",
      "Epoch 3 | Step 2335400 | Avg Loss: 0.0159 | Grad Norm: 0.00927484\n",
      "Epoch 3 | Step 2335500 | Avg Loss: 0.0159 | Grad Norm: 0.00892336\n",
      "Epoch 3 | Step 2335600 | Avg Loss: 0.0158 | Grad Norm: 0.00852004\n",
      "Epoch 3 | Step 2335700 | Avg Loss: 0.0157 | Grad Norm: 0.01071983\n",
      "Epoch 3 | Step 2335800 | Avg Loss: 0.0159 | Grad Norm: 0.01006945\n",
      "Epoch 3 | Step 2335900 | Avg Loss: 0.0158 | Grad Norm: 0.00915883\n",
      "Epoch 3 | Step 2336000 | Avg Loss: 0.0159 | Grad Norm: 0.01011519\n",
      "Epoch 3 | Step 2336100 | Avg Loss: 0.0158 | Grad Norm: 0.00817694\n",
      "Epoch 3 | Step 2336200 | Avg Loss: 0.0159 | Grad Norm: 0.00931086\n",
      "Epoch 3 | Step 2336300 | Avg Loss: 0.0158 | Grad Norm: 0.00877503\n",
      "Epoch 3 | Step 2336400 | Avg Loss: 0.0157 | Grad Norm: 0.00899907\n",
      "Epoch 3 | Step 2336500 | Avg Loss: 0.0157 | Grad Norm: 0.00846325\n",
      "Epoch 3 | Step 2336600 | Avg Loss: 0.0158 | Grad Norm: 0.00954429\n",
      "Epoch 3 | Step 2336700 | Avg Loss: 0.0158 | Grad Norm: 0.00814111\n",
      "Epoch 3 | Step 2336800 | Avg Loss: 0.0155 | Grad Norm: 0.00969864\n",
      "Epoch 3 | Step 2336900 | Avg Loss: 0.0157 | Grad Norm: 0.00958651\n",
      "Epoch 3 | Step 2337000 | Avg Loss: 0.0159 | Grad Norm: 0.01203374\n",
      "Epoch 3 | Step 2337100 | Avg Loss: 0.0156 | Grad Norm: 0.00863839\n",
      "Epoch 3 | Step 2337200 | Avg Loss: 0.0155 | Grad Norm: 0.00994903\n",
      "Epoch 3 | Step 2337300 | Avg Loss: 0.0157 | Grad Norm: 0.00935273\n",
      "Epoch 3 | Step 2337400 | Avg Loss: 0.0159 | Grad Norm: 0.00843818\n",
      "Epoch 3 | Step 2337500 | Avg Loss: 0.0158 | Grad Norm: 0.00910496\n",
      "Epoch 3 | Step 2337600 | Avg Loss: 0.0162 | Grad Norm: 0.00857418\n",
      "Epoch 3 | Step 2337700 | Avg Loss: 0.0160 | Grad Norm: 0.00937096\n",
      "Epoch 3 | Step 2337800 | Avg Loss: 0.0158 | Grad Norm: 0.00820015\n",
      "Epoch 3 | Step 2337900 | Avg Loss: 0.0157 | Grad Norm: 0.00959598\n",
      "Epoch 3 | Step 2338000 | Avg Loss: 0.0158 | Grad Norm: 0.00972564\n",
      "Epoch 3 | Step 2338100 | Avg Loss: 0.0160 | Grad Norm: 0.00911246\n",
      "Epoch 3 | Step 2338200 | Avg Loss: 0.0162 | Grad Norm: 0.00983946\n",
      "Epoch 3 | Step 2338300 | Avg Loss: 0.0156 | Grad Norm: 0.00967414\n",
      "Epoch 3 | Step 2338400 | Avg Loss: 0.0156 | Grad Norm: 0.01012232\n",
      "Epoch 3 | Step 2338500 | Avg Loss: 0.0158 | Grad Norm: 0.01009719\n",
      "Epoch 3 | Step 2338600 | Avg Loss: 0.0156 | Grad Norm: 0.00885291\n",
      "Epoch 3 | Step 2338700 | Avg Loss: 0.0157 | Grad Norm: 0.00993374\n",
      "Epoch 3 | Step 2338800 | Avg Loss: 0.0152 | Grad Norm: 0.01093511\n",
      "Epoch 3 | Step 2338900 | Avg Loss: 0.0158 | Grad Norm: 0.00884163\n",
      "Epoch 3 | Step 2339000 | Avg Loss: 0.0162 | Grad Norm: 0.00909510\n",
      "Epoch 3 | Step 2339100 | Avg Loss: 0.0156 | Grad Norm: 0.00879504\n",
      "Epoch 3 | Step 2339200 | Avg Loss: 0.0156 | Grad Norm: 0.00869956\n",
      "Epoch 3 | Step 2339300 | Avg Loss: 0.0157 | Grad Norm: 0.00876878\n",
      "Epoch 3 | Step 2339400 | Avg Loss: 0.0156 | Grad Norm: 0.00966488\n",
      "Epoch 3 | Step 2339500 | Avg Loss: 0.0156 | Grad Norm: 0.00865178\n",
      "Epoch 3 | Step 2339600 | Avg Loss: 0.0155 | Grad Norm: 0.01247416\n",
      "Epoch 3 | Step 2339700 | Avg Loss: 0.0153 | Grad Norm: 0.00909897\n",
      "Epoch 3 | Step 2339800 | Avg Loss: 0.0157 | Grad Norm: 0.00911510\n",
      "Epoch 3 | Step 2339900 | Avg Loss: 0.0157 | Grad Norm: 0.00989411\n",
      "Epoch 3 | Step 2340000 | Avg Loss: 0.0157 | Grad Norm: 0.00926222\n",
      "Epoch 3 | Step 2340100 | Avg Loss: 0.0158 | Grad Norm: 0.00913853\n",
      "Epoch 3 | Step 2340200 | Avg Loss: 0.0156 | Grad Norm: 0.00757085\n",
      "Epoch 3 | Step 2340300 | Avg Loss: 0.0153 | Grad Norm: 0.00804047\n",
      "Epoch 3 | Step 2340400 | Avg Loss: 0.0152 | Grad Norm: 0.00815657\n",
      "Epoch 3 | Step 2340500 | Avg Loss: 0.0153 | Grad Norm: 0.00909303\n",
      "Epoch 3 | Step 2340600 | Avg Loss: 0.0152 | Grad Norm: 0.00909117\n",
      "Epoch 3 | Step 2340700 | Avg Loss: 0.0154 | Grad Norm: 0.00999317\n",
      "Epoch 3 | Step 2340800 | Avg Loss: 0.0153 | Grad Norm: 0.00928149\n",
      "Epoch 3 | Step 2340900 | Avg Loss: 0.0153 | Grad Norm: 0.00951382\n",
      "Epoch 3 | Step 2341000 | Avg Loss: 0.0153 | Grad Norm: 0.00951544\n",
      "Epoch 3 | Step 2341100 | Avg Loss: 0.0152 | Grad Norm: 0.00872581\n",
      "Epoch 3 | Step 2341200 | Avg Loss: 0.0151 | Grad Norm: 0.00948158\n",
      "Epoch 3 | Step 2341300 | Avg Loss: 0.0152 | Grad Norm: 0.00819341\n",
      "Epoch 3 | Step 2341400 | Avg Loss: 0.0151 | Grad Norm: 0.00810591\n",
      "Epoch 3 | Step 2341500 | Avg Loss: 0.0151 | Grad Norm: 0.01048871\n",
      "Epoch 3 | Step 2341600 | Avg Loss: 0.0157 | Grad Norm: 0.00863973\n",
      "Epoch 3 | Step 2341700 | Avg Loss: 0.0156 | Grad Norm: 0.01005725\n",
      "Epoch 3 | Step 2341800 | Avg Loss: 0.0151 | Grad Norm: 0.01121145\n",
      "Epoch 3 | Step 2341900 | Avg Loss: 0.0156 | Grad Norm: 0.01034380\n",
      "Epoch 3 | Step 2342000 | Avg Loss: 0.0159 | Grad Norm: 0.00926918\n",
      "Epoch 3 | Step 2342100 | Avg Loss: 0.0153 | Grad Norm: 0.00906079\n",
      "Epoch 3 | Step 2342200 | Avg Loss: 0.0155 | Grad Norm: 0.00902758\n",
      "Epoch 3 | Step 2342300 | Avg Loss: 0.0155 | Grad Norm: 0.00983855\n",
      "Epoch 3 | Step 2342400 | Avg Loss: 0.0153 | Grad Norm: 0.00903316\n",
      "Epoch 3 | Step 2342500 | Avg Loss: 0.0154 | Grad Norm: 0.00844605\n",
      "Epoch 3 | Step 2342600 | Avg Loss: 0.0154 | Grad Norm: 0.00796060\n",
      "Epoch 3 | Step 2342700 | Avg Loss: 0.0151 | Grad Norm: 0.00882428\n",
      "Epoch 3 | Step 2342800 | Avg Loss: 0.0152 | Grad Norm: 0.00785049\n",
      "Epoch 3 | Step 2342900 | Avg Loss: 0.0154 | Grad Norm: 0.00932960\n",
      "Epoch 3 | Step 2343000 | Avg Loss: 0.0155 | Grad Norm: 0.00912969\n",
      "Epoch 3 | Step 2343100 | Avg Loss: 0.0155 | Grad Norm: 0.00974254\n",
      "Epoch 3 | Step 2343200 | Avg Loss: 0.0150 | Grad Norm: 0.00820201\n",
      "Epoch 3 | Step 2343300 | Avg Loss: 0.0149 | Grad Norm: 0.00849173\n",
      "Epoch 3 | Step 2343400 | Avg Loss: 0.0150 | Grad Norm: 0.00862685\n",
      "Epoch 3 | Step 2343500 | Avg Loss: 0.0150 | Grad Norm: 0.01152386\n",
      "Epoch 3 | Step 2343600 | Avg Loss: 0.0148 | Grad Norm: 0.00656496\n",
      "Epoch 3 | Step 2343700 | Avg Loss: 0.0147 | Grad Norm: 0.01131462\n",
      "Epoch 3, Loss: 0.0133\n",
      "Epoch 4 | Step 2343800 | Avg Loss: 0.0149 | Grad Norm: 0.00864108\n",
      "Epoch 4 | Step 2343900 | Avg Loss: 0.0155 | Grad Norm: 0.00823132\n",
      "Epoch 4 | Step 2344000 | Avg Loss: 0.0154 | Grad Norm: 0.00899744\n",
      "Epoch 4 | Step 2344100 | Avg Loss: 0.0152 | Grad Norm: 0.00884705\n",
      "Epoch 4 | Step 2344200 | Avg Loss: 0.0150 | Grad Norm: 0.00722912\n",
      "Epoch 4 | Step 2344300 | Avg Loss: 0.0154 | Grad Norm: 0.00823325\n",
      "Epoch 4 | Step 2344400 | Avg Loss: 0.0159 | Grad Norm: 0.01191016\n",
      "Epoch 4 | Step 2344500 | Avg Loss: 0.0157 | Grad Norm: 0.00981431\n",
      "Epoch 4 | Step 2344600 | Avg Loss: 0.0162 | Grad Norm: 0.00987381\n",
      "Epoch 4 | Step 2344700 | Avg Loss: 0.0163 | Grad Norm: 0.00973607\n",
      "Epoch 4 | Step 2344800 | Avg Loss: 0.0158 | Grad Norm: 0.00904387\n",
      "Epoch 4 | Step 2344900 | Avg Loss: 0.0156 | Grad Norm: 0.00981352\n",
      "Epoch 4 | Step 2345000 | Avg Loss: 0.0154 | Grad Norm: 0.00900036\n",
      "Epoch 4 | Step 2345100 | Avg Loss: 0.0155 | Grad Norm: 0.01002252\n",
      "Epoch 4 | Step 2345200 | Avg Loss: 0.0153 | Grad Norm: 0.00848092\n",
      "Epoch 4 | Step 2345300 | Avg Loss: 0.0149 | Grad Norm: 0.00881273\n",
      "Epoch 4 | Step 2345400 | Avg Loss: 0.0150 | Grad Norm: 0.00886239\n",
      "Epoch 4 | Step 2345500 | Avg Loss: 0.0151 | Grad Norm: 0.00949674\n",
      "Epoch 4 | Step 2345600 | Avg Loss: 0.0153 | Grad Norm: 0.01109534\n",
      "Epoch 4 | Step 2345700 | Avg Loss: 0.0154 | Grad Norm: 0.00823716\n",
      "Epoch 4 | Step 2345800 | Avg Loss: 0.0153 | Grad Norm: 0.00806423\n",
      "Epoch 4 | Step 2345900 | Avg Loss: 0.0154 | Grad Norm: 0.00867085\n",
      "Epoch 4 | Step 2346000 | Avg Loss: 0.0154 | Grad Norm: 0.00878139\n",
      "Epoch 4 | Step 2346100 | Avg Loss: 0.0156 | Grad Norm: 0.00910001\n",
      "Epoch 4 | Step 2346200 | Avg Loss: 0.0157 | Grad Norm: 0.00965592\n",
      "Epoch 4 | Step 2346300 | Avg Loss: 0.0157 | Grad Norm: 0.00810489\n",
      "Epoch 4 | Step 2346400 | Avg Loss: 0.0158 | Grad Norm: 0.01041262\n",
      "Epoch 4 | Step 2346500 | Avg Loss: 0.0160 | Grad Norm: 0.00996522\n",
      "Epoch 4 | Step 2346600 | Avg Loss: 0.0157 | Grad Norm: 0.00828637\n",
      "Epoch 4 | Step 2346700 | Avg Loss: 0.0157 | Grad Norm: 0.00904081\n",
      "Epoch 4 | Step 2346800 | Avg Loss: 0.0158 | Grad Norm: 0.00968946\n",
      "Epoch 4 | Step 2346900 | Avg Loss: 0.0154 | Grad Norm: 0.00978133\n",
      "Epoch 4 | Step 2347000 | Avg Loss: 0.0153 | Grad Norm: 0.00836052\n",
      "Epoch 4 | Step 2347100 | Avg Loss: 0.0155 | Grad Norm: 0.00923626\n",
      "Epoch 4 | Step 2347200 | Avg Loss: 0.0153 | Grad Norm: 0.00919243\n",
      "Epoch 4 | Step 2347300 | Avg Loss: 0.0152 | Grad Norm: 0.00853946\n",
      "Epoch 4 | Step 2347400 | Avg Loss: 0.0154 | Grad Norm: 0.00854620\n",
      "Epoch 4 | Step 2347500 | Avg Loss: 0.0157 | Grad Norm: 0.01115194\n",
      "Epoch 4 | Step 2347600 | Avg Loss: 0.0155 | Grad Norm: 0.00866107\n",
      "Epoch 4 | Step 2347700 | Avg Loss: 0.0156 | Grad Norm: 0.00999008\n",
      "Epoch 4 | Step 2347800 | Avg Loss: 0.0153 | Grad Norm: 0.00759248\n",
      "Epoch 4 | Step 2347900 | Avg Loss: 0.0150 | Grad Norm: 0.00772379\n",
      "Epoch 4 | Step 2348000 | Avg Loss: 0.0149 | Grad Norm: 0.00863455\n",
      "Epoch 4 | Step 2348100 | Avg Loss: 0.0154 | Grad Norm: 0.00906396\n",
      "Epoch 4 | Step 2348200 | Avg Loss: 0.0153 | Grad Norm: 0.00907336\n",
      "Epoch 4 | Step 2348300 | Avg Loss: 0.0151 | Grad Norm: 0.00879592\n",
      "Epoch 4 | Step 2348400 | Avg Loss: 0.0149 | Grad Norm: 0.01226277\n",
      "Epoch 4 | Step 2348500 | Avg Loss: 0.0151 | Grad Norm: 0.00862170\n",
      "Epoch 4 | Step 2348600 | Avg Loss: 0.0152 | Grad Norm: 0.00916742\n",
      "Epoch 4 | Step 2348700 | Avg Loss: 0.0152 | Grad Norm: 0.00912701\n",
      "Epoch 4 | Step 2348800 | Avg Loss: 0.0153 | Grad Norm: 0.00789462\n",
      "Epoch 4 | Step 2348900 | Avg Loss: 0.0154 | Grad Norm: 0.00951145\n",
      "Epoch 4 | Step 2349000 | Avg Loss: 0.0152 | Grad Norm: 0.00974669\n",
      "Epoch 4 | Step 2349100 | Avg Loss: 0.0156 | Grad Norm: 0.01205012\n",
      "Epoch 4 | Step 2349200 | Avg Loss: 0.0159 | Grad Norm: 0.00850655\n",
      "Epoch 4 | Step 2349300 | Avg Loss: 0.0155 | Grad Norm: 0.00881549\n",
      "Epoch 4 | Step 2349400 | Avg Loss: 0.0156 | Grad Norm: 0.00930172\n",
      "Epoch 4 | Step 2349500 | Avg Loss: 0.0155 | Grad Norm: 0.00887955\n",
      "Epoch 4 | Step 2349600 | Avg Loss: 0.0154 | Grad Norm: 0.00899779\n",
      "Epoch 4 | Step 2349700 | Avg Loss: 0.0154 | Grad Norm: 0.00933615\n",
      "Epoch 4 | Step 2349800 | Avg Loss: 0.0153 | Grad Norm: 0.00827118\n",
      "Epoch 4 | Step 2349900 | Avg Loss: 0.0155 | Grad Norm: 0.00913677\n",
      "Epoch 4 | Step 2350000 | Avg Loss: 0.0155 | Grad Norm: 0.01048250\n",
      "Epoch 4 | Step 2350100 | Avg Loss: 0.0156 | Grad Norm: 0.00894265\n",
      "Epoch 4 | Step 2350200 | Avg Loss: 0.0155 | Grad Norm: 0.01066590\n",
      "Epoch 4 | Step 2350300 | Avg Loss: 0.0159 | Grad Norm: 0.01215582\n",
      "Epoch 4 | Step 2350400 | Avg Loss: 0.0157 | Grad Norm: 0.01346993\n",
      "Epoch 4 | Step 2350500 | Avg Loss: 0.0158 | Grad Norm: 0.00878179\n",
      "Epoch 4 | Step 2350600 | Avg Loss: 0.0156 | Grad Norm: 0.00952683\n",
      "Epoch 4 | Step 2350700 | Avg Loss: 0.0154 | Grad Norm: 0.00958116\n",
      "Epoch 4 | Step 2350800 | Avg Loss: 0.0157 | Grad Norm: 0.00862839\n",
      "Epoch 4 | Step 2350900 | Avg Loss: 0.0153 | Grad Norm: 0.00811483\n",
      "Epoch 4 | Step 2351000 | Avg Loss: 0.0149 | Grad Norm: 0.01039397\n",
      "Epoch 4 | Step 2351100 | Avg Loss: 0.0152 | Grad Norm: 0.00774706\n",
      "Epoch 4 | Step 2351200 | Avg Loss: 0.0152 | Grad Norm: 0.00765583\n",
      "Epoch 4 | Step 2351300 | Avg Loss: 0.0154 | Grad Norm: 0.00898736\n",
      "Epoch 4 | Step 2351400 | Avg Loss: 0.0154 | Grad Norm: 0.00968083\n",
      "Epoch 4 | Step 2351500 | Avg Loss: 0.0154 | Grad Norm: 0.00848687\n",
      "Epoch 4 | Step 2351600 | Avg Loss: 0.0153 | Grad Norm: 0.00896803\n",
      "Epoch 4 | Step 2351700 | Avg Loss: 0.0157 | Grad Norm: 0.00788057\n",
      "Epoch 4 | Step 2351800 | Avg Loss: 0.0157 | Grad Norm: 0.00976671\n",
      "Epoch 4 | Step 2351900 | Avg Loss: 0.0156 | Grad Norm: 0.01031951\n",
      "Epoch 4 | Step 2352000 | Avg Loss: 0.0156 | Grad Norm: 0.00848845\n",
      "Epoch 4 | Step 2352100 | Avg Loss: 0.0156 | Grad Norm: 0.00848102\n",
      "Epoch 4 | Step 2352200 | Avg Loss: 0.0155 | Grad Norm: 0.00926039\n",
      "Epoch 4 | Step 2352300 | Avg Loss: 0.0158 | Grad Norm: 0.01014081\n",
      "Epoch 4 | Step 2352400 | Avg Loss: 0.0158 | Grad Norm: 0.01037377\n",
      "Epoch 4 | Step 2352500 | Avg Loss: 0.0160 | Grad Norm: 0.00992544\n",
      "Epoch 4 | Step 2352600 | Avg Loss: 0.0156 | Grad Norm: 0.01122556\n",
      "Epoch 4 | Step 2352700 | Avg Loss: 0.0155 | Grad Norm: 0.00978802\n",
      "Epoch 4 | Step 2352800 | Avg Loss: 0.0157 | Grad Norm: 0.00876637\n",
      "Epoch 4 | Step 2352900 | Avg Loss: 0.0155 | Grad Norm: 0.00898161\n",
      "Epoch 4 | Step 2353000 | Avg Loss: 0.0154 | Grad Norm: 0.00889325\n",
      "Epoch 4 | Step 2353100 | Avg Loss: 0.0154 | Grad Norm: 0.00769395\n",
      "Epoch 4 | Step 2353200 | Avg Loss: 0.0154 | Grad Norm: 0.00837283\n",
      "Epoch 4 | Step 2353300 | Avg Loss: 0.0155 | Grad Norm: 0.00922774\n",
      "Epoch 4 | Step 2353400 | Avg Loss: 0.0154 | Grad Norm: 0.00955563\n",
      "Epoch 4 | Step 2353500 | Avg Loss: 0.0155 | Grad Norm: 0.01050603\n",
      "Epoch 4 | Step 2353600 | Avg Loss: 0.0157 | Grad Norm: 0.00918395\n",
      "Epoch 4 | Step 2353700 | Avg Loss: 0.0157 | Grad Norm: 0.00860368\n",
      "Epoch 4 | Step 2353800 | Avg Loss: 0.0156 | Grad Norm: 0.00931310\n",
      "Epoch 4 | Step 2353900 | Avg Loss: 0.0158 | Grad Norm: 0.00976781\n",
      "Epoch 4 | Step 2354000 | Avg Loss: 0.0157 | Grad Norm: 0.00816334\n",
      "Epoch 4 | Step 2354100 | Avg Loss: 0.0158 | Grad Norm: 0.00943825\n",
      "Epoch 4 | Step 2354200 | Avg Loss: 0.0159 | Grad Norm: 0.00861537\n",
      "Epoch 4 | Step 2354300 | Avg Loss: 0.0163 | Grad Norm: 0.00937599\n",
      "Epoch 4 | Step 2354400 | Avg Loss: 0.0161 | Grad Norm: 0.00862602\n",
      "Epoch 4 | Step 2354500 | Avg Loss: 0.0162 | Grad Norm: 0.00929387\n",
      "Epoch 4 | Step 2354600 | Avg Loss: 0.0161 | Grad Norm: 0.00904320\n",
      "Epoch 4 | Step 2354700 | Avg Loss: 0.0157 | Grad Norm: 0.00902748\n",
      "Epoch 4 | Step 2354800 | Avg Loss: 0.0161 | Grad Norm: 0.01216766\n",
      "Epoch 4 | Step 2354900 | Avg Loss: 0.0161 | Grad Norm: 0.01080248\n",
      "Epoch 4 | Step 2355000 | Avg Loss: 0.0159 | Grad Norm: 0.00862394\n",
      "Epoch 4 | Step 2355100 | Avg Loss: 0.0158 | Grad Norm: 0.00968133\n",
      "Epoch 4 | Step 2355200 | Avg Loss: 0.0155 | Grad Norm: 0.01069675\n",
      "Epoch 4 | Step 2355300 | Avg Loss: 0.0161 | Grad Norm: 0.00896535\n",
      "Epoch 4 | Step 2355400 | Avg Loss: 0.0155 | Grad Norm: 0.00940188\n",
      "Epoch 4 | Step 2355500 | Avg Loss: 0.0154 | Grad Norm: 0.00956741\n",
      "Epoch 4 | Step 2355600 | Avg Loss: 0.0155 | Grad Norm: 0.00915237\n",
      "Epoch 4 | Step 2355700 | Avg Loss: 0.0152 | Grad Norm: 0.00914034\n",
      "Epoch 4 | Step 2355800 | Avg Loss: 0.0153 | Grad Norm: 0.00806440\n",
      "Epoch 4 | Step 2355900 | Avg Loss: 0.0150 | Grad Norm: 0.00933169\n",
      "Epoch 4 | Step 2356000 | Avg Loss: 0.0153 | Grad Norm: 0.01174790\n",
      "Epoch 4 | Step 2356100 | Avg Loss: 0.0158 | Grad Norm: 0.01069030\n",
      "Epoch 4 | Step 2356200 | Avg Loss: 0.0159 | Grad Norm: 0.00960732\n",
      "Epoch 4 | Step 2356300 | Avg Loss: 0.0159 | Grad Norm: 0.00857030\n",
      "Epoch 4 | Step 2356400 | Avg Loss: 0.0157 | Grad Norm: 0.00888266\n",
      "Epoch 4 | Step 2356500 | Avg Loss: 0.0156 | Grad Norm: 0.00844392\n",
      "Epoch 4 | Step 2356600 | Avg Loss: 0.0155 | Grad Norm: 0.00956996\n",
      "Epoch 4 | Step 2356700 | Avg Loss: 0.0157 | Grad Norm: 0.00935393\n",
      "Epoch 4 | Step 2356800 | Avg Loss: 0.0156 | Grad Norm: 0.00944514\n",
      "Epoch 4 | Step 2356900 | Avg Loss: 0.0154 | Grad Norm: 0.00849625\n",
      "Epoch 4 | Step 2357000 | Avg Loss: 0.0155 | Grad Norm: 0.00923136\n",
      "Epoch 4 | Step 2357100 | Avg Loss: 0.0153 | Grad Norm: 0.00902614\n",
      "Epoch 4 | Step 2357200 | Avg Loss: 0.0156 | Grad Norm: 0.00885915\n",
      "Epoch 4 | Step 2357300 | Avg Loss: 0.0155 | Grad Norm: 0.00887266\n",
      "Epoch 4 | Step 2357400 | Avg Loss: 0.0153 | Grad Norm: 0.00894142\n",
      "Epoch 4 | Step 2357500 | Avg Loss: 0.0151 | Grad Norm: 0.00923872\n",
      "Epoch 4 | Step 2357600 | Avg Loss: 0.0156 | Grad Norm: 0.00926188\n",
      "Epoch 4 | Step 2357700 | Avg Loss: 0.0158 | Grad Norm: 0.00863048\n",
      "Epoch 4 | Step 2357800 | Avg Loss: 0.0155 | Grad Norm: 0.00918031\n",
      "Epoch 4 | Step 2357900 | Avg Loss: 0.0153 | Grad Norm: 0.00867858\n",
      "Epoch 4 | Step 2358000 | Avg Loss: 0.0155 | Grad Norm: 0.00786900\n",
      "Epoch 4 | Step 2358100 | Avg Loss: 0.0153 | Grad Norm: 0.01008338\n",
      "Epoch 4 | Step 2358200 | Avg Loss: 0.0159 | Grad Norm: 0.01043616\n",
      "Epoch 4 | Step 2358300 | Avg Loss: 0.0156 | Grad Norm: 0.00931835\n",
      "Epoch 4 | Step 2358400 | Avg Loss: 0.0159 | Grad Norm: 0.00913696\n",
      "Epoch 4 | Step 2358500 | Avg Loss: 0.0157 | Grad Norm: 0.01150638\n",
      "Epoch 4 | Step 2358600 | Avg Loss: 0.0156 | Grad Norm: 0.00772872\n",
      "Epoch 4 | Step 2358700 | Avg Loss: 0.0156 | Grad Norm: 0.00958295\n",
      "Epoch 4 | Step 2358800 | Avg Loss: 0.0157 | Grad Norm: 0.00864652\n",
      "Epoch 4 | Step 2358900 | Avg Loss: 0.0156 | Grad Norm: 0.00912679\n",
      "Epoch 4 | Step 2359000 | Avg Loss: 0.0155 | Grad Norm: 0.01046189\n",
      "Epoch 4 | Step 2359100 | Avg Loss: 0.0155 | Grad Norm: 0.00996663\n",
      "Epoch 4 | Step 2359200 | Avg Loss: 0.0154 | Grad Norm: 0.00813163\n",
      "Epoch 4 | Step 2359300 | Avg Loss: 0.0157 | Grad Norm: 0.01022540\n",
      "Epoch 4 | Step 2359400 | Avg Loss: 0.0161 | Grad Norm: 0.00870999\n",
      "Epoch 4 | Step 2359500 | Avg Loss: 0.0163 | Grad Norm: 0.01040029\n",
      "Epoch 4 | Step 2359600 | Avg Loss: 0.0163 | Grad Norm: 0.00951693\n",
      "Epoch 4 | Step 2359700 | Avg Loss: 0.0166 | Grad Norm: 0.00934154\n",
      "Epoch 4 | Step 2359800 | Avg Loss: 0.0162 | Grad Norm: 0.00887046\n",
      "Epoch 4 | Step 2359900 | Avg Loss: 0.0166 | Grad Norm: 0.01043451\n",
      "Epoch 4 | Step 2360000 | Avg Loss: 0.0162 | Grad Norm: 0.00877822\n",
      "Epoch 4 | Step 2360100 | Avg Loss: 0.0163 | Grad Norm: 0.00887785\n",
      "Epoch 4 | Step 2360200 | Avg Loss: 0.0161 | Grad Norm: 0.01064738\n",
      "Epoch 4 | Step 2360300 | Avg Loss: 0.0161 | Grad Norm: 0.00768049\n",
      "Epoch 4 | Step 2360400 | Avg Loss: 0.0154 | Grad Norm: 0.00952560\n",
      "Epoch 4 | Step 2360500 | Avg Loss: 0.0154 | Grad Norm: 0.00873731\n",
      "Epoch 4 | Step 2360600 | Avg Loss: 0.0158 | Grad Norm: 0.00798761\n",
      "Epoch 4 | Step 2360700 | Avg Loss: 0.0159 | Grad Norm: 0.01041031\n",
      "Epoch 4 | Step 2360800 | Avg Loss: 0.0157 | Grad Norm: 0.00892815\n",
      "Epoch 4 | Step 2360900 | Avg Loss: 0.0158 | Grad Norm: 0.00930147\n",
      "Epoch 4 | Step 2361000 | Avg Loss: 0.0154 | Grad Norm: 0.00853501\n",
      "Epoch 4 | Step 2361100 | Avg Loss: 0.0156 | Grad Norm: 0.00907636\n",
      "Epoch 4 | Step 2361200 | Avg Loss: 0.0155 | Grad Norm: 0.01062774\n",
      "Epoch 4 | Step 2361300 | Avg Loss: 0.0154 | Grad Norm: 0.01033851\n",
      "Epoch 4 | Step 2361400 | Avg Loss: 0.0152 | Grad Norm: 0.00836783\n",
      "Epoch 4 | Step 2361500 | Avg Loss: 0.0153 | Grad Norm: 0.00794282\n",
      "Epoch 4 | Step 2361600 | Avg Loss: 0.0153 | Grad Norm: 0.00810317\n",
      "Epoch 4 | Step 2361700 | Avg Loss: 0.0156 | Grad Norm: 0.00773257\n",
      "Epoch 4 | Step 2361800 | Avg Loss: 0.0154 | Grad Norm: 0.00902167\n",
      "Epoch 4 | Step 2361900 | Avg Loss: 0.0152 | Grad Norm: 0.00802359\n",
      "Epoch 4 | Step 2362000 | Avg Loss: 0.0153 | Grad Norm: 0.00895837\n",
      "Epoch 4 | Step 2362100 | Avg Loss: 0.0156 | Grad Norm: 0.00902200\n",
      "Epoch 4 | Step 2362200 | Avg Loss: 0.0154 | Grad Norm: 0.00850844\n",
      "Epoch 4 | Step 2362300 | Avg Loss: 0.0156 | Grad Norm: 0.00962991\n",
      "Epoch 4 | Step 2362400 | Avg Loss: 0.0158 | Grad Norm: 0.00871096\n",
      "Epoch 4 | Step 2362500 | Avg Loss: 0.0159 | Grad Norm: 0.00881927\n",
      "Epoch 4 | Step 2362600 | Avg Loss: 0.0161 | Grad Norm: 0.00904640\n",
      "Epoch 4 | Step 2362700 | Avg Loss: 0.0164 | Grad Norm: 0.01264008\n",
      "Epoch 4 | Step 2362800 | Avg Loss: 0.0163 | Grad Norm: 0.00908504\n",
      "Epoch 4 | Step 2362900 | Avg Loss: 0.0163 | Grad Norm: 0.01028619\n",
      "Epoch 4 | Step 2363000 | Avg Loss: 0.0164 | Grad Norm: 0.00902397\n",
      "Epoch 4 | Step 2363100 | Avg Loss: 0.0159 | Grad Norm: 0.00904382\n",
      "Epoch 4 | Step 2363200 | Avg Loss: 0.0156 | Grad Norm: 0.00913658\n",
      "Epoch 4 | Step 2363300 | Avg Loss: 0.0152 | Grad Norm: 0.00880197\n",
      "Epoch 4 | Step 2363400 | Avg Loss: 0.0154 | Grad Norm: 0.00877607\n",
      "Epoch 4 | Step 2363500 | Avg Loss: 0.0152 | Grad Norm: 0.00951221\n",
      "Epoch 4 | Step 2363600 | Avg Loss: 0.0151 | Grad Norm: 0.00817295\n",
      "Epoch 4 | Step 2363700 | Avg Loss: 0.0154 | Grad Norm: 0.00844432\n",
      "Epoch 4 | Step 2363800 | Avg Loss: 0.0156 | Grad Norm: 0.00913855\n",
      "Epoch 4 | Step 2363900 | Avg Loss: 0.0152 | Grad Norm: 0.00872975\n",
      "Epoch 4 | Step 2364000 | Avg Loss: 0.0152 | Grad Norm: 0.00794657\n",
      "Epoch 4 | Step 2364100 | Avg Loss: 0.0156 | Grad Norm: 0.00836338\n",
      "Epoch 4 | Step 2364200 | Avg Loss: 0.0158 | Grad Norm: 0.02620929\n",
      "Epoch 4 | Step 2364300 | Avg Loss: 0.0159 | Grad Norm: 0.00978379\n",
      "Epoch 4 | Step 2364400 | Avg Loss: 0.0161 | Grad Norm: 0.00929543\n",
      "Epoch 4 | Step 2364500 | Avg Loss: 0.0157 | Grad Norm: 0.00771188\n",
      "Epoch 4 | Step 2364600 | Avg Loss: 0.0153 | Grad Norm: 0.00823064\n",
      "Epoch 4 | Step 2364700 | Avg Loss: 0.0155 | Grad Norm: 0.00822084\n",
      "Epoch 4 | Step 2364800 | Avg Loss: 0.0155 | Grad Norm: 0.00719870\n",
      "Epoch 4 | Step 2364900 | Avg Loss: 0.0154 | Grad Norm: 0.00826695\n",
      "Epoch 4 | Step 2365000 | Avg Loss: 0.0157 | Grad Norm: 0.00953679\n",
      "Epoch 4 | Step 2365100 | Avg Loss: 0.0156 | Grad Norm: 0.00952545\n",
      "Epoch 4 | Step 2365200 | Avg Loss: 0.0159 | Grad Norm: 0.00768421\n",
      "Epoch 4 | Step 2365300 | Avg Loss: 0.0159 | Grad Norm: 0.00943576\n",
      "Epoch 4 | Step 2365400 | Avg Loss: 0.0159 | Grad Norm: 0.00826248\n",
      "Epoch 4 | Step 2365500 | Avg Loss: 0.0164 | Grad Norm: 0.00965167\n",
      "Epoch 4 | Step 2365600 | Avg Loss: 0.0160 | Grad Norm: 0.00960553\n",
      "Epoch 4 | Step 2365700 | Avg Loss: 0.0160 | Grad Norm: 0.00936232\n",
      "Epoch 4 | Step 2365800 | Avg Loss: 0.0158 | Grad Norm: 0.01070099\n",
      "Epoch 4 | Step 2365900 | Avg Loss: 0.0155 | Grad Norm: 0.00913450\n",
      "Epoch 4 | Step 2366000 | Avg Loss: 0.0156 | Grad Norm: 0.00990560\n",
      "Epoch 4 | Step 2366100 | Avg Loss: 0.0156 | Grad Norm: 0.00835437\n",
      "Epoch 4 | Step 2366200 | Avg Loss: 0.0155 | Grad Norm: 0.00890289\n",
      "Epoch 4 | Step 2366300 | Avg Loss: 0.0155 | Grad Norm: 0.00888435\n",
      "Epoch 4 | Step 2366400 | Avg Loss: 0.0156 | Grad Norm: 0.00851258\n",
      "Epoch 4 | Step 2366500 | Avg Loss: 0.0158 | Grad Norm: 0.00962024\n",
      "Epoch 4 | Step 2366600 | Avg Loss: 0.0160 | Grad Norm: 0.00890577\n",
      "Epoch 4 | Step 2366700 | Avg Loss: 0.0159 | Grad Norm: 0.00866064\n",
      "Epoch 4 | Step 2366800 | Avg Loss: 0.0156 | Grad Norm: 0.00878572\n",
      "Epoch 4 | Step 2366900 | Avg Loss: 0.0155 | Grad Norm: 0.00950371\n",
      "Epoch 4 | Step 2367000 | Avg Loss: 0.0152 | Grad Norm: 0.00968769\n",
      "Epoch 4 | Step 2367100 | Avg Loss: 0.0156 | Grad Norm: 0.00852824\n",
      "Epoch 4 | Step 2367200 | Avg Loss: 0.0155 | Grad Norm: 0.00891632\n",
      "Epoch 4 | Step 2367300 | Avg Loss: 0.0161 | Grad Norm: 0.00874161\n",
      "Epoch 4 | Step 2367400 | Avg Loss: 0.0161 | Grad Norm: 0.00846074\n",
      "Epoch 4 | Step 2367500 | Avg Loss: 0.0163 | Grad Norm: 0.00977752\n",
      "Epoch 4 | Step 2367600 | Avg Loss: 0.0159 | Grad Norm: 0.00831757\n",
      "Epoch 4 | Step 2367700 | Avg Loss: 0.0158 | Grad Norm: 0.00828439\n",
      "Epoch 4 | Step 2367800 | Avg Loss: 0.0159 | Grad Norm: 0.00993457\n",
      "Epoch 4 | Step 2367900 | Avg Loss: 0.0159 | Grad Norm: 0.00823043\n",
      "Epoch 4 | Step 2368000 | Avg Loss: 0.0160 | Grad Norm: 0.00926459\n",
      "Epoch 4 | Step 2368100 | Avg Loss: 0.0157 | Grad Norm: 0.00975390\n",
      "Epoch 4 | Step 2368200 | Avg Loss: 0.0156 | Grad Norm: 0.00899325\n",
      "Epoch 4 | Step 2368300 | Avg Loss: 0.0155 | Grad Norm: 0.00986300\n",
      "Epoch 4 | Step 2368400 | Avg Loss: 0.0161 | Grad Norm: 0.00781851\n",
      "Epoch 4 | Step 2368500 | Avg Loss: 0.0164 | Grad Norm: 0.00985533\n",
      "Epoch 4 | Step 2368600 | Avg Loss: 0.0157 | Grad Norm: 0.00862409\n",
      "Epoch 4 | Step 2368700 | Avg Loss: 0.0158 | Grad Norm: 0.00953494\n",
      "Epoch 4 | Step 2368800 | Avg Loss: 0.0158 | Grad Norm: 0.00973789\n",
      "Epoch 4 | Step 2368900 | Avg Loss: 0.0159 | Grad Norm: 0.00882426\n",
      "Epoch 4 | Step 2369000 | Avg Loss: 0.0162 | Grad Norm: 0.00913151\n",
      "Epoch 4 | Step 2369100 | Avg Loss: 0.0165 | Grad Norm: 0.00843787\n",
      "Epoch 4 | Step 2369200 | Avg Loss: 0.0161 | Grad Norm: 0.00771409\n",
      "Epoch 4 | Step 2369300 | Avg Loss: 0.0162 | Grad Norm: 0.01116306\n",
      "Epoch 4 | Step 2369400 | Avg Loss: 0.0161 | Grad Norm: 0.00855602\n",
      "Epoch 4 | Step 2369500 | Avg Loss: 0.0160 | Grad Norm: 0.00985711\n",
      "Epoch 4 | Step 2369600 | Avg Loss: 0.0161 | Grad Norm: 0.00969884\n",
      "Epoch 4 | Step 2369700 | Avg Loss: 0.0162 | Grad Norm: 0.00846887\n",
      "Epoch 4 | Step 2369800 | Avg Loss: 0.0155 | Grad Norm: 0.00948533\n",
      "Epoch 4 | Step 2369900 | Avg Loss: 0.0154 | Grad Norm: 0.00839615\n",
      "Epoch 4 | Step 2370000 | Avg Loss: 0.0159 | Grad Norm: 0.01061232\n",
      "Epoch 4 | Step 2370100 | Avg Loss: 0.0157 | Grad Norm: 0.00976677\n",
      "Epoch 4 | Step 2370200 | Avg Loss: 0.0155 | Grad Norm: 0.00971000\n",
      "Epoch 4 | Step 2370300 | Avg Loss: 0.0153 | Grad Norm: 0.00817858\n",
      "Epoch 4 | Step 2370400 | Avg Loss: 0.0154 | Grad Norm: 0.00740986\n",
      "Epoch 4 | Step 2370500 | Avg Loss: 0.0155 | Grad Norm: 0.00910597\n",
      "Epoch 4 | Step 2370600 | Avg Loss: 0.0157 | Grad Norm: 0.00902252\n",
      "Epoch 4 | Step 2370700 | Avg Loss: 0.0160 | Grad Norm: 0.00988983\n",
      "Epoch 4 | Step 2370800 | Avg Loss: 0.0158 | Grad Norm: 0.00851390\n",
      "Epoch 4 | Step 2370900 | Avg Loss: 0.0157 | Grad Norm: 0.01225990\n",
      "Epoch 4 | Step 2371000 | Avg Loss: 0.0155 | Grad Norm: 0.00794598\n",
      "Epoch 4 | Step 2371100 | Avg Loss: 0.0156 | Grad Norm: 0.00890535\n",
      "Epoch 4 | Step 2371200 | Avg Loss: 0.0161 | Grad Norm: 0.00872073\n",
      "Epoch 4 | Step 2371300 | Avg Loss: 0.0159 | Grad Norm: 0.00858061\n",
      "Epoch 4 | Step 2371400 | Avg Loss: 0.0158 | Grad Norm: 0.00917646\n",
      "Epoch 4 | Step 2371500 | Avg Loss: 0.0160 | Grad Norm: 0.00835285\n",
      "Epoch 4 | Step 2371600 | Avg Loss: 0.0159 | Grad Norm: 0.01035241\n",
      "Epoch 4 | Step 2371700 | Avg Loss: 0.0159 | Grad Norm: 0.00977210\n",
      "Epoch 4 | Step 2371800 | Avg Loss: 0.0158 | Grad Norm: 0.00837523\n",
      "Epoch 4 | Step 2371900 | Avg Loss: 0.0159 | Grad Norm: 0.00863507\n",
      "Epoch 4 | Step 2372000 | Avg Loss: 0.0156 | Grad Norm: 0.00945915\n",
      "Epoch 4 | Step 2372100 | Avg Loss: 0.0153 | Grad Norm: 0.00905531\n",
      "Epoch 4 | Step 2372200 | Avg Loss: 0.0156 | Grad Norm: 0.00895076\n",
      "Epoch 4 | Step 2372300 | Avg Loss: 0.0158 | Grad Norm: 0.00843959\n",
      "Epoch 4 | Step 2372400 | Avg Loss: 0.0156 | Grad Norm: 0.00863265\n",
      "Epoch 4 | Step 2372500 | Avg Loss: 0.0156 | Grad Norm: 0.00794749\n",
      "Epoch 4 | Step 2372600 | Avg Loss: 0.0154 | Grad Norm: 0.00891914\n",
      "Epoch 4 | Step 2372700 | Avg Loss: 0.0153 | Grad Norm: 0.00934551\n",
      "Epoch 4 | Step 2372800 | Avg Loss: 0.0154 | Grad Norm: 0.00864923\n",
      "Epoch 4 | Step 2372900 | Avg Loss: 0.0158 | Grad Norm: 0.00991701\n",
      "Epoch 4 | Step 2373000 | Avg Loss: 0.0154 | Grad Norm: 0.00908281\n",
      "Epoch 4 | Step 2373100 | Avg Loss: 0.0152 | Grad Norm: 0.00877971\n",
      "Epoch 4 | Step 2373200 | Avg Loss: 0.0153 | Grad Norm: 0.00873011\n",
      "Epoch 4 | Step 2373300 | Avg Loss: 0.0156 | Grad Norm: 0.00979706\n",
      "Epoch 4 | Step 2373400 | Avg Loss: 0.0155 | Grad Norm: 0.00943069\n",
      "Epoch 4 | Step 2373500 | Avg Loss: 0.0156 | Grad Norm: 0.01236177\n",
      "Epoch 4 | Step 2373600 | Avg Loss: 0.0159 | Grad Norm: 0.00909130\n",
      "Epoch 4 | Step 2373700 | Avg Loss: 0.0157 | Grad Norm: 0.00917209\n",
      "Epoch 4 | Step 2373800 | Avg Loss: 0.0158 | Grad Norm: 0.01155020\n",
      "Epoch 4 | Step 2373900 | Avg Loss: 0.0158 | Grad Norm: 0.00895599\n",
      "Epoch 4 | Step 2374000 | Avg Loss: 0.0158 | Grad Norm: 0.00866728\n",
      "Epoch 4 | Step 2374100 | Avg Loss: 0.0159 | Grad Norm: 0.00861984\n",
      "Epoch 4 | Step 2374200 | Avg Loss: 0.0157 | Grad Norm: 0.00872485\n",
      "Epoch 4 | Step 2374300 | Avg Loss: 0.0157 | Grad Norm: 0.00881091\n",
      "Epoch 4 | Step 2374400 | Avg Loss: 0.0155 | Grad Norm: 0.00893220\n",
      "Epoch 4 | Step 2374500 | Avg Loss: 0.0158 | Grad Norm: 0.00848003\n",
      "Epoch 4 | Step 2374600 | Avg Loss: 0.0157 | Grad Norm: 0.00860169\n",
      "Epoch 4 | Step 2374700 | Avg Loss: 0.0158 | Grad Norm: 0.00931489\n",
      "Epoch 4 | Step 2374800 | Avg Loss: 0.0156 | Grad Norm: 0.00903442\n",
      "Epoch 4 | Step 2374900 | Avg Loss: 0.0154 | Grad Norm: 0.00812638\n",
      "Epoch 4 | Step 2375000 | Avg Loss: 0.0153 | Grad Norm: 0.00879310\n",
      "Epoch 4 | Step 2375100 | Avg Loss: 0.0154 | Grad Norm: 0.00788727\n",
      "Epoch 4 | Step 2375200 | Avg Loss: 0.0155 | Grad Norm: 0.00879849\n",
      "Epoch 4 | Step 2375300 | Avg Loss: 0.0153 | Grad Norm: 0.00888812\n",
      "Epoch 4 | Step 2375400 | Avg Loss: 0.0154 | Grad Norm: 0.00868860\n",
      "Epoch 4 | Step 2375500 | Avg Loss: 0.0154 | Grad Norm: 0.00874317\n",
      "Epoch 4 | Step 2375600 | Avg Loss: 0.0157 | Grad Norm: 0.00911206\n",
      "Epoch 4 | Step 2375700 | Avg Loss: 0.0157 | Grad Norm: 0.01083542\n",
      "Epoch 4 | Step 2375800 | Avg Loss: 0.0157 | Grad Norm: 0.01095994\n",
      "Epoch 4 | Step 2375900 | Avg Loss: 0.0155 | Grad Norm: 0.00809690\n",
      "Epoch 4 | Step 2376000 | Avg Loss: 0.0159 | Grad Norm: 0.00794481\n",
      "Epoch 4 | Step 2376100 | Avg Loss: 0.0161 | Grad Norm: 0.00881070\n",
      "Epoch 4 | Step 2376200 | Avg Loss: 0.0159 | Grad Norm: 0.01775445\n",
      "Epoch 4 | Step 2376300 | Avg Loss: 0.0160 | Grad Norm: 0.00924126\n",
      "Epoch 4 | Step 2376400 | Avg Loss: 0.0158 | Grad Norm: 0.00834048\n",
      "Epoch 4 | Step 2376500 | Avg Loss: 0.0161 | Grad Norm: 0.01200607\n",
      "Epoch 4 | Step 2376600 | Avg Loss: 0.0157 | Grad Norm: 0.00831526\n",
      "Epoch 4 | Step 2376700 | Avg Loss: 0.0156 | Grad Norm: 0.00920221\n",
      "Epoch 4 | Step 2376800 | Avg Loss: 0.0157 | Grad Norm: 0.00898507\n",
      "Epoch 4 | Step 2376900 | Avg Loss: 0.0156 | Grad Norm: 0.00979489\n",
      "Epoch 4 | Step 2377000 | Avg Loss: 0.0155 | Grad Norm: 0.01049586\n",
      "Epoch 4 | Step 2377100 | Avg Loss: 0.0154 | Grad Norm: 0.00915713\n",
      "Epoch 4 | Step 2377200 | Avg Loss: 0.0154 | Grad Norm: 0.00879047\n",
      "Epoch 4 | Step 2377300 | Avg Loss: 0.0153 | Grad Norm: 0.00955891\n",
      "Epoch 4 | Step 2377400 | Avg Loss: 0.0155 | Grad Norm: 0.00894676\n",
      "Epoch 4 | Step 2377500 | Avg Loss: 0.0158 | Grad Norm: 0.00852089\n",
      "Epoch 4 | Step 2377600 | Avg Loss: 0.0156 | Grad Norm: 0.00948239\n",
      "Epoch 4 | Step 2377700 | Avg Loss: 0.0153 | Grad Norm: 0.00931323\n",
      "Epoch 4 | Step 2377800 | Avg Loss: 0.0150 | Grad Norm: 0.00967046\n",
      "Epoch 4 | Step 2377900 | Avg Loss: 0.0146 | Grad Norm: 0.00862420\n",
      "Epoch 4 | Step 2378000 | Avg Loss: 0.0147 | Grad Norm: 0.00843191\n",
      "Epoch 4 | Step 2378100 | Avg Loss: 0.0150 | Grad Norm: 0.00816420\n",
      "Epoch 4 | Step 2378200 | Avg Loss: 0.0155 | Grad Norm: 0.00922011\n",
      "Epoch 4 | Step 2378300 | Avg Loss: 0.0157 | Grad Norm: 0.00953301\n",
      "Epoch 4 | Step 2378400 | Avg Loss: 0.0153 | Grad Norm: 0.00925506\n",
      "Epoch 4 | Step 2378500 | Avg Loss: 0.0154 | Grad Norm: 0.00931134\n",
      "Epoch 4 | Step 2378600 | Avg Loss: 0.0154 | Grad Norm: 0.00809741\n",
      "Epoch 4 | Step 2378700 | Avg Loss: 0.0156 | Grad Norm: 0.00943474\n",
      "Epoch 4 | Step 2378800 | Avg Loss: 0.0154 | Grad Norm: 0.01115737\n",
      "Epoch 4 | Step 2378900 | Avg Loss: 0.0158 | Grad Norm: 0.00871201\n",
      "Epoch 4 | Step 2379000 | Avg Loss: 0.0158 | Grad Norm: 0.00852419\n",
      "Epoch 4 | Step 2379100 | Avg Loss: 0.0159 | Grad Norm: 0.00944939\n",
      "Epoch 4 | Step 2379200 | Avg Loss: 0.0159 | Grad Norm: 0.00944177\n",
      "Epoch 4 | Step 2379300 | Avg Loss: 0.0161 | Grad Norm: 0.00796394\n",
      "Epoch 4 | Step 2379400 | Avg Loss: 0.0163 | Grad Norm: 0.00865670\n",
      "Epoch 4 | Step 2379500 | Avg Loss: 0.0156 | Grad Norm: 0.00837875\n",
      "Epoch 4 | Step 2379600 | Avg Loss: 0.0156 | Grad Norm: 0.00797910\n",
      "Epoch 4 | Step 2379700 | Avg Loss: 0.0153 | Grad Norm: 0.00913283\n",
      "Epoch 4 | Step 2379800 | Avg Loss: 0.0151 | Grad Norm: 0.01161865\n",
      "Epoch 4 | Step 2379900 | Avg Loss: 0.0155 | Grad Norm: 0.00842556\n",
      "Epoch 4 | Step 2380000 | Avg Loss: 0.0158 | Grad Norm: 0.00981815\n",
      "Epoch 4 | Step 2380100 | Avg Loss: 0.0160 | Grad Norm: 0.01075828\n",
      "Epoch 4 | Step 2380200 | Avg Loss: 0.0161 | Grad Norm: 0.00870257\n",
      "Epoch 4 | Step 2380300 | Avg Loss: 0.0159 | Grad Norm: 0.00936902\n",
      "Epoch 4 | Step 2380400 | Avg Loss: 0.0159 | Grad Norm: 0.00863412\n",
      "Epoch 4 | Step 2380500 | Avg Loss: 0.0160 | Grad Norm: 0.01172492\n",
      "Epoch 4 | Step 2380600 | Avg Loss: 0.0157 | Grad Norm: 0.00929742\n",
      "Epoch 4 | Step 2380700 | Avg Loss: 0.0162 | Grad Norm: 0.00954578\n",
      "Epoch 4 | Step 2380800 | Avg Loss: 0.0164 | Grad Norm: 0.00909104\n",
      "Epoch 4 | Step 2380900 | Avg Loss: 0.0160 | Grad Norm: 0.00750683\n",
      "Epoch 4 | Step 2381000 | Avg Loss: 0.0158 | Grad Norm: 0.00892042\n",
      "Epoch 4 | Step 2381100 | Avg Loss: 0.0159 | Grad Norm: 0.00934359\n",
      "Epoch 4 | Step 2381200 | Avg Loss: 0.0159 | Grad Norm: 0.01030655\n",
      "Epoch 4 | Step 2381300 | Avg Loss: 0.0157 | Grad Norm: 0.01038552\n",
      "Epoch 4 | Step 2381400 | Avg Loss: 0.0158 | Grad Norm: 0.00856365\n",
      "Epoch 4 | Step 2381500 | Avg Loss: 0.0157 | Grad Norm: 0.00949892\n",
      "Epoch 4 | Step 2381600 | Avg Loss: 0.0159 | Grad Norm: 0.00897093\n",
      "Epoch 4 | Step 2381700 | Avg Loss: 0.0158 | Grad Norm: 0.00878792\n",
      "Epoch 4 | Step 2381800 | Avg Loss: 0.0162 | Grad Norm: 0.00973930\n",
      "Epoch 4 | Step 2381900 | Avg Loss: 0.0163 | Grad Norm: 0.00898899\n",
      "Epoch 4 | Step 2382000 | Avg Loss: 0.0164 | Grad Norm: 0.01126868\n",
      "Epoch 4 | Step 2382100 | Avg Loss: 0.0163 | Grad Norm: 0.00904533\n",
      "Epoch 4 | Step 2382200 | Avg Loss: 0.0163 | Grad Norm: 0.00901396\n",
      "Epoch 4 | Step 2382300 | Avg Loss: 0.0161 | Grad Norm: 0.00829384\n",
      "Epoch 4 | Step 2382400 | Avg Loss: 0.0158 | Grad Norm: 0.00899882\n",
      "Epoch 4 | Step 2382500 | Avg Loss: 0.0161 | Grad Norm: 0.00898210\n",
      "Epoch 4 | Step 2382600 | Avg Loss: 0.0157 | Grad Norm: 0.00834960\n",
      "Epoch 4 | Step 2382700 | Avg Loss: 0.0158 | Grad Norm: 0.00876737\n",
      "Epoch 4 | Step 2382800 | Avg Loss: 0.0158 | Grad Norm: 0.00936252\n",
      "Epoch 4 | Step 2382900 | Avg Loss: 0.0159 | Grad Norm: 0.00971488\n",
      "Epoch 4 | Step 2383000 | Avg Loss: 0.0155 | Grad Norm: 0.01014105\n",
      "Epoch 4 | Step 2383100 | Avg Loss: 0.0157 | Grad Norm: 0.00921308\n",
      "Epoch 4 | Step 2383200 | Avg Loss: 0.0155 | Grad Norm: 0.00977353\n",
      "Epoch 4 | Step 2383300 | Avg Loss: 0.0155 | Grad Norm: 0.00960081\n",
      "Epoch 4 | Step 2383400 | Avg Loss: 0.0155 | Grad Norm: 0.00925476\n",
      "Epoch 4 | Step 2383500 | Avg Loss: 0.0157 | Grad Norm: 0.00943933\n",
      "Epoch 4 | Step 2383600 | Avg Loss: 0.0157 | Grad Norm: 0.00870591\n",
      "Epoch 4 | Step 2383700 | Avg Loss: 0.0155 | Grad Norm: 0.00960558\n",
      "Epoch 4 | Step 2383800 | Avg Loss: 0.0155 | Grad Norm: 0.00928460\n",
      "Epoch 4 | Step 2383900 | Avg Loss: 0.0154 | Grad Norm: 0.00861763\n",
      "Epoch 4 | Step 2384000 | Avg Loss: 0.0156 | Grad Norm: 0.00840428\n",
      "Epoch 4 | Step 2384100 | Avg Loss: 0.0153 | Grad Norm: 0.00886111\n",
      "Epoch 4 | Step 2384200 | Avg Loss: 0.0154 | Grad Norm: 0.00912994\n",
      "Epoch 4 | Step 2384300 | Avg Loss: 0.0155 | Grad Norm: 0.00841038\n",
      "Epoch 4 | Step 2384400 | Avg Loss: 0.0152 | Grad Norm: 0.00809616\n",
      "Epoch 4 | Step 2384500 | Avg Loss: 0.0154 | Grad Norm: 0.00878090\n",
      "Epoch 4 | Step 2384600 | Avg Loss: 0.0149 | Grad Norm: 0.00826738\n",
      "Epoch 4 | Step 2384700 | Avg Loss: 0.0153 | Grad Norm: 0.00882940\n",
      "Epoch 4 | Step 2384800 | Avg Loss: 0.0152 | Grad Norm: 0.00819607\n",
      "Epoch 4 | Step 2384900 | Avg Loss: 0.0152 | Grad Norm: 0.00830254\n",
      "Epoch 4 | Step 2385000 | Avg Loss: 0.0156 | Grad Norm: 0.00928940\n",
      "Epoch 4 | Step 2385100 | Avg Loss: 0.0156 | Grad Norm: 0.00814035\n",
      "Epoch 4 | Step 2385200 | Avg Loss: 0.0153 | Grad Norm: 0.00856933\n",
      "Epoch 4 | Step 2385300 | Avg Loss: 0.0155 | Grad Norm: 0.00830066\n",
      "Epoch 4 | Step 2385400 | Avg Loss: 0.0158 | Grad Norm: 0.00870634\n",
      "Epoch 4 | Step 2385500 | Avg Loss: 0.0155 | Grad Norm: 0.00867691\n",
      "Epoch 4 | Step 2385600 | Avg Loss: 0.0155 | Grad Norm: 0.00876597\n",
      "Epoch 4 | Step 2385700 | Avg Loss: 0.0158 | Grad Norm: 0.01063644\n",
      "Epoch 4 | Step 2385800 | Avg Loss: 0.0161 | Grad Norm: 0.00892650\n",
      "Epoch 4 | Step 2385900 | Avg Loss: 0.0160 | Grad Norm: 0.00999891\n",
      "Epoch 4 | Step 2386000 | Avg Loss: 0.0160 | Grad Norm: 0.01215078\n",
      "Epoch 4 | Step 2386100 | Avg Loss: 0.0163 | Grad Norm: 0.01134183\n",
      "Epoch 4 | Step 2386200 | Avg Loss: 0.0165 | Grad Norm: 0.00885632\n",
      "Epoch 4 | Step 2386300 | Avg Loss: 0.0162 | Grad Norm: 0.01023749\n",
      "Epoch 4 | Step 2386400 | Avg Loss: 0.0158 | Grad Norm: 0.00877496\n",
      "Epoch 4 | Step 2386500 | Avg Loss: 0.0156 | Grad Norm: 0.00825699\n",
      "Epoch 4 | Step 2386600 | Avg Loss: 0.0154 | Grad Norm: 0.01278503\n",
      "Epoch 4 | Step 2386700 | Avg Loss: 0.0151 | Grad Norm: 0.00880631\n",
      "Epoch 4 | Step 2386800 | Avg Loss: 0.0154 | Grad Norm: 0.01019757\n",
      "Epoch 4 | Step 2386900 | Avg Loss: 0.0158 | Grad Norm: 0.00834257\n",
      "Epoch 4 | Step 2387000 | Avg Loss: 0.0156 | Grad Norm: 0.00840780\n",
      "Epoch 4 | Step 2387100 | Avg Loss: 0.0156 | Grad Norm: 0.00788484\n",
      "Epoch 4 | Step 2387200 | Avg Loss: 0.0155 | Grad Norm: 0.00772835\n",
      "Epoch 4 | Step 2387300 | Avg Loss: 0.0158 | Grad Norm: 0.00937027\n",
      "Epoch 4 | Step 2387400 | Avg Loss: 0.0155 | Grad Norm: 0.01010954\n",
      "Epoch 4 | Step 2387500 | Avg Loss: 0.0153 | Grad Norm: 0.00786204\n",
      "Epoch 4 | Step 2387600 | Avg Loss: 0.0156 | Grad Norm: 0.00920844\n",
      "Epoch 4 | Step 2387700 | Avg Loss: 0.0155 | Grad Norm: 0.01056131\n",
      "Epoch 4 | Step 2387800 | Avg Loss: 0.0153 | Grad Norm: 0.00845175\n",
      "Epoch 4 | Step 2387900 | Avg Loss: 0.0156 | Grad Norm: 0.00940127\n",
      "Epoch 4 | Step 2388000 | Avg Loss: 0.0158 | Grad Norm: 0.00903054\n",
      "Epoch 4 | Step 2388100 | Avg Loss: 0.0159 | Grad Norm: 0.00969760\n",
      "Epoch 4 | Step 2388200 | Avg Loss: 0.0160 | Grad Norm: 0.00927850\n",
      "Epoch 4 | Step 2388300 | Avg Loss: 0.0159 | Grad Norm: 0.01056802\n",
      "Epoch 4 | Step 2388400 | Avg Loss: 0.0159 | Grad Norm: 0.00960756\n",
      "Epoch 4 | Step 2388500 | Avg Loss: 0.0164 | Grad Norm: 0.01012645\n",
      "Epoch 4 | Step 2388600 | Avg Loss: 0.0162 | Grad Norm: 0.00965519\n",
      "Epoch 4 | Step 2388700 | Avg Loss: 0.0155 | Grad Norm: 0.00772772\n",
      "Epoch 4 | Step 2388800 | Avg Loss: 0.0157 | Grad Norm: 0.00817643\n",
      "Epoch 4 | Step 2388900 | Avg Loss: 0.0153 | Grad Norm: 0.00907341\n",
      "Epoch 4 | Step 2389000 | Avg Loss: 0.0153 | Grad Norm: 0.00904820\n",
      "Epoch 4 | Step 2389100 | Avg Loss: 0.0151 | Grad Norm: 0.01020695\n",
      "Epoch 4 | Step 2389200 | Avg Loss: 0.0154 | Grad Norm: 0.00908948\n",
      "Epoch 4 | Step 2389300 | Avg Loss: 0.0152 | Grad Norm: 0.00830623\n",
      "Epoch 4 | Step 2389400 | Avg Loss: 0.0156 | Grad Norm: 0.00970761\n",
      "Epoch 4 | Step 2389500 | Avg Loss: 0.0157 | Grad Norm: 0.00889899\n",
      "Epoch 4 | Step 2389600 | Avg Loss: 0.0152 | Grad Norm: 0.00791379\n",
      "Epoch 4 | Step 2389700 | Avg Loss: 0.0149 | Grad Norm: 0.00820687\n",
      "Epoch 4 | Step 2389800 | Avg Loss: 0.0152 | Grad Norm: 0.01039502\n",
      "Epoch 4 | Step 2389900 | Avg Loss: 0.0152 | Grad Norm: 0.00922330\n",
      "Epoch 4 | Step 2390000 | Avg Loss: 0.0149 | Grad Norm: 0.00880346\n",
      "Epoch 4 | Step 2390100 | Avg Loss: 0.0153 | Grad Norm: 0.00847950\n",
      "Epoch 4 | Step 2390200 | Avg Loss: 0.0153 | Grad Norm: 0.01155978\n",
      "Epoch 4 | Step 2390300 | Avg Loss: 0.0149 | Grad Norm: 0.00823049\n",
      "Epoch 4 | Step 2390400 | Avg Loss: 0.0150 | Grad Norm: 0.00881967\n",
      "Epoch 4 | Step 2390500 | Avg Loss: 0.0152 | Grad Norm: 0.00862563\n",
      "Epoch 4 | Step 2390600 | Avg Loss: 0.0151 | Grad Norm: 0.00918008\n",
      "Epoch 4 | Step 2390700 | Avg Loss: 0.0152 | Grad Norm: 0.00946864\n",
      "Epoch 4 | Step 2390800 | Avg Loss: 0.0154 | Grad Norm: 0.00700570\n",
      "Epoch 4 | Step 2390900 | Avg Loss: 0.0156 | Grad Norm: 0.00864141\n",
      "Epoch 4 | Step 2391000 | Avg Loss: 0.0154 | Grad Norm: 0.00888394\n",
      "Epoch 4 | Step 2391100 | Avg Loss: 0.0156 | Grad Norm: 0.00873948\n",
      "Epoch 4 | Step 2391200 | Avg Loss: 0.0159 | Grad Norm: 0.00899875\n",
      "Epoch 4 | Step 2391300 | Avg Loss: 0.0156 | Grad Norm: 0.00884942\n",
      "Epoch 4 | Step 2391400 | Avg Loss: 0.0159 | Grad Norm: 0.00827247\n",
      "Epoch 4 | Step 2391500 | Avg Loss: 0.0165 | Grad Norm: 0.00861716\n",
      "Epoch 4 | Step 2391600 | Avg Loss: 0.0160 | Grad Norm: 0.01119603\n",
      "Epoch 4 | Step 2391700 | Avg Loss: 0.0161 | Grad Norm: 0.00887233\n",
      "Epoch 4 | Step 2391800 | Avg Loss: 0.0157 | Grad Norm: 0.00832827\n",
      "Epoch 4 | Step 2391900 | Avg Loss: 0.0155 | Grad Norm: 0.00905104\n",
      "Epoch 4 | Step 2392000 | Avg Loss: 0.0152 | Grad Norm: 0.01038061\n",
      "Epoch 4 | Step 2392100 | Avg Loss: 0.0155 | Grad Norm: 0.00866310\n",
      "Epoch 4 | Step 2392200 | Avg Loss: 0.0158 | Grad Norm: 0.01029588\n",
      "Epoch 4 | Step 2392300 | Avg Loss: 0.0154 | Grad Norm: 0.00927677\n",
      "Epoch 4 | Step 2392400 | Avg Loss: 0.0155 | Grad Norm: 0.00957492\n",
      "Epoch 4 | Step 2392500 | Avg Loss: 0.0157 | Grad Norm: 0.00883194\n",
      "Epoch 4 | Step 2392600 | Avg Loss: 0.0155 | Grad Norm: 0.00878789\n",
      "Epoch 4 | Step 2392700 | Avg Loss: 0.0154 | Grad Norm: 0.00858671\n",
      "Epoch 4 | Step 2392800 | Avg Loss: 0.0154 | Grad Norm: 0.00930224\n",
      "Epoch 4 | Step 2392900 | Avg Loss: 0.0153 | Grad Norm: 0.00809323\n",
      "Epoch 4 | Step 2393000 | Avg Loss: 0.0155 | Grad Norm: 0.00827288\n",
      "Epoch 4 | Step 2393100 | Avg Loss: 0.0160 | Grad Norm: 0.00968864\n",
      "Epoch 4 | Step 2393200 | Avg Loss: 0.0162 | Grad Norm: 0.00889097\n",
      "Epoch 4 | Step 2393300 | Avg Loss: 0.0158 | Grad Norm: 0.01090568\n",
      "Epoch 4 | Step 2393400 | Avg Loss: 0.0158 | Grad Norm: 0.00889735\n",
      "Epoch 4 | Step 2393500 | Avg Loss: 0.0156 | Grad Norm: 0.01004138\n",
      "Epoch 4 | Step 2393600 | Avg Loss: 0.0154 | Grad Norm: 0.00915685\n",
      "Epoch 4 | Step 2393700 | Avg Loss: 0.0155 | Grad Norm: 0.01087682\n",
      "Epoch 4 | Step 2393800 | Avg Loss: 0.0156 | Grad Norm: 0.00962488\n",
      "Epoch 4 | Step 2393900 | Avg Loss: 0.0159 | Grad Norm: 0.00874384\n",
      "Epoch 4 | Step 2394000 | Avg Loss: 0.0155 | Grad Norm: 0.01008401\n",
      "Epoch 4 | Step 2394100 | Avg Loss: 0.0154 | Grad Norm: 0.01015718\n",
      "Epoch 4 | Step 2394200 | Avg Loss: 0.0158 | Grad Norm: 0.00853531\n",
      "Epoch 4 | Step 2394300 | Avg Loss: 0.0156 | Grad Norm: 0.01009298\n",
      "Epoch 4 | Step 2394400 | Avg Loss: 0.0156 | Grad Norm: 0.01223999\n",
      "Epoch 4 | Step 2394500 | Avg Loss: 0.0152 | Grad Norm: 0.00838591\n",
      "Epoch 4 | Step 2394600 | Avg Loss: 0.0155 | Grad Norm: 0.00756013\n",
      "Epoch 4 | Step 2394700 | Avg Loss: 0.0156 | Grad Norm: 0.00946711\n",
      "Epoch 4 | Step 2394800 | Avg Loss: 0.0158 | Grad Norm: 0.00905775\n",
      "Epoch 4 | Step 2394900 | Avg Loss: 0.0156 | Grad Norm: 0.01117300\n",
      "Epoch 4 | Step 2395000 | Avg Loss: 0.0157 | Grad Norm: 0.01033274\n",
      "Epoch 4 | Step 2395100 | Avg Loss: 0.0156 | Grad Norm: 0.01096359\n",
      "Epoch 4 | Step 2395200 | Avg Loss: 0.0155 | Grad Norm: 0.00766679\n",
      "Epoch 4 | Step 2395300 | Avg Loss: 0.0156 | Grad Norm: 0.00982778\n",
      "Epoch 4 | Step 2395400 | Avg Loss: 0.0158 | Grad Norm: 0.00878369\n",
      "Epoch 4 | Step 2395500 | Avg Loss: 0.0156 | Grad Norm: 0.00818240\n",
      "Epoch 4 | Step 2395600 | Avg Loss: 0.0160 | Grad Norm: 0.00919388\n",
      "Epoch 4 | Step 2395700 | Avg Loss: 0.0158 | Grad Norm: 0.00860469\n",
      "Epoch 4 | Step 2395800 | Avg Loss: 0.0156 | Grad Norm: 0.00923564\n",
      "Epoch 4 | Step 2395900 | Avg Loss: 0.0159 | Grad Norm: 0.01028700\n",
      "Epoch 4 | Step 2396000 | Avg Loss: 0.0160 | Grad Norm: 0.01127309\n",
      "Epoch 4 | Step 2396100 | Avg Loss: 0.0156 | Grad Norm: 0.00749161\n",
      "Epoch 4 | Step 2396200 | Avg Loss: 0.0153 | Grad Norm: 0.01058828\n",
      "Epoch 4 | Step 2396300 | Avg Loss: 0.0155 | Grad Norm: 0.00926605\n",
      "Epoch 4 | Step 2396400 | Avg Loss: 0.0157 | Grad Norm: 0.00905389\n",
      "Epoch 4 | Step 2396500 | Avg Loss: 0.0151 | Grad Norm: 0.00963917\n",
      "Epoch 4 | Step 2396600 | Avg Loss: 0.0152 | Grad Norm: 0.01012255\n",
      "Epoch 4 | Step 2396700 | Avg Loss: 0.0150 | Grad Norm: 0.00830172\n",
      "Epoch 4 | Step 2396800 | Avg Loss: 0.0150 | Grad Norm: 0.00829109\n",
      "Epoch 4 | Step 2396900 | Avg Loss: 0.0151 | Grad Norm: 0.00956399\n",
      "Epoch 4 | Step 2397000 | Avg Loss: 0.0151 | Grad Norm: 0.00897139\n",
      "Epoch 4 | Step 2397100 | Avg Loss: 0.0155 | Grad Norm: 0.00898758\n",
      "Epoch 4 | Step 2397200 | Avg Loss: 0.0155 | Grad Norm: 0.00841331\n",
      "Epoch 4 | Step 2397300 | Avg Loss: 0.0155 | Grad Norm: 0.00865322\n",
      "Epoch 4 | Step 2397400 | Avg Loss: 0.0154 | Grad Norm: 0.00897088\n",
      "Epoch 4 | Step 2397500 | Avg Loss: 0.0156 | Grad Norm: 0.00790819\n",
      "Epoch 4 | Step 2397600 | Avg Loss: 0.0156 | Grad Norm: 0.00901384\n",
      "Epoch 4 | Step 2397700 | Avg Loss: 0.0159 | Grad Norm: 0.00948278\n",
      "Epoch 4 | Step 2397800 | Avg Loss: 0.0161 | Grad Norm: 0.00945203\n",
      "Epoch 4 | Step 2397900 | Avg Loss: 0.0159 | Grad Norm: 0.00906767\n",
      "Epoch 4 | Step 2398000 | Avg Loss: 0.0160 | Grad Norm: 0.01011463\n",
      "Epoch 4 | Step 2398100 | Avg Loss: 0.0162 | Grad Norm: 0.01041895\n",
      "Epoch 4 | Step 2398200 | Avg Loss: 0.0160 | Grad Norm: 0.00977211\n",
      "Epoch 4 | Step 2398300 | Avg Loss: 0.0162 | Grad Norm: 0.00862389\n",
      "Epoch 4 | Step 2398400 | Avg Loss: 0.0161 | Grad Norm: 0.00899968\n",
      "Epoch 4 | Step 2398500 | Avg Loss: 0.0164 | Grad Norm: 0.01229840\n",
      "Epoch 4 | Step 2398600 | Avg Loss: 0.0159 | Grad Norm: 0.01122762\n",
      "Epoch 4 | Step 2398700 | Avg Loss: 0.0160 | Grad Norm: 0.01016185\n",
      "Epoch 4 | Step 2398800 | Avg Loss: 0.0161 | Grad Norm: 0.00945434\n",
      "Epoch 4 | Step 2398900 | Avg Loss: 0.0161 | Grad Norm: 0.00896154\n",
      "Epoch 4 | Step 2399000 | Avg Loss: 0.0160 | Grad Norm: 0.00909043\n",
      "Epoch 4 | Step 2399100 | Avg Loss: 0.0157 | Grad Norm: 0.00911469\n",
      "Epoch 4 | Step 2399200 | Avg Loss: 0.0158 | Grad Norm: 0.00841377\n",
      "Epoch 4 | Step 2399300 | Avg Loss: 0.0158 | Grad Norm: 0.00773851\n",
      "Epoch 4 | Step 2399400 | Avg Loss: 0.0157 | Grad Norm: 0.00741858\n",
      "Epoch 4 | Step 2399500 | Avg Loss: 0.0157 | Grad Norm: 0.00820821\n",
      "Epoch 4 | Step 2399600 | Avg Loss: 0.0158 | Grad Norm: 0.00914934\n",
      "Epoch 4 | Step 2399700 | Avg Loss: 0.0157 | Grad Norm: 0.00986759\n",
      "Epoch 4 | Step 2399800 | Avg Loss: 0.0157 | Grad Norm: 0.00981588\n",
      "Epoch 4 | Step 2399900 | Avg Loss: 0.0163 | Grad Norm: 0.00917394\n",
      "Epoch 4 | Step 2400000 | Avg Loss: 0.0161 | Grad Norm: 0.01072383\n",
      "Saving model at step2400000\n",
      "Epoch 4 | Step 2400100 | Avg Loss: 0.0160 | Grad Norm: 0.00917712\n",
      "Epoch 4 | Step 2400200 | Avg Loss: 0.0161 | Grad Norm: 0.00911802\n",
      "Epoch 4 | Step 2400300 | Avg Loss: 0.0159 | Grad Norm: 0.01046863\n",
      "Epoch 4 | Step 2400400 | Avg Loss: 0.0162 | Grad Norm: 0.00960357\n",
      "Epoch 4 | Step 2400500 | Avg Loss: 0.0163 | Grad Norm: 0.00915033\n",
      "Epoch 4 | Step 2400600 | Avg Loss: 0.0168 | Grad Norm: 0.00867040\n",
      "Epoch 4 | Step 2400700 | Avg Loss: 0.0166 | Grad Norm: 0.00865750\n",
      "Epoch 4 | Step 2400800 | Avg Loss: 0.0158 | Grad Norm: 0.00893402\n",
      "Epoch 4 | Step 2400900 | Avg Loss: 0.0157 | Grad Norm: 0.00858138\n",
      "Epoch 4 | Step 2401000 | Avg Loss: 0.0157 | Grad Norm: 0.01012151\n",
      "Epoch 4 | Step 2401100 | Avg Loss: 0.0156 | Grad Norm: 0.00876393\n",
      "Epoch 4 | Step 2401200 | Avg Loss: 0.0153 | Grad Norm: 0.01041295\n",
      "Epoch 4 | Step 2401300 | Avg Loss: 0.0152 | Grad Norm: 0.00808138\n",
      "Epoch 4 | Step 2401400 | Avg Loss: 0.0155 | Grad Norm: 0.00850455\n",
      "Epoch 4 | Step 2401500 | Avg Loss: 0.0152 | Grad Norm: 0.00881694\n",
      "Epoch 4 | Step 2401600 | Avg Loss: 0.0153 | Grad Norm: 0.00864694\n",
      "Epoch 4 | Step 2401700 | Avg Loss: 0.0153 | Grad Norm: 0.00864596\n",
      "Epoch 4 | Step 2401800 | Avg Loss: 0.0153 | Grad Norm: 0.00867721\n",
      "Epoch 4 | Step 2401900 | Avg Loss: 0.0153 | Grad Norm: 0.00848001\n",
      "Epoch 4 | Step 2402000 | Avg Loss: 0.0155 | Grad Norm: 0.00799383\n",
      "Epoch 4 | Step 2402100 | Avg Loss: 0.0157 | Grad Norm: 0.00880807\n",
      "Epoch 4 | Step 2402200 | Avg Loss: 0.0158 | Grad Norm: 0.00971661\n",
      "Epoch 4 | Step 2402300 | Avg Loss: 0.0156 | Grad Norm: 0.00927053\n",
      "Epoch 4 | Step 2402400 | Avg Loss: 0.0154 | Grad Norm: 0.00812921\n",
      "Epoch 4 | Step 2402500 | Avg Loss: 0.0154 | Grad Norm: 0.00995736\n",
      "Epoch 4 | Step 2402600 | Avg Loss: 0.0156 | Grad Norm: 0.00832330\n",
      "Epoch 4 | Step 2402700 | Avg Loss: 0.0154 | Grad Norm: 0.00903430\n",
      "Epoch 4 | Step 2402800 | Avg Loss: 0.0154 | Grad Norm: 0.00867741\n",
      "Epoch 4 | Step 2402900 | Avg Loss: 0.0158 | Grad Norm: 0.00822245\n",
      "Epoch 4 | Step 2403000 | Avg Loss: 0.0156 | Grad Norm: 0.00866252\n",
      "Epoch 4 | Step 2403100 | Avg Loss: 0.0156 | Grad Norm: 0.00894809\n",
      "Epoch 4 | Step 2403200 | Avg Loss: 0.0157 | Grad Norm: 0.00817861\n",
      "Epoch 4 | Step 2403300 | Avg Loss: 0.0155 | Grad Norm: 0.00880385\n",
      "Epoch 4 | Step 2403400 | Avg Loss: 0.0153 | Grad Norm: 0.00823399\n",
      "Epoch 4 | Step 2403500 | Avg Loss: 0.0150 | Grad Norm: 0.00965944\n",
      "Epoch 4 | Step 2403600 | Avg Loss: 0.0153 | Grad Norm: 0.00871452\n",
      "Epoch 4 | Step 2403700 | Avg Loss: 0.0153 | Grad Norm: 0.00824980\n",
      "Epoch 4 | Step 2403800 | Avg Loss: 0.0155 | Grad Norm: 0.00944475\n",
      "Epoch 4 | Step 2403900 | Avg Loss: 0.0159 | Grad Norm: 0.00765446\n",
      "Epoch 4 | Step 2404000 | Avg Loss: 0.0155 | Grad Norm: 0.00886029\n",
      "Epoch 4 | Step 2404100 | Avg Loss: 0.0156 | Grad Norm: 0.01222171\n",
      "Epoch 4 | Step 2404200 | Avg Loss: 0.0155 | Grad Norm: 0.00792784\n",
      "Epoch 4 | Step 2404300 | Avg Loss: 0.0154 | Grad Norm: 0.00845417\n",
      "Epoch 4 | Step 2404400 | Avg Loss: 0.0153 | Grad Norm: 0.00949909\n",
      "Epoch 4 | Step 2404500 | Avg Loss: 0.0154 | Grad Norm: 0.01075702\n",
      "Epoch 4 | Step 2404600 | Avg Loss: 0.0155 | Grad Norm: 0.00971612\n",
      "Epoch 4 | Step 2404700 | Avg Loss: 0.0154 | Grad Norm: 0.00952772\n",
      "Epoch 4 | Step 2404800 | Avg Loss: 0.0154 | Grad Norm: 0.00850334\n",
      "Epoch 4 | Step 2404900 | Avg Loss: 0.0154 | Grad Norm: 0.00948008\n",
      "Epoch 4 | Step 2405000 | Avg Loss: 0.0155 | Grad Norm: 0.00774794\n",
      "Epoch 4 | Step 2405100 | Avg Loss: 0.0152 | Grad Norm: 0.00852066\n",
      "Epoch 4 | Step 2405200 | Avg Loss: 0.0155 | Grad Norm: 0.00900335\n",
      "Epoch 4 | Step 2405300 | Avg Loss: 0.0155 | Grad Norm: 0.00858779\n",
      "Epoch 4 | Step 2405400 | Avg Loss: 0.0154 | Grad Norm: 0.01153416\n",
      "Epoch 4 | Step 2405500 | Avg Loss: 0.0152 | Grad Norm: 0.00908004\n",
      "Epoch 4 | Step 2405600 | Avg Loss: 0.0155 | Grad Norm: 0.00879353\n",
      "Epoch 4 | Step 2405700 | Avg Loss: 0.0156 | Grad Norm: 0.00928264\n",
      "Epoch 4 | Step 2405800 | Avg Loss: 0.0162 | Grad Norm: 0.01227723\n",
      "Epoch 4 | Step 2405900 | Avg Loss: 0.0161 | Grad Norm: 0.01029282\n",
      "Epoch 4 | Step 2406000 | Avg Loss: 0.0157 | Grad Norm: 0.00898661\n",
      "Epoch 4 | Step 2406100 | Avg Loss: 0.0154 | Grad Norm: 0.00983073\n",
      "Epoch 4 | Step 2406200 | Avg Loss: 0.0148 | Grad Norm: 0.00855084\n",
      "Epoch 4 | Step 2406300 | Avg Loss: 0.0152 | Grad Norm: 0.00912550\n",
      "Epoch 4 | Step 2406400 | Avg Loss: 0.0154 | Grad Norm: 0.01113311\n",
      "Epoch 4 | Step 2406500 | Avg Loss: 0.0156 | Grad Norm: 0.00836111\n",
      "Epoch 4 | Step 2406600 | Avg Loss: 0.0157 | Grad Norm: 0.00865493\n",
      "Epoch 4 | Step 2406700 | Avg Loss: 0.0157 | Grad Norm: 0.00950820\n",
      "Epoch 4 | Step 2406800 | Avg Loss: 0.0160 | Grad Norm: 0.00876583\n",
      "Epoch 4 | Step 2406900 | Avg Loss: 0.0157 | Grad Norm: 0.00798318\n",
      "Epoch 4 | Step 2407000 | Avg Loss: 0.0157 | Grad Norm: 0.00863845\n",
      "Epoch 4 | Step 2407100 | Avg Loss: 0.0157 | Grad Norm: 0.00883076\n",
      "Epoch 4 | Step 2407200 | Avg Loss: 0.0155 | Grad Norm: 0.00908799\n",
      "Epoch 4 | Step 2407300 | Avg Loss: 0.0151 | Grad Norm: 0.00987284\n",
      "Epoch 4 | Step 2407400 | Avg Loss: 0.0147 | Grad Norm: 0.01000791\n",
      "Epoch 4 | Step 2407500 | Avg Loss: 0.0149 | Grad Norm: 0.00993257\n",
      "Epoch 4 | Step 2407600 | Avg Loss: 0.0150 | Grad Norm: 0.00767336\n",
      "Epoch 4 | Step 2407700 | Avg Loss: 0.0148 | Grad Norm: 0.00796373\n",
      "Epoch 4 | Step 2407800 | Avg Loss: 0.0149 | Grad Norm: 0.00806119\n",
      "Epoch 4 | Step 2407900 | Avg Loss: 0.0150 | Grad Norm: 0.00864856\n",
      "Epoch 4 | Step 2408000 | Avg Loss: 0.0150 | Grad Norm: 0.00841259\n",
      "Epoch 4 | Step 2408100 | Avg Loss: 0.0154 | Grad Norm: 0.00841701\n",
      "Epoch 4 | Step 2408200 | Avg Loss: 0.0158 | Grad Norm: 0.00944822\n",
      "Epoch 4 | Step 2408300 | Avg Loss: 0.0158 | Grad Norm: 0.01043886\n",
      "Epoch 4 | Step 2408400 | Avg Loss: 0.0160 | Grad Norm: 0.00918346\n",
      "Epoch 4 | Step 2408500 | Avg Loss: 0.0156 | Grad Norm: 0.00869814\n",
      "Epoch 4 | Step 2408600 | Avg Loss: 0.0155 | Grad Norm: 0.00981003\n",
      "Epoch 4 | Step 2408700 | Avg Loss: 0.0155 | Grad Norm: 0.00903337\n",
      "Epoch 4 | Step 2408800 | Avg Loss: 0.0156 | Grad Norm: 0.00821355\n",
      "Epoch 4 | Step 2408900 | Avg Loss: 0.0153 | Grad Norm: 0.00927714\n",
      "Epoch 4 | Step 2409000 | Avg Loss: 0.0152 | Grad Norm: 0.00850761\n",
      "Epoch 4 | Step 2409100 | Avg Loss: 0.0150 | Grad Norm: 0.01131306\n",
      "Epoch 4 | Step 2409200 | Avg Loss: 0.0147 | Grad Norm: 0.00861372\n",
      "Epoch 4 | Step 2409300 | Avg Loss: 0.0150 | Grad Norm: 0.01076218\n",
      "Epoch 4 | Step 2409400 | Avg Loss: 0.0149 | Grad Norm: 0.00874413\n",
      "Epoch 4 | Step 2409500 | Avg Loss: 0.0149 | Grad Norm: 0.00890005\n",
      "Epoch 4 | Step 2409600 | Avg Loss: 0.0150 | Grad Norm: 0.00801801\n",
      "Epoch 4 | Step 2409700 | Avg Loss: 0.0151 | Grad Norm: 0.00828805\n",
      "Epoch 4 | Step 2409800 | Avg Loss: 0.0153 | Grad Norm: 0.00881819\n",
      "Epoch 4 | Step 2409900 | Avg Loss: 0.0153 | Grad Norm: 0.00932034\n",
      "Epoch 4 | Step 2410000 | Avg Loss: 0.0151 | Grad Norm: 0.00853746\n",
      "Epoch 4 | Step 2410100 | Avg Loss: 0.0152 | Grad Norm: 0.00937635\n",
      "Epoch 4 | Step 2410200 | Avg Loss: 0.0157 | Grad Norm: 0.00795036\n",
      "Epoch 4 | Step 2410300 | Avg Loss: 0.0160 | Grad Norm: 0.00812794\n",
      "Epoch 4 | Step 2410400 | Avg Loss: 0.0160 | Grad Norm: 0.00924221\n",
      "Epoch 4 | Step 2410500 | Avg Loss: 0.0155 | Grad Norm: 0.00978009\n",
      "Epoch 4 | Step 2410600 | Avg Loss: 0.0147 | Grad Norm: 0.00867463\n",
      "Epoch 4 | Step 2410700 | Avg Loss: 0.0153 | Grad Norm: 0.00837373\n",
      "Epoch 4 | Step 2410800 | Avg Loss: 0.0154 | Grad Norm: 0.00867540\n",
      "Epoch 4 | Step 2410900 | Avg Loss: 0.0152 | Grad Norm: 0.00931319\n",
      "Epoch 4 | Step 2411000 | Avg Loss: 0.0155 | Grad Norm: 0.00863496\n",
      "Epoch 4 | Step 2411100 | Avg Loss: 0.0154 | Grad Norm: 0.00870444\n",
      "Epoch 4 | Step 2411200 | Avg Loss: 0.0154 | Grad Norm: 0.00944308\n",
      "Epoch 4 | Step 2411300 | Avg Loss: 0.0160 | Grad Norm: 0.00823955\n",
      "Epoch 4 | Step 2411400 | Avg Loss: 0.0161 | Grad Norm: 0.00898900\n",
      "Epoch 4 | Step 2411500 | Avg Loss: 0.0158 | Grad Norm: 0.00934099\n",
      "Epoch 4 | Step 2411600 | Avg Loss: 0.0157 | Grad Norm: 0.01160763\n",
      "Epoch 4 | Step 2411700 | Avg Loss: 0.0153 | Grad Norm: 0.00844794\n",
      "Epoch 4 | Step 2411800 | Avg Loss: 0.0152 | Grad Norm: 0.01291476\n",
      "Epoch 4 | Step 2411900 | Avg Loss: 0.0153 | Grad Norm: 0.00921928\n",
      "Epoch 4 | Step 2412000 | Avg Loss: 0.0153 | Grad Norm: 0.01197541\n",
      "Epoch 4 | Step 2412100 | Avg Loss: 0.0155 | Grad Norm: 0.00903392\n",
      "Epoch 4 | Step 2412200 | Avg Loss: 0.0156 | Grad Norm: 0.00882517\n",
      "Epoch 4 | Step 2412300 | Avg Loss: 0.0155 | Grad Norm: 0.00917656\n",
      "Epoch 4 | Step 2412400 | Avg Loss: 0.0156 | Grad Norm: 0.01134490\n",
      "Epoch 4 | Step 2412500 | Avg Loss: 0.0153 | Grad Norm: 0.00942008\n",
      "Epoch 4 | Step 2412600 | Avg Loss: 0.0156 | Grad Norm: 0.00951229\n",
      "Epoch 4 | Step 2412700 | Avg Loss: 0.0162 | Grad Norm: 0.00879042\n",
      "Epoch 4 | Step 2412800 | Avg Loss: 0.0161 | Grad Norm: 0.00965745\n",
      "Epoch 4 | Step 2412900 | Avg Loss: 0.0157 | Grad Norm: 0.00939295\n",
      "Epoch 4 | Step 2413000 | Avg Loss: 0.0155 | Grad Norm: 0.00945313\n",
      "Epoch 4 | Step 2413100 | Avg Loss: 0.0155 | Grad Norm: 0.01068539\n",
      "Epoch 4 | Step 2413200 | Avg Loss: 0.0154 | Grad Norm: 0.00974411\n",
      "Epoch 4 | Step 2413300 | Avg Loss: 0.0158 | Grad Norm: 0.00824123\n",
      "Epoch 4 | Step 2413400 | Avg Loss: 0.0155 | Grad Norm: 0.00798631\n",
      "Epoch 4 | Step 2413500 | Avg Loss: 0.0154 | Grad Norm: 0.00853051\n",
      "Epoch 4 | Step 2413600 | Avg Loss: 0.0156 | Grad Norm: 0.00980265\n",
      "Epoch 4 | Step 2413700 | Avg Loss: 0.0156 | Grad Norm: 0.00888983\n",
      "Epoch 4 | Step 2413800 | Avg Loss: 0.0157 | Grad Norm: 0.00996084\n",
      "Epoch 4 | Step 2413900 | Avg Loss: 0.0162 | Grad Norm: 0.00956854\n",
      "Epoch 4 | Step 2414000 | Avg Loss: 0.0154 | Grad Norm: 0.00816402\n",
      "Epoch 4 | Step 2414100 | Avg Loss: 0.0154 | Grad Norm: 0.00859809\n",
      "Epoch 4 | Step 2414200 | Avg Loss: 0.0156 | Grad Norm: 0.00955455\n",
      "Epoch 4 | Step 2414300 | Avg Loss: 0.0156 | Grad Norm: 0.01005192\n",
      "Epoch 4 | Step 2414400 | Avg Loss: 0.0155 | Grad Norm: 0.01114977\n",
      "Epoch 4 | Step 2414500 | Avg Loss: 0.0155 | Grad Norm: 0.01024055\n",
      "Epoch 4 | Step 2414600 | Avg Loss: 0.0149 | Grad Norm: 0.00825912\n",
      "Epoch 4 | Step 2414700 | Avg Loss: 0.0154 | Grad Norm: 0.00843690\n",
      "Epoch 4 | Step 2414800 | Avg Loss: 0.0156 | Grad Norm: 0.01136191\n",
      "Epoch 4 | Step 2414900 | Avg Loss: 0.0158 | Grad Norm: 0.00984867\n",
      "Epoch 4 | Step 2415000 | Avg Loss: 0.0158 | Grad Norm: 0.00858889\n",
      "Epoch 4 | Step 2415100 | Avg Loss: 0.0156 | Grad Norm: 0.00915183\n",
      "Epoch 4 | Step 2415200 | Avg Loss: 0.0154 | Grad Norm: 0.00951495\n",
      "Epoch 4 | Step 2415300 | Avg Loss: 0.0155 | Grad Norm: 0.00875656\n",
      "Epoch 4 | Step 2415400 | Avg Loss: 0.0157 | Grad Norm: 0.01204917\n",
      "Epoch 4 | Step 2415500 | Avg Loss: 0.0156 | Grad Norm: 0.00984631\n",
      "Epoch 4 | Step 2415600 | Avg Loss: 0.0154 | Grad Norm: 0.00930240\n",
      "Epoch 4 | Step 2415700 | Avg Loss: 0.0156 | Grad Norm: 0.00925728\n",
      "Epoch 4 | Step 2415800 | Avg Loss: 0.0158 | Grad Norm: 0.00863055\n",
      "Epoch 4 | Step 2415900 | Avg Loss: 0.0154 | Grad Norm: 0.01084137\n",
      "Epoch 4 | Step 2416000 | Avg Loss: 0.0155 | Grad Norm: 0.00962999\n",
      "Epoch 4 | Step 2416100 | Avg Loss: 0.0155 | Grad Norm: 0.01005166\n",
      "Epoch 4 | Step 2416200 | Avg Loss: 0.0156 | Grad Norm: 0.01115921\n",
      "Epoch 4 | Step 2416300 | Avg Loss: 0.0155 | Grad Norm: 0.00853671\n",
      "Epoch 4 | Step 2416400 | Avg Loss: 0.0156 | Grad Norm: 0.00865686\n",
      "Epoch 4 | Step 2416500 | Avg Loss: 0.0154 | Grad Norm: 0.00894781\n",
      "Epoch 4 | Step 2416600 | Avg Loss: 0.0154 | Grad Norm: 0.01012942\n",
      "Epoch 4 | Step 2416700 | Avg Loss: 0.0159 | Grad Norm: 0.00962338\n",
      "Epoch 4 | Step 2416800 | Avg Loss: 0.0160 | Grad Norm: 0.00902347\n",
      "Epoch 4 | Step 2416900 | Avg Loss: 0.0161 | Grad Norm: 0.00851975\n",
      "Epoch 4 | Step 2417000 | Avg Loss: 0.0160 | Grad Norm: 0.00926426\n",
      "Epoch 4 | Step 2417100 | Avg Loss: 0.0157 | Grad Norm: 0.00868266\n",
      "Epoch 4 | Step 2417200 | Avg Loss: 0.0154 | Grad Norm: 0.00907503\n",
      "Epoch 4 | Step 2417300 | Avg Loss: 0.0153 | Grad Norm: 0.00880124\n",
      "Epoch 4 | Step 2417400 | Avg Loss: 0.0154 | Grad Norm: 0.00747862\n",
      "Epoch 4 | Step 2417500 | Avg Loss: 0.0155 | Grad Norm: 0.00826246\n",
      "Epoch 4 | Step 2417600 | Avg Loss: 0.0153 | Grad Norm: 0.00876273\n",
      "Epoch 4 | Step 2417700 | Avg Loss: 0.0148 | Grad Norm: 0.01101866\n",
      "Epoch 4 | Step 2417800 | Avg Loss: 0.0153 | Grad Norm: 0.01135330\n",
      "Epoch 4 | Step 2417900 | Avg Loss: 0.0151 | Grad Norm: 0.00906709\n",
      "Epoch 4 | Step 2418000 | Avg Loss: 0.0151 | Grad Norm: 0.00976155\n",
      "Epoch 4 | Step 2418100 | Avg Loss: 0.0155 | Grad Norm: 0.00788769\n",
      "Epoch 4 | Step 2418200 | Avg Loss: 0.0156 | Grad Norm: 0.00843707\n",
      "Epoch 4 | Step 2418300 | Avg Loss: 0.0157 | Grad Norm: 0.00935347\n",
      "Epoch 4 | Step 2418400 | Avg Loss: 0.0157 | Grad Norm: 0.00864002\n",
      "Epoch 4 | Step 2418500 | Avg Loss: 0.0155 | Grad Norm: 0.00813575\n",
      "Epoch 4 | Step 2418600 | Avg Loss: 0.0156 | Grad Norm: 0.00767765\n",
      "Epoch 4 | Step 2418700 | Avg Loss: 0.0159 | Grad Norm: 0.00966072\n",
      "Epoch 4 | Step 2418800 | Avg Loss: 0.0160 | Grad Norm: 0.00833208\n",
      "Epoch 4 | Step 2418900 | Avg Loss: 0.0158 | Grad Norm: 0.00911377\n",
      "Epoch 4 | Step 2419000 | Avg Loss: 0.0156 | Grad Norm: 0.00989892\n",
      "Epoch 4 | Step 2419100 | Avg Loss: 0.0155 | Grad Norm: 0.00998142\n",
      "Epoch 4 | Step 2419200 | Avg Loss: 0.0154 | Grad Norm: 0.00945799\n",
      "Epoch 4 | Step 2419300 | Avg Loss: 0.0152 | Grad Norm: 0.00851951\n",
      "Epoch 4 | Step 2419400 | Avg Loss: 0.0150 | Grad Norm: 0.00827618\n",
      "Epoch 4 | Step 2419500 | Avg Loss: 0.0154 | Grad Norm: 0.00952264\n",
      "Epoch 4 | Step 2419600 | Avg Loss: 0.0156 | Grad Norm: 0.00798830\n",
      "Epoch 4 | Step 2419700 | Avg Loss: 0.0158 | Grad Norm: 0.00783802\n",
      "Epoch 4 | Step 2419800 | Avg Loss: 0.0157 | Grad Norm: 0.00980664\n",
      "Epoch 4 | Step 2419900 | Avg Loss: 0.0156 | Grad Norm: 0.00971494\n",
      "Epoch 4 | Step 2420000 | Avg Loss: 0.0160 | Grad Norm: 0.00994277\n",
      "Epoch 4 | Step 2420100 | Avg Loss: 0.0157 | Grad Norm: 0.00859551\n",
      "Epoch 4 | Step 2420200 | Avg Loss: 0.0157 | Grad Norm: 0.00969796\n",
      "Epoch 4 | Step 2420300 | Avg Loss: 0.0154 | Grad Norm: 0.00973499\n",
      "Epoch 4 | Step 2420400 | Avg Loss: 0.0153 | Grad Norm: 0.00930549\n",
      "Epoch 4 | Step 2420500 | Avg Loss: 0.0155 | Grad Norm: 0.00913289\n",
      "Epoch 4 | Step 2420600 | Avg Loss: 0.0155 | Grad Norm: 0.01031610\n",
      "Epoch 4 | Step 2420700 | Avg Loss: 0.0156 | Grad Norm: 0.00891756\n",
      "Epoch 4 | Step 2420800 | Avg Loss: 0.0157 | Grad Norm: 0.01069287\n",
      "Epoch 4 | Step 2420900 | Avg Loss: 0.0157 | Grad Norm: 0.00856978\n",
      "Epoch 4 | Step 2421000 | Avg Loss: 0.0157 | Grad Norm: 0.00819834\n",
      "Epoch 4 | Step 2421100 | Avg Loss: 0.0157 | Grad Norm: 0.00842856\n",
      "Epoch 4 | Step 2421200 | Avg Loss: 0.0159 | Grad Norm: 0.00917806\n",
      "Epoch 4 | Step 2421300 | Avg Loss: 0.0160 | Grad Norm: 0.00998046\n",
      "Epoch 4 | Step 2421400 | Avg Loss: 0.0163 | Grad Norm: 0.00962905\n",
      "Epoch 4 | Step 2421500 | Avg Loss: 0.0163 | Grad Norm: 0.01048656\n",
      "Epoch 4 | Step 2421600 | Avg Loss: 0.0163 | Grad Norm: 0.00892754\n",
      "Epoch 4 | Step 2421700 | Avg Loss: 0.0160 | Grad Norm: 0.01008581\n",
      "Epoch 4 | Step 2421800 | Avg Loss: 0.0159 | Grad Norm: 0.00931924\n",
      "Epoch 4 | Step 2421900 | Avg Loss: 0.0160 | Grad Norm: 0.00922329\n",
      "Epoch 4 | Step 2422000 | Avg Loss: 0.0159 | Grad Norm: 0.01011116\n",
      "Epoch 4 | Step 2422100 | Avg Loss: 0.0164 | Grad Norm: 0.01104570\n",
      "Epoch 4 | Step 2422200 | Avg Loss: 0.0161 | Grad Norm: 0.00870560\n",
      "Epoch 4 | Step 2422300 | Avg Loss: 0.0161 | Grad Norm: 0.01018861\n",
      "Epoch 4 | Step 2422400 | Avg Loss: 0.0159 | Grad Norm: 0.01025361\n",
      "Epoch 4 | Step 2422500 | Avg Loss: 0.0161 | Grad Norm: 0.01025780\n",
      "Epoch 4 | Step 2422600 | Avg Loss: 0.0164 | Grad Norm: 0.00887246\n",
      "Epoch 4 | Step 2422700 | Avg Loss: 0.0162 | Grad Norm: 0.00896203\n",
      "Epoch 4 | Step 2422800 | Avg Loss: 0.0159 | Grad Norm: 0.01014344\n",
      "Epoch 4 | Step 2422900 | Avg Loss: 0.0157 | Grad Norm: 0.00940924\n",
      "Epoch 4 | Step 2423000 | Avg Loss: 0.0154 | Grad Norm: 0.00891018\n",
      "Epoch 4 | Step 2423100 | Avg Loss: 0.0155 | Grad Norm: 0.00896413\n",
      "Epoch 4 | Step 2423200 | Avg Loss: 0.0156 | Grad Norm: 0.00969855\n",
      "Epoch 4 | Step 2423300 | Avg Loss: 0.0157 | Grad Norm: 0.00903913\n",
      "Epoch 4 | Step 2423400 | Avg Loss: 0.0159 | Grad Norm: 0.00857044\n",
      "Epoch 4 | Step 2423500 | Avg Loss: 0.0163 | Grad Norm: 0.00968678\n",
      "Epoch 4 | Step 2423600 | Avg Loss: 0.0161 | Grad Norm: 0.00931561\n",
      "Epoch 4 | Step 2423700 | Avg Loss: 0.0162 | Grad Norm: 0.00985844\n",
      "Epoch 4 | Step 2423800 | Avg Loss: 0.0160 | Grad Norm: 0.00823857\n",
      "Epoch 4 | Step 2423900 | Avg Loss: 0.0158 | Grad Norm: 0.00851382\n",
      "Epoch 4 | Step 2424000 | Avg Loss: 0.0158 | Grad Norm: 0.01025754\n",
      "Epoch 4 | Step 2424100 | Avg Loss: 0.0157 | Grad Norm: 0.00995294\n",
      "Epoch 4 | Step 2424200 | Avg Loss: 0.0158 | Grad Norm: 0.00984627\n",
      "Epoch 4 | Step 2424300 | Avg Loss: 0.0156 | Grad Norm: 0.01179920\n",
      "Epoch 4 | Step 2424400 | Avg Loss: 0.0159 | Grad Norm: 0.00863005\n",
      "Epoch 4 | Step 2424500 | Avg Loss: 0.0158 | Grad Norm: 0.00772802\n",
      "Epoch 4 | Step 2424600 | Avg Loss: 0.0152 | Grad Norm: 0.01050008\n",
      "Epoch 4 | Step 2424700 | Avg Loss: 0.0153 | Grad Norm: 0.00842642\n",
      "Epoch 4 | Step 2424800 | Avg Loss: 0.0153 | Grad Norm: 0.00939602\n",
      "Epoch 4 | Step 2424900 | Avg Loss: 0.0152 | Grad Norm: 0.00946818\n",
      "Epoch 4 | Step 2425000 | Avg Loss: 0.0147 | Grad Norm: 0.00823130\n",
      "Epoch 4 | Step 2425100 | Avg Loss: 0.0150 | Grad Norm: 0.00918880\n",
      "Epoch 4 | Step 2425200 | Avg Loss: 0.0153 | Grad Norm: 0.01013397\n",
      "Epoch 4 | Step 2425300 | Avg Loss: 0.0153 | Grad Norm: 0.00935271\n",
      "Epoch 4 | Step 2425400 | Avg Loss: 0.0153 | Grad Norm: 0.00821198\n",
      "Epoch 4 | Step 2425500 | Avg Loss: 0.0155 | Grad Norm: 0.00792899\n",
      "Epoch 4 | Step 2425600 | Avg Loss: 0.0156 | Grad Norm: 0.00950496\n",
      "Epoch 4 | Step 2425700 | Avg Loss: 0.0155 | Grad Norm: 0.01156700\n",
      "Epoch 4 | Step 2425800 | Avg Loss: 0.0156 | Grad Norm: 0.00992370\n",
      "Epoch 4 | Step 2425900 | Avg Loss: 0.0153 | Grad Norm: 0.00791728\n",
      "Epoch 4 | Step 2426000 | Avg Loss: 0.0151 | Grad Norm: 0.01026384\n",
      "Epoch 4 | Step 2426100 | Avg Loss: 0.0152 | Grad Norm: 0.00860480\n",
      "Epoch 4 | Step 2426200 | Avg Loss: 0.0155 | Grad Norm: 0.00943031\n",
      "Epoch 4 | Step 2426300 | Avg Loss: 0.0153 | Grad Norm: 0.00921210\n",
      "Epoch 4 | Step 2426400 | Avg Loss: 0.0151 | Grad Norm: 0.00923355\n",
      "Epoch 4 | Step 2426500 | Avg Loss: 0.0153 | Grad Norm: 0.00923151\n",
      "Epoch 4 | Step 2426600 | Avg Loss: 0.0155 | Grad Norm: 0.00878456\n",
      "Epoch 4 | Step 2426700 | Avg Loss: 0.0155 | Grad Norm: 0.01033180\n",
      "Epoch 4 | Step 2426800 | Avg Loss: 0.0156 | Grad Norm: 0.01031732\n",
      "Epoch 4 | Step 2426900 | Avg Loss: 0.0155 | Grad Norm: 0.00897839\n",
      "Epoch 4 | Step 2427000 | Avg Loss: 0.0153 | Grad Norm: 0.00891389\n",
      "Epoch 4 | Step 2427100 | Avg Loss: 0.0155 | Grad Norm: 0.01038783\n",
      "Epoch 4 | Step 2427200 | Avg Loss: 0.0157 | Grad Norm: 0.00784160\n",
      "Epoch 4 | Step 2427300 | Avg Loss: 0.0160 | Grad Norm: 0.00898804\n",
      "Epoch 4 | Step 2427400 | Avg Loss: 0.0154 | Grad Norm: 0.00917553\n",
      "Epoch 4 | Step 2427500 | Avg Loss: 0.0159 | Grad Norm: 0.00880641\n",
      "Epoch 4 | Step 2427600 | Avg Loss: 0.0161 | Grad Norm: 0.01042269\n",
      "Epoch 4 | Step 2427700 | Avg Loss: 0.0159 | Grad Norm: 0.00882373\n",
      "Epoch 4 | Step 2427800 | Avg Loss: 0.0157 | Grad Norm: 0.01095189\n",
      "Epoch 4 | Step 2427900 | Avg Loss: 0.0152 | Grad Norm: 0.00802640\n",
      "Epoch 4 | Step 2428000 | Avg Loss: 0.0156 | Grad Norm: 0.00890243\n",
      "Epoch 4 | Step 2428100 | Avg Loss: 0.0159 | Grad Norm: 0.01855553\n",
      "Epoch 4 | Step 2428200 | Avg Loss: 0.0159 | Grad Norm: 0.01055291\n",
      "Epoch 4 | Step 2428300 | Avg Loss: 0.0160 | Grad Norm: 0.00845481\n",
      "Epoch 4 | Step 2428400 | Avg Loss: 0.0156 | Grad Norm: 0.00889901\n",
      "Epoch 4 | Step 2428500 | Avg Loss: 0.0156 | Grad Norm: 0.00897984\n",
      "Epoch 4 | Step 2428600 | Avg Loss: 0.0158 | Grad Norm: 0.00877131\n",
      "Epoch 4 | Step 2428700 | Avg Loss: 0.0155 | Grad Norm: 0.00859919\n",
      "Epoch 4 | Step 2428800 | Avg Loss: 0.0154 | Grad Norm: 0.01032803\n",
      "Epoch 4 | Step 2428900 | Avg Loss: 0.0152 | Grad Norm: 0.00934068\n",
      "Epoch 4 | Step 2429000 | Avg Loss: 0.0155 | Grad Norm: 0.00918851\n",
      "Epoch 4 | Step 2429100 | Avg Loss: 0.0152 | Grad Norm: 0.00873961\n",
      "Epoch 4 | Step 2429200 | Avg Loss: 0.0151 | Grad Norm: 0.01074762\n",
      "Epoch 4 | Step 2429300 | Avg Loss: 0.0149 | Grad Norm: 0.00795202\n",
      "Epoch 4 | Step 2429400 | Avg Loss: 0.0148 | Grad Norm: 0.00966618\n",
      "Epoch 4 | Step 2429500 | Avg Loss: 0.0148 | Grad Norm: 0.00904624\n",
      "Epoch 4 | Step 2429600 | Avg Loss: 0.0152 | Grad Norm: 0.00884058\n",
      "Epoch 4 | Step 2429700 | Avg Loss: 0.0154 | Grad Norm: 0.00978119\n",
      "Epoch 4 | Step 2429800 | Avg Loss: 0.0155 | Grad Norm: 0.00908775\n",
      "Epoch 4 | Step 2429900 | Avg Loss: 0.0157 | Grad Norm: 0.00817024\n",
      "Epoch 4 | Step 2430000 | Avg Loss: 0.0156 | Grad Norm: 0.00771108\n",
      "Epoch 4 | Step 2430100 | Avg Loss: 0.0156 | Grad Norm: 0.00707835\n",
      "Epoch 4 | Step 2430200 | Avg Loss: 0.0156 | Grad Norm: 0.00913809\n",
      "Epoch 4 | Step 2430300 | Avg Loss: 0.0155 | Grad Norm: 0.01013858\n",
      "Epoch 4 | Step 2430400 | Avg Loss: 0.0154 | Grad Norm: 0.00897245\n",
      "Epoch 4 | Step 2430500 | Avg Loss: 0.0156 | Grad Norm: 0.01011579\n",
      "Epoch 4 | Step 2430600 | Avg Loss: 0.0156 | Grad Norm: 0.00793050\n",
      "Epoch 4 | Step 2430700 | Avg Loss: 0.0156 | Grad Norm: 0.00885289\n",
      "Epoch 4 | Step 2430800 | Avg Loss: 0.0158 | Grad Norm: 0.01143673\n",
      "Epoch 4 | Step 2430900 | Avg Loss: 0.0159 | Grad Norm: 0.00843388\n",
      "Epoch 4 | Step 2431000 | Avg Loss: 0.0160 | Grad Norm: 0.00966125\n",
      "Epoch 4 | Step 2431100 | Avg Loss: 0.0161 | Grad Norm: 0.00840304\n",
      "Epoch 4 | Step 2431200 | Avg Loss: 0.0159 | Grad Norm: 0.01009386\n",
      "Epoch 4 | Step 2431300 | Avg Loss: 0.0161 | Grad Norm: 0.00955288\n",
      "Epoch 4 | Step 2431400 | Avg Loss: 0.0162 | Grad Norm: 0.01066949\n",
      "Epoch 4 | Step 2431500 | Avg Loss: 0.0161 | Grad Norm: 0.00841806\n",
      "Epoch 4 | Step 2431600 | Avg Loss: 0.0159 | Grad Norm: 0.00907799\n",
      "Epoch 4 | Step 2431700 | Avg Loss: 0.0159 | Grad Norm: 0.00957942\n",
      "Epoch 4 | Step 2431800 | Avg Loss: 0.0157 | Grad Norm: 0.01117894\n",
      "Epoch 4 | Step 2431900 | Avg Loss: 0.0156 | Grad Norm: 0.00972975\n",
      "Epoch 4 | Step 2432000 | Avg Loss: 0.0158 | Grad Norm: 0.00892065\n",
      "Epoch 4 | Step 2432100 | Avg Loss: 0.0156 | Grad Norm: 0.00896488\n",
      "Epoch 4 | Step 2432200 | Avg Loss: 0.0156 | Grad Norm: 0.00881885\n",
      "Epoch 4 | Step 2432300 | Avg Loss: 0.0152 | Grad Norm: 0.01186330\n",
      "Epoch 4 | Step 2432400 | Avg Loss: 0.0152 | Grad Norm: 0.00942305\n",
      "Epoch 4 | Step 2432500 | Avg Loss: 0.0153 | Grad Norm: 0.00813576\n",
      "Epoch 4 | Step 2432600 | Avg Loss: 0.0153 | Grad Norm: 0.00869877\n",
      "Epoch 4 | Step 2432700 | Avg Loss: 0.0156 | Grad Norm: 0.00966776\n",
      "Epoch 4 | Step 2432800 | Avg Loss: 0.0154 | Grad Norm: 0.00907328\n",
      "Epoch 4 | Step 2432900 | Avg Loss: 0.0156 | Grad Norm: 0.00914872\n",
      "Epoch 4 | Step 2433000 | Avg Loss: 0.0160 | Grad Norm: 0.00988537\n",
      "Epoch 4 | Step 2433100 | Avg Loss: 0.0158 | Grad Norm: 0.00871858\n",
      "Epoch 4 | Step 2433200 | Avg Loss: 0.0160 | Grad Norm: 0.00838578\n",
      "Epoch 4 | Step 2433300 | Avg Loss: 0.0160 | Grad Norm: 0.00970962\n",
      "Epoch 4 | Step 2433400 | Avg Loss: 0.0154 | Grad Norm: 0.00841154\n",
      "Epoch 4 | Step 2433500 | Avg Loss: 0.0149 | Grad Norm: 0.00820847\n",
      "Epoch 4 | Step 2433600 | Avg Loss: 0.0149 | Grad Norm: 0.00880642\n",
      "Epoch 4 | Step 2433700 | Avg Loss: 0.0156 | Grad Norm: 0.00918493\n",
      "Epoch 4 | Step 2433800 | Avg Loss: 0.0156 | Grad Norm: 0.00801607\n",
      "Epoch 4 | Step 2433900 | Avg Loss: 0.0157 | Grad Norm: 0.01151823\n",
      "Epoch 4 | Step 2434000 | Avg Loss: 0.0157 | Grad Norm: 0.01071930\n",
      "Epoch 4 | Step 2434100 | Avg Loss: 0.0159 | Grad Norm: 0.00798617\n",
      "Epoch 4 | Step 2434200 | Avg Loss: 0.0160 | Grad Norm: 0.01030012\n",
      "Epoch 4 | Step 2434300 | Avg Loss: 0.0158 | Grad Norm: 0.00822245\n",
      "Epoch 4 | Step 2434400 | Avg Loss: 0.0157 | Grad Norm: 0.00899249\n",
      "Epoch 4 | Step 2434500 | Avg Loss: 0.0157 | Grad Norm: 0.00803919\n",
      "Epoch 4 | Step 2434600 | Avg Loss: 0.0151 | Grad Norm: 0.00895446\n",
      "Epoch 4 | Step 2434700 | Avg Loss: 0.0150 | Grad Norm: 0.00972345\n",
      "Epoch 4 | Step 2434800 | Avg Loss: 0.0150 | Grad Norm: 0.00872996\n",
      "Epoch 4 | Step 2434900 | Avg Loss: 0.0153 | Grad Norm: 0.00792913\n",
      "Epoch 4 | Step 2435000 | Avg Loss: 0.0157 | Grad Norm: 0.00827170\n",
      "Epoch 4 | Step 2435100 | Avg Loss: 0.0159 | Grad Norm: 0.00868072\n",
      "Epoch 4 | Step 2435200 | Avg Loss: 0.0158 | Grad Norm: 0.00907761\n",
      "Epoch 4 | Step 2435300 | Avg Loss: 0.0152 | Grad Norm: 0.01045820\n",
      "Epoch 4 | Step 2435400 | Avg Loss: 0.0153 | Grad Norm: 0.00841502\n",
      "Epoch 4 | Step 2435500 | Avg Loss: 0.0156 | Grad Norm: 0.00952505\n",
      "Epoch 4 | Step 2435600 | Avg Loss: 0.0153 | Grad Norm: 0.00941283\n",
      "Epoch 4 | Step 2435700 | Avg Loss: 0.0156 | Grad Norm: 0.01076589\n",
      "Epoch 4 | Step 2435800 | Avg Loss: 0.0158 | Grad Norm: 0.00882213\n",
      "Epoch 4 | Step 2435900 | Avg Loss: 0.0154 | Grad Norm: 0.01463417\n",
      "Epoch 4 | Step 2436000 | Avg Loss: 0.0158 | Grad Norm: 0.00836886\n",
      "Epoch 4 | Step 2436100 | Avg Loss: 0.0157 | Grad Norm: 0.00792525\n",
      "Epoch 4 | Step 2436200 | Avg Loss: 0.0155 | Grad Norm: 0.00860776\n",
      "Epoch 4 | Step 2436300 | Avg Loss: 0.0154 | Grad Norm: 0.00930557\n",
      "Epoch 4 | Step 2436400 | Avg Loss: 0.0154 | Grad Norm: 0.00839617\n",
      "Epoch 4 | Step 2436500 | Avg Loss: 0.0155 | Grad Norm: 0.00848566\n",
      "Epoch 4 | Step 2436600 | Avg Loss: 0.0153 | Grad Norm: 0.00910330\n",
      "Epoch 4 | Step 2436700 | Avg Loss: 0.0152 | Grad Norm: 0.00952349\n",
      "Epoch 4 | Step 2436800 | Avg Loss: 0.0154 | Grad Norm: 0.01080063\n",
      "Epoch 4 | Step 2436900 | Avg Loss: 0.0154 | Grad Norm: 0.00942892\n",
      "Epoch 4 | Step 2437000 | Avg Loss: 0.0149 | Grad Norm: 0.00960676\n",
      "Epoch 4 | Step 2437100 | Avg Loss: 0.0149 | Grad Norm: 0.00948972\n",
      "Epoch 4 | Step 2437200 | Avg Loss: 0.0151 | Grad Norm: 0.00861389\n",
      "Epoch 4 | Step 2437300 | Avg Loss: 0.0153 | Grad Norm: 0.00934854\n",
      "Epoch 4 | Step 2437400 | Avg Loss: 0.0152 | Grad Norm: 0.00941347\n",
      "Epoch 4 | Step 2437500 | Avg Loss: 0.0153 | Grad Norm: 0.00959736\n",
      "Epoch 4 | Step 2437600 | Avg Loss: 0.0156 | Grad Norm: 0.00949314\n",
      "Epoch 4 | Step 2437700 | Avg Loss: 0.0159 | Grad Norm: 0.00930145\n",
      "Epoch 4 | Step 2437800 | Avg Loss: 0.0156 | Grad Norm: 0.00879593\n",
      "Epoch 4 | Step 2437900 | Avg Loss: 0.0157 | Grad Norm: 0.01014445\n",
      "Epoch 4 | Step 2438000 | Avg Loss: 0.0157 | Grad Norm: 0.00938365\n",
      "Epoch 4 | Step 2438100 | Avg Loss: 0.0155 | Grad Norm: 0.00953991\n",
      "Epoch 4 | Step 2438200 | Avg Loss: 0.0154 | Grad Norm: 0.00869977\n",
      "Epoch 4 | Step 2438300 | Avg Loss: 0.0153 | Grad Norm: 0.01123940\n",
      "Epoch 4 | Step 2438400 | Avg Loss: 0.0151 | Grad Norm: 0.00840798\n",
      "Epoch 4 | Step 2438500 | Avg Loss: 0.0150 | Grad Norm: 0.00802350\n",
      "Epoch 4 | Step 2438600 | Avg Loss: 0.0151 | Grad Norm: 0.00919940\n",
      "Epoch 4 | Step 2438700 | Avg Loss: 0.0152 | Grad Norm: 0.01930313\n",
      "Epoch 4 | Step 2438800 | Avg Loss: 0.0154 | Grad Norm: 0.00994727\n",
      "Epoch 4 | Step 2438900 | Avg Loss: 0.0157 | Grad Norm: 0.01225907\n",
      "Epoch 4 | Step 2439000 | Avg Loss: 0.0155 | Grad Norm: 0.00918854\n",
      "Epoch 4 | Step 2439100 | Avg Loss: 0.0156 | Grad Norm: 0.00866654\n",
      "Epoch 4 | Step 2439200 | Avg Loss: 0.0153 | Grad Norm: 0.00824578\n",
      "Epoch 4 | Step 2439300 | Avg Loss: 0.0153 | Grad Norm: 0.01133308\n",
      "Epoch 4 | Step 2439400 | Avg Loss: 0.0153 | Grad Norm: 0.01382013\n",
      "Epoch 4 | Step 2439500 | Avg Loss: 0.0150 | Grad Norm: 0.00800121\n",
      "Epoch 4 | Step 2439600 | Avg Loss: 0.0152 | Grad Norm: 0.00801545\n",
      "Epoch 4 | Step 2439700 | Avg Loss: 0.0150 | Grad Norm: 0.00635167\n",
      "Epoch 4 | Step 2439800 | Avg Loss: 0.0147 | Grad Norm: 0.00825126\n",
      "Epoch 4 | Step 2439900 | Avg Loss: 0.0146 | Grad Norm: 0.00935027\n",
      "Epoch 4 | Step 2440000 | Avg Loss: 0.0145 | Grad Norm: 0.00867112\n",
      "Epoch 4 | Step 2440100 | Avg Loss: 0.0147 | Grad Norm: 0.00864675\n",
      "Epoch 4 | Step 2440200 | Avg Loss: 0.0152 | Grad Norm: 0.00884021\n",
      "Epoch 4 | Step 2440300 | Avg Loss: 0.0155 | Grad Norm: 0.00972299\n",
      "Epoch 4 | Step 2440400 | Avg Loss: 0.0156 | Grad Norm: 0.00897883\n",
      "Epoch 4 | Step 2440500 | Avg Loss: 0.0156 | Grad Norm: 0.00796424\n",
      "Epoch 4 | Step 2440600 | Avg Loss: 0.0156 | Grad Norm: 0.00817349\n",
      "Epoch 4 | Step 2440700 | Avg Loss: 0.0157 | Grad Norm: 0.00911731\n",
      "Epoch 4 | Step 2440800 | Avg Loss: 0.0157 | Grad Norm: 0.00891510\n",
      "Epoch 4 | Step 2440900 | Avg Loss: 0.0156 | Grad Norm: 0.01075480\n",
      "Epoch 4 | Step 2441000 | Avg Loss: 0.0160 | Grad Norm: 0.00970978\n",
      "Epoch 4 | Step 2441100 | Avg Loss: 0.0159 | Grad Norm: 0.01009618\n",
      "Epoch 4 | Step 2441200 | Avg Loss: 0.0159 | Grad Norm: 0.00968982\n",
      "Epoch 4 | Step 2441300 | Avg Loss: 0.0159 | Grad Norm: 0.01126956\n",
      "Epoch 4 | Step 2441400 | Avg Loss: 0.0158 | Grad Norm: 0.00983265\n",
      "Epoch 4 | Step 2441500 | Avg Loss: 0.0160 | Grad Norm: 0.00986155\n",
      "Epoch 4 | Step 2441600 | Avg Loss: 0.0164 | Grad Norm: 0.00954243\n",
      "Epoch 4 | Step 2441700 | Avg Loss: 0.0165 | Grad Norm: 0.01214945\n",
      "Epoch 4 | Step 2441800 | Avg Loss: 0.0159 | Grad Norm: 0.01021196\n",
      "Epoch 4 | Step 2441900 | Avg Loss: 0.0159 | Grad Norm: 0.00938121\n",
      "Epoch 4 | Step 2442000 | Avg Loss: 0.0158 | Grad Norm: 0.01269783\n",
      "Epoch 4 | Step 2442100 | Avg Loss: 0.0158 | Grad Norm: 0.00783765\n",
      "Epoch 4 | Step 2442200 | Avg Loss: 0.0159 | Grad Norm: 0.00853556\n",
      "Epoch 4 | Step 2442300 | Avg Loss: 0.0154 | Grad Norm: 0.00955635\n",
      "Epoch 4 | Step 2442400 | Avg Loss: 0.0156 | Grad Norm: 0.02008106\n",
      "Epoch 4 | Step 2442500 | Avg Loss: 0.0156 | Grad Norm: 0.00829213\n",
      "Epoch 4 | Step 2442600 | Avg Loss: 0.0158 | Grad Norm: 0.01206787\n",
      "Epoch 4 | Step 2442700 | Avg Loss: 0.0155 | Grad Norm: 0.01003678\n",
      "Epoch 4 | Step 2442800 | Avg Loss: 0.0157 | Grad Norm: 0.00929967\n",
      "Epoch 4 | Step 2442900 | Avg Loss: 0.0154 | Grad Norm: 0.00795468\n",
      "Epoch 4 | Step 2443000 | Avg Loss: 0.0155 | Grad Norm: 0.00865452\n",
      "Epoch 4 | Step 2443100 | Avg Loss: 0.0153 | Grad Norm: 0.00882557\n",
      "Epoch 4 | Step 2443200 | Avg Loss: 0.0147 | Grad Norm: 0.00791910\n",
      "Epoch 4 | Step 2443300 | Avg Loss: 0.0150 | Grad Norm: 0.00732245\n",
      "Epoch 4 | Step 2443400 | Avg Loss: 0.0150 | Grad Norm: 0.00784320\n",
      "Epoch 4 | Step 2443500 | Avg Loss: 0.0147 | Grad Norm: 0.00877376\n",
      "Epoch 4 | Step 2443600 | Avg Loss: 0.0150 | Grad Norm: 0.00932696\n",
      "Epoch 4 | Step 2443700 | Avg Loss: 0.0152 | Grad Norm: 0.00854054\n",
      "Epoch 4 | Step 2443800 | Avg Loss: 0.0153 | Grad Norm: 0.00936250\n",
      "Epoch 4 | Step 2443900 | Avg Loss: 0.0151 | Grad Norm: 0.00771243\n",
      "Epoch 4 | Step 2444000 | Avg Loss: 0.0152 | Grad Norm: 0.00845605\n",
      "Epoch 4 | Step 2444100 | Avg Loss: 0.0152 | Grad Norm: 0.00898897\n",
      "Epoch 4 | Step 2444200 | Avg Loss: 0.0156 | Grad Norm: 0.00932049\n",
      "Epoch 4 | Step 2444300 | Avg Loss: 0.0157 | Grad Norm: 0.01627585\n",
      "Epoch 4 | Step 2444400 | Avg Loss: 0.0163 | Grad Norm: 0.00806254\n",
      "Epoch 4 | Step 2444500 | Avg Loss: 0.0164 | Grad Norm: 0.00959822\n",
      "Epoch 4 | Step 2444600 | Avg Loss: 0.0160 | Grad Norm: 0.00972726\n",
      "Epoch 4 | Step 2444700 | Avg Loss: 0.0161 | Grad Norm: 0.01012567\n",
      "Epoch 4 | Step 2444800 | Avg Loss: 0.0162 | Grad Norm: 0.00888357\n",
      "Epoch 4 | Step 2444900 | Avg Loss: 0.0158 | Grad Norm: 0.00892980\n",
      "Epoch 4 | Step 2445000 | Avg Loss: 0.0157 | Grad Norm: 0.00901552\n",
      "Epoch 4 | Step 2445100 | Avg Loss: 0.0158 | Grad Norm: 0.00875061\n",
      "Epoch 4 | Step 2445200 | Avg Loss: 0.0153 | Grad Norm: 0.00923710\n",
      "Epoch 4 | Step 2445300 | Avg Loss: 0.0149 | Grad Norm: 0.00899554\n",
      "Epoch 4 | Step 2445400 | Avg Loss: 0.0149 | Grad Norm: 0.00832527\n",
      "Epoch 4 | Step 2445500 | Avg Loss: 0.0149 | Grad Norm: 0.00744170\n",
      "Epoch 4 | Step 2445600 | Avg Loss: 0.0147 | Grad Norm: 0.00939976\n",
      "Epoch 4 | Step 2445700 | Avg Loss: 0.0150 | Grad Norm: 0.00856059\n",
      "Epoch 4 | Step 2445800 | Avg Loss: 0.0151 | Grad Norm: 0.00941352\n",
      "Epoch 4 | Step 2445900 | Avg Loss: 0.0151 | Grad Norm: 0.00882698\n",
      "Epoch 4 | Step 2446000 | Avg Loss: 0.0149 | Grad Norm: 0.00816663\n",
      "Epoch 4 | Step 2446100 | Avg Loss: 0.0149 | Grad Norm: 0.00887275\n",
      "Epoch 4 | Step 2446200 | Avg Loss: 0.0152 | Grad Norm: 0.00848919\n",
      "Epoch 4 | Step 2446300 | Avg Loss: 0.0155 | Grad Norm: 0.00949810\n",
      "Epoch 4 | Step 2446400 | Avg Loss: 0.0159 | Grad Norm: 0.00832426\n",
      "Epoch 4 | Step 2446500 | Avg Loss: 0.0155 | Grad Norm: 0.00974667\n",
      "Epoch 4 | Step 2446600 | Avg Loss: 0.0155 | Grad Norm: 0.00845380\n",
      "Epoch 4 | Step 2446700 | Avg Loss: 0.0157 | Grad Norm: 0.00808633\n",
      "Epoch 4 | Step 2446800 | Avg Loss: 0.0154 | Grad Norm: 0.00855139\n",
      "Epoch 4 | Step 2446900 | Avg Loss: 0.0153 | Grad Norm: 0.00956494\n",
      "Epoch 4 | Step 2447000 | Avg Loss: 0.0151 | Grad Norm: 0.00904380\n",
      "Epoch 4 | Step 2447100 | Avg Loss: 0.0149 | Grad Norm: 0.01016148\n",
      "Epoch 4 | Step 2447200 | Avg Loss: 0.0156 | Grad Norm: 0.01255599\n",
      "Epoch 4 | Step 2447300 | Avg Loss: 0.0157 | Grad Norm: 0.01020673\n",
      "Epoch 4 | Step 2447400 | Avg Loss: 0.0155 | Grad Norm: 0.00898338\n",
      "Epoch 4 | Step 2447500 | Avg Loss: 0.0155 | Grad Norm: 0.00776044\n",
      "Epoch 4 | Step 2447600 | Avg Loss: 0.0153 | Grad Norm: 0.00856411\n",
      "Epoch 4 | Step 2447700 | Avg Loss: 0.0152 | Grad Norm: 0.00884445\n",
      "Epoch 4 | Step 2447800 | Avg Loss: 0.0154 | Grad Norm: 0.00894861\n",
      "Epoch 4 | Step 2447900 | Avg Loss: 0.0156 | Grad Norm: 0.00918016\n",
      "Epoch 4 | Step 2448000 | Avg Loss: 0.0152 | Grad Norm: 0.00908058\n",
      "Epoch 4 | Step 2448100 | Avg Loss: 0.0153 | Grad Norm: 0.00974909\n",
      "Epoch 4 | Step 2448200 | Avg Loss: 0.0156 | Grad Norm: 0.00923933\n",
      "Epoch 4 | Step 2448300 | Avg Loss: 0.0159 | Grad Norm: 0.01422681\n",
      "Epoch 4 | Step 2448400 | Avg Loss: 0.0155 | Grad Norm: 0.00835927\n",
      "Epoch 4 | Step 2448500 | Avg Loss: 0.0154 | Grad Norm: 0.00997807\n",
      "Epoch 4 | Step 2448600 | Avg Loss: 0.0155 | Grad Norm: 0.00941472\n",
      "Epoch 4 | Step 2448700 | Avg Loss: 0.0151 | Grad Norm: 0.00956362\n",
      "Epoch 4 | Step 2448800 | Avg Loss: 0.0155 | Grad Norm: 0.00925783\n",
      "Epoch 4 | Step 2448900 | Avg Loss: 0.0157 | Grad Norm: 0.00986270\n",
      "Epoch 4 | Step 2449000 | Avg Loss: 0.0158 | Grad Norm: 0.00892935\n",
      "Epoch 4 | Step 2449100 | Avg Loss: 0.0158 | Grad Norm: 0.00878071\n",
      "Epoch 4 | Step 2449200 | Avg Loss: 0.0154 | Grad Norm: 0.00852330\n",
      "Epoch 4 | Step 2449300 | Avg Loss: 0.0148 | Grad Norm: 0.00784926\n",
      "Epoch 4 | Step 2449400 | Avg Loss: 0.0152 | Grad Norm: 0.00871379\n",
      "Epoch 4 | Step 2449500 | Avg Loss: 0.0154 | Grad Norm: 0.01077850\n",
      "Epoch 4 | Step 2449600 | Avg Loss: 0.0154 | Grad Norm: 0.00929408\n",
      "Epoch 4 | Step 2449700 | Avg Loss: 0.0155 | Grad Norm: 0.00875386\n",
      "Epoch 4 | Step 2449800 | Avg Loss: 0.0157 | Grad Norm: 0.00952153\n",
      "Epoch 4 | Step 2449900 | Avg Loss: 0.0158 | Grad Norm: 0.00865147\n",
      "Epoch 4 | Step 2450000 | Avg Loss: 0.0156 | Grad Norm: 0.00996543\n",
      "Epoch 4 | Step 2450100 | Avg Loss: 0.0157 | Grad Norm: 0.00909367\n",
      "Epoch 4 | Step 2450200 | Avg Loss: 0.0155 | Grad Norm: 0.00917773\n",
      "Epoch 4 | Step 2450300 | Avg Loss: 0.0152 | Grad Norm: 0.00820762\n",
      "Epoch 4 | Step 2450400 | Avg Loss: 0.0149 | Grad Norm: 0.00891170\n",
      "Epoch 4 | Step 2450500 | Avg Loss: 0.0151 | Grad Norm: 0.00824932\n",
      "Epoch 4 | Step 2450600 | Avg Loss: 0.0151 | Grad Norm: 0.00939672\n",
      "Epoch 4 | Step 2450700 | Avg Loss: 0.0149 | Grad Norm: 0.00882241\n",
      "Epoch 4 | Step 2450800 | Avg Loss: 0.0152 | Grad Norm: 0.00859307\n",
      "Epoch 4 | Step 2450900 | Avg Loss: 0.0156 | Grad Norm: 0.01011529\n",
      "Epoch 4 | Step 2451000 | Avg Loss: 0.0161 | Grad Norm: 0.01009685\n",
      "Epoch 4 | Step 2451100 | Avg Loss: 0.0163 | Grad Norm: 0.00921991\n",
      "Epoch 4 | Step 2451200 | Avg Loss: 0.0163 | Grad Norm: 0.00903728\n",
      "Epoch 4 | Step 2451300 | Avg Loss: 0.0163 | Grad Norm: 0.01059036\n",
      "Epoch 4 | Step 2451400 | Avg Loss: 0.0161 | Grad Norm: 0.01054797\n",
      "Epoch 4 | Step 2451500 | Avg Loss: 0.0158 | Grad Norm: 0.00858312\n",
      "Epoch 4 | Step 2451600 | Avg Loss: 0.0162 | Grad Norm: 0.00879833\n",
      "Epoch 4 | Step 2451700 | Avg Loss: 0.0160 | Grad Norm: 0.00891112\n",
      "Epoch 4 | Step 2451800 | Avg Loss: 0.0162 | Grad Norm: 0.00862381\n",
      "Epoch 4 | Step 2451900 | Avg Loss: 0.0161 | Grad Norm: 0.00840398\n",
      "Epoch 4 | Step 2452000 | Avg Loss: 0.0156 | Grad Norm: 0.00985240\n",
      "Epoch 4 | Step 2452100 | Avg Loss: 0.0155 | Grad Norm: 0.00926761\n",
      "Epoch 4 | Step 2452200 | Avg Loss: 0.0159 | Grad Norm: 0.00915370\n",
      "Epoch 4 | Step 2452300 | Avg Loss: 0.0153 | Grad Norm: 0.00824889\n",
      "Epoch 4 | Step 2452400 | Avg Loss: 0.0153 | Grad Norm: 0.01084022\n",
      "Epoch 4 | Step 2452500 | Avg Loss: 0.0154 | Grad Norm: 0.00979182\n",
      "Epoch 4 | Step 2452600 | Avg Loss: 0.0156 | Grad Norm: 0.00849208\n",
      "Epoch 4 | Step 2452700 | Avg Loss: 0.0158 | Grad Norm: 0.00825776\n",
      "Epoch 4 | Step 2452800 | Avg Loss: 0.0157 | Grad Norm: 0.00817813\n",
      "Epoch 4 | Step 2452900 | Avg Loss: 0.0153 | Grad Norm: 0.00927258\n",
      "Epoch 4 | Step 2453000 | Avg Loss: 0.0155 | Grad Norm: 0.00811915\n",
      "Epoch 4 | Step 2453100 | Avg Loss: 0.0157 | Grad Norm: 0.00872081\n",
      "Epoch 4 | Step 2453200 | Avg Loss: 0.0156 | Grad Norm: 0.00979533\n",
      "Epoch 4 | Step 2453300 | Avg Loss: 0.0158 | Grad Norm: 0.01053825\n",
      "Epoch 4 | Step 2453400 | Avg Loss: 0.0155 | Grad Norm: 0.00926143\n",
      "Epoch 4 | Step 2453500 | Avg Loss: 0.0156 | Grad Norm: 0.00789332\n",
      "Epoch 4 | Step 2453600 | Avg Loss: 0.0154 | Grad Norm: 0.00959369\n",
      "Epoch 4 | Step 2453700 | Avg Loss: 0.0153 | Grad Norm: 0.00871864\n",
      "Epoch 4 | Step 2453800 | Avg Loss: 0.0153 | Grad Norm: 0.00915401\n",
      "Epoch 4 | Step 2453900 | Avg Loss: 0.0152 | Grad Norm: 0.01132375\n",
      "Epoch 4 | Step 2454000 | Avg Loss: 0.0149 | Grad Norm: 0.00862399\n",
      "Epoch 4 | Step 2454100 | Avg Loss: 0.0151 | Grad Norm: 0.01108245\n",
      "Epoch 4 | Step 2454200 | Avg Loss: 0.0153 | Grad Norm: 0.01170807\n",
      "Epoch 4 | Step 2454300 | Avg Loss: 0.0153 | Grad Norm: 0.00965985\n",
      "Epoch 4 | Step 2454400 | Avg Loss: 0.0155 | Grad Norm: 0.00791167\n",
      "Epoch 4 | Step 2454500 | Avg Loss: 0.0159 | Grad Norm: 0.00934923\n",
      "Epoch 4 | Step 2454600 | Avg Loss: 0.0157 | Grad Norm: 0.00895682\n",
      "Epoch 4 | Step 2454700 | Avg Loss: 0.0157 | Grad Norm: 0.00860330\n",
      "Epoch 4 | Step 2454800 | Avg Loss: 0.0158 | Grad Norm: 0.00855197\n",
      "Epoch 4 | Step 2454900 | Avg Loss: 0.0157 | Grad Norm: 0.00876768\n",
      "Epoch 4 | Step 2455000 | Avg Loss: 0.0156 | Grad Norm: 0.00964669\n",
      "Epoch 4 | Step 2455100 | Avg Loss: 0.0156 | Grad Norm: 0.00898907\n",
      "Epoch 4 | Step 2455200 | Avg Loss: 0.0156 | Grad Norm: 0.00978259\n",
      "Epoch 4 | Step 2455300 | Avg Loss: 0.0157 | Grad Norm: 0.01015203\n",
      "Epoch 4 | Step 2455400 | Avg Loss: 0.0158 | Grad Norm: 0.00976894\n",
      "Epoch 4 | Step 2455500 | Avg Loss: 0.0155 | Grad Norm: 0.00899175\n",
      "Epoch 4 | Step 2455600 | Avg Loss: 0.0155 | Grad Norm: 0.00871870\n",
      "Epoch 4 | Step 2455700 | Avg Loss: 0.0154 | Grad Norm: 0.00932305\n",
      "Epoch 4 | Step 2455800 | Avg Loss: 0.0153 | Grad Norm: 0.01024889\n",
      "Epoch 4 | Step 2455900 | Avg Loss: 0.0156 | Grad Norm: 0.00946367\n",
      "Epoch 4 | Step 2456000 | Avg Loss: 0.0156 | Grad Norm: 0.00926314\n",
      "Epoch 4 | Step 2456100 | Avg Loss: 0.0156 | Grad Norm: 0.00902123\n",
      "Epoch 4 | Step 2456200 | Avg Loss: 0.0152 | Grad Norm: 0.00887080\n",
      "Epoch 4 | Step 2456300 | Avg Loss: 0.0154 | Grad Norm: 0.00974470\n",
      "Epoch 4 | Step 2456400 | Avg Loss: 0.0154 | Grad Norm: 0.00851430\n",
      "Epoch 4 | Step 2456500 | Avg Loss: 0.0155 | Grad Norm: 0.00823141\n",
      "Epoch 4 | Step 2456600 | Avg Loss: 0.0156 | Grad Norm: 0.00883365\n",
      "Epoch 4 | Step 2456700 | Avg Loss: 0.0156 | Grad Norm: 0.00854432\n",
      "Epoch 4 | Step 2456800 | Avg Loss: 0.0159 | Grad Norm: 0.00893601\n",
      "Epoch 4 | Step 2456900 | Avg Loss: 0.0161 | Grad Norm: 0.00762679\n",
      "Epoch 4 | Step 2457000 | Avg Loss: 0.0159 | Grad Norm: 0.00971778\n",
      "Epoch 4 | Step 2457100 | Avg Loss: 0.0159 | Grad Norm: 0.00987978\n",
      "Epoch 4 | Step 2457200 | Avg Loss: 0.0157 | Grad Norm: 0.00815710\n",
      "Epoch 4 | Step 2457300 | Avg Loss: 0.0153 | Grad Norm: 0.00910569\n",
      "Epoch 4 | Step 2457400 | Avg Loss: 0.0152 | Grad Norm: 0.01087256\n",
      "Epoch 4 | Step 2457500 | Avg Loss: 0.0153 | Grad Norm: 0.00987209\n",
      "Epoch 4 | Step 2457600 | Avg Loss: 0.0154 | Grad Norm: 0.00909159\n",
      "Epoch 4 | Step 2457700 | Avg Loss: 0.0157 | Grad Norm: 0.00917825\n",
      "Epoch 4 | Step 2457800 | Avg Loss: 0.0154 | Grad Norm: 0.00958359\n",
      "Epoch 4 | Step 2457900 | Avg Loss: 0.0155 | Grad Norm: 0.00863532\n",
      "Epoch 4 | Step 2458000 | Avg Loss: 0.0158 | Grad Norm: 0.00866620\n",
      "Epoch 4 | Step 2458100 | Avg Loss: 0.0158 | Grad Norm: 0.00848638\n",
      "Epoch 4 | Step 2458200 | Avg Loss: 0.0158 | Grad Norm: 0.00936351\n",
      "Epoch 4 | Step 2458300 | Avg Loss: 0.0155 | Grad Norm: 0.00969234\n",
      "Epoch 4 | Step 2458400 | Avg Loss: 0.0155 | Grad Norm: 0.00863437\n",
      "Epoch 4 | Step 2458500 | Avg Loss: 0.0152 | Grad Norm: 0.00812278\n",
      "Epoch 4 | Step 2458600 | Avg Loss: 0.0152 | Grad Norm: 0.00833774\n",
      "Epoch 4 | Step 2458700 | Avg Loss: 0.0156 | Grad Norm: 0.00954173\n",
      "Epoch 4 | Step 2458800 | Avg Loss: 0.0159 | Grad Norm: 0.00915585\n",
      "Epoch 4 | Step 2458900 | Avg Loss: 0.0157 | Grad Norm: 0.00914665\n",
      "Epoch 4 | Step 2459000 | Avg Loss: 0.0157 | Grad Norm: 0.00857486\n",
      "Epoch 4 | Step 2459100 | Avg Loss: 0.0154 | Grad Norm: 0.00946758\n",
      "Epoch 4 | Step 2459200 | Avg Loss: 0.0152 | Grad Norm: 0.00830572\n",
      "Epoch 4 | Step 2459300 | Avg Loss: 0.0150 | Grad Norm: 0.00871411\n",
      "Epoch 4 | Step 2459400 | Avg Loss: 0.0154 | Grad Norm: 0.00759258\n",
      "Epoch 4 | Step 2459500 | Avg Loss: 0.0153 | Grad Norm: 0.00910354\n",
      "Epoch 4 | Step 2459600 | Avg Loss: 0.0154 | Grad Norm: 0.00984192\n",
      "Epoch 4 | Step 2459700 | Avg Loss: 0.0154 | Grad Norm: 0.00808877\n",
      "Epoch 4 | Step 2459800 | Avg Loss: 0.0154 | Grad Norm: 0.00893357\n",
      "Epoch 4 | Step 2459900 | Avg Loss: 0.0156 | Grad Norm: 0.00769384\n",
      "Epoch 4 | Step 2460000 | Avg Loss: 0.0158 | Grad Norm: 0.00839014\n",
      "Epoch 4 | Step 2460100 | Avg Loss: 0.0161 | Grad Norm: 0.00856913\n",
      "Epoch 4 | Step 2460200 | Avg Loss: 0.0161 | Grad Norm: 0.00829484\n",
      "Epoch 4 | Step 2460300 | Avg Loss: 0.0158 | Grad Norm: 0.00895829\n",
      "Epoch 4 | Step 2460400 | Avg Loss: 0.0158 | Grad Norm: 0.00884430\n",
      "Epoch 4 | Step 2460500 | Avg Loss: 0.0157 | Grad Norm: 0.01027643\n",
      "Epoch 4 | Step 2460600 | Avg Loss: 0.0157 | Grad Norm: 0.01017232\n",
      "Epoch 4 | Step 2460700 | Avg Loss: 0.0156 | Grad Norm: 0.00908261\n",
      "Epoch 4 | Step 2460800 | Avg Loss: 0.0155 | Grad Norm: 0.00892038\n",
      "Epoch 4 | Step 2460900 | Avg Loss: 0.0155 | Grad Norm: 0.00896710\n",
      "Epoch 4 | Step 2461000 | Avg Loss: 0.0155 | Grad Norm: 0.00885505\n",
      "Epoch 4 | Step 2461100 | Avg Loss: 0.0155 | Grad Norm: 0.00877970\n",
      "Epoch 4 | Step 2461200 | Avg Loss: 0.0155 | Grad Norm: 0.00895224\n",
      "Epoch 4 | Step 2461300 | Avg Loss: 0.0156 | Grad Norm: 0.01030519\n",
      "Epoch 4 | Step 2461400 | Avg Loss: 0.0160 | Grad Norm: 0.00813725\n",
      "Epoch 4 | Step 2461500 | Avg Loss: 0.0160 | Grad Norm: 0.00932894\n",
      "Epoch 4 | Step 2461600 | Avg Loss: 0.0159 | Grad Norm: 0.00779536\n",
      "Epoch 4 | Step 2461700 | Avg Loss: 0.0157 | Grad Norm: 0.00755916\n",
      "Epoch 4 | Step 2461800 | Avg Loss: 0.0159 | Grad Norm: 0.00927876\n",
      "Epoch 4 | Step 2461900 | Avg Loss: 0.0161 | Grad Norm: 0.00927808\n",
      "Epoch 4 | Step 2462000 | Avg Loss: 0.0157 | Grad Norm: 0.00852709\n",
      "Epoch 4 | Step 2462100 | Avg Loss: 0.0158 | Grad Norm: 0.01032907\n",
      "Epoch 4 | Step 2462200 | Avg Loss: 0.0159 | Grad Norm: 0.01059095\n",
      "Epoch 4 | Step 2462300 | Avg Loss: 0.0157 | Grad Norm: 0.00852295\n",
      "Epoch 4 | Step 2462400 | Avg Loss: 0.0156 | Grad Norm: 0.00893820\n",
      "Epoch 4 | Step 2462500 | Avg Loss: 0.0156 | Grad Norm: 0.00883627\n",
      "Epoch 4 | Step 2462600 | Avg Loss: 0.0154 | Grad Norm: 0.00915984\n",
      "Epoch 4 | Step 2462700 | Avg Loss: 0.0152 | Grad Norm: 0.00973671\n",
      "Epoch 4 | Step 2462800 | Avg Loss: 0.0150 | Grad Norm: 0.00869431\n",
      "Epoch 4 | Step 2462900 | Avg Loss: 0.0151 | Grad Norm: 0.00854126\n",
      "Epoch 4 | Step 2463000 | Avg Loss: 0.0151 | Grad Norm: 0.00881719\n",
      "Epoch 4 | Step 2463100 | Avg Loss: 0.0154 | Grad Norm: 0.00872611\n",
      "Epoch 4 | Step 2463200 | Avg Loss: 0.0149 | Grad Norm: 0.01058272\n",
      "Epoch 4 | Step 2463300 | Avg Loss: 0.0150 | Grad Norm: 0.00830282\n",
      "Epoch 4 | Step 2463400 | Avg Loss: 0.0149 | Grad Norm: 0.00814739\n",
      "Epoch 4 | Step 2463500 | Avg Loss: 0.0151 | Grad Norm: 0.00873332\n",
      "Epoch 4 | Step 2463600 | Avg Loss: 0.0154 | Grad Norm: 0.00896085\n",
      "Epoch 4 | Step 2463700 | Avg Loss: 0.0158 | Grad Norm: 0.00834641\n",
      "Epoch 4 | Step 2463800 | Avg Loss: 0.0158 | Grad Norm: 0.00894691\n",
      "Epoch 4 | Step 2463900 | Avg Loss: 0.0153 | Grad Norm: 0.00882045\n",
      "Epoch 4 | Step 2464000 | Avg Loss: 0.0157 | Grad Norm: 0.00837357\n",
      "Epoch 4 | Step 2464100 | Avg Loss: 0.0154 | Grad Norm: 0.00804358\n",
      "Epoch 4 | Step 2464200 | Avg Loss: 0.0154 | Grad Norm: 0.00938259\n",
      "Epoch 4 | Step 2464300 | Avg Loss: 0.0154 | Grad Norm: 0.00888115\n",
      "Epoch 4 | Step 2464400 | Avg Loss: 0.0153 | Grad Norm: 0.00961684\n",
      "Epoch 4 | Step 2464500 | Avg Loss: 0.0152 | Grad Norm: 0.00881162\n",
      "Epoch 4 | Step 2464600 | Avg Loss: 0.0150 | Grad Norm: 0.00833215\n",
      "Epoch 4 | Step 2464700 | Avg Loss: 0.0151 | Grad Norm: 0.00906473\n",
      "Epoch 4 | Step 2464800 | Avg Loss: 0.0151 | Grad Norm: 0.01028984\n",
      "Epoch 4 | Step 2464900 | Avg Loss: 0.0152 | Grad Norm: 0.01077668\n",
      "Epoch 4 | Step 2465000 | Avg Loss: 0.0151 | Grad Norm: 0.00849341\n",
      "Epoch 4 | Step 2465100 | Avg Loss: 0.0152 | Grad Norm: 0.00910735\n",
      "Epoch 4 | Step 2465200 | Avg Loss: 0.0153 | Grad Norm: 0.00954164\n",
      "Epoch 4 | Step 2465300 | Avg Loss: 0.0153 | Grad Norm: 0.00913017\n",
      "Epoch 4 | Step 2465400 | Avg Loss: 0.0155 | Grad Norm: 0.00852769\n",
      "Epoch 4 | Step 2465500 | Avg Loss: 0.0156 | Grad Norm: 0.00852308\n",
      "Epoch 4 | Step 2465600 | Avg Loss: 0.0154 | Grad Norm: 0.00809071\n",
      "Epoch 4 | Step 2465700 | Avg Loss: 0.0154 | Grad Norm: 0.00968269\n",
      "Epoch 4 | Step 2465800 | Avg Loss: 0.0156 | Grad Norm: 0.00851228\n",
      "Epoch 4 | Step 2465900 | Avg Loss: 0.0154 | Grad Norm: 0.00844577\n",
      "Epoch 4 | Step 2466000 | Avg Loss: 0.0155 | Grad Norm: 0.00741997\n",
      "Epoch 4 | Step 2466100 | Avg Loss: 0.0153 | Grad Norm: 0.01058374\n",
      "Epoch 4 | Step 2466200 | Avg Loss: 0.0153 | Grad Norm: 0.01035661\n",
      "Epoch 4 | Step 2466300 | Avg Loss: 0.0153 | Grad Norm: 0.00936015\n",
      "Epoch 4 | Step 2466400 | Avg Loss: 0.0151 | Grad Norm: 0.00926199\n",
      "Epoch 4 | Step 2466500 | Avg Loss: 0.0153 | Grad Norm: 0.01002234\n",
      "Epoch 4 | Step 2466600 | Avg Loss: 0.0154 | Grad Norm: 0.01078328\n",
      "Epoch 4 | Step 2466700 | Avg Loss: 0.0150 | Grad Norm: 0.00870893\n",
      "Epoch 4 | Step 2466800 | Avg Loss: 0.0149 | Grad Norm: 0.00975489\n",
      "Epoch 4 | Step 2466900 | Avg Loss: 0.0152 | Grad Norm: 0.00900254\n",
      "Epoch 4 | Step 2467000 | Avg Loss: 0.0152 | Grad Norm: 0.00837793\n",
      "Epoch 4 | Step 2467100 | Avg Loss: 0.0152 | Grad Norm: 0.00868976\n",
      "Epoch 4 | Step 2467200 | Avg Loss: 0.0152 | Grad Norm: 0.00931342\n",
      "Epoch 4 | Step 2467300 | Avg Loss: 0.0153 | Grad Norm: 0.00888930\n",
      "Epoch 4 | Step 2467400 | Avg Loss: 0.0152 | Grad Norm: 0.00868424\n",
      "Epoch 4 | Step 2467500 | Avg Loss: 0.0153 | Grad Norm: 0.00909635\n",
      "Epoch 4 | Step 2467600 | Avg Loss: 0.0153 | Grad Norm: 0.00761311\n",
      "Epoch 4 | Step 2467700 | Avg Loss: 0.0154 | Grad Norm: 0.00921446\n",
      "Epoch 4 | Step 2467800 | Avg Loss: 0.0158 | Grad Norm: 0.00995558\n",
      "Epoch 4 | Step 2467900 | Avg Loss: 0.0158 | Grad Norm: 0.00865912\n",
      "Epoch 4 | Step 2468000 | Avg Loss: 0.0156 | Grad Norm: 0.00774131\n",
      "Epoch 4 | Step 2468100 | Avg Loss: 0.0154 | Grad Norm: 0.00894276\n",
      "Epoch 4 | Step 2468200 | Avg Loss: 0.0156 | Grad Norm: 0.00893825\n",
      "Epoch 4 | Step 2468300 | Avg Loss: 0.0154 | Grad Norm: 0.00872064\n",
      "Epoch 4 | Step 2468400 | Avg Loss: 0.0155 | Grad Norm: 0.00881514\n",
      "Epoch 4 | Step 2468500 | Avg Loss: 0.0150 | Grad Norm: 0.00913279\n",
      "Epoch 4 | Step 2468600 | Avg Loss: 0.0155 | Grad Norm: 0.00995514\n",
      "Epoch 4 | Step 2468700 | Avg Loss: 0.0155 | Grad Norm: 0.00879471\n",
      "Epoch 4 | Step 2468800 | Avg Loss: 0.0157 | Grad Norm: 0.00921805\n",
      "Epoch 4 | Step 2468900 | Avg Loss: 0.0153 | Grad Norm: 0.01312546\n",
      "Epoch 4 | Step 2469000 | Avg Loss: 0.0157 | Grad Norm: 0.00888635\n",
      "Epoch 4 | Step 2469100 | Avg Loss: 0.0153 | Grad Norm: 0.01192899\n",
      "Epoch 4 | Step 2469200 | Avg Loss: 0.0159 | Grad Norm: 0.00859098\n",
      "Epoch 4 | Step 2469300 | Avg Loss: 0.0155 | Grad Norm: 0.00812778\n",
      "Epoch 4 | Step 2469400 | Avg Loss: 0.0157 | Grad Norm: 0.00810737\n",
      "Epoch 4 | Step 2469500 | Avg Loss: 0.0154 | Grad Norm: 0.00873343\n",
      "Epoch 4 | Step 2469600 | Avg Loss: 0.0153 | Grad Norm: 0.00860437\n",
      "Epoch 4 | Step 2469700 | Avg Loss: 0.0155 | Grad Norm: 0.01592100\n",
      "Epoch 4 | Step 2469800 | Avg Loss: 0.0155 | Grad Norm: 0.00864860\n",
      "Epoch 4 | Step 2469900 | Avg Loss: 0.0157 | Grad Norm: 0.01083065\n",
      "Epoch 4 | Step 2470000 | Avg Loss: 0.0158 | Grad Norm: 0.00840296\n",
      "Epoch 4 | Step 2470100 | Avg Loss: 0.0158 | Grad Norm: 0.00894094\n",
      "Epoch 4 | Step 2470200 | Avg Loss: 0.0158 | Grad Norm: 0.00856963\n",
      "Epoch 4 | Step 2470300 | Avg Loss: 0.0156 | Grad Norm: 0.00802371\n",
      "Epoch 4 | Step 2470400 | Avg Loss: 0.0157 | Grad Norm: 0.00798260\n",
      "Epoch 4 | Step 2470500 | Avg Loss: 0.0156 | Grad Norm: 0.00888205\n",
      "Epoch 4 | Step 2470600 | Avg Loss: 0.0152 | Grad Norm: 0.00771570\n",
      "Epoch 4 | Step 2470700 | Avg Loss: 0.0152 | Grad Norm: 0.00931490\n",
      "Epoch 4 | Step 2470800 | Avg Loss: 0.0156 | Grad Norm: 0.00838789\n",
      "Epoch 4 | Step 2470900 | Avg Loss: 0.0157 | Grad Norm: 0.01025050\n",
      "Epoch 4 | Step 2471000 | Avg Loss: 0.0155 | Grad Norm: 0.01048949\n",
      "Epoch 4 | Step 2471100 | Avg Loss: 0.0157 | Grad Norm: 0.00879261\n",
      "Epoch 4 | Step 2471200 | Avg Loss: 0.0150 | Grad Norm: 0.00901689\n",
      "Epoch 4 | Step 2471300 | Avg Loss: 0.0153 | Grad Norm: 0.01047880\n",
      "Epoch 4 | Step 2471400 | Avg Loss: 0.0153 | Grad Norm: 0.00896323\n",
      "Epoch 4 | Step 2471500 | Avg Loss: 0.0157 | Grad Norm: 0.00831387\n",
      "Epoch 4 | Step 2471600 | Avg Loss: 0.0157 | Grad Norm: 0.01037621\n",
      "Epoch 4 | Step 2471700 | Avg Loss: 0.0159 | Grad Norm: 0.00858517\n",
      "Epoch 4 | Step 2471800 | Avg Loss: 0.0155 | Grad Norm: 0.00816505\n",
      "Epoch 4 | Step 2471900 | Avg Loss: 0.0153 | Grad Norm: 0.00892326\n",
      "Epoch 4 | Step 2472000 | Avg Loss: 0.0154 | Grad Norm: 0.00865001\n",
      "Epoch 4 | Step 2472100 | Avg Loss: 0.0153 | Grad Norm: 0.00951306\n",
      "Epoch 4 | Step 2472200 | Avg Loss: 0.0155 | Grad Norm: 0.00891344\n",
      "Epoch 4 | Step 2472300 | Avg Loss: 0.0152 | Grad Norm: 0.00847211\n",
      "Epoch 4 | Step 2472400 | Avg Loss: 0.0152 | Grad Norm: 0.00841434\n",
      "Epoch 4 | Step 2472500 | Avg Loss: 0.0154 | Grad Norm: 0.00988915\n",
      "Epoch 4 | Step 2472600 | Avg Loss: 0.0157 | Grad Norm: 0.01042253\n",
      "Epoch 4 | Step 2472700 | Avg Loss: 0.0157 | Grad Norm: 0.00813445\n",
      "Epoch 4 | Step 2472800 | Avg Loss: 0.0157 | Grad Norm: 0.00927569\n",
      "Epoch 4 | Step 2472900 | Avg Loss: 0.0155 | Grad Norm: 0.00796913\n",
      "Epoch 4 | Step 2473000 | Avg Loss: 0.0158 | Grad Norm: 0.00844295\n",
      "Epoch 4 | Step 2473100 | Avg Loss: 0.0156 | Grad Norm: 0.00884381\n",
      "Epoch 4 | Step 2473200 | Avg Loss: 0.0157 | Grad Norm: 0.01042932\n",
      "Epoch 4 | Step 2473300 | Avg Loss: 0.0156 | Grad Norm: 0.00969968\n",
      "Epoch 4 | Step 2473400 | Avg Loss: 0.0160 | Grad Norm: 0.00888123\n",
      "Epoch 4 | Step 2473500 | Avg Loss: 0.0157 | Grad Norm: 0.00836327\n",
      "Epoch 4 | Step 2473600 | Avg Loss: 0.0162 | Grad Norm: 0.00918712\n",
      "Epoch 4 | Step 2473700 | Avg Loss: 0.0157 | Grad Norm: 0.01012466\n",
      "Epoch 4 | Step 2473800 | Avg Loss: 0.0155 | Grad Norm: 0.00908001\n",
      "Epoch 4 | Step 2473900 | Avg Loss: 0.0155 | Grad Norm: 0.01149923\n",
      "Epoch 4 | Step 2474000 | Avg Loss: 0.0156 | Grad Norm: 0.00763914\n",
      "Epoch 4 | Step 2474100 | Avg Loss: 0.0155 | Grad Norm: 0.00902890\n",
      "Epoch 4 | Step 2474200 | Avg Loss: 0.0152 | Grad Norm: 0.00989325\n",
      "Epoch 4 | Step 2474300 | Avg Loss: 0.0158 | Grad Norm: 0.00916973\n",
      "Epoch 4 | Step 2474400 | Avg Loss: 0.0161 | Grad Norm: 0.01000560\n",
      "Epoch 4 | Step 2474500 | Avg Loss: 0.0158 | Grad Norm: 0.01018248\n",
      "Epoch 4 | Step 2474600 | Avg Loss: 0.0158 | Grad Norm: 0.00857424\n",
      "Epoch 4 | Step 2474700 | Avg Loss: 0.0158 | Grad Norm: 0.00997311\n",
      "Epoch 4 | Step 2474800 | Avg Loss: 0.0157 | Grad Norm: 0.00911328\n",
      "Epoch 4 | Step 2474900 | Avg Loss: 0.0156 | Grad Norm: 0.01041100\n",
      "Epoch 4 | Step 2475000 | Avg Loss: 0.0157 | Grad Norm: 0.00858329\n",
      "Epoch 4 | Step 2475100 | Avg Loss: 0.0161 | Grad Norm: 0.00955797\n",
      "Epoch 4 | Step 2475200 | Avg Loss: 0.0160 | Grad Norm: 0.00938149\n",
      "Epoch 4 | Step 2475300 | Avg Loss: 0.0157 | Grad Norm: 0.00958674\n",
      "Epoch 4 | Step 2475400 | Avg Loss: 0.0159 | Grad Norm: 0.00903758\n",
      "Epoch 4 | Step 2475500 | Avg Loss: 0.0162 | Grad Norm: 0.00821581\n",
      "Epoch 4 | Step 2475600 | Avg Loss: 0.0157 | Grad Norm: 0.00956707\n",
      "Epoch 4 | Step 2475700 | Avg Loss: 0.0154 | Grad Norm: 0.00895973\n",
      "Epoch 4 | Step 2475800 | Avg Loss: 0.0151 | Grad Norm: 0.00892032\n",
      "Epoch 4 | Step 2475900 | Avg Loss: 0.0157 | Grad Norm: 0.00770126\n",
      "Epoch 4 | Step 2476000 | Avg Loss: 0.0158 | Grad Norm: 0.00947731\n",
      "Epoch 4 | Step 2476100 | Avg Loss: 0.0160 | Grad Norm: 0.00919090\n",
      "Epoch 4 | Step 2476200 | Avg Loss: 0.0159 | Grad Norm: 0.00903331\n",
      "Epoch 4 | Step 2476300 | Avg Loss: 0.0157 | Grad Norm: 0.00889272\n",
      "Epoch 4 | Step 2476400 | Avg Loss: 0.0161 | Grad Norm: 0.00942345\n",
      "Epoch 4 | Step 2476500 | Avg Loss: 0.0161 | Grad Norm: 0.00882718\n",
      "Epoch 4 | Step 2476600 | Avg Loss: 0.0157 | Grad Norm: 0.00877553\n",
      "Epoch 4 | Step 2476700 | Avg Loss: 0.0158 | Grad Norm: 0.01010799\n",
      "Epoch 4 | Step 2476800 | Avg Loss: 0.0159 | Grad Norm: 0.00779219\n",
      "Epoch 4 | Step 2476900 | Avg Loss: 0.0157 | Grad Norm: 0.00921712\n",
      "Epoch 4 | Step 2477000 | Avg Loss: 0.0156 | Grad Norm: 0.00833889\n",
      "Epoch 4 | Step 2477100 | Avg Loss: 0.0156 | Grad Norm: 0.00873835\n",
      "Epoch 4 | Step 2477200 | Avg Loss: 0.0158 | Grad Norm: 0.00856665\n",
      "Epoch 4 | Step 2477300 | Avg Loss: 0.0156 | Grad Norm: 0.01113262\n",
      "Epoch 4 | Step 2477400 | Avg Loss: 0.0155 | Grad Norm: 0.00890556\n",
      "Epoch 4 | Step 2477500 | Avg Loss: 0.0154 | Grad Norm: 0.00788156\n",
      "Epoch 4 | Step 2477600 | Avg Loss: 0.0151 | Grad Norm: 0.00969003\n",
      "Epoch 4 | Step 2477700 | Avg Loss: 0.0150 | Grad Norm: 0.00859312\n",
      "Epoch 4 | Step 2477800 | Avg Loss: 0.0150 | Grad Norm: 0.01124814\n",
      "Epoch 4 | Step 2477900 | Avg Loss: 0.0151 | Grad Norm: 0.00834651\n",
      "Epoch 4 | Step 2478000 | Avg Loss: 0.0151 | Grad Norm: 0.00927754\n",
      "Epoch 4 | Step 2478100 | Avg Loss: 0.0153 | Grad Norm: 0.01020661\n",
      "Epoch 4 | Step 2478200 | Avg Loss: 0.0154 | Grad Norm: 0.00912599\n",
      "Epoch 4 | Step 2478300 | Avg Loss: 0.0155 | Grad Norm: 0.00864647\n",
      "Epoch 4 | Step 2478400 | Avg Loss: 0.0154 | Grad Norm: 0.00954254\n",
      "Epoch 4 | Step 2478500 | Avg Loss: 0.0152 | Grad Norm: 0.00886852\n",
      "Epoch 4 | Step 2478600 | Avg Loss: 0.0155 | Grad Norm: 0.00803922\n",
      "Epoch 4 | Step 2478700 | Avg Loss: 0.0154 | Grad Norm: 0.00974398\n",
      "Epoch 4 | Step 2478800 | Avg Loss: 0.0157 | Grad Norm: 0.00867704\n",
      "Epoch 4 | Step 2478900 | Avg Loss: 0.0158 | Grad Norm: 0.00912360\n",
      "Epoch 4 | Step 2479000 | Avg Loss: 0.0159 | Grad Norm: 0.00949957\n",
      "Epoch 4 | Step 2479100 | Avg Loss: 0.0157 | Grad Norm: 0.00904902\n",
      "Epoch 4 | Step 2479200 | Avg Loss: 0.0160 | Grad Norm: 0.00866712\n",
      "Epoch 4 | Step 2479300 | Avg Loss: 0.0158 | Grad Norm: 0.01300473\n",
      "Epoch 4 | Step 2479400 | Avg Loss: 0.0159 | Grad Norm: 0.01149038\n",
      "Epoch 4 | Step 2479500 | Avg Loss: 0.0157 | Grad Norm: 0.00843933\n",
      "Epoch 4 | Step 2479600 | Avg Loss: 0.0159 | Grad Norm: 0.00835561\n",
      "Epoch 4 | Step 2479700 | Avg Loss: 0.0157 | Grad Norm: 0.00796318\n",
      "Epoch 4 | Step 2479800 | Avg Loss: 0.0154 | Grad Norm: 0.00956114\n",
      "Epoch 4 | Step 2479900 | Avg Loss: 0.0157 | Grad Norm: 0.00866209\n",
      "Epoch 4 | Step 2480000 | Avg Loss: 0.0154 | Grad Norm: 0.00893976\n",
      "Epoch 4 | Step 2480100 | Avg Loss: 0.0150 | Grad Norm: 0.00959411\n",
      "Epoch 4 | Step 2480200 | Avg Loss: 0.0151 | Grad Norm: 0.00781250\n",
      "Epoch 4 | Step 2480300 | Avg Loss: 0.0148 | Grad Norm: 0.00964912\n",
      "Epoch 4 | Step 2480400 | Avg Loss: 0.0148 | Grad Norm: 0.00813918\n",
      "Epoch 4 | Step 2480500 | Avg Loss: 0.0149 | Grad Norm: 0.00839015\n",
      "Epoch 4 | Step 2480600 | Avg Loss: 0.0151 | Grad Norm: 0.00916277\n",
      "Epoch 4 | Step 2480700 | Avg Loss: 0.0154 | Grad Norm: 0.01017245\n",
      "Epoch 4 | Step 2480800 | Avg Loss: 0.0153 | Grad Norm: 0.00890778\n",
      "Epoch 4 | Step 2480900 | Avg Loss: 0.0152 | Grad Norm: 0.01004959\n",
      "Epoch 4 | Step 2481000 | Avg Loss: 0.0152 | Grad Norm: 0.00863473\n",
      "Epoch 4 | Step 2481100 | Avg Loss: 0.0153 | Grad Norm: 0.00862266\n",
      "Epoch 4 | Step 2481200 | Avg Loss: 0.0154 | Grad Norm: 0.00860855\n",
      "Epoch 4 | Step 2481300 | Avg Loss: 0.0153 | Grad Norm: 0.01093655\n",
      "Epoch 4 | Step 2481400 | Avg Loss: 0.0155 | Grad Norm: 0.00911866\n",
      "Epoch 4 | Step 2481500 | Avg Loss: 0.0154 | Grad Norm: 0.00903546\n",
      "Epoch 4 | Step 2481600 | Avg Loss: 0.0156 | Grad Norm: 0.00880858\n",
      "Epoch 4 | Step 2481700 | Avg Loss: 0.0153 | Grad Norm: 0.00830046\n",
      "Epoch 4 | Step 2481800 | Avg Loss: 0.0156 | Grad Norm: 0.00823585\n",
      "Epoch 4 | Step 2481900 | Avg Loss: 0.0157 | Grad Norm: 0.01331019\n",
      "Epoch 4 | Step 2482000 | Avg Loss: 0.0160 | Grad Norm: 0.00997762\n",
      "Epoch 4 | Step 2482100 | Avg Loss: 0.0158 | Grad Norm: 0.00815323\n",
      "Epoch 4 | Step 2482200 | Avg Loss: 0.0154 | Grad Norm: 0.01031648\n",
      "Epoch 4 | Step 2482300 | Avg Loss: 0.0154 | Grad Norm: 0.00944652\n",
      "Epoch 4 | Step 2482400 | Avg Loss: 0.0155 | Grad Norm: 0.00993860\n",
      "Epoch 4 | Step 2482500 | Avg Loss: 0.0154 | Grad Norm: 0.00974109\n",
      "Epoch 4 | Step 2482600 | Avg Loss: 0.0153 | Grad Norm: 0.00735326\n",
      "Epoch 4 | Step 2482700 | Avg Loss: 0.0152 | Grad Norm: 0.00847234\n",
      "Epoch 4 | Step 2482800 | Avg Loss: 0.0154 | Grad Norm: 0.01010329\n",
      "Epoch 4 | Step 2482900 | Avg Loss: 0.0156 | Grad Norm: 0.00939589\n",
      "Epoch 4 | Step 2483000 | Avg Loss: 0.0159 | Grad Norm: 0.00848475\n",
      "Epoch 4 | Step 2483100 | Avg Loss: 0.0159 | Grad Norm: 0.00866930\n",
      "Epoch 4 | Step 2483200 | Avg Loss: 0.0157 | Grad Norm: 0.00848397\n",
      "Epoch 4 | Step 2483300 | Avg Loss: 0.0156 | Grad Norm: 0.00948428\n",
      "Epoch 4 | Step 2483400 | Avg Loss: 0.0160 | Grad Norm: 0.01212035\n",
      "Epoch 4 | Step 2483500 | Avg Loss: 0.0160 | Grad Norm: 0.00833770\n",
      "Epoch 4 | Step 2483600 | Avg Loss: 0.0158 | Grad Norm: 0.01091242\n",
      "Epoch 4 | Step 2483700 | Avg Loss: 0.0154 | Grad Norm: 0.00949911\n",
      "Epoch 4 | Step 2483800 | Avg Loss: 0.0152 | Grad Norm: 0.00922713\n",
      "Epoch 4 | Step 2483900 | Avg Loss: 0.0151 | Grad Norm: 0.00934594\n",
      "Epoch 4 | Step 2484000 | Avg Loss: 0.0148 | Grad Norm: 0.00898938\n",
      "Epoch 4 | Step 2484100 | Avg Loss: 0.0151 | Grad Norm: 0.00768326\n",
      "Epoch 4 | Step 2484200 | Avg Loss: 0.0155 | Grad Norm: 0.01087373\n",
      "Epoch 4 | Step 2484300 | Avg Loss: 0.0157 | Grad Norm: 0.00896366\n",
      "Epoch 4 | Step 2484400 | Avg Loss: 0.0159 | Grad Norm: 0.01055490\n",
      "Epoch 4 | Step 2484500 | Avg Loss: 0.0159 | Grad Norm: 0.00808697\n",
      "Epoch 4 | Step 2484600 | Avg Loss: 0.0161 | Grad Norm: 0.00861511\n",
      "Epoch 4 | Step 2484700 | Avg Loss: 0.0160 | Grad Norm: 0.00967786\n",
      "Epoch 4 | Step 2484800 | Avg Loss: 0.0159 | Grad Norm: 0.00980800\n",
      "Epoch 4 | Step 2484900 | Avg Loss: 0.0159 | Grad Norm: 0.00888261\n",
      "Epoch 4 | Step 2485000 | Avg Loss: 0.0157 | Grad Norm: 0.01059152\n",
      "Epoch 4 | Step 2485100 | Avg Loss: 0.0158 | Grad Norm: 0.00983334\n",
      "Epoch 4 | Step 2485200 | Avg Loss: 0.0153 | Grad Norm: 0.00921186\n",
      "Epoch 4 | Step 2485300 | Avg Loss: 0.0154 | Grad Norm: 0.00800565\n",
      "Epoch 4 | Step 2485400 | Avg Loss: 0.0151 | Grad Norm: 0.00789056\n",
      "Epoch 4 | Step 2485500 | Avg Loss: 0.0152 | Grad Norm: 0.00831400\n",
      "Epoch 4 | Step 2485600 | Avg Loss: 0.0149 | Grad Norm: 0.01085732\n",
      "Epoch 4 | Step 2485700 | Avg Loss: 0.0149 | Grad Norm: 0.00811172\n",
      "Epoch 4 | Step 2485800 | Avg Loss: 0.0151 | Grad Norm: 0.01067755\n",
      "Epoch 4 | Step 2485900 | Avg Loss: 0.0156 | Grad Norm: 0.00807615\n",
      "Epoch 4 | Step 2486000 | Avg Loss: 0.0155 | Grad Norm: 0.00977468\n",
      "Epoch 4 | Step 2486100 | Avg Loss: 0.0160 | Grad Norm: 0.00843846\n",
      "Epoch 4 | Step 2486200 | Avg Loss: 0.0158 | Grad Norm: 0.01074212\n",
      "Epoch 4 | Step 2486300 | Avg Loss: 0.0154 | Grad Norm: 0.00833797\n",
      "Epoch 4 | Step 2486400 | Avg Loss: 0.0150 | Grad Norm: 0.00874429\n",
      "Epoch 4 | Step 2486500 | Avg Loss: 0.0151 | Grad Norm: 0.00886719\n",
      "Epoch 4 | Step 2486600 | Avg Loss: 0.0146 | Grad Norm: 0.00890917\n",
      "Epoch 4 | Step 2486700 | Avg Loss: 0.0148 | Grad Norm: 0.00984279\n",
      "Epoch 4 | Step 2486800 | Avg Loss: 0.0153 | Grad Norm: 0.00917178\n",
      "Epoch 4 | Step 2486900 | Avg Loss: 0.0154 | Grad Norm: 0.00945464\n",
      "Epoch 4 | Step 2487000 | Avg Loss: 0.0153 | Grad Norm: 0.00936351\n",
      "Epoch 4 | Step 2487100 | Avg Loss: 0.0152 | Grad Norm: 0.00888697\n",
      "Epoch 4 | Step 2487200 | Avg Loss: 0.0156 | Grad Norm: 0.00841751\n",
      "Epoch 4 | Step 2487300 | Avg Loss: 0.0154 | Grad Norm: 0.00801487\n",
      "Epoch 4 | Step 2487400 | Avg Loss: 0.0151 | Grad Norm: 0.00919905\n",
      "Epoch 4 | Step 2487500 | Avg Loss: 0.0151 | Grad Norm: 0.00999448\n",
      "Epoch 4 | Step 2487600 | Avg Loss: 0.0152 | Grad Norm: 0.00871817\n",
      "Epoch 4 | Step 2487700 | Avg Loss: 0.0151 | Grad Norm: 0.00882583\n",
      "Epoch 4 | Step 2487800 | Avg Loss: 0.0151 | Grad Norm: 0.00808259\n",
      "Epoch 4 | Step 2487900 | Avg Loss: 0.0153 | Grad Norm: 0.00734319\n",
      "Epoch 4 | Step 2488000 | Avg Loss: 0.0153 | Grad Norm: 0.00828518\n",
      "Epoch 4 | Step 2488100 | Avg Loss: 0.0158 | Grad Norm: 0.00889566\n",
      "Epoch 4 | Step 2488200 | Avg Loss: 0.0158 | Grad Norm: 0.01066403\n",
      "Epoch 4 | Step 2488300 | Avg Loss: 0.0157 | Grad Norm: 0.00934489\n",
      "Epoch 4 | Step 2488400 | Avg Loss: 0.0159 | Grad Norm: 0.00862445\n",
      "Epoch 4 | Step 2488500 | Avg Loss: 0.0162 | Grad Norm: 0.00830923\n",
      "Epoch 4 | Step 2488600 | Avg Loss: 0.0161 | Grad Norm: 0.00799156\n",
      "Epoch 4 | Step 2488700 | Avg Loss: 0.0159 | Grad Norm: 0.00978540\n",
      "Epoch 4 | Step 2488800 | Avg Loss: 0.0156 | Grad Norm: 0.00819714\n",
      "Epoch 4 | Step 2488900 | Avg Loss: 0.0154 | Grad Norm: 0.00799803\n",
      "Epoch 4 | Step 2489000 | Avg Loss: 0.0155 | Grad Norm: 0.00877704\n",
      "Epoch 4 | Step 2489100 | Avg Loss: 0.0156 | Grad Norm: 0.00914060\n",
      "Epoch 4 | Step 2489200 | Avg Loss: 0.0159 | Grad Norm: 0.01217834\n",
      "Epoch 4 | Step 2489300 | Avg Loss: 0.0158 | Grad Norm: 0.01013698\n",
      "Epoch 4 | Step 2489400 | Avg Loss: 0.0157 | Grad Norm: 0.00871820\n",
      "Epoch 4 | Step 2489500 | Avg Loss: 0.0155 | Grad Norm: 0.01011438\n",
      "Epoch 4 | Step 2489600 | Avg Loss: 0.0154 | Grad Norm: 0.00979817\n",
      "Epoch 4 | Step 2489700 | Avg Loss: 0.0154 | Grad Norm: 0.00914280\n",
      "Epoch 4 | Step 2489800 | Avg Loss: 0.0156 | Grad Norm: 0.00784630\n",
      "Epoch 4 | Step 2489900 | Avg Loss: 0.0156 | Grad Norm: 0.00920059\n",
      "Epoch 4 | Step 2490000 | Avg Loss: 0.0155 | Grad Norm: 0.00886456\n",
      "Epoch 4 | Step 2490100 | Avg Loss: 0.0156 | Grad Norm: 0.00833450\n",
      "Epoch 4 | Step 2490200 | Avg Loss: 0.0157 | Grad Norm: 0.00968114\n",
      "Epoch 4 | Step 2490300 | Avg Loss: 0.0150 | Grad Norm: 0.00865338\n",
      "Epoch 4 | Step 2490400 | Avg Loss: 0.0148 | Grad Norm: 0.00893858\n",
      "Epoch 4 | Step 2490500 | Avg Loss: 0.0151 | Grad Norm: 0.00879463\n",
      "Epoch 4 | Step 2490600 | Avg Loss: 0.0151 | Grad Norm: 0.00970822\n",
      "Epoch 4 | Step 2490700 | Avg Loss: 0.0155 | Grad Norm: 0.00849302\n",
      "Epoch 4 | Step 2490800 | Avg Loss: 0.0161 | Grad Norm: 0.00917846\n",
      "Epoch 4 | Step 2490900 | Avg Loss: 0.0162 | Grad Norm: 0.00860427\n",
      "Epoch 4 | Step 2491000 | Avg Loss: 0.0162 | Grad Norm: 0.00839483\n",
      "Epoch 4 | Step 2491100 | Avg Loss: 0.0159 | Grad Norm: 0.00891815\n",
      "Epoch 4 | Step 2491200 | Avg Loss: 0.0156 | Grad Norm: 0.00865703\n",
      "Epoch 4 | Step 2491300 | Avg Loss: 0.0154 | Grad Norm: 0.00834853\n",
      "Epoch 4 | Step 2491400 | Avg Loss: 0.0154 | Grad Norm: 0.00872698\n",
      "Epoch 4 | Step 2491500 | Avg Loss: 0.0154 | Grad Norm: 0.00878857\n",
      "Epoch 4 | Step 2491600 | Avg Loss: 0.0154 | Grad Norm: 0.00986345\n",
      "Epoch 4 | Step 2491700 | Avg Loss: 0.0155 | Grad Norm: 0.00906265\n",
      "Epoch 4 | Step 2491800 | Avg Loss: 0.0153 | Grad Norm: 0.00934095\n",
      "Epoch 4 | Step 2491900 | Avg Loss: 0.0152 | Grad Norm: 0.00840231\n",
      "Epoch 4 | Step 2492000 | Avg Loss: 0.0154 | Grad Norm: 0.01226352\n",
      "Epoch 4 | Step 2492100 | Avg Loss: 0.0152 | Grad Norm: 0.00909954\n",
      "Epoch 4 | Step 2492200 | Avg Loss: 0.0153 | Grad Norm: 0.00926306\n",
      "Epoch 4 | Step 2492300 | Avg Loss: 0.0158 | Grad Norm: 0.00946702\n",
      "Epoch 4 | Step 2492400 | Avg Loss: 0.0160 | Grad Norm: 0.01265876\n",
      "Epoch 4 | Step 2492500 | Avg Loss: 0.0159 | Grad Norm: 0.01245534\n",
      "Epoch 4 | Step 2492600 | Avg Loss: 0.0156 | Grad Norm: 0.00763127\n",
      "Epoch 4 | Step 2492700 | Avg Loss: 0.0156 | Grad Norm: 0.00835598\n",
      "Epoch 4 | Step 2492800 | Avg Loss: 0.0152 | Grad Norm: 0.00931024\n",
      "Epoch 4 | Step 2492900 | Avg Loss: 0.0154 | Grad Norm: 0.00876542\n",
      "Epoch 4 | Step 2493000 | Avg Loss: 0.0154 | Grad Norm: 0.00847632\n",
      "Epoch 4 | Step 2493100 | Avg Loss: 0.0157 | Grad Norm: 0.00867461\n",
      "Epoch 4 | Step 2493200 | Avg Loss: 0.0157 | Grad Norm: 0.00985258\n",
      "Epoch 4 | Step 2493300 | Avg Loss: 0.0153 | Grad Norm: 0.00851519\n",
      "Epoch 4 | Step 2493400 | Avg Loss: 0.0154 | Grad Norm: 0.00991493\n",
      "Epoch 4 | Step 2493500 | Avg Loss: 0.0154 | Grad Norm: 0.01149260\n",
      "Epoch 4 | Step 2493600 | Avg Loss: 0.0150 | Grad Norm: 0.01013788\n",
      "Epoch 4 | Step 2493700 | Avg Loss: 0.0150 | Grad Norm: 0.00815604\n",
      "Epoch 4 | Step 2493800 | Avg Loss: 0.0150 | Grad Norm: 0.00893933\n",
      "Epoch 4 | Step 2493900 | Avg Loss: 0.0151 | Grad Norm: 0.00901418\n",
      "Epoch 4 | Step 2494000 | Avg Loss: 0.0152 | Grad Norm: 0.00815389\n",
      "Epoch 4 | Step 2494100 | Avg Loss: 0.0155 | Grad Norm: 0.00758173\n",
      "Epoch 4 | Step 2494200 | Avg Loss: 0.0156 | Grad Norm: 0.00803152\n",
      "Epoch 4 | Step 2494300 | Avg Loss: 0.0155 | Grad Norm: 0.00897343\n",
      "Epoch 4 | Step 2494400 | Avg Loss: 0.0152 | Grad Norm: 0.00886717\n",
      "Epoch 4 | Step 2494500 | Avg Loss: 0.0151 | Grad Norm: 0.00994908\n",
      "Epoch 4 | Step 2494600 | Avg Loss: 0.0150 | Grad Norm: 0.00900819\n",
      "Epoch 4 | Step 2494700 | Avg Loss: 0.0147 | Grad Norm: 0.00925189\n",
      "Epoch 4 | Step 2494800 | Avg Loss: 0.0150 | Grad Norm: 0.00928189\n",
      "Epoch 4 | Step 2494900 | Avg Loss: 0.0149 | Grad Norm: 0.01046046\n",
      "Epoch 4 | Step 2495000 | Avg Loss: 0.0146 | Grad Norm: 0.00866257\n",
      "Epoch 4 | Step 2495100 | Avg Loss: 0.0149 | Grad Norm: 0.00785759\n",
      "Epoch 4 | Step 2495200 | Avg Loss: 0.0148 | Grad Norm: 0.01039883\n",
      "Epoch 4 | Step 2495300 | Avg Loss: 0.0148 | Grad Norm: 0.00906351\n",
      "Epoch 4 | Step 2495400 | Avg Loss: 0.0148 | Grad Norm: 0.00814482\n",
      "Epoch 4 | Step 2495500 | Avg Loss: 0.0149 | Grad Norm: 0.00787385\n",
      "Epoch 4 | Step 2495600 | Avg Loss: 0.0152 | Grad Norm: 0.00868285\n",
      "Epoch 4 | Step 2495700 | Avg Loss: 0.0150 | Grad Norm: 0.00993531\n",
      "Epoch 4 | Step 2495800 | Avg Loss: 0.0149 | Grad Norm: 0.00827661\n",
      "Epoch 4 | Step 2495900 | Avg Loss: 0.0151 | Grad Norm: 0.00919950\n",
      "Epoch 4 | Step 2496000 | Avg Loss: 0.0151 | Grad Norm: 0.00929470\n",
      "Epoch 4 | Step 2496100 | Avg Loss: 0.0156 | Grad Norm: 0.01129847\n",
      "Epoch 4 | Step 2496200 | Avg Loss: 0.0155 | Grad Norm: 0.00892391\n",
      "Epoch 4 | Step 2496300 | Avg Loss: 0.0154 | Grad Norm: 0.00757584\n",
      "Epoch 4 | Step 2496400 | Avg Loss: 0.0153 | Grad Norm: 0.00829218\n",
      "Epoch 4 | Step 2496500 | Avg Loss: 0.0157 | Grad Norm: 0.00969415\n",
      "Epoch 4 | Step 2496600 | Avg Loss: 0.0155 | Grad Norm: 0.00940334\n",
      "Epoch 4 | Step 2496700 | Avg Loss: 0.0154 | Grad Norm: 0.00859096\n",
      "Epoch 4 | Step 2496800 | Avg Loss: 0.0150 | Grad Norm: 0.00919511\n",
      "Epoch 4 | Step 2496900 | Avg Loss: 0.0150 | Grad Norm: 0.00848947\n",
      "Epoch 4 | Step 2497000 | Avg Loss: 0.0149 | Grad Norm: 0.01002547\n",
      "Epoch 4 | Step 2497100 | Avg Loss: 0.0150 | Grad Norm: 0.01146719\n",
      "Epoch 4 | Step 2497200 | Avg Loss: 0.0157 | Grad Norm: 0.00954949\n",
      "Epoch 4 | Step 2497300 | Avg Loss: 0.0154 | Grad Norm: 0.00942989\n",
      "Epoch 4 | Step 2497400 | Avg Loss: 0.0149 | Grad Norm: 0.00821562\n",
      "Epoch 4 | Step 2497500 | Avg Loss: 0.0152 | Grad Norm: 0.00815566\n",
      "Epoch 4 | Step 2497600 | Avg Loss: 0.0155 | Grad Norm: 0.00906748\n",
      "Epoch 4 | Step 2497700 | Avg Loss: 0.0158 | Grad Norm: 0.00933362\n",
      "Epoch 4 | Step 2497800 | Avg Loss: 0.0158 | Grad Norm: 0.00955167\n",
      "Epoch 4 | Step 2497900 | Avg Loss: 0.0158 | Grad Norm: 0.01136513\n",
      "Epoch 4 | Step 2498000 | Avg Loss: 0.0158 | Grad Norm: 0.00847055\n",
      "Epoch 4 | Step 2498100 | Avg Loss: 0.0153 | Grad Norm: 0.00833273\n",
      "Epoch 4 | Step 2498200 | Avg Loss: 0.0150 | Grad Norm: 0.00960452\n",
      "Epoch 4 | Step 2498300 | Avg Loss: 0.0149 | Grad Norm: 0.00861766\n",
      "Epoch 4 | Step 2498400 | Avg Loss: 0.0149 | Grad Norm: 0.00933430\n",
      "Epoch 4 | Step 2498500 | Avg Loss: 0.0152 | Grad Norm: 0.00767055\n",
      "Epoch 4 | Step 2498600 | Avg Loss: 0.0156 | Grad Norm: 0.00724800\n",
      "Epoch 4 | Step 2498700 | Avg Loss: 0.0157 | Grad Norm: 0.00886367\n",
      "Epoch 4 | Step 2498800 | Avg Loss: 0.0156 | Grad Norm: 0.00916123\n",
      "Epoch 4 | Step 2498900 | Avg Loss: 0.0155 | Grad Norm: 0.00675720\n",
      "Epoch 4 | Step 2499000 | Avg Loss: 0.0154 | Grad Norm: 0.00983382\n",
      "Epoch 4 | Step 2499100 | Avg Loss: 0.0153 | Grad Norm: 0.01057281\n",
      "Epoch 4 | Step 2499200 | Avg Loss: 0.0151 | Grad Norm: 0.00815422\n",
      "Epoch 4 | Step 2499300 | Avg Loss: 0.0150 | Grad Norm: 0.00890451\n",
      "Epoch 4 | Step 2499400 | Avg Loss: 0.0150 | Grad Norm: 0.00736901\n",
      "Epoch 4 | Step 2499500 | Avg Loss: 0.0148 | Grad Norm: 0.00881382\n",
      "Epoch 4 | Step 2499600 | Avg Loss: 0.0144 | Grad Norm: 0.00875356\n",
      "Epoch 4 | Step 2499700 | Avg Loss: 0.0144 | Grad Norm: 0.00812987\n",
      "Epoch 4 | Step 2499800 | Avg Loss: 0.0148 | Grad Norm: 0.00927409\n",
      "Epoch 4 | Step 2499900 | Avg Loss: 0.0153 | Grad Norm: 0.01070525\n",
      "Epoch 4 | Step 2500000 | Avg Loss: 0.0156 | Grad Norm: 0.00824008\n",
      "Saving model at step2500000\n",
      "Epoch 4 | Step 2500100 | Avg Loss: 0.0158 | Grad Norm: 0.00783449\n",
      "Epoch 4 | Step 2500200 | Avg Loss: 0.0158 | Grad Norm: 0.01055266\n",
      "Epoch 4 | Step 2500300 | Avg Loss: 0.0155 | Grad Norm: 0.00971332\n",
      "Epoch 4 | Step 2500400 | Avg Loss: 0.0154 | Grad Norm: 0.00960217\n",
      "Epoch 4 | Step 2500500 | Avg Loss: 0.0153 | Grad Norm: 0.00795503\n",
      "Epoch 4 | Step 2500600 | Avg Loss: 0.0149 | Grad Norm: 0.01013043\n",
      "Epoch 4 | Step 2500700 | Avg Loss: 0.0150 | Grad Norm: 0.00908068\n",
      "Epoch 4 | Step 2500800 | Avg Loss: 0.0152 | Grad Norm: 0.00823989\n",
      "Epoch 4 | Step 2500900 | Avg Loss: 0.0150 | Grad Norm: 0.00900104\n",
      "Epoch 4 | Step 2501000 | Avg Loss: 0.0152 | Grad Norm: 0.00818981\n",
      "Epoch 4 | Step 2501100 | Avg Loss: 0.0154 | Grad Norm: 0.00950498\n",
      "Epoch 4 | Step 2501200 | Avg Loss: 0.0155 | Grad Norm: 0.00961138\n",
      "Epoch 4 | Step 2501300 | Avg Loss: 0.0154 | Grad Norm: 0.01069116\n",
      "Epoch 4 | Step 2501400 | Avg Loss: 0.0153 | Grad Norm: 0.01075159\n",
      "Epoch 4 | Step 2501500 | Avg Loss: 0.0152 | Grad Norm: 0.00841294\n",
      "Epoch 4 | Step 2501600 | Avg Loss: 0.0151 | Grad Norm: 0.00967516\n",
      "Epoch 4 | Step 2501700 | Avg Loss: 0.0150 | Grad Norm: 0.00843072\n",
      "Epoch 4 | Step 2501800 | Avg Loss: 0.0155 | Grad Norm: 0.00864392\n",
      "Epoch 4 | Step 2501900 | Avg Loss: 0.0153 | Grad Norm: 0.01010733\n",
      "Epoch 4 | Step 2502000 | Avg Loss: 0.0154 | Grad Norm: 0.00790782\n",
      "Epoch 4 | Step 2502100 | Avg Loss: 0.0153 | Grad Norm: 0.00871023\n",
      "Epoch 4 | Step 2502200 | Avg Loss: 0.0157 | Grad Norm: 0.00817895\n",
      "Epoch 4 | Step 2502300 | Avg Loss: 0.0153 | Grad Norm: 0.00917380\n",
      "Epoch 4 | Step 2502400 | Avg Loss: 0.0156 | Grad Norm: 0.00960886\n",
      "Epoch 4 | Step 2502500 | Avg Loss: 0.0155 | Grad Norm: 0.00889245\n",
      "Epoch 4 | Step 2502600 | Avg Loss: 0.0156 | Grad Norm: 0.00841148\n",
      "Epoch 4 | Step 2502700 | Avg Loss: 0.0155 | Grad Norm: 0.00867246\n",
      "Epoch 4 | Step 2502800 | Avg Loss: 0.0157 | Grad Norm: 0.00960830\n",
      "Epoch 4 | Step 2502900 | Avg Loss: 0.0156 | Grad Norm: 0.00890904\n",
      "Epoch 4 | Step 2503000 | Avg Loss: 0.0156 | Grad Norm: 0.00892192\n",
      "Epoch 4 | Step 2503100 | Avg Loss: 0.0157 | Grad Norm: 0.00827635\n",
      "Epoch 4 | Step 2503200 | Avg Loss: 0.0159 | Grad Norm: 0.00930970\n",
      "Epoch 4 | Step 2503300 | Avg Loss: 0.0160 | Grad Norm: 0.00810636\n",
      "Epoch 4 | Step 2503400 | Avg Loss: 0.0156 | Grad Norm: 0.00921881\n",
      "Epoch 4 | Step 2503500 | Avg Loss: 0.0159 | Grad Norm: 0.01095970\n",
      "Epoch 4 | Step 2503600 | Avg Loss: 0.0160 | Grad Norm: 0.00801485\n",
      "Epoch 4 | Step 2503700 | Avg Loss: 0.0156 | Grad Norm: 0.00872563\n",
      "Epoch 4 | Step 2503800 | Avg Loss: 0.0154 | Grad Norm: 0.01054677\n",
      "Epoch 4 | Step 2503900 | Avg Loss: 0.0155 | Grad Norm: 0.00927639\n",
      "Epoch 4 | Step 2504000 | Avg Loss: 0.0155 | Grad Norm: 0.00843393\n",
      "Epoch 4 | Step 2504100 | Avg Loss: 0.0153 | Grad Norm: 0.00902668\n",
      "Epoch 4 | Step 2504200 | Avg Loss: 0.0153 | Grad Norm: 0.00811381\n",
      "Epoch 4 | Step 2504300 | Avg Loss: 0.0155 | Grad Norm: 0.00831377\n",
      "Epoch 4 | Step 2504400 | Avg Loss: 0.0154 | Grad Norm: 0.00835970\n",
      "Epoch 4 | Step 2504500 | Avg Loss: 0.0152 | Grad Norm: 0.00959914\n",
      "Epoch 4 | Step 2504600 | Avg Loss: 0.0153 | Grad Norm: 0.00930869\n",
      "Epoch 4 | Step 2504700 | Avg Loss: 0.0150 | Grad Norm: 0.00805975\n",
      "Epoch 4 | Step 2504800 | Avg Loss: 0.0151 | Grad Norm: 0.00751121\n",
      "Epoch 4 | Step 2504900 | Avg Loss: 0.0154 | Grad Norm: 0.00809293\n",
      "Epoch 4 | Step 2505000 | Avg Loss: 0.0152 | Grad Norm: 0.00800359\n",
      "Epoch 4 | Step 2505100 | Avg Loss: 0.0154 | Grad Norm: 0.00847207\n",
      "Epoch 4 | Step 2505200 | Avg Loss: 0.0151 | Grad Norm: 0.00906129\n",
      "Epoch 4 | Step 2505300 | Avg Loss: 0.0153 | Grad Norm: 0.00867687\n",
      "Epoch 4 | Step 2505400 | Avg Loss: 0.0151 | Grad Norm: 0.00892125\n",
      "Epoch 4 | Step 2505500 | Avg Loss: 0.0156 | Grad Norm: 0.01031074\n",
      "Epoch 4 | Step 2505600 | Avg Loss: 0.0152 | Grad Norm: 0.00766278\n",
      "Epoch 4 | Step 2505700 | Avg Loss: 0.0148 | Grad Norm: 0.00946195\n",
      "Epoch 4 | Step 2505800 | Avg Loss: 0.0150 | Grad Norm: 0.01041758\n",
      "Epoch 4 | Step 2505900 | Avg Loss: 0.0155 | Grad Norm: 0.00961325\n",
      "Epoch 4 | Step 2506000 | Avg Loss: 0.0154 | Grad Norm: 0.00918096\n",
      "Epoch 4 | Step 2506100 | Avg Loss: 0.0153 | Grad Norm: 0.01019005\n",
      "Epoch 4 | Step 2506200 | Avg Loss: 0.0149 | Grad Norm: 0.00768484\n",
      "Epoch 4 | Step 2506300 | Avg Loss: 0.0150 | Grad Norm: 0.00819761\n",
      "Epoch 4 | Step 2506400 | Avg Loss: 0.0153 | Grad Norm: 0.00959275\n",
      "Epoch 4 | Step 2506500 | Avg Loss: 0.0155 | Grad Norm: 0.01024562\n",
      "Epoch 4 | Step 2506600 | Avg Loss: 0.0160 | Grad Norm: 0.01221657\n",
      "Epoch 4 | Step 2506700 | Avg Loss: 0.0155 | Grad Norm: 0.00804426\n",
      "Epoch 4 | Step 2506800 | Avg Loss: 0.0157 | Grad Norm: 0.00933577\n",
      "Epoch 4 | Step 2506900 | Avg Loss: 0.0158 | Grad Norm: 0.00937388\n",
      "Epoch 4 | Step 2507000 | Avg Loss: 0.0156 | Grad Norm: 0.01011312\n",
      "Epoch 4 | Step 2507100 | Avg Loss: 0.0156 | Grad Norm: 0.00935179\n",
      "Epoch 4 | Step 2507200 | Avg Loss: 0.0159 | Grad Norm: 0.00883593\n",
      "Epoch 4 | Step 2507300 | Avg Loss: 0.0158 | Grad Norm: 0.00823362\n",
      "Epoch 4 | Step 2507400 | Avg Loss: 0.0156 | Grad Norm: 0.00830324\n",
      "Epoch 4 | Step 2507500 | Avg Loss: 0.0154 | Grad Norm: 0.00923744\n",
      "Epoch 4 | Step 2507600 | Avg Loss: 0.0156 | Grad Norm: 0.01131290\n",
      "Epoch 4 | Step 2507700 | Avg Loss: 0.0158 | Grad Norm: 0.00882391\n",
      "Epoch 4 | Step 2507800 | Avg Loss: 0.0156 | Grad Norm: 0.00799662\n",
      "Epoch 4 | Step 2507900 | Avg Loss: 0.0159 | Grad Norm: 0.01013020\n",
      "Epoch 4 | Step 2508000 | Avg Loss: 0.0163 | Grad Norm: 0.00924492\n",
      "Epoch 4 | Step 2508100 | Avg Loss: 0.0160 | Grad Norm: 0.00953834\n",
      "Epoch 4 | Step 2508200 | Avg Loss: 0.0156 | Grad Norm: 0.00837069\n",
      "Epoch 4 | Step 2508300 | Avg Loss: 0.0154 | Grad Norm: 0.00999182\n",
      "Epoch 4 | Step 2508400 | Avg Loss: 0.0155 | Grad Norm: 0.00911668\n",
      "Epoch 4 | Step 2508500 | Avg Loss: 0.0151 | Grad Norm: 0.00905750\n",
      "Epoch 4 | Step 2508600 | Avg Loss: 0.0153 | Grad Norm: 0.00845347\n",
      "Epoch 4 | Step 2508700 | Avg Loss: 0.0156 | Grad Norm: 0.00925791\n",
      "Epoch 4 | Step 2508800 | Avg Loss: 0.0153 | Grad Norm: 0.00978764\n",
      "Epoch 4 | Step 2508900 | Avg Loss: 0.0151 | Grad Norm: 0.00810302\n",
      "Epoch 4 | Step 2509000 | Avg Loss: 0.0152 | Grad Norm: 0.00948654\n",
      "Epoch 4 | Step 2509100 | Avg Loss: 0.0157 | Grad Norm: 0.00776093\n",
      "Epoch 4 | Step 2509200 | Avg Loss: 0.0157 | Grad Norm: 0.00833363\n",
      "Epoch 4 | Step 2509300 | Avg Loss: 0.0154 | Grad Norm: 0.00804390\n",
      "Epoch 4 | Step 2509400 | Avg Loss: 0.0156 | Grad Norm: 0.00851263\n",
      "Epoch 4 | Step 2509500 | Avg Loss: 0.0156 | Grad Norm: 0.00874012\n",
      "Epoch 4 | Step 2509600 | Avg Loss: 0.0152 | Grad Norm: 0.00833203\n",
      "Epoch 4 | Step 2509700 | Avg Loss: 0.0153 | Grad Norm: 0.01060465\n",
      "Epoch 4 | Step 2509800 | Avg Loss: 0.0152 | Grad Norm: 0.00798367\n",
      "Epoch 4 | Step 2509900 | Avg Loss: 0.0150 | Grad Norm: 0.00934831\n",
      "Epoch 4 | Step 2510000 | Avg Loss: 0.0156 | Grad Norm: 0.00855508\n",
      "Epoch 4 | Step 2510100 | Avg Loss: 0.0159 | Grad Norm: 0.00945378\n",
      "Epoch 4 | Step 2510200 | Avg Loss: 0.0162 | Grad Norm: 0.00896483\n",
      "Epoch 4 | Step 2510300 | Avg Loss: 0.0160 | Grad Norm: 0.01045745\n",
      "Epoch 4 | Step 2510400 | Avg Loss: 0.0158 | Grad Norm: 0.00880932\n",
      "Epoch 4 | Step 2510500 | Avg Loss: 0.0161 | Grad Norm: 0.00929377\n",
      "Epoch 4 | Step 2510600 | Avg Loss: 0.0157 | Grad Norm: 0.00968418\n",
      "Epoch 4 | Step 2510700 | Avg Loss: 0.0157 | Grad Norm: 0.00888416\n",
      "Epoch 4 | Step 2510800 | Avg Loss: 0.0156 | Grad Norm: 0.00842647\n",
      "Epoch 4 | Step 2510900 | Avg Loss: 0.0160 | Grad Norm: 0.00843427\n",
      "Epoch 4 | Step 2511000 | Avg Loss: 0.0165 | Grad Norm: 0.01063277\n",
      "Epoch 4 | Step 2511100 | Avg Loss: 0.0163 | Grad Norm: 0.00910606\n",
      "Epoch 4 | Step 2511200 | Avg Loss: 0.0158 | Grad Norm: 0.01010523\n",
      "Epoch 4 | Step 2511300 | Avg Loss: 0.0162 | Grad Norm: 0.00873858\n",
      "Epoch 4 | Step 2511400 | Avg Loss: 0.0162 | Grad Norm: 0.00885158\n",
      "Epoch 4 | Step 2511500 | Avg Loss: 0.0162 | Grad Norm: 0.00914467\n",
      "Epoch 4 | Step 2511600 | Avg Loss: 0.0160 | Grad Norm: 0.00790852\n",
      "Epoch 4 | Step 2511700 | Avg Loss: 0.0156 | Grad Norm: 0.00959929\n",
      "Epoch 4 | Step 2511800 | Avg Loss: 0.0158 | Grad Norm: 0.01047574\n",
      "Epoch 4 | Step 2511900 | Avg Loss: 0.0159 | Grad Norm: 0.00885744\n",
      "Epoch 4 | Step 2512000 | Avg Loss: 0.0161 | Grad Norm: 0.01007314\n",
      "Epoch 4 | Step 2512100 | Avg Loss: 0.0159 | Grad Norm: 0.00866929\n",
      "Epoch 4 | Step 2512200 | Avg Loss: 0.0155 | Grad Norm: 0.00848850\n",
      "Epoch 4 | Step 2512300 | Avg Loss: 0.0158 | Grad Norm: 0.00984253\n",
      "Epoch 4 | Step 2512400 | Avg Loss: 0.0158 | Grad Norm: 0.00935704\n",
      "Epoch 4 | Step 2512500 | Avg Loss: 0.0156 | Grad Norm: 0.00796275\n",
      "Epoch 4 | Step 2512600 | Avg Loss: 0.0155 | Grad Norm: 0.00886521\n",
      "Epoch 4 | Step 2512700 | Avg Loss: 0.0154 | Grad Norm: 0.00974565\n",
      "Epoch 4 | Step 2512800 | Avg Loss: 0.0150 | Grad Norm: 0.00819856\n",
      "Epoch 4 | Step 2512900 | Avg Loss: 0.0149 | Grad Norm: 0.00855948\n",
      "Epoch 4 | Step 2513000 | Avg Loss: 0.0151 | Grad Norm: 0.00874763\n",
      "Epoch 4 | Step 2513100 | Avg Loss: 0.0151 | Grad Norm: 0.00912222\n",
      "Epoch 4 | Step 2513200 | Avg Loss: 0.0149 | Grad Norm: 0.00846002\n",
      "Epoch 4 | Step 2513300 | Avg Loss: 0.0150 | Grad Norm: 0.00965636\n",
      "Epoch 4 | Step 2513400 | Avg Loss: 0.0152 | Grad Norm: 0.00886493\n",
      "Epoch 4 | Step 2513500 | Avg Loss: 0.0152 | Grad Norm: 0.01486965\n",
      "Epoch 4 | Step 2513600 | Avg Loss: 0.0152 | Grad Norm: 0.00860700\n",
      "Epoch 4 | Step 2513700 | Avg Loss: 0.0151 | Grad Norm: 0.00864764\n",
      "Epoch 4 | Step 2513800 | Avg Loss: 0.0152 | Grad Norm: 0.00776400\n",
      "Epoch 4 | Step 2513900 | Avg Loss: 0.0149 | Grad Norm: 0.00918500\n",
      "Epoch 4 | Step 2514000 | Avg Loss: 0.0152 | Grad Norm: 0.00953328\n",
      "Epoch 4 | Step 2514100 | Avg Loss: 0.0155 | Grad Norm: 0.00855444\n",
      "Epoch 4 | Step 2514200 | Avg Loss: 0.0155 | Grad Norm: 0.00813385\n",
      "Epoch 4 | Step 2514300 | Avg Loss: 0.0156 | Grad Norm: 0.01000292\n",
      "Epoch 4 | Step 2514400 | Avg Loss: 0.0155 | Grad Norm: 0.00896659\n",
      "Epoch 4 | Step 2514500 | Avg Loss: 0.0155 | Grad Norm: 0.00791065\n",
      "Epoch 4 | Step 2514600 | Avg Loss: 0.0155 | Grad Norm: 0.01437604\n",
      "Epoch 4 | Step 2514700 | Avg Loss: 0.0153 | Grad Norm: 0.00970634\n",
      "Epoch 4 | Step 2514800 | Avg Loss: 0.0150 | Grad Norm: 0.01043515\n",
      "Epoch 4 | Step 2514900 | Avg Loss: 0.0155 | Grad Norm: 0.00950654\n",
      "Epoch 4 | Step 2515000 | Avg Loss: 0.0156 | Grad Norm: 0.00932330\n",
      "Epoch 4 | Step 2515100 | Avg Loss: 0.0156 | Grad Norm: 0.01060394\n",
      "Epoch 4 | Step 2515200 | Avg Loss: 0.0156 | Grad Norm: 0.00946426\n",
      "Epoch 4 | Step 2515300 | Avg Loss: 0.0158 | Grad Norm: 0.00837283\n",
      "Epoch 4 | Step 2515400 | Avg Loss: 0.0155 | Grad Norm: 0.00934840\n",
      "Epoch 4 | Step 2515500 | Avg Loss: 0.0156 | Grad Norm: 0.00838806\n",
      "Epoch 4 | Step 2515600 | Avg Loss: 0.0156 | Grad Norm: 0.00922963\n",
      "Epoch 4 | Step 2515700 | Avg Loss: 0.0159 | Grad Norm: 0.00820641\n",
      "Epoch 4 | Step 2515800 | Avg Loss: 0.0156 | Grad Norm: 0.00778908\n",
      "Epoch 4 | Step 2515900 | Avg Loss: 0.0155 | Grad Norm: 0.00901120\n",
      "Epoch 4 | Step 2516000 | Avg Loss: 0.0153 | Grad Norm: 0.00871658\n",
      "Epoch 4 | Step 2516100 | Avg Loss: 0.0152 | Grad Norm: 0.01087471\n",
      "Epoch 4 | Step 2516200 | Avg Loss: 0.0151 | Grad Norm: 0.00755915\n",
      "Epoch 4 | Step 2516300 | Avg Loss: 0.0151 | Grad Norm: 0.00854026\n",
      "Epoch 4 | Step 2516400 | Avg Loss: 0.0148 | Grad Norm: 0.00905430\n",
      "Epoch 4 | Step 2516500 | Avg Loss: 0.0148 | Grad Norm: 0.00844839\n",
      "Epoch 4 | Step 2516600 | Avg Loss: 0.0147 | Grad Norm: 0.00872762\n",
      "Epoch 4 | Step 2516700 | Avg Loss: 0.0149 | Grad Norm: 0.00874848\n",
      "Epoch 4 | Step 2516800 | Avg Loss: 0.0149 | Grad Norm: 0.00946576\n",
      "Epoch 4 | Step 2516900 | Avg Loss: 0.0154 | Grad Norm: 0.00897252\n",
      "Epoch 4 | Step 2517000 | Avg Loss: 0.0157 | Grad Norm: 0.00823815\n",
      "Epoch 4 | Step 2517100 | Avg Loss: 0.0157 | Grad Norm: 0.00863889\n",
      "Epoch 4 | Step 2517200 | Avg Loss: 0.0159 | Grad Norm: 0.00827380\n",
      "Epoch 4 | Step 2517300 | Avg Loss: 0.0158 | Grad Norm: 0.00834692\n",
      "Epoch 4 | Step 2517400 | Avg Loss: 0.0155 | Grad Norm: 0.00984731\n",
      "Epoch 4 | Step 2517500 | Avg Loss: 0.0158 | Grad Norm: 0.01008826\n",
      "Epoch 4 | Step 2517600 | Avg Loss: 0.0157 | Grad Norm: 0.00947067\n",
      "Epoch 4 | Step 2517700 | Avg Loss: 0.0161 | Grad Norm: 0.00926045\n",
      "Epoch 4 | Step 2517800 | Avg Loss: 0.0160 | Grad Norm: 0.01084188\n",
      "Epoch 4 | Step 2517900 | Avg Loss: 0.0160 | Grad Norm: 0.00791414\n",
      "Epoch 4 | Step 2518000 | Avg Loss: 0.0156 | Grad Norm: 0.01039321\n",
      "Epoch 4 | Step 2518100 | Avg Loss: 0.0157 | Grad Norm: 0.01024882\n",
      "Epoch 4 | Step 2518200 | Avg Loss: 0.0159 | Grad Norm: 0.01090285\n",
      "Epoch 4 | Step 2518300 | Avg Loss: 0.0158 | Grad Norm: 0.00844075\n",
      "Epoch 4 | Step 2518400 | Avg Loss: 0.0157 | Grad Norm: 0.00840063\n",
      "Epoch 4 | Step 2518500 | Avg Loss: 0.0160 | Grad Norm: 0.00880696\n",
      "Epoch 4 | Step 2518600 | Avg Loss: 0.0159 | Grad Norm: 0.00826537\n",
      "Epoch 4 | Step 2518700 | Avg Loss: 0.0159 | Grad Norm: 0.00907808\n",
      "Epoch 4 | Step 2518800 | Avg Loss: 0.0158 | Grad Norm: 0.00929462\n",
      "Epoch 4 | Step 2518900 | Avg Loss: 0.0156 | Grad Norm: 0.00842881\n",
      "Epoch 4 | Step 2519000 | Avg Loss: 0.0153 | Grad Norm: 0.00871583\n",
      "Epoch 4 | Step 2519100 | Avg Loss: 0.0153 | Grad Norm: 0.00840570\n",
      "Epoch 4 | Step 2519200 | Avg Loss: 0.0152 | Grad Norm: 0.00923785\n",
      "Epoch 4 | Step 2519300 | Avg Loss: 0.0154 | Grad Norm: 0.00848481\n",
      "Epoch 4 | Step 2519400 | Avg Loss: 0.0156 | Grad Norm: 0.00879843\n",
      "Epoch 4 | Step 2519500 | Avg Loss: 0.0158 | Grad Norm: 0.00853920\n",
      "Epoch 4 | Step 2519600 | Avg Loss: 0.0158 | Grad Norm: 0.01011739\n",
      "Epoch 4 | Step 2519700 | Avg Loss: 0.0156 | Grad Norm: 0.00846805\n",
      "Epoch 4 | Step 2519800 | Avg Loss: 0.0154 | Grad Norm: 0.00928987\n",
      "Epoch 4 | Step 2519900 | Avg Loss: 0.0154 | Grad Norm: 0.00855620\n",
      "Epoch 4 | Step 2520000 | Avg Loss: 0.0153 | Grad Norm: 0.00869134\n",
      "Epoch 4 | Step 2520100 | Avg Loss: 0.0152 | Grad Norm: 0.00958868\n",
      "Epoch 4 | Step 2520200 | Avg Loss: 0.0154 | Grad Norm: 0.00899607\n",
      "Epoch 4 | Step 2520300 | Avg Loss: 0.0156 | Grad Norm: 0.00894125\n",
      "Epoch 4 | Step 2520400 | Avg Loss: 0.0154 | Grad Norm: 0.01001113\n",
      "Epoch 4 | Step 2520500 | Avg Loss: 0.0158 | Grad Norm: 0.00859186\n",
      "Epoch 4 | Step 2520600 | Avg Loss: 0.0159 | Grad Norm: 0.00847373\n",
      "Epoch 4 | Step 2520700 | Avg Loss: 0.0161 | Grad Norm: 0.00996393\n",
      "Epoch 4 | Step 2520800 | Avg Loss: 0.0156 | Grad Norm: 0.00896542\n",
      "Epoch 4 | Step 2520900 | Avg Loss: 0.0156 | Grad Norm: 0.00880259\n",
      "Epoch 4 | Step 2521000 | Avg Loss: 0.0158 | Grad Norm: 0.00806102\n",
      "Epoch 4 | Step 2521100 | Avg Loss: 0.0159 | Grad Norm: 0.00881476\n",
      "Epoch 4 | Step 2521200 | Avg Loss: 0.0160 | Grad Norm: 0.00950144\n",
      "Epoch 4 | Step 2521300 | Avg Loss: 0.0157 | Grad Norm: 0.00772814\n",
      "Epoch 4 | Step 2521400 | Avg Loss: 0.0154 | Grad Norm: 0.00832593\n",
      "Epoch 4 | Step 2521500 | Avg Loss: 0.0152 | Grad Norm: 0.00772445\n",
      "Epoch 4 | Step 2521600 | Avg Loss: 0.0152 | Grad Norm: 0.00916847\n",
      "Epoch 4 | Step 2521700 | Avg Loss: 0.0154 | Grad Norm: 0.00984339\n",
      "Epoch 4 | Step 2521800 | Avg Loss: 0.0158 | Grad Norm: 0.00904350\n",
      "Epoch 4 | Step 2521900 | Avg Loss: 0.0157 | Grad Norm: 0.01004447\n",
      "Epoch 4 | Step 2522000 | Avg Loss: 0.0156 | Grad Norm: 0.00874142\n",
      "Epoch 4 | Step 2522100 | Avg Loss: 0.0154 | Grad Norm: 0.00826149\n",
      "Epoch 4 | Step 2522200 | Avg Loss: 0.0153 | Grad Norm: 0.01084667\n",
      "Epoch 4 | Step 2522300 | Avg Loss: 0.0152 | Grad Norm: 0.00883617\n",
      "Epoch 4 | Step 2522400 | Avg Loss: 0.0152 | Grad Norm: 0.00870581\n",
      "Epoch 4 | Step 2522500 | Avg Loss: 0.0158 | Grad Norm: 0.00839365\n",
      "Epoch 4 | Step 2522600 | Avg Loss: 0.0156 | Grad Norm: 0.00825504\n",
      "Epoch 4 | Step 2522700 | Avg Loss: 0.0159 | Grad Norm: 0.00833015\n",
      "Epoch 4 | Step 2522800 | Avg Loss: 0.0158 | Grad Norm: 0.00996295\n",
      "Epoch 4 | Step 2522900 | Avg Loss: 0.0159 | Grad Norm: 0.00858404\n",
      "Epoch 4 | Step 2523000 | Avg Loss: 0.0162 | Grad Norm: 0.00949150\n",
      "Epoch 4 | Step 2523100 | Avg Loss: 0.0159 | Grad Norm: 0.00882940\n",
      "Epoch 4 | Step 2523200 | Avg Loss: 0.0155 | Grad Norm: 0.00957848\n",
      "Epoch 4 | Step 2523300 | Avg Loss: 0.0160 | Grad Norm: 0.01053928\n",
      "Epoch 4 | Step 2523400 | Avg Loss: 0.0161 | Grad Norm: 0.00877676\n",
      "Epoch 4 | Step 2523500 | Avg Loss: 0.0160 | Grad Norm: 0.00936507\n",
      "Epoch 4 | Step 2523600 | Avg Loss: 0.0160 | Grad Norm: 0.00929645\n",
      "Epoch 4 | Step 2523700 | Avg Loss: 0.0159 | Grad Norm: 0.00936812\n",
      "Epoch 4 | Step 2523800 | Avg Loss: 0.0158 | Grad Norm: 0.00837447\n",
      "Epoch 4 | Step 2523900 | Avg Loss: 0.0158 | Grad Norm: 0.00946111\n",
      "Epoch 4 | Step 2524000 | Avg Loss: 0.0161 | Grad Norm: 0.00926538\n",
      "Epoch 4 | Step 2524100 | Avg Loss: 0.0162 | Grad Norm: 0.00910347\n",
      "Epoch 4 | Step 2524200 | Avg Loss: 0.0161 | Grad Norm: 0.00914030\n",
      "Epoch 4 | Step 2524300 | Avg Loss: 0.0161 | Grad Norm: 0.00918793\n",
      "Epoch 4 | Step 2524400 | Avg Loss: 0.0159 | Grad Norm: 0.00901775\n",
      "Epoch 4 | Step 2524500 | Avg Loss: 0.0157 | Grad Norm: 0.00839907\n",
      "Epoch 4 | Step 2524600 | Avg Loss: 0.0157 | Grad Norm: 0.00933393\n",
      "Epoch 4 | Step 2524700 | Avg Loss: 0.0157 | Grad Norm: 0.00824207\n",
      "Epoch 4 | Step 2524800 | Avg Loss: 0.0154 | Grad Norm: 0.00853249\n",
      "Epoch 4 | Step 2524900 | Avg Loss: 0.0160 | Grad Norm: 0.00899717\n",
      "Epoch 4 | Step 2525000 | Avg Loss: 0.0162 | Grad Norm: 0.00905973\n",
      "Epoch 4 | Step 2525100 | Avg Loss: 0.0160 | Grad Norm: 0.00906564\n",
      "Epoch 4 | Step 2525200 | Avg Loss: 0.0159 | Grad Norm: 0.00972784\n",
      "Epoch 4 | Step 2525300 | Avg Loss: 0.0156 | Grad Norm: 0.00945406\n",
      "Epoch 4 | Step 2525400 | Avg Loss: 0.0159 | Grad Norm: 0.00906183\n",
      "Epoch 4 | Step 2525500 | Avg Loss: 0.0157 | Grad Norm: 0.00924172\n",
      "Epoch 4 | Step 2525600 | Avg Loss: 0.0155 | Grad Norm: 0.01016418\n",
      "Epoch 4 | Step 2525700 | Avg Loss: 0.0155 | Grad Norm: 0.00795994\n",
      "Epoch 4 | Step 2525800 | Avg Loss: 0.0153 | Grad Norm: 0.00892982\n",
      "Epoch 4 | Step 2525900 | Avg Loss: 0.0154 | Grad Norm: 0.00957146\n",
      "Epoch 4 | Step 2526000 | Avg Loss: 0.0153 | Grad Norm: 0.00890478\n",
      "Epoch 4 | Step 2526100 | Avg Loss: 0.0157 | Grad Norm: 0.00885161\n",
      "Epoch 4 | Step 2526200 | Avg Loss: 0.0155 | Grad Norm: 0.00816253\n",
      "Epoch 4 | Step 2526300 | Avg Loss: 0.0152 | Grad Norm: 0.00926839\n",
      "Epoch 4 | Step 2526400 | Avg Loss: 0.0152 | Grad Norm: 0.00867318\n",
      "Epoch 4 | Step 2526500 | Avg Loss: 0.0149 | Grad Norm: 0.00936312\n",
      "Epoch 4 | Step 2526600 | Avg Loss: 0.0149 | Grad Norm: 0.00884703\n",
      "Epoch 4 | Step 2526700 | Avg Loss: 0.0150 | Grad Norm: 0.00816381\n",
      "Epoch 4 | Step 2526800 | Avg Loss: 0.0152 | Grad Norm: 0.00868367\n",
      "Epoch 4 | Step 2526900 | Avg Loss: 0.0155 | Grad Norm: 0.00952974\n",
      "Epoch 4 | Step 2527000 | Avg Loss: 0.0154 | Grad Norm: 0.00837426\n",
      "Epoch 4 | Step 2527100 | Avg Loss: 0.0151 | Grad Norm: 0.00921194\n",
      "Epoch 4 | Step 2527200 | Avg Loss: 0.0153 | Grad Norm: 0.01090375\n",
      "Epoch 4 | Step 2527300 | Avg Loss: 0.0153 | Grad Norm: 0.00934960\n",
      "Epoch 4 | Step 2527400 | Avg Loss: 0.0153 | Grad Norm: 0.00940993\n",
      "Epoch 4 | Step 2527500 | Avg Loss: 0.0155 | Grad Norm: 0.00906663\n",
      "Epoch 4 | Step 2527600 | Avg Loss: 0.0157 | Grad Norm: 0.00897110\n",
      "Epoch 4 | Step 2527700 | Avg Loss: 0.0154 | Grad Norm: 0.01328206\n",
      "Epoch 4 | Step 2527800 | Avg Loss: 0.0152 | Grad Norm: 0.00913195\n",
      "Epoch 4 | Step 2527900 | Avg Loss: 0.0155 | Grad Norm: 0.01083998\n",
      "Epoch 4 | Step 2528000 | Avg Loss: 0.0156 | Grad Norm: 0.00878319\n",
      "Epoch 4 | Step 2528100 | Avg Loss: 0.0154 | Grad Norm: 0.00861023\n",
      "Epoch 4 | Step 2528200 | Avg Loss: 0.0156 | Grad Norm: 0.00851007\n",
      "Epoch 4 | Step 2528300 | Avg Loss: 0.0153 | Grad Norm: 0.00872003\n",
      "Epoch 4 | Step 2528400 | Avg Loss: 0.0156 | Grad Norm: 0.00874385\n",
      "Epoch 4 | Step 2528500 | Avg Loss: 0.0154 | Grad Norm: 0.00911685\n",
      "Epoch 4 | Step 2528600 | Avg Loss: 0.0154 | Grad Norm: 0.00860784\n",
      "Epoch 4 | Step 2528700 | Avg Loss: 0.0153 | Grad Norm: 0.00907758\n",
      "Epoch 4 | Step 2528800 | Avg Loss: 0.0154 | Grad Norm: 0.00771466\n",
      "Epoch 4 | Step 2528900 | Avg Loss: 0.0155 | Grad Norm: 0.00820541\n",
      "Epoch 4 | Step 2529000 | Avg Loss: 0.0151 | Grad Norm: 0.00856638\n",
      "Epoch 4 | Step 2529100 | Avg Loss: 0.0152 | Grad Norm: 0.00767285\n",
      "Epoch 4 | Step 2529200 | Avg Loss: 0.0151 | Grad Norm: 0.00811362\n",
      "Epoch 4 | Step 2529300 | Avg Loss: 0.0153 | Grad Norm: 0.00854167\n",
      "Epoch 4 | Step 2529400 | Avg Loss: 0.0152 | Grad Norm: 0.00954512\n",
      "Epoch 4 | Step 2529500 | Avg Loss: 0.0150 | Grad Norm: 0.00957838\n",
      "Epoch 4 | Step 2529600 | Avg Loss: 0.0150 | Grad Norm: 0.00835740\n",
      "Epoch 4 | Step 2529700 | Avg Loss: 0.0152 | Grad Norm: 0.00857038\n",
      "Epoch 4 | Step 2529800 | Avg Loss: 0.0151 | Grad Norm: 0.00873964\n",
      "Epoch 4 | Step 2529900 | Avg Loss: 0.0152 | Grad Norm: 0.00831401\n",
      "Epoch 4 | Step 2530000 | Avg Loss: 0.0151 | Grad Norm: 0.00858568\n",
      "Epoch 4 | Step 2530100 | Avg Loss: 0.0151 | Grad Norm: 0.00776817\n",
      "Epoch 4 | Step 2530200 | Avg Loss: 0.0148 | Grad Norm: 0.00811665\n",
      "Epoch 4 | Step 2530300 | Avg Loss: 0.0149 | Grad Norm: 0.00946119\n",
      "Epoch 4 | Step 2530400 | Avg Loss: 0.0152 | Grad Norm: 0.00976804\n",
      "Epoch 4 | Step 2530500 | Avg Loss: 0.0154 | Grad Norm: 0.00851700\n",
      "Epoch 4 | Step 2530600 | Avg Loss: 0.0156 | Grad Norm: 0.00907970\n",
      "Epoch 4 | Step 2530700 | Avg Loss: 0.0158 | Grad Norm: 0.00883827\n",
      "Epoch 4 | Step 2530800 | Avg Loss: 0.0156 | Grad Norm: 0.00904292\n",
      "Epoch 4 | Step 2530900 | Avg Loss: 0.0155 | Grad Norm: 0.00920309\n",
      "Epoch 4 | Step 2531000 | Avg Loss: 0.0159 | Grad Norm: 0.00937248\n",
      "Epoch 4 | Step 2531100 | Avg Loss: 0.0161 | Grad Norm: 0.00870699\n",
      "Epoch 4 | Step 2531200 | Avg Loss: 0.0159 | Grad Norm: 0.00906624\n",
      "Epoch 4 | Step 2531300 | Avg Loss: 0.0158 | Grad Norm: 0.00802308\n",
      "Epoch 4 | Step 2531400 | Avg Loss: 0.0161 | Grad Norm: 0.00785091\n",
      "Epoch 4 | Step 2531500 | Avg Loss: 0.0161 | Grad Norm: 0.00860576\n",
      "Epoch 4 | Step 2531600 | Avg Loss: 0.0163 | Grad Norm: 0.00909355\n",
      "Epoch 4 | Step 2531700 | Avg Loss: 0.0166 | Grad Norm: 0.00868506\n",
      "Epoch 4 | Step 2531800 | Avg Loss: 0.0163 | Grad Norm: 0.00973104\n",
      "Epoch 4 | Step 2531900 | Avg Loss: 0.0159 | Grad Norm: 0.00848756\n",
      "Epoch 4 | Step 2532000 | Avg Loss: 0.0159 | Grad Norm: 0.00897331\n",
      "Epoch 4 | Step 2532100 | Avg Loss: 0.0158 | Grad Norm: 0.00885190\n",
      "Epoch 4 | Step 2532200 | Avg Loss: 0.0157 | Grad Norm: 0.00899336\n",
      "Epoch 4 | Step 2532300 | Avg Loss: 0.0160 | Grad Norm: 0.00903814\n",
      "Epoch 4 | Step 2532400 | Avg Loss: 0.0158 | Grad Norm: 0.00931488\n",
      "Epoch 4 | Step 2532500 | Avg Loss: 0.0157 | Grad Norm: 0.00932490\n",
      "Epoch 4 | Step 2532600 | Avg Loss: 0.0154 | Grad Norm: 0.00833845\n",
      "Epoch 4 | Step 2532700 | Avg Loss: 0.0156 | Grad Norm: 0.00876772\n",
      "Epoch 4 | Step 2532800 | Avg Loss: 0.0158 | Grad Norm: 0.00963245\n",
      "Epoch 4 | Step 2532900 | Avg Loss: 0.0160 | Grad Norm: 0.01081268\n",
      "Epoch 4 | Step 2533000 | Avg Loss: 0.0159 | Grad Norm: 0.00779493\n",
      "Epoch 4 | Step 2533100 | Avg Loss: 0.0158 | Grad Norm: 0.00821817\n",
      "Epoch 4 | Step 2533200 | Avg Loss: 0.0156 | Grad Norm: 0.00871599\n",
      "Epoch 4 | Step 2533300 | Avg Loss: 0.0153 | Grad Norm: 0.00869786\n",
      "Epoch 4 | Step 2533400 | Avg Loss: 0.0152 | Grad Norm: 0.00813001\n",
      "Epoch 4 | Step 2533500 | Avg Loss: 0.0152 | Grad Norm: 0.00871629\n",
      "Epoch 4 | Step 2533600 | Avg Loss: 0.0152 | Grad Norm: 0.00865066\n",
      "Epoch 4 | Step 2533700 | Avg Loss: 0.0153 | Grad Norm: 0.00955477\n",
      "Epoch 4 | Step 2533800 | Avg Loss: 0.0154 | Grad Norm: 0.00834684\n",
      "Epoch 4 | Step 2533900 | Avg Loss: 0.0150 | Grad Norm: 0.00900839\n",
      "Epoch 4 | Step 2534000 | Avg Loss: 0.0152 | Grad Norm: 0.00894831\n",
      "Epoch 4 | Step 2534100 | Avg Loss: 0.0158 | Grad Norm: 0.01079179\n",
      "Epoch 4 | Step 2534200 | Avg Loss: 0.0158 | Grad Norm: 0.01161451\n",
      "Epoch 4 | Step 2534300 | Avg Loss: 0.0159 | Grad Norm: 0.00849879\n",
      "Epoch 4 | Step 2534400 | Avg Loss: 0.0158 | Grad Norm: 0.00860612\n",
      "Epoch 4 | Step 2534500 | Avg Loss: 0.0155 | Grad Norm: 0.00839904\n",
      "Epoch 4 | Step 2534600 | Avg Loss: 0.0152 | Grad Norm: 0.00836657\n",
      "Epoch 4 | Step 2534700 | Avg Loss: 0.0154 | Grad Norm: 0.00852360\n",
      "Epoch 4 | Step 2534800 | Avg Loss: 0.0155 | Grad Norm: 0.00892189\n",
      "Epoch 4 | Step 2534900 | Avg Loss: 0.0152 | Grad Norm: 0.00799914\n",
      "Epoch 4 | Step 2535000 | Avg Loss: 0.0153 | Grad Norm: 0.00870735\n",
      "Epoch 4 | Step 2535100 | Avg Loss: 0.0152 | Grad Norm: 0.00769822\n",
      "Epoch 4 | Step 2535200 | Avg Loss: 0.0157 | Grad Norm: 0.00985564\n",
      "Epoch 4 | Step 2535300 | Avg Loss: 0.0152 | Grad Norm: 0.00950674\n",
      "Epoch 4 | Step 2535400 | Avg Loss: 0.0155 | Grad Norm: 0.00717176\n",
      "Epoch 4 | Step 2535500 | Avg Loss: 0.0154 | Grad Norm: 0.00997810\n",
      "Epoch 4 | Step 2535600 | Avg Loss: 0.0155 | Grad Norm: 0.00980231\n",
      "Epoch 4 | Step 2535700 | Avg Loss: 0.0156 | Grad Norm: 0.00938683\n",
      "Epoch 4 | Step 2535800 | Avg Loss: 0.0158 | Grad Norm: 0.00877978\n",
      "Epoch 4 | Step 2535900 | Avg Loss: 0.0155 | Grad Norm: 0.00873586\n",
      "Epoch 4 | Step 2536000 | Avg Loss: 0.0154 | Grad Norm: 0.00901633\n",
      "Epoch 4 | Step 2536100 | Avg Loss: 0.0152 | Grad Norm: 0.00852633\n",
      "Epoch 4 | Step 2536200 | Avg Loss: 0.0156 | Grad Norm: 0.00867792\n",
      "Epoch 4 | Step 2536300 | Avg Loss: 0.0160 | Grad Norm: 0.00954610\n",
      "Epoch 4 | Step 2536400 | Avg Loss: 0.0156 | Grad Norm: 0.00800359\n",
      "Epoch 4 | Step 2536500 | Avg Loss: 0.0152 | Grad Norm: 0.01238963\n",
      "Epoch 4 | Step 2536600 | Avg Loss: 0.0154 | Grad Norm: 0.01171524\n",
      "Epoch 4 | Step 2536700 | Avg Loss: 0.0160 | Grad Norm: 0.00910127\n",
      "Epoch 4 | Step 2536800 | Avg Loss: 0.0159 | Grad Norm: 0.01031674\n",
      "Epoch 4 | Step 2536900 | Avg Loss: 0.0157 | Grad Norm: 0.00882806\n",
      "Epoch 4 | Step 2537000 | Avg Loss: 0.0160 | Grad Norm: 0.01012576\n",
      "Epoch 4 | Step 2537100 | Avg Loss: 0.0159 | Grad Norm: 0.00863410\n",
      "Epoch 4 | Step 2537200 | Avg Loss: 0.0158 | Grad Norm: 0.00826103\n",
      "Epoch 4 | Step 2537300 | Avg Loss: 0.0160 | Grad Norm: 0.01161704\n",
      "Epoch 4 | Step 2537400 | Avg Loss: 0.0161 | Grad Norm: 0.00890243\n",
      "Epoch 4 | Step 2537500 | Avg Loss: 0.0155 | Grad Norm: 0.00873784\n",
      "Epoch 4 | Step 2537600 | Avg Loss: 0.0156 | Grad Norm: 0.01001369\n",
      "Epoch 4 | Step 2537700 | Avg Loss: 0.0156 | Grad Norm: 0.00865361\n",
      "Epoch 4 | Step 2537800 | Avg Loss: 0.0161 | Grad Norm: 0.00846679\n",
      "Epoch 4 | Step 2537900 | Avg Loss: 0.0158 | Grad Norm: 0.01007803\n",
      "Epoch 4 | Step 2538000 | Avg Loss: 0.0154 | Grad Norm: 0.01006258\n",
      "Epoch 4 | Step 2538100 | Avg Loss: 0.0156 | Grad Norm: 0.00882035\n",
      "Epoch 4 | Step 2538200 | Avg Loss: 0.0151 | Grad Norm: 0.00916833\n",
      "Epoch 4 | Step 2538300 | Avg Loss: 0.0151 | Grad Norm: 0.00973021\n",
      "Epoch 4 | Step 2538400 | Avg Loss: 0.0157 | Grad Norm: 0.00971885\n",
      "Epoch 4 | Step 2538500 | Avg Loss: 0.0154 | Grad Norm: 0.01033674\n",
      "Epoch 4 | Step 2538600 | Avg Loss: 0.0155 | Grad Norm: 0.00923053\n",
      "Epoch 4 | Step 2538700 | Avg Loss: 0.0152 | Grad Norm: 0.00820800\n",
      "Epoch 4 | Step 2538800 | Avg Loss: 0.0155 | Grad Norm: 0.00879885\n",
      "Epoch 4 | Step 2538900 | Avg Loss: 0.0158 | Grad Norm: 0.00894086\n",
      "Epoch 4 | Step 2539000 | Avg Loss: 0.0158 | Grad Norm: 0.00795288\n",
      "Epoch 4 | Step 2539100 | Avg Loss: 0.0159 | Grad Norm: 0.00909203\n",
      "Epoch 4 | Step 2539200 | Avg Loss: 0.0161 | Grad Norm: 0.00876550\n",
      "Epoch 4 | Step 2539300 | Avg Loss: 0.0159 | Grad Norm: 0.00885328\n",
      "Epoch 4 | Step 2539400 | Avg Loss: 0.0161 | Grad Norm: 0.00896904\n",
      "Epoch 4 | Step 2539500 | Avg Loss: 0.0159 | Grad Norm: 0.00817561\n",
      "Epoch 4 | Step 2539600 | Avg Loss: 0.0154 | Grad Norm: 0.00889125\n",
      "Epoch 4 | Step 2539700 | Avg Loss: 0.0156 | Grad Norm: 0.00937006\n",
      "Epoch 4 | Step 2539800 | Avg Loss: 0.0155 | Grad Norm: 0.00925440\n",
      "Epoch 4 | Step 2539900 | Avg Loss: 0.0158 | Grad Norm: 0.00977791\n",
      "Epoch 4 | Step 2540000 | Avg Loss: 0.0155 | Grad Norm: 0.00960073\n",
      "Epoch 4 | Step 2540100 | Avg Loss: 0.0155 | Grad Norm: 0.00952850\n",
      "Epoch 4 | Step 2540200 | Avg Loss: 0.0153 | Grad Norm: 0.00922765\n",
      "Epoch 4 | Step 2540300 | Avg Loss: 0.0153 | Grad Norm: 0.00852938\n",
      "Epoch 4 | Step 2540400 | Avg Loss: 0.0150 | Grad Norm: 0.01055745\n",
      "Epoch 4 | Step 2540500 | Avg Loss: 0.0153 | Grad Norm: 0.00946403\n",
      "Epoch 4 | Step 2540600 | Avg Loss: 0.0154 | Grad Norm: 0.00909789\n",
      "Epoch 4 | Step 2540700 | Avg Loss: 0.0154 | Grad Norm: 0.00881615\n",
      "Epoch 4 | Step 2540800 | Avg Loss: 0.0156 | Grad Norm: 0.01086560\n",
      "Epoch 4 | Step 2540900 | Avg Loss: 0.0156 | Grad Norm: 0.00906966\n",
      "Epoch 4 | Step 2541000 | Avg Loss: 0.0155 | Grad Norm: 0.01048481\n",
      "Epoch 4 | Step 2541100 | Avg Loss: 0.0152 | Grad Norm: 0.00869482\n",
      "Epoch 4 | Step 2541200 | Avg Loss: 0.0152 | Grad Norm: 0.00864246\n",
      "Epoch 4 | Step 2541300 | Avg Loss: 0.0152 | Grad Norm: 0.00854421\n",
      "Epoch 4 | Step 2541400 | Avg Loss: 0.0156 | Grad Norm: 0.01120011\n",
      "Epoch 4 | Step 2541500 | Avg Loss: 0.0157 | Grad Norm: 0.00890822\n",
      "Epoch 4 | Step 2541600 | Avg Loss: 0.0158 | Grad Norm: 0.00991817\n",
      "Epoch 4 | Step 2541700 | Avg Loss: 0.0162 | Grad Norm: 0.00846273\n",
      "Epoch 4 | Step 2541800 | Avg Loss: 0.0158 | Grad Norm: 0.01005245\n",
      "Epoch 4 | Step 2541900 | Avg Loss: 0.0157 | Grad Norm: 0.01051948\n",
      "Epoch 4 | Step 2542000 | Avg Loss: 0.0154 | Grad Norm: 0.00971277\n",
      "Epoch 4 | Step 2542100 | Avg Loss: 0.0154 | Grad Norm: 0.00982589\n",
      "Epoch 4 | Step 2542200 | Avg Loss: 0.0154 | Grad Norm: 0.00832293\n",
      "Epoch 4 | Step 2542300 | Avg Loss: 0.0154 | Grad Norm: 0.01061506\n",
      "Epoch 4 | Step 2542400 | Avg Loss: 0.0154 | Grad Norm: 0.00908987\n",
      "Epoch 4 | Step 2542500 | Avg Loss: 0.0154 | Grad Norm: 0.00888893\n",
      "Epoch 4 | Step 2542600 | Avg Loss: 0.0154 | Grad Norm: 0.00943450\n",
      "Epoch 4 | Step 2542700 | Avg Loss: 0.0156 | Grad Norm: 0.00908134\n",
      "Epoch 4 | Step 2542800 | Avg Loss: 0.0155 | Grad Norm: 0.00883252\n",
      "Epoch 4 | Step 2542900 | Avg Loss: 0.0158 | Grad Norm: 0.00896196\n",
      "Epoch 4 | Step 2543000 | Avg Loss: 0.0159 | Grad Norm: 0.00961261\n",
      "Epoch 4 | Step 2543100 | Avg Loss: 0.0159 | Grad Norm: 0.00882767\n",
      "Epoch 4 | Step 2543200 | Avg Loss: 0.0160 | Grad Norm: 0.01004063\n",
      "Epoch 4 | Step 2543300 | Avg Loss: 0.0161 | Grad Norm: 0.00888666\n",
      "Epoch 4 | Step 2543400 | Avg Loss: 0.0162 | Grad Norm: 0.01121889\n",
      "Epoch 4 | Step 2543500 | Avg Loss: 0.0158 | Grad Norm: 0.00898354\n",
      "Epoch 4 | Step 2543600 | Avg Loss: 0.0153 | Grad Norm: 0.01049462\n",
      "Epoch 4 | Step 2543700 | Avg Loss: 0.0156 | Grad Norm: 0.00938822\n",
      "Epoch 4 | Step 2543800 | Avg Loss: 0.0155 | Grad Norm: 0.00977849\n",
      "Epoch 4 | Step 2543900 | Avg Loss: 0.0153 | Grad Norm: 0.00912149\n",
      "Epoch 4 | Step 2544000 | Avg Loss: 0.0150 | Grad Norm: 0.00818700\n",
      "Epoch 4 | Step 2544100 | Avg Loss: 0.0154 | Grad Norm: 0.00942682\n",
      "Epoch 4 | Step 2544200 | Avg Loss: 0.0152 | Grad Norm: 0.00978539\n",
      "Epoch 4 | Step 2544300 | Avg Loss: 0.0157 | Grad Norm: 0.01032190\n",
      "Epoch 4 | Step 2544400 | Avg Loss: 0.0155 | Grad Norm: 0.00901640\n",
      "Epoch 4 | Step 2544500 | Avg Loss: 0.0157 | Grad Norm: 0.01097230\n",
      "Epoch 4 | Step 2544600 | Avg Loss: 0.0157 | Grad Norm: 0.00843779\n",
      "Epoch 4 | Step 2544700 | Avg Loss: 0.0159 | Grad Norm: 0.01132733\n",
      "Epoch 4 | Step 2544800 | Avg Loss: 0.0156 | Grad Norm: 0.00934354\n",
      "Epoch 4 | Step 2544900 | Avg Loss: 0.0158 | Grad Norm: 0.00866409\n",
      "Epoch 4 | Step 2545000 | Avg Loss: 0.0157 | Grad Norm: 0.00872870\n",
      "Epoch 4 | Step 2545100 | Avg Loss: 0.0155 | Grad Norm: 0.00856005\n",
      "Epoch 4 | Step 2545200 | Avg Loss: 0.0157 | Grad Norm: 0.00886329\n",
      "Epoch 4 | Step 2545300 | Avg Loss: 0.0154 | Grad Norm: 0.00788531\n",
      "Epoch 4 | Step 2545400 | Avg Loss: 0.0157 | Grad Norm: 0.00997264\n",
      "Epoch 4 | Step 2545500 | Avg Loss: 0.0154 | Grad Norm: 0.01006477\n",
      "Epoch 4 | Step 2545600 | Avg Loss: 0.0152 | Grad Norm: 0.01028619\n",
      "Epoch 4 | Step 2545700 | Avg Loss: 0.0154 | Grad Norm: 0.00832998\n",
      "Epoch 4 | Step 2545800 | Avg Loss: 0.0154 | Grad Norm: 0.00968608\n",
      "Epoch 4 | Step 2545900 | Avg Loss: 0.0153 | Grad Norm: 0.00847071\n",
      "Epoch 4 | Step 2546000 | Avg Loss: 0.0155 | Grad Norm: 0.00953924\n",
      "Epoch 4 | Step 2546100 | Avg Loss: 0.0153 | Grad Norm: 0.00848703\n",
      "Epoch 4 | Step 2546200 | Avg Loss: 0.0154 | Grad Norm: 0.00866186\n",
      "Epoch 4 | Step 2546300 | Avg Loss: 0.0158 | Grad Norm: 0.00896905\n",
      "Epoch 4 | Step 2546400 | Avg Loss: 0.0155 | Grad Norm: 0.00929761\n",
      "Epoch 4 | Step 2546500 | Avg Loss: 0.0162 | Grad Norm: 0.00854544\n",
      "Epoch 4 | Step 2546600 | Avg Loss: 0.0164 | Grad Norm: 0.01064627\n",
      "Epoch 4 | Step 2546700 | Avg Loss: 0.0164 | Grad Norm: 0.00937341\n",
      "Epoch 4 | Step 2546800 | Avg Loss: 0.0162 | Grad Norm: 0.00975212\n",
      "Epoch 4 | Step 2546900 | Avg Loss: 0.0163 | Grad Norm: 0.00955711\n",
      "Epoch 4 | Step 2547000 | Avg Loss: 0.0157 | Grad Norm: 0.00799830\n",
      "Epoch 4 | Step 2547100 | Avg Loss: 0.0156 | Grad Norm: 0.00912373\n",
      "Epoch 4 | Step 2547200 | Avg Loss: 0.0157 | Grad Norm: 0.00994224\n",
      "Epoch 4 | Step 2547300 | Avg Loss: 0.0157 | Grad Norm: 0.00861245\n",
      "Epoch 4 | Step 2547400 | Avg Loss: 0.0159 | Grad Norm: 0.00842323\n",
      "Epoch 4 | Step 2547500 | Avg Loss: 0.0164 | Grad Norm: 0.00873840\n",
      "Epoch 4 | Step 2547600 | Avg Loss: 0.0159 | Grad Norm: 0.01108011\n",
      "Epoch 4 | Step 2547700 | Avg Loss: 0.0155 | Grad Norm: 0.00970389\n",
      "Epoch 4 | Step 2547800 | Avg Loss: 0.0154 | Grad Norm: 0.00943374\n",
      "Epoch 4 | Step 2547900 | Avg Loss: 0.0159 | Grad Norm: 0.00966187\n",
      "Epoch 4 | Step 2548000 | Avg Loss: 0.0158 | Grad Norm: 0.00890081\n",
      "Epoch 4 | Step 2548100 | Avg Loss: 0.0158 | Grad Norm: 0.00990549\n",
      "Epoch 4 | Step 2548200 | Avg Loss: 0.0156 | Grad Norm: 0.00946571\n",
      "Epoch 4 | Step 2548300 | Avg Loss: 0.0154 | Grad Norm: 0.00840657\n",
      "Epoch 4 | Step 2548400 | Avg Loss: 0.0158 | Grad Norm: 0.00937264\n",
      "Epoch 4 | Step 2548500 | Avg Loss: 0.0155 | Grad Norm: 0.00889959\n",
      "Epoch 4 | Step 2548600 | Avg Loss: 0.0155 | Grad Norm: 0.00881100\n",
      "Epoch 4 | Step 2548700 | Avg Loss: 0.0154 | Grad Norm: 0.00844689\n",
      "Epoch 4 | Step 2548800 | Avg Loss: 0.0155 | Grad Norm: 0.00961290\n",
      "Epoch 4 | Step 2548900 | Avg Loss: 0.0152 | Grad Norm: 0.00874440\n",
      "Epoch 4 | Step 2549000 | Avg Loss: 0.0152 | Grad Norm: 0.01030736\n",
      "Epoch 4 | Step 2549100 | Avg Loss: 0.0155 | Grad Norm: 0.00872785\n",
      "Epoch 4 | Step 2549200 | Avg Loss: 0.0155 | Grad Norm: 0.00735362\n",
      "Epoch 4 | Step 2549300 | Avg Loss: 0.0154 | Grad Norm: 0.01086642\n",
      "Epoch 4 | Step 2549400 | Avg Loss: 0.0152 | Grad Norm: 0.01035995\n",
      "Epoch 4 | Step 2549500 | Avg Loss: 0.0155 | Grad Norm: 0.00923483\n",
      "Epoch 4 | Step 2549600 | Avg Loss: 0.0158 | Grad Norm: 0.00895984\n",
      "Epoch 4 | Step 2549700 | Avg Loss: 0.0161 | Grad Norm: 0.00905826\n",
      "Epoch 4 | Step 2549800 | Avg Loss: 0.0156 | Grad Norm: 0.00986494\n",
      "Epoch 4 | Step 2549900 | Avg Loss: 0.0158 | Grad Norm: 0.01002462\n",
      "Epoch 4 | Step 2550000 | Avg Loss: 0.0159 | Grad Norm: 0.00861541\n",
      "Epoch 4 | Step 2550100 | Avg Loss: 0.0158 | Grad Norm: 0.00863525\n",
      "Epoch 4 | Step 2550200 | Avg Loss: 0.0155 | Grad Norm: 0.00884345\n",
      "Epoch 4 | Step 2550300 | Avg Loss: 0.0156 | Grad Norm: 0.00830922\n",
      "Epoch 4 | Step 2550400 | Avg Loss: 0.0158 | Grad Norm: 0.00886356\n",
      "Epoch 4 | Step 2550500 | Avg Loss: 0.0159 | Grad Norm: 0.00941412\n",
      "Epoch 4 | Step 2550600 | Avg Loss: 0.0159 | Grad Norm: 0.00787079\n",
      "Epoch 4 | Step 2550700 | Avg Loss: 0.0157 | Grad Norm: 0.00867835\n",
      "Epoch 4 | Step 2550800 | Avg Loss: 0.0158 | Grad Norm: 0.00926940\n",
      "Epoch 4 | Step 2550900 | Avg Loss: 0.0156 | Grad Norm: 0.00807556\n",
      "Epoch 4 | Step 2551000 | Avg Loss: 0.0158 | Grad Norm: 0.00914679\n",
      "Epoch 4 | Step 2551100 | Avg Loss: 0.0158 | Grad Norm: 0.00891528\n",
      "Epoch 4 | Step 2551200 | Avg Loss: 0.0158 | Grad Norm: 0.00857980\n",
      "Epoch 4 | Step 2551300 | Avg Loss: 0.0154 | Grad Norm: 0.00893514\n",
      "Epoch 4 | Step 2551400 | Avg Loss: 0.0155 | Grad Norm: 0.00887518\n",
      "Epoch 4 | Step 2551500 | Avg Loss: 0.0155 | Grad Norm: 0.00859933\n",
      "Epoch 4 | Step 2551600 | Avg Loss: 0.0155 | Grad Norm: 0.00860914\n",
      "Epoch 4 | Step 2551700 | Avg Loss: 0.0155 | Grad Norm: 0.00920094\n",
      "Epoch 4 | Step 2551800 | Avg Loss: 0.0159 | Grad Norm: 0.00883778\n",
      "Epoch 4 | Step 2551900 | Avg Loss: 0.0162 | Grad Norm: 0.00833424\n",
      "Epoch 4 | Step 2552000 | Avg Loss: 0.0160 | Grad Norm: 0.00832301\n",
      "Epoch 4 | Step 2552100 | Avg Loss: 0.0158 | Grad Norm: 0.00844282\n",
      "Epoch 4 | Step 2552200 | Avg Loss: 0.0159 | Grad Norm: 0.00858354\n",
      "Epoch 4 | Step 2552300 | Avg Loss: 0.0162 | Grad Norm: 0.00793222\n",
      "Epoch 4 | Step 2552400 | Avg Loss: 0.0161 | Grad Norm: 0.00969536\n",
      "Epoch 4 | Step 2552500 | Avg Loss: 0.0157 | Grad Norm: 0.00975226\n",
      "Epoch 4 | Step 2552600 | Avg Loss: 0.0157 | Grad Norm: 0.00953729\n",
      "Epoch 4 | Step 2552700 | Avg Loss: 0.0160 | Grad Norm: 0.01017313\n",
      "Epoch 4 | Step 2552800 | Avg Loss: 0.0160 | Grad Norm: 0.00898343\n",
      "Epoch 4 | Step 2552900 | Avg Loss: 0.0160 | Grad Norm: 0.00918059\n",
      "Epoch 4 | Step 2553000 | Avg Loss: 0.0158 | Grad Norm: 0.01035314\n",
      "Epoch 4 | Step 2553100 | Avg Loss: 0.0165 | Grad Norm: 0.00964199\n",
      "Epoch 4 | Step 2553200 | Avg Loss: 0.0160 | Grad Norm: 0.00922536\n",
      "Epoch 4 | Step 2553300 | Avg Loss: 0.0159 | Grad Norm: 0.00970198\n",
      "Epoch 4 | Step 2553400 | Avg Loss: 0.0153 | Grad Norm: 0.01025139\n",
      "Epoch 4 | Step 2553500 | Avg Loss: 0.0155 | Grad Norm: 0.00814859\n",
      "Epoch 4 | Step 2553600 | Avg Loss: 0.0157 | Grad Norm: 0.00847426\n",
      "Epoch 4 | Step 2553700 | Avg Loss: 0.0156 | Grad Norm: 0.00975110\n",
      "Epoch 4 | Step 2553800 | Avg Loss: 0.0156 | Grad Norm: 0.00965758\n",
      "Epoch 4 | Step 2553900 | Avg Loss: 0.0159 | Grad Norm: 0.00886390\n",
      "Epoch 4 | Step 2554000 | Avg Loss: 0.0158 | Grad Norm: 0.00841148\n",
      "Epoch 4 | Step 2554100 | Avg Loss: 0.0158 | Grad Norm: 0.01027981\n",
      "Epoch 4 | Step 2554200 | Avg Loss: 0.0157 | Grad Norm: 0.00962919\n",
      "Epoch 4 | Step 2554300 | Avg Loss: 0.0158 | Grad Norm: 0.01141796\n",
      "Epoch 4 | Step 2554400 | Avg Loss: 0.0156 | Grad Norm: 0.01020550\n",
      "Epoch 4 | Step 2554500 | Avg Loss: 0.0158 | Grad Norm: 0.00821353\n",
      "Epoch 4 | Step 2554600 | Avg Loss: 0.0157 | Grad Norm: 0.00974760\n",
      "Epoch 4 | Step 2554700 | Avg Loss: 0.0154 | Grad Norm: 0.00951737\n",
      "Epoch 4 | Step 2554800 | Avg Loss: 0.0149 | Grad Norm: 0.00865153\n",
      "Epoch 4 | Step 2554900 | Avg Loss: 0.0150 | Grad Norm: 0.00880437\n",
      "Epoch 4 | Step 2555000 | Avg Loss: 0.0151 | Grad Norm: 0.00782569\n",
      "Epoch 4 | Step 2555100 | Avg Loss: 0.0149 | Grad Norm: 0.00782411\n",
      "Epoch 4 | Step 2555200 | Avg Loss: 0.0153 | Grad Norm: 0.01009436\n",
      "Epoch 4 | Step 2555300 | Avg Loss: 0.0150 | Grad Norm: 0.00795691\n",
      "Epoch 4 | Step 2555400 | Avg Loss: 0.0150 | Grad Norm: 0.00965123\n",
      "Epoch 4 | Step 2555500 | Avg Loss: 0.0151 | Grad Norm: 0.00784736\n",
      "Epoch 4 | Step 2555600 | Avg Loss: 0.0153 | Grad Norm: 0.00845053\n",
      "Epoch 4 | Step 2555700 | Avg Loss: 0.0157 | Grad Norm: 0.00841217\n",
      "Epoch 4 | Step 2555800 | Avg Loss: 0.0162 | Grad Norm: 0.00985402\n",
      "Epoch 4 | Step 2555900 | Avg Loss: 0.0160 | Grad Norm: 0.00973364\n",
      "Epoch 4 | Step 2556000 | Avg Loss: 0.0158 | Grad Norm: 0.00956015\n",
      "Epoch 4 | Step 2556100 | Avg Loss: 0.0160 | Grad Norm: 0.00976824\n",
      "Epoch 4 | Step 2556200 | Avg Loss: 0.0160 | Grad Norm: 0.00853963\n",
      "Epoch 4 | Step 2556300 | Avg Loss: 0.0160 | Grad Norm: 0.00963856\n",
      "Epoch 4 | Step 2556400 | Avg Loss: 0.0156 | Grad Norm: 0.00804835\n",
      "Epoch 4 | Step 2556500 | Avg Loss: 0.0156 | Grad Norm: 0.01066015\n",
      "Epoch 4 | Step 2556600 | Avg Loss: 0.0156 | Grad Norm: 0.01005204\n",
      "Epoch 4 | Step 2556700 | Avg Loss: 0.0156 | Grad Norm: 0.00947241\n",
      "Epoch 4 | Step 2556800 | Avg Loss: 0.0154 | Grad Norm: 0.00909793\n",
      "Epoch 4 | Step 2556900 | Avg Loss: 0.0150 | Grad Norm: 0.00947384\n",
      "Epoch 4 | Step 2557000 | Avg Loss: 0.0154 | Grad Norm: 0.00832815\n",
      "Epoch 4 | Step 2557100 | Avg Loss: 0.0150 | Grad Norm: 0.00959005\n",
      "Epoch 4 | Step 2557200 | Avg Loss: 0.0150 | Grad Norm: 0.00930508\n",
      "Epoch 4 | Step 2557300 | Avg Loss: 0.0153 | Grad Norm: 0.00928750\n",
      "Epoch 4 | Step 2557400 | Avg Loss: 0.0155 | Grad Norm: 0.00805544\n",
      "Epoch 4 | Step 2557500 | Avg Loss: 0.0157 | Grad Norm: 0.00933533\n",
      "Epoch 4 | Step 2557600 | Avg Loss: 0.0156 | Grad Norm: 0.00955972\n",
      "Epoch 4 | Step 2557700 | Avg Loss: 0.0152 | Grad Norm: 0.00969820\n",
      "Epoch 4 | Step 2557800 | Avg Loss: 0.0156 | Grad Norm: 0.00860135\n",
      "Epoch 4 | Step 2557900 | Avg Loss: 0.0155 | Grad Norm: 0.00990404\n",
      "Epoch 4 | Step 2558000 | Avg Loss: 0.0160 | Grad Norm: 0.00845680\n",
      "Epoch 4 | Step 2558100 | Avg Loss: 0.0157 | Grad Norm: 0.00879568\n",
      "Epoch 4 | Step 2558200 | Avg Loss: 0.0158 | Grad Norm: 0.00874405\n",
      "Epoch 4 | Step 2558300 | Avg Loss: 0.0153 | Grad Norm: 0.00897210\n",
      "Epoch 4 | Step 2558400 | Avg Loss: 0.0149 | Grad Norm: 0.00858238\n",
      "Epoch 4 | Step 2558500 | Avg Loss: 0.0150 | Grad Norm: 0.00878140\n",
      "Epoch 4 | Step 2558600 | Avg Loss: 0.0151 | Grad Norm: 0.00901677\n",
      "Epoch 4 | Step 2558700 | Avg Loss: 0.0152 | Grad Norm: 0.00906701\n",
      "Epoch 4 | Step 2558800 | Avg Loss: 0.0156 | Grad Norm: 0.01145392\n",
      "Epoch 4 | Step 2558900 | Avg Loss: 0.0156 | Grad Norm: 0.00890780\n",
      "Epoch 4 | Step 2559000 | Avg Loss: 0.0155 | Grad Norm: 0.00850995\n",
      "Epoch 4 | Step 2559100 | Avg Loss: 0.0154 | Grad Norm: 0.00894188\n",
      "Epoch 4 | Step 2559200 | Avg Loss: 0.0152 | Grad Norm: 0.00887681\n",
      "Epoch 4 | Step 2559300 | Avg Loss: 0.0154 | Grad Norm: 0.00903128\n",
      "Epoch 4 | Step 2559400 | Avg Loss: 0.0155 | Grad Norm: 0.00894119\n",
      "Epoch 4 | Step 2559500 | Avg Loss: 0.0152 | Grad Norm: 0.00959007\n",
      "Epoch 4 | Step 2559600 | Avg Loss: 0.0152 | Grad Norm: 0.00927915\n",
      "Epoch 4 | Step 2559700 | Avg Loss: 0.0149 | Grad Norm: 0.00860927\n",
      "Epoch 4 | Step 2559800 | Avg Loss: 0.0150 | Grad Norm: 0.00801753\n",
      "Epoch 4 | Step 2559900 | Avg Loss: 0.0153 | Grad Norm: 0.00894453\n",
      "Epoch 4 | Step 2560000 | Avg Loss: 0.0154 | Grad Norm: 0.00945197\n",
      "Epoch 4 | Step 2560100 | Avg Loss: 0.0157 | Grad Norm: 0.00903496\n",
      "Epoch 4 | Step 2560200 | Avg Loss: 0.0154 | Grad Norm: 0.00824906\n",
      "Epoch 4 | Step 2560300 | Avg Loss: 0.0154 | Grad Norm: 0.00813766\n",
      "Epoch 4 | Step 2560400 | Avg Loss: 0.0150 | Grad Norm: 0.00857914\n",
      "Epoch 4 | Step 2560500 | Avg Loss: 0.0153 | Grad Norm: 0.00850424\n",
      "Epoch 4 | Step 2560600 | Avg Loss: 0.0151 | Grad Norm: 0.00908183\n",
      "Epoch 4 | Step 2560700 | Avg Loss: 0.0154 | Grad Norm: 0.00880337\n",
      "Epoch 4 | Step 2560800 | Avg Loss: 0.0156 | Grad Norm: 0.00984212\n",
      "Epoch 4 | Step 2560900 | Avg Loss: 0.0159 | Grad Norm: 0.00886384\n",
      "Epoch 4 | Step 2561000 | Avg Loss: 0.0161 | Grad Norm: 0.00868633\n",
      "Epoch 4 | Step 2561100 | Avg Loss: 0.0161 | Grad Norm: 0.00926909\n",
      "Epoch 4 | Step 2561200 | Avg Loss: 0.0158 | Grad Norm: 0.01012850\n",
      "Epoch 4 | Step 2561300 | Avg Loss: 0.0158 | Grad Norm: 0.00812843\n",
      "Epoch 4 | Step 2561400 | Avg Loss: 0.0159 | Grad Norm: 0.00831278\n",
      "Epoch 4 | Step 2561500 | Avg Loss: 0.0160 | Grad Norm: 0.00947667\n",
      "Epoch 4 | Step 2561600 | Avg Loss: 0.0163 | Grad Norm: 0.00888983\n",
      "Epoch 4 | Step 2561700 | Avg Loss: 0.0161 | Grad Norm: 0.00826546\n",
      "Epoch 4 | Step 2561800 | Avg Loss: 0.0157 | Grad Norm: 0.00864759\n",
      "Epoch 4 | Step 2561900 | Avg Loss: 0.0160 | Grad Norm: 0.00864515\n",
      "Epoch 4 | Step 2562000 | Avg Loss: 0.0159 | Grad Norm: 0.00925347\n",
      "Epoch 4 | Step 2562100 | Avg Loss: 0.0156 | Grad Norm: 0.01091845\n",
      "Epoch 4 | Step 2562200 | Avg Loss: 0.0158 | Grad Norm: 0.00916162\n",
      "Epoch 4 | Step 2562300 | Avg Loss: 0.0154 | Grad Norm: 0.00882993\n",
      "Epoch 4 | Step 2562400 | Avg Loss: 0.0153 | Grad Norm: 0.00875543\n",
      "Epoch 4 | Step 2562500 | Avg Loss: 0.0150 | Grad Norm: 0.00850076\n",
      "Epoch 4 | Step 2562600 | Avg Loss: 0.0154 | Grad Norm: 0.00843981\n",
      "Epoch 4 | Step 2562700 | Avg Loss: 0.0156 | Grad Norm: 0.00904188\n",
      "Epoch 4 | Step 2562800 | Avg Loss: 0.0160 | Grad Norm: 0.00885417\n",
      "Epoch 4 | Step 2562900 | Avg Loss: 0.0160 | Grad Norm: 0.00891886\n",
      "Epoch 4 | Step 2563000 | Avg Loss: 0.0158 | Grad Norm: 0.01115511\n",
      "Epoch 4 | Step 2563100 | Avg Loss: 0.0156 | Grad Norm: 0.00837703\n",
      "Epoch 4 | Step 2563200 | Avg Loss: 0.0158 | Grad Norm: 0.00952653\n",
      "Epoch 4 | Step 2563300 | Avg Loss: 0.0156 | Grad Norm: 0.00868932\n",
      "Epoch 4 | Step 2563400 | Avg Loss: 0.0154 | Grad Norm: 0.00817459\n",
      "Epoch 4 | Step 2563500 | Avg Loss: 0.0155 | Grad Norm: 0.00877494\n",
      "Epoch 4 | Step 2563600 | Avg Loss: 0.0155 | Grad Norm: 0.01018050\n",
      "Epoch 4 | Step 2563700 | Avg Loss: 0.0156 | Grad Norm: 0.01007357\n",
      "Epoch 4 | Step 2563800 | Avg Loss: 0.0156 | Grad Norm: 0.00999572\n",
      "Epoch 4 | Step 2563900 | Avg Loss: 0.0154 | Grad Norm: 0.00759301\n",
      "Epoch 4 | Step 2564000 | Avg Loss: 0.0151 | Grad Norm: 0.01214877\n",
      "Epoch 4 | Step 2564100 | Avg Loss: 0.0154 | Grad Norm: 0.00921766\n",
      "Epoch 4 | Step 2564200 | Avg Loss: 0.0153 | Grad Norm: 0.00969332\n",
      "Epoch 4 | Step 2564300 | Avg Loss: 0.0153 | Grad Norm: 0.00889948\n",
      "Epoch 4 | Step 2564400 | Avg Loss: 0.0150 | Grad Norm: 0.00829072\n",
      "Epoch 4 | Step 2564500 | Avg Loss: 0.0152 | Grad Norm: 0.00882538\n",
      "Epoch 4 | Step 2564600 | Avg Loss: 0.0150 | Grad Norm: 0.00968604\n",
      "Epoch 4 | Step 2564700 | Avg Loss: 0.0153 | Grad Norm: 0.00891519\n",
      "Epoch 4 | Step 2564800 | Avg Loss: 0.0152 | Grad Norm: 0.00947750\n",
      "Epoch 4 | Step 2564900 | Avg Loss: 0.0153 | Grad Norm: 0.00900200\n",
      "Epoch 4 | Step 2565000 | Avg Loss: 0.0154 | Grad Norm: 0.00912503\n",
      "Epoch 4 | Step 2565100 | Avg Loss: 0.0155 | Grad Norm: 0.00842780\n",
      "Epoch 4 | Step 2565200 | Avg Loss: 0.0159 | Grad Norm: 0.00928719\n",
      "Epoch 4 | Step 2565300 | Avg Loss: 0.0158 | Grad Norm: 0.00925085\n",
      "Epoch 4 | Step 2565400 | Avg Loss: 0.0158 | Grad Norm: 0.00932791\n",
      "Epoch 4 | Step 2565500 | Avg Loss: 0.0162 | Grad Norm: 0.01032853\n",
      "Epoch 4 | Step 2565600 | Avg Loss: 0.0162 | Grad Norm: 0.01210795\n",
      "Epoch 4 | Step 2565700 | Avg Loss: 0.0156 | Grad Norm: 0.01007314\n",
      "Epoch 4 | Step 2565800 | Avg Loss: 0.0158 | Grad Norm: 0.00840222\n",
      "Epoch 4 | Step 2565900 | Avg Loss: 0.0156 | Grad Norm: 0.00946081\n",
      "Epoch 4 | Step 2566000 | Avg Loss: 0.0154 | Grad Norm: 0.00748098\n",
      "Epoch 4 | Step 2566100 | Avg Loss: 0.0154 | Grad Norm: 0.00834699\n",
      "Epoch 4 | Step 2566200 | Avg Loss: 0.0156 | Grad Norm: 0.00926260\n",
      "Epoch 4 | Step 2566300 | Avg Loss: 0.0155 | Grad Norm: 0.00750045\n",
      "Epoch 4 | Step 2566400 | Avg Loss: 0.0153 | Grad Norm: 0.00873204\n",
      "Epoch 4 | Step 2566500 | Avg Loss: 0.0154 | Grad Norm: 0.01426951\n",
      "Epoch 4 | Step 2566600 | Avg Loss: 0.0156 | Grad Norm: 0.00843787\n",
      "Epoch 4 | Step 2566700 | Avg Loss: 0.0157 | Grad Norm: 0.00991561\n",
      "Epoch 4 | Step 2566800 | Avg Loss: 0.0157 | Grad Norm: 0.00892746\n",
      "Epoch 4 | Step 2566900 | Avg Loss: 0.0158 | Grad Norm: 0.00886899\n",
      "Epoch 4 | Step 2567000 | Avg Loss: 0.0156 | Grad Norm: 0.00819726\n",
      "Epoch 4 | Step 2567100 | Avg Loss: 0.0153 | Grad Norm: 0.00812214\n",
      "Epoch 4 | Step 2567200 | Avg Loss: 0.0156 | Grad Norm: 0.00955791\n",
      "Epoch 4 | Step 2567300 | Avg Loss: 0.0153 | Grad Norm: 0.00821355\n",
      "Epoch 4 | Step 2567400 | Avg Loss: 0.0155 | Grad Norm: 0.00887444\n",
      "Epoch 4 | Step 2567500 | Avg Loss: 0.0152 | Grad Norm: 0.00891696\n",
      "Epoch 4 | Step 2567600 | Avg Loss: 0.0151 | Grad Norm: 0.00880040\n",
      "Epoch 4 | Step 2567700 | Avg Loss: 0.0152 | Grad Norm: 0.00899508\n",
      "Epoch 4 | Step 2567800 | Avg Loss: 0.0150 | Grad Norm: 0.01298950\n",
      "Epoch 4 | Step 2567900 | Avg Loss: 0.0154 | Grad Norm: 0.00898778\n",
      "Epoch 4 | Step 2568000 | Avg Loss: 0.0154 | Grad Norm: 0.00941776\n",
      "Epoch 4 | Step 2568100 | Avg Loss: 0.0155 | Grad Norm: 0.00969482\n",
      "Epoch 4 | Step 2568200 | Avg Loss: 0.0155 | Grad Norm: 0.00861649\n",
      "Epoch 4 | Step 2568300 | Avg Loss: 0.0156 | Grad Norm: 0.01006091\n",
      "Epoch 4 | Step 2568400 | Avg Loss: 0.0157 | Grad Norm: 0.00959606\n",
      "Epoch 4 | Step 2568500 | Avg Loss: 0.0161 | Grad Norm: 0.00909590\n",
      "Epoch 4 | Step 2568600 | Avg Loss: 0.0160 | Grad Norm: 0.00814357\n",
      "Epoch 4 | Step 2568700 | Avg Loss: 0.0161 | Grad Norm: 0.00881219\n",
      "Epoch 4 | Step 2568800 | Avg Loss: 0.0160 | Grad Norm: 0.00923843\n",
      "Epoch 4 | Step 2568900 | Avg Loss: 0.0156 | Grad Norm: 0.00932766\n",
      "Epoch 4 | Step 2569000 | Avg Loss: 0.0156 | Grad Norm: 0.00836249\n",
      "Epoch 4 | Step 2569100 | Avg Loss: 0.0156 | Grad Norm: 0.00891586\n",
      "Epoch 4 | Step 2569200 | Avg Loss: 0.0157 | Grad Norm: 0.00875646\n",
      "Epoch 4 | Step 2569300 | Avg Loss: 0.0157 | Grad Norm: 0.00963227\n",
      "Epoch 4 | Step 2569400 | Avg Loss: 0.0156 | Grad Norm: 0.00949414\n",
      "Epoch 4 | Step 2569500 | Avg Loss: 0.0160 | Grad Norm: 0.01106963\n",
      "Epoch 4 | Step 2569600 | Avg Loss: 0.0158 | Grad Norm: 0.00958997\n",
      "Epoch 4 | Step 2569700 | Avg Loss: 0.0159 | Grad Norm: 0.00782759\n",
      "Epoch 4 | Step 2569800 | Avg Loss: 0.0156 | Grad Norm: 0.00936647\n",
      "Epoch 4 | Step 2569900 | Avg Loss: 0.0156 | Grad Norm: 0.00900396\n",
      "Epoch 4 | Step 2570000 | Avg Loss: 0.0157 | Grad Norm: 0.00895916\n",
      "Epoch 4 | Step 2570100 | Avg Loss: 0.0153 | Grad Norm: 0.00837270\n",
      "Epoch 4 | Step 2570200 | Avg Loss: 0.0156 | Grad Norm: 0.00879081\n",
      "Epoch 4 | Step 2570300 | Avg Loss: 0.0157 | Grad Norm: 0.00941440\n",
      "Epoch 4 | Step 2570400 | Avg Loss: 0.0155 | Grad Norm: 0.00799287\n",
      "Epoch 4 | Step 2570500 | Avg Loss: 0.0153 | Grad Norm: 0.00938533\n",
      "Epoch 4 | Step 2570600 | Avg Loss: 0.0152 | Grad Norm: 0.00839361\n",
      "Epoch 4 | Step 2570700 | Avg Loss: 0.0148 | Grad Norm: 0.00863300\n",
      "Epoch 4 | Step 2570800 | Avg Loss: 0.0148 | Grad Norm: 0.00955536\n",
      "Epoch 4 | Step 2570900 | Avg Loss: 0.0149 | Grad Norm: 0.00857272\n",
      "Epoch 4 | Step 2571000 | Avg Loss: 0.0153 | Grad Norm: 0.00954761\n",
      "Epoch 4 | Step 2571100 | Avg Loss: 0.0154 | Grad Norm: 0.00784942\n",
      "Epoch 4 | Step 2571200 | Avg Loss: 0.0160 | Grad Norm: 0.00937768\n",
      "Epoch 4 | Step 2571300 | Avg Loss: 0.0159 | Grad Norm: 0.01003576\n",
      "Epoch 4 | Step 2571400 | Avg Loss: 0.0160 | Grad Norm: 0.00830545\n",
      "Epoch 4 | Step 2571500 | Avg Loss: 0.0158 | Grad Norm: 0.00920574\n",
      "Epoch 4 | Step 2571600 | Avg Loss: 0.0160 | Grad Norm: 0.00828329\n",
      "Epoch 4 | Step 2571700 | Avg Loss: 0.0155 | Grad Norm: 0.00835412\n",
      "Epoch 4 | Step 2571800 | Avg Loss: 0.0156 | Grad Norm: 0.00849411\n",
      "Epoch 4 | Step 2571900 | Avg Loss: 0.0158 | Grad Norm: 0.00840296\n",
      "Epoch 4 | Step 2572000 | Avg Loss: 0.0158 | Grad Norm: 0.01039805\n",
      "Epoch 4 | Step 2572100 | Avg Loss: 0.0157 | Grad Norm: 0.00918989\n",
      "Epoch 4 | Step 2572200 | Avg Loss: 0.0159 | Grad Norm: 0.00943357\n",
      "Epoch 4 | Step 2572300 | Avg Loss: 0.0158 | Grad Norm: 0.00995886\n",
      "Epoch 4 | Step 2572400 | Avg Loss: 0.0159 | Grad Norm: 0.00854526\n",
      "Epoch 4 | Step 2572500 | Avg Loss: 0.0156 | Grad Norm: 0.00926514\n",
      "Epoch 4 | Step 2572600 | Avg Loss: 0.0156 | Grad Norm: 0.00896785\n",
      "Epoch 4 | Step 2572700 | Avg Loss: 0.0153 | Grad Norm: 0.00860228\n",
      "Epoch 4 | Step 2572800 | Avg Loss: 0.0153 | Grad Norm: 0.00782147\n",
      "Epoch 4 | Step 2572900 | Avg Loss: 0.0156 | Grad Norm: 0.00867087\n",
      "Epoch 4 | Step 2573000 | Avg Loss: 0.0156 | Grad Norm: 0.00936404\n",
      "Epoch 4 | Step 2573100 | Avg Loss: 0.0153 | Grad Norm: 0.01084225\n",
      "Epoch 4 | Step 2573200 | Avg Loss: 0.0149 | Grad Norm: 0.00834152\n",
      "Epoch 4 | Step 2573300 | Avg Loss: 0.0152 | Grad Norm: 0.00949307\n",
      "Epoch 4 | Step 2573400 | Avg Loss: 0.0155 | Grad Norm: 0.00996549\n",
      "Epoch 4 | Step 2573500 | Avg Loss: 0.0153 | Grad Norm: 0.00857332\n",
      "Epoch 4 | Step 2573600 | Avg Loss: 0.0153 | Grad Norm: 0.00854843\n",
      "Epoch 4 | Step 2573700 | Avg Loss: 0.0152 | Grad Norm: 0.00894881\n",
      "Epoch 4 | Step 2573800 | Avg Loss: 0.0152 | Grad Norm: 0.00829508\n",
      "Epoch 4 | Step 2573900 | Avg Loss: 0.0148 | Grad Norm: 0.00935084\n",
      "Epoch 4 | Step 2574000 | Avg Loss: 0.0148 | Grad Norm: 0.00866898\n",
      "Epoch 4 | Step 2574100 | Avg Loss: 0.0150 | Grad Norm: 0.00981757\n",
      "Epoch 4 | Step 2574200 | Avg Loss: 0.0146 | Grad Norm: 0.00848189\n",
      "Epoch 4 | Step 2574300 | Avg Loss: 0.0148 | Grad Norm: 0.00916000\n",
      "Epoch 4 | Step 2574400 | Avg Loss: 0.0153 | Grad Norm: 0.00913905\n",
      "Epoch 4 | Step 2574500 | Avg Loss: 0.0150 | Grad Norm: 0.00976161\n",
      "Epoch 4 | Step 2574600 | Avg Loss: 0.0154 | Grad Norm: 0.00799294\n",
      "Epoch 4 | Step 2574700 | Avg Loss: 0.0152 | Grad Norm: 0.00808297\n",
      "Epoch 4 | Step 2574800 | Avg Loss: 0.0153 | Grad Norm: 0.00793014\n",
      "Epoch 4 | Step 2574900 | Avg Loss: 0.0153 | Grad Norm: 0.00779553\n",
      "Epoch 4 | Step 2575000 | Avg Loss: 0.0153 | Grad Norm: 0.00852000\n",
      "Epoch 4 | Step 2575100 | Avg Loss: 0.0157 | Grad Norm: 0.01009258\n",
      "Epoch 4 | Step 2575200 | Avg Loss: 0.0158 | Grad Norm: 0.00953975\n",
      "Epoch 4 | Step 2575300 | Avg Loss: 0.0156 | Grad Norm: 0.01025357\n",
      "Epoch 4 | Step 2575400 | Avg Loss: 0.0156 | Grad Norm: 0.00879991\n",
      "Epoch 4 | Step 2575500 | Avg Loss: 0.0159 | Grad Norm: 0.00937161\n",
      "Epoch 4 | Step 2575600 | Avg Loss: 0.0155 | Grad Norm: 0.01026421\n",
      "Epoch 4 | Step 2575700 | Avg Loss: 0.0157 | Grad Norm: 0.00896546\n",
      "Epoch 4 | Step 2575800 | Avg Loss: 0.0157 | Grad Norm: 0.00990268\n",
      "Epoch 4 | Step 2575900 | Avg Loss: 0.0158 | Grad Norm: 0.00844044\n",
      "Epoch 4 | Step 2576000 | Avg Loss: 0.0157 | Grad Norm: 0.00950930\n",
      "Epoch 4 | Step 2576100 | Avg Loss: 0.0153 | Grad Norm: 0.00939160\n",
      "Epoch 4 | Step 2576200 | Avg Loss: 0.0152 | Grad Norm: 0.00942586\n",
      "Epoch 4 | Step 2576300 | Avg Loss: 0.0152 | Grad Norm: 0.00864157\n",
      "Epoch 4 | Step 2576400 | Avg Loss: 0.0158 | Grad Norm: 0.00865789\n",
      "Epoch 4 | Step 2576500 | Avg Loss: 0.0158 | Grad Norm: 0.01019568\n",
      "Epoch 4 | Step 2576600 | Avg Loss: 0.0154 | Grad Norm: 0.00917119\n",
      "Epoch 4 | Step 2576700 | Avg Loss: 0.0155 | Grad Norm: 0.01001486\n",
      "Epoch 4 | Step 2576800 | Avg Loss: 0.0161 | Grad Norm: 0.00912563\n",
      "Epoch 4 | Step 2576900 | Avg Loss: 0.0158 | Grad Norm: 0.00971060\n",
      "Epoch 4 | Step 2577000 | Avg Loss: 0.0157 | Grad Norm: 0.00957195\n",
      "Epoch 4 | Step 2577100 | Avg Loss: 0.0154 | Grad Norm: 0.00957668\n",
      "Epoch 4 | Step 2577200 | Avg Loss: 0.0155 | Grad Norm: 0.01008868\n",
      "Epoch 4 | Step 2577300 | Avg Loss: 0.0157 | Grad Norm: 0.00831376\n",
      "Epoch 4 | Step 2577400 | Avg Loss: 0.0156 | Grad Norm: 0.00984919\n",
      "Epoch 4 | Step 2577500 | Avg Loss: 0.0154 | Grad Norm: 0.01180399\n",
      "Epoch 4 | Step 2577600 | Avg Loss: 0.0152 | Grad Norm: 0.00946104\n",
      "Epoch 4 | Step 2577700 | Avg Loss: 0.0152 | Grad Norm: 0.00846102\n",
      "Epoch 4 | Step 2577800 | Avg Loss: 0.0152 | Grad Norm: 0.00881027\n",
      "Epoch 4 | Step 2577900 | Avg Loss: 0.0159 | Grad Norm: 0.00961893\n",
      "Epoch 4 | Step 2578000 | Avg Loss: 0.0158 | Grad Norm: 0.01034965\n",
      "Epoch 4 | Step 2578100 | Avg Loss: 0.0152 | Grad Norm: 0.00941072\n",
      "Epoch 4 | Step 2578200 | Avg Loss: 0.0156 | Grad Norm: 0.01216063\n",
      "Epoch 4 | Step 2578300 | Avg Loss: 0.0153 | Grad Norm: 0.00819303\n",
      "Epoch 4 | Step 2578400 | Avg Loss: 0.0154 | Grad Norm: 0.01031222\n",
      "Epoch 4 | Step 2578500 | Avg Loss: 0.0156 | Grad Norm: 0.00713097\n",
      "Epoch 4 | Step 2578600 | Avg Loss: 0.0155 | Grad Norm: 0.00883350\n",
      "Epoch 4 | Step 2578700 | Avg Loss: 0.0156 | Grad Norm: 0.00926094\n",
      "Epoch 4 | Step 2578800 | Avg Loss: 0.0154 | Grad Norm: 0.00925681\n",
      "Epoch 4 | Step 2578900 | Avg Loss: 0.0154 | Grad Norm: 0.00893203\n",
      "Epoch 4 | Step 2579000 | Avg Loss: 0.0152 | Grad Norm: 0.00907322\n",
      "Epoch 4 | Step 2579100 | Avg Loss: 0.0155 | Grad Norm: 0.00994253\n",
      "Epoch 4 | Step 2579200 | Avg Loss: 0.0153 | Grad Norm: 0.00989393\n",
      "Epoch 4 | Step 2579300 | Avg Loss: 0.0154 | Grad Norm: 0.00925219\n",
      "Epoch 4 | Step 2579400 | Avg Loss: 0.0155 | Grad Norm: 0.00990769\n",
      "Epoch 4 | Step 2579500 | Avg Loss: 0.0151 | Grad Norm: 0.00933370\n",
      "Epoch 4 | Step 2579600 | Avg Loss: 0.0154 | Grad Norm: 0.01029304\n",
      "Epoch 4 | Step 2579700 | Avg Loss: 0.0155 | Grad Norm: 0.00951254\n",
      "Epoch 4 | Step 2579800 | Avg Loss: 0.0156 | Grad Norm: 0.00841517\n",
      "Epoch 4 | Step 2579900 | Avg Loss: 0.0154 | Grad Norm: 0.00837180\n",
      "Epoch 4 | Step 2580000 | Avg Loss: 0.0155 | Grad Norm: 0.00849751\n",
      "Epoch 4 | Step 2580100 | Avg Loss: 0.0152 | Grad Norm: 0.00878950\n",
      "Epoch 4 | Step 2580200 | Avg Loss: 0.0154 | Grad Norm: 0.00797882\n",
      "Epoch 4 | Step 2580300 | Avg Loss: 0.0153 | Grad Norm: 0.00931656\n",
      "Epoch 4 | Step 2580400 | Avg Loss: 0.0149 | Grad Norm: 0.00903618\n",
      "Epoch 4 | Step 2580500 | Avg Loss: 0.0152 | Grad Norm: 0.00968670\n",
      "Epoch 4 | Step 2580600 | Avg Loss: 0.0154 | Grad Norm: 0.00845342\n",
      "Epoch 4 | Step 2580700 | Avg Loss: 0.0156 | Grad Norm: 0.00881999\n",
      "Epoch 4 | Step 2580800 | Avg Loss: 0.0158 | Grad Norm: 0.00928914\n",
      "Epoch 4 | Step 2580900 | Avg Loss: 0.0158 | Grad Norm: 0.00892873\n",
      "Epoch 4 | Step 2581000 | Avg Loss: 0.0157 | Grad Norm: 0.00785784\n",
      "Epoch 4 | Step 2581100 | Avg Loss: 0.0159 | Grad Norm: 0.00920392\n",
      "Epoch 4 | Step 2581200 | Avg Loss: 0.0158 | Grad Norm: 0.01043173\n",
      "Epoch 4 | Step 2581300 | Avg Loss: 0.0158 | Grad Norm: 0.00861790\n",
      "Epoch 4 | Step 2581400 | Avg Loss: 0.0158 | Grad Norm: 0.01036272\n",
      "Epoch 4 | Step 2581500 | Avg Loss: 0.0156 | Grad Norm: 0.00859276\n",
      "Epoch 4 | Step 2581600 | Avg Loss: 0.0156 | Grad Norm: 0.00862013\n",
      "Epoch 4 | Step 2581700 | Avg Loss: 0.0155 | Grad Norm: 0.00913985\n",
      "Epoch 4 | Step 2581800 | Avg Loss: 0.0155 | Grad Norm: 0.00942124\n",
      "Epoch 4 | Step 2581900 | Avg Loss: 0.0157 | Grad Norm: 0.00865533\n",
      "Epoch 4 | Step 2582000 | Avg Loss: 0.0157 | Grad Norm: 0.00909433\n",
      "Epoch 4 | Step 2582100 | Avg Loss: 0.0155 | Grad Norm: 0.00903742\n",
      "Epoch 4 | Step 2582200 | Avg Loss: 0.0152 | Grad Norm: 0.01096835\n",
      "Epoch 4 | Step 2582300 | Avg Loss: 0.0154 | Grad Norm: 0.00961114\n",
      "Epoch 4 | Step 2582400 | Avg Loss: 0.0154 | Grad Norm: 0.00909198\n",
      "Epoch 4 | Step 2582500 | Avg Loss: 0.0157 | Grad Norm: 0.00953338\n",
      "Epoch 4 | Step 2582600 | Avg Loss: 0.0157 | Grad Norm: 0.00788798\n",
      "Epoch 4 | Step 2582700 | Avg Loss: 0.0156 | Grad Norm: 0.00885359\n",
      "Epoch 4 | Step 2582800 | Avg Loss: 0.0153 | Grad Norm: 0.00775059\n",
      "Epoch 4 | Step 2582900 | Avg Loss: 0.0150 | Grad Norm: 0.00916604\n",
      "Epoch 4 | Step 2583000 | Avg Loss: 0.0150 | Grad Norm: 0.00814050\n",
      "Epoch 4 | Step 2583100 | Avg Loss: 0.0153 | Grad Norm: 0.00935528\n",
      "Epoch 4 | Step 2583200 | Avg Loss: 0.0156 | Grad Norm: 0.00890943\n",
      "Epoch 4 | Step 2583300 | Avg Loss: 0.0157 | Grad Norm: 0.00787332\n",
      "Epoch 4 | Step 2583400 | Avg Loss: 0.0156 | Grad Norm: 0.00871131\n",
      "Epoch 4 | Step 2583500 | Avg Loss: 0.0154 | Grad Norm: 0.00887171\n",
      "Epoch 4 | Step 2583600 | Avg Loss: 0.0153 | Grad Norm: 0.00810101\n",
      "Epoch 4 | Step 2583700 | Avg Loss: 0.0155 | Grad Norm: 0.00914603\n",
      "Epoch 4 | Step 2583800 | Avg Loss: 0.0156 | Grad Norm: 0.00947283\n",
      "Epoch 4 | Step 2583900 | Avg Loss: 0.0153 | Grad Norm: 0.00985659\n",
      "Epoch 4 | Step 2584000 | Avg Loss: 0.0153 | Grad Norm: 0.00859421\n",
      "Epoch 4 | Step 2584100 | Avg Loss: 0.0158 | Grad Norm: 0.00831780\n",
      "Epoch 4 | Step 2584200 | Avg Loss: 0.0159 | Grad Norm: 0.00958009\n",
      "Epoch 4 | Step 2584300 | Avg Loss: 0.0159 | Grad Norm: 0.01038948\n",
      "Epoch 4 | Step 2584400 | Avg Loss: 0.0158 | Grad Norm: 0.00846520\n",
      "Epoch 4 | Step 2584500 | Avg Loss: 0.0157 | Grad Norm: 0.00889316\n",
      "Epoch 4 | Step 2584600 | Avg Loss: 0.0157 | Grad Norm: 0.00854767\n",
      "Epoch 4 | Step 2584700 | Avg Loss: 0.0158 | Grad Norm: 0.00911916\n",
      "Epoch 4 | Step 2584800 | Avg Loss: 0.0157 | Grad Norm: 0.00852744\n",
      "Epoch 4 | Step 2584900 | Avg Loss: 0.0158 | Grad Norm: 0.00949620\n",
      "Epoch 4 | Step 2585000 | Avg Loss: 0.0158 | Grad Norm: 0.00842331\n",
      "Epoch 4 | Step 2585100 | Avg Loss: 0.0157 | Grad Norm: 0.00982689\n",
      "Epoch 4 | Step 2585200 | Avg Loss: 0.0158 | Grad Norm: 0.00965092\n",
      "Epoch 4 | Step 2585300 | Avg Loss: 0.0156 | Grad Norm: 0.01052598\n",
      "Epoch 4 | Step 2585400 | Avg Loss: 0.0157 | Grad Norm: 0.00877803\n",
      "Epoch 4 | Step 2585500 | Avg Loss: 0.0157 | Grad Norm: 0.00945761\n",
      "Epoch 4 | Step 2585600 | Avg Loss: 0.0158 | Grad Norm: 0.01269491\n",
      "Epoch 4 | Step 2585700 | Avg Loss: 0.0162 | Grad Norm: 0.00907987\n",
      "Epoch 4 | Step 2585800 | Avg Loss: 0.0159 | Grad Norm: 0.00811386\n",
      "Epoch 4 | Step 2585900 | Avg Loss: 0.0156 | Grad Norm: 0.00896392\n",
      "Epoch 4 | Step 2586000 | Avg Loss: 0.0154 | Grad Norm: 0.00789617\n",
      "Epoch 4 | Step 2586100 | Avg Loss: 0.0154 | Grad Norm: 0.00773379\n",
      "Epoch 4 | Step 2586200 | Avg Loss: 0.0152 | Grad Norm: 0.00810986\n",
      "Epoch 4 | Step 2586300 | Avg Loss: 0.0152 | Grad Norm: 0.00945985\n",
      "Epoch 4 | Step 2586400 | Avg Loss: 0.0155 | Grad Norm: 0.00922211\n",
      "Epoch 4 | Step 2586500 | Avg Loss: 0.0154 | Grad Norm: 0.00966430\n",
      "Epoch 4 | Step 2586600 | Avg Loss: 0.0153 | Grad Norm: 0.00790855\n",
      "Epoch 4 | Step 2586700 | Avg Loss: 0.0149 | Grad Norm: 0.00768790\n",
      "Epoch 4 | Step 2586800 | Avg Loss: 0.0147 | Grad Norm: 0.00815061\n",
      "Epoch 4 | Step 2586900 | Avg Loss: 0.0150 | Grad Norm: 0.00950386\n",
      "Epoch 4 | Step 2587000 | Avg Loss: 0.0151 | Grad Norm: 0.00888061\n",
      "Epoch 4 | Step 2587100 | Avg Loss: 0.0154 | Grad Norm: 0.00852324\n",
      "Epoch 4 | Step 2587200 | Avg Loss: 0.0155 | Grad Norm: 0.00992503\n",
      "Epoch 4 | Step 2587300 | Avg Loss: 0.0156 | Grad Norm: 0.00804489\n",
      "Epoch 4 | Step 2587400 | Avg Loss: 0.0155 | Grad Norm: 0.01023015\n",
      "Epoch 4 | Step 2587500 | Avg Loss: 0.0154 | Grad Norm: 0.00953389\n",
      "Epoch 4 | Step 2587600 | Avg Loss: 0.0151 | Grad Norm: 0.00839480\n",
      "Epoch 4 | Step 2587700 | Avg Loss: 0.0156 | Grad Norm: 0.00890312\n",
      "Epoch 4 | Step 2587800 | Avg Loss: 0.0157 | Grad Norm: 0.00816558\n",
      "Epoch 4 | Step 2587900 | Avg Loss: 0.0154 | Grad Norm: 0.01093347\n",
      "Epoch 4 | Step 2588000 | Avg Loss: 0.0155 | Grad Norm: 0.00967638\n",
      "Epoch 4 | Step 2588100 | Avg Loss: 0.0154 | Grad Norm: 0.00963453\n",
      "Epoch 4 | Step 2588200 | Avg Loss: 0.0152 | Grad Norm: 0.00865465\n",
      "Epoch 4 | Step 2588300 | Avg Loss: 0.0151 | Grad Norm: 0.00792708\n",
      "Epoch 4 | Step 2588400 | Avg Loss: 0.0149 | Grad Norm: 0.01004127\n",
      "Epoch 4 | Step 2588500 | Avg Loss: 0.0151 | Grad Norm: 0.01009233\n",
      "Epoch 4 | Step 2588600 | Avg Loss: 0.0147 | Grad Norm: 0.00894371\n",
      "Epoch 4 | Step 2588700 | Avg Loss: 0.0152 | Grad Norm: 0.01044107\n",
      "Epoch 4 | Step 2588800 | Avg Loss: 0.0150 | Grad Norm: 0.00807217\n",
      "Epoch 4 | Step 2588900 | Avg Loss: 0.0154 | Grad Norm: 0.00805298\n",
      "Epoch 4 | Step 2589000 | Avg Loss: 0.0152 | Grad Norm: 0.00824964\n",
      "Epoch 4 | Step 2589100 | Avg Loss: 0.0152 | Grad Norm: 0.00978581\n",
      "Epoch 4 | Step 2589200 | Avg Loss: 0.0153 | Grad Norm: 0.00964196\n",
      "Epoch 4 | Step 2589300 | Avg Loss: 0.0154 | Grad Norm: 0.00978676\n",
      "Epoch 4 | Step 2589400 | Avg Loss: 0.0152 | Grad Norm: 0.00776056\n",
      "Epoch 4 | Step 2589500 | Avg Loss: 0.0153 | Grad Norm: 0.01004618\n",
      "Epoch 4 | Step 2589600 | Avg Loss: 0.0158 | Grad Norm: 0.00849961\n",
      "Epoch 4 | Step 2589700 | Avg Loss: 0.0161 | Grad Norm: 0.00953707\n",
      "Epoch 4 | Step 2589800 | Avg Loss: 0.0159 | Grad Norm: 0.00824594\n",
      "Epoch 4 | Step 2589900 | Avg Loss: 0.0157 | Grad Norm: 0.00851127\n",
      "Epoch 4 | Step 2590000 | Avg Loss: 0.0154 | Grad Norm: 0.00931199\n",
      "Epoch 4 | Step 2590100 | Avg Loss: 0.0156 | Grad Norm: 0.00984697\n",
      "Epoch 4 | Step 2590200 | Avg Loss: 0.0157 | Grad Norm: 0.01035333\n",
      "Epoch 4 | Step 2590300 | Avg Loss: 0.0156 | Grad Norm: 0.00909061\n",
      "Epoch 4 | Step 2590400 | Avg Loss: 0.0156 | Grad Norm: 0.00977350\n",
      "Epoch 4 | Step 2590500 | Avg Loss: 0.0157 | Grad Norm: 0.00957236\n",
      "Epoch 4 | Step 2590600 | Avg Loss: 0.0151 | Grad Norm: 0.00862633\n",
      "Epoch 4 | Step 2590700 | Avg Loss: 0.0155 | Grad Norm: 0.00931662\n",
      "Epoch 4 | Step 2590800 | Avg Loss: 0.0154 | Grad Norm: 0.01077965\n",
      "Epoch 4 | Step 2590900 | Avg Loss: 0.0158 | Grad Norm: 0.01362059\n",
      "Epoch 4 | Step 2591000 | Avg Loss: 0.0157 | Grad Norm: 0.00821097\n",
      "Epoch 4 | Step 2591100 | Avg Loss: 0.0156 | Grad Norm: 0.00890160\n",
      "Epoch 4 | Step 2591200 | Avg Loss: 0.0152 | Grad Norm: 0.00926031\n",
      "Epoch 4 | Step 2591300 | Avg Loss: 0.0155 | Grad Norm: 0.00861531\n",
      "Epoch 4 | Step 2591400 | Avg Loss: 0.0157 | Grad Norm: 0.00899097\n",
      "Epoch 4 | Step 2591500 | Avg Loss: 0.0157 | Grad Norm: 0.00864288\n",
      "Epoch 4 | Step 2591600 | Avg Loss: 0.0161 | Grad Norm: 0.00897176\n",
      "Epoch 4 | Step 2591700 | Avg Loss: 0.0159 | Grad Norm: 0.00861121\n",
      "Epoch 4 | Step 2591800 | Avg Loss: 0.0160 | Grad Norm: 0.00892625\n",
      "Epoch 4 | Step 2591900 | Avg Loss: 0.0159 | Grad Norm: 0.00909065\n",
      "Epoch 4 | Step 2592000 | Avg Loss: 0.0160 | Grad Norm: 0.00945418\n",
      "Epoch 4 | Step 2592100 | Avg Loss: 0.0156 | Grad Norm: 0.00832689\n",
      "Epoch 4 | Step 2592200 | Avg Loss: 0.0151 | Grad Norm: 0.00783558\n",
      "Epoch 4 | Step 2592300 | Avg Loss: 0.0153 | Grad Norm: 0.00903857\n",
      "Epoch 4 | Step 2592400 | Avg Loss: 0.0153 | Grad Norm: 0.01012269\n",
      "Epoch 4 | Step 2592500 | Avg Loss: 0.0158 | Grad Norm: 0.01108694\n",
      "Epoch 4 | Step 2592600 | Avg Loss: 0.0159 | Grad Norm: 0.00968010\n",
      "Epoch 4 | Step 2592700 | Avg Loss: 0.0157 | Grad Norm: 0.00833034\n",
      "Epoch 4 | Step 2592800 | Avg Loss: 0.0156 | Grad Norm: 0.00916493\n",
      "Epoch 4 | Step 2592900 | Avg Loss: 0.0155 | Grad Norm: 0.00905039\n",
      "Epoch 4 | Step 2593000 | Avg Loss: 0.0154 | Grad Norm: 0.00899174\n",
      "Epoch 4 | Step 2593100 | Avg Loss: 0.0150 | Grad Norm: 0.00780597\n",
      "Epoch 4 | Step 2593200 | Avg Loss: 0.0154 | Grad Norm: 0.00853720\n",
      "Epoch 4 | Step 2593300 | Avg Loss: 0.0154 | Grad Norm: 0.00837993\n",
      "Epoch 4 | Step 2593400 | Avg Loss: 0.0157 | Grad Norm: 0.01084109\n",
      "Epoch 4 | Step 2593500 | Avg Loss: 0.0156 | Grad Norm: 0.00938351\n",
      "Epoch 4 | Step 2593600 | Avg Loss: 0.0153 | Grad Norm: 0.00893563\n",
      "Epoch 4 | Step 2593700 | Avg Loss: 0.0154 | Grad Norm: 0.00931111\n",
      "Epoch 4 | Step 2593800 | Avg Loss: 0.0153 | Grad Norm: 0.01003813\n",
      "Epoch 4 | Step 2593900 | Avg Loss: 0.0151 | Grad Norm: 0.00862761\n",
      "Epoch 4 | Step 2594000 | Avg Loss: 0.0150 | Grad Norm: 0.00925390\n",
      "Epoch 4 | Step 2594100 | Avg Loss: 0.0148 | Grad Norm: 0.00894107\n",
      "Epoch 4 | Step 2594200 | Avg Loss: 0.0146 | Grad Norm: 0.00856042\n",
      "Epoch 4 | Step 2594300 | Avg Loss: 0.0147 | Grad Norm: 0.00899739\n",
      "Epoch 4 | Step 2594400 | Avg Loss: 0.0150 | Grad Norm: 0.00861078\n",
      "Epoch 4 | Step 2594500 | Avg Loss: 0.0149 | Grad Norm: 0.01044301\n",
      "Epoch 4 | Step 2594600 | Avg Loss: 0.0147 | Grad Norm: 0.00869994\n",
      "Epoch 4 | Step 2594700 | Avg Loss: 0.0149 | Grad Norm: 0.00857761\n",
      "Epoch 4 | Step 2594800 | Avg Loss: 0.0146 | Grad Norm: 0.00822179\n",
      "Epoch 4 | Step 2594900 | Avg Loss: 0.0142 | Grad Norm: 0.01020919\n",
      "Epoch 4 | Step 2595000 | Avg Loss: 0.0143 | Grad Norm: 0.01005856\n",
      "Epoch 4 | Step 2595100 | Avg Loss: 0.0148 | Grad Norm: 0.00938883\n",
      "Epoch 4 | Step 2595200 | Avg Loss: 0.0147 | Grad Norm: 0.00788258\n",
      "Epoch 4 | Step 2595300 | Avg Loss: 0.0148 | Grad Norm: 0.00816738\n",
      "Epoch 4 | Step 2595400 | Avg Loss: 0.0152 | Grad Norm: 0.01002613\n",
      "Epoch 4 | Step 2595500 | Avg Loss: 0.0152 | Grad Norm: 0.00861473\n",
      "Epoch 4 | Step 2595600 | Avg Loss: 0.0154 | Grad Norm: 0.01048684\n",
      "Epoch 4 | Step 2595700 | Avg Loss: 0.0157 | Grad Norm: 0.00933799\n",
      "Epoch 4 | Step 2595800 | Avg Loss: 0.0158 | Grad Norm: 0.01000964\n",
      "Epoch 4 | Step 2595900 | Avg Loss: 0.0157 | Grad Norm: 0.00975991\n",
      "Epoch 4 | Step 2596000 | Avg Loss: 0.0154 | Grad Norm: 0.00842916\n",
      "Epoch 4 | Step 2596100 | Avg Loss: 0.0155 | Grad Norm: 0.00779026\n",
      "Epoch 4 | Step 2596200 | Avg Loss: 0.0158 | Grad Norm: 0.00883523\n",
      "Epoch 4 | Step 2596300 | Avg Loss: 0.0158 | Grad Norm: 0.00803508\n",
      "Epoch 4 | Step 2596400 | Avg Loss: 0.0156 | Grad Norm: 0.00896138\n",
      "Epoch 4 | Step 2596500 | Avg Loss: 0.0157 | Grad Norm: 0.00853210\n",
      "Epoch 4 | Step 2596600 | Avg Loss: 0.0155 | Grad Norm: 0.00959082\n",
      "Epoch 4 | Step 2596700 | Avg Loss: 0.0156 | Grad Norm: 0.00853176\n",
      "Epoch 4 | Step 2596800 | Avg Loss: 0.0151 | Grad Norm: 0.00978733\n",
      "Epoch 4 | Step 2596900 | Avg Loss: 0.0154 | Grad Norm: 0.00801445\n",
      "Epoch 4 | Step 2597000 | Avg Loss: 0.0152 | Grad Norm: 0.00918022\n",
      "Epoch 4 | Step 2597100 | Avg Loss: 0.0154 | Grad Norm: 0.00809392\n",
      "Epoch 4 | Step 2597200 | Avg Loss: 0.0155 | Grad Norm: 0.00929384\n",
      "Epoch 4 | Step 2597300 | Avg Loss: 0.0154 | Grad Norm: 0.00881518\n",
      "Epoch 4 | Step 2597400 | Avg Loss: 0.0154 | Grad Norm: 0.00891164\n",
      "Epoch 4 | Step 2597500 | Avg Loss: 0.0149 | Grad Norm: 0.00871243\n",
      "Epoch 4 | Step 2597600 | Avg Loss: 0.0148 | Grad Norm: 0.01802419\n",
      "Epoch 4 | Step 2597700 | Avg Loss: 0.0150 | Grad Norm: 0.00794631\n",
      "Epoch 4 | Step 2597800 | Avg Loss: 0.0151 | Grad Norm: 0.00855609\n",
      "Epoch 4 | Step 2597900 | Avg Loss: 0.0151 | Grad Norm: 0.00901355\n",
      "Epoch 4 | Step 2598000 | Avg Loss: 0.0154 | Grad Norm: 0.00958886\n",
      "Epoch 4 | Step 2598100 | Avg Loss: 0.0158 | Grad Norm: 0.00902461\n",
      "Epoch 4 | Step 2598200 | Avg Loss: 0.0155 | Grad Norm: 0.00899912\n",
      "Epoch 4 | Step 2598300 | Avg Loss: 0.0156 | Grad Norm: 0.00998300\n",
      "Epoch 4 | Step 2598400 | Avg Loss: 0.0155 | Grad Norm: 0.00842862\n",
      "Epoch 4 | Step 2598500 | Avg Loss: 0.0156 | Grad Norm: 0.00921392\n",
      "Epoch 4 | Step 2598600 | Avg Loss: 0.0155 | Grad Norm: 0.00837441\n",
      "Epoch 4 | Step 2598700 | Avg Loss: 0.0154 | Grad Norm: 0.00802267\n",
      "Epoch 4 | Step 2598800 | Avg Loss: 0.0152 | Grad Norm: 0.00944803\n",
      "Epoch 4 | Step 2598900 | Avg Loss: 0.0154 | Grad Norm: 0.00848697\n",
      "Epoch 4 | Step 2599000 | Avg Loss: 0.0151 | Grad Norm: 0.00912725\n",
      "Epoch 4 | Step 2599100 | Avg Loss: 0.0154 | Grad Norm: 0.00814523\n",
      "Epoch 4 | Step 2599200 | Avg Loss: 0.0153 | Grad Norm: 0.00881760\n",
      "Epoch 4 | Step 2599300 | Avg Loss: 0.0153 | Grad Norm: 0.00852045\n",
      "Epoch 4 | Step 2599400 | Avg Loss: 0.0155 | Grad Norm: 0.00949133\n",
      "Epoch 4 | Step 2599500 | Avg Loss: 0.0157 | Grad Norm: 0.00952051\n",
      "Epoch 4 | Step 2599600 | Avg Loss: 0.0156 | Grad Norm: 0.00809075\n",
      "Epoch 4 | Step 2599700 | Avg Loss: 0.0157 | Grad Norm: 0.00806610\n",
      "Epoch 4 | Step 2599800 | Avg Loss: 0.0156 | Grad Norm: 0.00857554\n",
      "Epoch 4 | Step 2599900 | Avg Loss: 0.0155 | Grad Norm: 0.01008503\n",
      "Epoch 4 | Step 2600000 | Avg Loss: 0.0152 | Grad Norm: 0.01174256\n",
      "Saving model at step2600000\n",
      "Epoch 4 | Step 2600100 | Avg Loss: 0.0154 | Grad Norm: 0.00829718\n",
      "Epoch 4 | Step 2600200 | Avg Loss: 0.0156 | Grad Norm: 0.00740401\n",
      "Epoch 4 | Step 2600300 | Avg Loss: 0.0153 | Grad Norm: 0.00827791\n",
      "Epoch 4 | Step 2600400 | Avg Loss: 0.0151 | Grad Norm: 0.00750641\n",
      "Epoch 4 | Step 2600500 | Avg Loss: 0.0152 | Grad Norm: 0.00959320\n",
      "Epoch 4 | Step 2600600 | Avg Loss: 0.0151 | Grad Norm: 0.01016775\n",
      "Epoch 4 | Step 2600700 | Avg Loss: 0.0153 | Grad Norm: 0.00974923\n",
      "Epoch 4 | Step 2600800 | Avg Loss: 0.0153 | Grad Norm: 0.00957393\n",
      "Epoch 4 | Step 2600900 | Avg Loss: 0.0154 | Grad Norm: 0.00965038\n",
      "Epoch 4 | Step 2601000 | Avg Loss: 0.0156 | Grad Norm: 0.01011598\n",
      "Epoch 4 | Step 2601100 | Avg Loss: 0.0152 | Grad Norm: 0.00864457\n",
      "Epoch 4 | Step 2601200 | Avg Loss: 0.0154 | Grad Norm: 0.00864619\n",
      "Epoch 4 | Step 2601300 | Avg Loss: 0.0156 | Grad Norm: 0.00854155\n",
      "Epoch 4 | Step 2601400 | Avg Loss: 0.0156 | Grad Norm: 0.00890863\n",
      "Epoch 4 | Step 2601500 | Avg Loss: 0.0158 | Grad Norm: 0.01112369\n",
      "Epoch 4 | Step 2601600 | Avg Loss: 0.0159 | Grad Norm: 0.00937242\n",
      "Epoch 4 | Step 2601700 | Avg Loss: 0.0160 | Grad Norm: 0.01082631\n",
      "Epoch 4 | Step 2601800 | Avg Loss: 0.0157 | Grad Norm: 0.00952142\n",
      "Epoch 4 | Step 2601900 | Avg Loss: 0.0153 | Grad Norm: 0.00887629\n",
      "Epoch 4 | Step 2602000 | Avg Loss: 0.0151 | Grad Norm: 0.00865560\n",
      "Epoch 4 | Step 2602100 | Avg Loss: 0.0153 | Grad Norm: 0.00807259\n",
      "Epoch 4 | Step 2602200 | Avg Loss: 0.0149 | Grad Norm: 0.00825593\n",
      "Epoch 4 | Step 2602300 | Avg Loss: 0.0152 | Grad Norm: 0.00984083\n",
      "Epoch 4 | Step 2602400 | Avg Loss: 0.0155 | Grad Norm: 0.00995366\n",
      "Epoch 4 | Step 2602500 | Avg Loss: 0.0159 | Grad Norm: 0.00835448\n",
      "Epoch 4 | Step 2602600 | Avg Loss: 0.0158 | Grad Norm: 0.01004998\n",
      "Epoch 4 | Step 2602700 | Avg Loss: 0.0159 | Grad Norm: 0.00904200\n",
      "Epoch 4 | Step 2602800 | Avg Loss: 0.0158 | Grad Norm: 0.01996327\n",
      "Epoch 4 | Step 2602900 | Avg Loss: 0.0157 | Grad Norm: 0.00917055\n",
      "Epoch 4 | Step 2603000 | Avg Loss: 0.0152 | Grad Norm: 0.00876950\n",
      "Epoch 4 | Step 2603100 | Avg Loss: 0.0153 | Grad Norm: 0.00873713\n",
      "Epoch 4 | Step 2603200 | Avg Loss: 0.0151 | Grad Norm: 0.01100418\n",
      "Epoch 4 | Step 2603300 | Avg Loss: 0.0154 | Grad Norm: 0.00955004\n",
      "Epoch 4 | Step 2603400 | Avg Loss: 0.0150 | Grad Norm: 0.00969011\n",
      "Epoch 4 | Step 2603500 | Avg Loss: 0.0153 | Grad Norm: 0.00810698\n",
      "Epoch 4 | Step 2603600 | Avg Loss: 0.0152 | Grad Norm: 0.00825058\n",
      "Epoch 4 | Step 2603700 | Avg Loss: 0.0151 | Grad Norm: 0.00940722\n",
      "Epoch 4 | Step 2603800 | Avg Loss: 0.0152 | Grad Norm: 0.00906398\n",
      "Epoch 4 | Step 2603900 | Avg Loss: 0.0152 | Grad Norm: 0.01022609\n",
      "Epoch 4 | Step 2604000 | Avg Loss: 0.0150 | Grad Norm: 0.00872182\n",
      "Epoch 4 | Step 2604100 | Avg Loss: 0.0153 | Grad Norm: 0.00880133\n",
      "Epoch 4 | Step 2604200 | Avg Loss: 0.0154 | Grad Norm: 0.00956527\n",
      "Epoch 4 | Step 2604300 | Avg Loss: 0.0156 | Grad Norm: 0.00854949\n",
      "Epoch 4 | Step 2604400 | Avg Loss: 0.0154 | Grad Norm: 0.00913319\n",
      "Epoch 4 | Step 2604500 | Avg Loss: 0.0160 | Grad Norm: 0.00963793\n",
      "Epoch 4 | Step 2604600 | Avg Loss: 0.0159 | Grad Norm: 0.01002960\n",
      "Epoch 4 | Step 2604700 | Avg Loss: 0.0156 | Grad Norm: 0.01059003\n",
      "Epoch 4 | Step 2604800 | Avg Loss: 0.0155 | Grad Norm: 0.00864726\n",
      "Epoch 4 | Step 2604900 | Avg Loss: 0.0153 | Grad Norm: 0.00885429\n",
      "Epoch 4 | Step 2605000 | Avg Loss: 0.0155 | Grad Norm: 0.00801657\n",
      "Epoch 4 | Step 2605100 | Avg Loss: 0.0156 | Grad Norm: 0.01022406\n",
      "Epoch 4 | Step 2605200 | Avg Loss: 0.0157 | Grad Norm: 0.00825275\n",
      "Epoch 4 | Step 2605300 | Avg Loss: 0.0154 | Grad Norm: 0.00885225\n",
      "Epoch 4 | Step 2605400 | Avg Loss: 0.0156 | Grad Norm: 0.00779914\n",
      "Epoch 4 | Step 2605500 | Avg Loss: 0.0154 | Grad Norm: 0.00874563\n",
      "Epoch 4 | Step 2605600 | Avg Loss: 0.0153 | Grad Norm: 0.00891633\n",
      "Epoch 4 | Step 2605700 | Avg Loss: 0.0157 | Grad Norm: 0.00896732\n",
      "Epoch 4 | Step 2605800 | Avg Loss: 0.0156 | Grad Norm: 0.00903175\n",
      "Epoch 4 | Step 2605900 | Avg Loss: 0.0160 | Grad Norm: 0.00884580\n",
      "Epoch 4 | Step 2606000 | Avg Loss: 0.0157 | Grad Norm: 0.00927187\n",
      "Epoch 4 | Step 2606100 | Avg Loss: 0.0154 | Grad Norm: 0.00947450\n",
      "Epoch 4 | Step 2606200 | Avg Loss: 0.0155 | Grad Norm: 0.00921339\n",
      "Epoch 4 | Step 2606300 | Avg Loss: 0.0159 | Grad Norm: 0.00917996\n",
      "Epoch 4 | Step 2606400 | Avg Loss: 0.0154 | Grad Norm: 0.00899976\n",
      "Epoch 4 | Step 2606500 | Avg Loss: 0.0154 | Grad Norm: 0.00912622\n",
      "Epoch 4 | Step 2606600 | Avg Loss: 0.0156 | Grad Norm: 0.00885105\n",
      "Epoch 4 | Step 2606700 | Avg Loss: 0.0158 | Grad Norm: 0.00989706\n",
      "Epoch 4 | Step 2606800 | Avg Loss: 0.0152 | Grad Norm: 0.00901286\n",
      "Epoch 4 | Step 2606900 | Avg Loss: 0.0157 | Grad Norm: 0.00952578\n",
      "Epoch 4 | Step 2607000 | Avg Loss: 0.0153 | Grad Norm: 0.00757962\n",
      "Epoch 4 | Step 2607100 | Avg Loss: 0.0157 | Grad Norm: 0.00868175\n",
      "Epoch 4 | Step 2607200 | Avg Loss: 0.0156 | Grad Norm: 0.00879521\n",
      "Epoch 4 | Step 2607300 | Avg Loss: 0.0155 | Grad Norm: 0.00950152\n",
      "Epoch 4 | Step 2607400 | Avg Loss: 0.0159 | Grad Norm: 0.00950746\n",
      "Epoch 4 | Step 2607500 | Avg Loss: 0.0159 | Grad Norm: 0.00909415\n",
      "Epoch 4 | Step 2607600 | Avg Loss: 0.0159 | Grad Norm: 0.00879020\n",
      "Epoch 4 | Step 2607700 | Avg Loss: 0.0159 | Grad Norm: 0.00901723\n",
      "Epoch 4 | Step 2607800 | Avg Loss: 0.0158 | Grad Norm: 0.00931033\n",
      "Epoch 4 | Step 2607900 | Avg Loss: 0.0163 | Grad Norm: 0.00972424\n",
      "Epoch 4 | Step 2608000 | Avg Loss: 0.0162 | Grad Norm: 0.00924458\n",
      "Epoch 4 | Step 2608100 | Avg Loss: 0.0160 | Grad Norm: 0.00839264\n",
      "Epoch 4 | Step 2608200 | Avg Loss: 0.0161 | Grad Norm: 0.00814772\n",
      "Epoch 4 | Step 2608300 | Avg Loss: 0.0157 | Grad Norm: 0.00940732\n",
      "Epoch 4 | Step 2608400 | Avg Loss: 0.0157 | Grad Norm: 0.00837379\n",
      "Epoch 4 | Step 2608500 | Avg Loss: 0.0158 | Grad Norm: 0.01019386\n",
      "Epoch 4 | Step 2608600 | Avg Loss: 0.0162 | Grad Norm: 0.00884078\n",
      "Epoch 4 | Step 2608700 | Avg Loss: 0.0159 | Grad Norm: 0.00832947\n",
      "Epoch 4 | Step 2608800 | Avg Loss: 0.0161 | Grad Norm: 0.00962562\n",
      "Epoch 4 | Step 2608900 | Avg Loss: 0.0159 | Grad Norm: 0.00880920\n",
      "Epoch 4 | Step 2609000 | Avg Loss: 0.0156 | Grad Norm: 0.00862211\n",
      "Epoch 4 | Step 2609100 | Avg Loss: 0.0151 | Grad Norm: 0.01039291\n",
      "Epoch 4 | Step 2609200 | Avg Loss: 0.0158 | Grad Norm: 0.00867336\n",
      "Epoch 4 | Step 2609300 | Avg Loss: 0.0163 | Grad Norm: 0.00930785\n",
      "Epoch 4 | Step 2609400 | Avg Loss: 0.0158 | Grad Norm: 0.00884408\n",
      "Epoch 4 | Step 2609500 | Avg Loss: 0.0158 | Grad Norm: 0.00772293\n",
      "Epoch 4 | Step 2609600 | Avg Loss: 0.0158 | Grad Norm: 0.00960193\n",
      "Epoch 4 | Step 2609700 | Avg Loss: 0.0154 | Grad Norm: 0.00978935\n",
      "Epoch 4 | Step 2609800 | Avg Loss: 0.0153 | Grad Norm: 0.00815489\n",
      "Epoch 4 | Step 2609900 | Avg Loss: 0.0155 | Grad Norm: 0.00840756\n",
      "Epoch 4 | Step 2610000 | Avg Loss: 0.0157 | Grad Norm: 0.00833898\n",
      "Epoch 4 | Step 2610100 | Avg Loss: 0.0156 | Grad Norm: 0.01036594\n",
      "Epoch 4 | Step 2610200 | Avg Loss: 0.0159 | Grad Norm: 0.00915865\n",
      "Epoch 4 | Step 2610300 | Avg Loss: 0.0160 | Grad Norm: 0.00934182\n",
      "Epoch 4 | Step 2610400 | Avg Loss: 0.0157 | Grad Norm: 0.00907497\n",
      "Epoch 4 | Step 2610500 | Avg Loss: 0.0160 | Grad Norm: 0.00896841\n",
      "Epoch 4 | Step 2610600 | Avg Loss: 0.0157 | Grad Norm: 0.00876562\n",
      "Epoch 4 | Step 2610700 | Avg Loss: 0.0154 | Grad Norm: 0.00995653\n",
      "Epoch 4 | Step 2610800 | Avg Loss: 0.0155 | Grad Norm: 0.00906491\n",
      "Epoch 4 | Step 2610900 | Avg Loss: 0.0155 | Grad Norm: 0.00814807\n",
      "Epoch 4 | Step 2611000 | Avg Loss: 0.0154 | Grad Norm: 0.00937254\n",
      "Epoch 4 | Step 2611100 | Avg Loss: 0.0150 | Grad Norm: 0.00891034\n",
      "Epoch 4 | Step 2611200 | Avg Loss: 0.0150 | Grad Norm: 0.00992469\n",
      "Epoch 4 | Step 2611300 | Avg Loss: 0.0154 | Grad Norm: 0.01117583\n",
      "Epoch 4 | Step 2611400 | Avg Loss: 0.0154 | Grad Norm: 0.01205538\n",
      "Epoch 4 | Step 2611500 | Avg Loss: 0.0152 | Grad Norm: 0.00832082\n",
      "Epoch 4 | Step 2611600 | Avg Loss: 0.0154 | Grad Norm: 0.00744876\n",
      "Epoch 4 | Step 2611700 | Avg Loss: 0.0154 | Grad Norm: 0.00782262\n",
      "Epoch 4 | Step 2611800 | Avg Loss: 0.0153 | Grad Norm: 0.01035586\n",
      "Epoch 4 | Step 2611900 | Avg Loss: 0.0152 | Grad Norm: 0.00856567\n",
      "Epoch 4 | Step 2612000 | Avg Loss: 0.0154 | Grad Norm: 0.00913493\n",
      "Epoch 4 | Step 2612100 | Avg Loss: 0.0154 | Grad Norm: 0.00858003\n",
      "Epoch 4 | Step 2612200 | Avg Loss: 0.0156 | Grad Norm: 0.00986637\n",
      "Epoch 4 | Step 2612300 | Avg Loss: 0.0158 | Grad Norm: 0.00850643\n",
      "Epoch 4 | Step 2612400 | Avg Loss: 0.0156 | Grad Norm: 0.00814647\n",
      "Epoch 4 | Step 2612500 | Avg Loss: 0.0157 | Grad Norm: 0.00911767\n",
      "Epoch 4 | Step 2612600 | Avg Loss: 0.0158 | Grad Norm: 0.00951207\n",
      "Epoch 4 | Step 2612700 | Avg Loss: 0.0157 | Grad Norm: 0.00837357\n",
      "Epoch 4 | Step 2612800 | Avg Loss: 0.0158 | Grad Norm: 0.01245390\n",
      "Epoch 4 | Step 2612900 | Avg Loss: 0.0157 | Grad Norm: 0.00858730\n",
      "Epoch 4 | Step 2613000 | Avg Loss: 0.0157 | Grad Norm: 0.00855444\n",
      "Epoch 4 | Step 2613100 | Avg Loss: 0.0157 | Grad Norm: 0.00923800\n",
      "Epoch 4 | Step 2613200 | Avg Loss: 0.0158 | Grad Norm: 0.00828876\n",
      "Epoch 4 | Step 2613300 | Avg Loss: 0.0159 | Grad Norm: 0.00935546\n",
      "Epoch 4 | Step 2613400 | Avg Loss: 0.0155 | Grad Norm: 0.00917950\n",
      "Epoch 4 | Step 2613500 | Avg Loss: 0.0153 | Grad Norm: 0.00878347\n",
      "Epoch 4 | Step 2613600 | Avg Loss: 0.0154 | Grad Norm: 0.00912306\n",
      "Epoch 4 | Step 2613700 | Avg Loss: 0.0155 | Grad Norm: 0.00750628\n",
      "Epoch 4 | Step 2613800 | Avg Loss: 0.0157 | Grad Norm: 0.00883900\n",
      "Epoch 4 | Step 2613900 | Avg Loss: 0.0157 | Grad Norm: 0.01086557\n",
      "Epoch 4 | Step 2614000 | Avg Loss: 0.0158 | Grad Norm: 0.01111710\n",
      "Epoch 4 | Step 2614100 | Avg Loss: 0.0157 | Grad Norm: 0.01131339\n",
      "Epoch 4 | Step 2614200 | Avg Loss: 0.0153 | Grad Norm: 0.00899336\n",
      "Epoch 4 | Step 2614300 | Avg Loss: 0.0156 | Grad Norm: 0.00868952\n",
      "Epoch 4 | Step 2614400 | Avg Loss: 0.0155 | Grad Norm: 0.00955911\n",
      "Epoch 4 | Step 2614500 | Avg Loss: 0.0153 | Grad Norm: 0.00816615\n",
      "Epoch 4 | Step 2614600 | Avg Loss: 0.0151 | Grad Norm: 0.00945366\n",
      "Epoch 4 | Step 2614700 | Avg Loss: 0.0148 | Grad Norm: 0.00928030\n",
      "Epoch 4 | Step 2614800 | Avg Loss: 0.0147 | Grad Norm: 0.00900682\n",
      "Epoch 4 | Step 2614900 | Avg Loss: 0.0150 | Grad Norm: 0.00904677\n",
      "Epoch 4 | Step 2615000 | Avg Loss: 0.0152 | Grad Norm: 0.00848654\n",
      "Epoch 4 | Step 2615100 | Avg Loss: 0.0151 | Grad Norm: 0.00886279\n",
      "Epoch 4 | Step 2615200 | Avg Loss: 0.0153 | Grad Norm: 0.00835534\n",
      "Epoch 4 | Step 2615300 | Avg Loss: 0.0155 | Grad Norm: 0.01172759\n",
      "Epoch 4 | Step 2615400 | Avg Loss: 0.0153 | Grad Norm: 0.00945564\n",
      "Epoch 4 | Step 2615500 | Avg Loss: 0.0152 | Grad Norm: 0.01009403\n",
      "Epoch 4 | Step 2615600 | Avg Loss: 0.0154 | Grad Norm: 0.00801086\n",
      "Epoch 4 | Step 2615700 | Avg Loss: 0.0153 | Grad Norm: 0.01069890\n",
      "Epoch 4 | Step 2615800 | Avg Loss: 0.0152 | Grad Norm: 0.01027189\n",
      "Epoch 4 | Step 2615900 | Avg Loss: 0.0153 | Grad Norm: 0.00862747\n",
      "Epoch 4 | Step 2616000 | Avg Loss: 0.0158 | Grad Norm: 0.00873399\n",
      "Epoch 4 | Step 2616100 | Avg Loss: 0.0157 | Grad Norm: 0.00921781\n",
      "Epoch 4 | Step 2616200 | Avg Loss: 0.0155 | Grad Norm: 0.00850072\n",
      "Epoch 4 | Step 2616300 | Avg Loss: 0.0157 | Grad Norm: 0.00909332\n",
      "Epoch 4 | Step 2616400 | Avg Loss: 0.0159 | Grad Norm: 0.00897378\n",
      "Epoch 4 | Step 2616500 | Avg Loss: 0.0157 | Grad Norm: 0.00954044\n",
      "Epoch 4 | Step 2616600 | Avg Loss: 0.0158 | Grad Norm: 0.00753526\n",
      "Epoch 4 | Step 2616700 | Avg Loss: 0.0153 | Grad Norm: 0.00805365\n",
      "Epoch 4 | Step 2616800 | Avg Loss: 0.0150 | Grad Norm: 0.00895811\n",
      "Epoch 4 | Step 2616900 | Avg Loss: 0.0152 | Grad Norm: 0.00987208\n",
      "Epoch 4 | Step 2617000 | Avg Loss: 0.0153 | Grad Norm: 0.01065435\n",
      "Epoch 4 | Step 2617100 | Avg Loss: 0.0154 | Grad Norm: 0.00807205\n",
      "Epoch 4 | Step 2617200 | Avg Loss: 0.0151 | Grad Norm: 0.00775714\n",
      "Epoch 4 | Step 2617300 | Avg Loss: 0.0151 | Grad Norm: 0.00889965\n",
      "Epoch 4 | Step 2617400 | Avg Loss: 0.0153 | Grad Norm: 0.00886674\n",
      "Epoch 4 | Step 2617500 | Avg Loss: 0.0154 | Grad Norm: 0.00979737\n",
      "Epoch 4 | Step 2617600 | Avg Loss: 0.0151 | Grad Norm: 0.00913031\n",
      "Epoch 4 | Step 2617700 | Avg Loss: 0.0153 | Grad Norm: 0.00986326\n",
      "Epoch 4 | Step 2617800 | Avg Loss: 0.0153 | Grad Norm: 0.00790725\n",
      "Epoch 4 | Step 2617900 | Avg Loss: 0.0153 | Grad Norm: 0.00978053\n",
      "Epoch 4 | Step 2618000 | Avg Loss: 0.0151 | Grad Norm: 0.00925590\n",
      "Epoch 4 | Step 2618100 | Avg Loss: 0.0153 | Grad Norm: 0.00885810\n",
      "Epoch 4 | Step 2618200 | Avg Loss: 0.0151 | Grad Norm: 0.00851051\n",
      "Epoch 4 | Step 2618300 | Avg Loss: 0.0149 | Grad Norm: 0.00902115\n",
      "Epoch 4 | Step 2618400 | Avg Loss: 0.0149 | Grad Norm: 0.00898874\n",
      "Epoch 4 | Step 2618500 | Avg Loss: 0.0150 | Grad Norm: 0.00846048\n",
      "Epoch 4 | Step 2618600 | Avg Loss: 0.0153 | Grad Norm: 0.00813403\n",
      "Epoch 4 | Step 2618700 | Avg Loss: 0.0157 | Grad Norm: 0.00893336\n",
      "Epoch 4 | Step 2618800 | Avg Loss: 0.0157 | Grad Norm: 0.00805936\n",
      "Epoch 4 | Step 2618900 | Avg Loss: 0.0157 | Grad Norm: 0.00970790\n",
      "Epoch 4 | Step 2619000 | Avg Loss: 0.0156 | Grad Norm: 0.00857817\n",
      "Epoch 4 | Step 2619100 | Avg Loss: 0.0154 | Grad Norm: 0.00822804\n",
      "Epoch 4 | Step 2619200 | Avg Loss: 0.0153 | Grad Norm: 0.00897555\n",
      "Epoch 4 | Step 2619300 | Avg Loss: 0.0152 | Grad Norm: 0.00788740\n",
      "Epoch 4 | Step 2619400 | Avg Loss: 0.0155 | Grad Norm: 0.00873157\n",
      "Epoch 4 | Step 2619500 | Avg Loss: 0.0151 | Grad Norm: 0.00953339\n",
      "Epoch 4 | Step 2619600 | Avg Loss: 0.0153 | Grad Norm: 0.01073680\n",
      "Epoch 4 | Step 2619700 | Avg Loss: 0.0157 | Grad Norm: 0.00914960\n",
      "Epoch 4 | Step 2619800 | Avg Loss: 0.0158 | Grad Norm: 0.01052914\n",
      "Epoch 4 | Step 2619900 | Avg Loss: 0.0155 | Grad Norm: 0.00857860\n",
      "Epoch 4 | Step 2620000 | Avg Loss: 0.0156 | Grad Norm: 0.00821005\n",
      "Epoch 4 | Step 2620100 | Avg Loss: 0.0157 | Grad Norm: 0.01000471\n",
      "Epoch 4 | Step 2620200 | Avg Loss: 0.0162 | Grad Norm: 0.00950203\n",
      "Epoch 4 | Step 2620300 | Avg Loss: 0.0156 | Grad Norm: 0.00963977\n",
      "Epoch 4 | Step 2620400 | Avg Loss: 0.0151 | Grad Norm: 0.00802343\n",
      "Epoch 4 | Step 2620500 | Avg Loss: 0.0150 | Grad Norm: 0.00961626\n",
      "Epoch 4 | Step 2620600 | Avg Loss: 0.0155 | Grad Norm: 0.00789242\n",
      "Epoch 4 | Step 2620700 | Avg Loss: 0.0154 | Grad Norm: 0.00874248\n",
      "Epoch 4 | Step 2620800 | Avg Loss: 0.0155 | Grad Norm: 0.00844032\n",
      "Epoch 4 | Step 2620900 | Avg Loss: 0.0151 | Grad Norm: 0.00965703\n",
      "Epoch 4 | Step 2621000 | Avg Loss: 0.0150 | Grad Norm: 0.00852189\n",
      "Epoch 4 | Step 2621100 | Avg Loss: 0.0152 | Grad Norm: 0.01118759\n",
      "Epoch 4 | Step 2621200 | Avg Loss: 0.0154 | Grad Norm: 0.00827418\n",
      "Epoch 4 | Step 2621300 | Avg Loss: 0.0154 | Grad Norm: 0.01017255\n",
      "Epoch 4 | Step 2621400 | Avg Loss: 0.0151 | Grad Norm: 0.00943918\n",
      "Epoch 4 | Step 2621500 | Avg Loss: 0.0150 | Grad Norm: 0.00990266\n",
      "Epoch 4 | Step 2621600 | Avg Loss: 0.0151 | Grad Norm: 0.00797162\n",
      "Epoch 4 | Step 2621700 | Avg Loss: 0.0152 | Grad Norm: 0.00987723\n",
      "Epoch 4 | Step 2621800 | Avg Loss: 0.0155 | Grad Norm: 0.00987707\n",
      "Epoch 4 | Step 2621900 | Avg Loss: 0.0155 | Grad Norm: 0.00817613\n",
      "Epoch 4 | Step 2622000 | Avg Loss: 0.0151 | Grad Norm: 0.00928895\n",
      "Epoch 4 | Step 2622100 | Avg Loss: 0.0154 | Grad Norm: 0.00888223\n",
      "Epoch 4 | Step 2622200 | Avg Loss: 0.0152 | Grad Norm: 0.00860385\n",
      "Epoch 4 | Step 2622300 | Avg Loss: 0.0151 | Grad Norm: 0.00824121\n",
      "Epoch 4 | Step 2622400 | Avg Loss: 0.0152 | Grad Norm: 0.00811096\n",
      "Epoch 4 | Step 2622500 | Avg Loss: 0.0151 | Grad Norm: 0.00833753\n",
      "Epoch 4 | Step 2622600 | Avg Loss: 0.0157 | Grad Norm: 0.00979882\n",
      "Epoch 4 | Step 2622700 | Avg Loss: 0.0154 | Grad Norm: 0.00791037\n",
      "Epoch 4 | Step 2622800 | Avg Loss: 0.0157 | Grad Norm: 0.00843399\n",
      "Epoch 4 | Step 2622900 | Avg Loss: 0.0156 | Grad Norm: 0.00855956\n",
      "Epoch 4 | Step 2623000 | Avg Loss: 0.0157 | Grad Norm: 0.00828469\n",
      "Epoch 4 | Step 2623100 | Avg Loss: 0.0155 | Grad Norm: 0.01061262\n",
      "Epoch 4 | Step 2623200 | Avg Loss: 0.0160 | Grad Norm: 0.00861915\n",
      "Epoch 4 | Step 2623300 | Avg Loss: 0.0164 | Grad Norm: 0.00917278\n",
      "Epoch 4 | Step 2623400 | Avg Loss: 0.0158 | Grad Norm: 0.00894042\n",
      "Epoch 4 | Step 2623500 | Avg Loss: 0.0159 | Grad Norm: 0.00819540\n",
      "Epoch 4 | Step 2623600 | Avg Loss: 0.0157 | Grad Norm: 0.00911338\n",
      "Epoch 4 | Step 2623700 | Avg Loss: 0.0162 | Grad Norm: 0.00947990\n",
      "Epoch 4 | Step 2623800 | Avg Loss: 0.0158 | Grad Norm: 0.01130289\n",
      "Epoch 4 | Step 2623900 | Avg Loss: 0.0156 | Grad Norm: 0.00844566\n",
      "Epoch 4 | Step 2624000 | Avg Loss: 0.0155 | Grad Norm: 0.00942161\n",
      "Epoch 4 | Step 2624100 | Avg Loss: 0.0155 | Grad Norm: 0.00855840\n",
      "Epoch 4 | Step 2624200 | Avg Loss: 0.0155 | Grad Norm: 0.01223803\n",
      "Epoch 4 | Step 2624300 | Avg Loss: 0.0156 | Grad Norm: 0.00874037\n",
      "Epoch 4 | Step 2624400 | Avg Loss: 0.0154 | Grad Norm: 0.01082820\n",
      "Epoch 4 | Step 2624500 | Avg Loss: 0.0156 | Grad Norm: 0.00863177\n",
      "Epoch 4 | Step 2624600 | Avg Loss: 0.0155 | Grad Norm: 0.01309674\n",
      "Epoch 4 | Step 2624700 | Avg Loss: 0.0156 | Grad Norm: 0.00825661\n",
      "Epoch 4 | Step 2624800 | Avg Loss: 0.0156 | Grad Norm: 0.00991000\n",
      "Epoch 4 | Step 2624900 | Avg Loss: 0.0156 | Grad Norm: 0.00846357\n",
      "Epoch 4 | Step 2625000 | Avg Loss: 0.0159 | Grad Norm: 0.00856444\n",
      "Epoch 4 | Step 2625100 | Avg Loss: 0.0159 | Grad Norm: 0.00902143\n",
      "Epoch 4 | Step 2625200 | Avg Loss: 0.0162 | Grad Norm: 0.00905901\n",
      "Epoch 4 | Step 2625300 | Avg Loss: 0.0158 | Grad Norm: 0.01058483\n",
      "Epoch 4 | Step 2625400 | Avg Loss: 0.0157 | Grad Norm: 0.00888009\n",
      "Epoch 4 | Step 2625500 | Avg Loss: 0.0157 | Grad Norm: 0.00944309\n",
      "Epoch 4 | Step 2625600 | Avg Loss: 0.0158 | Grad Norm: 0.00951989\n",
      "Epoch 4 | Step 2625700 | Avg Loss: 0.0157 | Grad Norm: 0.00932691\n",
      "Epoch 4 | Step 2625800 | Avg Loss: 0.0156 | Grad Norm: 0.01009368\n",
      "Epoch 4 | Step 2625900 | Avg Loss: 0.0153 | Grad Norm: 0.01067094\n",
      "Epoch 4 | Step 2626000 | Avg Loss: 0.0155 | Grad Norm: 0.01222572\n",
      "Epoch 4 | Step 2626100 | Avg Loss: 0.0156 | Grad Norm: 0.00917154\n",
      "Epoch 4 | Step 2626200 | Avg Loss: 0.0155 | Grad Norm: 0.00892452\n",
      "Epoch 4 | Step 2626300 | Avg Loss: 0.0154 | Grad Norm: 0.00961030\n",
      "Epoch 4 | Step 2626400 | Avg Loss: 0.0154 | Grad Norm: 0.00865557\n",
      "Epoch 4 | Step 2626500 | Avg Loss: 0.0151 | Grad Norm: 0.00854497\n",
      "Epoch 4 | Step 2626600 | Avg Loss: 0.0152 | Grad Norm: 0.00999981\n",
      "Epoch 4 | Step 2626700 | Avg Loss: 0.0158 | Grad Norm: 0.00826524\n",
      "Epoch 4 | Step 2626800 | Avg Loss: 0.0157 | Grad Norm: 0.00986753\n",
      "Epoch 4 | Step 2626900 | Avg Loss: 0.0156 | Grad Norm: 0.00911656\n",
      "Epoch 4 | Step 2627000 | Avg Loss: 0.0156 | Grad Norm: 0.01011794\n",
      "Epoch 4 | Step 2627100 | Avg Loss: 0.0158 | Grad Norm: 0.00904156\n",
      "Epoch 4 | Step 2627200 | Avg Loss: 0.0157 | Grad Norm: 0.00937913\n",
      "Epoch 4 | Step 2627300 | Avg Loss: 0.0156 | Grad Norm: 0.01041604\n",
      "Epoch 4 | Step 2627400 | Avg Loss: 0.0154 | Grad Norm: 0.00851004\n",
      "Epoch 4 | Step 2627500 | Avg Loss: 0.0155 | Grad Norm: 0.00958467\n",
      "Epoch 4 | Step 2627600 | Avg Loss: 0.0154 | Grad Norm: 0.00980918\n",
      "Epoch 4 | Step 2627700 | Avg Loss: 0.0152 | Grad Norm: 0.00980010\n",
      "Epoch 4 | Step 2627800 | Avg Loss: 0.0153 | Grad Norm: 0.01074974\n",
      "Epoch 4 | Step 2627900 | Avg Loss: 0.0152 | Grad Norm: 0.00845832\n",
      "Epoch 4 | Step 2628000 | Avg Loss: 0.0152 | Grad Norm: 0.00952355\n",
      "Epoch 4 | Step 2628100 | Avg Loss: 0.0152 | Grad Norm: 0.00877800\n",
      "Epoch 4 | Step 2628200 | Avg Loss: 0.0153 | Grad Norm: 0.00870477\n",
      "Epoch 4 | Step 2628300 | Avg Loss: 0.0154 | Grad Norm: 0.00804220\n",
      "Epoch 4 | Step 2628400 | Avg Loss: 0.0160 | Grad Norm: 0.01052541\n",
      "Epoch 4 | Step 2628500 | Avg Loss: 0.0161 | Grad Norm: 0.01060556\n",
      "Epoch 4 | Step 2628600 | Avg Loss: 0.0161 | Grad Norm: 0.01082091\n",
      "Epoch 4 | Step 2628700 | Avg Loss: 0.0158 | Grad Norm: 0.00859592\n",
      "Epoch 4 | Step 2628800 | Avg Loss: 0.0163 | Grad Norm: 0.01056452\n",
      "Epoch 4 | Step 2628900 | Avg Loss: 0.0158 | Grad Norm: 0.00933165\n",
      "Epoch 4 | Step 2629000 | Avg Loss: 0.0159 | Grad Norm: 0.00878364\n",
      "Epoch 4 | Step 2629100 | Avg Loss: 0.0154 | Grad Norm: 0.00958059\n",
      "Epoch 4 | Step 2629200 | Avg Loss: 0.0152 | Grad Norm: 0.00929545\n",
      "Epoch 4 | Step 2629300 | Avg Loss: 0.0151 | Grad Norm: 0.00788437\n",
      "Epoch 4 | Step 2629400 | Avg Loss: 0.0151 | Grad Norm: 0.00883816\n",
      "Epoch 4 | Step 2629500 | Avg Loss: 0.0149 | Grad Norm: 0.00931130\n",
      "Epoch 4 | Step 2629600 | Avg Loss: 0.0150 | Grad Norm: 0.00853207\n",
      "Epoch 4 | Step 2629700 | Avg Loss: 0.0147 | Grad Norm: 0.00865045\n",
      "Epoch 4 | Step 2629800 | Avg Loss: 0.0151 | Grad Norm: 0.00736216\n",
      "Epoch 4 | Step 2629900 | Avg Loss: 0.0152 | Grad Norm: 0.00842435\n",
      "Epoch 4 | Step 2630000 | Avg Loss: 0.0153 | Grad Norm: 0.00896350\n",
      "Epoch 4 | Step 2630100 | Avg Loss: 0.0152 | Grad Norm: 0.00919240\n",
      "Epoch 4 | Step 2630200 | Avg Loss: 0.0152 | Grad Norm: 0.00888775\n",
      "Epoch 4 | Step 2630300 | Avg Loss: 0.0149 | Grad Norm: 0.00957914\n",
      "Epoch 4 | Step 2630400 | Avg Loss: 0.0154 | Grad Norm: 0.00933118\n",
      "Epoch 4 | Step 2630500 | Avg Loss: 0.0155 | Grad Norm: 0.00865604\n",
      "Epoch 4 | Step 2630600 | Avg Loss: 0.0158 | Grad Norm: 0.00968635\n",
      "Epoch 4 | Step 2630700 | Avg Loss: 0.0161 | Grad Norm: 0.00833092\n",
      "Epoch 4 | Step 2630800 | Avg Loss: 0.0160 | Grad Norm: 0.00898440\n",
      "Epoch 4 | Step 2630900 | Avg Loss: 0.0160 | Grad Norm: 0.00855440\n",
      "Epoch 4 | Step 2631000 | Avg Loss: 0.0163 | Grad Norm: 0.00940513\n",
      "Epoch 4 | Step 2631100 | Avg Loss: 0.0159 | Grad Norm: 0.01006425\n",
      "Epoch 4 | Step 2631200 | Avg Loss: 0.0157 | Grad Norm: 0.00882752\n",
      "Epoch 4 | Step 2631300 | Avg Loss: 0.0162 | Grad Norm: 0.01109445\n",
      "Epoch 4 | Step 2631400 | Avg Loss: 0.0162 | Grad Norm: 0.00786299\n",
      "Epoch 4 | Step 2631500 | Avg Loss: 0.0159 | Grad Norm: 0.00902477\n",
      "Epoch 4 | Step 2631600 | Avg Loss: 0.0159 | Grad Norm: 0.00903082\n",
      "Epoch 4 | Step 2631700 | Avg Loss: 0.0157 | Grad Norm: 0.00943525\n",
      "Epoch 4 | Step 2631800 | Avg Loss: 0.0153 | Grad Norm: 0.00918698\n",
      "Epoch 4 | Step 2631900 | Avg Loss: 0.0153 | Grad Norm: 0.00860824\n",
      "Epoch 4 | Step 2632000 | Avg Loss: 0.0152 | Grad Norm: 0.00840275\n",
      "Epoch 4 | Step 2632100 | Avg Loss: 0.0154 | Grad Norm: 0.00769603\n",
      "Epoch 4 | Step 2632200 | Avg Loss: 0.0153 | Grad Norm: 0.00850506\n",
      "Epoch 4 | Step 2632300 | Avg Loss: 0.0154 | Grad Norm: 0.00977209\n",
      "Epoch 4 | Step 2632400 | Avg Loss: 0.0157 | Grad Norm: 0.00836713\n",
      "Epoch 4 | Step 2632500 | Avg Loss: 0.0158 | Grad Norm: 0.00869155\n",
      "Epoch 4 | Step 2632600 | Avg Loss: 0.0157 | Grad Norm: 0.00848819\n",
      "Epoch 4 | Step 2632700 | Avg Loss: 0.0156 | Grad Norm: 0.00873671\n",
      "Epoch 4 | Step 2632800 | Avg Loss: 0.0158 | Grad Norm: 0.00977830\n",
      "Epoch 4 | Step 2632900 | Avg Loss: 0.0160 | Grad Norm: 0.00827874\n",
      "Epoch 4 | Step 2633000 | Avg Loss: 0.0158 | Grad Norm: 0.00866713\n",
      "Epoch 4 | Step 2633100 | Avg Loss: 0.0156 | Grad Norm: 0.00900281\n",
      "Epoch 4 | Step 2633200 | Avg Loss: 0.0155 | Grad Norm: 0.01102388\n",
      "Epoch 4 | Step 2633300 | Avg Loss: 0.0152 | Grad Norm: 0.01092674\n",
      "Epoch 4 | Step 2633400 | Avg Loss: 0.0155 | Grad Norm: 0.00868307\n",
      "Epoch 4 | Step 2633500 | Avg Loss: 0.0158 | Grad Norm: 0.00810419\n",
      "Epoch 4 | Step 2633600 | Avg Loss: 0.0157 | Grad Norm: 0.00865769\n",
      "Epoch 4 | Step 2633700 | Avg Loss: 0.0157 | Grad Norm: 0.00956734\n",
      "Epoch 4 | Step 2633800 | Avg Loss: 0.0159 | Grad Norm: 0.01002150\n",
      "Epoch 4 | Step 2633900 | Avg Loss: 0.0160 | Grad Norm: 0.00836552\n",
      "Epoch 4 | Step 2634000 | Avg Loss: 0.0156 | Grad Norm: 0.00963981\n",
      "Epoch 4 | Step 2634100 | Avg Loss: 0.0154 | Grad Norm: 0.00868492\n",
      "Epoch 4 | Step 2634200 | Avg Loss: 0.0151 | Grad Norm: 0.00964279\n",
      "Epoch 4 | Step 2634300 | Avg Loss: 0.0151 | Grad Norm: 0.00908020\n",
      "Epoch 4 | Step 2634400 | Avg Loss: 0.0151 | Grad Norm: 0.00895007\n",
      "Epoch 4 | Step 2634500 | Avg Loss: 0.0147 | Grad Norm: 0.00871475\n",
      "Epoch 4 | Step 2634600 | Avg Loss: 0.0149 | Grad Norm: 0.00952321\n",
      "Epoch 4 | Step 2634700 | Avg Loss: 0.0153 | Grad Norm: 0.00825782\n",
      "Epoch 4 | Step 2634800 | Avg Loss: 0.0155 | Grad Norm: 0.00997366\n",
      "Epoch 4 | Step 2634900 | Avg Loss: 0.0156 | Grad Norm: 0.00868245\n",
      "Epoch 4 | Step 2635000 | Avg Loss: 0.0158 | Grad Norm: 0.01246932\n",
      "Epoch 4 | Step 2635100 | Avg Loss: 0.0159 | Grad Norm: 0.00890309\n",
      "Epoch 4 | Step 2635200 | Avg Loss: 0.0162 | Grad Norm: 0.00969840\n",
      "Epoch 4 | Step 2635300 | Avg Loss: 0.0153 | Grad Norm: 0.00742757\n",
      "Epoch 4 | Step 2635400 | Avg Loss: 0.0155 | Grad Norm: 0.00887179\n",
      "Epoch 4 | Step 2635500 | Avg Loss: 0.0160 | Grad Norm: 0.00872263\n",
      "Epoch 4 | Step 2635600 | Avg Loss: 0.0159 | Grad Norm: 0.00968575\n",
      "Epoch 4 | Step 2635700 | Avg Loss: 0.0158 | Grad Norm: 0.00842481\n",
      "Epoch 4 | Step 2635800 | Avg Loss: 0.0162 | Grad Norm: 0.00984837\n",
      "Epoch 4 | Step 2635900 | Avg Loss: 0.0160 | Grad Norm: 0.01034714\n",
      "Epoch 4 | Step 2636000 | Avg Loss: 0.0157 | Grad Norm: 0.00971122\n",
      "Epoch 4 | Step 2636100 | Avg Loss: 0.0154 | Grad Norm: 0.00876191\n",
      "Epoch 4 | Step 2636200 | Avg Loss: 0.0154 | Grad Norm: 0.00890852\n",
      "Epoch 4 | Step 2636300 | Avg Loss: 0.0152 | Grad Norm: 0.00817250\n",
      "Epoch 4 | Step 2636400 | Avg Loss: 0.0154 | Grad Norm: 0.00896158\n",
      "Epoch 4 | Step 2636500 | Avg Loss: 0.0154 | Grad Norm: 0.00927006\n",
      "Epoch 4 | Step 2636600 | Avg Loss: 0.0158 | Grad Norm: 0.01031095\n",
      "Epoch 4 | Step 2636700 | Avg Loss: 0.0159 | Grad Norm: 0.01172568\n",
      "Epoch 4 | Step 2636800 | Avg Loss: 0.0161 | Grad Norm: 0.00871233\n",
      "Epoch 4 | Step 2636900 | Avg Loss: 0.0161 | Grad Norm: 0.00980845\n",
      "Epoch 4 | Step 2637000 | Avg Loss: 0.0159 | Grad Norm: 0.00918285\n",
      "Epoch 4 | Step 2637100 | Avg Loss: 0.0158 | Grad Norm: 0.00896242\n",
      "Epoch 4 | Step 2637200 | Avg Loss: 0.0157 | Grad Norm: 0.00836086\n",
      "Epoch 4 | Step 2637300 | Avg Loss: 0.0154 | Grad Norm: 0.00863294\n",
      "Epoch 4 | Step 2637400 | Avg Loss: 0.0155 | Grad Norm: 0.00892301\n",
      "Epoch 4 | Step 2637500 | Avg Loss: 0.0153 | Grad Norm: 0.00999627\n",
      "Epoch 4 | Step 2637600 | Avg Loss: 0.0154 | Grad Norm: 0.00954392\n",
      "Epoch 4 | Step 2637700 | Avg Loss: 0.0151 | Grad Norm: 0.00976114\n",
      "Epoch 4 | Step 2637800 | Avg Loss: 0.0148 | Grad Norm: 0.00985047\n",
      "Epoch 4 | Step 2637900 | Avg Loss: 0.0151 | Grad Norm: 0.00824999\n",
      "Epoch 4 | Step 2638000 | Avg Loss: 0.0153 | Grad Norm: 0.01000191\n",
      "Epoch 4 | Step 2638100 | Avg Loss: 0.0155 | Grad Norm: 0.00966773\n",
      "Epoch 4 | Step 2638200 | Avg Loss: 0.0157 | Grad Norm: 0.01033814\n",
      "Epoch 4 | Step 2638300 | Avg Loss: 0.0156 | Grad Norm: 0.00923886\n",
      "Epoch 4 | Step 2638400 | Avg Loss: 0.0161 | Grad Norm: 0.00913081\n",
      "Epoch 4 | Step 2638500 | Avg Loss: 0.0161 | Grad Norm: 0.01145083\n",
      "Epoch 4 | Step 2638600 | Avg Loss: 0.0160 | Grad Norm: 0.00934385\n",
      "Epoch 4 | Step 2638700 | Avg Loss: 0.0161 | Grad Norm: 0.00877265\n",
      "Epoch 4 | Step 2638800 | Avg Loss: 0.0157 | Grad Norm: 0.00844712\n",
      "Epoch 4 | Step 2638900 | Avg Loss: 0.0155 | Grad Norm: 0.00900142\n",
      "Epoch 4 | Step 2639000 | Avg Loss: 0.0154 | Grad Norm: 0.00874458\n",
      "Epoch 4 | Step 2639100 | Avg Loss: 0.0154 | Grad Norm: 0.00877531\n",
      "Epoch 4 | Step 2639200 | Avg Loss: 0.0155 | Grad Norm: 0.00864178\n",
      "Epoch 4 | Step 2639300 | Avg Loss: 0.0157 | Grad Norm: 0.00878753\n",
      "Epoch 4 | Step 2639400 | Avg Loss: 0.0156 | Grad Norm: 0.01054575\n",
      "Epoch 4 | Step 2639500 | Avg Loss: 0.0153 | Grad Norm: 0.00870440\n",
      "Epoch 4 | Step 2639600 | Avg Loss: 0.0155 | Grad Norm: 0.00866472\n",
      "Epoch 4 | Step 2639700 | Avg Loss: 0.0157 | Grad Norm: 0.00936926\n",
      "Epoch 4 | Step 2639800 | Avg Loss: 0.0159 | Grad Norm: 0.00887985\n",
      "Epoch 4 | Step 2639900 | Avg Loss: 0.0161 | Grad Norm: 0.01076104\n",
      "Epoch 4 | Step 2640000 | Avg Loss: 0.0159 | Grad Norm: 0.00978656\n",
      "Epoch 4 | Step 2640100 | Avg Loss: 0.0163 | Grad Norm: 0.01066074\n",
      "Epoch 4 | Step 2640200 | Avg Loss: 0.0158 | Grad Norm: 0.00925265\n",
      "Epoch 4 | Step 2640300 | Avg Loss: 0.0156 | Grad Norm: 0.00900898\n",
      "Epoch 4 | Step 2640400 | Avg Loss: 0.0155 | Grad Norm: 0.01128974\n",
      "Epoch 4 | Step 2640500 | Avg Loss: 0.0155 | Grad Norm: 0.00931410\n",
      "Epoch 4 | Step 2640600 | Avg Loss: 0.0155 | Grad Norm: 0.00897870\n",
      "Epoch 4 | Step 2640700 | Avg Loss: 0.0156 | Grad Norm: 0.00995654\n",
      "Epoch 4 | Step 2640800 | Avg Loss: 0.0160 | Grad Norm: 0.00909855\n",
      "Epoch 4 | Step 2640900 | Avg Loss: 0.0160 | Grad Norm: 0.00988662\n",
      "Epoch 4 | Step 2641000 | Avg Loss: 0.0158 | Grad Norm: 0.00843967\n",
      "Epoch 4 | Step 2641100 | Avg Loss: 0.0157 | Grad Norm: 0.00923605\n",
      "Epoch 4 | Step 2641200 | Avg Loss: 0.0160 | Grad Norm: 0.00857597\n",
      "Epoch 4 | Step 2641300 | Avg Loss: 0.0164 | Grad Norm: 0.00851680\n",
      "Epoch 4 | Step 2641400 | Avg Loss: 0.0163 | Grad Norm: 0.00856755\n",
      "Epoch 4 | Step 2641500 | Avg Loss: 0.0164 | Grad Norm: 0.00939887\n",
      "Epoch 4 | Step 2641600 | Avg Loss: 0.0166 | Grad Norm: 0.00931673\n",
      "Epoch 4 | Step 2641700 | Avg Loss: 0.0164 | Grad Norm: 0.01209655\n",
      "Epoch 4 | Step 2641800 | Avg Loss: 0.0162 | Grad Norm: 0.00840949\n",
      "Epoch 4 | Step 2641900 | Avg Loss: 0.0156 | Grad Norm: 0.00973979\n",
      "Epoch 4 | Step 2642000 | Avg Loss: 0.0154 | Grad Norm: 0.01177895\n",
      "Epoch 4 | Step 2642100 | Avg Loss: 0.0157 | Grad Norm: 0.00922767\n",
      "Epoch 4 | Step 2642200 | Avg Loss: 0.0154 | Grad Norm: 0.01020377\n",
      "Epoch 4 | Step 2642300 | Avg Loss: 0.0155 | Grad Norm: 0.00908865\n",
      "Epoch 4 | Step 2642400 | Avg Loss: 0.0158 | Grad Norm: 0.00967952\n",
      "Epoch 4 | Step 2642500 | Avg Loss: 0.0154 | Grad Norm: 0.00888340\n",
      "Epoch 4 | Step 2642600 | Avg Loss: 0.0157 | Grad Norm: 0.00872364\n",
      "Epoch 4 | Step 2642700 | Avg Loss: 0.0153 | Grad Norm: 0.01003181\n",
      "Epoch 4 | Step 2642800 | Avg Loss: 0.0151 | Grad Norm: 0.00903897\n",
      "Epoch 4 | Step 2642900 | Avg Loss: 0.0154 | Grad Norm: 0.00853118\n",
      "Epoch 4 | Step 2643000 | Avg Loss: 0.0154 | Grad Norm: 0.00976490\n",
      "Epoch 4 | Step 2643100 | Avg Loss: 0.0153 | Grad Norm: 0.00958610\n",
      "Epoch 4 | Step 2643200 | Avg Loss: 0.0155 | Grad Norm: 0.00844489\n",
      "Epoch 4 | Step 2643300 | Avg Loss: 0.0153 | Grad Norm: 0.00833752\n",
      "Epoch 4 | Step 2643400 | Avg Loss: 0.0153 | Grad Norm: 0.01014651\n",
      "Epoch 4 | Step 2643500 | Avg Loss: 0.0153 | Grad Norm: 0.00862989\n",
      "Epoch 4 | Step 2643600 | Avg Loss: 0.0151 | Grad Norm: 0.00956566\n",
      "Epoch 4 | Step 2643700 | Avg Loss: 0.0150 | Grad Norm: 0.00887359\n",
      "Epoch 4 | Step 2643800 | Avg Loss: 0.0151 | Grad Norm: 0.00871866\n",
      "Epoch 4 | Step 2643900 | Avg Loss: 0.0151 | Grad Norm: 0.00814230\n",
      "Epoch 4 | Step 2644000 | Avg Loss: 0.0152 | Grad Norm: 0.00803019\n",
      "Epoch 4 | Step 2644100 | Avg Loss: 0.0148 | Grad Norm: 0.01125559\n",
      "Epoch 4 | Step 2644200 | Avg Loss: 0.0147 | Grad Norm: 0.00859730\n",
      "Epoch 4 | Step 2644300 | Avg Loss: 0.0148 | Grad Norm: 0.00843256\n",
      "Epoch 4 | Step 2644400 | Avg Loss: 0.0149 | Grad Norm: 0.00849032\n",
      "Epoch 4 | Step 2644500 | Avg Loss: 0.0146 | Grad Norm: 0.00856959\n",
      "Epoch 4 | Step 2644600 | Avg Loss: 0.0147 | Grad Norm: 0.00750575\n",
      "Epoch 4 | Step 2644700 | Avg Loss: 0.0151 | Grad Norm: 0.00991687\n",
      "Epoch 4 | Step 2644800 | Avg Loss: 0.0152 | Grad Norm: 0.01006002\n",
      "Epoch 4 | Step 2644900 | Avg Loss: 0.0151 | Grad Norm: 0.00883303\n",
      "Epoch 4 | Step 2645000 | Avg Loss: 0.0153 | Grad Norm: 0.00996825\n",
      "Epoch 4 | Step 2645100 | Avg Loss: 0.0154 | Grad Norm: 0.01352267\n",
      "Epoch 4 | Step 2645200 | Avg Loss: 0.0156 | Grad Norm: 0.01163715\n",
      "Epoch 4 | Step 2645300 | Avg Loss: 0.0153 | Grad Norm: 0.00885368\n",
      "Epoch 4 | Step 2645400 | Avg Loss: 0.0155 | Grad Norm: 0.01099728\n",
      "Epoch 4 | Step 2645500 | Avg Loss: 0.0157 | Grad Norm: 0.01012483\n",
      "Epoch 4 | Step 2645600 | Avg Loss: 0.0155 | Grad Norm: 0.00920486\n",
      "Epoch 4 | Step 2645700 | Avg Loss: 0.0151 | Grad Norm: 0.00810895\n",
      "Epoch 4 | Step 2645800 | Avg Loss: 0.0153 | Grad Norm: 0.00923482\n",
      "Epoch 4 | Step 2645900 | Avg Loss: 0.0151 | Grad Norm: 0.01029162\n",
      "Epoch 4 | Step 2646000 | Avg Loss: 0.0151 | Grad Norm: 0.01118176\n",
      "Epoch 4 | Step 2646100 | Avg Loss: 0.0149 | Grad Norm: 0.00893570\n",
      "Epoch 4 | Step 2646200 | Avg Loss: 0.0150 | Grad Norm: 0.01085430\n",
      "Epoch 4 | Step 2646300 | Avg Loss: 0.0151 | Grad Norm: 0.00890362\n",
      "Epoch 4 | Step 2646400 | Avg Loss: 0.0150 | Grad Norm: 0.00904051\n",
      "Epoch 4 | Step 2646500 | Avg Loss: 0.0152 | Grad Norm: 0.01314373\n",
      "Epoch 4 | Step 2646600 | Avg Loss: 0.0154 | Grad Norm: 0.01165304\n",
      "Epoch 4 | Step 2646700 | Avg Loss: 0.0154 | Grad Norm: 0.00914050\n",
      "Epoch 4 | Step 2646800 | Avg Loss: 0.0156 | Grad Norm: 0.00910760\n",
      "Epoch 4 | Step 2646900 | Avg Loss: 0.0155 | Grad Norm: 0.01051987\n",
      "Epoch 4 | Step 2647000 | Avg Loss: 0.0156 | Grad Norm: 0.00797744\n",
      "Epoch 4 | Step 2647100 | Avg Loss: 0.0155 | Grad Norm: 0.01159839\n",
      "Epoch 4 | Step 2647200 | Avg Loss: 0.0154 | Grad Norm: 0.01008505\n",
      "Epoch 4 | Step 2647300 | Avg Loss: 0.0152 | Grad Norm: 0.00915210\n",
      "Epoch 4 | Step 2647400 | Avg Loss: 0.0153 | Grad Norm: 0.01088135\n",
      "Epoch 4 | Step 2647500 | Avg Loss: 0.0154 | Grad Norm: 0.00916776\n",
      "Epoch 4 | Step 2647600 | Avg Loss: 0.0152 | Grad Norm: 0.00836874\n",
      "Epoch 4 | Step 2647700 | Avg Loss: 0.0153 | Grad Norm: 0.00865708\n",
      "Epoch 4 | Step 2647800 | Avg Loss: 0.0154 | Grad Norm: 0.00849619\n",
      "Epoch 4 | Step 2647900 | Avg Loss: 0.0154 | Grad Norm: 0.00798447\n",
      "Epoch 4 | Step 2648000 | Avg Loss: 0.0154 | Grad Norm: 0.00841199\n",
      "Epoch 4 | Step 2648100 | Avg Loss: 0.0154 | Grad Norm: 0.00810415\n",
      "Epoch 4 | Step 2648200 | Avg Loss: 0.0150 | Grad Norm: 0.01129488\n",
      "Epoch 4 | Step 2648300 | Avg Loss: 0.0155 | Grad Norm: 0.00919258\n",
      "Epoch 4 | Step 2648400 | Avg Loss: 0.0153 | Grad Norm: 0.00790922\n",
      "Epoch 4 | Step 2648500 | Avg Loss: 0.0153 | Grad Norm: 0.00909263\n",
      "Epoch 4 | Step 2648600 | Avg Loss: 0.0151 | Grad Norm: 0.00896315\n",
      "Epoch 4 | Step 2648700 | Avg Loss: 0.0152 | Grad Norm: 0.00951995\n",
      "Epoch 4 | Step 2648800 | Avg Loss: 0.0153 | Grad Norm: 0.00947502\n",
      "Epoch 4 | Step 2648900 | Avg Loss: 0.0154 | Grad Norm: 0.00867865\n",
      "Epoch 4 | Step 2649000 | Avg Loss: 0.0153 | Grad Norm: 0.00874241\n",
      "Epoch 4 | Step 2649100 | Avg Loss: 0.0153 | Grad Norm: 0.00762781\n",
      "Epoch 4 | Step 2649200 | Avg Loss: 0.0150 | Grad Norm: 0.00915881\n",
      "Epoch 4 | Step 2649300 | Avg Loss: 0.0155 | Grad Norm: 0.00840793\n",
      "Epoch 4 | Step 2649400 | Avg Loss: 0.0154 | Grad Norm: 0.00900216\n",
      "Epoch 4 | Step 2649500 | Avg Loss: 0.0155 | Grad Norm: 0.00840504\n",
      "Epoch 4 | Step 2649600 | Avg Loss: 0.0154 | Grad Norm: 0.00972747\n",
      "Epoch 4 | Step 2649700 | Avg Loss: 0.0154 | Grad Norm: 0.00934278\n",
      "Epoch 4 | Step 2649800 | Avg Loss: 0.0156 | Grad Norm: 0.00959061\n",
      "Epoch 4 | Step 2649900 | Avg Loss: 0.0151 | Grad Norm: 0.00822051\n",
      "Epoch 4 | Step 2650000 | Avg Loss: 0.0150 | Grad Norm: 0.00919343\n",
      "Epoch 4 | Step 2650100 | Avg Loss: 0.0152 | Grad Norm: 0.00930249\n",
      "Epoch 4 | Step 2650200 | Avg Loss: 0.0154 | Grad Norm: 0.01045905\n",
      "Epoch 4 | Step 2650300 | Avg Loss: 0.0154 | Grad Norm: 0.00890238\n",
      "Epoch 4 | Step 2650400 | Avg Loss: 0.0155 | Grad Norm: 0.00854703\n",
      "Epoch 4 | Step 2650500 | Avg Loss: 0.0153 | Grad Norm: 0.00789588\n",
      "Epoch 4 | Step 2650600 | Avg Loss: 0.0153 | Grad Norm: 0.00803970\n",
      "Epoch 4 | Step 2650700 | Avg Loss: 0.0154 | Grad Norm: 0.00796805\n",
      "Epoch 4 | Step 2650800 | Avg Loss: 0.0156 | Grad Norm: 0.00857909\n",
      "Epoch 4 | Step 2650900 | Avg Loss: 0.0159 | Grad Norm: 0.00864144\n",
      "Epoch 4 | Step 2651000 | Avg Loss: 0.0155 | Grad Norm: 0.00842327\n",
      "Epoch 4 | Step 2651100 | Avg Loss: 0.0155 | Grad Norm: 0.00879927\n",
      "Epoch 4 | Step 2651200 | Avg Loss: 0.0157 | Grad Norm: 0.00875142\n",
      "Epoch 4 | Step 2651300 | Avg Loss: 0.0155 | Grad Norm: 0.00816537\n",
      "Epoch 4 | Step 2651400 | Avg Loss: 0.0153 | Grad Norm: 0.00802739\n",
      "Epoch 4 | Step 2651500 | Avg Loss: 0.0154 | Grad Norm: 0.00774356\n",
      "Epoch 4 | Step 2651600 | Avg Loss: 0.0154 | Grad Norm: 0.01221633\n",
      "Epoch 4 | Step 2651700 | Avg Loss: 0.0155 | Grad Norm: 0.00860148\n",
      "Epoch 4 | Step 2651800 | Avg Loss: 0.0161 | Grad Norm: 0.00978763\n",
      "Epoch 4 | Step 2651900 | Avg Loss: 0.0156 | Grad Norm: 0.00913593\n",
      "Epoch 4 | Step 2652000 | Avg Loss: 0.0152 | Grad Norm: 0.00936164\n",
      "Epoch 4 | Step 2652100 | Avg Loss: 0.0152 | Grad Norm: 0.01006401\n",
      "Epoch 4 | Step 2652200 | Avg Loss: 0.0156 | Grad Norm: 0.01106853\n",
      "Epoch 4 | Step 2652300 | Avg Loss: 0.0153 | Grad Norm: 0.00783360\n",
      "Epoch 4 | Step 2652400 | Avg Loss: 0.0154 | Grad Norm: 0.01161922\n",
      "Epoch 4 | Step 2652500 | Avg Loss: 0.0151 | Grad Norm: 0.00861629\n",
      "Epoch 4 | Step 2652600 | Avg Loss: 0.0154 | Grad Norm: 0.01037285\n",
      "Epoch 4 | Step 2652700 | Avg Loss: 0.0153 | Grad Norm: 0.00883153\n",
      "Epoch 4 | Step 2652800 | Avg Loss: 0.0154 | Grad Norm: 0.00808264\n",
      "Epoch 4 | Step 2652900 | Avg Loss: 0.0149 | Grad Norm: 0.01122657\n",
      "Epoch 4 | Step 2653000 | Avg Loss: 0.0153 | Grad Norm: 0.00832616\n",
      "Epoch 4 | Step 2653100 | Avg Loss: 0.0154 | Grad Norm: 0.00876690\n",
      "Epoch 4 | Step 2653200 | Avg Loss: 0.0151 | Grad Norm: 0.00925189\n",
      "Epoch 4 | Step 2653300 | Avg Loss: 0.0149 | Grad Norm: 0.00871886\n",
      "Epoch 4 | Step 2653400 | Avg Loss: 0.0150 | Grad Norm: 0.00866782\n",
      "Epoch 4 | Step 2653500 | Avg Loss: 0.0151 | Grad Norm: 0.00954139\n",
      "Epoch 4 | Step 2653600 | Avg Loss: 0.0151 | Grad Norm: 0.00838718\n",
      "Epoch 4 | Step 2653700 | Avg Loss: 0.0156 | Grad Norm: 0.00928774\n",
      "Epoch 4 | Step 2653800 | Avg Loss: 0.0157 | Grad Norm: 0.01114985\n",
      "Epoch 4 | Step 2653900 | Avg Loss: 0.0154 | Grad Norm: 0.00871701\n",
      "Epoch 4 | Step 2654000 | Avg Loss: 0.0158 | Grad Norm: 0.00771146\n",
      "Epoch 4 | Step 2654100 | Avg Loss: 0.0154 | Grad Norm: 0.00790597\n",
      "Epoch 4 | Step 2654200 | Avg Loss: 0.0154 | Grad Norm: 0.00891702\n",
      "Epoch 4 | Step 2654300 | Avg Loss: 0.0153 | Grad Norm: 0.00892942\n",
      "Epoch 4 | Step 2654400 | Avg Loss: 0.0152 | Grad Norm: 0.00975157\n",
      "Epoch 4 | Step 2654500 | Avg Loss: 0.0152 | Grad Norm: 0.00842691\n",
      "Epoch 4 | Step 2654600 | Avg Loss: 0.0155 | Grad Norm: 0.00993057\n",
      "Epoch 4 | Step 2654700 | Avg Loss: 0.0154 | Grad Norm: 0.00842024\n",
      "Epoch 4 | Step 2654800 | Avg Loss: 0.0154 | Grad Norm: 0.00882650\n",
      "Epoch 4 | Step 2654900 | Avg Loss: 0.0155 | Grad Norm: 0.00900899\n",
      "Epoch 4 | Step 2655000 | Avg Loss: 0.0160 | Grad Norm: 0.01040832\n",
      "Epoch 4 | Step 2655100 | Avg Loss: 0.0160 | Grad Norm: 0.00916228\n",
      "Epoch 4 | Step 2655200 | Avg Loss: 0.0156 | Grad Norm: 0.00918903\n",
      "Epoch 4 | Step 2655300 | Avg Loss: 0.0153 | Grad Norm: 0.00775038\n",
      "Epoch 4 | Step 2655400 | Avg Loss: 0.0154 | Grad Norm: 0.00793247\n",
      "Epoch 4 | Step 2655500 | Avg Loss: 0.0152 | Grad Norm: 0.00832457\n",
      "Epoch 4 | Step 2655600 | Avg Loss: 0.0152 | Grad Norm: 0.00829806\n",
      "Epoch 4 | Step 2655700 | Avg Loss: 0.0152 | Grad Norm: 0.01008297\n",
      "Epoch 4 | Step 2655800 | Avg Loss: 0.0154 | Grad Norm: 0.00927689\n",
      "Epoch 4 | Step 2655900 | Avg Loss: 0.0157 | Grad Norm: 0.00924796\n",
      "Epoch 4 | Step 2656000 | Avg Loss: 0.0157 | Grad Norm: 0.01095042\n",
      "Epoch 4 | Step 2656100 | Avg Loss: 0.0156 | Grad Norm: 0.00803507\n",
      "Epoch 4 | Step 2656200 | Avg Loss: 0.0158 | Grad Norm: 0.00900600\n",
      "Epoch 4 | Step 2656300 | Avg Loss: 0.0158 | Grad Norm: 0.00889520\n",
      "Epoch 4 | Step 2656400 | Avg Loss: 0.0157 | Grad Norm: 0.00967626\n",
      "Epoch 4 | Step 2656500 | Avg Loss: 0.0157 | Grad Norm: 0.00835517\n",
      "Epoch 4 | Step 2656600 | Avg Loss: 0.0156 | Grad Norm: 0.00930824\n",
      "Epoch 4 | Step 2656700 | Avg Loss: 0.0157 | Grad Norm: 0.00916673\n",
      "Epoch 4 | Step 2656800 | Avg Loss: 0.0157 | Grad Norm: 0.00945209\n",
      "Epoch 4 | Step 2656900 | Avg Loss: 0.0153 | Grad Norm: 0.00871258\n",
      "Epoch 4 | Step 2657000 | Avg Loss: 0.0155 | Grad Norm: 0.00857930\n",
      "Epoch 4 | Step 2657100 | Avg Loss: 0.0152 | Grad Norm: 0.00806162\n",
      "Epoch 4 | Step 2657200 | Avg Loss: 0.0149 | Grad Norm: 0.00890273\n",
      "Epoch 4 | Step 2657300 | Avg Loss: 0.0151 | Grad Norm: 0.00901908\n",
      "Epoch 4 | Step 2657400 | Avg Loss: 0.0148 | Grad Norm: 0.00893065\n",
      "Epoch 4 | Step 2657500 | Avg Loss: 0.0151 | Grad Norm: 0.00855009\n",
      "Epoch 4 | Step 2657600 | Avg Loss: 0.0153 | Grad Norm: 0.00810635\n",
      "Epoch 4 | Step 2657700 | Avg Loss: 0.0151 | Grad Norm: 0.00970863\n",
      "Epoch 4 | Step 2657800 | Avg Loss: 0.0151 | Grad Norm: 0.00818325\n",
      "Epoch 4 | Step 2657900 | Avg Loss: 0.0150 | Grad Norm: 0.00890966\n",
      "Epoch 4 | Step 2658000 | Avg Loss: 0.0149 | Grad Norm: 0.00857037\n",
      "Epoch 4 | Step 2658100 | Avg Loss: 0.0147 | Grad Norm: 0.00888281\n",
      "Epoch 4 | Step 2658200 | Avg Loss: 0.0149 | Grad Norm: 0.00830173\n",
      "Epoch 4 | Step 2658300 | Avg Loss: 0.0153 | Grad Norm: 0.00893336\n",
      "Epoch 4 | Step 2658400 | Avg Loss: 0.0152 | Grad Norm: 0.00802744\n",
      "Epoch 4 | Step 2658500 | Avg Loss: 0.0154 | Grad Norm: 0.00915052\n",
      "Epoch 4 | Step 2658600 | Avg Loss: 0.0159 | Grad Norm: 0.00812577\n",
      "Epoch 4 | Step 2658700 | Avg Loss: 0.0160 | Grad Norm: 0.00917254\n",
      "Epoch 4 | Step 2658800 | Avg Loss: 0.0160 | Grad Norm: 0.01043006\n",
      "Epoch 4 | Step 2658900 | Avg Loss: 0.0161 | Grad Norm: 0.01057091\n",
      "Epoch 4 | Step 2659000 | Avg Loss: 0.0159 | Grad Norm: 0.01042093\n",
      "Epoch 4 | Step 2659100 | Avg Loss: 0.0156 | Grad Norm: 0.00922426\n",
      "Epoch 4 | Step 2659200 | Avg Loss: 0.0158 | Grad Norm: 0.01013540\n",
      "Epoch 4 | Step 2659300 | Avg Loss: 0.0157 | Grad Norm: 0.00978565\n",
      "Epoch 4 | Step 2659400 | Avg Loss: 0.0156 | Grad Norm: 0.01069391\n",
      "Epoch 4 | Step 2659500 | Avg Loss: 0.0159 | Grad Norm: 0.01009124\n",
      "Epoch 4 | Step 2659600 | Avg Loss: 0.0160 | Grad Norm: 0.00867322\n",
      "Epoch 4 | Step 2659700 | Avg Loss: 0.0162 | Grad Norm: 0.00890311\n",
      "Epoch 4 | Step 2659800 | Avg Loss: 0.0159 | Grad Norm: 0.00925030\n",
      "Epoch 4 | Step 2659900 | Avg Loss: 0.0155 | Grad Norm: 0.00936909\n",
      "Epoch 4 | Step 2660000 | Avg Loss: 0.0156 | Grad Norm: 0.01005887\n",
      "Epoch 4 | Step 2660100 | Avg Loss: 0.0154 | Grad Norm: 0.00878588\n",
      "Epoch 4 | Step 2660200 | Avg Loss: 0.0153 | Grad Norm: 0.00783929\n",
      "Epoch 4 | Step 2660300 | Avg Loss: 0.0155 | Grad Norm: 0.00949804\n",
      "Epoch 4 | Step 2660400 | Avg Loss: 0.0156 | Grad Norm: 0.00850639\n",
      "Epoch 4 | Step 2660500 | Avg Loss: 0.0154 | Grad Norm: 0.00884623\n",
      "Epoch 4 | Step 2660600 | Avg Loss: 0.0156 | Grad Norm: 0.01403665\n",
      "Epoch 4 | Step 2660700 | Avg Loss: 0.0155 | Grad Norm: 0.01006177\n",
      "Epoch 4 | Step 2660800 | Avg Loss: 0.0159 | Grad Norm: 0.01074121\n",
      "Epoch 4 | Step 2660900 | Avg Loss: 0.0163 | Grad Norm: 0.00889903\n",
      "Epoch 4 | Step 2661000 | Avg Loss: 0.0164 | Grad Norm: 0.00951765\n",
      "Epoch 4 | Step 2661100 | Avg Loss: 0.0162 | Grad Norm: 0.01036214\n",
      "Epoch 4 | Step 2661200 | Avg Loss: 0.0161 | Grad Norm: 0.01017600\n",
      "Epoch 4 | Step 2661300 | Avg Loss: 0.0158 | Grad Norm: 0.00863427\n",
      "Epoch 4 | Step 2661400 | Avg Loss: 0.0158 | Grad Norm: 0.00876385\n",
      "Epoch 4 | Step 2661500 | Avg Loss: 0.0155 | Grad Norm: 0.00935289\n",
      "Epoch 4 | Step 2661600 | Avg Loss: 0.0155 | Grad Norm: 0.00895644\n",
      "Epoch 4 | Step 2661700 | Avg Loss: 0.0154 | Grad Norm: 0.00846318\n",
      "Epoch 4 | Step 2661800 | Avg Loss: 0.0157 | Grad Norm: 0.00996025\n",
      "Epoch 4 | Step 2661900 | Avg Loss: 0.0157 | Grad Norm: 0.00825484\n",
      "Epoch 4 | Step 2662000 | Avg Loss: 0.0155 | Grad Norm: 0.00816336\n",
      "Epoch 4 | Step 2662100 | Avg Loss: 0.0149 | Grad Norm: 0.00831070\n",
      "Epoch 4 | Step 2662200 | Avg Loss: 0.0152 | Grad Norm: 0.00946263\n",
      "Epoch 4 | Step 2662300 | Avg Loss: 0.0152 | Grad Norm: 0.01016821\n",
      "Epoch 4 | Step 2662400 | Avg Loss: 0.0153 | Grad Norm: 0.01013189\n",
      "Epoch 4 | Step 2662500 | Avg Loss: 0.0155 | Grad Norm: 0.00991286\n",
      "Epoch 4 | Step 2662600 | Avg Loss: 0.0158 | Grad Norm: 0.00867364\n",
      "Epoch 4 | Step 2662700 | Avg Loss: 0.0159 | Grad Norm: 0.00981066\n",
      "Epoch 4 | Step 2662800 | Avg Loss: 0.0157 | Grad Norm: 0.01002302\n",
      "Epoch 4 | Step 2662900 | Avg Loss: 0.0158 | Grad Norm: 0.00928867\n",
      "Epoch 4 | Step 2663000 | Avg Loss: 0.0157 | Grad Norm: 0.00886132\n",
      "Epoch 4 | Step 2663100 | Avg Loss: 0.0157 | Grad Norm: 0.00875057\n",
      "Epoch 4 | Step 2663200 | Avg Loss: 0.0155 | Grad Norm: 0.00961041\n",
      "Epoch 4 | Step 2663300 | Avg Loss: 0.0158 | Grad Norm: 0.00902005\n",
      "Epoch 4 | Step 2663400 | Avg Loss: 0.0161 | Grad Norm: 0.00879221\n",
      "Epoch 4 | Step 2663500 | Avg Loss: 0.0164 | Grad Norm: 0.00920609\n",
      "Epoch 4 | Step 2663600 | Avg Loss: 0.0166 | Grad Norm: 0.00854071\n",
      "Epoch 4 | Step 2663700 | Avg Loss: 0.0163 | Grad Norm: 0.00903128\n",
      "Epoch 4 | Step 2663800 | Avg Loss: 0.0160 | Grad Norm: 0.00914926\n",
      "Epoch 4 | Step 2663900 | Avg Loss: 0.0156 | Grad Norm: 0.00919589\n",
      "Epoch 4 | Step 2664000 | Avg Loss: 0.0156 | Grad Norm: 0.00989275\n",
      "Epoch 4 | Step 2664100 | Avg Loss: 0.0160 | Grad Norm: 0.00814522\n",
      "Epoch 4 | Step 2664200 | Avg Loss: 0.0156 | Grad Norm: 0.00857944\n",
      "Epoch 4 | Step 2664300 | Avg Loss: 0.0163 | Grad Norm: 0.00936009\n",
      "Epoch 4 | Step 2664400 | Avg Loss: 0.0160 | Grad Norm: 0.00814566\n",
      "Epoch 4 | Step 2664500 | Avg Loss: 0.0159 | Grad Norm: 0.00989159\n",
      "Epoch 4 | Step 2664600 | Avg Loss: 0.0156 | Grad Norm: 0.00945373\n",
      "Epoch 4 | Step 2664700 | Avg Loss: 0.0156 | Grad Norm: 0.00875608\n",
      "Epoch 4 | Step 2664800 | Avg Loss: 0.0156 | Grad Norm: 0.00913471\n",
      "Epoch 4 | Step 2664900 | Avg Loss: 0.0159 | Grad Norm: 0.00984294\n",
      "Epoch 4 | Step 2665000 | Avg Loss: 0.0154 | Grad Norm: 0.00977248\n",
      "Epoch 4 | Step 2665100 | Avg Loss: 0.0155 | Grad Norm: 0.00961753\n",
      "Epoch 4 | Step 2665200 | Avg Loss: 0.0154 | Grad Norm: 0.01047518\n",
      "Epoch 4 | Step 2665300 | Avg Loss: 0.0155 | Grad Norm: 0.00951048\n",
      "Epoch 4 | Step 2665400 | Avg Loss: 0.0153 | Grad Norm: 0.00983875\n",
      "Epoch 4 | Step 2665500 | Avg Loss: 0.0157 | Grad Norm: 0.00911234\n",
      "Epoch 4 | Step 2665600 | Avg Loss: 0.0157 | Grad Norm: 0.00919185\n",
      "Epoch 4 | Step 2665700 | Avg Loss: 0.0161 | Grad Norm: 0.01107103\n",
      "Epoch 4 | Step 2665800 | Avg Loss: 0.0156 | Grad Norm: 0.00879554\n",
      "Epoch 4 | Step 2665900 | Avg Loss: 0.0154 | Grad Norm: 0.00814969\n",
      "Epoch 4 | Step 2666000 | Avg Loss: 0.0152 | Grad Norm: 0.01007382\n",
      "Epoch 4 | Step 2666100 | Avg Loss: 0.0153 | Grad Norm: 0.00878216\n",
      "Epoch 4 | Step 2666200 | Avg Loss: 0.0154 | Grad Norm: 0.00862797\n",
      "Epoch 4 | Step 2666300 | Avg Loss: 0.0156 | Grad Norm: 0.01018337\n",
      "Epoch 4 | Step 2666400 | Avg Loss: 0.0153 | Grad Norm: 0.00931974\n",
      "Epoch 4 | Step 2666500 | Avg Loss: 0.0152 | Grad Norm: 0.01057117\n",
      "Epoch 4 | Step 2666600 | Avg Loss: 0.0156 | Grad Norm: 0.01017439\n",
      "Epoch 4 | Step 2666700 | Avg Loss: 0.0154 | Grad Norm: 0.00807169\n",
      "Epoch 4 | Step 2666800 | Avg Loss: 0.0153 | Grad Norm: 0.00898824\n",
      "Epoch 4 | Step 2666900 | Avg Loss: 0.0156 | Grad Norm: 0.00944479\n",
      "Epoch 4 | Step 2667000 | Avg Loss: 0.0156 | Grad Norm: 0.00869671\n",
      "Epoch 4 | Step 2667100 | Avg Loss: 0.0155 | Grad Norm: 0.00901535\n",
      "Epoch 4 | Step 2667200 | Avg Loss: 0.0154 | Grad Norm: 0.00922235\n",
      "Epoch 4 | Step 2667300 | Avg Loss: 0.0159 | Grad Norm: 0.00934102\n",
      "Epoch 4 | Step 2667400 | Avg Loss: 0.0162 | Grad Norm: 0.00824293\n",
      "Epoch 4 | Step 2667500 | Avg Loss: 0.0157 | Grad Norm: 0.00855521\n",
      "Epoch 4 | Step 2667600 | Avg Loss: 0.0153 | Grad Norm: 0.00877838\n",
      "Epoch 4 | Step 2667700 | Avg Loss: 0.0150 | Grad Norm: 0.01022523\n",
      "Epoch 4 | Step 2667800 | Avg Loss: 0.0151 | Grad Norm: 0.00915888\n",
      "Epoch 4 | Step 2667900 | Avg Loss: 0.0152 | Grad Norm: 0.00940338\n",
      "Epoch 4 | Step 2668000 | Avg Loss: 0.0153 | Grad Norm: 0.00937663\n",
      "Epoch 4 | Step 2668100 | Avg Loss: 0.0152 | Grad Norm: 0.00974750\n",
      "Epoch 4 | Step 2668200 | Avg Loss: 0.0152 | Grad Norm: 0.00950336\n",
      "Epoch 4 | Step 2668300 | Avg Loss: 0.0152 | Grad Norm: 0.00849043\n",
      "Epoch 4 | Step 2668400 | Avg Loss: 0.0154 | Grad Norm: 0.00981225\n",
      "Epoch 4 | Step 2668500 | Avg Loss: 0.0156 | Grad Norm: 0.01015504\n",
      "Epoch 4 | Step 2668600 | Avg Loss: 0.0159 | Grad Norm: 0.00950150\n",
      "Epoch 4 | Step 2668700 | Avg Loss: 0.0162 | Grad Norm: 0.00794767\n",
      "Epoch 4 | Step 2668800 | Avg Loss: 0.0159 | Grad Norm: 0.00976511\n",
      "Epoch 4 | Step 2668900 | Avg Loss: 0.0158 | Grad Norm: 0.00840171\n",
      "Epoch 4 | Step 2669000 | Avg Loss: 0.0152 | Grad Norm: 0.00923451\n",
      "Epoch 4 | Step 2669100 | Avg Loss: 0.0151 | Grad Norm: 0.01141093\n",
      "Epoch 4 | Step 2669200 | Avg Loss: 0.0152 | Grad Norm: 0.00947877\n",
      "Epoch 4 | Step 2669300 | Avg Loss: 0.0154 | Grad Norm: 0.01032589\n",
      "Epoch 4 | Step 2669400 | Avg Loss: 0.0154 | Grad Norm: 0.00978563\n",
      "Epoch 4 | Step 2669500 | Avg Loss: 0.0153 | Grad Norm: 0.00845007\n",
      "Epoch 4 | Step 2669600 | Avg Loss: 0.0154 | Grad Norm: 0.00823202\n",
      "Epoch 4 | Step 2669700 | Avg Loss: 0.0154 | Grad Norm: 0.00822288\n",
      "Epoch 4 | Step 2669800 | Avg Loss: 0.0152 | Grad Norm: 0.00992696\n",
      "Epoch 4 | Step 2669900 | Avg Loss: 0.0151 | Grad Norm: 0.00894685\n",
      "Epoch 4 | Step 2670000 | Avg Loss: 0.0148 | Grad Norm: 0.01015925\n",
      "Epoch 4 | Step 2670100 | Avg Loss: 0.0144 | Grad Norm: 0.00881357\n",
      "Epoch 4 | Step 2670200 | Avg Loss: 0.0147 | Grad Norm: 0.00893181\n",
      "Epoch 4 | Step 2670300 | Avg Loss: 0.0149 | Grad Norm: 0.00890854\n",
      "Epoch 4 | Step 2670400 | Avg Loss: 0.0154 | Grad Norm: 0.01084926\n",
      "Epoch 4 | Step 2670500 | Avg Loss: 0.0151 | Grad Norm: 0.00794761\n",
      "Epoch 4 | Step 2670600 | Avg Loss: 0.0154 | Grad Norm: 0.01064436\n",
      "Epoch 4 | Step 2670700 | Avg Loss: 0.0157 | Grad Norm: 0.00973008\n",
      "Epoch 4 | Step 2670800 | Avg Loss: 0.0158 | Grad Norm: 0.00929341\n",
      "Epoch 4 | Step 2670900 | Avg Loss: 0.0158 | Grad Norm: 0.00829265\n",
      "Epoch 4 | Step 2671000 | Avg Loss: 0.0160 | Grad Norm: 0.00881244\n",
      "Epoch 4 | Step 2671100 | Avg Loss: 0.0155 | Grad Norm: 0.00794382\n",
      "Epoch 4 | Step 2671200 | Avg Loss: 0.0158 | Grad Norm: 0.00888184\n",
      "Epoch 4 | Step 2671300 | Avg Loss: 0.0155 | Grad Norm: 0.00792821\n",
      "Epoch 4 | Step 2671400 | Avg Loss: 0.0153 | Grad Norm: 0.00918737\n",
      "Epoch 4 | Step 2671500 | Avg Loss: 0.0151 | Grad Norm: 0.00868531\n",
      "Epoch 4 | Step 2671600 | Avg Loss: 0.0147 | Grad Norm: 0.01059450\n",
      "Epoch 4 | Step 2671700 | Avg Loss: 0.0149 | Grad Norm: 0.00950375\n",
      "Epoch 4 | Step 2671800 | Avg Loss: 0.0150 | Grad Norm: 0.00916049\n",
      "Epoch 4 | Step 2671900 | Avg Loss: 0.0149 | Grad Norm: 0.00871892\n",
      "Epoch 4 | Step 2672000 | Avg Loss: 0.0151 | Grad Norm: 0.00909531\n",
      "Epoch 4 | Step 2672100 | Avg Loss: 0.0149 | Grad Norm: 0.00921918\n",
      "Epoch 4 | Step 2672200 | Avg Loss: 0.0150 | Grad Norm: 0.00843823\n",
      "Epoch 4 | Step 2672300 | Avg Loss: 0.0153 | Grad Norm: 0.00966828\n",
      "Epoch 4 | Step 2672400 | Avg Loss: 0.0154 | Grad Norm: 0.01023347\n",
      "Epoch 4 | Step 2672500 | Avg Loss: 0.0154 | Grad Norm: 0.00877869\n",
      "Epoch 4 | Step 2672600 | Avg Loss: 0.0158 | Grad Norm: 0.00792183\n",
      "Epoch 4 | Step 2672700 | Avg Loss: 0.0158 | Grad Norm: 0.00829675\n",
      "Epoch 4 | Step 2672800 | Avg Loss: 0.0155 | Grad Norm: 0.01000165\n",
      "Epoch 4 | Step 2672900 | Avg Loss: 0.0155 | Grad Norm: 0.01059103\n",
      "Epoch 4 | Step 2673000 | Avg Loss: 0.0155 | Grad Norm: 0.00845667\n",
      "Epoch 4 | Step 2673100 | Avg Loss: 0.0156 | Grad Norm: 0.00850446\n",
      "Epoch 4 | Step 2673200 | Avg Loss: 0.0150 | Grad Norm: 0.00863814\n",
      "Epoch 4 | Step 2673300 | Avg Loss: 0.0149 | Grad Norm: 0.00903070\n",
      "Epoch 4 | Step 2673400 | Avg Loss: 0.0154 | Grad Norm: 0.00938333\n",
      "Epoch 4 | Step 2673500 | Avg Loss: 0.0151 | Grad Norm: 0.01115312\n",
      "Epoch 4 | Step 2673600 | Avg Loss: 0.0155 | Grad Norm: 0.00852061\n",
      "Epoch 4 | Step 2673700 | Avg Loss: 0.0153 | Grad Norm: 0.00911911\n",
      "Epoch 4 | Step 2673800 | Avg Loss: 0.0151 | Grad Norm: 0.00858096\n",
      "Epoch 4 | Step 2673900 | Avg Loss: 0.0151 | Grad Norm: 0.00993692\n",
      "Epoch 4 | Step 2674000 | Avg Loss: 0.0155 | Grad Norm: 0.00953242\n",
      "Epoch 4 | Step 2674100 | Avg Loss: 0.0154 | Grad Norm: 0.00922730\n",
      "Epoch 4 | Step 2674200 | Avg Loss: 0.0156 | Grad Norm: 0.00811484\n",
      "Epoch 4 | Step 2674300 | Avg Loss: 0.0157 | Grad Norm: 0.00859815\n",
      "Epoch 4 | Step 2674400 | Avg Loss: 0.0159 | Grad Norm: 0.01047266\n",
      "Epoch 4 | Step 2674500 | Avg Loss: 0.0162 | Grad Norm: 0.01016903\n",
      "Epoch 4 | Step 2674600 | Avg Loss: 0.0161 | Grad Norm: 0.00861697\n",
      "Epoch 4 | Step 2674700 | Avg Loss: 0.0161 | Grad Norm: 0.01051521\n",
      "Epoch 4 | Step 2674800 | Avg Loss: 0.0161 | Grad Norm: 0.00914790\n",
      "Epoch 4 | Step 2674900 | Avg Loss: 0.0162 | Grad Norm: 0.00930620\n",
      "Epoch 4 | Step 2675000 | Avg Loss: 0.0159 | Grad Norm: 0.00859565\n",
      "Epoch 4 | Step 2675100 | Avg Loss: 0.0157 | Grad Norm: 0.01040821\n",
      "Epoch 4 | Step 2675200 | Avg Loss: 0.0157 | Grad Norm: 0.00886596\n",
      "Epoch 4 | Step 2675300 | Avg Loss: 0.0167 | Grad Norm: 0.00917378\n",
      "Epoch 4 | Step 2675400 | Avg Loss: 0.0162 | Grad Norm: 0.00938413\n",
      "Epoch 4 | Step 2675500 | Avg Loss: 0.0162 | Grad Norm: 0.00904947\n",
      "Epoch 4 | Step 2675600 | Avg Loss: 0.0159 | Grad Norm: 0.00935394\n",
      "Epoch 4 | Step 2675700 | Avg Loss: 0.0157 | Grad Norm: 0.00789833\n",
      "Epoch 4 | Step 2675800 | Avg Loss: 0.0158 | Grad Norm: 0.01044803\n",
      "Epoch 4 | Step 2675900 | Avg Loss: 0.0160 | Grad Norm: 0.01025018\n",
      "Epoch 4 | Step 2676000 | Avg Loss: 0.0155 | Grad Norm: 0.00925281\n",
      "Epoch 4 | Step 2676100 | Avg Loss: 0.0155 | Grad Norm: 0.00839890\n",
      "Epoch 4 | Step 2676200 | Avg Loss: 0.0156 | Grad Norm: 0.01021804\n",
      "Epoch 4 | Step 2676300 | Avg Loss: 0.0156 | Grad Norm: 0.00948896\n",
      "Epoch 4 | Step 2676400 | Avg Loss: 0.0156 | Grad Norm: 0.00979930\n",
      "Epoch 4 | Step 2676500 | Avg Loss: 0.0152 | Grad Norm: 0.00825882\n",
      "Epoch 4 | Step 2676600 | Avg Loss: 0.0157 | Grad Norm: 0.00924278\n",
      "Epoch 4 | Step 2676700 | Avg Loss: 0.0161 | Grad Norm: 0.00826628\n",
      "Epoch 4 | Step 2676800 | Avg Loss: 0.0158 | Grad Norm: 0.00871480\n",
      "Epoch 4 | Step 2676900 | Avg Loss: 0.0157 | Grad Norm: 0.00975170\n",
      "Epoch 4 | Step 2677000 | Avg Loss: 0.0156 | Grad Norm: 0.00892966\n",
      "Epoch 4 | Step 2677100 | Avg Loss: 0.0160 | Grad Norm: 0.00874652\n",
      "Epoch 4 | Step 2677200 | Avg Loss: 0.0157 | Grad Norm: 0.00822540\n",
      "Epoch 4 | Step 2677300 | Avg Loss: 0.0159 | Grad Norm: 0.00837896\n",
      "Epoch 4 | Step 2677400 | Avg Loss: 0.0155 | Grad Norm: 0.00894372\n",
      "Epoch 4 | Step 2677500 | Avg Loss: 0.0154 | Grad Norm: 0.00856194\n",
      "Epoch 4 | Step 2677600 | Avg Loss: 0.0154 | Grad Norm: 0.00816422\n",
      "Epoch 4 | Step 2677700 | Avg Loss: 0.0154 | Grad Norm: 0.00891132\n",
      "Epoch 4 | Step 2677800 | Avg Loss: 0.0156 | Grad Norm: 0.00946344\n",
      "Epoch 4 | Step 2677900 | Avg Loss: 0.0159 | Grad Norm: 0.00839308\n",
      "Epoch 4 | Step 2678000 | Avg Loss: 0.0161 | Grad Norm: 0.00932699\n",
      "Epoch 4 | Step 2678100 | Avg Loss: 0.0158 | Grad Norm: 0.00748367\n",
      "Epoch 4 | Step 2678200 | Avg Loss: 0.0156 | Grad Norm: 0.00969475\n",
      "Epoch 4 | Step 2678300 | Avg Loss: 0.0152 | Grad Norm: 0.00927603\n",
      "Epoch 4 | Step 2678400 | Avg Loss: 0.0153 | Grad Norm: 0.00869992\n",
      "Epoch 4 | Step 2678500 | Avg Loss: 0.0156 | Grad Norm: 0.00983292\n",
      "Epoch 4 | Step 2678600 | Avg Loss: 0.0154 | Grad Norm: 0.00950920\n",
      "Epoch 4 | Step 2678700 | Avg Loss: 0.0156 | Grad Norm: 0.01085340\n",
      "Epoch 4 | Step 2678800 | Avg Loss: 0.0158 | Grad Norm: 0.00811539\n",
      "Epoch 4 | Step 2678900 | Avg Loss: 0.0161 | Grad Norm: 0.00981502\n",
      "Epoch 4 | Step 2679000 | Avg Loss: 0.0157 | Grad Norm: 0.01039413\n",
      "Epoch 4 | Step 2679100 | Avg Loss: 0.0154 | Grad Norm: 0.00966573\n",
      "Epoch 4 | Step 2679200 | Avg Loss: 0.0154 | Grad Norm: 0.00907146\n",
      "Epoch 4 | Step 2679300 | Avg Loss: 0.0155 | Grad Norm: 0.00868489\n",
      "Epoch 4 | Step 2679400 | Avg Loss: 0.0157 | Grad Norm: 0.01008188\n",
      "Epoch 4 | Step 2679500 | Avg Loss: 0.0156 | Grad Norm: 0.00964697\n",
      "Epoch 4 | Step 2679600 | Avg Loss: 0.0154 | Grad Norm: 0.00914856\n",
      "Epoch 4 | Step 2679700 | Avg Loss: 0.0156 | Grad Norm: 0.01170454\n",
      "Epoch 4 | Step 2679800 | Avg Loss: 0.0154 | Grad Norm: 0.00867679\n",
      "Epoch 4 | Step 2679900 | Avg Loss: 0.0156 | Grad Norm: 0.00953667\n",
      "Epoch 4 | Step 2680000 | Avg Loss: 0.0154 | Grad Norm: 0.00828990\n",
      "Epoch 4 | Step 2680100 | Avg Loss: 0.0155 | Grad Norm: 0.00943048\n",
      "Epoch 4 | Step 2680200 | Avg Loss: 0.0155 | Grad Norm: 0.00908123\n",
      "Epoch 4 | Step 2680300 | Avg Loss: 0.0151 | Grad Norm: 0.00857512\n",
      "Epoch 4 | Step 2680400 | Avg Loss: 0.0151 | Grad Norm: 0.00931697\n",
      "Epoch 4 | Step 2680500 | Avg Loss: 0.0155 | Grad Norm: 0.00952550\n",
      "Epoch 4 | Step 2680600 | Avg Loss: 0.0155 | Grad Norm: 0.00964848\n",
      "Epoch 4 | Step 2680700 | Avg Loss: 0.0153 | Grad Norm: 0.00942519\n",
      "Epoch 4 | Step 2680800 | Avg Loss: 0.0154 | Grad Norm: 0.01018412\n",
      "Epoch 4 | Step 2680900 | Avg Loss: 0.0157 | Grad Norm: 0.00955785\n",
      "Epoch 4 | Step 2681000 | Avg Loss: 0.0155 | Grad Norm: 0.00816344\n",
      "Epoch 4 | Step 2681100 | Avg Loss: 0.0152 | Grad Norm: 0.00973520\n",
      "Epoch 4 | Step 2681200 | Avg Loss: 0.0154 | Grad Norm: 0.00870982\n",
      "Epoch 4 | Step 2681300 | Avg Loss: 0.0153 | Grad Norm: 0.00989613\n",
      "Epoch 4 | Step 2681400 | Avg Loss: 0.0155 | Grad Norm: 0.01055676\n",
      "Epoch 4 | Step 2681500 | Avg Loss: 0.0154 | Grad Norm: 0.00890517\n",
      "Epoch 4 | Step 2681600 | Avg Loss: 0.0154 | Grad Norm: 0.00909566\n",
      "Epoch 4 | Step 2681700 | Avg Loss: 0.0154 | Grad Norm: 0.00829026\n",
      "Epoch 4 | Step 2681800 | Avg Loss: 0.0156 | Grad Norm: 0.00920683\n",
      "Epoch 4 | Step 2681900 | Avg Loss: 0.0157 | Grad Norm: 0.00817343\n",
      "Epoch 4 | Step 2682000 | Avg Loss: 0.0153 | Grad Norm: 0.00948679\n",
      "Epoch 4 | Step 2682100 | Avg Loss: 0.0156 | Grad Norm: 0.00806151\n",
      "Epoch 4 | Step 2682200 | Avg Loss: 0.0157 | Grad Norm: 0.00856101\n",
      "Epoch 4 | Step 2682300 | Avg Loss: 0.0157 | Grad Norm: 0.00903230\n",
      "Epoch 4 | Step 2682400 | Avg Loss: 0.0155 | Grad Norm: 0.00944905\n",
      "Epoch 4 | Step 2682500 | Avg Loss: 0.0151 | Grad Norm: 0.00840115\n",
      "Epoch 4 | Step 2682600 | Avg Loss: 0.0151 | Grad Norm: 0.00955600\n",
      "Epoch 4 | Step 2682700 | Avg Loss: 0.0156 | Grad Norm: 0.00847745\n",
      "Epoch 4 | Step 2682800 | Avg Loss: 0.0156 | Grad Norm: 0.00888587\n",
      "Epoch 4 | Step 2682900 | Avg Loss: 0.0152 | Grad Norm: 0.00941713\n",
      "Epoch 4 | Step 2683000 | Avg Loss: 0.0149 | Grad Norm: 0.01076257\n",
      "Epoch 4 | Step 2683100 | Avg Loss: 0.0150 | Grad Norm: 0.00822129\n",
      "Epoch 4 | Step 2683200 | Avg Loss: 0.0149 | Grad Norm: 0.00900417\n",
      "Epoch 4 | Step 2683300 | Avg Loss: 0.0149 | Grad Norm: 0.00946435\n",
      "Epoch 4 | Step 2683400 | Avg Loss: 0.0147 | Grad Norm: 0.00939578\n",
      "Epoch 4 | Step 2683500 | Avg Loss: 0.0147 | Grad Norm: 0.00711258\n",
      "Epoch 4 | Step 2683600 | Avg Loss: 0.0146 | Grad Norm: 0.00798488\n",
      "Epoch 4 | Step 2683700 | Avg Loss: 0.0149 | Grad Norm: 0.00804180\n",
      "Epoch 4 | Step 2683800 | Avg Loss: 0.0155 | Grad Norm: 0.00847958\n",
      "Epoch 4 | Step 2683900 | Avg Loss: 0.0152 | Grad Norm: 0.00806520\n",
      "Epoch 4 | Step 2684000 | Avg Loss: 0.0150 | Grad Norm: 0.00832329\n",
      "Epoch 4 | Step 2684100 | Avg Loss: 0.0148 | Grad Norm: 0.00900274\n",
      "Epoch 4 | Step 2684200 | Avg Loss: 0.0151 | Grad Norm: 0.00823771\n",
      "Epoch 4 | Step 2684300 | Avg Loss: 0.0151 | Grad Norm: 0.00861605\n",
      "Epoch 4 | Step 2684400 | Avg Loss: 0.0155 | Grad Norm: 0.00858998\n",
      "Epoch 4 | Step 2684500 | Avg Loss: 0.0153 | Grad Norm: 0.00836933\n",
      "Epoch 4 | Step 2684600 | Avg Loss: 0.0155 | Grad Norm: 0.00938082\n",
      "Epoch 4 | Step 2684700 | Avg Loss: 0.0154 | Grad Norm: 0.00832145\n",
      "Epoch 4 | Step 2684800 | Avg Loss: 0.0154 | Grad Norm: 0.00821484\n",
      "Epoch 4 | Step 2684900 | Avg Loss: 0.0152 | Grad Norm: 0.00826516\n",
      "Epoch 4 | Step 2685000 | Avg Loss: 0.0148 | Grad Norm: 0.00878043\n",
      "Epoch 4 | Step 2685100 | Avg Loss: 0.0150 | Grad Norm: 0.00888598\n",
      "Epoch 4 | Step 2685200 | Avg Loss: 0.0153 | Grad Norm: 0.00848910\n",
      "Epoch 4 | Step 2685300 | Avg Loss: 0.0153 | Grad Norm: 0.00964872\n",
      "Epoch 4 | Step 2685400 | Avg Loss: 0.0152 | Grad Norm: 0.00849037\n",
      "Epoch 4 | Step 2685500 | Avg Loss: 0.0153 | Grad Norm: 0.00880283\n",
      "Epoch 4 | Step 2685600 | Avg Loss: 0.0156 | Grad Norm: 0.00901132\n",
      "Epoch 4 | Step 2685700 | Avg Loss: 0.0158 | Grad Norm: 0.00900748\n",
      "Epoch 4 | Step 2685800 | Avg Loss: 0.0159 | Grad Norm: 0.00978116\n",
      "Epoch 4 | Step 2685900 | Avg Loss: 0.0159 | Grad Norm: 0.00920304\n",
      "Epoch 4 | Step 2686000 | Avg Loss: 0.0156 | Grad Norm: 0.01092488\n",
      "Epoch 4 | Step 2686100 | Avg Loss: 0.0153 | Grad Norm: 0.00826754\n",
      "Epoch 4 | Step 2686200 | Avg Loss: 0.0151 | Grad Norm: 0.00893368\n",
      "Epoch 4 | Step 2686300 | Avg Loss: 0.0154 | Grad Norm: 0.00898965\n",
      "Epoch 4 | Step 2686400 | Avg Loss: 0.0156 | Grad Norm: 0.00926881\n",
      "Epoch 4 | Step 2686500 | Avg Loss: 0.0156 | Grad Norm: 0.00904072\n",
      "Epoch 4 | Step 2686600 | Avg Loss: 0.0156 | Grad Norm: 0.00777935\n",
      "Epoch 4 | Step 2686700 | Avg Loss: 0.0154 | Grad Norm: 0.00830110\n",
      "Epoch 4 | Step 2686800 | Avg Loss: 0.0154 | Grad Norm: 0.00838422\n",
      "Epoch 4 | Step 2686900 | Avg Loss: 0.0153 | Grad Norm: 0.00792730\n",
      "Epoch 4 | Step 2687000 | Avg Loss: 0.0153 | Grad Norm: 0.01063786\n",
      "Epoch 4 | Step 2687100 | Avg Loss: 0.0154 | Grad Norm: 0.01063192\n",
      "Epoch 4 | Step 2687200 | Avg Loss: 0.0155 | Grad Norm: 0.00878429\n",
      "Epoch 4 | Step 2687300 | Avg Loss: 0.0152 | Grad Norm: 0.01029966\n",
      "Epoch 4 | Step 2687400 | Avg Loss: 0.0155 | Grad Norm: 0.01021330\n",
      "Epoch 4 | Step 2687500 | Avg Loss: 0.0156 | Grad Norm: 0.00802473\n",
      "Epoch 4 | Step 2687600 | Avg Loss: 0.0160 | Grad Norm: 0.00886017\n",
      "Epoch 4 | Step 2687700 | Avg Loss: 0.0158 | Grad Norm: 0.00949829\n",
      "Epoch 4 | Step 2687800 | Avg Loss: 0.0155 | Grad Norm: 0.00914752\n",
      "Epoch 4 | Step 2687900 | Avg Loss: 0.0158 | Grad Norm: 0.00922996\n",
      "Epoch 4 | Step 2688000 | Avg Loss: 0.0158 | Grad Norm: 0.00897443\n",
      "Epoch 4 | Step 2688100 | Avg Loss: 0.0158 | Grad Norm: 0.00867835\n",
      "Epoch 4 | Step 2688200 | Avg Loss: 0.0155 | Grad Norm: 0.00887600\n",
      "Epoch 4 | Step 2688300 | Avg Loss: 0.0159 | Grad Norm: 0.01004571\n",
      "Epoch 4 | Step 2688400 | Avg Loss: 0.0160 | Grad Norm: 0.00908718\n",
      "Epoch 4 | Step 2688500 | Avg Loss: 0.0158 | Grad Norm: 0.00854919\n",
      "Epoch 4 | Step 2688600 | Avg Loss: 0.0156 | Grad Norm: 0.00877469\n",
      "Epoch 4 | Step 2688700 | Avg Loss: 0.0156 | Grad Norm: 0.00777528\n",
      "Epoch 4 | Step 2688800 | Avg Loss: 0.0154 | Grad Norm: 0.00789507\n",
      "Epoch 4 | Step 2688900 | Avg Loss: 0.0152 | Grad Norm: 0.00863359\n",
      "Epoch 4 | Step 2689000 | Avg Loss: 0.0153 | Grad Norm: 0.00890374\n",
      "Epoch 4 | Step 2689100 | Avg Loss: 0.0155 | Grad Norm: 0.00860671\n",
      "Epoch 4 | Step 2689200 | Avg Loss: 0.0153 | Grad Norm: 0.00762313\n",
      "Epoch 4 | Step 2689300 | Avg Loss: 0.0154 | Grad Norm: 0.01078109\n",
      "Epoch 4 | Step 2689400 | Avg Loss: 0.0153 | Grad Norm: 0.00885207\n",
      "Epoch 4 | Step 2689500 | Avg Loss: 0.0153 | Grad Norm: 0.00835996\n",
      "Epoch 4 | Step 2689600 | Avg Loss: 0.0153 | Grad Norm: 0.00867660\n",
      "Epoch 4 | Step 2689700 | Avg Loss: 0.0152 | Grad Norm: 0.00869202\n",
      "Epoch 4 | Step 2689800 | Avg Loss: 0.0155 | Grad Norm: 0.00865942\n",
      "Epoch 4 | Step 2689900 | Avg Loss: 0.0154 | Grad Norm: 0.00866345\n",
      "Epoch 4 | Step 2690000 | Avg Loss: 0.0152 | Grad Norm: 0.00869344\n",
      "Epoch 4 | Step 2690100 | Avg Loss: 0.0154 | Grad Norm: 0.00803360\n",
      "Epoch 4 | Step 2690200 | Avg Loss: 0.0155 | Grad Norm: 0.00878791\n",
      "Epoch 4 | Step 2690300 | Avg Loss: 0.0154 | Grad Norm: 0.00772947\n",
      "Epoch 4 | Step 2690400 | Avg Loss: 0.0152 | Grad Norm: 0.00895385\n",
      "Epoch 4 | Step 2690500 | Avg Loss: 0.0150 | Grad Norm: 0.00899243\n",
      "Epoch 4 | Step 2690600 | Avg Loss: 0.0149 | Grad Norm: 0.01216243\n",
      "Epoch 4 | Step 2690700 | Avg Loss: 0.0153 | Grad Norm: 0.01415316\n",
      "Epoch 4 | Step 2690800 | Avg Loss: 0.0153 | Grad Norm: 0.00982061\n",
      "Epoch 4 | Step 2690900 | Avg Loss: 0.0154 | Grad Norm: 0.00787201\n",
      "Epoch 4 | Step 2691000 | Avg Loss: 0.0151 | Grad Norm: 0.01033929\n",
      "Epoch 4 | Step 2691100 | Avg Loss: 0.0147 | Grad Norm: 0.00813365\n",
      "Epoch 4 | Step 2691200 | Avg Loss: 0.0148 | Grad Norm: 0.00890180\n",
      "Epoch 4 | Step 2691300 | Avg Loss: 0.0152 | Grad Norm: 0.00934828\n",
      "Epoch 4 | Step 2691400 | Avg Loss: 0.0155 | Grad Norm: 0.00840018\n",
      "Epoch 4 | Step 2691500 | Avg Loss: 0.0158 | Grad Norm: 0.00842534\n",
      "Epoch 4 | Step 2691600 | Avg Loss: 0.0155 | Grad Norm: 0.00914769\n",
      "Epoch 4 | Step 2691700 | Avg Loss: 0.0157 | Grad Norm: 0.01173663\n",
      "Epoch 4 | Step 2691800 | Avg Loss: 0.0158 | Grad Norm: 0.00995812\n",
      "Epoch 4 | Step 2691900 | Avg Loss: 0.0154 | Grad Norm: 0.00763041\n",
      "Epoch 4 | Step 2692000 | Avg Loss: 0.0158 | Grad Norm: 0.00808397\n",
      "Epoch 4 | Step 2692100 | Avg Loss: 0.0158 | Grad Norm: 0.00968173\n",
      "Epoch 4 | Step 2692200 | Avg Loss: 0.0154 | Grad Norm: 0.00873256\n",
      "Epoch 4 | Step 2692300 | Avg Loss: 0.0153 | Grad Norm: 0.00807715\n",
      "Epoch 4 | Step 2692400 | Avg Loss: 0.0155 | Grad Norm: 0.01002455\n",
      "Epoch 4 | Step 2692500 | Avg Loss: 0.0155 | Grad Norm: 0.00792001\n",
      "Epoch 4 | Step 2692600 | Avg Loss: 0.0156 | Grad Norm: 0.01033852\n",
      "Epoch 4 | Step 2692700 | Avg Loss: 0.0154 | Grad Norm: 0.00938428\n",
      "Epoch 4 | Step 2692800 | Avg Loss: 0.0154 | Grad Norm: 0.00932683\n",
      "Epoch 4 | Step 2692900 | Avg Loss: 0.0153 | Grad Norm: 0.01044475\n",
      "Epoch 4 | Step 2693000 | Avg Loss: 0.0153 | Grad Norm: 0.00796434\n",
      "Epoch 4 | Step 2693100 | Avg Loss: 0.0157 | Grad Norm: 0.00907403\n",
      "Epoch 4 | Step 2693200 | Avg Loss: 0.0156 | Grad Norm: 0.00847833\n",
      "Epoch 4 | Step 2693300 | Avg Loss: 0.0156 | Grad Norm: 0.00853588\n",
      "Epoch 4 | Step 2693400 | Avg Loss: 0.0158 | Grad Norm: 0.01026859\n",
      "Epoch 4 | Step 2693500 | Avg Loss: 0.0156 | Grad Norm: 0.00849261\n",
      "Epoch 4 | Step 2693600 | Avg Loss: 0.0157 | Grad Norm: 0.00852496\n",
      "Epoch 4 | Step 2693700 | Avg Loss: 0.0155 | Grad Norm: 0.00863812\n",
      "Epoch 4 | Step 2693800 | Avg Loss: 0.0154 | Grad Norm: 0.00977524\n",
      "Epoch 4 | Step 2693900 | Avg Loss: 0.0154 | Grad Norm: 0.00920100\n",
      "Epoch 4 | Step 2694000 | Avg Loss: 0.0157 | Grad Norm: 0.01034654\n",
      "Epoch 4 | Step 2694100 | Avg Loss: 0.0155 | Grad Norm: 0.00892574\n",
      "Epoch 4 | Step 2694200 | Avg Loss: 0.0156 | Grad Norm: 0.00796721\n",
      "Epoch 4 | Step 2694300 | Avg Loss: 0.0156 | Grad Norm: 0.01086979\n",
      "Epoch 4 | Step 2694400 | Avg Loss: 0.0159 | Grad Norm: 0.00876817\n",
      "Epoch 4 | Step 2694500 | Avg Loss: 0.0159 | Grad Norm: 0.01009535\n",
      "Epoch 4 | Step 2694600 | Avg Loss: 0.0160 | Grad Norm: 0.01096589\n",
      "Epoch 4 | Step 2694700 | Avg Loss: 0.0158 | Grad Norm: 0.00922114\n",
      "Epoch 4 | Step 2694800 | Avg Loss: 0.0161 | Grad Norm: 0.00953751\n",
      "Epoch 4 | Step 2694900 | Avg Loss: 0.0159 | Grad Norm: 0.00831389\n",
      "Epoch 4 | Step 2695000 | Avg Loss: 0.0156 | Grad Norm: 0.00820646\n",
      "Epoch 4 | Step 2695100 | Avg Loss: 0.0155 | Grad Norm: 0.00943577\n",
      "Epoch 4 | Step 2695200 | Avg Loss: 0.0153 | Grad Norm: 0.00958128\n",
      "Epoch 4 | Step 2695300 | Avg Loss: 0.0152 | Grad Norm: 0.00868758\n",
      "Epoch 4 | Step 2695400 | Avg Loss: 0.0155 | Grad Norm: 0.00984158\n",
      "Epoch 4 | Step 2695500 | Avg Loss: 0.0159 | Grad Norm: 0.00893272\n",
      "Epoch 4 | Step 2695600 | Avg Loss: 0.0158 | Grad Norm: 0.00893679\n",
      "Epoch 4 | Step 2695700 | Avg Loss: 0.0158 | Grad Norm: 0.00965726\n",
      "Epoch 4 | Step 2695800 | Avg Loss: 0.0158 | Grad Norm: 0.00997490\n",
      "Epoch 4 | Step 2695900 | Avg Loss: 0.0160 | Grad Norm: 0.00891901\n",
      "Epoch 4 | Step 2696000 | Avg Loss: 0.0157 | Grad Norm: 0.00872466\n",
      "Epoch 4 | Step 2696100 | Avg Loss: 0.0155 | Grad Norm: 0.00880038\n",
      "Epoch 4 | Step 2696200 | Avg Loss: 0.0155 | Grad Norm: 0.00975673\n",
      "Epoch 4 | Step 2696300 | Avg Loss: 0.0157 | Grad Norm: 0.00840422\n",
      "Epoch 4 | Step 2696400 | Avg Loss: 0.0159 | Grad Norm: 0.01063584\n",
      "Epoch 4 | Step 2696500 | Avg Loss: 0.0158 | Grad Norm: 0.00877832\n",
      "Epoch 4 | Step 2696600 | Avg Loss: 0.0155 | Grad Norm: 0.00936230\n",
      "Epoch 4 | Step 2696700 | Avg Loss: 0.0151 | Grad Norm: 0.00999480\n",
      "Epoch 4 | Step 2696800 | Avg Loss: 0.0153 | Grad Norm: 0.00844549\n",
      "Epoch 4 | Step 2696900 | Avg Loss: 0.0154 | Grad Norm: 0.00897917\n",
      "Epoch 4 | Step 2697000 | Avg Loss: 0.0155 | Grad Norm: 0.00867947\n",
      "Epoch 4 | Step 2697100 | Avg Loss: 0.0152 | Grad Norm: 0.00792756\n",
      "Epoch 4 | Step 2697200 | Avg Loss: 0.0154 | Grad Norm: 0.00918705\n",
      "Epoch 4 | Step 2697300 | Avg Loss: 0.0149 | Grad Norm: 0.00816676\n",
      "Epoch 4 | Step 2697400 | Avg Loss: 0.0150 | Grad Norm: 0.00897599\n",
      "Epoch 4 | Step 2697500 | Avg Loss: 0.0152 | Grad Norm: 0.01229286\n",
      "Epoch 4 | Step 2697600 | Avg Loss: 0.0156 | Grad Norm: 0.00876779\n",
      "Epoch 4 | Step 2697700 | Avg Loss: 0.0154 | Grad Norm: 0.00796779\n",
      "Epoch 4 | Step 2697800 | Avg Loss: 0.0154 | Grad Norm: 0.00796822\n",
      "Epoch 4 | Step 2697900 | Avg Loss: 0.0154 | Grad Norm: 0.00796557\n",
      "Epoch 4 | Step 2698000 | Avg Loss: 0.0155 | Grad Norm: 0.00980999\n",
      "Epoch 4 | Step 2698100 | Avg Loss: 0.0158 | Grad Norm: 0.00990084\n",
      "Epoch 4 | Step 2698200 | Avg Loss: 0.0154 | Grad Norm: 0.00906809\n",
      "Epoch 4 | Step 2698300 | Avg Loss: 0.0158 | Grad Norm: 0.00880700\n",
      "Epoch 4 | Step 2698400 | Avg Loss: 0.0154 | Grad Norm: 0.00987350\n",
      "Epoch 4 | Step 2698500 | Avg Loss: 0.0150 | Grad Norm: 0.00854535\n",
      "Epoch 4 | Step 2698600 | Avg Loss: 0.0154 | Grad Norm: 0.00867411\n",
      "Epoch 4 | Step 2698700 | Avg Loss: 0.0152 | Grad Norm: 0.00870186\n",
      "Epoch 4 | Step 2698800 | Avg Loss: 0.0153 | Grad Norm: 0.00989518\n",
      "Epoch 4 | Step 2698900 | Avg Loss: 0.0151 | Grad Norm: 0.01230994\n",
      "Epoch 4 | Step 2699000 | Avg Loss: 0.0151 | Grad Norm: 0.00985614\n",
      "Epoch 4 | Step 2699100 | Avg Loss: 0.0153 | Grad Norm: 0.00762577\n",
      "Epoch 4 | Step 2699200 | Avg Loss: 0.0154 | Grad Norm: 0.00906232\n",
      "Epoch 4 | Step 2699300 | Avg Loss: 0.0152 | Grad Norm: 0.00770431\n",
      "Epoch 4 | Step 2699400 | Avg Loss: 0.0151 | Grad Norm: 0.01042687\n",
      "Epoch 4 | Step 2699500 | Avg Loss: 0.0150 | Grad Norm: 0.00992661\n",
      "Epoch 4 | Step 2699600 | Avg Loss: 0.0150 | Grad Norm: 0.00803390\n",
      "Epoch 4 | Step 2699700 | Avg Loss: 0.0153 | Grad Norm: 0.00795575\n",
      "Epoch 4 | Step 2699800 | Avg Loss: 0.0155 | Grad Norm: 0.00938725\n",
      "Epoch 4 | Step 2699900 | Avg Loss: 0.0153 | Grad Norm: 0.00870148\n",
      "Epoch 4 | Step 2700000 | Avg Loss: 0.0154 | Grad Norm: 0.00972509\n",
      "Saving model at step2700000\n",
      "Epoch 4 | Step 2700100 | Avg Loss: 0.0157 | Grad Norm: 0.00980181\n",
      "Epoch 4 | Step 2700200 | Avg Loss: 0.0158 | Grad Norm: 0.00895111\n",
      "Epoch 4 | Step 2700300 | Avg Loss: 0.0154 | Grad Norm: 0.01069828\n",
      "Epoch 4 | Step 2700400 | Avg Loss: 0.0154 | Grad Norm: 0.00815753\n",
      "Epoch 4 | Step 2700500 | Avg Loss: 0.0150 | Grad Norm: 0.00863929\n",
      "Epoch 4 | Step 2700600 | Avg Loss: 0.0150 | Grad Norm: 0.01062711\n",
      "Epoch 4 | Step 2700700 | Avg Loss: 0.0148 | Grad Norm: 0.00813187\n",
      "Epoch 4 | Step 2700800 | Avg Loss: 0.0148 | Grad Norm: 0.00825762\n",
      "Epoch 4 | Step 2700900 | Avg Loss: 0.0149 | Grad Norm: 0.01012567\n",
      "Epoch 4 | Step 2701000 | Avg Loss: 0.0149 | Grad Norm: 0.01034064\n",
      "Epoch 4 | Step 2701100 | Avg Loss: 0.0149 | Grad Norm: 0.00878728\n",
      "Epoch 4 | Step 2701200 | Avg Loss: 0.0149 | Grad Norm: 0.00864122\n",
      "Epoch 4 | Step 2701300 | Avg Loss: 0.0148 | Grad Norm: 0.00851092\n",
      "Epoch 4 | Step 2701400 | Avg Loss: 0.0153 | Grad Norm: 0.01067360\n",
      "Epoch 4 | Step 2701500 | Avg Loss: 0.0152 | Grad Norm: 0.00858993\n",
      "Epoch 4 | Step 2701600 | Avg Loss: 0.0152 | Grad Norm: 0.01117668\n",
      "Epoch 4 | Step 2701700 | Avg Loss: 0.0153 | Grad Norm: 0.00884673\n",
      "Epoch 4 | Step 2701800 | Avg Loss: 0.0154 | Grad Norm: 0.00882962\n",
      "Epoch 4 | Step 2701900 | Avg Loss: 0.0152 | Grad Norm: 0.00925393\n",
      "Epoch 4 | Step 2702000 | Avg Loss: 0.0147 | Grad Norm: 0.00873196\n",
      "Epoch 4 | Step 2702100 | Avg Loss: 0.0146 | Grad Norm: 0.00837174\n",
      "Epoch 4 | Step 2702200 | Avg Loss: 0.0152 | Grad Norm: 0.00925839\n",
      "Epoch 4 | Step 2702300 | Avg Loss: 0.0156 | Grad Norm: 0.00837107\n",
      "Epoch 4 | Step 2702400 | Avg Loss: 0.0158 | Grad Norm: 0.00955078\n",
      "Epoch 4 | Step 2702500 | Avg Loss: 0.0159 | Grad Norm: 0.00900034\n",
      "Epoch 4 | Step 2702600 | Avg Loss: 0.0157 | Grad Norm: 0.00956459\n",
      "Epoch 4 | Step 2702700 | Avg Loss: 0.0158 | Grad Norm: 0.00795704\n",
      "Epoch 4 | Step 2702800 | Avg Loss: 0.0155 | Grad Norm: 0.00814515\n",
      "Epoch 4 | Step 2702900 | Avg Loss: 0.0156 | Grad Norm: 0.00892867\n",
      "Epoch 4 | Step 2703000 | Avg Loss: 0.0152 | Grad Norm: 0.00827062\n",
      "Epoch 4 | Step 2703100 | Avg Loss: 0.0152 | Grad Norm: 0.00958871\n",
      "Epoch 4 | Step 2703200 | Avg Loss: 0.0152 | Grad Norm: 0.00816751\n",
      "Epoch 4 | Step 2703300 | Avg Loss: 0.0152 | Grad Norm: 0.00875595\n",
      "Epoch 4 | Step 2703400 | Avg Loss: 0.0153 | Grad Norm: 0.00772608\n",
      "Epoch 4 | Step 2703500 | Avg Loss: 0.0154 | Grad Norm: 0.00812205\n",
      "Epoch 4 | Step 2703600 | Avg Loss: 0.0158 | Grad Norm: 0.00890709\n",
      "Epoch 4 | Step 2703700 | Avg Loss: 0.0157 | Grad Norm: 0.00925850\n",
      "Epoch 4 | Step 2703800 | Avg Loss: 0.0157 | Grad Norm: 0.00790714\n",
      "Epoch 4 | Step 2703900 | Avg Loss: 0.0157 | Grad Norm: 0.00834641\n",
      "Epoch 4 | Step 2704000 | Avg Loss: 0.0153 | Grad Norm: 0.00861867\n",
      "Epoch 4 | Step 2704100 | Avg Loss: 0.0150 | Grad Norm: 0.00867500\n",
      "Epoch 4 | Step 2704200 | Avg Loss: 0.0152 | Grad Norm: 0.00953456\n",
      "Epoch 4 | Step 2704300 | Avg Loss: 0.0155 | Grad Norm: 0.01024950\n",
      "Epoch 4 | Step 2704400 | Avg Loss: 0.0156 | Grad Norm: 0.00992695\n",
      "Epoch 4 | Step 2704500 | Avg Loss: 0.0160 | Grad Norm: 0.00859796\n",
      "Epoch 4 | Step 2704600 | Avg Loss: 0.0156 | Grad Norm: 0.00965282\n",
      "Epoch 4 | Step 2704700 | Avg Loss: 0.0152 | Grad Norm: 0.00934346\n",
      "Epoch 4 | Step 2704800 | Avg Loss: 0.0152 | Grad Norm: 0.00885172\n",
      "Epoch 4 | Step 2704900 | Avg Loss: 0.0157 | Grad Norm: 0.00932895\n",
      "Epoch 4 | Step 2705000 | Avg Loss: 0.0151 | Grad Norm: 0.00879329\n",
      "Epoch 4 | Step 2705100 | Avg Loss: 0.0154 | Grad Norm: 0.00920361\n",
      "Epoch 4 | Step 2705200 | Avg Loss: 0.0156 | Grad Norm: 0.01171144\n",
      "Epoch 4 | Step 2705300 | Avg Loss: 0.0152 | Grad Norm: 0.00894638\n",
      "Epoch 4 | Step 2705400 | Avg Loss: 0.0153 | Grad Norm: 0.00833145\n",
      "Epoch 4 | Step 2705500 | Avg Loss: 0.0151 | Grad Norm: 0.00914013\n",
      "Epoch 4 | Step 2705600 | Avg Loss: 0.0155 | Grad Norm: 0.00904784\n",
      "Epoch 4 | Step 2705700 | Avg Loss: 0.0156 | Grad Norm: 0.00927650\n",
      "Epoch 4 | Step 2705800 | Avg Loss: 0.0155 | Grad Norm: 0.00951614\n",
      "Epoch 4 | Step 2705900 | Avg Loss: 0.0154 | Grad Norm: 0.00842506\n",
      "Epoch 4 | Step 2706000 | Avg Loss: 0.0152 | Grad Norm: 0.00884264\n",
      "Epoch 4 | Step 2706100 | Avg Loss: 0.0150 | Grad Norm: 0.00924160\n",
      "Epoch 4 | Step 2706200 | Avg Loss: 0.0151 | Grad Norm: 0.00912952\n",
      "Epoch 4 | Step 2706300 | Avg Loss: 0.0155 | Grad Norm: 0.00948823\n",
      "Epoch 4 | Step 2706400 | Avg Loss: 0.0154 | Grad Norm: 0.00921819\n",
      "Epoch 4 | Step 2706500 | Avg Loss: 0.0151 | Grad Norm: 0.00904899\n",
      "Epoch 4 | Step 2706600 | Avg Loss: 0.0151 | Grad Norm: 0.00937185\n",
      "Epoch 4 | Step 2706700 | Avg Loss: 0.0150 | Grad Norm: 0.00883013\n",
      "Epoch 4 | Step 2706800 | Avg Loss: 0.0154 | Grad Norm: 0.00898648\n",
      "Epoch 4 | Step 2706900 | Avg Loss: 0.0156 | Grad Norm: 0.00949241\n",
      "Epoch 4 | Step 2707000 | Avg Loss: 0.0159 | Grad Norm: 0.00898875\n",
      "Epoch 4 | Step 2707100 | Avg Loss: 0.0156 | Grad Norm: 0.01208331\n",
      "Epoch 4 | Step 2707200 | Avg Loss: 0.0158 | Grad Norm: 0.00923946\n",
      "Epoch 4 | Step 2707300 | Avg Loss: 0.0153 | Grad Norm: 0.00793229\n",
      "Epoch 4 | Step 2707400 | Avg Loss: 0.0154 | Grad Norm: 0.00966846\n",
      "Epoch 4 | Step 2707500 | Avg Loss: 0.0157 | Grad Norm: 0.00807829\n",
      "Epoch 4 | Step 2707600 | Avg Loss: 0.0152 | Grad Norm: 0.00923962\n",
      "Epoch 4 | Step 2707700 | Avg Loss: 0.0154 | Grad Norm: 0.00856229\n",
      "Epoch 4 | Step 2707800 | Avg Loss: 0.0158 | Grad Norm: 0.01061469\n",
      "Epoch 4 | Step 2707900 | Avg Loss: 0.0152 | Grad Norm: 0.00882110\n",
      "Epoch 4 | Step 2708000 | Avg Loss: 0.0153 | Grad Norm: 0.01019651\n",
      "Epoch 4 | Step 2708100 | Avg Loss: 0.0155 | Grad Norm: 0.00967795\n",
      "Epoch 4 | Step 2708200 | Avg Loss: 0.0156 | Grad Norm: 0.01065852\n",
      "Epoch 4 | Step 2708300 | Avg Loss: 0.0158 | Grad Norm: 0.00929107\n",
      "Epoch 4 | Step 2708400 | Avg Loss: 0.0156 | Grad Norm: 0.00836265\n",
      "Epoch 4 | Step 2708500 | Avg Loss: 0.0159 | Grad Norm: 0.00931758\n",
      "Epoch 4 | Step 2708600 | Avg Loss: 0.0161 | Grad Norm: 0.00868464\n",
      "Epoch 4 | Step 2708700 | Avg Loss: 0.0159 | Grad Norm: 0.00984671\n",
      "Epoch 4 | Step 2708800 | Avg Loss: 0.0155 | Grad Norm: 0.01036490\n",
      "Epoch 4 | Step 2708900 | Avg Loss: 0.0156 | Grad Norm: 0.00927541\n",
      "Epoch 4 | Step 2709000 | Avg Loss: 0.0155 | Grad Norm: 0.00846927\n",
      "Epoch 4 | Step 2709100 | Avg Loss: 0.0154 | Grad Norm: 0.01036802\n",
      "Epoch 4 | Step 2709200 | Avg Loss: 0.0153 | Grad Norm: 0.00862095\n",
      "Epoch 4 | Step 2709300 | Avg Loss: 0.0150 | Grad Norm: 0.00817095\n",
      "Epoch 4 | Step 2709400 | Avg Loss: 0.0151 | Grad Norm: 0.00866525\n",
      "Epoch 4 | Step 2709500 | Avg Loss: 0.0152 | Grad Norm: 0.00961308\n",
      "Epoch 4 | Step 2709600 | Avg Loss: 0.0155 | Grad Norm: 0.01067233\n",
      "Epoch 4 | Step 2709700 | Avg Loss: 0.0157 | Grad Norm: 0.00943725\n",
      "Epoch 4 | Step 2709800 | Avg Loss: 0.0155 | Grad Norm: 0.00853395\n",
      "Epoch 4 | Step 2709900 | Avg Loss: 0.0153 | Grad Norm: 0.00903049\n",
      "Epoch 4 | Step 2710000 | Avg Loss: 0.0154 | Grad Norm: 0.00869320\n",
      "Epoch 4 | Step 2710100 | Avg Loss: 0.0157 | Grad Norm: 0.00953426\n",
      "Epoch 4 | Step 2710200 | Avg Loss: 0.0155 | Grad Norm: 0.00829829\n",
      "Epoch 4 | Step 2710300 | Avg Loss: 0.0154 | Grad Norm: 0.00855455\n",
      "Epoch 4 | Step 2710400 | Avg Loss: 0.0153 | Grad Norm: 0.00826732\n",
      "Epoch 4 | Step 2710500 | Avg Loss: 0.0151 | Grad Norm: 0.00984524\n",
      "Epoch 4 | Step 2710600 | Avg Loss: 0.0151 | Grad Norm: 0.01005001\n",
      "Epoch 4 | Step 2710700 | Avg Loss: 0.0150 | Grad Norm: 0.00864233\n",
      "Epoch 4 | Step 2710800 | Avg Loss: 0.0153 | Grad Norm: 0.00888186\n",
      "Epoch 4 | Step 2710900 | Avg Loss: 0.0157 | Grad Norm: 0.01085591\n",
      "Epoch 4 | Step 2711000 | Avg Loss: 0.0155 | Grad Norm: 0.00852870\n",
      "Epoch 4 | Step 2711100 | Avg Loss: 0.0150 | Grad Norm: 0.01075790\n",
      "Epoch 4 | Step 2711200 | Avg Loss: 0.0152 | Grad Norm: 0.00895863\n",
      "Epoch 4 | Step 2711300 | Avg Loss: 0.0153 | Grad Norm: 0.00917653\n",
      "Epoch 4 | Step 2711400 | Avg Loss: 0.0156 | Grad Norm: 0.01257967\n",
      "Epoch 4 | Step 2711500 | Avg Loss: 0.0152 | Grad Norm: 0.01136737\n",
      "Epoch 4 | Step 2711600 | Avg Loss: 0.0155 | Grad Norm: 0.00844608\n",
      "Epoch 4 | Step 2711700 | Avg Loss: 0.0155 | Grad Norm: 0.00870177\n",
      "Epoch 4 | Step 2711800 | Avg Loss: 0.0157 | Grad Norm: 0.00974617\n",
      "Epoch 4 | Step 2711900 | Avg Loss: 0.0151 | Grad Norm: 0.00956110\n",
      "Epoch 4 | Step 2712000 | Avg Loss: 0.0156 | Grad Norm: 0.00942247\n",
      "Epoch 4 | Step 2712100 | Avg Loss: 0.0159 | Grad Norm: 0.00953291\n",
      "Epoch 4 | Step 2712200 | Avg Loss: 0.0158 | Grad Norm: 0.00830481\n",
      "Epoch 4 | Step 2712300 | Avg Loss: 0.0159 | Grad Norm: 0.00940708\n",
      "Epoch 4 | Step 2712400 | Avg Loss: 0.0155 | Grad Norm: 0.00859601\n",
      "Epoch 4 | Step 2712500 | Avg Loss: 0.0155 | Grad Norm: 0.00805604\n",
      "Epoch 4 | Step 2712600 | Avg Loss: 0.0153 | Grad Norm: 0.01827878\n",
      "Epoch 4 | Step 2712700 | Avg Loss: 0.0151 | Grad Norm: 0.00804259\n",
      "Epoch 4 | Step 2712800 | Avg Loss: 0.0153 | Grad Norm: 0.00959427\n",
      "Epoch 4 | Step 2712900 | Avg Loss: 0.0151 | Grad Norm: 0.00926916\n",
      "Epoch 4 | Step 2713000 | Avg Loss: 0.0153 | Grad Norm: 0.00908571\n",
      "Epoch 4 | Step 2713100 | Avg Loss: 0.0155 | Grad Norm: 0.00925510\n",
      "Epoch 4 | Step 2713200 | Avg Loss: 0.0158 | Grad Norm: 0.01028513\n",
      "Epoch 4 | Step 2713300 | Avg Loss: 0.0157 | Grad Norm: 0.01733348\n",
      "Epoch 4 | Step 2713400 | Avg Loss: 0.0155 | Grad Norm: 0.00880585\n",
      "Epoch 4 | Step 2713500 | Avg Loss: 0.0155 | Grad Norm: 0.00999174\n",
      "Epoch 4 | Step 2713600 | Avg Loss: 0.0154 | Grad Norm: 0.01169460\n",
      "Epoch 4 | Step 2713700 | Avg Loss: 0.0156 | Grad Norm: 0.00889464\n",
      "Epoch 4 | Step 2713800 | Avg Loss: 0.0152 | Grad Norm: 0.00816626\n",
      "Epoch 4 | Step 2713900 | Avg Loss: 0.0153 | Grad Norm: 0.00861867\n",
      "Epoch 4 | Step 2714000 | Avg Loss: 0.0151 | Grad Norm: 0.00803610\n",
      "Epoch 4 | Step 2714100 | Avg Loss: 0.0150 | Grad Norm: 0.01118364\n",
      "Epoch 4 | Step 2714200 | Avg Loss: 0.0149 | Grad Norm: 0.00853712\n",
      "Epoch 4 | Step 2714300 | Avg Loss: 0.0143 | Grad Norm: 0.00835918\n",
      "Epoch 4 | Step 2714400 | Avg Loss: 0.0146 | Grad Norm: 0.01070877\n",
      "Epoch 4 | Step 2714500 | Avg Loss: 0.0148 | Grad Norm: 0.00831665\n",
      "Epoch 4 | Step 2714600 | Avg Loss: 0.0149 | Grad Norm: 0.00890056\n",
      "Epoch 4 | Step 2714700 | Avg Loss: 0.0150 | Grad Norm: 0.01002429\n",
      "Epoch 4 | Step 2714800 | Avg Loss: 0.0152 | Grad Norm: 0.00837778\n",
      "Epoch 4 | Step 2714900 | Avg Loss: 0.0153 | Grad Norm: 0.00859417\n",
      "Epoch 4 | Step 2715000 | Avg Loss: 0.0153 | Grad Norm: 0.00893773\n",
      "Epoch 4 | Step 2715100 | Avg Loss: 0.0152 | Grad Norm: 0.01015750\n",
      "Epoch 4 | Step 2715200 | Avg Loss: 0.0152 | Grad Norm: 0.00884228\n",
      "Epoch 4 | Step 2715300 | Avg Loss: 0.0154 | Grad Norm: 0.00826499\n",
      "Epoch 4 | Step 2715400 | Avg Loss: 0.0152 | Grad Norm: 0.00902716\n",
      "Epoch 4 | Step 2715500 | Avg Loss: 0.0153 | Grad Norm: 0.01175427\n",
      "Epoch 4 | Step 2715600 | Avg Loss: 0.0149 | Grad Norm: 0.00799175\n",
      "Epoch 4 | Step 2715700 | Avg Loss: 0.0147 | Grad Norm: 0.00774739\n",
      "Epoch 4 | Step 2715800 | Avg Loss: 0.0146 | Grad Norm: 0.00784885\n",
      "Epoch 4 | Step 2715900 | Avg Loss: 0.0149 | Grad Norm: 0.00804037\n",
      "Epoch 4 | Step 2716000 | Avg Loss: 0.0153 | Grad Norm: 0.00911534\n",
      "Epoch 4 | Step 2716100 | Avg Loss: 0.0156 | Grad Norm: 0.00876024\n",
      "Epoch 4 | Step 2716200 | Avg Loss: 0.0157 | Grad Norm: 0.01032329\n",
      "Epoch 4 | Step 2716300 | Avg Loss: 0.0154 | Grad Norm: 0.00829839\n",
      "Epoch 4 | Step 2716400 | Avg Loss: 0.0152 | Grad Norm: 0.00808986\n",
      "Epoch 4 | Step 2716500 | Avg Loss: 0.0151 | Grad Norm: 0.00856384\n",
      "Epoch 4 | Step 2716600 | Avg Loss: 0.0152 | Grad Norm: 0.00936455\n",
      "Epoch 4 | Step 2716700 | Avg Loss: 0.0148 | Grad Norm: 0.01145069\n",
      "Epoch 4 | Step 2716800 | Avg Loss: 0.0151 | Grad Norm: 0.00829306\n",
      "Epoch 4 | Step 2716900 | Avg Loss: 0.0156 | Grad Norm: 0.00934091\n",
      "Epoch 4 | Step 2717000 | Avg Loss: 0.0157 | Grad Norm: 0.00921941\n",
      "Epoch 4 | Step 2717100 | Avg Loss: 0.0153 | Grad Norm: 0.00840102\n",
      "Epoch 4 | Step 2717200 | Avg Loss: 0.0154 | Grad Norm: 0.01066403\n",
      "Epoch 4 | Step 2717300 | Avg Loss: 0.0155 | Grad Norm: 0.00875664\n",
      "Epoch 4 | Step 2717400 | Avg Loss: 0.0154 | Grad Norm: 0.00808407\n",
      "Epoch 4 | Step 2717500 | Avg Loss: 0.0153 | Grad Norm: 0.00827886\n",
      "Epoch 4 | Step 2717600 | Avg Loss: 0.0155 | Grad Norm: 0.00908733\n",
      "Epoch 4 | Step 2717700 | Avg Loss: 0.0154 | Grad Norm: 0.00932459\n",
      "Epoch 4 | Step 2717800 | Avg Loss: 0.0153 | Grad Norm: 0.00922378\n",
      "Epoch 4 | Step 2717900 | Avg Loss: 0.0151 | Grad Norm: 0.00981188\n",
      "Epoch 4 | Step 2718000 | Avg Loss: 0.0151 | Grad Norm: 0.00909695\n",
      "Epoch 4 | Step 2718100 | Avg Loss: 0.0150 | Grad Norm: 0.00945854\n",
      "Epoch 4 | Step 2718200 | Avg Loss: 0.0149 | Grad Norm: 0.00978736\n",
      "Epoch 4 | Step 2718300 | Avg Loss: 0.0152 | Grad Norm: 0.00978258\n",
      "Epoch 4 | Step 2718400 | Avg Loss: 0.0153 | Grad Norm: 0.00915668\n",
      "Epoch 4 | Step 2718500 | Avg Loss: 0.0157 | Grad Norm: 0.00903567\n",
      "Epoch 4 | Step 2718600 | Avg Loss: 0.0153 | Grad Norm: 0.00921520\n",
      "Epoch 4 | Step 2718700 | Avg Loss: 0.0151 | Grad Norm: 0.00909447\n",
      "Epoch 4 | Step 2718800 | Avg Loss: 0.0151 | Grad Norm: 0.00809340\n",
      "Epoch 4 | Step 2718900 | Avg Loss: 0.0152 | Grad Norm: 0.00853810\n",
      "Epoch 4 | Step 2719000 | Avg Loss: 0.0153 | Grad Norm: 0.00868106\n",
      "Epoch 4 | Step 2719100 | Avg Loss: 0.0154 | Grad Norm: 0.00901800\n",
      "Epoch 4 | Step 2719200 | Avg Loss: 0.0150 | Grad Norm: 0.00786314\n",
      "Epoch 4 | Step 2719300 | Avg Loss: 0.0152 | Grad Norm: 0.00849015\n",
      "Epoch 4 | Step 2719400 | Avg Loss: 0.0153 | Grad Norm: 0.00912715\n",
      "Epoch 4 | Step 2719500 | Avg Loss: 0.0156 | Grad Norm: 0.00884053\n",
      "Epoch 4 | Step 2719600 | Avg Loss: 0.0158 | Grad Norm: 0.00763769\n",
      "Epoch 4 | Step 2719700 | Avg Loss: 0.0156 | Grad Norm: 0.00891910\n",
      "Epoch 4 | Step 2719800 | Avg Loss: 0.0157 | Grad Norm: 0.00854100\n",
      "Epoch 4 | Step 2719900 | Avg Loss: 0.0162 | Grad Norm: 0.01140993\n",
      "Epoch 4 | Step 2720000 | Avg Loss: 0.0167 | Grad Norm: 0.00839424\n",
      "Epoch 4 | Step 2720100 | Avg Loss: 0.0166 | Grad Norm: 0.00952748\n",
      "Epoch 4 | Step 2720200 | Avg Loss: 0.0165 | Grad Norm: 0.00902756\n",
      "Epoch 4 | Step 2720300 | Avg Loss: 0.0163 | Grad Norm: 0.00804229\n",
      "Epoch 4 | Step 2720400 | Avg Loss: 0.0162 | Grad Norm: 0.00952519\n",
      "Epoch 4 | Step 2720500 | Avg Loss: 0.0163 | Grad Norm: 0.01015743\n",
      "Epoch 4 | Step 2720600 | Avg Loss: 0.0163 | Grad Norm: 0.01092676\n",
      "Epoch 4 | Step 2720700 | Avg Loss: 0.0164 | Grad Norm: 0.00879586\n",
      "Epoch 4 | Step 2720800 | Avg Loss: 0.0162 | Grad Norm: 0.00912150\n",
      "Epoch 4 | Step 2720900 | Avg Loss: 0.0159 | Grad Norm: 0.00913709\n",
      "Epoch 4 | Step 2721000 | Avg Loss: 0.0155 | Grad Norm: 0.00748040\n",
      "Epoch 4 | Step 2721100 | Avg Loss: 0.0154 | Grad Norm: 0.01065332\n",
      "Epoch 4 | Step 2721200 | Avg Loss: 0.0152 | Grad Norm: 0.00890125\n",
      "Epoch 4 | Step 2721300 | Avg Loss: 0.0155 | Grad Norm: 0.01083300\n",
      "Epoch 4 | Step 2721400 | Avg Loss: 0.0154 | Grad Norm: 0.01297783\n",
      "Epoch 4 | Step 2721500 | Avg Loss: 0.0154 | Grad Norm: 0.00853750\n",
      "Epoch 4 | Step 2721600 | Avg Loss: 0.0154 | Grad Norm: 0.01111150\n",
      "Epoch 4 | Step 2721700 | Avg Loss: 0.0155 | Grad Norm: 0.01060828\n",
      "Epoch 4 | Step 2721800 | Avg Loss: 0.0153 | Grad Norm: 0.00973041\n",
      "Epoch 4 | Step 2721900 | Avg Loss: 0.0154 | Grad Norm: 0.00860805\n",
      "Epoch 4 | Step 2722000 | Avg Loss: 0.0155 | Grad Norm: 0.00873151\n",
      "Epoch 4 | Step 2722100 | Avg Loss: 0.0153 | Grad Norm: 0.00919247\n",
      "Epoch 4 | Step 2722200 | Avg Loss: 0.0153 | Grad Norm: 0.00872722\n",
      "Epoch 4 | Step 2722300 | Avg Loss: 0.0155 | Grad Norm: 0.00835715\n",
      "Epoch 4 | Step 2722400 | Avg Loss: 0.0158 | Grad Norm: 0.00867520\n",
      "Epoch 4 | Step 2722500 | Avg Loss: 0.0157 | Grad Norm: 0.01023972\n",
      "Epoch 4 | Step 2722600 | Avg Loss: 0.0158 | Grad Norm: 0.00890624\n",
      "Epoch 4 | Step 2722700 | Avg Loss: 0.0157 | Grad Norm: 0.00839407\n",
      "Epoch 4 | Step 2722800 | Avg Loss: 0.0156 | Grad Norm: 0.01019949\n",
      "Epoch 4 | Step 2722900 | Avg Loss: 0.0158 | Grad Norm: 0.01143039\n",
      "Epoch 4 | Step 2723000 | Avg Loss: 0.0156 | Grad Norm: 0.00998274\n",
      "Epoch 4 | Step 2723100 | Avg Loss: 0.0157 | Grad Norm: 0.00864791\n",
      "Epoch 4 | Step 2723200 | Avg Loss: 0.0158 | Grad Norm: 0.00909141\n",
      "Epoch 4 | Step 2723300 | Avg Loss: 0.0159 | Grad Norm: 0.01013482\n",
      "Epoch 4 | Step 2723400 | Avg Loss: 0.0155 | Grad Norm: 0.00791578\n",
      "Epoch 4 | Step 2723500 | Avg Loss: 0.0155 | Grad Norm: 0.00983362\n",
      "Epoch 4 | Step 2723600 | Avg Loss: 0.0155 | Grad Norm: 0.00852637\n",
      "Epoch 4 | Step 2723700 | Avg Loss: 0.0152 | Grad Norm: 0.00895850\n",
      "Epoch 4 | Step 2723800 | Avg Loss: 0.0152 | Grad Norm: 0.01075345\n",
      "Epoch 4 | Step 2723900 | Avg Loss: 0.0152 | Grad Norm: 0.00896958\n",
      "Epoch 4 | Step 2724000 | Avg Loss: 0.0156 | Grad Norm: 0.01273443\n",
      "Epoch 4 | Step 2724100 | Avg Loss: 0.0156 | Grad Norm: 0.00851813\n",
      "Epoch 4 | Step 2724200 | Avg Loss: 0.0156 | Grad Norm: 0.00868389\n",
      "Epoch 4 | Step 2724300 | Avg Loss: 0.0153 | Grad Norm: 0.00925652\n",
      "Epoch 4 | Step 2724400 | Avg Loss: 0.0155 | Grad Norm: 0.00863937\n",
      "Epoch 4 | Step 2724500 | Avg Loss: 0.0157 | Grad Norm: 0.00866629\n",
      "Epoch 4 | Step 2724600 | Avg Loss: 0.0165 | Grad Norm: 0.00959893\n",
      "Epoch 4 | Step 2724700 | Avg Loss: 0.0161 | Grad Norm: 0.00916060\n",
      "Epoch 4 | Step 2724800 | Avg Loss: 0.0161 | Grad Norm: 0.00866170\n",
      "Epoch 4 | Step 2724900 | Avg Loss: 0.0156 | Grad Norm: 0.01009389\n",
      "Epoch 4 | Step 2725000 | Avg Loss: 0.0156 | Grad Norm: 0.00935642\n",
      "Epoch 4 | Step 2725100 | Avg Loss: 0.0152 | Grad Norm: 0.00974523\n",
      "Epoch 4 | Step 2725200 | Avg Loss: 0.0153 | Grad Norm: 0.00793285\n",
      "Epoch 4 | Step 2725300 | Avg Loss: 0.0157 | Grad Norm: 0.00939512\n",
      "Epoch 4 | Step 2725400 | Avg Loss: 0.0156 | Grad Norm: 0.00831525\n",
      "Epoch 4 | Step 2725500 | Avg Loss: 0.0153 | Grad Norm: 0.00886951\n",
      "Epoch 4 | Step 2725600 | Avg Loss: 0.0152 | Grad Norm: 0.00834110\n",
      "Epoch 4 | Step 2725700 | Avg Loss: 0.0154 | Grad Norm: 0.00787444\n",
      "Epoch 4 | Step 2725800 | Avg Loss: 0.0151 | Grad Norm: 0.00892332\n",
      "Epoch 4 | Step 2725900 | Avg Loss: 0.0156 | Grad Norm: 0.00810568\n",
      "Epoch 4 | Step 2726000 | Avg Loss: 0.0162 | Grad Norm: 0.00808289\n",
      "Epoch 4 | Step 2726100 | Avg Loss: 0.0158 | Grad Norm: 0.00900082\n",
      "Epoch 4 | Step 2726200 | Avg Loss: 0.0156 | Grad Norm: 0.00863167\n",
      "Epoch 4 | Step 2726300 | Avg Loss: 0.0157 | Grad Norm: 0.00839244\n",
      "Epoch 4 | Step 2726400 | Avg Loss: 0.0156 | Grad Norm: 0.00808672\n",
      "Epoch 4 | Step 2726500 | Avg Loss: 0.0154 | Grad Norm: 0.00961729\n",
      "Epoch 4 | Step 2726600 | Avg Loss: 0.0152 | Grad Norm: 0.00924717\n",
      "Epoch 4 | Step 2726700 | Avg Loss: 0.0153 | Grad Norm: 0.00854180\n",
      "Epoch 4 | Step 2726800 | Avg Loss: 0.0155 | Grad Norm: 0.01307908\n",
      "Epoch 4 | Step 2726900 | Avg Loss: 0.0155 | Grad Norm: 0.00949924\n",
      "Epoch 4 | Step 2727000 | Avg Loss: 0.0154 | Grad Norm: 0.01041864\n",
      "Epoch 4 | Step 2727100 | Avg Loss: 0.0152 | Grad Norm: 0.01136145\n",
      "Epoch 4 | Step 2727200 | Avg Loss: 0.0151 | Grad Norm: 0.00791425\n",
      "Epoch 4 | Step 2727300 | Avg Loss: 0.0152 | Grad Norm: 0.00838172\n",
      "Epoch 4 | Step 2727400 | Avg Loss: 0.0154 | Grad Norm: 0.00923707\n",
      "Epoch 4 | Step 2727500 | Avg Loss: 0.0151 | Grad Norm: 0.00985132\n",
      "Epoch 4 | Step 2727600 | Avg Loss: 0.0148 | Grad Norm: 0.00834107\n",
      "Epoch 4 | Step 2727700 | Avg Loss: 0.0150 | Grad Norm: 0.00990008\n",
      "Epoch 4 | Step 2727800 | Avg Loss: 0.0152 | Grad Norm: 0.00903976\n",
      "Epoch 4 | Step 2727900 | Avg Loss: 0.0153 | Grad Norm: 0.00863332\n",
      "Epoch 4 | Step 2728000 | Avg Loss: 0.0154 | Grad Norm: 0.00830809\n",
      "Epoch 4 | Step 2728100 | Avg Loss: 0.0153 | Grad Norm: 0.00888167\n",
      "Epoch 4 | Step 2728200 | Avg Loss: 0.0155 | Grad Norm: 0.00849752\n",
      "Epoch 4 | Step 2728300 | Avg Loss: 0.0157 | Grad Norm: 0.00921636\n",
      "Epoch 4 | Step 2728400 | Avg Loss: 0.0158 | Grad Norm: 0.00876126\n",
      "Epoch 4 | Step 2728500 | Avg Loss: 0.0158 | Grad Norm: 0.00879946\n",
      "Epoch 4 | Step 2728600 | Avg Loss: 0.0157 | Grad Norm: 0.00911161\n",
      "Epoch 4 | Step 2728700 | Avg Loss: 0.0155 | Grad Norm: 0.01011961\n",
      "Epoch 4 | Step 2728800 | Avg Loss: 0.0153 | Grad Norm: 0.00757594\n",
      "Epoch 4 | Step 2728900 | Avg Loss: 0.0153 | Grad Norm: 0.00913514\n",
      "Epoch 4 | Step 2729000 | Avg Loss: 0.0153 | Grad Norm: 0.00975106\n",
      "Epoch 4 | Step 2729100 | Avg Loss: 0.0155 | Grad Norm: 0.01063533\n",
      "Epoch 4 | Step 2729200 | Avg Loss: 0.0155 | Grad Norm: 0.00852981\n",
      "Epoch 4 | Step 2729300 | Avg Loss: 0.0155 | Grad Norm: 0.00995380\n",
      "Epoch 4 | Step 2729400 | Avg Loss: 0.0155 | Grad Norm: 0.00792698\n",
      "Epoch 4 | Step 2729500 | Avg Loss: 0.0156 | Grad Norm: 0.00939610\n",
      "Epoch 4 | Step 2729600 | Avg Loss: 0.0153 | Grad Norm: 0.00995081\n",
      "Epoch 4 | Step 2729700 | Avg Loss: 0.0153 | Grad Norm: 0.00911820\n",
      "Epoch 4 | Step 2729800 | Avg Loss: 0.0151 | Grad Norm: 0.00796381\n",
      "Epoch 4 | Step 2729900 | Avg Loss: 0.0151 | Grad Norm: 0.00911113\n",
      "Epoch 4 | Step 2730000 | Avg Loss: 0.0149 | Grad Norm: 0.00802313\n",
      "Epoch 4 | Step 2730100 | Avg Loss: 0.0151 | Grad Norm: 0.00843589\n",
      "Epoch 4 | Step 2730200 | Avg Loss: 0.0152 | Grad Norm: 0.01153780\n",
      "Epoch 4 | Step 2730300 | Avg Loss: 0.0150 | Grad Norm: 0.00919405\n",
      "Epoch 4 | Step 2730400 | Avg Loss: 0.0148 | Grad Norm: 0.00966402\n",
      "Epoch 4 | Step 2730500 | Avg Loss: 0.0149 | Grad Norm: 0.00876123\n",
      "Epoch 4 | Step 2730600 | Avg Loss: 0.0147 | Grad Norm: 0.00910556\n",
      "Epoch 4 | Step 2730700 | Avg Loss: 0.0154 | Grad Norm: 0.00824100\n",
      "Epoch 4 | Step 2730800 | Avg Loss: 0.0148 | Grad Norm: 0.01197727\n",
      "Epoch 4 | Step 2730900 | Avg Loss: 0.0152 | Grad Norm: 0.00848777\n",
      "Epoch 4 | Step 2731000 | Avg Loss: 0.0148 | Grad Norm: 0.00904493\n",
      "Epoch 4 | Step 2731100 | Avg Loss: 0.0150 | Grad Norm: 0.00814893\n",
      "Epoch 4 | Step 2731200 | Avg Loss: 0.0152 | Grad Norm: 0.00951681\n",
      "Epoch 4 | Step 2731300 | Avg Loss: 0.0155 | Grad Norm: 0.00989967\n",
      "Epoch 4 | Step 2731400 | Avg Loss: 0.0157 | Grad Norm: 0.00896830\n",
      "Epoch 4 | Step 2731500 | Avg Loss: 0.0157 | Grad Norm: 0.00939490\n",
      "Epoch 4 | Step 2731600 | Avg Loss: 0.0159 | Grad Norm: 0.00940233\n",
      "Epoch 4 | Step 2731700 | Avg Loss: 0.0155 | Grad Norm: 0.00920060\n",
      "Epoch 4 | Step 2731800 | Avg Loss: 0.0157 | Grad Norm: 0.01162324\n",
      "Epoch 4 | Step 2731900 | Avg Loss: 0.0157 | Grad Norm: 0.00927223\n",
      "Epoch 4 | Step 2732000 | Avg Loss: 0.0157 | Grad Norm: 0.01032522\n",
      "Epoch 4 | Step 2732100 | Avg Loss: 0.0159 | Grad Norm: 0.00896352\n",
      "Epoch 4 | Step 2732200 | Avg Loss: 0.0161 | Grad Norm: 0.00868010\n",
      "Epoch 4 | Step 2732300 | Avg Loss: 0.0156 | Grad Norm: 0.00811939\n",
      "Epoch 4 | Step 2732400 | Avg Loss: 0.0159 | Grad Norm: 0.00898784\n",
      "Epoch 4 | Step 2732500 | Avg Loss: 0.0159 | Grad Norm: 0.00946345\n",
      "Epoch 4 | Step 2732600 | Avg Loss: 0.0159 | Grad Norm: 0.01065878\n",
      "Epoch 4 | Step 2732700 | Avg Loss: 0.0159 | Grad Norm: 0.00880977\n",
      "Epoch 4 | Step 2732800 | Avg Loss: 0.0158 | Grad Norm: 0.00895852\n",
      "Epoch 4 | Step 2732900 | Avg Loss: 0.0158 | Grad Norm: 0.01115650\n",
      "Epoch 4 | Step 2733000 | Avg Loss: 0.0157 | Grad Norm: 0.00949767\n",
      "Epoch 4 | Step 2733100 | Avg Loss: 0.0157 | Grad Norm: 0.00792839\n",
      "Epoch 4 | Step 2733200 | Avg Loss: 0.0157 | Grad Norm: 0.00863574\n",
      "Epoch 4 | Step 2733300 | Avg Loss: 0.0159 | Grad Norm: 0.00896253\n",
      "Epoch 4 | Step 2733400 | Avg Loss: 0.0158 | Grad Norm: 0.00893817\n",
      "Epoch 4 | Step 2733500 | Avg Loss: 0.0159 | Grad Norm: 0.00897254\n",
      "Epoch 4 | Step 2733600 | Avg Loss: 0.0155 | Grad Norm: 0.00852224\n",
      "Epoch 4 | Step 2733700 | Avg Loss: 0.0157 | Grad Norm: 0.00788523\n",
      "Epoch 4 | Step 2733800 | Avg Loss: 0.0158 | Grad Norm: 0.00907000\n",
      "Epoch 4 | Step 2733900 | Avg Loss: 0.0160 | Grad Norm: 0.00924861\n",
      "Epoch 4 | Step 2734000 | Avg Loss: 0.0161 | Grad Norm: 0.00911156\n",
      "Epoch 4 | Step 2734100 | Avg Loss: 0.0163 | Grad Norm: 0.00966407\n",
      "Epoch 4 | Step 2734200 | Avg Loss: 0.0164 | Grad Norm: 0.00903225\n",
      "Epoch 4 | Step 2734300 | Avg Loss: 0.0162 | Grad Norm: 0.00927998\n",
      "Epoch 4 | Step 2734400 | Avg Loss: 0.0160 | Grad Norm: 0.01081815\n",
      "Epoch 4 | Step 2734500 | Avg Loss: 0.0160 | Grad Norm: 0.00882526\n",
      "Epoch 4 | Step 2734600 | Avg Loss: 0.0161 | Grad Norm: 0.00946190\n",
      "Epoch 4 | Step 2734700 | Avg Loss: 0.0158 | Grad Norm: 0.00970621\n",
      "Epoch 4 | Step 2734800 | Avg Loss: 0.0160 | Grad Norm: 0.00861231\n",
      "Epoch 4 | Step 2734900 | Avg Loss: 0.0158 | Grad Norm: 0.01101430\n",
      "Epoch 4 | Step 2735000 | Avg Loss: 0.0160 | Grad Norm: 0.00921452\n",
      "Epoch 4 | Step 2735100 | Avg Loss: 0.0161 | Grad Norm: 0.00954187\n",
      "Epoch 4 | Step 2735200 | Avg Loss: 0.0159 | Grad Norm: 0.00853761\n",
      "Epoch 4 | Step 2735300 | Avg Loss: 0.0159 | Grad Norm: 0.01020989\n",
      "Epoch 4 | Step 2735400 | Avg Loss: 0.0159 | Grad Norm: 0.01052147\n",
      "Epoch 4 | Step 2735500 | Avg Loss: 0.0159 | Grad Norm: 0.00927779\n",
      "Epoch 4 | Step 2735600 | Avg Loss: 0.0160 | Grad Norm: 0.00990503\n",
      "Epoch 4 | Step 2735700 | Avg Loss: 0.0158 | Grad Norm: 0.00941608\n",
      "Epoch 4 | Step 2735800 | Avg Loss: 0.0160 | Grad Norm: 0.00934896\n",
      "Epoch 4 | Step 2735900 | Avg Loss: 0.0157 | Grad Norm: 0.00897340\n",
      "Epoch 4 | Step 2736000 | Avg Loss: 0.0155 | Grad Norm: 0.00937677\n",
      "Epoch 4 | Step 2736100 | Avg Loss: 0.0154 | Grad Norm: 0.00930764\n",
      "Epoch 4 | Step 2736200 | Avg Loss: 0.0155 | Grad Norm: 0.00937388\n",
      "Epoch 4 | Step 2736300 | Avg Loss: 0.0156 | Grad Norm: 0.00934207\n",
      "Epoch 4 | Step 2736400 | Avg Loss: 0.0157 | Grad Norm: 0.00945720\n",
      "Epoch 4 | Step 2736500 | Avg Loss: 0.0156 | Grad Norm: 0.00797458\n",
      "Epoch 4 | Step 2736600 | Avg Loss: 0.0154 | Grad Norm: 0.01107305\n",
      "Epoch 4 | Step 2736700 | Avg Loss: 0.0152 | Grad Norm: 0.00869456\n",
      "Epoch 4 | Step 2736800 | Avg Loss: 0.0159 | Grad Norm: 0.01186393\n",
      "Epoch 4 | Step 2736900 | Avg Loss: 0.0155 | Grad Norm: 0.00959927\n",
      "Epoch 4 | Step 2737000 | Avg Loss: 0.0155 | Grad Norm: 0.00932272\n",
      "Epoch 4 | Step 2737100 | Avg Loss: 0.0156 | Grad Norm: 0.00910099\n",
      "Epoch 4 | Step 2737200 | Avg Loss: 0.0156 | Grad Norm: 0.00917364\n",
      "Epoch 4 | Step 2737300 | Avg Loss: 0.0159 | Grad Norm: 0.00933472\n",
      "Epoch 4 | Step 2737400 | Avg Loss: 0.0158 | Grad Norm: 0.00956842\n",
      "Epoch 4 | Step 2737500 | Avg Loss: 0.0161 | Grad Norm: 0.00949546\n",
      "Epoch 4 | Step 2737600 | Avg Loss: 0.0163 | Grad Norm: 0.00936274\n",
      "Epoch 4 | Step 2737700 | Avg Loss: 0.0163 | Grad Norm: 0.01005828\n",
      "Epoch 4 | Step 2737800 | Avg Loss: 0.0161 | Grad Norm: 0.00893563\n",
      "Epoch 4 | Step 2737900 | Avg Loss: 0.0160 | Grad Norm: 0.00821582\n",
      "Epoch 4 | Step 2738000 | Avg Loss: 0.0161 | Grad Norm: 0.00890502\n",
      "Epoch 4 | Step 2738100 | Avg Loss: 0.0161 | Grad Norm: 0.01078824\n",
      "Epoch 4 | Step 2738200 | Avg Loss: 0.0160 | Grad Norm: 0.00891705\n",
      "Epoch 4 | Step 2738300 | Avg Loss: 0.0159 | Grad Norm: 0.00910504\n",
      "Epoch 4 | Step 2738400 | Avg Loss: 0.0156 | Grad Norm: 0.00824440\n",
      "Epoch 4 | Step 2738500 | Avg Loss: 0.0157 | Grad Norm: 0.00896066\n",
      "Epoch 4 | Step 2738600 | Avg Loss: 0.0154 | Grad Norm: 0.00955428\n",
      "Epoch 4 | Step 2738700 | Avg Loss: 0.0156 | Grad Norm: 0.00747636\n",
      "Epoch 4 | Step 2738800 | Avg Loss: 0.0153 | Grad Norm: 0.01018528\n",
      "Epoch 4 | Step 2738900 | Avg Loss: 0.0155 | Grad Norm: 0.01129970\n",
      "Epoch 4 | Step 2739000 | Avg Loss: 0.0154 | Grad Norm: 0.01188208\n",
      "Epoch 4 | Step 2739100 | Avg Loss: 0.0155 | Grad Norm: 0.00837225\n",
      "Epoch 4 | Step 2739200 | Avg Loss: 0.0154 | Grad Norm: 0.00871449\n",
      "Epoch 4 | Step 2739300 | Avg Loss: 0.0158 | Grad Norm: 0.01099265\n",
      "Epoch 4 | Step 2739400 | Avg Loss: 0.0154 | Grad Norm: 0.00896298\n",
      "Epoch 4 | Step 2739500 | Avg Loss: 0.0154 | Grad Norm: 0.00950673\n",
      "Epoch 4 | Step 2739600 | Avg Loss: 0.0157 | Grad Norm: 0.01170907\n",
      "Epoch 4 | Step 2739700 | Avg Loss: 0.0154 | Grad Norm: 0.00827721\n",
      "Epoch 4 | Step 2739800 | Avg Loss: 0.0153 | Grad Norm: 0.00905591\n",
      "Epoch 4 | Step 2739900 | Avg Loss: 0.0154 | Grad Norm: 0.00852267\n",
      "Epoch 4 | Step 2740000 | Avg Loss: 0.0154 | Grad Norm: 0.00981609\n",
      "Epoch 4 | Step 2740100 | Avg Loss: 0.0154 | Grad Norm: 0.01113463\n",
      "Epoch 4 | Step 2740200 | Avg Loss: 0.0152 | Grad Norm: 0.00800132\n",
      "Epoch 4 | Step 2740300 | Avg Loss: 0.0151 | Grad Norm: 0.00948741\n",
      "Epoch 4 | Step 2740400 | Avg Loss: 0.0153 | Grad Norm: 0.00983206\n",
      "Epoch 4 | Step 2740500 | Avg Loss: 0.0153 | Grad Norm: 0.00887893\n",
      "Epoch 4 | Step 2740600 | Avg Loss: 0.0155 | Grad Norm: 0.01124183\n",
      "Epoch 4 | Step 2740700 | Avg Loss: 0.0158 | Grad Norm: 0.00923712\n",
      "Epoch 4 | Step 2740800 | Avg Loss: 0.0156 | Grad Norm: 0.01318251\n",
      "Epoch 4 | Step 2740900 | Avg Loss: 0.0156 | Grad Norm: 0.00923299\n",
      "Epoch 4 | Step 2741000 | Avg Loss: 0.0157 | Grad Norm: 0.00894618\n",
      "Epoch 4 | Step 2741100 | Avg Loss: 0.0156 | Grad Norm: 0.00774007\n",
      "Epoch 4 | Step 2741200 | Avg Loss: 0.0158 | Grad Norm: 0.00963099\n",
      "Epoch 4 | Step 2741300 | Avg Loss: 0.0155 | Grad Norm: 0.00783033\n",
      "Epoch 4 | Step 2741400 | Avg Loss: 0.0153 | Grad Norm: 0.00889593\n",
      "Epoch 4 | Step 2741500 | Avg Loss: 0.0152 | Grad Norm: 0.01043379\n",
      "Epoch 4 | Step 2741600 | Avg Loss: 0.0153 | Grad Norm: 0.01047501\n",
      "Epoch 4 | Step 2741700 | Avg Loss: 0.0154 | Grad Norm: 0.00918505\n",
      "Epoch 4 | Step 2741800 | Avg Loss: 0.0153 | Grad Norm: 0.00919675\n",
      "Epoch 4 | Step 2741900 | Avg Loss: 0.0153 | Grad Norm: 0.00884548\n",
      "Epoch 4 | Step 2742000 | Avg Loss: 0.0154 | Grad Norm: 0.00836957\n",
      "Epoch 4 | Step 2742100 | Avg Loss: 0.0155 | Grad Norm: 0.00881161\n",
      "Epoch 4 | Step 2742200 | Avg Loss: 0.0158 | Grad Norm: 0.00919252\n",
      "Epoch 4 | Step 2742300 | Avg Loss: 0.0156 | Grad Norm: 0.00763068\n",
      "Epoch 4 | Step 2742400 | Avg Loss: 0.0155 | Grad Norm: 0.00888678\n",
      "Epoch 4 | Step 2742500 | Avg Loss: 0.0155 | Grad Norm: 0.00817215\n",
      "Epoch 4 | Step 2742600 | Avg Loss: 0.0155 | Grad Norm: 0.00890289\n",
      "Epoch 4 | Step 2742700 | Avg Loss: 0.0159 | Grad Norm: 0.00861929\n",
      "Epoch 4 | Step 2742800 | Avg Loss: 0.0157 | Grad Norm: 0.00818449\n",
      "Epoch 4 | Step 2742900 | Avg Loss: 0.0153 | Grad Norm: 0.00873281\n",
      "Epoch 4 | Step 2743000 | Avg Loss: 0.0154 | Grad Norm: 0.00884564\n",
      "Epoch 4 | Step 2743100 | Avg Loss: 0.0156 | Grad Norm: 0.00829378\n",
      "Epoch 4 | Step 2743200 | Avg Loss: 0.0160 | Grad Norm: 0.00840128\n",
      "Epoch 4 | Step 2743300 | Avg Loss: 0.0163 | Grad Norm: 0.00910679\n",
      "Epoch 4 | Step 2743400 | Avg Loss: 0.0170 | Grad Norm: 0.01039036\n",
      "Epoch 4 | Step 2743500 | Avg Loss: 0.0163 | Grad Norm: 0.00956191\n",
      "Epoch 4 | Step 2743600 | Avg Loss: 0.0163 | Grad Norm: 0.00870364\n",
      "Epoch 4 | Step 2743700 | Avg Loss: 0.0160 | Grad Norm: 0.00966577\n",
      "Epoch 4 | Step 2743800 | Avg Loss: 0.0159 | Grad Norm: 0.01000422\n",
      "Epoch 4 | Step 2743900 | Avg Loss: 0.0156 | Grad Norm: 0.01561658\n",
      "Epoch 4 | Step 2744000 | Avg Loss: 0.0157 | Grad Norm: 0.00947402\n",
      "Epoch 4 | Step 2744100 | Avg Loss: 0.0159 | Grad Norm: 0.00876769\n",
      "Epoch 4 | Step 2744200 | Avg Loss: 0.0155 | Grad Norm: 0.00900743\n",
      "Epoch 4 | Step 2744300 | Avg Loss: 0.0152 | Grad Norm: 0.00676469\n",
      "Epoch 4 | Step 2744400 | Avg Loss: 0.0152 | Grad Norm: 0.00945980\n",
      "Epoch 4 | Step 2744500 | Avg Loss: 0.0151 | Grad Norm: 0.00910887\n",
      "Epoch 4 | Step 2744600 | Avg Loss: 0.0151 | Grad Norm: 0.00903317\n",
      "Epoch 4 | Step 2744700 | Avg Loss: 0.0150 | Grad Norm: 0.00816662\n",
      "Epoch 4 | Step 2744800 | Avg Loss: 0.0151 | Grad Norm: 0.00904531\n",
      "Epoch 4 | Step 2744900 | Avg Loss: 0.0149 | Grad Norm: 0.00909204\n",
      "Epoch 4 | Step 2745000 | Avg Loss: 0.0150 | Grad Norm: 0.00968818\n",
      "Epoch 4 | Step 2745100 | Avg Loss: 0.0150 | Grad Norm: 0.01055584\n",
      "Epoch 4 | Step 2745200 | Avg Loss: 0.0152 | Grad Norm: 0.00908240\n",
      "Epoch 4 | Step 2745300 | Avg Loss: 0.0154 | Grad Norm: 0.01251880\n",
      "Epoch 4 | Step 2745400 | Avg Loss: 0.0155 | Grad Norm: 0.01018682\n",
      "Epoch 4 | Step 2745500 | Avg Loss: 0.0151 | Grad Norm: 0.00922308\n",
      "Epoch 4 | Step 2745600 | Avg Loss: 0.0150 | Grad Norm: 0.00792172\n",
      "Epoch 4 | Step 2745700 | Avg Loss: 0.0146 | Grad Norm: 0.00874100\n",
      "Epoch 4 | Step 2745800 | Avg Loss: 0.0151 | Grad Norm: 0.01026629\n",
      "Epoch 4 | Step 2745900 | Avg Loss: 0.0156 | Grad Norm: 0.00844560\n",
      "Epoch 4 | Step 2746000 | Avg Loss: 0.0155 | Grad Norm: 0.00924490\n",
      "Epoch 4 | Step 2746100 | Avg Loss: 0.0153 | Grad Norm: 0.00962342\n",
      "Epoch 4 | Step 2746200 | Avg Loss: 0.0151 | Grad Norm: 0.00912878\n",
      "Epoch 4 | Step 2746300 | Avg Loss: 0.0153 | Grad Norm: 0.00899402\n",
      "Epoch 4 | Step 2746400 | Avg Loss: 0.0153 | Grad Norm: 0.00864431\n",
      "Epoch 4 | Step 2746500 | Avg Loss: 0.0154 | Grad Norm: 0.00921071\n",
      "Epoch 4 | Step 2746600 | Avg Loss: 0.0149 | Grad Norm: 0.00831148\n",
      "Epoch 4 | Step 2746700 | Avg Loss: 0.0150 | Grad Norm: 0.00963493\n",
      "Epoch 4 | Step 2746800 | Avg Loss: 0.0151 | Grad Norm: 0.00836125\n",
      "Epoch 4 | Step 2746900 | Avg Loss: 0.0152 | Grad Norm: 0.00858121\n",
      "Epoch 4 | Step 2747000 | Avg Loss: 0.0152 | Grad Norm: 0.00815159\n",
      "Epoch 4 | Step 2747100 | Avg Loss: 0.0153 | Grad Norm: 0.00851609\n",
      "Epoch 4 | Step 2747200 | Avg Loss: 0.0153 | Grad Norm: 0.00829738\n",
      "Epoch 4 | Step 2747300 | Avg Loss: 0.0153 | Grad Norm: 0.01051329\n",
      "Epoch 4 | Step 2747400 | Avg Loss: 0.0153 | Grad Norm: 0.00967822\n",
      "Epoch 4 | Step 2747500 | Avg Loss: 0.0157 | Grad Norm: 0.01009983\n",
      "Epoch 4 | Step 2747600 | Avg Loss: 0.0157 | Grad Norm: 0.00902554\n",
      "Epoch 4 | Step 2747700 | Avg Loss: 0.0157 | Grad Norm: 0.00886926\n",
      "Epoch 4 | Step 2747800 | Avg Loss: 0.0158 | Grad Norm: 0.00925005\n",
      "Epoch 4 | Step 2747900 | Avg Loss: 0.0156 | Grad Norm: 0.01034558\n",
      "Epoch 4 | Step 2748000 | Avg Loss: 0.0157 | Grad Norm: 0.00787705\n",
      "Epoch 4 | Step 2748100 | Avg Loss: 0.0153 | Grad Norm: 0.00812865\n",
      "Epoch 4 | Step 2748200 | Avg Loss: 0.0159 | Grad Norm: 0.00965094\n",
      "Epoch 4 | Step 2748300 | Avg Loss: 0.0163 | Grad Norm: 0.00925319\n",
      "Epoch 4 | Step 2748400 | Avg Loss: 0.0164 | Grad Norm: 0.00895902\n",
      "Epoch 4 | Step 2748500 | Avg Loss: 0.0162 | Grad Norm: 0.00867954\n",
      "Epoch 4 | Step 2748600 | Avg Loss: 0.0160 | Grad Norm: 0.00841032\n",
      "Epoch 4 | Step 2748700 | Avg Loss: 0.0159 | Grad Norm: 0.00937740\n",
      "Epoch 4 | Step 2748800 | Avg Loss: 0.0157 | Grad Norm: 0.00876272\n",
      "Epoch 4 | Step 2748900 | Avg Loss: 0.0158 | Grad Norm: 0.00826588\n",
      "Epoch 4 | Step 2749000 | Avg Loss: 0.0156 | Grad Norm: 0.00830099\n",
      "Epoch 4 | Step 2749100 | Avg Loss: 0.0158 | Grad Norm: 0.01179242\n",
      "Epoch 4 | Step 2749200 | Avg Loss: 0.0157 | Grad Norm: 0.00973553\n",
      "Epoch 4 | Step 2749300 | Avg Loss: 0.0157 | Grad Norm: 0.00899338\n",
      "Epoch 4 | Step 2749400 | Avg Loss: 0.0159 | Grad Norm: 0.00895204\n",
      "Epoch 4 | Step 2749500 | Avg Loss: 0.0157 | Grad Norm: 0.00934538\n",
      "Epoch 4 | Step 2749600 | Avg Loss: 0.0160 | Grad Norm: 0.00973549\n",
      "Epoch 4 | Step 2749700 | Avg Loss: 0.0156 | Grad Norm: 0.00862887\n",
      "Epoch 4 | Step 2749800 | Avg Loss: 0.0156 | Grad Norm: 0.00811877\n",
      "Epoch 4 | Step 2749900 | Avg Loss: 0.0155 | Grad Norm: 0.00983078\n",
      "Epoch 4 | Step 2750000 | Avg Loss: 0.0153 | Grad Norm: 0.00789047\n",
      "Epoch 4 | Step 2750100 | Avg Loss: 0.0154 | Grad Norm: 0.00939875\n",
      "Epoch 4 | Step 2750200 | Avg Loss: 0.0157 | Grad Norm: 0.00831077\n",
      "Epoch 4 | Step 2750300 | Avg Loss: 0.0154 | Grad Norm: 0.00989315\n",
      "Epoch 4 | Step 2750400 | Avg Loss: 0.0158 | Grad Norm: 0.00817796\n",
      "Epoch 4 | Step 2750500 | Avg Loss: 0.0160 | Grad Norm: 0.01036636\n",
      "Epoch 4 | Step 2750600 | Avg Loss: 0.0161 | Grad Norm: 0.01127317\n",
      "Epoch 4 | Step 2750700 | Avg Loss: 0.0156 | Grad Norm: 0.01060810\n",
      "Epoch 4 | Step 2750800 | Avg Loss: 0.0162 | Grad Norm: 0.00993448\n",
      "Epoch 4 | Step 2750900 | Avg Loss: 0.0158 | Grad Norm: 0.00983906\n",
      "Epoch 4 | Step 2751000 | Avg Loss: 0.0157 | Grad Norm: 0.00984182\n",
      "Epoch 4 | Step 2751100 | Avg Loss: 0.0156 | Grad Norm: 0.00889288\n",
      "Epoch 4 | Step 2751200 | Avg Loss: 0.0158 | Grad Norm: 0.00955870\n",
      "Epoch 4 | Step 2751300 | Avg Loss: 0.0157 | Grad Norm: 0.00899070\n",
      "Epoch 4 | Step 2751400 | Avg Loss: 0.0158 | Grad Norm: 0.00897039\n",
      "Epoch 4 | Step 2751500 | Avg Loss: 0.0156 | Grad Norm: 0.00945967\n",
      "Epoch 4 | Step 2751600 | Avg Loss: 0.0158 | Grad Norm: 0.00878395\n",
      "Epoch 4 | Step 2751700 | Avg Loss: 0.0155 | Grad Norm: 0.00902238\n",
      "Epoch 4 | Step 2751800 | Avg Loss: 0.0155 | Grad Norm: 0.00899728\n",
      "Epoch 4 | Step 2751900 | Avg Loss: 0.0156 | Grad Norm: 0.00959654\n",
      "Epoch 4 | Step 2752000 | Avg Loss: 0.0157 | Grad Norm: 0.00892541\n",
      "Epoch 4 | Step 2752100 | Avg Loss: 0.0154 | Grad Norm: 0.00847462\n",
      "Epoch 4 | Step 2752200 | Avg Loss: 0.0154 | Grad Norm: 0.00816670\n",
      "Epoch 4 | Step 2752300 | Avg Loss: 0.0153 | Grad Norm: 0.00812588\n",
      "Epoch 4 | Step 2752400 | Avg Loss: 0.0151 | Grad Norm: 0.00965515\n",
      "Epoch 4 | Step 2752500 | Avg Loss: 0.0156 | Grad Norm: 0.00793396\n",
      "Epoch 4 | Step 2752600 | Avg Loss: 0.0158 | Grad Norm: 0.00808230\n",
      "Epoch 4 | Step 2752700 | Avg Loss: 0.0159 | Grad Norm: 0.00910487\n",
      "Epoch 4 | Step 2752800 | Avg Loss: 0.0155 | Grad Norm: 0.01042240\n",
      "Epoch 4 | Step 2752900 | Avg Loss: 0.0155 | Grad Norm: 0.00849662\n",
      "Epoch 4 | Step 2753000 | Avg Loss: 0.0159 | Grad Norm: 0.00928711\n",
      "Epoch 4 | Step 2753100 | Avg Loss: 0.0162 | Grad Norm: 0.00970468\n",
      "Epoch 4 | Step 2753200 | Avg Loss: 0.0163 | Grad Norm: 0.00895751\n",
      "Epoch 4 | Step 2753300 | Avg Loss: 0.0162 | Grad Norm: 0.00932161\n",
      "Epoch 4 | Step 2753400 | Avg Loss: 0.0160 | Grad Norm: 0.00864792\n",
      "Epoch 4 | Step 2753500 | Avg Loss: 0.0157 | Grad Norm: 0.00905031\n",
      "Epoch 4 | Step 2753600 | Avg Loss: 0.0150 | Grad Norm: 0.00914026\n",
      "Epoch 4 | Step 2753700 | Avg Loss: 0.0153 | Grad Norm: 0.01082417\n",
      "Epoch 4 | Step 2753800 | Avg Loss: 0.0153 | Grad Norm: 0.00891086\n",
      "Epoch 4 | Step 2753900 | Avg Loss: 0.0149 | Grad Norm: 0.01033226\n",
      "Epoch 4 | Step 2754000 | Avg Loss: 0.0153 | Grad Norm: 0.01012931\n",
      "Epoch 4 | Step 2754100 | Avg Loss: 0.0153 | Grad Norm: 0.01129322\n",
      "Epoch 4 | Step 2754200 | Avg Loss: 0.0152 | Grad Norm: 0.00880523\n",
      "Epoch 4 | Step 2754300 | Avg Loss: 0.0153 | Grad Norm: 0.00923409\n",
      "Epoch 4 | Step 2754400 | Avg Loss: 0.0152 | Grad Norm: 0.00835318\n",
      "Epoch 4 | Step 2754500 | Avg Loss: 0.0152 | Grad Norm: 0.01084632\n",
      "Epoch 4 | Step 2754600 | Avg Loss: 0.0152 | Grad Norm: 0.00953936\n",
      "Epoch 4 | Step 2754700 | Avg Loss: 0.0154 | Grad Norm: 0.00969601\n",
      "Epoch 4 | Step 2754800 | Avg Loss: 0.0153 | Grad Norm: 0.00807098\n",
      "Epoch 4 | Step 2754900 | Avg Loss: 0.0155 | Grad Norm: 0.00833877\n",
      "Epoch 4 | Step 2755000 | Avg Loss: 0.0159 | Grad Norm: 0.00973558\n",
      "Epoch 4 | Step 2755100 | Avg Loss: 0.0159 | Grad Norm: 0.01281601\n",
      "Epoch 4 | Step 2755200 | Avg Loss: 0.0161 | Grad Norm: 0.00875370\n",
      "Epoch 4 | Step 2755300 | Avg Loss: 0.0157 | Grad Norm: 0.00844779\n",
      "Epoch 4 | Step 2755400 | Avg Loss: 0.0158 | Grad Norm: 0.01219419\n",
      "Epoch 4 | Step 2755500 | Avg Loss: 0.0157 | Grad Norm: 0.00878724\n",
      "Epoch 4 | Step 2755600 | Avg Loss: 0.0156 | Grad Norm: 0.00820921\n",
      "Epoch 4 | Step 2755700 | Avg Loss: 0.0155 | Grad Norm: 0.00971951\n",
      "Epoch 4 | Step 2755800 | Avg Loss: 0.0156 | Grad Norm: 0.00859177\n",
      "Epoch 4 | Step 2755900 | Avg Loss: 0.0156 | Grad Norm: 0.00826847\n",
      "Epoch 4 | Step 2756000 | Avg Loss: 0.0153 | Grad Norm: 0.00911762\n",
      "Epoch 4 | Step 2756100 | Avg Loss: 0.0157 | Grad Norm: 0.00880441\n",
      "Epoch 4 | Step 2756200 | Avg Loss: 0.0156 | Grad Norm: 0.00963829\n",
      "Epoch 4 | Step 2756300 | Avg Loss: 0.0156 | Grad Norm: 0.01021287\n",
      "Epoch 4 | Step 2756400 | Avg Loss: 0.0153 | Grad Norm: 0.00858416\n",
      "Epoch 4 | Step 2756500 | Avg Loss: 0.0154 | Grad Norm: 0.01118043\n",
      "Epoch 4 | Step 2756600 | Avg Loss: 0.0154 | Grad Norm: 0.00876820\n",
      "Epoch 4 | Step 2756700 | Avg Loss: 0.0157 | Grad Norm: 0.00968279\n",
      "Epoch 4 | Step 2756800 | Avg Loss: 0.0155 | Grad Norm: 0.01131801\n",
      "Epoch 4 | Step 2756900 | Avg Loss: 0.0153 | Grad Norm: 0.00952490\n",
      "Epoch 4 | Step 2757000 | Avg Loss: 0.0154 | Grad Norm: 0.00726888\n",
      "Epoch 4 | Step 2757100 | Avg Loss: 0.0154 | Grad Norm: 0.00921753\n",
      "Epoch 4 | Step 2757200 | Avg Loss: 0.0154 | Grad Norm: 0.01161601\n",
      "Epoch 4 | Step 2757300 | Avg Loss: 0.0156 | Grad Norm: 0.00764424\n",
      "Epoch 4 | Step 2757400 | Avg Loss: 0.0157 | Grad Norm: 0.01093633\n",
      "Epoch 4 | Step 2757500 | Avg Loss: 0.0154 | Grad Norm: 0.00849106\n",
      "Epoch 4 | Step 2757600 | Avg Loss: 0.0154 | Grad Norm: 0.00893268\n",
      "Epoch 4 | Step 2757700 | Avg Loss: 0.0158 | Grad Norm: 0.00849196\n",
      "Epoch 4 | Step 2757800 | Avg Loss: 0.0157 | Grad Norm: 0.00888739\n",
      "Epoch 4 | Step 2757900 | Avg Loss: 0.0157 | Grad Norm: 0.00863904\n",
      "Epoch 4 | Step 2758000 | Avg Loss: 0.0159 | Grad Norm: 0.00948251\n",
      "Epoch 4 | Step 2758100 | Avg Loss: 0.0159 | Grad Norm: 0.01049276\n",
      "Epoch 4 | Step 2758200 | Avg Loss: 0.0162 | Grad Norm: 0.00998872\n",
      "Epoch 4 | Step 2758300 | Avg Loss: 0.0160 | Grad Norm: 0.00926600\n",
      "Epoch 4 | Step 2758400 | Avg Loss: 0.0159 | Grad Norm: 0.00941242\n",
      "Epoch 4 | Step 2758500 | Avg Loss: 0.0161 | Grad Norm: 0.01092866\n",
      "Epoch 4 | Step 2758600 | Avg Loss: 0.0157 | Grad Norm: 0.00885705\n",
      "Epoch 4 | Step 2758700 | Avg Loss: 0.0158 | Grad Norm: 0.00941552\n",
      "Epoch 4 | Step 2758800 | Avg Loss: 0.0158 | Grad Norm: 0.00927865\n",
      "Epoch 4 | Step 2758900 | Avg Loss: 0.0152 | Grad Norm: 0.00876221\n",
      "Epoch 4 | Step 2759000 | Avg Loss: 0.0150 | Grad Norm: 0.00884784\n",
      "Epoch 4 | Step 2759100 | Avg Loss: 0.0152 | Grad Norm: 0.00794680\n",
      "Epoch 4 | Step 2759200 | Avg Loss: 0.0156 | Grad Norm: 0.00925980\n",
      "Epoch 4 | Step 2759300 | Avg Loss: 0.0156 | Grad Norm: 0.00935802\n",
      "Epoch 4 | Step 2759400 | Avg Loss: 0.0154 | Grad Norm: 0.00982658\n",
      "Epoch 4 | Step 2759500 | Avg Loss: 0.0155 | Grad Norm: 0.01008531\n",
      "Epoch 4 | Step 2759600 | Avg Loss: 0.0158 | Grad Norm: 0.00903813\n",
      "Epoch 4 | Step 2759700 | Avg Loss: 0.0159 | Grad Norm: 0.00984484\n",
      "Epoch 4 | Step 2759800 | Avg Loss: 0.0159 | Grad Norm: 0.01019993\n",
      "Epoch 4 | Step 2759900 | Avg Loss: 0.0158 | Grad Norm: 0.00820972\n",
      "Epoch 4 | Step 2760000 | Avg Loss: 0.0156 | Grad Norm: 0.00921528\n",
      "Epoch 4 | Step 2760100 | Avg Loss: 0.0157 | Grad Norm: 0.00886111\n",
      "Epoch 4 | Step 2760200 | Avg Loss: 0.0154 | Grad Norm: 0.00910144\n",
      "Epoch 4 | Step 2760300 | Avg Loss: 0.0152 | Grad Norm: 0.00928301\n",
      "Epoch 4 | Step 2760400 | Avg Loss: 0.0153 | Grad Norm: 0.00891441\n",
      "Epoch 4 | Step 2760500 | Avg Loss: 0.0156 | Grad Norm: 0.01047232\n",
      "Epoch 4 | Step 2760600 | Avg Loss: 0.0158 | Grad Norm: 0.01023317\n",
      "Epoch 4 | Step 2760700 | Avg Loss: 0.0157 | Grad Norm: 0.00877380\n",
      "Epoch 4 | Step 2760800 | Avg Loss: 0.0160 | Grad Norm: 0.01167972\n",
      "Epoch 4 | Step 2760900 | Avg Loss: 0.0156 | Grad Norm: 0.00934170\n",
      "Epoch 4 | Step 2761000 | Avg Loss: 0.0158 | Grad Norm: 0.00970126\n",
      "Epoch 4 | Step 2761100 | Avg Loss: 0.0162 | Grad Norm: 0.00861420\n",
      "Epoch 4 | Step 2761200 | Avg Loss: 0.0162 | Grad Norm: 0.01202197\n",
      "Epoch 4 | Step 2761300 | Avg Loss: 0.0163 | Grad Norm: 0.00866607\n",
      "Epoch 4 | Step 2761400 | Avg Loss: 0.0159 | Grad Norm: 0.01033688\n",
      "Epoch 4 | Step 2761500 | Avg Loss: 0.0160 | Grad Norm: 0.01000572\n",
      "Epoch 4 | Step 2761600 | Avg Loss: 0.0157 | Grad Norm: 0.00850180\n",
      "Epoch 4 | Step 2761700 | Avg Loss: 0.0157 | Grad Norm: 0.00857109\n",
      "Epoch 4 | Step 2761800 | Avg Loss: 0.0157 | Grad Norm: 0.01071739\n",
      "Epoch 4 | Step 2761900 | Avg Loss: 0.0156 | Grad Norm: 0.00924807\n",
      "Epoch 4 | Step 2762000 | Avg Loss: 0.0159 | Grad Norm: 0.00915425\n",
      "Epoch 4 | Step 2762100 | Avg Loss: 0.0158 | Grad Norm: 0.00878872\n",
      "Epoch 4 | Step 2762200 | Avg Loss: 0.0156 | Grad Norm: 0.00986087\n",
      "Epoch 4 | Step 2762300 | Avg Loss: 0.0155 | Grad Norm: 0.00909826\n",
      "Epoch 4 | Step 2762400 | Avg Loss: 0.0160 | Grad Norm: 0.00791731\n",
      "Epoch 4 | Step 2762500 | Avg Loss: 0.0160 | Grad Norm: 0.00841542\n",
      "Epoch 4 | Step 2762600 | Avg Loss: 0.0161 | Grad Norm: 0.01230354\n",
      "Epoch 4 | Step 2762700 | Avg Loss: 0.0161 | Grad Norm: 0.00972248\n",
      "Epoch 4 | Step 2762800 | Avg Loss: 0.0162 | Grad Norm: 0.00899389\n",
      "Epoch 4 | Step 2762900 | Avg Loss: 0.0159 | Grad Norm: 0.01115503\n",
      "Epoch 4 | Step 2763000 | Avg Loss: 0.0160 | Grad Norm: 0.00862304\n",
      "Epoch 4 | Step 2763100 | Avg Loss: 0.0159 | Grad Norm: 0.00932688\n",
      "Epoch 4 | Step 2763200 | Avg Loss: 0.0161 | Grad Norm: 0.00922897\n",
      "Epoch 4 | Step 2763300 | Avg Loss: 0.0158 | Grad Norm: 0.00929887\n",
      "Epoch 4 | Step 2763400 | Avg Loss: 0.0156 | Grad Norm: 0.00933055\n",
      "Epoch 4 | Step 2763500 | Avg Loss: 0.0155 | Grad Norm: 0.00904630\n",
      "Epoch 4 | Step 2763600 | Avg Loss: 0.0151 | Grad Norm: 0.00863070\n",
      "Epoch 4 | Step 2763700 | Avg Loss: 0.0146 | Grad Norm: 0.00823408\n",
      "Epoch 4 | Step 2763800 | Avg Loss: 0.0149 | Grad Norm: 0.01484150\n",
      "Epoch 4 | Step 2763900 | Avg Loss: 0.0150 | Grad Norm: 0.00928210\n",
      "Epoch 4 | Step 2764000 | Avg Loss: 0.0152 | Grad Norm: 0.00943486\n",
      "Epoch 4 | Step 2764100 | Avg Loss: 0.0155 | Grad Norm: 0.00874601\n",
      "Epoch 4 | Step 2764200 | Avg Loss: 0.0155 | Grad Norm: 0.01009194\n",
      "Epoch 4 | Step 2764300 | Avg Loss: 0.0154 | Grad Norm: 0.01170672\n",
      "Epoch 4 | Step 2764400 | Avg Loss: 0.0155 | Grad Norm: 0.00868953\n",
      "Epoch 4 | Step 2764500 | Avg Loss: 0.0158 | Grad Norm: 0.01149784\n",
      "Epoch 4 | Step 2764600 | Avg Loss: 0.0161 | Grad Norm: 0.01028777\n",
      "Epoch 4 | Step 2764700 | Avg Loss: 0.0158 | Grad Norm: 0.00981803\n",
      "Epoch 4 | Step 2764800 | Avg Loss: 0.0157 | Grad Norm: 0.00859887\n",
      "Epoch 4 | Step 2764900 | Avg Loss: 0.0157 | Grad Norm: 0.00932766\n",
      "Epoch 4 | Step 2765000 | Avg Loss: 0.0155 | Grad Norm: 0.00837827\n",
      "Epoch 4 | Step 2765100 | Avg Loss: 0.0155 | Grad Norm: 0.00909708\n",
      "Epoch 4 | Step 2765200 | Avg Loss: 0.0151 | Grad Norm: 0.00822097\n",
      "Epoch 4 | Step 2765300 | Avg Loss: 0.0153 | Grad Norm: 0.00967672\n",
      "Epoch 4 | Step 2765400 | Avg Loss: 0.0153 | Grad Norm: 0.00960037\n",
      "Epoch 4 | Step 2765500 | Avg Loss: 0.0154 | Grad Norm: 0.00822650\n",
      "Epoch 4 | Step 2765600 | Avg Loss: 0.0159 | Grad Norm: 0.01027979\n",
      "Epoch 4 | Step 2765700 | Avg Loss: 0.0158 | Grad Norm: 0.00936879\n",
      "Epoch 4 | Step 2765800 | Avg Loss: 0.0154 | Grad Norm: 0.00908934\n",
      "Epoch 4 | Step 2765900 | Avg Loss: 0.0154 | Grad Norm: 0.00870996\n",
      "Epoch 4 | Step 2766000 | Avg Loss: 0.0154 | Grad Norm: 0.01012645\n",
      "Epoch 4 | Step 2766100 | Avg Loss: 0.0153 | Grad Norm: 0.00820179\n",
      "Epoch 4 | Step 2766200 | Avg Loss: 0.0155 | Grad Norm: 0.00831244\n",
      "Epoch 4 | Step 2766300 | Avg Loss: 0.0155 | Grad Norm: 0.01059757\n",
      "Epoch 4 | Step 2766400 | Avg Loss: 0.0154 | Grad Norm: 0.00878194\n",
      "Epoch 4 | Step 2766500 | Avg Loss: 0.0156 | Grad Norm: 0.00952090\n",
      "Epoch 4 | Step 2766600 | Avg Loss: 0.0155 | Grad Norm: 0.00928295\n",
      "Epoch 4 | Step 2766700 | Avg Loss: 0.0158 | Grad Norm: 0.01129725\n",
      "Epoch 4 | Step 2766800 | Avg Loss: 0.0157 | Grad Norm: 0.00958516\n",
      "Epoch 4 | Step 2766900 | Avg Loss: 0.0155 | Grad Norm: 0.00996280\n",
      "Epoch 4 | Step 2767000 | Avg Loss: 0.0158 | Grad Norm: 0.00852801\n",
      "Epoch 4 | Step 2767100 | Avg Loss: 0.0157 | Grad Norm: 0.01085228\n",
      "Epoch 4 | Step 2767200 | Avg Loss: 0.0154 | Grad Norm: 0.00932599\n",
      "Epoch 4 | Step 2767300 | Avg Loss: 0.0156 | Grad Norm: 0.00979183\n",
      "Epoch 4 | Step 2767400 | Avg Loss: 0.0155 | Grad Norm: 0.00989500\n",
      "Epoch 4 | Step 2767500 | Avg Loss: 0.0156 | Grad Norm: 0.00969022\n",
      "Epoch 4 | Step 2767600 | Avg Loss: 0.0152 | Grad Norm: 0.00916763\n",
      "Epoch 4 | Step 2767700 | Avg Loss: 0.0150 | Grad Norm: 0.00819933\n",
      "Epoch 4 | Step 2767800 | Avg Loss: 0.0152 | Grad Norm: 0.00911607\n",
      "Epoch 4 | Step 2767900 | Avg Loss: 0.0152 | Grad Norm: 0.00923942\n",
      "Epoch 4 | Step 2768000 | Avg Loss: 0.0154 | Grad Norm: 0.00864415\n",
      "Epoch 4 | Step 2768100 | Avg Loss: 0.0156 | Grad Norm: 0.00870211\n",
      "Epoch 4 | Step 2768200 | Avg Loss: 0.0154 | Grad Norm: 0.00759527\n",
      "Epoch 4 | Step 2768300 | Avg Loss: 0.0154 | Grad Norm: 0.00852564\n",
      "Epoch 4 | Step 2768400 | Avg Loss: 0.0153 | Grad Norm: 0.00964522\n",
      "Epoch 4 | Step 2768500 | Avg Loss: 0.0157 | Grad Norm: 0.00789027\n",
      "Epoch 4 | Step 2768600 | Avg Loss: 0.0156 | Grad Norm: 0.00720393\n",
      "Epoch 4 | Step 2768700 | Avg Loss: 0.0156 | Grad Norm: 0.01055948\n",
      "Epoch 4 | Step 2768800 | Avg Loss: 0.0160 | Grad Norm: 0.00847626\n",
      "Epoch 4 | Step 2768900 | Avg Loss: 0.0161 | Grad Norm: 0.01063508\n",
      "Epoch 4 | Step 2769000 | Avg Loss: 0.0158 | Grad Norm: 0.01350381\n",
      "Epoch 4 | Step 2769100 | Avg Loss: 0.0157 | Grad Norm: 0.00883313\n",
      "Epoch 4 | Step 2769200 | Avg Loss: 0.0157 | Grad Norm: 0.01006533\n",
      "Epoch 4 | Step 2769300 | Avg Loss: 0.0156 | Grad Norm: 0.00902417\n",
      "Epoch 4 | Step 2769400 | Avg Loss: 0.0155 | Grad Norm: 0.00869947\n",
      "Epoch 4 | Step 2769500 | Avg Loss: 0.0155 | Grad Norm: 0.00983758\n",
      "Epoch 4 | Step 2769600 | Avg Loss: 0.0154 | Grad Norm: 0.00860506\n",
      "Epoch 4 | Step 2769700 | Avg Loss: 0.0154 | Grad Norm: 0.00991256\n",
      "Epoch 4 | Step 2769800 | Avg Loss: 0.0153 | Grad Norm: 0.00897189\n",
      "Epoch 4 | Step 2769900 | Avg Loss: 0.0150 | Grad Norm: 0.00929591\n",
      "Epoch 4 | Step 2770000 | Avg Loss: 0.0152 | Grad Norm: 0.00850193\n",
      "Epoch 4 | Step 2770100 | Avg Loss: 0.0154 | Grad Norm: 0.00893649\n",
      "Epoch 4 | Step 2770200 | Avg Loss: 0.0151 | Grad Norm: 0.00901108\n",
      "Epoch 4 | Step 2770300 | Avg Loss: 0.0152 | Grad Norm: 0.00751497\n",
      "Epoch 4 | Step 2770400 | Avg Loss: 0.0149 | Grad Norm: 0.01058771\n",
      "Epoch 4 | Step 2770500 | Avg Loss: 0.0152 | Grad Norm: 0.00877959\n",
      "Epoch 4 | Step 2770600 | Avg Loss: 0.0155 | Grad Norm: 0.00847293\n",
      "Epoch 4 | Step 2770700 | Avg Loss: 0.0155 | Grad Norm: 0.00902085\n",
      "Epoch 4 | Step 2770800 | Avg Loss: 0.0154 | Grad Norm: 0.00817812\n",
      "Epoch 4 | Step 2770900 | Avg Loss: 0.0153 | Grad Norm: 0.00972266\n",
      "Epoch 4 | Step 2771000 | Avg Loss: 0.0154 | Grad Norm: 0.00918420\n",
      "Epoch 4 | Step 2771100 | Avg Loss: 0.0157 | Grad Norm: 0.00963250\n",
      "Epoch 4 | Step 2771200 | Avg Loss: 0.0158 | Grad Norm: 0.01072250\n",
      "Epoch 4 | Step 2771300 | Avg Loss: 0.0157 | Grad Norm: 0.01066216\n",
      "Epoch 4 | Step 2771400 | Avg Loss: 0.0161 | Grad Norm: 0.00870589\n",
      "Epoch 4 | Step 2771500 | Avg Loss: 0.0156 | Grad Norm: 0.00894317\n",
      "Epoch 4 | Step 2771600 | Avg Loss: 0.0157 | Grad Norm: 0.00905749\n",
      "Epoch 4 | Step 2771700 | Avg Loss: 0.0158 | Grad Norm: 0.00929633\n",
      "Epoch 4 | Step 2771800 | Avg Loss: 0.0154 | Grad Norm: 0.00867186\n",
      "Epoch 4 | Step 2771900 | Avg Loss: 0.0155 | Grad Norm: 0.01101379\n",
      "Epoch 4 | Step 2772000 | Avg Loss: 0.0156 | Grad Norm: 0.00933129\n",
      "Epoch 4 | Step 2772100 | Avg Loss: 0.0158 | Grad Norm: 0.00875269\n",
      "Epoch 4 | Step 2772200 | Avg Loss: 0.0157 | Grad Norm: 0.00993621\n",
      "Epoch 4 | Step 2772300 | Avg Loss: 0.0153 | Grad Norm: 0.00874266\n",
      "Epoch 4 | Step 2772400 | Avg Loss: 0.0152 | Grad Norm: 0.00935863\n",
      "Epoch 4 | Step 2772500 | Avg Loss: 0.0157 | Grad Norm: 0.00917907\n",
      "Epoch 4 | Step 2772600 | Avg Loss: 0.0153 | Grad Norm: 0.00956920\n",
      "Epoch 4 | Step 2772700 | Avg Loss: 0.0158 | Grad Norm: 0.00872795\n",
      "Epoch 4 | Step 2772800 | Avg Loss: 0.0157 | Grad Norm: 0.00932047\n",
      "Epoch 4 | Step 2772900 | Avg Loss: 0.0157 | Grad Norm: 0.01063666\n",
      "Epoch 4 | Step 2773000 | Avg Loss: 0.0157 | Grad Norm: 0.01005038\n",
      "Epoch 4 | Step 2773100 | Avg Loss: 0.0155 | Grad Norm: 0.00980176\n",
      "Epoch 4 | Step 2773200 | Avg Loss: 0.0157 | Grad Norm: 0.00917893\n",
      "Epoch 4 | Step 2773300 | Avg Loss: 0.0158 | Grad Norm: 0.00924910\n",
      "Epoch 4 | Step 2773400 | Avg Loss: 0.0159 | Grad Norm: 0.01047180\n",
      "Epoch 4 | Step 2773500 | Avg Loss: 0.0157 | Grad Norm: 0.00943592\n",
      "Epoch 4 | Step 2773600 | Avg Loss: 0.0159 | Grad Norm: 0.00888502\n",
      "Epoch 4 | Step 2773700 | Avg Loss: 0.0160 | Grad Norm: 0.00866530\n",
      "Epoch 4 | Step 2773800 | Avg Loss: 0.0161 | Grad Norm: 0.00901033\n",
      "Epoch 4 | Step 2773900 | Avg Loss: 0.0158 | Grad Norm: 0.00939284\n",
      "Epoch 4 | Step 2774000 | Avg Loss: 0.0154 | Grad Norm: 0.00932511\n",
      "Epoch 4 | Step 2774100 | Avg Loss: 0.0155 | Grad Norm: 0.00874647\n",
      "Epoch 4 | Step 2774200 | Avg Loss: 0.0154 | Grad Norm: 0.00825835\n",
      "Epoch 4 | Step 2774300 | Avg Loss: 0.0153 | Grad Norm: 0.00872773\n",
      "Epoch 4 | Step 2774400 | Avg Loss: 0.0152 | Grad Norm: 0.01088386\n",
      "Epoch 4 | Step 2774500 | Avg Loss: 0.0154 | Grad Norm: 0.01003849\n",
      "Epoch 4 | Step 2774600 | Avg Loss: 0.0153 | Grad Norm: 0.00905418\n",
      "Epoch 4 | Step 2774700 | Avg Loss: 0.0152 | Grad Norm: 0.00910791\n",
      "Epoch 4 | Step 2774800 | Avg Loss: 0.0155 | Grad Norm: 0.00988489\n",
      "Epoch 4 | Step 2774900 | Avg Loss: 0.0155 | Grad Norm: 0.00943141\n",
      "Epoch 4 | Step 2775000 | Avg Loss: 0.0156 | Grad Norm: 0.00942253\n",
      "Epoch 4 | Step 2775100 | Avg Loss: 0.0155 | Grad Norm: 0.01015569\n",
      "Epoch 4 | Step 2775200 | Avg Loss: 0.0152 | Grad Norm: 0.00930017\n",
      "Epoch 4 | Step 2775300 | Avg Loss: 0.0155 | Grad Norm: 0.00822087\n",
      "Epoch 4 | Step 2775400 | Avg Loss: 0.0155 | Grad Norm: 0.00869906\n",
      "Epoch 4 | Step 2775500 | Avg Loss: 0.0155 | Grad Norm: 0.00914892\n",
      "Epoch 4 | Step 2775600 | Avg Loss: 0.0157 | Grad Norm: 0.01008929\n",
      "Epoch 4 | Step 2775700 | Avg Loss: 0.0158 | Grad Norm: 0.00981270\n",
      "Epoch 4 | Step 2775800 | Avg Loss: 0.0158 | Grad Norm: 0.00840250\n",
      "Epoch 4 | Step 2775900 | Avg Loss: 0.0155 | Grad Norm: 0.00986954\n",
      "Epoch 4 | Step 2776000 | Avg Loss: 0.0156 | Grad Norm: 0.00981326\n",
      "Epoch 4 | Step 2776100 | Avg Loss: 0.0162 | Grad Norm: 0.00899262\n",
      "Epoch 4 | Step 2776200 | Avg Loss: 0.0157 | Grad Norm: 0.00900452\n",
      "Epoch 4 | Step 2776300 | Avg Loss: 0.0160 | Grad Norm: 0.00908860\n",
      "Epoch 4 | Step 2776400 | Avg Loss: 0.0161 | Grad Norm: 0.00805417\n",
      "Epoch 4 | Step 2776500 | Avg Loss: 0.0161 | Grad Norm: 0.01004983\n",
      "Epoch 4 | Step 2776600 | Avg Loss: 0.0161 | Grad Norm: 0.01062610\n",
      "Epoch 4 | Step 2776700 | Avg Loss: 0.0161 | Grad Norm: 0.00887938\n",
      "Epoch 4 | Step 2776800 | Avg Loss: 0.0157 | Grad Norm: 0.01048939\n",
      "Epoch 4 | Step 2776900 | Avg Loss: 0.0154 | Grad Norm: 0.00918484\n",
      "Epoch 4 | Step 2777000 | Avg Loss: 0.0158 | Grad Norm: 0.00935974\n",
      "Epoch 4 | Step 2777100 | Avg Loss: 0.0157 | Grad Norm: 0.01042159\n",
      "Epoch 4 | Step 2777200 | Avg Loss: 0.0158 | Grad Norm: 0.00903918\n",
      "Epoch 4 | Step 2777300 | Avg Loss: 0.0154 | Grad Norm: 0.01053406\n",
      "Epoch 4 | Step 2777400 | Avg Loss: 0.0155 | Grad Norm: 0.00945705\n",
      "Epoch 4 | Step 2777500 | Avg Loss: 0.0155 | Grad Norm: 0.00797199\n",
      "Epoch 4 | Step 2777600 | Avg Loss: 0.0149 | Grad Norm: 0.00859283\n",
      "Epoch 4 | Step 2777700 | Avg Loss: 0.0155 | Grad Norm: 0.01120870\n",
      "Epoch 4 | Step 2777800 | Avg Loss: 0.0157 | Grad Norm: 0.00793771\n",
      "Epoch 4 | Step 2777900 | Avg Loss: 0.0155 | Grad Norm: 0.01231558\n",
      "Epoch 4 | Step 2778000 | Avg Loss: 0.0156 | Grad Norm: 0.00882820\n",
      "Epoch 4 | Step 2778100 | Avg Loss: 0.0157 | Grad Norm: 0.01058836\n",
      "Epoch 4 | Step 2778200 | Avg Loss: 0.0158 | Grad Norm: 0.01475471\n",
      "Epoch 4 | Step 2778300 | Avg Loss: 0.0156 | Grad Norm: 0.00906412\n",
      "Epoch 4 | Step 2778400 | Avg Loss: 0.0154 | Grad Norm: 0.00864998\n",
      "Epoch 4 | Step 2778500 | Avg Loss: 0.0152 | Grad Norm: 0.00833273\n",
      "Epoch 4 | Step 2778600 | Avg Loss: 0.0155 | Grad Norm: 0.00958058\n",
      "Epoch 4 | Step 2778700 | Avg Loss: 0.0155 | Grad Norm: 0.00912469\n",
      "Epoch 4 | Step 2778800 | Avg Loss: 0.0155 | Grad Norm: 0.00809704\n",
      "Epoch 4 | Step 2778900 | Avg Loss: 0.0151 | Grad Norm: 0.00945343\n",
      "Epoch 4 | Step 2779000 | Avg Loss: 0.0150 | Grad Norm: 0.00863845\n",
      "Epoch 4 | Step 2779100 | Avg Loss: 0.0152 | Grad Norm: 0.00984646\n",
      "Epoch 4 | Step 2779200 | Avg Loss: 0.0155 | Grad Norm: 0.00846514\n",
      "Epoch 4 | Step 2779300 | Avg Loss: 0.0152 | Grad Norm: 0.00878343\n",
      "Epoch 4 | Step 2779400 | Avg Loss: 0.0155 | Grad Norm: 0.01049238\n",
      "Epoch 4 | Step 2779500 | Avg Loss: 0.0154 | Grad Norm: 0.00913980\n",
      "Epoch 4 | Step 2779600 | Avg Loss: 0.0156 | Grad Norm: 0.00948344\n",
      "Epoch 4 | Step 2779700 | Avg Loss: 0.0155 | Grad Norm: 0.00990379\n",
      "Epoch 4 | Step 2779800 | Avg Loss: 0.0154 | Grad Norm: 0.00941968\n",
      "Epoch 4 | Step 2779900 | Avg Loss: 0.0153 | Grad Norm: 0.00969403\n",
      "Epoch 4 | Step 2780000 | Avg Loss: 0.0155 | Grad Norm: 0.00922381\n",
      "Epoch 4 | Step 2780100 | Avg Loss: 0.0159 | Grad Norm: 0.00765059\n",
      "Epoch 4 | Step 2780200 | Avg Loss: 0.0157 | Grad Norm: 0.01100079\n",
      "Epoch 4 | Step 2780300 | Avg Loss: 0.0157 | Grad Norm: 0.00874460\n",
      "Epoch 4 | Step 2780400 | Avg Loss: 0.0157 | Grad Norm: 0.00933048\n",
      "Epoch 4 | Step 2780500 | Avg Loss: 0.0157 | Grad Norm: 0.00897742\n",
      "Epoch 4 | Step 2780600 | Avg Loss: 0.0159 | Grad Norm: 0.00844735\n",
      "Epoch 4 | Step 2780700 | Avg Loss: 0.0153 | Grad Norm: 0.00902545\n",
      "Epoch 4 | Step 2780800 | Avg Loss: 0.0152 | Grad Norm: 0.00887029\n",
      "Epoch 4 | Step 2780900 | Avg Loss: 0.0151 | Grad Norm: 0.01028336\n",
      "Epoch 4 | Step 2781000 | Avg Loss: 0.0155 | Grad Norm: 0.01009758\n",
      "Epoch 4 | Step 2781100 | Avg Loss: 0.0160 | Grad Norm: 0.01027488\n",
      "Epoch 4 | Step 2781200 | Avg Loss: 0.0162 | Grad Norm: 0.00982108\n",
      "Epoch 4 | Step 2781300 | Avg Loss: 0.0159 | Grad Norm: 0.00876872\n",
      "Epoch 4 | Step 2781400 | Avg Loss: 0.0161 | Grad Norm: 0.01063829\n",
      "Epoch 4 | Step 2781500 | Avg Loss: 0.0158 | Grad Norm: 0.01013777\n",
      "Epoch 4 | Step 2781600 | Avg Loss: 0.0157 | Grad Norm: 0.00881479\n",
      "Epoch 4 | Step 2781700 | Avg Loss: 0.0153 | Grad Norm: 0.00892228\n",
      "Epoch 4 | Step 2781800 | Avg Loss: 0.0152 | Grad Norm: 0.01012912\n",
      "Epoch 4 | Step 2781900 | Avg Loss: 0.0153 | Grad Norm: 0.00852004\n",
      "Epoch 4 | Step 2782000 | Avg Loss: 0.0157 | Grad Norm: 0.00871226\n",
      "Epoch 4 | Step 2782100 | Avg Loss: 0.0157 | Grad Norm: 0.00833354\n",
      "Epoch 4 | Step 2782200 | Avg Loss: 0.0155 | Grad Norm: 0.00914751\n",
      "Epoch 4 | Step 2782300 | Avg Loss: 0.0151 | Grad Norm: 0.00955068\n",
      "Epoch 4 | Step 2782400 | Avg Loss: 0.0150 | Grad Norm: 0.00859827\n",
      "Epoch 4 | Step 2782500 | Avg Loss: 0.0150 | Grad Norm: 0.00908299\n",
      "Epoch 4 | Step 2782600 | Avg Loss: 0.0147 | Grad Norm: 0.00903928\n",
      "Epoch 4 | Step 2782700 | Avg Loss: 0.0144 | Grad Norm: 0.00842224\n",
      "Epoch 4 | Step 2782800 | Avg Loss: 0.0149 | Grad Norm: 0.00935291\n",
      "Epoch 4 | Step 2782900 | Avg Loss: 0.0151 | Grad Norm: 0.00812562\n",
      "Epoch 4 | Step 2783000 | Avg Loss: 0.0154 | Grad Norm: 0.00827027\n",
      "Epoch 4 | Step 2783100 | Avg Loss: 0.0153 | Grad Norm: 0.00896278\n",
      "Epoch 4 | Step 2783200 | Avg Loss: 0.0156 | Grad Norm: 0.00785326\n",
      "Epoch 4 | Step 2783300 | Avg Loss: 0.0157 | Grad Norm: 0.00865143\n",
      "Epoch 4 | Step 2783400 | Avg Loss: 0.0157 | Grad Norm: 0.01004026\n",
      "Epoch 4 | Step 2783500 | Avg Loss: 0.0160 | Grad Norm: 0.00954259\n",
      "Epoch 4 | Step 2783600 | Avg Loss: 0.0162 | Grad Norm: 0.00930162\n",
      "Epoch 4 | Step 2783700 | Avg Loss: 0.0162 | Grad Norm: 0.01062436\n",
      "Epoch 4 | Step 2783800 | Avg Loss: 0.0165 | Grad Norm: 0.00918031\n",
      "Epoch 4 | Step 2783900 | Avg Loss: 0.0159 | Grad Norm: 0.00886629\n",
      "Epoch 4 | Step 2784000 | Avg Loss: 0.0155 | Grad Norm: 0.00900823\n",
      "Epoch 4 | Step 2784100 | Avg Loss: 0.0149 | Grad Norm: 0.00833626\n",
      "Epoch 4 | Step 2784200 | Avg Loss: 0.0149 | Grad Norm: 0.00984489\n",
      "Epoch 4 | Step 2784300 | Avg Loss: 0.0148 | Grad Norm: 0.01245190\n",
      "Epoch 4 | Step 2784400 | Avg Loss: 0.0150 | Grad Norm: 0.01166345\n",
      "Epoch 4 | Step 2784500 | Avg Loss: 0.0148 | Grad Norm: 0.01045218\n",
      "Epoch 4 | Step 2784600 | Avg Loss: 0.0144 | Grad Norm: 0.01217418\n",
      "Epoch 4 | Step 2784700 | Avg Loss: 0.0147 | Grad Norm: 0.01022102\n",
      "Epoch 4 | Step 2784800 | Avg Loss: 0.0152 | Grad Norm: 0.01067318\n",
      "Epoch 4 | Step 2784900 | Avg Loss: 0.0155 | Grad Norm: 0.01043609\n",
      "Epoch 4 | Step 2785000 | Avg Loss: 0.0156 | Grad Norm: 0.00906140\n",
      "Epoch 4 | Step 2785100 | Avg Loss: 0.0156 | Grad Norm: 0.00992760\n",
      "Epoch 4 | Step 2785200 | Avg Loss: 0.0160 | Grad Norm: 0.01188448\n",
      "Epoch 4 | Step 2785300 | Avg Loss: 0.0159 | Grad Norm: 0.00906747\n",
      "Epoch 4 | Step 2785400 | Avg Loss: 0.0157 | Grad Norm: 0.00996987\n",
      "Epoch 4 | Step 2785500 | Avg Loss: 0.0156 | Grad Norm: 0.01070221\n",
      "Epoch 4 | Step 2785600 | Avg Loss: 0.0157 | Grad Norm: 0.00865931\n",
      "Epoch 4 | Step 2785700 | Avg Loss: 0.0154 | Grad Norm: 0.00861181\n",
      "Epoch 4 | Step 2785800 | Avg Loss: 0.0153 | Grad Norm: 0.00941250\n",
      "Epoch 4 | Step 2785900 | Avg Loss: 0.0150 | Grad Norm: 0.00851093\n",
      "Epoch 4 | Step 2786000 | Avg Loss: 0.0147 | Grad Norm: 0.00751645\n",
      "Epoch 4 | Step 2786100 | Avg Loss: 0.0153 | Grad Norm: 0.00892145\n",
      "Epoch 4 | Step 2786200 | Avg Loss: 0.0152 | Grad Norm: 0.00903607\n",
      "Epoch 4 | Step 2786300 | Avg Loss: 0.0154 | Grad Norm: 0.00910429\n",
      "Epoch 4 | Step 2786400 | Avg Loss: 0.0156 | Grad Norm: 0.01009717\n",
      "Epoch 4 | Step 2786500 | Avg Loss: 0.0155 | Grad Norm: 0.00909588\n",
      "Epoch 4 | Step 2786600 | Avg Loss: 0.0154 | Grad Norm: 0.00928355\n",
      "Epoch 4 | Step 2786700 | Avg Loss: 0.0151 | Grad Norm: 0.00858118\n",
      "Epoch 4 | Step 2786800 | Avg Loss: 0.0150 | Grad Norm: 0.00921246\n",
      "Epoch 4 | Step 2786900 | Avg Loss: 0.0152 | Grad Norm: 0.00865890\n",
      "Epoch 4 | Step 2787000 | Avg Loss: 0.0157 | Grad Norm: 0.00887025\n",
      "Epoch 4 | Step 2787100 | Avg Loss: 0.0161 | Grad Norm: 0.00942536\n",
      "Epoch 4 | Step 2787200 | Avg Loss: 0.0158 | Grad Norm: 0.00926289\n",
      "Epoch 4 | Step 2787300 | Avg Loss: 0.0157 | Grad Norm: 0.00973187\n",
      "Epoch 4 | Step 2787400 | Avg Loss: 0.0158 | Grad Norm: 0.00824829\n",
      "Epoch 4 | Step 2787500 | Avg Loss: 0.0158 | Grad Norm: 0.01060973\n",
      "Epoch 4 | Step 2787600 | Avg Loss: 0.0157 | Grad Norm: 0.00946434\n",
      "Epoch 4 | Step 2787700 | Avg Loss: 0.0161 | Grad Norm: 0.00805413\n",
      "Epoch 4 | Step 2787800 | Avg Loss: 0.0160 | Grad Norm: 0.00932523\n",
      "Epoch 4 | Step 2787900 | Avg Loss: 0.0159 | Grad Norm: 0.00860626\n",
      "Epoch 4 | Step 2788000 | Avg Loss: 0.0159 | Grad Norm: 0.01014758\n",
      "Epoch 4 | Step 2788100 | Avg Loss: 0.0162 | Grad Norm: 0.01005047\n",
      "Epoch 4 | Step 2788200 | Avg Loss: 0.0161 | Grad Norm: 0.00847430\n",
      "Epoch 4 | Step 2788300 | Avg Loss: 0.0158 | Grad Norm: 0.01264767\n",
      "Epoch 4 | Step 2788400 | Avg Loss: 0.0158 | Grad Norm: 0.01006076\n",
      "Epoch 4 | Step 2788500 | Avg Loss: 0.0158 | Grad Norm: 0.00984840\n",
      "Epoch 4 | Step 2788600 | Avg Loss: 0.0161 | Grad Norm: 0.00862981\n",
      "Epoch 4 | Step 2788700 | Avg Loss: 0.0161 | Grad Norm: 0.00884265\n",
      "Epoch 4 | Step 2788800 | Avg Loss: 0.0157 | Grad Norm: 0.00791727\n",
      "Epoch 4 | Step 2788900 | Avg Loss: 0.0157 | Grad Norm: 0.01131343\n",
      "Epoch 4 | Step 2789000 | Avg Loss: 0.0156 | Grad Norm: 0.00832385\n",
      "Epoch 4 | Step 2789100 | Avg Loss: 0.0156 | Grad Norm: 0.00997823\n",
      "Epoch 4 | Step 2789200 | Avg Loss: 0.0158 | Grad Norm: 0.00846074\n",
      "Epoch 4 | Step 2789300 | Avg Loss: 0.0159 | Grad Norm: 0.00800095\n",
      "Epoch 4 | Step 2789400 | Avg Loss: 0.0160 | Grad Norm: 0.00816297\n",
      "Epoch 4 | Step 2789500 | Avg Loss: 0.0158 | Grad Norm: 0.00897976\n",
      "Epoch 4 | Step 2789600 | Avg Loss: 0.0158 | Grad Norm: 0.00932854\n",
      "Epoch 4 | Step 2789700 | Avg Loss: 0.0157 | Grad Norm: 0.00795489\n",
      "Epoch 4 | Step 2789800 | Avg Loss: 0.0158 | Grad Norm: 0.01029941\n",
      "Epoch 4 | Step 2789900 | Avg Loss: 0.0152 | Grad Norm: 0.00894408\n",
      "Epoch 4 | Step 2790000 | Avg Loss: 0.0153 | Grad Norm: 0.01004096\n",
      "Epoch 4 | Step 2790100 | Avg Loss: 0.0155 | Grad Norm: 0.00784014\n",
      "Epoch 4 | Step 2790200 | Avg Loss: 0.0155 | Grad Norm: 0.00906382\n",
      "Epoch 4 | Step 2790300 | Avg Loss: 0.0154 | Grad Norm: 0.00840320\n",
      "Epoch 4 | Step 2790400 | Avg Loss: 0.0156 | Grad Norm: 0.00988117\n",
      "Epoch 4 | Step 2790500 | Avg Loss: 0.0159 | Grad Norm: 0.00916917\n",
      "Epoch 4 | Step 2790600 | Avg Loss: 0.0155 | Grad Norm: 0.00933594\n",
      "Epoch 4 | Step 2790700 | Avg Loss: 0.0159 | Grad Norm: 0.00899683\n",
      "Epoch 4 | Step 2790800 | Avg Loss: 0.0163 | Grad Norm: 0.00998656\n",
      "Epoch 4 | Step 2790900 | Avg Loss: 0.0162 | Grad Norm: 0.01018709\n",
      "Epoch 4 | Step 2791000 | Avg Loss: 0.0158 | Grad Norm: 0.00904014\n",
      "Epoch 4 | Step 2791100 | Avg Loss: 0.0161 | Grad Norm: 0.00908656\n",
      "Epoch 4 | Step 2791200 | Avg Loss: 0.0157 | Grad Norm: 0.00959386\n",
      "Epoch 4 | Step 2791300 | Avg Loss: 0.0154 | Grad Norm: 0.00905041\n",
      "Epoch 4 | Step 2791400 | Avg Loss: 0.0154 | Grad Norm: 0.00851085\n",
      "Epoch 4 | Step 2791500 | Avg Loss: 0.0159 | Grad Norm: 0.00897424\n",
      "Epoch 4 | Step 2791600 | Avg Loss: 0.0159 | Grad Norm: 0.00960282\n",
      "Epoch 4 | Step 2791700 | Avg Loss: 0.0161 | Grad Norm: 0.00734683\n",
      "Epoch 4 | Step 2791800 | Avg Loss: 0.0162 | Grad Norm: 0.00877456\n",
      "Epoch 4 | Step 2791900 | Avg Loss: 0.0165 | Grad Norm: 0.00962480\n",
      "Epoch 4 | Step 2792000 | Avg Loss: 0.0162 | Grad Norm: 0.00896574\n",
      "Epoch 4 | Step 2792100 | Avg Loss: 0.0157 | Grad Norm: 0.01112267\n",
      "Epoch 4 | Step 2792200 | Avg Loss: 0.0156 | Grad Norm: 0.00967290\n",
      "Epoch 4 | Step 2792300 | Avg Loss: 0.0156 | Grad Norm: 0.00862302\n",
      "Epoch 4 | Step 2792400 | Avg Loss: 0.0156 | Grad Norm: 0.00882606\n",
      "Epoch 4 | Step 2792500 | Avg Loss: 0.0155 | Grad Norm: 0.00995294\n",
      "Epoch 4 | Step 2792600 | Avg Loss: 0.0159 | Grad Norm: 0.00866736\n",
      "Epoch 4 | Step 2792700 | Avg Loss: 0.0157 | Grad Norm: 0.00933824\n",
      "Epoch 4 | Step 2792800 | Avg Loss: 0.0154 | Grad Norm: 0.00987535\n",
      "Epoch 4 | Step 2792900 | Avg Loss: 0.0156 | Grad Norm: 0.00953432\n",
      "Epoch 4 | Step 2793000 | Avg Loss: 0.0159 | Grad Norm: 0.00933111\n",
      "Epoch 4 | Step 2793100 | Avg Loss: 0.0157 | Grad Norm: 0.01150948\n",
      "Epoch 4 | Step 2793200 | Avg Loss: 0.0155 | Grad Norm: 0.00947964\n",
      "Epoch 4 | Step 2793300 | Avg Loss: 0.0156 | Grad Norm: 0.01179936\n",
      "Epoch 4 | Step 2793400 | Avg Loss: 0.0156 | Grad Norm: 0.00915354\n",
      "Epoch 4 | Step 2793500 | Avg Loss: 0.0157 | Grad Norm: 0.00990850\n",
      "Epoch 4 | Step 2793600 | Avg Loss: 0.0159 | Grad Norm: 0.00810510\n",
      "Epoch 4 | Step 2793700 | Avg Loss: 0.0155 | Grad Norm: 0.00945901\n",
      "Epoch 4 | Step 2793800 | Avg Loss: 0.0154 | Grad Norm: 0.00892545\n",
      "Epoch 4 | Step 2793900 | Avg Loss: 0.0152 | Grad Norm: 0.00960048\n",
      "Epoch 4 | Step 2794000 | Avg Loss: 0.0151 | Grad Norm: 0.00884951\n",
      "Epoch 4 | Step 2794100 | Avg Loss: 0.0150 | Grad Norm: 0.00970299\n",
      "Epoch 4 | Step 2794200 | Avg Loss: 0.0154 | Grad Norm: 0.00876834\n",
      "Epoch 4 | Step 2794300 | Avg Loss: 0.0148 | Grad Norm: 0.00903968\n",
      "Epoch 4 | Step 2794400 | Avg Loss: 0.0153 | Grad Norm: 0.00952250\n",
      "Epoch 4 | Step 2794500 | Avg Loss: 0.0151 | Grad Norm: 0.00934350\n",
      "Epoch 4 | Step 2794600 | Avg Loss: 0.0151 | Grad Norm: 0.01024542\n",
      "Epoch 4 | Step 2794700 | Avg Loss: 0.0154 | Grad Norm: 0.01023582\n",
      "Epoch 4 | Step 2794800 | Avg Loss: 0.0156 | Grad Norm: 0.00909893\n",
      "Epoch 4 | Step 2794900 | Avg Loss: 0.0159 | Grad Norm: 0.00962170\n",
      "Epoch 4 | Step 2795000 | Avg Loss: 0.0158 | Grad Norm: 0.00906292\n",
      "Epoch 4 | Step 2795100 | Avg Loss: 0.0158 | Grad Norm: 0.00923847\n",
      "Epoch 4 | Step 2795200 | Avg Loss: 0.0157 | Grad Norm: 0.00859391\n",
      "Epoch 4 | Step 2795300 | Avg Loss: 0.0155 | Grad Norm: 0.01004261\n",
      "Epoch 4 | Step 2795400 | Avg Loss: 0.0153 | Grad Norm: 0.00774901\n",
      "Epoch 4 | Step 2795500 | Avg Loss: 0.0160 | Grad Norm: 0.00914920\n",
      "Epoch 4 | Step 2795600 | Avg Loss: 0.0160 | Grad Norm: 0.01044153\n",
      "Epoch 4 | Step 2795700 | Avg Loss: 0.0160 | Grad Norm: 0.00945610\n",
      "Epoch 4 | Step 2795800 | Avg Loss: 0.0158 | Grad Norm: 0.00887199\n",
      "Epoch 4 | Step 2795900 | Avg Loss: 0.0165 | Grad Norm: 0.00918806\n",
      "Epoch 4 | Step 2796000 | Avg Loss: 0.0165 | Grad Norm: 0.00808955\n",
      "Epoch 4 | Step 2796100 | Avg Loss: 0.0162 | Grad Norm: 0.00876048\n",
      "Epoch 4 | Step 2796200 | Avg Loss: 0.0159 | Grad Norm: 0.01068906\n",
      "Epoch 4 | Step 2796300 | Avg Loss: 0.0160 | Grad Norm: 0.00930159\n",
      "Epoch 4 | Step 2796400 | Avg Loss: 0.0159 | Grad Norm: 0.00778306\n",
      "Epoch 4 | Step 2796500 | Avg Loss: 0.0157 | Grad Norm: 0.01024660\n",
      "Epoch 4 | Step 2796600 | Avg Loss: 0.0157 | Grad Norm: 0.00844832\n",
      "Epoch 4 | Step 2796700 | Avg Loss: 0.0156 | Grad Norm: 0.00870484\n",
      "Epoch 4 | Step 2796800 | Avg Loss: 0.0155 | Grad Norm: 0.00857211\n",
      "Epoch 4 | Step 2796900 | Avg Loss: 0.0156 | Grad Norm: 0.00906391\n",
      "Epoch 4 | Step 2797000 | Avg Loss: 0.0159 | Grad Norm: 0.00841567\n",
      "Epoch 4 | Step 2797100 | Avg Loss: 0.0155 | Grad Norm: 0.00904057\n",
      "Epoch 4 | Step 2797200 | Avg Loss: 0.0155 | Grad Norm: 0.00908454\n",
      "Epoch 4 | Step 2797300 | Avg Loss: 0.0154 | Grad Norm: 0.01004344\n",
      "Epoch 4 | Step 2797400 | Avg Loss: 0.0155 | Grad Norm: 0.00903843\n",
      "Epoch 4 | Step 2797500 | Avg Loss: 0.0156 | Grad Norm: 0.00868661\n",
      "Epoch 4 | Step 2797600 | Avg Loss: 0.0154 | Grad Norm: 0.00892517\n",
      "Epoch 4 | Step 2797700 | Avg Loss: 0.0155 | Grad Norm: 0.01388305\n",
      "Epoch 4 | Step 2797800 | Avg Loss: 0.0158 | Grad Norm: 0.00946562\n",
      "Epoch 4 | Step 2797900 | Avg Loss: 0.0156 | Grad Norm: 0.01024317\n",
      "Epoch 4 | Step 2798000 | Avg Loss: 0.0159 | Grad Norm: 0.00921864\n",
      "Epoch 4 | Step 2798100 | Avg Loss: 0.0162 | Grad Norm: 0.00900059\n",
      "Epoch 4 | Step 2798200 | Avg Loss: 0.0161 | Grad Norm: 0.00952377\n",
      "Epoch 4 | Step 2798300 | Avg Loss: 0.0160 | Grad Norm: 0.01103655\n",
      "Epoch 4 | Step 2798400 | Avg Loss: 0.0161 | Grad Norm: 0.00909517\n",
      "Epoch 4 | Step 2798500 | Avg Loss: 0.0163 | Grad Norm: 0.00861108\n",
      "Epoch 4 | Step 2798600 | Avg Loss: 0.0161 | Grad Norm: 0.01003380\n",
      "Epoch 4 | Step 2798700 | Avg Loss: 0.0161 | Grad Norm: 0.00901976\n",
      "Epoch 4 | Step 2798800 | Avg Loss: 0.0165 | Grad Norm: 0.01073560\n",
      "Epoch 4 | Step 2798900 | Avg Loss: 0.0164 | Grad Norm: 0.00837991\n",
      "Epoch 4 | Step 2799000 | Avg Loss: 0.0158 | Grad Norm: 0.00874370\n",
      "Epoch 4 | Step 2799100 | Avg Loss: 0.0157 | Grad Norm: 0.00955065\n",
      "Epoch 4 | Step 2799200 | Avg Loss: 0.0161 | Grad Norm: 0.00767790\n",
      "Epoch 4 | Step 2799300 | Avg Loss: 0.0160 | Grad Norm: 0.00911995\n",
      "Epoch 4 | Step 2799400 | Avg Loss: 0.0156 | Grad Norm: 0.00794671\n",
      "Epoch 4 | Step 2799500 | Avg Loss: 0.0154 | Grad Norm: 0.00901087\n",
      "Epoch 4 | Step 2799600 | Avg Loss: 0.0150 | Grad Norm: 0.00817727\n",
      "Epoch 4 | Step 2799700 | Avg Loss: 0.0148 | Grad Norm: 0.00908722\n",
      "Epoch 4 | Step 2799800 | Avg Loss: 0.0148 | Grad Norm: 0.00883119\n",
      "Epoch 4 | Step 2799900 | Avg Loss: 0.0149 | Grad Norm: 0.01236588\n",
      "Epoch 4 | Step 2800000 | Avg Loss: 0.0151 | Grad Norm: 0.00955039\n",
      "Saving model at step2800000\n",
      "Epoch 4 | Step 2800100 | Avg Loss: 0.0150 | Grad Norm: 0.00862758\n",
      "Epoch 4 | Step 2800200 | Avg Loss: 0.0151 | Grad Norm: 0.00822076\n",
      "Epoch 4 | Step 2800300 | Avg Loss: 0.0151 | Grad Norm: 0.00820375\n",
      "Epoch 4 | Step 2800400 | Avg Loss: 0.0152 | Grad Norm: 0.00960799\n",
      "Epoch 4 | Step 2800500 | Avg Loss: 0.0152 | Grad Norm: 0.00918785\n",
      "Epoch 4 | Step 2800600 | Avg Loss: 0.0153 | Grad Norm: 0.00940604\n",
      "Epoch 4 | Step 2800700 | Avg Loss: 0.0152 | Grad Norm: 0.00767266\n",
      "Epoch 4 | Step 2800800 | Avg Loss: 0.0150 | Grad Norm: 0.00894842\n",
      "Epoch 4 | Step 2800900 | Avg Loss: 0.0151 | Grad Norm: 0.00860597\n",
      "Epoch 4 | Step 2801000 | Avg Loss: 0.0146 | Grad Norm: 0.00909728\n",
      "Epoch 4 | Step 2801100 | Avg Loss: 0.0149 | Grad Norm: 0.00876140\n",
      "Epoch 4 | Step 2801200 | Avg Loss: 0.0151 | Grad Norm: 0.01005706\n",
      "Epoch 4 | Step 2801300 | Avg Loss: 0.0158 | Grad Norm: 0.01073804\n",
      "Epoch 4 | Step 2801400 | Avg Loss: 0.0155 | Grad Norm: 0.00818292\n",
      "Epoch 4 | Step 2801500 | Avg Loss: 0.0152 | Grad Norm: 0.00932797\n",
      "Epoch 4 | Step 2801600 | Avg Loss: 0.0153 | Grad Norm: 0.00909921\n",
      "Epoch 4 | Step 2801700 | Avg Loss: 0.0150 | Grad Norm: 0.00878380\n",
      "Epoch 4 | Step 2801800 | Avg Loss: 0.0154 | Grad Norm: 0.00840122\n",
      "Epoch 4 | Step 2801900 | Avg Loss: 0.0152 | Grad Norm: 0.00857743\n",
      "Epoch 4 | Step 2802000 | Avg Loss: 0.0154 | Grad Norm: 0.00864552\n",
      "Epoch 4 | Step 2802100 | Avg Loss: 0.0158 | Grad Norm: 0.00913155\n",
      "Epoch 4 | Step 2802200 | Avg Loss: 0.0154 | Grad Norm: 0.00921328\n",
      "Epoch 4 | Step 2802300 | Avg Loss: 0.0155 | Grad Norm: 0.00765051\n",
      "Epoch 4 | Step 2802400 | Avg Loss: 0.0160 | Grad Norm: 0.00974609\n",
      "Epoch 4 | Step 2802500 | Avg Loss: 0.0161 | Grad Norm: 0.00820220\n",
      "Epoch 4 | Step 2802600 | Avg Loss: 0.0160 | Grad Norm: 0.00873015\n",
      "Epoch 4 | Step 2802700 | Avg Loss: 0.0158 | Grad Norm: 0.00914644\n",
      "Epoch 4 | Step 2802800 | Avg Loss: 0.0157 | Grad Norm: 0.00934438\n",
      "Epoch 4 | Step 2802900 | Avg Loss: 0.0157 | Grad Norm: 0.00892715\n",
      "Epoch 4 | Step 2803000 | Avg Loss: 0.0154 | Grad Norm: 0.00907882\n",
      "Epoch 4 | Step 2803100 | Avg Loss: 0.0154 | Grad Norm: 0.00786883\n",
      "Epoch 4 | Step 2803200 | Avg Loss: 0.0153 | Grad Norm: 0.00809051\n",
      "Epoch 4 | Step 2803300 | Avg Loss: 0.0152 | Grad Norm: 0.00876809\n",
      "Epoch 4 | Step 2803400 | Avg Loss: 0.0153 | Grad Norm: 0.00806848\n",
      "Epoch 4 | Step 2803500 | Avg Loss: 0.0153 | Grad Norm: 0.00905733\n",
      "Epoch 4 | Step 2803600 | Avg Loss: 0.0152 | Grad Norm: 0.00950745\n",
      "Epoch 4 | Step 2803700 | Avg Loss: 0.0152 | Grad Norm: 0.01031294\n",
      "Epoch 4 | Step 2803800 | Avg Loss: 0.0150 | Grad Norm: 0.00982257\n",
      "Epoch 4 | Step 2803900 | Avg Loss: 0.0150 | Grad Norm: 0.00907821\n",
      "Epoch 4 | Step 2804000 | Avg Loss: 0.0152 | Grad Norm: 0.00811565\n",
      "Epoch 4 | Step 2804100 | Avg Loss: 0.0151 | Grad Norm: 0.00817142\n",
      "Epoch 4 | Step 2804200 | Avg Loss: 0.0153 | Grad Norm: 0.00959040\n",
      "Epoch 4 | Step 2804300 | Avg Loss: 0.0150 | Grad Norm: 0.01065631\n",
      "Epoch 4 | Step 2804400 | Avg Loss: 0.0151 | Grad Norm: 0.00769818\n",
      "Epoch 4 | Step 2804500 | Avg Loss: 0.0149 | Grad Norm: 0.01031965\n",
      "Epoch 4 | Step 2804600 | Avg Loss: 0.0148 | Grad Norm: 0.00726053\n",
      "Epoch 4 | Step 2804700 | Avg Loss: 0.0152 | Grad Norm: 0.00875253\n",
      "Epoch 4 | Step 2804800 | Avg Loss: 0.0150 | Grad Norm: 0.00898250\n",
      "Epoch 4 | Step 2804900 | Avg Loss: 0.0150 | Grad Norm: 0.01110240\n",
      "Epoch 4 | Step 2805000 | Avg Loss: 0.0149 | Grad Norm: 0.00894101\n",
      "Epoch 4 | Step 2805100 | Avg Loss: 0.0151 | Grad Norm: 0.00813153\n",
      "Epoch 4 | Step 2805200 | Avg Loss: 0.0149 | Grad Norm: 0.00752110\n",
      "Epoch 4 | Step 2805300 | Avg Loss: 0.0151 | Grad Norm: 0.00852049\n",
      "Epoch 4 | Step 2805400 | Avg Loss: 0.0148 | Grad Norm: 0.00833419\n",
      "Epoch 4 | Step 2805500 | Avg Loss: 0.0152 | Grad Norm: 0.00903312\n",
      "Epoch 4 | Step 2805600 | Avg Loss: 0.0149 | Grad Norm: 0.01002790\n",
      "Epoch 4 | Step 2805700 | Avg Loss: 0.0147 | Grad Norm: 0.00731985\n",
      "Epoch 4 | Step 2805800 | Avg Loss: 0.0149 | Grad Norm: 0.00911154\n",
      "Epoch 4 | Step 2805900 | Avg Loss: 0.0152 | Grad Norm: 0.00880930\n",
      "Epoch 4 | Step 2806000 | Avg Loss: 0.0152 | Grad Norm: 0.00895933\n",
      "Epoch 4 | Step 2806100 | Avg Loss: 0.0150 | Grad Norm: 0.00760676\n",
      "Epoch 4 | Step 2806200 | Avg Loss: 0.0150 | Grad Norm: 0.00983559\n",
      "Epoch 4 | Step 2806300 | Avg Loss: 0.0152 | Grad Norm: 0.00926793\n",
      "Epoch 4 | Step 2806400 | Avg Loss: 0.0152 | Grad Norm: 0.00809352\n",
      "Epoch 4 | Step 2806500 | Avg Loss: 0.0154 | Grad Norm: 0.00859064\n",
      "Epoch 4 | Step 2806600 | Avg Loss: 0.0153 | Grad Norm: 0.01039550\n",
      "Epoch 4 | Step 2806700 | Avg Loss: 0.0158 | Grad Norm: 0.00911177\n",
      "Epoch 4 | Step 2806800 | Avg Loss: 0.0158 | Grad Norm: 0.00857955\n",
      "Epoch 4 | Step 2806900 | Avg Loss: 0.0157 | Grad Norm: 0.00916630\n",
      "Epoch 4 | Step 2807000 | Avg Loss: 0.0157 | Grad Norm: 0.01003004\n",
      "Epoch 4 | Step 2807100 | Avg Loss: 0.0159 | Grad Norm: 0.00978050\n",
      "Epoch 4 | Step 2807200 | Avg Loss: 0.0156 | Grad Norm: 0.00895095\n",
      "Epoch 4 | Step 2807300 | Avg Loss: 0.0159 | Grad Norm: 0.00842475\n",
      "Epoch 4 | Step 2807400 | Avg Loss: 0.0156 | Grad Norm: 0.00928912\n",
      "Epoch 4 | Step 2807500 | Avg Loss: 0.0156 | Grad Norm: 0.00855989\n",
      "Epoch 4 | Step 2807600 | Avg Loss: 0.0155 | Grad Norm: 0.00961641\n",
      "Epoch 4 | Step 2807700 | Avg Loss: 0.0150 | Grad Norm: 0.00849183\n",
      "Epoch 4 | Step 2807800 | Avg Loss: 0.0151 | Grad Norm: 0.00881652\n",
      "Epoch 4 | Step 2807900 | Avg Loss: 0.0154 | Grad Norm: 0.01045373\n",
      "Epoch 4 | Step 2808000 | Avg Loss: 0.0157 | Grad Norm: 0.01017940\n",
      "Epoch 4 | Step 2808100 | Avg Loss: 0.0159 | Grad Norm: 0.00904117\n",
      "Epoch 4 | Step 2808200 | Avg Loss: 0.0156 | Grad Norm: 0.00870185\n",
      "Epoch 4 | Step 2808300 | Avg Loss: 0.0154 | Grad Norm: 0.00849405\n",
      "Epoch 4 | Step 2808400 | Avg Loss: 0.0154 | Grad Norm: 0.00947394\n",
      "Epoch 4 | Step 2808500 | Avg Loss: 0.0154 | Grad Norm: 0.00799026\n",
      "Epoch 4 | Step 2808600 | Avg Loss: 0.0154 | Grad Norm: 0.00979849\n",
      "Epoch 4 | Step 2808700 | Avg Loss: 0.0157 | Grad Norm: 0.00762075\n",
      "Epoch 4 | Step 2808800 | Avg Loss: 0.0157 | Grad Norm: 0.00880290\n",
      "Epoch 4 | Step 2808900 | Avg Loss: 0.0157 | Grad Norm: 0.00968156\n",
      "Epoch 4 | Step 2809000 | Avg Loss: 0.0159 | Grad Norm: 0.00850816\n",
      "Epoch 4 | Step 2809100 | Avg Loss: 0.0156 | Grad Norm: 0.00902562\n",
      "Epoch 4 | Step 2809200 | Avg Loss: 0.0155 | Grad Norm: 0.00907126\n",
      "Epoch 4 | Step 2809300 | Avg Loss: 0.0160 | Grad Norm: 0.01022503\n",
      "Epoch 4 | Step 2809400 | Avg Loss: 0.0156 | Grad Norm: 0.00946702\n",
      "Epoch 4 | Step 2809500 | Avg Loss: 0.0155 | Grad Norm: 0.01040999\n",
      "Epoch 4 | Step 2809600 | Avg Loss: 0.0159 | Grad Norm: 0.00993308\n",
      "Epoch 4 | Step 2809700 | Avg Loss: 0.0158 | Grad Norm: 0.00865130\n",
      "Epoch 4 | Step 2809800 | Avg Loss: 0.0160 | Grad Norm: 0.00836197\n",
      "Epoch 4 | Step 2809900 | Avg Loss: 0.0159 | Grad Norm: 0.00855861\n",
      "Epoch 4 | Step 2810000 | Avg Loss: 0.0156 | Grad Norm: 0.01006722\n",
      "Epoch 4 | Step 2810100 | Avg Loss: 0.0159 | Grad Norm: 0.00770389\n",
      "Epoch 4 | Step 2810200 | Avg Loss: 0.0155 | Grad Norm: 0.00911858\n",
      "Epoch 4 | Step 2810300 | Avg Loss: 0.0156 | Grad Norm: 0.00967673\n",
      "Epoch 4 | Step 2810400 | Avg Loss: 0.0153 | Grad Norm: 0.00807589\n",
      "Epoch 4 | Step 2810500 | Avg Loss: 0.0154 | Grad Norm: 0.00918170\n",
      "Epoch 4 | Step 2810600 | Avg Loss: 0.0153 | Grad Norm: 0.00867599\n",
      "Epoch 4 | Step 2810700 | Avg Loss: 0.0158 | Grad Norm: 0.00868970\n",
      "Epoch 4 | Step 2810800 | Avg Loss: 0.0153 | Grad Norm: 0.00869487\n",
      "Epoch 4 | Step 2810900 | Avg Loss: 0.0155 | Grad Norm: 0.00857366\n",
      "Epoch 4 | Step 2811000 | Avg Loss: 0.0156 | Grad Norm: 0.00942253\n",
      "Epoch 4 | Step 2811100 | Avg Loss: 0.0155 | Grad Norm: 0.00914507\n",
      "Epoch 4 | Step 2811200 | Avg Loss: 0.0155 | Grad Norm: 0.00829724\n",
      "Epoch 4 | Step 2811300 | Avg Loss: 0.0153 | Grad Norm: 0.00763669\n",
      "Epoch 4 | Step 2811400 | Avg Loss: 0.0158 | Grad Norm: 0.00922949\n",
      "Epoch 4 | Step 2811500 | Avg Loss: 0.0156 | Grad Norm: 0.00846003\n",
      "Epoch 4 | Step 2811600 | Avg Loss: 0.0156 | Grad Norm: 0.00892263\n",
      "Epoch 4 | Step 2811700 | Avg Loss: 0.0156 | Grad Norm: 0.00963704\n",
      "Epoch 4 | Step 2811800 | Avg Loss: 0.0152 | Grad Norm: 0.00853274\n",
      "Epoch 4 | Step 2811900 | Avg Loss: 0.0153 | Grad Norm: 0.01038963\n",
      "Epoch 4 | Step 2812000 | Avg Loss: 0.0155 | Grad Norm: 0.00894475\n",
      "Epoch 4 | Step 2812100 | Avg Loss: 0.0156 | Grad Norm: 0.01010236\n",
      "Epoch 4 | Step 2812200 | Avg Loss: 0.0155 | Grad Norm: 0.01009970\n",
      "Epoch 4 | Step 2812300 | Avg Loss: 0.0157 | Grad Norm: 0.01034894\n",
      "Epoch 4 | Step 2812400 | Avg Loss: 0.0157 | Grad Norm: 0.01056695\n",
      "Epoch 4 | Step 2812500 | Avg Loss: 0.0153 | Grad Norm: 0.00916240\n",
      "Epoch 4 | Step 2812600 | Avg Loss: 0.0154 | Grad Norm: 0.00919727\n",
      "Epoch 4 | Step 2812700 | Avg Loss: 0.0154 | Grad Norm: 0.00998975\n",
      "Epoch 4 | Step 2812800 | Avg Loss: 0.0153 | Grad Norm: 0.00899665\n",
      "Epoch 4 | Step 2812900 | Avg Loss: 0.0157 | Grad Norm: 0.00862274\n",
      "Epoch 4 | Step 2813000 | Avg Loss: 0.0153 | Grad Norm: 0.00846017\n",
      "Epoch 4 | Step 2813100 | Avg Loss: 0.0156 | Grad Norm: 0.00870906\n",
      "Epoch 4 | Step 2813200 | Avg Loss: 0.0155 | Grad Norm: 0.00969433\n",
      "Epoch 4 | Step 2813300 | Avg Loss: 0.0156 | Grad Norm: 0.00849504\n",
      "Epoch 4 | Step 2813400 | Avg Loss: 0.0156 | Grad Norm: 0.01020204\n",
      "Epoch 4 | Step 2813500 | Avg Loss: 0.0156 | Grad Norm: 0.01089280\n",
      "Epoch 4 | Step 2813600 | Avg Loss: 0.0156 | Grad Norm: 0.00809093\n",
      "Epoch 4 | Step 2813700 | Avg Loss: 0.0155 | Grad Norm: 0.00902592\n",
      "Epoch 4 | Step 2813800 | Avg Loss: 0.0154 | Grad Norm: 0.00846761\n",
      "Epoch 4 | Step 2813900 | Avg Loss: 0.0157 | Grad Norm: 0.00765365\n",
      "Epoch 4 | Step 2814000 | Avg Loss: 0.0154 | Grad Norm: 0.00832084\n",
      "Epoch 4 | Step 2814100 | Avg Loss: 0.0154 | Grad Norm: 0.00932487\n",
      "Epoch 4 | Step 2814200 | Avg Loss: 0.0153 | Grad Norm: 0.00938264\n",
      "Epoch 4 | Step 2814300 | Avg Loss: 0.0155 | Grad Norm: 0.00931839\n",
      "Epoch 4 | Step 2814400 | Avg Loss: 0.0156 | Grad Norm: 0.00916065\n",
      "Epoch 4 | Step 2814500 | Avg Loss: 0.0153 | Grad Norm: 0.00836681\n",
      "Epoch 4 | Step 2814600 | Avg Loss: 0.0150 | Grad Norm: 0.00867069\n",
      "Epoch 4 | Step 2814700 | Avg Loss: 0.0150 | Grad Norm: 0.00878729\n",
      "Epoch 4 | Step 2814800 | Avg Loss: 0.0151 | Grad Norm: 0.00872211\n",
      "Epoch 4 | Step 2814900 | Avg Loss: 0.0152 | Grad Norm: 0.01017837\n",
      "Epoch 4 | Step 2815000 | Avg Loss: 0.0155 | Grad Norm: 0.01309918\n",
      "Epoch 4 | Step 2815100 | Avg Loss: 0.0154 | Grad Norm: 0.00879342\n",
      "Epoch 4 | Step 2815200 | Avg Loss: 0.0156 | Grad Norm: 0.00832230\n",
      "Epoch 4 | Step 2815300 | Avg Loss: 0.0152 | Grad Norm: 0.00903039\n",
      "Epoch 4 | Step 2815400 | Avg Loss: 0.0153 | Grad Norm: 0.00871531\n",
      "Epoch 4 | Step 2815500 | Avg Loss: 0.0155 | Grad Norm: 0.00892208\n",
      "Epoch 4 | Step 2815600 | Avg Loss: 0.0157 | Grad Norm: 0.00914636\n",
      "Epoch 4 | Step 2815700 | Avg Loss: 0.0157 | Grad Norm: 0.00824122\n",
      "Epoch 4 | Step 2815800 | Avg Loss: 0.0154 | Grad Norm: 0.01028851\n",
      "Epoch 4 | Step 2815900 | Avg Loss: 0.0152 | Grad Norm: 0.00909089\n",
      "Epoch 4 | Step 2816000 | Avg Loss: 0.0154 | Grad Norm: 0.00880751\n",
      "Epoch 4 | Step 2816100 | Avg Loss: 0.0151 | Grad Norm: 0.00913352\n",
      "Epoch 4 | Step 2816200 | Avg Loss: 0.0151 | Grad Norm: 0.00823936\n",
      "Epoch 4 | Step 2816300 | Avg Loss: 0.0153 | Grad Norm: 0.00938690\n",
      "Epoch 4 | Step 2816400 | Avg Loss: 0.0156 | Grad Norm: 0.00859570\n",
      "Epoch 4 | Step 2816500 | Avg Loss: 0.0156 | Grad Norm: 0.00831208\n",
      "Epoch 4 | Step 2816600 | Avg Loss: 0.0156 | Grad Norm: 0.00960159\n",
      "Epoch 4 | Step 2816700 | Avg Loss: 0.0157 | Grad Norm: 0.00962175\n",
      "Epoch 4 | Step 2816800 | Avg Loss: 0.0155 | Grad Norm: 0.00953381\n",
      "Epoch 4 | Step 2816900 | Avg Loss: 0.0152 | Grad Norm: 0.00801905\n",
      "Epoch 4 | Step 2817000 | Avg Loss: 0.0152 | Grad Norm: 0.00806648\n",
      "Epoch 4 | Step 2817100 | Avg Loss: 0.0152 | Grad Norm: 0.00903926\n",
      "Epoch 4 | Step 2817200 | Avg Loss: 0.0154 | Grad Norm: 0.00871702\n",
      "Epoch 4 | Step 2817300 | Avg Loss: 0.0153 | Grad Norm: 0.00931114\n",
      "Epoch 4 | Step 2817400 | Avg Loss: 0.0155 | Grad Norm: 0.01236665\n",
      "Epoch 4 | Step 2817500 | Avg Loss: 0.0155 | Grad Norm: 0.00900387\n",
      "Epoch 4 | Step 2817600 | Avg Loss: 0.0158 | Grad Norm: 0.00958503\n",
      "Epoch 4 | Step 2817700 | Avg Loss: 0.0158 | Grad Norm: 0.01001010\n",
      "Epoch 4 | Step 2817800 | Avg Loss: 0.0158 | Grad Norm: 0.00814852\n",
      "Epoch 4 | Step 2817900 | Avg Loss: 0.0159 | Grad Norm: 0.00823380\n",
      "Epoch 4 | Step 2818000 | Avg Loss: 0.0157 | Grad Norm: 0.01005710\n",
      "Epoch 4 | Step 2818100 | Avg Loss: 0.0160 | Grad Norm: 0.00978379\n",
      "Epoch 4 | Step 2818200 | Avg Loss: 0.0161 | Grad Norm: 0.00976602\n",
      "Epoch 4 | Step 2818300 | Avg Loss: 0.0163 | Grad Norm: 0.01008017\n",
      "Epoch 4 | Step 2818400 | Avg Loss: 0.0156 | Grad Norm: 0.01024665\n",
      "Epoch 4 | Step 2818500 | Avg Loss: 0.0160 | Grad Norm: 0.00886096\n",
      "Epoch 4 | Step 2818600 | Avg Loss: 0.0160 | Grad Norm: 0.00828507\n",
      "Epoch 4 | Step 2818700 | Avg Loss: 0.0159 | Grad Norm: 0.00842475\n",
      "Epoch 4 | Step 2818800 | Avg Loss: 0.0160 | Grad Norm: 0.00865079\n",
      "Epoch 4 | Step 2818900 | Avg Loss: 0.0159 | Grad Norm: 0.00926846\n",
      "Epoch 4 | Step 2819000 | Avg Loss: 0.0158 | Grad Norm: 0.00909047\n",
      "Epoch 4 | Step 2819100 | Avg Loss: 0.0156 | Grad Norm: 0.00850016\n",
      "Epoch 4 | Step 2819200 | Avg Loss: 0.0151 | Grad Norm: 0.00884094\n",
      "Epoch 4 | Step 2819300 | Avg Loss: 0.0150 | Grad Norm: 0.00932301\n",
      "Epoch 4 | Step 2819400 | Avg Loss: 0.0149 | Grad Norm: 0.00908937\n",
      "Epoch 4 | Step 2819500 | Avg Loss: 0.0151 | Grad Norm: 0.00936122\n",
      "Epoch 4 | Step 2819600 | Avg Loss: 0.0150 | Grad Norm: 0.00838224\n",
      "Epoch 4 | Step 2819700 | Avg Loss: 0.0151 | Grad Norm: 0.00885036\n",
      "Epoch 4 | Step 2819800 | Avg Loss: 0.0149 | Grad Norm: 0.00936061\n",
      "Epoch 4 | Step 2819900 | Avg Loss: 0.0151 | Grad Norm: 0.00801941\n",
      "Epoch 4 | Step 2820000 | Avg Loss: 0.0151 | Grad Norm: 0.00834118\n",
      "Epoch 4 | Step 2820100 | Avg Loss: 0.0153 | Grad Norm: 0.00857258\n",
      "Epoch 4 | Step 2820200 | Avg Loss: 0.0157 | Grad Norm: 0.00816724\n",
      "Epoch 4 | Step 2820300 | Avg Loss: 0.0157 | Grad Norm: 0.00853067\n",
      "Epoch 4 | Step 2820400 | Avg Loss: 0.0159 | Grad Norm: 0.00947750\n",
      "Epoch 4 | Step 2820500 | Avg Loss: 0.0156 | Grad Norm: 0.00859370\n",
      "Epoch 4 | Step 2820600 | Avg Loss: 0.0156 | Grad Norm: 0.01064123\n",
      "Epoch 4 | Step 2820700 | Avg Loss: 0.0157 | Grad Norm: 0.01075551\n",
      "Epoch 4 | Step 2820800 | Avg Loss: 0.0157 | Grad Norm: 0.01100567\n",
      "Epoch 4 | Step 2820900 | Avg Loss: 0.0154 | Grad Norm: 0.00853365\n",
      "Epoch 4 | Step 2821000 | Avg Loss: 0.0152 | Grad Norm: 0.00925669\n",
      "Epoch 4 | Step 2821100 | Avg Loss: 0.0156 | Grad Norm: 0.01009089\n",
      "Epoch 4 | Step 2821200 | Avg Loss: 0.0158 | Grad Norm: 0.01011318\n",
      "Epoch 4 | Step 2821300 | Avg Loss: 0.0159 | Grad Norm: 0.00838057\n",
      "Epoch 4 | Step 2821400 | Avg Loss: 0.0157 | Grad Norm: 0.00966629\n",
      "Epoch 4 | Step 2821500 | Avg Loss: 0.0157 | Grad Norm: 0.00803200\n",
      "Epoch 4 | Step 2821600 | Avg Loss: 0.0157 | Grad Norm: 0.00843023\n",
      "Epoch 4 | Step 2821700 | Avg Loss: 0.0158 | Grad Norm: 0.01163583\n",
      "Epoch 4 | Step 2821800 | Avg Loss: 0.0162 | Grad Norm: 0.00939971\n",
      "Epoch 4 | Step 2821900 | Avg Loss: 0.0160 | Grad Norm: 0.00840445\n",
      "Epoch 4 | Step 2822000 | Avg Loss: 0.0161 | Grad Norm: 0.00845361\n",
      "Epoch 4 | Step 2822100 | Avg Loss: 0.0157 | Grad Norm: 0.01067194\n",
      "Epoch 4 | Step 2822200 | Avg Loss: 0.0160 | Grad Norm: 0.00787429\n",
      "Epoch 4 | Step 2822300 | Avg Loss: 0.0158 | Grad Norm: 0.00798188\n",
      "Epoch 4 | Step 2822400 | Avg Loss: 0.0159 | Grad Norm: 0.00877061\n",
      "Epoch 4 | Step 2822500 | Avg Loss: 0.0156 | Grad Norm: 0.00880726\n",
      "Epoch 4 | Step 2822600 | Avg Loss: 0.0155 | Grad Norm: 0.00849145\n",
      "Epoch 4 | Step 2822700 | Avg Loss: 0.0155 | Grad Norm: 0.00913153\n",
      "Epoch 4 | Step 2822800 | Avg Loss: 0.0157 | Grad Norm: 0.00837247\n",
      "Epoch 4 | Step 2822900 | Avg Loss: 0.0156 | Grad Norm: 0.01066202\n",
      "Epoch 4 | Step 2823000 | Avg Loss: 0.0151 | Grad Norm: 0.00945992\n",
      "Epoch 4 | Step 2823100 | Avg Loss: 0.0152 | Grad Norm: 0.01292668\n",
      "Epoch 4 | Step 2823200 | Avg Loss: 0.0156 | Grad Norm: 0.00871349\n",
      "Epoch 4 | Step 2823300 | Avg Loss: 0.0161 | Grad Norm: 0.00891466\n",
      "Epoch 4 | Step 2823400 | Avg Loss: 0.0159 | Grad Norm: 0.00886913\n",
      "Epoch 4 | Step 2823500 | Avg Loss: 0.0160 | Grad Norm: 0.00903722\n",
      "Epoch 4 | Step 2823600 | Avg Loss: 0.0159 | Grad Norm: 0.00901322\n",
      "Epoch 4 | Step 2823700 | Avg Loss: 0.0160 | Grad Norm: 0.00900242\n",
      "Epoch 4 | Step 2823800 | Avg Loss: 0.0165 | Grad Norm: 0.00868532\n",
      "Epoch 4 | Step 2823900 | Avg Loss: 0.0162 | Grad Norm: 0.00910681\n",
      "Epoch 4 | Step 2824000 | Avg Loss: 0.0160 | Grad Norm: 0.00838668\n",
      "Epoch 4 | Step 2824100 | Avg Loss: 0.0158 | Grad Norm: 0.00795396\n",
      "Epoch 4 | Step 2824200 | Avg Loss: 0.0156 | Grad Norm: 0.01090343\n",
      "Epoch 4 | Step 2824300 | Avg Loss: 0.0154 | Grad Norm: 0.00878501\n",
      "Epoch 4 | Step 2824400 | Avg Loss: 0.0152 | Grad Norm: 0.00783782\n",
      "Epoch 4 | Step 2824500 | Avg Loss: 0.0152 | Grad Norm: 0.00762843\n",
      "Epoch 4 | Step 2824600 | Avg Loss: 0.0155 | Grad Norm: 0.00915078\n",
      "Epoch 4 | Step 2824700 | Avg Loss: 0.0156 | Grad Norm: 0.00968361\n",
      "Epoch 4 | Step 2824800 | Avg Loss: 0.0158 | Grad Norm: 0.00865435\n",
      "Epoch 4 | Step 2824900 | Avg Loss: 0.0160 | Grad Norm: 0.00904914\n",
      "Epoch 4 | Step 2825000 | Avg Loss: 0.0162 | Grad Norm: 0.00957969\n",
      "Epoch 4 | Step 2825100 | Avg Loss: 0.0160 | Grad Norm: 0.00981708\n",
      "Epoch 4 | Step 2825200 | Avg Loss: 0.0162 | Grad Norm: 0.00878404\n",
      "Epoch 4 | Step 2825300 | Avg Loss: 0.0158 | Grad Norm: 0.00910130\n",
      "Epoch 4 | Step 2825400 | Avg Loss: 0.0157 | Grad Norm: 0.01101157\n",
      "Epoch 4 | Step 2825500 | Avg Loss: 0.0158 | Grad Norm: 0.00856909\n",
      "Epoch 4 | Step 2825600 | Avg Loss: 0.0157 | Grad Norm: 0.00882663\n",
      "Epoch 4 | Step 2825700 | Avg Loss: 0.0150 | Grad Norm: 0.00791779\n",
      "Epoch 4 | Step 2825800 | Avg Loss: 0.0152 | Grad Norm: 0.00917818\n",
      "Epoch 4 | Step 2825900 | Avg Loss: 0.0155 | Grad Norm: 0.00876466\n",
      "Epoch 4 | Step 2826000 | Avg Loss: 0.0152 | Grad Norm: 0.00834558\n",
      "Epoch 4 | Step 2826100 | Avg Loss: 0.0152 | Grad Norm: 0.00801065\n",
      "Epoch 4 | Step 2826200 | Avg Loss: 0.0157 | Grad Norm: 0.00952625\n",
      "Epoch 4 | Step 2826300 | Avg Loss: 0.0160 | Grad Norm: 0.00900123\n",
      "Epoch 4 | Step 2826400 | Avg Loss: 0.0158 | Grad Norm: 0.00856876\n",
      "Epoch 4 | Step 2826500 | Avg Loss: 0.0158 | Grad Norm: 0.01062019\n",
      "Epoch 4 | Step 2826600 | Avg Loss: 0.0159 | Grad Norm: 0.00920266\n",
      "Epoch 4 | Step 2826700 | Avg Loss: 0.0158 | Grad Norm: 0.00934253\n",
      "Epoch 4 | Step 2826800 | Avg Loss: 0.0158 | Grad Norm: 0.00849743\n",
      "Epoch 4 | Step 2826900 | Avg Loss: 0.0157 | Grad Norm: 0.00921695\n",
      "Epoch 4 | Step 2827000 | Avg Loss: 0.0157 | Grad Norm: 0.00808610\n",
      "Epoch 4 | Step 2827100 | Avg Loss: 0.0157 | Grad Norm: 0.00923087\n",
      "Epoch 4 | Step 2827200 | Avg Loss: 0.0153 | Grad Norm: 0.00919091\n",
      "Epoch 4 | Step 2827300 | Avg Loss: 0.0156 | Grad Norm: 0.00881060\n",
      "Epoch 4 | Step 2827400 | Avg Loss: 0.0151 | Grad Norm: 0.00917815\n",
      "Epoch 4 | Step 2827500 | Avg Loss: 0.0151 | Grad Norm: 0.01092429\n",
      "Epoch 4 | Step 2827600 | Avg Loss: 0.0155 | Grad Norm: 0.00816948\n",
      "Epoch 4 | Step 2827700 | Avg Loss: 0.0152 | Grad Norm: 0.01415928\n",
      "Epoch 4 | Step 2827800 | Avg Loss: 0.0155 | Grad Norm: 0.00961267\n",
      "Epoch 4 | Step 2827900 | Avg Loss: 0.0154 | Grad Norm: 0.00935732\n",
      "Epoch 4 | Step 2828000 | Avg Loss: 0.0157 | Grad Norm: 0.00977545\n",
      "Epoch 4 | Step 2828100 | Avg Loss: 0.0155 | Grad Norm: 0.00909601\n",
      "Epoch 4 | Step 2828200 | Avg Loss: 0.0157 | Grad Norm: 0.00868456\n",
      "Epoch 4 | Step 2828300 | Avg Loss: 0.0154 | Grad Norm: 0.00907485\n",
      "Epoch 4 | Step 2828400 | Avg Loss: 0.0156 | Grad Norm: 0.00936013\n",
      "Epoch 4 | Step 2828500 | Avg Loss: 0.0155 | Grad Norm: 0.00847033\n",
      "Epoch 4 | Step 2828600 | Avg Loss: 0.0157 | Grad Norm: 0.00923449\n",
      "Epoch 4 | Step 2828700 | Avg Loss: 0.0158 | Grad Norm: 0.00861551\n",
      "Epoch 4 | Step 2828800 | Avg Loss: 0.0153 | Grad Norm: 0.01012791\n",
      "Epoch 4 | Step 2828900 | Avg Loss: 0.0154 | Grad Norm: 0.00824363\n",
      "Epoch 4 | Step 2829000 | Avg Loss: 0.0151 | Grad Norm: 0.00833217\n",
      "Epoch 4 | Step 2829100 | Avg Loss: 0.0151 | Grad Norm: 0.00750328\n",
      "Epoch 4 | Step 2829200 | Avg Loss: 0.0153 | Grad Norm: 0.00967921\n",
      "Epoch 4 | Step 2829300 | Avg Loss: 0.0152 | Grad Norm: 0.00835104\n",
      "Epoch 4 | Step 2829400 | Avg Loss: 0.0152 | Grad Norm: 0.00929938\n",
      "Epoch 4 | Step 2829500 | Avg Loss: 0.0154 | Grad Norm: 0.00871384\n",
      "Epoch 4 | Step 2829600 | Avg Loss: 0.0153 | Grad Norm: 0.01340739\n",
      "Epoch 4 | Step 2829700 | Avg Loss: 0.0152 | Grad Norm: 0.00914174\n",
      "Epoch 4 | Step 2829800 | Avg Loss: 0.0154 | Grad Norm: 0.01075404\n",
      "Epoch 4 | Step 2829900 | Avg Loss: 0.0156 | Grad Norm: 0.00845149\n",
      "Epoch 4 | Step 2830000 | Avg Loss: 0.0154 | Grad Norm: 0.00944104\n",
      "Epoch 4 | Step 2830100 | Avg Loss: 0.0158 | Grad Norm: 0.01025683\n",
      "Epoch 4 | Step 2830200 | Avg Loss: 0.0160 | Grad Norm: 0.00874957\n",
      "Epoch 4 | Step 2830300 | Avg Loss: 0.0156 | Grad Norm: 0.00937260\n",
      "Epoch 4 | Step 2830400 | Avg Loss: 0.0155 | Grad Norm: 0.01036535\n",
      "Epoch 4 | Step 2830500 | Avg Loss: 0.0154 | Grad Norm: 0.00972956\n",
      "Epoch 4 | Step 2830600 | Avg Loss: 0.0154 | Grad Norm: 0.00847875\n",
      "Epoch 4 | Step 2830700 | Avg Loss: 0.0155 | Grad Norm: 0.00878244\n",
      "Epoch 4 | Step 2830800 | Avg Loss: 0.0157 | Grad Norm: 0.00905889\n",
      "Epoch 4 | Step 2830900 | Avg Loss: 0.0157 | Grad Norm: 0.01116922\n",
      "Epoch 4 | Step 2831000 | Avg Loss: 0.0158 | Grad Norm: 0.01105893\n",
      "Epoch 4 | Step 2831100 | Avg Loss: 0.0155 | Grad Norm: 0.00871460\n",
      "Epoch 4 | Step 2831200 | Avg Loss: 0.0152 | Grad Norm: 0.00839431\n",
      "Epoch 4 | Step 2831300 | Avg Loss: 0.0148 | Grad Norm: 0.00856321\n",
      "Epoch 4 | Step 2831400 | Avg Loss: 0.0147 | Grad Norm: 0.01015344\n",
      "Epoch 4 | Step 2831500 | Avg Loss: 0.0146 | Grad Norm: 0.00909669\n",
      "Epoch 4 | Step 2831600 | Avg Loss: 0.0151 | Grad Norm: 0.00797144\n",
      "Epoch 4 | Step 2831700 | Avg Loss: 0.0152 | Grad Norm: 0.00866948\n",
      "Epoch 4 | Step 2831800 | Avg Loss: 0.0150 | Grad Norm: 0.01074397\n",
      "Epoch 4 | Step 2831900 | Avg Loss: 0.0152 | Grad Norm: 0.00904548\n",
      "Epoch 4 | Step 2832000 | Avg Loss: 0.0152 | Grad Norm: 0.01054224\n",
      "Epoch 4 | Step 2832100 | Avg Loss: 0.0153 | Grad Norm: 0.00770608\n",
      "Epoch 4 | Step 2832200 | Avg Loss: 0.0155 | Grad Norm: 0.00937694\n",
      "Epoch 4 | Step 2832300 | Avg Loss: 0.0154 | Grad Norm: 0.00932119\n",
      "Epoch 4 | Step 2832400 | Avg Loss: 0.0154 | Grad Norm: 0.01119640\n",
      "Epoch 4 | Step 2832500 | Avg Loss: 0.0159 | Grad Norm: 0.00931801\n",
      "Epoch 4 | Step 2832600 | Avg Loss: 0.0160 | Grad Norm: 0.01005171\n",
      "Epoch 4 | Step 2832700 | Avg Loss: 0.0162 | Grad Norm: 0.01045070\n",
      "Epoch 4 | Step 2832800 | Avg Loss: 0.0157 | Grad Norm: 0.00993576\n",
      "Epoch 4 | Step 2832900 | Avg Loss: 0.0155 | Grad Norm: 0.00928733\n",
      "Epoch 4 | Step 2833000 | Avg Loss: 0.0156 | Grad Norm: 0.00832073\n",
      "Epoch 4 | Step 2833100 | Avg Loss: 0.0157 | Grad Norm: 0.00988409\n",
      "Epoch 4 | Step 2833200 | Avg Loss: 0.0155 | Grad Norm: 0.01007451\n",
      "Epoch 4 | Step 2833300 | Avg Loss: 0.0154 | Grad Norm: 0.00883464\n",
      "Epoch 4 | Step 2833400 | Avg Loss: 0.0156 | Grad Norm: 0.00990306\n",
      "Epoch 4 | Step 2833500 | Avg Loss: 0.0157 | Grad Norm: 0.00896801\n",
      "Epoch 4 | Step 2833600 | Avg Loss: 0.0157 | Grad Norm: 0.00936118\n",
      "Epoch 4 | Step 2833700 | Avg Loss: 0.0160 | Grad Norm: 0.01010385\n",
      "Epoch 4 | Step 2833800 | Avg Loss: 0.0163 | Grad Norm: 0.01019013\n",
      "Epoch 4 | Step 2833900 | Avg Loss: 0.0161 | Grad Norm: 0.00907624\n",
      "Epoch 4 | Step 2834000 | Avg Loss: 0.0164 | Grad Norm: 0.00939250\n",
      "Epoch 4 | Step 2834100 | Avg Loss: 0.0162 | Grad Norm: 0.00916008\n",
      "Epoch 4 | Step 2834200 | Avg Loss: 0.0161 | Grad Norm: 0.01085074\n",
      "Epoch 4 | Step 2834300 | Avg Loss: 0.0164 | Grad Norm: 0.00974828\n",
      "Epoch 4 | Step 2834400 | Avg Loss: 0.0160 | Grad Norm: 0.00884119\n",
      "Epoch 4 | Step 2834500 | Avg Loss: 0.0157 | Grad Norm: 0.01400808\n",
      "Epoch 4 | Step 2834600 | Avg Loss: 0.0156 | Grad Norm: 0.00870717\n",
      "Epoch 4 | Step 2834700 | Avg Loss: 0.0159 | Grad Norm: 0.00913527\n",
      "Epoch 4 | Step 2834800 | Avg Loss: 0.0158 | Grad Norm: 0.01048637\n",
      "Epoch 4 | Step 2834900 | Avg Loss: 0.0158 | Grad Norm: 0.00963001\n",
      "Epoch 4 | Step 2835000 | Avg Loss: 0.0155 | Grad Norm: 0.01038648\n",
      "Epoch 4 | Step 2835100 | Avg Loss: 0.0153 | Grad Norm: 0.00961388\n",
      "Epoch 4 | Step 2835200 | Avg Loss: 0.0153 | Grad Norm: 0.00942419\n",
      "Epoch 4 | Step 2835300 | Avg Loss: 0.0155 | Grad Norm: 0.00924209\n",
      "Epoch 4 | Step 2835400 | Avg Loss: 0.0155 | Grad Norm: 0.00909455\n",
      "Epoch 4 | Step 2835500 | Avg Loss: 0.0155 | Grad Norm: 0.01032206\n",
      "Epoch 4 | Step 2835600 | Avg Loss: 0.0154 | Grad Norm: 0.00884563\n",
      "Epoch 4 | Step 2835700 | Avg Loss: 0.0149 | Grad Norm: 0.00826938\n",
      "Epoch 4 | Step 2835800 | Avg Loss: 0.0151 | Grad Norm: 0.00793148\n",
      "Epoch 4 | Step 2835900 | Avg Loss: 0.0153 | Grad Norm: 0.00898135\n",
      "Epoch 4 | Step 2836000 | Avg Loss: 0.0151 | Grad Norm: 0.00759888\n",
      "Epoch 4 | Step 2836100 | Avg Loss: 0.0152 | Grad Norm: 0.01004234\n",
      "Epoch 4 | Step 2836200 | Avg Loss: 0.0153 | Grad Norm: 0.00954100\n",
      "Epoch 4 | Step 2836300 | Avg Loss: 0.0157 | Grad Norm: 0.01016397\n",
      "Epoch 4 | Step 2836400 | Avg Loss: 0.0159 | Grad Norm: 0.00833676\n",
      "Epoch 4 | Step 2836500 | Avg Loss: 0.0157 | Grad Norm: 0.00927666\n",
      "Epoch 4 | Step 2836600 | Avg Loss: 0.0155 | Grad Norm: 0.00934363\n",
      "Epoch 4 | Step 2836700 | Avg Loss: 0.0156 | Grad Norm: 0.00776679\n",
      "Epoch 4 | Step 2836800 | Avg Loss: 0.0158 | Grad Norm: 0.00939324\n",
      "Epoch 4 | Step 2836900 | Avg Loss: 0.0157 | Grad Norm: 0.00942554\n",
      "Epoch 4 | Step 2837000 | Avg Loss: 0.0158 | Grad Norm: 0.00869007\n",
      "Epoch 4 | Step 2837100 | Avg Loss: 0.0158 | Grad Norm: 0.00976828\n",
      "Epoch 4 | Step 2837200 | Avg Loss: 0.0159 | Grad Norm: 0.00970321\n",
      "Epoch 4 | Step 2837300 | Avg Loss: 0.0159 | Grad Norm: 0.00881837\n",
      "Epoch 4 | Step 2837400 | Avg Loss: 0.0158 | Grad Norm: 0.01024770\n",
      "Epoch 4 | Step 2837500 | Avg Loss: 0.0158 | Grad Norm: 0.00875843\n",
      "Epoch 4 | Step 2837600 | Avg Loss: 0.0159 | Grad Norm: 0.01189648\n",
      "Epoch 4 | Step 2837700 | Avg Loss: 0.0156 | Grad Norm: 0.00846017\n",
      "Epoch 4 | Step 2837800 | Avg Loss: 0.0155 | Grad Norm: 0.00770812\n",
      "Epoch 4 | Step 2837900 | Avg Loss: 0.0154 | Grad Norm: 0.00817339\n",
      "Epoch 4 | Step 2838000 | Avg Loss: 0.0157 | Grad Norm: 0.00920465\n",
      "Epoch 4 | Step 2838100 | Avg Loss: 0.0153 | Grad Norm: 0.00951261\n",
      "Epoch 4 | Step 2838200 | Avg Loss: 0.0152 | Grad Norm: 0.00823549\n",
      "Epoch 4 | Step 2838300 | Avg Loss: 0.0152 | Grad Norm: 0.00866873\n",
      "Epoch 4 | Step 2838400 | Avg Loss: 0.0151 | Grad Norm: 0.01006293\n",
      "Epoch 4 | Step 2838500 | Avg Loss: 0.0152 | Grad Norm: 0.00889493\n",
      "Epoch 4 | Step 2838600 | Avg Loss: 0.0157 | Grad Norm: 0.00900537\n",
      "Epoch 4 | Step 2838700 | Avg Loss: 0.0158 | Grad Norm: 0.01223805\n",
      "Epoch 4 | Step 2838800 | Avg Loss: 0.0159 | Grad Norm: 0.01122696\n",
      "Epoch 4 | Step 2838900 | Avg Loss: 0.0158 | Grad Norm: 0.00994769\n",
      "Epoch 4 | Step 2839000 | Avg Loss: 0.0159 | Grad Norm: 0.00995854\n",
      "Epoch 4 | Step 2839100 | Avg Loss: 0.0157 | Grad Norm: 0.01095972\n",
      "Epoch 4 | Step 2839200 | Avg Loss: 0.0160 | Grad Norm: 0.00920988\n",
      "Epoch 4 | Step 2839300 | Avg Loss: 0.0156 | Grad Norm: 0.00882693\n",
      "Epoch 4 | Step 2839400 | Avg Loss: 0.0158 | Grad Norm: 0.00903475\n",
      "Epoch 4 | Step 2839500 | Avg Loss: 0.0157 | Grad Norm: 0.00977935\n",
      "Epoch 4 | Step 2839600 | Avg Loss: 0.0154 | Grad Norm: 0.00891578\n",
      "Epoch 4 | Step 2839700 | Avg Loss: 0.0155 | Grad Norm: 0.00821052\n",
      "Epoch 4 | Step 2839800 | Avg Loss: 0.0153 | Grad Norm: 0.00888818\n",
      "Epoch 4 | Step 2839900 | Avg Loss: 0.0153 | Grad Norm: 0.01205908\n",
      "Epoch 4 | Step 2840000 | Avg Loss: 0.0153 | Grad Norm: 0.00905470\n",
      "Epoch 4 | Step 2840100 | Avg Loss: 0.0154 | Grad Norm: 0.00917283\n",
      "Epoch 4 | Step 2840200 | Avg Loss: 0.0149 | Grad Norm: 0.00962445\n",
      "Epoch 4 | Step 2840300 | Avg Loss: 0.0151 | Grad Norm: 0.00948776\n",
      "Epoch 4 | Step 2840400 | Avg Loss: 0.0156 | Grad Norm: 0.01064152\n",
      "Epoch 4 | Step 2840500 | Avg Loss: 0.0160 | Grad Norm: 0.01044204\n",
      "Epoch 4 | Step 2840600 | Avg Loss: 0.0160 | Grad Norm: 0.00911125\n",
      "Epoch 4 | Step 2840700 | Avg Loss: 0.0159 | Grad Norm: 0.00934045\n",
      "Epoch 4 | Step 2840800 | Avg Loss: 0.0152 | Grad Norm: 0.00959857\n",
      "Epoch 4 | Step 2840900 | Avg Loss: 0.0155 | Grad Norm: 0.00815760\n",
      "Epoch 4 | Step 2841000 | Avg Loss: 0.0158 | Grad Norm: 0.00920107\n",
      "Epoch 4 | Step 2841100 | Avg Loss: 0.0155 | Grad Norm: 0.00915524\n",
      "Epoch 4 | Step 2841200 | Avg Loss: 0.0151 | Grad Norm: 0.00862001\n",
      "Epoch 4 | Step 2841300 | Avg Loss: 0.0155 | Grad Norm: 0.00918491\n",
      "Epoch 4 | Step 2841400 | Avg Loss: 0.0154 | Grad Norm: 0.01286134\n",
      "Epoch 4 | Step 2841500 | Avg Loss: 0.0148 | Grad Norm: 0.00899050\n",
      "Epoch 4 | Step 2841600 | Avg Loss: 0.0149 | Grad Norm: 0.00886986\n",
      "Epoch 4 | Step 2841700 | Avg Loss: 0.0148 | Grad Norm: 0.00812006\n",
      "Epoch 4 | Step 2841800 | Avg Loss: 0.0149 | Grad Norm: 0.00823408\n",
      "Epoch 4 | Step 2841900 | Avg Loss: 0.0149 | Grad Norm: 0.01115630\n",
      "Epoch 4 | Step 2842000 | Avg Loss: 0.0149 | Grad Norm: 0.01052417\n",
      "Epoch 4 | Step 2842100 | Avg Loss: 0.0149 | Grad Norm: 0.00769529\n",
      "Epoch 4 | Step 2842200 | Avg Loss: 0.0149 | Grad Norm: 0.00829933\n",
      "Epoch 4 | Step 2842300 | Avg Loss: 0.0152 | Grad Norm: 0.00885994\n",
      "Epoch 4 | Step 2842400 | Avg Loss: 0.0153 | Grad Norm: 0.00799910\n",
      "Epoch 4 | Step 2842500 | Avg Loss: 0.0159 | Grad Norm: 0.00932080\n",
      "Epoch 4 | Step 2842600 | Avg Loss: 0.0158 | Grad Norm: 0.00842625\n",
      "Epoch 4 | Step 2842700 | Avg Loss: 0.0155 | Grad Norm: 0.01144940\n",
      "Epoch 4 | Step 2842800 | Avg Loss: 0.0158 | Grad Norm: 0.00870837\n",
      "Epoch 4 | Step 2842900 | Avg Loss: 0.0161 | Grad Norm: 0.00880600\n",
      "Epoch 4 | Step 2843000 | Avg Loss: 0.0158 | Grad Norm: 0.00848382\n",
      "Epoch 4 | Step 2843100 | Avg Loss: 0.0155 | Grad Norm: 0.01033599\n",
      "Epoch 4 | Step 2843200 | Avg Loss: 0.0156 | Grad Norm: 0.00946106\n",
      "Epoch 4 | Step 2843300 | Avg Loss: 0.0156 | Grad Norm: 0.00933318\n",
      "Epoch 4 | Step 2843400 | Avg Loss: 0.0158 | Grad Norm: 0.00934281\n",
      "Epoch 4 | Step 2843500 | Avg Loss: 0.0160 | Grad Norm: 0.01022442\n",
      "Epoch 4 | Step 2843600 | Avg Loss: 0.0160 | Grad Norm: 0.00980089\n",
      "Epoch 4 | Step 2843700 | Avg Loss: 0.0162 | Grad Norm: 0.01051283\n",
      "Epoch 4 | Step 2843800 | Avg Loss: 0.0164 | Grad Norm: 0.00880254\n",
      "Epoch 4 | Step 2843900 | Avg Loss: 0.0159 | Grad Norm: 0.00922473\n",
      "Epoch 4 | Step 2844000 | Avg Loss: 0.0158 | Grad Norm: 0.00832163\n",
      "Epoch 4 | Step 2844100 | Avg Loss: 0.0159 | Grad Norm: 0.00852062\n",
      "Epoch 4 | Step 2844200 | Avg Loss: 0.0159 | Grad Norm: 0.00993338\n",
      "Epoch 4 | Step 2844300 | Avg Loss: 0.0157 | Grad Norm: 0.01061494\n",
      "Epoch 4 | Step 2844400 | Avg Loss: 0.0155 | Grad Norm: 0.00791784\n",
      "Epoch 4 | Step 2844500 | Avg Loss: 0.0155 | Grad Norm: 0.00990126\n",
      "Epoch 4 | Step 2844600 | Avg Loss: 0.0155 | Grad Norm: 0.00914359\n",
      "Epoch 4 | Step 2844700 | Avg Loss: 0.0153 | Grad Norm: 0.00825065\n",
      "Epoch 4 | Step 2844800 | Avg Loss: 0.0155 | Grad Norm: 0.00955957\n",
      "Epoch 4 | Step 2844900 | Avg Loss: 0.0155 | Grad Norm: 0.00772169\n",
      "Epoch 4 | Step 2845000 | Avg Loss: 0.0154 | Grad Norm: 0.00995775\n",
      "Epoch 4 | Step 2845100 | Avg Loss: 0.0156 | Grad Norm: 0.00839127\n",
      "Epoch 4 | Step 2845200 | Avg Loss: 0.0153 | Grad Norm: 0.00919174\n",
      "Epoch 4 | Step 2845300 | Avg Loss: 0.0152 | Grad Norm: 0.00849762\n",
      "Epoch 4 | Step 2845400 | Avg Loss: 0.0151 | Grad Norm: 0.00855851\n",
      "Epoch 4 | Step 2845500 | Avg Loss: 0.0147 | Grad Norm: 0.00945441\n",
      "Epoch 4 | Step 2845600 | Avg Loss: 0.0143 | Grad Norm: 0.00822206\n",
      "Epoch 4 | Step 2845700 | Avg Loss: 0.0145 | Grad Norm: 0.00953705\n",
      "Epoch 4 | Step 2845800 | Avg Loss: 0.0144 | Grad Norm: 0.00853852\n",
      "Epoch 4 | Step 2845900 | Avg Loss: 0.0145 | Grad Norm: 0.00808292\n",
      "Epoch 4 | Step 2846000 | Avg Loss: 0.0146 | Grad Norm: 0.00924257\n",
      "Epoch 4 | Step 2846100 | Avg Loss: 0.0151 | Grad Norm: 0.00864440\n",
      "Epoch 4 | Step 2846200 | Avg Loss: 0.0154 | Grad Norm: 0.00858179\n",
      "Epoch 4 | Step 2846300 | Avg Loss: 0.0150 | Grad Norm: 0.00993496\n",
      "Epoch 4 | Step 2846400 | Avg Loss: 0.0147 | Grad Norm: 0.00793155\n",
      "Epoch 4 | Step 2846500 | Avg Loss: 0.0145 | Grad Norm: 0.00845509\n",
      "Epoch 4 | Step 2846600 | Avg Loss: 0.0146 | Grad Norm: 0.00847395\n",
      "Epoch 4 | Step 2846700 | Avg Loss: 0.0149 | Grad Norm: 0.00853511\n",
      "Epoch 4 | Step 2846800 | Avg Loss: 0.0149 | Grad Norm: 0.00846134\n",
      "Epoch 4 | Step 2846900 | Avg Loss: 0.0150 | Grad Norm: 0.00782435\n",
      "Epoch 4 | Step 2847000 | Avg Loss: 0.0151 | Grad Norm: 0.00929506\n",
      "Epoch 4 | Step 2847100 | Avg Loss: 0.0149 | Grad Norm: 0.00802812\n",
      "Epoch 4 | Step 2847200 | Avg Loss: 0.0151 | Grad Norm: 0.01077757\n",
      "Epoch 4 | Step 2847300 | Avg Loss: 0.0157 | Grad Norm: 0.01165854\n",
      "Epoch 4 | Step 2847400 | Avg Loss: 0.0157 | Grad Norm: 0.00929781\n",
      "Epoch 4 | Step 2847500 | Avg Loss: 0.0157 | Grad Norm: 0.01001595\n",
      "Epoch 4 | Step 2847600 | Avg Loss: 0.0153 | Grad Norm: 0.00892765\n",
      "Epoch 4 | Step 2847700 | Avg Loss: 0.0154 | Grad Norm: 0.00927061\n",
      "Epoch 4 | Step 2847800 | Avg Loss: 0.0156 | Grad Norm: 0.01100891\n",
      "Epoch 4 | Step 2847900 | Avg Loss: 0.0157 | Grad Norm: 0.00897748\n",
      "Epoch 4 | Step 2848000 | Avg Loss: 0.0158 | Grad Norm: 0.01004276\n",
      "Epoch 4 | Step 2848100 | Avg Loss: 0.0155 | Grad Norm: 0.00867741\n",
      "Epoch 4 | Step 2848200 | Avg Loss: 0.0157 | Grad Norm: 0.00981326\n",
      "Epoch 4 | Step 2848300 | Avg Loss: 0.0157 | Grad Norm: 0.00874488\n",
      "Epoch 4 | Step 2848400 | Avg Loss: 0.0156 | Grad Norm: 0.00833652\n",
      "Epoch 4 | Step 2848500 | Avg Loss: 0.0160 | Grad Norm: 0.00855005\n",
      "Epoch 4 | Step 2848600 | Avg Loss: 0.0164 | Grad Norm: 0.00959533\n",
      "Epoch 4 | Step 2848700 | Avg Loss: 0.0158 | Grad Norm: 0.00962959\n",
      "Epoch 4 | Step 2848800 | Avg Loss: 0.0155 | Grad Norm: 0.00897803\n",
      "Epoch 4 | Step 2848900 | Avg Loss: 0.0151 | Grad Norm: 0.01053713\n",
      "Epoch 4 | Step 2849000 | Avg Loss: 0.0154 | Grad Norm: 0.00880539\n",
      "Epoch 4 | Step 2849100 | Avg Loss: 0.0155 | Grad Norm: 0.01072075\n",
      "Epoch 4 | Step 2849200 | Avg Loss: 0.0156 | Grad Norm: 0.00960426\n",
      "Epoch 4 | Step 2849300 | Avg Loss: 0.0160 | Grad Norm: 0.01220195\n",
      "Epoch 4 | Step 2849400 | Avg Loss: 0.0161 | Grad Norm: 0.00882440\n",
      "Epoch 4 | Step 2849500 | Avg Loss: 0.0165 | Grad Norm: 0.00987509\n",
      "Epoch 4 | Step 2849600 | Avg Loss: 0.0162 | Grad Norm: 0.00899276\n",
      "Epoch 4 | Step 2849700 | Avg Loss: 0.0164 | Grad Norm: 0.00951170\n",
      "Epoch 4 | Step 2849800 | Avg Loss: 0.0163 | Grad Norm: 0.00955418\n",
      "Epoch 4 | Step 2849900 | Avg Loss: 0.0161 | Grad Norm: 0.00884527\n",
      "Epoch 4 | Step 2850000 | Avg Loss: 0.0160 | Grad Norm: 0.01048797\n",
      "Epoch 4 | Step 2850100 | Avg Loss: 0.0160 | Grad Norm: 0.00877631\n",
      "Epoch 4 | Step 2850200 | Avg Loss: 0.0156 | Grad Norm: 0.00987377\n",
      "Epoch 4 | Step 2850300 | Avg Loss: 0.0154 | Grad Norm: 0.00885066\n",
      "Epoch 4 | Step 2850400 | Avg Loss: 0.0156 | Grad Norm: 0.00881619\n",
      "Epoch 4 | Step 2850500 | Avg Loss: 0.0156 | Grad Norm: 0.00837076\n",
      "Epoch 4 | Step 2850600 | Avg Loss: 0.0160 | Grad Norm: 0.00848433\n",
      "Epoch 4 | Step 2850700 | Avg Loss: 0.0161 | Grad Norm: 0.00877364\n",
      "Epoch 4 | Step 2850800 | Avg Loss: 0.0162 | Grad Norm: 0.01084503\n",
      "Epoch 4 | Step 2850900 | Avg Loss: 0.0158 | Grad Norm: 0.00815736\n",
      "Epoch 4 | Step 2851000 | Avg Loss: 0.0154 | Grad Norm: 0.00863601\n",
      "Epoch 4 | Step 2851100 | Avg Loss: 0.0154 | Grad Norm: 0.00747149\n",
      "Epoch 4 | Step 2851200 | Avg Loss: 0.0154 | Grad Norm: 0.00895441\n",
      "Epoch 4 | Step 2851300 | Avg Loss: 0.0157 | Grad Norm: 0.00796947\n",
      "Epoch 4 | Step 2851400 | Avg Loss: 0.0159 | Grad Norm: 0.00898897\n",
      "Epoch 4 | Step 2851500 | Avg Loss: 0.0159 | Grad Norm: 0.01016539\n",
      "Epoch 4 | Step 2851600 | Avg Loss: 0.0161 | Grad Norm: 0.00930463\n",
      "Epoch 4 | Step 2851700 | Avg Loss: 0.0159 | Grad Norm: 0.00853950\n",
      "Epoch 4 | Step 2851800 | Avg Loss: 0.0160 | Grad Norm: 0.01058443\n",
      "Epoch 4 | Step 2851900 | Avg Loss: 0.0157 | Grad Norm: 0.00849252\n",
      "Epoch 4 | Step 2852000 | Avg Loss: 0.0156 | Grad Norm: 0.00825490\n",
      "Epoch 4 | Step 2852100 | Avg Loss: 0.0154 | Grad Norm: 0.00822702\n",
      "Epoch 4 | Step 2852200 | Avg Loss: 0.0155 | Grad Norm: 0.01329638\n",
      "Epoch 4 | Step 2852300 | Avg Loss: 0.0160 | Grad Norm: 0.00792616\n",
      "Epoch 4 | Step 2852400 | Avg Loss: 0.0155 | Grad Norm: 0.01001041\n",
      "Epoch 4 | Step 2852500 | Avg Loss: 0.0157 | Grad Norm: 0.00915290\n",
      "Epoch 4 | Step 2852600 | Avg Loss: 0.0157 | Grad Norm: 0.00889343\n",
      "Epoch 4 | Step 2852700 | Avg Loss: 0.0153 | Grad Norm: 0.00763620\n",
      "Epoch 4 | Step 2852800 | Avg Loss: 0.0153 | Grad Norm: 0.00863062\n",
      "Epoch 4 | Step 2852900 | Avg Loss: 0.0150 | Grad Norm: 0.00867400\n",
      "Epoch 4 | Step 2853000 | Avg Loss: 0.0151 | Grad Norm: 0.00827319\n",
      "Epoch 4 | Step 2853100 | Avg Loss: 0.0152 | Grad Norm: 0.01007134\n",
      "Epoch 4 | Step 2853200 | Avg Loss: 0.0146 | Grad Norm: 0.00870772\n",
      "Epoch 4 | Step 2853300 | Avg Loss: 0.0147 | Grad Norm: 0.00754867\n",
      "Epoch 4 | Step 2853400 | Avg Loss: 0.0145 | Grad Norm: 0.00910175\n",
      "Epoch 4 | Step 2853500 | Avg Loss: 0.0145 | Grad Norm: 0.00765914\n",
      "Epoch 4 | Step 2853600 | Avg Loss: 0.0147 | Grad Norm: 0.00755172\n",
      "Epoch 4 | Step 2853700 | Avg Loss: 0.0145 | Grad Norm: 0.00849687\n",
      "Epoch 4 | Step 2853800 | Avg Loss: 0.0144 | Grad Norm: 0.00931914\n",
      "Epoch 4 | Step 2853900 | Avg Loss: 0.0144 | Grad Norm: 0.00775871\n",
      "Epoch 4 | Step 2854000 | Avg Loss: 0.0151 | Grad Norm: 0.00838248\n",
      "Epoch 4 | Step 2854100 | Avg Loss: 0.0151 | Grad Norm: 0.01961889\n",
      "Epoch 4 | Step 2854200 | Avg Loss: 0.0150 | Grad Norm: 0.00996025\n",
      "Epoch 4 | Step 2854300 | Avg Loss: 0.0153 | Grad Norm: 0.00913611\n",
      "Epoch 4 | Step 2854400 | Avg Loss: 0.0158 | Grad Norm: 0.00971115\n",
      "Epoch 4 | Step 2854500 | Avg Loss: 0.0157 | Grad Norm: 0.00842606\n",
      "Epoch 4 | Step 2854600 | Avg Loss: 0.0156 | Grad Norm: 0.00917324\n",
      "Epoch 4 | Step 2854700 | Avg Loss: 0.0157 | Grad Norm: 0.01073940\n",
      "Epoch 4 | Step 2854800 | Avg Loss: 0.0158 | Grad Norm: 0.00950421\n",
      "Epoch 4 | Step 2854900 | Avg Loss: 0.0157 | Grad Norm: 0.01185006\n",
      "Epoch 4 | Step 2855000 | Avg Loss: 0.0157 | Grad Norm: 0.01071629\n",
      "Epoch 4 | Step 2855100 | Avg Loss: 0.0160 | Grad Norm: 0.00874509\n",
      "Epoch 4 | Step 2855200 | Avg Loss: 0.0157 | Grad Norm: 0.00891648\n",
      "Epoch 4 | Step 2855300 | Avg Loss: 0.0160 | Grad Norm: 0.00858857\n",
      "Epoch 4 | Step 2855400 | Avg Loss: 0.0158 | Grad Norm: 0.00843656\n",
      "Epoch 4 | Step 2855500 | Avg Loss: 0.0157 | Grad Norm: 0.00896246\n",
      "Epoch 4 | Step 2855600 | Avg Loss: 0.0155 | Grad Norm: 0.00821656\n",
      "Epoch 4 | Step 2855700 | Avg Loss: 0.0159 | Grad Norm: 0.00896165\n",
      "Epoch 4 | Step 2855800 | Avg Loss: 0.0157 | Grad Norm: 0.00959310\n",
      "Epoch 4 | Step 2855900 | Avg Loss: 0.0157 | Grad Norm: 0.00812684\n",
      "Epoch 4 | Step 2856000 | Avg Loss: 0.0155 | Grad Norm: 0.01027887\n",
      "Epoch 4 | Step 2856100 | Avg Loss: 0.0163 | Grad Norm: 0.01337429\n",
      "Epoch 4 | Step 2856200 | Avg Loss: 0.0161 | Grad Norm: 0.00882135\n",
      "Epoch 4 | Step 2856300 | Avg Loss: 0.0159 | Grad Norm: 0.01169888\n",
      "Epoch 4 | Step 2856400 | Avg Loss: 0.0158 | Grad Norm: 0.00969462\n",
      "Epoch 4 | Step 2856500 | Avg Loss: 0.0153 | Grad Norm: 0.00996925\n",
      "Epoch 4 | Step 2856600 | Avg Loss: 0.0152 | Grad Norm: 0.00847760\n",
      "Epoch 4 | Step 2856700 | Avg Loss: 0.0157 | Grad Norm: 0.00807062\n",
      "Epoch 4 | Step 2856800 | Avg Loss: 0.0158 | Grad Norm: 0.01140647\n",
      "Epoch 4 | Step 2856900 | Avg Loss: 0.0156 | Grad Norm: 0.00983263\n",
      "Epoch 4 | Step 2857000 | Avg Loss: 0.0154 | Grad Norm: 0.01108009\n",
      "Epoch 4 | Step 2857100 | Avg Loss: 0.0153 | Grad Norm: 0.00895291\n",
      "Epoch 4 | Step 2857200 | Avg Loss: 0.0152 | Grad Norm: 0.00871516\n",
      "Epoch 4 | Step 2857300 | Avg Loss: 0.0151 | Grad Norm: 0.00844451\n",
      "Epoch 4 | Step 2857400 | Avg Loss: 0.0151 | Grad Norm: 0.00889474\n",
      "Epoch 4 | Step 2857500 | Avg Loss: 0.0154 | Grad Norm: 0.00847321\n",
      "Epoch 4 | Step 2857600 | Avg Loss: 0.0156 | Grad Norm: 0.00778022\n",
      "Epoch 4 | Step 2857700 | Avg Loss: 0.0159 | Grad Norm: 0.00893761\n",
      "Epoch 4 | Step 2857800 | Avg Loss: 0.0156 | Grad Norm: 0.00893857\n",
      "Epoch 4 | Step 2857900 | Avg Loss: 0.0156 | Grad Norm: 0.00760719\n",
      "Epoch 4 | Step 2858000 | Avg Loss: 0.0156 | Grad Norm: 0.01018584\n",
      "Epoch 4 | Step 2858100 | Avg Loss: 0.0161 | Grad Norm: 0.01182984\n",
      "Epoch 4 | Step 2858200 | Avg Loss: 0.0160 | Grad Norm: 0.01073409\n",
      "Epoch 4 | Step 2858300 | Avg Loss: 0.0158 | Grad Norm: 0.00866916\n",
      "Epoch 4 | Step 2858400 | Avg Loss: 0.0161 | Grad Norm: 0.00904896\n",
      "Epoch 4 | Step 2858500 | Avg Loss: 0.0160 | Grad Norm: 0.00903373\n",
      "Epoch 4 | Step 2858600 | Avg Loss: 0.0162 | Grad Norm: 0.01208698\n",
      "Epoch 4 | Step 2858700 | Avg Loss: 0.0160 | Grad Norm: 0.00935104\n",
      "Epoch 4 | Step 2858800 | Avg Loss: 0.0160 | Grad Norm: 0.01040346\n",
      "Epoch 4 | Step 2858900 | Avg Loss: 0.0159 | Grad Norm: 0.00916835\n",
      "Epoch 4 | Step 2859000 | Avg Loss: 0.0161 | Grad Norm: 0.00982205\n",
      "Epoch 4 | Step 2859100 | Avg Loss: 0.0156 | Grad Norm: 0.01071267\n",
      "Epoch 4 | Step 2859200 | Avg Loss: 0.0158 | Grad Norm: 0.00925199\n",
      "Epoch 4 | Step 2859300 | Avg Loss: 0.0160 | Grad Norm: 0.00905521\n",
      "Epoch 4 | Step 2859400 | Avg Loss: 0.0160 | Grad Norm: 0.00829789\n",
      "Epoch 4 | Step 2859500 | Avg Loss: 0.0159 | Grad Norm: 0.01033319\n",
      "Epoch 4 | Step 2859600 | Avg Loss: 0.0160 | Grad Norm: 0.00883673\n",
      "Epoch 4 | Step 2859700 | Avg Loss: 0.0161 | Grad Norm: 0.00793887\n",
      "Epoch 4 | Step 2859800 | Avg Loss: 0.0156 | Grad Norm: 0.00927806\n",
      "Epoch 4 | Step 2859900 | Avg Loss: 0.0157 | Grad Norm: 0.00827762\n",
      "Epoch 4 | Step 2860000 | Avg Loss: 0.0158 | Grad Norm: 0.00894559\n",
      "Epoch 4 | Step 2860100 | Avg Loss: 0.0158 | Grad Norm: 0.00976198\n",
      "Epoch 4 | Step 2860200 | Avg Loss: 0.0156 | Grad Norm: 0.00939740\n",
      "Epoch 4 | Step 2860300 | Avg Loss: 0.0159 | Grad Norm: 0.00834479\n",
      "Epoch 4 | Step 2860400 | Avg Loss: 0.0160 | Grad Norm: 0.00903801\n",
      "Epoch 4 | Step 2860500 | Avg Loss: 0.0156 | Grad Norm: 0.01193407\n",
      "Epoch 4 | Step 2860600 | Avg Loss: 0.0159 | Grad Norm: 0.00842893\n",
      "Epoch 4 | Step 2860700 | Avg Loss: 0.0158 | Grad Norm: 0.01004136\n",
      "Epoch 4 | Step 2860800 | Avg Loss: 0.0158 | Grad Norm: 0.00976434\n",
      "Epoch 4 | Step 2860900 | Avg Loss: 0.0153 | Grad Norm: 0.00942899\n",
      "Epoch 4 | Step 2861000 | Avg Loss: 0.0156 | Grad Norm: 0.00938470\n",
      "Epoch 4 | Step 2861100 | Avg Loss: 0.0153 | Grad Norm: 0.01025958\n",
      "Epoch 4 | Step 2861200 | Avg Loss: 0.0150 | Grad Norm: 0.00987703\n",
      "Epoch 4 | Step 2861300 | Avg Loss: 0.0153 | Grad Norm: 0.00768252\n",
      "Epoch 4 | Step 2861400 | Avg Loss: 0.0155 | Grad Norm: 0.00857808\n",
      "Epoch 4 | Step 2861500 | Avg Loss: 0.0153 | Grad Norm: 0.00887850\n",
      "Epoch 4 | Step 2861600 | Avg Loss: 0.0150 | Grad Norm: 0.00815188\n",
      "Epoch 4 | Step 2861700 | Avg Loss: 0.0150 | Grad Norm: 0.00931562\n",
      "Epoch 4 | Step 2861800 | Avg Loss: 0.0151 | Grad Norm: 0.00849225\n",
      "Epoch 4 | Step 2861900 | Avg Loss: 0.0154 | Grad Norm: 0.00798436\n",
      "Epoch 4 | Step 2862000 | Avg Loss: 0.0157 | Grad Norm: 0.00907673\n",
      "Epoch 4 | Step 2862100 | Avg Loss: 0.0158 | Grad Norm: 0.01003971\n",
      "Epoch 4 | Step 2862200 | Avg Loss: 0.0157 | Grad Norm: 0.00842266\n",
      "Epoch 4 | Step 2862300 | Avg Loss: 0.0157 | Grad Norm: 0.00875888\n",
      "Epoch 4 | Step 2862400 | Avg Loss: 0.0154 | Grad Norm: 0.00788425\n",
      "Epoch 4 | Step 2862500 | Avg Loss: 0.0155 | Grad Norm: 0.00891677\n",
      "Epoch 4 | Step 2862600 | Avg Loss: 0.0152 | Grad Norm: 0.01166394\n",
      "Epoch 4 | Step 2862700 | Avg Loss: 0.0154 | Grad Norm: 0.00887883\n",
      "Epoch 4 | Step 2862800 | Avg Loss: 0.0155 | Grad Norm: 0.00800057\n",
      "Epoch 4 | Step 2862900 | Avg Loss: 0.0153 | Grad Norm: 0.01061284\n",
      "Epoch 4 | Step 2863000 | Avg Loss: 0.0151 | Grad Norm: 0.00898516\n",
      "Epoch 4 | Step 2863100 | Avg Loss: 0.0153 | Grad Norm: 0.00842033\n",
      "Epoch 4 | Step 2863200 | Avg Loss: 0.0151 | Grad Norm: 0.01435442\n",
      "Epoch 4 | Step 2863300 | Avg Loss: 0.0152 | Grad Norm: 0.00786944\n",
      "Epoch 4 | Step 2863400 | Avg Loss: 0.0147 | Grad Norm: 0.00911293\n",
      "Epoch 4 | Step 2863500 | Avg Loss: 0.0149 | Grad Norm: 0.00751150\n",
      "Epoch 4 | Step 2863600 | Avg Loss: 0.0152 | Grad Norm: 0.00833555\n",
      "Epoch 4 | Step 2863700 | Avg Loss: 0.0154 | Grad Norm: 0.00943921\n",
      "Epoch 4 | Step 2863800 | Avg Loss: 0.0156 | Grad Norm: 0.00813694\n",
      "Epoch 4 | Step 2863900 | Avg Loss: 0.0155 | Grad Norm: 0.00924292\n",
      "Epoch 4 | Step 2864000 | Avg Loss: 0.0155 | Grad Norm: 0.01053682\n",
      "Epoch 4 | Step 2864100 | Avg Loss: 0.0154 | Grad Norm: 0.00891861\n",
      "Epoch 4 | Step 2864200 | Avg Loss: 0.0154 | Grad Norm: 0.00939736\n",
      "Epoch 4 | Step 2864300 | Avg Loss: 0.0156 | Grad Norm: 0.00772717\n",
      "Epoch 4 | Step 2864400 | Avg Loss: 0.0156 | Grad Norm: 0.00826423\n",
      "Epoch 4 | Step 2864500 | Avg Loss: 0.0160 | Grad Norm: 0.00965250\n",
      "Epoch 4 | Step 2864600 | Avg Loss: 0.0161 | Grad Norm: 0.00851783\n",
      "Epoch 4 | Step 2864700 | Avg Loss: 0.0155 | Grad Norm: 0.00999037\n",
      "Epoch 4 | Step 2864800 | Avg Loss: 0.0151 | Grad Norm: 0.00893464\n",
      "Epoch 4 | Step 2864900 | Avg Loss: 0.0149 | Grad Norm: 0.00908571\n",
      "Epoch 4 | Step 2865000 | Avg Loss: 0.0152 | Grad Norm: 0.00851384\n",
      "Epoch 4 | Step 2865100 | Avg Loss: 0.0151 | Grad Norm: 0.00886825\n",
      "Epoch 4 | Step 2865200 | Avg Loss: 0.0151 | Grad Norm: 0.00923291\n",
      "Epoch 4 | Step 2865300 | Avg Loss: 0.0152 | Grad Norm: 0.00905975\n",
      "Epoch 4 | Step 2865400 | Avg Loss: 0.0157 | Grad Norm: 0.01076659\n",
      "Epoch 4 | Step 2865500 | Avg Loss: 0.0156 | Grad Norm: 0.00854288\n",
      "Epoch 4 | Step 2865600 | Avg Loss: 0.0156 | Grad Norm: 0.00993169\n",
      "Epoch 4 | Step 2865700 | Avg Loss: 0.0157 | Grad Norm: 0.01015777\n",
      "Epoch 4 | Step 2865800 | Avg Loss: 0.0160 | Grad Norm: 0.00834676\n",
      "Epoch 4 | Step 2865900 | Avg Loss: 0.0154 | Grad Norm: 0.00887663\n",
      "Epoch 4 | Step 2866000 | Avg Loss: 0.0150 | Grad Norm: 0.00897914\n",
      "Epoch 4 | Step 2866100 | Avg Loss: 0.0151 | Grad Norm: 0.00773696\n",
      "Epoch 4 | Step 2866200 | Avg Loss: 0.0155 | Grad Norm: 0.00777484\n",
      "Epoch 4 | Step 2866300 | Avg Loss: 0.0154 | Grad Norm: 0.00929724\n",
      "Epoch 4 | Step 2866400 | Avg Loss: 0.0156 | Grad Norm: 0.01029134\n",
      "Epoch 4 | Step 2866500 | Avg Loss: 0.0153 | Grad Norm: 0.00916452\n",
      "Epoch 4 | Step 2866600 | Avg Loss: 0.0153 | Grad Norm: 0.00902559\n",
      "Epoch 4 | Step 2866700 | Avg Loss: 0.0156 | Grad Norm: 0.01043729\n",
      "Epoch 4 | Step 2866800 | Avg Loss: 0.0157 | Grad Norm: 0.00766945\n",
      "Epoch 4 | Step 2866900 | Avg Loss: 0.0154 | Grad Norm: 0.00898927\n",
      "Epoch 4 | Step 2867000 | Avg Loss: 0.0157 | Grad Norm: 0.00794478\n",
      "Epoch 4 | Step 2867100 | Avg Loss: 0.0156 | Grad Norm: 0.00861938\n",
      "Epoch 4 | Step 2867200 | Avg Loss: 0.0159 | Grad Norm: 0.00969102\n",
      "Epoch 4 | Step 2867300 | Avg Loss: 0.0159 | Grad Norm: 0.00935595\n",
      "Epoch 4 | Step 2867400 | Avg Loss: 0.0157 | Grad Norm: 0.01000456\n",
      "Epoch 4 | Step 2867500 | Avg Loss: 0.0155 | Grad Norm: 0.00831903\n",
      "Epoch 4 | Step 2867600 | Avg Loss: 0.0151 | Grad Norm: 0.01035407\n",
      "Epoch 4 | Step 2867700 | Avg Loss: 0.0152 | Grad Norm: 0.00935830\n",
      "Epoch 4 | Step 2867800 | Avg Loss: 0.0151 | Grad Norm: 0.00872172\n",
      "Epoch 4 | Step 2867900 | Avg Loss: 0.0149 | Grad Norm: 0.00774736\n",
      "Epoch 4 | Step 2868000 | Avg Loss: 0.0150 | Grad Norm: 0.00834537\n",
      "Epoch 4 | Step 2868100 | Avg Loss: 0.0155 | Grad Norm: 0.00923280\n",
      "Epoch 4 | Step 2868200 | Avg Loss: 0.0155 | Grad Norm: 0.00897775\n",
      "Epoch 4 | Step 2868300 | Avg Loss: 0.0152 | Grad Norm: 0.00845472\n",
      "Epoch 4 | Step 2868400 | Avg Loss: 0.0153 | Grad Norm: 0.00849359\n",
      "Epoch 4 | Step 2868500 | Avg Loss: 0.0154 | Grad Norm: 0.00975594\n",
      "Epoch 4 | Step 2868600 | Avg Loss: 0.0153 | Grad Norm: 0.00886408\n",
      "Epoch 4 | Step 2868700 | Avg Loss: 0.0154 | Grad Norm: 0.00900316\n",
      "Epoch 4 | Step 2868800 | Avg Loss: 0.0155 | Grad Norm: 0.00989707\n",
      "Epoch 4 | Step 2868900 | Avg Loss: 0.0158 | Grad Norm: 0.00917124\n",
      "Epoch 4 | Step 2869000 | Avg Loss: 0.0157 | Grad Norm: 0.00876622\n",
      "Epoch 4 | Step 2869100 | Avg Loss: 0.0156 | Grad Norm: 0.00948573\n",
      "Epoch 4 | Step 2869200 | Avg Loss: 0.0158 | Grad Norm: 0.00842130\n",
      "Epoch 4 | Step 2869300 | Avg Loss: 0.0160 | Grad Norm: 0.00854189\n",
      "Epoch 4 | Step 2869400 | Avg Loss: 0.0159 | Grad Norm: 0.00911770\n",
      "Epoch 4 | Step 2869500 | Avg Loss: 0.0158 | Grad Norm: 0.00977255\n",
      "Epoch 4 | Step 2869600 | Avg Loss: 0.0156 | Grad Norm: 0.00937793\n",
      "Epoch 4 | Step 2869700 | Avg Loss: 0.0151 | Grad Norm: 0.00951031\n",
      "Epoch 4 | Step 2869800 | Avg Loss: 0.0153 | Grad Norm: 0.00856263\n",
      "Epoch 4 | Step 2869900 | Avg Loss: 0.0152 | Grad Norm: 0.01140765\n",
      "Epoch 4 | Step 2870000 | Avg Loss: 0.0152 | Grad Norm: 0.00816995\n",
      "Epoch 4 | Step 2870100 | Avg Loss: 0.0151 | Grad Norm: 0.00939356\n",
      "Epoch 4 | Step 2870200 | Avg Loss: 0.0150 | Grad Norm: 0.00878762\n",
      "Epoch 4 | Step 2870300 | Avg Loss: 0.0152 | Grad Norm: 0.00857874\n",
      "Epoch 4 | Step 2870400 | Avg Loss: 0.0152 | Grad Norm: 0.00880306\n",
      "Epoch 4 | Step 2870500 | Avg Loss: 0.0153 | Grad Norm: 0.00878980\n",
      "Epoch 4 | Step 2870600 | Avg Loss: 0.0152 | Grad Norm: 0.00981156\n",
      "Epoch 4 | Step 2870700 | Avg Loss: 0.0155 | Grad Norm: 0.00897603\n",
      "Epoch 4 | Step 2870800 | Avg Loss: 0.0156 | Grad Norm: 0.00862590\n",
      "Epoch 4 | Step 2870900 | Avg Loss: 0.0156 | Grad Norm: 0.00945811\n",
      "Epoch 4 | Step 2871000 | Avg Loss: 0.0159 | Grad Norm: 0.00947734\n",
      "Epoch 4 | Step 2871100 | Avg Loss: 0.0159 | Grad Norm: 0.01180242\n",
      "Epoch 4 | Step 2871200 | Avg Loss: 0.0160 | Grad Norm: 0.00875649\n",
      "Epoch 4 | Step 2871300 | Avg Loss: 0.0161 | Grad Norm: 0.00904971\n",
      "Epoch 4 | Step 2871400 | Avg Loss: 0.0160 | Grad Norm: 0.01071220\n",
      "Epoch 4 | Step 2871500 | Avg Loss: 0.0162 | Grad Norm: 0.00831088\n",
      "Epoch 4 | Step 2871600 | Avg Loss: 0.0158 | Grad Norm: 0.01114438\n",
      "Epoch 4 | Step 2871700 | Avg Loss: 0.0154 | Grad Norm: 0.00943364\n",
      "Epoch 4 | Step 2871800 | Avg Loss: 0.0155 | Grad Norm: 0.00866729\n",
      "Epoch 4 | Step 2871900 | Avg Loss: 0.0152 | Grad Norm: 0.00863428\n",
      "Epoch 4 | Step 2872000 | Avg Loss: 0.0148 | Grad Norm: 0.00847527\n",
      "Epoch 4 | Step 2872100 | Avg Loss: 0.0149 | Grad Norm: 0.00844711\n",
      "Epoch 4 | Step 2872200 | Avg Loss: 0.0149 | Grad Norm: 0.00862095\n",
      "Epoch 4 | Step 2872300 | Avg Loss: 0.0151 | Grad Norm: 0.00844017\n",
      "Epoch 4 | Step 2872400 | Avg Loss: 0.0152 | Grad Norm: 0.00860718\n",
      "Epoch 4 | Step 2872500 | Avg Loss: 0.0150 | Grad Norm: 0.00944240\n",
      "Epoch 4 | Step 2872600 | Avg Loss: 0.0155 | Grad Norm: 0.00863423\n",
      "Epoch 4 | Step 2872700 | Avg Loss: 0.0154 | Grad Norm: 0.00871297\n",
      "Epoch 4 | Step 2872800 | Avg Loss: 0.0156 | Grad Norm: 0.00883434\n",
      "Epoch 4 | Step 2872900 | Avg Loss: 0.0157 | Grad Norm: 0.00844367\n",
      "Epoch 4 | Step 2873000 | Avg Loss: 0.0158 | Grad Norm: 0.00898465\n",
      "Epoch 4 | Step 2873100 | Avg Loss: 0.0158 | Grad Norm: 0.01080375\n",
      "Epoch 4 | Step 2873200 | Avg Loss: 0.0158 | Grad Norm: 0.01060109\n",
      "Epoch 4 | Step 2873300 | Avg Loss: 0.0158 | Grad Norm: 0.00980019\n",
      "Epoch 4 | Step 2873400 | Avg Loss: 0.0155 | Grad Norm: 0.00915646\n",
      "Epoch 4 | Step 2873500 | Avg Loss: 0.0158 | Grad Norm: 0.00769470\n",
      "Epoch 4 | Step 2873600 | Avg Loss: 0.0156 | Grad Norm: 0.01105095\n",
      "Epoch 4 | Step 2873700 | Avg Loss: 0.0153 | Grad Norm: 0.00980198\n",
      "Epoch 4 | Step 2873800 | Avg Loss: 0.0157 | Grad Norm: 0.00935279\n",
      "Epoch 4 | Step 2873900 | Avg Loss: 0.0157 | Grad Norm: 0.00878448\n",
      "Epoch 4 | Step 2874000 | Avg Loss: 0.0157 | Grad Norm: 0.00891377\n",
      "Epoch 4 | Step 2874100 | Avg Loss: 0.0155 | Grad Norm: 0.01180627\n",
      "Epoch 4 | Step 2874200 | Avg Loss: 0.0153 | Grad Norm: 0.01060083\n",
      "Epoch 4 | Step 2874300 | Avg Loss: 0.0154 | Grad Norm: 0.00928288\n",
      "Epoch 4 | Step 2874400 | Avg Loss: 0.0156 | Grad Norm: 0.00932762\n",
      "Epoch 4 | Step 2874500 | Avg Loss: 0.0153 | Grad Norm: 0.00897196\n",
      "Epoch 4 | Step 2874600 | Avg Loss: 0.0153 | Grad Norm: 0.00873134\n",
      "Epoch 4 | Step 2874700 | Avg Loss: 0.0149 | Grad Norm: 0.00813221\n",
      "Epoch 4 | Step 2874800 | Avg Loss: 0.0151 | Grad Norm: 0.00890020\n",
      "Epoch 4 | Step 2874900 | Avg Loss: 0.0147 | Grad Norm: 0.00833307\n",
      "Epoch 4 | Step 2875000 | Avg Loss: 0.0150 | Grad Norm: 0.00800474\n",
      "Epoch 4 | Step 2875100 | Avg Loss: 0.0153 | Grad Norm: 0.00837250\n",
      "Epoch 4 | Step 2875200 | Avg Loss: 0.0149 | Grad Norm: 0.00892648\n",
      "Epoch 4 | Step 2875300 | Avg Loss: 0.0150 | Grad Norm: 0.00817473\n",
      "Epoch 4 | Step 2875400 | Avg Loss: 0.0150 | Grad Norm: 0.00844858\n",
      "Epoch 4 | Step 2875500 | Avg Loss: 0.0153 | Grad Norm: 0.00866542\n",
      "Epoch 4 | Step 2875600 | Avg Loss: 0.0154 | Grad Norm: 0.00892425\n",
      "Epoch 4 | Step 2875700 | Avg Loss: 0.0155 | Grad Norm: 0.00781462\n",
      "Epoch 4 | Step 2875800 | Avg Loss: 0.0158 | Grad Norm: 0.00956362\n",
      "Epoch 4 | Step 2875900 | Avg Loss: 0.0156 | Grad Norm: 0.01061856\n",
      "Epoch 4 | Step 2876000 | Avg Loss: 0.0160 | Grad Norm: 0.00969309\n",
      "Epoch 4 | Step 2876100 | Avg Loss: 0.0156 | Grad Norm: 0.00934234\n",
      "Epoch 4 | Step 2876200 | Avg Loss: 0.0155 | Grad Norm: 0.00770898\n",
      "Epoch 4 | Step 2876300 | Avg Loss: 0.0154 | Grad Norm: 0.00952919\n",
      "Epoch 4 | Step 2876400 | Avg Loss: 0.0154 | Grad Norm: 0.00759646\n",
      "Epoch 4 | Step 2876500 | Avg Loss: 0.0154 | Grad Norm: 0.00821669\n",
      "Epoch 4 | Step 2876600 | Avg Loss: 0.0148 | Grad Norm: 0.01009474\n",
      "Epoch 4 | Step 2876700 | Avg Loss: 0.0148 | Grad Norm: 0.00827579\n",
      "Epoch 4 | Step 2876800 | Avg Loss: 0.0146 | Grad Norm: 0.00849422\n",
      "Epoch 4 | Step 2876900 | Avg Loss: 0.0149 | Grad Norm: 0.00818929\n",
      "Epoch 4 | Step 2877000 | Avg Loss: 0.0151 | Grad Norm: 0.00872045\n",
      "Epoch 4 | Step 2877100 | Avg Loss: 0.0154 | Grad Norm: 0.00781118\n",
      "Epoch 4 | Step 2877200 | Avg Loss: 0.0153 | Grad Norm: 0.00879789\n",
      "Epoch 4 | Step 2877300 | Avg Loss: 0.0155 | Grad Norm: 0.00958032\n",
      "Epoch 4 | Step 2877400 | Avg Loss: 0.0151 | Grad Norm: 0.01423875\n",
      "Epoch 4 | Step 2877500 | Avg Loss: 0.0152 | Grad Norm: 0.00868970\n",
      "Epoch 4 | Step 2877600 | Avg Loss: 0.0155 | Grad Norm: 0.00958540\n",
      "Epoch 4 | Step 2877700 | Avg Loss: 0.0151 | Grad Norm: 0.00883992\n",
      "Epoch 4 | Step 2877800 | Avg Loss: 0.0153 | Grad Norm: 0.00902553\n",
      "Epoch 4 | Step 2877900 | Avg Loss: 0.0151 | Grad Norm: 0.00863450\n",
      "Epoch 4 | Step 2878000 | Avg Loss: 0.0150 | Grad Norm: 0.00764742\n",
      "Epoch 4 | Step 2878100 | Avg Loss: 0.0153 | Grad Norm: 0.01037016\n",
      "Epoch 4 | Step 2878200 | Avg Loss: 0.0152 | Grad Norm: 0.00869521\n",
      "Epoch 4 | Step 2878300 | Avg Loss: 0.0153 | Grad Norm: 0.00970318\n",
      "Epoch 4 | Step 2878400 | Avg Loss: 0.0153 | Grad Norm: 0.00844619\n",
      "Epoch 4 | Step 2878500 | Avg Loss: 0.0151 | Grad Norm: 0.00773611\n",
      "Epoch 4 | Step 2878600 | Avg Loss: 0.0149 | Grad Norm: 0.01033766\n",
      "Epoch 4 | Step 2878700 | Avg Loss: 0.0153 | Grad Norm: 0.00923485\n",
      "Epoch 4 | Step 2878800 | Avg Loss: 0.0153 | Grad Norm: 0.01029422\n",
      "Epoch 4 | Step 2878900 | Avg Loss: 0.0152 | Grad Norm: 0.01224441\n",
      "Epoch 4 | Step 2879000 | Avg Loss: 0.0156 | Grad Norm: 0.01007923\n",
      "Epoch 4 | Step 2879100 | Avg Loss: 0.0150 | Grad Norm: 0.00882987\n",
      "Epoch 4 | Step 2879200 | Avg Loss: 0.0148 | Grad Norm: 0.01063089\n",
      "Epoch 4 | Step 2879300 | Avg Loss: 0.0148 | Grad Norm: 0.00854874\n",
      "Epoch 4 | Step 2879400 | Avg Loss: 0.0149 | Grad Norm: 0.00817571\n",
      "Epoch 4 | Step 2879500 | Avg Loss: 0.0150 | Grad Norm: 0.00768363\n",
      "Epoch 4 | Step 2879600 | Avg Loss: 0.0151 | Grad Norm: 0.00863366\n",
      "Epoch 4 | Step 2879700 | Avg Loss: 0.0152 | Grad Norm: 0.01140833\n",
      "Epoch 4 | Step 2879800 | Avg Loss: 0.0150 | Grad Norm: 0.01071894\n",
      "Epoch 4 | Step 2879900 | Avg Loss: 0.0148 | Grad Norm: 0.00844478\n",
      "Epoch 4 | Step 2880000 | Avg Loss: 0.0152 | Grad Norm: 0.00903684\n",
      "Epoch 4 | Step 2880100 | Avg Loss: 0.0150 | Grad Norm: 0.00907050\n",
      "Epoch 4 | Step 2880200 | Avg Loss: 0.0151 | Grad Norm: 0.00970766\n",
      "Epoch 4 | Step 2880300 | Avg Loss: 0.0154 | Grad Norm: 0.00858459\n",
      "Epoch 4 | Step 2880400 | Avg Loss: 0.0150 | Grad Norm: 0.00851698\n",
      "Epoch 4 | Step 2880500 | Avg Loss: 0.0150 | Grad Norm: 0.01040576\n",
      "Epoch 4 | Step 2880600 | Avg Loss: 0.0146 | Grad Norm: 0.00900500\n",
      "Epoch 4 | Step 2880700 | Avg Loss: 0.0146 | Grad Norm: 0.00809041\n",
      "Epoch 4 | Step 2880800 | Avg Loss: 0.0146 | Grad Norm: 0.00772416\n",
      "Epoch 4 | Step 2880900 | Avg Loss: 0.0148 | Grad Norm: 0.00900708\n",
      "Epoch 4 | Step 2881000 | Avg Loss: 0.0147 | Grad Norm: 0.00831491\n",
      "Epoch 4 | Step 2881100 | Avg Loss: 0.0149 | Grad Norm: 0.00880923\n",
      "Epoch 4 | Step 2881200 | Avg Loss: 0.0151 | Grad Norm: 0.01060153\n",
      "Epoch 4 | Step 2881300 | Avg Loss: 0.0150 | Grad Norm: 0.00804603\n",
      "Epoch 4 | Step 2881400 | Avg Loss: 0.0152 | Grad Norm: 0.00958264\n",
      "Epoch 4 | Step 2881500 | Avg Loss: 0.0152 | Grad Norm: 0.00929104\n",
      "Epoch 4 | Step 2881600 | Avg Loss: 0.0153 | Grad Norm: 0.00966495\n",
      "Epoch 4 | Step 2881700 | Avg Loss: 0.0155 | Grad Norm: 0.01113464\n",
      "Epoch 4 | Step 2881800 | Avg Loss: 0.0160 | Grad Norm: 0.00835912\n",
      "Epoch 4 | Step 2881900 | Avg Loss: 0.0156 | Grad Norm: 0.00785531\n",
      "Epoch 4 | Step 2882000 | Avg Loss: 0.0155 | Grad Norm: 0.00926628\n",
      "Epoch 4 | Step 2882100 | Avg Loss: 0.0158 | Grad Norm: 0.00942946\n",
      "Epoch 4 | Step 2882200 | Avg Loss: 0.0156 | Grad Norm: 0.00902194\n",
      "Epoch 4 | Step 2882300 | Avg Loss: 0.0158 | Grad Norm: 0.00857659\n",
      "Epoch 4 | Step 2882400 | Avg Loss: 0.0158 | Grad Norm: 0.00971233\n",
      "Epoch 4 | Step 2882500 | Avg Loss: 0.0161 | Grad Norm: 0.00914806\n",
      "Epoch 4 | Step 2882600 | Avg Loss: 0.0160 | Grad Norm: 0.00886600\n",
      "Epoch 4 | Step 2882700 | Avg Loss: 0.0160 | Grad Norm: 0.00858476\n",
      "Epoch 4 | Step 2882800 | Avg Loss: 0.0159 | Grad Norm: 0.01114334\n",
      "Epoch 4 | Step 2882900 | Avg Loss: 0.0162 | Grad Norm: 0.01054827\n",
      "Epoch 4 | Step 2883000 | Avg Loss: 0.0161 | Grad Norm: 0.00811575\n",
      "Epoch 4 | Step 2883100 | Avg Loss: 0.0159 | Grad Norm: 0.01035190\n",
      "Epoch 4 | Step 2883200 | Avg Loss: 0.0158 | Grad Norm: 0.00804462\n",
      "Epoch 4 | Step 2883300 | Avg Loss: 0.0156 | Grad Norm: 0.00945307\n",
      "Epoch 4 | Step 2883400 | Avg Loss: 0.0156 | Grad Norm: 0.00902158\n",
      "Epoch 4 | Step 2883500 | Avg Loss: 0.0155 | Grad Norm: 0.00871795\n",
      "Epoch 4 | Step 2883600 | Avg Loss: 0.0152 | Grad Norm: 0.00871249\n",
      "Epoch 4 | Step 2883700 | Avg Loss: 0.0155 | Grad Norm: 0.00750225\n",
      "Epoch 4 | Step 2883800 | Avg Loss: 0.0160 | Grad Norm: 0.00998342\n",
      "Epoch 4 | Step 2883900 | Avg Loss: 0.0160 | Grad Norm: 0.00817146\n",
      "Epoch 4 | Step 2884000 | Avg Loss: 0.0160 | Grad Norm: 0.00967106\n",
      "Epoch 4 | Step 2884100 | Avg Loss: 0.0161 | Grad Norm: 0.01018142\n",
      "Epoch 4 | Step 2884200 | Avg Loss: 0.0161 | Grad Norm: 0.00827167\n",
      "Epoch 4 | Step 2884300 | Avg Loss: 0.0163 | Grad Norm: 0.01034955\n",
      "Epoch 4 | Step 2884400 | Avg Loss: 0.0162 | Grad Norm: 0.00872878\n",
      "Epoch 4 | Step 2884500 | Avg Loss: 0.0165 | Grad Norm: 0.00874019\n",
      "Epoch 4 | Step 2884600 | Avg Loss: 0.0163 | Grad Norm: 0.00824792\n",
      "Epoch 4 | Step 2884700 | Avg Loss: 0.0162 | Grad Norm: 0.00891451\n",
      "Epoch 4 | Step 2884800 | Avg Loss: 0.0161 | Grad Norm: 0.00991630\n",
      "Epoch 4 | Step 2884900 | Avg Loss: 0.0157 | Grad Norm: 0.00881756\n",
      "Epoch 4 | Step 2885000 | Avg Loss: 0.0161 | Grad Norm: 0.00972945\n",
      "Epoch 4 | Step 2885100 | Avg Loss: 0.0162 | Grad Norm: 0.01266422\n",
      "Epoch 4 | Step 2885200 | Avg Loss: 0.0163 | Grad Norm: 0.00916814\n",
      "Epoch 4 | Step 2885300 | Avg Loss: 0.0156 | Grad Norm: 0.01005221\n",
      "Epoch 4 | Step 2885400 | Avg Loss: 0.0159 | Grad Norm: 0.00925458\n",
      "Epoch 4 | Step 2885500 | Avg Loss: 0.0155 | Grad Norm: 0.00840603\n",
      "Epoch 4 | Step 2885600 | Avg Loss: 0.0156 | Grad Norm: 0.00998294\n",
      "Epoch 4 | Step 2885700 | Avg Loss: 0.0153 | Grad Norm: 0.00834973\n",
      "Epoch 4 | Step 2885800 | Avg Loss: 0.0156 | Grad Norm: 0.00935271\n",
      "Epoch 4 | Step 2885900 | Avg Loss: 0.0159 | Grad Norm: 0.01013209\n",
      "Epoch 4 | Step 2886000 | Avg Loss: 0.0157 | Grad Norm: 0.01032190\n",
      "Epoch 4 | Step 2886100 | Avg Loss: 0.0160 | Grad Norm: 0.00859255\n",
      "Epoch 4 | Step 2886200 | Avg Loss: 0.0159 | Grad Norm: 0.00811041\n",
      "Epoch 4 | Step 2886300 | Avg Loss: 0.0158 | Grad Norm: 0.00936367\n",
      "Epoch 4 | Step 2886400 | Avg Loss: 0.0159 | Grad Norm: 0.00974629\n",
      "Epoch 4 | Step 2886500 | Avg Loss: 0.0158 | Grad Norm: 0.01082320\n",
      "Epoch 4 | Step 2886600 | Avg Loss: 0.0160 | Grad Norm: 0.00959006\n",
      "Epoch 4 | Step 2886700 | Avg Loss: 0.0159 | Grad Norm: 0.00935443\n",
      "Epoch 4 | Step 2886800 | Avg Loss: 0.0157 | Grad Norm: 0.00848523\n",
      "Epoch 4 | Step 2886900 | Avg Loss: 0.0158 | Grad Norm: 0.00842349\n",
      "Epoch 4 | Step 2887000 | Avg Loss: 0.0162 | Grad Norm: 0.00956432\n",
      "Epoch 4 | Step 2887100 | Avg Loss: 0.0161 | Grad Norm: 0.01037913\n",
      "Epoch 4 | Step 2887200 | Avg Loss: 0.0159 | Grad Norm: 0.01002933\n",
      "Epoch 4 | Step 2887300 | Avg Loss: 0.0158 | Grad Norm: 0.00748962\n",
      "Epoch 4 | Step 2887400 | Avg Loss: 0.0157 | Grad Norm: 0.00957204\n",
      "Epoch 4 | Step 2887500 | Avg Loss: 0.0155 | Grad Norm: 0.00847557\n",
      "Epoch 4 | Step 2887600 | Avg Loss: 0.0154 | Grad Norm: 0.00887984\n",
      "Epoch 4 | Step 2887700 | Avg Loss: 0.0155 | Grad Norm: 0.00842481\n",
      "Epoch 4 | Step 2887800 | Avg Loss: 0.0155 | Grad Norm: 0.01160501\n",
      "Epoch 4 | Step 2887900 | Avg Loss: 0.0154 | Grad Norm: 0.00893867\n",
      "Epoch 4 | Step 2888000 | Avg Loss: 0.0157 | Grad Norm: 0.00995955\n",
      "Epoch 4 | Step 2888100 | Avg Loss: 0.0159 | Grad Norm: 0.00937284\n",
      "Epoch 4 | Step 2888200 | Avg Loss: 0.0156 | Grad Norm: 0.00851491\n",
      "Epoch 4 | Step 2888300 | Avg Loss: 0.0155 | Grad Norm: 0.00828080\n",
      "Epoch 4 | Step 2888400 | Avg Loss: 0.0155 | Grad Norm: 0.01003134\n",
      "Epoch 4 | Step 2888500 | Avg Loss: 0.0155 | Grad Norm: 0.00750877\n",
      "Epoch 4 | Step 2888600 | Avg Loss: 0.0154 | Grad Norm: 0.01363100\n",
      "Epoch 4 | Step 2888700 | Avg Loss: 0.0155 | Grad Norm: 0.00851415\n",
      "Epoch 4 | Step 2888800 | Avg Loss: 0.0154 | Grad Norm: 0.00930280\n",
      "Epoch 4 | Step 2888900 | Avg Loss: 0.0158 | Grad Norm: 0.01094989\n",
      "Epoch 4 | Step 2889000 | Avg Loss: 0.0158 | Grad Norm: 0.00805214\n",
      "Epoch 4 | Step 2889100 | Avg Loss: 0.0158 | Grad Norm: 0.00957852\n",
      "Epoch 4 | Step 2889200 | Avg Loss: 0.0159 | Grad Norm: 0.00908968\n",
      "Epoch 4 | Step 2889300 | Avg Loss: 0.0154 | Grad Norm: 0.01047018\n",
      "Epoch 4 | Step 2889400 | Avg Loss: 0.0153 | Grad Norm: 0.00847625\n",
      "Epoch 4 | Step 2889500 | Avg Loss: 0.0149 | Grad Norm: 0.00921750\n",
      "Epoch 4 | Step 2889600 | Avg Loss: 0.0146 | Grad Norm: 0.00821091\n",
      "Epoch 4 | Step 2889700 | Avg Loss: 0.0150 | Grad Norm: 0.00895620\n",
      "Epoch 4 | Step 2889800 | Avg Loss: 0.0153 | Grad Norm: 0.00902846\n",
      "Epoch 4 | Step 2889900 | Avg Loss: 0.0153 | Grad Norm: 0.00922861\n",
      "Epoch 4 | Step 2890000 | Avg Loss: 0.0154 | Grad Norm: 0.00777491\n",
      "Epoch 4 | Step 2890100 | Avg Loss: 0.0153 | Grad Norm: 0.01190080\n",
      "Epoch 4 | Step 2890200 | Avg Loss: 0.0151 | Grad Norm: 0.00755635\n",
      "Epoch 4 | Step 2890300 | Avg Loss: 0.0146 | Grad Norm: 0.01082164\n",
      "Epoch 4 | Step 2890400 | Avg Loss: 0.0151 | Grad Norm: 0.01012465\n",
      "Epoch 4 | Step 2890500 | Avg Loss: 0.0153 | Grad Norm: 0.00845703\n",
      "Epoch 4 | Step 2890600 | Avg Loss: 0.0151 | Grad Norm: 0.00970570\n",
      "Epoch 4 | Step 2890700 | Avg Loss: 0.0152 | Grad Norm: 0.00923277\n",
      "Epoch 4 | Step 2890800 | Avg Loss: 0.0152 | Grad Norm: 0.00846244\n",
      "Epoch 4 | Step 2890900 | Avg Loss: 0.0151 | Grad Norm: 0.00778795\n",
      "Epoch 4 | Step 2891000 | Avg Loss: 0.0154 | Grad Norm: 0.00921536\n",
      "Epoch 4 | Step 2891100 | Avg Loss: 0.0154 | Grad Norm: 0.00748754\n",
      "Epoch 4 | Step 2891200 | Avg Loss: 0.0155 | Grad Norm: 0.01110489\n",
      "Epoch 4 | Step 2891300 | Avg Loss: 0.0152 | Grad Norm: 0.00880357\n",
      "Epoch 4 | Step 2891400 | Avg Loss: 0.0154 | Grad Norm: 0.00859346\n",
      "Epoch 4 | Step 2891500 | Avg Loss: 0.0149 | Grad Norm: 0.00954636\n",
      "Epoch 4 | Step 2891600 | Avg Loss: 0.0153 | Grad Norm: 0.00808792\n",
      "Epoch 4 | Step 2891700 | Avg Loss: 0.0157 | Grad Norm: 0.00979309\n",
      "Epoch 4 | Step 2891800 | Avg Loss: 0.0156 | Grad Norm: 0.00877754\n",
      "Epoch 4 | Step 2891900 | Avg Loss: 0.0156 | Grad Norm: 0.00885839\n",
      "Epoch 4 | Step 2892000 | Avg Loss: 0.0154 | Grad Norm: 0.00834428\n",
      "Epoch 4 | Step 2892100 | Avg Loss: 0.0156 | Grad Norm: 0.00835582\n",
      "Epoch 4 | Step 2892200 | Avg Loss: 0.0156 | Grad Norm: 0.00962388\n",
      "Epoch 4 | Step 2892300 | Avg Loss: 0.0153 | Grad Norm: 0.01000844\n",
      "Epoch 4 | Step 2892400 | Avg Loss: 0.0154 | Grad Norm: 0.00882120\n",
      "Epoch 4 | Step 2892500 | Avg Loss: 0.0158 | Grad Norm: 0.01050269\n",
      "Epoch 4 | Step 2892600 | Avg Loss: 0.0160 | Grad Norm: 0.01091810\n",
      "Epoch 4 | Step 2892700 | Avg Loss: 0.0157 | Grad Norm: 0.00874495\n",
      "Epoch 4 | Step 2892800 | Avg Loss: 0.0160 | Grad Norm: 0.00939525\n",
      "Epoch 4 | Step 2892900 | Avg Loss: 0.0160 | Grad Norm: 0.00897022\n",
      "Epoch 4 | Step 2893000 | Avg Loss: 0.0159 | Grad Norm: 0.01070385\n",
      "Epoch 4 | Step 2893100 | Avg Loss: 0.0159 | Grad Norm: 0.00938987\n",
      "Epoch 4 | Step 2893200 | Avg Loss: 0.0158 | Grad Norm: 0.00908886\n",
      "Epoch 4 | Step 2893300 | Avg Loss: 0.0158 | Grad Norm: 0.00926824\n",
      "Epoch 4 | Step 2893400 | Avg Loss: 0.0156 | Grad Norm: 0.00997560\n",
      "Epoch 4 | Step 2893500 | Avg Loss: 0.0154 | Grad Norm: 0.00954646\n",
      "Epoch 4 | Step 2893600 | Avg Loss: 0.0154 | Grad Norm: 0.00996581\n",
      "Epoch 4 | Step 2893700 | Avg Loss: 0.0154 | Grad Norm: 0.01099852\n",
      "Epoch 4 | Step 2893800 | Avg Loss: 0.0154 | Grad Norm: 0.00745713\n",
      "Epoch 4 | Step 2893900 | Avg Loss: 0.0153 | Grad Norm: 0.01623677\n",
      "Epoch 4 | Step 2894000 | Avg Loss: 0.0151 | Grad Norm: 0.00868753\n",
      "Epoch 4 | Step 2894100 | Avg Loss: 0.0150 | Grad Norm: 0.00806302\n",
      "Epoch 4 | Step 2894200 | Avg Loss: 0.0153 | Grad Norm: 0.00866268\n",
      "Epoch 4 | Step 2894300 | Avg Loss: 0.0155 | Grad Norm: 0.00955901\n",
      "Epoch 4 | Step 2894400 | Avg Loss: 0.0159 | Grad Norm: 0.01015623\n",
      "Epoch 4 | Step 2894500 | Avg Loss: 0.0156 | Grad Norm: 0.00942124\n",
      "Epoch 4 | Step 2894600 | Avg Loss: 0.0156 | Grad Norm: 0.01017017\n",
      "Epoch 4 | Step 2894700 | Avg Loss: 0.0158 | Grad Norm: 0.01099697\n",
      "Epoch 4 | Step 2894800 | Avg Loss: 0.0156 | Grad Norm: 0.01029282\n",
      "Epoch 4 | Step 2894900 | Avg Loss: 0.0155 | Grad Norm: 0.01081786\n",
      "Epoch 4 | Step 2895000 | Avg Loss: 0.0154 | Grad Norm: 0.00975145\n",
      "Epoch 4 | Step 2895100 | Avg Loss: 0.0155 | Grad Norm: 0.00912592\n",
      "Epoch 4 | Step 2895200 | Avg Loss: 0.0155 | Grad Norm: 0.00850965\n",
      "Epoch 4 | Step 2895300 | Avg Loss: 0.0154 | Grad Norm: 0.00894957\n",
      "Epoch 4 | Step 2895400 | Avg Loss: 0.0155 | Grad Norm: 0.01134209\n",
      "Epoch 4 | Step 2895500 | Avg Loss: 0.0156 | Grad Norm: 0.01054041\n",
      "Epoch 4 | Step 2895600 | Avg Loss: 0.0157 | Grad Norm: 0.00835267\n",
      "Epoch 4 | Step 2895700 | Avg Loss: 0.0157 | Grad Norm: 0.00866218\n",
      "Epoch 4 | Step 2895800 | Avg Loss: 0.0155 | Grad Norm: 0.00907346\n",
      "Epoch 4 | Step 2895900 | Avg Loss: 0.0154 | Grad Norm: 0.00873888\n",
      "Epoch 4 | Step 2896000 | Avg Loss: 0.0156 | Grad Norm: 0.00938981\n",
      "Epoch 4 | Step 2896100 | Avg Loss: 0.0154 | Grad Norm: 0.00962210\n",
      "Epoch 4 | Step 2896200 | Avg Loss: 0.0151 | Grad Norm: 0.01156831\n",
      "Epoch 4 | Step 2896300 | Avg Loss: 0.0153 | Grad Norm: 0.00916268\n",
      "Epoch 4 | Step 2896400 | Avg Loss: 0.0154 | Grad Norm: 0.00933693\n",
      "Epoch 4 | Step 2896500 | Avg Loss: 0.0158 | Grad Norm: 0.01119472\n",
      "Epoch 4 | Step 2896600 | Avg Loss: 0.0158 | Grad Norm: 0.00927772\n",
      "Epoch 4 | Step 2896700 | Avg Loss: 0.0156 | Grad Norm: 0.01026533\n",
      "Epoch 4 | Step 2896800 | Avg Loss: 0.0152 | Grad Norm: 0.00987934\n",
      "Epoch 4 | Step 2896900 | Avg Loss: 0.0154 | Grad Norm: 0.01038735\n",
      "Epoch 4 | Step 2897000 | Avg Loss: 0.0153 | Grad Norm: 0.00887172\n",
      "Epoch 4 | Step 2897100 | Avg Loss: 0.0158 | Grad Norm: 0.00947460\n",
      "Epoch 4 | Step 2897200 | Avg Loss: 0.0158 | Grad Norm: 0.00926103\n",
      "Epoch 4 | Step 2897300 | Avg Loss: 0.0157 | Grad Norm: 0.00963083\n",
      "Epoch 4 | Step 2897400 | Avg Loss: 0.0157 | Grad Norm: 0.01042120\n",
      "Epoch 4 | Step 2897500 | Avg Loss: 0.0158 | Grad Norm: 0.00852955\n",
      "Epoch 4 | Step 2897600 | Avg Loss: 0.0156 | Grad Norm: 0.00811516\n",
      "Epoch 4 | Step 2897700 | Avg Loss: 0.0155 | Grad Norm: 0.00906899\n",
      "Epoch 4 | Step 2897800 | Avg Loss: 0.0153 | Grad Norm: 0.00952036\n",
      "Epoch 4 | Step 2897900 | Avg Loss: 0.0154 | Grad Norm: 0.00850892\n",
      "Epoch 4 | Step 2898000 | Avg Loss: 0.0155 | Grad Norm: 0.00863444\n",
      "Epoch 4 | Step 2898100 | Avg Loss: 0.0155 | Grad Norm: 0.00905269\n",
      "Epoch 4 | Step 2898200 | Avg Loss: 0.0157 | Grad Norm: 0.00937140\n",
      "Epoch 4 | Step 2898300 | Avg Loss: 0.0159 | Grad Norm: 0.00970088\n",
      "Epoch 4 | Step 2898400 | Avg Loss: 0.0159 | Grad Norm: 0.00935119\n",
      "Epoch 4 | Step 2898500 | Avg Loss: 0.0153 | Grad Norm: 0.00922757\n",
      "Epoch 4 | Step 2898600 | Avg Loss: 0.0152 | Grad Norm: 0.00896834\n",
      "Epoch 4 | Step 2898700 | Avg Loss: 0.0155 | Grad Norm: 0.00905938\n",
      "Epoch 4 | Step 2898800 | Avg Loss: 0.0156 | Grad Norm: 0.00783691\n",
      "Epoch 4 | Step 2898900 | Avg Loss: 0.0155 | Grad Norm: 0.00878823\n",
      "Epoch 4 | Step 2899000 | Avg Loss: 0.0157 | Grad Norm: 0.00921609\n",
      "Epoch 4 | Step 2899100 | Avg Loss: 0.0158 | Grad Norm: 0.00865095\n",
      "Epoch 4 | Step 2899200 | Avg Loss: 0.0158 | Grad Norm: 0.00973274\n",
      "Epoch 4 | Step 2899300 | Avg Loss: 0.0158 | Grad Norm: 0.00861772\n",
      "Epoch 4 | Step 2899400 | Avg Loss: 0.0158 | Grad Norm: 0.00853493\n",
      "Epoch 4 | Step 2899500 | Avg Loss: 0.0156 | Grad Norm: 0.00945436\n",
      "Epoch 4 | Step 2899600 | Avg Loss: 0.0158 | Grad Norm: 0.01016561\n",
      "Epoch 4 | Step 2899700 | Avg Loss: 0.0159 | Grad Norm: 0.00899966\n",
      "Epoch 4 | Step 2899800 | Avg Loss: 0.0161 | Grad Norm: 0.01080848\n",
      "Epoch 4 | Step 2899900 | Avg Loss: 0.0158 | Grad Norm: 0.01050715\n",
      "Epoch 4 | Step 2900000 | Avg Loss: 0.0156 | Grad Norm: 0.00996767\n",
      "Saving model at step2900000\n",
      "Epoch 4 | Step 2900100 | Avg Loss: 0.0158 | Grad Norm: 0.00877474\n",
      "Epoch 4 | Step 2900200 | Avg Loss: 0.0157 | Grad Norm: 0.00902176\n",
      "Epoch 4 | Step 2900300 | Avg Loss: 0.0160 | Grad Norm: 0.00877388\n",
      "Epoch 4 | Step 2900400 | Avg Loss: 0.0159 | Grad Norm: 0.01094823\n",
      "Epoch 4 | Step 2900500 | Avg Loss: 0.0156 | Grad Norm: 0.00920491\n",
      "Epoch 4 | Step 2900600 | Avg Loss: 0.0156 | Grad Norm: 0.00907486\n",
      "Epoch 4 | Step 2900700 | Avg Loss: 0.0159 | Grad Norm: 0.00876102\n",
      "Epoch 4 | Step 2900800 | Avg Loss: 0.0156 | Grad Norm: 0.00955383\n",
      "Epoch 4 | Step 2900900 | Avg Loss: 0.0157 | Grad Norm: 0.00862190\n",
      "Epoch 4 | Step 2901000 | Avg Loss: 0.0156 | Grad Norm: 0.00904952\n",
      "Epoch 4 | Step 2901100 | Avg Loss: 0.0161 | Grad Norm: 0.00852310\n",
      "Epoch 4 | Step 2901200 | Avg Loss: 0.0162 | Grad Norm: 0.00893547\n",
      "Epoch 4 | Step 2901300 | Avg Loss: 0.0164 | Grad Norm: 0.00968123\n",
      "Epoch 4 | Step 2901400 | Avg Loss: 0.0163 | Grad Norm: 0.00847604\n",
      "Epoch 4 | Step 2901500 | Avg Loss: 0.0162 | Grad Norm: 0.00879895\n",
      "Epoch 4 | Step 2901600 | Avg Loss: 0.0159 | Grad Norm: 0.00929301\n",
      "Epoch 4 | Step 2901700 | Avg Loss: 0.0159 | Grad Norm: 0.00808861\n",
      "Epoch 4 | Step 2901800 | Avg Loss: 0.0155 | Grad Norm: 0.00939856\n",
      "Epoch 4 | Step 2901900 | Avg Loss: 0.0157 | Grad Norm: 0.00890737\n",
      "Epoch 4 | Step 2902000 | Avg Loss: 0.0157 | Grad Norm: 0.01020289\n",
      "Epoch 4 | Step 2902100 | Avg Loss: 0.0157 | Grad Norm: 0.00896885\n",
      "Epoch 4 | Step 2902200 | Avg Loss: 0.0155 | Grad Norm: 0.00919867\n",
      "Epoch 4 | Step 2902300 | Avg Loss: 0.0155 | Grad Norm: 0.00897987\n",
      "Epoch 4 | Step 2902400 | Avg Loss: 0.0157 | Grad Norm: 0.00893387\n",
      "Epoch 4 | Step 2902500 | Avg Loss: 0.0156 | Grad Norm: 0.00847696\n",
      "Epoch 4 | Step 2902600 | Avg Loss: 0.0157 | Grad Norm: 0.00995358\n",
      "Epoch 4 | Step 2902700 | Avg Loss: 0.0161 | Grad Norm: 0.00910293\n",
      "Epoch 4 | Step 2902800 | Avg Loss: 0.0160 | Grad Norm: 0.01024511\n",
      "Epoch 4 | Step 2902900 | Avg Loss: 0.0161 | Grad Norm: 0.00909084\n",
      "Epoch 4 | Step 2903000 | Avg Loss: 0.0158 | Grad Norm: 0.00926088\n",
      "Epoch 4 | Step 2903100 | Avg Loss: 0.0158 | Grad Norm: 0.00907264\n",
      "Epoch 4 | Step 2903200 | Avg Loss: 0.0160 | Grad Norm: 0.01017958\n",
      "Epoch 4 | Step 2903300 | Avg Loss: 0.0157 | Grad Norm: 0.01126472\n",
      "Epoch 4 | Step 2903400 | Avg Loss: 0.0161 | Grad Norm: 0.00928292\n",
      "Epoch 4 | Step 2903500 | Avg Loss: 0.0161 | Grad Norm: 0.00808077\n",
      "Epoch 4 | Step 2903600 | Avg Loss: 0.0159 | Grad Norm: 0.00837618\n",
      "Epoch 4 | Step 2903700 | Avg Loss: 0.0157 | Grad Norm: 0.00997973\n",
      "Epoch 4 | Step 2903800 | Avg Loss: 0.0156 | Grad Norm: 0.00842818\n",
      "Epoch 4 | Step 2903900 | Avg Loss: 0.0160 | Grad Norm: 0.00913439\n",
      "Epoch 4 | Step 2904000 | Avg Loss: 0.0156 | Grad Norm: 0.00811898\n",
      "Epoch 4 | Step 2904100 | Avg Loss: 0.0154 | Grad Norm: 0.00862810\n",
      "Epoch 4 | Step 2904200 | Avg Loss: 0.0155 | Grad Norm: 0.00839927\n",
      "Epoch 4 | Step 2904300 | Avg Loss: 0.0153 | Grad Norm: 0.01010164\n",
      "Epoch 4 | Step 2904400 | Avg Loss: 0.0154 | Grad Norm: 0.00892409\n",
      "Epoch 4 | Step 2904500 | Avg Loss: 0.0152 | Grad Norm: 0.00827129\n",
      "Epoch 4 | Step 2904600 | Avg Loss: 0.0150 | Grad Norm: 0.00747863\n",
      "Epoch 4 | Step 2904700 | Avg Loss: 0.0148 | Grad Norm: 0.00919953\n",
      "Epoch 4 | Step 2904800 | Avg Loss: 0.0151 | Grad Norm: 0.00883216\n",
      "Epoch 4 | Step 2904900 | Avg Loss: 0.0158 | Grad Norm: 0.00947360\n",
      "Epoch 4 | Step 2905000 | Avg Loss: 0.0156 | Grad Norm: 0.00861388\n",
      "Epoch 4 | Step 2905100 | Avg Loss: 0.0154 | Grad Norm: 0.00817086\n",
      "Epoch 4 | Step 2905200 | Avg Loss: 0.0154 | Grad Norm: 0.00904251\n",
      "Epoch 4 | Step 2905300 | Avg Loss: 0.0155 | Grad Norm: 0.01110158\n",
      "Epoch 4 | Step 2905400 | Avg Loss: 0.0157 | Grad Norm: 0.00853805\n",
      "Epoch 4 | Step 2905500 | Avg Loss: 0.0156 | Grad Norm: 0.00893656\n",
      "Epoch 4 | Step 2905600 | Avg Loss: 0.0155 | Grad Norm: 0.00793433\n",
      "Epoch 4 | Step 2905700 | Avg Loss: 0.0157 | Grad Norm: 0.00886684\n",
      "Epoch 4 | Step 2905800 | Avg Loss: 0.0157 | Grad Norm: 0.00956519\n",
      "Epoch 4 | Step 2905900 | Avg Loss: 0.0156 | Grad Norm: 0.01120421\n",
      "Epoch 4 | Step 2906000 | Avg Loss: 0.0158 | Grad Norm: 0.01038605\n",
      "Epoch 4 | Step 2906100 | Avg Loss: 0.0155 | Grad Norm: 0.00880822\n",
      "Epoch 4 | Step 2906200 | Avg Loss: 0.0157 | Grad Norm: 0.00928750\n",
      "Epoch 4 | Step 2906300 | Avg Loss: 0.0153 | Grad Norm: 0.01003715\n",
      "Epoch 4 | Step 2906400 | Avg Loss: 0.0154 | Grad Norm: 0.00884808\n",
      "Epoch 4 | Step 2906500 | Avg Loss: 0.0158 | Grad Norm: 0.01017786\n",
      "Epoch 4 | Step 2906600 | Avg Loss: 0.0161 | Grad Norm: 0.00797213\n",
      "Epoch 4 | Step 2906700 | Avg Loss: 0.0160 | Grad Norm: 0.00815673\n",
      "Epoch 4 | Step 2906800 | Avg Loss: 0.0161 | Grad Norm: 0.01052198\n",
      "Epoch 4 | Step 2906900 | Avg Loss: 0.0155 | Grad Norm: 0.00985916\n",
      "Epoch 4 | Step 2907000 | Avg Loss: 0.0155 | Grad Norm: 0.00882501\n",
      "Epoch 4 | Step 2907100 | Avg Loss: 0.0153 | Grad Norm: 0.01222326\n",
      "Epoch 4 | Step 2907200 | Avg Loss: 0.0152 | Grad Norm: 0.00863251\n",
      "Epoch 4 | Step 2907300 | Avg Loss: 0.0149 | Grad Norm: 0.00952556\n",
      "Epoch 4 | Step 2907400 | Avg Loss: 0.0152 | Grad Norm: 0.00861441\n",
      "Epoch 4 | Step 2907500 | Avg Loss: 0.0153 | Grad Norm: 0.00956281\n",
      "Epoch 4 | Step 2907600 | Avg Loss: 0.0153 | Grad Norm: 0.00960865\n",
      "Epoch 4 | Step 2907700 | Avg Loss: 0.0152 | Grad Norm: 0.00854773\n",
      "Epoch 4 | Step 2907800 | Avg Loss: 0.0153 | Grad Norm: 0.00926811\n",
      "Epoch 4 | Step 2907900 | Avg Loss: 0.0155 | Grad Norm: 0.00950703\n",
      "Epoch 4 | Step 2908000 | Avg Loss: 0.0154 | Grad Norm: 0.00847334\n",
      "Epoch 4 | Step 2908100 | Avg Loss: 0.0153 | Grad Norm: 0.00809714\n",
      "Epoch 4 | Step 2908200 | Avg Loss: 0.0154 | Grad Norm: 0.01024029\n",
      "Epoch 4 | Step 2908300 | Avg Loss: 0.0153 | Grad Norm: 0.00923995\n",
      "Epoch 4 | Step 2908400 | Avg Loss: 0.0154 | Grad Norm: 0.00923849\n",
      "Epoch 4 | Step 2908500 | Avg Loss: 0.0154 | Grad Norm: 0.00990448\n",
      "Epoch 4 | Step 2908600 | Avg Loss: 0.0151 | Grad Norm: 0.00959640\n",
      "Epoch 4 | Step 2908700 | Avg Loss: 0.0155 | Grad Norm: 0.00799826\n",
      "Epoch 4 | Step 2908800 | Avg Loss: 0.0157 | Grad Norm: 0.00916652\n",
      "Epoch 4 | Step 2908900 | Avg Loss: 0.0160 | Grad Norm: 0.01093886\n",
      "Epoch 4 | Step 2909000 | Avg Loss: 0.0160 | Grad Norm: 0.01040059\n",
      "Epoch 4 | Step 2909100 | Avg Loss: 0.0155 | Grad Norm: 0.00964823\n",
      "Epoch 4 | Step 2909200 | Avg Loss: 0.0155 | Grad Norm: 0.00882645\n",
      "Epoch 4 | Step 2909300 | Avg Loss: 0.0157 | Grad Norm: 0.00852061\n",
      "Epoch 4 | Step 2909400 | Avg Loss: 0.0154 | Grad Norm: 0.01108270\n",
      "Epoch 4 | Step 2909500 | Avg Loss: 0.0154 | Grad Norm: 0.00871495\n",
      "Epoch 4 | Step 2909600 | Avg Loss: 0.0154 | Grad Norm: 0.00855956\n",
      "Epoch 4 | Step 2909700 | Avg Loss: 0.0153 | Grad Norm: 0.01039292\n",
      "Epoch 4 | Step 2909800 | Avg Loss: 0.0154 | Grad Norm: 0.01002598\n",
      "Epoch 4 | Step 2909900 | Avg Loss: 0.0156 | Grad Norm: 0.00898977\n",
      "Epoch 4 | Step 2910000 | Avg Loss: 0.0154 | Grad Norm: 0.00938925\n",
      "Epoch 4 | Step 2910100 | Avg Loss: 0.0155 | Grad Norm: 0.00889288\n",
      "Epoch 4 | Step 2910200 | Avg Loss: 0.0154 | Grad Norm: 0.00996395\n",
      "Epoch 4 | Step 2910300 | Avg Loss: 0.0155 | Grad Norm: 0.00896145\n",
      "Epoch 4 | Step 2910400 | Avg Loss: 0.0157 | Grad Norm: 0.01244911\n",
      "Epoch 4 | Step 2910500 | Avg Loss: 0.0154 | Grad Norm: 0.00847073\n",
      "Epoch 4 | Step 2910600 | Avg Loss: 0.0150 | Grad Norm: 0.00867029\n",
      "Epoch 4 | Step 2910700 | Avg Loss: 0.0151 | Grad Norm: 0.00840506\n",
      "Epoch 4 | Step 2910800 | Avg Loss: 0.0151 | Grad Norm: 0.01021839\n",
      "Epoch 4 | Step 2910900 | Avg Loss: 0.0153 | Grad Norm: 0.00882150\n",
      "Epoch 4 | Step 2911000 | Avg Loss: 0.0156 | Grad Norm: 0.00845067\n",
      "Epoch 4 | Step 2911100 | Avg Loss: 0.0155 | Grad Norm: 0.00920218\n",
      "Epoch 4 | Step 2911200 | Avg Loss: 0.0158 | Grad Norm: 0.00947946\n",
      "Epoch 4 | Step 2911300 | Avg Loss: 0.0160 | Grad Norm: 0.00849429\n",
      "Epoch 4 | Step 2911400 | Avg Loss: 0.0157 | Grad Norm: 0.00833119\n",
      "Epoch 4 | Step 2911500 | Avg Loss: 0.0158 | Grad Norm: 0.00766980\n",
      "Epoch 4 | Step 2911600 | Avg Loss: 0.0155 | Grad Norm: 0.00726266\n",
      "Epoch 4 | Step 2911700 | Avg Loss: 0.0157 | Grad Norm: 0.00864852\n",
      "Epoch 4 | Step 2911800 | Avg Loss: 0.0158 | Grad Norm: 0.01002144\n",
      "Epoch 4 | Step 2911900 | Avg Loss: 0.0157 | Grad Norm: 0.00909904\n",
      "Epoch 4 | Step 2912000 | Avg Loss: 0.0159 | Grad Norm: 0.00868083\n",
      "Epoch 4 | Step 2912100 | Avg Loss: 0.0158 | Grad Norm: 0.00945321\n",
      "Epoch 4 | Step 2912200 | Avg Loss: 0.0157 | Grad Norm: 0.00934939\n",
      "Epoch 4 | Step 2912300 | Avg Loss: 0.0161 | Grad Norm: 0.00925797\n",
      "Epoch 4 | Step 2912400 | Avg Loss: 0.0162 | Grad Norm: 0.00970679\n",
      "Epoch 4 | Step 2912500 | Avg Loss: 0.0157 | Grad Norm: 0.00901219\n",
      "Epoch 4 | Step 2912600 | Avg Loss: 0.0153 | Grad Norm: 0.00838843\n",
      "Epoch 4 | Step 2912700 | Avg Loss: 0.0154 | Grad Norm: 0.01460588\n",
      "Epoch 4 | Step 2912800 | Avg Loss: 0.0152 | Grad Norm: 0.00982850\n",
      "Epoch 4 | Step 2912900 | Avg Loss: 0.0152 | Grad Norm: 0.00838864\n",
      "Epoch 4 | Step 2913000 | Avg Loss: 0.0152 | Grad Norm: 0.00900834\n",
      "Epoch 4 | Step 2913100 | Avg Loss: 0.0157 | Grad Norm: 0.00893181\n",
      "Epoch 4 | Step 2913200 | Avg Loss: 0.0156 | Grad Norm: 0.00839193\n",
      "Epoch 4 | Step 2913300 | Avg Loss: 0.0157 | Grad Norm: 0.00919615\n",
      "Epoch 4 | Step 2913400 | Avg Loss: 0.0157 | Grad Norm: 0.00961012\n",
      "Epoch 4 | Step 2913500 | Avg Loss: 0.0157 | Grad Norm: 0.00791480\n",
      "Epoch 4 | Step 2913600 | Avg Loss: 0.0154 | Grad Norm: 0.00818058\n",
      "Epoch 4 | Step 2913700 | Avg Loss: 0.0153 | Grad Norm: 0.00816926\n",
      "Epoch 4 | Step 2913800 | Avg Loss: 0.0155 | Grad Norm: 0.01006852\n",
      "Epoch 4 | Step 2913900 | Avg Loss: 0.0155 | Grad Norm: 0.00787512\n",
      "Epoch 4 | Step 2914000 | Avg Loss: 0.0157 | Grad Norm: 0.00966599\n",
      "Epoch 4 | Step 2914100 | Avg Loss: 0.0159 | Grad Norm: 0.00989598\n",
      "Epoch 4 | Step 2914200 | Avg Loss: 0.0157 | Grad Norm: 0.00951644\n",
      "Epoch 4 | Step 2914300 | Avg Loss: 0.0158 | Grad Norm: 0.01090069\n",
      "Epoch 4 | Step 2914400 | Avg Loss: 0.0157 | Grad Norm: 0.00904237\n",
      "Epoch 4 | Step 2914500 | Avg Loss: 0.0158 | Grad Norm: 0.00841061\n",
      "Epoch 4 | Step 2914600 | Avg Loss: 0.0155 | Grad Norm: 0.01024148\n",
      "Epoch 4 | Step 2914700 | Avg Loss: 0.0157 | Grad Norm: 0.00928308\n",
      "Epoch 4 | Step 2914800 | Avg Loss: 0.0158 | Grad Norm: 0.00883631\n",
      "Epoch 4 | Step 2914900 | Avg Loss: 0.0158 | Grad Norm: 0.00835558\n",
      "Epoch 4 | Step 2915000 | Avg Loss: 0.0154 | Grad Norm: 0.00949219\n",
      "Epoch 4 | Step 2915100 | Avg Loss: 0.0160 | Grad Norm: 0.00889767\n",
      "Epoch 4 | Step 2915200 | Avg Loss: 0.0160 | Grad Norm: 0.00936666\n",
      "Epoch 4 | Step 2915300 | Avg Loss: 0.0156 | Grad Norm: 0.00921955\n",
      "Epoch 4 | Step 2915400 | Avg Loss: 0.0160 | Grad Norm: 0.00951567\n",
      "Epoch 4 | Step 2915500 | Avg Loss: 0.0154 | Grad Norm: 0.00961701\n",
      "Epoch 4 | Step 2915600 | Avg Loss: 0.0152 | Grad Norm: 0.00833726\n",
      "Epoch 4 | Step 2915700 | Avg Loss: 0.0153 | Grad Norm: 0.00911034\n",
      "Epoch 4 | Step 2915800 | Avg Loss: 0.0156 | Grad Norm: 0.01101030\n",
      "Epoch 4 | Step 2915900 | Avg Loss: 0.0156 | Grad Norm: 0.01122329\n",
      "Epoch 4 | Step 2916000 | Avg Loss: 0.0154 | Grad Norm: 0.00859105\n",
      "Epoch 4 | Step 2916100 | Avg Loss: 0.0155 | Grad Norm: 0.00881584\n",
      "Epoch 4 | Step 2916200 | Avg Loss: 0.0157 | Grad Norm: 0.00838095\n",
      "Epoch 4 | Step 2916300 | Avg Loss: 0.0157 | Grad Norm: 0.01037935\n",
      "Epoch 4 | Step 2916400 | Avg Loss: 0.0157 | Grad Norm: 0.01059406\n",
      "Epoch 4 | Step 2916500 | Avg Loss: 0.0156 | Grad Norm: 0.00834680\n",
      "Epoch 4 | Step 2916600 | Avg Loss: 0.0155 | Grad Norm: 0.00820994\n",
      "Epoch 4 | Step 2916700 | Avg Loss: 0.0152 | Grad Norm: 0.00814233\n",
      "Epoch 4 | Step 2916800 | Avg Loss: 0.0154 | Grad Norm: 0.00859721\n",
      "Epoch 4 | Step 2916900 | Avg Loss: 0.0150 | Grad Norm: 0.01035365\n",
      "Epoch 4 | Step 2917000 | Avg Loss: 0.0151 | Grad Norm: 0.00924376\n",
      "Epoch 4 | Step 2917100 | Avg Loss: 0.0149 | Grad Norm: 0.00831401\n",
      "Epoch 4 | Step 2917200 | Avg Loss: 0.0152 | Grad Norm: 0.00856918\n",
      "Epoch 4 | Step 2917300 | Avg Loss: 0.0152 | Grad Norm: 0.00974655\n",
      "Epoch 4 | Step 2917400 | Avg Loss: 0.0152 | Grad Norm: 0.00882555\n",
      "Epoch 4 | Step 2917500 | Avg Loss: 0.0154 | Grad Norm: 0.00989375\n",
      "Epoch 4 | Step 2917600 | Avg Loss: 0.0153 | Grad Norm: 0.01001081\n",
      "Epoch 4 | Step 2917700 | Avg Loss: 0.0154 | Grad Norm: 0.00886803\n",
      "Epoch 4 | Step 2917800 | Avg Loss: 0.0155 | Grad Norm: 0.00865348\n",
      "Epoch 4 | Step 2917900 | Avg Loss: 0.0154 | Grad Norm: 0.00964899\n",
      "Epoch 4 | Step 2918000 | Avg Loss: 0.0150 | Grad Norm: 0.00851513\n",
      "Epoch 4 | Step 2918100 | Avg Loss: 0.0153 | Grad Norm: 0.01008776\n",
      "Epoch 4 | Step 2918200 | Avg Loss: 0.0154 | Grad Norm: 0.00881131\n",
      "Epoch 4 | Step 2918300 | Avg Loss: 0.0155 | Grad Norm: 0.00837542\n",
      "Epoch 4 | Step 2918400 | Avg Loss: 0.0155 | Grad Norm: 0.00866225\n",
      "Epoch 4 | Step 2918500 | Avg Loss: 0.0154 | Grad Norm: 0.00882725\n",
      "Epoch 4 | Step 2918600 | Avg Loss: 0.0158 | Grad Norm: 0.00948722\n",
      "Epoch 4 | Step 2918700 | Avg Loss: 0.0153 | Grad Norm: 0.00850014\n",
      "Epoch 4 | Step 2918800 | Avg Loss: 0.0158 | Grad Norm: 0.01112469\n",
      "Epoch 4 | Step 2918900 | Avg Loss: 0.0155 | Grad Norm: 0.00901684\n",
      "Epoch 4 | Step 2919000 | Avg Loss: 0.0156 | Grad Norm: 0.01036531\n",
      "Epoch 4 | Step 2919100 | Avg Loss: 0.0156 | Grad Norm: 0.00953122\n",
      "Epoch 4 | Step 2919200 | Avg Loss: 0.0157 | Grad Norm: 0.00942266\n",
      "Epoch 4 | Step 2919300 | Avg Loss: 0.0157 | Grad Norm: 0.00952851\n",
      "Epoch 4 | Step 2919400 | Avg Loss: 0.0155 | Grad Norm: 0.00987444\n",
      "Epoch 4 | Step 2919500 | Avg Loss: 0.0153 | Grad Norm: 0.00872819\n",
      "Epoch 4 | Step 2919600 | Avg Loss: 0.0154 | Grad Norm: 0.00864516\n",
      "Epoch 4 | Step 2919700 | Avg Loss: 0.0154 | Grad Norm: 0.00874466\n",
      "Epoch 4 | Step 2919800 | Avg Loss: 0.0153 | Grad Norm: 0.00853065\n",
      "Epoch 4 | Step 2919900 | Avg Loss: 0.0154 | Grad Norm: 0.00916852\n",
      "Epoch 4 | Step 2920000 | Avg Loss: 0.0155 | Grad Norm: 0.00807276\n",
      "Epoch 4 | Step 2920100 | Avg Loss: 0.0149 | Grad Norm: 0.00929753\n",
      "Epoch 4 | Step 2920200 | Avg Loss: 0.0149 | Grad Norm: 0.00926566\n",
      "Epoch 4 | Step 2920300 | Avg Loss: 0.0152 | Grad Norm: 0.00838495\n",
      "Epoch 4 | Step 2920400 | Avg Loss: 0.0155 | Grad Norm: 0.01004740\n",
      "Epoch 4 | Step 2920500 | Avg Loss: 0.0154 | Grad Norm: 0.00834000\n",
      "Epoch 4 | Step 2920600 | Avg Loss: 0.0155 | Grad Norm: 0.00942507\n",
      "Epoch 4 | Step 2920700 | Avg Loss: 0.0149 | Grad Norm: 0.00799212\n",
      "Epoch 4 | Step 2920800 | Avg Loss: 0.0149 | Grad Norm: 0.00903712\n",
      "Epoch 4 | Step 2920900 | Avg Loss: 0.0151 | Grad Norm: 0.00730743\n",
      "Epoch 4 | Step 2921000 | Avg Loss: 0.0147 | Grad Norm: 0.00824197\n",
      "Epoch 4 | Step 2921100 | Avg Loss: 0.0150 | Grad Norm: 0.00937803\n",
      "Epoch 4 | Step 2921200 | Avg Loss: 0.0154 | Grad Norm: 0.01014279\n",
      "Epoch 4 | Step 2921300 | Avg Loss: 0.0154 | Grad Norm: 0.00972787\n",
      "Epoch 4 | Step 2921400 | Avg Loss: 0.0149 | Grad Norm: 0.00817237\n",
      "Epoch 4 | Step 2921500 | Avg Loss: 0.0153 | Grad Norm: 0.00881686\n",
      "Epoch 4 | Step 2921600 | Avg Loss: 0.0154 | Grad Norm: 0.01114252\n",
      "Epoch 4 | Step 2921700 | Avg Loss: 0.0152 | Grad Norm: 0.00937097\n",
      "Epoch 4 | Step 2921800 | Avg Loss: 0.0153 | Grad Norm: 0.01054312\n",
      "Epoch 4 | Step 2921900 | Avg Loss: 0.0155 | Grad Norm: 0.00940811\n",
      "Epoch 4 | Step 2922000 | Avg Loss: 0.0151 | Grad Norm: 0.00882461\n",
      "Epoch 4 | Step 2922100 | Avg Loss: 0.0150 | Grad Norm: 0.00984784\n",
      "Epoch 4 | Step 2922200 | Avg Loss: 0.0148 | Grad Norm: 0.01051107\n",
      "Epoch 4 | Step 2922300 | Avg Loss: 0.0149 | Grad Norm: 0.00760000\n",
      "Epoch 4 | Step 2922400 | Avg Loss: 0.0153 | Grad Norm: 0.01017755\n",
      "Epoch 4 | Step 2922500 | Avg Loss: 0.0154 | Grad Norm: 0.00916712\n",
      "Epoch 4 | Step 2922600 | Avg Loss: 0.0154 | Grad Norm: 0.00962544\n",
      "Epoch 4 | Step 2922700 | Avg Loss: 0.0151 | Grad Norm: 0.00839801\n",
      "Epoch 4 | Step 2922800 | Avg Loss: 0.0154 | Grad Norm: 0.00842408\n",
      "Epoch 4 | Step 2922900 | Avg Loss: 0.0152 | Grad Norm: 0.01215055\n",
      "Epoch 4 | Step 2923000 | Avg Loss: 0.0152 | Grad Norm: 0.00879073\n",
      "Epoch 4 | Step 2923100 | Avg Loss: 0.0152 | Grad Norm: 0.00847413\n",
      "Epoch 4 | Step 2923200 | Avg Loss: 0.0153 | Grad Norm: 0.00811078\n",
      "Epoch 4 | Step 2923300 | Avg Loss: 0.0156 | Grad Norm: 0.00822059\n",
      "Epoch 4 | Step 2923400 | Avg Loss: 0.0156 | Grad Norm: 0.00855854\n",
      "Epoch 4 | Step 2923500 | Avg Loss: 0.0153 | Grad Norm: 0.00939158\n",
      "Epoch 4 | Step 2923600 | Avg Loss: 0.0151 | Grad Norm: 0.00944277\n",
      "Epoch 4 | Step 2923700 | Avg Loss: 0.0151 | Grad Norm: 0.01083310\n",
      "Epoch 4 | Step 2923800 | Avg Loss: 0.0153 | Grad Norm: 0.00944766\n",
      "Epoch 4 | Step 2923900 | Avg Loss: 0.0154 | Grad Norm: 0.00993076\n",
      "Epoch 4 | Step 2924000 | Avg Loss: 0.0152 | Grad Norm: 0.01221794\n",
      "Epoch 4 | Step 2924100 | Avg Loss: 0.0150 | Grad Norm: 0.01045205\n",
      "Epoch 4 | Step 2924200 | Avg Loss: 0.0152 | Grad Norm: 0.00933572\n",
      "Epoch 4 | Step 2924300 | Avg Loss: 0.0156 | Grad Norm: 0.00895075\n",
      "Epoch 4 | Step 2924400 | Avg Loss: 0.0154 | Grad Norm: 0.01007817\n",
      "Epoch 4 | Step 2924500 | Avg Loss: 0.0152 | Grad Norm: 0.00980512\n",
      "Epoch 4 | Step 2924600 | Avg Loss: 0.0153 | Grad Norm: 0.00816246\n",
      "Epoch 4 | Step 2924700 | Avg Loss: 0.0156 | Grad Norm: 0.00907631\n",
      "Epoch 4 | Step 2924800 | Avg Loss: 0.0154 | Grad Norm: 0.00904193\n",
      "Epoch 4 | Step 2924900 | Avg Loss: 0.0151 | Grad Norm: 0.00933623\n",
      "Epoch 4 | Step 2925000 | Avg Loss: 0.0153 | Grad Norm: 0.00857242\n",
      "Epoch 4 | Step 2925100 | Avg Loss: 0.0156 | Grad Norm: 0.00913430\n",
      "Epoch 4 | Step 2925200 | Avg Loss: 0.0154 | Grad Norm: 0.00871481\n",
      "Epoch 4 | Step 2925300 | Avg Loss: 0.0155 | Grad Norm: 0.00834658\n",
      "Epoch 4 | Step 2925400 | Avg Loss: 0.0154 | Grad Norm: 0.00920249\n",
      "Epoch 4 | Step 2925500 | Avg Loss: 0.0154 | Grad Norm: 0.00866408\n",
      "Epoch 4 | Step 2925600 | Avg Loss: 0.0149 | Grad Norm: 0.00928454\n",
      "Epoch 4 | Step 2925700 | Avg Loss: 0.0149 | Grad Norm: 0.00964513\n",
      "Epoch 4 | Step 2925800 | Avg Loss: 0.0153 | Grad Norm: 0.00875910\n",
      "Epoch 4 | Step 2925900 | Avg Loss: 0.0152 | Grad Norm: 0.00870510\n",
      "Epoch 4 | Step 2926000 | Avg Loss: 0.0153 | Grad Norm: 0.00917828\n",
      "Epoch 4 | Step 2926100 | Avg Loss: 0.0153 | Grad Norm: 0.00948098\n",
      "Epoch 4 | Step 2926200 | Avg Loss: 0.0153 | Grad Norm: 0.00859833\n",
      "Epoch 4 | Step 2926300 | Avg Loss: 0.0153 | Grad Norm: 0.00896325\n",
      "Epoch 4 | Step 2926400 | Avg Loss: 0.0155 | Grad Norm: 0.00946379\n",
      "Epoch 4 | Step 2926500 | Avg Loss: 0.0152 | Grad Norm: 0.00885428\n",
      "Epoch 4 | Step 2926600 | Avg Loss: 0.0155 | Grad Norm: 0.00781353\n",
      "Epoch 4 | Step 2926700 | Avg Loss: 0.0152 | Grad Norm: 0.00818553\n",
      "Epoch 4 | Step 2926800 | Avg Loss: 0.0151 | Grad Norm: 0.01104362\n",
      "Epoch 4 | Step 2926900 | Avg Loss: 0.0150 | Grad Norm: 0.00819608\n",
      "Epoch 4 | Step 2927000 | Avg Loss: 0.0153 | Grad Norm: 0.00962261\n",
      "Epoch 4 | Step 2927100 | Avg Loss: 0.0155 | Grad Norm: 0.00888314\n",
      "Epoch 4 | Step 2927200 | Avg Loss: 0.0155 | Grad Norm: 0.00954144\n",
      "Epoch 4 | Step 2927300 | Avg Loss: 0.0157 | Grad Norm: 0.00871084\n",
      "Epoch 4 | Step 2927400 | Avg Loss: 0.0158 | Grad Norm: 0.01091934\n",
      "Epoch 4 | Step 2927500 | Avg Loss: 0.0159 | Grad Norm: 0.00815001\n",
      "Epoch 4 | Step 2927600 | Avg Loss: 0.0160 | Grad Norm: 0.00950847\n",
      "Epoch 4 | Step 2927700 | Avg Loss: 0.0157 | Grad Norm: 0.00913782\n",
      "Epoch 4 | Step 2927800 | Avg Loss: 0.0155 | Grad Norm: 0.00893501\n",
      "Epoch 4 | Step 2927900 | Avg Loss: 0.0156 | Grad Norm: 0.00886905\n",
      "Epoch 4 | Step 2928000 | Avg Loss: 0.0159 | Grad Norm: 0.00885036\n",
      "Epoch 4 | Step 2928100 | Avg Loss: 0.0157 | Grad Norm: 0.00933408\n",
      "Epoch 4 | Step 2928200 | Avg Loss: 0.0158 | Grad Norm: 0.00863394\n",
      "Epoch 4 | Step 2928300 | Avg Loss: 0.0159 | Grad Norm: 0.01379778\n",
      "Epoch 4 | Step 2928400 | Avg Loss: 0.0161 | Grad Norm: 0.00961991\n",
      "Epoch 4 | Step 2928500 | Avg Loss: 0.0161 | Grad Norm: 0.01342373\n",
      "Epoch 4 | Step 2928600 | Avg Loss: 0.0165 | Grad Norm: 0.00857704\n",
      "Epoch 4 | Step 2928700 | Avg Loss: 0.0163 | Grad Norm: 0.00951978\n",
      "Epoch 4 | Step 2928800 | Avg Loss: 0.0161 | Grad Norm: 0.00969440\n",
      "Epoch 4 | Step 2928900 | Avg Loss: 0.0163 | Grad Norm: 0.01202507\n",
      "Epoch 4 | Step 2929000 | Avg Loss: 0.0167 | Grad Norm: 0.01049832\n",
      "Epoch 4 | Step 2929100 | Avg Loss: 0.0163 | Grad Norm: 0.00918743\n",
      "Epoch 4 | Step 2929200 | Avg Loss: 0.0162 | Grad Norm: 0.01146773\n",
      "Epoch 4 | Step 2929300 | Avg Loss: 0.0160 | Grad Norm: 0.00885522\n",
      "Epoch 4 | Step 2929400 | Avg Loss: 0.0159 | Grad Norm: 0.00850122\n",
      "Epoch 4 | Step 2929500 | Avg Loss: 0.0162 | Grad Norm: 0.00898916\n",
      "Epoch 4 | Step 2929600 | Avg Loss: 0.0159 | Grad Norm: 0.00867629\n",
      "Epoch 4 | Step 2929700 | Avg Loss: 0.0162 | Grad Norm: 0.00788912\n",
      "Epoch 4 | Step 2929800 | Avg Loss: 0.0156 | Grad Norm: 0.00754377\n",
      "Epoch 4 | Step 2929900 | Avg Loss: 0.0153 | Grad Norm: 0.00906596\n",
      "Epoch 4 | Step 2930000 | Avg Loss: 0.0154 | Grad Norm: 0.00907249\n",
      "Epoch 4 | Step 2930100 | Avg Loss: 0.0155 | Grad Norm: 0.01792811\n",
      "Epoch 4 | Step 2930200 | Avg Loss: 0.0154 | Grad Norm: 0.00825669\n",
      "Epoch 4 | Step 2930300 | Avg Loss: 0.0156 | Grad Norm: 0.01063616\n",
      "Epoch 4 | Step 2930400 | Avg Loss: 0.0160 | Grad Norm: 0.00858829\n",
      "Epoch 4 | Step 2930500 | Avg Loss: 0.0157 | Grad Norm: 0.01000737\n",
      "Epoch 4 | Step 2930600 | Avg Loss: 0.0159 | Grad Norm: 0.00847457\n",
      "Epoch 4 | Step 2930700 | Avg Loss: 0.0158 | Grad Norm: 0.01042239\n",
      "Epoch 4 | Step 2930800 | Avg Loss: 0.0158 | Grad Norm: 0.00925347\n",
      "Epoch 4 | Step 2930900 | Avg Loss: 0.0158 | Grad Norm: 0.01077500\n",
      "Epoch 4 | Step 2931000 | Avg Loss: 0.0157 | Grad Norm: 0.00916666\n",
      "Epoch 4 | Step 2931100 | Avg Loss: 0.0156 | Grad Norm: 0.00857646\n",
      "Epoch 4 | Step 2931200 | Avg Loss: 0.0160 | Grad Norm: 0.01068624\n",
      "Epoch 4 | Step 2931300 | Avg Loss: 0.0159 | Grad Norm: 0.00925377\n",
      "Epoch 4 | Step 2931400 | Avg Loss: 0.0160 | Grad Norm: 0.01045152\n",
      "Epoch 4 | Step 2931500 | Avg Loss: 0.0160 | Grad Norm: 0.00911124\n",
      "Epoch 4 | Step 2931600 | Avg Loss: 0.0158 | Grad Norm: 0.00965701\n",
      "Epoch 4 | Step 2931700 | Avg Loss: 0.0160 | Grad Norm: 0.00947592\n",
      "Epoch 4 | Step 2931800 | Avg Loss: 0.0159 | Grad Norm: 0.00820689\n",
      "Epoch 4 | Step 2931900 | Avg Loss: 0.0160 | Grad Norm: 0.00977946\n",
      "Epoch 4 | Step 2932000 | Avg Loss: 0.0155 | Grad Norm: 0.00931476\n",
      "Epoch 4 | Step 2932100 | Avg Loss: 0.0153 | Grad Norm: 0.01144367\n",
      "Epoch 4 | Step 2932200 | Avg Loss: 0.0153 | Grad Norm: 0.01067991\n",
      "Epoch 4 | Step 2932300 | Avg Loss: 0.0158 | Grad Norm: 0.00735315\n",
      "Epoch 4 | Step 2932400 | Avg Loss: 0.0154 | Grad Norm: 0.00978373\n",
      "Epoch 4 | Step 2932500 | Avg Loss: 0.0155 | Grad Norm: 0.00898816\n",
      "Epoch 4 | Step 2932600 | Avg Loss: 0.0154 | Grad Norm: 0.00924389\n",
      "Epoch 4 | Step 2932700 | Avg Loss: 0.0153 | Grad Norm: 0.00915448\n",
      "Epoch 4 | Step 2932800 | Avg Loss: 0.0155 | Grad Norm: 0.00998290\n",
      "Epoch 4 | Step 2932900 | Avg Loss: 0.0152 | Grad Norm: 0.00820298\n",
      "Epoch 4 | Step 2933000 | Avg Loss: 0.0152 | Grad Norm: 0.00881706\n",
      "Epoch 4 | Step 2933100 | Avg Loss: 0.0153 | Grad Norm: 0.00879849\n",
      "Epoch 4 | Step 2933200 | Avg Loss: 0.0154 | Grad Norm: 0.01077328\n",
      "Epoch 4 | Step 2933300 | Avg Loss: 0.0151 | Grad Norm: 0.00924179\n",
      "Epoch 4 | Step 2933400 | Avg Loss: 0.0153 | Grad Norm: 0.00822303\n",
      "Epoch 4 | Step 2933500 | Avg Loss: 0.0156 | Grad Norm: 0.00815593\n",
      "Epoch 4 | Step 2933600 | Avg Loss: 0.0154 | Grad Norm: 0.00852955\n",
      "Epoch 4 | Step 2933700 | Avg Loss: 0.0154 | Grad Norm: 0.00843366\n",
      "Epoch 4 | Step 2933800 | Avg Loss: 0.0154 | Grad Norm: 0.00830370\n",
      "Epoch 4 | Step 2933900 | Avg Loss: 0.0153 | Grad Norm: 0.00735342\n",
      "Epoch 4 | Step 2934000 | Avg Loss: 0.0150 | Grad Norm: 0.00820823\n",
      "Epoch 4 | Step 2934100 | Avg Loss: 0.0154 | Grad Norm: 0.00885935\n",
      "Epoch 4 | Step 2934200 | Avg Loss: 0.0153 | Grad Norm: 0.00920222\n",
      "Epoch 4 | Step 2934300 | Avg Loss: 0.0154 | Grad Norm: 0.00998940\n",
      "Epoch 4 | Step 2934400 | Avg Loss: 0.0153 | Grad Norm: 0.01053673\n",
      "Epoch 4 | Step 2934500 | Avg Loss: 0.0152 | Grad Norm: 0.00962443\n",
      "Epoch 4 | Step 2934600 | Avg Loss: 0.0152 | Grad Norm: 0.00941325\n",
      "Epoch 4 | Step 2934700 | Avg Loss: 0.0152 | Grad Norm: 0.00845425\n",
      "Epoch 4 | Step 2934800 | Avg Loss: 0.0150 | Grad Norm: 0.00859608\n",
      "Epoch 4 | Step 2934900 | Avg Loss: 0.0152 | Grad Norm: 0.01197279\n",
      "Epoch 4 | Step 2935000 | Avg Loss: 0.0155 | Grad Norm: 0.00947710\n",
      "Epoch 4 | Step 2935100 | Avg Loss: 0.0154 | Grad Norm: 0.00843822\n",
      "Epoch 4 | Step 2935200 | Avg Loss: 0.0157 | Grad Norm: 0.00893580\n",
      "Epoch 4 | Step 2935300 | Avg Loss: 0.0159 | Grad Norm: 0.01221585\n",
      "Epoch 4 | Step 2935400 | Avg Loss: 0.0160 | Grad Norm: 0.00990590\n",
      "Epoch 4 | Step 2935500 | Avg Loss: 0.0159 | Grad Norm: 0.00892407\n",
      "Epoch 4 | Step 2935600 | Avg Loss: 0.0159 | Grad Norm: 0.00995841\n",
      "Epoch 4 | Step 2935700 | Avg Loss: 0.0162 | Grad Norm: 0.01233986\n",
      "Epoch 4 | Step 2935800 | Avg Loss: 0.0163 | Grad Norm: 0.00883442\n",
      "Epoch 4 | Step 2935900 | Avg Loss: 0.0160 | Grad Norm: 0.00869125\n",
      "Epoch 4 | Step 2936000 | Avg Loss: 0.0157 | Grad Norm: 0.00889596\n",
      "Epoch 4 | Step 2936100 | Avg Loss: 0.0156 | Grad Norm: 0.00971854\n",
      "Epoch 4 | Step 2936200 | Avg Loss: 0.0155 | Grad Norm: 0.01050143\n",
      "Epoch 4 | Step 2936300 | Avg Loss: 0.0153 | Grad Norm: 0.01009186\n",
      "Epoch 4 | Step 2936400 | Avg Loss: 0.0155 | Grad Norm: 0.00788601\n",
      "Epoch 4 | Step 2936500 | Avg Loss: 0.0152 | Grad Norm: 0.01869671\n",
      "Epoch 4 | Step 2936600 | Avg Loss: 0.0153 | Grad Norm: 0.00840608\n",
      "Epoch 4 | Step 2936700 | Avg Loss: 0.0153 | Grad Norm: 0.00888982\n",
      "Epoch 4 | Step 2936800 | Avg Loss: 0.0153 | Grad Norm: 0.00809085\n",
      "Epoch 4 | Step 2936900 | Avg Loss: 0.0150 | Grad Norm: 0.00883964\n",
      "Epoch 4 | Step 2937000 | Avg Loss: 0.0154 | Grad Norm: 0.00806939\n",
      "Epoch 4 | Step 2937100 | Avg Loss: 0.0156 | Grad Norm: 0.01008861\n",
      "Epoch 4 | Step 2937200 | Avg Loss: 0.0154 | Grad Norm: 0.00823935\n",
      "Epoch 4 | Step 2937300 | Avg Loss: 0.0153 | Grad Norm: 0.00846433\n",
      "Epoch 4 | Step 2937400 | Avg Loss: 0.0155 | Grad Norm: 0.00862425\n",
      "Epoch 4 | Step 2937500 | Avg Loss: 0.0159 | Grad Norm: 0.00934624\n",
      "Epoch 4 | Step 2937600 | Avg Loss: 0.0156 | Grad Norm: 0.01123386\n",
      "Epoch 4 | Step 2937700 | Avg Loss: 0.0155 | Grad Norm: 0.00854958\n",
      "Epoch 4 | Step 2937800 | Avg Loss: 0.0158 | Grad Norm: 0.01008095\n",
      "Epoch 4 | Step 2937900 | Avg Loss: 0.0155 | Grad Norm: 0.00903697\n",
      "Epoch 4 | Step 2938000 | Avg Loss: 0.0155 | Grad Norm: 0.01184606\n",
      "Epoch 4 | Step 2938100 | Avg Loss: 0.0156 | Grad Norm: 0.00830745\n",
      "Epoch 4 | Step 2938200 | Avg Loss: 0.0153 | Grad Norm: 0.00926926\n",
      "Epoch 4 | Step 2938300 | Avg Loss: 0.0156 | Grad Norm: 0.01008589\n",
      "Epoch 4 | Step 2938400 | Avg Loss: 0.0153 | Grad Norm: 0.00950286\n",
      "Epoch 4 | Step 2938500 | Avg Loss: 0.0152 | Grad Norm: 0.00956136\n",
      "Epoch 4 | Step 2938600 | Avg Loss: 0.0153 | Grad Norm: 0.00851291\n",
      "Epoch 4 | Step 2938700 | Avg Loss: 0.0152 | Grad Norm: 0.00915083\n",
      "Epoch 4 | Step 2938800 | Avg Loss: 0.0155 | Grad Norm: 0.01160353\n",
      "Epoch 4 | Step 2938900 | Avg Loss: 0.0158 | Grad Norm: 0.00828872\n",
      "Epoch 4 | Step 2939000 | Avg Loss: 0.0154 | Grad Norm: 0.00945873\n",
      "Epoch 4 | Step 2939100 | Avg Loss: 0.0155 | Grad Norm: 0.00869981\n",
      "Epoch 4 | Step 2939200 | Avg Loss: 0.0156 | Grad Norm: 0.00926669\n",
      "Epoch 4 | Step 2939300 | Avg Loss: 0.0157 | Grad Norm: 0.00875730\n",
      "Epoch 4 | Step 2939400 | Avg Loss: 0.0159 | Grad Norm: 0.00993102\n",
      "Epoch 4 | Step 2939500 | Avg Loss: 0.0157 | Grad Norm: 0.00898555\n",
      "Epoch 4 | Step 2939600 | Avg Loss: 0.0155 | Grad Norm: 0.00902393\n",
      "Epoch 4 | Step 2939700 | Avg Loss: 0.0158 | Grad Norm: 0.00986668\n",
      "Epoch 4 | Step 2939800 | Avg Loss: 0.0158 | Grad Norm: 0.00860860\n",
      "Epoch 4 | Step 2939900 | Avg Loss: 0.0160 | Grad Norm: 0.00919294\n",
      "Epoch 4 | Step 2940000 | Avg Loss: 0.0160 | Grad Norm: 0.00818991\n",
      "Epoch 4 | Step 2940100 | Avg Loss: 0.0159 | Grad Norm: 0.00882006\n",
      "Epoch 4 | Step 2940200 | Avg Loss: 0.0156 | Grad Norm: 0.01550182\n",
      "Epoch 4 | Step 2940300 | Avg Loss: 0.0158 | Grad Norm: 0.00901723\n",
      "Epoch 4 | Step 2940400 | Avg Loss: 0.0155 | Grad Norm: 0.00827631\n",
      "Epoch 4 | Step 2940500 | Avg Loss: 0.0156 | Grad Norm: 0.01280396\n",
      "Epoch 4 | Step 2940600 | Avg Loss: 0.0156 | Grad Norm: 0.00992310\n",
      "Epoch 4 | Step 2940700 | Avg Loss: 0.0156 | Grad Norm: 0.00919286\n",
      "Epoch 4 | Step 2940800 | Avg Loss: 0.0158 | Grad Norm: 0.00848894\n",
      "Epoch 4 | Step 2940900 | Avg Loss: 0.0158 | Grad Norm: 0.00761267\n",
      "Epoch 4 | Step 2941000 | Avg Loss: 0.0158 | Grad Norm: 0.00834378\n",
      "Epoch 4 | Step 2941100 | Avg Loss: 0.0156 | Grad Norm: 0.01193325\n",
      "Epoch 4 | Step 2941200 | Avg Loss: 0.0151 | Grad Norm: 0.00904519\n",
      "Epoch 4 | Step 2941300 | Avg Loss: 0.0155 | Grad Norm: 0.00819174\n",
      "Epoch 4 | Step 2941400 | Avg Loss: 0.0157 | Grad Norm: 0.00827454\n",
      "Epoch 4 | Step 2941500 | Avg Loss: 0.0153 | Grad Norm: 0.00804062\n",
      "Epoch 4 | Step 2941600 | Avg Loss: 0.0153 | Grad Norm: 0.00748264\n",
      "Epoch 4 | Step 2941700 | Avg Loss: 0.0156 | Grad Norm: 0.00855198\n",
      "Epoch 4 | Step 2941800 | Avg Loss: 0.0156 | Grad Norm: 0.00945173\n",
      "Epoch 4 | Step 2941900 | Avg Loss: 0.0157 | Grad Norm: 0.00938199\n",
      "Epoch 4 | Step 2942000 | Avg Loss: 0.0159 | Grad Norm: 0.00842879\n",
      "Epoch 4 | Step 2942100 | Avg Loss: 0.0158 | Grad Norm: 0.01189866\n",
      "Epoch 4 | Step 2942200 | Avg Loss: 0.0154 | Grad Norm: 0.00902153\n",
      "Epoch 4 | Step 2942300 | Avg Loss: 0.0153 | Grad Norm: 0.00858271\n",
      "Epoch 4 | Step 2942400 | Avg Loss: 0.0150 | Grad Norm: 0.00877350\n",
      "Epoch 4 | Step 2942500 | Avg Loss: 0.0149 | Grad Norm: 0.00908058\n",
      "Epoch 4 | Step 2942600 | Avg Loss: 0.0148 | Grad Norm: 0.01009561\n",
      "Epoch 4 | Step 2942700 | Avg Loss: 0.0153 | Grad Norm: 0.00935820\n",
      "Epoch 4 | Step 2942800 | Avg Loss: 0.0153 | Grad Norm: 0.00861996\n",
      "Epoch 4 | Step 2942900 | Avg Loss: 0.0155 | Grad Norm: 0.00860659\n",
      "Epoch 4 | Step 2943000 | Avg Loss: 0.0155 | Grad Norm: 0.00850191\n",
      "Epoch 4 | Step 2943100 | Avg Loss: 0.0158 | Grad Norm: 0.00865606\n",
      "Epoch 4 | Step 2943200 | Avg Loss: 0.0156 | Grad Norm: 0.00970294\n",
      "Epoch 4 | Step 2943300 | Avg Loss: 0.0154 | Grad Norm: 0.00865728\n",
      "Epoch 4 | Step 2943400 | Avg Loss: 0.0153 | Grad Norm: 0.01417944\n",
      "Epoch 4 | Step 2943500 | Avg Loss: 0.0154 | Grad Norm: 0.01028192\n",
      "Epoch 4 | Step 2943600 | Avg Loss: 0.0152 | Grad Norm: 0.00983447\n",
      "Epoch 4 | Step 2943700 | Avg Loss: 0.0154 | Grad Norm: 0.00784712\n",
      "Epoch 4 | Step 2943800 | Avg Loss: 0.0150 | Grad Norm: 0.00895024\n",
      "Epoch 4 | Step 2943900 | Avg Loss: 0.0153 | Grad Norm: 0.00830498\n",
      "Epoch 4 | Step 2944000 | Avg Loss: 0.0153 | Grad Norm: 0.00863957\n",
      "Epoch 4 | Step 2944100 | Avg Loss: 0.0150 | Grad Norm: 0.00943490\n",
      "Epoch 4 | Step 2944200 | Avg Loss: 0.0151 | Grad Norm: 0.00827141\n",
      "Epoch 4 | Step 2944300 | Avg Loss: 0.0153 | Grad Norm: 0.00875078\n",
      "Epoch 4 | Step 2944400 | Avg Loss: 0.0157 | Grad Norm: 0.00946296\n",
      "Epoch 4 | Step 2944500 | Avg Loss: 0.0158 | Grad Norm: 0.00904073\n",
      "Epoch 4 | Step 2944600 | Avg Loss: 0.0158 | Grad Norm: 0.00943697\n",
      "Epoch 4 | Step 2944700 | Avg Loss: 0.0157 | Grad Norm: 0.00903289\n",
      "Epoch 4 | Step 2944800 | Avg Loss: 0.0157 | Grad Norm: 0.00871853\n",
      "Epoch 4 | Step 2944900 | Avg Loss: 0.0158 | Grad Norm: 0.00807434\n",
      "Epoch 4 | Step 2945000 | Avg Loss: 0.0155 | Grad Norm: 0.00939617\n",
      "Epoch 4 | Step 2945100 | Avg Loss: 0.0157 | Grad Norm: 0.01101826\n",
      "Epoch 4 | Step 2945200 | Avg Loss: 0.0153 | Grad Norm: 0.00944490\n",
      "Epoch 4 | Step 2945300 | Avg Loss: 0.0156 | Grad Norm: 0.00943986\n",
      "Epoch 4 | Step 2945400 | Avg Loss: 0.0156 | Grad Norm: 0.01037528\n",
      "Epoch 4 | Step 2945500 | Avg Loss: 0.0157 | Grad Norm: 0.00987622\n",
      "Epoch 4 | Step 2945600 | Avg Loss: 0.0156 | Grad Norm: 0.00917545\n",
      "Epoch 4 | Step 2945700 | Avg Loss: 0.0154 | Grad Norm: 0.00870232\n",
      "Epoch 4 | Step 2945800 | Avg Loss: 0.0154 | Grad Norm: 0.00791166\n",
      "Epoch 4 | Step 2945900 | Avg Loss: 0.0157 | Grad Norm: 0.00936298\n",
      "Epoch 4 | Step 2946000 | Avg Loss: 0.0152 | Grad Norm: 0.00866844\n",
      "Epoch 4 | Step 2946100 | Avg Loss: 0.0153 | Grad Norm: 0.01075733\n",
      "Epoch 4 | Step 2946200 | Avg Loss: 0.0156 | Grad Norm: 0.00979012\n",
      "Epoch 4 | Step 2946300 | Avg Loss: 0.0155 | Grad Norm: 0.01095764\n",
      "Epoch 4 | Step 2946400 | Avg Loss: 0.0157 | Grad Norm: 0.00981312\n",
      "Epoch 4 | Step 2946500 | Avg Loss: 0.0153 | Grad Norm: 0.00993528\n",
      "Epoch 4 | Step 2946600 | Avg Loss: 0.0153 | Grad Norm: 0.00793159\n",
      "Epoch 4 | Step 2946700 | Avg Loss: 0.0153 | Grad Norm: 0.00889173\n",
      "Epoch 4 | Step 2946800 | Avg Loss: 0.0154 | Grad Norm: 0.01007383\n",
      "Epoch 4 | Step 2946900 | Avg Loss: 0.0154 | Grad Norm: 0.01008389\n",
      "Epoch 4 | Step 2947000 | Avg Loss: 0.0153 | Grad Norm: 0.00918288\n",
      "Epoch 4 | Step 2947100 | Avg Loss: 0.0152 | Grad Norm: 0.01123689\n",
      "Epoch 4 | Step 2947200 | Avg Loss: 0.0152 | Grad Norm: 0.00960988\n",
      "Epoch 4 | Step 2947300 | Avg Loss: 0.0156 | Grad Norm: 0.00893915\n",
      "Epoch 4 | Step 2947400 | Avg Loss: 0.0156 | Grad Norm: 0.00881455\n",
      "Epoch 4 | Step 2947500 | Avg Loss: 0.0153 | Grad Norm: 0.00801841\n",
      "Epoch 4 | Step 2947600 | Avg Loss: 0.0155 | Grad Norm: 0.00875558\n",
      "Epoch 4 | Step 2947700 | Avg Loss: 0.0157 | Grad Norm: 0.00844215\n",
      "Epoch 4 | Step 2947800 | Avg Loss: 0.0156 | Grad Norm: 0.00819390\n",
      "Epoch 4 | Step 2947900 | Avg Loss: 0.0157 | Grad Norm: 0.00908804\n",
      "Epoch 4 | Step 2948000 | Avg Loss: 0.0155 | Grad Norm: 0.00873867\n",
      "Epoch 4 | Step 2948100 | Avg Loss: 0.0156 | Grad Norm: 0.00876246\n",
      "Epoch 4 | Step 2948200 | Avg Loss: 0.0155 | Grad Norm: 0.00877230\n",
      "Epoch 4 | Step 2948300 | Avg Loss: 0.0157 | Grad Norm: 0.00867422\n",
      "Epoch 4 | Step 2948400 | Avg Loss: 0.0159 | Grad Norm: 0.00844863\n",
      "Epoch 4 | Step 2948500 | Avg Loss: 0.0157 | Grad Norm: 0.01083724\n",
      "Epoch 4 | Step 2948600 | Avg Loss: 0.0156 | Grad Norm: 0.01068848\n",
      "Epoch 4 | Step 2948700 | Avg Loss: 0.0155 | Grad Norm: 0.00869608\n",
      "Epoch 4 | Step 2948800 | Avg Loss: 0.0160 | Grad Norm: 0.00910875\n",
      "Epoch 4 | Step 2948900 | Avg Loss: 0.0162 | Grad Norm: 0.00972913\n",
      "Epoch 4 | Step 2949000 | Avg Loss: 0.0156 | Grad Norm: 0.00787475\n",
      "Epoch 4 | Step 2949100 | Avg Loss: 0.0154 | Grad Norm: 0.01008110\n",
      "Epoch 4 | Step 2949200 | Avg Loss: 0.0155 | Grad Norm: 0.00872787\n",
      "Epoch 4 | Step 2949300 | Avg Loss: 0.0156 | Grad Norm: 0.00930705\n",
      "Epoch 4 | Step 2949400 | Avg Loss: 0.0159 | Grad Norm: 0.00853336\n",
      "Epoch 4 | Step 2949500 | Avg Loss: 0.0154 | Grad Norm: 0.00791216\n",
      "Epoch 4 | Step 2949600 | Avg Loss: 0.0151 | Grad Norm: 0.00904610\n",
      "Epoch 4 | Step 2949700 | Avg Loss: 0.0145 | Grad Norm: 0.00882115\n",
      "Epoch 4 | Step 2949800 | Avg Loss: 0.0147 | Grad Norm: 0.00782574\n",
      "Epoch 4 | Step 2949900 | Avg Loss: 0.0151 | Grad Norm: 0.01042234\n",
      "Epoch 4 | Step 2950000 | Avg Loss: 0.0156 | Grad Norm: 0.00817921\n",
      "Epoch 4 | Step 2950100 | Avg Loss: 0.0159 | Grad Norm: 0.00882028\n",
      "Epoch 4 | Step 2950200 | Avg Loss: 0.0158 | Grad Norm: 0.00930057\n",
      "Epoch 4 | Step 2950300 | Avg Loss: 0.0158 | Grad Norm: 0.00955810\n",
      "Epoch 4 | Step 2950400 | Avg Loss: 0.0157 | Grad Norm: 0.00920250\n",
      "Epoch 4 | Step 2950500 | Avg Loss: 0.0156 | Grad Norm: 0.00878602\n",
      "Epoch 4 | Step 2950600 | Avg Loss: 0.0156 | Grad Norm: 0.00809166\n",
      "Epoch 4 | Step 2950700 | Avg Loss: 0.0154 | Grad Norm: 0.00931400\n",
      "Epoch 4 | Step 2950800 | Avg Loss: 0.0154 | Grad Norm: 0.00857416\n",
      "Epoch 4 | Step 2950900 | Avg Loss: 0.0154 | Grad Norm: 0.00789639\n",
      "Epoch 4 | Step 2951000 | Avg Loss: 0.0153 | Grad Norm: 0.00908404\n",
      "Epoch 4 | Step 2951100 | Avg Loss: 0.0151 | Grad Norm: 0.00899783\n",
      "Epoch 4 | Step 2951200 | Avg Loss: 0.0152 | Grad Norm: 0.00865971\n",
      "Epoch 4 | Step 2951300 | Avg Loss: 0.0155 | Grad Norm: 0.00963817\n",
      "Epoch 4 | Step 2951400 | Avg Loss: 0.0156 | Grad Norm: 0.00916979\n",
      "Epoch 4 | Step 2951500 | Avg Loss: 0.0156 | Grad Norm: 0.00998290\n",
      "Epoch 4 | Step 2951600 | Avg Loss: 0.0154 | Grad Norm: 0.00793205\n",
      "Epoch 4 | Step 2951700 | Avg Loss: 0.0154 | Grad Norm: 0.00905328\n",
      "Epoch 4 | Step 2951800 | Avg Loss: 0.0157 | Grad Norm: 0.00960683\n",
      "Epoch 4 | Step 2951900 | Avg Loss: 0.0154 | Grad Norm: 0.00857917\n",
      "Epoch 4 | Step 2952000 | Avg Loss: 0.0154 | Grad Norm: 0.00993826\n",
      "Epoch 4 | Step 2952100 | Avg Loss: 0.0155 | Grad Norm: 0.00930908\n",
      "Epoch 4 | Step 2952200 | Avg Loss: 0.0157 | Grad Norm: 0.00779543\n",
      "Epoch 4 | Step 2952300 | Avg Loss: 0.0155 | Grad Norm: 0.00832546\n",
      "Epoch 4 | Step 2952400 | Avg Loss: 0.0157 | Grad Norm: 0.00903225\n",
      "Epoch 4 | Step 2952500 | Avg Loss: 0.0160 | Grad Norm: 0.00895970\n",
      "Epoch 4 | Step 2952600 | Avg Loss: 0.0157 | Grad Norm: 0.01079912\n",
      "Epoch 4 | Step 2952700 | Avg Loss: 0.0158 | Grad Norm: 0.00925305\n",
      "Epoch 4 | Step 2952800 | Avg Loss: 0.0156 | Grad Norm: 0.00888731\n",
      "Epoch 4 | Step 2952900 | Avg Loss: 0.0153 | Grad Norm: 0.00969150\n",
      "Epoch 4 | Step 2953000 | Avg Loss: 0.0153 | Grad Norm: 0.00877816\n",
      "Epoch 4 | Step 2953100 | Avg Loss: 0.0154 | Grad Norm: 0.00865928\n",
      "Epoch 4 | Step 2953200 | Avg Loss: 0.0155 | Grad Norm: 0.00854402\n",
      "Epoch 4 | Step 2953300 | Avg Loss: 0.0151 | Grad Norm: 0.00953738\n",
      "Epoch 4 | Step 2953400 | Avg Loss: 0.0153 | Grad Norm: 0.00928813\n",
      "Epoch 4 | Step 2953500 | Avg Loss: 0.0154 | Grad Norm: 0.00943202\n",
      "Epoch 4 | Step 2953600 | Avg Loss: 0.0155 | Grad Norm: 0.00814440\n",
      "Epoch 4 | Step 2953700 | Avg Loss: 0.0156 | Grad Norm: 0.00821772\n",
      "Epoch 4 | Step 2953800 | Avg Loss: 0.0158 | Grad Norm: 0.00799556\n",
      "Epoch 4 | Step 2953900 | Avg Loss: 0.0161 | Grad Norm: 0.00842235\n",
      "Epoch 4 | Step 2954000 | Avg Loss: 0.0159 | Grad Norm: 0.00867012\n",
      "Epoch 4 | Step 2954100 | Avg Loss: 0.0155 | Grad Norm: 0.01107309\n",
      "Epoch 4 | Step 2954200 | Avg Loss: 0.0153 | Grad Norm: 0.01085171\n",
      "Epoch 4 | Step 2954300 | Avg Loss: 0.0154 | Grad Norm: 0.00855414\n",
      "Epoch 4 | Step 2954400 | Avg Loss: 0.0158 | Grad Norm: 0.00866002\n",
      "Epoch 4 | Step 2954500 | Avg Loss: 0.0155 | Grad Norm: 0.00853215\n",
      "Epoch 4 | Step 2954600 | Avg Loss: 0.0156 | Grad Norm: 0.00972565\n",
      "Epoch 4 | Step 2954700 | Avg Loss: 0.0156 | Grad Norm: 0.00930256\n",
      "Epoch 4 | Step 2954800 | Avg Loss: 0.0157 | Grad Norm: 0.00930033\n",
      "Epoch 4 | Step 2954900 | Avg Loss: 0.0156 | Grad Norm: 0.00957857\n",
      "Epoch 4 | Step 2955000 | Avg Loss: 0.0154 | Grad Norm: 0.01199143\n",
      "Epoch 4 | Step 2955100 | Avg Loss: 0.0151 | Grad Norm: 0.00930139\n",
      "Epoch 4 | Step 2955200 | Avg Loss: 0.0152 | Grad Norm: 0.00905866\n",
      "Epoch 4 | Step 2955300 | Avg Loss: 0.0156 | Grad Norm: 0.01001438\n",
      "Epoch 4 | Step 2955400 | Avg Loss: 0.0158 | Grad Norm: 0.00855904\n",
      "Epoch 4 | Step 2955500 | Avg Loss: 0.0157 | Grad Norm: 0.00873628\n",
      "Epoch 4 | Step 2955600 | Avg Loss: 0.0157 | Grad Norm: 0.00919937\n",
      "Epoch 4 | Step 2955700 | Avg Loss: 0.0156 | Grad Norm: 0.00975212\n",
      "Epoch 4 | Step 2955800 | Avg Loss: 0.0155 | Grad Norm: 0.01177770\n",
      "Epoch 4 | Step 2955900 | Avg Loss: 0.0160 | Grad Norm: 0.01140341\n",
      "Epoch 4 | Step 2956000 | Avg Loss: 0.0157 | Grad Norm: 0.00967515\n",
      "Epoch 4 | Step 2956100 | Avg Loss: 0.0156 | Grad Norm: 0.00919567\n",
      "Epoch 4 | Step 2956200 | Avg Loss: 0.0156 | Grad Norm: 0.00807091\n",
      "Epoch 4 | Step 2956300 | Avg Loss: 0.0155 | Grad Norm: 0.00845817\n",
      "Epoch 4 | Step 2956400 | Avg Loss: 0.0155 | Grad Norm: 0.01075293\n",
      "Epoch 4 | Step 2956500 | Avg Loss: 0.0154 | Grad Norm: 0.01756987\n",
      "Epoch 4 | Step 2956600 | Avg Loss: 0.0153 | Grad Norm: 0.00920253\n",
      "Epoch 4 | Step 2956700 | Avg Loss: 0.0153 | Grad Norm: 0.00913002\n",
      "Epoch 4 | Step 2956800 | Avg Loss: 0.0157 | Grad Norm: 0.00760936\n",
      "Epoch 4 | Step 2956900 | Avg Loss: 0.0158 | Grad Norm: 0.00904523\n",
      "Epoch 4 | Step 2957000 | Avg Loss: 0.0155 | Grad Norm: 0.00852176\n",
      "Epoch 4 | Step 2957100 | Avg Loss: 0.0158 | Grad Norm: 0.00983008\n",
      "Epoch 4 | Step 2957200 | Avg Loss: 0.0157 | Grad Norm: 0.00874151\n",
      "Epoch 4 | Step 2957300 | Avg Loss: 0.0157 | Grad Norm: 0.00879161\n",
      "Epoch 4 | Step 2957400 | Avg Loss: 0.0160 | Grad Norm: 0.00831545\n",
      "Epoch 4 | Step 2957500 | Avg Loss: 0.0161 | Grad Norm: 0.00955306\n",
      "Epoch 4 | Step 2957600 | Avg Loss: 0.0158 | Grad Norm: 0.01071704\n",
      "Epoch 4 | Step 2957700 | Avg Loss: 0.0157 | Grad Norm: 0.00968716\n",
      "Epoch 4 | Step 2957800 | Avg Loss: 0.0157 | Grad Norm: 0.00873047\n",
      "Epoch 4 | Step 2957900 | Avg Loss: 0.0158 | Grad Norm: 0.00848054\n",
      "Epoch 4 | Step 2958000 | Avg Loss: 0.0159 | Grad Norm: 0.00964307\n",
      "Epoch 4 | Step 2958100 | Avg Loss: 0.0156 | Grad Norm: 0.00861137\n",
      "Epoch 4 | Step 2958200 | Avg Loss: 0.0153 | Grad Norm: 0.00827936\n",
      "Epoch 4 | Step 2958300 | Avg Loss: 0.0155 | Grad Norm: 0.00939319\n",
      "Epoch 4 | Step 2958400 | Avg Loss: 0.0155 | Grad Norm: 0.00997343\n",
      "Epoch 4 | Step 2958500 | Avg Loss: 0.0154 | Grad Norm: 0.00936233\n",
      "Epoch 4 | Step 2958600 | Avg Loss: 0.0156 | Grad Norm: 0.00865697\n",
      "Epoch 4 | Step 2958700 | Avg Loss: 0.0159 | Grad Norm: 0.01155574\n",
      "Epoch 4 | Step 2958800 | Avg Loss: 0.0158 | Grad Norm: 0.01114782\n",
      "Epoch 4 | Step 2958900 | Avg Loss: 0.0155 | Grad Norm: 0.00842272\n",
      "Epoch 4 | Step 2959000 | Avg Loss: 0.0156 | Grad Norm: 0.01028645\n",
      "Epoch 4 | Step 2959100 | Avg Loss: 0.0157 | Grad Norm: 0.00913156\n",
      "Epoch 4 | Step 2959200 | Avg Loss: 0.0157 | Grad Norm: 0.00923229\n",
      "Epoch 4 | Step 2959300 | Avg Loss: 0.0156 | Grad Norm: 0.00917319\n",
      "Epoch 4 | Step 2959400 | Avg Loss: 0.0156 | Grad Norm: 0.00839986\n",
      "Epoch 4 | Step 2959500 | Avg Loss: 0.0155 | Grad Norm: 0.00864856\n",
      "Epoch 4 | Step 2959600 | Avg Loss: 0.0157 | Grad Norm: 0.01016694\n",
      "Epoch 4 | Step 2959700 | Avg Loss: 0.0159 | Grad Norm: 0.00893492\n",
      "Epoch 4 | Step 2959800 | Avg Loss: 0.0158 | Grad Norm: 0.00864028\n",
      "Epoch 4 | Step 2959900 | Avg Loss: 0.0160 | Grad Norm: 0.00921302\n",
      "Epoch 4 | Step 2960000 | Avg Loss: 0.0157 | Grad Norm: 0.00830606\n",
      "Epoch 4 | Step 2960100 | Avg Loss: 0.0158 | Grad Norm: 0.01075830\n",
      "Epoch 4 | Step 2960200 | Avg Loss: 0.0157 | Grad Norm: 0.00821217\n",
      "Epoch 4 | Step 2960300 | Avg Loss: 0.0159 | Grad Norm: 0.00814362\n",
      "Epoch 4 | Step 2960400 | Avg Loss: 0.0156 | Grad Norm: 0.00883642\n",
      "Epoch 4 | Step 2960500 | Avg Loss: 0.0159 | Grad Norm: 0.00956352\n",
      "Epoch 4 | Step 2960600 | Avg Loss: 0.0157 | Grad Norm: 0.00779045\n",
      "Epoch 4 | Step 2960700 | Avg Loss: 0.0161 | Grad Norm: 0.00805619\n",
      "Epoch 4 | Step 2960800 | Avg Loss: 0.0158 | Grad Norm: 0.00890202\n",
      "Epoch 4 | Step 2960900 | Avg Loss: 0.0154 | Grad Norm: 0.01073572\n",
      "Epoch 4 | Step 2961000 | Avg Loss: 0.0154 | Grad Norm: 0.01034342\n",
      "Epoch 4 | Step 2961100 | Avg Loss: 0.0155 | Grad Norm: 0.01138954\n",
      "Epoch 4 | Step 2961200 | Avg Loss: 0.0155 | Grad Norm: 0.00922440\n",
      "Epoch 4 | Step 2961300 | Avg Loss: 0.0158 | Grad Norm: 0.00900501\n",
      "Epoch 4 | Step 2961400 | Avg Loss: 0.0158 | Grad Norm: 0.00981842\n",
      "Epoch 4 | Step 2961500 | Avg Loss: 0.0159 | Grad Norm: 0.01138624\n",
      "Epoch 4 | Step 2961600 | Avg Loss: 0.0157 | Grad Norm: 0.00954754\n",
      "Epoch 4 | Step 2961700 | Avg Loss: 0.0159 | Grad Norm: 0.00880809\n",
      "Epoch 4 | Step 2961800 | Avg Loss: 0.0157 | Grad Norm: 0.00873591\n",
      "Epoch 4 | Step 2961900 | Avg Loss: 0.0157 | Grad Norm: 0.00805625\n",
      "Epoch 4 | Step 2962000 | Avg Loss: 0.0152 | Grad Norm: 0.00991075\n",
      "Epoch 4 | Step 2962100 | Avg Loss: 0.0154 | Grad Norm: 0.00980148\n",
      "Epoch 4 | Step 2962200 | Avg Loss: 0.0152 | Grad Norm: 0.00888535\n",
      "Epoch 4 | Step 2962300 | Avg Loss: 0.0150 | Grad Norm: 0.00895871\n",
      "Epoch 4 | Step 2962400 | Avg Loss: 0.0153 | Grad Norm: 0.00843352\n",
      "Epoch 4 | Step 2962500 | Avg Loss: 0.0153 | Grad Norm: 0.00800265\n",
      "Epoch 4 | Step 2962600 | Avg Loss: 0.0154 | Grad Norm: 0.00854520\n",
      "Epoch 4 | Step 2962700 | Avg Loss: 0.0155 | Grad Norm: 0.01113450\n",
      "Epoch 4 | Step 2962800 | Avg Loss: 0.0153 | Grad Norm: 0.00936426\n",
      "Epoch 4 | Step 2962900 | Avg Loss: 0.0153 | Grad Norm: 0.00874375\n",
      "Epoch 4 | Step 2963000 | Avg Loss: 0.0155 | Grad Norm: 0.00943455\n",
      "Epoch 4 | Step 2963100 | Avg Loss: 0.0157 | Grad Norm: 0.00878004\n",
      "Epoch 4 | Step 2963200 | Avg Loss: 0.0158 | Grad Norm: 0.00901834\n",
      "Epoch 4 | Step 2963300 | Avg Loss: 0.0158 | Grad Norm: 0.00790566\n",
      "Epoch 4 | Step 2963400 | Avg Loss: 0.0159 | Grad Norm: 0.01001001\n",
      "Epoch 4 | Step 2963500 | Avg Loss: 0.0158 | Grad Norm: 0.01070107\n",
      "Epoch 4 | Step 2963600 | Avg Loss: 0.0157 | Grad Norm: 0.00945895\n",
      "Epoch 4 | Step 2963700 | Avg Loss: 0.0158 | Grad Norm: 0.00942285\n",
      "Epoch 4 | Step 2963800 | Avg Loss: 0.0157 | Grad Norm: 0.00944813\n",
      "Epoch 4 | Step 2963900 | Avg Loss: 0.0155 | Grad Norm: 0.00862807\n",
      "Epoch 4 | Step 2964000 | Avg Loss: 0.0156 | Grad Norm: 0.01044643\n",
      "Epoch 4 | Step 2964100 | Avg Loss: 0.0157 | Grad Norm: 0.01003766\n",
      "Epoch 4 | Step 2964200 | Avg Loss: 0.0159 | Grad Norm: 0.01076314\n",
      "Epoch 4 | Step 2964300 | Avg Loss: 0.0156 | Grad Norm: 0.00910099\n",
      "Epoch 4 | Step 2964400 | Avg Loss: 0.0156 | Grad Norm: 0.00932296\n",
      "Epoch 4 | Step 2964500 | Avg Loss: 0.0159 | Grad Norm: 0.00942901\n",
      "Epoch 4 | Step 2964600 | Avg Loss: 0.0158 | Grad Norm: 0.00897638\n",
      "Epoch 4 | Step 2964700 | Avg Loss: 0.0156 | Grad Norm: 0.00956410\n",
      "Epoch 4 | Step 2964800 | Avg Loss: 0.0155 | Grad Norm: 0.00934518\n",
      "Epoch 4 | Step 2964900 | Avg Loss: 0.0156 | Grad Norm: 0.00816238\n",
      "Epoch 4 | Step 2965000 | Avg Loss: 0.0154 | Grad Norm: 0.00938017\n",
      "Epoch 4 | Step 2965100 | Avg Loss: 0.0151 | Grad Norm: 0.00891423\n",
      "Epoch 4 | Step 2965200 | Avg Loss: 0.0152 | Grad Norm: 0.00846627\n",
      "Epoch 4 | Step 2965300 | Avg Loss: 0.0151 | Grad Norm: 0.00826436\n",
      "Epoch 4 | Step 2965400 | Avg Loss: 0.0154 | Grad Norm: 0.00909920\n",
      "Epoch 4 | Step 2965500 | Avg Loss: 0.0155 | Grad Norm: 0.00843981\n",
      "Epoch 4 | Step 2965600 | Avg Loss: 0.0155 | Grad Norm: 0.00880696\n",
      "Epoch 4 | Step 2965700 | Avg Loss: 0.0155 | Grad Norm: 0.00782091\n",
      "Epoch 4 | Step 2965800 | Avg Loss: 0.0154 | Grad Norm: 0.00984813\n",
      "Epoch 4 | Step 2965900 | Avg Loss: 0.0154 | Grad Norm: 0.00890455\n",
      "Epoch 4 | Step 2966000 | Avg Loss: 0.0154 | Grad Norm: 0.00868413\n",
      "Epoch 4 | Step 2966100 | Avg Loss: 0.0152 | Grad Norm: 0.00885256\n",
      "Epoch 4 | Step 2966200 | Avg Loss: 0.0153 | Grad Norm: 0.00850283\n",
      "Epoch 4 | Step 2966300 | Avg Loss: 0.0155 | Grad Norm: 0.00866758\n",
      "Epoch 4 | Step 2966400 | Avg Loss: 0.0158 | Grad Norm: 0.00751537\n",
      "Epoch 4 | Step 2966500 | Avg Loss: 0.0149 | Grad Norm: 0.00758863\n",
      "Epoch 4 | Step 2966600 | Avg Loss: 0.0150 | Grad Norm: 0.01101564\n",
      "Epoch 4 | Step 2966700 | Avg Loss: 0.0154 | Grad Norm: 0.00854401\n",
      "Epoch 4 | Step 2966800 | Avg Loss: 0.0152 | Grad Norm: 0.00855165\n",
      "Epoch 4 | Step 2966900 | Avg Loss: 0.0151 | Grad Norm: 0.00965104\n",
      "Epoch 4 | Step 2967000 | Avg Loss: 0.0152 | Grad Norm: 0.00936529\n",
      "Epoch 4 | Step 2967100 | Avg Loss: 0.0151 | Grad Norm: 0.00901074\n",
      "Epoch 4 | Step 2967200 | Avg Loss: 0.0150 | Grad Norm: 0.00745879\n",
      "Epoch 4 | Step 2967300 | Avg Loss: 0.0146 | Grad Norm: 0.00844750\n",
      "Epoch 4 | Step 2967400 | Avg Loss: 0.0148 | Grad Norm: 0.00905066\n",
      "Epoch 4 | Step 2967500 | Avg Loss: 0.0151 | Grad Norm: 0.00975147\n",
      "Epoch 4 | Step 2967600 | Avg Loss: 0.0155 | Grad Norm: 0.00796712\n",
      "Epoch 4 | Step 2967700 | Avg Loss: 0.0155 | Grad Norm: 0.00981485\n",
      "Epoch 4 | Step 2967800 | Avg Loss: 0.0151 | Grad Norm: 0.00870700\n",
      "Epoch 4 | Step 2967900 | Avg Loss: 0.0152 | Grad Norm: 0.00829823\n",
      "Epoch 4 | Step 2968000 | Avg Loss: 0.0154 | Grad Norm: 0.00961450\n",
      "Epoch 4 | Step 2968100 | Avg Loss: 0.0154 | Grad Norm: 0.00859240\n",
      "Epoch 4 | Step 2968200 | Avg Loss: 0.0153 | Grad Norm: 0.00947050\n",
      "Epoch 4 | Step 2968300 | Avg Loss: 0.0153 | Grad Norm: 0.01030610\n",
      "Epoch 4 | Step 2968400 | Avg Loss: 0.0152 | Grad Norm: 0.00777394\n",
      "Epoch 4 | Step 2968500 | Avg Loss: 0.0155 | Grad Norm: 0.00880025\n",
      "Epoch 4 | Step 2968600 | Avg Loss: 0.0154 | Grad Norm: 0.00855980\n",
      "Epoch 4 | Step 2968700 | Avg Loss: 0.0153 | Grad Norm: 0.00947183\n",
      "Epoch 4 | Step 2968800 | Avg Loss: 0.0153 | Grad Norm: 0.00907237\n",
      "Epoch 4 | Step 2968900 | Avg Loss: 0.0150 | Grad Norm: 0.00806149\n",
      "Epoch 4 | Step 2969000 | Avg Loss: 0.0148 | Grad Norm: 0.00838200\n",
      "Epoch 4 | Step 2969100 | Avg Loss: 0.0147 | Grad Norm: 0.00808038\n",
      "Epoch 4 | Step 2969200 | Avg Loss: 0.0149 | Grad Norm: 0.01113069\n",
      "Epoch 4 | Step 2969300 | Avg Loss: 0.0150 | Grad Norm: 0.00902288\n",
      "Epoch 4 | Step 2969400 | Avg Loss: 0.0153 | Grad Norm: 0.01008590\n",
      "Epoch 4 | Step 2969500 | Avg Loss: 0.0154 | Grad Norm: 0.01017995\n",
      "Epoch 4 | Step 2969600 | Avg Loss: 0.0150 | Grad Norm: 0.00724218\n",
      "Epoch 4 | Step 2969700 | Avg Loss: 0.0150 | Grad Norm: 0.00906219\n",
      "Epoch 4 | Step 2969800 | Avg Loss: 0.0153 | Grad Norm: 0.01029472\n",
      "Epoch 4 | Step 2969900 | Avg Loss: 0.0152 | Grad Norm: 0.00955148\n",
      "Epoch 4 | Step 2970000 | Avg Loss: 0.0150 | Grad Norm: 0.00831586\n",
      "Epoch 4 | Step 2970100 | Avg Loss: 0.0154 | Grad Norm: 0.01178859\n",
      "Epoch 4 | Step 2970200 | Avg Loss: 0.0154 | Grad Norm: 0.00808348\n",
      "Epoch 4 | Step 2970300 | Avg Loss: 0.0156 | Grad Norm: 0.01043571\n",
      "Epoch 4 | Step 2970400 | Avg Loss: 0.0157 | Grad Norm: 0.00898880\n",
      "Epoch 4 | Step 2970500 | Avg Loss: 0.0155 | Grad Norm: 0.00767052\n",
      "Epoch 4 | Step 2970600 | Avg Loss: 0.0157 | Grad Norm: 0.00837988\n",
      "Epoch 4 | Step 2970700 | Avg Loss: 0.0158 | Grad Norm: 0.00810047\n",
      "Epoch 4 | Step 2970800 | Avg Loss: 0.0155 | Grad Norm: 0.00962007\n",
      "Epoch 4 | Step 2970900 | Avg Loss: 0.0152 | Grad Norm: 0.00936784\n",
      "Epoch 4 | Step 2971000 | Avg Loss: 0.0153 | Grad Norm: 0.00834088\n",
      "Epoch 4 | Step 2971100 | Avg Loss: 0.0153 | Grad Norm: 0.00863808\n",
      "Epoch 4 | Step 2971200 | Avg Loss: 0.0156 | Grad Norm: 0.00817336\n",
      "Epoch 4 | Step 2971300 | Avg Loss: 0.0151 | Grad Norm: 0.00798383\n",
      "Epoch 4 | Step 2971400 | Avg Loss: 0.0151 | Grad Norm: 0.00871026\n",
      "Epoch 4 | Step 2971500 | Avg Loss: 0.0155 | Grad Norm: 0.00929561\n",
      "Epoch 4 | Step 2971600 | Avg Loss: 0.0155 | Grad Norm: 0.00759171\n",
      "Epoch 4 | Step 2971700 | Avg Loss: 0.0157 | Grad Norm: 0.00829989\n",
      "Epoch 4 | Step 2971800 | Avg Loss: 0.0154 | Grad Norm: 0.01045762\n",
      "Epoch 4 | Step 2971900 | Avg Loss: 0.0151 | Grad Norm: 0.00849538\n",
      "Epoch 4 | Step 2972000 | Avg Loss: 0.0150 | Grad Norm: 0.00809685\n",
      "Epoch 4 | Step 2972100 | Avg Loss: 0.0150 | Grad Norm: 0.00808395\n",
      "Epoch 4 | Step 2972200 | Avg Loss: 0.0148 | Grad Norm: 0.00864121\n",
      "Epoch 4 | Step 2972300 | Avg Loss: 0.0155 | Grad Norm: 0.00892999\n",
      "Epoch 4 | Step 2972400 | Avg Loss: 0.0155 | Grad Norm: 0.00945976\n",
      "Epoch 4 | Step 2972500 | Avg Loss: 0.0157 | Grad Norm: 0.00910722\n",
      "Epoch 4 | Step 2972600 | Avg Loss: 0.0151 | Grad Norm: 0.00902729\n",
      "Epoch 4 | Step 2972700 | Avg Loss: 0.0151 | Grad Norm: 0.00887381\n",
      "Epoch 4 | Step 2972800 | Avg Loss: 0.0154 | Grad Norm: 0.00938189\n",
      "Epoch 4 | Step 2972900 | Avg Loss: 0.0154 | Grad Norm: 0.00902924\n",
      "Epoch 4 | Step 2973000 | Avg Loss: 0.0154 | Grad Norm: 0.00791784\n",
      "Epoch 4 | Step 2973100 | Avg Loss: 0.0151 | Grad Norm: 0.00914589\n",
      "Epoch 4 | Step 2973200 | Avg Loss: 0.0156 | Grad Norm: 0.00813595\n",
      "Epoch 4 | Step 2973300 | Avg Loss: 0.0156 | Grad Norm: 0.01354242\n",
      "Epoch 4 | Step 2973400 | Avg Loss: 0.0158 | Grad Norm: 0.00845284\n",
      "Epoch 4 | Step 2973500 | Avg Loss: 0.0160 | Grad Norm: 0.00946114\n",
      "Epoch 4 | Step 2973600 | Avg Loss: 0.0159 | Grad Norm: 0.00906874\n",
      "Epoch 4 | Step 2973700 | Avg Loss: 0.0158 | Grad Norm: 0.00933442\n",
      "Epoch 4 | Step 2973800 | Avg Loss: 0.0155 | Grad Norm: 0.00942333\n",
      "Epoch 4 | Step 2973900 | Avg Loss: 0.0156 | Grad Norm: 0.01061991\n",
      "Epoch 4 | Step 2974000 | Avg Loss: 0.0155 | Grad Norm: 0.01089565\n",
      "Epoch 4 | Step 2974100 | Avg Loss: 0.0157 | Grad Norm: 0.00801281\n",
      "Epoch 4 | Step 2974200 | Avg Loss: 0.0158 | Grad Norm: 0.00927884\n",
      "Epoch 4 | Step 2974300 | Avg Loss: 0.0156 | Grad Norm: 0.00769145\n",
      "Epoch 4 | Step 2974400 | Avg Loss: 0.0155 | Grad Norm: 0.00973090\n",
      "Epoch 4 | Step 2974500 | Avg Loss: 0.0157 | Grad Norm: 0.00850071\n",
      "Epoch 4 | Step 2974600 | Avg Loss: 0.0156 | Grad Norm: 0.00963235\n",
      "Epoch 4 | Step 2974700 | Avg Loss: 0.0160 | Grad Norm: 0.00924587\n",
      "Epoch 4 | Step 2974800 | Avg Loss: 0.0163 | Grad Norm: 0.01106141\n",
      "Epoch 4 | Step 2974900 | Avg Loss: 0.0160 | Grad Norm: 0.00883625\n",
      "Epoch 4 | Step 2975000 | Avg Loss: 0.0156 | Grad Norm: 0.00889959\n",
      "Epoch 4 | Step 2975100 | Avg Loss: 0.0158 | Grad Norm: 0.00859927\n",
      "Epoch 4 | Step 2975200 | Avg Loss: 0.0162 | Grad Norm: 0.00894083\n",
      "Epoch 4 | Step 2975300 | Avg Loss: 0.0162 | Grad Norm: 0.00905041\n",
      "Epoch 4 | Step 2975400 | Avg Loss: 0.0161 | Grad Norm: 0.00908105\n",
      "Epoch 4 | Step 2975500 | Avg Loss: 0.0156 | Grad Norm: 0.00806918\n",
      "Epoch 4 | Step 2975600 | Avg Loss: 0.0157 | Grad Norm: 0.00829824\n",
      "Epoch 4 | Step 2975700 | Avg Loss: 0.0157 | Grad Norm: 0.00806068\n",
      "Epoch 4 | Step 2975800 | Avg Loss: 0.0156 | Grad Norm: 0.00964646\n",
      "Epoch 4 | Step 2975900 | Avg Loss: 0.0154 | Grad Norm: 0.01104763\n",
      "Epoch 4 | Step 2976000 | Avg Loss: 0.0152 | Grad Norm: 0.01055005\n",
      "Epoch 4 | Step 2976100 | Avg Loss: 0.0154 | Grad Norm: 0.01039994\n",
      "Epoch 4 | Step 2976200 | Avg Loss: 0.0151 | Grad Norm: 0.00918619\n",
      "Epoch 4 | Step 2976300 | Avg Loss: 0.0151 | Grad Norm: 0.00857292\n",
      "Epoch 4 | Step 2976400 | Avg Loss: 0.0154 | Grad Norm: 0.00946358\n",
      "Epoch 4 | Step 2976500 | Avg Loss: 0.0157 | Grad Norm: 0.00868556\n",
      "Epoch 4 | Step 2976600 | Avg Loss: 0.0155 | Grad Norm: 0.00960855\n",
      "Epoch 4 | Step 2976700 | Avg Loss: 0.0155 | Grad Norm: 0.00944105\n",
      "Epoch 4 | Step 2976800 | Avg Loss: 0.0156 | Grad Norm: 0.00969348\n",
      "Epoch 4 | Step 2976900 | Avg Loss: 0.0152 | Grad Norm: 0.00832722\n",
      "Epoch 4 | Step 2977000 | Avg Loss: 0.0149 | Grad Norm: 0.00988670\n",
      "Epoch 4 | Step 2977100 | Avg Loss: 0.0152 | Grad Norm: 0.00949452\n",
      "Epoch 4 | Step 2977200 | Avg Loss: 0.0154 | Grad Norm: 0.00984730\n",
      "Epoch 4 | Step 2977300 | Avg Loss: 0.0155 | Grad Norm: 0.00949555\n",
      "Epoch 4 | Step 2977400 | Avg Loss: 0.0152 | Grad Norm: 0.01049628\n",
      "Epoch 4 | Step 2977500 | Avg Loss: 0.0154 | Grad Norm: 0.00801130\n",
      "Epoch 4 | Step 2977600 | Avg Loss: 0.0152 | Grad Norm: 0.00968268\n",
      "Epoch 4 | Step 2977700 | Avg Loss: 0.0152 | Grad Norm: 0.00763658\n",
      "Epoch 4 | Step 2977800 | Avg Loss: 0.0151 | Grad Norm: 0.00936425\n",
      "Epoch 4 | Step 2977900 | Avg Loss: 0.0153 | Grad Norm: 0.00858493\n",
      "Epoch 4 | Step 2978000 | Avg Loss: 0.0153 | Grad Norm: 0.00850968\n",
      "Epoch 4 | Step 2978100 | Avg Loss: 0.0152 | Grad Norm: 0.00988803\n",
      "Epoch 4 | Step 2978200 | Avg Loss: 0.0158 | Grad Norm: 0.00958342\n",
      "Epoch 4 | Step 2978300 | Avg Loss: 0.0155 | Grad Norm: 0.01007675\n",
      "Epoch 4 | Step 2978400 | Avg Loss: 0.0153 | Grad Norm: 0.00905138\n",
      "Epoch 4 | Step 2978500 | Avg Loss: 0.0157 | Grad Norm: 0.01010278\n",
      "Epoch 4 | Step 2978600 | Avg Loss: 0.0158 | Grad Norm: 0.01036722\n",
      "Epoch 4 | Step 2978700 | Avg Loss: 0.0160 | Grad Norm: 0.00970796\n",
      "Epoch 4 | Step 2978800 | Avg Loss: 0.0156 | Grad Norm: 0.00977176\n",
      "Epoch 4 | Step 2978900 | Avg Loss: 0.0158 | Grad Norm: 0.00902180\n",
      "Epoch 4 | Step 2979000 | Avg Loss: 0.0161 | Grad Norm: 0.00984980\n",
      "Epoch 4 | Step 2979100 | Avg Loss: 0.0160 | Grad Norm: 0.00951790\n",
      "Epoch 4 | Step 2979200 | Avg Loss: 0.0153 | Grad Norm: 0.01019286\n",
      "Epoch 4 | Step 2979300 | Avg Loss: 0.0156 | Grad Norm: 0.00970492\n",
      "Epoch 4 | Step 2979400 | Avg Loss: 0.0160 | Grad Norm: 0.00953960\n",
      "Epoch 4 | Step 2979500 | Avg Loss: 0.0160 | Grad Norm: 0.00792309\n",
      "Epoch 4 | Step 2979600 | Avg Loss: 0.0158 | Grad Norm: 0.00860019\n",
      "Epoch 4 | Step 2979700 | Avg Loss: 0.0159 | Grad Norm: 0.00805500\n",
      "Epoch 4 | Step 2979800 | Avg Loss: 0.0160 | Grad Norm: 0.00898499\n",
      "Epoch 4 | Step 2979900 | Avg Loss: 0.0159 | Grad Norm: 0.00936564\n",
      "Epoch 4 | Step 2980000 | Avg Loss: 0.0161 | Grad Norm: 0.00915023\n",
      "Epoch 4 | Step 2980100 | Avg Loss: 0.0160 | Grad Norm: 0.00962319\n",
      "Epoch 4 | Step 2980200 | Avg Loss: 0.0157 | Grad Norm: 0.00925013\n",
      "Epoch 4 | Step 2980300 | Avg Loss: 0.0156 | Grad Norm: 0.00914342\n",
      "Epoch 4 | Step 2980400 | Avg Loss: 0.0153 | Grad Norm: 0.00873565\n",
      "Epoch 4 | Step 2980500 | Avg Loss: 0.0154 | Grad Norm: 0.00829124\n",
      "Epoch 4 | Step 2980600 | Avg Loss: 0.0153 | Grad Norm: 0.00831137\n",
      "Epoch 4 | Step 2980700 | Avg Loss: 0.0153 | Grad Norm: 0.00898698\n",
      "Epoch 4 | Step 2980800 | Avg Loss: 0.0154 | Grad Norm: 0.01433529\n",
      "Epoch 4 | Step 2980900 | Avg Loss: 0.0153 | Grad Norm: 0.00856379\n",
      "Epoch 4 | Step 2981000 | Avg Loss: 0.0154 | Grad Norm: 0.00949787\n",
      "Epoch 4 | Step 2981100 | Avg Loss: 0.0151 | Grad Norm: 0.00912724\n",
      "Epoch 4 | Step 2981200 | Avg Loss: 0.0157 | Grad Norm: 0.01048957\n",
      "Epoch 4 | Step 2981300 | Avg Loss: 0.0153 | Grad Norm: 0.00961826\n",
      "Epoch 4 | Step 2981400 | Avg Loss: 0.0157 | Grad Norm: 0.00931653\n",
      "Epoch 4 | Step 2981500 | Avg Loss: 0.0157 | Grad Norm: 0.00899372\n",
      "Epoch 4 | Step 2981600 | Avg Loss: 0.0157 | Grad Norm: 0.01039721\n",
      "Epoch 4 | Step 2981700 | Avg Loss: 0.0158 | Grad Norm: 0.01008561\n",
      "Epoch 4 | Step 2981800 | Avg Loss: 0.0158 | Grad Norm: 0.00907556\n",
      "Epoch 4 | Step 2981900 | Avg Loss: 0.0160 | Grad Norm: 0.01042882\n",
      "Epoch 4 | Step 2982000 | Avg Loss: 0.0155 | Grad Norm: 0.00763078\n",
      "Epoch 4 | Step 2982100 | Avg Loss: 0.0155 | Grad Norm: 0.00821081\n",
      "Epoch 4 | Step 2982200 | Avg Loss: 0.0155 | Grad Norm: 0.00989317\n",
      "Epoch 4 | Step 2982300 | Avg Loss: 0.0157 | Grad Norm: 0.00933219\n",
      "Epoch 4 | Step 2982400 | Avg Loss: 0.0156 | Grad Norm: 0.00992323\n",
      "Epoch 4 | Step 2982500 | Avg Loss: 0.0153 | Grad Norm: 0.01007779\n",
      "Epoch 4 | Step 2982600 | Avg Loss: 0.0154 | Grad Norm: 0.00761301\n",
      "Epoch 4 | Step 2982700 | Avg Loss: 0.0155 | Grad Norm: 0.00877849\n",
      "Epoch 4 | Step 2982800 | Avg Loss: 0.0154 | Grad Norm: 0.00835877\n",
      "Epoch 4 | Step 2982900 | Avg Loss: 0.0155 | Grad Norm: 0.01016968\n",
      "Epoch 4 | Step 2983000 | Avg Loss: 0.0153 | Grad Norm: 0.00902366\n",
      "Epoch 4 | Step 2983100 | Avg Loss: 0.0154 | Grad Norm: 0.00911791\n",
      "Epoch 4 | Step 2983200 | Avg Loss: 0.0160 | Grad Norm: 0.01006493\n",
      "Epoch 4 | Step 2983300 | Avg Loss: 0.0158 | Grad Norm: 0.00941216\n",
      "Epoch 4 | Step 2983400 | Avg Loss: 0.0157 | Grad Norm: 0.00862929\n",
      "Epoch 4 | Step 2983500 | Avg Loss: 0.0157 | Grad Norm: 0.00819976\n",
      "Epoch 4 | Step 2983600 | Avg Loss: 0.0157 | Grad Norm: 0.00949437\n",
      "Epoch 4 | Step 2983700 | Avg Loss: 0.0159 | Grad Norm: 0.00869995\n",
      "Epoch 4 | Step 2983800 | Avg Loss: 0.0160 | Grad Norm: 0.00996586\n",
      "Epoch 4 | Step 2983900 | Avg Loss: 0.0155 | Grad Norm: 0.01164935\n",
      "Epoch 4 | Step 2984000 | Avg Loss: 0.0153 | Grad Norm: 0.00828523\n",
      "Epoch 4 | Step 2984100 | Avg Loss: 0.0154 | Grad Norm: 0.00908134\n",
      "Epoch 4 | Step 2984200 | Avg Loss: 0.0152 | Grad Norm: 0.00891279\n",
      "Epoch 4 | Step 2984300 | Avg Loss: 0.0153 | Grad Norm: 0.00833292\n",
      "Epoch 4 | Step 2984400 | Avg Loss: 0.0153 | Grad Norm: 0.00834949\n",
      "Epoch 4 | Step 2984500 | Avg Loss: 0.0151 | Grad Norm: 0.00983724\n",
      "Epoch 4 | Step 2984600 | Avg Loss: 0.0150 | Grad Norm: 0.00905890\n",
      "Epoch 4 | Step 2984700 | Avg Loss: 0.0150 | Grad Norm: 0.00836216\n",
      "Epoch 4 | Step 2984800 | Avg Loss: 0.0154 | Grad Norm: 0.00743455\n",
      "Epoch 4 | Step 2984900 | Avg Loss: 0.0154 | Grad Norm: 0.00977671\n",
      "Epoch 4 | Step 2985000 | Avg Loss: 0.0153 | Grad Norm: 0.00858100\n",
      "Epoch 4 | Step 2985100 | Avg Loss: 0.0155 | Grad Norm: 0.01061976\n",
      "Epoch 4 | Step 2985200 | Avg Loss: 0.0157 | Grad Norm: 0.00916639\n",
      "Epoch 4 | Step 2985300 | Avg Loss: 0.0156 | Grad Norm: 0.01144231\n",
      "Epoch 4 | Step 2985400 | Avg Loss: 0.0151 | Grad Norm: 0.00943974\n",
      "Epoch 4 | Step 2985500 | Avg Loss: 0.0151 | Grad Norm: 0.01076448\n",
      "Epoch 4 | Step 2985600 | Avg Loss: 0.0152 | Grad Norm: 0.00932716\n",
      "Epoch 4 | Step 2985700 | Avg Loss: 0.0153 | Grad Norm: 0.00938126\n",
      "Epoch 4 | Step 2985800 | Avg Loss: 0.0155 | Grad Norm: 0.00923863\n",
      "Epoch 4 | Step 2985900 | Avg Loss: 0.0155 | Grad Norm: 0.00889790\n",
      "Epoch 4 | Step 2986000 | Avg Loss: 0.0154 | Grad Norm: 0.00830692\n",
      "Epoch 4 | Step 2986100 | Avg Loss: 0.0154 | Grad Norm: 0.00872795\n",
      "Epoch 4 | Step 2986200 | Avg Loss: 0.0155 | Grad Norm: 0.00912933\n",
      "Epoch 4 | Step 2986300 | Avg Loss: 0.0157 | Grad Norm: 0.00990039\n",
      "Epoch 4 | Step 2986400 | Avg Loss: 0.0155 | Grad Norm: 0.00975014\n",
      "Epoch 4 | Step 2986500 | Avg Loss: 0.0157 | Grad Norm: 0.00874389\n",
      "Epoch 4 | Step 2986600 | Avg Loss: 0.0154 | Grad Norm: 0.00950046\n",
      "Epoch 4 | Step 2986700 | Avg Loss: 0.0152 | Grad Norm: 0.01138675\n",
      "Epoch 4 | Step 2986800 | Avg Loss: 0.0155 | Grad Norm: 0.00892484\n",
      "Epoch 4 | Step 2986900 | Avg Loss: 0.0154 | Grad Norm: 0.00860431\n",
      "Epoch 4 | Step 2987000 | Avg Loss: 0.0154 | Grad Norm: 0.00851550\n",
      "Epoch 4 | Step 2987100 | Avg Loss: 0.0152 | Grad Norm: 0.00846374\n",
      "Epoch 4 | Step 2987200 | Avg Loss: 0.0153 | Grad Norm: 0.01662012\n",
      "Epoch 4 | Step 2987300 | Avg Loss: 0.0156 | Grad Norm: 0.01057316\n",
      "Epoch 4 | Step 2987400 | Avg Loss: 0.0156 | Grad Norm: 0.00858113\n",
      "Epoch 4 | Step 2987500 | Avg Loss: 0.0154 | Grad Norm: 0.00965427\n",
      "Epoch 4 | Step 2987600 | Avg Loss: 0.0155 | Grad Norm: 0.00881724\n",
      "Epoch 4 | Step 2987700 | Avg Loss: 0.0156 | Grad Norm: 0.01156077\n",
      "Epoch 4 | Step 2987800 | Avg Loss: 0.0157 | Grad Norm: 0.01048189\n",
      "Epoch 4 | Step 2987900 | Avg Loss: 0.0156 | Grad Norm: 0.00810381\n",
      "Epoch 4 | Step 2988000 | Avg Loss: 0.0156 | Grad Norm: 0.00962376\n",
      "Epoch 4 | Step 2988100 | Avg Loss: 0.0154 | Grad Norm: 0.01061977\n",
      "Epoch 4 | Step 2988200 | Avg Loss: 0.0155 | Grad Norm: 0.00934428\n",
      "Epoch 4 | Step 2988300 | Avg Loss: 0.0156 | Grad Norm: 0.00876104\n",
      "Epoch 4 | Step 2988400 | Avg Loss: 0.0157 | Grad Norm: 0.00831136\n",
      "Epoch 4 | Step 2988500 | Avg Loss: 0.0156 | Grad Norm: 0.00958891\n",
      "Epoch 4 | Step 2988600 | Avg Loss: 0.0155 | Grad Norm: 0.00869257\n",
      "Epoch 4 | Step 2988700 | Avg Loss: 0.0156 | Grad Norm: 0.01030110\n",
      "Epoch 4 | Step 2988800 | Avg Loss: 0.0153 | Grad Norm: 0.00805291\n",
      "Epoch 4 | Step 2988900 | Avg Loss: 0.0153 | Grad Norm: 0.00930111\n",
      "Epoch 4 | Step 2989000 | Avg Loss: 0.0152 | Grad Norm: 0.00814656\n",
      "Epoch 4 | Step 2989100 | Avg Loss: 0.0152 | Grad Norm: 0.00874807\n",
      "Epoch 4 | Step 2989200 | Avg Loss: 0.0153 | Grad Norm: 0.00870699\n",
      "Epoch 4 | Step 2989300 | Avg Loss: 0.0152 | Grad Norm: 0.00816231\n",
      "Epoch 4 | Step 2989400 | Avg Loss: 0.0151 | Grad Norm: 0.00771039\n",
      "Epoch 4 | Step 2989500 | Avg Loss: 0.0153 | Grad Norm: 0.00810810\n",
      "Epoch 4 | Step 2989600 | Avg Loss: 0.0149 | Grad Norm: 0.00812737\n",
      "Epoch 4 | Step 2989700 | Avg Loss: 0.0152 | Grad Norm: 0.00826941\n",
      "Epoch 4 | Step 2989800 | Avg Loss: 0.0152 | Grad Norm: 0.00892850\n",
      "Epoch 4 | Step 2989900 | Avg Loss: 0.0150 | Grad Norm: 0.00913197\n",
      "Epoch 4 | Step 2990000 | Avg Loss: 0.0151 | Grad Norm: 0.01081034\n",
      "Epoch 4 | Step 2990100 | Avg Loss: 0.0151 | Grad Norm: 0.00950485\n",
      "Epoch 4 | Step 2990200 | Avg Loss: 0.0153 | Grad Norm: 0.00872621\n",
      "Epoch 4 | Step 2990300 | Avg Loss: 0.0152 | Grad Norm: 0.00989298\n",
      "Epoch 4 | Step 2990400 | Avg Loss: 0.0157 | Grad Norm: 0.00848498\n",
      "Epoch 4 | Step 2990500 | Avg Loss: 0.0152 | Grad Norm: 0.00917325\n",
      "Epoch 4 | Step 2990600 | Avg Loss: 0.0150 | Grad Norm: 0.00835375\n",
      "Epoch 4 | Step 2990700 | Avg Loss: 0.0151 | Grad Norm: 0.01014802\n",
      "Epoch 4 | Step 2990800 | Avg Loss: 0.0156 | Grad Norm: 0.00947693\n",
      "Epoch 4 | Step 2990900 | Avg Loss: 0.0154 | Grad Norm: 0.01107200\n",
      "Epoch 4 | Step 2991000 | Avg Loss: 0.0154 | Grad Norm: 0.01001910\n",
      "Epoch 4 | Step 2991100 | Avg Loss: 0.0154 | Grad Norm: 0.00893210\n",
      "Epoch 4 | Step 2991200 | Avg Loss: 0.0155 | Grad Norm: 0.00861399\n",
      "Epoch 4 | Step 2991300 | Avg Loss: 0.0152 | Grad Norm: 0.00778695\n",
      "Epoch 4 | Step 2991400 | Avg Loss: 0.0151 | Grad Norm: 0.00886965\n",
      "Epoch 4 | Step 2991500 | Avg Loss: 0.0150 | Grad Norm: 0.00793636\n",
      "Epoch 4 | Step 2991600 | Avg Loss: 0.0153 | Grad Norm: 0.00932944\n",
      "Epoch 4 | Step 2991700 | Avg Loss: 0.0151 | Grad Norm: 0.01016303\n",
      "Epoch 4 | Step 2991800 | Avg Loss: 0.0156 | Grad Norm: 0.00889389\n",
      "Epoch 4 | Step 2991900 | Avg Loss: 0.0156 | Grad Norm: 0.00957707\n",
      "Epoch 4 | Step 2992000 | Avg Loss: 0.0156 | Grad Norm: 0.00869358\n",
      "Epoch 4 | Step 2992100 | Avg Loss: 0.0154 | Grad Norm: 0.01023922\n",
      "Epoch 4 | Step 2992200 | Avg Loss: 0.0156 | Grad Norm: 0.00873384\n",
      "Epoch 4 | Step 2992300 | Avg Loss: 0.0157 | Grad Norm: 0.01688660\n",
      "Epoch 4 | Step 2992400 | Avg Loss: 0.0153 | Grad Norm: 0.01029614\n",
      "Epoch 4 | Step 2992500 | Avg Loss: 0.0152 | Grad Norm: 0.00857731\n",
      "Epoch 4 | Step 2992600 | Avg Loss: 0.0153 | Grad Norm: 0.00829948\n",
      "Epoch 4 | Step 2992700 | Avg Loss: 0.0156 | Grad Norm: 0.01034375\n",
      "Epoch 4 | Step 2992800 | Avg Loss: 0.0161 | Grad Norm: 0.00899009\n",
      "Epoch 4 | Step 2992900 | Avg Loss: 0.0159 | Grad Norm: 0.00763234\n",
      "Epoch 4 | Step 2993000 | Avg Loss: 0.0157 | Grad Norm: 0.01181905\n",
      "Epoch 4 | Step 2993100 | Avg Loss: 0.0159 | Grad Norm: 0.01072428\n",
      "Epoch 4 | Step 2993200 | Avg Loss: 0.0160 | Grad Norm: 0.00931176\n",
      "Epoch 4 | Step 2993300 | Avg Loss: 0.0165 | Grad Norm: 0.00912089\n",
      "Epoch 4 | Step 2993400 | Avg Loss: 0.0159 | Grad Norm: 0.01129909\n",
      "Epoch 4 | Step 2993500 | Avg Loss: 0.0157 | Grad Norm: 0.00987420\n",
      "Epoch 4 | Step 2993600 | Avg Loss: 0.0152 | Grad Norm: 0.00879793\n",
      "Epoch 4 | Step 2993700 | Avg Loss: 0.0156 | Grad Norm: 0.00937105\n",
      "Epoch 4 | Step 2993800 | Avg Loss: 0.0155 | Grad Norm: 0.00923507\n",
      "Epoch 4 | Step 2993900 | Avg Loss: 0.0153 | Grad Norm: 0.00898385\n",
      "Epoch 4 | Step 2994000 | Avg Loss: 0.0155 | Grad Norm: 0.01039133\n",
      "Epoch 4 | Step 2994100 | Avg Loss: 0.0154 | Grad Norm: 0.00813815\n",
      "Epoch 4 | Step 2994200 | Avg Loss: 0.0158 | Grad Norm: 0.00965725\n",
      "Epoch 4 | Step 2994300 | Avg Loss: 0.0161 | Grad Norm: 0.00969049\n",
      "Epoch 4 | Step 2994400 | Avg Loss: 0.0163 | Grad Norm: 0.00951007\n",
      "Epoch 4 | Step 2994500 | Avg Loss: 0.0161 | Grad Norm: 0.00924000\n",
      "Epoch 4 | Step 2994600 | Avg Loss: 0.0162 | Grad Norm: 0.00833604\n",
      "Epoch 4 | Step 2994700 | Avg Loss: 0.0159 | Grad Norm: 0.00927879\n",
      "Epoch 4 | Step 2994800 | Avg Loss: 0.0161 | Grad Norm: 0.00906441\n",
      "Epoch 4 | Step 2994900 | Avg Loss: 0.0163 | Grad Norm: 0.00966505\n",
      "Epoch 4 | Step 2995000 | Avg Loss: 0.0160 | Grad Norm: 0.01049967\n",
      "Epoch 4 | Step 2995100 | Avg Loss: 0.0159 | Grad Norm: 0.00908334\n",
      "Epoch 4 | Step 2995200 | Avg Loss: 0.0156 | Grad Norm: 0.01010979\n",
      "Epoch 4 | Step 2995300 | Avg Loss: 0.0156 | Grad Norm: 0.00999125\n",
      "Epoch 4 | Step 2995400 | Avg Loss: 0.0155 | Grad Norm: 0.00887306\n",
      "Epoch 4 | Step 2995500 | Avg Loss: 0.0157 | Grad Norm: 0.01068273\n",
      "Epoch 4 | Step 2995600 | Avg Loss: 0.0153 | Grad Norm: 0.00868061\n",
      "Epoch 4 | Step 2995700 | Avg Loss: 0.0151 | Grad Norm: 0.00944888\n",
      "Epoch 4 | Step 2995800 | Avg Loss: 0.0146 | Grad Norm: 0.00947999\n",
      "Epoch 4 | Step 2995900 | Avg Loss: 0.0153 | Grad Norm: 0.00934810\n",
      "Epoch 4 | Step 2996000 | Avg Loss: 0.0150 | Grad Norm: 0.00842587\n",
      "Epoch 4 | Step 2996100 | Avg Loss: 0.0154 | Grad Norm: 0.01077321\n",
      "Epoch 4 | Step 2996200 | Avg Loss: 0.0155 | Grad Norm: 0.00908796\n",
      "Epoch 4 | Step 2996300 | Avg Loss: 0.0156 | Grad Norm: 0.00851758\n",
      "Epoch 4 | Step 2996400 | Avg Loss: 0.0156 | Grad Norm: 0.00828214\n",
      "Epoch 4 | Step 2996500 | Avg Loss: 0.0156 | Grad Norm: 0.00855168\n",
      "Epoch 4 | Step 2996600 | Avg Loss: 0.0156 | Grad Norm: 0.00916716\n",
      "Epoch 4 | Step 2996700 | Avg Loss: 0.0154 | Grad Norm: 0.00838288\n",
      "Epoch 4 | Step 2996800 | Avg Loss: 0.0154 | Grad Norm: 0.00940087\n",
      "Epoch 4 | Step 2996900 | Avg Loss: 0.0156 | Grad Norm: 0.00871197\n",
      "Epoch 4 | Step 2997000 | Avg Loss: 0.0157 | Grad Norm: 0.01074566\n",
      "Epoch 4 | Step 2997100 | Avg Loss: 0.0154 | Grad Norm: 0.01058001\n",
      "Epoch 4 | Step 2997200 | Avg Loss: 0.0157 | Grad Norm: 0.00905567\n",
      "Epoch 4 | Step 2997300 | Avg Loss: 0.0155 | Grad Norm: 0.00854376\n",
      "Epoch 4 | Step 2997400 | Avg Loss: 0.0155 | Grad Norm: 0.00865227\n",
      "Epoch 4 | Step 2997500 | Avg Loss: 0.0156 | Grad Norm: 0.00946051\n",
      "Epoch 4 | Step 2997600 | Avg Loss: 0.0156 | Grad Norm: 0.01026355\n",
      "Epoch 4 | Step 2997700 | Avg Loss: 0.0154 | Grad Norm: 0.00977197\n",
      "Epoch 4 | Step 2997800 | Avg Loss: 0.0153 | Grad Norm: 0.00914798\n",
      "Epoch 4 | Step 2997900 | Avg Loss: 0.0152 | Grad Norm: 0.00945187\n",
      "Epoch 4 | Step 2998000 | Avg Loss: 0.0150 | Grad Norm: 0.00780975\n",
      "Epoch 4 | Step 2998100 | Avg Loss: 0.0151 | Grad Norm: 0.00985805\n",
      "Epoch 4 | Step 2998200 | Avg Loss: 0.0155 | Grad Norm: 0.00854879\n",
      "Epoch 4 | Step 2998300 | Avg Loss: 0.0157 | Grad Norm: 0.00890966\n",
      "Epoch 4 | Step 2998400 | Avg Loss: 0.0159 | Grad Norm: 0.01149429\n",
      "Epoch 4 | Step 2998500 | Avg Loss: 0.0160 | Grad Norm: 0.00944476\n",
      "Epoch 4 | Step 2998600 | Avg Loss: 0.0159 | Grad Norm: 0.00994249\n",
      "Epoch 4 | Step 2998700 | Avg Loss: 0.0155 | Grad Norm: 0.00861071\n",
      "Epoch 4 | Step 2998800 | Avg Loss: 0.0156 | Grad Norm: 0.00841291\n",
      "Epoch 4 | Step 2998900 | Avg Loss: 0.0156 | Grad Norm: 0.01123915\n",
      "Epoch 4 | Step 2999000 | Avg Loss: 0.0156 | Grad Norm: 0.01063639\n",
      "Epoch 4 | Step 2999100 | Avg Loss: 0.0158 | Grad Norm: 0.00839101\n",
      "Epoch 4 | Step 2999200 | Avg Loss: 0.0161 | Grad Norm: 0.01167429\n",
      "Epoch 4 | Step 2999300 | Avg Loss: 0.0160 | Grad Norm: 0.00853499\n",
      "Epoch 4 | Step 2999400 | Avg Loss: 0.0155 | Grad Norm: 0.00930329\n",
      "Epoch 4 | Step 2999500 | Avg Loss: 0.0156 | Grad Norm: 0.00908044\n",
      "Epoch 4 | Step 2999600 | Avg Loss: 0.0155 | Grad Norm: 0.01044453\n",
      "Epoch 4 | Step 2999700 | Avg Loss: 0.0158 | Grad Norm: 0.01183765\n",
      "Epoch 4 | Step 2999800 | Avg Loss: 0.0158 | Grad Norm: 0.00884639\n",
      "Epoch 4 | Step 2999900 | Avg Loss: 0.0157 | Grad Norm: 0.01304169\n",
      "Epoch 4 | Step 3000000 | Avg Loss: 0.0153 | Grad Norm: 0.00927117\n",
      "Saving model at step3000000\n",
      "Epoch 4 | Step 3000100 | Avg Loss: 0.0154 | Grad Norm: 0.00858211\n",
      "Epoch 4 | Step 3000200 | Avg Loss: 0.0150 | Grad Norm: 0.00809447\n",
      "Epoch 4 | Step 3000300 | Avg Loss: 0.0149 | Grad Norm: 0.00828051\n",
      "Epoch 4 | Step 3000400 | Avg Loss: 0.0153 | Grad Norm: 0.00959020\n",
      "Epoch 4 | Step 3000500 | Avg Loss: 0.0159 | Grad Norm: 0.00951685\n",
      "Epoch 4 | Step 3000600 | Avg Loss: 0.0160 | Grad Norm: 0.01066284\n",
      "Epoch 4 | Step 3000700 | Avg Loss: 0.0158 | Grad Norm: 0.00902457\n",
      "Epoch 4 | Step 3000800 | Avg Loss: 0.0156 | Grad Norm: 0.01093318\n",
      "Epoch 4 | Step 3000900 | Avg Loss: 0.0157 | Grad Norm: 0.01138905\n",
      "Epoch 4 | Step 3001000 | Avg Loss: 0.0158 | Grad Norm: 0.00888837\n",
      "Epoch 4 | Step 3001100 | Avg Loss: 0.0156 | Grad Norm: 0.00938466\n",
      "Epoch 4 | Step 3001200 | Avg Loss: 0.0155 | Grad Norm: 0.00810193\n",
      "Epoch 4 | Step 3001300 | Avg Loss: 0.0156 | Grad Norm: 0.00786488\n",
      "Epoch 4 | Step 3001400 | Avg Loss: 0.0157 | Grad Norm: 0.00834741\n",
      "Epoch 4 | Step 3001500 | Avg Loss: 0.0157 | Grad Norm: 0.01023591\n",
      "Epoch 4 | Step 3001600 | Avg Loss: 0.0156 | Grad Norm: 0.00816521\n",
      "Epoch 4 | Step 3001700 | Avg Loss: 0.0152 | Grad Norm: 0.00948982\n",
      "Epoch 4 | Step 3001800 | Avg Loss: 0.0153 | Grad Norm: 0.00858693\n",
      "Epoch 4 | Step 3001900 | Avg Loss: 0.0152 | Grad Norm: 0.00773309\n",
      "Epoch 4 | Step 3002000 | Avg Loss: 0.0151 | Grad Norm: 0.00861738\n",
      "Epoch 4 | Step 3002100 | Avg Loss: 0.0149 | Grad Norm: 0.00815187\n",
      "Epoch 4 | Step 3002200 | Avg Loss: 0.0149 | Grad Norm: 0.00849194\n",
      "Epoch 4 | Step 3002300 | Avg Loss: 0.0148 | Grad Norm: 0.00938227\n",
      "Epoch 4 | Step 3002400 | Avg Loss: 0.0149 | Grad Norm: 0.00971972\n",
      "Epoch 4 | Step 3002500 | Avg Loss: 0.0148 | Grad Norm: 0.00922326\n",
      "Epoch 4 | Step 3002600 | Avg Loss: 0.0149 | Grad Norm: 0.00951229\n",
      "Epoch 4 | Step 3002700 | Avg Loss: 0.0149 | Grad Norm: 0.00868906\n",
      "Epoch 4 | Step 3002800 | Avg Loss: 0.0155 | Grad Norm: 0.01213917\n",
      "Epoch 4 | Step 3002900 | Avg Loss: 0.0158 | Grad Norm: 0.00825215\n",
      "Epoch 4 | Step 3003000 | Avg Loss: 0.0158 | Grad Norm: 0.00909458\n",
      "Epoch 4 | Step 3003100 | Avg Loss: 0.0165 | Grad Norm: 0.00904768\n",
      "Epoch 4 | Step 3003200 | Avg Loss: 0.0161 | Grad Norm: 0.01007845\n",
      "Epoch 4 | Step 3003300 | Avg Loss: 0.0159 | Grad Norm: 0.01019569\n",
      "Epoch 4 | Step 3003400 | Avg Loss: 0.0156 | Grad Norm: 0.00918571\n",
      "Epoch 4 | Step 3003500 | Avg Loss: 0.0155 | Grad Norm: 0.01790749\n",
      "Epoch 4 | Step 3003600 | Avg Loss: 0.0159 | Grad Norm: 0.00934733\n",
      "Epoch 4 | Step 3003700 | Avg Loss: 0.0161 | Grad Norm: 0.00861194\n",
      "Epoch 4 | Step 3003800 | Avg Loss: 0.0162 | Grad Norm: 0.00831123\n",
      "Epoch 4 | Step 3003900 | Avg Loss: 0.0161 | Grad Norm: 0.00881116\n",
      "Epoch 4 | Step 3004000 | Avg Loss: 0.0154 | Grad Norm: 0.00967500\n",
      "Epoch 4 | Step 3004100 | Avg Loss: 0.0156 | Grad Norm: 0.00811499\n",
      "Epoch 4 | Step 3004200 | Avg Loss: 0.0159 | Grad Norm: 0.00865598\n",
      "Epoch 4 | Step 3004300 | Avg Loss: 0.0159 | Grad Norm: 0.00831683\n",
      "Epoch 4 | Step 3004400 | Avg Loss: 0.0158 | Grad Norm: 0.00844330\n",
      "Epoch 4 | Step 3004500 | Avg Loss: 0.0159 | Grad Norm: 0.00963324\n",
      "Epoch 4 | Step 3004600 | Avg Loss: 0.0157 | Grad Norm: 0.00834975\n",
      "Epoch 4 | Step 3004700 | Avg Loss: 0.0157 | Grad Norm: 0.00798235\n",
      "Epoch 4 | Step 3004800 | Avg Loss: 0.0156 | Grad Norm: 0.00860165\n",
      "Epoch 4 | Step 3004900 | Avg Loss: 0.0160 | Grad Norm: 0.00960278\n",
      "Epoch 4 | Step 3005000 | Avg Loss: 0.0159 | Grad Norm: 0.01028251\n",
      "Epoch 4 | Step 3005100 | Avg Loss: 0.0157 | Grad Norm: 0.00996242\n",
      "Epoch 4 | Step 3005200 | Avg Loss: 0.0156 | Grad Norm: 0.00947822\n",
      "Epoch 4 | Step 3005300 | Avg Loss: 0.0157 | Grad Norm: 0.00968491\n",
      "Epoch 4 | Step 3005400 | Avg Loss: 0.0154 | Grad Norm: 0.00876300\n",
      "Epoch 4 | Step 3005500 | Avg Loss: 0.0153 | Grad Norm: 0.00906328\n",
      "Epoch 4 | Step 3005600 | Avg Loss: 0.0148 | Grad Norm: 0.00861359\n",
      "Epoch 4 | Step 3005700 | Avg Loss: 0.0149 | Grad Norm: 0.00818860\n",
      "Epoch 4 | Step 3005800 | Avg Loss: 0.0154 | Grad Norm: 0.00953209\n",
      "Epoch 4 | Step 3005900 | Avg Loss: 0.0152 | Grad Norm: 0.00872621\n",
      "Epoch 4 | Step 3006000 | Avg Loss: 0.0152 | Grad Norm: 0.00857224\n",
      "Epoch 4 | Step 3006100 | Avg Loss: 0.0152 | Grad Norm: 0.00823638\n",
      "Epoch 4 | Step 3006200 | Avg Loss: 0.0151 | Grad Norm: 0.00974707\n",
      "Epoch 4 | Step 3006300 | Avg Loss: 0.0150 | Grad Norm: 0.00811952\n",
      "Epoch 4 | Step 3006400 | Avg Loss: 0.0149 | Grad Norm: 0.00853659\n",
      "Epoch 4 | Step 3006500 | Avg Loss: 0.0151 | Grad Norm: 0.01021627\n",
      "Epoch 4 | Step 3006600 | Avg Loss: 0.0150 | Grad Norm: 0.00924197\n",
      "Epoch 4 | Step 3006700 | Avg Loss: 0.0152 | Grad Norm: 0.00825452\n",
      "Epoch 4 | Step 3006800 | Avg Loss: 0.0153 | Grad Norm: 0.00919579\n",
      "Epoch 4 | Step 3006900 | Avg Loss: 0.0152 | Grad Norm: 0.00880407\n",
      "Epoch 4 | Step 3007000 | Avg Loss: 0.0155 | Grad Norm: 0.00859746\n",
      "Epoch 4 | Step 3007100 | Avg Loss: 0.0157 | Grad Norm: 0.00776516\n",
      "Epoch 4 | Step 3007200 | Avg Loss: 0.0158 | Grad Norm: 0.00833438\n",
      "Epoch 4 | Step 3007300 | Avg Loss: 0.0159 | Grad Norm: 0.00929257\n",
      "Epoch 4 | Step 3007400 | Avg Loss: 0.0161 | Grad Norm: 0.00887860\n",
      "Epoch 4 | Step 3007500 | Avg Loss: 0.0162 | Grad Norm: 0.00920887\n",
      "Epoch 4 | Step 3007600 | Avg Loss: 0.0161 | Grad Norm: 0.00908016\n",
      "Epoch 4 | Step 3007700 | Avg Loss: 0.0162 | Grad Norm: 0.00914984\n",
      "Epoch 4 | Step 3007800 | Avg Loss: 0.0162 | Grad Norm: 0.00928945\n",
      "Epoch 4 | Step 3007900 | Avg Loss: 0.0159 | Grad Norm: 0.00825895\n",
      "Epoch 4 | Step 3008000 | Avg Loss: 0.0153 | Grad Norm: 0.01055831\n",
      "Epoch 4 | Step 3008100 | Avg Loss: 0.0152 | Grad Norm: 0.00978344\n",
      "Epoch 4 | Step 3008200 | Avg Loss: 0.0148 | Grad Norm: 0.00867827\n",
      "Epoch 4 | Step 3008300 | Avg Loss: 0.0146 | Grad Norm: 0.00935652\n",
      "Epoch 4 | Step 3008400 | Avg Loss: 0.0147 | Grad Norm: 0.00796207\n",
      "Epoch 4 | Step 3008500 | Avg Loss: 0.0147 | Grad Norm: 0.01011477\n",
      "Epoch 4 | Step 3008600 | Avg Loss: 0.0146 | Grad Norm: 0.00901189\n",
      "Epoch 4 | Step 3008700 | Avg Loss: 0.0148 | Grad Norm: 0.01045541\n",
      "Epoch 4 | Step 3008800 | Avg Loss: 0.0147 | Grad Norm: 0.00794500\n",
      "Epoch 4 | Step 3008900 | Avg Loss: 0.0146 | Grad Norm: 0.00758198\n",
      "Epoch 4 | Step 3009000 | Avg Loss: 0.0153 | Grad Norm: 0.00835631\n",
      "Epoch 4 | Step 3009100 | Avg Loss: 0.0155 | Grad Norm: 0.01023970\n",
      "Epoch 4 | Step 3009200 | Avg Loss: 0.0153 | Grad Norm: 0.01005000\n",
      "Epoch 4 | Step 3009300 | Avg Loss: 0.0154 | Grad Norm: 0.00878116\n",
      "Epoch 4 | Step 3009400 | Avg Loss: 0.0153 | Grad Norm: 0.00885583\n",
      "Epoch 4 | Step 3009500 | Avg Loss: 0.0155 | Grad Norm: 0.01124621\n",
      "Epoch 4 | Step 3009600 | Avg Loss: 0.0162 | Grad Norm: 0.00993539\n",
      "Epoch 4 | Step 3009700 | Avg Loss: 0.0158 | Grad Norm: 0.00920646\n",
      "Epoch 4 | Step 3009800 | Avg Loss: 0.0156 | Grad Norm: 0.01033152\n",
      "Epoch 4 | Step 3009900 | Avg Loss: 0.0152 | Grad Norm: 0.01019922\n",
      "Epoch 4 | Step 3010000 | Avg Loss: 0.0156 | Grad Norm: 0.00840977\n",
      "Epoch 4 | Step 3010100 | Avg Loss: 0.0155 | Grad Norm: 0.00827730\n",
      "Epoch 4 | Step 3010200 | Avg Loss: 0.0159 | Grad Norm: 0.00905950\n",
      "Epoch 4 | Step 3010300 | Avg Loss: 0.0155 | Grad Norm: 0.00852079\n",
      "Epoch 4 | Step 3010400 | Avg Loss: 0.0155 | Grad Norm: 0.00814410\n",
      "Epoch 4 | Step 3010500 | Avg Loss: 0.0155 | Grad Norm: 0.01001166\n",
      "Epoch 4 | Step 3010600 | Avg Loss: 0.0157 | Grad Norm: 0.00933128\n",
      "Epoch 4 | Step 3010700 | Avg Loss: 0.0158 | Grad Norm: 0.00890518\n",
      "Epoch 4 | Step 3010800 | Avg Loss: 0.0159 | Grad Norm: 0.00943088\n",
      "Epoch 4 | Step 3010900 | Avg Loss: 0.0158 | Grad Norm: 0.00976431\n",
      "Epoch 4 | Step 3011000 | Avg Loss: 0.0159 | Grad Norm: 0.00909304\n",
      "Epoch 4 | Step 3011100 | Avg Loss: 0.0156 | Grad Norm: 0.00910838\n",
      "Epoch 4 | Step 3011200 | Avg Loss: 0.0154 | Grad Norm: 0.00922604\n",
      "Epoch 4 | Step 3011300 | Avg Loss: 0.0155 | Grad Norm: 0.00824447\n",
      "Epoch 4 | Step 3011400 | Avg Loss: 0.0154 | Grad Norm: 0.00914902\n",
      "Epoch 4 | Step 3011500 | Avg Loss: 0.0156 | Grad Norm: 0.00904089\n",
      "Epoch 4 | Step 3011600 | Avg Loss: 0.0156 | Grad Norm: 0.00923101\n",
      "Epoch 4 | Step 3011700 | Avg Loss: 0.0153 | Grad Norm: 0.00971065\n",
      "Epoch 4 | Step 3011800 | Avg Loss: 0.0149 | Grad Norm: 0.00858073\n",
      "Epoch 4 | Step 3011900 | Avg Loss: 0.0148 | Grad Norm: 0.00842650\n",
      "Epoch 4 | Step 3012000 | Avg Loss: 0.0151 | Grad Norm: 0.00819641\n",
      "Epoch 4 | Step 3012100 | Avg Loss: 0.0152 | Grad Norm: 0.00979959\n",
      "Epoch 4 | Step 3012200 | Avg Loss: 0.0156 | Grad Norm: 0.00915445\n",
      "Epoch 4 | Step 3012300 | Avg Loss: 0.0154 | Grad Norm: 0.00869974\n",
      "Epoch 4 | Step 3012400 | Avg Loss: 0.0156 | Grad Norm: 0.00943253\n",
      "Epoch 4 | Step 3012500 | Avg Loss: 0.0156 | Grad Norm: 0.00810917\n",
      "Epoch 4 | Step 3012600 | Avg Loss: 0.0154 | Grad Norm: 0.00928834\n",
      "Epoch 4 | Step 3012700 | Avg Loss: 0.0159 | Grad Norm: 0.00951659\n",
      "Epoch 4 | Step 3012800 | Avg Loss: 0.0158 | Grad Norm: 0.00911524\n",
      "Epoch 4 | Step 3012900 | Avg Loss: 0.0156 | Grad Norm: 0.00873086\n",
      "Epoch 4 | Step 3013000 | Avg Loss: 0.0156 | Grad Norm: 0.01136861\n",
      "Epoch 4 | Step 3013100 | Avg Loss: 0.0161 | Grad Norm: 0.00896918\n",
      "Epoch 4 | Step 3013200 | Avg Loss: 0.0156 | Grad Norm: 0.00828978\n",
      "Epoch 4 | Step 3013300 | Avg Loss: 0.0158 | Grad Norm: 0.00927044\n",
      "Epoch 4 | Step 3013400 | Avg Loss: 0.0160 | Grad Norm: 0.00901867\n",
      "Epoch 4 | Step 3013500 | Avg Loss: 0.0161 | Grad Norm: 0.00941961\n",
      "Epoch 4 | Step 3013600 | Avg Loss: 0.0158 | Grad Norm: 0.00879545\n",
      "Epoch 4 | Step 3013700 | Avg Loss: 0.0157 | Grad Norm: 0.00893560\n",
      "Epoch 4 | Step 3013800 | Avg Loss: 0.0160 | Grad Norm: 0.01023958\n",
      "Epoch 4 | Step 3013900 | Avg Loss: 0.0156 | Grad Norm: 0.01004998\n",
      "Epoch 4 | Step 3014000 | Avg Loss: 0.0158 | Grad Norm: 0.00863195\n",
      "Epoch 4 | Step 3014100 | Avg Loss: 0.0156 | Grad Norm: 0.00852673\n",
      "Epoch 4 | Step 3014200 | Avg Loss: 0.0159 | Grad Norm: 0.00886147\n",
      "Epoch 4 | Step 3014300 | Avg Loss: 0.0161 | Grad Norm: 0.01012374\n",
      "Epoch 4 | Step 3014400 | Avg Loss: 0.0158 | Grad Norm: 0.00953600\n",
      "Epoch 4 | Step 3014500 | Avg Loss: 0.0158 | Grad Norm: 0.00999127\n",
      "Epoch 4 | Step 3014600 | Avg Loss: 0.0158 | Grad Norm: 0.00966796\n",
      "Epoch 4 | Step 3014700 | Avg Loss: 0.0150 | Grad Norm: 0.01172675\n",
      "Epoch 4 | Step 3014800 | Avg Loss: 0.0154 | Grad Norm: 0.00796098\n",
      "Epoch 4 | Step 3014900 | Avg Loss: 0.0150 | Grad Norm: 0.00882811\n",
      "Epoch 4 | Step 3015000 | Avg Loss: 0.0150 | Grad Norm: 0.00809657\n",
      "Epoch 4 | Step 3015100 | Avg Loss: 0.0149 | Grad Norm: 0.00919247\n",
      "Epoch 4 | Step 3015200 | Avg Loss: 0.0149 | Grad Norm: 0.00898724\n",
      "Epoch 4 | Step 3015300 | Avg Loss: 0.0150 | Grad Norm: 0.00996685\n",
      "Epoch 4 | Step 3015400 | Avg Loss: 0.0153 | Grad Norm: 0.00951278\n",
      "Epoch 4 | Step 3015500 | Avg Loss: 0.0153 | Grad Norm: 0.00827051\n",
      "Epoch 4 | Step 3015600 | Avg Loss: 0.0153 | Grad Norm: 0.00921017\n",
      "Epoch 4 | Step 3015700 | Avg Loss: 0.0158 | Grad Norm: 0.01022499\n",
      "Epoch 4 | Step 3015800 | Avg Loss: 0.0155 | Grad Norm: 0.01073595\n",
      "Epoch 4 | Step 3015900 | Avg Loss: 0.0157 | Grad Norm: 0.00815965\n",
      "Epoch 4 | Step 3016000 | Avg Loss: 0.0160 | Grad Norm: 0.00964260\n",
      "Epoch 4 | Step 3016100 | Avg Loss: 0.0160 | Grad Norm: 0.01107702\n",
      "Epoch 4 | Step 3016200 | Avg Loss: 0.0156 | Grad Norm: 0.00906809\n",
      "Epoch 4 | Step 3016300 | Avg Loss: 0.0158 | Grad Norm: 0.00910959\n",
      "Epoch 4 | Step 3016400 | Avg Loss: 0.0158 | Grad Norm: 0.00893880\n",
      "Epoch 4 | Step 3016500 | Avg Loss: 0.0159 | Grad Norm: 0.00995404\n",
      "Epoch 4 | Step 3016600 | Avg Loss: 0.0161 | Grad Norm: 0.00899297\n",
      "Epoch 4 | Step 3016700 | Avg Loss: 0.0160 | Grad Norm: 0.00837967\n",
      "Epoch 4 | Step 3016800 | Avg Loss: 0.0157 | Grad Norm: 0.00855557\n",
      "Epoch 4 | Step 3016900 | Avg Loss: 0.0156 | Grad Norm: 0.00935831\n",
      "Epoch 4 | Step 3017000 | Avg Loss: 0.0156 | Grad Norm: 0.01015488\n",
      "Epoch 4 | Step 3017100 | Avg Loss: 0.0155 | Grad Norm: 0.00887284\n",
      "Epoch 4 | Step 3017200 | Avg Loss: 0.0154 | Grad Norm: 0.00941440\n",
      "Epoch 4 | Step 3017300 | Avg Loss: 0.0155 | Grad Norm: 0.00865992\n",
      "Epoch 4 | Step 3017400 | Avg Loss: 0.0152 | Grad Norm: 0.01394411\n",
      "Epoch 4 | Step 3017500 | Avg Loss: 0.0156 | Grad Norm: 0.00953172\n",
      "Epoch 4 | Step 3017600 | Avg Loss: 0.0157 | Grad Norm: 0.00834442\n",
      "Epoch 4 | Step 3017700 | Avg Loss: 0.0152 | Grad Norm: 0.00916843\n",
      "Epoch 4 | Step 3017800 | Avg Loss: 0.0156 | Grad Norm: 0.00828073\n",
      "Epoch 4 | Step 3017900 | Avg Loss: 0.0153 | Grad Norm: 0.01053233\n",
      "Epoch 4 | Step 3018000 | Avg Loss: 0.0157 | Grad Norm: 0.01004274\n",
      "Epoch 4 | Step 3018100 | Avg Loss: 0.0156 | Grad Norm: 0.00995305\n",
      "Epoch 4 | Step 3018200 | Avg Loss: 0.0154 | Grad Norm: 0.00873069\n",
      "Epoch 4 | Step 3018300 | Avg Loss: 0.0152 | Grad Norm: 0.00940316\n",
      "Epoch 4 | Step 3018400 | Avg Loss: 0.0153 | Grad Norm: 0.00806509\n",
      "Epoch 4 | Step 3018500 | Avg Loss: 0.0153 | Grad Norm: 0.00878223\n",
      "Epoch 4 | Step 3018600 | Avg Loss: 0.0153 | Grad Norm: 0.00966562\n",
      "Epoch 4 | Step 3018700 | Avg Loss: 0.0155 | Grad Norm: 0.00846613\n",
      "Epoch 4 | Step 3018800 | Avg Loss: 0.0152 | Grad Norm: 0.01169230\n",
      "Epoch 4 | Step 3018900 | Avg Loss: 0.0147 | Grad Norm: 0.00958795\n",
      "Epoch 4 | Step 3019000 | Avg Loss: 0.0146 | Grad Norm: 0.00873999\n",
      "Epoch 4 | Step 3019100 | Avg Loss: 0.0147 | Grad Norm: 0.00884900\n",
      "Epoch 4 | Step 3019200 | Avg Loss: 0.0147 | Grad Norm: 0.01100235\n",
      "Epoch 4 | Step 3019300 | Avg Loss: 0.0147 | Grad Norm: 0.00777074\n",
      "Epoch 4 | Step 3019400 | Avg Loss: 0.0150 | Grad Norm: 0.00874293\n",
      "Epoch 4 | Step 3019500 | Avg Loss: 0.0150 | Grad Norm: 0.00918066\n",
      "Epoch 4 | Step 3019600 | Avg Loss: 0.0150 | Grad Norm: 0.01295308\n",
      "Epoch 4 | Step 3019700 | Avg Loss: 0.0151 | Grad Norm: 0.00829566\n",
      "Epoch 4 | Step 3019800 | Avg Loss: 0.0151 | Grad Norm: 0.00962609\n",
      "Epoch 4 | Step 3019900 | Avg Loss: 0.0151 | Grad Norm: 0.00822637\n",
      "Epoch 4 | Step 3020000 | Avg Loss: 0.0155 | Grad Norm: 0.00947953\n",
      "Epoch 4 | Step 3020100 | Avg Loss: 0.0152 | Grad Norm: 0.00820245\n",
      "Epoch 4 | Step 3020200 | Avg Loss: 0.0149 | Grad Norm: 0.00859437\n",
      "Epoch 4 | Step 3020300 | Avg Loss: 0.0148 | Grad Norm: 0.00900173\n",
      "Epoch 4 | Step 3020400 | Avg Loss: 0.0149 | Grad Norm: 0.00840735\n",
      "Epoch 4 | Step 3020500 | Avg Loss: 0.0147 | Grad Norm: 0.00801414\n",
      "Epoch 4 | Step 3020600 | Avg Loss: 0.0148 | Grad Norm: 0.00844705\n",
      "Epoch 4 | Step 3020700 | Avg Loss: 0.0153 | Grad Norm: 0.00890518\n",
      "Epoch 4 | Step 3020800 | Avg Loss: 0.0152 | Grad Norm: 0.00911006\n",
      "Epoch 4 | Step 3020900 | Avg Loss: 0.0157 | Grad Norm: 0.00906909\n",
      "Epoch 4 | Step 3021000 | Avg Loss: 0.0158 | Grad Norm: 0.00847196\n",
      "Epoch 4 | Step 3021100 | Avg Loss: 0.0162 | Grad Norm: 0.00954960\n",
      "Epoch 4 | Step 3021200 | Avg Loss: 0.0159 | Grad Norm: 0.01065520\n",
      "Epoch 4 | Step 3021300 | Avg Loss: 0.0156 | Grad Norm: 0.00922809\n",
      "Epoch 4 | Step 3021400 | Avg Loss: 0.0153 | Grad Norm: 0.00907840\n",
      "Epoch 4 | Step 3021500 | Avg Loss: 0.0156 | Grad Norm: 0.00797015\n",
      "Epoch 4 | Step 3021600 | Avg Loss: 0.0152 | Grad Norm: 0.00913022\n",
      "Epoch 4 | Step 3021700 | Avg Loss: 0.0152 | Grad Norm: 0.01128853\n",
      "Epoch 4 | Step 3021800 | Avg Loss: 0.0154 | Grad Norm: 0.00996406\n",
      "Epoch 4 | Step 3021900 | Avg Loss: 0.0153 | Grad Norm: 0.00901280\n",
      "Epoch 4 | Step 3022000 | Avg Loss: 0.0153 | Grad Norm: 0.00827046\n",
      "Epoch 4 | Step 3022100 | Avg Loss: 0.0150 | Grad Norm: 0.00925439\n",
      "Epoch 4 | Step 3022200 | Avg Loss: 0.0152 | Grad Norm: 0.00880679\n",
      "Epoch 4 | Step 3022300 | Avg Loss: 0.0155 | Grad Norm: 0.00978630\n",
      "Epoch 4 | Step 3022400 | Avg Loss: 0.0162 | Grad Norm: 0.00958204\n",
      "Epoch 4 | Step 3022500 | Avg Loss: 0.0159 | Grad Norm: 0.00843598\n",
      "Epoch 4 | Step 3022600 | Avg Loss: 0.0160 | Grad Norm: 0.00934833\n",
      "Epoch 4 | Step 3022700 | Avg Loss: 0.0159 | Grad Norm: 0.01004466\n",
      "Epoch 4 | Step 3022800 | Avg Loss: 0.0157 | Grad Norm: 0.00888530\n",
      "Epoch 4 | Step 3022900 | Avg Loss: 0.0159 | Grad Norm: 0.00927666\n",
      "Epoch 4 | Step 3023000 | Avg Loss: 0.0162 | Grad Norm: 0.00933371\n",
      "Epoch 4 | Step 3023100 | Avg Loss: 0.0162 | Grad Norm: 0.00849417\n",
      "Epoch 4 | Step 3023200 | Avg Loss: 0.0160 | Grad Norm: 0.01014669\n",
      "Epoch 4 | Step 3023300 | Avg Loss: 0.0159 | Grad Norm: 0.00918947\n",
      "Epoch 4 | Step 3023400 | Avg Loss: 0.0157 | Grad Norm: 0.00862634\n",
      "Epoch 4 | Step 3023500 | Avg Loss: 0.0160 | Grad Norm: 0.00992602\n",
      "Epoch 4 | Step 3023600 | Avg Loss: 0.0158 | Grad Norm: 0.00847875\n",
      "Epoch 4 | Step 3023700 | Avg Loss: 0.0160 | Grad Norm: 0.01011173\n",
      "Epoch 4 | Step 3023800 | Avg Loss: 0.0161 | Grad Norm: 0.01052465\n",
      "Epoch 4 | Step 3023900 | Avg Loss: 0.0161 | Grad Norm: 0.00883201\n",
      "Epoch 4 | Step 3024000 | Avg Loss: 0.0162 | Grad Norm: 0.00801033\n",
      "Epoch 4 | Step 3024100 | Avg Loss: 0.0162 | Grad Norm: 0.00942233\n",
      "Epoch 4 | Step 3024200 | Avg Loss: 0.0160 | Grad Norm: 0.01048246\n",
      "Epoch 4 | Step 3024300 | Avg Loss: 0.0159 | Grad Norm: 0.00903701\n",
      "Epoch 4 | Step 3024400 | Avg Loss: 0.0157 | Grad Norm: 0.01171181\n",
      "Epoch 4 | Step 3024500 | Avg Loss: 0.0159 | Grad Norm: 0.00960798\n",
      "Epoch 4 | Step 3024600 | Avg Loss: 0.0159 | Grad Norm: 0.00880353\n",
      "Epoch 4 | Step 3024700 | Avg Loss: 0.0157 | Grad Norm: 0.00976700\n",
      "Epoch 4 | Step 3024800 | Avg Loss: 0.0153 | Grad Norm: 0.00849865\n",
      "Epoch 4 | Step 3024900 | Avg Loss: 0.0156 | Grad Norm: 0.00916610\n",
      "Epoch 4 | Step 3025000 | Avg Loss: 0.0155 | Grad Norm: 0.00878964\n",
      "Epoch 4 | Step 3025100 | Avg Loss: 0.0156 | Grad Norm: 0.00876139\n",
      "Epoch 4 | Step 3025200 | Avg Loss: 0.0157 | Grad Norm: 0.00910413\n",
      "Epoch 4 | Step 3025300 | Avg Loss: 0.0154 | Grad Norm: 0.00840635\n",
      "Epoch 4 | Step 3025400 | Avg Loss: 0.0156 | Grad Norm: 0.00941432\n",
      "Epoch 4 | Step 3025500 | Avg Loss: 0.0156 | Grad Norm: 0.00880004\n",
      "Epoch 4 | Step 3025600 | Avg Loss: 0.0153 | Grad Norm: 0.00837895\n",
      "Epoch 4 | Step 3025700 | Avg Loss: 0.0151 | Grad Norm: 0.00961499\n",
      "Epoch 4 | Step 3025800 | Avg Loss: 0.0153 | Grad Norm: 0.01086821\n",
      "Epoch 4 | Step 3025900 | Avg Loss: 0.0154 | Grad Norm: 0.00786785\n",
      "Epoch 4 | Step 3026000 | Avg Loss: 0.0156 | Grad Norm: 0.01028413\n",
      "Epoch 4 | Step 3026100 | Avg Loss: 0.0153 | Grad Norm: 0.01187086\n",
      "Epoch 4 | Step 3026200 | Avg Loss: 0.0151 | Grad Norm: 0.00827574\n",
      "Epoch 4 | Step 3026300 | Avg Loss: 0.0150 | Grad Norm: 0.00854313\n",
      "Epoch 4 | Step 3026400 | Avg Loss: 0.0151 | Grad Norm: 0.00887056\n",
      "Epoch 4 | Step 3026500 | Avg Loss: 0.0150 | Grad Norm: 0.00843467\n",
      "Epoch 4 | Step 3026600 | Avg Loss: 0.0154 | Grad Norm: 0.00766097\n",
      "Epoch 4 | Step 3026700 | Avg Loss: 0.0156 | Grad Norm: 0.01021516\n",
      "Epoch 4 | Step 3026800 | Avg Loss: 0.0157 | Grad Norm: 0.00785226\n",
      "Epoch 4 | Step 3026900 | Avg Loss: 0.0154 | Grad Norm: 0.00974276\n",
      "Epoch 4 | Step 3027000 | Avg Loss: 0.0152 | Grad Norm: 0.00944508\n",
      "Epoch 4 | Step 3027100 | Avg Loss: 0.0153 | Grad Norm: 0.01007740\n",
      "Epoch 4 | Step 3027200 | Avg Loss: 0.0146 | Grad Norm: 0.00866671\n",
      "Epoch 4 | Step 3027300 | Avg Loss: 0.0146 | Grad Norm: 0.00857169\n",
      "Epoch 4 | Step 3027400 | Avg Loss: 0.0145 | Grad Norm: 0.00943104\n",
      "Epoch 4 | Step 3027500 | Avg Loss: 0.0148 | Grad Norm: 0.00794009\n",
      "Epoch 4 | Step 3027600 | Avg Loss: 0.0148 | Grad Norm: 0.00886406\n",
      "Epoch 4 | Step 3027700 | Avg Loss: 0.0152 | Grad Norm: 0.00849888\n",
      "Epoch 4 | Step 3027800 | Avg Loss: 0.0152 | Grad Norm: 0.00844536\n",
      "Epoch 4 | Step 3027900 | Avg Loss: 0.0151 | Grad Norm: 0.00907497\n",
      "Epoch 4 | Step 3028000 | Avg Loss: 0.0151 | Grad Norm: 0.00941118\n",
      "Epoch 4 | Step 3028100 | Avg Loss: 0.0153 | Grad Norm: 0.00883600\n",
      "Epoch 4 | Step 3028200 | Avg Loss: 0.0157 | Grad Norm: 0.00976355\n",
      "Epoch 4 | Step 3028300 | Avg Loss: 0.0158 | Grad Norm: 0.00953728\n",
      "Epoch 4 | Step 3028400 | Avg Loss: 0.0160 | Grad Norm: 0.00937287\n",
      "Epoch 4 | Step 3028500 | Avg Loss: 0.0158 | Grad Norm: 0.00767889\n",
      "Epoch 4 | Step 3028600 | Avg Loss: 0.0158 | Grad Norm: 0.00868699\n",
      "Epoch 4 | Step 3028700 | Avg Loss: 0.0161 | Grad Norm: 0.00916643\n",
      "Epoch 4 | Step 3028800 | Avg Loss: 0.0157 | Grad Norm: 0.00938332\n",
      "Epoch 4 | Step 3028900 | Avg Loss: 0.0160 | Grad Norm: 0.00918877\n",
      "Epoch 4 | Step 3029000 | Avg Loss: 0.0157 | Grad Norm: 0.00863917\n",
      "Epoch 4 | Step 3029100 | Avg Loss: 0.0155 | Grad Norm: 0.00961066\n",
      "Epoch 4 | Step 3029200 | Avg Loss: 0.0160 | Grad Norm: 0.00916167\n",
      "Epoch 4 | Step 3029300 | Avg Loss: 0.0158 | Grad Norm: 0.00850192\n",
      "Epoch 4 | Step 3029400 | Avg Loss: 0.0157 | Grad Norm: 0.01016051\n",
      "Epoch 4 | Step 3029500 | Avg Loss: 0.0157 | Grad Norm: 0.00917920\n",
      "Epoch 4 | Step 3029600 | Avg Loss: 0.0154 | Grad Norm: 0.01071481\n",
      "Epoch 4 | Step 3029700 | Avg Loss: 0.0153 | Grad Norm: 0.00930999\n",
      "Epoch 4 | Step 3029800 | Avg Loss: 0.0151 | Grad Norm: 0.00846252\n",
      "Epoch 4 | Step 3029900 | Avg Loss: 0.0152 | Grad Norm: 0.00961553\n",
      "Epoch 4 | Step 3030000 | Avg Loss: 0.0152 | Grad Norm: 0.01512377\n",
      "Epoch 4 | Step 3030100 | Avg Loss: 0.0155 | Grad Norm: 0.01066436\n",
      "Epoch 4 | Step 3030200 | Avg Loss: 0.0156 | Grad Norm: 0.01004754\n",
      "Epoch 4 | Step 3030300 | Avg Loss: 0.0158 | Grad Norm: 0.00922568\n",
      "Epoch 4 | Step 3030400 | Avg Loss: 0.0156 | Grad Norm: 0.00793927\n",
      "Epoch 4 | Step 3030500 | Avg Loss: 0.0158 | Grad Norm: 0.00869610\n",
      "Epoch 4 | Step 3030600 | Avg Loss: 0.0160 | Grad Norm: 0.01049207\n",
      "Epoch 4 | Step 3030700 | Avg Loss: 0.0161 | Grad Norm: 0.00972126\n",
      "Epoch 4 | Step 3030800 | Avg Loss: 0.0161 | Grad Norm: 0.00986848\n",
      "Epoch 4 | Step 3030900 | Avg Loss: 0.0159 | Grad Norm: 0.01050768\n",
      "Epoch 4 | Step 3031000 | Avg Loss: 0.0158 | Grad Norm: 0.00922072\n",
      "Epoch 4 | Step 3031100 | Avg Loss: 0.0158 | Grad Norm: 0.00936931\n",
      "Epoch 4 | Step 3031200 | Avg Loss: 0.0159 | Grad Norm: 0.00970484\n",
      "Epoch 4 | Step 3031300 | Avg Loss: 0.0160 | Grad Norm: 0.01027550\n",
      "Epoch 4 | Step 3031400 | Avg Loss: 0.0161 | Grad Norm: 0.00936527\n",
      "Epoch 4 | Step 3031500 | Avg Loss: 0.0160 | Grad Norm: 0.00930060\n",
      "Epoch 4 | Step 3031600 | Avg Loss: 0.0162 | Grad Norm: 0.00902660\n",
      "Epoch 4 | Step 3031700 | Avg Loss: 0.0162 | Grad Norm: 0.00913327\n",
      "Epoch 4 | Step 3031800 | Avg Loss: 0.0157 | Grad Norm: 0.01088728\n",
      "Epoch 4 | Step 3031900 | Avg Loss: 0.0155 | Grad Norm: 0.00954578\n",
      "Epoch 4 | Step 3032000 | Avg Loss: 0.0160 | Grad Norm: 0.00805060\n",
      "Epoch 4 | Step 3032100 | Avg Loss: 0.0160 | Grad Norm: 0.00961946\n",
      "Epoch 4 | Step 3032200 | Avg Loss: 0.0159 | Grad Norm: 0.00979372\n",
      "Epoch 4 | Step 3032300 | Avg Loss: 0.0159 | Grad Norm: 0.00871864\n",
      "Epoch 4 | Step 3032400 | Avg Loss: 0.0156 | Grad Norm: 0.00910112\n",
      "Epoch 4 | Step 3032500 | Avg Loss: 0.0156 | Grad Norm: 0.00834717\n",
      "Epoch 4 | Step 3032600 | Avg Loss: 0.0156 | Grad Norm: 0.00904562\n",
      "Epoch 4 | Step 3032700 | Avg Loss: 0.0153 | Grad Norm: 0.00809705\n",
      "Epoch 4 | Step 3032800 | Avg Loss: 0.0151 | Grad Norm: 0.00855957\n",
      "Epoch 4 | Step 3032900 | Avg Loss: 0.0151 | Grad Norm: 0.00918340\n",
      "Epoch 4 | Step 3033000 | Avg Loss: 0.0154 | Grad Norm: 0.00894884\n",
      "Epoch 4 | Step 3033100 | Avg Loss: 0.0157 | Grad Norm: 0.01027969\n",
      "Epoch 4 | Step 3033200 | Avg Loss: 0.0155 | Grad Norm: 0.01052025\n",
      "Epoch 4 | Step 3033300 | Avg Loss: 0.0156 | Grad Norm: 0.00797816\n",
      "Epoch 4 | Step 3033400 | Avg Loss: 0.0154 | Grad Norm: 0.00817906\n",
      "Epoch 4 | Step 3033500 | Avg Loss: 0.0152 | Grad Norm: 0.00867118\n",
      "Epoch 4 | Step 3033600 | Avg Loss: 0.0149 | Grad Norm: 0.00961147\n",
      "Epoch 4 | Step 3033700 | Avg Loss: 0.0151 | Grad Norm: 0.00778854\n",
      "Epoch 4 | Step 3033800 | Avg Loss: 0.0147 | Grad Norm: 0.01065940\n",
      "Epoch 4 | Step 3033900 | Avg Loss: 0.0149 | Grad Norm: 0.00895602\n",
      "Epoch 4 | Step 3034000 | Avg Loss: 0.0148 | Grad Norm: 0.01029876\n",
      "Epoch 4 | Step 3034100 | Avg Loss: 0.0149 | Grad Norm: 0.00917961\n",
      "Epoch 4 | Step 3034200 | Avg Loss: 0.0154 | Grad Norm: 0.00780239\n",
      "Epoch 4 | Step 3034300 | Avg Loss: 0.0154 | Grad Norm: 0.00840462\n",
      "Epoch 4 | Step 3034400 | Avg Loss: 0.0155 | Grad Norm: 0.00901397\n",
      "Epoch 4 | Step 3034500 | Avg Loss: 0.0155 | Grad Norm: 0.00966445\n",
      "Epoch 4 | Step 3034600 | Avg Loss: 0.0157 | Grad Norm: 0.00981604\n",
      "Epoch 4 | Step 3034700 | Avg Loss: 0.0154 | Grad Norm: 0.00952193\n",
      "Epoch 4 | Step 3034800 | Avg Loss: 0.0154 | Grad Norm: 0.00833141\n",
      "Epoch 4 | Step 3034900 | Avg Loss: 0.0148 | Grad Norm: 0.00837815\n",
      "Epoch 4 | Step 3035000 | Avg Loss: 0.0153 | Grad Norm: 0.00843298\n",
      "Epoch 4 | Step 3035100 | Avg Loss: 0.0155 | Grad Norm: 0.01114791\n",
      "Epoch 4 | Step 3035200 | Avg Loss: 0.0153 | Grad Norm: 0.00864866\n",
      "Epoch 4 | Step 3035300 | Avg Loss: 0.0152 | Grad Norm: 0.00830878\n",
      "Epoch 4 | Step 3035400 | Avg Loss: 0.0154 | Grad Norm: 0.01029626\n",
      "Epoch 4 | Step 3035500 | Avg Loss: 0.0155 | Grad Norm: 0.01016000\n",
      "Epoch 4 | Step 3035600 | Avg Loss: 0.0154 | Grad Norm: 0.01212513\n",
      "Epoch 4 | Step 3035700 | Avg Loss: 0.0152 | Grad Norm: 0.00832021\n",
      "Epoch 4 | Step 3035800 | Avg Loss: 0.0153 | Grad Norm: 0.00843790\n",
      "Epoch 4 | Step 3035900 | Avg Loss: 0.0155 | Grad Norm: 0.00808592\n",
      "Epoch 4 | Step 3036000 | Avg Loss: 0.0156 | Grad Norm: 0.00831789\n",
      "Epoch 4 | Step 3036100 | Avg Loss: 0.0158 | Grad Norm: 0.00869629\n",
      "Epoch 4 | Step 3036200 | Avg Loss: 0.0159 | Grad Norm: 0.00976505\n",
      "Epoch 4 | Step 3036300 | Avg Loss: 0.0159 | Grad Norm: 0.00873377\n",
      "Epoch 4 | Step 3036400 | Avg Loss: 0.0156 | Grad Norm: 0.00878442\n",
      "Epoch 4 | Step 3036500 | Avg Loss: 0.0154 | Grad Norm: 0.01008809\n",
      "Epoch 4 | Step 3036600 | Avg Loss: 0.0159 | Grad Norm: 0.00810165\n",
      "Epoch 4 | Step 3036700 | Avg Loss: 0.0159 | Grad Norm: 0.00886461\n",
      "Epoch 4 | Step 3036800 | Avg Loss: 0.0158 | Grad Norm: 0.00888504\n",
      "Epoch 4 | Step 3036900 | Avg Loss: 0.0160 | Grad Norm: 0.00982352\n",
      "Epoch 4 | Step 3037000 | Avg Loss: 0.0158 | Grad Norm: 0.01050660\n",
      "Epoch 4 | Step 3037100 | Avg Loss: 0.0156 | Grad Norm: 0.00914142\n",
      "Epoch 4 | Step 3037200 | Avg Loss: 0.0158 | Grad Norm: 0.00961743\n",
      "Epoch 4 | Step 3037300 | Avg Loss: 0.0154 | Grad Norm: 0.00829776\n",
      "Epoch 4 | Step 3037400 | Avg Loss: 0.0157 | Grad Norm: 0.01097693\n",
      "Epoch 4 | Step 3037500 | Avg Loss: 0.0156 | Grad Norm: 0.01002395\n",
      "Epoch 4 | Step 3037600 | Avg Loss: 0.0157 | Grad Norm: 0.00892228\n",
      "Epoch 4 | Step 3037700 | Avg Loss: 0.0159 | Grad Norm: 0.00914132\n",
      "Epoch 4 | Step 3037800 | Avg Loss: 0.0157 | Grad Norm: 0.00898609\n",
      "Epoch 4 | Step 3037900 | Avg Loss: 0.0160 | Grad Norm: 0.01085829\n",
      "Epoch 4 | Step 3038000 | Avg Loss: 0.0162 | Grad Norm: 0.00847559\n",
      "Epoch 4 | Step 3038100 | Avg Loss: 0.0163 | Grad Norm: 0.00952862\n",
      "Epoch 4 | Step 3038200 | Avg Loss: 0.0161 | Grad Norm: 0.00970863\n",
      "Epoch 4 | Step 3038300 | Avg Loss: 0.0159 | Grad Norm: 0.01041228\n",
      "Epoch 4 | Step 3038400 | Avg Loss: 0.0160 | Grad Norm: 0.01059055\n",
      "Epoch 4 | Step 3038500 | Avg Loss: 0.0159 | Grad Norm: 0.00943959\n",
      "Epoch 4 | Step 3038600 | Avg Loss: 0.0160 | Grad Norm: 0.00926189\n",
      "Epoch 4 | Step 3038700 | Avg Loss: 0.0157 | Grad Norm: 0.00913058\n",
      "Epoch 4 | Step 3038800 | Avg Loss: 0.0157 | Grad Norm: 0.01037640\n",
      "Epoch 4 | Step 3038900 | Avg Loss: 0.0158 | Grad Norm: 0.00963419\n",
      "Epoch 4 | Step 3039000 | Avg Loss: 0.0157 | Grad Norm: 0.00946313\n",
      "Epoch 4 | Step 3039100 | Avg Loss: 0.0154 | Grad Norm: 0.00809262\n",
      "Epoch 4 | Step 3039200 | Avg Loss: 0.0155 | Grad Norm: 0.01016473\n",
      "Epoch 4 | Step 3039300 | Avg Loss: 0.0156 | Grad Norm: 0.01066210\n",
      "Epoch 4 | Step 3039400 | Avg Loss: 0.0157 | Grad Norm: 0.00841288\n",
      "Epoch 4 | Step 3039500 | Avg Loss: 0.0155 | Grad Norm: 0.00863852\n",
      "Epoch 4 | Step 3039600 | Avg Loss: 0.0152 | Grad Norm: 0.01108257\n",
      "Epoch 4 | Step 3039700 | Avg Loss: 0.0156 | Grad Norm: 0.01057747\n",
      "Epoch 4 | Step 3039800 | Avg Loss: 0.0159 | Grad Norm: 0.00996174\n",
      "Epoch 4 | Step 3039900 | Avg Loss: 0.0156 | Grad Norm: 0.00929816\n",
      "Epoch 4 | Step 3040000 | Avg Loss: 0.0156 | Grad Norm: 0.00983764\n",
      "Epoch 4 | Step 3040100 | Avg Loss: 0.0155 | Grad Norm: 0.00901252\n",
      "Epoch 4 | Step 3040200 | Avg Loss: 0.0154 | Grad Norm: 0.00872385\n",
      "Epoch 4 | Step 3040300 | Avg Loss: 0.0153 | Grad Norm: 0.00915879\n",
      "Epoch 4 | Step 3040400 | Avg Loss: 0.0156 | Grad Norm: 0.00895516\n",
      "Epoch 4 | Step 3040500 | Avg Loss: 0.0153 | Grad Norm: 0.00886578\n",
      "Epoch 4 | Step 3040600 | Avg Loss: 0.0155 | Grad Norm: 0.01179478\n",
      "Epoch 4 | Step 3040700 | Avg Loss: 0.0154 | Grad Norm: 0.00816716\n",
      "Epoch 4 | Step 3040800 | Avg Loss: 0.0155 | Grad Norm: 0.00982813\n",
      "Epoch 4 | Step 3040900 | Avg Loss: 0.0154 | Grad Norm: 0.00942556\n",
      "Epoch 4 | Step 3041000 | Avg Loss: 0.0160 | Grad Norm: 0.00865271\n",
      "Epoch 4 | Step 3041100 | Avg Loss: 0.0162 | Grad Norm: 0.01255610\n",
      "Epoch 4 | Step 3041200 | Avg Loss: 0.0159 | Grad Norm: 0.00813364\n",
      "Epoch 4 | Step 3041300 | Avg Loss: 0.0161 | Grad Norm: 0.00955763\n",
      "Epoch 4 | Step 3041400 | Avg Loss: 0.0162 | Grad Norm: 0.01040620\n",
      "Epoch 4 | Step 3041500 | Avg Loss: 0.0160 | Grad Norm: 0.00999235\n",
      "Epoch 4 | Step 3041600 | Avg Loss: 0.0164 | Grad Norm: 0.01011250\n",
      "Epoch 4 | Step 3041700 | Avg Loss: 0.0157 | Grad Norm: 0.00962684\n",
      "Epoch 4 | Step 3041800 | Avg Loss: 0.0156 | Grad Norm: 0.00937196\n",
      "Epoch 4 | Step 3041900 | Avg Loss: 0.0161 | Grad Norm: 0.00857851\n",
      "Epoch 4 | Step 3042000 | Avg Loss: 0.0160 | Grad Norm: 0.00998602\n",
      "Epoch 4 | Step 3042100 | Avg Loss: 0.0160 | Grad Norm: 0.00932551\n",
      "Epoch 4 | Step 3042200 | Avg Loss: 0.0163 | Grad Norm: 0.00873559\n",
      "Epoch 4 | Step 3042300 | Avg Loss: 0.0158 | Grad Norm: 0.00990109\n",
      "Epoch 4 | Step 3042400 | Avg Loss: 0.0160 | Grad Norm: 0.01051473\n",
      "Epoch 4 | Step 3042500 | Avg Loss: 0.0160 | Grad Norm: 0.00881360\n",
      "Epoch 4 | Step 3042600 | Avg Loss: 0.0158 | Grad Norm: 0.00911007\n",
      "Epoch 4 | Step 3042700 | Avg Loss: 0.0154 | Grad Norm: 0.00826447\n",
      "Epoch 4 | Step 3042800 | Avg Loss: 0.0152 | Grad Norm: 0.00794923\n",
      "Epoch 4 | Step 3042900 | Avg Loss: 0.0153 | Grad Norm: 0.00777834\n",
      "Epoch 4 | Step 3043000 | Avg Loss: 0.0158 | Grad Norm: 0.00906242\n",
      "Epoch 4 | Step 3043100 | Avg Loss: 0.0157 | Grad Norm: 0.00855435\n",
      "Epoch 4 | Step 3043200 | Avg Loss: 0.0157 | Grad Norm: 0.00883078\n",
      "Epoch 4 | Step 3043300 | Avg Loss: 0.0156 | Grad Norm: 0.01326350\n",
      "Epoch 4 | Step 3043400 | Avg Loss: 0.0158 | Grad Norm: 0.00817295\n",
      "Epoch 4 | Step 3043500 | Avg Loss: 0.0156 | Grad Norm: 0.00968912\n",
      "Epoch 4 | Step 3043600 | Avg Loss: 0.0155 | Grad Norm: 0.00905188\n",
      "Epoch 4 | Step 3043700 | Avg Loss: 0.0154 | Grad Norm: 0.00828028\n",
      "Epoch 4 | Step 3043800 | Avg Loss: 0.0154 | Grad Norm: 0.00836378\n",
      "Epoch 4 | Step 3043900 | Avg Loss: 0.0158 | Grad Norm: 0.01054734\n",
      "Epoch 4 | Step 3044000 | Avg Loss: 0.0156 | Grad Norm: 0.00889559\n",
      "Epoch 4 | Step 3044100 | Avg Loss: 0.0158 | Grad Norm: 0.00941677\n",
      "Epoch 4 | Step 3044200 | Avg Loss: 0.0157 | Grad Norm: 0.00907305\n",
      "Epoch 4 | Step 3044300 | Avg Loss: 0.0158 | Grad Norm: 0.00825383\n",
      "Epoch 4 | Step 3044400 | Avg Loss: 0.0156 | Grad Norm: 0.00999445\n",
      "Epoch 4 | Step 3044500 | Avg Loss: 0.0151 | Grad Norm: 0.01006413\n",
      "Epoch 4 | Step 3044600 | Avg Loss: 0.0151 | Grad Norm: 0.00902332\n",
      "Epoch 4 | Step 3044700 | Avg Loss: 0.0152 | Grad Norm: 0.01002568\n",
      "Epoch 4 | Step 3044800 | Avg Loss: 0.0154 | Grad Norm: 0.01396926\n",
      "Epoch 4 | Step 3044900 | Avg Loss: 0.0157 | Grad Norm: 0.00798743\n",
      "Epoch 4 | Step 3045000 | Avg Loss: 0.0156 | Grad Norm: 0.00950475\n",
      "Epoch 4 | Step 3045100 | Avg Loss: 0.0153 | Grad Norm: 0.00903035\n",
      "Epoch 4 | Step 3045200 | Avg Loss: 0.0153 | Grad Norm: 0.00895003\n",
      "Epoch 4 | Step 3045300 | Avg Loss: 0.0154 | Grad Norm: 0.00941010\n",
      "Epoch 4 | Step 3045400 | Avg Loss: 0.0156 | Grad Norm: 0.00851653\n",
      "Epoch 4 | Step 3045500 | Avg Loss: 0.0155 | Grad Norm: 0.00913564\n",
      "Epoch 4 | Step 3045600 | Avg Loss: 0.0155 | Grad Norm: 0.00995185\n",
      "Epoch 4 | Step 3045700 | Avg Loss: 0.0157 | Grad Norm: 0.00955211\n",
      "Epoch 4 | Step 3045800 | Avg Loss: 0.0154 | Grad Norm: 0.00819991\n",
      "Epoch 4 | Step 3045900 | Avg Loss: 0.0158 | Grad Norm: 0.00882118\n",
      "Epoch 4 | Step 3046000 | Avg Loss: 0.0153 | Grad Norm: 0.00896790\n",
      "Epoch 4 | Step 3046100 | Avg Loss: 0.0153 | Grad Norm: 0.01006626\n",
      "Epoch 4 | Step 3046200 | Avg Loss: 0.0153 | Grad Norm: 0.00814827\n",
      "Epoch 4 | Step 3046300 | Avg Loss: 0.0158 | Grad Norm: 0.00936261\n",
      "Epoch 4 | Step 3046400 | Avg Loss: 0.0155 | Grad Norm: 0.01130029\n",
      "Epoch 4 | Step 3046500 | Avg Loss: 0.0154 | Grad Norm: 0.00837063\n",
      "Epoch 4 | Step 3046600 | Avg Loss: 0.0155 | Grad Norm: 0.00891788\n",
      "Epoch 4 | Step 3046700 | Avg Loss: 0.0151 | Grad Norm: 0.00922241\n",
      "Epoch 4 | Step 3046800 | Avg Loss: 0.0153 | Grad Norm: 0.00973684\n",
      "Epoch 4 | Step 3046900 | Avg Loss: 0.0155 | Grad Norm: 0.00872057\n",
      "Epoch 4 | Step 3047000 | Avg Loss: 0.0154 | Grad Norm: 0.00965721\n",
      "Epoch 4 | Step 3047100 | Avg Loss: 0.0155 | Grad Norm: 0.01013693\n",
      "Epoch 4 | Step 3047200 | Avg Loss: 0.0155 | Grad Norm: 0.01104501\n",
      "Epoch 4 | Step 3047300 | Avg Loss: 0.0153 | Grad Norm: 0.00909312\n",
      "Epoch 4 | Step 3047400 | Avg Loss: 0.0155 | Grad Norm: 0.01197573\n",
      "Epoch 4 | Step 3047500 | Avg Loss: 0.0153 | Grad Norm: 0.00864819\n",
      "Epoch 4 | Step 3047600 | Avg Loss: 0.0154 | Grad Norm: 0.00885863\n",
      "Epoch 4 | Step 3047700 | Avg Loss: 0.0157 | Grad Norm: 0.00926618\n",
      "Epoch 4 | Step 3047800 | Avg Loss: 0.0150 | Grad Norm: 0.00862615\n",
      "Epoch 4 | Step 3047900 | Avg Loss: 0.0155 | Grad Norm: 0.00852746\n",
      "Epoch 4 | Step 3048000 | Avg Loss: 0.0154 | Grad Norm: 0.00939130\n",
      "Epoch 4 | Step 3048100 | Avg Loss: 0.0155 | Grad Norm: 0.00943019\n",
      "Epoch 4 | Step 3048200 | Avg Loss: 0.0149 | Grad Norm: 0.00952140\n",
      "Epoch 4 | Step 3048300 | Avg Loss: 0.0152 | Grad Norm: 0.00936358\n",
      "Epoch 4 | Step 3048400 | Avg Loss: 0.0153 | Grad Norm: 0.00851007\n",
      "Epoch 4 | Step 3048500 | Avg Loss: 0.0151 | Grad Norm: 0.00911735\n",
      "Epoch 4 | Step 3048600 | Avg Loss: 0.0153 | Grad Norm: 0.00832061\n",
      "Epoch 4 | Step 3048700 | Avg Loss: 0.0153 | Grad Norm: 0.00758495\n",
      "Epoch 4 | Step 3048800 | Avg Loss: 0.0150 | Grad Norm: 0.00886606\n",
      "Epoch 4 | Step 3048900 | Avg Loss: 0.0153 | Grad Norm: 0.00945288\n",
      "Epoch 4 | Step 3049000 | Avg Loss: 0.0154 | Grad Norm: 0.00924560\n",
      "Epoch 4 | Step 3049100 | Avg Loss: 0.0155 | Grad Norm: 0.00948167\n",
      "Epoch 4 | Step 3049200 | Avg Loss: 0.0151 | Grad Norm: 0.00897198\n",
      "Epoch 4 | Step 3049300 | Avg Loss: 0.0155 | Grad Norm: 0.00968354\n",
      "Epoch 4 | Step 3049400 | Avg Loss: 0.0155 | Grad Norm: 0.00800503\n",
      "Epoch 4 | Step 3049500 | Avg Loss: 0.0155 | Grad Norm: 0.00929814\n",
      "Epoch 4 | Step 3049600 | Avg Loss: 0.0156 | Grad Norm: 0.00924917\n",
      "Epoch 4 | Step 3049700 | Avg Loss: 0.0155 | Grad Norm: 0.00916987\n",
      "Epoch 4 | Step 3049800 | Avg Loss: 0.0154 | Grad Norm: 0.00922364\n",
      "Epoch 4 | Step 3049900 | Avg Loss: 0.0153 | Grad Norm: 0.00895175\n",
      "Epoch 4 | Step 3050000 | Avg Loss: 0.0155 | Grad Norm: 0.01069871\n",
      "Epoch 4 | Step 3050100 | Avg Loss: 0.0153 | Grad Norm: 0.00889045\n",
      "Epoch 4 | Step 3050200 | Avg Loss: 0.0155 | Grad Norm: 0.00903354\n",
      "Epoch 4 | Step 3050300 | Avg Loss: 0.0159 | Grad Norm: 0.01131782\n",
      "Epoch 4 | Step 3050400 | Avg Loss: 0.0157 | Grad Norm: 0.00852453\n",
      "Epoch 4 | Step 3050500 | Avg Loss: 0.0156 | Grad Norm: 0.00921821\n",
      "Epoch 4 | Step 3050600 | Avg Loss: 0.0159 | Grad Norm: 0.00831653\n",
      "Epoch 4 | Step 3050700 | Avg Loss: 0.0157 | Grad Norm: 0.00987310\n",
      "Epoch 4 | Step 3050800 | Avg Loss: 0.0157 | Grad Norm: 0.01004317\n",
      "Epoch 4 | Step 3050900 | Avg Loss: 0.0155 | Grad Norm: 0.00818248\n",
      "Epoch 4 | Step 3051000 | Avg Loss: 0.0156 | Grad Norm: 0.00921403\n",
      "Epoch 4 | Step 3051100 | Avg Loss: 0.0155 | Grad Norm: 0.00911383\n",
      "Epoch 4 | Step 3051200 | Avg Loss: 0.0156 | Grad Norm: 0.00988368\n",
      "Epoch 4 | Step 3051300 | Avg Loss: 0.0155 | Grad Norm: 0.00922929\n",
      "Epoch 4 | Step 3051400 | Avg Loss: 0.0157 | Grad Norm: 0.00876585\n",
      "Epoch 4 | Step 3051500 | Avg Loss: 0.0160 | Grad Norm: 0.00937087\n",
      "Epoch 4 | Step 3051600 | Avg Loss: 0.0160 | Grad Norm: 0.00854461\n",
      "Epoch 4 | Step 3051700 | Avg Loss: 0.0159 | Grad Norm: 0.00826205\n",
      "Epoch 4 | Step 3051800 | Avg Loss: 0.0153 | Grad Norm: 0.00889487\n",
      "Epoch 4 | Step 3051900 | Avg Loss: 0.0154 | Grad Norm: 0.01223538\n",
      "Epoch 4 | Step 3052000 | Avg Loss: 0.0158 | Grad Norm: 0.00735977\n",
      "Epoch 4 | Step 3052100 | Avg Loss: 0.0157 | Grad Norm: 0.00958812\n",
      "Epoch 4 | Step 3052200 | Avg Loss: 0.0160 | Grad Norm: 0.00855723\n",
      "Epoch 4 | Step 3052300 | Avg Loss: 0.0159 | Grad Norm: 0.00894896\n",
      "Epoch 4 | Step 3052400 | Avg Loss: 0.0159 | Grad Norm: 0.01007339\n",
      "Epoch 4 | Step 3052500 | Avg Loss: 0.0155 | Grad Norm: 0.00934817\n",
      "Epoch 4 | Step 3052600 | Avg Loss: 0.0156 | Grad Norm: 0.00787498\n",
      "Epoch 4 | Step 3052700 | Avg Loss: 0.0155 | Grad Norm: 0.01103188\n",
      "Epoch 4 | Step 3052800 | Avg Loss: 0.0154 | Grad Norm: 0.01037927\n",
      "Epoch 4 | Step 3052900 | Avg Loss: 0.0155 | Grad Norm: 0.00947919\n",
      "Epoch 4 | Step 3053000 | Avg Loss: 0.0152 | Grad Norm: 0.00902561\n",
      "Epoch 4 | Step 3053100 | Avg Loss: 0.0154 | Grad Norm: 0.01100730\n",
      "Epoch 4 | Step 3053200 | Avg Loss: 0.0158 | Grad Norm: 0.00982923\n",
      "Epoch 4 | Step 3053300 | Avg Loss: 0.0154 | Grad Norm: 0.00995573\n",
      "Epoch 4 | Step 3053400 | Avg Loss: 0.0154 | Grad Norm: 0.00925473\n",
      "Epoch 4 | Step 3053500 | Avg Loss: 0.0153 | Grad Norm: 0.00847888\n",
      "Epoch 4 | Step 3053600 | Avg Loss: 0.0151 | Grad Norm: 0.00781133\n",
      "Epoch 4 | Step 3053700 | Avg Loss: 0.0153 | Grad Norm: 0.00945888\n",
      "Epoch 4 | Step 3053800 | Avg Loss: 0.0159 | Grad Norm: 0.00965337\n",
      "Epoch 4 | Step 3053900 | Avg Loss: 0.0160 | Grad Norm: 0.01011004\n",
      "Epoch 4 | Step 3054000 | Avg Loss: 0.0155 | Grad Norm: 0.00911011\n",
      "Epoch 4 | Step 3054100 | Avg Loss: 0.0154 | Grad Norm: 0.00843715\n",
      "Epoch 4 | Step 3054200 | Avg Loss: 0.0155 | Grad Norm: 0.01054028\n",
      "Epoch 4 | Step 3054300 | Avg Loss: 0.0154 | Grad Norm: 0.00932712\n",
      "Epoch 4 | Step 3054400 | Avg Loss: 0.0157 | Grad Norm: 0.01103489\n",
      "Epoch 4 | Step 3054500 | Avg Loss: 0.0159 | Grad Norm: 0.00921558\n",
      "Epoch 4 | Step 3054600 | Avg Loss: 0.0158 | Grad Norm: 0.00976957\n",
      "Epoch 4 | Step 3054700 | Avg Loss: 0.0156 | Grad Norm: 0.00905666\n",
      "Epoch 4 | Step 3054800 | Avg Loss: 0.0154 | Grad Norm: 0.00868636\n",
      "Epoch 4 | Step 3054900 | Avg Loss: 0.0155 | Grad Norm: 0.00898551\n",
      "Epoch 4 | Step 3055000 | Avg Loss: 0.0151 | Grad Norm: 0.00950303\n",
      "Epoch 4 | Step 3055100 | Avg Loss: 0.0151 | Grad Norm: 0.00869187\n",
      "Epoch 4 | Step 3055200 | Avg Loss: 0.0150 | Grad Norm: 0.00825700\n",
      "Epoch 4 | Step 3055300 | Avg Loss: 0.0152 | Grad Norm: 0.00943400\n",
      "Epoch 4 | Step 3055400 | Avg Loss: 0.0153 | Grad Norm: 0.01121953\n",
      "Epoch 4 | Step 3055500 | Avg Loss: 0.0149 | Grad Norm: 0.01015712\n",
      "Epoch 4 | Step 3055600 | Avg Loss: 0.0151 | Grad Norm: 0.00839520\n",
      "Epoch 4 | Step 3055700 | Avg Loss: 0.0153 | Grad Norm: 0.00830491\n",
      "Epoch 4 | Step 3055800 | Avg Loss: 0.0160 | Grad Norm: 0.00841805\n",
      "Epoch 4 | Step 3055900 | Avg Loss: 0.0159 | Grad Norm: 0.01030168\n",
      "Epoch 4 | Step 3056000 | Avg Loss: 0.0157 | Grad Norm: 0.00878728\n",
      "Epoch 4 | Step 3056100 | Avg Loss: 0.0157 | Grad Norm: 0.00891145\n",
      "Epoch 4 | Step 3056200 | Avg Loss: 0.0157 | Grad Norm: 0.00838457\n",
      "Epoch 4 | Step 3056300 | Avg Loss: 0.0157 | Grad Norm: 0.00902138\n",
      "Epoch 4 | Step 3056400 | Avg Loss: 0.0156 | Grad Norm: 0.00826232\n",
      "Epoch 4 | Step 3056500 | Avg Loss: 0.0156 | Grad Norm: 0.00924603\n",
      "Epoch 4 | Step 3056600 | Avg Loss: 0.0155 | Grad Norm: 0.00834760\n",
      "Epoch 4 | Step 3056700 | Avg Loss: 0.0154 | Grad Norm: 0.00790377\n",
      "Epoch 4 | Step 3056800 | Avg Loss: 0.0153 | Grad Norm: 0.00837145\n",
      "Epoch 4 | Step 3056900 | Avg Loss: 0.0154 | Grad Norm: 0.00848753\n",
      "Epoch 4 | Step 3057000 | Avg Loss: 0.0156 | Grad Norm: 0.00830533\n",
      "Epoch 4 | Step 3057100 | Avg Loss: 0.0157 | Grad Norm: 0.00932852\n",
      "Epoch 4 | Step 3057200 | Avg Loss: 0.0159 | Grad Norm: 0.00757421\n",
      "Epoch 4 | Step 3057300 | Avg Loss: 0.0160 | Grad Norm: 0.00874830\n",
      "Epoch 4 | Step 3057400 | Avg Loss: 0.0157 | Grad Norm: 0.00805184\n",
      "Epoch 4 | Step 3057500 | Avg Loss: 0.0161 | Grad Norm: 0.00912860\n",
      "Epoch 4 | Step 3057600 | Avg Loss: 0.0162 | Grad Norm: 0.01007580\n",
      "Epoch 4 | Step 3057700 | Avg Loss: 0.0159 | Grad Norm: 0.00911449\n",
      "Epoch 4 | Step 3057800 | Avg Loss: 0.0156 | Grad Norm: 0.00946597\n",
      "Epoch 4 | Step 3057900 | Avg Loss: 0.0155 | Grad Norm: 0.00770280\n",
      "Epoch 4 | Step 3058000 | Avg Loss: 0.0156 | Grad Norm: 0.00975304\n",
      "Epoch 4 | Step 3058100 | Avg Loss: 0.0151 | Grad Norm: 0.00829762\n",
      "Epoch 4 | Step 3058200 | Avg Loss: 0.0154 | Grad Norm: 0.00947140\n",
      "Epoch 4 | Step 3058300 | Avg Loss: 0.0155 | Grad Norm: 0.00854249\n",
      "Epoch 4 | Step 3058400 | Avg Loss: 0.0154 | Grad Norm: 0.00838969\n",
      "Epoch 4 | Step 3058500 | Avg Loss: 0.0155 | Grad Norm: 0.00843799\n",
      "Epoch 4 | Step 3058600 | Avg Loss: 0.0153 | Grad Norm: 0.00905114\n",
      "Epoch 4 | Step 3058700 | Avg Loss: 0.0150 | Grad Norm: 0.00831732\n",
      "Epoch 4 | Step 3058800 | Avg Loss: 0.0153 | Grad Norm: 0.00936877\n",
      "Epoch 4 | Step 3058900 | Avg Loss: 0.0157 | Grad Norm: 0.00920914\n",
      "Epoch 4 | Step 3059000 | Avg Loss: 0.0155 | Grad Norm: 0.00850099\n",
      "Epoch 4 | Step 3059100 | Avg Loss: 0.0152 | Grad Norm: 0.00886071\n",
      "Epoch 4 | Step 3059200 | Avg Loss: 0.0154 | Grad Norm: 0.00866248\n",
      "Epoch 4 | Step 3059300 | Avg Loss: 0.0158 | Grad Norm: 0.00847859\n",
      "Epoch 4 | Step 3059400 | Avg Loss: 0.0156 | Grad Norm: 0.00973282\n",
      "Epoch 4 | Step 3059500 | Avg Loss: 0.0160 | Grad Norm: 0.01053600\n",
      "Epoch 4 | Step 3059600 | Avg Loss: 0.0161 | Grad Norm: 0.00880765\n",
      "Epoch 4 | Step 3059700 | Avg Loss: 0.0160 | Grad Norm: 0.01068962\n",
      "Epoch 4 | Step 3059800 | Avg Loss: 0.0161 | Grad Norm: 0.01035461\n",
      "Epoch 4 | Step 3059900 | Avg Loss: 0.0161 | Grad Norm: 0.01013678\n",
      "Epoch 4 | Step 3060000 | Avg Loss: 0.0162 | Grad Norm: 0.00960063\n",
      "Epoch 4 | Step 3060100 | Avg Loss: 0.0161 | Grad Norm: 0.01054793\n",
      "Epoch 4 | Step 3060200 | Avg Loss: 0.0160 | Grad Norm: 0.01398050\n",
      "Epoch 4 | Step 3060300 | Avg Loss: 0.0157 | Grad Norm: 0.01133531\n",
      "Epoch 4 | Step 3060400 | Avg Loss: 0.0154 | Grad Norm: 0.01011714\n",
      "Epoch 4 | Step 3060500 | Avg Loss: 0.0152 | Grad Norm: 0.01029928\n",
      "Epoch 4 | Step 3060600 | Avg Loss: 0.0158 | Grad Norm: 0.00831993\n",
      "Epoch 4 | Step 3060700 | Avg Loss: 0.0159 | Grad Norm: 0.00931771\n",
      "Epoch 4 | Step 3060800 | Avg Loss: 0.0158 | Grad Norm: 0.01027891\n",
      "Epoch 4 | Step 3060900 | Avg Loss: 0.0157 | Grad Norm: 0.01004435\n",
      "Epoch 4 | Step 3061000 | Avg Loss: 0.0156 | Grad Norm: 0.00876072\n",
      "Epoch 4 | Step 3061100 | Avg Loss: 0.0158 | Grad Norm: 0.00958818\n",
      "Epoch 4 | Step 3061200 | Avg Loss: 0.0160 | Grad Norm: 0.00923049\n",
      "Epoch 4 | Step 3061300 | Avg Loss: 0.0158 | Grad Norm: 0.00845719\n",
      "Epoch 4 | Step 3061400 | Avg Loss: 0.0160 | Grad Norm: 0.00959600\n",
      "Epoch 4 | Step 3061500 | Avg Loss: 0.0163 | Grad Norm: 0.00961982\n",
      "Epoch 4 | Step 3061600 | Avg Loss: 0.0162 | Grad Norm: 0.00981406\n",
      "Epoch 4 | Step 3061700 | Avg Loss: 0.0159 | Grad Norm: 0.00889492\n",
      "Epoch 4 | Step 3061800 | Avg Loss: 0.0159 | Grad Norm: 0.00918229\n",
      "Epoch 4 | Step 3061900 | Avg Loss: 0.0159 | Grad Norm: 0.00997284\n",
      "Epoch 4 | Step 3062000 | Avg Loss: 0.0162 | Grad Norm: 0.00832017\n",
      "Epoch 4 | Step 3062100 | Avg Loss: 0.0160 | Grad Norm: 0.01032946\n",
      "Epoch 4 | Step 3062200 | Avg Loss: 0.0161 | Grad Norm: 0.01097658\n",
      "Epoch 4 | Step 3062300 | Avg Loss: 0.0161 | Grad Norm: 0.00847679\n",
      "Epoch 4 | Step 3062400 | Avg Loss: 0.0158 | Grad Norm: 0.00910546\n",
      "Epoch 4 | Step 3062500 | Avg Loss: 0.0157 | Grad Norm: 0.00853143\n",
      "Epoch 4 | Step 3062600 | Avg Loss: 0.0156 | Grad Norm: 0.00910713\n",
      "Epoch 4 | Step 3062700 | Avg Loss: 0.0154 | Grad Norm: 0.00910193\n",
      "Epoch 4 | Step 3062800 | Avg Loss: 0.0156 | Grad Norm: 0.00822532\n",
      "Epoch 4 | Step 3062900 | Avg Loss: 0.0153 | Grad Norm: 0.00823981\n",
      "Epoch 4 | Step 3063000 | Avg Loss: 0.0155 | Grad Norm: 0.00790514\n",
      "Epoch 4 | Step 3063100 | Avg Loss: 0.0156 | Grad Norm: 0.01300433\n",
      "Epoch 4 | Step 3063200 | Avg Loss: 0.0154 | Grad Norm: 0.00953849\n",
      "Epoch 4 | Step 3063300 | Avg Loss: 0.0153 | Grad Norm: 0.01007658\n",
      "Epoch 4 | Step 3063400 | Avg Loss: 0.0152 | Grad Norm: 0.00845212\n",
      "Epoch 4 | Step 3063500 | Avg Loss: 0.0150 | Grad Norm: 0.01145133\n",
      "Epoch 4 | Step 3063600 | Avg Loss: 0.0154 | Grad Norm: 0.00953842\n",
      "Epoch 4 | Step 3063700 | Avg Loss: 0.0156 | Grad Norm: 0.00837490\n",
      "Epoch 4 | Step 3063800 | Avg Loss: 0.0154 | Grad Norm: 0.01039030\n",
      "Epoch 4 | Step 3063900 | Avg Loss: 0.0153 | Grad Norm: 0.00865791\n",
      "Epoch 4 | Step 3064000 | Avg Loss: 0.0153 | Grad Norm: 0.00821647\n",
      "Epoch 4 | Step 3064100 | Avg Loss: 0.0149 | Grad Norm: 0.00941269\n",
      "Epoch 4 | Step 3064200 | Avg Loss: 0.0154 | Grad Norm: 0.00959723\n",
      "Epoch 4 | Step 3064300 | Avg Loss: 0.0154 | Grad Norm: 0.00955942\n",
      "Epoch 4 | Step 3064400 | Avg Loss: 0.0148 | Grad Norm: 0.00944070\n",
      "Epoch 4 | Step 3064500 | Avg Loss: 0.0148 | Grad Norm: 0.00864353\n",
      "Epoch 4 | Step 3064600 | Avg Loss: 0.0149 | Grad Norm: 0.00838306\n",
      "Epoch 4 | Step 3064700 | Avg Loss: 0.0151 | Grad Norm: 0.00890854\n",
      "Epoch 4 | Step 3064800 | Avg Loss: 0.0148 | Grad Norm: 0.00917166\n",
      "Epoch 4 | Step 3064900 | Avg Loss: 0.0151 | Grad Norm: 0.01004925\n",
      "Epoch 4 | Step 3065000 | Avg Loss: 0.0150 | Grad Norm: 0.00944626\n",
      "Epoch 4 | Step 3065100 | Avg Loss: 0.0153 | Grad Norm: 0.00846914\n",
      "Epoch 4 | Step 3065200 | Avg Loss: 0.0153 | Grad Norm: 0.00981447\n",
      "Epoch 4 | Step 3065300 | Avg Loss: 0.0157 | Grad Norm: 0.00926941\n",
      "Epoch 4 | Step 3065400 | Avg Loss: 0.0156 | Grad Norm: 0.00856388\n",
      "Epoch 4 | Step 3065500 | Avg Loss: 0.0153 | Grad Norm: 0.00872656\n",
      "Epoch 4 | Step 3065600 | Avg Loss: 0.0154 | Grad Norm: 0.00893464\n",
      "Epoch 4 | Step 3065700 | Avg Loss: 0.0153 | Grad Norm: 0.01029124\n",
      "Epoch 4 | Step 3065800 | Avg Loss: 0.0152 | Grad Norm: 0.00860243\n",
      "Epoch 4 | Step 3065900 | Avg Loss: 0.0152 | Grad Norm: 0.00949196\n",
      "Epoch 4 | Step 3066000 | Avg Loss: 0.0152 | Grad Norm: 0.00981403\n",
      "Epoch 4 | Step 3066100 | Avg Loss: 0.0153 | Grad Norm: 0.00890386\n",
      "Epoch 4 | Step 3066200 | Avg Loss: 0.0154 | Grad Norm: 0.00855411\n",
      "Epoch 4 | Step 3066300 | Avg Loss: 0.0158 | Grad Norm: 0.00889144\n",
      "Epoch 4 | Step 3066400 | Avg Loss: 0.0157 | Grad Norm: 0.00915961\n",
      "Epoch 4 | Step 3066500 | Avg Loss: 0.0155 | Grad Norm: 0.00815034\n",
      "Epoch 4 | Step 3066600 | Avg Loss: 0.0155 | Grad Norm: 0.00946312\n",
      "Epoch 4 | Step 3066700 | Avg Loss: 0.0157 | Grad Norm: 0.00940464\n",
      "Epoch 4 | Step 3066800 | Avg Loss: 0.0153 | Grad Norm: 0.01011934\n",
      "Epoch 4 | Step 3066900 | Avg Loss: 0.0156 | Grad Norm: 0.00927813\n",
      "Epoch 4 | Step 3067000 | Avg Loss: 0.0154 | Grad Norm: 0.00866567\n",
      "Epoch 4 | Step 3067100 | Avg Loss: 0.0154 | Grad Norm: 0.00857307\n",
      "Epoch 4 | Step 3067200 | Avg Loss: 0.0153 | Grad Norm: 0.00939319\n",
      "Epoch 4 | Step 3067300 | Avg Loss: 0.0153 | Grad Norm: 0.01002610\n",
      "Epoch 4 | Step 3067400 | Avg Loss: 0.0150 | Grad Norm: 0.01206579\n",
      "Epoch 4 | Step 3067500 | Avg Loss: 0.0152 | Grad Norm: 0.00935975\n",
      "Epoch 4 | Step 3067600 | Avg Loss: 0.0152 | Grad Norm: 0.00801311\n",
      "Epoch 4 | Step 3067700 | Avg Loss: 0.0155 | Grad Norm: 0.00806716\n",
      "Epoch 4 | Step 3067800 | Avg Loss: 0.0157 | Grad Norm: 0.00841072\n",
      "Epoch 4 | Step 3067900 | Avg Loss: 0.0156 | Grad Norm: 0.00912201\n",
      "Epoch 4 | Step 3068000 | Avg Loss: 0.0156 | Grad Norm: 0.00911606\n",
      "Epoch 4 | Step 3068100 | Avg Loss: 0.0158 | Grad Norm: 0.00855582\n",
      "Epoch 4 | Step 3068200 | Avg Loss: 0.0160 | Grad Norm: 0.00929567\n",
      "Epoch 4 | Step 3068300 | Avg Loss: 0.0155 | Grad Norm: 0.00848771\n",
      "Epoch 4 | Step 3068400 | Avg Loss: 0.0157 | Grad Norm: 0.00966621\n",
      "Epoch 4 | Step 3068500 | Avg Loss: 0.0157 | Grad Norm: 0.00993023\n",
      "Epoch 4 | Step 3068600 | Avg Loss: 0.0155 | Grad Norm: 0.01036211\n",
      "Epoch 4 | Step 3068700 | Avg Loss: 0.0151 | Grad Norm: 0.00882845\n",
      "Epoch 4 | Step 3068800 | Avg Loss: 0.0155 | Grad Norm: 0.00853696\n",
      "Epoch 4 | Step 3068900 | Avg Loss: 0.0153 | Grad Norm: 0.00925557\n",
      "Epoch 4 | Step 3069000 | Avg Loss: 0.0150 | Grad Norm: 0.00986217\n",
      "Epoch 4 | Step 3069100 | Avg Loss: 0.0156 | Grad Norm: 0.00892935\n",
      "Epoch 4 | Step 3069200 | Avg Loss: 0.0154 | Grad Norm: 0.00807866\n",
      "Epoch 4 | Step 3069300 | Avg Loss: 0.0148 | Grad Norm: 0.00921758\n",
      "Epoch 4 | Step 3069400 | Avg Loss: 0.0148 | Grad Norm: 0.00928791\n",
      "Epoch 4 | Step 3069500 | Avg Loss: 0.0146 | Grad Norm: 0.00923874\n",
      "Epoch 4 | Step 3069600 | Avg Loss: 0.0146 | Grad Norm: 0.01157237\n",
      "Epoch 4 | Step 3069700 | Avg Loss: 0.0147 | Grad Norm: 0.00941494\n",
      "Epoch 4 | Step 3069800 | Avg Loss: 0.0151 | Grad Norm: 0.00923208\n",
      "Epoch 4 | Step 3069900 | Avg Loss: 0.0152 | Grad Norm: 0.00827854\n",
      "Epoch 4 | Step 3070000 | Avg Loss: 0.0154 | Grad Norm: 0.00869600\n",
      "Epoch 4 | Step 3070100 | Avg Loss: 0.0151 | Grad Norm: 0.00912842\n",
      "Epoch 4 | Step 3070200 | Avg Loss: 0.0149 | Grad Norm: 0.00956004\n",
      "Epoch 4 | Step 3070300 | Avg Loss: 0.0151 | Grad Norm: 0.00922756\n",
      "Epoch 4 | Step 3070400 | Avg Loss: 0.0152 | Grad Norm: 0.00964153\n",
      "Epoch 4 | Step 3070500 | Avg Loss: 0.0153 | Grad Norm: 0.00900781\n",
      "Epoch 4 | Step 3070600 | Avg Loss: 0.0153 | Grad Norm: 0.00936674\n",
      "Epoch 4 | Step 3070700 | Avg Loss: 0.0155 | Grad Norm: 0.01067190\n",
      "Epoch 4 | Step 3070800 | Avg Loss: 0.0156 | Grad Norm: 0.00896479\n",
      "Epoch 4 | Step 3070900 | Avg Loss: 0.0158 | Grad Norm: 0.00859483\n",
      "Epoch 4 | Step 3071000 | Avg Loss: 0.0154 | Grad Norm: 0.00806504\n",
      "Epoch 4 | Step 3071100 | Avg Loss: 0.0156 | Grad Norm: 0.00887565\n",
      "Epoch 4 | Step 3071200 | Avg Loss: 0.0152 | Grad Norm: 0.00885668\n",
      "Epoch 4 | Step 3071300 | Avg Loss: 0.0152 | Grad Norm: 0.00838259\n",
      "Epoch 4 | Step 3071400 | Avg Loss: 0.0154 | Grad Norm: 0.00930302\n",
      "Epoch 4 | Step 3071500 | Avg Loss: 0.0155 | Grad Norm: 0.00843533\n",
      "Epoch 4 | Step 3071600 | Avg Loss: 0.0157 | Grad Norm: 0.00941080\n",
      "Epoch 4 | Step 3071700 | Avg Loss: 0.0155 | Grad Norm: 0.00805036\n",
      "Epoch 4 | Step 3071800 | Avg Loss: 0.0163 | Grad Norm: 0.00823610\n",
      "Epoch 4 | Step 3071900 | Avg Loss: 0.0153 | Grad Norm: 0.00856232\n",
      "Epoch 4 | Step 3072000 | Avg Loss: 0.0150 | Grad Norm: 0.00888946\n",
      "Epoch 4 | Step 3072100 | Avg Loss: 0.0151 | Grad Norm: 0.00933508\n",
      "Epoch 4 | Step 3072200 | Avg Loss: 0.0151 | Grad Norm: 0.00816028\n",
      "Epoch 4 | Step 3072300 | Avg Loss: 0.0152 | Grad Norm: 0.00884577\n",
      "Epoch 4 | Step 3072400 | Avg Loss: 0.0154 | Grad Norm: 0.00845467\n",
      "Epoch 4 | Step 3072500 | Avg Loss: 0.0154 | Grad Norm: 0.00947132\n",
      "Epoch 4 | Step 3072600 | Avg Loss: 0.0156 | Grad Norm: 0.01114265\n",
      "Epoch 4 | Step 3072700 | Avg Loss: 0.0157 | Grad Norm: 0.00935115\n",
      "Epoch 4 | Step 3072800 | Avg Loss: 0.0158 | Grad Norm: 0.00893714\n",
      "Epoch 4 | Step 3072900 | Avg Loss: 0.0159 | Grad Norm: 0.01115698\n",
      "Epoch 4 | Step 3073000 | Avg Loss: 0.0155 | Grad Norm: 0.00978396\n",
      "Epoch 4 | Step 3073100 | Avg Loss: 0.0157 | Grad Norm: 0.00859586\n",
      "Epoch 4 | Step 3073200 | Avg Loss: 0.0157 | Grad Norm: 0.00956886\n",
      "Epoch 4 | Step 3073300 | Avg Loss: 0.0159 | Grad Norm: 0.01085370\n",
      "Epoch 4 | Step 3073400 | Avg Loss: 0.0157 | Grad Norm: 0.00791049\n",
      "Epoch 4 | Step 3073500 | Avg Loss: 0.0159 | Grad Norm: 0.00862612\n",
      "Epoch 4 | Step 3073600 | Avg Loss: 0.0158 | Grad Norm: 0.00856401\n",
      "Epoch 4 | Step 3073700 | Avg Loss: 0.0160 | Grad Norm: 0.00911264\n",
      "Epoch 4 | Step 3073800 | Avg Loss: 0.0155 | Grad Norm: 0.00938825\n",
      "Epoch 4 | Step 3073900 | Avg Loss: 0.0157 | Grad Norm: 0.00960140\n",
      "Epoch 4 | Step 3074000 | Avg Loss: 0.0157 | Grad Norm: 0.00873938\n",
      "Epoch 4 | Step 3074100 | Avg Loss: 0.0158 | Grad Norm: 0.00843814\n",
      "Epoch 4 | Step 3074200 | Avg Loss: 0.0160 | Grad Norm: 0.00839695\n",
      "Epoch 4 | Step 3074300 | Avg Loss: 0.0161 | Grad Norm: 0.00826199\n",
      "Epoch 4 | Step 3074400 | Avg Loss: 0.0158 | Grad Norm: 0.01024101\n",
      "Epoch 4 | Step 3074500 | Avg Loss: 0.0157 | Grad Norm: 0.00991117\n",
      "Epoch 4 | Step 3074600 | Avg Loss: 0.0158 | Grad Norm: 0.00846954\n",
      "Epoch 4 | Step 3074700 | Avg Loss: 0.0156 | Grad Norm: 0.00794750\n",
      "Epoch 4 | Step 3074800 | Avg Loss: 0.0154 | Grad Norm: 0.00955685\n",
      "Epoch 4 | Step 3074900 | Avg Loss: 0.0154 | Grad Norm: 0.00826698\n",
      "Epoch 4 | Step 3075000 | Avg Loss: 0.0149 | Grad Norm: 0.00850439\n",
      "Epoch 4 | Step 3075100 | Avg Loss: 0.0153 | Grad Norm: 0.00967158\n",
      "Epoch 4 | Step 3075200 | Avg Loss: 0.0155 | Grad Norm: 0.00891331\n",
      "Epoch 4 | Step 3075300 | Avg Loss: 0.0156 | Grad Norm: 0.00845025\n",
      "Epoch 4 | Step 3075400 | Avg Loss: 0.0156 | Grad Norm: 0.00784368\n",
      "Epoch 4 | Step 3075500 | Avg Loss: 0.0158 | Grad Norm: 0.00810520\n",
      "Epoch 4 | Step 3075600 | Avg Loss: 0.0157 | Grad Norm: 0.00844688\n",
      "Epoch 4 | Step 3075700 | Avg Loss: 0.0154 | Grad Norm: 0.01956381\n",
      "Epoch 4 | Step 3075800 | Avg Loss: 0.0154 | Grad Norm: 0.01041402\n",
      "Epoch 4 | Step 3075900 | Avg Loss: 0.0151 | Grad Norm: 0.00939587\n",
      "Epoch 4 | Step 3076000 | Avg Loss: 0.0155 | Grad Norm: 0.00807271\n",
      "Epoch 4 | Step 3076100 | Avg Loss: 0.0156 | Grad Norm: 0.00920230\n",
      "Epoch 4 | Step 3076200 | Avg Loss: 0.0153 | Grad Norm: 0.00805293\n",
      "Epoch 4 | Step 3076300 | Avg Loss: 0.0145 | Grad Norm: 0.01147554\n",
      "Epoch 4 | Step 3076400 | Avg Loss: 0.0149 | Grad Norm: 0.01088747\n",
      "Epoch 4 | Step 3076500 | Avg Loss: 0.0150 | Grad Norm: 0.00861048\n",
      "Epoch 4 | Step 3076600 | Avg Loss: 0.0149 | Grad Norm: 0.00878976\n",
      "Epoch 4 | Step 3076700 | Avg Loss: 0.0154 | Grad Norm: 0.00848478\n",
      "Epoch 4 | Step 3076800 | Avg Loss: 0.0150 | Grad Norm: 0.00968832\n",
      "Epoch 4 | Step 3076900 | Avg Loss: 0.0153 | Grad Norm: 0.00873545\n",
      "Epoch 4 | Step 3077000 | Avg Loss: 0.0150 | Grad Norm: 0.00859776\n",
      "Epoch 4 | Step 3077100 | Avg Loss: 0.0152 | Grad Norm: 0.00862249\n",
      "Epoch 4 | Step 3077200 | Avg Loss: 0.0151 | Grad Norm: 0.01056071\n",
      "Epoch 4 | Step 3077300 | Avg Loss: 0.0149 | Grad Norm: 0.00962387\n",
      "Epoch 4 | Step 3077400 | Avg Loss: 0.0155 | Grad Norm: 0.00915170\n",
      "Epoch 4 | Step 3077500 | Avg Loss: 0.0152 | Grad Norm: 0.00811528\n",
      "Epoch 4 | Step 3077600 | Avg Loss: 0.0158 | Grad Norm: 0.00942378\n",
      "Epoch 4 | Step 3077700 | Avg Loss: 0.0157 | Grad Norm: 0.00931331\n",
      "Epoch 4 | Step 3077800 | Avg Loss: 0.0160 | Grad Norm: 0.00829450\n",
      "Epoch 4 | Step 3077900 | Avg Loss: 0.0156 | Grad Norm: 0.00967547\n",
      "Epoch 4 | Step 3078000 | Avg Loss: 0.0157 | Grad Norm: 0.00938367\n",
      "Epoch 4 | Step 3078100 | Avg Loss: 0.0157 | Grad Norm: 0.01044658\n",
      "Epoch 4 | Step 3078200 | Avg Loss: 0.0155 | Grad Norm: 0.01174240\n",
      "Epoch 4 | Step 3078300 | Avg Loss: 0.0158 | Grad Norm: 0.00878835\n",
      "Epoch 4 | Step 3078400 | Avg Loss: 0.0156 | Grad Norm: 0.00966840\n",
      "Epoch 4 | Step 3078500 | Avg Loss: 0.0156 | Grad Norm: 0.00797775\n",
      "Epoch 4 | Step 3078600 | Avg Loss: 0.0160 | Grad Norm: 0.01037776\n",
      "Epoch 4 | Step 3078700 | Avg Loss: 0.0159 | Grad Norm: 0.00844533\n",
      "Epoch 4 | Step 3078800 | Avg Loss: 0.0159 | Grad Norm: 0.01062506\n",
      "Epoch 4 | Step 3078900 | Avg Loss: 0.0158 | Grad Norm: 0.00949789\n",
      "Epoch 4 | Step 3079000 | Avg Loss: 0.0156 | Grad Norm: 0.00943710\n",
      "Epoch 4 | Step 3079100 | Avg Loss: 0.0157 | Grad Norm: 0.00843911\n",
      "Epoch 4 | Step 3079200 | Avg Loss: 0.0154 | Grad Norm: 0.01146256\n",
      "Epoch 4 | Step 3079300 | Avg Loss: 0.0150 | Grad Norm: 0.00926615\n",
      "Epoch 4 | Step 3079400 | Avg Loss: 0.0151 | Grad Norm: 0.00894936\n",
      "Epoch 4 | Step 3079500 | Avg Loss: 0.0151 | Grad Norm: 0.00823397\n",
      "Epoch 4 | Step 3079600 | Avg Loss: 0.0154 | Grad Norm: 0.00845599\n",
      "Epoch 4 | Step 3079700 | Avg Loss: 0.0154 | Grad Norm: 0.00863573\n",
      "Epoch 4 | Step 3079800 | Avg Loss: 0.0154 | Grad Norm: 0.00895987\n",
      "Epoch 4 | Step 3079900 | Avg Loss: 0.0153 | Grad Norm: 0.00883493\n",
      "Epoch 4 | Step 3080000 | Avg Loss: 0.0150 | Grad Norm: 0.00990236\n",
      "Epoch 4 | Step 3080100 | Avg Loss: 0.0152 | Grad Norm: 0.00867201\n",
      "Epoch 4 | Step 3080200 | Avg Loss: 0.0155 | Grad Norm: 0.00823985\n",
      "Epoch 4 | Step 3080300 | Avg Loss: 0.0153 | Grad Norm: 0.00893962\n",
      "Epoch 4 | Step 3080400 | Avg Loss: 0.0155 | Grad Norm: 0.00831202\n",
      "Epoch 4 | Step 3080500 | Avg Loss: 0.0155 | Grad Norm: 0.01012155\n",
      "Epoch 4 | Step 3080600 | Avg Loss: 0.0158 | Grad Norm: 0.00900788\n",
      "Epoch 4 | Step 3080700 | Avg Loss: 0.0156 | Grad Norm: 0.01343310\n",
      "Epoch 4 | Step 3080800 | Avg Loss: 0.0159 | Grad Norm: 0.00868464\n",
      "Epoch 4 | Step 3080900 | Avg Loss: 0.0156 | Grad Norm: 0.00943162\n",
      "Epoch 4 | Step 3081000 | Avg Loss: 0.0161 | Grad Norm: 0.01035540\n",
      "Epoch 4 | Step 3081100 | Avg Loss: 0.0162 | Grad Norm: 0.00886820\n",
      "Epoch 4 | Step 3081200 | Avg Loss: 0.0163 | Grad Norm: 0.01232461\n",
      "Epoch 4 | Step 3081300 | Avg Loss: 0.0162 | Grad Norm: 0.00912225\n",
      "Epoch 4 | Step 3081400 | Avg Loss: 0.0160 | Grad Norm: 0.00825838\n",
      "Epoch 4 | Step 3081500 | Avg Loss: 0.0161 | Grad Norm: 0.01227589\n",
      "Epoch 4 | Step 3081600 | Avg Loss: 0.0161 | Grad Norm: 0.00896573\n",
      "Epoch 4 | Step 3081700 | Avg Loss: 0.0156 | Grad Norm: 0.00973125\n",
      "Epoch 4 | Step 3081800 | Avg Loss: 0.0153 | Grad Norm: 0.01014291\n",
      "Epoch 4 | Step 3081900 | Avg Loss: 0.0154 | Grad Norm: 0.00857405\n",
      "Epoch 4 | Step 3082000 | Avg Loss: 0.0156 | Grad Norm: 0.00925186\n",
      "Epoch 4 | Step 3082100 | Avg Loss: 0.0156 | Grad Norm: 0.01011027\n",
      "Epoch 4 | Step 3082200 | Avg Loss: 0.0155 | Grad Norm: 0.01272413\n",
      "Epoch 4 | Step 3082300 | Avg Loss: 0.0152 | Grad Norm: 0.00767456\n",
      "Epoch 4 | Step 3082400 | Avg Loss: 0.0155 | Grad Norm: 0.00868390\n",
      "Epoch 4 | Step 3082500 | Avg Loss: 0.0150 | Grad Norm: 0.01112637\n",
      "Epoch 4 | Step 3082600 | Avg Loss: 0.0157 | Grad Norm: 0.00926695\n",
      "Epoch 4 | Step 3082700 | Avg Loss: 0.0156 | Grad Norm: 0.00824948\n",
      "Epoch 4 | Step 3082800 | Avg Loss: 0.0162 | Grad Norm: 0.00909480\n",
      "Epoch 4 | Step 3082900 | Avg Loss: 0.0163 | Grad Norm: 0.00842624\n",
      "Epoch 4 | Step 3083000 | Avg Loss: 0.0163 | Grad Norm: 0.01001473\n",
      "Epoch 4 | Step 3083100 | Avg Loss: 0.0162 | Grad Norm: 0.01136638\n",
      "Epoch 4 | Step 3083200 | Avg Loss: 0.0160 | Grad Norm: 0.01038125\n",
      "Epoch 4 | Step 3083300 | Avg Loss: 0.0163 | Grad Norm: 0.00941869\n",
      "Epoch 4 | Step 3083400 | Avg Loss: 0.0160 | Grad Norm: 0.00949073\n",
      "Epoch 4 | Step 3083500 | Avg Loss: 0.0159 | Grad Norm: 0.00804439\n",
      "Epoch 4 | Step 3083600 | Avg Loss: 0.0158 | Grad Norm: 0.00959523\n",
      "Epoch 4 | Step 3083700 | Avg Loss: 0.0154 | Grad Norm: 0.00969526\n",
      "Epoch 4 | Step 3083800 | Avg Loss: 0.0155 | Grad Norm: 0.00998867\n",
      "Epoch 4 | Step 3083900 | Avg Loss: 0.0157 | Grad Norm: 0.00885036\n",
      "Epoch 4 | Step 3084000 | Avg Loss: 0.0159 | Grad Norm: 0.00939362\n",
      "Epoch 4 | Step 3084100 | Avg Loss: 0.0160 | Grad Norm: 0.01112250\n",
      "Epoch 4 | Step 3084200 | Avg Loss: 0.0162 | Grad Norm: 0.00994859\n",
      "Epoch 4 | Step 3084300 | Avg Loss: 0.0162 | Grad Norm: 0.00919930\n",
      "Epoch 4 | Step 3084400 | Avg Loss: 0.0161 | Grad Norm: 0.00987190\n",
      "Epoch 4 | Step 3084500 | Avg Loss: 0.0159 | Grad Norm: 0.00878587\n",
      "Epoch 4 | Step 3084600 | Avg Loss: 0.0157 | Grad Norm: 0.00867098\n",
      "Epoch 4 | Step 3084700 | Avg Loss: 0.0159 | Grad Norm: 0.00994805\n",
      "Epoch 4 | Step 3084800 | Avg Loss: 0.0158 | Grad Norm: 0.00933893\n",
      "Epoch 4 | Step 3084900 | Avg Loss: 0.0157 | Grad Norm: 0.00899245\n",
      "Epoch 4 | Step 3085000 | Avg Loss: 0.0159 | Grad Norm: 0.00908409\n",
      "Epoch 4 | Step 3085100 | Avg Loss: 0.0158 | Grad Norm: 0.00899872\n",
      "Epoch 4 | Step 3085200 | Avg Loss: 0.0158 | Grad Norm: 0.00867467\n",
      "Epoch 4 | Step 3085300 | Avg Loss: 0.0157 | Grad Norm: 0.01056606\n",
      "Epoch 4 | Step 3085400 | Avg Loss: 0.0159 | Grad Norm: 0.01032683\n",
      "Epoch 4 | Step 3085500 | Avg Loss: 0.0159 | Grad Norm: 0.00950143\n",
      "Epoch 4 | Step 3085600 | Avg Loss: 0.0158 | Grad Norm: 0.00861574\n",
      "Epoch 4 | Step 3085700 | Avg Loss: 0.0157 | Grad Norm: 0.00924118\n",
      "Epoch 4 | Step 3085800 | Avg Loss: 0.0160 | Grad Norm: 0.00983194\n",
      "Epoch 4 | Step 3085900 | Avg Loss: 0.0157 | Grad Norm: 0.00845354\n",
      "Epoch 4 | Step 3086000 | Avg Loss: 0.0156 | Grad Norm: 0.00762785\n",
      "Epoch 4 | Step 3086100 | Avg Loss: 0.0154 | Grad Norm: 0.00923581\n",
      "Epoch 4 | Step 3086200 | Avg Loss: 0.0154 | Grad Norm: 0.00940787\n",
      "Epoch 4 | Step 3086300 | Avg Loss: 0.0156 | Grad Norm: 0.00913807\n",
      "Epoch 4 | Step 3086400 | Avg Loss: 0.0155 | Grad Norm: 0.00873595\n",
      "Epoch 4 | Step 3086500 | Avg Loss: 0.0159 | Grad Norm: 0.00933274\n",
      "Epoch 4 | Step 3086600 | Avg Loss: 0.0159 | Grad Norm: 0.00863655\n",
      "Epoch 4 | Step 3086700 | Avg Loss: 0.0156 | Grad Norm: 0.01074244\n",
      "Epoch 4 | Step 3086800 | Avg Loss: 0.0160 | Grad Norm: 0.00845453\n",
      "Epoch 4 | Step 3086900 | Avg Loss: 0.0161 | Grad Norm: 0.01088494\n",
      "Epoch 4 | Step 3087000 | Avg Loss: 0.0161 | Grad Norm: 0.01064161\n",
      "Epoch 4 | Step 3087100 | Avg Loss: 0.0160 | Grad Norm: 0.00905256\n",
      "Epoch 4 | Step 3087200 | Avg Loss: 0.0158 | Grad Norm: 0.00864129\n",
      "Epoch 4 | Step 3087300 | Avg Loss: 0.0158 | Grad Norm: 0.00943609\n",
      "Epoch 4 | Step 3087400 | Avg Loss: 0.0158 | Grad Norm: 0.01064923\n",
      "Epoch 4 | Step 3087500 | Avg Loss: 0.0157 | Grad Norm: 0.00851702\n",
      "Epoch 4 | Step 3087600 | Avg Loss: 0.0157 | Grad Norm: 0.00821669\n",
      "Epoch 4 | Step 3087700 | Avg Loss: 0.0154 | Grad Norm: 0.00838853\n",
      "Epoch 4 | Step 3087800 | Avg Loss: 0.0157 | Grad Norm: 0.00961492\n",
      "Epoch 4 | Step 3087900 | Avg Loss: 0.0157 | Grad Norm: 0.00823207\n",
      "Epoch 4 | Step 3088000 | Avg Loss: 0.0160 | Grad Norm: 0.00971833\n",
      "Epoch 4 | Step 3088100 | Avg Loss: 0.0163 | Grad Norm: 0.00951070\n",
      "Epoch 4 | Step 3088200 | Avg Loss: 0.0158 | Grad Norm: 0.00940688\n",
      "Epoch 4 | Step 3088300 | Avg Loss: 0.0159 | Grad Norm: 0.01051558\n",
      "Epoch 4 | Step 3088400 | Avg Loss: 0.0162 | Grad Norm: 0.00938047\n",
      "Epoch 4 | Step 3088500 | Avg Loss: 0.0161 | Grad Norm: 0.00954338\n",
      "Epoch 4 | Step 3088600 | Avg Loss: 0.0159 | Grad Norm: 0.00844649\n",
      "Epoch 4 | Step 3088700 | Avg Loss: 0.0156 | Grad Norm: 0.00864330\n",
      "Epoch 4 | Step 3088800 | Avg Loss: 0.0156 | Grad Norm: 0.00942936\n",
      "Epoch 4 | Step 3088900 | Avg Loss: 0.0156 | Grad Norm: 0.00904938\n",
      "Epoch 4 | Step 3089000 | Avg Loss: 0.0156 | Grad Norm: 0.00816125\n",
      "Epoch 4 | Step 3089100 | Avg Loss: 0.0154 | Grad Norm: 0.00850125\n",
      "Epoch 4 | Step 3089200 | Avg Loss: 0.0153 | Grad Norm: 0.00899640\n",
      "Epoch 4 | Step 3089300 | Avg Loss: 0.0149 | Grad Norm: 0.00842316\n",
      "Epoch 4 | Step 3089400 | Avg Loss: 0.0148 | Grad Norm: 0.00965171\n",
      "Epoch 4 | Step 3089500 | Avg Loss: 0.0147 | Grad Norm: 0.00728058\n",
      "Epoch 4 | Step 3089600 | Avg Loss: 0.0149 | Grad Norm: 0.00987062\n",
      "Epoch 4 | Step 3089700 | Avg Loss: 0.0149 | Grad Norm: 0.00884018\n",
      "Epoch 4 | Step 3089800 | Avg Loss: 0.0152 | Grad Norm: 0.00936342\n",
      "Epoch 4 | Step 3089900 | Avg Loss: 0.0157 | Grad Norm: 0.00841731\n",
      "Epoch 4 | Step 3090000 | Avg Loss: 0.0158 | Grad Norm: 0.00889553\n",
      "Epoch 4 | Step 3090100 | Avg Loss: 0.0161 | Grad Norm: 0.00852208\n",
      "Epoch 4 | Step 3090200 | Avg Loss: 0.0160 | Grad Norm: 0.00907038\n",
      "Epoch 4 | Step 3090300 | Avg Loss: 0.0158 | Grad Norm: 0.01650697\n",
      "Epoch 4 | Step 3090400 | Avg Loss: 0.0156 | Grad Norm: 0.01041723\n",
      "Epoch 4 | Step 3090500 | Avg Loss: 0.0157 | Grad Norm: 0.01429516\n",
      "Epoch 4 | Step 3090600 | Avg Loss: 0.0158 | Grad Norm: 0.01030280\n",
      "Epoch 4 | Step 3090700 | Avg Loss: 0.0154 | Grad Norm: 0.00913742\n",
      "Epoch 4 | Step 3090800 | Avg Loss: 0.0156 | Grad Norm: 0.01021239\n",
      "Epoch 4 | Step 3090900 | Avg Loss: 0.0158 | Grad Norm: 0.00909085\n",
      "Epoch 4 | Step 3091000 | Avg Loss: 0.0158 | Grad Norm: 0.00872581\n",
      "Epoch 4 | Step 3091100 | Avg Loss: 0.0158 | Grad Norm: 0.01196801\n",
      "Epoch 4 | Step 3091200 | Avg Loss: 0.0157 | Grad Norm: 0.00780244\n",
      "Epoch 4 | Step 3091300 | Avg Loss: 0.0158 | Grad Norm: 0.00916226\n",
      "Epoch 4 | Step 3091400 | Avg Loss: 0.0160 | Grad Norm: 0.00849248\n",
      "Epoch 4 | Step 3091500 | Avg Loss: 0.0156 | Grad Norm: 0.00918033\n",
      "Epoch 4 | Step 3091600 | Avg Loss: 0.0157 | Grad Norm: 0.00952381\n",
      "Epoch 4 | Step 3091700 | Avg Loss: 0.0157 | Grad Norm: 0.00969756\n",
      "Epoch 4 | Step 3091800 | Avg Loss: 0.0161 | Grad Norm: 0.01060027\n",
      "Epoch 4 | Step 3091900 | Avg Loss: 0.0161 | Grad Norm: 0.01041617\n",
      "Epoch 4 | Step 3092000 | Avg Loss: 0.0159 | Grad Norm: 0.00851317\n",
      "Epoch 4 | Step 3092100 | Avg Loss: 0.0157 | Grad Norm: 0.01023282\n",
      "Epoch 4 | Step 3092200 | Avg Loss: 0.0155 | Grad Norm: 0.00865675\n",
      "Epoch 4 | Step 3092300 | Avg Loss: 0.0154 | Grad Norm: 0.01004079\n",
      "Epoch 4 | Step 3092400 | Avg Loss: 0.0156 | Grad Norm: 0.01080197\n",
      "Epoch 4 | Step 3092500 | Avg Loss: 0.0155 | Grad Norm: 0.00999329\n",
      "Epoch 4 | Step 3092600 | Avg Loss: 0.0159 | Grad Norm: 0.00878398\n",
      "Epoch 4 | Step 3092700 | Avg Loss: 0.0154 | Grad Norm: 0.00879352\n",
      "Epoch 4 | Step 3092800 | Avg Loss: 0.0154 | Grad Norm: 0.00886454\n",
      "Epoch 4 | Step 3092900 | Avg Loss: 0.0155 | Grad Norm: 0.01057279\n",
      "Epoch 4 | Step 3093000 | Avg Loss: 0.0155 | Grad Norm: 0.00829868\n",
      "Epoch 4 | Step 3093100 | Avg Loss: 0.0157 | Grad Norm: 0.01023391\n",
      "Epoch 4 | Step 3093200 | Avg Loss: 0.0157 | Grad Norm: 0.00921528\n",
      "Epoch 4 | Step 3093300 | Avg Loss: 0.0162 | Grad Norm: 0.00985755\n",
      "Epoch 4 | Step 3093400 | Avg Loss: 0.0159 | Grad Norm: 0.00955068\n",
      "Epoch 4 | Step 3093500 | Avg Loss: 0.0159 | Grad Norm: 0.00971096\n",
      "Epoch 4 | Step 3093600 | Avg Loss: 0.0158 | Grad Norm: 0.01019620\n",
      "Epoch 4 | Step 3093700 | Avg Loss: 0.0154 | Grad Norm: 0.00958434\n",
      "Epoch 4 | Step 3093800 | Avg Loss: 0.0154 | Grad Norm: 0.00985700\n",
      "Epoch 4 | Step 3093900 | Avg Loss: 0.0155 | Grad Norm: 0.00914775\n",
      "Epoch 4 | Step 3094000 | Avg Loss: 0.0154 | Grad Norm: 0.00946363\n",
      "Epoch 4 | Step 3094100 | Avg Loss: 0.0153 | Grad Norm: 0.00868622\n",
      "Epoch 4 | Step 3094200 | Avg Loss: 0.0157 | Grad Norm: 0.00863930\n",
      "Epoch 4 | Step 3094300 | Avg Loss: 0.0159 | Grad Norm: 0.01047390\n",
      "Epoch 4 | Step 3094400 | Avg Loss: 0.0162 | Grad Norm: 0.00897871\n",
      "Epoch 4 | Step 3094500 | Avg Loss: 0.0158 | Grad Norm: 0.00911097\n",
      "Epoch 4 | Step 3094600 | Avg Loss: 0.0156 | Grad Norm: 0.00902747\n",
      "Epoch 4 | Step 3094700 | Avg Loss: 0.0158 | Grad Norm: 0.01198951\n",
      "Epoch 4 | Step 3094800 | Avg Loss: 0.0158 | Grad Norm: 0.00971803\n",
      "Epoch 4 | Step 3094900 | Avg Loss: 0.0152 | Grad Norm: 0.00983296\n",
      "Epoch 4 | Step 3095000 | Avg Loss: 0.0156 | Grad Norm: 0.01239288\n",
      "Epoch 4 | Step 3095100 | Avg Loss: 0.0155 | Grad Norm: 0.01001281\n",
      "Epoch 4 | Step 3095200 | Avg Loss: 0.0154 | Grad Norm: 0.00870526\n",
      "Epoch 4 | Step 3095300 | Avg Loss: 0.0156 | Grad Norm: 0.00818961\n",
      "Epoch 4 | Step 3095400 | Avg Loss: 0.0157 | Grad Norm: 0.00824467\n",
      "Epoch 4 | Step 3095500 | Avg Loss: 0.0158 | Grad Norm: 0.01017077\n",
      "Epoch 4 | Step 3095600 | Avg Loss: 0.0161 | Grad Norm: 0.00871527\n",
      "Epoch 4 | Step 3095700 | Avg Loss: 0.0166 | Grad Norm: 0.01140662\n",
      "Epoch 4 | Step 3095800 | Avg Loss: 0.0166 | Grad Norm: 0.01178727\n",
      "Epoch 4 | Step 3095900 | Avg Loss: 0.0165 | Grad Norm: 0.00931005\n",
      "Epoch 4 | Step 3096000 | Avg Loss: 0.0166 | Grad Norm: 0.01006768\n",
      "Epoch 4 | Step 3096100 | Avg Loss: 0.0161 | Grad Norm: 0.01034492\n",
      "Epoch 4 | Step 3096200 | Avg Loss: 0.0160 | Grad Norm: 0.00922960\n",
      "Epoch 4 | Step 3096300 | Avg Loss: 0.0164 | Grad Norm: 0.00864278\n",
      "Epoch 4 | Step 3096400 | Avg Loss: 0.0164 | Grad Norm: 0.00980970\n",
      "Epoch 4 | Step 3096500 | Avg Loss: 0.0160 | Grad Norm: 0.01080675\n",
      "Epoch 4 | Step 3096600 | Avg Loss: 0.0157 | Grad Norm: 0.01048451\n",
      "Epoch 4 | Step 3096700 | Avg Loss: 0.0155 | Grad Norm: 0.00880413\n",
      "Epoch 4 | Step 3096800 | Avg Loss: 0.0156 | Grad Norm: 0.00777977\n",
      "Epoch 4 | Step 3096900 | Avg Loss: 0.0157 | Grad Norm: 0.00859742\n",
      "Epoch 4 | Step 3097000 | Avg Loss: 0.0159 | Grad Norm: 0.00980215\n",
      "Epoch 4 | Step 3097100 | Avg Loss: 0.0156 | Grad Norm: 0.00832792\n",
      "Epoch 4 | Step 3097200 | Avg Loss: 0.0160 | Grad Norm: 0.00846414\n",
      "Epoch 4 | Step 3097300 | Avg Loss: 0.0159 | Grad Norm: 0.00942625\n",
      "Epoch 4 | Step 3097400 | Avg Loss: 0.0155 | Grad Norm: 0.00837283\n",
      "Epoch 4 | Step 3097500 | Avg Loss: 0.0155 | Grad Norm: 0.00974728\n",
      "Epoch 4 | Step 3097600 | Avg Loss: 0.0151 | Grad Norm: 0.00972670\n",
      "Epoch 4 | Step 3097700 | Avg Loss: 0.0156 | Grad Norm: 0.00850134\n",
      "Epoch 4 | Step 3097800 | Avg Loss: 0.0155 | Grad Norm: 0.00904814\n",
      "Epoch 4 | Step 3097900 | Avg Loss: 0.0156 | Grad Norm: 0.00987396\n",
      "Epoch 4 | Step 3098000 | Avg Loss: 0.0160 | Grad Norm: 0.00883390\n",
      "Epoch 4 | Step 3098100 | Avg Loss: 0.0156 | Grad Norm: 0.01024122\n",
      "Epoch 4 | Step 3098200 | Avg Loss: 0.0158 | Grad Norm: 0.00990701\n",
      "Epoch 4 | Step 3098300 | Avg Loss: 0.0160 | Grad Norm: 0.01003151\n",
      "Epoch 4 | Step 3098400 | Avg Loss: 0.0160 | Grad Norm: 0.00913108\n",
      "Epoch 4 | Step 3098500 | Avg Loss: 0.0158 | Grad Norm: 0.00858872\n",
      "Epoch 4 | Step 3098600 | Avg Loss: 0.0159 | Grad Norm: 0.00879401\n",
      "Epoch 4 | Step 3098700 | Avg Loss: 0.0159 | Grad Norm: 0.01041063\n",
      "Epoch 4 | Step 3098800 | Avg Loss: 0.0159 | Grad Norm: 0.00976551\n",
      "Epoch 4 | Step 3098900 | Avg Loss: 0.0159 | Grad Norm: 0.01012918\n",
      "Epoch 4 | Step 3099000 | Avg Loss: 0.0156 | Grad Norm: 0.00833855\n",
      "Epoch 4 | Step 3099100 | Avg Loss: 0.0157 | Grad Norm: 0.00975422\n",
      "Epoch 4 | Step 3099200 | Avg Loss: 0.0155 | Grad Norm: 0.01086556\n",
      "Epoch 4 | Step 3099300 | Avg Loss: 0.0151 | Grad Norm: 0.00930656\n",
      "Epoch 4 | Step 3099400 | Avg Loss: 0.0151 | Grad Norm: 0.00877086\n",
      "Epoch 4 | Step 3099500 | Avg Loss: 0.0159 | Grad Norm: 0.00996712\n",
      "Epoch 4 | Step 3099600 | Avg Loss: 0.0160 | Grad Norm: 0.00865205\n",
      "Epoch 4 | Step 3099700 | Avg Loss: 0.0156 | Grad Norm: 0.00865623\n",
      "Epoch 4 | Step 3099800 | Avg Loss: 0.0155 | Grad Norm: 0.00772149\n",
      "Epoch 4 | Step 3099900 | Avg Loss: 0.0158 | Grad Norm: 0.00964195\n",
      "Epoch 4 | Step 3100000 | Avg Loss: 0.0161 | Grad Norm: 0.01021145\n",
      "Saving model at step3100000\n",
      "Epoch 4 | Step 3100100 | Avg Loss: 0.0158 | Grad Norm: 0.00864381\n",
      "Epoch 4 | Step 3100200 | Avg Loss: 0.0158 | Grad Norm: 0.00992824\n",
      "Epoch 4 | Step 3100300 | Avg Loss: 0.0157 | Grad Norm: 0.00987458\n",
      "Epoch 4 | Step 3100400 | Avg Loss: 0.0159 | Grad Norm: 0.01044003\n",
      "Epoch 4 | Step 3100500 | Avg Loss: 0.0162 | Grad Norm: 0.00959460\n",
      "Epoch 4 | Step 3100600 | Avg Loss: 0.0157 | Grad Norm: 0.00912495\n",
      "Epoch 4 | Step 3100700 | Avg Loss: 0.0158 | Grad Norm: 0.00930621\n",
      "Epoch 4 | Step 3100800 | Avg Loss: 0.0159 | Grad Norm: 0.01134344\n",
      "Epoch 4 | Step 3100900 | Avg Loss: 0.0158 | Grad Norm: 0.00938418\n",
      "Epoch 4 | Step 3101000 | Avg Loss: 0.0156 | Grad Norm: 0.00852143\n",
      "Epoch 4 | Step 3101100 | Avg Loss: 0.0153 | Grad Norm: 0.00835603\n",
      "Epoch 4 | Step 3101200 | Avg Loss: 0.0151 | Grad Norm: 0.00836553\n",
      "Epoch 4 | Step 3101300 | Avg Loss: 0.0155 | Grad Norm: 0.01007703\n",
      "Epoch 4 | Step 3101400 | Avg Loss: 0.0156 | Grad Norm: 0.00946167\n",
      "Epoch 4 | Step 3101500 | Avg Loss: 0.0153 | Grad Norm: 0.00866571\n",
      "Epoch 4 | Step 3101600 | Avg Loss: 0.0154 | Grad Norm: 0.00824524\n",
      "Epoch 4 | Step 3101700 | Avg Loss: 0.0150 | Grad Norm: 0.00848162\n",
      "Epoch 4 | Step 3101800 | Avg Loss: 0.0152 | Grad Norm: 0.00851317\n",
      "Epoch 4 | Step 3101900 | Avg Loss: 0.0151 | Grad Norm: 0.00927901\n",
      "Epoch 4 | Step 3102000 | Avg Loss: 0.0150 | Grad Norm: 0.00874883\n",
      "Epoch 4 | Step 3102100 | Avg Loss: 0.0153 | Grad Norm: 0.00911330\n",
      "Epoch 4 | Step 3102200 | Avg Loss: 0.0156 | Grad Norm: 0.00865483\n",
      "Epoch 4 | Step 3102300 | Avg Loss: 0.0155 | Grad Norm: 0.01024377\n",
      "Epoch 4 | Step 3102400 | Avg Loss: 0.0154 | Grad Norm: 0.01050654\n",
      "Epoch 4 | Step 3102500 | Avg Loss: 0.0158 | Grad Norm: 0.00952801\n",
      "Epoch 4 | Step 3102600 | Avg Loss: 0.0160 | Grad Norm: 0.00910807\n",
      "Epoch 4 | Step 3102700 | Avg Loss: 0.0157 | Grad Norm: 0.00966744\n",
      "Epoch 4 | Step 3102800 | Avg Loss: 0.0156 | Grad Norm: 0.00843744\n",
      "Epoch 4 | Step 3102900 | Avg Loss: 0.0158 | Grad Norm: 0.00915756\n",
      "Epoch 4 | Step 3103000 | Avg Loss: 0.0156 | Grad Norm: 0.00987209\n",
      "Epoch 4 | Step 3103100 | Avg Loss: 0.0155 | Grad Norm: 0.00893265\n",
      "Epoch 4 | Step 3103200 | Avg Loss: 0.0157 | Grad Norm: 0.00856834\n",
      "Epoch 4 | Step 3103300 | Avg Loss: 0.0158 | Grad Norm: 0.00838281\n",
      "Epoch 4 | Step 3103400 | Avg Loss: 0.0155 | Grad Norm: 0.01029093\n",
      "Epoch 4 | Step 3103500 | Avg Loss: 0.0153 | Grad Norm: 0.00962560\n",
      "Epoch 4 | Step 3103600 | Avg Loss: 0.0152 | Grad Norm: 0.00913014\n",
      "Epoch 4 | Step 3103700 | Avg Loss: 0.0153 | Grad Norm: 0.00808428\n",
      "Epoch 4 | Step 3103800 | Avg Loss: 0.0153 | Grad Norm: 0.00907913\n",
      "Epoch 4 | Step 3103900 | Avg Loss: 0.0153 | Grad Norm: 0.00898341\n",
      "Epoch 4 | Step 3104000 | Avg Loss: 0.0150 | Grad Norm: 0.00768206\n",
      "Epoch 4 | Step 3104100 | Avg Loss: 0.0153 | Grad Norm: 0.00840045\n",
      "Epoch 4 | Step 3104200 | Avg Loss: 0.0157 | Grad Norm: 0.00980762\n",
      "Epoch 4 | Step 3104300 | Avg Loss: 0.0157 | Grad Norm: 0.00879163\n",
      "Epoch 4 | Step 3104400 | Avg Loss: 0.0158 | Grad Norm: 0.00894910\n",
      "Epoch 4 | Step 3104500 | Avg Loss: 0.0158 | Grad Norm: 0.00904277\n",
      "Epoch 4 | Step 3104600 | Avg Loss: 0.0156 | Grad Norm: 0.01007374\n",
      "Epoch 4 | Step 3104700 | Avg Loss: 0.0151 | Grad Norm: 0.00865249\n",
      "Epoch 4 | Step 3104800 | Avg Loss: 0.0154 | Grad Norm: 0.00989176\n",
      "Epoch 4 | Step 3104900 | Avg Loss: 0.0153 | Grad Norm: 0.00887596\n",
      "Epoch 4 | Step 3105000 | Avg Loss: 0.0156 | Grad Norm: 0.00871936\n",
      "Epoch 4 | Step 3105100 | Avg Loss: 0.0156 | Grad Norm: 0.00969780\n",
      "Epoch 4 | Step 3105200 | Avg Loss: 0.0161 | Grad Norm: 0.00941117\n",
      "Epoch 4 | Step 3105300 | Avg Loss: 0.0159 | Grad Norm: 0.00991062\n",
      "Epoch 4 | Step 3105400 | Avg Loss: 0.0158 | Grad Norm: 0.00978311\n",
      "Epoch 4 | Step 3105500 | Avg Loss: 0.0158 | Grad Norm: 0.00880881\n",
      "Epoch 4 | Step 3105600 | Avg Loss: 0.0157 | Grad Norm: 0.00871050\n",
      "Epoch 4 | Step 3105700 | Avg Loss: 0.0159 | Grad Norm: 0.00830947\n",
      "Epoch 4 | Step 3105800 | Avg Loss: 0.0159 | Grad Norm: 0.00974095\n",
      "Epoch 4 | Step 3105900 | Avg Loss: 0.0157 | Grad Norm: 0.00913100\n",
      "Epoch 4 | Step 3106000 | Avg Loss: 0.0156 | Grad Norm: 0.00968747\n",
      "Epoch 4 | Step 3106100 | Avg Loss: 0.0156 | Grad Norm: 0.01007755\n",
      "Epoch 4 | Step 3106200 | Avg Loss: 0.0159 | Grad Norm: 0.00873722\n",
      "Epoch 4 | Step 3106300 | Avg Loss: 0.0158 | Grad Norm: 0.00847956\n",
      "Epoch 4 | Step 3106400 | Avg Loss: 0.0158 | Grad Norm: 0.00989019\n",
      "Epoch 4 | Step 3106500 | Avg Loss: 0.0161 | Grad Norm: 0.00834779\n",
      "Epoch 4 | Step 3106600 | Avg Loss: 0.0161 | Grad Norm: 0.01117910\n",
      "Epoch 4 | Step 3106700 | Avg Loss: 0.0161 | Grad Norm: 0.00922474\n",
      "Epoch 4 | Step 3106800 | Avg Loss: 0.0159 | Grad Norm: 0.00881891\n",
      "Epoch 4 | Step 3106900 | Avg Loss: 0.0159 | Grad Norm: 0.00880638\n",
      "Epoch 4 | Step 3107000 | Avg Loss: 0.0164 | Grad Norm: 0.00960361\n",
      "Epoch 4 | Step 3107100 | Avg Loss: 0.0160 | Grad Norm: 0.00815222\n",
      "Epoch 4 | Step 3107200 | Avg Loss: 0.0161 | Grad Norm: 0.00880862\n",
      "Epoch 4 | Step 3107300 | Avg Loss: 0.0160 | Grad Norm: 0.00968015\n",
      "Epoch 4 | Step 3107400 | Avg Loss: 0.0156 | Grad Norm: 0.00755109\n",
      "Epoch 4 | Step 3107500 | Avg Loss: 0.0156 | Grad Norm: 0.00880736\n",
      "Epoch 4 | Step 3107600 | Avg Loss: 0.0158 | Grad Norm: 0.00892638\n",
      "Epoch 4 | Step 3107700 | Avg Loss: 0.0154 | Grad Norm: 0.00924691\n",
      "Epoch 4 | Step 3107800 | Avg Loss: 0.0150 | Grad Norm: 0.00800335\n",
      "Epoch 4 | Step 3107900 | Avg Loss: 0.0150 | Grad Norm: 0.01054476\n",
      "Epoch 4 | Step 3108000 | Avg Loss: 0.0150 | Grad Norm: 0.00849381\n",
      "Epoch 4 | Step 3108100 | Avg Loss: 0.0149 | Grad Norm: 0.00833731\n",
      "Epoch 4 | Step 3108200 | Avg Loss: 0.0150 | Grad Norm: 0.00849705\n",
      "Epoch 4 | Step 3108300 | Avg Loss: 0.0153 | Grad Norm: 0.00858453\n",
      "Epoch 4 | Step 3108400 | Avg Loss: 0.0151 | Grad Norm: 0.00774503\n",
      "Epoch 4 | Step 3108500 | Avg Loss: 0.0154 | Grad Norm: 0.00939173\n",
      "Epoch 4 | Step 3108600 | Avg Loss: 0.0152 | Grad Norm: 0.00788307\n",
      "Epoch 4 | Step 3108700 | Avg Loss: 0.0152 | Grad Norm: 0.00815612\n",
      "Epoch 4 | Step 3108800 | Avg Loss: 0.0153 | Grad Norm: 0.00737519\n",
      "Epoch 4 | Step 3108900 | Avg Loss: 0.0154 | Grad Norm: 0.00769953\n",
      "Epoch 4 | Step 3109000 | Avg Loss: 0.0153 | Grad Norm: 0.00877469\n",
      "Epoch 4 | Step 3109100 | Avg Loss: 0.0155 | Grad Norm: 0.01043215\n",
      "Epoch 4 | Step 3109200 | Avg Loss: 0.0156 | Grad Norm: 0.00852025\n",
      "Epoch 4 | Step 3109300 | Avg Loss: 0.0156 | Grad Norm: 0.01045553\n",
      "Epoch 4 | Step 3109400 | Avg Loss: 0.0153 | Grad Norm: 0.00768014\n",
      "Epoch 4 | Step 3109500 | Avg Loss: 0.0151 | Grad Norm: 0.00870110\n",
      "Epoch 4 | Step 3109600 | Avg Loss: 0.0151 | Grad Norm: 0.01091373\n",
      "Epoch 4 | Step 3109700 | Avg Loss: 0.0148 | Grad Norm: 0.00966741\n",
      "Epoch 4 | Step 3109800 | Avg Loss: 0.0150 | Grad Norm: 0.01121616\n",
      "Epoch 4 | Step 3109900 | Avg Loss: 0.0155 | Grad Norm: 0.00893296\n",
      "Epoch 4 | Step 3110000 | Avg Loss: 0.0160 | Grad Norm: 0.00936396\n",
      "Epoch 4 | Step 3110100 | Avg Loss: 0.0158 | Grad Norm: 0.01018795\n",
      "Epoch 4 | Step 3110200 | Avg Loss: 0.0156 | Grad Norm: 0.00999263\n",
      "Epoch 4 | Step 3110300 | Avg Loss: 0.0157 | Grad Norm: 0.00839421\n",
      "Epoch 4 | Step 3110400 | Avg Loss: 0.0157 | Grad Norm: 0.00800564\n",
      "Epoch 4 | Step 3110500 | Avg Loss: 0.0155 | Grad Norm: 0.00802139\n",
      "Epoch 4 | Step 3110600 | Avg Loss: 0.0154 | Grad Norm: 0.00843070\n",
      "Epoch 4 | Step 3110700 | Avg Loss: 0.0155 | Grad Norm: 0.00900068\n",
      "Epoch 4 | Step 3110800 | Avg Loss: 0.0154 | Grad Norm: 0.01398694\n",
      "Epoch 4 | Step 3110900 | Avg Loss: 0.0154 | Grad Norm: 0.00910655\n",
      "Epoch 4 | Step 3111000 | Avg Loss: 0.0156 | Grad Norm: 0.00797815\n",
      "Epoch 4 | Step 3111100 | Avg Loss: 0.0154 | Grad Norm: 0.01169510\n",
      "Epoch 4 | Step 3111200 | Avg Loss: 0.0154 | Grad Norm: 0.00785779\n",
      "Epoch 4 | Step 3111300 | Avg Loss: 0.0155 | Grad Norm: 0.01783260\n",
      "Epoch 4 | Step 3111400 | Avg Loss: 0.0158 | Grad Norm: 0.00938816\n",
      "Epoch 4 | Step 3111500 | Avg Loss: 0.0155 | Grad Norm: 0.00894477\n",
      "Epoch 4 | Step 3111600 | Avg Loss: 0.0152 | Grad Norm: 0.00850532\n",
      "Epoch 4 | Step 3111700 | Avg Loss: 0.0151 | Grad Norm: 0.00839010\n",
      "Epoch 4 | Step 3111800 | Avg Loss: 0.0151 | Grad Norm: 0.01103084\n",
      "Epoch 4 | Step 3111900 | Avg Loss: 0.0151 | Grad Norm: 0.00909396\n",
      "Epoch 4 | Step 3112000 | Avg Loss: 0.0155 | Grad Norm: 0.00906061\n",
      "Epoch 4 | Step 3112100 | Avg Loss: 0.0157 | Grad Norm: 0.00804806\n",
      "Epoch 4 | Step 3112200 | Avg Loss: 0.0158 | Grad Norm: 0.00870348\n",
      "Epoch 4 | Step 3112300 | Avg Loss: 0.0159 | Grad Norm: 0.00958004\n",
      "Epoch 4 | Step 3112400 | Avg Loss: 0.0157 | Grad Norm: 0.01033811\n",
      "Epoch 4 | Step 3112500 | Avg Loss: 0.0160 | Grad Norm: 0.00900831\n",
      "Epoch 4 | Step 3112600 | Avg Loss: 0.0160 | Grad Norm: 0.00947999\n",
      "Epoch 4 | Step 3112700 | Avg Loss: 0.0157 | Grad Norm: 0.01123547\n",
      "Epoch 4 | Step 3112800 | Avg Loss: 0.0158 | Grad Norm: 0.00919826\n",
      "Epoch 4 | Step 3112900 | Avg Loss: 0.0157 | Grad Norm: 0.00975589\n",
      "Epoch 4 | Step 3113000 | Avg Loss: 0.0153 | Grad Norm: 0.00841456\n",
      "Epoch 4 | Step 3113100 | Avg Loss: 0.0153 | Grad Norm: 0.00870718\n",
      "Epoch 4 | Step 3113200 | Avg Loss: 0.0154 | Grad Norm: 0.00844845\n",
      "Epoch 4 | Step 3113300 | Avg Loss: 0.0152 | Grad Norm: 0.00856280\n",
      "Epoch 4 | Step 3113400 | Avg Loss: 0.0154 | Grad Norm: 0.00792563\n",
      "Epoch 4 | Step 3113500 | Avg Loss: 0.0156 | Grad Norm: 0.00898114\n",
      "Epoch 4 | Step 3113600 | Avg Loss: 0.0153 | Grad Norm: 0.01041571\n",
      "Epoch 4 | Step 3113700 | Avg Loss: 0.0158 | Grad Norm: 0.00848130\n",
      "Epoch 4 | Step 3113800 | Avg Loss: 0.0157 | Grad Norm: 0.01018086\n",
      "Epoch 4 | Step 3113900 | Avg Loss: 0.0155 | Grad Norm: 0.00862080\n",
      "Epoch 4 | Step 3114000 | Avg Loss: 0.0156 | Grad Norm: 0.00986354\n",
      "Epoch 4 | Step 3114100 | Avg Loss: 0.0158 | Grad Norm: 0.00959455\n",
      "Epoch 4 | Step 3114200 | Avg Loss: 0.0158 | Grad Norm: 0.00945324\n",
      "Epoch 4 | Step 3114300 | Avg Loss: 0.0157 | Grad Norm: 0.00817089\n",
      "Epoch 4 | Step 3114400 | Avg Loss: 0.0158 | Grad Norm: 0.00953912\n",
      "Epoch 4 | Step 3114500 | Avg Loss: 0.0158 | Grad Norm: 0.00950783\n",
      "Epoch 4 | Step 3114600 | Avg Loss: 0.0155 | Grad Norm: 0.01044262\n",
      "Epoch 4 | Step 3114700 | Avg Loss: 0.0159 | Grad Norm: 0.00901040\n",
      "Epoch 4 | Step 3114800 | Avg Loss: 0.0153 | Grad Norm: 0.00867103\n",
      "Epoch 4 | Step 3114900 | Avg Loss: 0.0153 | Grad Norm: 0.00985295\n",
      "Epoch 4 | Step 3115000 | Avg Loss: 0.0152 | Grad Norm: 0.01126380\n",
      "Epoch 4 | Step 3115100 | Avg Loss: 0.0157 | Grad Norm: 0.00930826\n",
      "Epoch 4 | Step 3115200 | Avg Loss: 0.0155 | Grad Norm: 0.00908663\n",
      "Epoch 4 | Step 3115300 | Avg Loss: 0.0153 | Grad Norm: 0.00875381\n",
      "Epoch 4 | Step 3115400 | Avg Loss: 0.0150 | Grad Norm: 0.00959530\n",
      "Epoch 4 | Step 3115500 | Avg Loss: 0.0151 | Grad Norm: 0.00980165\n",
      "Epoch 4 | Step 3115600 | Avg Loss: 0.0149 | Grad Norm: 0.00841770\n",
      "Epoch 4 | Step 3115700 | Avg Loss: 0.0149 | Grad Norm: 0.00895999\n",
      "Epoch 4 | Step 3115800 | Avg Loss: 0.0147 | Grad Norm: 0.00858097\n",
      "Epoch 4 | Step 3115900 | Avg Loss: 0.0153 | Grad Norm: 0.00981313\n",
      "Epoch 4 | Step 3116000 | Avg Loss: 0.0154 | Grad Norm: 0.00887078\n",
      "Epoch 4 | Step 3116100 | Avg Loss: 0.0154 | Grad Norm: 0.00759594\n",
      "Epoch 4 | Step 3116200 | Avg Loss: 0.0155 | Grad Norm: 0.00958193\n",
      "Epoch 4 | Step 3116300 | Avg Loss: 0.0156 | Grad Norm: 0.00834720\n",
      "Epoch 4 | Step 3116400 | Avg Loss: 0.0155 | Grad Norm: 0.00859330\n",
      "Epoch 4 | Step 3116500 | Avg Loss: 0.0155 | Grad Norm: 0.00980620\n",
      "Epoch 4 | Step 3116600 | Avg Loss: 0.0154 | Grad Norm: 0.00854898\n",
      "Epoch 4 | Step 3116700 | Avg Loss: 0.0157 | Grad Norm: 0.00834407\n",
      "Epoch 4 | Step 3116800 | Avg Loss: 0.0157 | Grad Norm: 0.00854207\n",
      "Epoch 4 | Step 3116900 | Avg Loss: 0.0156 | Grad Norm: 0.00906200\n",
      "Epoch 4 | Step 3117000 | Avg Loss: 0.0155 | Grad Norm: 0.00837480\n",
      "Epoch 4 | Step 3117100 | Avg Loss: 0.0157 | Grad Norm: 0.00907995\n",
      "Epoch 4 | Step 3117200 | Avg Loss: 0.0160 | Grad Norm: 0.00828683\n",
      "Epoch 4 | Step 3117300 | Avg Loss: 0.0160 | Grad Norm: 0.00981279\n",
      "Epoch 4 | Step 3117400 | Avg Loss: 0.0160 | Grad Norm: 0.01024381\n",
      "Epoch 4 | Step 3117500 | Avg Loss: 0.0163 | Grad Norm: 0.01013861\n",
      "Epoch 4 | Step 3117600 | Avg Loss: 0.0157 | Grad Norm: 0.00942412\n",
      "Epoch 4 | Step 3117700 | Avg Loss: 0.0153 | Grad Norm: 0.00855315\n",
      "Epoch 4 | Step 3117800 | Avg Loss: 0.0157 | Grad Norm: 0.00823920\n",
      "Epoch 4 | Step 3117900 | Avg Loss: 0.0156 | Grad Norm: 0.01050469\n",
      "Epoch 4 | Step 3118000 | Avg Loss: 0.0157 | Grad Norm: 0.00724378\n",
      "Epoch 4 | Step 3118100 | Avg Loss: 0.0157 | Grad Norm: 0.00925413\n",
      "Epoch 4 | Step 3118200 | Avg Loss: 0.0160 | Grad Norm: 0.01039388\n",
      "Epoch 4 | Step 3118300 | Avg Loss: 0.0156 | Grad Norm: 0.00937081\n",
      "Epoch 4 | Step 3118400 | Avg Loss: 0.0154 | Grad Norm: 0.00843728\n",
      "Epoch 4 | Step 3118500 | Avg Loss: 0.0154 | Grad Norm: 0.00934765\n",
      "Epoch 4 | Step 3118600 | Avg Loss: 0.0157 | Grad Norm: 0.00872979\n",
      "Epoch 4 | Step 3118700 | Avg Loss: 0.0159 | Grad Norm: 0.01030247\n",
      "Epoch 4 | Step 3118800 | Avg Loss: 0.0157 | Grad Norm: 0.00859530\n",
      "Epoch 4 | Step 3118900 | Avg Loss: 0.0159 | Grad Norm: 0.00965226\n",
      "Epoch 4 | Step 3119000 | Avg Loss: 0.0159 | Grad Norm: 0.00925858\n",
      "Epoch 4 | Step 3119100 | Avg Loss: 0.0157 | Grad Norm: 0.00974748\n",
      "Epoch 4 | Step 3119200 | Avg Loss: 0.0158 | Grad Norm: 0.01021098\n",
      "Epoch 4 | Step 3119300 | Avg Loss: 0.0159 | Grad Norm: 0.00901962\n",
      "Epoch 4 | Step 3119400 | Avg Loss: 0.0160 | Grad Norm: 0.00862782\n",
      "Epoch 4 | Step 3119500 | Avg Loss: 0.0159 | Grad Norm: 0.01060629\n",
      "Epoch 4 | Step 3119600 | Avg Loss: 0.0159 | Grad Norm: 0.00817714\n",
      "Epoch 4 | Step 3119700 | Avg Loss: 0.0156 | Grad Norm: 0.01021456\n",
      "Epoch 4 | Step 3119800 | Avg Loss: 0.0159 | Grad Norm: 0.01003766\n",
      "Epoch 4 | Step 3119900 | Avg Loss: 0.0157 | Grad Norm: 0.00898929\n",
      "Epoch 4 | Step 3120000 | Avg Loss: 0.0154 | Grad Norm: 0.00997952\n",
      "Epoch 4 | Step 3120100 | Avg Loss: 0.0154 | Grad Norm: 0.01003008\n",
      "Epoch 4 | Step 3120200 | Avg Loss: 0.0158 | Grad Norm: 0.00859501\n",
      "Epoch 4 | Step 3120300 | Avg Loss: 0.0159 | Grad Norm: 0.00933906\n",
      "Epoch 4 | Step 3120400 | Avg Loss: 0.0155 | Grad Norm: 0.00924905\n",
      "Epoch 4 | Step 3120500 | Avg Loss: 0.0155 | Grad Norm: 0.01070400\n",
      "Epoch 4 | Step 3120600 | Avg Loss: 0.0158 | Grad Norm: 0.00993653\n",
      "Epoch 4 | Step 3120700 | Avg Loss: 0.0154 | Grad Norm: 0.00846571\n",
      "Epoch 4 | Step 3120800 | Avg Loss: 0.0155 | Grad Norm: 0.00967100\n",
      "Epoch 4 | Step 3120900 | Avg Loss: 0.0156 | Grad Norm: 0.00860529\n",
      "Epoch 4 | Step 3121000 | Avg Loss: 0.0155 | Grad Norm: 0.00964541\n",
      "Epoch 4 | Step 3121100 | Avg Loss: 0.0155 | Grad Norm: 0.00981988\n",
      "Epoch 4 | Step 3121200 | Avg Loss: 0.0154 | Grad Norm: 0.00953046\n",
      "Epoch 4 | Step 3121300 | Avg Loss: 0.0160 | Grad Norm: 0.00910345\n",
      "Epoch 4 | Step 3121400 | Avg Loss: 0.0155 | Grad Norm: 0.00859739\n",
      "Epoch 4 | Step 3121500 | Avg Loss: 0.0155 | Grad Norm: 0.00880312\n",
      "Epoch 4 | Step 3121600 | Avg Loss: 0.0153 | Grad Norm: 0.00908342\n",
      "Epoch 4 | Step 3121700 | Avg Loss: 0.0151 | Grad Norm: 0.00915290\n",
      "Epoch 4 | Step 3121800 | Avg Loss: 0.0151 | Grad Norm: 0.00894387\n",
      "Epoch 4 | Step 3121900 | Avg Loss: 0.0148 | Grad Norm: 0.00814861\n",
      "Epoch 4 | Step 3122000 | Avg Loss: 0.0154 | Grad Norm: 0.00755798\n",
      "Epoch 4 | Step 3122100 | Avg Loss: 0.0152 | Grad Norm: 0.00863938\n",
      "Epoch 4 | Step 3122200 | Avg Loss: 0.0151 | Grad Norm: 0.01118411\n",
      "Epoch 4 | Step 3122300 | Avg Loss: 0.0155 | Grad Norm: 0.00794885\n",
      "Epoch 4 | Step 3122400 | Avg Loss: 0.0152 | Grad Norm: 0.00892009\n",
      "Epoch 4 | Step 3122500 | Avg Loss: 0.0151 | Grad Norm: 0.00802084\n",
      "Epoch 4 | Step 3122600 | Avg Loss: 0.0153 | Grad Norm: 0.00874590\n",
      "Epoch 4 | Step 3122700 | Avg Loss: 0.0148 | Grad Norm: 0.00928547\n",
      "Epoch 4 | Step 3122800 | Avg Loss: 0.0153 | Grad Norm: 0.01112064\n",
      "Epoch 4 | Step 3122900 | Avg Loss: 0.0154 | Grad Norm: 0.00833950\n",
      "Epoch 4 | Step 3123000 | Avg Loss: 0.0153 | Grad Norm: 0.00821141\n",
      "Epoch 4 | Step 3123100 | Avg Loss: 0.0151 | Grad Norm: 0.00862722\n",
      "Epoch 4 | Step 3123200 | Avg Loss: 0.0155 | Grad Norm: 0.00996558\n",
      "Epoch 4 | Step 3123300 | Avg Loss: 0.0155 | Grad Norm: 0.00955400\n",
      "Epoch 4 | Step 3123400 | Avg Loss: 0.0154 | Grad Norm: 0.00975860\n",
      "Epoch 4 | Step 3123500 | Avg Loss: 0.0154 | Grad Norm: 0.01104417\n",
      "Epoch 4 | Step 3123600 | Avg Loss: 0.0153 | Grad Norm: 0.00954306\n",
      "Epoch 4 | Step 3123700 | Avg Loss: 0.0154 | Grad Norm: 0.00948227\n",
      "Epoch 4 | Step 3123800 | Avg Loss: 0.0152 | Grad Norm: 0.00919551\n",
      "Epoch 4 | Step 3123900 | Avg Loss: 0.0151 | Grad Norm: 0.00925364\n",
      "Epoch 4 | Step 3124000 | Avg Loss: 0.0151 | Grad Norm: 0.00863658\n",
      "Epoch 4 | Step 3124100 | Avg Loss: 0.0153 | Grad Norm: 0.00918537\n",
      "Epoch 4 | Step 3124200 | Avg Loss: 0.0155 | Grad Norm: 0.00858994\n",
      "Epoch 4 | Step 3124300 | Avg Loss: 0.0153 | Grad Norm: 0.00920296\n",
      "Epoch 4 | Step 3124400 | Avg Loss: 0.0154 | Grad Norm: 0.00836731\n",
      "Epoch 4 | Step 3124500 | Avg Loss: 0.0147 | Grad Norm: 0.00914727\n",
      "Epoch 4 | Step 3124600 | Avg Loss: 0.0150 | Grad Norm: 0.00992837\n",
      "Epoch 4 | Step 3124700 | Avg Loss: 0.0150 | Grad Norm: 0.00876373\n",
      "Epoch 4 | Step 3124800 | Avg Loss: 0.0148 | Grad Norm: 0.00791901\n",
      "Epoch 4 | Step 3124900 | Avg Loss: 0.0148 | Grad Norm: 0.00868608\n",
      "Epoch 4 | Step 3125000 | Avg Loss: 0.0145 | Grad Norm: 0.01165834\n",
      "Epoch 4, Loss: 0.0131\n",
      "Epoch 5 | Step 3125100 | Avg Loss: 0.0156 | Grad Norm: 0.00925361\n",
      "Epoch 5 | Step 3125200 | Avg Loss: 0.0154 | Grad Norm: 0.00837115\n",
      "Epoch 5 | Step 3125300 | Avg Loss: 0.0151 | Grad Norm: 0.00914677\n",
      "Epoch 5 | Step 3125400 | Avg Loss: 0.0151 | Grad Norm: 0.00852356\n",
      "Epoch 5 | Step 3125500 | Avg Loss: 0.0152 | Grad Norm: 0.00961067\n",
      "Epoch 5 | Step 3125600 | Avg Loss: 0.0154 | Grad Norm: 0.00883249\n",
      "Epoch 5 | Step 3125700 | Avg Loss: 0.0157 | Grad Norm: 0.01008100\n",
      "Epoch 5 | Step 3125800 | Avg Loss: 0.0161 | Grad Norm: 0.01242052\n",
      "Epoch 5 | Step 3125900 | Avg Loss: 0.0163 | Grad Norm: 0.00904088\n",
      "Epoch 5 | Step 3126000 | Avg Loss: 0.0159 | Grad Norm: 0.00841335\n",
      "Epoch 5 | Step 3126100 | Avg Loss: 0.0158 | Grad Norm: 0.00890163\n",
      "Epoch 5 | Step 3126200 | Avg Loss: 0.0155 | Grad Norm: 0.01272315\n",
      "Epoch 5 | Step 3126300 | Avg Loss: 0.0155 | Grad Norm: 0.00878084\n",
      "Epoch 5 | Step 3126400 | Avg Loss: 0.0153 | Grad Norm: 0.00922885\n",
      "Epoch 5 | Step 3126500 | Avg Loss: 0.0150 | Grad Norm: 0.00898055\n",
      "Epoch 5 | Step 3126600 | Avg Loss: 0.0149 | Grad Norm: 0.00888750\n",
      "Epoch 5 | Step 3126700 | Avg Loss: 0.0150 | Grad Norm: 0.01090256\n",
      "Epoch 5 | Step 3126800 | Avg Loss: 0.0151 | Grad Norm: 0.00834612\n",
      "Epoch 5 | Step 3126900 | Avg Loss: 0.0152 | Grad Norm: 0.00865916\n",
      "Epoch 5 | Step 3127000 | Avg Loss: 0.0154 | Grad Norm: 0.00953817\n",
      "Epoch 5 | Step 3127100 | Avg Loss: 0.0152 | Grad Norm: 0.00951019\n",
      "Epoch 5 | Step 3127200 | Avg Loss: 0.0154 | Grad Norm: 0.00844907\n",
      "Epoch 5 | Step 3127300 | Avg Loss: 0.0156 | Grad Norm: 0.00935987\n",
      "Epoch 5 | Step 3127400 | Avg Loss: 0.0155 | Grad Norm: 0.00946482\n",
      "Epoch 5 | Step 3127500 | Avg Loss: 0.0154 | Grad Norm: 0.00891987\n",
      "Epoch 5 | Step 3127600 | Avg Loss: 0.0158 | Grad Norm: 0.00817337\n",
      "Epoch 5 | Step 3127700 | Avg Loss: 0.0159 | Grad Norm: 0.00987159\n",
      "Epoch 5 | Step 3127800 | Avg Loss: 0.0159 | Grad Norm: 0.00878874\n",
      "Epoch 5 | Step 3127900 | Avg Loss: 0.0158 | Grad Norm: 0.00864349\n",
      "Epoch 5 | Step 3128000 | Avg Loss: 0.0157 | Grad Norm: 0.00818221\n",
      "Epoch 5 | Step 3128100 | Avg Loss: 0.0153 | Grad Norm: 0.00966678\n",
      "Epoch 5 | Step 3128200 | Avg Loss: 0.0154 | Grad Norm: 0.00943301\n",
      "Epoch 5 | Step 3128300 | Avg Loss: 0.0150 | Grad Norm: 0.00850142\n",
      "Epoch 5 | Step 3128400 | Avg Loss: 0.0153 | Grad Norm: 0.00978885\n",
      "Epoch 5 | Step 3128500 | Avg Loss: 0.0152 | Grad Norm: 0.00885340\n",
      "Epoch 5 | Step 3128600 | Avg Loss: 0.0151 | Grad Norm: 0.00971549\n",
      "Epoch 5 | Step 3128700 | Avg Loss: 0.0155 | Grad Norm: 0.00925665\n",
      "Epoch 5 | Step 3128800 | Avg Loss: 0.0157 | Grad Norm: 0.00860578\n",
      "Epoch 5 | Step 3128900 | Avg Loss: 0.0156 | Grad Norm: 0.01138083\n",
      "Epoch 5 | Step 3129000 | Avg Loss: 0.0155 | Grad Norm: 0.00878356\n",
      "Epoch 5 | Step 3129100 | Avg Loss: 0.0151 | Grad Norm: 0.00892215\n",
      "Epoch 5 | Step 3129200 | Avg Loss: 0.0150 | Grad Norm: 0.00848201\n",
      "Epoch 5 | Step 3129300 | Avg Loss: 0.0151 | Grad Norm: 0.00821637\n",
      "Epoch 5 | Step 3129400 | Avg Loss: 0.0153 | Grad Norm: 0.00941724\n",
      "Epoch 5 | Step 3129500 | Avg Loss: 0.0153 | Grad Norm: 0.00769264\n",
      "Epoch 5 | Step 3129600 | Avg Loss: 0.0148 | Grad Norm: 0.00843971\n",
      "Epoch 5 | Step 3129700 | Avg Loss: 0.0149 | Grad Norm: 0.00936454\n",
      "Epoch 5 | Step 3129800 | Avg Loss: 0.0151 | Grad Norm: 0.00796204\n",
      "Epoch 5 | Step 3129900 | Avg Loss: 0.0150 | Grad Norm: 0.00981159\n",
      "Epoch 5 | Step 3130000 | Avg Loss: 0.0151 | Grad Norm: 0.00908934\n",
      "Epoch 5 | Step 3130100 | Avg Loss: 0.0152 | Grad Norm: 0.00915285\n",
      "Epoch 5 | Step 3130200 | Avg Loss: 0.0152 | Grad Norm: 0.00799849\n",
      "Epoch 5 | Step 3130300 | Avg Loss: 0.0154 | Grad Norm: 0.00942680\n",
      "Epoch 5 | Step 3130400 | Avg Loss: 0.0158 | Grad Norm: 0.00876694\n",
      "Epoch 5 | Step 3130500 | Avg Loss: 0.0157 | Grad Norm: 0.00883601\n",
      "Epoch 5 | Step 3130600 | Avg Loss: 0.0155 | Grad Norm: 0.00803562\n",
      "Epoch 5 | Step 3130700 | Avg Loss: 0.0152 | Grad Norm: 0.00947875\n",
      "Epoch 5 | Step 3130800 | Avg Loss: 0.0157 | Grad Norm: 0.00872835\n",
      "Epoch 5 | Step 3130900 | Avg Loss: 0.0153 | Grad Norm: 0.00909234\n",
      "Epoch 5 | Step 3131000 | Avg Loss: 0.0155 | Grad Norm: 0.00907351\n",
      "Epoch 5 | Step 3131100 | Avg Loss: 0.0153 | Grad Norm: 0.00823394\n",
      "Epoch 5 | Step 3131200 | Avg Loss: 0.0153 | Grad Norm: 0.00886242\n",
      "Epoch 5 | Step 3131300 | Avg Loss: 0.0158 | Grad Norm: 0.00855483\n",
      "Epoch 5 | Step 3131400 | Avg Loss: 0.0156 | Grad Norm: 0.00927811\n",
      "Epoch 5 | Step 3131500 | Avg Loss: 0.0157 | Grad Norm: 0.00903479\n",
      "Epoch 5 | Step 3131600 | Avg Loss: 0.0156 | Grad Norm: 0.01009229\n",
      "Epoch 5 | Step 3131700 | Avg Loss: 0.0157 | Grad Norm: 0.00896394\n",
      "Epoch 5 | Step 3131800 | Avg Loss: 0.0155 | Grad Norm: 0.01021506\n",
      "Epoch 5 | Step 3131900 | Avg Loss: 0.0157 | Grad Norm: 0.00982610\n",
      "Epoch 5 | Step 3132000 | Avg Loss: 0.0153 | Grad Norm: 0.00968892\n",
      "Epoch 5 | Step 3132100 | Avg Loss: 0.0153 | Grad Norm: 0.00891243\n",
      "Epoch 5 | Step 3132200 | Avg Loss: 0.0151 | Grad Norm: 0.00941518\n",
      "Epoch 5 | Step 3132300 | Avg Loss: 0.0147 | Grad Norm: 0.00955580\n",
      "Epoch 5 | Step 3132400 | Avg Loss: 0.0151 | Grad Norm: 0.00857165\n",
      "Epoch 5 | Step 3132500 | Avg Loss: 0.0152 | Grad Norm: 0.01073062\n",
      "Epoch 5 | Step 3132600 | Avg Loss: 0.0154 | Grad Norm: 0.00906261\n",
      "Epoch 5 | Step 3132700 | Avg Loss: 0.0154 | Grad Norm: 0.00762151\n",
      "Epoch 5 | Step 3132800 | Avg Loss: 0.0153 | Grad Norm: 0.00894078\n",
      "Epoch 5 | Step 3132900 | Avg Loss: 0.0155 | Grad Norm: 0.00895766\n",
      "Epoch 5 | Step 3133000 | Avg Loss: 0.0157 | Grad Norm: 0.00750304\n",
      "Epoch 5 | Step 3133100 | Avg Loss: 0.0157 | Grad Norm: 0.00864072\n",
      "Epoch 5 | Step 3133200 | Avg Loss: 0.0154 | Grad Norm: 0.00961345\n",
      "Epoch 5 | Step 3133300 | Avg Loss: 0.0155 | Grad Norm: 0.00878388\n",
      "Epoch 5 | Step 3133400 | Avg Loss: 0.0156 | Grad Norm: 0.00826596\n",
      "Epoch 5 | Step 3133500 | Avg Loss: 0.0155 | Grad Norm: 0.00934026\n",
      "Epoch 5 | Step 3133600 | Avg Loss: 0.0158 | Grad Norm: 0.01017365\n",
      "Epoch 5 | Step 3133700 | Avg Loss: 0.0157 | Grad Norm: 0.00917172\n",
      "Epoch 5 | Step 3133800 | Avg Loss: 0.0157 | Grad Norm: 0.00969363\n",
      "Epoch 5 | Step 3133900 | Avg Loss: 0.0154 | Grad Norm: 0.00896853\n",
      "Epoch 5 | Step 3134000 | Avg Loss: 0.0156 | Grad Norm: 0.01241540\n",
      "Epoch 5 | Step 3134100 | Avg Loss: 0.0155 | Grad Norm: 0.00865082\n",
      "Epoch 5 | Step 3134200 | Avg Loss: 0.0154 | Grad Norm: 0.00919618\n",
      "Epoch 5 | Step 3134300 | Avg Loss: 0.0155 | Grad Norm: 0.00845430\n",
      "Epoch 5 | Step 3134400 | Avg Loss: 0.0153 | Grad Norm: 0.00892824\n",
      "Epoch 5 | Step 3134500 | Avg Loss: 0.0154 | Grad Norm: 0.01101828\n",
      "Epoch 5 | Step 3134600 | Avg Loss: 0.0155 | Grad Norm: 0.00938314\n",
      "Epoch 5 | Step 3134700 | Avg Loss: 0.0155 | Grad Norm: 0.00902567\n",
      "Epoch 5 | Step 3134800 | Avg Loss: 0.0154 | Grad Norm: 0.00761409\n",
      "Epoch 5 | Step 3134900 | Avg Loss: 0.0155 | Grad Norm: 0.00997610\n",
      "Epoch 5 | Step 3135000 | Avg Loss: 0.0155 | Grad Norm: 0.00886183\n",
      "Epoch 5 | Step 3135100 | Avg Loss: 0.0158 | Grad Norm: 0.00894522\n",
      "Epoch 5 | Step 3135200 | Avg Loss: 0.0160 | Grad Norm: 0.00846284\n",
      "Epoch 5 | Step 3135300 | Avg Loss: 0.0156 | Grad Norm: 0.00882832\n",
      "Epoch 5 | Step 3135400 | Avg Loss: 0.0157 | Grad Norm: 0.00969571\n",
      "Epoch 5 | Step 3135500 | Avg Loss: 0.0161 | Grad Norm: 0.00874427\n",
      "Epoch 5 | Step 3135600 | Avg Loss: 0.0163 | Grad Norm: 0.00870657\n",
      "Epoch 5 | Step 3135700 | Avg Loss: 0.0161 | Grad Norm: 0.00901204\n",
      "Epoch 5 | Step 3135800 | Avg Loss: 0.0162 | Grad Norm: 0.00852320\n",
      "Epoch 5 | Step 3135900 | Avg Loss: 0.0157 | Grad Norm: 0.00890253\n",
      "Epoch 5 | Step 3136000 | Avg Loss: 0.0157 | Grad Norm: 0.00945436\n",
      "Epoch 5 | Step 3136100 | Avg Loss: 0.0161 | Grad Norm: 0.01004294\n",
      "Epoch 5 | Step 3136200 | Avg Loss: 0.0159 | Grad Norm: 0.00884546\n",
      "Epoch 5 | Step 3136300 | Avg Loss: 0.0159 | Grad Norm: 0.01028123\n",
      "Epoch 5 | Step 3136400 | Avg Loss: 0.0156 | Grad Norm: 0.00970726\n",
      "Epoch 5 | Step 3136500 | Avg Loss: 0.0157 | Grad Norm: 0.01029747\n",
      "Epoch 5 | Step 3136600 | Avg Loss: 0.0160 | Grad Norm: 0.00850943\n",
      "Epoch 5 | Step 3136700 | Avg Loss: 0.0154 | Grad Norm: 0.00941070\n",
      "Epoch 5 | Step 3136800 | Avg Loss: 0.0155 | Grad Norm: 0.00825290\n",
      "Epoch 5 | Step 3136900 | Avg Loss: 0.0151 | Grad Norm: 0.00937860\n",
      "Epoch 5 | Step 3137000 | Avg Loss: 0.0151 | Grad Norm: 0.01016943\n",
      "Epoch 5 | Step 3137100 | Avg Loss: 0.0150 | Grad Norm: 0.00815550\n",
      "Epoch 5 | Step 3137200 | Avg Loss: 0.0152 | Grad Norm: 0.00906029\n",
      "Epoch 5 | Step 3137300 | Avg Loss: 0.0151 | Grad Norm: 0.00807564\n",
      "Epoch 5 | Step 3137400 | Avg Loss: 0.0157 | Grad Norm: 0.00950383\n",
      "Epoch 5 | Step 3137500 | Avg Loss: 0.0160 | Grad Norm: 0.01114550\n",
      "Epoch 5 | Step 3137600 | Avg Loss: 0.0158 | Grad Norm: 0.00890806\n",
      "Epoch 5 | Step 3137700 | Avg Loss: 0.0155 | Grad Norm: 0.00803919\n",
      "Epoch 5 | Step 3137800 | Avg Loss: 0.0155 | Grad Norm: 0.00855456\n",
      "Epoch 5 | Step 3137900 | Avg Loss: 0.0157 | Grad Norm: 0.00906599\n",
      "Epoch 5 | Step 3138000 | Avg Loss: 0.0155 | Grad Norm: 0.00852119\n",
      "Epoch 5 | Step 3138100 | Avg Loss: 0.0156 | Grad Norm: 0.00897959\n",
      "Epoch 5 | Step 3138200 | Avg Loss: 0.0155 | Grad Norm: 0.00931420\n",
      "Epoch 5 | Step 3138300 | Avg Loss: 0.0152 | Grad Norm: 0.00889736\n",
      "Epoch 5 | Step 3138400 | Avg Loss: 0.0155 | Grad Norm: 0.00828197\n",
      "Epoch 5 | Step 3138500 | Avg Loss: 0.0155 | Grad Norm: 0.00967025\n",
      "Epoch 5 | Step 3138600 | Avg Loss: 0.0153 | Grad Norm: 0.01024044\n",
      "Epoch 5 | Step 3138700 | Avg Loss: 0.0151 | Grad Norm: 0.00936431\n",
      "Epoch 5 | Step 3138800 | Avg Loss: 0.0152 | Grad Norm: 0.00904851\n",
      "Epoch 5 | Step 3138900 | Avg Loss: 0.0159 | Grad Norm: 0.00900575\n",
      "Epoch 5 | Step 3139000 | Avg Loss: 0.0157 | Grad Norm: 0.00857719\n",
      "Epoch 5 | Step 3139100 | Avg Loss: 0.0154 | Grad Norm: 0.00850772\n",
      "Epoch 5 | Step 3139200 | Avg Loss: 0.0152 | Grad Norm: 0.00870718\n",
      "Epoch 5 | Step 3139300 | Avg Loss: 0.0154 | Grad Norm: 0.01039932\n",
      "Epoch 5 | Step 3139400 | Avg Loss: 0.0155 | Grad Norm: 0.01035791\n",
      "Epoch 5 | Step 3139500 | Avg Loss: 0.0154 | Grad Norm: 0.00848361\n",
      "Epoch 5 | Step 3139600 | Avg Loss: 0.0159 | Grad Norm: 0.00860677\n",
      "Epoch 5 | Step 3139700 | Avg Loss: 0.0158 | Grad Norm: 0.00959823\n",
      "Epoch 5 | Step 3139800 | Avg Loss: 0.0153 | Grad Norm: 0.00812451\n",
      "Epoch 5 | Step 3139900 | Avg Loss: 0.0154 | Grad Norm: 0.00971308\n",
      "Epoch 5 | Step 3140000 | Avg Loss: 0.0158 | Grad Norm: 0.00952790\n",
      "Epoch 5 | Step 3140100 | Avg Loss: 0.0155 | Grad Norm: 0.01065608\n",
      "Epoch 5 | Step 3140200 | Avg Loss: 0.0155 | Grad Norm: 0.00863837\n",
      "Epoch 5 | Step 3140300 | Avg Loss: 0.0154 | Grad Norm: 0.00887818\n",
      "Epoch 5 | Step 3140400 | Avg Loss: 0.0153 | Grad Norm: 0.01011879\n",
      "Epoch 5 | Step 3140500 | Avg Loss: 0.0154 | Grad Norm: 0.00990876\n",
      "Epoch 5 | Step 3140600 | Avg Loss: 0.0157 | Grad Norm: 0.00911744\n",
      "Epoch 5 | Step 3140700 | Avg Loss: 0.0162 | Grad Norm: 0.01006469\n",
      "Epoch 5 | Step 3140800 | Avg Loss: 0.0163 | Grad Norm: 0.00886167\n",
      "Epoch 5 | Step 3140900 | Avg Loss: 0.0166 | Grad Norm: 0.00943798\n",
      "Epoch 5 | Step 3141000 | Avg Loss: 0.0163 | Grad Norm: 0.01022147\n",
      "Epoch 5 | Step 3141100 | Avg Loss: 0.0165 | Grad Norm: 0.00882001\n",
      "Epoch 5 | Step 3141200 | Avg Loss: 0.0164 | Grad Norm: 0.01121874\n",
      "Epoch 5 | Step 3141300 | Avg Loss: 0.0161 | Grad Norm: 0.00840956\n",
      "Epoch 5 | Step 3141400 | Avg Loss: 0.0158 | Grad Norm: 0.01107577\n",
      "Epoch 5 | Step 3141500 | Avg Loss: 0.0162 | Grad Norm: 0.00988601\n",
      "Epoch 5 | Step 3141600 | Avg Loss: 0.0156 | Grad Norm: 0.01038543\n",
      "Epoch 5 | Step 3141700 | Avg Loss: 0.0154 | Grad Norm: 0.00846275\n",
      "Epoch 5 | Step 3141800 | Avg Loss: 0.0156 | Grad Norm: 0.01004340\n",
      "Epoch 5 | Step 3141900 | Avg Loss: 0.0158 | Grad Norm: 0.01011911\n",
      "Epoch 5 | Step 3142000 | Avg Loss: 0.0158 | Grad Norm: 0.00992074\n",
      "Epoch 5 | Step 3142100 | Avg Loss: 0.0155 | Grad Norm: 0.00892874\n",
      "Epoch 5 | Step 3142200 | Avg Loss: 0.0155 | Grad Norm: 0.01296877\n",
      "Epoch 5 | Step 3142300 | Avg Loss: 0.0155 | Grad Norm: 0.00910361\n",
      "Epoch 5 | Step 3142400 | Avg Loss: 0.0155 | Grad Norm: 0.00950334\n",
      "Epoch 5 | Step 3142500 | Avg Loss: 0.0154 | Grad Norm: 0.00968090\n",
      "Epoch 5 | Step 3142600 | Avg Loss: 0.0153 | Grad Norm: 0.00833770\n",
      "Epoch 5 | Step 3142700 | Avg Loss: 0.0153 | Grad Norm: 0.00974815\n",
      "Epoch 5 | Step 3142800 | Avg Loss: 0.0153 | Grad Norm: 0.00873579\n",
      "Epoch 5 | Step 3142900 | Avg Loss: 0.0154 | Grad Norm: 0.00929816\n",
      "Epoch 5 | Step 3143000 | Avg Loss: 0.0156 | Grad Norm: 0.00812722\n",
      "Epoch 5 | Step 3143100 | Avg Loss: 0.0151 | Grad Norm: 0.00988429\n",
      "Epoch 5 | Step 3143200 | Avg Loss: 0.0153 | Grad Norm: 0.00881471\n",
      "Epoch 5 | Step 3143300 | Avg Loss: 0.0151 | Grad Norm: 0.00840612\n",
      "Epoch 5 | Step 3143400 | Avg Loss: 0.0152 | Grad Norm: 0.00867720\n",
      "Epoch 5 | Step 3143500 | Avg Loss: 0.0154 | Grad Norm: 0.00966215\n",
      "Epoch 5 | Step 3143600 | Avg Loss: 0.0157 | Grad Norm: 0.00870010\n",
      "Epoch 5 | Step 3143700 | Avg Loss: 0.0159 | Grad Norm: 0.00750305\n",
      "Epoch 5 | Step 3143800 | Avg Loss: 0.0161 | Grad Norm: 0.01002780\n",
      "Epoch 5 | Step 3143900 | Avg Loss: 0.0163 | Grad Norm: 0.00831335\n",
      "Epoch 5 | Step 3144000 | Avg Loss: 0.0162 | Grad Norm: 0.00964599\n",
      "Epoch 5 | Step 3144100 | Avg Loss: 0.0163 | Grad Norm: 0.00892243\n",
      "Epoch 5 | Step 3144200 | Avg Loss: 0.0162 | Grad Norm: 0.00894832\n",
      "Epoch 5 | Step 3144300 | Avg Loss: 0.0161 | Grad Norm: 0.00912638\n",
      "Epoch 5 | Step 3144400 | Avg Loss: 0.0154 | Grad Norm: 0.00850061\n",
      "Epoch 5 | Step 3144500 | Avg Loss: 0.0153 | Grad Norm: 0.00875694\n",
      "Epoch 5 | Step 3144600 | Avg Loss: 0.0152 | Grad Norm: 0.00904931\n",
      "Epoch 5 | Step 3144700 | Avg Loss: 0.0153 | Grad Norm: 0.01001523\n",
      "Epoch 5 | Step 3144800 | Avg Loss: 0.0154 | Grad Norm: 0.00829372\n",
      "Epoch 5 | Step 3144900 | Avg Loss: 0.0152 | Grad Norm: 0.00945574\n",
      "Epoch 5 | Step 3145000 | Avg Loss: 0.0152 | Grad Norm: 0.00870895\n",
      "Epoch 5 | Step 3145100 | Avg Loss: 0.0154 | Grad Norm: 0.00896755\n",
      "Epoch 5 | Step 3145200 | Avg Loss: 0.0150 | Grad Norm: 0.00990335\n",
      "Epoch 5 | Step 3145300 | Avg Loss: 0.0156 | Grad Norm: 0.00979445\n",
      "Epoch 5 | Step 3145400 | Avg Loss: 0.0157 | Grad Norm: 0.00952758\n",
      "Epoch 5 | Step 3145500 | Avg Loss: 0.0156 | Grad Norm: 0.00840540\n",
      "Epoch 5 | Step 3145600 | Avg Loss: 0.0159 | Grad Norm: 0.00845639\n",
      "Epoch 5 | Step 3145700 | Avg Loss: 0.0159 | Grad Norm: 0.01013364\n",
      "Epoch 5 | Step 3145800 | Avg Loss: 0.0156 | Grad Norm: 0.00923531\n",
      "Epoch 5 | Step 3145900 | Avg Loss: 0.0153 | Grad Norm: 0.00869095\n",
      "Epoch 5 | Step 3146000 | Avg Loss: 0.0155 | Grad Norm: 0.01042238\n",
      "Epoch 5 | Step 3146100 | Avg Loss: 0.0154 | Grad Norm: 0.00885727\n",
      "Epoch 5 | Step 3146200 | Avg Loss: 0.0156 | Grad Norm: 0.00907328\n",
      "Epoch 5 | Step 3146300 | Avg Loss: 0.0155 | Grad Norm: 0.00835863\n",
      "Epoch 5 | Step 3146400 | Avg Loss: 0.0157 | Grad Norm: 0.00858775\n",
      "Epoch 5 | Step 3146500 | Avg Loss: 0.0158 | Grad Norm: 0.00960289\n",
      "Epoch 5 | Step 3146600 | Avg Loss: 0.0159 | Grad Norm: 0.01020845\n",
      "Epoch 5 | Step 3146700 | Avg Loss: 0.0159 | Grad Norm: 0.00892147\n",
      "Epoch 5 | Step 3146800 | Avg Loss: 0.0161 | Grad Norm: 0.01319745\n",
      "Epoch 5 | Step 3146900 | Avg Loss: 0.0159 | Grad Norm: 0.00832759\n",
      "Epoch 5 | Step 3147000 | Avg Loss: 0.0158 | Grad Norm: 0.00868472\n",
      "Epoch 5 | Step 3147100 | Avg Loss: 0.0156 | Grad Norm: 0.01019922\n",
      "Epoch 5 | Step 3147200 | Avg Loss: 0.0155 | Grad Norm: 0.00977455\n",
      "Epoch 5 | Step 3147300 | Avg Loss: 0.0154 | Grad Norm: 0.00785964\n",
      "Epoch 5 | Step 3147400 | Avg Loss: 0.0156 | Grad Norm: 0.01022773\n",
      "Epoch 5 | Step 3147500 | Avg Loss: 0.0155 | Grad Norm: 0.00785268\n",
      "Epoch 5 | Step 3147600 | Avg Loss: 0.0155 | Grad Norm: 0.00870848\n",
      "Epoch 5 | Step 3147700 | Avg Loss: 0.0156 | Grad Norm: 0.00959956\n",
      "Epoch 5 | Step 3147800 | Avg Loss: 0.0157 | Grad Norm: 0.00929276\n",
      "Epoch 5 | Step 3147900 | Avg Loss: 0.0160 | Grad Norm: 0.01435324\n",
      "Epoch 5 | Step 3148000 | Avg Loss: 0.0157 | Grad Norm: 0.00903510\n",
      "Epoch 5 | Step 3148100 | Avg Loss: 0.0155 | Grad Norm: 0.00974628\n",
      "Epoch 5 | Step 3148200 | Avg Loss: 0.0153 | Grad Norm: 0.00886216\n",
      "Epoch 5 | Step 3148300 | Avg Loss: 0.0153 | Grad Norm: 0.00928532\n",
      "Epoch 5 | Step 3148400 | Avg Loss: 0.0155 | Grad Norm: 0.00955286\n",
      "Epoch 5 | Step 3148500 | Avg Loss: 0.0156 | Grad Norm: 0.01023275\n",
      "Epoch 5 | Step 3148600 | Avg Loss: 0.0160 | Grad Norm: 0.01069666\n",
      "Epoch 5 | Step 3148700 | Avg Loss: 0.0162 | Grad Norm: 0.01075669\n",
      "Epoch 5 | Step 3148800 | Avg Loss: 0.0161 | Grad Norm: 0.01017036\n",
      "Epoch 5 | Step 3148900 | Avg Loss: 0.0158 | Grad Norm: 0.00966170\n",
      "Epoch 5 | Step 3149000 | Avg Loss: 0.0156 | Grad Norm: 0.00910147\n",
      "Epoch 5 | Step 3149100 | Avg Loss: 0.0155 | Grad Norm: 0.00872736\n",
      "Epoch 5 | Step 3149200 | Avg Loss: 0.0160 | Grad Norm: 0.00979001\n",
      "Epoch 5 | Step 3149300 | Avg Loss: 0.0157 | Grad Norm: 0.00872992\n",
      "Epoch 5 | Step 3149400 | Avg Loss: 0.0157 | Grad Norm: 0.00903672\n",
      "Epoch 5 | Step 3149500 | Avg Loss: 0.0156 | Grad Norm: 0.00932036\n",
      "Epoch 5 | Step 3149600 | Avg Loss: 0.0155 | Grad Norm: 0.00958212\n",
      "Epoch 5 | Step 3149700 | Avg Loss: 0.0162 | Grad Norm: 0.00878807\n",
      "Epoch 5 | Step 3149800 | Avg Loss: 0.0161 | Grad Norm: 0.00934840\n",
      "Epoch 5 | Step 3149900 | Avg Loss: 0.0158 | Grad Norm: 0.00801970\n",
      "Epoch 5 | Step 3150000 | Avg Loss: 0.0157 | Grad Norm: 0.00872351\n",
      "Epoch 5 | Step 3150100 | Avg Loss: 0.0159 | Grad Norm: 0.01078523\n",
      "Epoch 5 | Step 3150200 | Avg Loss: 0.0160 | Grad Norm: 0.00939342\n",
      "Epoch 5 | Step 3150300 | Avg Loss: 0.0162 | Grad Norm: 0.00886035\n",
      "Epoch 5 | Step 3150400 | Avg Loss: 0.0161 | Grad Norm: 0.00940492\n",
      "Epoch 5 | Step 3150500 | Avg Loss: 0.0161 | Grad Norm: 0.00848751\n",
      "Epoch 5 | Step 3150600 | Avg Loss: 0.0162 | Grad Norm: 0.00963638\n",
      "Epoch 5 | Step 3150700 | Avg Loss: 0.0159 | Grad Norm: 0.00961882\n",
      "Epoch 5 | Step 3150800 | Avg Loss: 0.0161 | Grad Norm: 0.00948671\n",
      "Epoch 5 | Step 3150900 | Avg Loss: 0.0162 | Grad Norm: 0.01043371\n",
      "Epoch 5 | Step 3151000 | Avg Loss: 0.0158 | Grad Norm: 0.01056817\n",
      "Epoch 5 | Step 3151100 | Avg Loss: 0.0152 | Grad Norm: 0.01166789\n",
      "Epoch 5 | Step 3151200 | Avg Loss: 0.0154 | Grad Norm: 0.00911795\n",
      "Epoch 5 | Step 3151300 | Avg Loss: 0.0158 | Grad Norm: 0.01003224\n",
      "Epoch 5 | Step 3151400 | Avg Loss: 0.0154 | Grad Norm: 0.01012859\n",
      "Epoch 5 | Step 3151500 | Avg Loss: 0.0154 | Grad Norm: 0.00926491\n",
      "Epoch 5 | Step 3151600 | Avg Loss: 0.0152 | Grad Norm: 0.01102690\n",
      "Epoch 5 | Step 3151700 | Avg Loss: 0.0155 | Grad Norm: 0.00908948\n",
      "Epoch 5 | Step 3151800 | Avg Loss: 0.0156 | Grad Norm: 0.00981537\n",
      "Epoch 5 | Step 3151900 | Avg Loss: 0.0158 | Grad Norm: 0.00943291\n",
      "Epoch 5 | Step 3152000 | Avg Loss: 0.0159 | Grad Norm: 0.00927357\n",
      "Epoch 5 | Step 3152100 | Avg Loss: 0.0159 | Grad Norm: 0.00894870\n",
      "Epoch 5 | Step 3152200 | Avg Loss: 0.0155 | Grad Norm: 0.00889962\n",
      "Epoch 5 | Step 3152300 | Avg Loss: 0.0155 | Grad Norm: 0.00914921\n",
      "Epoch 5 | Step 3152400 | Avg Loss: 0.0157 | Grad Norm: 0.00899467\n",
      "Epoch 5 | Step 3152500 | Avg Loss: 0.0160 | Grad Norm: 0.00850482\n",
      "Epoch 5 | Step 3152600 | Avg Loss: 0.0157 | Grad Norm: 0.00840468\n",
      "Epoch 5 | Step 3152700 | Avg Loss: 0.0158 | Grad Norm: 0.00957975\n",
      "Epoch 5 | Step 3152800 | Avg Loss: 0.0159 | Grad Norm: 0.00908248\n",
      "Epoch 5 | Step 3152900 | Avg Loss: 0.0158 | Grad Norm: 0.00987091\n",
      "Epoch 5 | Step 3153000 | Avg Loss: 0.0157 | Grad Norm: 0.00992011\n",
      "Epoch 5 | Step 3153100 | Avg Loss: 0.0159 | Grad Norm: 0.00888422\n",
      "Epoch 5 | Step 3153200 | Avg Loss: 0.0158 | Grad Norm: 0.00837576\n",
      "Epoch 5 | Step 3153300 | Avg Loss: 0.0154 | Grad Norm: 0.00990649\n",
      "Epoch 5 | Step 3153400 | Avg Loss: 0.0153 | Grad Norm: 0.00910826\n",
      "Epoch 5 | Step 3153500 | Avg Loss: 0.0161 | Grad Norm: 0.00837649\n",
      "Epoch 5 | Step 3153600 | Avg Loss: 0.0157 | Grad Norm: 0.00858786\n",
      "Epoch 5 | Step 3153700 | Avg Loss: 0.0156 | Grad Norm: 0.00884469\n",
      "Epoch 5 | Step 3153800 | Avg Loss: 0.0153 | Grad Norm: 0.00811238\n",
      "Epoch 5 | Step 3153900 | Avg Loss: 0.0154 | Grad Norm: 0.00975475\n",
      "Epoch 5 | Step 3154000 | Avg Loss: 0.0152 | Grad Norm: 0.00934139\n",
      "Epoch 5 | Step 3154100 | Avg Loss: 0.0155 | Grad Norm: 0.00857446\n",
      "Epoch 5 | Step 3154200 | Avg Loss: 0.0155 | Grad Norm: 0.01021918\n",
      "Epoch 5 | Step 3154300 | Avg Loss: 0.0152 | Grad Norm: 0.00908300\n",
      "Epoch 5 | Step 3154400 | Avg Loss: 0.0153 | Grad Norm: 0.00881367\n",
      "Epoch 5 | Step 3154500 | Avg Loss: 0.0155 | Grad Norm: 0.00954000\n",
      "Epoch 5 | Step 3154600 | Avg Loss: 0.0156 | Grad Norm: 0.00989669\n",
      "Epoch 5 | Step 3154700 | Avg Loss: 0.0154 | Grad Norm: 0.00823597\n",
      "Epoch 5 | Step 3154800 | Avg Loss: 0.0158 | Grad Norm: 0.01117854\n",
      "Epoch 5 | Step 3154900 | Avg Loss: 0.0158 | Grad Norm: 0.00806891\n",
      "Epoch 5 | Step 3155000 | Avg Loss: 0.0159 | Grad Norm: 0.00886198\n",
      "Epoch 5 | Step 3155100 | Avg Loss: 0.0154 | Grad Norm: 0.00825630\n",
      "Epoch 5 | Step 3155200 | Avg Loss: 0.0157 | Grad Norm: 0.00798314\n",
      "Epoch 5 | Step 3155300 | Avg Loss: 0.0159 | Grad Norm: 0.00847582\n",
      "Epoch 5 | Step 3155400 | Avg Loss: 0.0159 | Grad Norm: 0.01016531\n",
      "Epoch 5 | Step 3155500 | Avg Loss: 0.0156 | Grad Norm: 0.01058821\n",
      "Epoch 5 | Step 3155600 | Avg Loss: 0.0155 | Grad Norm: 0.00873980\n",
      "Epoch 5 | Step 3155700 | Avg Loss: 0.0154 | Grad Norm: 0.00991965\n",
      "Epoch 5 | Step 3155800 | Avg Loss: 0.0154 | Grad Norm: 0.00913350\n",
      "Epoch 5 | Step 3155900 | Avg Loss: 0.0156 | Grad Norm: 0.00857897\n",
      "Epoch 5 | Step 3156000 | Avg Loss: 0.0154 | Grad Norm: 0.00825817\n",
      "Epoch 5 | Step 3156100 | Avg Loss: 0.0155 | Grad Norm: 0.00971765\n",
      "Epoch 5 | Step 3156200 | Avg Loss: 0.0155 | Grad Norm: 0.00959138\n",
      "Epoch 5 | Step 3156300 | Avg Loss: 0.0152 | Grad Norm: 0.00918211\n",
      "Epoch 5 | Step 3156400 | Avg Loss: 0.0154 | Grad Norm: 0.01071352\n",
      "Epoch 5 | Step 3156500 | Avg Loss: 0.0153 | Grad Norm: 0.00863014\n",
      "Epoch 5 | Step 3156600 | Avg Loss: 0.0154 | Grad Norm: 0.00836319\n",
      "Epoch 5 | Step 3156700 | Avg Loss: 0.0154 | Grad Norm: 0.00865995\n",
      "Epoch 5 | Step 3156800 | Avg Loss: 0.0154 | Grad Norm: 0.00954860\n",
      "Epoch 5 | Step 3156900 | Avg Loss: 0.0157 | Grad Norm: 0.01321765\n",
      "Epoch 5 | Step 3157000 | Avg Loss: 0.0158 | Grad Norm: 0.00996696\n",
      "Epoch 5 | Step 3157100 | Avg Loss: 0.0154 | Grad Norm: 0.00804091\n",
      "Epoch 5 | Step 3157200 | Avg Loss: 0.0158 | Grad Norm: 0.00855970\n",
      "Epoch 5 | Step 3157300 | Avg Loss: 0.0159 | Grad Norm: 0.00859452\n",
      "Epoch 5 | Step 3157400 | Avg Loss: 0.0159 | Grad Norm: 0.00972659\n",
      "Epoch 5 | Step 3157500 | Avg Loss: 0.0159 | Grad Norm: 0.01044510\n",
      "Epoch 5 | Step 3157600 | Avg Loss: 0.0159 | Grad Norm: 0.00908581\n",
      "Epoch 5 | Step 3157700 | Avg Loss: 0.0161 | Grad Norm: 0.00867914\n",
      "Epoch 5 | Step 3157800 | Avg Loss: 0.0158 | Grad Norm: 0.01220181\n",
      "Epoch 5 | Step 3157900 | Avg Loss: 0.0157 | Grad Norm: 0.00975985\n",
      "Epoch 5 | Step 3158000 | Avg Loss: 0.0157 | Grad Norm: 0.00877822\n",
      "Epoch 5 | Step 3158100 | Avg Loss: 0.0154 | Grad Norm: 0.00857299\n",
      "Epoch 5 | Step 3158200 | Avg Loss: 0.0152 | Grad Norm: 0.00829621\n",
      "Epoch 5 | Step 3158300 | Avg Loss: 0.0154 | Grad Norm: 0.00898682\n",
      "Epoch 5 | Step 3158400 | Avg Loss: 0.0153 | Grad Norm: 0.00956658\n",
      "Epoch 5 | Step 3158500 | Avg Loss: 0.0154 | Grad Norm: 0.01028940\n",
      "Epoch 5 | Step 3158600 | Avg Loss: 0.0155 | Grad Norm: 0.00926459\n",
      "Epoch 5 | Step 3158700 | Avg Loss: 0.0155 | Grad Norm: 0.00937524\n",
      "Epoch 5 | Step 3158800 | Avg Loss: 0.0155 | Grad Norm: 0.00878034\n",
      "Epoch 5 | Step 3158900 | Avg Loss: 0.0153 | Grad Norm: 0.00828134\n",
      "Epoch 5 | Step 3159000 | Avg Loss: 0.0153 | Grad Norm: 0.00885699\n",
      "Epoch 5 | Step 3159100 | Avg Loss: 0.0147 | Grad Norm: 0.00855461\n",
      "Epoch 5 | Step 3159200 | Avg Loss: 0.0147 | Grad Norm: 0.00916000\n",
      "Epoch 5 | Step 3159300 | Avg Loss: 0.0146 | Grad Norm: 0.01069943\n",
      "Epoch 5 | Step 3159400 | Avg Loss: 0.0152 | Grad Norm: 0.00775363\n",
      "Epoch 5 | Step 3159500 | Avg Loss: 0.0154 | Grad Norm: 0.00969014\n",
      "Epoch 5 | Step 3159600 | Avg Loss: 0.0155 | Grad Norm: 0.00764009\n",
      "Epoch 5 | Step 3159700 | Avg Loss: 0.0152 | Grad Norm: 0.00896688\n",
      "Epoch 5 | Step 3159800 | Avg Loss: 0.0151 | Grad Norm: 0.01029280\n",
      "Epoch 5 | Step 3159900 | Avg Loss: 0.0155 | Grad Norm: 0.00790762\n",
      "Epoch 5 | Step 3160000 | Avg Loss: 0.0155 | Grad Norm: 0.00914660\n",
      "Epoch 5 | Step 3160100 | Avg Loss: 0.0154 | Grad Norm: 0.00932419\n",
      "Epoch 5 | Step 3160200 | Avg Loss: 0.0159 | Grad Norm: 0.00917877\n",
      "Epoch 5 | Step 3160300 | Avg Loss: 0.0158 | Grad Norm: 0.00855312\n",
      "Epoch 5 | Step 3160400 | Avg Loss: 0.0158 | Grad Norm: 0.00818327\n",
      "Epoch 5 | Step 3160500 | Avg Loss: 0.0159 | Grad Norm: 0.00894799\n",
      "Epoch 5 | Step 3160600 | Avg Loss: 0.0163 | Grad Norm: 0.00890883\n",
      "Epoch 5 | Step 3160700 | Avg Loss: 0.0159 | Grad Norm: 0.00862574\n",
      "Epoch 5 | Step 3160800 | Avg Loss: 0.0154 | Grad Norm: 0.00773914\n",
      "Epoch 5 | Step 3160900 | Avg Loss: 0.0157 | Grad Norm: 0.00916831\n",
      "Epoch 5 | Step 3161000 | Avg Loss: 0.0153 | Grad Norm: 0.00873693\n",
      "Epoch 5 | Step 3161100 | Avg Loss: 0.0150 | Grad Norm: 0.00912901\n",
      "Epoch 5 | Step 3161200 | Avg Loss: 0.0156 | Grad Norm: 0.01019144\n",
      "Epoch 5 | Step 3161300 | Avg Loss: 0.0157 | Grad Norm: 0.00883423\n",
      "Epoch 5 | Step 3161400 | Avg Loss: 0.0160 | Grad Norm: 0.00932315\n",
      "Epoch 5 | Step 3161500 | Avg Loss: 0.0160 | Grad Norm: 0.00955742\n",
      "Epoch 5 | Step 3161600 | Avg Loss: 0.0160 | Grad Norm: 0.01034654\n",
      "Epoch 5 | Step 3161700 | Avg Loss: 0.0158 | Grad Norm: 0.01016636\n",
      "Epoch 5 | Step 3161800 | Avg Loss: 0.0158 | Grad Norm: 0.00974825\n",
      "Epoch 5 | Step 3161900 | Avg Loss: 0.0160 | Grad Norm: 0.00939089\n",
      "Epoch 5 | Step 3162000 | Avg Loss: 0.0163 | Grad Norm: 0.00892018\n",
      "Epoch 5 | Step 3162100 | Avg Loss: 0.0161 | Grad Norm: 0.00850541\n",
      "Epoch 5 | Step 3162200 | Avg Loss: 0.0159 | Grad Norm: 0.00937428\n",
      "Epoch 5 | Step 3162300 | Avg Loss: 0.0156 | Grad Norm: 0.01027491\n",
      "Epoch 5 | Step 3162400 | Avg Loss: 0.0158 | Grad Norm: 0.00912079\n",
      "Epoch 5 | Step 3162500 | Avg Loss: 0.0157 | Grad Norm: 0.01085226\n",
      "Epoch 5 | Step 3162600 | Avg Loss: 0.0158 | Grad Norm: 0.00888545\n",
      "Epoch 5 | Step 3162700 | Avg Loss: 0.0157 | Grad Norm: 0.00861701\n",
      "Epoch 5 | Step 3162800 | Avg Loss: 0.0157 | Grad Norm: 0.01009150\n",
      "Epoch 5 | Step 3162900 | Avg Loss: 0.0156 | Grad Norm: 0.00862602\n",
      "Epoch 5 | Step 3163000 | Avg Loss: 0.0159 | Grad Norm: 0.00913012\n",
      "Epoch 5 | Step 3163100 | Avg Loss: 0.0163 | Grad Norm: 0.00958047\n",
      "Epoch 5 | Step 3163200 | Avg Loss: 0.0163 | Grad Norm: 0.00975228\n",
      "Epoch 5 | Step 3163300 | Avg Loss: 0.0162 | Grad Norm: 0.00897770\n",
      "Epoch 5 | Step 3163400 | Avg Loss: 0.0162 | Grad Norm: 0.01017070\n",
      "Epoch 5 | Step 3163500 | Avg Loss: 0.0163 | Grad Norm: 0.01055756\n",
      "Epoch 5 | Step 3163600 | Avg Loss: 0.0160 | Grad Norm: 0.00918345\n",
      "Epoch 5 | Step 3163700 | Avg Loss: 0.0160 | Grad Norm: 0.00908251\n",
      "Epoch 5 | Step 3163800 | Avg Loss: 0.0158 | Grad Norm: 0.00875994\n",
      "Epoch 5 | Step 3163900 | Avg Loss: 0.0158 | Grad Norm: 0.00901301\n",
      "Epoch 5 | Step 3164000 | Avg Loss: 0.0156 | Grad Norm: 0.01025477\n",
      "Epoch 5 | Step 3164100 | Avg Loss: 0.0160 | Grad Norm: 0.00852832\n",
      "Epoch 5 | Step 3164200 | Avg Loss: 0.0155 | Grad Norm: 0.00827313\n",
      "Epoch 5 | Step 3164300 | Avg Loss: 0.0153 | Grad Norm: 0.00914275\n",
      "Epoch 5 | Step 3164400 | Avg Loss: 0.0157 | Grad Norm: 0.00886358\n",
      "Epoch 5 | Step 3164500 | Avg Loss: 0.0156 | Grad Norm: 0.00923068\n",
      "Epoch 5 | Step 3164600 | Avg Loss: 0.0153 | Grad Norm: 0.00987642\n",
      "Epoch 5 | Step 3164700 | Avg Loss: 0.0157 | Grad Norm: 0.00945447\n",
      "Epoch 5 | Step 3164800 | Avg Loss: 0.0157 | Grad Norm: 0.00968847\n",
      "Epoch 5 | Step 3164900 | Avg Loss: 0.0155 | Grad Norm: 0.00914850\n",
      "Epoch 5 | Step 3165000 | Avg Loss: 0.0154 | Grad Norm: 0.00884461\n",
      "Epoch 5 | Step 3165100 | Avg Loss: 0.0154 | Grad Norm: 0.01035077\n",
      "Epoch 5 | Step 3165200 | Avg Loss: 0.0155 | Grad Norm: 0.00849521\n",
      "Epoch 5 | Step 3165300 | Avg Loss: 0.0155 | Grad Norm: 0.00784724\n",
      "Epoch 5 | Step 3165400 | Avg Loss: 0.0155 | Grad Norm: 0.00857368\n",
      "Epoch 5 | Step 3165500 | Avg Loss: 0.0154 | Grad Norm: 0.00999868\n",
      "Epoch 5 | Step 3165600 | Avg Loss: 0.0155 | Grad Norm: 0.00956039\n",
      "Epoch 5 | Step 3165700 | Avg Loss: 0.0153 | Grad Norm: 0.00934838\n",
      "Epoch 5 | Step 3165800 | Avg Loss: 0.0150 | Grad Norm: 0.00840451\n",
      "Epoch 5 | Step 3165900 | Avg Loss: 0.0149 | Grad Norm: 0.01187433\n",
      "Epoch 5 | Step 3166000 | Avg Loss: 0.0154 | Grad Norm: 0.00845421\n",
      "Epoch 5 | Step 3166100 | Avg Loss: 0.0152 | Grad Norm: 0.00777545\n",
      "Epoch 5 | Step 3166200 | Avg Loss: 0.0155 | Grad Norm: 0.00977108\n",
      "Epoch 5 | Step 3166300 | Avg Loss: 0.0157 | Grad Norm: 0.00826172\n",
      "Epoch 5 | Step 3166400 | Avg Loss: 0.0155 | Grad Norm: 0.00863279\n",
      "Epoch 5 | Step 3166500 | Avg Loss: 0.0153 | Grad Norm: 0.00827504\n",
      "Epoch 5 | Step 3166600 | Avg Loss: 0.0154 | Grad Norm: 0.00752261\n",
      "Epoch 5 | Step 3166700 | Avg Loss: 0.0158 | Grad Norm: 0.00916785\n",
      "Epoch 5 | Step 3166800 | Avg Loss: 0.0153 | Grad Norm: 0.00881372\n",
      "Epoch 5 | Step 3166900 | Avg Loss: 0.0154 | Grad Norm: 0.01024968\n",
      "Epoch 5 | Step 3167000 | Avg Loss: 0.0160 | Grad Norm: 0.00931719\n",
      "Epoch 5 | Step 3167100 | Avg Loss: 0.0159 | Grad Norm: 0.00972609\n",
      "Epoch 5 | Step 3167200 | Avg Loss: 0.0158 | Grad Norm: 0.00885878\n",
      "Epoch 5 | Step 3167300 | Avg Loss: 0.0164 | Grad Norm: 0.01114993\n",
      "Epoch 5 | Step 3167400 | Avg Loss: 0.0164 | Grad Norm: 0.00918689\n",
      "Epoch 5 | Step 3167500 | Avg Loss: 0.0161 | Grad Norm: 0.01127548\n",
      "Epoch 5 | Step 3167600 | Avg Loss: 0.0159 | Grad Norm: 0.01030823\n",
      "Epoch 5 | Step 3167700 | Avg Loss: 0.0157 | Grad Norm: 0.00843411\n",
      "Epoch 5 | Step 3167800 | Avg Loss: 0.0155 | Grad Norm: 0.00987897\n",
      "Epoch 5 | Step 3167900 | Avg Loss: 0.0151 | Grad Norm: 0.00934339\n",
      "Epoch 5 | Step 3168000 | Avg Loss: 0.0151 | Grad Norm: 0.00958560\n",
      "Epoch 5 | Step 3168100 | Avg Loss: 0.0154 | Grad Norm: 0.00871893\n",
      "Epoch 5 | Step 3168200 | Avg Loss: 0.0158 | Grad Norm: 0.00918756\n",
      "Epoch 5 | Step 3168300 | Avg Loss: 0.0154 | Grad Norm: 0.00886954\n",
      "Epoch 5 | Step 3168400 | Avg Loss: 0.0157 | Grad Norm: 0.00863028\n",
      "Epoch 5 | Step 3168500 | Avg Loss: 0.0158 | Grad Norm: 0.00825452\n",
      "Epoch 5 | Step 3168600 | Avg Loss: 0.0156 | Grad Norm: 0.00846616\n",
      "Epoch 5 | Step 3168700 | Avg Loss: 0.0153 | Grad Norm: 0.01073753\n",
      "Epoch 5 | Step 3168800 | Avg Loss: 0.0154 | Grad Norm: 0.00860002\n",
      "Epoch 5 | Step 3168900 | Avg Loss: 0.0155 | Grad Norm: 0.00842042\n",
      "Epoch 5 | Step 3169000 | Avg Loss: 0.0153 | Grad Norm: 0.00792023\n",
      "Epoch 5 | Step 3169100 | Avg Loss: 0.0156 | Grad Norm: 0.00927070\n",
      "Epoch 5 | Step 3169200 | Avg Loss: 0.0155 | Grad Norm: 0.01049658\n",
      "Epoch 5 | Step 3169300 | Avg Loss: 0.0160 | Grad Norm: 0.00979882\n",
      "Epoch 5 | Step 3169400 | Avg Loss: 0.0159 | Grad Norm: 0.00856769\n",
      "Epoch 5 | Step 3169500 | Avg Loss: 0.0159 | Grad Norm: 0.00834663\n",
      "Epoch 5 | Step 3169600 | Avg Loss: 0.0159 | Grad Norm: 0.00911225\n",
      "Epoch 5 | Step 3169700 | Avg Loss: 0.0160 | Grad Norm: 0.00883135\n",
      "Epoch 5 | Step 3169800 | Avg Loss: 0.0159 | Grad Norm: 0.00905655\n",
      "Epoch 5 | Step 3169900 | Avg Loss: 0.0157 | Grad Norm: 0.00964839\n",
      "Epoch 5 | Step 3170000 | Avg Loss: 0.0157 | Grad Norm: 0.00917096\n",
      "Epoch 5 | Step 3170100 | Avg Loss: 0.0156 | Grad Norm: 0.00995798\n",
      "Epoch 5 | Step 3170200 | Avg Loss: 0.0152 | Grad Norm: 0.00906308\n",
      "Epoch 5 | Step 3170300 | Avg Loss: 0.0153 | Grad Norm: 0.00804318\n",
      "Epoch 5 | Step 3170400 | Avg Loss: 0.0151 | Grad Norm: 0.00872157\n",
      "Epoch 5 | Step 3170500 | Avg Loss: 0.0152 | Grad Norm: 0.01004055\n",
      "Epoch 5 | Step 3170600 | Avg Loss: 0.0154 | Grad Norm: 0.00913428\n",
      "Epoch 5 | Step 3170700 | Avg Loss: 0.0155 | Grad Norm: 0.01109217\n",
      "Epoch 5 | Step 3170800 | Avg Loss: 0.0156 | Grad Norm: 0.00851063\n",
      "Epoch 5 | Step 3170900 | Avg Loss: 0.0149 | Grad Norm: 0.00805531\n",
      "Epoch 5 | Step 3171000 | Avg Loss: 0.0152 | Grad Norm: 0.00981337\n",
      "Epoch 5 | Step 3171100 | Avg Loss: 0.0153 | Grad Norm: 0.00936183\n",
      "Epoch 5 | Step 3171200 | Avg Loss: 0.0149 | Grad Norm: 0.00885705\n",
      "Epoch 5 | Step 3171300 | Avg Loss: 0.0153 | Grad Norm: 0.00894375\n",
      "Epoch 5 | Step 3171400 | Avg Loss: 0.0152 | Grad Norm: 0.00985340\n",
      "Epoch 5 | Step 3171500 | Avg Loss: 0.0151 | Grad Norm: 0.00908204\n",
      "Epoch 5 | Step 3171600 | Avg Loss: 0.0150 | Grad Norm: 0.00862821\n",
      "Epoch 5 | Step 3171700 | Avg Loss: 0.0150 | Grad Norm: 0.00858245\n",
      "Epoch 5 | Step 3171800 | Avg Loss: 0.0152 | Grad Norm: 0.01083803\n",
      "Epoch 5 | Step 3171900 | Avg Loss: 0.0152 | Grad Norm: 0.00917291\n",
      "Epoch 5 | Step 3172000 | Avg Loss: 0.0151 | Grad Norm: 0.00898536\n",
      "Epoch 5 | Step 3172100 | Avg Loss: 0.0157 | Grad Norm: 0.00948798\n",
      "Epoch 5 | Step 3172200 | Avg Loss: 0.0152 | Grad Norm: 0.00980581\n",
      "Epoch 5 | Step 3172300 | Avg Loss: 0.0155 | Grad Norm: 0.00854980\n",
      "Epoch 5 | Step 3172400 | Avg Loss: 0.0158 | Grad Norm: 0.00727197\n",
      "Epoch 5 | Step 3172500 | Avg Loss: 0.0158 | Grad Norm: 0.00839660\n",
      "Epoch 5 | Step 3172600 | Avg Loss: 0.0159 | Grad Norm: 0.00987772\n",
      "Epoch 5 | Step 3172700 | Avg Loss: 0.0162 | Grad Norm: 0.00874587\n",
      "Epoch 5 | Step 3172800 | Avg Loss: 0.0162 | Grad Norm: 0.00917090\n",
      "Epoch 5 | Step 3172900 | Avg Loss: 0.0159 | Grad Norm: 0.00864317\n",
      "Epoch 5 | Step 3173000 | Avg Loss: 0.0156 | Grad Norm: 0.00853203\n",
      "Epoch 5 | Step 3173100 | Avg Loss: 0.0154 | Grad Norm: 0.01010842\n",
      "Epoch 5 | Step 3173200 | Avg Loss: 0.0155 | Grad Norm: 0.00841661\n",
      "Epoch 5 | Step 3173300 | Avg Loss: 0.0152 | Grad Norm: 0.01273485\n",
      "Epoch 5 | Step 3173400 | Avg Loss: 0.0155 | Grad Norm: 0.01210492\n",
      "Epoch 5 | Step 3173500 | Avg Loss: 0.0157 | Grad Norm: 0.00765171\n",
      "Epoch 5 | Step 3173600 | Avg Loss: 0.0155 | Grad Norm: 0.00870879\n",
      "Epoch 5 | Step 3173700 | Avg Loss: 0.0159 | Grad Norm: 0.00872860\n",
      "Epoch 5 | Step 3173800 | Avg Loss: 0.0155 | Grad Norm: 0.01126765\n",
      "Epoch 5 | Step 3173900 | Avg Loss: 0.0152 | Grad Norm: 0.00967493\n",
      "Epoch 5 | Step 3174000 | Avg Loss: 0.0152 | Grad Norm: 0.00867605\n",
      "Epoch 5 | Step 3174100 | Avg Loss: 0.0153 | Grad Norm: 0.00752288\n",
      "Epoch 5 | Step 3174200 | Avg Loss: 0.0154 | Grad Norm: 0.00783641\n",
      "Epoch 5 | Step 3174300 | Avg Loss: 0.0156 | Grad Norm: 0.00771645\n",
      "Epoch 5 | Step 3174400 | Avg Loss: 0.0160 | Grad Norm: 0.00956121\n",
      "Epoch 5 | Step 3174500 | Avg Loss: 0.0159 | Grad Norm: 0.00860428\n",
      "Epoch 5 | Step 3174600 | Avg Loss: 0.0158 | Grad Norm: 0.00824059\n",
      "Epoch 5 | Step 3174700 | Avg Loss: 0.0157 | Grad Norm: 0.00843171\n",
      "Epoch 5 | Step 3174800 | Avg Loss: 0.0156 | Grad Norm: 0.00909463\n",
      "Epoch 5 | Step 3174900 | Avg Loss: 0.0156 | Grad Norm: 0.00927675\n",
      "Epoch 5 | Step 3175000 | Avg Loss: 0.0155 | Grad Norm: 0.00858881\n",
      "Epoch 5 | Step 3175100 | Avg Loss: 0.0155 | Grad Norm: 0.00969180\n",
      "Epoch 5 | Step 3175200 | Avg Loss: 0.0156 | Grad Norm: 0.00883342\n",
      "Epoch 5 | Step 3175300 | Avg Loss: 0.0155 | Grad Norm: 0.01073697\n",
      "Epoch 5 | Step 3175400 | Avg Loss: 0.0155 | Grad Norm: 0.00820499\n",
      "Epoch 5 | Step 3175500 | Avg Loss: 0.0156 | Grad Norm: 0.00830395\n",
      "Epoch 5 | Step 3175600 | Avg Loss: 0.0155 | Grad Norm: 0.00853127\n",
      "Epoch 5 | Step 3175700 | Avg Loss: 0.0154 | Grad Norm: 0.00941262\n",
      "Epoch 5 | Step 3175800 | Avg Loss: 0.0155 | Grad Norm: 0.00919575\n",
      "Epoch 5 | Step 3175900 | Avg Loss: 0.0154 | Grad Norm: 0.00950167\n",
      "Epoch 5 | Step 3176000 | Avg Loss: 0.0157 | Grad Norm: 0.01139817\n",
      "Epoch 5 | Step 3176100 | Avg Loss: 0.0157 | Grad Norm: 0.01252050\n",
      "Epoch 5 | Step 3176200 | Avg Loss: 0.0157 | Grad Norm: 0.00928783\n",
      "Epoch 5 | Step 3176300 | Avg Loss: 0.0156 | Grad Norm: 0.00924479\n",
      "Epoch 5 | Step 3176400 | Avg Loss: 0.0156 | Grad Norm: 0.00920899\n",
      "Epoch 5 | Step 3176500 | Avg Loss: 0.0156 | Grad Norm: 0.00928382\n",
      "Epoch 5 | Step 3176600 | Avg Loss: 0.0158 | Grad Norm: 0.00978918\n",
      "Epoch 5 | Step 3176700 | Avg Loss: 0.0155 | Grad Norm: 0.01047892\n",
      "Epoch 5 | Step 3176800 | Avg Loss: 0.0155 | Grad Norm: 0.01003042\n",
      "Epoch 5 | Step 3176900 | Avg Loss: 0.0160 | Grad Norm: 0.00904184\n",
      "Epoch 5 | Step 3177000 | Avg Loss: 0.0157 | Grad Norm: 0.00791105\n",
      "Epoch 5 | Step 3177100 | Avg Loss: 0.0157 | Grad Norm: 0.00827109\n",
      "Epoch 5 | Step 3177200 | Avg Loss: 0.0159 | Grad Norm: 0.00851365\n",
      "Epoch 5 | Step 3177300 | Avg Loss: 0.0156 | Grad Norm: 0.00877294\n",
      "Epoch 5 | Step 3177400 | Avg Loss: 0.0153 | Grad Norm: 0.00899696\n",
      "Epoch 5 | Step 3177500 | Avg Loss: 0.0152 | Grad Norm: 0.00911960\n",
      "Epoch 5 | Step 3177600 | Avg Loss: 0.0153 | Grad Norm: 0.00844962\n",
      "Epoch 5 | Step 3177700 | Avg Loss: 0.0155 | Grad Norm: 0.00923157\n",
      "Epoch 5 | Step 3177800 | Avg Loss: 0.0151 | Grad Norm: 0.00763922\n",
      "Epoch 5 | Step 3177900 | Avg Loss: 0.0150 | Grad Norm: 0.00900869\n",
      "Epoch 5 | Step 3178000 | Avg Loss: 0.0151 | Grad Norm: 0.00864078\n",
      "Epoch 5 | Step 3178100 | Avg Loss: 0.0151 | Grad Norm: 0.00842450\n",
      "Epoch 5 | Step 3178200 | Avg Loss: 0.0151 | Grad Norm: 0.00966703\n",
      "Epoch 5 | Step 3178300 | Avg Loss: 0.0152 | Grad Norm: 0.00863371\n",
      "Epoch 5 | Step 3178400 | Avg Loss: 0.0153 | Grad Norm: 0.00941869\n",
      "Epoch 5 | Step 3178500 | Avg Loss: 0.0156 | Grad Norm: 0.00870532\n",
      "Epoch 5 | Step 3178600 | Avg Loss: 0.0154 | Grad Norm: 0.00939711\n",
      "Epoch 5 | Step 3178700 | Avg Loss: 0.0154 | Grad Norm: 0.00888909\n",
      "Epoch 5 | Step 3178800 | Avg Loss: 0.0155 | Grad Norm: 0.00794057\n",
      "Epoch 5 | Step 3178900 | Avg Loss: 0.0158 | Grad Norm: 0.00978807\n",
      "Epoch 5 | Step 3179000 | Avg Loss: 0.0160 | Grad Norm: 0.00930004\n",
      "Epoch 5 | Step 3179100 | Avg Loss: 0.0160 | Grad Norm: 0.00842226\n",
      "Epoch 5 | Step 3179200 | Avg Loss: 0.0160 | Grad Norm: 0.00892112\n",
      "Epoch 5 | Step 3179300 | Avg Loss: 0.0161 | Grad Norm: 0.00862878\n",
      "Epoch 5 | Step 3179400 | Avg Loss: 0.0159 | Grad Norm: 0.00862848\n",
      "Epoch 5 | Step 3179500 | Avg Loss: 0.0160 | Grad Norm: 0.01029272\n",
      "Epoch 5 | Step 3179600 | Avg Loss: 0.0160 | Grad Norm: 0.00877691\n",
      "Epoch 5 | Step 3179700 | Avg Loss: 0.0159 | Grad Norm: 0.01294571\n",
      "Epoch 5 | Step 3179800 | Avg Loss: 0.0161 | Grad Norm: 0.00814330\n",
      "Epoch 5 | Step 3179900 | Avg Loss: 0.0160 | Grad Norm: 0.01010899\n",
      "Epoch 5 | Step 3180000 | Avg Loss: 0.0161 | Grad Norm: 0.00994327\n",
      "Epoch 5 | Step 3180100 | Avg Loss: 0.0162 | Grad Norm: 0.00947175\n",
      "Epoch 5 | Step 3180200 | Avg Loss: 0.0161 | Grad Norm: 0.00802499\n",
      "Epoch 5 | Step 3180300 | Avg Loss: 0.0160 | Grad Norm: 0.01262963\n",
      "Epoch 5 | Step 3180400 | Avg Loss: 0.0156 | Grad Norm: 0.01094938\n",
      "Epoch 5 | Step 3180500 | Avg Loss: 0.0158 | Grad Norm: 0.00906353\n",
      "Epoch 5 | Step 3180600 | Avg Loss: 0.0153 | Grad Norm: 0.00806313\n",
      "Epoch 5 | Step 3180700 | Avg Loss: 0.0157 | Grad Norm: 0.00885518\n",
      "Epoch 5 | Step 3180800 | Avg Loss: 0.0157 | Grad Norm: 0.00835708\n",
      "Epoch 5 | Step 3180900 | Avg Loss: 0.0157 | Grad Norm: 0.00834948\n",
      "Epoch 5 | Step 3181000 | Avg Loss: 0.0158 | Grad Norm: 0.00784337\n",
      "Epoch 5 | Step 3181100 | Avg Loss: 0.0159 | Grad Norm: 0.00878613\n",
      "Epoch 5 | Step 3181200 | Avg Loss: 0.0162 | Grad Norm: 0.00995160\n",
      "Epoch 5 | Step 3181300 | Avg Loss: 0.0161 | Grad Norm: 0.00885167\n",
      "Epoch 5 | Step 3181400 | Avg Loss: 0.0161 | Grad Norm: 0.01002091\n",
      "Epoch 5 | Step 3181500 | Avg Loss: 0.0161 | Grad Norm: 0.01003411\n",
      "Epoch 5 | Step 3181600 | Avg Loss: 0.0159 | Grad Norm: 0.01038378\n",
      "Epoch 5 | Step 3181700 | Avg Loss: 0.0158 | Grad Norm: 0.01038585\n",
      "Epoch 5 | Step 3181800 | Avg Loss: 0.0166 | Grad Norm: 0.01074743\n",
      "Epoch 5 | Step 3181900 | Avg Loss: 0.0167 | Grad Norm: 0.00888462\n",
      "Epoch 5 | Step 3182000 | Avg Loss: 0.0163 | Grad Norm: 0.00891579\n",
      "Epoch 5 | Step 3182100 | Avg Loss: 0.0158 | Grad Norm: 0.00982400\n",
      "Epoch 5 | Step 3182200 | Avg Loss: 0.0157 | Grad Norm: 0.00852169\n",
      "Epoch 5 | Step 3182300 | Avg Loss: 0.0155 | Grad Norm: 0.01032253\n",
      "Epoch 5 | Step 3182400 | Avg Loss: 0.0155 | Grad Norm: 0.00958752\n",
      "Epoch 5 | Step 3182500 | Avg Loss: 0.0154 | Grad Norm: 0.00930578\n",
      "Epoch 5 | Step 3182600 | Avg Loss: 0.0152 | Grad Norm: 0.00879338\n",
      "Epoch 5 | Step 3182700 | Avg Loss: 0.0155 | Grad Norm: 0.00905474\n",
      "Epoch 5 | Step 3182800 | Avg Loss: 0.0153 | Grad Norm: 0.00897016\n",
      "Epoch 5 | Step 3182900 | Avg Loss: 0.0151 | Grad Norm: 0.00902147\n",
      "Epoch 5 | Step 3183000 | Avg Loss: 0.0150 | Grad Norm: 0.00849077\n",
      "Epoch 5 | Step 3183100 | Avg Loss: 0.0152 | Grad Norm: 0.01034495\n",
      "Epoch 5 | Step 3183200 | Avg Loss: 0.0155 | Grad Norm: 0.00957430\n",
      "Epoch 5 | Step 3183300 | Avg Loss: 0.0155 | Grad Norm: 0.00790837\n",
      "Epoch 5 | Step 3183400 | Avg Loss: 0.0157 | Grad Norm: 0.00900119\n",
      "Epoch 5 | Step 3183500 | Avg Loss: 0.0159 | Grad Norm: 0.01301184\n",
      "Epoch 5 | Step 3183600 | Avg Loss: 0.0155 | Grad Norm: 0.01353375\n",
      "Epoch 5 | Step 3183700 | Avg Loss: 0.0153 | Grad Norm: 0.00957520\n",
      "Epoch 5 | Step 3183800 | Avg Loss: 0.0155 | Grad Norm: 0.00840486\n",
      "Epoch 5 | Step 3183900 | Avg Loss: 0.0156 | Grad Norm: 0.00818591\n",
      "Epoch 5 | Step 3184000 | Avg Loss: 0.0154 | Grad Norm: 0.01020098\n",
      "Epoch 5 | Step 3184100 | Avg Loss: 0.0154 | Grad Norm: 0.00854775\n",
      "Epoch 5 | Step 3184200 | Avg Loss: 0.0157 | Grad Norm: 0.00886382\n",
      "Epoch 5 | Step 3184300 | Avg Loss: 0.0155 | Grad Norm: 0.00911844\n",
      "Epoch 5 | Step 3184400 | Avg Loss: 0.0153 | Grad Norm: 0.00764685\n",
      "Epoch 5 | Step 3184500 | Avg Loss: 0.0152 | Grad Norm: 0.00862842\n",
      "Epoch 5 | Step 3184600 | Avg Loss: 0.0153 | Grad Norm: 0.01160396\n",
      "Epoch 5 | Step 3184700 | Avg Loss: 0.0151 | Grad Norm: 0.00870059\n",
      "Epoch 5 | Step 3184800 | Avg Loss: 0.0150 | Grad Norm: 0.00837605\n",
      "Epoch 5 | Step 3184900 | Avg Loss: 0.0151 | Grad Norm: 0.00808605\n",
      "Epoch 5 | Step 3185000 | Avg Loss: 0.0155 | Grad Norm: 0.00878759\n",
      "Epoch 5 | Step 3185100 | Avg Loss: 0.0156 | Grad Norm: 0.01068526\n",
      "Epoch 5 | Step 3185200 | Avg Loss: 0.0158 | Grad Norm: 0.01133201\n",
      "Epoch 5 | Step 3185300 | Avg Loss: 0.0154 | Grad Norm: 0.00962694\n",
      "Epoch 5 | Step 3185400 | Avg Loss: 0.0154 | Grad Norm: 0.00921529\n",
      "Epoch 5 | Step 3185500 | Avg Loss: 0.0156 | Grad Norm: 0.01134493\n",
      "Epoch 5 | Step 3185600 | Avg Loss: 0.0154 | Grad Norm: 0.00925148\n",
      "Epoch 5 | Step 3185700 | Avg Loss: 0.0152 | Grad Norm: 0.00938279\n",
      "Epoch 5 | Step 3185800 | Avg Loss: 0.0153 | Grad Norm: 0.00991961\n",
      "Epoch 5 | Step 3185900 | Avg Loss: 0.0154 | Grad Norm: 0.00811481\n",
      "Epoch 5 | Step 3186000 | Avg Loss: 0.0153 | Grad Norm: 0.00890860\n",
      "Epoch 5 | Step 3186100 | Avg Loss: 0.0154 | Grad Norm: 0.00930013\n",
      "Epoch 5 | Step 3186200 | Avg Loss: 0.0154 | Grad Norm: 0.00894485\n",
      "Epoch 5 | Step 3186300 | Avg Loss: 0.0154 | Grad Norm: 0.00952648\n",
      "Epoch 5 | Step 3186400 | Avg Loss: 0.0153 | Grad Norm: 0.00876132\n",
      "Epoch 5 | Step 3186500 | Avg Loss: 0.0155 | Grad Norm: 0.00962087\n",
      "Epoch 5 | Step 3186600 | Avg Loss: 0.0154 | Grad Norm: 0.00938116\n",
      "Epoch 5 | Step 3186700 | Avg Loss: 0.0153 | Grad Norm: 0.00873534\n",
      "Epoch 5 | Step 3186800 | Avg Loss: 0.0152 | Grad Norm: 0.00869873\n",
      "Epoch 5 | Step 3186900 | Avg Loss: 0.0156 | Grad Norm: 0.00827051\n",
      "Epoch 5 | Step 3187000 | Avg Loss: 0.0159 | Grad Norm: 0.00883602\n",
      "Epoch 5 | Step 3187100 | Avg Loss: 0.0162 | Grad Norm: 0.01009004\n",
      "Epoch 5 | Step 3187200 | Avg Loss: 0.0160 | Grad Norm: 0.00901162\n",
      "Epoch 5 | Step 3187300 | Avg Loss: 0.0155 | Grad Norm: 0.00843474\n",
      "Epoch 5 | Step 3187400 | Avg Loss: 0.0150 | Grad Norm: 0.00812760\n",
      "Epoch 5 | Step 3187500 | Avg Loss: 0.0150 | Grad Norm: 0.00776584\n",
      "Epoch 5 | Step 3187600 | Avg Loss: 0.0152 | Grad Norm: 0.00937678\n",
      "Epoch 5 | Step 3187700 | Avg Loss: 0.0155 | Grad Norm: 0.01017000\n",
      "Epoch 5 | Step 3187800 | Avg Loss: 0.0157 | Grad Norm: 0.00893661\n",
      "Epoch 5 | Step 3187900 | Avg Loss: 0.0154 | Grad Norm: 0.00957316\n",
      "Epoch 5 | Step 3188000 | Avg Loss: 0.0157 | Grad Norm: 0.00890908\n",
      "Epoch 5 | Step 3188100 | Avg Loss: 0.0160 | Grad Norm: 0.01073750\n",
      "Epoch 5 | Step 3188200 | Avg Loss: 0.0157 | Grad Norm: 0.00828716\n",
      "Epoch 5 | Step 3188300 | Avg Loss: 0.0156 | Grad Norm: 0.00824213\n",
      "Epoch 5 | Step 3188400 | Avg Loss: 0.0156 | Grad Norm: 0.01008546\n",
      "Epoch 5 | Step 3188500 | Avg Loss: 0.0152 | Grad Norm: 0.00989578\n",
      "Epoch 5 | Step 3188600 | Avg Loss: 0.0146 | Grad Norm: 0.00909168\n",
      "Epoch 5 | Step 3188700 | Avg Loss: 0.0147 | Grad Norm: 0.00930294\n",
      "Epoch 5 | Step 3188800 | Avg Loss: 0.0148 | Grad Norm: 0.00884007\n",
      "Epoch 5 | Step 3188900 | Avg Loss: 0.0148 | Grad Norm: 0.00900790\n",
      "Epoch 5 | Step 3189000 | Avg Loss: 0.0149 | Grad Norm: 0.00837440\n",
      "Epoch 5 | Step 3189100 | Avg Loss: 0.0150 | Grad Norm: 0.00915382\n",
      "Epoch 5 | Step 3189200 | Avg Loss: 0.0150 | Grad Norm: 0.01109214\n",
      "Epoch 5 | Step 3189300 | Avg Loss: 0.0152 | Grad Norm: 0.01006188\n",
      "Epoch 5 | Step 3189400 | Avg Loss: 0.0155 | Grad Norm: 0.00946356\n",
      "Epoch 5 | Step 3189500 | Avg Loss: 0.0157 | Grad Norm: 0.00909321\n",
      "Epoch 5 | Step 3189600 | Avg Loss: 0.0158 | Grad Norm: 0.01008335\n",
      "Epoch 5 | Step 3189700 | Avg Loss: 0.0159 | Grad Norm: 0.00943772\n",
      "Epoch 5 | Step 3189800 | Avg Loss: 0.0156 | Grad Norm: 0.00982017\n",
      "Epoch 5 | Step 3189900 | Avg Loss: 0.0155 | Grad Norm: 0.00872382\n",
      "Epoch 5 | Step 3190000 | Avg Loss: 0.0153 | Grad Norm: 0.01025369\n",
      "Epoch 5 | Step 3190100 | Avg Loss: 0.0153 | Grad Norm: 0.00821787\n",
      "Epoch 5 | Step 3190200 | Avg Loss: 0.0153 | Grad Norm: 0.00936742\n",
      "Epoch 5 | Step 3190300 | Avg Loss: 0.0152 | Grad Norm: 0.00949706\n",
      "Epoch 5 | Step 3190400 | Avg Loss: 0.0148 | Grad Norm: 0.00797521\n",
      "Epoch 5 | Step 3190500 | Avg Loss: 0.0148 | Grad Norm: 0.00891526\n",
      "Epoch 5 | Step 3190600 | Avg Loss: 0.0150 | Grad Norm: 0.00978756\n",
      "Epoch 5 | Step 3190700 | Avg Loss: 0.0150 | Grad Norm: 0.00955810\n",
      "Epoch 5 | Step 3190800 | Avg Loss: 0.0149 | Grad Norm: 0.00748720\n",
      "Epoch 5 | Step 3190900 | Avg Loss: 0.0149 | Grad Norm: 0.00787012\n",
      "Epoch 5 | Step 3191000 | Avg Loss: 0.0151 | Grad Norm: 0.00845024\n",
      "Epoch 5 | Step 3191100 | Avg Loss: 0.0153 | Grad Norm: 0.00819313\n",
      "Epoch 5 | Step 3191200 | Avg Loss: 0.0152 | Grad Norm: 0.00756750\n",
      "Epoch 5 | Step 3191300 | Avg Loss: 0.0152 | Grad Norm: 0.00952837\n",
      "Epoch 5 | Step 3191400 | Avg Loss: 0.0152 | Grad Norm: 0.00950977\n",
      "Epoch 5 | Step 3191500 | Avg Loss: 0.0159 | Grad Norm: 0.00985886\n",
      "Epoch 5 | Step 3191600 | Avg Loss: 0.0159 | Grad Norm: 0.00889968\n",
      "Epoch 5 | Step 3191700 | Avg Loss: 0.0156 | Grad Norm: 0.00907459\n",
      "Epoch 5 | Step 3191800 | Avg Loss: 0.0149 | Grad Norm: 0.00868131\n",
      "Epoch 5 | Step 3191900 | Avg Loss: 0.0150 | Grad Norm: 0.00881251\n",
      "Epoch 5 | Step 3192000 | Avg Loss: 0.0155 | Grad Norm: 0.00981492\n",
      "Epoch 5 | Step 3192100 | Avg Loss: 0.0154 | Grad Norm: 0.00882281\n",
      "Epoch 5 | Step 3192200 | Avg Loss: 0.0154 | Grad Norm: 0.00837835\n",
      "Epoch 5 | Step 3192300 | Avg Loss: 0.0152 | Grad Norm: 0.01050244\n",
      "Epoch 5 | Step 3192400 | Avg Loss: 0.0153 | Grad Norm: 0.01023216\n",
      "Epoch 5 | Step 3192500 | Avg Loss: 0.0154 | Grad Norm: 0.00864864\n",
      "Epoch 5 | Step 3192600 | Avg Loss: 0.0158 | Grad Norm: 0.01058484\n",
      "Epoch 5 | Step 3192700 | Avg Loss: 0.0159 | Grad Norm: 0.00845736\n",
      "Epoch 5 | Step 3192800 | Avg Loss: 0.0159 | Grad Norm: 0.00961005\n",
      "Epoch 5 | Step 3192900 | Avg Loss: 0.0156 | Grad Norm: 0.01013663\n",
      "Epoch 5 | Step 3193000 | Avg Loss: 0.0152 | Grad Norm: 0.00997898\n",
      "Epoch 5 | Step 3193100 | Avg Loss: 0.0154 | Grad Norm: 0.00844755\n",
      "Epoch 5 | Step 3193200 | Avg Loss: 0.0155 | Grad Norm: 0.00962948\n",
      "Epoch 5 | Step 3193300 | Avg Loss: 0.0154 | Grad Norm: 0.00874858\n",
      "Epoch 5 | Step 3193400 | Avg Loss: 0.0155 | Grad Norm: 0.00885928\n",
      "Epoch 5 | Step 3193500 | Avg Loss: 0.0154 | Grad Norm: 0.00879478\n",
      "Epoch 5 | Step 3193600 | Avg Loss: 0.0156 | Grad Norm: 0.00791480\n",
      "Epoch 5 | Step 3193700 | Avg Loss: 0.0152 | Grad Norm: 0.00952395\n",
      "Epoch 5 | Step 3193800 | Avg Loss: 0.0155 | Grad Norm: 0.00886745\n",
      "Epoch 5 | Step 3193900 | Avg Loss: 0.0158 | Grad Norm: 0.00917983\n",
      "Epoch 5 | Step 3194000 | Avg Loss: 0.0159 | Grad Norm: 0.01085592\n",
      "Epoch 5 | Step 3194100 | Avg Loss: 0.0161 | Grad Norm: 0.00870386\n",
      "Epoch 5 | Step 3194200 | Avg Loss: 0.0155 | Grad Norm: 0.00840268\n",
      "Epoch 5 | Step 3194300 | Avg Loss: 0.0155 | Grad Norm: 0.01016476\n",
      "Epoch 5 | Step 3194400 | Avg Loss: 0.0153 | Grad Norm: 0.00889197\n",
      "Epoch 5 | Step 3194500 | Avg Loss: 0.0155 | Grad Norm: 0.00795347\n",
      "Epoch 5 | Step 3194600 | Avg Loss: 0.0155 | Grad Norm: 0.00886006\n",
      "Epoch 5 | Step 3194700 | Avg Loss: 0.0153 | Grad Norm: 0.00869074\n",
      "Epoch 5 | Step 3194800 | Avg Loss: 0.0153 | Grad Norm: 0.01022381\n",
      "Epoch 5 | Step 3194900 | Avg Loss: 0.0157 | Grad Norm: 0.00832453\n",
      "Epoch 5 | Step 3195000 | Avg Loss: 0.0158 | Grad Norm: 0.00937864\n",
      "Epoch 5 | Step 3195100 | Avg Loss: 0.0161 | Grad Norm: 0.01092327\n",
      "Epoch 5 | Step 3195200 | Avg Loss: 0.0156 | Grad Norm: 0.00918378\n",
      "Epoch 5 | Step 3195300 | Avg Loss: 0.0155 | Grad Norm: 0.00958096\n",
      "Epoch 5 | Step 3195400 | Avg Loss: 0.0152 | Grad Norm: 0.00905074\n",
      "Epoch 5 | Step 3195500 | Avg Loss: 0.0155 | Grad Norm: 0.00902653\n",
      "Epoch 5 | Step 3195600 | Avg Loss: 0.0157 | Grad Norm: 0.00888900\n",
      "Epoch 5 | Step 3195700 | Avg Loss: 0.0154 | Grad Norm: 0.01047695\n",
      "Epoch 5 | Step 3195800 | Avg Loss: 0.0149 | Grad Norm: 0.00839409\n",
      "Epoch 5 | Step 3195900 | Avg Loss: 0.0151 | Grad Norm: 0.00892976\n",
      "Epoch 5 | Step 3196000 | Avg Loss: 0.0155 | Grad Norm: 0.00838340\n",
      "Epoch 5 | Step 3196100 | Avg Loss: 0.0158 | Grad Norm: 0.00804848\n",
      "Epoch 5 | Step 3196200 | Avg Loss: 0.0159 | Grad Norm: 0.00910485\n",
      "Epoch 5 | Step 3196300 | Avg Loss: 0.0154 | Grad Norm: 0.00891877\n",
      "Epoch 5 | Step 3196400 | Avg Loss: 0.0155 | Grad Norm: 0.00936180\n",
      "Epoch 5 | Step 3196500 | Avg Loss: 0.0152 | Grad Norm: 0.00943933\n",
      "Epoch 5 | Step 3196600 | Avg Loss: 0.0159 | Grad Norm: 0.01084590\n",
      "Epoch 5 | Step 3196700 | Avg Loss: 0.0155 | Grad Norm: 0.00873411\n",
      "Epoch 5 | Step 3196800 | Avg Loss: 0.0154 | Grad Norm: 0.00819568\n",
      "Epoch 5 | Step 3196900 | Avg Loss: 0.0154 | Grad Norm: 0.00852471\n",
      "Epoch 5 | Step 3197000 | Avg Loss: 0.0155 | Grad Norm: 0.01051075\n",
      "Epoch 5 | Step 3197100 | Avg Loss: 0.0155 | Grad Norm: 0.00999980\n",
      "Epoch 5 | Step 3197200 | Avg Loss: 0.0154 | Grad Norm: 0.00839522\n",
      "Epoch 5 | Step 3197300 | Avg Loss: 0.0154 | Grad Norm: 0.00757908\n",
      "Epoch 5 | Step 3197400 | Avg Loss: 0.0152 | Grad Norm: 0.00871072\n",
      "Epoch 5 | Step 3197500 | Avg Loss: 0.0156 | Grad Norm: 0.00849680\n",
      "Epoch 5 | Step 3197600 | Avg Loss: 0.0157 | Grad Norm: 0.00936729\n",
      "Epoch 5 | Step 3197700 | Avg Loss: 0.0155 | Grad Norm: 0.00966937\n",
      "Epoch 5 | Step 3197800 | Avg Loss: 0.0152 | Grad Norm: 0.00936682\n",
      "Epoch 5 | Step 3197900 | Avg Loss: 0.0156 | Grad Norm: 0.00930402\n",
      "Epoch 5 | Step 3198000 | Avg Loss: 0.0163 | Grad Norm: 0.01123401\n",
      "Epoch 5 | Step 3198100 | Avg Loss: 0.0159 | Grad Norm: 0.00946389\n",
      "Epoch 5 | Step 3198200 | Avg Loss: 0.0161 | Grad Norm: 0.00888514\n",
      "Epoch 5 | Step 3198300 | Avg Loss: 0.0158 | Grad Norm: 0.01017957\n",
      "Epoch 5 | Step 3198400 | Avg Loss: 0.0154 | Grad Norm: 0.00932549\n",
      "Epoch 5 | Step 3198500 | Avg Loss: 0.0152 | Grad Norm: 0.00760592\n",
      "Epoch 5 | Step 3198600 | Avg Loss: 0.0153 | Grad Norm: 0.00937189\n",
      "Epoch 5 | Step 3198700 | Avg Loss: 0.0154 | Grad Norm: 0.00989160\n",
      "Epoch 5 | Step 3198800 | Avg Loss: 0.0155 | Grad Norm: 0.01049211\n",
      "Epoch 5 | Step 3198900 | Avg Loss: 0.0152 | Grad Norm: 0.00881026\n",
      "Epoch 5 | Step 3199000 | Avg Loss: 0.0150 | Grad Norm: 0.00824585\n",
      "Epoch 5 | Step 3199100 | Avg Loss: 0.0153 | Grad Norm: 0.01093317\n",
      "Epoch 5 | Step 3199200 | Avg Loss: 0.0152 | Grad Norm: 0.00833336\n",
      "Epoch 5 | Step 3199300 | Avg Loss: 0.0151 | Grad Norm: 0.00912194\n",
      "Epoch 5 | Step 3199400 | Avg Loss: 0.0154 | Grad Norm: 0.01227444\n",
      "Epoch 5 | Step 3199500 | Avg Loss: 0.0157 | Grad Norm: 0.00910936\n",
      "Epoch 5 | Step 3199600 | Avg Loss: 0.0157 | Grad Norm: 0.00853147\n",
      "Epoch 5 | Step 3199700 | Avg Loss: 0.0157 | Grad Norm: 0.00883456\n",
      "Epoch 5 | Step 3199800 | Avg Loss: 0.0155 | Grad Norm: 0.00927104\n",
      "Epoch 5 | Step 3199900 | Avg Loss: 0.0159 | Grad Norm: 0.00942426\n",
      "Epoch 5 | Step 3200000 | Avg Loss: 0.0159 | Grad Norm: 0.01154407\n",
      "Saving model at step3200000\n",
      "Epoch 5 | Step 3200100 | Avg Loss: 0.0158 | Grad Norm: 0.00868756\n",
      "Epoch 5 | Step 3200200 | Avg Loss: 0.0155 | Grad Norm: 0.00831582\n",
      "Epoch 5 | Step 3200300 | Avg Loss: 0.0154 | Grad Norm: 0.01514969\n",
      "Epoch 5 | Step 3200400 | Avg Loss: 0.0154 | Grad Norm: 0.00882105\n",
      "Epoch 5 | Step 3200500 | Avg Loss: 0.0151 | Grad Norm: 0.00818809\n",
      "Epoch 5 | Step 3200600 | Avg Loss: 0.0152 | Grad Norm: 0.00898262\n",
      "Epoch 5 | Step 3200700 | Avg Loss: 0.0151 | Grad Norm: 0.00912542\n",
      "Epoch 5 | Step 3200800 | Avg Loss: 0.0155 | Grad Norm: 0.00945511\n",
      "Epoch 5 | Step 3200900 | Avg Loss: 0.0158 | Grad Norm: 0.00892999\n",
      "Epoch 5 | Step 3201000 | Avg Loss: 0.0157 | Grad Norm: 0.00894551\n",
      "Epoch 5 | Step 3201100 | Avg Loss: 0.0156 | Grad Norm: 0.00860023\n",
      "Epoch 5 | Step 3201200 | Avg Loss: 0.0156 | Grad Norm: 0.00826804\n",
      "Epoch 5 | Step 3201300 | Avg Loss: 0.0160 | Grad Norm: 0.00909186\n",
      "Epoch 5 | Step 3201400 | Avg Loss: 0.0156 | Grad Norm: 0.00895879\n",
      "Epoch 5 | Step 3201500 | Avg Loss: 0.0154 | Grad Norm: 0.01314270\n",
      "Epoch 5 | Step 3201600 | Avg Loss: 0.0154 | Grad Norm: 0.00884870\n",
      "Epoch 5 | Step 3201700 | Avg Loss: 0.0153 | Grad Norm: 0.00890207\n",
      "Epoch 5 | Step 3201800 | Avg Loss: 0.0152 | Grad Norm: 0.00928602\n",
      "Epoch 5 | Step 3201900 | Avg Loss: 0.0157 | Grad Norm: 0.00865996\n",
      "Epoch 5 | Step 3202000 | Avg Loss: 0.0158 | Grad Norm: 0.00909101\n",
      "Epoch 5 | Step 3202100 | Avg Loss: 0.0157 | Grad Norm: 0.01120209\n",
      "Epoch 5 | Step 3202200 | Avg Loss: 0.0155 | Grad Norm: 0.01294293\n",
      "Epoch 5 | Step 3202300 | Avg Loss: 0.0158 | Grad Norm: 0.00883956\n",
      "Epoch 5 | Step 3202400 | Avg Loss: 0.0157 | Grad Norm: 0.00951902\n",
      "Epoch 5 | Step 3202500 | Avg Loss: 0.0158 | Grad Norm: 0.00794741\n",
      "Epoch 5 | Step 3202600 | Avg Loss: 0.0160 | Grad Norm: 0.00843229\n",
      "Epoch 5 | Step 3202700 | Avg Loss: 0.0162 | Grad Norm: 0.00909427\n",
      "Epoch 5 | Step 3202800 | Avg Loss: 0.0165 | Grad Norm: 0.00913296\n",
      "Epoch 5 | Step 3202900 | Avg Loss: 0.0161 | Grad Norm: 0.00937931\n",
      "Epoch 5 | Step 3203000 | Avg Loss: 0.0159 | Grad Norm: 0.00989484\n",
      "Epoch 5 | Step 3203100 | Avg Loss: 0.0159 | Grad Norm: 0.00813686\n",
      "Epoch 5 | Step 3203200 | Avg Loss: 0.0160 | Grad Norm: 0.00969591\n",
      "Epoch 5 | Step 3203300 | Avg Loss: 0.0161 | Grad Norm: 0.01105169\n",
      "Epoch 5 | Step 3203400 | Avg Loss: 0.0162 | Grad Norm: 0.00845993\n",
      "Epoch 5 | Step 3203500 | Avg Loss: 0.0158 | Grad Norm: 0.00923647\n",
      "Epoch 5 | Step 3203600 | Avg Loss: 0.0160 | Grad Norm: 0.00949541\n",
      "Epoch 5 | Step 3203700 | Avg Loss: 0.0161 | Grad Norm: 0.00944237\n",
      "Epoch 5 | Step 3203800 | Avg Loss: 0.0161 | Grad Norm: 0.00992026\n",
      "Epoch 5 | Step 3203900 | Avg Loss: 0.0161 | Grad Norm: 0.00856933\n",
      "Epoch 5 | Step 3204000 | Avg Loss: 0.0161 | Grad Norm: 0.00932056\n",
      "Epoch 5 | Step 3204100 | Avg Loss: 0.0158 | Grad Norm: 0.00973253\n",
      "Epoch 5 | Step 3204200 | Avg Loss: 0.0157 | Grad Norm: 0.00834133\n",
      "Epoch 5 | Step 3204300 | Avg Loss: 0.0156 | Grad Norm: 0.01140821\n",
      "Epoch 5 | Step 3204400 | Avg Loss: 0.0156 | Grad Norm: 0.00871214\n",
      "Epoch 5 | Step 3204500 | Avg Loss: 0.0154 | Grad Norm: 0.01135042\n",
      "Epoch 5 | Step 3204600 | Avg Loss: 0.0158 | Grad Norm: 0.00912782\n",
      "Epoch 5 | Step 3204700 | Avg Loss: 0.0159 | Grad Norm: 0.00981748\n",
      "Epoch 5 | Step 3204800 | Avg Loss: 0.0161 | Grad Norm: 0.00945893\n",
      "Epoch 5 | Step 3204900 | Avg Loss: 0.0161 | Grad Norm: 0.01049985\n",
      "Epoch 5 | Step 3205000 | Avg Loss: 0.0162 | Grad Norm: 0.00991178\n",
      "Epoch 5 | Step 3205100 | Avg Loss: 0.0158 | Grad Norm: 0.00874366\n",
      "Epoch 5 | Step 3205200 | Avg Loss: 0.0155 | Grad Norm: 0.00786984\n",
      "Epoch 5 | Step 3205300 | Avg Loss: 0.0157 | Grad Norm: 0.00883176\n",
      "Epoch 5 | Step 3205400 | Avg Loss: 0.0157 | Grad Norm: 0.01070031\n",
      "Epoch 5 | Step 3205500 | Avg Loss: 0.0156 | Grad Norm: 0.00908808\n",
      "Epoch 5 | Step 3205600 | Avg Loss: 0.0157 | Grad Norm: 0.01003449\n",
      "Epoch 5 | Step 3205700 | Avg Loss: 0.0157 | Grad Norm: 0.01047502\n",
      "Epoch 5 | Step 3205800 | Avg Loss: 0.0153 | Grad Norm: 0.00976907\n",
      "Epoch 5 | Step 3205900 | Avg Loss: 0.0151 | Grad Norm: 0.00914639\n",
      "Epoch 5 | Step 3206000 | Avg Loss: 0.0155 | Grad Norm: 0.00811748\n",
      "Epoch 5 | Step 3206100 | Avg Loss: 0.0151 | Grad Norm: 0.00892896\n",
      "Epoch 5 | Step 3206200 | Avg Loss: 0.0150 | Grad Norm: 0.00879895\n",
      "Epoch 5 | Step 3206300 | Avg Loss: 0.0149 | Grad Norm: 0.00959341\n",
      "Epoch 5 | Step 3206400 | Avg Loss: 0.0150 | Grad Norm: 0.00870799\n",
      "Epoch 5 | Step 3206500 | Avg Loss: 0.0154 | Grad Norm: 0.00891445\n",
      "Epoch 5 | Step 3206600 | Avg Loss: 0.0152 | Grad Norm: 0.01187795\n",
      "Epoch 5 | Step 3206700 | Avg Loss: 0.0152 | Grad Norm: 0.00805218\n",
      "Epoch 5 | Step 3206800 | Avg Loss: 0.0156 | Grad Norm: 0.01080847\n",
      "Epoch 5 | Step 3206900 | Avg Loss: 0.0154 | Grad Norm: 0.00954700\n",
      "Epoch 5 | Step 3207000 | Avg Loss: 0.0155 | Grad Norm: 0.00931541\n",
      "Epoch 5 | Step 3207100 | Avg Loss: 0.0153 | Grad Norm: 0.00890952\n",
      "Epoch 5 | Step 3207200 | Avg Loss: 0.0151 | Grad Norm: 0.00878937\n",
      "Epoch 5 | Step 3207300 | Avg Loss: 0.0151 | Grad Norm: 0.00810793\n",
      "Epoch 5 | Step 3207400 | Avg Loss: 0.0154 | Grad Norm: 0.00943415\n",
      "Epoch 5 | Step 3207500 | Avg Loss: 0.0154 | Grad Norm: 0.01004063\n",
      "Epoch 5 | Step 3207600 | Avg Loss: 0.0149 | Grad Norm: 0.00880340\n",
      "Epoch 5 | Step 3207700 | Avg Loss: 0.0152 | Grad Norm: 0.00981468\n",
      "Epoch 5 | Step 3207800 | Avg Loss: 0.0154 | Grad Norm: 0.00817749\n",
      "Epoch 5 | Step 3207900 | Avg Loss: 0.0153 | Grad Norm: 0.00942133\n",
      "Epoch 5 | Step 3208000 | Avg Loss: 0.0157 | Grad Norm: 0.00886066\n",
      "Epoch 5 | Step 3208100 | Avg Loss: 0.0156 | Grad Norm: 0.00826167\n",
      "Epoch 5 | Step 3208200 | Avg Loss: 0.0154 | Grad Norm: 0.00849577\n",
      "Epoch 5 | Step 3208300 | Avg Loss: 0.0153 | Grad Norm: 0.00981780\n",
      "Epoch 5 | Step 3208400 | Avg Loss: 0.0156 | Grad Norm: 0.00910041\n",
      "Epoch 5 | Step 3208500 | Avg Loss: 0.0158 | Grad Norm: 0.00896373\n",
      "Epoch 5 | Step 3208600 | Avg Loss: 0.0156 | Grad Norm: 0.00839241\n",
      "Epoch 5 | Step 3208700 | Avg Loss: 0.0154 | Grad Norm: 0.00896400\n",
      "Epoch 5 | Step 3208800 | Avg Loss: 0.0160 | Grad Norm: 0.00817150\n",
      "Epoch 5 | Step 3208900 | Avg Loss: 0.0160 | Grad Norm: 0.00926030\n",
      "Epoch 5 | Step 3209000 | Avg Loss: 0.0156 | Grad Norm: 0.00822219\n",
      "Epoch 5 | Step 3209100 | Avg Loss: 0.0154 | Grad Norm: 0.01020626\n",
      "Epoch 5 | Step 3209200 | Avg Loss: 0.0154 | Grad Norm: 0.00857382\n",
      "Epoch 5 | Step 3209300 | Avg Loss: 0.0156 | Grad Norm: 0.00866538\n",
      "Epoch 5 | Step 3209400 | Avg Loss: 0.0162 | Grad Norm: 0.00957756\n",
      "Epoch 5 | Step 3209500 | Avg Loss: 0.0158 | Grad Norm: 0.00944142\n",
      "Epoch 5 | Step 3209600 | Avg Loss: 0.0157 | Grad Norm: 0.00913064\n",
      "Epoch 5 | Step 3209700 | Avg Loss: 0.0156 | Grad Norm: 0.00890700\n",
      "Epoch 5 | Step 3209800 | Avg Loss: 0.0156 | Grad Norm: 0.01287548\n",
      "Epoch 5 | Step 3209900 | Avg Loss: 0.0156 | Grad Norm: 0.01018313\n",
      "Epoch 5 | Step 3210000 | Avg Loss: 0.0153 | Grad Norm: 0.00835331\n",
      "Epoch 5 | Step 3210100 | Avg Loss: 0.0153 | Grad Norm: 0.00972556\n",
      "Epoch 5 | Step 3210200 | Avg Loss: 0.0155 | Grad Norm: 0.00885919\n",
      "Epoch 5 | Step 3210300 | Avg Loss: 0.0154 | Grad Norm: 0.00912664\n",
      "Epoch 5 | Step 3210400 | Avg Loss: 0.0151 | Grad Norm: 0.00916107\n",
      "Epoch 5 | Step 3210500 | Avg Loss: 0.0150 | Grad Norm: 0.01059993\n",
      "Epoch 5 | Step 3210600 | Avg Loss: 0.0149 | Grad Norm: 0.00874857\n",
      "Epoch 5 | Step 3210700 | Avg Loss: 0.0148 | Grad Norm: 0.00848485\n",
      "Epoch 5 | Step 3210800 | Avg Loss: 0.0150 | Grad Norm: 0.00891406\n",
      "Epoch 5 | Step 3210900 | Avg Loss: 0.0153 | Grad Norm: 0.00956521\n",
      "Epoch 5 | Step 3211000 | Avg Loss: 0.0152 | Grad Norm: 0.00871262\n",
      "Epoch 5 | Step 3211100 | Avg Loss: 0.0155 | Grad Norm: 0.00925319\n",
      "Epoch 5 | Step 3211200 | Avg Loss: 0.0155 | Grad Norm: 0.01006785\n",
      "Epoch 5 | Step 3211300 | Avg Loss: 0.0156 | Grad Norm: 0.00878880\n",
      "Epoch 5 | Step 3211400 | Avg Loss: 0.0156 | Grad Norm: 0.00801918\n",
      "Epoch 5 | Step 3211500 | Avg Loss: 0.0155 | Grad Norm: 0.00905315\n",
      "Epoch 5 | Step 3211600 | Avg Loss: 0.0155 | Grad Norm: 0.00833037\n",
      "Epoch 5 | Step 3211700 | Avg Loss: 0.0153 | Grad Norm: 0.00808924\n",
      "Epoch 5 | Step 3211800 | Avg Loss: 0.0156 | Grad Norm: 0.00809764\n",
      "Epoch 5 | Step 3211900 | Avg Loss: 0.0156 | Grad Norm: 0.00992156\n",
      "Epoch 5 | Step 3212000 | Avg Loss: 0.0155 | Grad Norm: 0.01200572\n",
      "Epoch 5 | Step 3212100 | Avg Loss: 0.0159 | Grad Norm: 0.01036158\n",
      "Epoch 5 | Step 3212200 | Avg Loss: 0.0159 | Grad Norm: 0.00886300\n",
      "Epoch 5 | Step 3212300 | Avg Loss: 0.0160 | Grad Norm: 0.00838336\n",
      "Epoch 5 | Step 3212400 | Avg Loss: 0.0160 | Grad Norm: 0.01019481\n",
      "Epoch 5 | Step 3212500 | Avg Loss: 0.0159 | Grad Norm: 0.00862728\n",
      "Epoch 5 | Step 3212600 | Avg Loss: 0.0160 | Grad Norm: 0.00937314\n",
      "Epoch 5 | Step 3212700 | Avg Loss: 0.0162 | Grad Norm: 0.00978681\n",
      "Epoch 5 | Step 3212800 | Avg Loss: 0.0162 | Grad Norm: 0.01020359\n",
      "Epoch 5 | Step 3212900 | Avg Loss: 0.0159 | Grad Norm: 0.00985011\n",
      "Epoch 5 | Step 3213000 | Avg Loss: 0.0157 | Grad Norm: 0.00888876\n",
      "Epoch 5 | Step 3213100 | Avg Loss: 0.0158 | Grad Norm: 0.00968448\n",
      "Epoch 5 | Step 3213200 | Avg Loss: 0.0157 | Grad Norm: 0.00988810\n",
      "Epoch 5 | Step 3213300 | Avg Loss: 0.0155 | Grad Norm: 0.00937431\n",
      "Epoch 5 | Step 3213400 | Avg Loss: 0.0154 | Grad Norm: 0.00844469\n",
      "Epoch 5 | Step 3213500 | Avg Loss: 0.0153 | Grad Norm: 0.01163235\n",
      "Epoch 5 | Step 3213600 | Avg Loss: 0.0150 | Grad Norm: 0.00901531\n",
      "Epoch 5 | Step 3213700 | Avg Loss: 0.0153 | Grad Norm: 0.00781455\n",
      "Epoch 5 | Step 3213800 | Avg Loss: 0.0153 | Grad Norm: 0.00814220\n",
      "Epoch 5 | Step 3213900 | Avg Loss: 0.0154 | Grad Norm: 0.00893773\n",
      "Epoch 5 | Step 3214000 | Avg Loss: 0.0156 | Grad Norm: 0.01103568\n",
      "Epoch 5 | Step 3214100 | Avg Loss: 0.0154 | Grad Norm: 0.00935902\n",
      "Epoch 5 | Step 3214200 | Avg Loss: 0.0158 | Grad Norm: 0.01003271\n",
      "Epoch 5 | Step 3214300 | Avg Loss: 0.0159 | Grad Norm: 0.00853550\n",
      "Epoch 5 | Step 3214400 | Avg Loss: 0.0158 | Grad Norm: 0.01031555\n",
      "Epoch 5 | Step 3214500 | Avg Loss: 0.0160 | Grad Norm: 0.00886769\n",
      "Epoch 5 | Step 3214600 | Avg Loss: 0.0159 | Grad Norm: 0.00779412\n",
      "Epoch 5 | Step 3214700 | Avg Loss: 0.0149 | Grad Norm: 0.00819496\n",
      "Epoch 5 | Step 3214800 | Avg Loss: 0.0147 | Grad Norm: 0.00947421\n",
      "Epoch 5 | Step 3214900 | Avg Loss: 0.0152 | Grad Norm: 0.00741044\n",
      "Epoch 5 | Step 3215000 | Avg Loss: 0.0156 | Grad Norm: 0.00828116\n",
      "Epoch 5 | Step 3215100 | Avg Loss: 0.0157 | Grad Norm: 0.00953467\n",
      "Epoch 5 | Step 3215200 | Avg Loss: 0.0157 | Grad Norm: 0.01090931\n",
      "Epoch 5 | Step 3215300 | Avg Loss: 0.0154 | Grad Norm: 0.00876307\n",
      "Epoch 5 | Step 3215400 | Avg Loss: 0.0159 | Grad Norm: 0.00892728\n",
      "Epoch 5 | Step 3215500 | Avg Loss: 0.0160 | Grad Norm: 0.00933623\n",
      "Epoch 5 | Step 3215600 | Avg Loss: 0.0160 | Grad Norm: 0.00855416\n",
      "Epoch 5 | Step 3215700 | Avg Loss: 0.0157 | Grad Norm: 0.00864516\n",
      "Epoch 5 | Step 3215800 | Avg Loss: 0.0155 | Grad Norm: 0.01027501\n",
      "Epoch 5 | Step 3215900 | Avg Loss: 0.0149 | Grad Norm: 0.00832129\n",
      "Epoch 5 | Step 3216000 | Avg Loss: 0.0149 | Grad Norm: 0.00867412\n",
      "Epoch 5 | Step 3216100 | Avg Loss: 0.0151 | Grad Norm: 0.01101719\n",
      "Epoch 5 | Step 3216200 | Avg Loss: 0.0155 | Grad Norm: 0.00957678\n",
      "Epoch 5 | Step 3216300 | Avg Loss: 0.0157 | Grad Norm: 0.01190288\n",
      "Epoch 5 | Step 3216400 | Avg Loss: 0.0157 | Grad Norm: 0.01043166\n",
      "Epoch 5 | Step 3216500 | Avg Loss: 0.0152 | Grad Norm: 0.00932507\n",
      "Epoch 5 | Step 3216600 | Avg Loss: 0.0154 | Grad Norm: 0.00917651\n",
      "Epoch 5 | Step 3216700 | Avg Loss: 0.0154 | Grad Norm: 0.00869323\n",
      "Epoch 5 | Step 3216800 | Avg Loss: 0.0150 | Grad Norm: 0.01107690\n",
      "Epoch 5 | Step 3216900 | Avg Loss: 0.0152 | Grad Norm: 0.00793041\n",
      "Epoch 5 | Step 3217000 | Avg Loss: 0.0156 | Grad Norm: 0.00992287\n",
      "Epoch 5 | Step 3217100 | Avg Loss: 0.0157 | Grad Norm: 0.00828646\n",
      "Epoch 5 | Step 3217200 | Avg Loss: 0.0154 | Grad Norm: 0.00999260\n",
      "Epoch 5 | Step 3217300 | Avg Loss: 0.0158 | Grad Norm: 0.00999218\n",
      "Epoch 5 | Step 3217400 | Avg Loss: 0.0154 | Grad Norm: 0.00852310\n",
      "Epoch 5 | Step 3217500 | Avg Loss: 0.0158 | Grad Norm: 0.00822178\n",
      "Epoch 5 | Step 3217600 | Avg Loss: 0.0154 | Grad Norm: 0.00850518\n",
      "Epoch 5 | Step 3217700 | Avg Loss: 0.0155 | Grad Norm: 0.00838932\n",
      "Epoch 5 | Step 3217800 | Avg Loss: 0.0154 | Grad Norm: 0.00884012\n",
      "Epoch 5 | Step 3217900 | Avg Loss: 0.0151 | Grad Norm: 0.01085371\n",
      "Epoch 5 | Step 3218000 | Avg Loss: 0.0152 | Grad Norm: 0.00893735\n",
      "Epoch 5 | Step 3218100 | Avg Loss: 0.0154 | Grad Norm: 0.00813497\n",
      "Epoch 5 | Step 3218200 | Avg Loss: 0.0152 | Grad Norm: 0.00922118\n",
      "Epoch 5 | Step 3218300 | Avg Loss: 0.0146 | Grad Norm: 0.00917330\n",
      "Epoch 5 | Step 3218400 | Avg Loss: 0.0148 | Grad Norm: 0.00850411\n",
      "Epoch 5 | Step 3218500 | Avg Loss: 0.0152 | Grad Norm: 0.00929952\n",
      "Epoch 5 | Step 3218600 | Avg Loss: 0.0152 | Grad Norm: 0.00924916\n",
      "Epoch 5 | Step 3218700 | Avg Loss: 0.0152 | Grad Norm: 0.00852350\n",
      "Epoch 5 | Step 3218800 | Avg Loss: 0.0155 | Grad Norm: 0.00938238\n",
      "Epoch 5 | Step 3218900 | Avg Loss: 0.0157 | Grad Norm: 0.00884562\n",
      "Epoch 5 | Step 3219000 | Avg Loss: 0.0157 | Grad Norm: 0.00998499\n",
      "Epoch 5 | Step 3219100 | Avg Loss: 0.0154 | Grad Norm: 0.01254911\n",
      "Epoch 5 | Step 3219200 | Avg Loss: 0.0157 | Grad Norm: 0.00905311\n",
      "Epoch 5 | Step 3219300 | Avg Loss: 0.0154 | Grad Norm: 0.01064660\n",
      "Epoch 5 | Step 3219400 | Avg Loss: 0.0154 | Grad Norm: 0.00938604\n",
      "Epoch 5 | Step 3219500 | Avg Loss: 0.0154 | Grad Norm: 0.00911320\n",
      "Epoch 5 | Step 3219600 | Avg Loss: 0.0152 | Grad Norm: 0.00950842\n",
      "Epoch 5 | Step 3219700 | Avg Loss: 0.0150 | Grad Norm: 0.00920049\n",
      "Epoch 5 | Step 3219800 | Avg Loss: 0.0150 | Grad Norm: 0.01030219\n",
      "Epoch 5 | Step 3219900 | Avg Loss: 0.0150 | Grad Norm: 0.00915883\n",
      "Epoch 5 | Step 3220000 | Avg Loss: 0.0152 | Grad Norm: 0.00914635\n",
      "Epoch 5 | Step 3220100 | Avg Loss: 0.0154 | Grad Norm: 0.00798104\n",
      "Epoch 5 | Step 3220200 | Avg Loss: 0.0156 | Grad Norm: 0.00893376\n",
      "Epoch 5 | Step 3220300 | Avg Loss: 0.0153 | Grad Norm: 0.00887245\n",
      "Epoch 5 | Step 3220400 | Avg Loss: 0.0156 | Grad Norm: 0.00821256\n",
      "Epoch 5 | Step 3220500 | Avg Loss: 0.0155 | Grad Norm: 0.00919140\n",
      "Epoch 5 | Step 3220600 | Avg Loss: 0.0152 | Grad Norm: 0.00987274\n",
      "Epoch 5 | Step 3220700 | Avg Loss: 0.0152 | Grad Norm: 0.00907223\n",
      "Epoch 5 | Step 3220800 | Avg Loss: 0.0152 | Grad Norm: 0.00918455\n",
      "Epoch 5 | Step 3220900 | Avg Loss: 0.0149 | Grad Norm: 0.00886296\n",
      "Epoch 5 | Step 3221000 | Avg Loss: 0.0149 | Grad Norm: 0.00827485\n",
      "Epoch 5 | Step 3221100 | Avg Loss: 0.0146 | Grad Norm: 0.00898710\n",
      "Epoch 5 | Step 3221200 | Avg Loss: 0.0145 | Grad Norm: 0.00791900\n",
      "Epoch 5 | Step 3221300 | Avg Loss: 0.0147 | Grad Norm: 0.00844182\n",
      "Epoch 5 | Step 3221400 | Avg Loss: 0.0149 | Grad Norm: 0.00814928\n",
      "Epoch 5 | Step 3221500 | Avg Loss: 0.0153 | Grad Norm: 0.00990020\n",
      "Epoch 5 | Step 3221600 | Avg Loss: 0.0152 | Grad Norm: 0.00855428\n",
      "Epoch 5 | Step 3221700 | Avg Loss: 0.0157 | Grad Norm: 0.01049456\n",
      "Epoch 5 | Step 3221800 | Avg Loss: 0.0155 | Grad Norm: 0.00897813\n",
      "Epoch 5 | Step 3221900 | Avg Loss: 0.0156 | Grad Norm: 0.01787177\n",
      "Epoch 5 | Step 3222000 | Avg Loss: 0.0156 | Grad Norm: 0.01002830\n",
      "Epoch 5 | Step 3222100 | Avg Loss: 0.0154 | Grad Norm: 0.00848690\n",
      "Epoch 5 | Step 3222200 | Avg Loss: 0.0159 | Grad Norm: 0.00990721\n",
      "Epoch 5 | Step 3222300 | Avg Loss: 0.0160 | Grad Norm: 0.01125898\n",
      "Epoch 5 | Step 3222400 | Avg Loss: 0.0159 | Grad Norm: 0.01053072\n",
      "Epoch 5 | Step 3222500 | Avg Loss: 0.0159 | Grad Norm: 0.00967979\n",
      "Epoch 5 | Step 3222600 | Avg Loss: 0.0157 | Grad Norm: 0.00938798\n",
      "Epoch 5 | Step 3222700 | Avg Loss: 0.0158 | Grad Norm: 0.00851656\n",
      "Epoch 5 | Step 3222800 | Avg Loss: 0.0160 | Grad Norm: 0.00988967\n",
      "Epoch 5 | Step 3222900 | Avg Loss: 0.0167 | Grad Norm: 0.00950566\n",
      "Epoch 5 | Step 3223000 | Avg Loss: 0.0163 | Grad Norm: 0.00891613\n",
      "Epoch 5 | Step 3223100 | Avg Loss: 0.0157 | Grad Norm: 0.01068051\n",
      "Epoch 5 | Step 3223200 | Avg Loss: 0.0159 | Grad Norm: 0.00837029\n",
      "Epoch 5 | Step 3223300 | Avg Loss: 0.0159 | Grad Norm: 0.00956631\n",
      "Epoch 5 | Step 3223400 | Avg Loss: 0.0159 | Grad Norm: 0.00884061\n",
      "Epoch 5 | Step 3223500 | Avg Loss: 0.0155 | Grad Norm: 0.00859669\n",
      "Epoch 5 | Step 3223600 | Avg Loss: 0.0154 | Grad Norm: 0.00902141\n",
      "Epoch 5 | Step 3223700 | Avg Loss: 0.0157 | Grad Norm: 0.00874365\n",
      "Epoch 5 | Step 3223800 | Avg Loss: 0.0157 | Grad Norm: 0.01032204\n",
      "Epoch 5 | Step 3223900 | Avg Loss: 0.0156 | Grad Norm: 0.00922771\n",
      "Epoch 5 | Step 3224000 | Avg Loss: 0.0156 | Grad Norm: 0.00936898\n",
      "Epoch 5 | Step 3224100 | Avg Loss: 0.0156 | Grad Norm: 0.00889708\n",
      "Epoch 5 | Step 3224200 | Avg Loss: 0.0155 | Grad Norm: 0.01022559\n",
      "Epoch 5 | Step 3224300 | Avg Loss: 0.0152 | Grad Norm: 0.00779136\n",
      "Epoch 5 | Step 3224400 | Avg Loss: 0.0148 | Grad Norm: 0.00924697\n",
      "Epoch 5 | Step 3224500 | Avg Loss: 0.0147 | Grad Norm: 0.00818453\n",
      "Epoch 5 | Step 3224600 | Avg Loss: 0.0151 | Grad Norm: 0.00817076\n",
      "Epoch 5 | Step 3224700 | Avg Loss: 0.0149 | Grad Norm: 0.00912377\n",
      "Epoch 5 | Step 3224800 | Avg Loss: 0.0146 | Grad Norm: 0.00830436\n",
      "Epoch 5 | Step 3224900 | Avg Loss: 0.0153 | Grad Norm: 0.00810005\n",
      "Epoch 5 | Step 3225000 | Avg Loss: 0.0150 | Grad Norm: 0.00881085\n",
      "Epoch 5 | Step 3225100 | Avg Loss: 0.0155 | Grad Norm: 0.00859618\n",
      "Epoch 5 | Step 3225200 | Avg Loss: 0.0149 | Grad Norm: 0.00959117\n",
      "Epoch 5 | Step 3225300 | Avg Loss: 0.0153 | Grad Norm: 0.00975889\n",
      "Epoch 5 | Step 3225400 | Avg Loss: 0.0152 | Grad Norm: 0.00899958\n",
      "Epoch 5 | Step 3225500 | Avg Loss: 0.0153 | Grad Norm: 0.00854957\n",
      "Epoch 5 | Step 3225600 | Avg Loss: 0.0160 | Grad Norm: 0.00865883\n",
      "Epoch 5 | Step 3225700 | Avg Loss: 0.0164 | Grad Norm: 0.00937183\n",
      "Epoch 5 | Step 3225800 | Avg Loss: 0.0162 | Grad Norm: 0.00837541\n",
      "Epoch 5 | Step 3225900 | Avg Loss: 0.0164 | Grad Norm: 0.00977687\n",
      "Epoch 5 | Step 3226000 | Avg Loss: 0.0160 | Grad Norm: 0.00890865\n",
      "Epoch 5 | Step 3226100 | Avg Loss: 0.0158 | Grad Norm: 0.00842373\n",
      "Epoch 5 | Step 3226200 | Avg Loss: 0.0156 | Grad Norm: 0.00926999\n",
      "Epoch 5 | Step 3226300 | Avg Loss: 0.0155 | Grad Norm: 0.01091989\n",
      "Epoch 5 | Step 3226400 | Avg Loss: 0.0156 | Grad Norm: 0.00878761\n",
      "Epoch 5 | Step 3226500 | Avg Loss: 0.0152 | Grad Norm: 0.00819271\n",
      "Epoch 5 | Step 3226600 | Avg Loss: 0.0148 | Grad Norm: 0.01022020\n",
      "Epoch 5 | Step 3226700 | Avg Loss: 0.0149 | Grad Norm: 0.01193400\n",
      "Epoch 5 | Step 3226800 | Avg Loss: 0.0146 | Grad Norm: 0.01024127\n",
      "Epoch 5 | Step 3226900 | Avg Loss: 0.0148 | Grad Norm: 0.00901876\n",
      "Epoch 5 | Step 3227000 | Avg Loss: 0.0152 | Grad Norm: 0.00779251\n",
      "Epoch 5 | Step 3227100 | Avg Loss: 0.0151 | Grad Norm: 0.00793908\n",
      "Epoch 5 | Step 3227200 | Avg Loss: 0.0149 | Grad Norm: 0.00861776\n",
      "Epoch 5 | Step 3227300 | Avg Loss: 0.0148 | Grad Norm: 0.01028919\n",
      "Epoch 5 | Step 3227400 | Avg Loss: 0.0152 | Grad Norm: 0.00846343\n",
      "Epoch 5 | Step 3227500 | Avg Loss: 0.0152 | Grad Norm: 0.00961649\n",
      "Epoch 5 | Step 3227600 | Avg Loss: 0.0156 | Grad Norm: 0.00892757\n",
      "Epoch 5 | Step 3227700 | Avg Loss: 0.0155 | Grad Norm: 0.01162980\n",
      "Epoch 5 | Step 3227800 | Avg Loss: 0.0152 | Grad Norm: 0.01361857\n",
      "Epoch 5 | Step 3227900 | Avg Loss: 0.0156 | Grad Norm: 0.00826687\n",
      "Epoch 5 | Step 3228000 | Avg Loss: 0.0155 | Grad Norm: 0.00967427\n",
      "Epoch 5 | Step 3228100 | Avg Loss: 0.0155 | Grad Norm: 0.00958434\n",
      "Epoch 5 | Step 3228200 | Avg Loss: 0.0154 | Grad Norm: 0.00875384\n",
      "Epoch 5 | Step 3228300 | Avg Loss: 0.0146 | Grad Norm: 0.00830400\n",
      "Epoch 5 | Step 3228400 | Avg Loss: 0.0150 | Grad Norm: 0.01020447\n",
      "Epoch 5 | Step 3228500 | Avg Loss: 0.0155 | Grad Norm: 0.00844813\n",
      "Epoch 5 | Step 3228600 | Avg Loss: 0.0158 | Grad Norm: 0.01163291\n",
      "Epoch 5 | Step 3228700 | Avg Loss: 0.0155 | Grad Norm: 0.00906611\n",
      "Epoch 5 | Step 3228800 | Avg Loss: 0.0153 | Grad Norm: 0.00922677\n",
      "Epoch 5 | Step 3228900 | Avg Loss: 0.0152 | Grad Norm: 0.01145106\n",
      "Epoch 5 | Step 3229000 | Avg Loss: 0.0152 | Grad Norm: 0.00915886\n",
      "Epoch 5 | Step 3229100 | Avg Loss: 0.0153 | Grad Norm: 0.00932092\n",
      "Epoch 5 | Step 3229200 | Avg Loss: 0.0152 | Grad Norm: 0.00890147\n",
      "Epoch 5 | Step 3229300 | Avg Loss: 0.0152 | Grad Norm: 0.00891170\n",
      "Epoch 5 | Step 3229400 | Avg Loss: 0.0154 | Grad Norm: 0.01109055\n",
      "Epoch 5 | Step 3229500 | Avg Loss: 0.0159 | Grad Norm: 0.00948412\n",
      "Epoch 5 | Step 3229600 | Avg Loss: 0.0156 | Grad Norm: 0.00866528\n",
      "Epoch 5 | Step 3229700 | Avg Loss: 0.0155 | Grad Norm: 0.00932597\n",
      "Epoch 5 | Step 3229800 | Avg Loss: 0.0154 | Grad Norm: 0.00994960\n",
      "Epoch 5 | Step 3229900 | Avg Loss: 0.0152 | Grad Norm: 0.00914464\n",
      "Epoch 5 | Step 3230000 | Avg Loss: 0.0153 | Grad Norm: 0.01051039\n",
      "Epoch 5 | Step 3230100 | Avg Loss: 0.0155 | Grad Norm: 0.01042384\n",
      "Epoch 5 | Step 3230200 | Avg Loss: 0.0156 | Grad Norm: 0.00927802\n",
      "Epoch 5 | Step 3230300 | Avg Loss: 0.0157 | Grad Norm: 0.00989309\n",
      "Epoch 5 | Step 3230400 | Avg Loss: 0.0158 | Grad Norm: 0.00860302\n",
      "Epoch 5 | Step 3230500 | Avg Loss: 0.0150 | Grad Norm: 0.00858604\n",
      "Epoch 5 | Step 3230600 | Avg Loss: 0.0151 | Grad Norm: 0.00927371\n",
      "Epoch 5 | Step 3230700 | Avg Loss: 0.0152 | Grad Norm: 0.00957179\n",
      "Epoch 5 | Step 3230800 | Avg Loss: 0.0154 | Grad Norm: 0.00926617\n",
      "Epoch 5 | Step 3230900 | Avg Loss: 0.0154 | Grad Norm: 0.01335705\n",
      "Epoch 5 | Step 3231000 | Avg Loss: 0.0156 | Grad Norm: 0.00920192\n",
      "Epoch 5 | Step 3231100 | Avg Loss: 0.0156 | Grad Norm: 0.00869444\n",
      "Epoch 5 | Step 3231200 | Avg Loss: 0.0157 | Grad Norm: 0.00963793\n",
      "Epoch 5 | Step 3231300 | Avg Loss: 0.0154 | Grad Norm: 0.00804113\n",
      "Epoch 5 | Step 3231400 | Avg Loss: 0.0155 | Grad Norm: 0.00938501\n",
      "Epoch 5 | Step 3231500 | Avg Loss: 0.0153 | Grad Norm: 0.00974087\n",
      "Epoch 5 | Step 3231600 | Avg Loss: 0.0148 | Grad Norm: 0.00913918\n",
      "Epoch 5 | Step 3231700 | Avg Loss: 0.0147 | Grad Norm: 0.01757399\n",
      "Epoch 5 | Step 3231800 | Avg Loss: 0.0151 | Grad Norm: 0.01290506\n",
      "Epoch 5 | Step 3231900 | Avg Loss: 0.0151 | Grad Norm: 0.00934665\n",
      "Epoch 5 | Step 3232000 | Avg Loss: 0.0152 | Grad Norm: 0.00813900\n",
      "Epoch 5 | Step 3232100 | Avg Loss: 0.0152 | Grad Norm: 0.00861757\n",
      "Epoch 5 | Step 3232200 | Avg Loss: 0.0159 | Grad Norm: 0.00943502\n",
      "Epoch 5 | Step 3232300 | Avg Loss: 0.0166 | Grad Norm: 0.00958049\n",
      "Epoch 5 | Step 3232400 | Avg Loss: 0.0162 | Grad Norm: 0.00802786\n",
      "Epoch 5 | Step 3232500 | Avg Loss: 0.0164 | Grad Norm: 0.01067555\n",
      "Epoch 5 | Step 3232600 | Avg Loss: 0.0160 | Grad Norm: 0.01052589\n",
      "Epoch 5 | Step 3232700 | Avg Loss: 0.0157 | Grad Norm: 0.00887639\n",
      "Epoch 5 | Step 3232800 | Avg Loss: 0.0158 | Grad Norm: 0.00933021\n",
      "Epoch 5 | Step 3232900 | Avg Loss: 0.0160 | Grad Norm: 0.00990890\n",
      "Epoch 5 | Step 3233000 | Avg Loss: 0.0160 | Grad Norm: 0.01012074\n",
      "Epoch 5 | Step 3233100 | Avg Loss: 0.0162 | Grad Norm: 0.01028286\n",
      "Epoch 5 | Step 3233200 | Avg Loss: 0.0160 | Grad Norm: 0.00940383\n",
      "Epoch 5 | Step 3233300 | Avg Loss: 0.0155 | Grad Norm: 0.01092508\n",
      "Epoch 5 | Step 3233400 | Avg Loss: 0.0158 | Grad Norm: 0.00893403\n",
      "Epoch 5 | Step 3233500 | Avg Loss: 0.0155 | Grad Norm: 0.00805732\n",
      "Epoch 5 | Step 3233600 | Avg Loss: 0.0150 | Grad Norm: 0.01057478\n",
      "Epoch 5 | Step 3233700 | Avg Loss: 0.0152 | Grad Norm: 0.00826267\n",
      "Epoch 5 | Step 3233800 | Avg Loss: 0.0154 | Grad Norm: 0.00914810\n",
      "Epoch 5 | Step 3233900 | Avg Loss: 0.0156 | Grad Norm: 0.00963512\n",
      "Epoch 5 | Step 3234000 | Avg Loss: 0.0157 | Grad Norm: 0.00820440\n",
      "Epoch 5 | Step 3234100 | Avg Loss: 0.0156 | Grad Norm: 0.00799828\n",
      "Epoch 5 | Step 3234200 | Avg Loss: 0.0154 | Grad Norm: 0.00945778\n",
      "Epoch 5 | Step 3234300 | Avg Loss: 0.0156 | Grad Norm: 0.00905944\n",
      "Epoch 5 | Step 3234400 | Avg Loss: 0.0157 | Grad Norm: 0.00792444\n",
      "Epoch 5 | Step 3234500 | Avg Loss: 0.0157 | Grad Norm: 0.00945871\n",
      "Epoch 5 | Step 3234600 | Avg Loss: 0.0158 | Grad Norm: 0.01032041\n",
      "Epoch 5 | Step 3234700 | Avg Loss: 0.0155 | Grad Norm: 0.00871301\n",
      "Epoch 5 | Step 3234800 | Avg Loss: 0.0152 | Grad Norm: 0.00872007\n",
      "Epoch 5 | Step 3234900 | Avg Loss: 0.0154 | Grad Norm: 0.00945446\n",
      "Epoch 5 | Step 3235000 | Avg Loss: 0.0154 | Grad Norm: 0.00885080\n",
      "Epoch 5 | Step 3235100 | Avg Loss: 0.0153 | Grad Norm: 0.00887422\n",
      "Epoch 5 | Step 3235200 | Avg Loss: 0.0149 | Grad Norm: 0.00851247\n",
      "Epoch 5 | Step 3235300 | Avg Loss: 0.0148 | Grad Norm: 0.00784479\n",
      "Epoch 5 | Step 3235400 | Avg Loss: 0.0153 | Grad Norm: 0.00997100\n",
      "Epoch 5 | Step 3235500 | Avg Loss: 0.0152 | Grad Norm: 0.00868632\n",
      "Epoch 5 | Step 3235600 | Avg Loss: 0.0154 | Grad Norm: 0.00881368\n",
      "Epoch 5 | Step 3235700 | Avg Loss: 0.0157 | Grad Norm: 0.00996052\n",
      "Epoch 5 | Step 3235800 | Avg Loss: 0.0159 | Grad Norm: 0.00917011\n",
      "Epoch 5 | Step 3235900 | Avg Loss: 0.0158 | Grad Norm: 0.00907891\n",
      "Epoch 5 | Step 3236000 | Avg Loss: 0.0157 | Grad Norm: 0.01037415\n",
      "Epoch 5 | Step 3236100 | Avg Loss: 0.0156 | Grad Norm: 0.00970973\n",
      "Epoch 5 | Step 3236200 | Avg Loss: 0.0156 | Grad Norm: 0.00806195\n",
      "Epoch 5 | Step 3236300 | Avg Loss: 0.0155 | Grad Norm: 0.00915961\n",
      "Epoch 5 | Step 3236400 | Avg Loss: 0.0156 | Grad Norm: 0.00878426\n",
      "Epoch 5 | Step 3236500 | Avg Loss: 0.0155 | Grad Norm: 0.00808390\n",
      "Epoch 5 | Step 3236600 | Avg Loss: 0.0156 | Grad Norm: 0.00849087\n",
      "Epoch 5 | Step 3236700 | Avg Loss: 0.0158 | Grad Norm: 0.00845010\n",
      "Epoch 5 | Step 3236800 | Avg Loss: 0.0151 | Grad Norm: 0.00809318\n",
      "Epoch 5 | Step 3236900 | Avg Loss: 0.0156 | Grad Norm: 0.00856266\n",
      "Epoch 5 | Step 3237000 | Avg Loss: 0.0153 | Grad Norm: 0.00930592\n",
      "Epoch 5 | Step 3237100 | Avg Loss: 0.0153 | Grad Norm: 0.00913541\n",
      "Epoch 5 | Step 3237200 | Avg Loss: 0.0154 | Grad Norm: 0.00907281\n",
      "Epoch 5 | Step 3237300 | Avg Loss: 0.0157 | Grad Norm: 0.00922644\n",
      "Epoch 5 | Step 3237400 | Avg Loss: 0.0153 | Grad Norm: 0.00907005\n",
      "Epoch 5 | Step 3237500 | Avg Loss: 0.0152 | Grad Norm: 0.00819158\n",
      "Epoch 5 | Step 3237600 | Avg Loss: 0.0154 | Grad Norm: 0.00877806\n",
      "Epoch 5 | Step 3237700 | Avg Loss: 0.0153 | Grad Norm: 0.00874796\n",
      "Epoch 5 | Step 3237800 | Avg Loss: 0.0153 | Grad Norm: 0.00964919\n",
      "Epoch 5 | Step 3237900 | Avg Loss: 0.0155 | Grad Norm: 0.00829468\n",
      "Epoch 5 | Step 3238000 | Avg Loss: 0.0159 | Grad Norm: 0.00919650\n",
      "Epoch 5 | Step 3238100 | Avg Loss: 0.0160 | Grad Norm: 0.00826569\n",
      "Epoch 5 | Step 3238200 | Avg Loss: 0.0161 | Grad Norm: 0.00920368\n",
      "Epoch 5 | Step 3238300 | Avg Loss: 0.0160 | Grad Norm: 0.00965709\n",
      "Epoch 5 | Step 3238400 | Avg Loss: 0.0156 | Grad Norm: 0.00899197\n",
      "Epoch 5 | Step 3238500 | Avg Loss: 0.0153 | Grad Norm: 0.01019239\n",
      "Epoch 5 | Step 3238600 | Avg Loss: 0.0155 | Grad Norm: 0.00857996\n",
      "Epoch 5 | Step 3238700 | Avg Loss: 0.0150 | Grad Norm: 0.00797674\n",
      "Epoch 5 | Step 3238800 | Avg Loss: 0.0152 | Grad Norm: 0.00957838\n",
      "Epoch 5 | Step 3238900 | Avg Loss: 0.0154 | Grad Norm: 0.01150439\n",
      "Epoch 5 | Step 3239000 | Avg Loss: 0.0154 | Grad Norm: 0.00782157\n",
      "Epoch 5 | Step 3239100 | Avg Loss: 0.0153 | Grad Norm: 0.00989185\n",
      "Epoch 5 | Step 3239200 | Avg Loss: 0.0156 | Grad Norm: 0.00790594\n",
      "Epoch 5 | Step 3239300 | Avg Loss: 0.0157 | Grad Norm: 0.00969690\n",
      "Epoch 5 | Step 3239400 | Avg Loss: 0.0157 | Grad Norm: 0.00851268\n",
      "Epoch 5 | Step 3239500 | Avg Loss: 0.0159 | Grad Norm: 0.00957605\n",
      "Epoch 5 | Step 3239600 | Avg Loss: 0.0154 | Grad Norm: 0.00980647\n",
      "Epoch 5 | Step 3239700 | Avg Loss: 0.0152 | Grad Norm: 0.00907709\n",
      "Epoch 5 | Step 3239800 | Avg Loss: 0.0151 | Grad Norm: 0.01025388\n",
      "Epoch 5 | Step 3239900 | Avg Loss: 0.0154 | Grad Norm: 0.00875095\n",
      "Epoch 5 | Step 3240000 | Avg Loss: 0.0158 | Grad Norm: 0.01005971\n",
      "Epoch 5 | Step 3240100 | Avg Loss: 0.0157 | Grad Norm: 0.00922714\n",
      "Epoch 5 | Step 3240200 | Avg Loss: 0.0156 | Grad Norm: 0.00908039\n",
      "Epoch 5 | Step 3240300 | Avg Loss: 0.0156 | Grad Norm: 0.01145006\n",
      "Epoch 5 | Step 3240400 | Avg Loss: 0.0153 | Grad Norm: 0.00873587\n",
      "Epoch 5 | Step 3240500 | Avg Loss: 0.0149 | Grad Norm: 0.00817293\n",
      "Epoch 5 | Step 3240600 | Avg Loss: 0.0150 | Grad Norm: 0.00893741\n",
      "Epoch 5 | Step 3240700 | Avg Loss: 0.0152 | Grad Norm: 0.01175727\n",
      "Epoch 5 | Step 3240800 | Avg Loss: 0.0152 | Grad Norm: 0.01105823\n",
      "Epoch 5 | Step 3240900 | Avg Loss: 0.0153 | Grad Norm: 0.00874506\n",
      "Epoch 5 | Step 3241000 | Avg Loss: 0.0152 | Grad Norm: 0.00908208\n",
      "Epoch 5 | Step 3241100 | Avg Loss: 0.0154 | Grad Norm: 0.00806598\n",
      "Epoch 5 | Step 3241200 | Avg Loss: 0.0156 | Grad Norm: 0.00758169\n",
      "Epoch 5 | Step 3241300 | Avg Loss: 0.0160 | Grad Norm: 0.00984200\n",
      "Epoch 5 | Step 3241400 | Avg Loss: 0.0163 | Grad Norm: 0.00949639\n",
      "Epoch 5 | Step 3241500 | Avg Loss: 0.0159 | Grad Norm: 0.00916788\n",
      "Epoch 5 | Step 3241600 | Avg Loss: 0.0156 | Grad Norm: 0.00902485\n",
      "Epoch 5 | Step 3241700 | Avg Loss: 0.0155 | Grad Norm: 0.01005010\n",
      "Epoch 5 | Step 3241800 | Avg Loss: 0.0157 | Grad Norm: 0.00751288\n",
      "Epoch 5 | Step 3241900 | Avg Loss: 0.0158 | Grad Norm: 0.01030533\n",
      "Epoch 5 | Step 3242000 | Avg Loss: 0.0155 | Grad Norm: 0.00906543\n",
      "Epoch 5 | Step 3242100 | Avg Loss: 0.0153 | Grad Norm: 0.00846891\n",
      "Epoch 5 | Step 3242200 | Avg Loss: 0.0155 | Grad Norm: 0.00882559\n",
      "Epoch 5 | Step 3242300 | Avg Loss: 0.0155 | Grad Norm: 0.00901069\n",
      "Epoch 5 | Step 3242400 | Avg Loss: 0.0158 | Grad Norm: 0.01026027\n",
      "Epoch 5 | Step 3242500 | Avg Loss: 0.0154 | Grad Norm: 0.01079537\n",
      "Epoch 5 | Step 3242600 | Avg Loss: 0.0157 | Grad Norm: 0.00917338\n",
      "Epoch 5 | Step 3242700 | Avg Loss: 0.0161 | Grad Norm: 0.00902546\n",
      "Epoch 5 | Step 3242800 | Avg Loss: 0.0159 | Grad Norm: 0.00910271\n",
      "Epoch 5 | Step 3242900 | Avg Loss: 0.0158 | Grad Norm: 0.01073267\n",
      "Epoch 5 | Step 3243000 | Avg Loss: 0.0156 | Grad Norm: 0.00801145\n",
      "Epoch 5 | Step 3243100 | Avg Loss: 0.0160 | Grad Norm: 0.00962454\n",
      "Epoch 5 | Step 3243200 | Avg Loss: 0.0159 | Grad Norm: 0.00843853\n",
      "Epoch 5 | Step 3243300 | Avg Loss: 0.0156 | Grad Norm: 0.01050511\n",
      "Epoch 5 | Step 3243400 | Avg Loss: 0.0156 | Grad Norm: 0.00828910\n",
      "Epoch 5 | Step 3243500 | Avg Loss: 0.0158 | Grad Norm: 0.00942998\n",
      "Epoch 5 | Step 3243600 | Avg Loss: 0.0157 | Grad Norm: 0.00935613\n",
      "Epoch 5 | Step 3243700 | Avg Loss: 0.0156 | Grad Norm: 0.00968887\n",
      "Epoch 5 | Step 3243800 | Avg Loss: 0.0155 | Grad Norm: 0.00921766\n",
      "Epoch 5 | Step 3243900 | Avg Loss: 0.0151 | Grad Norm: 0.00878634\n",
      "Epoch 5 | Step 3244000 | Avg Loss: 0.0152 | Grad Norm: 0.00993503\n",
      "Epoch 5 | Step 3244100 | Avg Loss: 0.0150 | Grad Norm: 0.00918380\n",
      "Epoch 5 | Step 3244200 | Avg Loss: 0.0150 | Grad Norm: 0.01155066\n",
      "Epoch 5 | Step 3244300 | Avg Loss: 0.0152 | Grad Norm: 0.00875904\n",
      "Epoch 5 | Step 3244400 | Avg Loss: 0.0151 | Grad Norm: 0.01002815\n",
      "Epoch 5 | Step 3244500 | Avg Loss: 0.0150 | Grad Norm: 0.00890686\n",
      "Epoch 5 | Step 3244600 | Avg Loss: 0.0148 | Grad Norm: 0.00801560\n",
      "Epoch 5 | Step 3244700 | Avg Loss: 0.0149 | Grad Norm: 0.00824936\n",
      "Epoch 5 | Step 3244800 | Avg Loss: 0.0151 | Grad Norm: 0.00844410\n",
      "Epoch 5 | Step 3244900 | Avg Loss: 0.0156 | Grad Norm: 0.01024374\n",
      "Epoch 5 | Step 3245000 | Avg Loss: 0.0158 | Grad Norm: 0.00948741\n",
      "Epoch 5 | Step 3245100 | Avg Loss: 0.0158 | Grad Norm: 0.01014500\n",
      "Epoch 5 | Step 3245200 | Avg Loss: 0.0157 | Grad Norm: 0.01073671\n",
      "Epoch 5 | Step 3245300 | Avg Loss: 0.0154 | Grad Norm: 0.00837247\n",
      "Epoch 5 | Step 3245400 | Avg Loss: 0.0153 | Grad Norm: 0.00907102\n",
      "Epoch 5 | Step 3245500 | Avg Loss: 0.0155 | Grad Norm: 0.00878614\n",
      "Epoch 5 | Step 3245600 | Avg Loss: 0.0152 | Grad Norm: 0.00840906\n",
      "Epoch 5 | Step 3245700 | Avg Loss: 0.0151 | Grad Norm: 0.00969292\n",
      "Epoch 5 | Step 3245800 | Avg Loss: 0.0150 | Grad Norm: 0.00877218\n",
      "Epoch 5 | Step 3245900 | Avg Loss: 0.0151 | Grad Norm: 0.00832215\n",
      "Epoch 5 | Step 3246000 | Avg Loss: 0.0151 | Grad Norm: 0.00928091\n",
      "Epoch 5 | Step 3246100 | Avg Loss: 0.0150 | Grad Norm: 0.00891501\n",
      "Epoch 5 | Step 3246200 | Avg Loss: 0.0152 | Grad Norm: 0.00887464\n",
      "Epoch 5 | Step 3246300 | Avg Loss: 0.0152 | Grad Norm: 0.00965661\n",
      "Epoch 5 | Step 3246400 | Avg Loss: 0.0152 | Grad Norm: 0.00832709\n",
      "Epoch 5 | Step 3246500 | Avg Loss: 0.0153 | Grad Norm: 0.00848097\n",
      "Epoch 5 | Step 3246600 | Avg Loss: 0.0156 | Grad Norm: 0.00857450\n",
      "Epoch 5 | Step 3246700 | Avg Loss: 0.0155 | Grad Norm: 0.01193686\n",
      "Epoch 5 | Step 3246800 | Avg Loss: 0.0153 | Grad Norm: 0.00873676\n",
      "Epoch 5 | Step 3246900 | Avg Loss: 0.0154 | Grad Norm: 0.01542447\n",
      "Epoch 5 | Step 3247000 | Avg Loss: 0.0154 | Grad Norm: 0.00875920\n",
      "Epoch 5 | Step 3247100 | Avg Loss: 0.0154 | Grad Norm: 0.00922622\n",
      "Epoch 5 | Step 3247200 | Avg Loss: 0.0153 | Grad Norm: 0.00780646\n",
      "Epoch 5 | Step 3247300 | Avg Loss: 0.0153 | Grad Norm: 0.01101924\n",
      "Epoch 5 | Step 3247400 | Avg Loss: 0.0154 | Grad Norm: 0.00994917\n",
      "Epoch 5 | Step 3247500 | Avg Loss: 0.0151 | Grad Norm: 0.00898853\n",
      "Epoch 5 | Step 3247600 | Avg Loss: 0.0153 | Grad Norm: 0.00862575\n",
      "Epoch 5 | Step 3247700 | Avg Loss: 0.0150 | Grad Norm: 0.00778325\n",
      "Epoch 5 | Step 3247800 | Avg Loss: 0.0154 | Grad Norm: 0.00943929\n",
      "Epoch 5 | Step 3247900 | Avg Loss: 0.0151 | Grad Norm: 0.01171096\n",
      "Epoch 5 | Step 3248000 | Avg Loss: 0.0150 | Grad Norm: 0.01024239\n",
      "Epoch 5 | Step 3248100 | Avg Loss: 0.0154 | Grad Norm: 0.00829755\n",
      "Epoch 5 | Step 3248200 | Avg Loss: 0.0151 | Grad Norm: 0.00816094\n",
      "Epoch 5 | Step 3248300 | Avg Loss: 0.0151 | Grad Norm: 0.00903786\n",
      "Epoch 5 | Step 3248400 | Avg Loss: 0.0151 | Grad Norm: 0.00813277\n",
      "Epoch 5 | Step 3248500 | Avg Loss: 0.0151 | Grad Norm: 0.00843503\n",
      "Epoch 5 | Step 3248600 | Avg Loss: 0.0151 | Grad Norm: 0.00999114\n",
      "Epoch 5 | Step 3248700 | Avg Loss: 0.0152 | Grad Norm: 0.00757541\n",
      "Epoch 5 | Step 3248800 | Avg Loss: 0.0152 | Grad Norm: 0.00813522\n",
      "Epoch 5 | Step 3248900 | Avg Loss: 0.0152 | Grad Norm: 0.00824238\n",
      "Epoch 5 | Step 3249000 | Avg Loss: 0.0157 | Grad Norm: 0.00772887\n",
      "Epoch 5 | Step 3249100 | Avg Loss: 0.0157 | Grad Norm: 0.00837858\n",
      "Epoch 5 | Step 3249200 | Avg Loss: 0.0156 | Grad Norm: 0.01027434\n",
      "Epoch 5 | Step 3249300 | Avg Loss: 0.0157 | Grad Norm: 0.01017942\n",
      "Epoch 5 | Step 3249400 | Avg Loss: 0.0155 | Grad Norm: 0.00922401\n",
      "Epoch 5 | Step 3249500 | Avg Loss: 0.0153 | Grad Norm: 0.00935279\n",
      "Epoch 5 | Step 3249600 | Avg Loss: 0.0153 | Grad Norm: 0.00931216\n",
      "Epoch 5 | Step 3249700 | Avg Loss: 0.0153 | Grad Norm: 0.01106969\n",
      "Epoch 5 | Step 3249800 | Avg Loss: 0.0152 | Grad Norm: 0.00891600\n",
      "Epoch 5 | Step 3249900 | Avg Loss: 0.0152 | Grad Norm: 0.00932483\n",
      "Epoch 5 | Step 3250000 | Avg Loss: 0.0158 | Grad Norm: 0.00862501\n",
      "Epoch 5 | Step 3250100 | Avg Loss: 0.0155 | Grad Norm: 0.00884947\n",
      "Epoch 5 | Step 3250200 | Avg Loss: 0.0156 | Grad Norm: 0.00886846\n",
      "Epoch 5 | Step 3250300 | Avg Loss: 0.0155 | Grad Norm: 0.01048379\n",
      "Epoch 5 | Step 3250400 | Avg Loss: 0.0155 | Grad Norm: 0.00737256\n",
      "Epoch 5 | Step 3250500 | Avg Loss: 0.0155 | Grad Norm: 0.00866043\n",
      "Epoch 5 | Step 3250600 | Avg Loss: 0.0156 | Grad Norm: 0.01243895\n",
      "Epoch 5 | Step 3250700 | Avg Loss: 0.0156 | Grad Norm: 0.00789773\n",
      "Epoch 5 | Step 3250800 | Avg Loss: 0.0153 | Grad Norm: 0.00905919\n",
      "Epoch 5 | Step 3250900 | Avg Loss: 0.0152 | Grad Norm: 0.00822391\n",
      "Epoch 5 | Step 3251000 | Avg Loss: 0.0154 | Grad Norm: 0.00860889\n",
      "Epoch 5 | Step 3251100 | Avg Loss: 0.0157 | Grad Norm: 0.00919392\n",
      "Epoch 5 | Step 3251200 | Avg Loss: 0.0156 | Grad Norm: 0.00843307\n",
      "Epoch 5 | Step 3251300 | Avg Loss: 0.0158 | Grad Norm: 0.00992649\n",
      "Epoch 5 | Step 3251400 | Avg Loss: 0.0157 | Grad Norm: 0.00885702\n",
      "Epoch 5 | Step 3251500 | Avg Loss: 0.0157 | Grad Norm: 0.00984442\n",
      "Epoch 5 | Step 3251600 | Avg Loss: 0.0154 | Grad Norm: 0.00783629\n",
      "Epoch 5 | Step 3251700 | Avg Loss: 0.0156 | Grad Norm: 0.00879300\n",
      "Epoch 5 | Step 3251800 | Avg Loss: 0.0152 | Grad Norm: 0.00919232\n",
      "Epoch 5 | Step 3251900 | Avg Loss: 0.0149 | Grad Norm: 0.00779099\n",
      "Epoch 5 | Step 3252000 | Avg Loss: 0.0153 | Grad Norm: 0.01101926\n",
      "Epoch 5 | Step 3252100 | Avg Loss: 0.0155 | Grad Norm: 0.00961612\n",
      "Epoch 5 | Step 3252200 | Avg Loss: 0.0157 | Grad Norm: 0.00883679\n",
      "Epoch 5 | Step 3252300 | Avg Loss: 0.0157 | Grad Norm: 0.01256096\n",
      "Epoch 5 | Step 3252400 | Avg Loss: 0.0152 | Grad Norm: 0.01033430\n",
      "Epoch 5 | Step 3252500 | Avg Loss: 0.0151 | Grad Norm: 0.00795613\n",
      "Epoch 5 | Step 3252600 | Avg Loss: 0.0151 | Grad Norm: 0.00909588\n",
      "Epoch 5 | Step 3252700 | Avg Loss: 0.0153 | Grad Norm: 0.01077512\n",
      "Epoch 5 | Step 3252800 | Avg Loss: 0.0157 | Grad Norm: 0.00854869\n",
      "Epoch 5 | Step 3252900 | Avg Loss: 0.0158 | Grad Norm: 0.00983227\n",
      "Epoch 5 | Step 3253000 | Avg Loss: 0.0157 | Grad Norm: 0.00796287\n",
      "Epoch 5 | Step 3253100 | Avg Loss: 0.0153 | Grad Norm: 0.00868913\n",
      "Epoch 5 | Step 3253200 | Avg Loss: 0.0153 | Grad Norm: 0.00945771\n",
      "Epoch 5 | Step 3253300 | Avg Loss: 0.0154 | Grad Norm: 0.00970847\n",
      "Epoch 5 | Step 3253400 | Avg Loss: 0.0154 | Grad Norm: 0.00999899\n",
      "Epoch 5 | Step 3253500 | Avg Loss: 0.0152 | Grad Norm: 0.00886097\n",
      "Epoch 5 | Step 3253600 | Avg Loss: 0.0153 | Grad Norm: 0.00862277\n",
      "Epoch 5 | Step 3253700 | Avg Loss: 0.0153 | Grad Norm: 0.00870346\n",
      "Epoch 5 | Step 3253800 | Avg Loss: 0.0156 | Grad Norm: 0.00918495\n",
      "Epoch 5 | Step 3253900 | Avg Loss: 0.0156 | Grad Norm: 0.00939998\n",
      "Epoch 5 | Step 3254000 | Avg Loss: 0.0156 | Grad Norm: 0.01088366\n",
      "Epoch 5 | Step 3254100 | Avg Loss: 0.0156 | Grad Norm: 0.00761102\n",
      "Epoch 5 | Step 3254200 | Avg Loss: 0.0158 | Grad Norm: 0.00920968\n",
      "Epoch 5 | Step 3254300 | Avg Loss: 0.0157 | Grad Norm: 0.00974277\n",
      "Epoch 5 | Step 3254400 | Avg Loss: 0.0156 | Grad Norm: 0.00847274\n",
      "Epoch 5 | Step 3254500 | Avg Loss: 0.0157 | Grad Norm: 0.00846845\n",
      "Epoch 5 | Step 3254600 | Avg Loss: 0.0157 | Grad Norm: 0.01224955\n",
      "Epoch 5 | Step 3254700 | Avg Loss: 0.0157 | Grad Norm: 0.00828300\n",
      "Epoch 5 | Step 3254800 | Avg Loss: 0.0158 | Grad Norm: 0.01177144\n",
      "Epoch 5 | Step 3254900 | Avg Loss: 0.0162 | Grad Norm: 0.01061838\n",
      "Epoch 5 | Step 3255000 | Avg Loss: 0.0156 | Grad Norm: 0.01050060\n",
      "Epoch 5 | Step 3255100 | Avg Loss: 0.0154 | Grad Norm: 0.00897214\n",
      "Epoch 5 | Step 3255200 | Avg Loss: 0.0154 | Grad Norm: 0.00826644\n",
      "Epoch 5 | Step 3255300 | Avg Loss: 0.0154 | Grad Norm: 0.00947032\n",
      "Epoch 5 | Step 3255400 | Avg Loss: 0.0151 | Grad Norm: 0.00931754\n",
      "Epoch 5 | Step 3255500 | Avg Loss: 0.0157 | Grad Norm: 0.01113580\n",
      "Epoch 5 | Step 3255600 | Avg Loss: 0.0158 | Grad Norm: 0.00929722\n",
      "Epoch 5 | Step 3255700 | Avg Loss: 0.0160 | Grad Norm: 0.00937612\n",
      "Epoch 5 | Step 3255800 | Avg Loss: 0.0156 | Grad Norm: 0.00902408\n",
      "Epoch 5 | Step 3255900 | Avg Loss: 0.0159 | Grad Norm: 0.00820619\n",
      "Epoch 5 | Step 3256000 | Avg Loss: 0.0158 | Grad Norm: 0.00829192\n",
      "Epoch 5 | Step 3256100 | Avg Loss: 0.0157 | Grad Norm: 0.01083499\n",
      "Epoch 5 | Step 3256200 | Avg Loss: 0.0158 | Grad Norm: 0.00960099\n",
      "Epoch 5 | Step 3256300 | Avg Loss: 0.0159 | Grad Norm: 0.00814944\n",
      "Epoch 5 | Step 3256400 | Avg Loss: 0.0160 | Grad Norm: 0.01034742\n",
      "Epoch 5 | Step 3256500 | Avg Loss: 0.0159 | Grad Norm: 0.00754127\n",
      "Epoch 5 | Step 3256600 | Avg Loss: 0.0158 | Grad Norm: 0.00902940\n",
      "Epoch 5 | Step 3256700 | Avg Loss: 0.0158 | Grad Norm: 0.00877117\n",
      "Epoch 5 | Step 3256800 | Avg Loss: 0.0158 | Grad Norm: 0.00960047\n",
      "Epoch 5 | Step 3256900 | Avg Loss: 0.0155 | Grad Norm: 0.00916010\n",
      "Epoch 5 | Step 3257000 | Avg Loss: 0.0153 | Grad Norm: 0.01027633\n",
      "Epoch 5 | Step 3257100 | Avg Loss: 0.0153 | Grad Norm: 0.00746968\n",
      "Epoch 5 | Step 3257200 | Avg Loss: 0.0157 | Grad Norm: 0.00838077\n",
      "Epoch 5 | Step 3257300 | Avg Loss: 0.0160 | Grad Norm: 0.00929740\n",
      "Epoch 5 | Step 3257400 | Avg Loss: 0.0159 | Grad Norm: 0.00861821\n",
      "Epoch 5 | Step 3257500 | Avg Loss: 0.0159 | Grad Norm: 0.00809471\n",
      "Epoch 5 | Step 3257600 | Avg Loss: 0.0159 | Grad Norm: 0.00922553\n",
      "Epoch 5 | Step 3257700 | Avg Loss: 0.0157 | Grad Norm: 0.01012060\n",
      "Epoch 5 | Step 3257800 | Avg Loss: 0.0159 | Grad Norm: 0.00909079\n",
      "Epoch 5 | Step 3257900 | Avg Loss: 0.0157 | Grad Norm: 0.01074000\n",
      "Epoch 5 | Step 3258000 | Avg Loss: 0.0159 | Grad Norm: 0.00841005\n",
      "Epoch 5 | Step 3258100 | Avg Loss: 0.0155 | Grad Norm: 0.00775202\n",
      "Epoch 5 | Step 3258200 | Avg Loss: 0.0156 | Grad Norm: 0.00926434\n",
      "Epoch 5 | Step 3258300 | Avg Loss: 0.0156 | Grad Norm: 0.00949699\n",
      "Epoch 5 | Step 3258400 | Avg Loss: 0.0158 | Grad Norm: 0.00910283\n",
      "Epoch 5 | Step 3258500 | Avg Loss: 0.0154 | Grad Norm: 0.00945190\n",
      "Epoch 5 | Step 3258600 | Avg Loss: 0.0155 | Grad Norm: 0.00964957\n",
      "Epoch 5 | Step 3258700 | Avg Loss: 0.0154 | Grad Norm: 0.00844736\n",
      "Epoch 5 | Step 3258800 | Avg Loss: 0.0151 | Grad Norm: 0.00889539\n",
      "Epoch 5 | Step 3258900 | Avg Loss: 0.0150 | Grad Norm: 0.00882212\n",
      "Epoch 5 | Step 3259000 | Avg Loss: 0.0150 | Grad Norm: 0.00946557\n",
      "Epoch 5 | Step 3259100 | Avg Loss: 0.0149 | Grad Norm: 0.00766829\n",
      "Epoch 5 | Step 3259200 | Avg Loss: 0.0150 | Grad Norm: 0.01049232\n",
      "Epoch 5 | Step 3259300 | Avg Loss: 0.0152 | Grad Norm: 0.00921166\n",
      "Epoch 5 | Step 3259400 | Avg Loss: 0.0155 | Grad Norm: 0.01129675\n",
      "Epoch 5 | Step 3259500 | Avg Loss: 0.0155 | Grad Norm: 0.00979912\n",
      "Epoch 5 | Step 3259600 | Avg Loss: 0.0155 | Grad Norm: 0.01145296\n",
      "Epoch 5 | Step 3259700 | Avg Loss: 0.0153 | Grad Norm: 0.01514617\n",
      "Epoch 5 | Step 3259800 | Avg Loss: 0.0152 | Grad Norm: 0.00912744\n",
      "Epoch 5 | Step 3259900 | Avg Loss: 0.0154 | Grad Norm: 0.01120842\n",
      "Epoch 5 | Step 3260000 | Avg Loss: 0.0154 | Grad Norm: 0.00824499\n",
      "Epoch 5 | Step 3260100 | Avg Loss: 0.0156 | Grad Norm: 0.00800501\n",
      "Epoch 5 | Step 3260200 | Avg Loss: 0.0159 | Grad Norm: 0.00786260\n",
      "Epoch 5 | Step 3260300 | Avg Loss: 0.0158 | Grad Norm: 0.00806521\n",
      "Epoch 5 | Step 3260400 | Avg Loss: 0.0157 | Grad Norm: 0.00901618\n",
      "Epoch 5 | Step 3260500 | Avg Loss: 0.0159 | Grad Norm: 0.00973202\n",
      "Epoch 5 | Step 3260600 | Avg Loss: 0.0158 | Grad Norm: 0.00812997\n",
      "Epoch 5 | Step 3260700 | Avg Loss: 0.0157 | Grad Norm: 0.01003760\n",
      "Epoch 5 | Step 3260800 | Avg Loss: 0.0156 | Grad Norm: 0.00969003\n",
      "Epoch 5 | Step 3260900 | Avg Loss: 0.0156 | Grad Norm: 0.00889526\n",
      "Epoch 5 | Step 3261000 | Avg Loss: 0.0156 | Grad Norm: 0.00851255\n",
      "Epoch 5 | Step 3261100 | Avg Loss: 0.0154 | Grad Norm: 0.01023435\n",
      "Epoch 5 | Step 3261200 | Avg Loss: 0.0155 | Grad Norm: 0.00894801\n",
      "Epoch 5 | Step 3261300 | Avg Loss: 0.0152 | Grad Norm: 0.00963252\n",
      "Epoch 5 | Step 3261400 | Avg Loss: 0.0150 | Grad Norm: 0.00960552\n",
      "Epoch 5 | Step 3261500 | Avg Loss: 0.0149 | Grad Norm: 0.00926772\n",
      "Epoch 5 | Step 3261600 | Avg Loss: 0.0147 | Grad Norm: 0.01002317\n",
      "Epoch 5 | Step 3261700 | Avg Loss: 0.0147 | Grad Norm: 0.00955196\n",
      "Epoch 5 | Step 3261800 | Avg Loss: 0.0150 | Grad Norm: 0.01085829\n",
      "Epoch 5 | Step 3261900 | Avg Loss: 0.0151 | Grad Norm: 0.00852840\n",
      "Epoch 5 | Step 3262000 | Avg Loss: 0.0154 | Grad Norm: 0.01022076\n",
      "Epoch 5 | Step 3262100 | Avg Loss: 0.0153 | Grad Norm: 0.00908844\n",
      "Epoch 5 | Step 3262200 | Avg Loss: 0.0153 | Grad Norm: 0.00961818\n",
      "Epoch 5 | Step 3262300 | Avg Loss: 0.0151 | Grad Norm: 0.00903650\n",
      "Epoch 5 | Step 3262400 | Avg Loss: 0.0154 | Grad Norm: 0.01128686\n",
      "Epoch 5 | Step 3262500 | Avg Loss: 0.0155 | Grad Norm: 0.00834944\n",
      "Epoch 5 | Step 3262600 | Avg Loss: 0.0153 | Grad Norm: 0.00958095\n",
      "Epoch 5 | Step 3262700 | Avg Loss: 0.0155 | Grad Norm: 0.00904736\n",
      "Epoch 5 | Step 3262800 | Avg Loss: 0.0154 | Grad Norm: 0.00850679\n",
      "Epoch 5 | Step 3262900 | Avg Loss: 0.0153 | Grad Norm: 0.00939185\n",
      "Epoch 5 | Step 3263000 | Avg Loss: 0.0153 | Grad Norm: 0.00881351\n",
      "Epoch 5 | Step 3263100 | Avg Loss: 0.0156 | Grad Norm: 0.00907905\n",
      "Epoch 5 | Step 3263200 | Avg Loss: 0.0157 | Grad Norm: 0.00855682\n",
      "Epoch 5 | Step 3263300 | Avg Loss: 0.0158 | Grad Norm: 0.00994722\n",
      "Epoch 5 | Step 3263400 | Avg Loss: 0.0158 | Grad Norm: 0.00892130\n",
      "Epoch 5 | Step 3263500 | Avg Loss: 0.0152 | Grad Norm: 0.00992793\n",
      "Epoch 5 | Step 3263600 | Avg Loss: 0.0155 | Grad Norm: 0.00839701\n",
      "Epoch 5 | Step 3263700 | Avg Loss: 0.0155 | Grad Norm: 0.00869517\n",
      "Epoch 5 | Step 3263800 | Avg Loss: 0.0153 | Grad Norm: 0.00904202\n",
      "Epoch 5 | Step 3263900 | Avg Loss: 0.0154 | Grad Norm: 0.01060531\n",
      "Epoch 5 | Step 3264000 | Avg Loss: 0.0151 | Grad Norm: 0.00772310\n",
      "Epoch 5 | Step 3264100 | Avg Loss: 0.0154 | Grad Norm: 0.00939202\n",
      "Epoch 5 | Step 3264200 | Avg Loss: 0.0158 | Grad Norm: 0.00876206\n",
      "Epoch 5 | Step 3264300 | Avg Loss: 0.0159 | Grad Norm: 0.00916922\n",
      "Epoch 5 | Step 3264400 | Avg Loss: 0.0158 | Grad Norm: 0.00818911\n",
      "Epoch 5 | Step 3264500 | Avg Loss: 0.0156 | Grad Norm: 0.00981141\n",
      "Epoch 5 | Step 3264600 | Avg Loss: 0.0158 | Grad Norm: 0.00996863\n",
      "Epoch 5 | Step 3264700 | Avg Loss: 0.0158 | Grad Norm: 0.01015122\n",
      "Epoch 5 | Step 3264800 | Avg Loss: 0.0160 | Grad Norm: 0.01117020\n",
      "Epoch 5 | Step 3264900 | Avg Loss: 0.0157 | Grad Norm: 0.00822227\n",
      "Epoch 5 | Step 3265000 | Avg Loss: 0.0153 | Grad Norm: 0.00893703\n",
      "Epoch 5 | Step 3265100 | Avg Loss: 0.0152 | Grad Norm: 0.00802649\n",
      "Epoch 5 | Step 3265200 | Avg Loss: 0.0149 | Grad Norm: 0.01156285\n",
      "Epoch 5 | Step 3265300 | Avg Loss: 0.0151 | Grad Norm: 0.00914833\n",
      "Epoch 5 | Step 3265400 | Avg Loss: 0.0149 | Grad Norm: 0.00918181\n",
      "Epoch 5 | Step 3265500 | Avg Loss: 0.0154 | Grad Norm: 0.00956991\n",
      "Epoch 5 | Step 3265600 | Avg Loss: 0.0157 | Grad Norm: 0.00890624\n",
      "Epoch 5 | Step 3265700 | Avg Loss: 0.0159 | Grad Norm: 0.00843826\n",
      "Epoch 5 | Step 3265800 | Avg Loss: 0.0159 | Grad Norm: 0.00907986\n",
      "Epoch 5 | Step 3265900 | Avg Loss: 0.0159 | Grad Norm: 0.00867505\n",
      "Epoch 5 | Step 3266000 | Avg Loss: 0.0160 | Grad Norm: 0.00819941\n",
      "Epoch 5 | Step 3266100 | Avg Loss: 0.0158 | Grad Norm: 0.00885738\n",
      "Epoch 5 | Step 3266200 | Avg Loss: 0.0157 | Grad Norm: 0.00917317\n",
      "Epoch 5 | Step 3266300 | Avg Loss: 0.0158 | Grad Norm: 0.00972280\n",
      "Epoch 5 | Step 3266400 | Avg Loss: 0.0156 | Grad Norm: 0.00909092\n",
      "Epoch 5 | Step 3266500 | Avg Loss: 0.0153 | Grad Norm: 0.01135992\n",
      "Epoch 5 | Step 3266600 | Avg Loss: 0.0151 | Grad Norm: 0.00778682\n",
      "Epoch 5 | Step 3266700 | Avg Loss: 0.0150 | Grad Norm: 0.00858288\n",
      "Epoch 5 | Step 3266800 | Avg Loss: 0.0150 | Grad Norm: 0.00740880\n",
      "Epoch 5 | Step 3266900 | Avg Loss: 0.0149 | Grad Norm: 0.01113712\n",
      "Epoch 5 | Step 3267000 | Avg Loss: 0.0152 | Grad Norm: 0.01074685\n",
      "Epoch 5 | Step 3267100 | Avg Loss: 0.0152 | Grad Norm: 0.00990181\n",
      "Epoch 5 | Step 3267200 | Avg Loss: 0.0155 | Grad Norm: 0.00950797\n",
      "Epoch 5 | Step 3267300 | Avg Loss: 0.0156 | Grad Norm: 0.00875147\n",
      "Epoch 5 | Step 3267400 | Avg Loss: 0.0159 | Grad Norm: 0.00935364\n",
      "Epoch 5 | Step 3267500 | Avg Loss: 0.0156 | Grad Norm: 0.00809525\n",
      "Epoch 5 | Step 3267600 | Avg Loss: 0.0148 | Grad Norm: 0.00835834\n",
      "Epoch 5 | Step 3267700 | Avg Loss: 0.0151 | Grad Norm: 0.00895666\n",
      "Epoch 5 | Step 3267800 | Avg Loss: 0.0147 | Grad Norm: 0.00893356\n",
      "Epoch 5 | Step 3267900 | Avg Loss: 0.0146 | Grad Norm: 0.01077469\n",
      "Epoch 5 | Step 3268000 | Avg Loss: 0.0149 | Grad Norm: 0.00850594\n",
      "Epoch 5 | Step 3268100 | Avg Loss: 0.0152 | Grad Norm: 0.00873022\n",
      "Epoch 5 | Step 3268200 | Avg Loss: 0.0155 | Grad Norm: 0.01040025\n",
      "Epoch 5 | Step 3268300 | Avg Loss: 0.0153 | Grad Norm: 0.00903836\n",
      "Epoch 5 | Step 3268400 | Avg Loss: 0.0151 | Grad Norm: 0.00841980\n",
      "Epoch 5 | Step 3268500 | Avg Loss: 0.0154 | Grad Norm: 0.00952551\n",
      "Epoch 5 | Step 3268600 | Avg Loss: 0.0152 | Grad Norm: 0.00864713\n",
      "Epoch 5 | Step 3268700 | Avg Loss: 0.0151 | Grad Norm: 0.00807983\n",
      "Epoch 5 | Step 3268800 | Avg Loss: 0.0151 | Grad Norm: 0.00834948\n",
      "Epoch 5 | Step 3268900 | Avg Loss: 0.0153 | Grad Norm: 0.00845931\n",
      "Epoch 5 | Step 3269000 | Avg Loss: 0.0151 | Grad Norm: 0.00873153\n",
      "Epoch 5 | Step 3269100 | Avg Loss: 0.0151 | Grad Norm: 0.00784408\n",
      "Epoch 5 | Step 3269200 | Avg Loss: 0.0154 | Grad Norm: 0.01153297\n",
      "Epoch 5 | Step 3269300 | Avg Loss: 0.0156 | Grad Norm: 0.00935420\n",
      "Epoch 5 | Step 3269400 | Avg Loss: 0.0160 | Grad Norm: 0.00964426\n",
      "Epoch 5 | Step 3269500 | Avg Loss: 0.0158 | Grad Norm: 0.01267475\n",
      "Epoch 5 | Step 3269600 | Avg Loss: 0.0158 | Grad Norm: 0.00872893\n",
      "Epoch 5 | Step 3269700 | Avg Loss: 0.0160 | Grad Norm: 0.00985463\n",
      "Epoch 5 | Step 3269800 | Avg Loss: 0.0157 | Grad Norm: 0.01124493\n",
      "Epoch 5 | Step 3269900 | Avg Loss: 0.0159 | Grad Norm: 0.00787555\n",
      "Epoch 5 | Step 3270000 | Avg Loss: 0.0157 | Grad Norm: 0.00940677\n",
      "Epoch 5 | Step 3270100 | Avg Loss: 0.0156 | Grad Norm: 0.00739642\n",
      "Epoch 5 | Step 3270200 | Avg Loss: 0.0152 | Grad Norm: 0.01144373\n",
      "Epoch 5 | Step 3270300 | Avg Loss: 0.0153 | Grad Norm: 0.00838223\n",
      "Epoch 5 | Step 3270400 | Avg Loss: 0.0155 | Grad Norm: 0.00833887\n",
      "Epoch 5 | Step 3270500 | Avg Loss: 0.0158 | Grad Norm: 0.00865849\n",
      "Epoch 5 | Step 3270600 | Avg Loss: 0.0158 | Grad Norm: 0.00980075\n",
      "Epoch 5 | Step 3270700 | Avg Loss: 0.0157 | Grad Norm: 0.00971372\n",
      "Epoch 5 | Step 3270800 | Avg Loss: 0.0155 | Grad Norm: 0.00763926\n",
      "Epoch 5 | Step 3270900 | Avg Loss: 0.0155 | Grad Norm: 0.01076251\n",
      "Epoch 5 | Step 3271000 | Avg Loss: 0.0154 | Grad Norm: 0.00940131\n",
      "Epoch 5 | Step 3271100 | Avg Loss: 0.0153 | Grad Norm: 0.00930491\n",
      "Epoch 5 | Step 3271200 | Avg Loss: 0.0155 | Grad Norm: 0.01045534\n",
      "Epoch 5 | Step 3271300 | Avg Loss: 0.0154 | Grad Norm: 0.01058645\n",
      "Epoch 5 | Step 3271400 | Avg Loss: 0.0156 | Grad Norm: 0.01138110\n",
      "Epoch 5 | Step 3271500 | Avg Loss: 0.0153 | Grad Norm: 0.00858365\n",
      "Epoch 5 | Step 3271600 | Avg Loss: 0.0150 | Grad Norm: 0.00806724\n",
      "Epoch 5 | Step 3271700 | Avg Loss: 0.0149 | Grad Norm: 0.00880403\n",
      "Epoch 5 | Step 3271800 | Avg Loss: 0.0150 | Grad Norm: 0.00958211\n",
      "Epoch 5 | Step 3271900 | Avg Loss: 0.0153 | Grad Norm: 0.01101384\n",
      "Epoch 5 | Step 3272000 | Avg Loss: 0.0159 | Grad Norm: 0.01005252\n",
      "Epoch 5 | Step 3272100 | Avg Loss: 0.0160 | Grad Norm: 0.00850267\n",
      "Epoch 5 | Step 3272200 | Avg Loss: 0.0162 | Grad Norm: 0.01019155\n",
      "Epoch 5 | Step 3272300 | Avg Loss: 0.0160 | Grad Norm: 0.00884580\n",
      "Epoch 5 | Step 3272400 | Avg Loss: 0.0158 | Grad Norm: 0.00829826\n",
      "Epoch 5 | Step 3272500 | Avg Loss: 0.0156 | Grad Norm: 0.01033735\n",
      "Epoch 5 | Step 3272600 | Avg Loss: 0.0152 | Grad Norm: 0.00868462\n",
      "Epoch 5 | Step 3272700 | Avg Loss: 0.0154 | Grad Norm: 0.00998644\n",
      "Epoch 5 | Step 3272800 | Avg Loss: 0.0154 | Grad Norm: 0.00898342\n",
      "Epoch 5 | Step 3272900 | Avg Loss: 0.0152 | Grad Norm: 0.00867371\n",
      "Epoch 5 | Step 3273000 | Avg Loss: 0.0155 | Grad Norm: 0.00882920\n",
      "Epoch 5 | Step 3273100 | Avg Loss: 0.0152 | Grad Norm: 0.00940067\n",
      "Epoch 5 | Step 3273200 | Avg Loss: 0.0151 | Grad Norm: 0.00923090\n",
      "Epoch 5 | Step 3273300 | Avg Loss: 0.0152 | Grad Norm: 0.00939688\n",
      "Epoch 5 | Step 3273400 | Avg Loss: 0.0152 | Grad Norm: 0.00995333\n",
      "Epoch 5 | Step 3273500 | Avg Loss: 0.0155 | Grad Norm: 0.00786290\n",
      "Epoch 5 | Step 3273600 | Avg Loss: 0.0158 | Grad Norm: 0.00987557\n",
      "Epoch 5 | Step 3273700 | Avg Loss: 0.0157 | Grad Norm: 0.00834517\n",
      "Epoch 5 | Step 3273800 | Avg Loss: 0.0156 | Grad Norm: 0.00810518\n",
      "Epoch 5 | Step 3273900 | Avg Loss: 0.0155 | Grad Norm: 0.01008603\n",
      "Epoch 5 | Step 3274000 | Avg Loss: 0.0153 | Grad Norm: 0.00823613\n",
      "Epoch 5 | Step 3274100 | Avg Loss: 0.0157 | Grad Norm: 0.00990470\n",
      "Epoch 5 | Step 3274200 | Avg Loss: 0.0156 | Grad Norm: 0.00990900\n",
      "Epoch 5 | Step 3274300 | Avg Loss: 0.0154 | Grad Norm: 0.00823020\n",
      "Epoch 5 | Step 3274400 | Avg Loss: 0.0156 | Grad Norm: 0.00974548\n",
      "Epoch 5 | Step 3274500 | Avg Loss: 0.0154 | Grad Norm: 0.00867572\n",
      "Epoch 5 | Step 3274600 | Avg Loss: 0.0152 | Grad Norm: 0.00928872\n",
      "Epoch 5 | Step 3274700 | Avg Loss: 0.0153 | Grad Norm: 0.00824343\n",
      "Epoch 5 | Step 3274800 | Avg Loss: 0.0154 | Grad Norm: 0.00894553\n",
      "Epoch 5 | Step 3274900 | Avg Loss: 0.0148 | Grad Norm: 0.00868011\n",
      "Epoch 5 | Step 3275000 | Avg Loss: 0.0149 | Grad Norm: 0.00759693\n",
      "Epoch 5 | Step 3275100 | Avg Loss: 0.0151 | Grad Norm: 0.00914174\n",
      "Epoch 5 | Step 3275200 | Avg Loss: 0.0151 | Grad Norm: 0.00947670\n",
      "Epoch 5 | Step 3275300 | Avg Loss: 0.0153 | Grad Norm: 0.00919346\n",
      "Epoch 5 | Step 3275400 | Avg Loss: 0.0153 | Grad Norm: 0.00946680\n",
      "Epoch 5 | Step 3275500 | Avg Loss: 0.0154 | Grad Norm: 0.00874426\n",
      "Epoch 5 | Step 3275600 | Avg Loss: 0.0153 | Grad Norm: 0.01279334\n",
      "Epoch 5 | Step 3275700 | Avg Loss: 0.0152 | Grad Norm: 0.00826372\n",
      "Epoch 5 | Step 3275800 | Avg Loss: 0.0152 | Grad Norm: 0.00877645\n",
      "Epoch 5 | Step 3275900 | Avg Loss: 0.0147 | Grad Norm: 0.00797311\n",
      "Epoch 5 | Step 3276000 | Avg Loss: 0.0147 | Grad Norm: 0.01100731\n",
      "Epoch 5 | Step 3276100 | Avg Loss: 0.0149 | Grad Norm: 0.00847701\n",
      "Epoch 5 | Step 3276200 | Avg Loss: 0.0148 | Grad Norm: 0.00955700\n",
      "Epoch 5 | Step 3276300 | Avg Loss: 0.0149 | Grad Norm: 0.00891396\n",
      "Epoch 5 | Step 3276400 | Avg Loss: 0.0146 | Grad Norm: 0.00789605\n",
      "Epoch 5 | Step 3276500 | Avg Loss: 0.0146 | Grad Norm: 0.00965050\n",
      "Epoch 5 | Step 3276600 | Avg Loss: 0.0146 | Grad Norm: 0.00872991\n",
      "Epoch 5 | Step 3276700 | Avg Loss: 0.0150 | Grad Norm: 0.00942793\n",
      "Epoch 5 | Step 3276800 | Avg Loss: 0.0148 | Grad Norm: 0.00828259\n",
      "Epoch 5 | Step 3276900 | Avg Loss: 0.0153 | Grad Norm: 0.01208363\n",
      "Epoch 5 | Step 3277000 | Avg Loss: 0.0148 | Grad Norm: 0.00957229\n",
      "Epoch 5 | Step 3277100 | Avg Loss: 0.0149 | Grad Norm: 0.00881211\n",
      "Epoch 5 | Step 3277200 | Avg Loss: 0.0149 | Grad Norm: 0.00885981\n",
      "Epoch 5 | Step 3277300 | Avg Loss: 0.0153 | Grad Norm: 0.00954841\n",
      "Epoch 5 | Step 3277400 | Avg Loss: 0.0156 | Grad Norm: 0.01024124\n",
      "Epoch 5 | Step 3277500 | Avg Loss: 0.0154 | Grad Norm: 0.00883581\n",
      "Epoch 5 | Step 3277600 | Avg Loss: 0.0154 | Grad Norm: 0.00935354\n",
      "Epoch 5 | Step 3277700 | Avg Loss: 0.0156 | Grad Norm: 0.00873084\n",
      "Epoch 5 | Step 3277800 | Avg Loss: 0.0154 | Grad Norm: 0.00877878\n",
      "Epoch 5 | Step 3277900 | Avg Loss: 0.0155 | Grad Norm: 0.01101748\n",
      "Epoch 5 | Step 3278000 | Avg Loss: 0.0152 | Grad Norm: 0.01017306\n",
      "Epoch 5 | Step 3278100 | Avg Loss: 0.0153 | Grad Norm: 0.00920474\n",
      "Epoch 5 | Step 3278200 | Avg Loss: 0.0148 | Grad Norm: 0.00916236\n",
      "Epoch 5 | Step 3278300 | Avg Loss: 0.0149 | Grad Norm: 0.00963409\n",
      "Epoch 5 | Step 3278400 | Avg Loss: 0.0152 | Grad Norm: 0.00940040\n",
      "Epoch 5 | Step 3278500 | Avg Loss: 0.0155 | Grad Norm: 0.01452607\n",
      "Epoch 5 | Step 3278600 | Avg Loss: 0.0150 | Grad Norm: 0.01053429\n",
      "Epoch 5 | Step 3278700 | Avg Loss: 0.0150 | Grad Norm: 0.00946727\n",
      "Epoch 5 | Step 3278800 | Avg Loss: 0.0152 | Grad Norm: 0.00944242\n",
      "Epoch 5 | Step 3278900 | Avg Loss: 0.0155 | Grad Norm: 0.00777822\n",
      "Epoch 5 | Step 3279000 | Avg Loss: 0.0157 | Grad Norm: 0.01097991\n",
      "Epoch 5 | Step 3279100 | Avg Loss: 0.0158 | Grad Norm: 0.00866759\n",
      "Epoch 5 | Step 3279200 | Avg Loss: 0.0158 | Grad Norm: 0.00855390\n",
      "Epoch 5 | Step 3279300 | Avg Loss: 0.0153 | Grad Norm: 0.00840482\n",
      "Epoch 5 | Step 3279400 | Avg Loss: 0.0151 | Grad Norm: 0.00795660\n",
      "Epoch 5 | Step 3279500 | Avg Loss: 0.0148 | Grad Norm: 0.00801698\n",
      "Epoch 5 | Step 3279600 | Avg Loss: 0.0151 | Grad Norm: 0.00847905\n",
      "Epoch 5 | Step 3279700 | Avg Loss: 0.0149 | Grad Norm: 0.00929307\n",
      "Epoch 5 | Step 3279800 | Avg Loss: 0.0153 | Grad Norm: 0.00805673\n",
      "Epoch 5 | Step 3279900 | Avg Loss: 0.0156 | Grad Norm: 0.01043739\n",
      "Epoch 5 | Step 3280000 | Avg Loss: 0.0156 | Grad Norm: 0.00894581\n",
      "Epoch 5 | Step 3280100 | Avg Loss: 0.0157 | Grad Norm: 0.01001870\n",
      "Epoch 5 | Step 3280200 | Avg Loss: 0.0154 | Grad Norm: 0.00883924\n",
      "Epoch 5 | Step 3280300 | Avg Loss: 0.0151 | Grad Norm: 0.00854000\n",
      "Epoch 5 | Step 3280400 | Avg Loss: 0.0153 | Grad Norm: 0.00886809\n",
      "Epoch 5 | Step 3280500 | Avg Loss: 0.0150 | Grad Norm: 0.00886226\n",
      "Epoch 5 | Step 3280600 | Avg Loss: 0.0149 | Grad Norm: 0.00918162\n",
      "Epoch 5 | Step 3280700 | Avg Loss: 0.0147 | Grad Norm: 0.00922514\n",
      "Epoch 5 | Step 3280800 | Avg Loss: 0.0143 | Grad Norm: 0.00701146\n",
      "Epoch 5 | Step 3280900 | Avg Loss: 0.0146 | Grad Norm: 0.00922067\n",
      "Epoch 5 | Step 3281000 | Avg Loss: 0.0147 | Grad Norm: 0.00926058\n",
      "Epoch 5 | Step 3281100 | Avg Loss: 0.0151 | Grad Norm: 0.00942804\n",
      "Epoch 5 | Step 3281200 | Avg Loss: 0.0154 | Grad Norm: 0.00889838\n",
      "Epoch 5 | Step 3281300 | Avg Loss: 0.0156 | Grad Norm: 0.00858840\n",
      "Epoch 5 | Step 3281400 | Avg Loss: 0.0156 | Grad Norm: 0.00944363\n",
      "Epoch 5 | Step 3281500 | Avg Loss: 0.0155 | Grad Norm: 0.00988437\n",
      "Epoch 5 | Step 3281600 | Avg Loss: 0.0153 | Grad Norm: 0.00786364\n",
      "Epoch 5 | Step 3281700 | Avg Loss: 0.0153 | Grad Norm: 0.00838138\n",
      "Epoch 5 | Step 3281800 | Avg Loss: 0.0150 | Grad Norm: 0.00970048\n",
      "Epoch 5 | Step 3281900 | Avg Loss: 0.0149 | Grad Norm: 0.01010863\n",
      "Epoch 5 | Step 3282000 | Avg Loss: 0.0150 | Grad Norm: 0.00809279\n",
      "Epoch 5 | Step 3282100 | Avg Loss: 0.0151 | Grad Norm: 0.00798053\n",
      "Epoch 5 | Step 3282200 | Avg Loss: 0.0153 | Grad Norm: 0.00855973\n",
      "Epoch 5 | Step 3282300 | Avg Loss: 0.0151 | Grad Norm: 0.00956976\n",
      "Epoch 5 | Step 3282400 | Avg Loss: 0.0155 | Grad Norm: 0.00880880\n",
      "Epoch 5 | Step 3282500 | Avg Loss: 0.0154 | Grad Norm: 0.00909273\n",
      "Epoch 5 | Step 3282600 | Avg Loss: 0.0153 | Grad Norm: 0.00850940\n",
      "Epoch 5 | Step 3282700 | Avg Loss: 0.0152 | Grad Norm: 0.00948483\n",
      "Epoch 5 | Step 3282800 | Avg Loss: 0.0151 | Grad Norm: 0.00811848\n",
      "Epoch 5 | Step 3282900 | Avg Loss: 0.0150 | Grad Norm: 0.01022635\n",
      "Epoch 5 | Step 3283000 | Avg Loss: 0.0153 | Grad Norm: 0.00929338\n",
      "Epoch 5 | Step 3283100 | Avg Loss: 0.0154 | Grad Norm: 0.00862184\n",
      "Epoch 5 | Step 3283200 | Avg Loss: 0.0151 | Grad Norm: 0.00946650\n",
      "Epoch 5 | Step 3283300 | Avg Loss: 0.0153 | Grad Norm: 0.00918922\n",
      "Epoch 5 | Step 3283400 | Avg Loss: 0.0155 | Grad Norm: 0.00963044\n",
      "Epoch 5 | Step 3283500 | Avg Loss: 0.0156 | Grad Norm: 0.00975633\n",
      "Epoch 5 | Step 3283600 | Avg Loss: 0.0156 | Grad Norm: 0.01075503\n",
      "Epoch 5 | Step 3283700 | Avg Loss: 0.0156 | Grad Norm: 0.00998045\n",
      "Epoch 5 | Step 3283800 | Avg Loss: 0.0153 | Grad Norm: 0.00959133\n",
      "Epoch 5 | Step 3283900 | Avg Loss: 0.0156 | Grad Norm: 0.00903231\n",
      "Epoch 5 | Step 3284000 | Avg Loss: 0.0154 | Grad Norm: 0.01282649\n",
      "Epoch 5 | Step 3284100 | Avg Loss: 0.0156 | Grad Norm: 0.00810591\n",
      "Epoch 5 | Step 3284200 | Avg Loss: 0.0155 | Grad Norm: 0.00972086\n",
      "Epoch 5 | Step 3284300 | Avg Loss: 0.0158 | Grad Norm: 0.00965163\n",
      "Epoch 5 | Step 3284400 | Avg Loss: 0.0156 | Grad Norm: 0.01126912\n",
      "Epoch 5 | Step 3284500 | Avg Loss: 0.0160 | Grad Norm: 0.00792091\n",
      "Epoch 5 | Step 3284600 | Avg Loss: 0.0157 | Grad Norm: 0.00874928\n",
      "Epoch 5 | Step 3284700 | Avg Loss: 0.0155 | Grad Norm: 0.00902169\n",
      "Epoch 5 | Step 3284800 | Avg Loss: 0.0159 | Grad Norm: 0.00976983\n",
      "Epoch 5 | Step 3284900 | Avg Loss: 0.0158 | Grad Norm: 0.00909017\n",
      "Epoch 5 | Step 3285000 | Avg Loss: 0.0157 | Grad Norm: 0.00831012\n",
      "Epoch 5 | Step 3285100 | Avg Loss: 0.0156 | Grad Norm: 0.00876422\n",
      "Epoch 5 | Step 3285200 | Avg Loss: 0.0154 | Grad Norm: 0.00819834\n",
      "Epoch 5 | Step 3285300 | Avg Loss: 0.0153 | Grad Norm: 0.00895852\n",
      "Epoch 5 | Step 3285400 | Avg Loss: 0.0152 | Grad Norm: 0.00987661\n",
      "Epoch 5 | Step 3285500 | Avg Loss: 0.0156 | Grad Norm: 0.00854411\n",
      "Epoch 5 | Step 3285600 | Avg Loss: 0.0153 | Grad Norm: 0.00812478\n",
      "Epoch 5 | Step 3285700 | Avg Loss: 0.0154 | Grad Norm: 0.00902690\n",
      "Epoch 5 | Step 3285800 | Avg Loss: 0.0154 | Grad Norm: 0.00928774\n",
      "Epoch 5 | Step 3285900 | Avg Loss: 0.0152 | Grad Norm: 0.01123537\n",
      "Epoch 5 | Step 3286000 | Avg Loss: 0.0150 | Grad Norm: 0.01188671\n",
      "Epoch 5 | Step 3286100 | Avg Loss: 0.0150 | Grad Norm: 0.01220372\n",
      "Epoch 5 | Step 3286200 | Avg Loss: 0.0151 | Grad Norm: 0.00836073\n",
      "Epoch 5 | Step 3286300 | Avg Loss: 0.0153 | Grad Norm: 0.00781534\n",
      "Epoch 5 | Step 3286400 | Avg Loss: 0.0153 | Grad Norm: 0.00882972\n",
      "Epoch 5 | Step 3286500 | Avg Loss: 0.0150 | Grad Norm: 0.00922793\n",
      "Epoch 5 | Step 3286600 | Avg Loss: 0.0151 | Grad Norm: 0.00838590\n",
      "Epoch 5 | Step 3286700 | Avg Loss: 0.0154 | Grad Norm: 0.00962528\n",
      "Epoch 5 | Step 3286800 | Avg Loss: 0.0154 | Grad Norm: 0.00874253\n",
      "Epoch 5 | Step 3286900 | Avg Loss: 0.0150 | Grad Norm: 0.00977501\n",
      "Epoch 5 | Step 3287000 | Avg Loss: 0.0149 | Grad Norm: 0.00948283\n",
      "Epoch 5 | Step 3287100 | Avg Loss: 0.0152 | Grad Norm: 0.00924459\n",
      "Epoch 5 | Step 3287200 | Avg Loss: 0.0154 | Grad Norm: 0.00895447\n",
      "Epoch 5 | Step 3287300 | Avg Loss: 0.0153 | Grad Norm: 0.00886739\n",
      "Epoch 5 | Step 3287400 | Avg Loss: 0.0152 | Grad Norm: 0.00878256\n",
      "Epoch 5 | Step 3287500 | Avg Loss: 0.0149 | Grad Norm: 0.01040609\n",
      "Epoch 5 | Step 3287600 | Avg Loss: 0.0150 | Grad Norm: 0.00919463\n",
      "Epoch 5 | Step 3287700 | Avg Loss: 0.0155 | Grad Norm: 0.00989376\n",
      "Epoch 5 | Step 3287800 | Avg Loss: 0.0158 | Grad Norm: 0.00976627\n",
      "Epoch 5 | Step 3287900 | Avg Loss: 0.0154 | Grad Norm: 0.00827155\n",
      "Epoch 5 | Step 3288000 | Avg Loss: 0.0156 | Grad Norm: 0.00760052\n",
      "Epoch 5 | Step 3288100 | Avg Loss: 0.0158 | Grad Norm: 0.00968168\n",
      "Epoch 5 | Step 3288200 | Avg Loss: 0.0155 | Grad Norm: 0.01185378\n",
      "Epoch 5 | Step 3288300 | Avg Loss: 0.0156 | Grad Norm: 0.00947349\n",
      "Epoch 5 | Step 3288400 | Avg Loss: 0.0156 | Grad Norm: 0.00992543\n",
      "Epoch 5 | Step 3288500 | Avg Loss: 0.0158 | Grad Norm: 0.00897811\n",
      "Epoch 5 | Step 3288600 | Avg Loss: 0.0155 | Grad Norm: 0.01031452\n",
      "Epoch 5 | Step 3288700 | Avg Loss: 0.0154 | Grad Norm: 0.00896028\n",
      "Epoch 5 | Step 3288800 | Avg Loss: 0.0153 | Grad Norm: 0.01058092\n",
      "Epoch 5 | Step 3288900 | Avg Loss: 0.0157 | Grad Norm: 0.00894617\n",
      "Epoch 5 | Step 3289000 | Avg Loss: 0.0157 | Grad Norm: 0.01131350\n",
      "Epoch 5 | Step 3289100 | Avg Loss: 0.0157 | Grad Norm: 0.00888253\n",
      "Epoch 5 | Step 3289200 | Avg Loss: 0.0161 | Grad Norm: 0.01001622\n",
      "Epoch 5 | Step 3289300 | Avg Loss: 0.0160 | Grad Norm: 0.00881060\n",
      "Epoch 5 | Step 3289400 | Avg Loss: 0.0160 | Grad Norm: 0.00869474\n",
      "Epoch 5 | Step 3289500 | Avg Loss: 0.0156 | Grad Norm: 0.00898247\n",
      "Epoch 5 | Step 3289600 | Avg Loss: 0.0153 | Grad Norm: 0.00852961\n",
      "Epoch 5 | Step 3289700 | Avg Loss: 0.0152 | Grad Norm: 0.00798478\n",
      "Epoch 5 | Step 3289800 | Avg Loss: 0.0152 | Grad Norm: 0.00850958\n",
      "Epoch 5 | Step 3289900 | Avg Loss: 0.0155 | Grad Norm: 0.00750151\n",
      "Epoch 5 | Step 3290000 | Avg Loss: 0.0151 | Grad Norm: 0.00879503\n",
      "Epoch 5 | Step 3290100 | Avg Loss: 0.0154 | Grad Norm: 0.01175542\n",
      "Epoch 5 | Step 3290200 | Avg Loss: 0.0152 | Grad Norm: 0.01146117\n",
      "Epoch 5 | Step 3290300 | Avg Loss: 0.0153 | Grad Norm: 0.00852149\n",
      "Epoch 5 | Step 3290400 | Avg Loss: 0.0157 | Grad Norm: 0.00915351\n",
      "Epoch 5 | Step 3290500 | Avg Loss: 0.0157 | Grad Norm: 0.01108840\n",
      "Epoch 5 | Step 3290600 | Avg Loss: 0.0156 | Grad Norm: 0.00972582\n",
      "Epoch 5 | Step 3290700 | Avg Loss: 0.0156 | Grad Norm: 0.00836788\n",
      "Epoch 5 | Step 3290800 | Avg Loss: 0.0153 | Grad Norm: 0.01025714\n",
      "Epoch 5 | Step 3290900 | Avg Loss: 0.0151 | Grad Norm: 0.00977941\n",
      "Epoch 5 | Step 3291000 | Avg Loss: 0.0152 | Grad Norm: 0.00882980\n",
      "Epoch 5 | Step 3291100 | Avg Loss: 0.0153 | Grad Norm: 0.00879564\n",
      "Epoch 5 | Step 3291200 | Avg Loss: 0.0150 | Grad Norm: 0.00873473\n",
      "Epoch 5 | Step 3291300 | Avg Loss: 0.0156 | Grad Norm: 0.00945128\n",
      "Epoch 5 | Step 3291400 | Avg Loss: 0.0162 | Grad Norm: 0.00900238\n",
      "Epoch 5 | Step 3291500 | Avg Loss: 0.0160 | Grad Norm: 0.00997930\n",
      "Epoch 5 | Step 3291600 | Avg Loss: 0.0159 | Grad Norm: 0.00844660\n",
      "Epoch 5 | Step 3291700 | Avg Loss: 0.0158 | Grad Norm: 0.00863172\n",
      "Epoch 5 | Step 3291800 | Avg Loss: 0.0159 | Grad Norm: 0.00915638\n",
      "Epoch 5 | Step 3291900 | Avg Loss: 0.0157 | Grad Norm: 0.00792223\n",
      "Epoch 5 | Step 3292000 | Avg Loss: 0.0154 | Grad Norm: 0.00815553\n",
      "Epoch 5 | Step 3292100 | Avg Loss: 0.0158 | Grad Norm: 0.01082555\n",
      "Epoch 5 | Step 3292200 | Avg Loss: 0.0162 | Grad Norm: 0.00855167\n",
      "Epoch 5 | Step 3292300 | Avg Loss: 0.0162 | Grad Norm: 0.00952168\n",
      "Epoch 5 | Step 3292400 | Avg Loss: 0.0160 | Grad Norm: 0.01140014\n",
      "Epoch 5 | Step 3292500 | Avg Loss: 0.0160 | Grad Norm: 0.01091583\n",
      "Epoch 5 | Step 3292600 | Avg Loss: 0.0163 | Grad Norm: 0.00954281\n",
      "Epoch 5 | Step 3292700 | Avg Loss: 0.0163 | Grad Norm: 0.00981199\n",
      "Epoch 5 | Step 3292800 | Avg Loss: 0.0161 | Grad Norm: 0.01062637\n",
      "Epoch 5 | Step 3292900 | Avg Loss: 0.0158 | Grad Norm: 0.00920530\n",
      "Epoch 5 | Step 3293000 | Avg Loss: 0.0155 | Grad Norm: 0.00787929\n",
      "Epoch 5 | Step 3293100 | Avg Loss: 0.0159 | Grad Norm: 0.00960695\n",
      "Epoch 5 | Step 3293200 | Avg Loss: 0.0161 | Grad Norm: 0.00925557\n",
      "Epoch 5 | Step 3293300 | Avg Loss: 0.0159 | Grad Norm: 0.00822635\n",
      "Epoch 5 | Step 3293400 | Avg Loss: 0.0156 | Grad Norm: 0.01070217\n",
      "Epoch 5 | Step 3293500 | Avg Loss: 0.0154 | Grad Norm: 0.00792894\n",
      "Epoch 5 | Step 3293600 | Avg Loss: 0.0159 | Grad Norm: 0.01014750\n",
      "Epoch 5 | Step 3293700 | Avg Loss: 0.0155 | Grad Norm: 0.00873543\n",
      "Epoch 5 | Step 3293800 | Avg Loss: 0.0153 | Grad Norm: 0.01032890\n",
      "Epoch 5 | Step 3293900 | Avg Loss: 0.0156 | Grad Norm: 0.00832343\n",
      "Epoch 5 | Step 3294000 | Avg Loss: 0.0151 | Grad Norm: 0.00877208\n",
      "Epoch 5 | Step 3294100 | Avg Loss: 0.0147 | Grad Norm: 0.00931379\n",
      "Epoch 5 | Step 3294200 | Avg Loss: 0.0151 | Grad Norm: 0.00772494\n",
      "Epoch 5 | Step 3294300 | Avg Loss: 0.0151 | Grad Norm: 0.00925287\n",
      "Epoch 5 | Step 3294400 | Avg Loss: 0.0150 | Grad Norm: 0.00804627\n",
      "Epoch 5 | Step 3294500 | Avg Loss: 0.0150 | Grad Norm: 0.01055742\n",
      "Epoch 5 | Step 3294600 | Avg Loss: 0.0149 | Grad Norm: 0.00851350\n",
      "Epoch 5 | Step 3294700 | Avg Loss: 0.0152 | Grad Norm: 0.00852903\n",
      "Epoch 5 | Step 3294800 | Avg Loss: 0.0152 | Grad Norm: 0.00892777\n",
      "Epoch 5 | Step 3294900 | Avg Loss: 0.0150 | Grad Norm: 0.01143754\n",
      "Epoch 5 | Step 3295000 | Avg Loss: 0.0148 | Grad Norm: 0.00863245\n",
      "Epoch 5 | Step 3295100 | Avg Loss: 0.0149 | Grad Norm: 0.00827256\n",
      "Epoch 5 | Step 3295200 | Avg Loss: 0.0150 | Grad Norm: 0.00901387\n",
      "Epoch 5 | Step 3295300 | Avg Loss: 0.0155 | Grad Norm: 0.01096528\n",
      "Epoch 5 | Step 3295400 | Avg Loss: 0.0154 | Grad Norm: 0.00898888\n",
      "Epoch 5 | Step 3295500 | Avg Loss: 0.0154 | Grad Norm: 0.01009641\n",
      "Epoch 5 | Step 3295600 | Avg Loss: 0.0156 | Grad Norm: 0.00810157\n",
      "Epoch 5 | Step 3295700 | Avg Loss: 0.0155 | Grad Norm: 0.00834433\n",
      "Epoch 5 | Step 3295800 | Avg Loss: 0.0156 | Grad Norm: 0.01017973\n",
      "Epoch 5 | Step 3295900 | Avg Loss: 0.0156 | Grad Norm: 0.00966024\n",
      "Epoch 5 | Step 3296000 | Avg Loss: 0.0154 | Grad Norm: 0.00947891\n",
      "Epoch 5 | Step 3296100 | Avg Loss: 0.0150 | Grad Norm: 0.01067452\n",
      "Epoch 5 | Step 3296200 | Avg Loss: 0.0154 | Grad Norm: 0.00984547\n",
      "Epoch 5 | Step 3296300 | Avg Loss: 0.0157 | Grad Norm: 0.00939425\n",
      "Epoch 5 | Step 3296400 | Avg Loss: 0.0154 | Grad Norm: 0.00849591\n",
      "Epoch 5 | Step 3296500 | Avg Loss: 0.0156 | Grad Norm: 0.00985213\n",
      "Epoch 5 | Step 3296600 | Avg Loss: 0.0157 | Grad Norm: 0.00860870\n",
      "Epoch 5 | Step 3296700 | Avg Loss: 0.0155 | Grad Norm: 0.00871825\n",
      "Epoch 5 | Step 3296800 | Avg Loss: 0.0153 | Grad Norm: 0.00959356\n",
      "Epoch 5 | Step 3296900 | Avg Loss: 0.0156 | Grad Norm: 0.00950856\n",
      "Epoch 5 | Step 3297000 | Avg Loss: 0.0155 | Grad Norm: 0.00909931\n",
      "Epoch 5 | Step 3297100 | Avg Loss: 0.0156 | Grad Norm: 0.00832800\n",
      "Epoch 5 | Step 3297200 | Avg Loss: 0.0154 | Grad Norm: 0.00942023\n",
      "Epoch 5 | Step 3297300 | Avg Loss: 0.0154 | Grad Norm: 0.00817787\n",
      "Epoch 5 | Step 3297400 | Avg Loss: 0.0151 | Grad Norm: 0.00865357\n",
      "Epoch 5 | Step 3297500 | Avg Loss: 0.0151 | Grad Norm: 0.00960881\n",
      "Epoch 5 | Step 3297600 | Avg Loss: 0.0149 | Grad Norm: 0.01017221\n",
      "Epoch 5 | Step 3297700 | Avg Loss: 0.0146 | Grad Norm: 0.01050566\n",
      "Epoch 5 | Step 3297800 | Avg Loss: 0.0148 | Grad Norm: 0.00800960\n",
      "Epoch 5 | Step 3297900 | Avg Loss: 0.0148 | Grad Norm: 0.00806296\n",
      "Epoch 5 | Step 3298000 | Avg Loss: 0.0149 | Grad Norm: 0.00907533\n",
      "Epoch 5 | Step 3298100 | Avg Loss: 0.0151 | Grad Norm: 0.00933237\n",
      "Epoch 5 | Step 3298200 | Avg Loss: 0.0156 | Grad Norm: 0.00919469\n",
      "Epoch 5 | Step 3298300 | Avg Loss: 0.0158 | Grad Norm: 0.01069783\n",
      "Epoch 5 | Step 3298400 | Avg Loss: 0.0157 | Grad Norm: 0.00923406\n",
      "Epoch 5 | Step 3298500 | Avg Loss: 0.0158 | Grad Norm: 0.01062410\n",
      "Epoch 5 | Step 3298600 | Avg Loss: 0.0155 | Grad Norm: 0.00907362\n",
      "Epoch 5 | Step 3298700 | Avg Loss: 0.0156 | Grad Norm: 0.00962799\n",
      "Epoch 5 | Step 3298800 | Avg Loss: 0.0156 | Grad Norm: 0.00931407\n",
      "Epoch 5 | Step 3298900 | Avg Loss: 0.0159 | Grad Norm: 0.01101536\n",
      "Epoch 5 | Step 3299000 | Avg Loss: 0.0161 | Grad Norm: 0.00850381\n",
      "Epoch 5 | Step 3299100 | Avg Loss: 0.0159 | Grad Norm: 0.00935946\n",
      "Epoch 5 | Step 3299200 | Avg Loss: 0.0158 | Grad Norm: 0.00936124\n",
      "Epoch 5 | Step 3299300 | Avg Loss: 0.0154 | Grad Norm: 0.00819480\n",
      "Epoch 5 | Step 3299400 | Avg Loss: 0.0158 | Grad Norm: 0.00940336\n",
      "Epoch 5 | Step 3299500 | Avg Loss: 0.0160 | Grad Norm: 0.00922397\n",
      "Epoch 5 | Step 3299600 | Avg Loss: 0.0158 | Grad Norm: 0.00837738\n",
      "Epoch 5 | Step 3299700 | Avg Loss: 0.0157 | Grad Norm: 0.00908714\n",
      "Epoch 5 | Step 3299800 | Avg Loss: 0.0160 | Grad Norm: 0.01014890\n",
      "Epoch 5 | Step 3299900 | Avg Loss: 0.0159 | Grad Norm: 0.00813215\n",
      "Epoch 5 | Step 3300000 | Avg Loss: 0.0156 | Grad Norm: 0.00801078\n",
      "Saving model at step3300000\n",
      "Epoch 5 | Step 3300100 | Avg Loss: 0.0158 | Grad Norm: 0.00887485\n",
      "Epoch 5 | Step 3300200 | Avg Loss: 0.0154 | Grad Norm: 0.00817414\n",
      "Epoch 5 | Step 3300300 | Avg Loss: 0.0153 | Grad Norm: 0.00835975\n",
      "Epoch 5 | Step 3300400 | Avg Loss: 0.0151 | Grad Norm: 0.00815827\n",
      "Epoch 5 | Step 3300500 | Avg Loss: 0.0151 | Grad Norm: 0.00898547\n",
      "Epoch 5 | Step 3300600 | Avg Loss: 0.0153 | Grad Norm: 0.00871641\n",
      "Epoch 5 | Step 3300700 | Avg Loss: 0.0158 | Grad Norm: 0.00780612\n",
      "Epoch 5 | Step 3300800 | Avg Loss: 0.0158 | Grad Norm: 0.00977007\n",
      "Epoch 5 | Step 3300900 | Avg Loss: 0.0156 | Grad Norm: 0.00926966\n",
      "Epoch 5 | Step 3301000 | Avg Loss: 0.0155 | Grad Norm: 0.00874462\n",
      "Epoch 5 | Step 3301100 | Avg Loss: 0.0153 | Grad Norm: 0.00834946\n",
      "Epoch 5 | Step 3301200 | Avg Loss: 0.0153 | Grad Norm: 0.00922066\n",
      "Epoch 5 | Step 3301300 | Avg Loss: 0.0152 | Grad Norm: 0.00847927\n",
      "Epoch 5 | Step 3301400 | Avg Loss: 0.0153 | Grad Norm: 0.00863414\n",
      "Epoch 5 | Step 3301500 | Avg Loss: 0.0156 | Grad Norm: 0.00987316\n",
      "Epoch 5 | Step 3301600 | Avg Loss: 0.0156 | Grad Norm: 0.00922032\n",
      "Epoch 5 | Step 3301700 | Avg Loss: 0.0155 | Grad Norm: 0.00833289\n",
      "Epoch 5 | Step 3301800 | Avg Loss: 0.0157 | Grad Norm: 0.01010400\n",
      "Epoch 5 | Step 3301900 | Avg Loss: 0.0159 | Grad Norm: 0.00857473\n",
      "Epoch 5 | Step 3302000 | Avg Loss: 0.0160 | Grad Norm: 0.00802390\n",
      "Epoch 5 | Step 3302100 | Avg Loss: 0.0156 | Grad Norm: 0.01152134\n",
      "Epoch 5 | Step 3302200 | Avg Loss: 0.0156 | Grad Norm: 0.00944967\n",
      "Epoch 5 | Step 3302300 | Avg Loss: 0.0158 | Grad Norm: 0.00944440\n",
      "Epoch 5 | Step 3302400 | Avg Loss: 0.0158 | Grad Norm: 0.00956540\n",
      "Epoch 5 | Step 3302500 | Avg Loss: 0.0157 | Grad Norm: 0.00992563\n",
      "Epoch 5 | Step 3302600 | Avg Loss: 0.0155 | Grad Norm: 0.00898995\n",
      "Epoch 5 | Step 3302700 | Avg Loss: 0.0151 | Grad Norm: 0.00845562\n",
      "Epoch 5 | Step 3302800 | Avg Loss: 0.0152 | Grad Norm: 0.00807887\n",
      "Epoch 5 | Step 3302900 | Avg Loss: 0.0154 | Grad Norm: 0.00865221\n",
      "Epoch 5 | Step 3303000 | Avg Loss: 0.0158 | Grad Norm: 0.00922327\n",
      "Epoch 5 | Step 3303100 | Avg Loss: 0.0155 | Grad Norm: 0.00928232\n",
      "Epoch 5 | Step 3303200 | Avg Loss: 0.0157 | Grad Norm: 0.00875064\n",
      "Epoch 5 | Step 3303300 | Avg Loss: 0.0154 | Grad Norm: 0.00983002\n",
      "Epoch 5 | Step 3303400 | Avg Loss: 0.0155 | Grad Norm: 0.00900588\n",
      "Epoch 5 | Step 3303500 | Avg Loss: 0.0152 | Grad Norm: 0.00859246\n",
      "Epoch 5 | Step 3303600 | Avg Loss: 0.0151 | Grad Norm: 0.00903538\n",
      "Epoch 5 | Step 3303700 | Avg Loss: 0.0154 | Grad Norm: 0.01399348\n",
      "Epoch 5 | Step 3303800 | Avg Loss: 0.0156 | Grad Norm: 0.00962285\n",
      "Epoch 5 | Step 3303900 | Avg Loss: 0.0157 | Grad Norm: 0.00819435\n",
      "Epoch 5 | Step 3304000 | Avg Loss: 0.0158 | Grad Norm: 0.00883734\n",
      "Epoch 5 | Step 3304100 | Avg Loss: 0.0155 | Grad Norm: 0.00960992\n",
      "Epoch 5 | Step 3304200 | Avg Loss: 0.0160 | Grad Norm: 0.01023313\n",
      "Epoch 5 | Step 3304300 | Avg Loss: 0.0161 | Grad Norm: 0.01749149\n",
      "Epoch 5 | Step 3304400 | Avg Loss: 0.0158 | Grad Norm: 0.00806698\n",
      "Epoch 5 | Step 3304500 | Avg Loss: 0.0161 | Grad Norm: 0.01192313\n",
      "Epoch 5 | Step 3304600 | Avg Loss: 0.0161 | Grad Norm: 0.00911350\n",
      "Epoch 5 | Step 3304700 | Avg Loss: 0.0161 | Grad Norm: 0.00908511\n",
      "Epoch 5 | Step 3304800 | Avg Loss: 0.0159 | Grad Norm: 0.01021574\n",
      "Epoch 5 | Step 3304900 | Avg Loss: 0.0159 | Grad Norm: 0.00901338\n",
      "Epoch 5 | Step 3305000 | Avg Loss: 0.0157 | Grad Norm: 0.00977994\n",
      "Epoch 5 | Step 3305100 | Avg Loss: 0.0158 | Grad Norm: 0.00823440\n",
      "Epoch 5 | Step 3305200 | Avg Loss: 0.0159 | Grad Norm: 0.00998987\n",
      "Epoch 5 | Step 3305300 | Avg Loss: 0.0162 | Grad Norm: 0.00985855\n",
      "Epoch 5 | Step 3305400 | Avg Loss: 0.0162 | Grad Norm: 0.00827243\n",
      "Epoch 5 | Step 3305500 | Avg Loss: 0.0161 | Grad Norm: 0.00910740\n",
      "Epoch 5 | Step 3305600 | Avg Loss: 0.0159 | Grad Norm: 0.00876285\n",
      "Epoch 5 | Step 3305700 | Avg Loss: 0.0156 | Grad Norm: 0.00879527\n",
      "Epoch 5 | Step 3305800 | Avg Loss: 0.0158 | Grad Norm: 0.00972768\n",
      "Epoch 5 | Step 3305900 | Avg Loss: 0.0156 | Grad Norm: 0.00851465\n",
      "Epoch 5 | Step 3306000 | Avg Loss: 0.0155 | Grad Norm: 0.00842037\n",
      "Epoch 5 | Step 3306100 | Avg Loss: 0.0158 | Grad Norm: 0.00869632\n",
      "Epoch 5 | Step 3306200 | Avg Loss: 0.0160 | Grad Norm: 0.00916573\n",
      "Epoch 5 | Step 3306300 | Avg Loss: 0.0160 | Grad Norm: 0.00863629\n",
      "Epoch 5 | Step 3306400 | Avg Loss: 0.0161 | Grad Norm: 0.00930655\n",
      "Epoch 5 | Step 3306500 | Avg Loss: 0.0158 | Grad Norm: 0.00846362\n",
      "Epoch 5 | Step 3306600 | Avg Loss: 0.0157 | Grad Norm: 0.00990037\n",
      "Epoch 5 | Step 3306700 | Avg Loss: 0.0156 | Grad Norm: 0.00946851\n",
      "Epoch 5 | Step 3306800 | Avg Loss: 0.0154 | Grad Norm: 0.01017232\n",
      "Epoch 5 | Step 3306900 | Avg Loss: 0.0153 | Grad Norm: 0.00921598\n",
      "Epoch 5 | Step 3307000 | Avg Loss: 0.0154 | Grad Norm: 0.00877313\n",
      "Epoch 5 | Step 3307100 | Avg Loss: 0.0153 | Grad Norm: 0.00740060\n",
      "Epoch 5 | Step 3307200 | Avg Loss: 0.0155 | Grad Norm: 0.01013760\n",
      "Epoch 5 | Step 3307300 | Avg Loss: 0.0156 | Grad Norm: 0.00864974\n",
      "Epoch 5 | Step 3307400 | Avg Loss: 0.0157 | Grad Norm: 0.00844065\n",
      "Epoch 5 | Step 3307500 | Avg Loss: 0.0152 | Grad Norm: 0.00850752\n",
      "Epoch 5 | Step 3307600 | Avg Loss: 0.0152 | Grad Norm: 0.00952791\n",
      "Epoch 5 | Step 3307700 | Avg Loss: 0.0147 | Grad Norm: 0.00853171\n",
      "Epoch 5 | Step 3307800 | Avg Loss: 0.0149 | Grad Norm: 0.00826107\n",
      "Epoch 5 | Step 3307900 | Avg Loss: 0.0150 | Grad Norm: 0.00995921\n",
      "Epoch 5 | Step 3308000 | Avg Loss: 0.0151 | Grad Norm: 0.00870008\n",
      "Epoch 5 | Step 3308100 | Avg Loss: 0.0153 | Grad Norm: 0.00917792\n",
      "Epoch 5 | Step 3308200 | Avg Loss: 0.0153 | Grad Norm: 0.01043455\n",
      "Epoch 5 | Step 3308300 | Avg Loss: 0.0154 | Grad Norm: 0.01011684\n",
      "Epoch 5 | Step 3308400 | Avg Loss: 0.0150 | Grad Norm: 0.01144754\n",
      "Epoch 5 | Step 3308500 | Avg Loss: 0.0153 | Grad Norm: 0.00913878\n",
      "Epoch 5 | Step 3308600 | Avg Loss: 0.0153 | Grad Norm: 0.00939223\n",
      "Epoch 5 | Step 3308700 | Avg Loss: 0.0154 | Grad Norm: 0.00941378\n",
      "Epoch 5 | Step 3308800 | Avg Loss: 0.0155 | Grad Norm: 0.00896096\n",
      "Epoch 5 | Step 3308900 | Avg Loss: 0.0156 | Grad Norm: 0.00835453\n",
      "Epoch 5 | Step 3309000 | Avg Loss: 0.0151 | Grad Norm: 0.01117421\n",
      "Epoch 5 | Step 3309100 | Avg Loss: 0.0154 | Grad Norm: 0.00886588\n",
      "Epoch 5 | Step 3309200 | Avg Loss: 0.0156 | Grad Norm: 0.00892923\n",
      "Epoch 5 | Step 3309300 | Avg Loss: 0.0154 | Grad Norm: 0.00790709\n",
      "Epoch 5 | Step 3309400 | Avg Loss: 0.0156 | Grad Norm: 0.00860699\n",
      "Epoch 5 | Step 3309500 | Avg Loss: 0.0153 | Grad Norm: 0.00895462\n",
      "Epoch 5 | Step 3309600 | Avg Loss: 0.0153 | Grad Norm: 0.00814606\n",
      "Epoch 5 | Step 3309700 | Avg Loss: 0.0153 | Grad Norm: 0.00851869\n",
      "Epoch 5 | Step 3309800 | Avg Loss: 0.0155 | Grad Norm: 0.00912791\n",
      "Epoch 5 | Step 3309900 | Avg Loss: 0.0154 | Grad Norm: 0.01254551\n",
      "Epoch 5 | Step 3310000 | Avg Loss: 0.0155 | Grad Norm: 0.00779646\n",
      "Epoch 5 | Step 3310100 | Avg Loss: 0.0153 | Grad Norm: 0.00949549\n",
      "Epoch 5 | Step 3310200 | Avg Loss: 0.0151 | Grad Norm: 0.00824676\n",
      "Epoch 5 | Step 3310300 | Avg Loss: 0.0149 | Grad Norm: 0.00832589\n",
      "Epoch 5 | Step 3310400 | Avg Loss: 0.0150 | Grad Norm: 0.00857863\n",
      "Epoch 5 | Step 3310500 | Avg Loss: 0.0151 | Grad Norm: 0.00862820\n",
      "Epoch 5 | Step 3310600 | Avg Loss: 0.0152 | Grad Norm: 0.00928079\n",
      "Epoch 5 | Step 3310700 | Avg Loss: 0.0152 | Grad Norm: 0.00911497\n",
      "Epoch 5 | Step 3310800 | Avg Loss: 0.0150 | Grad Norm: 0.00874739\n",
      "Epoch 5 | Step 3310900 | Avg Loss: 0.0148 | Grad Norm: 0.00941580\n",
      "Epoch 5 | Step 3311000 | Avg Loss: 0.0153 | Grad Norm: 0.00798135\n",
      "Epoch 5 | Step 3311100 | Avg Loss: 0.0151 | Grad Norm: 0.00802652\n",
      "Epoch 5 | Step 3311200 | Avg Loss: 0.0149 | Grad Norm: 0.00792684\n",
      "Epoch 5 | Step 3311300 | Avg Loss: 0.0151 | Grad Norm: 0.00805788\n",
      "Epoch 5 | Step 3311400 | Avg Loss: 0.0147 | Grad Norm: 0.00780537\n",
      "Epoch 5 | Step 3311500 | Avg Loss: 0.0147 | Grad Norm: 0.00765905\n",
      "Epoch 5 | Step 3311600 | Avg Loss: 0.0151 | Grad Norm: 0.00856020\n",
      "Epoch 5 | Step 3311700 | Avg Loss: 0.0156 | Grad Norm: 0.00923661\n",
      "Epoch 5 | Step 3311800 | Avg Loss: 0.0152 | Grad Norm: 0.01017060\n",
      "Epoch 5 | Step 3311900 | Avg Loss: 0.0156 | Grad Norm: 0.01304693\n",
      "Epoch 5 | Step 3312000 | Avg Loss: 0.0158 | Grad Norm: 0.00878720\n",
      "Epoch 5 | Step 3312100 | Avg Loss: 0.0152 | Grad Norm: 0.01053577\n",
      "Epoch 5 | Step 3312200 | Avg Loss: 0.0158 | Grad Norm: 0.00794050\n",
      "Epoch 5 | Step 3312300 | Avg Loss: 0.0159 | Grad Norm: 0.01173799\n",
      "Epoch 5 | Step 3312400 | Avg Loss: 0.0158 | Grad Norm: 0.00849064\n",
      "Epoch 5 | Step 3312500 | Avg Loss: 0.0160 | Grad Norm: 0.01044849\n",
      "Epoch 5 | Step 3312600 | Avg Loss: 0.0159 | Grad Norm: 0.00913359\n",
      "Epoch 5 | Step 3312700 | Avg Loss: 0.0162 | Grad Norm: 0.00906994\n",
      "Epoch 5 | Step 3312800 | Avg Loss: 0.0160 | Grad Norm: 0.00937218\n",
      "Epoch 5 | Step 3312900 | Avg Loss: 0.0165 | Grad Norm: 0.00832693\n",
      "Epoch 5 | Step 3313000 | Avg Loss: 0.0163 | Grad Norm: 0.00857770\n",
      "Epoch 5 | Step 3313100 | Avg Loss: 0.0161 | Grad Norm: 0.01126247\n",
      "Epoch 5 | Step 3313200 | Avg Loss: 0.0160 | Grad Norm: 0.00858133\n",
      "Epoch 5 | Step 3313300 | Avg Loss: 0.0161 | Grad Norm: 0.00942172\n",
      "Epoch 5 | Step 3313400 | Avg Loss: 0.0155 | Grad Norm: 0.00960248\n",
      "Epoch 5 | Step 3313500 | Avg Loss: 0.0158 | Grad Norm: 0.00926463\n",
      "Epoch 5 | Step 3313600 | Avg Loss: 0.0157 | Grad Norm: 0.00793310\n",
      "Epoch 5 | Step 3313700 | Avg Loss: 0.0159 | Grad Norm: 0.01015488\n",
      "Epoch 5 | Step 3313800 | Avg Loss: 0.0156 | Grad Norm: 0.00986904\n",
      "Epoch 5 | Step 3313900 | Avg Loss: 0.0153 | Grad Norm: 0.00883450\n",
      "Epoch 5 | Step 3314000 | Avg Loss: 0.0155 | Grad Norm: 0.01392231\n",
      "Epoch 5 | Step 3314100 | Avg Loss: 0.0158 | Grad Norm: 0.01013096\n",
      "Epoch 5 | Step 3314200 | Avg Loss: 0.0160 | Grad Norm: 0.00846975\n",
      "Epoch 5 | Step 3314300 | Avg Loss: 0.0158 | Grad Norm: 0.00920727\n",
      "Epoch 5 | Step 3314400 | Avg Loss: 0.0157 | Grad Norm: 0.01005925\n",
      "Epoch 5 | Step 3314500 | Avg Loss: 0.0153 | Grad Norm: 0.00897345\n",
      "Epoch 5 | Step 3314600 | Avg Loss: 0.0152 | Grad Norm: 0.00739341\n",
      "Epoch 5 | Step 3314700 | Avg Loss: 0.0152 | Grad Norm: 0.00911916\n",
      "Epoch 5 | Step 3314800 | Avg Loss: 0.0152 | Grad Norm: 0.01193468\n",
      "Epoch 5 | Step 3314900 | Avg Loss: 0.0150 | Grad Norm: 0.00881008\n",
      "Epoch 5 | Step 3315000 | Avg Loss: 0.0152 | Grad Norm: 0.00845722\n",
      "Epoch 5 | Step 3315100 | Avg Loss: 0.0153 | Grad Norm: 0.00898158\n",
      "Epoch 5 | Step 3315200 | Avg Loss: 0.0152 | Grad Norm: 0.00895036\n",
      "Epoch 5 | Step 3315300 | Avg Loss: 0.0153 | Grad Norm: 0.01144032\n",
      "Epoch 5 | Step 3315400 | Avg Loss: 0.0159 | Grad Norm: 0.01178788\n",
      "Epoch 5 | Step 3315500 | Avg Loss: 0.0158 | Grad Norm: 0.01068086\n",
      "Epoch 5 | Step 3315600 | Avg Loss: 0.0157 | Grad Norm: 0.00942569\n",
      "Epoch 5 | Step 3315700 | Avg Loss: 0.0157 | Grad Norm: 0.01006372\n",
      "Epoch 5 | Step 3315800 | Avg Loss: 0.0154 | Grad Norm: 0.00911398\n",
      "Epoch 5 | Step 3315900 | Avg Loss: 0.0153 | Grad Norm: 0.00951487\n",
      "Epoch 5 | Step 3316000 | Avg Loss: 0.0152 | Grad Norm: 0.00908819\n",
      "Epoch 5 | Step 3316100 | Avg Loss: 0.0155 | Grad Norm: 0.00961227\n",
      "Epoch 5 | Step 3316200 | Avg Loss: 0.0152 | Grad Norm: 0.00820690\n",
      "Epoch 5 | Step 3316300 | Avg Loss: 0.0151 | Grad Norm: 0.01008768\n",
      "Epoch 5 | Step 3316400 | Avg Loss: 0.0153 | Grad Norm: 0.01003388\n",
      "Epoch 5 | Step 3316500 | Avg Loss: 0.0155 | Grad Norm: 0.01078577\n",
      "Epoch 5 | Step 3316600 | Avg Loss: 0.0153 | Grad Norm: 0.00916133\n",
      "Epoch 5 | Step 3316700 | Avg Loss: 0.0153 | Grad Norm: 0.01093018\n",
      "Epoch 5 | Step 3316800 | Avg Loss: 0.0153 | Grad Norm: 0.00965500\n",
      "Epoch 5 | Step 3316900 | Avg Loss: 0.0156 | Grad Norm: 0.00828625\n",
      "Epoch 5 | Step 3317000 | Avg Loss: 0.0155 | Grad Norm: 0.01095739\n",
      "Epoch 5 | Step 3317100 | Avg Loss: 0.0155 | Grad Norm: 0.00878809\n",
      "Epoch 5 | Step 3317200 | Avg Loss: 0.0155 | Grad Norm: 0.00846657\n",
      "Epoch 5 | Step 3317300 | Avg Loss: 0.0155 | Grad Norm: 0.00845806\n",
      "Epoch 5 | Step 3317400 | Avg Loss: 0.0154 | Grad Norm: 0.00958565\n",
      "Epoch 5 | Step 3317500 | Avg Loss: 0.0157 | Grad Norm: 0.00949701\n",
      "Epoch 5 | Step 3317600 | Avg Loss: 0.0156 | Grad Norm: 0.00928132\n",
      "Epoch 5 | Step 3317700 | Avg Loss: 0.0152 | Grad Norm: 0.00910897\n",
      "Epoch 5 | Step 3317800 | Avg Loss: 0.0152 | Grad Norm: 0.00950563\n",
      "Epoch 5 | Step 3317900 | Avg Loss: 0.0157 | Grad Norm: 0.00975363\n",
      "Epoch 5 | Step 3318000 | Avg Loss: 0.0158 | Grad Norm: 0.00965842\n",
      "Epoch 5 | Step 3318100 | Avg Loss: 0.0158 | Grad Norm: 0.00976818\n",
      "Epoch 5 | Step 3318200 | Avg Loss: 0.0155 | Grad Norm: 0.01013687\n",
      "Epoch 5 | Step 3318300 | Avg Loss: 0.0160 | Grad Norm: 0.00755521\n",
      "Epoch 5 | Step 3318400 | Avg Loss: 0.0159 | Grad Norm: 0.00896728\n",
      "Epoch 5 | Step 3318500 | Avg Loss: 0.0160 | Grad Norm: 0.00966831\n",
      "Epoch 5 | Step 3318600 | Avg Loss: 0.0160 | Grad Norm: 0.00918618\n",
      "Epoch 5 | Step 3318700 | Avg Loss: 0.0156 | Grad Norm: 0.00930948\n",
      "Epoch 5 | Step 3318800 | Avg Loss: 0.0153 | Grad Norm: 0.00791290\n",
      "Epoch 5 | Step 3318900 | Avg Loss: 0.0156 | Grad Norm: 0.00790329\n",
      "Epoch 5 | Step 3319000 | Avg Loss: 0.0159 | Grad Norm: 0.00841880\n",
      "Epoch 5 | Step 3319100 | Avg Loss: 0.0159 | Grad Norm: 0.00933322\n",
      "Epoch 5 | Step 3319200 | Avg Loss: 0.0158 | Grad Norm: 0.01053441\n",
      "Epoch 5 | Step 3319300 | Avg Loss: 0.0156 | Grad Norm: 0.01086578\n",
      "Epoch 5 | Step 3319400 | Avg Loss: 0.0152 | Grad Norm: 0.01093823\n",
      "Epoch 5 | Step 3319500 | Avg Loss: 0.0151 | Grad Norm: 0.00861739\n",
      "Epoch 5 | Step 3319600 | Avg Loss: 0.0154 | Grad Norm: 0.00885960\n",
      "Epoch 5 | Step 3319700 | Avg Loss: 0.0152 | Grad Norm: 0.00897391\n",
      "Epoch 5 | Step 3319800 | Avg Loss: 0.0152 | Grad Norm: 0.01034850\n",
      "Epoch 5 | Step 3319900 | Avg Loss: 0.0156 | Grad Norm: 0.00903893\n",
      "Epoch 5 | Step 3320000 | Avg Loss: 0.0151 | Grad Norm: 0.00893551\n",
      "Epoch 5 | Step 3320100 | Avg Loss: 0.0157 | Grad Norm: 0.01094223\n",
      "Epoch 5 | Step 3320200 | Avg Loss: 0.0157 | Grad Norm: 0.00961309\n",
      "Epoch 5 | Step 3320300 | Avg Loss: 0.0158 | Grad Norm: 0.01069268\n",
      "Epoch 5 | Step 3320400 | Avg Loss: 0.0159 | Grad Norm: 0.00838345\n",
      "Epoch 5 | Step 3320500 | Avg Loss: 0.0160 | Grad Norm: 0.00855169\n",
      "Epoch 5 | Step 3320600 | Avg Loss: 0.0159 | Grad Norm: 0.00900097\n",
      "Epoch 5 | Step 3320700 | Avg Loss: 0.0155 | Grad Norm: 0.00892210\n",
      "Epoch 5 | Step 3320800 | Avg Loss: 0.0157 | Grad Norm: 0.00910554\n",
      "Epoch 5 | Step 3320900 | Avg Loss: 0.0156 | Grad Norm: 0.00931197\n",
      "Epoch 5 | Step 3321000 | Avg Loss: 0.0157 | Grad Norm: 0.00982201\n",
      "Epoch 5 | Step 3321100 | Avg Loss: 0.0157 | Grad Norm: 0.00820142\n",
      "Epoch 5 | Step 3321200 | Avg Loss: 0.0156 | Grad Norm: 0.00903463\n",
      "Epoch 5 | Step 3321300 | Avg Loss: 0.0155 | Grad Norm: 0.00936153\n",
      "Epoch 5 | Step 3321400 | Avg Loss: 0.0153 | Grad Norm: 0.00916611\n",
      "Epoch 5 | Step 3321500 | Avg Loss: 0.0153 | Grad Norm: 0.00916313\n",
      "Epoch 5 | Step 3321600 | Avg Loss: 0.0153 | Grad Norm: 0.00889838\n",
      "Epoch 5 | Step 3321700 | Avg Loss: 0.0151 | Grad Norm: 0.00886856\n",
      "Epoch 5 | Step 3321800 | Avg Loss: 0.0154 | Grad Norm: 0.00825760\n",
      "Epoch 5 | Step 3321900 | Avg Loss: 0.0153 | Grad Norm: 0.00773574\n",
      "Epoch 5 | Step 3322000 | Avg Loss: 0.0153 | Grad Norm: 0.00913431\n",
      "Epoch 5 | Step 3322100 | Avg Loss: 0.0157 | Grad Norm: 0.00976575\n",
      "Epoch 5 | Step 3322200 | Avg Loss: 0.0155 | Grad Norm: 0.00882891\n",
      "Epoch 5 | Step 3322300 | Avg Loss: 0.0154 | Grad Norm: 0.00881829\n",
      "Epoch 5 | Step 3322400 | Avg Loss: 0.0151 | Grad Norm: 0.00849203\n",
      "Epoch 5 | Step 3322500 | Avg Loss: 0.0152 | Grad Norm: 0.00874862\n",
      "Epoch 5 | Step 3322600 | Avg Loss: 0.0156 | Grad Norm: 0.00918405\n",
      "Epoch 5 | Step 3322700 | Avg Loss: 0.0155 | Grad Norm: 0.00926085\n",
      "Epoch 5 | Step 3322800 | Avg Loss: 0.0158 | Grad Norm: 0.01071699\n",
      "Epoch 5 | Step 3322900 | Avg Loss: 0.0157 | Grad Norm: 0.00920138\n",
      "Epoch 5 | Step 3323000 | Avg Loss: 0.0158 | Grad Norm: 0.00997103\n",
      "Epoch 5 | Step 3323100 | Avg Loss: 0.0156 | Grad Norm: 0.00880001\n",
      "Epoch 5 | Step 3323200 | Avg Loss: 0.0154 | Grad Norm: 0.00976618\n",
      "Epoch 5 | Step 3323300 | Avg Loss: 0.0154 | Grad Norm: 0.00834834\n",
      "Epoch 5 | Step 3323400 | Avg Loss: 0.0154 | Grad Norm: 0.00863661\n",
      "Epoch 5 | Step 3323500 | Avg Loss: 0.0156 | Grad Norm: 0.00869788\n",
      "Epoch 5 | Step 3323600 | Avg Loss: 0.0154 | Grad Norm: 0.00906958\n",
      "Epoch 5 | Step 3323700 | Avg Loss: 0.0154 | Grad Norm: 0.00916944\n",
      "Epoch 5 | Step 3323800 | Avg Loss: 0.0153 | Grad Norm: 0.00899921\n",
      "Epoch 5 | Step 3323900 | Avg Loss: 0.0155 | Grad Norm: 0.00998040\n",
      "Epoch 5 | Step 3324000 | Avg Loss: 0.0156 | Grad Norm: 0.00903616\n",
      "Epoch 5 | Step 3324100 | Avg Loss: 0.0156 | Grad Norm: 0.01023566\n",
      "Epoch 5 | Step 3324200 | Avg Loss: 0.0156 | Grad Norm: 0.00887505\n",
      "Epoch 5 | Step 3324300 | Avg Loss: 0.0159 | Grad Norm: 0.00965570\n",
      "Epoch 5 | Step 3324400 | Avg Loss: 0.0159 | Grad Norm: 0.00955902\n",
      "Epoch 5 | Step 3324500 | Avg Loss: 0.0161 | Grad Norm: 0.00849650\n",
      "Epoch 5 | Step 3324600 | Avg Loss: 0.0161 | Grad Norm: 0.00881141\n",
      "Epoch 5 | Step 3324700 | Avg Loss: 0.0161 | Grad Norm: 0.01076346\n",
      "Epoch 5 | Step 3324800 | Avg Loss: 0.0158 | Grad Norm: 0.00919440\n",
      "Epoch 5 | Step 3324900 | Avg Loss: 0.0153 | Grad Norm: 0.00898176\n",
      "Epoch 5 | Step 3325000 | Avg Loss: 0.0155 | Grad Norm: 0.00900637\n",
      "Epoch 5 | Step 3325100 | Avg Loss: 0.0154 | Grad Norm: 0.00942389\n",
      "Epoch 5 | Step 3325200 | Avg Loss: 0.0151 | Grad Norm: 0.01046745\n",
      "Epoch 5 | Step 3325300 | Avg Loss: 0.0153 | Grad Norm: 0.01044504\n",
      "Epoch 5 | Step 3325400 | Avg Loss: 0.0153 | Grad Norm: 0.00925905\n",
      "Epoch 5 | Step 3325500 | Avg Loss: 0.0154 | Grad Norm: 0.01061843\n",
      "Epoch 5 | Step 3325600 | Avg Loss: 0.0156 | Grad Norm: 0.00830652\n",
      "Epoch 5 | Step 3325700 | Avg Loss: 0.0155 | Grad Norm: 0.00889035\n",
      "Epoch 5 | Step 3325800 | Avg Loss: 0.0154 | Grad Norm: 0.00885875\n",
      "Epoch 5 | Step 3325900 | Avg Loss: 0.0157 | Grad Norm: 0.00951106\n",
      "Epoch 5 | Step 3326000 | Avg Loss: 0.0158 | Grad Norm: 0.00943341\n",
      "Epoch 5 | Step 3326100 | Avg Loss: 0.0155 | Grad Norm: 0.00794737\n",
      "Epoch 5 | Step 3326200 | Avg Loss: 0.0157 | Grad Norm: 0.01545730\n",
      "Epoch 5 | Step 3326300 | Avg Loss: 0.0156 | Grad Norm: 0.00874439\n",
      "Epoch 5 | Step 3326400 | Avg Loss: 0.0154 | Grad Norm: 0.00791262\n",
      "Epoch 5 | Step 3326500 | Avg Loss: 0.0156 | Grad Norm: 0.00942873\n",
      "Epoch 5 | Step 3326600 | Avg Loss: 0.0157 | Grad Norm: 0.00895980\n",
      "Epoch 5 | Step 3326700 | Avg Loss: 0.0154 | Grad Norm: 0.00922693\n",
      "Epoch 5 | Step 3326800 | Avg Loss: 0.0151 | Grad Norm: 0.00817281\n",
      "Epoch 5 | Step 3326900 | Avg Loss: 0.0150 | Grad Norm: 0.00921211\n",
      "Epoch 5 | Step 3327000 | Avg Loss: 0.0153 | Grad Norm: 0.00914660\n",
      "Epoch 5 | Step 3327100 | Avg Loss: 0.0154 | Grad Norm: 0.00884871\n",
      "Epoch 5 | Step 3327200 | Avg Loss: 0.0156 | Grad Norm: 0.01003566\n",
      "Epoch 5 | Step 3327300 | Avg Loss: 0.0154 | Grad Norm: 0.00842064\n",
      "Epoch 5 | Step 3327400 | Avg Loss: 0.0151 | Grad Norm: 0.00786478\n",
      "Epoch 5 | Step 3327500 | Avg Loss: 0.0154 | Grad Norm: 0.01029098\n",
      "Epoch 5 | Step 3327600 | Avg Loss: 0.0158 | Grad Norm: 0.00905705\n",
      "Epoch 5 | Step 3327700 | Avg Loss: 0.0158 | Grad Norm: 0.00935192\n",
      "Epoch 5 | Step 3327800 | Avg Loss: 0.0164 | Grad Norm: 0.00925400\n",
      "Epoch 5 | Step 3327900 | Avg Loss: 0.0163 | Grad Norm: 0.00833573\n",
      "Epoch 5 | Step 3328000 | Avg Loss: 0.0161 | Grad Norm: 0.00936528\n",
      "Epoch 5 | Step 3328100 | Avg Loss: 0.0164 | Grad Norm: 0.00940617\n",
      "Epoch 5 | Step 3328200 | Avg Loss: 0.0161 | Grad Norm: 0.00806992\n",
      "Epoch 5 | Step 3328300 | Avg Loss: 0.0156 | Grad Norm: 0.00838782\n",
      "Epoch 5 | Step 3328400 | Avg Loss: 0.0155 | Grad Norm: 0.00841739\n",
      "Epoch 5 | Step 3328500 | Avg Loss: 0.0156 | Grad Norm: 0.00909738\n",
      "Epoch 5 | Step 3328600 | Avg Loss: 0.0156 | Grad Norm: 0.00850244\n",
      "Epoch 5 | Step 3328700 | Avg Loss: 0.0160 | Grad Norm: 0.00894688\n",
      "Epoch 5 | Step 3328800 | Avg Loss: 0.0161 | Grad Norm: 0.01073114\n",
      "Epoch 5 | Step 3328900 | Avg Loss: 0.0158 | Grad Norm: 0.00821306\n",
      "Epoch 5 | Step 3329000 | Avg Loss: 0.0156 | Grad Norm: 0.01019199\n",
      "Epoch 5 | Step 3329100 | Avg Loss: 0.0157 | Grad Norm: 0.00833412\n",
      "Epoch 5 | Step 3329200 | Avg Loss: 0.0159 | Grad Norm: 0.00919146\n",
      "Epoch 5 | Step 3329300 | Avg Loss: 0.0157 | Grad Norm: 0.00942067\n",
      "Epoch 5 | Step 3329400 | Avg Loss: 0.0157 | Grad Norm: 0.00941620\n",
      "Epoch 5 | Step 3329500 | Avg Loss: 0.0153 | Grad Norm: 0.00916565\n",
      "Epoch 5 | Step 3329600 | Avg Loss: 0.0156 | Grad Norm: 0.00938876\n",
      "Epoch 5 | Step 3329700 | Avg Loss: 0.0156 | Grad Norm: 0.01073616\n",
      "Epoch 5 | Step 3329800 | Avg Loss: 0.0153 | Grad Norm: 0.01022525\n",
      "Epoch 5 | Step 3329900 | Avg Loss: 0.0153 | Grad Norm: 0.00972743\n",
      "Epoch 5 | Step 3330000 | Avg Loss: 0.0152 | Grad Norm: 0.01018955\n",
      "Epoch 5 | Step 3330100 | Avg Loss: 0.0154 | Grad Norm: 0.00933541\n",
      "Epoch 5 | Step 3330200 | Avg Loss: 0.0150 | Grad Norm: 0.00933459\n",
      "Epoch 5 | Step 3330300 | Avg Loss: 0.0154 | Grad Norm: 0.00826820\n",
      "Epoch 5 | Step 3330400 | Avg Loss: 0.0155 | Grad Norm: 0.01029154\n",
      "Epoch 5 | Step 3330500 | Avg Loss: 0.0155 | Grad Norm: 0.00985091\n",
      "Epoch 5 | Step 3330600 | Avg Loss: 0.0151 | Grad Norm: 0.00718697\n",
      "Epoch 5 | Step 3330700 | Avg Loss: 0.0152 | Grad Norm: 0.00975346\n",
      "Epoch 5 | Step 3330800 | Avg Loss: 0.0158 | Grad Norm: 0.00961829\n",
      "Epoch 5 | Step 3330900 | Avg Loss: 0.0161 | Grad Norm: 0.00960008\n",
      "Epoch 5 | Step 3331000 | Avg Loss: 0.0156 | Grad Norm: 0.01038280\n",
      "Epoch 5 | Step 3331100 | Avg Loss: 0.0161 | Grad Norm: 0.01203582\n",
      "Epoch 5 | Step 3331200 | Avg Loss: 0.0157 | Grad Norm: 0.00847631\n",
      "Epoch 5 | Step 3331300 | Avg Loss: 0.0158 | Grad Norm: 0.00776906\n",
      "Epoch 5 | Step 3331400 | Avg Loss: 0.0156 | Grad Norm: 0.00958855\n",
      "Epoch 5 | Step 3331500 | Avg Loss: 0.0155 | Grad Norm: 0.01002379\n",
      "Epoch 5 | Step 3331600 | Avg Loss: 0.0156 | Grad Norm: 0.00910664\n",
      "Epoch 5 | Step 3331700 | Avg Loss: 0.0158 | Grad Norm: 0.00906432\n",
      "Epoch 5 | Step 3331800 | Avg Loss: 0.0160 | Grad Norm: 0.00870320\n",
      "Epoch 5 | Step 3331900 | Avg Loss: 0.0158 | Grad Norm: 0.00912716\n",
      "Epoch 5 | Step 3332000 | Avg Loss: 0.0158 | Grad Norm: 0.00933675\n",
      "Epoch 5 | Step 3332100 | Avg Loss: 0.0156 | Grad Norm: 0.00997068\n",
      "Epoch 5 | Step 3332200 | Avg Loss: 0.0159 | Grad Norm: 0.00870677\n",
      "Epoch 5 | Step 3332300 | Avg Loss: 0.0154 | Grad Norm: 0.01025901\n",
      "Epoch 5 | Step 3332400 | Avg Loss: 0.0158 | Grad Norm: 0.00852475\n",
      "Epoch 5 | Step 3332500 | Avg Loss: 0.0156 | Grad Norm: 0.01066693\n",
      "Epoch 5 | Step 3332600 | Avg Loss: 0.0151 | Grad Norm: 0.00849953\n",
      "Epoch 5 | Step 3332700 | Avg Loss: 0.0155 | Grad Norm: 0.00991169\n",
      "Epoch 5 | Step 3332800 | Avg Loss: 0.0155 | Grad Norm: 0.00883533\n",
      "Epoch 5 | Step 3332900 | Avg Loss: 0.0156 | Grad Norm: 0.00844193\n",
      "Epoch 5 | Step 3333000 | Avg Loss: 0.0157 | Grad Norm: 0.00970750\n",
      "Epoch 5 | Step 3333100 | Avg Loss: 0.0159 | Grad Norm: 0.00943583\n",
      "Epoch 5 | Step 3333200 | Avg Loss: 0.0160 | Grad Norm: 0.00923463\n",
      "Epoch 5 | Step 3333300 | Avg Loss: 0.0158 | Grad Norm: 0.00922947\n",
      "Epoch 5 | Step 3333400 | Avg Loss: 0.0158 | Grad Norm: 0.00938121\n",
      "Epoch 5 | Step 3333500 | Avg Loss: 0.0160 | Grad Norm: 0.01043701\n",
      "Epoch 5 | Step 3333600 | Avg Loss: 0.0162 | Grad Norm: 0.00819815\n",
      "Epoch 5 | Step 3333700 | Avg Loss: 0.0160 | Grad Norm: 0.00976795\n",
      "Epoch 5 | Step 3333800 | Avg Loss: 0.0156 | Grad Norm: 0.00895164\n",
      "Epoch 5 | Step 3333900 | Avg Loss: 0.0159 | Grad Norm: 0.00900344\n",
      "Epoch 5 | Step 3334000 | Avg Loss: 0.0160 | Grad Norm: 0.00879598\n",
      "Epoch 5 | Step 3334100 | Avg Loss: 0.0159 | Grad Norm: 0.00879430\n",
      "Epoch 5 | Step 3334200 | Avg Loss: 0.0160 | Grad Norm: 0.01115801\n",
      "Epoch 5 | Step 3334300 | Avg Loss: 0.0160 | Grad Norm: 0.00903851\n",
      "Epoch 5 | Step 3334400 | Avg Loss: 0.0161 | Grad Norm: 0.00949505\n",
      "Epoch 5 | Step 3334500 | Avg Loss: 0.0157 | Grad Norm: 0.00837476\n",
      "Epoch 5 | Step 3334600 | Avg Loss: 0.0156 | Grad Norm: 0.00921813\n",
      "Epoch 5 | Step 3334700 | Avg Loss: 0.0154 | Grad Norm: 0.00965796\n",
      "Epoch 5 | Step 3334800 | Avg Loss: 0.0154 | Grad Norm: 0.00866219\n",
      "Epoch 5 | Step 3334900 | Avg Loss: 0.0156 | Grad Norm: 0.00833107\n",
      "Epoch 5 | Step 3335000 | Avg Loss: 0.0158 | Grad Norm: 0.01004918\n",
      "Epoch 5 | Step 3335100 | Avg Loss: 0.0155 | Grad Norm: 0.00940663\n",
      "Epoch 5 | Step 3335200 | Avg Loss: 0.0160 | Grad Norm: 0.00832341\n",
      "Epoch 5 | Step 3335300 | Avg Loss: 0.0158 | Grad Norm: 0.00900727\n",
      "Epoch 5 | Step 3335400 | Avg Loss: 0.0157 | Grad Norm: 0.00930346\n",
      "Epoch 5 | Step 3335500 | Avg Loss: 0.0155 | Grad Norm: 0.00847238\n",
      "Epoch 5 | Step 3335600 | Avg Loss: 0.0156 | Grad Norm: 0.00942375\n",
      "Epoch 5 | Step 3335700 | Avg Loss: 0.0157 | Grad Norm: 0.00856467\n",
      "Epoch 5 | Step 3335800 | Avg Loss: 0.0156 | Grad Norm: 0.01152613\n",
      "Epoch 5 | Step 3335900 | Avg Loss: 0.0156 | Grad Norm: 0.01041210\n",
      "Epoch 5 | Step 3336000 | Avg Loss: 0.0151 | Grad Norm: 0.00872396\n",
      "Epoch 5 | Step 3336100 | Avg Loss: 0.0148 | Grad Norm: 0.00766302\n",
      "Epoch 5 | Step 3336200 | Avg Loss: 0.0150 | Grad Norm: 0.00903529\n",
      "Epoch 5 | Step 3336300 | Avg Loss: 0.0150 | Grad Norm: 0.01028780\n",
      "Epoch 5 | Step 3336400 | Avg Loss: 0.0151 | Grad Norm: 0.00913926\n",
      "Epoch 5 | Step 3336500 | Avg Loss: 0.0151 | Grad Norm: 0.00872388\n",
      "Epoch 5 | Step 3336600 | Avg Loss: 0.0151 | Grad Norm: 0.00822007\n",
      "Epoch 5 | Step 3336700 | Avg Loss: 0.0152 | Grad Norm: 0.00850632\n",
      "Epoch 5 | Step 3336800 | Avg Loss: 0.0153 | Grad Norm: 0.00931695\n",
      "Epoch 5 | Step 3336900 | Avg Loss: 0.0153 | Grad Norm: 0.00889821\n",
      "Epoch 5 | Step 3337000 | Avg Loss: 0.0159 | Grad Norm: 0.01018450\n",
      "Epoch 5 | Step 3337100 | Avg Loss: 0.0162 | Grad Norm: 0.00856632\n",
      "Epoch 5 | Step 3337200 | Avg Loss: 0.0159 | Grad Norm: 0.00828377\n",
      "Epoch 5 | Step 3337300 | Avg Loss: 0.0158 | Grad Norm: 0.01082249\n",
      "Epoch 5 | Step 3337400 | Avg Loss: 0.0159 | Grad Norm: 0.00939418\n",
      "Epoch 5 | Step 3337500 | Avg Loss: 0.0159 | Grad Norm: 0.00818591\n",
      "Epoch 5 | Step 3337600 | Avg Loss: 0.0157 | Grad Norm: 0.00955211\n",
      "Epoch 5 | Step 3337700 | Avg Loss: 0.0156 | Grad Norm: 0.00995757\n",
      "Epoch 5 | Step 3337800 | Avg Loss: 0.0157 | Grad Norm: 0.01195495\n",
      "Epoch 5 | Step 3337900 | Avg Loss: 0.0154 | Grad Norm: 0.00970173\n",
      "Epoch 5 | Step 3338000 | Avg Loss: 0.0154 | Grad Norm: 0.00829727\n",
      "Epoch 5 | Step 3338100 | Avg Loss: 0.0151 | Grad Norm: 0.00839625\n",
      "Epoch 5 | Step 3338200 | Avg Loss: 0.0151 | Grad Norm: 0.00979261\n",
      "Epoch 5 | Step 3338300 | Avg Loss: 0.0152 | Grad Norm: 0.00945828\n",
      "Epoch 5 | Step 3338400 | Avg Loss: 0.0148 | Grad Norm: 0.00946011\n",
      "Epoch 5 | Step 3338500 | Avg Loss: 0.0150 | Grad Norm: 0.00935072\n",
      "Epoch 5 | Step 3338600 | Avg Loss: 0.0154 | Grad Norm: 0.00787078\n",
      "Epoch 5 | Step 3338700 | Avg Loss: 0.0157 | Grad Norm: 0.00854000\n",
      "Epoch 5 | Step 3338800 | Avg Loss: 0.0157 | Grad Norm: 0.01014395\n",
      "Epoch 5 | Step 3338900 | Avg Loss: 0.0155 | Grad Norm: 0.00947123\n",
      "Epoch 5 | Step 3339000 | Avg Loss: 0.0155 | Grad Norm: 0.00983684\n",
      "Epoch 5 | Step 3339100 | Avg Loss: 0.0158 | Grad Norm: 0.00867665\n",
      "Epoch 5 | Step 3339200 | Avg Loss: 0.0156 | Grad Norm: 0.01116850\n",
      "Epoch 5 | Step 3339300 | Avg Loss: 0.0156 | Grad Norm: 0.00954275\n",
      "Epoch 5 | Step 3339400 | Avg Loss: 0.0157 | Grad Norm: 0.00938560\n",
      "Epoch 5 | Step 3339500 | Avg Loss: 0.0155 | Grad Norm: 0.00961225\n",
      "Epoch 5 | Step 3339600 | Avg Loss: 0.0152 | Grad Norm: 0.00874132\n",
      "Epoch 5 | Step 3339700 | Avg Loss: 0.0151 | Grad Norm: 0.00913479\n",
      "Epoch 5 | Step 3339800 | Avg Loss: 0.0147 | Grad Norm: 0.00803634\n",
      "Epoch 5 | Step 3339900 | Avg Loss: 0.0153 | Grad Norm: 0.00811608\n",
      "Epoch 5 | Step 3340000 | Avg Loss: 0.0153 | Grad Norm: 0.00823326\n",
      "Epoch 5 | Step 3340100 | Avg Loss: 0.0154 | Grad Norm: 0.00892975\n",
      "Epoch 5 | Step 3340200 | Avg Loss: 0.0155 | Grad Norm: 0.00901434\n",
      "Epoch 5 | Step 3340300 | Avg Loss: 0.0154 | Grad Norm: 0.00849265\n",
      "Epoch 5 | Step 3340400 | Avg Loss: 0.0152 | Grad Norm: 0.00856359\n",
      "Epoch 5 | Step 3340500 | Avg Loss: 0.0154 | Grad Norm: 0.00819030\n",
      "Epoch 5 | Step 3340600 | Avg Loss: 0.0153 | Grad Norm: 0.00825056\n",
      "Epoch 5 | Step 3340700 | Avg Loss: 0.0154 | Grad Norm: 0.00843595\n",
      "Epoch 5 | Step 3340800 | Avg Loss: 0.0151 | Grad Norm: 0.00842603\n",
      "Epoch 5 | Step 3340900 | Avg Loss: 0.0150 | Grad Norm: 0.00911024\n",
      "Epoch 5 | Step 3341000 | Avg Loss: 0.0148 | Grad Norm: 0.00882460\n",
      "Epoch 5 | Step 3341100 | Avg Loss: 0.0151 | Grad Norm: 0.00766211\n",
      "Epoch 5 | Step 3341200 | Avg Loss: 0.0154 | Grad Norm: 0.00986786\n",
      "Epoch 5 | Step 3341300 | Avg Loss: 0.0154 | Grad Norm: 0.00858314\n",
      "Epoch 5 | Step 3341400 | Avg Loss: 0.0154 | Grad Norm: 0.00824901\n",
      "Epoch 5 | Step 3341500 | Avg Loss: 0.0152 | Grad Norm: 0.00824629\n",
      "Epoch 5 | Step 3341600 | Avg Loss: 0.0152 | Grad Norm: 0.00835556\n",
      "Epoch 5 | Step 3341700 | Avg Loss: 0.0151 | Grad Norm: 0.00846163\n",
      "Epoch 5 | Step 3341800 | Avg Loss: 0.0154 | Grad Norm: 0.01080812\n",
      "Epoch 5 | Step 3341900 | Avg Loss: 0.0153 | Grad Norm: 0.00895268\n",
      "Epoch 5 | Step 3342000 | Avg Loss: 0.0153 | Grad Norm: 0.00763921\n",
      "Epoch 5 | Step 3342100 | Avg Loss: 0.0157 | Grad Norm: 0.00913525\n",
      "Epoch 5 | Step 3342200 | Avg Loss: 0.0161 | Grad Norm: 0.00860743\n",
      "Epoch 5 | Step 3342300 | Avg Loss: 0.0157 | Grad Norm: 0.00929366\n",
      "Epoch 5 | Step 3342400 | Avg Loss: 0.0158 | Grad Norm: 0.00899598\n",
      "Epoch 5 | Step 3342500 | Avg Loss: 0.0156 | Grad Norm: 0.00887095\n",
      "Epoch 5 | Step 3342600 | Avg Loss: 0.0160 | Grad Norm: 0.00861914\n",
      "Epoch 5 | Step 3342700 | Avg Loss: 0.0159 | Grad Norm: 0.00836018\n",
      "Epoch 5 | Step 3342800 | Avg Loss: 0.0161 | Grad Norm: 0.00953878\n",
      "Epoch 5 | Step 3342900 | Avg Loss: 0.0164 | Grad Norm: 0.00883474\n",
      "Epoch 5 | Step 3343000 | Avg Loss: 0.0160 | Grad Norm: 0.01080249\n",
      "Epoch 5 | Step 3343100 | Avg Loss: 0.0156 | Grad Norm: 0.00917708\n",
      "Epoch 5 | Step 3343200 | Avg Loss: 0.0158 | Grad Norm: 0.00860036\n",
      "Epoch 5 | Step 3343300 | Avg Loss: 0.0154 | Grad Norm: 0.00895161\n",
      "Epoch 5 | Step 3343400 | Avg Loss: 0.0156 | Grad Norm: 0.00968261\n",
      "Epoch 5 | Step 3343500 | Avg Loss: 0.0156 | Grad Norm: 0.00986212\n",
      "Epoch 5 | Step 3343600 | Avg Loss: 0.0152 | Grad Norm: 0.01106128\n",
      "Epoch 5 | Step 3343700 | Avg Loss: 0.0150 | Grad Norm: 0.00898430\n",
      "Epoch 5 | Step 3343800 | Avg Loss: 0.0152 | Grad Norm: 0.01168984\n",
      "Epoch 5 | Step 3343900 | Avg Loss: 0.0155 | Grad Norm: 0.00962796\n",
      "Epoch 5 | Step 3344000 | Avg Loss: 0.0157 | Grad Norm: 0.01022904\n",
      "Epoch 5 | Step 3344100 | Avg Loss: 0.0161 | Grad Norm: 0.01030984\n",
      "Epoch 5 | Step 3344200 | Avg Loss: 0.0160 | Grad Norm: 0.01081720\n",
      "Epoch 5 | Step 3344300 | Avg Loss: 0.0157 | Grad Norm: 0.01019830\n",
      "Epoch 5 | Step 3344400 | Avg Loss: 0.0155 | Grad Norm: 0.01087981\n",
      "Epoch 5 | Step 3344500 | Avg Loss: 0.0157 | Grad Norm: 0.00891541\n",
      "Epoch 5 | Step 3344600 | Avg Loss: 0.0155 | Grad Norm: 0.00876631\n",
      "Epoch 5 | Step 3344700 | Avg Loss: 0.0154 | Grad Norm: 0.00788308\n",
      "Epoch 5 | Step 3344800 | Avg Loss: 0.0157 | Grad Norm: 0.01134973\n",
      "Epoch 5 | Step 3344900 | Avg Loss: 0.0155 | Grad Norm: 0.01010129\n",
      "Epoch 5 | Step 3345000 | Avg Loss: 0.0154 | Grad Norm: 0.00866890\n",
      "Epoch 5 | Step 3345100 | Avg Loss: 0.0152 | Grad Norm: 0.00943465\n",
      "Epoch 5 | Step 3345200 | Avg Loss: 0.0151 | Grad Norm: 0.00823578\n",
      "Epoch 5 | Step 3345300 | Avg Loss: 0.0154 | Grad Norm: 0.01122276\n",
      "Epoch 5 | Step 3345400 | Avg Loss: 0.0154 | Grad Norm: 0.00889387\n",
      "Epoch 5 | Step 3345500 | Avg Loss: 0.0153 | Grad Norm: 0.01109621\n",
      "Epoch 5 | Step 3345600 | Avg Loss: 0.0151 | Grad Norm: 0.00890681\n",
      "Epoch 5 | Step 3345700 | Avg Loss: 0.0153 | Grad Norm: 0.00876175\n",
      "Epoch 5 | Step 3345800 | Avg Loss: 0.0149 | Grad Norm: 0.01054573\n",
      "Epoch 5 | Step 3345900 | Avg Loss: 0.0153 | Grad Norm: 0.00979792\n",
      "Epoch 5 | Step 3346000 | Avg Loss: 0.0153 | Grad Norm: 0.01035970\n",
      "Epoch 5 | Step 3346100 | Avg Loss: 0.0152 | Grad Norm: 0.01147828\n",
      "Epoch 5 | Step 3346200 | Avg Loss: 0.0154 | Grad Norm: 0.00884535\n",
      "Epoch 5 | Step 3346300 | Avg Loss: 0.0152 | Grad Norm: 0.00812150\n",
      "Epoch 5 | Step 3346400 | Avg Loss: 0.0156 | Grad Norm: 0.00888221\n",
      "Epoch 5 | Step 3346500 | Avg Loss: 0.0160 | Grad Norm: 0.00788837\n",
      "Epoch 5 | Step 3346600 | Avg Loss: 0.0157 | Grad Norm: 0.00893504\n",
      "Epoch 5 | Step 3346700 | Avg Loss: 0.0159 | Grad Norm: 0.00997951\n",
      "Epoch 5 | Step 3346800 | Avg Loss: 0.0161 | Grad Norm: 0.00963105\n",
      "Epoch 5 | Step 3346900 | Avg Loss: 0.0158 | Grad Norm: 0.00973390\n",
      "Epoch 5 | Step 3347000 | Avg Loss: 0.0158 | Grad Norm: 0.00827877\n",
      "Epoch 5 | Step 3347100 | Avg Loss: 0.0155 | Grad Norm: 0.00997293\n",
      "Epoch 5 | Step 3347200 | Avg Loss: 0.0155 | Grad Norm: 0.01026614\n",
      "Epoch 5 | Step 3347300 | Avg Loss: 0.0155 | Grad Norm: 0.00952544\n",
      "Epoch 5 | Step 3347400 | Avg Loss: 0.0156 | Grad Norm: 0.00936860\n",
      "Epoch 5 | Step 3347500 | Avg Loss: 0.0154 | Grad Norm: 0.00919891\n",
      "Epoch 5 | Step 3347600 | Avg Loss: 0.0151 | Grad Norm: 0.00808185\n",
      "Epoch 5 | Step 3347700 | Avg Loss: 0.0154 | Grad Norm: 0.00847007\n",
      "Epoch 5 | Step 3347800 | Avg Loss: 0.0154 | Grad Norm: 0.00916768\n",
      "Epoch 5 | Step 3347900 | Avg Loss: 0.0155 | Grad Norm: 0.00851978\n",
      "Epoch 5 | Step 3348000 | Avg Loss: 0.0157 | Grad Norm: 0.00948086\n",
      "Epoch 5 | Step 3348100 | Avg Loss: 0.0158 | Grad Norm: 0.01320758\n",
      "Epoch 5 | Step 3348200 | Avg Loss: 0.0158 | Grad Norm: 0.00833836\n",
      "Epoch 5 | Step 3348300 | Avg Loss: 0.0154 | Grad Norm: 0.00833557\n",
      "Epoch 5 | Step 3348400 | Avg Loss: 0.0154 | Grad Norm: 0.00917650\n",
      "Epoch 5 | Step 3348500 | Avg Loss: 0.0154 | Grad Norm: 0.01114187\n",
      "Epoch 5 | Step 3348600 | Avg Loss: 0.0152 | Grad Norm: 0.01133166\n",
      "Epoch 5 | Step 3348700 | Avg Loss: 0.0153 | Grad Norm: 0.00795657\n",
      "Epoch 5 | Step 3348800 | Avg Loss: 0.0151 | Grad Norm: 0.00865192\n",
      "Epoch 5 | Step 3348900 | Avg Loss: 0.0149 | Grad Norm: 0.00998490\n",
      "Epoch 5 | Step 3349000 | Avg Loss: 0.0152 | Grad Norm: 0.00855113\n",
      "Epoch 5 | Step 3349100 | Avg Loss: 0.0151 | Grad Norm: 0.00928302\n",
      "Epoch 5 | Step 3349200 | Avg Loss: 0.0153 | Grad Norm: 0.00806992\n",
      "Epoch 5 | Step 3349300 | Avg Loss: 0.0155 | Grad Norm: 0.00843097\n",
      "Epoch 5 | Step 3349400 | Avg Loss: 0.0156 | Grad Norm: 0.00819142\n",
      "Epoch 5 | Step 3349500 | Avg Loss: 0.0155 | Grad Norm: 0.00910510\n",
      "Epoch 5 | Step 3349600 | Avg Loss: 0.0155 | Grad Norm: 0.00822385\n",
      "Epoch 5 | Step 3349700 | Avg Loss: 0.0159 | Grad Norm: 0.01042652\n",
      "Epoch 5 | Step 3349800 | Avg Loss: 0.0161 | Grad Norm: 0.00997850\n",
      "Epoch 5 | Step 3349900 | Avg Loss: 0.0158 | Grad Norm: 0.00927944\n",
      "Epoch 5 | Step 3350000 | Avg Loss: 0.0162 | Grad Norm: 0.00930647\n",
      "Epoch 5 | Step 3350100 | Avg Loss: 0.0159 | Grad Norm: 0.00872208\n",
      "Epoch 5 | Step 3350200 | Avg Loss: 0.0156 | Grad Norm: 0.00803947\n",
      "Epoch 5 | Step 3350300 | Avg Loss: 0.0155 | Grad Norm: 0.00930263\n",
      "Epoch 5 | Step 3350400 | Avg Loss: 0.0155 | Grad Norm: 0.00910167\n",
      "Epoch 5 | Step 3350500 | Avg Loss: 0.0156 | Grad Norm: 0.00825239\n",
      "Epoch 5 | Step 3350600 | Avg Loss: 0.0156 | Grad Norm: 0.00899456\n",
      "Epoch 5 | Step 3350700 | Avg Loss: 0.0157 | Grad Norm: 0.01001794\n",
      "Epoch 5 | Step 3350800 | Avg Loss: 0.0159 | Grad Norm: 0.00825900\n",
      "Epoch 5 | Step 3350900 | Avg Loss: 0.0159 | Grad Norm: 0.01536608\n",
      "Epoch 5 | Step 3351000 | Avg Loss: 0.0157 | Grad Norm: 0.00887324\n",
      "Epoch 5 | Step 3351100 | Avg Loss: 0.0156 | Grad Norm: 0.00954723\n",
      "Epoch 5 | Step 3351200 | Avg Loss: 0.0155 | Grad Norm: 0.00827172\n",
      "Epoch 5 | Step 3351300 | Avg Loss: 0.0152 | Grad Norm: 0.00998640\n",
      "Epoch 5 | Step 3351400 | Avg Loss: 0.0155 | Grad Norm: 0.00808350\n",
      "Epoch 5 | Step 3351500 | Avg Loss: 0.0158 | Grad Norm: 0.00905078\n",
      "Epoch 5 | Step 3351600 | Avg Loss: 0.0154 | Grad Norm: 0.00837259\n",
      "Epoch 5 | Step 3351700 | Avg Loss: 0.0152 | Grad Norm: 0.00966796\n",
      "Epoch 5 | Step 3351800 | Avg Loss: 0.0153 | Grad Norm: 0.00837689\n",
      "Epoch 5 | Step 3351900 | Avg Loss: 0.0149 | Grad Norm: 0.00982717\n",
      "Epoch 5 | Step 3352000 | Avg Loss: 0.0150 | Grad Norm: 0.00799395\n",
      "Epoch 5 | Step 3352100 | Avg Loss: 0.0151 | Grad Norm: 0.01072591\n",
      "Epoch 5 | Step 3352200 | Avg Loss: 0.0150 | Grad Norm: 0.00840373\n",
      "Epoch 5 | Step 3352300 | Avg Loss: 0.0151 | Grad Norm: 0.00881325\n",
      "Epoch 5 | Step 3352400 | Avg Loss: 0.0154 | Grad Norm: 0.00919132\n",
      "Epoch 5 | Step 3352500 | Avg Loss: 0.0158 | Grad Norm: 0.01007768\n",
      "Epoch 5 | Step 3352600 | Avg Loss: 0.0160 | Grad Norm: 0.00921306\n",
      "Epoch 5 | Step 3352700 | Avg Loss: 0.0159 | Grad Norm: 0.00877757\n",
      "Epoch 5 | Step 3352800 | Avg Loss: 0.0159 | Grad Norm: 0.00821443\n",
      "Epoch 5 | Step 3352900 | Avg Loss: 0.0158 | Grad Norm: 0.00918831\n",
      "Epoch 5 | Step 3353000 | Avg Loss: 0.0155 | Grad Norm: 0.00918793\n",
      "Epoch 5 | Step 3353100 | Avg Loss: 0.0157 | Grad Norm: 0.01007465\n",
      "Epoch 5 | Step 3353200 | Avg Loss: 0.0156 | Grad Norm: 0.01089509\n",
      "Epoch 5 | Step 3353300 | Avg Loss: 0.0157 | Grad Norm: 0.00865481\n",
      "Epoch 5 | Step 3353400 | Avg Loss: 0.0160 | Grad Norm: 0.00941475\n",
      "Epoch 5 | Step 3353500 | Avg Loss: 0.0158 | Grad Norm: 0.00916899\n",
      "Epoch 5 | Step 3353600 | Avg Loss: 0.0157 | Grad Norm: 0.01048675\n",
      "Epoch 5 | Step 3353700 | Avg Loss: 0.0158 | Grad Norm: 0.00958165\n",
      "Epoch 5 | Step 3353800 | Avg Loss: 0.0156 | Grad Norm: 0.00956864\n",
      "Epoch 5 | Step 3353900 | Avg Loss: 0.0151 | Grad Norm: 0.01071568\n",
      "Epoch 5 | Step 3354000 | Avg Loss: 0.0152 | Grad Norm: 0.00855565\n",
      "Epoch 5 | Step 3354100 | Avg Loss: 0.0153 | Grad Norm: 0.00881587\n",
      "Epoch 5 | Step 3354200 | Avg Loss: 0.0155 | Grad Norm: 0.00783350\n",
      "Epoch 5 | Step 3354300 | Avg Loss: 0.0155 | Grad Norm: 0.01075950\n",
      "Epoch 5 | Step 3354400 | Avg Loss: 0.0150 | Grad Norm: 0.00868237\n",
      "Epoch 5 | Step 3354500 | Avg Loss: 0.0150 | Grad Norm: 0.01010798\n",
      "Epoch 5 | Step 3354600 | Avg Loss: 0.0154 | Grad Norm: 0.00940201\n",
      "Epoch 5 | Step 3354700 | Avg Loss: 0.0151 | Grad Norm: 0.00889694\n",
      "Epoch 5 | Step 3354800 | Avg Loss: 0.0151 | Grad Norm: 0.00825512\n",
      "Epoch 5 | Step 3354900 | Avg Loss: 0.0154 | Grad Norm: 0.00908986\n",
      "Epoch 5 | Step 3355000 | Avg Loss: 0.0150 | Grad Norm: 0.01053423\n",
      "Epoch 5 | Step 3355100 | Avg Loss: 0.0151 | Grad Norm: 0.00880368\n",
      "Epoch 5 | Step 3355200 | Avg Loss: 0.0148 | Grad Norm: 0.00837869\n",
      "Epoch 5 | Step 3355300 | Avg Loss: 0.0148 | Grad Norm: 0.00803092\n",
      "Epoch 5 | Step 3355400 | Avg Loss: 0.0147 | Grad Norm: 0.00921989\n",
      "Epoch 5 | Step 3355500 | Avg Loss: 0.0143 | Grad Norm: 0.00884433\n",
      "Epoch 5 | Step 3355600 | Avg Loss: 0.0151 | Grad Norm: 0.00892305\n",
      "Epoch 5 | Step 3355700 | Avg Loss: 0.0153 | Grad Norm: 0.00951026\n",
      "Epoch 5 | Step 3355800 | Avg Loss: 0.0153 | Grad Norm: 0.00904949\n",
      "Epoch 5 | Step 3355900 | Avg Loss: 0.0154 | Grad Norm: 0.00858646\n",
      "Epoch 5 | Step 3356000 | Avg Loss: 0.0153 | Grad Norm: 0.00964542\n",
      "Epoch 5 | Step 3356100 | Avg Loss: 0.0153 | Grad Norm: 0.00895986\n",
      "Epoch 5 | Step 3356200 | Avg Loss: 0.0153 | Grad Norm: 0.00982807\n",
      "Epoch 5 | Step 3356300 | Avg Loss: 0.0156 | Grad Norm: 0.01134360\n",
      "Epoch 5 | Step 3356400 | Avg Loss: 0.0154 | Grad Norm: 0.01039987\n",
      "Epoch 5 | Step 3356500 | Avg Loss: 0.0158 | Grad Norm: 0.00942827\n",
      "Epoch 5 | Step 3356600 | Avg Loss: 0.0156 | Grad Norm: 0.00879835\n",
      "Epoch 5 | Step 3356700 | Avg Loss: 0.0156 | Grad Norm: 0.00919353\n",
      "Epoch 5 | Step 3356800 | Avg Loss: 0.0156 | Grad Norm: 0.01076610\n",
      "Epoch 5 | Step 3356900 | Avg Loss: 0.0155 | Grad Norm: 0.00882742\n",
      "Epoch 5 | Step 3357000 | Avg Loss: 0.0159 | Grad Norm: 0.00862662\n",
      "Epoch 5 | Step 3357100 | Avg Loss: 0.0157 | Grad Norm: 0.00907080\n",
      "Epoch 5 | Step 3357200 | Avg Loss: 0.0157 | Grad Norm: 0.01138124\n",
      "Epoch 5 | Step 3357300 | Avg Loss: 0.0155 | Grad Norm: 0.00951979\n",
      "Epoch 5 | Step 3357400 | Avg Loss: 0.0155 | Grad Norm: 0.00862377\n",
      "Epoch 5 | Step 3357500 | Avg Loss: 0.0152 | Grad Norm: 0.00933630\n",
      "Epoch 5 | Step 3357600 | Avg Loss: 0.0154 | Grad Norm: 0.00885886\n",
      "Epoch 5 | Step 3357700 | Avg Loss: 0.0155 | Grad Norm: 0.00948734\n",
      "Epoch 5 | Step 3357800 | Avg Loss: 0.0156 | Grad Norm: 0.01017918\n",
      "Epoch 5 | Step 3357900 | Avg Loss: 0.0156 | Grad Norm: 0.00929054\n",
      "Epoch 5 | Step 3358000 | Avg Loss: 0.0158 | Grad Norm: 0.00957420\n",
      "Epoch 5 | Step 3358100 | Avg Loss: 0.0157 | Grad Norm: 0.00929002\n",
      "Epoch 5 | Step 3358200 | Avg Loss: 0.0160 | Grad Norm: 0.00867973\n",
      "Epoch 5 | Step 3358300 | Avg Loss: 0.0152 | Grad Norm: 0.00903611\n",
      "Epoch 5 | Step 3358400 | Avg Loss: 0.0151 | Grad Norm: 0.00842774\n",
      "Epoch 5 | Step 3358500 | Avg Loss: 0.0156 | Grad Norm: 0.00943868\n",
      "Epoch 5 | Step 3358600 | Avg Loss: 0.0157 | Grad Norm: 0.00968355\n",
      "Epoch 5 | Step 3358700 | Avg Loss: 0.0156 | Grad Norm: 0.01051494\n",
      "Epoch 5 | Step 3358800 | Avg Loss: 0.0149 | Grad Norm: 0.00832822\n",
      "Epoch 5 | Step 3358900 | Avg Loss: 0.0153 | Grad Norm: 0.00949967\n",
      "Epoch 5 | Step 3359000 | Avg Loss: 0.0153 | Grad Norm: 0.00833678\n",
      "Epoch 5 | Step 3359100 | Avg Loss: 0.0153 | Grad Norm: 0.00953886\n",
      "Epoch 5 | Step 3359200 | Avg Loss: 0.0158 | Grad Norm: 0.00821357\n",
      "Epoch 5 | Step 3359300 | Avg Loss: 0.0157 | Grad Norm: 0.00949554\n",
      "Epoch 5 | Step 3359400 | Avg Loss: 0.0153 | Grad Norm: 0.00873866\n",
      "Epoch 5 | Step 3359500 | Avg Loss: 0.0153 | Grad Norm: 0.01007664\n",
      "Epoch 5 | Step 3359600 | Avg Loss: 0.0155 | Grad Norm: 0.00913527\n",
      "Epoch 5 | Step 3359700 | Avg Loss: 0.0154 | Grad Norm: 0.00946530\n",
      "Epoch 5 | Step 3359800 | Avg Loss: 0.0154 | Grad Norm: 0.00855802\n",
      "Epoch 5 | Step 3359900 | Avg Loss: 0.0156 | Grad Norm: 0.00942048\n",
      "Epoch 5 | Step 3360000 | Avg Loss: 0.0154 | Grad Norm: 0.00897900\n",
      "Epoch 5 | Step 3360100 | Avg Loss: 0.0153 | Grad Norm: 0.00867297\n",
      "Epoch 5 | Step 3360200 | Avg Loss: 0.0153 | Grad Norm: 0.00862416\n",
      "Epoch 5 | Step 3360300 | Avg Loss: 0.0152 | Grad Norm: 0.00866897\n",
      "Epoch 5 | Step 3360400 | Avg Loss: 0.0154 | Grad Norm: 0.00958845\n",
      "Epoch 5 | Step 3360500 | Avg Loss: 0.0151 | Grad Norm: 0.00831728\n",
      "Epoch 5 | Step 3360600 | Avg Loss: 0.0155 | Grad Norm: 0.00843985\n",
      "Epoch 5 | Step 3360700 | Avg Loss: 0.0155 | Grad Norm: 0.00927365\n",
      "Epoch 5 | Step 3360800 | Avg Loss: 0.0151 | Grad Norm: 0.00904836\n",
      "Epoch 5 | Step 3360900 | Avg Loss: 0.0154 | Grad Norm: 0.01066962\n",
      "Epoch 5 | Step 3361000 | Avg Loss: 0.0155 | Grad Norm: 0.00921147\n",
      "Epoch 5 | Step 3361100 | Avg Loss: 0.0154 | Grad Norm: 0.00948574\n",
      "Epoch 5 | Step 3361200 | Avg Loss: 0.0153 | Grad Norm: 0.00993987\n",
      "Epoch 5 | Step 3361300 | Avg Loss: 0.0154 | Grad Norm: 0.00887847\n",
      "Epoch 5 | Step 3361400 | Avg Loss: 0.0155 | Grad Norm: 0.01104190\n",
      "Epoch 5 | Step 3361500 | Avg Loss: 0.0153 | Grad Norm: 0.00910857\n",
      "Epoch 5 | Step 3361600 | Avg Loss: 0.0152 | Grad Norm: 0.00829408\n",
      "Epoch 5 | Step 3361700 | Avg Loss: 0.0150 | Grad Norm: 0.00860851\n",
      "Epoch 5 | Step 3361800 | Avg Loss: 0.0154 | Grad Norm: 0.00938189\n",
      "Epoch 5 | Step 3361900 | Avg Loss: 0.0156 | Grad Norm: 0.00906538\n",
      "Epoch 5 | Step 3362000 | Avg Loss: 0.0156 | Grad Norm: 0.00949789\n",
      "Epoch 5 | Step 3362100 | Avg Loss: 0.0157 | Grad Norm: 0.00884687\n",
      "Epoch 5 | Step 3362200 | Avg Loss: 0.0160 | Grad Norm: 0.00911721\n",
      "Epoch 5 | Step 3362300 | Avg Loss: 0.0156 | Grad Norm: 0.01064733\n",
      "Epoch 5 | Step 3362400 | Avg Loss: 0.0158 | Grad Norm: 0.00866539\n",
      "Epoch 5 | Step 3362500 | Avg Loss: 0.0158 | Grad Norm: 0.00998880\n",
      "Epoch 5 | Step 3362600 | Avg Loss: 0.0157 | Grad Norm: 0.00891011\n",
      "Epoch 5 | Step 3362700 | Avg Loss: 0.0155 | Grad Norm: 0.00942104\n",
      "Epoch 5 | Step 3362800 | Avg Loss: 0.0156 | Grad Norm: 0.00855100\n",
      "Epoch 5 | Step 3362900 | Avg Loss: 0.0152 | Grad Norm: 0.00901457\n",
      "Epoch 5 | Step 3363000 | Avg Loss: 0.0154 | Grad Norm: 0.00747020\n",
      "Epoch 5 | Step 3363100 | Avg Loss: 0.0155 | Grad Norm: 0.00810329\n",
      "Epoch 5 | Step 3363200 | Avg Loss: 0.0159 | Grad Norm: 0.00883858\n",
      "Epoch 5 | Step 3363300 | Avg Loss: 0.0154 | Grad Norm: 0.00959459\n",
      "Epoch 5 | Step 3363400 | Avg Loss: 0.0152 | Grad Norm: 0.01065427\n",
      "Epoch 5 | Step 3363500 | Avg Loss: 0.0152 | Grad Norm: 0.00815746\n",
      "Epoch 5 | Step 3363600 | Avg Loss: 0.0156 | Grad Norm: 0.00946244\n",
      "Epoch 5 | Step 3363700 | Avg Loss: 0.0155 | Grad Norm: 0.00993084\n",
      "Epoch 5 | Step 3363800 | Avg Loss: 0.0156 | Grad Norm: 0.00947359\n",
      "Epoch 5 | Step 3363900 | Avg Loss: 0.0154 | Grad Norm: 0.00902970\n",
      "Epoch 5 | Step 3364000 | Avg Loss: 0.0155 | Grad Norm: 0.00937879\n",
      "Epoch 5 | Step 3364100 | Avg Loss: 0.0152 | Grad Norm: 0.00821188\n",
      "Epoch 5 | Step 3364200 | Avg Loss: 0.0150 | Grad Norm: 0.00812345\n",
      "Epoch 5 | Step 3364300 | Avg Loss: 0.0152 | Grad Norm: 0.00892933\n",
      "Epoch 5 | Step 3364400 | Avg Loss: 0.0153 | Grad Norm: 0.00916561\n",
      "Epoch 5 | Step 3364500 | Avg Loss: 0.0156 | Grad Norm: 0.00911590\n",
      "Epoch 5 | Step 3364600 | Avg Loss: 0.0157 | Grad Norm: 0.00910289\n",
      "Epoch 5 | Step 3364700 | Avg Loss: 0.0153 | Grad Norm: 0.00878311\n",
      "Epoch 5 | Step 3364800 | Avg Loss: 0.0153 | Grad Norm: 0.00917741\n",
      "Epoch 5 | Step 3364900 | Avg Loss: 0.0153 | Grad Norm: 0.00869824\n",
      "Epoch 5 | Step 3365000 | Avg Loss: 0.0155 | Grad Norm: 0.00900624\n",
      "Epoch 5 | Step 3365100 | Avg Loss: 0.0152 | Grad Norm: 0.00886188\n",
      "Epoch 5 | Step 3365200 | Avg Loss: 0.0153 | Grad Norm: 0.00965075\n",
      "Epoch 5 | Step 3365300 | Avg Loss: 0.0157 | Grad Norm: 0.00950450\n",
      "Epoch 5 | Step 3365400 | Avg Loss: 0.0159 | Grad Norm: 0.01030817\n",
      "Epoch 5 | Step 3365500 | Avg Loss: 0.0157 | Grad Norm: 0.01004137\n",
      "Epoch 5 | Step 3365600 | Avg Loss: 0.0156 | Grad Norm: 0.00984629\n",
      "Epoch 5 | Step 3365700 | Avg Loss: 0.0157 | Grad Norm: 0.01128475\n",
      "Epoch 5 | Step 3365800 | Avg Loss: 0.0156 | Grad Norm: 0.00847492\n",
      "Epoch 5 | Step 3365900 | Avg Loss: 0.0159 | Grad Norm: 0.00802565\n",
      "Epoch 5 | Step 3366000 | Avg Loss: 0.0158 | Grad Norm: 0.00876253\n",
      "Epoch 5 | Step 3366100 | Avg Loss: 0.0156 | Grad Norm: 0.00825003\n",
      "Epoch 5 | Step 3366200 | Avg Loss: 0.0157 | Grad Norm: 0.00833516\n",
      "Epoch 5 | Step 3366300 | Avg Loss: 0.0157 | Grad Norm: 0.00907110\n",
      "Epoch 5 | Step 3366400 | Avg Loss: 0.0159 | Grad Norm: 0.00975903\n",
      "Epoch 5 | Step 3366500 | Avg Loss: 0.0158 | Grad Norm: 0.01035019\n",
      "Epoch 5 | Step 3366600 | Avg Loss: 0.0155 | Grad Norm: 0.00931196\n",
      "Epoch 5 | Step 3366700 | Avg Loss: 0.0158 | Grad Norm: 0.00827656\n",
      "Epoch 5 | Step 3366800 | Avg Loss: 0.0156 | Grad Norm: 0.00865429\n",
      "Epoch 5 | Step 3366900 | Avg Loss: 0.0162 | Grad Norm: 0.00920919\n",
      "Epoch 5 | Step 3367000 | Avg Loss: 0.0161 | Grad Norm: 0.00945144\n",
      "Epoch 5 | Step 3367100 | Avg Loss: 0.0158 | Grad Norm: 0.00973344\n",
      "Epoch 5 | Step 3367200 | Avg Loss: 0.0155 | Grad Norm: 0.00856745\n",
      "Epoch 5 | Step 3367300 | Avg Loss: 0.0153 | Grad Norm: 0.00828108\n",
      "Epoch 5 | Step 3367400 | Avg Loss: 0.0152 | Grad Norm: 0.00825033\n",
      "Epoch 5 | Step 3367500 | Avg Loss: 0.0152 | Grad Norm: 0.00917044\n",
      "Epoch 5 | Step 3367600 | Avg Loss: 0.0153 | Grad Norm: 0.00820027\n",
      "Epoch 5 | Step 3367700 | Avg Loss: 0.0153 | Grad Norm: 0.00826519\n",
      "Epoch 5 | Step 3367800 | Avg Loss: 0.0155 | Grad Norm: 0.00926626\n",
      "Epoch 5 | Step 3367900 | Avg Loss: 0.0150 | Grad Norm: 0.00832629\n",
      "Epoch 5 | Step 3368000 | Avg Loss: 0.0149 | Grad Norm: 0.00830312\n",
      "Epoch 5 | Step 3368100 | Avg Loss: 0.0148 | Grad Norm: 0.01072991\n",
      "Epoch 5 | Step 3368200 | Avg Loss: 0.0147 | Grad Norm: 0.00810021\n",
      "Epoch 5 | Step 3368300 | Avg Loss: 0.0151 | Grad Norm: 0.00951005\n",
      "Epoch 5 | Step 3368400 | Avg Loss: 0.0152 | Grad Norm: 0.01029307\n",
      "Epoch 5 | Step 3368500 | Avg Loss: 0.0156 | Grad Norm: 0.01037052\n",
      "Epoch 5 | Step 3368600 | Avg Loss: 0.0156 | Grad Norm: 0.00930330\n",
      "Epoch 5 | Step 3368700 | Avg Loss: 0.0154 | Grad Norm: 0.00884872\n",
      "Epoch 5 | Step 3368800 | Avg Loss: 0.0150 | Grad Norm: 0.01015568\n",
      "Epoch 5 | Step 3368900 | Avg Loss: 0.0151 | Grad Norm: 0.01024928\n",
      "Epoch 5 | Step 3369000 | Avg Loss: 0.0155 | Grad Norm: 0.00973024\n",
      "Epoch 5 | Step 3369100 | Avg Loss: 0.0156 | Grad Norm: 0.00997944\n",
      "Epoch 5 | Step 3369200 | Avg Loss: 0.0153 | Grad Norm: 0.00914433\n",
      "Epoch 5 | Step 3369300 | Avg Loss: 0.0155 | Grad Norm: 0.01039227\n",
      "Epoch 5 | Step 3369400 | Avg Loss: 0.0154 | Grad Norm: 0.00982368\n",
      "Epoch 5 | Step 3369500 | Avg Loss: 0.0154 | Grad Norm: 0.00790367\n",
      "Epoch 5 | Step 3369600 | Avg Loss: 0.0149 | Grad Norm: 0.00889940\n",
      "Epoch 5 | Step 3369700 | Avg Loss: 0.0148 | Grad Norm: 0.00900915\n",
      "Epoch 5 | Step 3369800 | Avg Loss: 0.0150 | Grad Norm: 0.00920432\n",
      "Epoch 5 | Step 3369900 | Avg Loss: 0.0151 | Grad Norm: 0.00806443\n",
      "Epoch 5 | Step 3370000 | Avg Loss: 0.0151 | Grad Norm: 0.00857935\n",
      "Epoch 5 | Step 3370100 | Avg Loss: 0.0148 | Grad Norm: 0.00845405\n",
      "Epoch 5 | Step 3370200 | Avg Loss: 0.0153 | Grad Norm: 0.00876062\n",
      "Epoch 5 | Step 3370300 | Avg Loss: 0.0152 | Grad Norm: 0.00875596\n",
      "Epoch 5 | Step 3370400 | Avg Loss: 0.0152 | Grad Norm: 0.01000246\n",
      "Epoch 5 | Step 3370500 | Avg Loss: 0.0154 | Grad Norm: 0.00863561\n",
      "Epoch 5 | Step 3370600 | Avg Loss: 0.0154 | Grad Norm: 0.00939749\n",
      "Epoch 5 | Step 3370700 | Avg Loss: 0.0152 | Grad Norm: 0.00909300\n",
      "Epoch 5 | Step 3370800 | Avg Loss: 0.0158 | Grad Norm: 0.00911074\n",
      "Epoch 5 | Step 3370900 | Avg Loss: 0.0157 | Grad Norm: 0.00882446\n",
      "Epoch 5 | Step 3371000 | Avg Loss: 0.0159 | Grad Norm: 0.00947449\n",
      "Epoch 5 | Step 3371100 | Avg Loss: 0.0159 | Grad Norm: 0.01059485\n",
      "Epoch 5 | Step 3371200 | Avg Loss: 0.0156 | Grad Norm: 0.00880561\n",
      "Epoch 5 | Step 3371300 | Avg Loss: 0.0156 | Grad Norm: 0.00849024\n",
      "Epoch 5 | Step 3371400 | Avg Loss: 0.0155 | Grad Norm: 0.00952844\n",
      "Epoch 5 | Step 3371500 | Avg Loss: 0.0154 | Grad Norm: 0.00894058\n",
      "Epoch 5 | Step 3371600 | Avg Loss: 0.0156 | Grad Norm: 0.00905844\n",
      "Epoch 5 | Step 3371700 | Avg Loss: 0.0155 | Grad Norm: 0.01051116\n",
      "Epoch 5 | Step 3371800 | Avg Loss: 0.0155 | Grad Norm: 0.00884639\n",
      "Epoch 5 | Step 3371900 | Avg Loss: 0.0151 | Grad Norm: 0.00901089\n",
      "Epoch 5 | Step 3372000 | Avg Loss: 0.0153 | Grad Norm: 0.00917063\n",
      "Epoch 5 | Step 3372100 | Avg Loss: 0.0156 | Grad Norm: 0.00913669\n",
      "Epoch 5 | Step 3372200 | Avg Loss: 0.0157 | Grad Norm: 0.00980395\n",
      "Epoch 5 | Step 3372300 | Avg Loss: 0.0154 | Grad Norm: 0.00950737\n",
      "Epoch 5 | Step 3372400 | Avg Loss: 0.0154 | Grad Norm: 0.00942433\n",
      "Epoch 5 | Step 3372500 | Avg Loss: 0.0151 | Grad Norm: 0.01025874\n",
      "Epoch 5 | Step 3372600 | Avg Loss: 0.0157 | Grad Norm: 0.00896529\n",
      "Epoch 5 | Step 3372700 | Avg Loss: 0.0157 | Grad Norm: 0.00921160\n",
      "Epoch 5 | Step 3372800 | Avg Loss: 0.0159 | Grad Norm: 0.00998895\n",
      "Epoch 5 | Step 3372900 | Avg Loss: 0.0160 | Grad Norm: 0.00844657\n",
      "Epoch 5 | Step 3373000 | Avg Loss: 0.0159 | Grad Norm: 0.00903621\n",
      "Epoch 5 | Step 3373100 | Avg Loss: 0.0161 | Grad Norm: 0.01030721\n",
      "Epoch 5 | Step 3373200 | Avg Loss: 0.0160 | Grad Norm: 0.00877825\n",
      "Epoch 5 | Step 3373300 | Avg Loss: 0.0157 | Grad Norm: 0.01017754\n",
      "Epoch 5 | Step 3373400 | Avg Loss: 0.0153 | Grad Norm: 0.00881423\n",
      "Epoch 5 | Step 3373500 | Avg Loss: 0.0151 | Grad Norm: 0.00908328\n",
      "Epoch 5 | Step 3373600 | Avg Loss: 0.0152 | Grad Norm: 0.00939905\n",
      "Epoch 5 | Step 3373700 | Avg Loss: 0.0156 | Grad Norm: 0.00848088\n",
      "Epoch 5 | Step 3373800 | Avg Loss: 0.0158 | Grad Norm: 0.00957986\n",
      "Epoch 5 | Step 3373900 | Avg Loss: 0.0155 | Grad Norm: 0.00904522\n",
      "Epoch 5 | Step 3374000 | Avg Loss: 0.0156 | Grad Norm: 0.00986373\n",
      "Epoch 5 | Step 3374100 | Avg Loss: 0.0155 | Grad Norm: 0.01005909\n",
      "Epoch 5 | Step 3374200 | Avg Loss: 0.0154 | Grad Norm: 0.00989866\n",
      "Epoch 5 | Step 3374300 | Avg Loss: 0.0152 | Grad Norm: 0.00941984\n",
      "Epoch 5 | Step 3374400 | Avg Loss: 0.0152 | Grad Norm: 0.01006106\n",
      "Epoch 5 | Step 3374500 | Avg Loss: 0.0152 | Grad Norm: 0.00915456\n",
      "Epoch 5 | Step 3374600 | Avg Loss: 0.0152 | Grad Norm: 0.00861506\n",
      "Epoch 5 | Step 3374700 | Avg Loss: 0.0156 | Grad Norm: 0.00938760\n",
      "Epoch 5 | Step 3374800 | Avg Loss: 0.0155 | Grad Norm: 0.00909991\n",
      "Epoch 5 | Step 3374900 | Avg Loss: 0.0154 | Grad Norm: 0.00858475\n",
      "Epoch 5 | Step 3375000 | Avg Loss: 0.0152 | Grad Norm: 0.00907628\n",
      "Epoch 5 | Step 3375100 | Avg Loss: 0.0153 | Grad Norm: 0.01116843\n",
      "Epoch 5 | Step 3375200 | Avg Loss: 0.0151 | Grad Norm: 0.00980649\n",
      "Epoch 5 | Step 3375300 | Avg Loss: 0.0150 | Grad Norm: 0.00948905\n",
      "Epoch 5 | Step 3375400 | Avg Loss: 0.0145 | Grad Norm: 0.00900120\n",
      "Epoch 5 | Step 3375500 | Avg Loss: 0.0145 | Grad Norm: 0.00892405\n",
      "Epoch 5 | Step 3375600 | Avg Loss: 0.0148 | Grad Norm: 0.00950043\n",
      "Epoch 5 | Step 3375700 | Avg Loss: 0.0149 | Grad Norm: 0.00778908\n",
      "Epoch 5 | Step 3375800 | Avg Loss: 0.0149 | Grad Norm: 0.00963722\n",
      "Epoch 5 | Step 3375900 | Avg Loss: 0.0149 | Grad Norm: 0.00847897\n",
      "Epoch 5 | Step 3376000 | Avg Loss: 0.0149 | Grad Norm: 0.01137933\n",
      "Epoch 5 | Step 3376100 | Avg Loss: 0.0142 | Grad Norm: 0.00929404\n",
      "Epoch 5 | Step 3376200 | Avg Loss: 0.0143 | Grad Norm: 0.01012530\n",
      "Epoch 5 | Step 3376300 | Avg Loss: 0.0144 | Grad Norm: 0.00860600\n",
      "Epoch 5 | Step 3376400 | Avg Loss: 0.0147 | Grad Norm: 0.00894675\n",
      "Epoch 5 | Step 3376500 | Avg Loss: 0.0148 | Grad Norm: 0.00920278\n",
      "Epoch 5 | Step 3376600 | Avg Loss: 0.0148 | Grad Norm: 0.00798341\n",
      "Epoch 5 | Step 3376700 | Avg Loss: 0.0153 | Grad Norm: 0.00774943\n",
      "Epoch 5 | Step 3376800 | Avg Loss: 0.0153 | Grad Norm: 0.00748645\n",
      "Epoch 5 | Step 3376900 | Avg Loss: 0.0156 | Grad Norm: 0.00958192\n",
      "Epoch 5 | Step 3377000 | Avg Loss: 0.0156 | Grad Norm: 0.00908568\n",
      "Epoch 5 | Step 3377100 | Avg Loss: 0.0157 | Grad Norm: 0.00863575\n",
      "Epoch 5 | Step 3377200 | Avg Loss: 0.0155 | Grad Norm: 0.00778389\n",
      "Epoch 5 | Step 3377300 | Avg Loss: 0.0154 | Grad Norm: 0.00867779\n",
      "Epoch 5 | Step 3377400 | Avg Loss: 0.0158 | Grad Norm: 0.01250747\n",
      "Epoch 5 | Step 3377500 | Avg Loss: 0.0159 | Grad Norm: 0.00880171\n",
      "Epoch 5 | Step 3377600 | Avg Loss: 0.0156 | Grad Norm: 0.00937205\n",
      "Epoch 5 | Step 3377700 | Avg Loss: 0.0156 | Grad Norm: 0.00931004\n",
      "Epoch 5 | Step 3377800 | Avg Loss: 0.0154 | Grad Norm: 0.00863893\n",
      "Epoch 5 | Step 3377900 | Avg Loss: 0.0156 | Grad Norm: 0.01056778\n",
      "Epoch 5 | Step 3378000 | Avg Loss: 0.0152 | Grad Norm: 0.00811898\n",
      "Epoch 5 | Step 3378100 | Avg Loss: 0.0153 | Grad Norm: 0.00972161\n",
      "Epoch 5 | Step 3378200 | Avg Loss: 0.0152 | Grad Norm: 0.00958792\n",
      "Epoch 5 | Step 3378300 | Avg Loss: 0.0151 | Grad Norm: 0.00925538\n",
      "Epoch 5 | Step 3378400 | Avg Loss: 0.0153 | Grad Norm: 0.00961725\n",
      "Epoch 5 | Step 3378500 | Avg Loss: 0.0153 | Grad Norm: 0.00860896\n",
      "Epoch 5 | Step 3378600 | Avg Loss: 0.0154 | Grad Norm: 0.01027017\n",
      "Epoch 5 | Step 3378700 | Avg Loss: 0.0151 | Grad Norm: 0.00877183\n",
      "Epoch 5 | Step 3378800 | Avg Loss: 0.0150 | Grad Norm: 0.00976843\n",
      "Epoch 5 | Step 3378900 | Avg Loss: 0.0149 | Grad Norm: 0.00825804\n",
      "Epoch 5 | Step 3379000 | Avg Loss: 0.0151 | Grad Norm: 0.00875184\n",
      "Epoch 5 | Step 3379100 | Avg Loss: 0.0149 | Grad Norm: 0.00839912\n",
      "Epoch 5 | Step 3379200 | Avg Loss: 0.0151 | Grad Norm: 0.00846306\n",
      "Epoch 5 | Step 3379300 | Avg Loss: 0.0155 | Grad Norm: 0.00890203\n",
      "Epoch 5 | Step 3379400 | Avg Loss: 0.0157 | Grad Norm: 0.00952331\n",
      "Epoch 5 | Step 3379500 | Avg Loss: 0.0154 | Grad Norm: 0.01062459\n",
      "Epoch 5 | Step 3379600 | Avg Loss: 0.0157 | Grad Norm: 0.00972896\n",
      "Epoch 5 | Step 3379700 | Avg Loss: 0.0154 | Grad Norm: 0.00972147\n",
      "Epoch 5 | Step 3379800 | Avg Loss: 0.0154 | Grad Norm: 0.00856462\n",
      "Epoch 5 | Step 3379900 | Avg Loss: 0.0156 | Grad Norm: 0.00940188\n",
      "Epoch 5 | Step 3380000 | Avg Loss: 0.0152 | Grad Norm: 0.00977273\n",
      "Epoch 5 | Step 3380100 | Avg Loss: 0.0154 | Grad Norm: 0.00887616\n",
      "Epoch 5 | Step 3380200 | Avg Loss: 0.0152 | Grad Norm: 0.00690846\n",
      "Epoch 5 | Step 3380300 | Avg Loss: 0.0154 | Grad Norm: 0.00957358\n",
      "Epoch 5 | Step 3380400 | Avg Loss: 0.0153 | Grad Norm: 0.00932821\n",
      "Epoch 5 | Step 3380500 | Avg Loss: 0.0153 | Grad Norm: 0.00786682\n",
      "Epoch 5 | Step 3380600 | Avg Loss: 0.0154 | Grad Norm: 0.00933628\n",
      "Epoch 5 | Step 3380700 | Avg Loss: 0.0153 | Grad Norm: 0.00801547\n",
      "Epoch 5 | Step 3380800 | Avg Loss: 0.0157 | Grad Norm: 0.00734994\n",
      "Epoch 5 | Step 3380900 | Avg Loss: 0.0155 | Grad Norm: 0.00888386\n",
      "Epoch 5 | Step 3381000 | Avg Loss: 0.0156 | Grad Norm: 0.00983484\n",
      "Epoch 5 | Step 3381100 | Avg Loss: 0.0155 | Grad Norm: 0.00831746\n",
      "Epoch 5 | Step 3381200 | Avg Loss: 0.0153 | Grad Norm: 0.00890217\n",
      "Epoch 5 | Step 3381300 | Avg Loss: 0.0153 | Grad Norm: 0.01556122\n",
      "Epoch 5 | Step 3381400 | Avg Loss: 0.0152 | Grad Norm: 0.00893851\n",
      "Epoch 5 | Step 3381500 | Avg Loss: 0.0154 | Grad Norm: 0.00844992\n",
      "Epoch 5 | Step 3381600 | Avg Loss: 0.0150 | Grad Norm: 0.00865791\n",
      "Epoch 5 | Step 3381700 | Avg Loss: 0.0152 | Grad Norm: 0.00831957\n",
      "Epoch 5 | Step 3381800 | Avg Loss: 0.0151 | Grad Norm: 0.00812246\n",
      "Epoch 5 | Step 3381900 | Avg Loss: 0.0151 | Grad Norm: 0.00869468\n",
      "Epoch 5 | Step 3382000 | Avg Loss: 0.0152 | Grad Norm: 0.00817621\n",
      "Epoch 5 | Step 3382100 | Avg Loss: 0.0152 | Grad Norm: 0.00934164\n",
      "Epoch 5 | Step 3382200 | Avg Loss: 0.0157 | Grad Norm: 0.01032787\n",
      "Epoch 5 | Step 3382300 | Avg Loss: 0.0154 | Grad Norm: 0.00863398\n",
      "Epoch 5 | Step 3382400 | Avg Loss: 0.0150 | Grad Norm: 0.00789032\n",
      "Epoch 5 | Step 3382500 | Avg Loss: 0.0157 | Grad Norm: 0.00898413\n",
      "Epoch 5 | Step 3382600 | Avg Loss: 0.0157 | Grad Norm: 0.00956841\n",
      "Epoch 5 | Step 3382700 | Avg Loss: 0.0155 | Grad Norm: 0.00884784\n",
      "Epoch 5 | Step 3382800 | Avg Loss: 0.0156 | Grad Norm: 0.00937730\n",
      "Epoch 5 | Step 3382900 | Avg Loss: 0.0159 | Grad Norm: 0.00926160\n",
      "Epoch 5 | Step 3383000 | Avg Loss: 0.0156 | Grad Norm: 0.00925947\n",
      "Epoch 5 | Step 3383100 | Avg Loss: 0.0156 | Grad Norm: 0.00881441\n",
      "Epoch 5 | Step 3383200 | Avg Loss: 0.0154 | Grad Norm: 0.00904115\n",
      "Epoch 5 | Step 3383300 | Avg Loss: 0.0151 | Grad Norm: 0.00848329\n",
      "Epoch 5 | Step 3383400 | Avg Loss: 0.0149 | Grad Norm: 0.00802530\n",
      "Epoch 5 | Step 3383500 | Avg Loss: 0.0150 | Grad Norm: 0.00845636\n",
      "Epoch 5 | Step 3383600 | Avg Loss: 0.0153 | Grad Norm: 0.00839583\n",
      "Epoch 5 | Step 3383700 | Avg Loss: 0.0157 | Grad Norm: 0.00940692\n",
      "Epoch 5 | Step 3383800 | Avg Loss: 0.0161 | Grad Norm: 0.00966503\n",
      "Epoch 5 | Step 3383900 | Avg Loss: 0.0158 | Grad Norm: 0.00930595\n",
      "Epoch 5 | Step 3384000 | Avg Loss: 0.0156 | Grad Norm: 0.00870695\n",
      "Epoch 5 | Step 3384100 | Avg Loss: 0.0156 | Grad Norm: 0.01009058\n",
      "Epoch 5 | Step 3384200 | Avg Loss: 0.0155 | Grad Norm: 0.00934121\n",
      "Epoch 5 | Step 3384300 | Avg Loss: 0.0152 | Grad Norm: 0.01071840\n",
      "Epoch 5 | Step 3384400 | Avg Loss: 0.0150 | Grad Norm: 0.01073569\n",
      "Epoch 5 | Step 3384500 | Avg Loss: 0.0151 | Grad Norm: 0.00847563\n",
      "Epoch 5 | Step 3384600 | Avg Loss: 0.0151 | Grad Norm: 0.00888935\n",
      "Epoch 5 | Step 3384700 | Avg Loss: 0.0153 | Grad Norm: 0.01094668\n",
      "Epoch 5 | Step 3384800 | Avg Loss: 0.0152 | Grad Norm: 0.00897821\n",
      "Epoch 5 | Step 3384900 | Avg Loss: 0.0152 | Grad Norm: 0.00906212\n",
      "Epoch 5 | Step 3385000 | Avg Loss: 0.0152 | Grad Norm: 0.00897386\n",
      "Epoch 5 | Step 3385100 | Avg Loss: 0.0154 | Grad Norm: 0.00925947\n",
      "Epoch 5 | Step 3385200 | Avg Loss: 0.0150 | Grad Norm: 0.01024630\n",
      "Epoch 5 | Step 3385300 | Avg Loss: 0.0148 | Grad Norm: 0.00810786\n",
      "Epoch 5 | Step 3385400 | Avg Loss: 0.0150 | Grad Norm: 0.00922972\n",
      "Epoch 5 | Step 3385500 | Avg Loss: 0.0154 | Grad Norm: 0.00847610\n",
      "Epoch 5 | Step 3385600 | Avg Loss: 0.0155 | Grad Norm: 0.01129242\n",
      "Epoch 5 | Step 3385700 | Avg Loss: 0.0158 | Grad Norm: 0.00949065\n",
      "Epoch 5 | Step 3385800 | Avg Loss: 0.0160 | Grad Norm: 0.00857678\n",
      "Epoch 5 | Step 3385900 | Avg Loss: 0.0158 | Grad Norm: 0.00921209\n",
      "Epoch 5 | Step 3386000 | Avg Loss: 0.0153 | Grad Norm: 0.00942370\n",
      "Epoch 5 | Step 3386100 | Avg Loss: 0.0151 | Grad Norm: 0.00951124\n",
      "Epoch 5 | Step 3386200 | Avg Loss: 0.0153 | Grad Norm: 0.00884019\n",
      "Epoch 5 | Step 3386300 | Avg Loss: 0.0155 | Grad Norm: 0.01259274\n",
      "Epoch 5 | Step 3386400 | Avg Loss: 0.0157 | Grad Norm: 0.00923642\n",
      "Epoch 5 | Step 3386500 | Avg Loss: 0.0155 | Grad Norm: 0.01000253\n",
      "Epoch 5 | Step 3386600 | Avg Loss: 0.0154 | Grad Norm: 0.00928470\n",
      "Epoch 5 | Step 3386700 | Avg Loss: 0.0156 | Grad Norm: 0.00868573\n",
      "Epoch 5 | Step 3386800 | Avg Loss: 0.0153 | Grad Norm: 0.00848470\n",
      "Epoch 5 | Step 3386900 | Avg Loss: 0.0154 | Grad Norm: 0.00960609\n",
      "Epoch 5 | Step 3387000 | Avg Loss: 0.0156 | Grad Norm: 0.00962674\n",
      "Epoch 5 | Step 3387100 | Avg Loss: 0.0157 | Grad Norm: 0.00916347\n",
      "Epoch 5 | Step 3387200 | Avg Loss: 0.0157 | Grad Norm: 0.00850966\n",
      "Epoch 5 | Step 3387300 | Avg Loss: 0.0155 | Grad Norm: 0.00918482\n",
      "Epoch 5 | Step 3387400 | Avg Loss: 0.0153 | Grad Norm: 0.00962044\n",
      "Epoch 5 | Step 3387500 | Avg Loss: 0.0156 | Grad Norm: 0.00862870\n",
      "Epoch 5 | Step 3387600 | Avg Loss: 0.0155 | Grad Norm: 0.01045631\n",
      "Epoch 5 | Step 3387700 | Avg Loss: 0.0153 | Grad Norm: 0.00882009\n",
      "Epoch 5 | Step 3387800 | Avg Loss: 0.0154 | Grad Norm: 0.01073773\n",
      "Epoch 5 | Step 3387900 | Avg Loss: 0.0157 | Grad Norm: 0.01067201\n",
      "Epoch 5 | Step 3388000 | Avg Loss: 0.0156 | Grad Norm: 0.00904096\n",
      "Epoch 5 | Step 3388100 | Avg Loss: 0.0154 | Grad Norm: 0.00866263\n",
      "Epoch 5 | Step 3388200 | Avg Loss: 0.0156 | Grad Norm: 0.00886417\n",
      "Epoch 5 | Step 3388300 | Avg Loss: 0.0155 | Grad Norm: 0.00828228\n",
      "Epoch 5 | Step 3388400 | Avg Loss: 0.0155 | Grad Norm: 0.00999353\n",
      "Epoch 5 | Step 3388500 | Avg Loss: 0.0157 | Grad Norm: 0.00891550\n",
      "Epoch 5 | Step 3388600 | Avg Loss: 0.0157 | Grad Norm: 0.00912292\n",
      "Epoch 5 | Step 3388700 | Avg Loss: 0.0158 | Grad Norm: 0.00914653\n",
      "Epoch 5 | Step 3388800 | Avg Loss: 0.0157 | Grad Norm: 0.00874147\n",
      "Epoch 5 | Step 3388900 | Avg Loss: 0.0158 | Grad Norm: 0.00952288\n",
      "Epoch 5 | Step 3389000 | Avg Loss: 0.0158 | Grad Norm: 0.01122192\n",
      "Epoch 5 | Step 3389100 | Avg Loss: 0.0160 | Grad Norm: 0.01086923\n",
      "Epoch 5 | Step 3389200 | Avg Loss: 0.0163 | Grad Norm: 0.00867940\n",
      "Epoch 5 | Step 3389300 | Avg Loss: 0.0162 | Grad Norm: 0.00885266\n",
      "Epoch 5 | Step 3389400 | Avg Loss: 0.0160 | Grad Norm: 0.00902287\n",
      "Epoch 5 | Step 3389500 | Avg Loss: 0.0159 | Grad Norm: 0.00813100\n",
      "Epoch 5 | Step 3389600 | Avg Loss: 0.0156 | Grad Norm: 0.00876788\n",
      "Epoch 5 | Step 3389700 | Avg Loss: 0.0157 | Grad Norm: 0.00926409\n",
      "Epoch 5 | Step 3389800 | Avg Loss: 0.0158 | Grad Norm: 0.00872199\n",
      "Epoch 5 | Step 3389900 | Avg Loss: 0.0160 | Grad Norm: 0.00934339\n",
      "Epoch 5 | Step 3390000 | Avg Loss: 0.0161 | Grad Norm: 0.01250380\n",
      "Epoch 5 | Step 3390100 | Avg Loss: 0.0160 | Grad Norm: 0.00904237\n",
      "Epoch 5 | Step 3390200 | Avg Loss: 0.0156 | Grad Norm: 0.01127958\n",
      "Epoch 5 | Step 3390300 | Avg Loss: 0.0152 | Grad Norm: 0.01003832\n",
      "Epoch 5 | Step 3390400 | Avg Loss: 0.0152 | Grad Norm: 0.01144364\n",
      "Epoch 5 | Step 3390500 | Avg Loss: 0.0160 | Grad Norm: 0.00901035\n",
      "Epoch 5 | Step 3390600 | Avg Loss: 0.0160 | Grad Norm: 0.00990160\n",
      "Epoch 5 | Step 3390700 | Avg Loss: 0.0157 | Grad Norm: 0.00844074\n",
      "Epoch 5 | Step 3390800 | Avg Loss: 0.0160 | Grad Norm: 0.00819429\n",
      "Epoch 5 | Step 3390900 | Avg Loss: 0.0157 | Grad Norm: 0.00973448\n",
      "Epoch 5 | Step 3391000 | Avg Loss: 0.0154 | Grad Norm: 0.00920636\n",
      "Epoch 5 | Step 3391100 | Avg Loss: 0.0155 | Grad Norm: 0.00910516\n",
      "Epoch 5 | Step 3391200 | Avg Loss: 0.0157 | Grad Norm: 0.00688386\n",
      "Epoch 5 | Step 3391300 | Avg Loss: 0.0156 | Grad Norm: 0.01001682\n",
      "Epoch 5 | Step 3391400 | Avg Loss: 0.0156 | Grad Norm: 0.00855212\n",
      "Epoch 5 | Step 3391500 | Avg Loss: 0.0160 | Grad Norm: 0.00812740\n",
      "Epoch 5 | Step 3391600 | Avg Loss: 0.0159 | Grad Norm: 0.00789644\n",
      "Epoch 5 | Step 3391700 | Avg Loss: 0.0156 | Grad Norm: 0.00838129\n",
      "Epoch 5 | Step 3391800 | Avg Loss: 0.0156 | Grad Norm: 0.01061149\n",
      "Epoch 5 | Step 3391900 | Avg Loss: 0.0153 | Grad Norm: 0.00937149\n",
      "Epoch 5 | Step 3392000 | Avg Loss: 0.0155 | Grad Norm: 0.00858633\n",
      "Epoch 5 | Step 3392100 | Avg Loss: 0.0156 | Grad Norm: 0.01302623\n",
      "Epoch 5 | Step 3392200 | Avg Loss: 0.0153 | Grad Norm: 0.00923434\n",
      "Epoch 5 | Step 3392300 | Avg Loss: 0.0151 | Grad Norm: 0.00941273\n",
      "Epoch 5 | Step 3392400 | Avg Loss: 0.0148 | Grad Norm: 0.00740134\n",
      "Epoch 5 | Step 3392500 | Avg Loss: 0.0150 | Grad Norm: 0.00838969\n",
      "Epoch 5 | Step 3392600 | Avg Loss: 0.0153 | Grad Norm: 0.00847504\n",
      "Epoch 5 | Step 3392700 | Avg Loss: 0.0151 | Grad Norm: 0.00787912\n",
      "Epoch 5 | Step 3392800 | Avg Loss: 0.0154 | Grad Norm: 0.00862100\n",
      "Epoch 5 | Step 3392900 | Avg Loss: 0.0153 | Grad Norm: 0.00896015\n",
      "Epoch 5 | Step 3393000 | Avg Loss: 0.0155 | Grad Norm: 0.01049194\n",
      "Epoch 5 | Step 3393100 | Avg Loss: 0.0153 | Grad Norm: 0.00839625\n",
      "Epoch 5 | Step 3393200 | Avg Loss: 0.0153 | Grad Norm: 0.00780131\n",
      "Epoch 5 | Step 3393300 | Avg Loss: 0.0156 | Grad Norm: 0.00866829\n",
      "Epoch 5 | Step 3393400 | Avg Loss: 0.0155 | Grad Norm: 0.00842896\n",
      "Epoch 5 | Step 3393500 | Avg Loss: 0.0154 | Grad Norm: 0.00940881\n",
      "Epoch 5 | Step 3393600 | Avg Loss: 0.0157 | Grad Norm: 0.00924314\n",
      "Epoch 5 | Step 3393700 | Avg Loss: 0.0156 | Grad Norm: 0.01342123\n",
      "Epoch 5 | Step 3393800 | Avg Loss: 0.0157 | Grad Norm: 0.00905799\n",
      "Epoch 5 | Step 3393900 | Avg Loss: 0.0158 | Grad Norm: 0.00869554\n",
      "Epoch 5 | Step 3394000 | Avg Loss: 0.0157 | Grad Norm: 0.00881905\n",
      "Epoch 5 | Step 3394100 | Avg Loss: 0.0155 | Grad Norm: 0.00909942\n",
      "Epoch 5 | Step 3394200 | Avg Loss: 0.0157 | Grad Norm: 0.00930432\n",
      "Epoch 5 | Step 3394300 | Avg Loss: 0.0158 | Grad Norm: 0.00940945\n",
      "Epoch 5 | Step 3394400 | Avg Loss: 0.0158 | Grad Norm: 0.00803698\n",
      "Epoch 5 | Step 3394500 | Avg Loss: 0.0158 | Grad Norm: 0.01105284\n",
      "Epoch 5 | Step 3394600 | Avg Loss: 0.0158 | Grad Norm: 0.00778435\n",
      "Epoch 5 | Step 3394700 | Avg Loss: 0.0154 | Grad Norm: 0.00870035\n",
      "Epoch 5 | Step 3394800 | Avg Loss: 0.0156 | Grad Norm: 0.00884134\n",
      "Epoch 5 | Step 3394900 | Avg Loss: 0.0153 | Grad Norm: 0.00847126\n",
      "Epoch 5 | Step 3395000 | Avg Loss: 0.0156 | Grad Norm: 0.00870195\n",
      "Epoch 5 | Step 3395100 | Avg Loss: 0.0155 | Grad Norm: 0.00973438\n",
      "Epoch 5 | Step 3395200 | Avg Loss: 0.0156 | Grad Norm: 0.00797400\n",
      "Epoch 5 | Step 3395300 | Avg Loss: 0.0156 | Grad Norm: 0.00869679\n",
      "Epoch 5 | Step 3395400 | Avg Loss: 0.0153 | Grad Norm: 0.00916630\n",
      "Epoch 5 | Step 3395500 | Avg Loss: 0.0155 | Grad Norm: 0.01002056\n",
      "Epoch 5 | Step 3395600 | Avg Loss: 0.0153 | Grad Norm: 0.00853754\n",
      "Epoch 5 | Step 3395700 | Avg Loss: 0.0155 | Grad Norm: 0.00935169\n",
      "Epoch 5 | Step 3395800 | Avg Loss: 0.0152 | Grad Norm: 0.00762397\n",
      "Epoch 5 | Step 3395900 | Avg Loss: 0.0148 | Grad Norm: 0.00828318\n",
      "Epoch 5 | Step 3396000 | Avg Loss: 0.0146 | Grad Norm: 0.00844133\n",
      "Epoch 5 | Step 3396100 | Avg Loss: 0.0148 | Grad Norm: 0.01302445\n",
      "Epoch 5 | Step 3396200 | Avg Loss: 0.0152 | Grad Norm: 0.00844985\n",
      "Epoch 5 | Step 3396300 | Avg Loss: 0.0152 | Grad Norm: 0.00866044\n",
      "Epoch 5 | Step 3396400 | Avg Loss: 0.0151 | Grad Norm: 0.01067521\n",
      "Epoch 5 | Step 3396500 | Avg Loss: 0.0153 | Grad Norm: 0.00851611\n",
      "Epoch 5 | Step 3396600 | Avg Loss: 0.0154 | Grad Norm: 0.00913185\n",
      "Epoch 5 | Step 3396700 | Avg Loss: 0.0152 | Grad Norm: 0.00835249\n",
      "Epoch 5 | Step 3396800 | Avg Loss: 0.0152 | Grad Norm: 0.00804114\n",
      "Epoch 5 | Step 3396900 | Avg Loss: 0.0155 | Grad Norm: 0.00844758\n",
      "Epoch 5 | Step 3397000 | Avg Loss: 0.0151 | Grad Norm: 0.00945542\n",
      "Epoch 5 | Step 3397100 | Avg Loss: 0.0152 | Grad Norm: 0.00887312\n",
      "Epoch 5 | Step 3397200 | Avg Loss: 0.0154 | Grad Norm: 0.00921762\n",
      "Epoch 5 | Step 3397300 | Avg Loss: 0.0158 | Grad Norm: 0.01080315\n",
      "Epoch 5 | Step 3397400 | Avg Loss: 0.0156 | Grad Norm: 0.00826148\n",
      "Epoch 5 | Step 3397500 | Avg Loss: 0.0154 | Grad Norm: 0.00908513\n",
      "Epoch 5 | Step 3397600 | Avg Loss: 0.0156 | Grad Norm: 0.01028129\n",
      "Epoch 5 | Step 3397700 | Avg Loss: 0.0157 | Grad Norm: 0.00842397\n",
      "Epoch 5 | Step 3397800 | Avg Loss: 0.0160 | Grad Norm: 0.01009694\n",
      "Epoch 5 | Step 3397900 | Avg Loss: 0.0155 | Grad Norm: 0.00864770\n",
      "Epoch 5 | Step 3398000 | Avg Loss: 0.0150 | Grad Norm: 0.00904648\n",
      "Epoch 5 | Step 3398100 | Avg Loss: 0.0149 | Grad Norm: 0.00915229\n",
      "Epoch 5 | Step 3398200 | Avg Loss: 0.0153 | Grad Norm: 0.00836359\n",
      "Epoch 5 | Step 3398300 | Avg Loss: 0.0154 | Grad Norm: 0.01005807\n",
      "Epoch 5 | Step 3398400 | Avg Loss: 0.0150 | Grad Norm: 0.00970731\n",
      "Epoch 5 | Step 3398500 | Avg Loss: 0.0151 | Grad Norm: 0.00938708\n",
      "Epoch 5 | Step 3398600 | Avg Loss: 0.0150 | Grad Norm: 0.00814592\n",
      "Epoch 5 | Step 3398700 | Avg Loss: 0.0152 | Grad Norm: 0.00913083\n",
      "Epoch 5 | Step 3398800 | Avg Loss: 0.0153 | Grad Norm: 0.00868293\n",
      "Epoch 5 | Step 3398900 | Avg Loss: 0.0153 | Grad Norm: 0.00871294\n",
      "Epoch 5 | Step 3399000 | Avg Loss: 0.0152 | Grad Norm: 0.01000215\n",
      "Epoch 5 | Step 3399100 | Avg Loss: 0.0153 | Grad Norm: 0.00893946\n",
      "Epoch 5 | Step 3399200 | Avg Loss: 0.0151 | Grad Norm: 0.00898784\n",
      "Epoch 5 | Step 3399300 | Avg Loss: 0.0153 | Grad Norm: 0.00904886\n",
      "Epoch 5 | Step 3399400 | Avg Loss: 0.0153 | Grad Norm: 0.01011026\n",
      "Epoch 5 | Step 3399500 | Avg Loss: 0.0152 | Grad Norm: 0.01018154\n",
      "Epoch 5 | Step 3399600 | Avg Loss: 0.0147 | Grad Norm: 0.00828923\n",
      "Epoch 5 | Step 3399700 | Avg Loss: 0.0148 | Grad Norm: 0.00758289\n",
      "Epoch 5 | Step 3399800 | Avg Loss: 0.0149 | Grad Norm: 0.00851812\n",
      "Epoch 5 | Step 3399900 | Avg Loss: 0.0157 | Grad Norm: 0.00937670\n",
      "Epoch 5 | Step 3400000 | Avg Loss: 0.0154 | Grad Norm: 0.00863419\n",
      "Saving model at step3400000\n",
      "Epoch 5 | Step 3400100 | Avg Loss: 0.0157 | Grad Norm: 0.00786276\n",
      "Epoch 5 | Step 3400200 | Avg Loss: 0.0158 | Grad Norm: 0.00878505\n",
      "Epoch 5 | Step 3400300 | Avg Loss: 0.0153 | Grad Norm: 0.01024420\n",
      "Epoch 5 | Step 3400400 | Avg Loss: 0.0153 | Grad Norm: 0.01062690\n",
      "Epoch 5 | Step 3400500 | Avg Loss: 0.0152 | Grad Norm: 0.01075101\n",
      "Epoch 5 | Step 3400600 | Avg Loss: 0.0152 | Grad Norm: 0.00898225\n",
      "Epoch 5 | Step 3400700 | Avg Loss: 0.0152 | Grad Norm: 0.00843516\n",
      "Epoch 5 | Step 3400800 | Avg Loss: 0.0151 | Grad Norm: 0.00871391\n",
      "Epoch 5 | Step 3400900 | Avg Loss: 0.0155 | Grad Norm: 0.00954212\n",
      "Epoch 5 | Step 3401000 | Avg Loss: 0.0157 | Grad Norm: 0.00902272\n",
      "Epoch 5 | Step 3401100 | Avg Loss: 0.0157 | Grad Norm: 0.00849506\n",
      "Epoch 5 | Step 3401200 | Avg Loss: 0.0156 | Grad Norm: 0.00883333\n",
      "Epoch 5 | Step 3401300 | Avg Loss: 0.0156 | Grad Norm: 0.00906504\n",
      "Epoch 5 | Step 3401400 | Avg Loss: 0.0159 | Grad Norm: 0.01247251\n",
      "Epoch 5 | Step 3401500 | Avg Loss: 0.0159 | Grad Norm: 0.00887411\n",
      "Epoch 5 | Step 3401600 | Avg Loss: 0.0152 | Grad Norm: 0.01056249\n",
      "Epoch 5 | Step 3401700 | Avg Loss: 0.0150 | Grad Norm: 0.00821054\n",
      "Epoch 5 | Step 3401800 | Avg Loss: 0.0154 | Grad Norm: 0.00965967\n",
      "Epoch 5 | Step 3401900 | Avg Loss: 0.0155 | Grad Norm: 0.00947695\n",
      "Epoch 5 | Step 3402000 | Avg Loss: 0.0155 | Grad Norm: 0.00922533\n",
      "Epoch 5 | Step 3402100 | Avg Loss: 0.0151 | Grad Norm: 0.01025318\n",
      "Epoch 5 | Step 3402200 | Avg Loss: 0.0151 | Grad Norm: 0.00840348\n",
      "Epoch 5 | Step 3402300 | Avg Loss: 0.0150 | Grad Norm: 0.01648245\n",
      "Epoch 5 | Step 3402400 | Avg Loss: 0.0153 | Grad Norm: 0.00893023\n",
      "Epoch 5 | Step 3402500 | Avg Loss: 0.0154 | Grad Norm: 0.00893844\n",
      "Epoch 5 | Step 3402600 | Avg Loss: 0.0152 | Grad Norm: 0.00994402\n",
      "Epoch 5 | Step 3402700 | Avg Loss: 0.0149 | Grad Norm: 0.00781415\n",
      "Epoch 5 | Step 3402800 | Avg Loss: 0.0150 | Grad Norm: 0.00787866\n",
      "Epoch 5 | Step 3402900 | Avg Loss: 0.0150 | Grad Norm: 0.00758410\n",
      "Epoch 5 | Step 3403000 | Avg Loss: 0.0152 | Grad Norm: 0.00897310\n",
      "Epoch 5 | Step 3403100 | Avg Loss: 0.0154 | Grad Norm: 0.00945331\n",
      "Epoch 5 | Step 3403200 | Avg Loss: 0.0153 | Grad Norm: 0.00885421\n",
      "Epoch 5 | Step 3403300 | Avg Loss: 0.0152 | Grad Norm: 0.00874796\n",
      "Epoch 5 | Step 3403400 | Avg Loss: 0.0153 | Grad Norm: 0.00921371\n",
      "Epoch 5 | Step 3403500 | Avg Loss: 0.0150 | Grad Norm: 0.00950305\n",
      "Epoch 5 | Step 3403600 | Avg Loss: 0.0150 | Grad Norm: 0.00826460\n",
      "Epoch 5 | Step 3403700 | Avg Loss: 0.0152 | Grad Norm: 0.00839486\n",
      "Epoch 5 | Step 3403800 | Avg Loss: 0.0153 | Grad Norm: 0.00917869\n",
      "Epoch 5 | Step 3403900 | Avg Loss: 0.0156 | Grad Norm: 0.00945004\n",
      "Epoch 5 | Step 3404000 | Avg Loss: 0.0155 | Grad Norm: 0.00876765\n",
      "Epoch 5 | Step 3404100 | Avg Loss: 0.0154 | Grad Norm: 0.00766105\n",
      "Epoch 5 | Step 3404200 | Avg Loss: 0.0156 | Grad Norm: 0.00895277\n",
      "Epoch 5 | Step 3404300 | Avg Loss: 0.0156 | Grad Norm: 0.00800905\n",
      "Epoch 5 | Step 3404400 | Avg Loss: 0.0157 | Grad Norm: 0.00891384\n",
      "Epoch 5 | Step 3404500 | Avg Loss: 0.0160 | Grad Norm: 0.00812194\n",
      "Epoch 5 | Step 3404600 | Avg Loss: 0.0160 | Grad Norm: 0.00801126\n",
      "Epoch 5 | Step 3404700 | Avg Loss: 0.0159 | Grad Norm: 0.01071054\n",
      "Epoch 5 | Step 3404800 | Avg Loss: 0.0158 | Grad Norm: 0.00902147\n",
      "Epoch 5 | Step 3404900 | Avg Loss: 0.0159 | Grad Norm: 0.00831974\n",
      "Epoch 5 | Step 3405000 | Avg Loss: 0.0163 | Grad Norm: 0.01383579\n",
      "Epoch 5 | Step 3405100 | Avg Loss: 0.0155 | Grad Norm: 0.00960714\n",
      "Epoch 5 | Step 3405200 | Avg Loss: 0.0156 | Grad Norm: 0.00818994\n",
      "Epoch 5 | Step 3405300 | Avg Loss: 0.0154 | Grad Norm: 0.01121968\n",
      "Epoch 5 | Step 3405400 | Avg Loss: 0.0155 | Grad Norm: 0.00792242\n",
      "Epoch 5 | Step 3405500 | Avg Loss: 0.0154 | Grad Norm: 0.01059006\n",
      "Epoch 5 | Step 3405600 | Avg Loss: 0.0153 | Grad Norm: 0.00958644\n",
      "Epoch 5 | Step 3405700 | Avg Loss: 0.0156 | Grad Norm: 0.00879146\n",
      "Epoch 5 | Step 3405800 | Avg Loss: 0.0153 | Grad Norm: 0.00890623\n",
      "Epoch 5 | Step 3405900 | Avg Loss: 0.0152 | Grad Norm: 0.00877082\n",
      "Epoch 5 | Step 3406000 | Avg Loss: 0.0156 | Grad Norm: 0.00946560\n",
      "Epoch 5 | Step 3406100 | Avg Loss: 0.0155 | Grad Norm: 0.00932541\n",
      "Epoch 5 | Step 3406200 | Avg Loss: 0.0158 | Grad Norm: 0.01039898\n",
      "Epoch 5 | Step 3406300 | Avg Loss: 0.0160 | Grad Norm: 0.00906487\n",
      "Epoch 5 | Step 3406400 | Avg Loss: 0.0160 | Grad Norm: 0.00879322\n",
      "Epoch 5 | Step 3406500 | Avg Loss: 0.0160 | Grad Norm: 0.01006811\n",
      "Epoch 5 | Step 3406600 | Avg Loss: 0.0157 | Grad Norm: 0.00866912\n",
      "Epoch 5 | Step 3406700 | Avg Loss: 0.0157 | Grad Norm: 0.01105965\n",
      "Epoch 5 | Step 3406800 | Avg Loss: 0.0155 | Grad Norm: 0.00874396\n",
      "Epoch 5 | Step 3406900 | Avg Loss: 0.0157 | Grad Norm: 0.01169911\n",
      "Epoch 5 | Step 3407000 | Avg Loss: 0.0156 | Grad Norm: 0.00861957\n",
      "Epoch 5 | Step 3407100 | Avg Loss: 0.0156 | Grad Norm: 0.00951985\n",
      "Epoch 5 | Step 3407200 | Avg Loss: 0.0154 | Grad Norm: 0.00809557\n",
      "Epoch 5 | Step 3407300 | Avg Loss: 0.0154 | Grad Norm: 0.00889429\n",
      "Epoch 5 | Step 3407400 | Avg Loss: 0.0153 | Grad Norm: 0.00866730\n",
      "Epoch 5 | Step 3407500 | Avg Loss: 0.0154 | Grad Norm: 0.01033001\n",
      "Epoch 5 | Step 3407600 | Avg Loss: 0.0155 | Grad Norm: 0.00819784\n",
      "Epoch 5 | Step 3407700 | Avg Loss: 0.0152 | Grad Norm: 0.00850539\n",
      "Epoch 5 | Step 3407800 | Avg Loss: 0.0152 | Grad Norm: 0.00861736\n",
      "Epoch 5 | Step 3407900 | Avg Loss: 0.0155 | Grad Norm: 0.01018545\n",
      "Epoch 5 | Step 3408000 | Avg Loss: 0.0156 | Grad Norm: 0.00799456\n",
      "Epoch 5 | Step 3408100 | Avg Loss: 0.0156 | Grad Norm: 0.01007679\n",
      "Epoch 5 | Step 3408200 | Avg Loss: 0.0155 | Grad Norm: 0.01049850\n",
      "Epoch 5 | Step 3408300 | Avg Loss: 0.0158 | Grad Norm: 0.00930113\n",
      "Epoch 5 | Step 3408400 | Avg Loss: 0.0154 | Grad Norm: 0.00860791\n",
      "Epoch 5 | Step 3408500 | Avg Loss: 0.0156 | Grad Norm: 0.00805763\n",
      "Epoch 5 | Step 3408600 | Avg Loss: 0.0156 | Grad Norm: 0.00901745\n",
      "Epoch 5 | Step 3408700 | Avg Loss: 0.0155 | Grad Norm: 0.00829980\n",
      "Epoch 5 | Step 3408800 | Avg Loss: 0.0152 | Grad Norm: 0.00784833\n",
      "Epoch 5 | Step 3408900 | Avg Loss: 0.0155 | Grad Norm: 0.00792293\n",
      "Epoch 5 | Step 3409000 | Avg Loss: 0.0153 | Grad Norm: 0.00951940\n",
      "Epoch 5 | Step 3409100 | Avg Loss: 0.0153 | Grad Norm: 0.00974833\n",
      "Epoch 5 | Step 3409200 | Avg Loss: 0.0151 | Grad Norm: 0.00906741\n",
      "Epoch 5 | Step 3409300 | Avg Loss: 0.0154 | Grad Norm: 0.00861507\n",
      "Epoch 5 | Step 3409400 | Avg Loss: 0.0152 | Grad Norm: 0.00939472\n",
      "Epoch 5 | Step 3409500 | Avg Loss: 0.0150 | Grad Norm: 0.01078007\n",
      "Epoch 5 | Step 3409600 | Avg Loss: 0.0157 | Grad Norm: 0.00934991\n",
      "Epoch 5 | Step 3409700 | Avg Loss: 0.0159 | Grad Norm: 0.01103901\n",
      "Epoch 5 | Step 3409800 | Avg Loss: 0.0160 | Grad Norm: 0.00828798\n",
      "Epoch 5 | Step 3409900 | Avg Loss: 0.0157 | Grad Norm: 0.00808010\n",
      "Epoch 5 | Step 3410000 | Avg Loss: 0.0160 | Grad Norm: 0.00929547\n",
      "Epoch 5 | Step 3410100 | Avg Loss: 0.0160 | Grad Norm: 0.00947091\n",
      "Epoch 5 | Step 3410200 | Avg Loss: 0.0159 | Grad Norm: 0.01306016\n",
      "Epoch 5 | Step 3410300 | Avg Loss: 0.0157 | Grad Norm: 0.00919494\n",
      "Epoch 5 | Step 3410400 | Avg Loss: 0.0154 | Grad Norm: 0.01118513\n",
      "Epoch 5 | Step 3410500 | Avg Loss: 0.0150 | Grad Norm: 0.00955958\n",
      "Epoch 5 | Step 3410600 | Avg Loss: 0.0151 | Grad Norm: 0.00946883\n",
      "Epoch 5 | Step 3410700 | Avg Loss: 0.0151 | Grad Norm: 0.00998654\n",
      "Epoch 5 | Step 3410800 | Avg Loss: 0.0149 | Grad Norm: 0.00985504\n",
      "Epoch 5 | Step 3410900 | Avg Loss: 0.0149 | Grad Norm: 0.00948622\n",
      "Epoch 5 | Step 3411000 | Avg Loss: 0.0149 | Grad Norm: 0.01160515\n",
      "Epoch 5 | Step 3411100 | Avg Loss: 0.0149 | Grad Norm: 0.00977292\n",
      "Epoch 5 | Step 3411200 | Avg Loss: 0.0153 | Grad Norm: 0.00838063\n",
      "Epoch 5 | Step 3411300 | Avg Loss: 0.0150 | Grad Norm: 0.01005220\n",
      "Epoch 5 | Step 3411400 | Avg Loss: 0.0151 | Grad Norm: 0.00881127\n",
      "Epoch 5 | Step 3411500 | Avg Loss: 0.0152 | Grad Norm: 0.00798979\n",
      "Epoch 5 | Step 3411600 | Avg Loss: 0.0149 | Grad Norm: 0.00866869\n",
      "Epoch 5 | Step 3411700 | Avg Loss: 0.0154 | Grad Norm: 0.01019151\n",
      "Epoch 5 | Step 3411800 | Avg Loss: 0.0157 | Grad Norm: 0.00888108\n",
      "Epoch 5 | Step 3411900 | Avg Loss: 0.0161 | Grad Norm: 0.00982166\n",
      "Epoch 5 | Step 3412000 | Avg Loss: 0.0163 | Grad Norm: 0.01122336\n",
      "Epoch 5 | Step 3412100 | Avg Loss: 0.0158 | Grad Norm: 0.01070739\n",
      "Epoch 5 | Step 3412200 | Avg Loss: 0.0161 | Grad Norm: 0.00892613\n",
      "Epoch 5 | Step 3412300 | Avg Loss: 0.0161 | Grad Norm: 0.00927589\n",
      "Epoch 5 | Step 3412400 | Avg Loss: 0.0155 | Grad Norm: 0.00969348\n",
      "Epoch 5 | Step 3412500 | Avg Loss: 0.0160 | Grad Norm: 0.00985335\n",
      "Epoch 5 | Step 3412600 | Avg Loss: 0.0161 | Grad Norm: 0.00977009\n",
      "Epoch 5 | Step 3412700 | Avg Loss: 0.0159 | Grad Norm: 0.01019861\n",
      "Epoch 5 | Step 3412800 | Avg Loss: 0.0159 | Grad Norm: 0.01043795\n",
      "Epoch 5 | Step 3412900 | Avg Loss: 0.0157 | Grad Norm: 0.00844375\n",
      "Epoch 5 | Step 3413000 | Avg Loss: 0.0155 | Grad Norm: 0.00860511\n",
      "Epoch 5 | Step 3413100 | Avg Loss: 0.0153 | Grad Norm: 0.00900059\n",
      "Epoch 5 | Step 3413200 | Avg Loss: 0.0152 | Grad Norm: 0.00873943\n",
      "Epoch 5 | Step 3413300 | Avg Loss: 0.0152 | Grad Norm: 0.01275360\n",
      "Epoch 5 | Step 3413400 | Avg Loss: 0.0154 | Grad Norm: 0.00891207\n",
      "Epoch 5 | Step 3413500 | Avg Loss: 0.0151 | Grad Norm: 0.00964719\n",
      "Epoch 5 | Step 3413600 | Avg Loss: 0.0156 | Grad Norm: 0.00840294\n",
      "Epoch 5 | Step 3413700 | Avg Loss: 0.0158 | Grad Norm: 0.00904278\n",
      "Epoch 5 | Step 3413800 | Avg Loss: 0.0157 | Grad Norm: 0.00901212\n",
      "Epoch 5 | Step 3413900 | Avg Loss: 0.0154 | Grad Norm: 0.00938227\n",
      "Epoch 5 | Step 3414000 | Avg Loss: 0.0156 | Grad Norm: 0.01079855\n",
      "Epoch 5 | Step 3414100 | Avg Loss: 0.0159 | Grad Norm: 0.01003438\n",
      "Epoch 5 | Step 3414200 | Avg Loss: 0.0161 | Grad Norm: 0.00947607\n",
      "Epoch 5 | Step 3414300 | Avg Loss: 0.0159 | Grad Norm: 0.00982616\n",
      "Epoch 5 | Step 3414400 | Avg Loss: 0.0155 | Grad Norm: 0.00841172\n",
      "Epoch 5 | Step 3414500 | Avg Loss: 0.0150 | Grad Norm: 0.01077275\n",
      "Epoch 5 | Step 3414600 | Avg Loss: 0.0154 | Grad Norm: 0.00940179\n",
      "Epoch 5 | Step 3414700 | Avg Loss: 0.0153 | Grad Norm: 0.00865013\n",
      "Epoch 5 | Step 3414800 | Avg Loss: 0.0158 | Grad Norm: 0.00940468\n",
      "Epoch 5 | Step 3414900 | Avg Loss: 0.0154 | Grad Norm: 0.00919641\n",
      "Epoch 5 | Step 3415000 | Avg Loss: 0.0158 | Grad Norm: 0.00831124\n",
      "Epoch 5 | Step 3415100 | Avg Loss: 0.0160 | Grad Norm: 0.00862509\n",
      "Epoch 5 | Step 3415200 | Avg Loss: 0.0156 | Grad Norm: 0.00970605\n",
      "Epoch 5 | Step 3415300 | Avg Loss: 0.0155 | Grad Norm: 0.00948970\n",
      "Epoch 5 | Step 3415400 | Avg Loss: 0.0151 | Grad Norm: 0.00813177\n",
      "Epoch 5 | Step 3415500 | Avg Loss: 0.0151 | Grad Norm: 0.00963283\n",
      "Epoch 5 | Step 3415600 | Avg Loss: 0.0151 | Grad Norm: 0.00858804\n",
      "Epoch 5 | Step 3415700 | Avg Loss: 0.0149 | Grad Norm: 0.01064000\n",
      "Epoch 5 | Step 3415800 | Avg Loss: 0.0147 | Grad Norm: 0.00798017\n",
      "Epoch 5 | Step 3415900 | Avg Loss: 0.0149 | Grad Norm: 0.00928023\n",
      "Epoch 5 | Step 3416000 | Avg Loss: 0.0154 | Grad Norm: 0.00933276\n",
      "Epoch 5 | Step 3416100 | Avg Loss: 0.0155 | Grad Norm: 0.00953592\n",
      "Epoch 5 | Step 3416200 | Avg Loss: 0.0156 | Grad Norm: 0.00858099\n",
      "Epoch 5 | Step 3416300 | Avg Loss: 0.0157 | Grad Norm: 0.00836103\n",
      "Epoch 5 | Step 3416400 | Avg Loss: 0.0159 | Grad Norm: 0.00900660\n",
      "Epoch 5 | Step 3416500 | Avg Loss: 0.0158 | Grad Norm: 0.01012664\n",
      "Epoch 5 | Step 3416600 | Avg Loss: 0.0156 | Grad Norm: 0.00866214\n",
      "Epoch 5 | Step 3416700 | Avg Loss: 0.0158 | Grad Norm: 0.01150501\n",
      "Epoch 5 | Step 3416800 | Avg Loss: 0.0160 | Grad Norm: 0.00969878\n",
      "Epoch 5 | Step 3416900 | Avg Loss: 0.0158 | Grad Norm: 0.01004562\n",
      "Epoch 5 | Step 3417000 | Avg Loss: 0.0158 | Grad Norm: 0.01061785\n",
      "Epoch 5 | Step 3417100 | Avg Loss: 0.0162 | Grad Norm: 0.00911203\n",
      "Epoch 5 | Step 3417200 | Avg Loss: 0.0158 | Grad Norm: 0.00952643\n",
      "Epoch 5 | Step 3417300 | Avg Loss: 0.0154 | Grad Norm: 0.00992487\n",
      "Epoch 5 | Step 3417400 | Avg Loss: 0.0153 | Grad Norm: 0.00843840\n",
      "Epoch 5 | Step 3417500 | Avg Loss: 0.0153 | Grad Norm: 0.00829660\n",
      "Epoch 5 | Step 3417600 | Avg Loss: 0.0151 | Grad Norm: 0.00814603\n",
      "Epoch 5 | Step 3417700 | Avg Loss: 0.0151 | Grad Norm: 0.00959954\n",
      "Epoch 5 | Step 3417800 | Avg Loss: 0.0157 | Grad Norm: 0.00813924\n",
      "Epoch 5 | Step 3417900 | Avg Loss: 0.0159 | Grad Norm: 0.00828776\n",
      "Epoch 5 | Step 3418000 | Avg Loss: 0.0159 | Grad Norm: 0.00794784\n",
      "Epoch 5 | Step 3418100 | Avg Loss: 0.0159 | Grad Norm: 0.00975244\n",
      "Epoch 5 | Step 3418200 | Avg Loss: 0.0159 | Grad Norm: 0.01023429\n",
      "Epoch 5 | Step 3418300 | Avg Loss: 0.0156 | Grad Norm: 0.00953459\n",
      "Epoch 5 | Step 3418400 | Avg Loss: 0.0158 | Grad Norm: 0.00933839\n",
      "Epoch 5 | Step 3418500 | Avg Loss: 0.0154 | Grad Norm: 0.01058499\n",
      "Epoch 5 | Step 3418600 | Avg Loss: 0.0156 | Grad Norm: 0.00899035\n",
      "Epoch 5 | Step 3418700 | Avg Loss: 0.0153 | Grad Norm: 0.01077587\n",
      "Epoch 5 | Step 3418800 | Avg Loss: 0.0152 | Grad Norm: 0.00899549\n",
      "Epoch 5 | Step 3418900 | Avg Loss: 0.0153 | Grad Norm: 0.00916232\n",
      "Epoch 5 | Step 3419000 | Avg Loss: 0.0148 | Grad Norm: 0.00889079\n",
      "Epoch 5 | Step 3419100 | Avg Loss: 0.0150 | Grad Norm: 0.00881622\n",
      "Epoch 5 | Step 3419200 | Avg Loss: 0.0151 | Grad Norm: 0.01004616\n",
      "Epoch 5 | Step 3419300 | Avg Loss: 0.0155 | Grad Norm: 0.00951149\n",
      "Epoch 5 | Step 3419400 | Avg Loss: 0.0156 | Grad Norm: 0.00945715\n",
      "Epoch 5 | Step 3419500 | Avg Loss: 0.0154 | Grad Norm: 0.01018588\n",
      "Epoch 5 | Step 3419600 | Avg Loss: 0.0157 | Grad Norm: 0.00878309\n",
      "Epoch 5 | Step 3419700 | Avg Loss: 0.0159 | Grad Norm: 0.01010100\n",
      "Epoch 5 | Step 3419800 | Avg Loss: 0.0161 | Grad Norm: 0.01105930\n",
      "Epoch 5 | Step 3419900 | Avg Loss: 0.0160 | Grad Norm: 0.01053812\n",
      "Epoch 5 | Step 3420000 | Avg Loss: 0.0159 | Grad Norm: 0.01048210\n",
      "Epoch 5 | Step 3420100 | Avg Loss: 0.0155 | Grad Norm: 0.00899617\n",
      "Epoch 5 | Step 3420200 | Avg Loss: 0.0155 | Grad Norm: 0.00897234\n",
      "Epoch 5 | Step 3420300 | Avg Loss: 0.0154 | Grad Norm: 0.00980584\n",
      "Epoch 5 | Step 3420400 | Avg Loss: 0.0154 | Grad Norm: 0.00888738\n",
      "Epoch 5 | Step 3420500 | Avg Loss: 0.0157 | Grad Norm: 0.01088955\n",
      "Epoch 5 | Step 3420600 | Avg Loss: 0.0157 | Grad Norm: 0.00922717\n",
      "Epoch 5 | Step 3420700 | Avg Loss: 0.0153 | Grad Norm: 0.00882916\n",
      "Epoch 5 | Step 3420800 | Avg Loss: 0.0154 | Grad Norm: 0.00851752\n",
      "Epoch 5 | Step 3420900 | Avg Loss: 0.0153 | Grad Norm: 0.00863474\n",
      "Epoch 5 | Step 3421000 | Avg Loss: 0.0159 | Grad Norm: 0.01169199\n",
      "Epoch 5 | Step 3421100 | Avg Loss: 0.0157 | Grad Norm: 0.01000828\n",
      "Epoch 5 | Step 3421200 | Avg Loss: 0.0160 | Grad Norm: 0.00852466\n",
      "Epoch 5 | Step 3421300 | Avg Loss: 0.0163 | Grad Norm: 0.00864932\n",
      "Epoch 5 | Step 3421400 | Avg Loss: 0.0162 | Grad Norm: 0.00896440\n",
      "Epoch 5 | Step 3421500 | Avg Loss: 0.0154 | Grad Norm: 0.00878379\n",
      "Epoch 5 | Step 3421600 | Avg Loss: 0.0155 | Grad Norm: 0.00818761\n",
      "Epoch 5 | Step 3421700 | Avg Loss: 0.0154 | Grad Norm: 0.01012801\n",
      "Epoch 5 | Step 3421800 | Avg Loss: 0.0153 | Grad Norm: 0.00941695\n",
      "Epoch 5 | Step 3421900 | Avg Loss: 0.0154 | Grad Norm: 0.00937007\n",
      "Epoch 5 | Step 3422000 | Avg Loss: 0.0159 | Grad Norm: 0.00915267\n",
      "Epoch 5 | Step 3422100 | Avg Loss: 0.0162 | Grad Norm: 0.00855137\n",
      "Epoch 5 | Step 3422200 | Avg Loss: 0.0159 | Grad Norm: 0.00944972\n",
      "Epoch 5 | Step 3422300 | Avg Loss: 0.0156 | Grad Norm: 0.00832566\n",
      "Epoch 5 | Step 3422400 | Avg Loss: 0.0157 | Grad Norm: 0.01017424\n",
      "Epoch 5 | Step 3422500 | Avg Loss: 0.0164 | Grad Norm: 0.00935893\n",
      "Epoch 5 | Step 3422600 | Avg Loss: 0.0163 | Grad Norm: 0.00928162\n",
      "Epoch 5 | Step 3422700 | Avg Loss: 0.0163 | Grad Norm: 0.00974988\n",
      "Epoch 5 | Step 3422800 | Avg Loss: 0.0163 | Grad Norm: 0.00803581\n",
      "Epoch 5 | Step 3422900 | Avg Loss: 0.0165 | Grad Norm: 0.01080504\n",
      "Epoch 5 | Step 3423000 | Avg Loss: 0.0162 | Grad Norm: 0.00876272\n",
      "Epoch 5 | Step 3423100 | Avg Loss: 0.0159 | Grad Norm: 0.01022693\n",
      "Epoch 5 | Step 3423200 | Avg Loss: 0.0153 | Grad Norm: 0.00816565\n",
      "Epoch 5 | Step 3423300 | Avg Loss: 0.0155 | Grad Norm: 0.00913150\n",
      "Epoch 5 | Step 3423400 | Avg Loss: 0.0155 | Grad Norm: 0.01194756\n",
      "Epoch 5 | Step 3423500 | Avg Loss: 0.0156 | Grad Norm: 0.01017803\n",
      "Epoch 5 | Step 3423600 | Avg Loss: 0.0155 | Grad Norm: 0.00893537\n",
      "Epoch 5 | Step 3423700 | Avg Loss: 0.0156 | Grad Norm: 0.00907024\n",
      "Epoch 5 | Step 3423800 | Avg Loss: 0.0156 | Grad Norm: 0.00970905\n",
      "Epoch 5 | Step 3423900 | Avg Loss: 0.0156 | Grad Norm: 0.00928434\n",
      "Epoch 5 | Step 3424000 | Avg Loss: 0.0150 | Grad Norm: 0.00859604\n",
      "Epoch 5 | Step 3424100 | Avg Loss: 0.0153 | Grad Norm: 0.00899618\n",
      "Epoch 5 | Step 3424200 | Avg Loss: 0.0152 | Grad Norm: 0.00780598\n",
      "Epoch 5 | Step 3424300 | Avg Loss: 0.0153 | Grad Norm: 0.01019376\n",
      "Epoch 5 | Step 3424400 | Avg Loss: 0.0153 | Grad Norm: 0.01006280\n",
      "Epoch 5 | Step 3424500 | Avg Loss: 0.0154 | Grad Norm: 0.00955727\n",
      "Epoch 5 | Step 3424600 | Avg Loss: 0.0153 | Grad Norm: 0.00848354\n",
      "Epoch 5 | Step 3424700 | Avg Loss: 0.0152 | Grad Norm: 0.01438023\n",
      "Epoch 5 | Step 3424800 | Avg Loss: 0.0150 | Grad Norm: 0.00911140\n",
      "Epoch 5 | Step 3424900 | Avg Loss: 0.0151 | Grad Norm: 0.00895644\n",
      "Epoch 5 | Step 3425000 | Avg Loss: 0.0150 | Grad Norm: 0.01089484\n",
      "Epoch 5 | Step 3425100 | Avg Loss: 0.0149 | Grad Norm: 0.00931596\n",
      "Epoch 5 | Step 3425200 | Avg Loss: 0.0152 | Grad Norm: 0.00974960\n",
      "Epoch 5 | Step 3425300 | Avg Loss: 0.0151 | Grad Norm: 0.00903896\n",
      "Epoch 5 | Step 3425400 | Avg Loss: 0.0146 | Grad Norm: 0.00793255\n",
      "Epoch 5 | Step 3425500 | Avg Loss: 0.0146 | Grad Norm: 0.00899234\n",
      "Epoch 5 | Step 3425600 | Avg Loss: 0.0148 | Grad Norm: 0.00982713\n",
      "Epoch 5 | Step 3425700 | Avg Loss: 0.0148 | Grad Norm: 0.00856602\n",
      "Epoch 5 | Step 3425800 | Avg Loss: 0.0148 | Grad Norm: 0.00859814\n",
      "Epoch 5 | Step 3425900 | Avg Loss: 0.0149 | Grad Norm: 0.00771184\n",
      "Epoch 5 | Step 3426000 | Avg Loss: 0.0149 | Grad Norm: 0.00878808\n",
      "Epoch 5 | Step 3426100 | Avg Loss: 0.0149 | Grad Norm: 0.00803628\n",
      "Epoch 5 | Step 3426200 | Avg Loss: 0.0150 | Grad Norm: 0.00907113\n",
      "Epoch 5 | Step 3426300 | Avg Loss: 0.0154 | Grad Norm: 0.00984123\n",
      "Epoch 5 | Step 3426400 | Avg Loss: 0.0154 | Grad Norm: 0.00965554\n",
      "Epoch 5 | Step 3426500 | Avg Loss: 0.0154 | Grad Norm: 0.00895161\n",
      "Epoch 5 | Step 3426600 | Avg Loss: 0.0155 | Grad Norm: 0.00793986\n",
      "Epoch 5 | Step 3426700 | Avg Loss: 0.0156 | Grad Norm: 0.00964509\n",
      "Epoch 5 | Step 3426800 | Avg Loss: 0.0155 | Grad Norm: 0.00856355\n",
      "Epoch 5 | Step 3426900 | Avg Loss: 0.0154 | Grad Norm: 0.00951586\n",
      "Epoch 5 | Step 3427000 | Avg Loss: 0.0150 | Grad Norm: 0.01083852\n",
      "Epoch 5 | Step 3427100 | Avg Loss: 0.0152 | Grad Norm: 0.00878217\n",
      "Epoch 5 | Step 3427200 | Avg Loss: 0.0150 | Grad Norm: 0.00831514\n",
      "Epoch 5 | Step 3427300 | Avg Loss: 0.0150 | Grad Norm: 0.00741229\n",
      "Epoch 5 | Step 3427400 | Avg Loss: 0.0148 | Grad Norm: 0.01040819\n",
      "Epoch 5 | Step 3427500 | Avg Loss: 0.0148 | Grad Norm: 0.00839280\n",
      "Epoch 5 | Step 3427600 | Avg Loss: 0.0148 | Grad Norm: 0.00902208\n",
      "Epoch 5 | Step 3427700 | Avg Loss: 0.0152 | Grad Norm: 0.00846237\n",
      "Epoch 5 | Step 3427800 | Avg Loss: 0.0151 | Grad Norm: 0.00993206\n",
      "Epoch 5 | Step 3427900 | Avg Loss: 0.0153 | Grad Norm: 0.00843878\n",
      "Epoch 5 | Step 3428000 | Avg Loss: 0.0153 | Grad Norm: 0.01055573\n",
      "Epoch 5 | Step 3428100 | Avg Loss: 0.0158 | Grad Norm: 0.00863703\n",
      "Epoch 5 | Step 3428200 | Avg Loss: 0.0155 | Grad Norm: 0.00876248\n",
      "Epoch 5 | Step 3428300 | Avg Loss: 0.0155 | Grad Norm: 0.00969379\n",
      "Epoch 5 | Step 3428400 | Avg Loss: 0.0154 | Grad Norm: 0.00874952\n",
      "Epoch 5 | Step 3428500 | Avg Loss: 0.0155 | Grad Norm: 0.00975740\n",
      "Epoch 5 | Step 3428600 | Avg Loss: 0.0150 | Grad Norm: 0.00935688\n",
      "Epoch 5 | Step 3428700 | Avg Loss: 0.0154 | Grad Norm: 0.00860045\n",
      "Epoch 5 | Step 3428800 | Avg Loss: 0.0150 | Grad Norm: 0.00868424\n",
      "Epoch 5 | Step 3428900 | Avg Loss: 0.0150 | Grad Norm: 0.01050596\n",
      "Epoch 5 | Step 3429000 | Avg Loss: 0.0154 | Grad Norm: 0.00934811\n",
      "Epoch 5 | Step 3429100 | Avg Loss: 0.0152 | Grad Norm: 0.00913794\n",
      "Epoch 5 | Step 3429200 | Avg Loss: 0.0153 | Grad Norm: 0.00968865\n",
      "Epoch 5 | Step 3429300 | Avg Loss: 0.0154 | Grad Norm: 0.01070675\n",
      "Epoch 5 | Step 3429400 | Avg Loss: 0.0152 | Grad Norm: 0.01086399\n",
      "Epoch 5 | Step 3429500 | Avg Loss: 0.0154 | Grad Norm: 0.00896524\n",
      "Epoch 5 | Step 3429600 | Avg Loss: 0.0154 | Grad Norm: 0.00946123\n",
      "Epoch 5 | Step 3429700 | Avg Loss: 0.0152 | Grad Norm: 0.00912387\n",
      "Epoch 5 | Step 3429800 | Avg Loss: 0.0151 | Grad Norm: 0.00868845\n",
      "Epoch 5 | Step 3429900 | Avg Loss: 0.0152 | Grad Norm: 0.00943885\n",
      "Epoch 5 | Step 3430000 | Avg Loss: 0.0152 | Grad Norm: 0.01026021\n",
      "Epoch 5 | Step 3430100 | Avg Loss: 0.0151 | Grad Norm: 0.00922499\n",
      "Epoch 5 | Step 3430200 | Avg Loss: 0.0154 | Grad Norm: 0.00883304\n",
      "Epoch 5 | Step 3430300 | Avg Loss: 0.0153 | Grad Norm: 0.00873832\n",
      "Epoch 5 | Step 3430400 | Avg Loss: 0.0152 | Grad Norm: 0.00857156\n",
      "Epoch 5 | Step 3430500 | Avg Loss: 0.0153 | Grad Norm: 0.00917695\n",
      "Epoch 5 | Step 3430600 | Avg Loss: 0.0155 | Grad Norm: 0.00939957\n",
      "Epoch 5 | Step 3430700 | Avg Loss: 0.0152 | Grad Norm: 0.01014442\n",
      "Epoch 5 | Step 3430800 | Avg Loss: 0.0155 | Grad Norm: 0.00905113\n",
      "Epoch 5 | Step 3430900 | Avg Loss: 0.0152 | Grad Norm: 0.00872877\n",
      "Epoch 5 | Step 3431000 | Avg Loss: 0.0154 | Grad Norm: 0.00920337\n",
      "Epoch 5 | Step 3431100 | Avg Loss: 0.0156 | Grad Norm: 0.00899062\n",
      "Epoch 5 | Step 3431200 | Avg Loss: 0.0149 | Grad Norm: 0.00760617\n",
      "Epoch 5 | Step 3431300 | Avg Loss: 0.0149 | Grad Norm: 0.01017038\n",
      "Epoch 5 | Step 3431400 | Avg Loss: 0.0155 | Grad Norm: 0.00838052\n",
      "Epoch 5 | Step 3431500 | Avg Loss: 0.0154 | Grad Norm: 0.00784368\n",
      "Epoch 5 | Step 3431600 | Avg Loss: 0.0155 | Grad Norm: 0.00814242\n",
      "Epoch 5 | Step 3431700 | Avg Loss: 0.0154 | Grad Norm: 0.00869199\n",
      "Epoch 5 | Step 3431800 | Avg Loss: 0.0151 | Grad Norm: 0.00886934\n",
      "Epoch 5 | Step 3431900 | Avg Loss: 0.0153 | Grad Norm: 0.00908796\n",
      "Epoch 5 | Step 3432000 | Avg Loss: 0.0154 | Grad Norm: 0.00896134\n",
      "Epoch 5 | Step 3432100 | Avg Loss: 0.0158 | Grad Norm: 0.00896228\n",
      "Epoch 5 | Step 3432200 | Avg Loss: 0.0154 | Grad Norm: 0.00872300\n",
      "Epoch 5 | Step 3432300 | Avg Loss: 0.0154 | Grad Norm: 0.00942424\n",
      "Epoch 5 | Step 3432400 | Avg Loss: 0.0156 | Grad Norm: 0.01090099\n",
      "Epoch 5 | Step 3432500 | Avg Loss: 0.0158 | Grad Norm: 0.00951525\n",
      "Epoch 5 | Step 3432600 | Avg Loss: 0.0152 | Grad Norm: 0.00851565\n",
      "Epoch 5 | Step 3432700 | Avg Loss: 0.0153 | Grad Norm: 0.00992556\n",
      "Epoch 5 | Step 3432800 | Avg Loss: 0.0154 | Grad Norm: 0.00962037\n",
      "Epoch 5 | Step 3432900 | Avg Loss: 0.0153 | Grad Norm: 0.00909247\n",
      "Epoch 5 | Step 3433000 | Avg Loss: 0.0157 | Grad Norm: 0.00860708\n",
      "Epoch 5 | Step 3433100 | Avg Loss: 0.0159 | Grad Norm: 0.00908453\n",
      "Epoch 5 | Step 3433200 | Avg Loss: 0.0152 | Grad Norm: 0.00810310\n",
      "Epoch 5 | Step 3433300 | Avg Loss: 0.0150 | Grad Norm: 0.00923587\n",
      "Epoch 5 | Step 3433400 | Avg Loss: 0.0155 | Grad Norm: 0.00835235\n",
      "Epoch 5 | Step 3433500 | Avg Loss: 0.0154 | Grad Norm: 0.00838026\n",
      "Epoch 5 | Step 3433600 | Avg Loss: 0.0153 | Grad Norm: 0.01034600\n",
      "Epoch 5 | Step 3433700 | Avg Loss: 0.0152 | Grad Norm: 0.00836779\n",
      "Epoch 5 | Step 3433800 | Avg Loss: 0.0154 | Grad Norm: 0.01130498\n",
      "Epoch 5 | Step 3433900 | Avg Loss: 0.0154 | Grad Norm: 0.00793210\n",
      "Epoch 5 | Step 3434000 | Avg Loss: 0.0152 | Grad Norm: 0.00897489\n",
      "Epoch 5 | Step 3434100 | Avg Loss: 0.0150 | Grad Norm: 0.00882969\n",
      "Epoch 5 | Step 3434200 | Avg Loss: 0.0151 | Grad Norm: 0.00864831\n",
      "Epoch 5 | Step 3434300 | Avg Loss: 0.0153 | Grad Norm: 0.00903305\n",
      "Epoch 5 | Step 3434400 | Avg Loss: 0.0154 | Grad Norm: 0.00899312\n",
      "Epoch 5 | Step 3434500 | Avg Loss: 0.0151 | Grad Norm: 0.00898902\n",
      "Epoch 5 | Step 3434600 | Avg Loss: 0.0150 | Grad Norm: 0.00843790\n",
      "Epoch 5 | Step 3434700 | Avg Loss: 0.0151 | Grad Norm: 0.00919306\n",
      "Epoch 5 | Step 3434800 | Avg Loss: 0.0150 | Grad Norm: 0.00866776\n",
      "Epoch 5 | Step 3434900 | Avg Loss: 0.0151 | Grad Norm: 0.00894294\n",
      "Epoch 5 | Step 3435000 | Avg Loss: 0.0157 | Grad Norm: 0.01046019\n",
      "Epoch 5 | Step 3435100 | Avg Loss: 0.0154 | Grad Norm: 0.00942211\n",
      "Epoch 5 | Step 3435200 | Avg Loss: 0.0157 | Grad Norm: 0.01056389\n",
      "Epoch 5 | Step 3435300 | Avg Loss: 0.0157 | Grad Norm: 0.00794663\n",
      "Epoch 5 | Step 3435400 | Avg Loss: 0.0154 | Grad Norm: 0.00983088\n",
      "Epoch 5 | Step 3435500 | Avg Loss: 0.0154 | Grad Norm: 0.00888999\n",
      "Epoch 5 | Step 3435600 | Avg Loss: 0.0152 | Grad Norm: 0.00821018\n",
      "Epoch 5 | Step 3435700 | Avg Loss: 0.0152 | Grad Norm: 0.00804567\n",
      "Epoch 5 | Step 3435800 | Avg Loss: 0.0151 | Grad Norm: 0.00949056\n",
      "Epoch 5 | Step 3435900 | Avg Loss: 0.0152 | Grad Norm: 0.01026597\n",
      "Epoch 5 | Step 3436000 | Avg Loss: 0.0152 | Grad Norm: 0.01409190\n",
      "Epoch 5 | Step 3436100 | Avg Loss: 0.0153 | Grad Norm: 0.00866710\n",
      "Epoch 5 | Step 3436200 | Avg Loss: 0.0155 | Grad Norm: 0.00823087\n",
      "Epoch 5 | Step 3436300 | Avg Loss: 0.0161 | Grad Norm: 0.00948391\n",
      "Epoch 5 | Step 3436400 | Avg Loss: 0.0156 | Grad Norm: 0.00915846\n",
      "Epoch 5 | Step 3436500 | Avg Loss: 0.0154 | Grad Norm: 0.01131926\n",
      "Epoch 5 | Step 3436600 | Avg Loss: 0.0156 | Grad Norm: 0.00812063\n",
      "Epoch 5 | Step 3436700 | Avg Loss: 0.0154 | Grad Norm: 0.00992329\n",
      "Epoch 5 | Step 3436800 | Avg Loss: 0.0150 | Grad Norm: 0.00906691\n",
      "Epoch 5 | Step 3436900 | Avg Loss: 0.0151 | Grad Norm: 0.00976225\n",
      "Epoch 5 | Step 3437000 | Avg Loss: 0.0151 | Grad Norm: 0.01001899\n",
      "Epoch 5 | Step 3437100 | Avg Loss: 0.0155 | Grad Norm: 0.00951548\n",
      "Epoch 5 | Step 3437200 | Avg Loss: 0.0157 | Grad Norm: 0.00904368\n",
      "Epoch 5 | Step 3437300 | Avg Loss: 0.0157 | Grad Norm: 0.00905529\n",
      "Epoch 5 | Step 3437400 | Avg Loss: 0.0156 | Grad Norm: 0.00901598\n",
      "Epoch 5 | Step 3437500 | Avg Loss: 0.0158 | Grad Norm: 0.01102203\n",
      "Epoch 5 | Step 3437600 | Avg Loss: 0.0157 | Grad Norm: 0.01187137\n",
      "Epoch 5 | Step 3437700 | Avg Loss: 0.0157 | Grad Norm: 0.01006600\n",
      "Epoch 5 | Step 3437800 | Avg Loss: 0.0159 | Grad Norm: 0.00842300\n",
      "Epoch 5 | Step 3437900 | Avg Loss: 0.0156 | Grad Norm: 0.00983831\n",
      "Epoch 5 | Step 3438000 | Avg Loss: 0.0155 | Grad Norm: 0.00815520\n",
      "Epoch 5 | Step 3438100 | Avg Loss: 0.0152 | Grad Norm: 0.00974812\n",
      "Epoch 5 | Step 3438200 | Avg Loss: 0.0151 | Grad Norm: 0.00878404\n",
      "Epoch 5 | Step 3438300 | Avg Loss: 0.0152 | Grad Norm: 0.01098673\n",
      "Epoch 5 | Step 3438400 | Avg Loss: 0.0150 | Grad Norm: 0.00767290\n",
      "Epoch 5 | Step 3438500 | Avg Loss: 0.0150 | Grad Norm: 0.00777353\n",
      "Epoch 5 | Step 3438600 | Avg Loss: 0.0148 | Grad Norm: 0.00829821\n",
      "Epoch 5 | Step 3438700 | Avg Loss: 0.0150 | Grad Norm: 0.00787198\n",
      "Epoch 5 | Step 3438800 | Avg Loss: 0.0150 | Grad Norm: 0.01012322\n",
      "Epoch 5 | Step 3438900 | Avg Loss: 0.0150 | Grad Norm: 0.01166167\n",
      "Epoch 5 | Step 3439000 | Avg Loss: 0.0152 | Grad Norm: 0.00891057\n",
      "Epoch 5 | Step 3439100 | Avg Loss: 0.0150 | Grad Norm: 0.00888584\n",
      "Epoch 5 | Step 3439200 | Avg Loss: 0.0149 | Grad Norm: 0.01135739\n",
      "Epoch 5 | Step 3439300 | Avg Loss: 0.0146 | Grad Norm: 0.00895601\n",
      "Epoch 5 | Step 3439400 | Avg Loss: 0.0148 | Grad Norm: 0.01120588\n",
      "Epoch 5 | Step 3439500 | Avg Loss: 0.0151 | Grad Norm: 0.00834828\n",
      "Epoch 5 | Step 3439600 | Avg Loss: 0.0154 | Grad Norm: 0.00874339\n",
      "Epoch 5 | Step 3439700 | Avg Loss: 0.0153 | Grad Norm: 0.01071515\n",
      "Epoch 5 | Step 3439800 | Avg Loss: 0.0157 | Grad Norm: 0.00920687\n",
      "Epoch 5 | Step 3439900 | Avg Loss: 0.0160 | Grad Norm: 0.01192682\n",
      "Epoch 5 | Step 3440000 | Avg Loss: 0.0159 | Grad Norm: 0.00855003\n",
      "Epoch 5 | Step 3440100 | Avg Loss: 0.0157 | Grad Norm: 0.00977278\n",
      "Epoch 5 | Step 3440200 | Avg Loss: 0.0159 | Grad Norm: 0.00953478\n",
      "Epoch 5 | Step 3440300 | Avg Loss: 0.0156 | Grad Norm: 0.01006703\n",
      "Epoch 5 | Step 3440400 | Avg Loss: 0.0157 | Grad Norm: 0.00943670\n",
      "Epoch 5 | Step 3440500 | Avg Loss: 0.0158 | Grad Norm: 0.00944920\n",
      "Epoch 5 | Step 3440600 | Avg Loss: 0.0155 | Grad Norm: 0.00874738\n",
      "Epoch 5 | Step 3440700 | Avg Loss: 0.0159 | Grad Norm: 0.00886963\n",
      "Epoch 5 | Step 3440800 | Avg Loss: 0.0159 | Grad Norm: 0.00942779\n",
      "Epoch 5 | Step 3440900 | Avg Loss: 0.0159 | Grad Norm: 0.00787083\n",
      "Epoch 5 | Step 3441000 | Avg Loss: 0.0159 | Grad Norm: 0.00979908\n",
      "Epoch 5 | Step 3441100 | Avg Loss: 0.0157 | Grad Norm: 0.00965966\n",
      "Epoch 5 | Step 3441200 | Avg Loss: 0.0158 | Grad Norm: 0.00938855\n",
      "Epoch 5 | Step 3441300 | Avg Loss: 0.0154 | Grad Norm: 0.00963796\n",
      "Epoch 5 | Step 3441400 | Avg Loss: 0.0153 | Grad Norm: 0.00822439\n",
      "Epoch 5 | Step 3441500 | Avg Loss: 0.0154 | Grad Norm: 0.00771184\n",
      "Epoch 5 | Step 3441600 | Avg Loss: 0.0155 | Grad Norm: 0.00854311\n",
      "Epoch 5 | Step 3441700 | Avg Loss: 0.0156 | Grad Norm: 0.01003257\n",
      "Epoch 5 | Step 3441800 | Avg Loss: 0.0158 | Grad Norm: 0.00912558\n",
      "Epoch 5 | Step 3441900 | Avg Loss: 0.0155 | Grad Norm: 0.00968853\n",
      "Epoch 5 | Step 3442000 | Avg Loss: 0.0155 | Grad Norm: 0.00851786\n",
      "Epoch 5 | Step 3442100 | Avg Loss: 0.0160 | Grad Norm: 0.00845213\n",
      "Epoch 5 | Step 3442200 | Avg Loss: 0.0163 | Grad Norm: 0.00861275\n",
      "Epoch 5 | Step 3442300 | Avg Loss: 0.0161 | Grad Norm: 0.00905338\n",
      "Epoch 5 | Step 3442400 | Avg Loss: 0.0161 | Grad Norm: 0.01031757\n",
      "Epoch 5 | Step 3442500 | Avg Loss: 0.0159 | Grad Norm: 0.00848410\n",
      "Epoch 5 | Step 3442600 | Avg Loss: 0.0159 | Grad Norm: 0.00860982\n",
      "Epoch 5 | Step 3442700 | Avg Loss: 0.0156 | Grad Norm: 0.00865001\n",
      "Epoch 5 | Step 3442800 | Avg Loss: 0.0155 | Grad Norm: 0.00851462\n",
      "Epoch 5 | Step 3442900 | Avg Loss: 0.0153 | Grad Norm: 0.00758405\n",
      "Epoch 5 | Step 3443000 | Avg Loss: 0.0155 | Grad Norm: 0.00854060\n",
      "Epoch 5 | Step 3443100 | Avg Loss: 0.0155 | Grad Norm: 0.00825550\n",
      "Epoch 5 | Step 3443200 | Avg Loss: 0.0156 | Grad Norm: 0.00895437\n",
      "Epoch 5 | Step 3443300 | Avg Loss: 0.0150 | Grad Norm: 0.00938669\n",
      "Epoch 5 | Step 3443400 | Avg Loss: 0.0152 | Grad Norm: 0.00929192\n",
      "Epoch 5 | Step 3443500 | Avg Loss: 0.0150 | Grad Norm: 0.00862908\n",
      "Epoch 5 | Step 3443600 | Avg Loss: 0.0152 | Grad Norm: 0.00963917\n",
      "Epoch 5 | Step 3443700 | Avg Loss: 0.0155 | Grad Norm: 0.00909790\n",
      "Epoch 5 | Step 3443800 | Avg Loss: 0.0152 | Grad Norm: 0.00865771\n",
      "Epoch 5 | Step 3443900 | Avg Loss: 0.0159 | Grad Norm: 0.01183498\n",
      "Epoch 5 | Step 3444000 | Avg Loss: 0.0159 | Grad Norm: 0.00931861\n",
      "Epoch 5 | Step 3444100 | Avg Loss: 0.0158 | Grad Norm: 0.00948312\n",
      "Epoch 5 | Step 3444200 | Avg Loss: 0.0157 | Grad Norm: 0.01127064\n",
      "Epoch 5 | Step 3444300 | Avg Loss: 0.0156 | Grad Norm: 0.01000956\n",
      "Epoch 5 | Step 3444400 | Avg Loss: 0.0157 | Grad Norm: 0.00903721\n",
      "Epoch 5 | Step 3444500 | Avg Loss: 0.0157 | Grad Norm: 0.00990696\n",
      "Epoch 5 | Step 3444600 | Avg Loss: 0.0158 | Grad Norm: 0.00898533\n",
      "Epoch 5 | Step 3444700 | Avg Loss: 0.0161 | Grad Norm: 0.00852699\n",
      "Epoch 5 | Step 3444800 | Avg Loss: 0.0165 | Grad Norm: 0.01002936\n",
      "Epoch 5 | Step 3444900 | Avg Loss: 0.0162 | Grad Norm: 0.00929378\n",
      "Epoch 5 | Step 3445000 | Avg Loss: 0.0160 | Grad Norm: 0.00858746\n",
      "Epoch 5 | Step 3445100 | Avg Loss: 0.0158 | Grad Norm: 0.00912098\n",
      "Epoch 5 | Step 3445200 | Avg Loss: 0.0156 | Grad Norm: 0.00828176\n",
      "Epoch 5 | Step 3445300 | Avg Loss: 0.0159 | Grad Norm: 0.00764780\n",
      "Epoch 5 | Step 3445400 | Avg Loss: 0.0158 | Grad Norm: 0.00974196\n",
      "Epoch 5 | Step 3445500 | Avg Loss: 0.0158 | Grad Norm: 0.01025019\n",
      "Epoch 5 | Step 3445600 | Avg Loss: 0.0161 | Grad Norm: 0.01015953\n",
      "Epoch 5 | Step 3445700 | Avg Loss: 0.0159 | Grad Norm: 0.00917303\n",
      "Epoch 5 | Step 3445800 | Avg Loss: 0.0154 | Grad Norm: 0.00917451\n",
      "Epoch 5 | Step 3445900 | Avg Loss: 0.0158 | Grad Norm: 0.01018645\n",
      "Epoch 5 | Step 3446000 | Avg Loss: 0.0154 | Grad Norm: 0.00901201\n",
      "Epoch 5 | Step 3446100 | Avg Loss: 0.0157 | Grad Norm: 0.00857707\n",
      "Epoch 5 | Step 3446200 | Avg Loss: 0.0156 | Grad Norm: 0.00958950\n",
      "Epoch 5 | Step 3446300 | Avg Loss: 0.0154 | Grad Norm: 0.00823226\n",
      "Epoch 5 | Step 3446400 | Avg Loss: 0.0155 | Grad Norm: 0.00856851\n",
      "Epoch 5 | Step 3446500 | Avg Loss: 0.0156 | Grad Norm: 0.00835914\n",
      "Epoch 5 | Step 3446600 | Avg Loss: 0.0152 | Grad Norm: 0.00981828\n",
      "Epoch 5 | Step 3446700 | Avg Loss: 0.0153 | Grad Norm: 0.00997860\n",
      "Epoch 5 | Step 3446800 | Avg Loss: 0.0156 | Grad Norm: 0.00835040\n",
      "Epoch 5 | Step 3446900 | Avg Loss: 0.0158 | Grad Norm: 0.01022070\n",
      "Epoch 5 | Step 3447000 | Avg Loss: 0.0157 | Grad Norm: 0.00973322\n",
      "Epoch 5 | Step 3447100 | Avg Loss: 0.0153 | Grad Norm: 0.00931113\n",
      "Epoch 5 | Step 3447200 | Avg Loss: 0.0153 | Grad Norm: 0.00926056\n",
      "Epoch 5 | Step 3447300 | Avg Loss: 0.0153 | Grad Norm: 0.00840051\n",
      "Epoch 5 | Step 3447400 | Avg Loss: 0.0156 | Grad Norm: 0.00976505\n",
      "Epoch 5 | Step 3447500 | Avg Loss: 0.0154 | Grad Norm: 0.01263871\n",
      "Epoch 5 | Step 3447600 | Avg Loss: 0.0154 | Grad Norm: 0.00959645\n",
      "Epoch 5 | Step 3447700 | Avg Loss: 0.0151 | Grad Norm: 0.00758402\n",
      "Epoch 5 | Step 3447800 | Avg Loss: 0.0152 | Grad Norm: 0.00899068\n",
      "Epoch 5 | Step 3447900 | Avg Loss: 0.0156 | Grad Norm: 0.01084722\n",
      "Epoch 5 | Step 3448000 | Avg Loss: 0.0155 | Grad Norm: 0.01003515\n",
      "Epoch 5 | Step 3448100 | Avg Loss: 0.0154 | Grad Norm: 0.00892245\n",
      "Epoch 5 | Step 3448200 | Avg Loss: 0.0153 | Grad Norm: 0.00856102\n",
      "Epoch 5 | Step 3448300 | Avg Loss: 0.0155 | Grad Norm: 0.01014645\n",
      "Epoch 5 | Step 3448400 | Avg Loss: 0.0155 | Grad Norm: 0.01007397\n",
      "Epoch 5 | Step 3448500 | Avg Loss: 0.0155 | Grad Norm: 0.00866484\n",
      "Epoch 5 | Step 3448600 | Avg Loss: 0.0159 | Grad Norm: 0.01032688\n",
      "Epoch 5 | Step 3448700 | Avg Loss: 0.0161 | Grad Norm: 0.00965085\n",
      "Epoch 5 | Step 3448800 | Avg Loss: 0.0154 | Grad Norm: 0.00852329\n",
      "Epoch 5 | Step 3448900 | Avg Loss: 0.0153 | Grad Norm: 0.00892473\n",
      "Epoch 5 | Step 3449000 | Avg Loss: 0.0150 | Grad Norm: 0.00815548\n",
      "Epoch 5 | Step 3449100 | Avg Loss: 0.0151 | Grad Norm: 0.00922166\n",
      "Epoch 5 | Step 3449200 | Avg Loss: 0.0153 | Grad Norm: 0.00976532\n",
      "Epoch 5 | Step 3449300 | Avg Loss: 0.0151 | Grad Norm: 0.00849463\n",
      "Epoch 5 | Step 3449400 | Avg Loss: 0.0152 | Grad Norm: 0.00954881\n",
      "Epoch 5 | Step 3449500 | Avg Loss: 0.0153 | Grad Norm: 0.00823493\n",
      "Epoch 5 | Step 3449600 | Avg Loss: 0.0153 | Grad Norm: 0.01011807\n",
      "Epoch 5 | Step 3449700 | Avg Loss: 0.0155 | Grad Norm: 0.01018564\n",
      "Epoch 5 | Step 3449800 | Avg Loss: 0.0155 | Grad Norm: 0.01021695\n",
      "Epoch 5 | Step 3449900 | Avg Loss: 0.0161 | Grad Norm: 0.01069075\n",
      "Epoch 5 | Step 3450000 | Avg Loss: 0.0161 | Grad Norm: 0.00988851\n",
      "Epoch 5 | Step 3450100 | Avg Loss: 0.0159 | Grad Norm: 0.00958056\n",
      "Epoch 5 | Step 3450200 | Avg Loss: 0.0153 | Grad Norm: 0.00899561\n",
      "Epoch 5 | Step 3450300 | Avg Loss: 0.0151 | Grad Norm: 0.01003226\n",
      "Epoch 5 | Step 3450400 | Avg Loss: 0.0152 | Grad Norm: 0.00914604\n",
      "Epoch 5 | Step 3450500 | Avg Loss: 0.0152 | Grad Norm: 0.00887526\n",
      "Epoch 5 | Step 3450600 | Avg Loss: 0.0155 | Grad Norm: 0.00794436\n",
      "Epoch 5 | Step 3450700 | Avg Loss: 0.0154 | Grad Norm: 0.00979229\n",
      "Epoch 5 | Step 3450800 | Avg Loss: 0.0153 | Grad Norm: 0.00938901\n",
      "Epoch 5 | Step 3450900 | Avg Loss: 0.0152 | Grad Norm: 0.01119989\n",
      "Epoch 5 | Step 3451000 | Avg Loss: 0.0154 | Grad Norm: 0.00860110\n",
      "Epoch 5 | Step 3451100 | Avg Loss: 0.0150 | Grad Norm: 0.00945613\n",
      "Epoch 5 | Step 3451200 | Avg Loss: 0.0148 | Grad Norm: 0.01037386\n",
      "Epoch 5 | Step 3451300 | Avg Loss: 0.0144 | Grad Norm: 0.00820044\n",
      "Epoch 5 | Step 3451400 | Avg Loss: 0.0146 | Grad Norm: 0.01010928\n",
      "Epoch 5 | Step 3451500 | Avg Loss: 0.0149 | Grad Norm: 0.01006863\n",
      "Epoch 5 | Step 3451600 | Avg Loss: 0.0150 | Grad Norm: 0.00866713\n",
      "Epoch 5 | Step 3451700 | Avg Loss: 0.0153 | Grad Norm: 0.01035090\n",
      "Epoch 5 | Step 3451800 | Avg Loss: 0.0152 | Grad Norm: 0.00858070\n",
      "Epoch 5 | Step 3451900 | Avg Loss: 0.0154 | Grad Norm: 0.00870406\n",
      "Epoch 5 | Step 3452000 | Avg Loss: 0.0156 | Grad Norm: 0.00961003\n",
      "Epoch 5 | Step 3452100 | Avg Loss: 0.0156 | Grad Norm: 0.00842687\n",
      "Epoch 5 | Step 3452200 | Avg Loss: 0.0160 | Grad Norm: 0.01005930\n",
      "Epoch 5 | Step 3452300 | Avg Loss: 0.0156 | Grad Norm: 0.00909823\n",
      "Epoch 5 | Step 3452400 | Avg Loss: 0.0158 | Grad Norm: 0.01633683\n",
      "Epoch 5 | Step 3452500 | Avg Loss: 0.0156 | Grad Norm: 0.00923233\n",
      "Epoch 5 | Step 3452600 | Avg Loss: 0.0154 | Grad Norm: 0.01226194\n",
      "Epoch 5 | Step 3452700 | Avg Loss: 0.0153 | Grad Norm: 0.00806310\n",
      "Epoch 5 | Step 3452800 | Avg Loss: 0.0149 | Grad Norm: 0.00782847\n",
      "Epoch 5 | Step 3452900 | Avg Loss: 0.0149 | Grad Norm: 0.00924768\n",
      "Epoch 5 | Step 3453000 | Avg Loss: 0.0147 | Grad Norm: 0.01036799\n",
      "Epoch 5 | Step 3453100 | Avg Loss: 0.0148 | Grad Norm: 0.01239969\n",
      "Epoch 5 | Step 3453200 | Avg Loss: 0.0149 | Grad Norm: 0.01160562\n",
      "Epoch 5 | Step 3453300 | Avg Loss: 0.0149 | Grad Norm: 0.00820617\n",
      "Epoch 5 | Step 3453400 | Avg Loss: 0.0150 | Grad Norm: 0.00913984\n",
      "Epoch 5 | Step 3453500 | Avg Loss: 0.0151 | Grad Norm: 0.00913680\n",
      "Epoch 5 | Step 3453600 | Avg Loss: 0.0152 | Grad Norm: 0.00847684\n",
      "Epoch 5 | Step 3453700 | Avg Loss: 0.0153 | Grad Norm: 0.01031196\n",
      "Epoch 5 | Step 3453800 | Avg Loss: 0.0157 | Grad Norm: 0.00884639\n",
      "Epoch 5 | Step 3453900 | Avg Loss: 0.0157 | Grad Norm: 0.00939062\n",
      "Epoch 5 | Step 3454000 | Avg Loss: 0.0156 | Grad Norm: 0.00975721\n",
      "Epoch 5 | Step 3454100 | Avg Loss: 0.0155 | Grad Norm: 0.00948015\n",
      "Epoch 5 | Step 3454200 | Avg Loss: 0.0153 | Grad Norm: 0.00907500\n",
      "Epoch 5 | Step 3454300 | Avg Loss: 0.0156 | Grad Norm: 0.00841667\n",
      "Epoch 5 | Step 3454400 | Avg Loss: 0.0152 | Grad Norm: 0.00886718\n",
      "Epoch 5 | Step 3454500 | Avg Loss: 0.0149 | Grad Norm: 0.00844390\n",
      "Epoch 5 | Step 3454600 | Avg Loss: 0.0153 | Grad Norm: 0.00855403\n",
      "Epoch 5 | Step 3454700 | Avg Loss: 0.0150 | Grad Norm: 0.00963915\n",
      "Epoch 5 | Step 3454800 | Avg Loss: 0.0152 | Grad Norm: 0.00935394\n",
      "Epoch 5 | Step 3454900 | Avg Loss: 0.0153 | Grad Norm: 0.00917715\n",
      "Epoch 5 | Step 3455000 | Avg Loss: 0.0152 | Grad Norm: 0.00894660\n",
      "Epoch 5 | Step 3455100 | Avg Loss: 0.0150 | Grad Norm: 0.01125184\n",
      "Epoch 5 | Step 3455200 | Avg Loss: 0.0153 | Grad Norm: 0.00837473\n",
      "Epoch 5 | Step 3455300 | Avg Loss: 0.0154 | Grad Norm: 0.01109844\n",
      "Epoch 5 | Step 3455400 | Avg Loss: 0.0158 | Grad Norm: 0.00882057\n",
      "Epoch 5 | Step 3455500 | Avg Loss: 0.0154 | Grad Norm: 0.01073599\n",
      "Epoch 5 | Step 3455600 | Avg Loss: 0.0156 | Grad Norm: 0.00975906\n",
      "Epoch 5 | Step 3455700 | Avg Loss: 0.0159 | Grad Norm: 0.00989906\n",
      "Epoch 5 | Step 3455800 | Avg Loss: 0.0160 | Grad Norm: 0.01041829\n",
      "Epoch 5 | Step 3455900 | Avg Loss: 0.0161 | Grad Norm: 0.00896954\n",
      "Epoch 5 | Step 3456000 | Avg Loss: 0.0160 | Grad Norm: 0.00912120\n",
      "Epoch 5 | Step 3456100 | Avg Loss: 0.0161 | Grad Norm: 0.00990651\n",
      "Epoch 5 | Step 3456200 | Avg Loss: 0.0162 | Grad Norm: 0.00947919\n",
      "Epoch 5 | Step 3456300 | Avg Loss: 0.0157 | Grad Norm: 0.00836539\n",
      "Epoch 5 | Step 3456400 | Avg Loss: 0.0156 | Grad Norm: 0.00864467\n",
      "Epoch 5 | Step 3456500 | Avg Loss: 0.0160 | Grad Norm: 0.00911498\n",
      "Epoch 5 | Step 3456600 | Avg Loss: 0.0165 | Grad Norm: 0.01004024\n",
      "Epoch 5 | Step 3456700 | Avg Loss: 0.0162 | Grad Norm: 0.01122277\n",
      "Epoch 5 | Step 3456800 | Avg Loss: 0.0161 | Grad Norm: 0.00870427\n",
      "Epoch 5 | Step 3456900 | Avg Loss: 0.0157 | Grad Norm: 0.00956016\n",
      "Epoch 5 | Step 3457000 | Avg Loss: 0.0155 | Grad Norm: 0.01005256\n",
      "Epoch 5 | Step 3457100 | Avg Loss: 0.0158 | Grad Norm: 0.00847441\n",
      "Epoch 5 | Step 3457200 | Avg Loss: 0.0157 | Grad Norm: 0.00818302\n",
      "Epoch 5 | Step 3457300 | Avg Loss: 0.0157 | Grad Norm: 0.00860082\n",
      "Epoch 5 | Step 3457400 | Avg Loss: 0.0154 | Grad Norm: 0.00813342\n",
      "Epoch 5 | Step 3457500 | Avg Loss: 0.0155 | Grad Norm: 0.00901704\n",
      "Epoch 5 | Step 3457600 | Avg Loss: 0.0156 | Grad Norm: 0.00866399\n",
      "Epoch 5 | Step 3457700 | Avg Loss: 0.0155 | Grad Norm: 0.00886419\n",
      "Epoch 5 | Step 3457800 | Avg Loss: 0.0154 | Grad Norm: 0.00868464\n",
      "Epoch 5 | Step 3457900 | Avg Loss: 0.0159 | Grad Norm: 0.01110843\n",
      "Epoch 5 | Step 3458000 | Avg Loss: 0.0159 | Grad Norm: 0.00865271\n",
      "Epoch 5 | Step 3458100 | Avg Loss: 0.0158 | Grad Norm: 0.00941119\n",
      "Epoch 5 | Step 3458200 | Avg Loss: 0.0156 | Grad Norm: 0.01078131\n",
      "Epoch 5 | Step 3458300 | Avg Loss: 0.0157 | Grad Norm: 0.00876335\n",
      "Epoch 5 | Step 3458400 | Avg Loss: 0.0159 | Grad Norm: 0.00852262\n",
      "Epoch 5 | Step 3458500 | Avg Loss: 0.0159 | Grad Norm: 0.01041288\n",
      "Epoch 5 | Step 3458600 | Avg Loss: 0.0155 | Grad Norm: 0.00786366\n",
      "Epoch 5 | Step 3458700 | Avg Loss: 0.0154 | Grad Norm: 0.00875310\n",
      "Epoch 5 | Step 3458800 | Avg Loss: 0.0155 | Grad Norm: 0.00831607\n",
      "Epoch 5 | Step 3458900 | Avg Loss: 0.0157 | Grad Norm: 0.00930755\n",
      "Epoch 5 | Step 3459000 | Avg Loss: 0.0154 | Grad Norm: 0.01028245\n",
      "Epoch 5 | Step 3459100 | Avg Loss: 0.0157 | Grad Norm: 0.00985464\n",
      "Epoch 5 | Step 3459200 | Avg Loss: 0.0159 | Grad Norm: 0.01067561\n",
      "Epoch 5 | Step 3459300 | Avg Loss: 0.0158 | Grad Norm: 0.00903522\n",
      "Epoch 5 | Step 3459400 | Avg Loss: 0.0155 | Grad Norm: 0.00897764\n",
      "Epoch 5 | Step 3459500 | Avg Loss: 0.0152 | Grad Norm: 0.00793373\n",
      "Epoch 5 | Step 3459600 | Avg Loss: 0.0153 | Grad Norm: 0.00797599\n",
      "Epoch 5 | Step 3459700 | Avg Loss: 0.0156 | Grad Norm: 0.00962412\n",
      "Epoch 5 | Step 3459800 | Avg Loss: 0.0154 | Grad Norm: 0.01184850\n",
      "Epoch 5 | Step 3459900 | Avg Loss: 0.0154 | Grad Norm: 0.00980413\n",
      "Epoch 5 | Step 3460000 | Avg Loss: 0.0158 | Grad Norm: 0.00953088\n",
      "Epoch 5 | Step 3460100 | Avg Loss: 0.0159 | Grad Norm: 0.00983719\n",
      "Epoch 5 | Step 3460200 | Avg Loss: 0.0156 | Grad Norm: 0.00995548\n",
      "Epoch 5 | Step 3460300 | Avg Loss: 0.0156 | Grad Norm: 0.01031270\n",
      "Epoch 5 | Step 3460400 | Avg Loss: 0.0154 | Grad Norm: 0.00831008\n",
      "Epoch 5 | Step 3460500 | Avg Loss: 0.0156 | Grad Norm: 0.00919268\n",
      "Epoch 5 | Step 3460600 | Avg Loss: 0.0157 | Grad Norm: 0.00892488\n",
      "Epoch 5 | Step 3460700 | Avg Loss: 0.0152 | Grad Norm: 0.01074095\n",
      "Epoch 5 | Step 3460800 | Avg Loss: 0.0155 | Grad Norm: 0.00846828\n",
      "Epoch 5 | Step 3460900 | Avg Loss: 0.0155 | Grad Norm: 0.00918981\n",
      "Epoch 5 | Step 3461000 | Avg Loss: 0.0155 | Grad Norm: 0.00925433\n",
      "Epoch 5 | Step 3461100 | Avg Loss: 0.0154 | Grad Norm: 0.00971497\n",
      "Epoch 5 | Step 3461200 | Avg Loss: 0.0155 | Grad Norm: 0.00883487\n",
      "Epoch 5 | Step 3461300 | Avg Loss: 0.0154 | Grad Norm: 0.00997715\n",
      "Epoch 5 | Step 3461400 | Avg Loss: 0.0156 | Grad Norm: 0.00979000\n",
      "Epoch 5 | Step 3461500 | Avg Loss: 0.0154 | Grad Norm: 0.01182260\n",
      "Epoch 5 | Step 3461600 | Avg Loss: 0.0151 | Grad Norm: 0.00843838\n",
      "Epoch 5 | Step 3461700 | Avg Loss: 0.0153 | Grad Norm: 0.01032319\n",
      "Epoch 5 | Step 3461800 | Avg Loss: 0.0154 | Grad Norm: 0.00824758\n",
      "Epoch 5 | Step 3461900 | Avg Loss: 0.0153 | Grad Norm: 0.00994834\n",
      "Epoch 5 | Step 3462000 | Avg Loss: 0.0154 | Grad Norm: 0.00974532\n",
      "Epoch 5 | Step 3462100 | Avg Loss: 0.0155 | Grad Norm: 0.00815441\n",
      "Epoch 5 | Step 3462200 | Avg Loss: 0.0156 | Grad Norm: 0.00931211\n",
      "Epoch 5 | Step 3462300 | Avg Loss: 0.0153 | Grad Norm: 0.00840566\n",
      "Epoch 5 | Step 3462400 | Avg Loss: 0.0151 | Grad Norm: 0.00848160\n",
      "Epoch 5 | Step 3462500 | Avg Loss: 0.0153 | Grad Norm: 0.00921481\n",
      "Epoch 5 | Step 3462600 | Avg Loss: 0.0153 | Grad Norm: 0.00879142\n",
      "Epoch 5 | Step 3462700 | Avg Loss: 0.0155 | Grad Norm: 0.01082118\n",
      "Epoch 5 | Step 3462800 | Avg Loss: 0.0153 | Grad Norm: 0.00922502\n",
      "Epoch 5 | Step 3462900 | Avg Loss: 0.0152 | Grad Norm: 0.00831665\n",
      "Epoch 5 | Step 3463000 | Avg Loss: 0.0155 | Grad Norm: 0.01008393\n",
      "Epoch 5 | Step 3463100 | Avg Loss: 0.0155 | Grad Norm: 0.00952961\n",
      "Epoch 5 | Step 3463200 | Avg Loss: 0.0155 | Grad Norm: 0.00792235\n",
      "Epoch 5 | Step 3463300 | Avg Loss: 0.0154 | Grad Norm: 0.00974909\n",
      "Epoch 5 | Step 3463400 | Avg Loss: 0.0157 | Grad Norm: 0.00963734\n",
      "Epoch 5 | Step 3463500 | Avg Loss: 0.0155 | Grad Norm: 0.01023334\n",
      "Epoch 5 | Step 3463600 | Avg Loss: 0.0155 | Grad Norm: 0.00978023\n",
      "Epoch 5 | Step 3463700 | Avg Loss: 0.0153 | Grad Norm: 0.00956842\n",
      "Epoch 5 | Step 3463800 | Avg Loss: 0.0149 | Grad Norm: 0.00899044\n",
      "Epoch 5 | Step 3463900 | Avg Loss: 0.0153 | Grad Norm: 0.00909723\n",
      "Epoch 5 | Step 3464000 | Avg Loss: 0.0157 | Grad Norm: 0.01088783\n",
      "Epoch 5 | Step 3464100 | Avg Loss: 0.0153 | Grad Norm: 0.00912290\n",
      "Epoch 5 | Step 3464200 | Avg Loss: 0.0150 | Grad Norm: 0.00803521\n",
      "Epoch 5 | Step 3464300 | Avg Loss: 0.0148 | Grad Norm: 0.00859333\n",
      "Epoch 5 | Step 3464400 | Avg Loss: 0.0151 | Grad Norm: 0.00860281\n",
      "Epoch 5 | Step 3464500 | Avg Loss: 0.0148 | Grad Norm: 0.00839508\n",
      "Epoch 5 | Step 3464600 | Avg Loss: 0.0149 | Grad Norm: 0.00801404\n",
      "Epoch 5 | Step 3464700 | Avg Loss: 0.0145 | Grad Norm: 0.01033862\n",
      "Epoch 5 | Step 3464800 | Avg Loss: 0.0145 | Grad Norm: 0.00963903\n",
      "Epoch 5 | Step 3464900 | Avg Loss: 0.0148 | Grad Norm: 0.00903573\n",
      "Epoch 5 | Step 3465000 | Avg Loss: 0.0151 | Grad Norm: 0.01000503\n",
      "Epoch 5 | Step 3465100 | Avg Loss: 0.0153 | Grad Norm: 0.00918932\n",
      "Epoch 5 | Step 3465200 | Avg Loss: 0.0153 | Grad Norm: 0.00964256\n",
      "Epoch 5 | Step 3465300 | Avg Loss: 0.0149 | Grad Norm: 0.01074016\n",
      "Epoch 5 | Step 3465400 | Avg Loss: 0.0149 | Grad Norm: 0.00844511\n",
      "Epoch 5 | Step 3465500 | Avg Loss: 0.0150 | Grad Norm: 0.00844741\n",
      "Epoch 5 | Step 3465600 | Avg Loss: 0.0151 | Grad Norm: 0.00974232\n",
      "Epoch 5 | Step 3465700 | Avg Loss: 0.0155 | Grad Norm: 0.00786874\n",
      "Epoch 5 | Step 3465800 | Avg Loss: 0.0152 | Grad Norm: 0.00865017\n",
      "Epoch 5 | Step 3465900 | Avg Loss: 0.0152 | Grad Norm: 0.01038623\n",
      "Epoch 5 | Step 3466000 | Avg Loss: 0.0154 | Grad Norm: 0.01018517\n",
      "Epoch 5 | Step 3466100 | Avg Loss: 0.0153 | Grad Norm: 0.00904841\n",
      "Epoch 5 | Step 3466200 | Avg Loss: 0.0150 | Grad Norm: 0.00813528\n",
      "Epoch 5 | Step 3466300 | Avg Loss: 0.0151 | Grad Norm: 0.01376531\n",
      "Epoch 5 | Step 3466400 | Avg Loss: 0.0150 | Grad Norm: 0.00876846\n",
      "Epoch 5 | Step 3466500 | Avg Loss: 0.0151 | Grad Norm: 0.00877762\n",
      "Epoch 5 | Step 3466600 | Avg Loss: 0.0151 | Grad Norm: 0.00895714\n",
      "Epoch 5 | Step 3466700 | Avg Loss: 0.0151 | Grad Norm: 0.00830310\n",
      "Epoch 5 | Step 3466800 | Avg Loss: 0.0154 | Grad Norm: 0.00937238\n",
      "Epoch 5 | Step 3466900 | Avg Loss: 0.0156 | Grad Norm: 0.01253141\n",
      "Epoch 5 | Step 3467000 | Avg Loss: 0.0159 | Grad Norm: 0.00919334\n",
      "Epoch 5 | Step 3467100 | Avg Loss: 0.0158 | Grad Norm: 0.00906840\n",
      "Epoch 5 | Step 3467200 | Avg Loss: 0.0157 | Grad Norm: 0.00895116\n",
      "Epoch 5 | Step 3467300 | Avg Loss: 0.0153 | Grad Norm: 0.00919618\n",
      "Epoch 5 | Step 3467400 | Avg Loss: 0.0151 | Grad Norm: 0.00966753\n",
      "Epoch 5 | Step 3467500 | Avg Loss: 0.0153 | Grad Norm: 0.00873164\n",
      "Epoch 5 | Step 3467600 | Avg Loss: 0.0155 | Grad Norm: 0.00867866\n",
      "Epoch 5 | Step 3467700 | Avg Loss: 0.0156 | Grad Norm: 0.00928966\n",
      "Epoch 5 | Step 3467800 | Avg Loss: 0.0155 | Grad Norm: 0.00939385\n",
      "Epoch 5 | Step 3467900 | Avg Loss: 0.0155 | Grad Norm: 0.00839659\n",
      "Epoch 5 | Step 3468000 | Avg Loss: 0.0152 | Grad Norm: 0.00970193\n",
      "Epoch 5 | Step 3468100 | Avg Loss: 0.0155 | Grad Norm: 0.00875228\n",
      "Epoch 5 | Step 3468200 | Avg Loss: 0.0151 | Grad Norm: 0.00808754\n",
      "Epoch 5 | Step 3468300 | Avg Loss: 0.0154 | Grad Norm: 0.00784139\n",
      "Epoch 5 | Step 3468400 | Avg Loss: 0.0156 | Grad Norm: 0.01031658\n",
      "Epoch 5 | Step 3468500 | Avg Loss: 0.0155 | Grad Norm: 0.00916460\n",
      "Epoch 5 | Step 3468600 | Avg Loss: 0.0152 | Grad Norm: 0.01031587\n",
      "Epoch 5 | Step 3468700 | Avg Loss: 0.0154 | Grad Norm: 0.01342135\n",
      "Epoch 5 | Step 3468800 | Avg Loss: 0.0155 | Grad Norm: 0.00923934\n",
      "Epoch 5 | Step 3468900 | Avg Loss: 0.0161 | Grad Norm: 0.00993319\n",
      "Epoch 5 | Step 3469000 | Avg Loss: 0.0156 | Grad Norm: 0.01013510\n",
      "Epoch 5 | Step 3469100 | Avg Loss: 0.0156 | Grad Norm: 0.00843408\n",
      "Epoch 5 | Step 3469200 | Avg Loss: 0.0160 | Grad Norm: 0.00993746\n",
      "Epoch 5 | Step 3469300 | Avg Loss: 0.0159 | Grad Norm: 0.00855947\n",
      "Epoch 5 | Step 3469400 | Avg Loss: 0.0155 | Grad Norm: 0.00841298\n",
      "Epoch 5 | Step 3469500 | Avg Loss: 0.0157 | Grad Norm: 0.00937557\n",
      "Epoch 5 | Step 3469600 | Avg Loss: 0.0159 | Grad Norm: 0.00892176\n",
      "Epoch 5 | Step 3469700 | Avg Loss: 0.0161 | Grad Norm: 0.00962782\n",
      "Epoch 5 | Step 3469800 | Avg Loss: 0.0155 | Grad Norm: 0.01015258\n",
      "Epoch 5 | Step 3469900 | Avg Loss: 0.0155 | Grad Norm: 0.00814577\n",
      "Epoch 5 | Step 3470000 | Avg Loss: 0.0152 | Grad Norm: 0.00979077\n",
      "Epoch 5 | Step 3470100 | Avg Loss: 0.0154 | Grad Norm: 0.00918690\n",
      "Epoch 5 | Step 3470200 | Avg Loss: 0.0154 | Grad Norm: 0.00843165\n",
      "Epoch 5 | Step 3470300 | Avg Loss: 0.0151 | Grad Norm: 0.00897322\n",
      "Epoch 5 | Step 3470400 | Avg Loss: 0.0152 | Grad Norm: 0.00832492\n",
      "Epoch 5 | Step 3470500 | Avg Loss: 0.0154 | Grad Norm: 0.00862346\n",
      "Epoch 5 | Step 3470600 | Avg Loss: 0.0151 | Grad Norm: 0.00977668\n",
      "Epoch 5 | Step 3470700 | Avg Loss: 0.0152 | Grad Norm: 0.00822889\n",
      "Epoch 5 | Step 3470800 | Avg Loss: 0.0155 | Grad Norm: 0.01069480\n",
      "Epoch 5 | Step 3470900 | Avg Loss: 0.0152 | Grad Norm: 0.00763872\n",
      "Epoch 5 | Step 3471000 | Avg Loss: 0.0153 | Grad Norm: 0.00779004\n",
      "Epoch 5 | Step 3471100 | Avg Loss: 0.0156 | Grad Norm: 0.00865045\n",
      "Epoch 5 | Step 3471200 | Avg Loss: 0.0156 | Grad Norm: 0.01172192\n",
      "Epoch 5 | Step 3471300 | Avg Loss: 0.0154 | Grad Norm: 0.00759433\n",
      "Epoch 5 | Step 3471400 | Avg Loss: 0.0155 | Grad Norm: 0.00892578\n",
      "Epoch 5 | Step 3471500 | Avg Loss: 0.0153 | Grad Norm: 0.00894974\n",
      "Epoch 5 | Step 3471600 | Avg Loss: 0.0153 | Grad Norm: 0.00924150\n",
      "Epoch 5 | Step 3471700 | Avg Loss: 0.0148 | Grad Norm: 0.00995379\n",
      "Epoch 5 | Step 3471800 | Avg Loss: 0.0151 | Grad Norm: 0.01048988\n",
      "Epoch 5 | Step 3471900 | Avg Loss: 0.0150 | Grad Norm: 0.00978074\n",
      "Epoch 5 | Step 3472000 | Avg Loss: 0.0153 | Grad Norm: 0.00945375\n",
      "Epoch 5 | Step 3472100 | Avg Loss: 0.0152 | Grad Norm: 0.00808001\n",
      "Epoch 5 | Step 3472200 | Avg Loss: 0.0151 | Grad Norm: 0.00832362\n",
      "Epoch 5 | Step 3472300 | Avg Loss: 0.0146 | Grad Norm: 0.00799004\n",
      "Epoch 5 | Step 3472400 | Avg Loss: 0.0149 | Grad Norm: 0.01011155\n",
      "Epoch 5 | Step 3472500 | Avg Loss: 0.0151 | Grad Norm: 0.00866095\n",
      "Epoch 5 | Step 3472600 | Avg Loss: 0.0151 | Grad Norm: 0.00999795\n",
      "Epoch 5 | Step 3472700 | Avg Loss: 0.0153 | Grad Norm: 0.00976367\n",
      "Epoch 5 | Step 3472800 | Avg Loss: 0.0155 | Grad Norm: 0.00924000\n",
      "Epoch 5 | Step 3472900 | Avg Loss: 0.0154 | Grad Norm: 0.00984947\n",
      "Epoch 5 | Step 3473000 | Avg Loss: 0.0157 | Grad Norm: 0.00845540\n",
      "Epoch 5 | Step 3473100 | Avg Loss: 0.0155 | Grad Norm: 0.00927298\n",
      "Epoch 5 | Step 3473200 | Avg Loss: 0.0157 | Grad Norm: 0.00926537\n",
      "Epoch 5 | Step 3473300 | Avg Loss: 0.0157 | Grad Norm: 0.01173200\n",
      "Epoch 5 | Step 3473400 | Avg Loss: 0.0157 | Grad Norm: 0.00840832\n",
      "Epoch 5 | Step 3473500 | Avg Loss: 0.0153 | Grad Norm: 0.00951035\n",
      "Epoch 5 | Step 3473600 | Avg Loss: 0.0151 | Grad Norm: 0.01167287\n",
      "Epoch 5 | Step 3473700 | Avg Loss: 0.0156 | Grad Norm: 0.00982307\n",
      "Epoch 5 | Step 3473800 | Avg Loss: 0.0155 | Grad Norm: 0.00827800\n",
      "Epoch 5 | Step 3473900 | Avg Loss: 0.0156 | Grad Norm: 0.00927172\n",
      "Epoch 5 | Step 3474000 | Avg Loss: 0.0154 | Grad Norm: 0.00965453\n",
      "Epoch 5 | Step 3474100 | Avg Loss: 0.0150 | Grad Norm: 0.00945068\n",
      "Epoch 5 | Step 3474200 | Avg Loss: 0.0153 | Grad Norm: 0.00974610\n",
      "Epoch 5 | Step 3474300 | Avg Loss: 0.0153 | Grad Norm: 0.00956963\n",
      "Epoch 5 | Step 3474400 | Avg Loss: 0.0155 | Grad Norm: 0.00916463\n",
      "Epoch 5 | Step 3474500 | Avg Loss: 0.0155 | Grad Norm: 0.00907930\n",
      "Epoch 5 | Step 3474600 | Avg Loss: 0.0159 | Grad Norm: 0.00889905\n",
      "Epoch 5 | Step 3474700 | Avg Loss: 0.0156 | Grad Norm: 0.00835204\n",
      "Epoch 5 | Step 3474800 | Avg Loss: 0.0156 | Grad Norm: 0.01046806\n",
      "Epoch 5 | Step 3474900 | Avg Loss: 0.0155 | Grad Norm: 0.00877626\n",
      "Epoch 5 | Step 3475000 | Avg Loss: 0.0154 | Grad Norm: 0.00992984\n",
      "Epoch 5 | Step 3475100 | Avg Loss: 0.0153 | Grad Norm: 0.00880516\n",
      "Epoch 5 | Step 3475200 | Avg Loss: 0.0155 | Grad Norm: 0.00854340\n",
      "Epoch 5 | Step 3475300 | Avg Loss: 0.0157 | Grad Norm: 0.01023744\n",
      "Epoch 5 | Step 3475400 | Avg Loss: 0.0155 | Grad Norm: 0.00875729\n",
      "Epoch 5 | Step 3475500 | Avg Loss: 0.0157 | Grad Norm: 0.00908307\n",
      "Epoch 5 | Step 3475600 | Avg Loss: 0.0156 | Grad Norm: 0.00916258\n",
      "Epoch 5 | Step 3475700 | Avg Loss: 0.0159 | Grad Norm: 0.00822356\n",
      "Epoch 5 | Step 3475800 | Avg Loss: 0.0159 | Grad Norm: 0.00866554\n",
      "Epoch 5 | Step 3475900 | Avg Loss: 0.0158 | Grad Norm: 0.01043662\n",
      "Epoch 5 | Step 3476000 | Avg Loss: 0.0161 | Grad Norm: 0.00738324\n",
      "Epoch 5 | Step 3476100 | Avg Loss: 0.0159 | Grad Norm: 0.00869798\n",
      "Epoch 5 | Step 3476200 | Avg Loss: 0.0157 | Grad Norm: 0.01009895\n",
      "Epoch 5 | Step 3476300 | Avg Loss: 0.0155 | Grad Norm: 0.00921162\n",
      "Epoch 5 | Step 3476400 | Avg Loss: 0.0155 | Grad Norm: 0.00833789\n",
      "Epoch 5 | Step 3476500 | Avg Loss: 0.0150 | Grad Norm: 0.00831679\n",
      "Epoch 5 | Step 3476600 | Avg Loss: 0.0155 | Grad Norm: 0.00878896\n",
      "Epoch 5 | Step 3476700 | Avg Loss: 0.0155 | Grad Norm: 0.00865398\n",
      "Epoch 5 | Step 3476800 | Avg Loss: 0.0161 | Grad Norm: 0.00876223\n",
      "Epoch 5 | Step 3476900 | Avg Loss: 0.0156 | Grad Norm: 0.00923331\n",
      "Epoch 5 | Step 3477000 | Avg Loss: 0.0157 | Grad Norm: 0.00936521\n",
      "Epoch 5 | Step 3477100 | Avg Loss: 0.0159 | Grad Norm: 0.01145993\n",
      "Epoch 5 | Step 3477200 | Avg Loss: 0.0157 | Grad Norm: 0.01050438\n",
      "Epoch 5 | Step 3477300 | Avg Loss: 0.0154 | Grad Norm: 0.00905745\n",
      "Epoch 5 | Step 3477400 | Avg Loss: 0.0153 | Grad Norm: 0.01772315\n",
      "Epoch 5 | Step 3477500 | Avg Loss: 0.0154 | Grad Norm: 0.00873094\n",
      "Epoch 5 | Step 3477600 | Avg Loss: 0.0157 | Grad Norm: 0.00924353\n",
      "Epoch 5 | Step 3477700 | Avg Loss: 0.0155 | Grad Norm: 0.00861276\n",
      "Epoch 5 | Step 3477800 | Avg Loss: 0.0154 | Grad Norm: 0.00947556\n",
      "Epoch 5 | Step 3477900 | Avg Loss: 0.0153 | Grad Norm: 0.00883919\n",
      "Epoch 5 | Step 3478000 | Avg Loss: 0.0153 | Grad Norm: 0.00882604\n",
      "Epoch 5 | Step 3478100 | Avg Loss: 0.0151 | Grad Norm: 0.00836481\n",
      "Epoch 5 | Step 3478200 | Avg Loss: 0.0153 | Grad Norm: 0.00904531\n",
      "Epoch 5 | Step 3478300 | Avg Loss: 0.0155 | Grad Norm: 0.00952919\n",
      "Epoch 5 | Step 3478400 | Avg Loss: 0.0155 | Grad Norm: 0.00816834\n",
      "Epoch 5 | Step 3478500 | Avg Loss: 0.0153 | Grad Norm: 0.01090393\n",
      "Epoch 5 | Step 3478600 | Avg Loss: 0.0148 | Grad Norm: 0.00888329\n",
      "Epoch 5 | Step 3478700 | Avg Loss: 0.0149 | Grad Norm: 0.01155289\n",
      "Epoch 5 | Step 3478800 | Avg Loss: 0.0155 | Grad Norm: 0.01040483\n",
      "Epoch 5 | Step 3478900 | Avg Loss: 0.0154 | Grad Norm: 0.00808761\n",
      "Epoch 5 | Step 3479000 | Avg Loss: 0.0154 | Grad Norm: 0.01172670\n",
      "Epoch 5 | Step 3479100 | Avg Loss: 0.0153 | Grad Norm: 0.00971511\n",
      "Epoch 5 | Step 3479200 | Avg Loss: 0.0154 | Grad Norm: 0.00812736\n",
      "Epoch 5 | Step 3479300 | Avg Loss: 0.0155 | Grad Norm: 0.00907667\n",
      "Epoch 5 | Step 3479400 | Avg Loss: 0.0154 | Grad Norm: 0.01143098\n",
      "Epoch 5 | Step 3479500 | Avg Loss: 0.0155 | Grad Norm: 0.00985286\n",
      "Epoch 5 | Step 3479600 | Avg Loss: 0.0156 | Grad Norm: 0.00849581\n",
      "Epoch 5 | Step 3479700 | Avg Loss: 0.0151 | Grad Norm: 0.00876177\n",
      "Epoch 5 | Step 3479800 | Avg Loss: 0.0153 | Grad Norm: 0.00910830\n",
      "Epoch 5 | Step 3479900 | Avg Loss: 0.0153 | Grad Norm: 0.00873428\n",
      "Epoch 5 | Step 3480000 | Avg Loss: 0.0155 | Grad Norm: 0.00923615\n",
      "Epoch 5 | Step 3480100 | Avg Loss: 0.0150 | Grad Norm: 0.00840372\n",
      "Epoch 5 | Step 3480200 | Avg Loss: 0.0150 | Grad Norm: 0.00836716\n",
      "Epoch 5 | Step 3480300 | Avg Loss: 0.0151 | Grad Norm: 0.00876432\n",
      "Epoch 5 | Step 3480400 | Avg Loss: 0.0152 | Grad Norm: 0.01117867\n",
      "Epoch 5 | Step 3480500 | Avg Loss: 0.0152 | Grad Norm: 0.00872317\n",
      "Epoch 5 | Step 3480600 | Avg Loss: 0.0153 | Grad Norm: 0.00858509\n",
      "Epoch 5 | Step 3480700 | Avg Loss: 0.0151 | Grad Norm: 0.00868197\n",
      "Epoch 5 | Step 3480800 | Avg Loss: 0.0149 | Grad Norm: 0.00983728\n",
      "Epoch 5 | Step 3480900 | Avg Loss: 0.0150 | Grad Norm: 0.00840390\n",
      "Epoch 5 | Step 3481000 | Avg Loss: 0.0153 | Grad Norm: 0.01039918\n",
      "Epoch 5 | Step 3481100 | Avg Loss: 0.0155 | Grad Norm: 0.00929959\n",
      "Epoch 5 | Step 3481200 | Avg Loss: 0.0152 | Grad Norm: 0.00908219\n",
      "Epoch 5 | Step 3481300 | Avg Loss: 0.0155 | Grad Norm: 0.00862884\n",
      "Epoch 5 | Step 3481400 | Avg Loss: 0.0159 | Grad Norm: 0.00796890\n",
      "Epoch 5 | Step 3481500 | Avg Loss: 0.0155 | Grad Norm: 0.00846456\n",
      "Epoch 5 | Step 3481600 | Avg Loss: 0.0154 | Grad Norm: 0.01046126\n",
      "Epoch 5 | Step 3481700 | Avg Loss: 0.0151 | Grad Norm: 0.00890249\n",
      "Epoch 5 | Step 3481800 | Avg Loss: 0.0149 | Grad Norm: 0.00830138\n",
      "Epoch 5 | Step 3481900 | Avg Loss: 0.0151 | Grad Norm: 0.00836381\n",
      "Epoch 5 | Step 3482000 | Avg Loss: 0.0145 | Grad Norm: 0.00945577\n",
      "Epoch 5 | Step 3482100 | Avg Loss: 0.0151 | Grad Norm: 0.00924157\n",
      "Epoch 5 | Step 3482200 | Avg Loss: 0.0149 | Grad Norm: 0.00881959\n",
      "Epoch 5 | Step 3482300 | Avg Loss: 0.0148 | Grad Norm: 0.00842730\n",
      "Epoch 5 | Step 3482400 | Avg Loss: 0.0150 | Grad Norm: 0.00857582\n",
      "Epoch 5 | Step 3482500 | Avg Loss: 0.0146 | Grad Norm: 0.00865826\n",
      "Epoch 5 | Step 3482600 | Avg Loss: 0.0150 | Grad Norm: 0.00978316\n",
      "Epoch 5 | Step 3482700 | Avg Loss: 0.0152 | Grad Norm: 0.01019650\n",
      "Epoch 5 | Step 3482800 | Avg Loss: 0.0151 | Grad Norm: 0.00824565\n",
      "Epoch 5 | Step 3482900 | Avg Loss: 0.0151 | Grad Norm: 0.00976260\n",
      "Epoch 5 | Step 3483000 | Avg Loss: 0.0152 | Grad Norm: 0.00909844\n",
      "Epoch 5 | Step 3483100 | Avg Loss: 0.0153 | Grad Norm: 0.00882783\n",
      "Epoch 5 | Step 3483200 | Avg Loss: 0.0147 | Grad Norm: 0.01072301\n",
      "Epoch 5 | Step 3483300 | Avg Loss: 0.0146 | Grad Norm: 0.00855597\n",
      "Epoch 5 | Step 3483400 | Avg Loss: 0.0150 | Grad Norm: 0.00849244\n",
      "Epoch 5 | Step 3483500 | Avg Loss: 0.0152 | Grad Norm: 0.00892521\n",
      "Epoch 5 | Step 3483600 | Avg Loss: 0.0157 | Grad Norm: 0.00866591\n",
      "Epoch 5 | Step 3483700 | Avg Loss: 0.0158 | Grad Norm: 0.00945678\n",
      "Epoch 5 | Step 3483800 | Avg Loss: 0.0157 | Grad Norm: 0.01064332\n",
      "Epoch 5 | Step 3483900 | Avg Loss: 0.0157 | Grad Norm: 0.00883299\n",
      "Epoch 5 | Step 3484000 | Avg Loss: 0.0155 | Grad Norm: 0.00850806\n",
      "Epoch 5 | Step 3484100 | Avg Loss: 0.0157 | Grad Norm: 0.00862654\n",
      "Epoch 5 | Step 3484200 | Avg Loss: 0.0155 | Grad Norm: 0.01022950\n",
      "Epoch 5 | Step 3484300 | Avg Loss: 0.0151 | Grad Norm: 0.00906570\n",
      "Epoch 5 | Step 3484400 | Avg Loss: 0.0152 | Grad Norm: 0.00907233\n",
      "Epoch 5 | Step 3484500 | Avg Loss: 0.0152 | Grad Norm: 0.00956893\n",
      "Epoch 5 | Step 3484600 | Avg Loss: 0.0151 | Grad Norm: 0.00928012\n",
      "Epoch 5 | Step 3484700 | Avg Loss: 0.0152 | Grad Norm: 0.00883930\n",
      "Epoch 5 | Step 3484800 | Avg Loss: 0.0156 | Grad Norm: 0.01224439\n",
      "Epoch 5 | Step 3484900 | Avg Loss: 0.0156 | Grad Norm: 0.00794102\n",
      "Epoch 5 | Step 3485000 | Avg Loss: 0.0156 | Grad Norm: 0.00811712\n",
      "Epoch 5 | Step 3485100 | Avg Loss: 0.0157 | Grad Norm: 0.00821860\n",
      "Epoch 5 | Step 3485200 | Avg Loss: 0.0156 | Grad Norm: 0.00821164\n",
      "Epoch 5 | Step 3485300 | Avg Loss: 0.0149 | Grad Norm: 0.00895414\n",
      "Epoch 5 | Step 3485400 | Avg Loss: 0.0150 | Grad Norm: 0.00724332\n",
      "Epoch 5 | Step 3485500 | Avg Loss: 0.0153 | Grad Norm: 0.01064084\n",
      "Epoch 5 | Step 3485600 | Avg Loss: 0.0154 | Grad Norm: 0.00941902\n",
      "Epoch 5 | Step 3485700 | Avg Loss: 0.0156 | Grad Norm: 0.01028994\n",
      "Epoch 5 | Step 3485800 | Avg Loss: 0.0157 | Grad Norm: 0.00951670\n",
      "Epoch 5 | Step 3485900 | Avg Loss: 0.0155 | Grad Norm: 0.00846480\n",
      "Epoch 5 | Step 3486000 | Avg Loss: 0.0151 | Grad Norm: 0.00890390\n",
      "Epoch 5 | Step 3486100 | Avg Loss: 0.0155 | Grad Norm: 0.00936020\n",
      "Epoch 5 | Step 3486200 | Avg Loss: 0.0153 | Grad Norm: 0.01065894\n",
      "Epoch 5 | Step 3486300 | Avg Loss: 0.0152 | Grad Norm: 0.00794047\n",
      "Epoch 5 | Step 3486400 | Avg Loss: 0.0156 | Grad Norm: 0.00941655\n",
      "Epoch 5 | Step 3486500 | Avg Loss: 0.0153 | Grad Norm: 0.01031276\n",
      "Epoch 5 | Step 3486600 | Avg Loss: 0.0153 | Grad Norm: 0.00876690\n",
      "Epoch 5 | Step 3486700 | Avg Loss: 0.0152 | Grad Norm: 0.00815117\n",
      "Epoch 5 | Step 3486800 | Avg Loss: 0.0153 | Grad Norm: 0.01021501\n",
      "Epoch 5 | Step 3486900 | Avg Loss: 0.0155 | Grad Norm: 0.00886827\n",
      "Epoch 5 | Step 3487000 | Avg Loss: 0.0158 | Grad Norm: 0.01260567\n",
      "Epoch 5 | Step 3487100 | Avg Loss: 0.0153 | Grad Norm: 0.01062119\n",
      "Epoch 5 | Step 3487200 | Avg Loss: 0.0150 | Grad Norm: 0.01014783\n",
      "Epoch 5 | Step 3487300 | Avg Loss: 0.0149 | Grad Norm: 0.01433016\n",
      "Epoch 5 | Step 3487400 | Avg Loss: 0.0150 | Grad Norm: 0.00878960\n",
      "Epoch 5 | Step 3487500 | Avg Loss: 0.0155 | Grad Norm: 0.00965040\n",
      "Epoch 5 | Step 3487600 | Avg Loss: 0.0154 | Grad Norm: 0.00899450\n",
      "Epoch 5 | Step 3487700 | Avg Loss: 0.0155 | Grad Norm: 0.00783140\n",
      "Epoch 5 | Step 3487800 | Avg Loss: 0.0150 | Grad Norm: 0.00951740\n",
      "Epoch 5 | Step 3487900 | Avg Loss: 0.0150 | Grad Norm: 0.00806919\n",
      "Epoch 5 | Step 3488000 | Avg Loss: 0.0150 | Grad Norm: 0.00862808\n",
      "Epoch 5 | Step 3488100 | Avg Loss: 0.0154 | Grad Norm: 0.01098784\n",
      "Epoch 5 | Step 3488200 | Avg Loss: 0.0157 | Grad Norm: 0.01213668\n",
      "Epoch 5 | Step 3488300 | Avg Loss: 0.0158 | Grad Norm: 0.00854470\n",
      "Epoch 5 | Step 3488400 | Avg Loss: 0.0156 | Grad Norm: 0.01111698\n",
      "Epoch 5 | Step 3488500 | Avg Loss: 0.0154 | Grad Norm: 0.00794247\n",
      "Epoch 5 | Step 3488600 | Avg Loss: 0.0154 | Grad Norm: 0.00799398\n",
      "Epoch 5 | Step 3488700 | Avg Loss: 0.0155 | Grad Norm: 0.00883796\n",
      "Epoch 5 | Step 3488800 | Avg Loss: 0.0153 | Grad Norm: 0.00879836\n",
      "Epoch 5 | Step 3488900 | Avg Loss: 0.0153 | Grad Norm: 0.00957996\n",
      "Epoch 5 | Step 3489000 | Avg Loss: 0.0154 | Grad Norm: 0.01009615\n",
      "Epoch 5 | Step 3489100 | Avg Loss: 0.0154 | Grad Norm: 0.00780016\n",
      "Epoch 5 | Step 3489200 | Avg Loss: 0.0151 | Grad Norm: 0.01028553\n",
      "Epoch 5 | Step 3489300 | Avg Loss: 0.0154 | Grad Norm: 0.00881596\n",
      "Epoch 5 | Step 3489400 | Avg Loss: 0.0155 | Grad Norm: 0.00900759\n",
      "Epoch 5 | Step 3489500 | Avg Loss: 0.0157 | Grad Norm: 0.00866305\n",
      "Epoch 5 | Step 3489600 | Avg Loss: 0.0156 | Grad Norm: 0.00967830\n",
      "Epoch 5 | Step 3489700 | Avg Loss: 0.0159 | Grad Norm: 0.00972425\n",
      "Epoch 5 | Step 3489800 | Avg Loss: 0.0156 | Grad Norm: 0.00874287\n",
      "Epoch 5 | Step 3489900 | Avg Loss: 0.0159 | Grad Norm: 0.00936016\n",
      "Epoch 5 | Step 3490000 | Avg Loss: 0.0154 | Grad Norm: 0.00980859\n",
      "Epoch 5 | Step 3490100 | Avg Loss: 0.0154 | Grad Norm: 0.00981732\n",
      "Epoch 5 | Step 3490200 | Avg Loss: 0.0156 | Grad Norm: 0.00872242\n",
      "Epoch 5 | Step 3490300 | Avg Loss: 0.0153 | Grad Norm: 0.00988401\n",
      "Epoch 5 | Step 3490400 | Avg Loss: 0.0155 | Grad Norm: 0.00971112\n",
      "Epoch 5 | Step 3490500 | Avg Loss: 0.0151 | Grad Norm: 0.00995182\n",
      "Epoch 5 | Step 3490600 | Avg Loss: 0.0150 | Grad Norm: 0.01066051\n",
      "Epoch 5 | Step 3490700 | Avg Loss: 0.0152 | Grad Norm: 0.00942561\n",
      "Epoch 5 | Step 3490800 | Avg Loss: 0.0153 | Grad Norm: 0.00979508\n",
      "Epoch 5 | Step 3490900 | Avg Loss: 0.0157 | Grad Norm: 0.00764001\n",
      "Epoch 5 | Step 3491000 | Avg Loss: 0.0154 | Grad Norm: 0.00878751\n",
      "Epoch 5 | Step 3491100 | Avg Loss: 0.0153 | Grad Norm: 0.00890932\n",
      "Epoch 5 | Step 3491200 | Avg Loss: 0.0152 | Grad Norm: 0.00905829\n",
      "Epoch 5 | Step 3491300 | Avg Loss: 0.0157 | Grad Norm: 0.00879042\n",
      "Epoch 5 | Step 3491400 | Avg Loss: 0.0155 | Grad Norm: 0.00916080\n",
      "Epoch 5 | Step 3491500 | Avg Loss: 0.0154 | Grad Norm: 0.01065572\n",
      "Epoch 5 | Step 3491600 | Avg Loss: 0.0154 | Grad Norm: 0.01126239\n",
      "Epoch 5 | Step 3491700 | Avg Loss: 0.0152 | Grad Norm: 0.00886214\n",
      "Epoch 5 | Step 3491800 | Avg Loss: 0.0149 | Grad Norm: 0.01036708\n",
      "Epoch 5 | Step 3491900 | Avg Loss: 0.0148 | Grad Norm: 0.00883401\n",
      "Epoch 5 | Step 3492000 | Avg Loss: 0.0152 | Grad Norm: 0.00817492\n",
      "Epoch 5 | Step 3492100 | Avg Loss: 0.0152 | Grad Norm: 0.00970284\n",
      "Epoch 5 | Step 3492200 | Avg Loss: 0.0157 | Grad Norm: 0.00863185\n",
      "Epoch 5 | Step 3492300 | Avg Loss: 0.0151 | Grad Norm: 0.00848827\n",
      "Epoch 5 | Step 3492400 | Avg Loss: 0.0151 | Grad Norm: 0.00897104\n",
      "Epoch 5 | Step 3492500 | Avg Loss: 0.0154 | Grad Norm: 0.00937861\n",
      "Epoch 5 | Step 3492600 | Avg Loss: 0.0152 | Grad Norm: 0.00828381\n",
      "Epoch 5 | Step 3492700 | Avg Loss: 0.0153 | Grad Norm: 0.01014977\n",
      "Epoch 5 | Step 3492800 | Avg Loss: 0.0151 | Grad Norm: 0.00956830\n",
      "Epoch 5 | Step 3492900 | Avg Loss: 0.0155 | Grad Norm: 0.00858520\n",
      "Epoch 5 | Step 3493000 | Avg Loss: 0.0155 | Grad Norm: 0.00993102\n",
      "Epoch 5 | Step 3493100 | Avg Loss: 0.0156 | Grad Norm: 0.00760165\n",
      "Epoch 5 | Step 3493200 | Avg Loss: 0.0154 | Grad Norm: 0.00830439\n",
      "Epoch 5 | Step 3493300 | Avg Loss: 0.0155 | Grad Norm: 0.00823312\n",
      "Epoch 5 | Step 3493400 | Avg Loss: 0.0155 | Grad Norm: 0.00854901\n",
      "Epoch 5 | Step 3493500 | Avg Loss: 0.0157 | Grad Norm: 0.00915987\n",
      "Epoch 5 | Step 3493600 | Avg Loss: 0.0157 | Grad Norm: 0.00833415\n",
      "Epoch 5 | Step 3493700 | Avg Loss: 0.0155 | Grad Norm: 0.00902150\n",
      "Epoch 5 | Step 3493800 | Avg Loss: 0.0153 | Grad Norm: 0.00778556\n",
      "Epoch 5 | Step 3493900 | Avg Loss: 0.0151 | Grad Norm: 0.00862067\n",
      "Epoch 5 | Step 3494000 | Avg Loss: 0.0153 | Grad Norm: 0.00778248\n",
      "Epoch 5 | Step 3494100 | Avg Loss: 0.0151 | Grad Norm: 0.00894768\n",
      "Epoch 5 | Step 3494200 | Avg Loss: 0.0151 | Grad Norm: 0.00858890\n",
      "Epoch 5 | Step 3494300 | Avg Loss: 0.0152 | Grad Norm: 0.00967347\n",
      "Epoch 5 | Step 3494400 | Avg Loss: 0.0158 | Grad Norm: 0.01009777\n",
      "Epoch 5 | Step 3494500 | Avg Loss: 0.0160 | Grad Norm: 0.00866483\n",
      "Epoch 5 | Step 3494600 | Avg Loss: 0.0157 | Grad Norm: 0.00866412\n",
      "Epoch 5 | Step 3494700 | Avg Loss: 0.0155 | Grad Norm: 0.00870227\n",
      "Epoch 5 | Step 3494800 | Avg Loss: 0.0153 | Grad Norm: 0.00882995\n",
      "Epoch 5 | Step 3494900 | Avg Loss: 0.0154 | Grad Norm: 0.00873826\n",
      "Epoch 5 | Step 3495000 | Avg Loss: 0.0155 | Grad Norm: 0.00894666\n",
      "Epoch 5 | Step 3495100 | Avg Loss: 0.0152 | Grad Norm: 0.00799570\n",
      "Epoch 5 | Step 3495200 | Avg Loss: 0.0152 | Grad Norm: 0.01047679\n",
      "Epoch 5 | Step 3495300 | Avg Loss: 0.0151 | Grad Norm: 0.00998616\n",
      "Epoch 5 | Step 3495400 | Avg Loss: 0.0149 | Grad Norm: 0.00885470\n",
      "Epoch 5 | Step 3495500 | Avg Loss: 0.0147 | Grad Norm: 0.01043907\n",
      "Epoch 5 | Step 3495600 | Avg Loss: 0.0145 | Grad Norm: 0.00884913\n",
      "Epoch 5 | Step 3495700 | Avg Loss: 0.0144 | Grad Norm: 0.00848605\n",
      "Epoch 5 | Step 3495800 | Avg Loss: 0.0147 | Grad Norm: 0.01486730\n",
      "Epoch 5 | Step 3495900 | Avg Loss: 0.0148 | Grad Norm: 0.00835274\n",
      "Epoch 5 | Step 3496000 | Avg Loss: 0.0150 | Grad Norm: 0.01055385\n",
      "Epoch 5 | Step 3496100 | Avg Loss: 0.0153 | Grad Norm: 0.01060193\n",
      "Epoch 5 | Step 3496200 | Avg Loss: 0.0150 | Grad Norm: 0.01097589\n",
      "Epoch 5 | Step 3496300 | Avg Loss: 0.0152 | Grad Norm: 0.00841185\n",
      "Epoch 5 | Step 3496400 | Avg Loss: 0.0153 | Grad Norm: 0.00889538\n",
      "Epoch 5 | Step 3496500 | Avg Loss: 0.0152 | Grad Norm: 0.00907692\n",
      "Epoch 5 | Step 3496600 | Avg Loss: 0.0152 | Grad Norm: 0.00923824\n",
      "Epoch 5 | Step 3496700 | Avg Loss: 0.0152 | Grad Norm: 0.00800064\n",
      "Epoch 5 | Step 3496800 | Avg Loss: 0.0151 | Grad Norm: 0.00981082\n",
      "Epoch 5 | Step 3496900 | Avg Loss: 0.0149 | Grad Norm: 0.00960023\n",
      "Epoch 5 | Step 3497000 | Avg Loss: 0.0146 | Grad Norm: 0.00778651\n",
      "Epoch 5 | Step 3497100 | Avg Loss: 0.0146 | Grad Norm: 0.00830572\n",
      "Epoch 5 | Step 3497200 | Avg Loss: 0.0151 | Grad Norm: 0.00810524\n",
      "Epoch 5 | Step 3497300 | Avg Loss: 0.0154 | Grad Norm: 0.00884768\n",
      "Epoch 5 | Step 3497400 | Avg Loss: 0.0156 | Grad Norm: 0.00906371\n",
      "Epoch 5 | Step 3497500 | Avg Loss: 0.0156 | Grad Norm: 0.00932113\n",
      "Epoch 5 | Step 3497600 | Avg Loss: 0.0151 | Grad Norm: 0.00896443\n",
      "Epoch 5 | Step 3497700 | Avg Loss: 0.0152 | Grad Norm: 0.00960902\n",
      "Epoch 5 | Step 3497800 | Avg Loss: 0.0150 | Grad Norm: 0.00883648\n",
      "Epoch 5 | Step 3497900 | Avg Loss: 0.0149 | Grad Norm: 0.00910704\n",
      "Epoch 5 | Step 3498000 | Avg Loss: 0.0149 | Grad Norm: 0.00835225\n",
      "Epoch 5 | Step 3498100 | Avg Loss: 0.0154 | Grad Norm: 0.00902656\n",
      "Epoch 5 | Step 3498200 | Avg Loss: 0.0156 | Grad Norm: 0.00899264\n",
      "Epoch 5 | Step 3498300 | Avg Loss: 0.0154 | Grad Norm: 0.00929224\n",
      "Epoch 5 | Step 3498400 | Avg Loss: 0.0155 | Grad Norm: 0.00905669\n",
      "Epoch 5 | Step 3498500 | Avg Loss: 0.0153 | Grad Norm: 0.01065661\n",
      "Epoch 5 | Step 3498600 | Avg Loss: 0.0153 | Grad Norm: 0.01190003\n",
      "Epoch 5 | Step 3498700 | Avg Loss: 0.0151 | Grad Norm: 0.00887174\n",
      "Epoch 5 | Step 3498800 | Avg Loss: 0.0154 | Grad Norm: 0.00927299\n",
      "Epoch 5 | Step 3498900 | Avg Loss: 0.0154 | Grad Norm: 0.01331307\n",
      "Epoch 5 | Step 3499000 | Avg Loss: 0.0152 | Grad Norm: 0.00866011\n",
      "Epoch 5 | Step 3499100 | Avg Loss: 0.0151 | Grad Norm: 0.00933799\n",
      "Epoch 5 | Step 3499200 | Avg Loss: 0.0151 | Grad Norm: 0.00866308\n",
      "Epoch 5 | Step 3499300 | Avg Loss: 0.0151 | Grad Norm: 0.01603080\n",
      "Epoch 5 | Step 3499400 | Avg Loss: 0.0149 | Grad Norm: 0.00797745\n",
      "Epoch 5 | Step 3499500 | Avg Loss: 0.0150 | Grad Norm: 0.00965262\n",
      "Epoch 5 | Step 3499600 | Avg Loss: 0.0150 | Grad Norm: 0.00915641\n",
      "Epoch 5 | Step 3499700 | Avg Loss: 0.0156 | Grad Norm: 0.00959735\n",
      "Epoch 5 | Step 3499800 | Avg Loss: 0.0155 | Grad Norm: 0.00815511\n",
      "Epoch 5 | Step 3499900 | Avg Loss: 0.0151 | Grad Norm: 0.01026789\n",
      "Epoch 5 | Step 3500000 | Avg Loss: 0.0153 | Grad Norm: 0.00930894\n",
      "Saving model at step3500000\n",
      "Epoch 5 | Step 3500100 | Avg Loss: 0.0152 | Grad Norm: 0.00998313\n",
      "Epoch 5 | Step 3500200 | Avg Loss: 0.0151 | Grad Norm: 0.00853094\n",
      "Epoch 5 | Step 3500300 | Avg Loss: 0.0155 | Grad Norm: 0.01106019\n",
      "Epoch 5 | Step 3500400 | Avg Loss: 0.0152 | Grad Norm: 0.00821621\n",
      "Epoch 5 | Step 3500500 | Avg Loss: 0.0149 | Grad Norm: 0.00894757\n",
      "Epoch 5 | Step 3500600 | Avg Loss: 0.0151 | Grad Norm: 0.00909962\n",
      "Epoch 5 | Step 3500700 | Avg Loss: 0.0155 | Grad Norm: 0.00951652\n",
      "Epoch 5 | Step 3500800 | Avg Loss: 0.0157 | Grad Norm: 0.00943781\n",
      "Epoch 5 | Step 3500900 | Avg Loss: 0.0159 | Grad Norm: 0.00916930\n",
      "Epoch 5 | Step 3501000 | Avg Loss: 0.0155 | Grad Norm: 0.01001977\n",
      "Epoch 5 | Step 3501100 | Avg Loss: 0.0158 | Grad Norm: 0.00878385\n",
      "Epoch 5 | Step 3501200 | Avg Loss: 0.0163 | Grad Norm: 0.00954200\n",
      "Epoch 5 | Step 3501300 | Avg Loss: 0.0164 | Grad Norm: 0.00955127\n",
      "Epoch 5 | Step 3501400 | Avg Loss: 0.0165 | Grad Norm: 0.01364004\n",
      "Epoch 5 | Step 3501500 | Avg Loss: 0.0166 | Grad Norm: 0.00934819\n",
      "Epoch 5 | Step 3501600 | Avg Loss: 0.0161 | Grad Norm: 0.00900124\n",
      "Epoch 5 | Step 3501700 | Avg Loss: 0.0162 | Grad Norm: 0.00836831\n",
      "Epoch 5 | Step 3501800 | Avg Loss: 0.0161 | Grad Norm: 0.00890773\n",
      "Epoch 5 | Step 3501900 | Avg Loss: 0.0164 | Grad Norm: 0.01088965\n",
      "Epoch 5 | Step 3502000 | Avg Loss: 0.0164 | Grad Norm: 0.01169951\n",
      "Epoch 5 | Step 3502100 | Avg Loss: 0.0162 | Grad Norm: 0.00915943\n",
      "Epoch 5 | Step 3502200 | Avg Loss: 0.0158 | Grad Norm: 0.01160353\n",
      "Epoch 5 | Step 3502300 | Avg Loss: 0.0152 | Grad Norm: 0.00939402\n",
      "Epoch 5 | Step 3502400 | Avg Loss: 0.0152 | Grad Norm: 0.00857205\n",
      "Epoch 5 | Step 3502500 | Avg Loss: 0.0153 | Grad Norm: 0.01038611\n",
      "Epoch 5 | Step 3502600 | Avg Loss: 0.0154 | Grad Norm: 0.00805691\n",
      "Epoch 5 | Step 3502700 | Avg Loss: 0.0155 | Grad Norm: 0.00924832\n",
      "Epoch 5 | Step 3502800 | Avg Loss: 0.0152 | Grad Norm: 0.00952441\n",
      "Epoch 5 | Step 3502900 | Avg Loss: 0.0156 | Grad Norm: 0.00878953\n",
      "Epoch 5 | Step 3503000 | Avg Loss: 0.0154 | Grad Norm: 0.01107404\n",
      "Epoch 5 | Step 3503100 | Avg Loss: 0.0153 | Grad Norm: 0.00886256\n",
      "Epoch 5 | Step 3503200 | Avg Loss: 0.0153 | Grad Norm: 0.00910384\n",
      "Epoch 5 | Step 3503300 | Avg Loss: 0.0156 | Grad Norm: 0.01232955\n",
      "Epoch 5 | Step 3503400 | Avg Loss: 0.0153 | Grad Norm: 0.00931702\n",
      "Epoch 5 | Step 3503500 | Avg Loss: 0.0151 | Grad Norm: 0.00947616\n",
      "Epoch 5 | Step 3503600 | Avg Loss: 0.0156 | Grad Norm: 0.00943490\n",
      "Epoch 5 | Step 3503700 | Avg Loss: 0.0158 | Grad Norm: 0.00857953\n",
      "Epoch 5 | Step 3503800 | Avg Loss: 0.0157 | Grad Norm: 0.00855355\n",
      "Epoch 5 | Step 3503900 | Avg Loss: 0.0157 | Grad Norm: 0.00854698\n",
      "Epoch 5 | Step 3504000 | Avg Loss: 0.0154 | Grad Norm: 0.00896848\n",
      "Epoch 5 | Step 3504100 | Avg Loss: 0.0158 | Grad Norm: 0.00804190\n",
      "Epoch 5 | Step 3504200 | Avg Loss: 0.0156 | Grad Norm: 0.00926349\n",
      "Epoch 5 | Step 3504300 | Avg Loss: 0.0159 | Grad Norm: 0.00852554\n",
      "Epoch 5 | Step 3504400 | Avg Loss: 0.0157 | Grad Norm: 0.01097251\n",
      "Epoch 5 | Step 3504500 | Avg Loss: 0.0158 | Grad Norm: 0.01059392\n",
      "Epoch 5 | Step 3504600 | Avg Loss: 0.0156 | Grad Norm: 0.00967272\n",
      "Epoch 5 | Step 3504700 | Avg Loss: 0.0153 | Grad Norm: 0.01051365\n",
      "Epoch 5 | Step 3504800 | Avg Loss: 0.0153 | Grad Norm: 0.00803106\n",
      "Epoch 5 | Step 3504900 | Avg Loss: 0.0152 | Grad Norm: 0.00829817\n",
      "Epoch 5 | Step 3505000 | Avg Loss: 0.0153 | Grad Norm: 0.00866347\n",
      "Epoch 5 | Step 3505100 | Avg Loss: 0.0153 | Grad Norm: 0.00955479\n",
      "Epoch 5 | Step 3505200 | Avg Loss: 0.0155 | Grad Norm: 0.00914378\n",
      "Epoch 5 | Step 3505300 | Avg Loss: 0.0157 | Grad Norm: 0.01009524\n",
      "Epoch 5 | Step 3505400 | Avg Loss: 0.0155 | Grad Norm: 0.00831200\n",
      "Epoch 5 | Step 3505500 | Avg Loss: 0.0154 | Grad Norm: 0.00833236\n",
      "Epoch 5 | Step 3505600 | Avg Loss: 0.0152 | Grad Norm: 0.01081438\n",
      "Epoch 5 | Step 3505700 | Avg Loss: 0.0157 | Grad Norm: 0.00987070\n",
      "Epoch 5 | Step 3505800 | Avg Loss: 0.0161 | Grad Norm: 0.00987028\n",
      "Epoch 5 | Step 3505900 | Avg Loss: 0.0162 | Grad Norm: 0.00953363\n",
      "Epoch 5 | Step 3506000 | Avg Loss: 0.0163 | Grad Norm: 0.00864624\n",
      "Epoch 5 | Step 3506100 | Avg Loss: 0.0158 | Grad Norm: 0.00923905\n",
      "Epoch 5 | Step 3506200 | Avg Loss: 0.0155 | Grad Norm: 0.01006710\n",
      "Epoch 5 | Step 3506300 | Avg Loss: 0.0151 | Grad Norm: 0.00917461\n",
      "Epoch 5 | Step 3506400 | Avg Loss: 0.0152 | Grad Norm: 0.00927870\n",
      "Epoch 5 | Step 3506500 | Avg Loss: 0.0154 | Grad Norm: 0.00842142\n",
      "Epoch 5 | Step 3506600 | Avg Loss: 0.0160 | Grad Norm: 0.00867464\n",
      "Epoch 5 | Step 3506700 | Avg Loss: 0.0155 | Grad Norm: 0.00845332\n",
      "Epoch 5 | Step 3506800 | Avg Loss: 0.0153 | Grad Norm: 0.00871270\n",
      "Epoch 5 | Step 3506900 | Avg Loss: 0.0153 | Grad Norm: 0.00937860\n",
      "Epoch 5 | Step 3507000 | Avg Loss: 0.0151 | Grad Norm: 0.00834116\n",
      "Epoch 5 | Step 3507100 | Avg Loss: 0.0153 | Grad Norm: 0.00884079\n",
      "Epoch 5 | Step 3507200 | Avg Loss: 0.0158 | Grad Norm: 0.01061071\n",
      "Epoch 5 | Step 3507300 | Avg Loss: 0.0160 | Grad Norm: 0.00911436\n",
      "Epoch 5 | Step 3507400 | Avg Loss: 0.0156 | Grad Norm: 0.00886459\n",
      "Epoch 5 | Step 3507500 | Avg Loss: 0.0156 | Grad Norm: 0.00978059\n",
      "Epoch 5 | Step 3507600 | Avg Loss: 0.0154 | Grad Norm: 0.00952814\n",
      "Epoch 5 | Step 3507700 | Avg Loss: 0.0153 | Grad Norm: 0.00813257\n",
      "Epoch 5 | Step 3507800 | Avg Loss: 0.0153 | Grad Norm: 0.00882865\n",
      "Epoch 5 | Step 3507900 | Avg Loss: 0.0150 | Grad Norm: 0.01148592\n",
      "Epoch 5 | Step 3508000 | Avg Loss: 0.0154 | Grad Norm: 0.00814339\n",
      "Epoch 5 | Step 3508100 | Avg Loss: 0.0155 | Grad Norm: 0.01370164\n",
      "Epoch 5 | Step 3508200 | Avg Loss: 0.0155 | Grad Norm: 0.00942725\n",
      "Epoch 5 | Step 3508300 | Avg Loss: 0.0155 | Grad Norm: 0.00938751\n",
      "Epoch 5 | Step 3508400 | Avg Loss: 0.0151 | Grad Norm: 0.00867883\n",
      "Epoch 5 | Step 3508500 | Avg Loss: 0.0151 | Grad Norm: 0.00887913\n",
      "Epoch 5 | Step 3508600 | Avg Loss: 0.0151 | Grad Norm: 0.00918944\n",
      "Epoch 5 | Step 3508700 | Avg Loss: 0.0151 | Grad Norm: 0.00937627\n",
      "Epoch 5 | Step 3508800 | Avg Loss: 0.0150 | Grad Norm: 0.00876608\n",
      "Epoch 5 | Step 3508900 | Avg Loss: 0.0148 | Grad Norm: 0.00808157\n",
      "Epoch 5 | Step 3509000 | Avg Loss: 0.0151 | Grad Norm: 0.00965400\n",
      "Epoch 5 | Step 3509100 | Avg Loss: 0.0151 | Grad Norm: 0.01160860\n",
      "Epoch 5 | Step 3509200 | Avg Loss: 0.0154 | Grad Norm: 0.00824593\n",
      "Epoch 5 | Step 3509300 | Avg Loss: 0.0152 | Grad Norm: 0.00913721\n",
      "Epoch 5 | Step 3509400 | Avg Loss: 0.0154 | Grad Norm: 0.00844020\n",
      "Epoch 5 | Step 3509500 | Avg Loss: 0.0156 | Grad Norm: 0.00981372\n",
      "Epoch 5 | Step 3509600 | Avg Loss: 0.0158 | Grad Norm: 0.00997962\n",
      "Epoch 5 | Step 3509700 | Avg Loss: 0.0157 | Grad Norm: 0.00902847\n",
      "Epoch 5 | Step 3509800 | Avg Loss: 0.0157 | Grad Norm: 0.00931774\n",
      "Epoch 5 | Step 3509900 | Avg Loss: 0.0156 | Grad Norm: 0.00820344\n",
      "Epoch 5 | Step 3510000 | Avg Loss: 0.0154 | Grad Norm: 0.00859361\n",
      "Epoch 5 | Step 3510100 | Avg Loss: 0.0153 | Grad Norm: 0.00920245\n",
      "Epoch 5 | Step 3510200 | Avg Loss: 0.0152 | Grad Norm: 0.00871663\n",
      "Epoch 5 | Step 3510300 | Avg Loss: 0.0154 | Grad Norm: 0.00946381\n",
      "Epoch 5 | Step 3510400 | Avg Loss: 0.0154 | Grad Norm: 0.01139147\n",
      "Epoch 5 | Step 3510500 | Avg Loss: 0.0153 | Grad Norm: 0.00901433\n",
      "Epoch 5 | Step 3510600 | Avg Loss: 0.0155 | Grad Norm: 0.01045583\n",
      "Epoch 5 | Step 3510700 | Avg Loss: 0.0156 | Grad Norm: 0.00866655\n",
      "Epoch 5 | Step 3510800 | Avg Loss: 0.0155 | Grad Norm: 0.00815870\n",
      "Epoch 5 | Step 3510900 | Avg Loss: 0.0155 | Grad Norm: 0.01152973\n",
      "Epoch 5 | Step 3511000 | Avg Loss: 0.0153 | Grad Norm: 0.00961235\n",
      "Epoch 5 | Step 3511100 | Avg Loss: 0.0149 | Grad Norm: 0.01025002\n",
      "Epoch 5 | Step 3511200 | Avg Loss: 0.0149 | Grad Norm: 0.00797697\n",
      "Epoch 5 | Step 3511300 | Avg Loss: 0.0148 | Grad Norm: 0.00981223\n",
      "Epoch 5 | Step 3511400 | Avg Loss: 0.0151 | Grad Norm: 0.00837232\n",
      "Epoch 5 | Step 3511500 | Avg Loss: 0.0149 | Grad Norm: 0.00963936\n",
      "Epoch 5 | Step 3511600 | Avg Loss: 0.0148 | Grad Norm: 0.00869187\n",
      "Epoch 5 | Step 3511700 | Avg Loss: 0.0150 | Grad Norm: 0.00852356\n",
      "Epoch 5 | Step 3511800 | Avg Loss: 0.0146 | Grad Norm: 0.00874265\n",
      "Epoch 5 | Step 3511900 | Avg Loss: 0.0152 | Grad Norm: 0.00994934\n",
      "Epoch 5 | Step 3512000 | Avg Loss: 0.0151 | Grad Norm: 0.01133488\n",
      "Epoch 5 | Step 3512100 | Avg Loss: 0.0149 | Grad Norm: 0.00879398\n",
      "Epoch 5 | Step 3512200 | Avg Loss: 0.0149 | Grad Norm: 0.01001487\n",
      "Epoch 5 | Step 3512300 | Avg Loss: 0.0148 | Grad Norm: 0.00866526\n",
      "Epoch 5 | Step 3512400 | Avg Loss: 0.0150 | Grad Norm: 0.00752560\n",
      "Epoch 5 | Step 3512500 | Avg Loss: 0.0150 | Grad Norm: 0.00902034\n",
      "Epoch 5 | Step 3512600 | Avg Loss: 0.0158 | Grad Norm: 0.00994531\n",
      "Epoch 5 | Step 3512700 | Avg Loss: 0.0158 | Grad Norm: 0.00984471\n",
      "Epoch 5 | Step 3512800 | Avg Loss: 0.0156 | Grad Norm: 0.00861108\n",
      "Epoch 5 | Step 3512900 | Avg Loss: 0.0156 | Grad Norm: 0.01145541\n",
      "Epoch 5 | Step 3513000 | Avg Loss: 0.0157 | Grad Norm: 0.00794307\n",
      "Epoch 5 | Step 3513100 | Avg Loss: 0.0155 | Grad Norm: 0.01031087\n",
      "Epoch 5 | Step 3513200 | Avg Loss: 0.0158 | Grad Norm: 0.01029976\n",
      "Epoch 5 | Step 3513300 | Avg Loss: 0.0157 | Grad Norm: 0.00835905\n",
      "Epoch 5 | Step 3513400 | Avg Loss: 0.0161 | Grad Norm: 0.00906860\n",
      "Epoch 5 | Step 3513500 | Avg Loss: 0.0157 | Grad Norm: 0.01068735\n",
      "Epoch 5 | Step 3513600 | Avg Loss: 0.0157 | Grad Norm: 0.00970825\n",
      "Epoch 5 | Step 3513700 | Avg Loss: 0.0159 | Grad Norm: 0.00854797\n",
      "Epoch 5 | Step 3513800 | Avg Loss: 0.0158 | Grad Norm: 0.00857129\n",
      "Epoch 5 | Step 3513900 | Avg Loss: 0.0158 | Grad Norm: 0.00833593\n",
      "Epoch 5 | Step 3514000 | Avg Loss: 0.0158 | Grad Norm: 0.00852991\n",
      "Epoch 5 | Step 3514100 | Avg Loss: 0.0157 | Grad Norm: 0.01036349\n",
      "Epoch 5 | Step 3514200 | Avg Loss: 0.0156 | Grad Norm: 0.00957365\n",
      "Epoch 5 | Step 3514300 | Avg Loss: 0.0159 | Grad Norm: 0.00852296\n",
      "Epoch 5 | Step 3514400 | Avg Loss: 0.0156 | Grad Norm: 0.00899752\n",
      "Epoch 5 | Step 3514500 | Avg Loss: 0.0158 | Grad Norm: 0.01104592\n",
      "Epoch 5 | Step 3514600 | Avg Loss: 0.0156 | Grad Norm: 0.00932268\n",
      "Epoch 5 | Step 3514700 | Avg Loss: 0.0157 | Grad Norm: 0.00992949\n",
      "Epoch 5 | Step 3514800 | Avg Loss: 0.0156 | Grad Norm: 0.00853666\n",
      "Epoch 5 | Step 3514900 | Avg Loss: 0.0156 | Grad Norm: 0.00992452\n",
      "Epoch 5 | Step 3515000 | Avg Loss: 0.0155 | Grad Norm: 0.00861695\n",
      "Epoch 5 | Step 3515100 | Avg Loss: 0.0158 | Grad Norm: 0.00868350\n",
      "Epoch 5 | Step 3515200 | Avg Loss: 0.0161 | Grad Norm: 0.01220140\n",
      "Epoch 5 | Step 3515300 | Avg Loss: 0.0161 | Grad Norm: 0.00996248\n",
      "Epoch 5 | Step 3515400 | Avg Loss: 0.0165 | Grad Norm: 0.00931007\n",
      "Epoch 5 | Step 3515500 | Avg Loss: 0.0161 | Grad Norm: 0.00943413\n",
      "Epoch 5 | Step 3515600 | Avg Loss: 0.0164 | Grad Norm: 0.00888447\n",
      "Epoch 5 | Step 3515700 | Avg Loss: 0.0159 | Grad Norm: 0.00885224\n",
      "Epoch 5 | Step 3515800 | Avg Loss: 0.0158 | Grad Norm: 0.01222774\n",
      "Epoch 5 | Step 3515900 | Avg Loss: 0.0158 | Grad Norm: 0.00861456\n",
      "Epoch 5 | Step 3516000 | Avg Loss: 0.0157 | Grad Norm: 0.00996088\n",
      "Epoch 5 | Step 3516100 | Avg Loss: 0.0159 | Grad Norm: 0.00976115\n",
      "Epoch 5 | Step 3516200 | Avg Loss: 0.0159 | Grad Norm: 0.00854752\n",
      "Epoch 5 | Step 3516300 | Avg Loss: 0.0160 | Grad Norm: 0.01045260\n",
      "Epoch 5 | Step 3516400 | Avg Loss: 0.0159 | Grad Norm: 0.01000641\n",
      "Epoch 5 | Step 3516500 | Avg Loss: 0.0160 | Grad Norm: 0.00931565\n",
      "Epoch 5 | Step 3516600 | Avg Loss: 0.0158 | Grad Norm: 0.01180414\n",
      "Epoch 5 | Step 3516700 | Avg Loss: 0.0160 | Grad Norm: 0.00949273\n",
      "Epoch 5 | Step 3516800 | Avg Loss: 0.0158 | Grad Norm: 0.00953716\n",
      "Epoch 5 | Step 3516900 | Avg Loss: 0.0157 | Grad Norm: 0.00858325\n",
      "Epoch 5 | Step 3517000 | Avg Loss: 0.0157 | Grad Norm: 0.00866113\n",
      "Epoch 5 | Step 3517100 | Avg Loss: 0.0156 | Grad Norm: 0.00815001\n",
      "Epoch 5 | Step 3517200 | Avg Loss: 0.0157 | Grad Norm: 0.01025910\n",
      "Epoch 5 | Step 3517300 | Avg Loss: 0.0156 | Grad Norm: 0.00948784\n",
      "Epoch 5 | Step 3517400 | Avg Loss: 0.0155 | Grad Norm: 0.00876900\n",
      "Epoch 5 | Step 3517500 | Avg Loss: 0.0154 | Grad Norm: 0.00891848\n",
      "Epoch 5 | Step 3517600 | Avg Loss: 0.0154 | Grad Norm: 0.00939904\n",
      "Epoch 5 | Step 3517700 | Avg Loss: 0.0154 | Grad Norm: 0.00910545\n",
      "Epoch 5 | Step 3517800 | Avg Loss: 0.0158 | Grad Norm: 0.00912103\n",
      "Epoch 5 | Step 3517900 | Avg Loss: 0.0154 | Grad Norm: 0.00812271\n",
      "Epoch 5 | Step 3518000 | Avg Loss: 0.0157 | Grad Norm: 0.00949454\n",
      "Epoch 5 | Step 3518100 | Avg Loss: 0.0158 | Grad Norm: 0.00900601\n",
      "Epoch 5 | Step 3518200 | Avg Loss: 0.0157 | Grad Norm: 0.00988941\n",
      "Epoch 5 | Step 3518300 | Avg Loss: 0.0155 | Grad Norm: 0.00897348\n",
      "Epoch 5 | Step 3518400 | Avg Loss: 0.0156 | Grad Norm: 0.00958215\n",
      "Epoch 5 | Step 3518500 | Avg Loss: 0.0158 | Grad Norm: 0.01042132\n",
      "Epoch 5 | Step 3518600 | Avg Loss: 0.0157 | Grad Norm: 0.00860398\n",
      "Epoch 5 | Step 3518700 | Avg Loss: 0.0158 | Grad Norm: 0.01173223\n",
      "Epoch 5 | Step 3518800 | Avg Loss: 0.0161 | Grad Norm: 0.00928101\n",
      "Epoch 5 | Step 3518900 | Avg Loss: 0.0161 | Grad Norm: 0.01013914\n",
      "Epoch 5 | Step 3519000 | Avg Loss: 0.0164 | Grad Norm: 0.00909750\n",
      "Epoch 5 | Step 3519100 | Avg Loss: 0.0159 | Grad Norm: 0.00888437\n",
      "Epoch 5 | Step 3519200 | Avg Loss: 0.0160 | Grad Norm: 0.00948024\n",
      "Epoch 5 | Step 3519300 | Avg Loss: 0.0161 | Grad Norm: 0.00836297\n",
      "Epoch 5 | Step 3519400 | Avg Loss: 0.0160 | Grad Norm: 0.01080272\n",
      "Epoch 5 | Step 3519500 | Avg Loss: 0.0159 | Grad Norm: 0.00794273\n",
      "Epoch 5 | Step 3519600 | Avg Loss: 0.0156 | Grad Norm: 0.00834516\n",
      "Epoch 5 | Step 3519700 | Avg Loss: 0.0157 | Grad Norm: 0.00897923\n",
      "Epoch 5 | Step 3519800 | Avg Loss: 0.0155 | Grad Norm: 0.01066126\n",
      "Epoch 5 | Step 3519900 | Avg Loss: 0.0155 | Grad Norm: 0.00839183\n",
      "Epoch 5 | Step 3520000 | Avg Loss: 0.0153 | Grad Norm: 0.00961570\n",
      "Epoch 5 | Step 3520100 | Avg Loss: 0.0152 | Grad Norm: 0.01103308\n",
      "Epoch 5 | Step 3520200 | Avg Loss: 0.0153 | Grad Norm: 0.00880158\n",
      "Epoch 5 | Step 3520300 | Avg Loss: 0.0153 | Grad Norm: 0.00969647\n",
      "Epoch 5 | Step 3520400 | Avg Loss: 0.0156 | Grad Norm: 0.00842479\n",
      "Epoch 5 | Step 3520500 | Avg Loss: 0.0155 | Grad Norm: 0.00971624\n",
      "Epoch 5 | Step 3520600 | Avg Loss: 0.0155 | Grad Norm: 0.00951620\n",
      "Epoch 5 | Step 3520700 | Avg Loss: 0.0154 | Grad Norm: 0.00880855\n",
      "Epoch 5 | Step 3520800 | Avg Loss: 0.0154 | Grad Norm: 0.00920506\n",
      "Epoch 5 | Step 3520900 | Avg Loss: 0.0156 | Grad Norm: 0.01653756\n",
      "Epoch 5 | Step 3521000 | Avg Loss: 0.0155 | Grad Norm: 0.01068830\n",
      "Epoch 5 | Step 3521100 | Avg Loss: 0.0148 | Grad Norm: 0.00873689\n",
      "Epoch 5 | Step 3521200 | Avg Loss: 0.0154 | Grad Norm: 0.00850436\n",
      "Epoch 5 | Step 3521300 | Avg Loss: 0.0155 | Grad Norm: 0.00908816\n",
      "Epoch 5 | Step 3521400 | Avg Loss: 0.0154 | Grad Norm: 0.00845187\n",
      "Epoch 5 | Step 3521500 | Avg Loss: 0.0149 | Grad Norm: 0.00826749\n",
      "Epoch 5 | Step 3521600 | Avg Loss: 0.0154 | Grad Norm: 0.00796219\n",
      "Epoch 5 | Step 3521700 | Avg Loss: 0.0149 | Grad Norm: 0.00865124\n",
      "Epoch 5 | Step 3521800 | Avg Loss: 0.0153 | Grad Norm: 0.00901188\n",
      "Epoch 5 | Step 3521900 | Avg Loss: 0.0156 | Grad Norm: 0.00929513\n",
      "Epoch 5 | Step 3522000 | Avg Loss: 0.0157 | Grad Norm: 0.01103680\n",
      "Epoch 5 | Step 3522100 | Avg Loss: 0.0153 | Grad Norm: 0.00740943\n",
      "Epoch 5 | Step 3522200 | Avg Loss: 0.0157 | Grad Norm: 0.01068714\n",
      "Epoch 5 | Step 3522300 | Avg Loss: 0.0157 | Grad Norm: 0.00960109\n",
      "Epoch 5 | Step 3522400 | Avg Loss: 0.0158 | Grad Norm: 0.00886808\n",
      "Epoch 5 | Step 3522500 | Avg Loss: 0.0157 | Grad Norm: 0.00855027\n",
      "Epoch 5 | Step 3522600 | Avg Loss: 0.0155 | Grad Norm: 0.00875959\n",
      "Epoch 5 | Step 3522700 | Avg Loss: 0.0152 | Grad Norm: 0.01022582\n",
      "Epoch 5 | Step 3522800 | Avg Loss: 0.0150 | Grad Norm: 0.00863194\n",
      "Epoch 5 | Step 3522900 | Avg Loss: 0.0152 | Grad Norm: 0.00953440\n",
      "Epoch 5 | Step 3523000 | Avg Loss: 0.0154 | Grad Norm: 0.01171325\n",
      "Epoch 5 | Step 3523100 | Avg Loss: 0.0152 | Grad Norm: 0.00808917\n",
      "Epoch 5 | Step 3523200 | Avg Loss: 0.0153 | Grad Norm: 0.00824376\n",
      "Epoch 5 | Step 3523300 | Avg Loss: 0.0152 | Grad Norm: 0.00965248\n",
      "Epoch 5 | Step 3523400 | Avg Loss: 0.0157 | Grad Norm: 0.00917480\n",
      "Epoch 5 | Step 3523500 | Avg Loss: 0.0157 | Grad Norm: 0.00955711\n",
      "Epoch 5 | Step 3523600 | Avg Loss: 0.0154 | Grad Norm: 0.00953950\n",
      "Epoch 5 | Step 3523700 | Avg Loss: 0.0157 | Grad Norm: 0.00835248\n",
      "Epoch 5 | Step 3523800 | Avg Loss: 0.0155 | Grad Norm: 0.00815593\n",
      "Epoch 5 | Step 3523900 | Avg Loss: 0.0156 | Grad Norm: 0.00939224\n",
      "Epoch 5 | Step 3524000 | Avg Loss: 0.0157 | Grad Norm: 0.00926494\n",
      "Epoch 5 | Step 3524100 | Avg Loss: 0.0153 | Grad Norm: 0.00961104\n",
      "Epoch 5 | Step 3524200 | Avg Loss: 0.0154 | Grad Norm: 0.01247779\n",
      "Epoch 5 | Step 3524300 | Avg Loss: 0.0155 | Grad Norm: 0.00966530\n",
      "Epoch 5 | Step 3524400 | Avg Loss: 0.0158 | Grad Norm: 0.00866118\n",
      "Epoch 5 | Step 3524500 | Avg Loss: 0.0160 | Grad Norm: 0.00937298\n",
      "Epoch 5 | Step 3524600 | Avg Loss: 0.0168 | Grad Norm: 0.01117665\n",
      "Epoch 5 | Step 3524700 | Avg Loss: 0.0164 | Grad Norm: 0.00917438\n",
      "Epoch 5 | Step 3524800 | Avg Loss: 0.0163 | Grad Norm: 0.00987423\n",
      "Epoch 5 | Step 3524900 | Avg Loss: 0.0162 | Grad Norm: 0.00814555\n",
      "Epoch 5 | Step 3525000 | Avg Loss: 0.0161 | Grad Norm: 0.01265199\n",
      "Epoch 5 | Step 3525100 | Avg Loss: 0.0158 | Grad Norm: 0.00862361\n",
      "Epoch 5 | Step 3525200 | Avg Loss: 0.0155 | Grad Norm: 0.00955722\n",
      "Epoch 5 | Step 3525300 | Avg Loss: 0.0157 | Grad Norm: 0.00893224\n",
      "Epoch 5 | Step 3525400 | Avg Loss: 0.0157 | Grad Norm: 0.00909121\n",
      "Epoch 5 | Step 3525500 | Avg Loss: 0.0153 | Grad Norm: 0.00865513\n",
      "Epoch 5 | Step 3525600 | Avg Loss: 0.0150 | Grad Norm: 0.00795208\n",
      "Epoch 5 | Step 3525700 | Avg Loss: 0.0153 | Grad Norm: 0.00854376\n",
      "Epoch 5 | Step 3525800 | Avg Loss: 0.0152 | Grad Norm: 0.00912880\n",
      "Epoch 5 | Step 3525900 | Avg Loss: 0.0149 | Grad Norm: 0.00920294\n",
      "Epoch 5 | Step 3526000 | Avg Loss: 0.0149 | Grad Norm: 0.00933578\n",
      "Epoch 5 | Step 3526100 | Avg Loss: 0.0149 | Grad Norm: 0.00766786\n",
      "Epoch 5 | Step 3526200 | Avg Loss: 0.0151 | Grad Norm: 0.00942954\n",
      "Epoch 5 | Step 3526300 | Avg Loss: 0.0150 | Grad Norm: 0.00844405\n",
      "Epoch 5 | Step 3526400 | Avg Loss: 0.0151 | Grad Norm: 0.01173781\n",
      "Epoch 5 | Step 3526500 | Avg Loss: 0.0152 | Grad Norm: 0.00887763\n",
      "Epoch 5 | Step 3526600 | Avg Loss: 0.0153 | Grad Norm: 0.00889620\n",
      "Epoch 5 | Step 3526700 | Avg Loss: 0.0152 | Grad Norm: 0.00855356\n",
      "Epoch 5 | Step 3526800 | Avg Loss: 0.0149 | Grad Norm: 0.00835199\n",
      "Epoch 5 | Step 3526900 | Avg Loss: 0.0149 | Grad Norm: 0.00867259\n",
      "Epoch 5 | Step 3527000 | Avg Loss: 0.0149 | Grad Norm: 0.00898335\n",
      "Epoch 5 | Step 3527100 | Avg Loss: 0.0152 | Grad Norm: 0.01003128\n",
      "Epoch 5 | Step 3527200 | Avg Loss: 0.0157 | Grad Norm: 0.00843301\n",
      "Epoch 5 | Step 3527300 | Avg Loss: 0.0153 | Grad Norm: 0.00966812\n",
      "Epoch 5 | Step 3527400 | Avg Loss: 0.0148 | Grad Norm: 0.00929914\n",
      "Epoch 5 | Step 3527500 | Avg Loss: 0.0151 | Grad Norm: 0.00807379\n",
      "Epoch 5 | Step 3527600 | Avg Loss: 0.0150 | Grad Norm: 0.00857588\n",
      "Epoch 5 | Step 3527700 | Avg Loss: 0.0152 | Grad Norm: 0.01008514\n",
      "Epoch 5 | Step 3527800 | Avg Loss: 0.0152 | Grad Norm: 0.00841372\n",
      "Epoch 5 | Step 3527900 | Avg Loss: 0.0153 | Grad Norm: 0.00819251\n",
      "Epoch 5 | Step 3528000 | Avg Loss: 0.0152 | Grad Norm: 0.00768158\n",
      "Epoch 5 | Step 3528100 | Avg Loss: 0.0151 | Grad Norm: 0.00877439\n",
      "Epoch 5 | Step 3528200 | Avg Loss: 0.0151 | Grad Norm: 0.00936985\n",
      "Epoch 5 | Step 3528300 | Avg Loss: 0.0151 | Grad Norm: 0.00933269\n",
      "Epoch 5 | Step 3528400 | Avg Loss: 0.0151 | Grad Norm: 0.00817614\n",
      "Epoch 5 | Step 3528500 | Avg Loss: 0.0155 | Grad Norm: 0.00939021\n",
      "Epoch 5 | Step 3528600 | Avg Loss: 0.0154 | Grad Norm: 0.00915589\n",
      "Epoch 5 | Step 3528700 | Avg Loss: 0.0155 | Grad Norm: 0.00856224\n",
      "Epoch 5 | Step 3528800 | Avg Loss: 0.0156 | Grad Norm: 0.00892042\n",
      "Epoch 5 | Step 3528900 | Avg Loss: 0.0156 | Grad Norm: 0.01019929\n",
      "Epoch 5 | Step 3529000 | Avg Loss: 0.0157 | Grad Norm: 0.00885406\n",
      "Epoch 5 | Step 3529100 | Avg Loss: 0.0158 | Grad Norm: 0.00859311\n",
      "Epoch 5 | Step 3529200 | Avg Loss: 0.0157 | Grad Norm: 0.00970690\n",
      "Epoch 5 | Step 3529300 | Avg Loss: 0.0154 | Grad Norm: 0.00957709\n",
      "Epoch 5 | Step 3529400 | Avg Loss: 0.0155 | Grad Norm: 0.00994113\n",
      "Epoch 5 | Step 3529500 | Avg Loss: 0.0161 | Grad Norm: 0.01053787\n",
      "Epoch 5 | Step 3529600 | Avg Loss: 0.0163 | Grad Norm: 0.00851551\n",
      "Epoch 5 | Step 3529700 | Avg Loss: 0.0163 | Grad Norm: 0.01065887\n",
      "Epoch 5 | Step 3529800 | Avg Loss: 0.0163 | Grad Norm: 0.00963082\n",
      "Epoch 5 | Step 3529900 | Avg Loss: 0.0158 | Grad Norm: 0.00853308\n",
      "Epoch 5 | Step 3530000 | Avg Loss: 0.0156 | Grad Norm: 0.00939745\n",
      "Epoch 5 | Step 3530100 | Avg Loss: 0.0156 | Grad Norm: 0.00913559\n",
      "Epoch 5 | Step 3530200 | Avg Loss: 0.0156 | Grad Norm: 0.01001667\n",
      "Epoch 5 | Step 3530300 | Avg Loss: 0.0157 | Grad Norm: 0.01032812\n",
      "Epoch 5 | Step 3530400 | Avg Loss: 0.0155 | Grad Norm: 0.01035160\n",
      "Epoch 5 | Step 3530500 | Avg Loss: 0.0156 | Grad Norm: 0.00936552\n",
      "Epoch 5 | Step 3530600 | Avg Loss: 0.0157 | Grad Norm: 0.01102511\n",
      "Epoch 5 | Step 3530700 | Avg Loss: 0.0159 | Grad Norm: 0.00872694\n",
      "Epoch 5 | Step 3530800 | Avg Loss: 0.0159 | Grad Norm: 0.00978899\n",
      "Epoch 5 | Step 3530900 | Avg Loss: 0.0159 | Grad Norm: 0.01397358\n",
      "Epoch 5 | Step 3531000 | Avg Loss: 0.0153 | Grad Norm: 0.00984369\n",
      "Epoch 5 | Step 3531100 | Avg Loss: 0.0155 | Grad Norm: 0.00809873\n",
      "Epoch 5 | Step 3531200 | Avg Loss: 0.0154 | Grad Norm: 0.00880440\n",
      "Epoch 5 | Step 3531300 | Avg Loss: 0.0154 | Grad Norm: 0.00922086\n",
      "Epoch 5 | Step 3531400 | Avg Loss: 0.0156 | Grad Norm: 0.01033723\n",
      "Epoch 5 | Step 3531500 | Avg Loss: 0.0153 | Grad Norm: 0.01110266\n",
      "Epoch 5 | Step 3531600 | Avg Loss: 0.0156 | Grad Norm: 0.01038509\n",
      "Epoch 5 | Step 3531700 | Avg Loss: 0.0158 | Grad Norm: 0.00892589\n",
      "Epoch 5 | Step 3531800 | Avg Loss: 0.0161 | Grad Norm: 0.00825557\n",
      "Epoch 5 | Step 3531900 | Avg Loss: 0.0159 | Grad Norm: 0.00871874\n",
      "Epoch 5 | Step 3532000 | Avg Loss: 0.0160 | Grad Norm: 0.01177689\n",
      "Epoch 5 | Step 3532100 | Avg Loss: 0.0157 | Grad Norm: 0.00906371\n",
      "Epoch 5 | Step 3532200 | Avg Loss: 0.0158 | Grad Norm: 0.00923602\n",
      "Epoch 5 | Step 3532300 | Avg Loss: 0.0158 | Grad Norm: 0.00817251\n",
      "Epoch 5 | Step 3532400 | Avg Loss: 0.0155 | Grad Norm: 0.00831364\n",
      "Epoch 5 | Step 3532500 | Avg Loss: 0.0156 | Grad Norm: 0.00858301\n",
      "Epoch 5 | Step 3532600 | Avg Loss: 0.0158 | Grad Norm: 0.00862707\n",
      "Epoch 5 | Step 3532700 | Avg Loss: 0.0159 | Grad Norm: 0.01106275\n",
      "Epoch 5 | Step 3532800 | Avg Loss: 0.0155 | Grad Norm: 0.00906924\n",
      "Epoch 5 | Step 3532900 | Avg Loss: 0.0157 | Grad Norm: 0.01038284\n",
      "Epoch 5 | Step 3533000 | Avg Loss: 0.0155 | Grad Norm: 0.00976596\n",
      "Epoch 5 | Step 3533100 | Avg Loss: 0.0156 | Grad Norm: 0.00956574\n",
      "Epoch 5 | Step 3533200 | Avg Loss: 0.0155 | Grad Norm: 0.00991917\n",
      "Epoch 5 | Step 3533300 | Avg Loss: 0.0154 | Grad Norm: 0.00906158\n",
      "Epoch 5 | Step 3533400 | Avg Loss: 0.0153 | Grad Norm: 0.00930808\n",
      "Epoch 5 | Step 3533500 | Avg Loss: 0.0154 | Grad Norm: 0.00924166\n",
      "Epoch 5 | Step 3533600 | Avg Loss: 0.0150 | Grad Norm: 0.01067510\n",
      "Epoch 5 | Step 3533700 | Avg Loss: 0.0154 | Grad Norm: 0.00861624\n",
      "Epoch 5 | Step 3533800 | Avg Loss: 0.0159 | Grad Norm: 0.00942724\n",
      "Epoch 5 | Step 3533900 | Avg Loss: 0.0158 | Grad Norm: 0.00947344\n",
      "Epoch 5 | Step 3534000 | Avg Loss: 0.0157 | Grad Norm: 0.00904440\n",
      "Epoch 5 | Step 3534100 | Avg Loss: 0.0153 | Grad Norm: 0.00805256\n",
      "Epoch 5 | Step 3534200 | Avg Loss: 0.0159 | Grad Norm: 0.01055436\n",
      "Epoch 5 | Step 3534300 | Avg Loss: 0.0159 | Grad Norm: 0.00975987\n",
      "Epoch 5 | Step 3534400 | Avg Loss: 0.0161 | Grad Norm: 0.00972053\n",
      "Epoch 5 | Step 3534500 | Avg Loss: 0.0161 | Grad Norm: 0.00938674\n",
      "Epoch 5 | Step 3534600 | Avg Loss: 0.0163 | Grad Norm: 0.00962278\n",
      "Epoch 5 | Step 3534700 | Avg Loss: 0.0159 | Grad Norm: 0.00899122\n",
      "Epoch 5 | Step 3534800 | Avg Loss: 0.0154 | Grad Norm: 0.01019250\n",
      "Epoch 5 | Step 3534900 | Avg Loss: 0.0150 | Grad Norm: 0.00905032\n",
      "Epoch 5 | Step 3535000 | Avg Loss: 0.0153 | Grad Norm: 0.00933295\n",
      "Epoch 5 | Step 3535100 | Avg Loss: 0.0151 | Grad Norm: 0.00950075\n",
      "Epoch 5 | Step 3535200 | Avg Loss: 0.0150 | Grad Norm: 0.00840613\n",
      "Epoch 5 | Step 3535300 | Avg Loss: 0.0153 | Grad Norm: 0.00916574\n",
      "Epoch 5 | Step 3535400 | Avg Loss: 0.0152 | Grad Norm: 0.00885467\n",
      "Epoch 5 | Step 3535500 | Avg Loss: 0.0153 | Grad Norm: 0.00916895\n",
      "Epoch 5 | Step 3535600 | Avg Loss: 0.0153 | Grad Norm: 0.00921032\n",
      "Epoch 5 | Step 3535700 | Avg Loss: 0.0151 | Grad Norm: 0.00737106\n",
      "Epoch 5 | Step 3535800 | Avg Loss: 0.0153 | Grad Norm: 0.00926188\n",
      "Epoch 5 | Step 3535900 | Avg Loss: 0.0154 | Grad Norm: 0.00816225\n",
      "Epoch 5 | Step 3536000 | Avg Loss: 0.0153 | Grad Norm: 0.00930535\n",
      "Epoch 5 | Step 3536100 | Avg Loss: 0.0152 | Grad Norm: 0.00939552\n",
      "Epoch 5 | Step 3536200 | Avg Loss: 0.0158 | Grad Norm: 0.00946988\n",
      "Epoch 5 | Step 3536300 | Avg Loss: 0.0157 | Grad Norm: 0.00892034\n",
      "Epoch 5 | Step 3536400 | Avg Loss: 0.0161 | Grad Norm: 0.01062703\n",
      "Epoch 5 | Step 3536500 | Avg Loss: 0.0159 | Grad Norm: 0.00938053\n",
      "Epoch 5 | Step 3536600 | Avg Loss: 0.0158 | Grad Norm: 0.00937626\n",
      "Epoch 5 | Step 3536700 | Avg Loss: 0.0156 | Grad Norm: 0.00841055\n",
      "Epoch 5 | Step 3536800 | Avg Loss: 0.0154 | Grad Norm: 0.00924772\n",
      "Epoch 5 | Step 3536900 | Avg Loss: 0.0154 | Grad Norm: 0.00763242\n",
      "Epoch 5 | Step 3537000 | Avg Loss: 0.0156 | Grad Norm: 0.01003196\n",
      "Epoch 5 | Step 3537100 | Avg Loss: 0.0156 | Grad Norm: 0.01015414\n",
      "Epoch 5 | Step 3537200 | Avg Loss: 0.0153 | Grad Norm: 0.01110991\n",
      "Epoch 5 | Step 3537300 | Avg Loss: 0.0155 | Grad Norm: 0.01006998\n",
      "Epoch 5 | Step 3537400 | Avg Loss: 0.0156 | Grad Norm: 0.00913359\n",
      "Epoch 5 | Step 3537500 | Avg Loss: 0.0155 | Grad Norm: 0.00918444\n",
      "Epoch 5 | Step 3537600 | Avg Loss: 0.0155 | Grad Norm: 0.00886119\n",
      "Epoch 5 | Step 3537700 | Avg Loss: 0.0152 | Grad Norm: 0.00845311\n",
      "Epoch 5 | Step 3537800 | Avg Loss: 0.0153 | Grad Norm: 0.00920105\n",
      "Epoch 5 | Step 3537900 | Avg Loss: 0.0156 | Grad Norm: 0.00831005\n",
      "Epoch 5 | Step 3538000 | Avg Loss: 0.0156 | Grad Norm: 0.00883178\n",
      "Epoch 5 | Step 3538100 | Avg Loss: 0.0156 | Grad Norm: 0.00891016\n",
      "Epoch 5 | Step 3538200 | Avg Loss: 0.0153 | Grad Norm: 0.01059204\n",
      "Epoch 5 | Step 3538300 | Avg Loss: 0.0154 | Grad Norm: 0.00915933\n",
      "Epoch 5 | Step 3538400 | Avg Loss: 0.0149 | Grad Norm: 0.00798650\n",
      "Epoch 5 | Step 3538500 | Avg Loss: 0.0154 | Grad Norm: 0.00865420\n",
      "Epoch 5 | Step 3538600 | Avg Loss: 0.0156 | Grad Norm: 0.00818832\n",
      "Epoch 5 | Step 3538700 | Avg Loss: 0.0158 | Grad Norm: 0.00982176\n",
      "Epoch 5 | Step 3538800 | Avg Loss: 0.0153 | Grad Norm: 0.01309914\n",
      "Epoch 5 | Step 3538900 | Avg Loss: 0.0155 | Grad Norm: 0.00866286\n",
      "Epoch 5 | Step 3539000 | Avg Loss: 0.0156 | Grad Norm: 0.00948640\n",
      "Epoch 5 | Step 3539100 | Avg Loss: 0.0156 | Grad Norm: 0.01049842\n",
      "Epoch 5 | Step 3539200 | Avg Loss: 0.0158 | Grad Norm: 0.00783678\n",
      "Epoch 5 | Step 3539300 | Avg Loss: 0.0159 | Grad Norm: 0.00924200\n",
      "Epoch 5 | Step 3539400 | Avg Loss: 0.0161 | Grad Norm: 0.00859934\n",
      "Epoch 5 | Step 3539500 | Avg Loss: 0.0162 | Grad Norm: 0.01116932\n",
      "Epoch 5 | Step 3539600 | Avg Loss: 0.0161 | Grad Norm: 0.00997870\n",
      "Epoch 5 | Step 3539700 | Avg Loss: 0.0158 | Grad Norm: 0.00971381\n",
      "Epoch 5 | Step 3539800 | Avg Loss: 0.0159 | Grad Norm: 0.00877743\n",
      "Epoch 5 | Step 3539900 | Avg Loss: 0.0157 | Grad Norm: 0.01083961\n",
      "Epoch 5 | Step 3540000 | Avg Loss: 0.0157 | Grad Norm: 0.00907291\n",
      "Epoch 5 | Step 3540100 | Avg Loss: 0.0154 | Grad Norm: 0.00956106\n",
      "Epoch 5 | Step 3540200 | Avg Loss: 0.0151 | Grad Norm: 0.00875213\n",
      "Epoch 5 | Step 3540300 | Avg Loss: 0.0151 | Grad Norm: 0.00965395\n",
      "Epoch 5 | Step 3540400 | Avg Loss: 0.0153 | Grad Norm: 0.01071226\n",
      "Epoch 5 | Step 3540500 | Avg Loss: 0.0153 | Grad Norm: 0.01008758\n",
      "Epoch 5 | Step 3540600 | Avg Loss: 0.0152 | Grad Norm: 0.00861991\n",
      "Epoch 5 | Step 3540700 | Avg Loss: 0.0155 | Grad Norm: 0.00896946\n",
      "Epoch 5 | Step 3540800 | Avg Loss: 0.0153 | Grad Norm: 0.00923831\n",
      "Epoch 5 | Step 3540900 | Avg Loss: 0.0158 | Grad Norm: 0.00927015\n",
      "Epoch 5 | Step 3541000 | Avg Loss: 0.0158 | Grad Norm: 0.01026706\n",
      "Epoch 5 | Step 3541100 | Avg Loss: 0.0160 | Grad Norm: 0.00888238\n",
      "Epoch 5 | Step 3541200 | Avg Loss: 0.0157 | Grad Norm: 0.00930763\n",
      "Epoch 5 | Step 3541300 | Avg Loss: 0.0156 | Grad Norm: 0.01084281\n",
      "Epoch 5 | Step 3541400 | Avg Loss: 0.0156 | Grad Norm: 0.00923169\n",
      "Epoch 5 | Step 3541500 | Avg Loss: 0.0153 | Grad Norm: 0.00814251\n",
      "Epoch 5 | Step 3541600 | Avg Loss: 0.0152 | Grad Norm: 0.00833586\n",
      "Epoch 5 | Step 3541700 | Avg Loss: 0.0154 | Grad Norm: 0.00977455\n",
      "Epoch 5 | Step 3541800 | Avg Loss: 0.0157 | Grad Norm: 0.01003275\n",
      "Epoch 5 | Step 3541900 | Avg Loss: 0.0159 | Grad Norm: 0.00929817\n",
      "Epoch 5 | Step 3542000 | Avg Loss: 0.0158 | Grad Norm: 0.00872323\n",
      "Epoch 5 | Step 3542100 | Avg Loss: 0.0156 | Grad Norm: 0.00887485\n",
      "Epoch 5 | Step 3542200 | Avg Loss: 0.0159 | Grad Norm: 0.01132103\n",
      "Epoch 5 | Step 3542300 | Avg Loss: 0.0160 | Grad Norm: 0.00887237\n",
      "Epoch 5 | Step 3542400 | Avg Loss: 0.0163 | Grad Norm: 0.00996648\n",
      "Epoch 5 | Step 3542500 | Avg Loss: 0.0159 | Grad Norm: 0.00867966\n",
      "Epoch 5 | Step 3542600 | Avg Loss: 0.0162 | Grad Norm: 0.00994291\n",
      "Epoch 5 | Step 3542700 | Avg Loss: 0.0158 | Grad Norm: 0.00888357\n",
      "Epoch 5 | Step 3542800 | Avg Loss: 0.0161 | Grad Norm: 0.01070493\n",
      "Epoch 5 | Step 3542900 | Avg Loss: 0.0155 | Grad Norm: 0.00951292\n",
      "Epoch 5 | Step 3543000 | Avg Loss: 0.0156 | Grad Norm: 0.00852752\n",
      "Epoch 5 | Step 3543100 | Avg Loss: 0.0155 | Grad Norm: 0.01005158\n",
      "Epoch 5 | Step 3543200 | Avg Loss: 0.0156 | Grad Norm: 0.00981422\n",
      "Epoch 5 | Step 3543300 | Avg Loss: 0.0157 | Grad Norm: 0.01031078\n",
      "Epoch 5 | Step 3543400 | Avg Loss: 0.0156 | Grad Norm: 0.00943606\n",
      "Epoch 5 | Step 3543500 | Avg Loss: 0.0156 | Grad Norm: 0.00902153\n",
      "Epoch 5 | Step 3543600 | Avg Loss: 0.0156 | Grad Norm: 0.00842476\n",
      "Epoch 5 | Step 3543700 | Avg Loss: 0.0161 | Grad Norm: 0.00901504\n",
      "Epoch 5 | Step 3543800 | Avg Loss: 0.0160 | Grad Norm: 0.01057374\n",
      "Epoch 5 | Step 3543900 | Avg Loss: 0.0160 | Grad Norm: 0.01095430\n",
      "Epoch 5 | Step 3544000 | Avg Loss: 0.0163 | Grad Norm: 0.01020345\n",
      "Epoch 5 | Step 3544100 | Avg Loss: 0.0157 | Grad Norm: 0.00964261\n",
      "Epoch 5 | Step 3544200 | Avg Loss: 0.0159 | Grad Norm: 0.00980805\n",
      "Epoch 5 | Step 3544300 | Avg Loss: 0.0159 | Grad Norm: 0.01139534\n",
      "Epoch 5 | Step 3544400 | Avg Loss: 0.0161 | Grad Norm: 0.00895926\n",
      "Epoch 5 | Step 3544500 | Avg Loss: 0.0158 | Grad Norm: 0.00891464\n",
      "Epoch 5 | Step 3544600 | Avg Loss: 0.0160 | Grad Norm: 0.01028818\n",
      "Epoch 5 | Step 3544700 | Avg Loss: 0.0158 | Grad Norm: 0.01027172\n",
      "Epoch 5 | Step 3544800 | Avg Loss: 0.0152 | Grad Norm: 0.00873610\n",
      "Epoch 5 | Step 3544900 | Avg Loss: 0.0147 | Grad Norm: 0.00768069\n",
      "Epoch 5 | Step 3545000 | Avg Loss: 0.0145 | Grad Norm: 0.00942062\n",
      "Epoch 5 | Step 3545100 | Avg Loss: 0.0149 | Grad Norm: 0.00892433\n",
      "Epoch 5 | Step 3545200 | Avg Loss: 0.0150 | Grad Norm: 0.00971033\n",
      "Epoch 5 | Step 3545300 | Avg Loss: 0.0151 | Grad Norm: 0.00973521\n",
      "Epoch 5 | Step 3545400 | Avg Loss: 0.0155 | Grad Norm: 0.00890362\n",
      "Epoch 5 | Step 3545500 | Avg Loss: 0.0155 | Grad Norm: 0.00822213\n",
      "Epoch 5 | Step 3545600 | Avg Loss: 0.0155 | Grad Norm: 0.00853031\n",
      "Epoch 5 | Step 3545700 | Avg Loss: 0.0156 | Grad Norm: 0.01146536\n",
      "Epoch 5 | Step 3545800 | Avg Loss: 0.0157 | Grad Norm: 0.00913985\n",
      "Epoch 5 | Step 3545900 | Avg Loss: 0.0157 | Grad Norm: 0.00914265\n",
      "Epoch 5 | Step 3546000 | Avg Loss: 0.0157 | Grad Norm: 0.00985644\n",
      "Epoch 5 | Step 3546100 | Avg Loss: 0.0159 | Grad Norm: 0.00942609\n",
      "Epoch 5 | Step 3546200 | Avg Loss: 0.0156 | Grad Norm: 0.00933814\n",
      "Epoch 5 | Step 3546300 | Avg Loss: 0.0153 | Grad Norm: 0.00830936\n",
      "Epoch 5 | Step 3546400 | Avg Loss: 0.0153 | Grad Norm: 0.00859364\n",
      "Epoch 5 | Step 3546500 | Avg Loss: 0.0153 | Grad Norm: 0.01138644\n",
      "Epoch 5 | Step 3546600 | Avg Loss: 0.0155 | Grad Norm: 0.00910711\n",
      "Epoch 5 | Step 3546700 | Avg Loss: 0.0154 | Grad Norm: 0.00882651\n",
      "Epoch 5 | Step 3546800 | Avg Loss: 0.0157 | Grad Norm: 0.00927854\n",
      "Epoch 5 | Step 3546900 | Avg Loss: 0.0158 | Grad Norm: 0.00937829\n",
      "Epoch 5 | Step 3547000 | Avg Loss: 0.0155 | Grad Norm: 0.00952424\n",
      "Epoch 5 | Step 3547100 | Avg Loss: 0.0152 | Grad Norm: 0.01062490\n",
      "Epoch 5 | Step 3547200 | Avg Loss: 0.0151 | Grad Norm: 0.00964681\n",
      "Epoch 5 | Step 3547300 | Avg Loss: 0.0153 | Grad Norm: 0.00946672\n",
      "Epoch 5 | Step 3547400 | Avg Loss: 0.0151 | Grad Norm: 0.00968467\n",
      "Epoch 5 | Step 3547500 | Avg Loss: 0.0155 | Grad Norm: 0.00923657\n",
      "Epoch 5 | Step 3547600 | Avg Loss: 0.0155 | Grad Norm: 0.00838318\n",
      "Epoch 5 | Step 3547700 | Avg Loss: 0.0154 | Grad Norm: 0.00891362\n",
      "Epoch 5 | Step 3547800 | Avg Loss: 0.0157 | Grad Norm: 0.01179022\n",
      "Epoch 5 | Step 3547900 | Avg Loss: 0.0154 | Grad Norm: 0.00935466\n",
      "Epoch 5 | Step 3548000 | Avg Loss: 0.0158 | Grad Norm: 0.00855317\n",
      "Epoch 5 | Step 3548100 | Avg Loss: 0.0156 | Grad Norm: 0.00992469\n",
      "Epoch 5 | Step 3548200 | Avg Loss: 0.0156 | Grad Norm: 0.00968203\n",
      "Epoch 5 | Step 3548300 | Avg Loss: 0.0158 | Grad Norm: 0.00998253\n",
      "Epoch 5 | Step 3548400 | Avg Loss: 0.0157 | Grad Norm: 0.00991329\n",
      "Epoch 5 | Step 3548500 | Avg Loss: 0.0154 | Grad Norm: 0.01161571\n",
      "Epoch 5 | Step 3548600 | Avg Loss: 0.0155 | Grad Norm: 0.00883208\n",
      "Epoch 5 | Step 3548700 | Avg Loss: 0.0155 | Grad Norm: 0.01110587\n",
      "Epoch 5 | Step 3548800 | Avg Loss: 0.0152 | Grad Norm: 0.00940347\n",
      "Epoch 5 | Step 3548900 | Avg Loss: 0.0150 | Grad Norm: 0.00861040\n",
      "Epoch 5 | Step 3549000 | Avg Loss: 0.0153 | Grad Norm: 0.00975708\n",
      "Epoch 5 | Step 3549100 | Avg Loss: 0.0153 | Grad Norm: 0.00768012\n",
      "Epoch 5 | Step 3549200 | Avg Loss: 0.0153 | Grad Norm: 0.00988523\n",
      "Epoch 5 | Step 3549300 | Avg Loss: 0.0153 | Grad Norm: 0.00944834\n",
      "Epoch 5 | Step 3549400 | Avg Loss: 0.0153 | Grad Norm: 0.00943813\n",
      "Epoch 5 | Step 3549500 | Avg Loss: 0.0152 | Grad Norm: 0.00968969\n",
      "Epoch 5 | Step 3549600 | Avg Loss: 0.0154 | Grad Norm: 0.00915319\n",
      "Epoch 5 | Step 3549700 | Avg Loss: 0.0158 | Grad Norm: 0.00978016\n",
      "Epoch 5 | Step 3549800 | Avg Loss: 0.0156 | Grad Norm: 0.00879688\n",
      "Epoch 5 | Step 3549900 | Avg Loss: 0.0157 | Grad Norm: 0.00930258\n",
      "Epoch 5 | Step 3550000 | Avg Loss: 0.0155 | Grad Norm: 0.00954857\n",
      "Epoch 5 | Step 3550100 | Avg Loss: 0.0158 | Grad Norm: 0.00986893\n",
      "Epoch 5 | Step 3550200 | Avg Loss: 0.0160 | Grad Norm: 0.00897739\n",
      "Epoch 5 | Step 3550300 | Avg Loss: 0.0154 | Grad Norm: 0.01031134\n",
      "Epoch 5 | Step 3550400 | Avg Loss: 0.0159 | Grad Norm: 0.00829561\n",
      "Epoch 5 | Step 3550500 | Avg Loss: 0.0157 | Grad Norm: 0.00913507\n",
      "Epoch 5 | Step 3550600 | Avg Loss: 0.0155 | Grad Norm: 0.00901659\n",
      "Epoch 5 | Step 3550700 | Avg Loss: 0.0154 | Grad Norm: 0.00872977\n",
      "Epoch 5 | Step 3550800 | Avg Loss: 0.0156 | Grad Norm: 0.00930851\n",
      "Epoch 5 | Step 3550900 | Avg Loss: 0.0155 | Grad Norm: 0.00793640\n",
      "Epoch 5 | Step 3551000 | Avg Loss: 0.0154 | Grad Norm: 0.00815638\n",
      "Epoch 5 | Step 3551100 | Avg Loss: 0.0151 | Grad Norm: 0.00897290\n",
      "Epoch 5 | Step 3551200 | Avg Loss: 0.0153 | Grad Norm: 0.00805562\n",
      "Epoch 5 | Step 3551300 | Avg Loss: 0.0152 | Grad Norm: 0.01092888\n",
      "Epoch 5 | Step 3551400 | Avg Loss: 0.0152 | Grad Norm: 0.00890982\n",
      "Epoch 5 | Step 3551500 | Avg Loss: 0.0152 | Grad Norm: 0.00896039\n",
      "Epoch 5 | Step 3551600 | Avg Loss: 0.0148 | Grad Norm: 0.00876198\n",
      "Epoch 5 | Step 3551700 | Avg Loss: 0.0150 | Grad Norm: 0.00991832\n",
      "Epoch 5 | Step 3551800 | Avg Loss: 0.0152 | Grad Norm: 0.00811155\n",
      "Epoch 5 | Step 3551900 | Avg Loss: 0.0154 | Grad Norm: 0.00841692\n",
      "Epoch 5 | Step 3552000 | Avg Loss: 0.0154 | Grad Norm: 0.00815809\n",
      "Epoch 5 | Step 3552100 | Avg Loss: 0.0151 | Grad Norm: 0.00834516\n",
      "Epoch 5 | Step 3552200 | Avg Loss: 0.0155 | Grad Norm: 0.00933664\n",
      "Epoch 5 | Step 3552300 | Avg Loss: 0.0156 | Grad Norm: 0.01266238\n",
      "Epoch 5 | Step 3552400 | Avg Loss: 0.0155 | Grad Norm: 0.00893470\n",
      "Epoch 5 | Step 3552500 | Avg Loss: 0.0158 | Grad Norm: 0.00971585\n",
      "Epoch 5 | Step 3552600 | Avg Loss: 0.0158 | Grad Norm: 0.00937768\n",
      "Epoch 5 | Step 3552700 | Avg Loss: 0.0158 | Grad Norm: 0.01011126\n",
      "Epoch 5 | Step 3552800 | Avg Loss: 0.0157 | Grad Norm: 0.01110127\n",
      "Epoch 5 | Step 3552900 | Avg Loss: 0.0156 | Grad Norm: 0.00962406\n",
      "Epoch 5 | Step 3553000 | Avg Loss: 0.0155 | Grad Norm: 0.00896367\n",
      "Epoch 5 | Step 3553100 | Avg Loss: 0.0154 | Grad Norm: 0.00898910\n",
      "Epoch 5 | Step 3553200 | Avg Loss: 0.0154 | Grad Norm: 0.00926185\n",
      "Epoch 5 | Step 3553300 | Avg Loss: 0.0157 | Grad Norm: 0.01027286\n",
      "Epoch 5 | Step 3553400 | Avg Loss: 0.0158 | Grad Norm: 0.00833260\n",
      "Epoch 5 | Step 3553500 | Avg Loss: 0.0154 | Grad Norm: 0.00837140\n",
      "Epoch 5 | Step 3553600 | Avg Loss: 0.0152 | Grad Norm: 0.00943759\n",
      "Epoch 5 | Step 3553700 | Avg Loss: 0.0155 | Grad Norm: 0.00866067\n",
      "Epoch 5 | Step 3553800 | Avg Loss: 0.0156 | Grad Norm: 0.00990139\n",
      "Epoch 5 | Step 3553900 | Avg Loss: 0.0152 | Grad Norm: 0.00913120\n",
      "Epoch 5 | Step 3554000 | Avg Loss: 0.0158 | Grad Norm: 0.00856456\n",
      "Epoch 5 | Step 3554100 | Avg Loss: 0.0160 | Grad Norm: 0.00844528\n",
      "Epoch 5 | Step 3554200 | Avg Loss: 0.0154 | Grad Norm: 0.00998327\n",
      "Epoch 5 | Step 3554300 | Avg Loss: 0.0155 | Grad Norm: 0.01067885\n",
      "Epoch 5 | Step 3554400 | Avg Loss: 0.0157 | Grad Norm: 0.00869093\n",
      "Epoch 5 | Step 3554500 | Avg Loss: 0.0158 | Grad Norm: 0.00885294\n",
      "Epoch 5 | Step 3554600 | Avg Loss: 0.0157 | Grad Norm: 0.00826219\n",
      "Epoch 5 | Step 3554700 | Avg Loss: 0.0159 | Grad Norm: 0.01076856\n",
      "Epoch 5 | Step 3554800 | Avg Loss: 0.0157 | Grad Norm: 0.00937718\n",
      "Epoch 5 | Step 3554900 | Avg Loss: 0.0160 | Grad Norm: 0.00934782\n",
      "Epoch 5 | Step 3555000 | Avg Loss: 0.0160 | Grad Norm: 0.01099920\n",
      "Epoch 5 | Step 3555100 | Avg Loss: 0.0159 | Grad Norm: 0.00901625\n",
      "Epoch 5 | Step 3555200 | Avg Loss: 0.0156 | Grad Norm: 0.00872907\n",
      "Epoch 5 | Step 3555300 | Avg Loss: 0.0157 | Grad Norm: 0.00880431\n",
      "Epoch 5 | Step 3555400 | Avg Loss: 0.0153 | Grad Norm: 0.00908449\n",
      "Epoch 5 | Step 3555500 | Avg Loss: 0.0152 | Grad Norm: 0.00856865\n",
      "Epoch 5 | Step 3555600 | Avg Loss: 0.0152 | Grad Norm: 0.00757874\n",
      "Epoch 5 | Step 3555700 | Avg Loss: 0.0152 | Grad Norm: 0.00816100\n",
      "Epoch 5 | Step 3555800 | Avg Loss: 0.0152 | Grad Norm: 0.00969751\n",
      "Epoch 5 | Step 3555900 | Avg Loss: 0.0151 | Grad Norm: 0.00950864\n",
      "Epoch 5 | Step 3556000 | Avg Loss: 0.0153 | Grad Norm: 0.00899257\n",
      "Epoch 5 | Step 3556100 | Avg Loss: 0.0156 | Grad Norm: 0.00918957\n",
      "Epoch 5 | Step 3556200 | Avg Loss: 0.0156 | Grad Norm: 0.00869624\n",
      "Epoch 5 | Step 3556300 | Avg Loss: 0.0156 | Grad Norm: 0.00875926\n",
      "Epoch 5 | Step 3556400 | Avg Loss: 0.0155 | Grad Norm: 0.00990498\n",
      "Epoch 5 | Step 3556500 | Avg Loss: 0.0153 | Grad Norm: 0.00895891\n",
      "Epoch 5 | Step 3556600 | Avg Loss: 0.0155 | Grad Norm: 0.00981672\n",
      "Epoch 5 | Step 3556700 | Avg Loss: 0.0154 | Grad Norm: 0.00932787\n",
      "Epoch 5 | Step 3556800 | Avg Loss: 0.0156 | Grad Norm: 0.00978873\n",
      "Epoch 5 | Step 3556900 | Avg Loss: 0.0156 | Grad Norm: 0.00748752\n",
      "Epoch 5 | Step 3557000 | Avg Loss: 0.0159 | Grad Norm: 0.00955259\n",
      "Epoch 5 | Step 3557100 | Avg Loss: 0.0155 | Grad Norm: 0.01045588\n",
      "Epoch 5 | Step 3557200 | Avg Loss: 0.0154 | Grad Norm: 0.00908961\n",
      "Epoch 5 | Step 3557300 | Avg Loss: 0.0156 | Grad Norm: 0.01062380\n",
      "Epoch 5 | Step 3557400 | Avg Loss: 0.0158 | Grad Norm: 0.00919594\n",
      "Epoch 5 | Step 3557500 | Avg Loss: 0.0157 | Grad Norm: 0.00803600\n",
      "Epoch 5 | Step 3557600 | Avg Loss: 0.0158 | Grad Norm: 0.00926493\n",
      "Epoch 5 | Step 3557700 | Avg Loss: 0.0163 | Grad Norm: 0.01003414\n",
      "Epoch 5 | Step 3557800 | Avg Loss: 0.0162 | Grad Norm: 0.00926196\n",
      "Epoch 5 | Step 3557900 | Avg Loss: 0.0161 | Grad Norm: 0.00930956\n",
      "Epoch 5 | Step 3558000 | Avg Loss: 0.0160 | Grad Norm: 0.00954509\n",
      "Epoch 5 | Step 3558100 | Avg Loss: 0.0154 | Grad Norm: 0.01142420\n",
      "Epoch 5 | Step 3558200 | Avg Loss: 0.0156 | Grad Norm: 0.00920521\n",
      "Epoch 5 | Step 3558300 | Avg Loss: 0.0156 | Grad Norm: 0.00911411\n",
      "Epoch 5 | Step 3558400 | Avg Loss: 0.0158 | Grad Norm: 0.00879668\n",
      "Epoch 5 | Step 3558500 | Avg Loss: 0.0153 | Grad Norm: 0.00984174\n",
      "Epoch 5 | Step 3558600 | Avg Loss: 0.0154 | Grad Norm: 0.00861235\n",
      "Epoch 5 | Step 3558700 | Avg Loss: 0.0153 | Grad Norm: 0.00829591\n",
      "Epoch 5 | Step 3558800 | Avg Loss: 0.0151 | Grad Norm: 0.00760945\n",
      "Epoch 5 | Step 3558900 | Avg Loss: 0.0153 | Grad Norm: 0.01185483\n",
      "Epoch 5 | Step 3559000 | Avg Loss: 0.0156 | Grad Norm: 0.01017886\n",
      "Epoch 5 | Step 3559100 | Avg Loss: 0.0157 | Grad Norm: 0.00880951\n",
      "Epoch 5 | Step 3559200 | Avg Loss: 0.0157 | Grad Norm: 0.00915302\n",
      "Epoch 5 | Step 3559300 | Avg Loss: 0.0156 | Grad Norm: 0.00886808\n",
      "Epoch 5 | Step 3559400 | Avg Loss: 0.0159 | Grad Norm: 0.00976048\n",
      "Epoch 5 | Step 3559500 | Avg Loss: 0.0159 | Grad Norm: 0.00902353\n",
      "Epoch 5 | Step 3559600 | Avg Loss: 0.0152 | Grad Norm: 0.00883334\n",
      "Epoch 5 | Step 3559700 | Avg Loss: 0.0152 | Grad Norm: 0.00828146\n",
      "Epoch 5 | Step 3559800 | Avg Loss: 0.0153 | Grad Norm: 0.00757908\n",
      "Epoch 5 | Step 3559900 | Avg Loss: 0.0154 | Grad Norm: 0.00873966\n",
      "Epoch 5 | Step 3560000 | Avg Loss: 0.0154 | Grad Norm: 0.01026233\n",
      "Epoch 5 | Step 3560100 | Avg Loss: 0.0152 | Grad Norm: 0.00931036\n",
      "Epoch 5 | Step 3560200 | Avg Loss: 0.0152 | Grad Norm: 0.00794355\n",
      "Epoch 5 | Step 3560300 | Avg Loss: 0.0150 | Grad Norm: 0.01040744\n",
      "Epoch 5 | Step 3560400 | Avg Loss: 0.0152 | Grad Norm: 0.00843983\n",
      "Epoch 5 | Step 3560500 | Avg Loss: 0.0154 | Grad Norm: 0.00806262\n",
      "Epoch 5 | Step 3560600 | Avg Loss: 0.0153 | Grad Norm: 0.00810418\n",
      "Epoch 5 | Step 3560700 | Avg Loss: 0.0152 | Grad Norm: 0.00873307\n",
      "Epoch 5 | Step 3560800 | Avg Loss: 0.0154 | Grad Norm: 0.00856156\n",
      "Epoch 5 | Step 3560900 | Avg Loss: 0.0155 | Grad Norm: 0.00942439\n",
      "Epoch 5 | Step 3561000 | Avg Loss: 0.0154 | Grad Norm: 0.00909863\n",
      "Epoch 5 | Step 3561100 | Avg Loss: 0.0151 | Grad Norm: 0.00797958\n",
      "Epoch 5 | Step 3561200 | Avg Loss: 0.0152 | Grad Norm: 0.01438962\n",
      "Epoch 5 | Step 3561300 | Avg Loss: 0.0155 | Grad Norm: 0.01000942\n",
      "Epoch 5 | Step 3561400 | Avg Loss: 0.0158 | Grad Norm: 0.00893324\n",
      "Epoch 5 | Step 3561500 | Avg Loss: 0.0156 | Grad Norm: 0.00902476\n",
      "Epoch 5 | Step 3561600 | Avg Loss: 0.0158 | Grad Norm: 0.00918006\n",
      "Epoch 5 | Step 3561700 | Avg Loss: 0.0155 | Grad Norm: 0.00939341\n",
      "Epoch 5 | Step 3561800 | Avg Loss: 0.0160 | Grad Norm: 0.01041448\n",
      "Epoch 5 | Step 3561900 | Avg Loss: 0.0156 | Grad Norm: 0.00801828\n",
      "Epoch 5 | Step 3562000 | Avg Loss: 0.0154 | Grad Norm: 0.00858617\n",
      "Epoch 5 | Step 3562100 | Avg Loss: 0.0150 | Grad Norm: 0.00965609\n",
      "Epoch 5 | Step 3562200 | Avg Loss: 0.0153 | Grad Norm: 0.00912227\n",
      "Epoch 5 | Step 3562300 | Avg Loss: 0.0155 | Grad Norm: 0.00877178\n",
      "Epoch 5 | Step 3562400 | Avg Loss: 0.0161 | Grad Norm: 0.00954760\n",
      "Epoch 5 | Step 3562500 | Avg Loss: 0.0161 | Grad Norm: 0.01060340\n",
      "Epoch 5 | Step 3562600 | Avg Loss: 0.0158 | Grad Norm: 0.01036789\n",
      "Epoch 5 | Step 3562700 | Avg Loss: 0.0160 | Grad Norm: 0.00979941\n",
      "Epoch 5 | Step 3562800 | Avg Loss: 0.0158 | Grad Norm: 0.01104774\n",
      "Epoch 5 | Step 3562900 | Avg Loss: 0.0153 | Grad Norm: 0.00802905\n",
      "Epoch 5 | Step 3563000 | Avg Loss: 0.0153 | Grad Norm: 0.00961760\n",
      "Epoch 5 | Step 3563100 | Avg Loss: 0.0154 | Grad Norm: 0.00809994\n",
      "Epoch 5 | Step 3563200 | Avg Loss: 0.0154 | Grad Norm: 0.00951046\n",
      "Epoch 5 | Step 3563300 | Avg Loss: 0.0157 | Grad Norm: 0.01594510\n",
      "Epoch 5 | Step 3563400 | Avg Loss: 0.0157 | Grad Norm: 0.00831609\n",
      "Epoch 5 | Step 3563500 | Avg Loss: 0.0153 | Grad Norm: 0.01008776\n",
      "Epoch 5 | Step 3563600 | Avg Loss: 0.0150 | Grad Norm: 0.00842087\n",
      "Epoch 5 | Step 3563700 | Avg Loss: 0.0148 | Grad Norm: 0.00878776\n",
      "Epoch 5 | Step 3563800 | Avg Loss: 0.0148 | Grad Norm: 0.00847247\n",
      "Epoch 5 | Step 3563900 | Avg Loss: 0.0145 | Grad Norm: 0.00906170\n",
      "Epoch 5 | Step 3564000 | Avg Loss: 0.0146 | Grad Norm: 0.00861019\n",
      "Epoch 5 | Step 3564100 | Avg Loss: 0.0149 | Grad Norm: 0.00850456\n",
      "Epoch 5 | Step 3564200 | Avg Loss: 0.0152 | Grad Norm: 0.00827276\n",
      "Epoch 5 | Step 3564300 | Avg Loss: 0.0153 | Grad Norm: 0.00859172\n",
      "Epoch 5 | Step 3564400 | Avg Loss: 0.0154 | Grad Norm: 0.01514404\n",
      "Epoch 5 | Step 3564500 | Avg Loss: 0.0155 | Grad Norm: 0.00924279\n",
      "Epoch 5 | Step 3564600 | Avg Loss: 0.0157 | Grad Norm: 0.00805449\n",
      "Epoch 5 | Step 3564700 | Avg Loss: 0.0161 | Grad Norm: 0.00917933\n",
      "Epoch 5 | Step 3564800 | Avg Loss: 0.0162 | Grad Norm: 0.00805211\n",
      "Epoch 5 | Step 3564900 | Avg Loss: 0.0161 | Grad Norm: 0.00861573\n",
      "Epoch 5 | Step 3565000 | Avg Loss: 0.0163 | Grad Norm: 0.00924328\n",
      "Epoch 5 | Step 3565100 | Avg Loss: 0.0158 | Grad Norm: 0.00854028\n",
      "Epoch 5 | Step 3565200 | Avg Loss: 0.0156 | Grad Norm: 0.00842101\n",
      "Epoch 5 | Step 3565300 | Avg Loss: 0.0152 | Grad Norm: 0.00988650\n",
      "Epoch 5 | Step 3565400 | Avg Loss: 0.0148 | Grad Norm: 0.00838835\n",
      "Epoch 5 | Step 3565500 | Avg Loss: 0.0148 | Grad Norm: 0.00901523\n",
      "Epoch 5 | Step 3565600 | Avg Loss: 0.0149 | Grad Norm: 0.00975320\n",
      "Epoch 5 | Step 3565700 | Avg Loss: 0.0151 | Grad Norm: 0.01161210\n",
      "Epoch 5 | Step 3565800 | Avg Loss: 0.0145 | Grad Norm: 0.00874311\n",
      "Epoch 5 | Step 3565900 | Avg Loss: 0.0145 | Grad Norm: 0.00803029\n",
      "Epoch 5 | Step 3566000 | Avg Loss: 0.0149 | Grad Norm: 0.00877727\n",
      "Epoch 5 | Step 3566100 | Avg Loss: 0.0153 | Grad Norm: 0.01041012\n",
      "Epoch 5 | Step 3566200 | Avg Loss: 0.0152 | Grad Norm: 0.01052424\n",
      "Epoch 5 | Step 3566300 | Avg Loss: 0.0154 | Grad Norm: 0.00905871\n",
      "Epoch 5 | Step 3566400 | Avg Loss: 0.0156 | Grad Norm: 0.00965892\n",
      "Epoch 5 | Step 3566500 | Avg Loss: 0.0158 | Grad Norm: 0.00827867\n",
      "Epoch 5 | Step 3566600 | Avg Loss: 0.0159 | Grad Norm: 0.00931384\n",
      "Epoch 5 | Step 3566700 | Avg Loss: 0.0156 | Grad Norm: 0.01672489\n",
      "Epoch 5 | Step 3566800 | Avg Loss: 0.0155 | Grad Norm: 0.01109102\n",
      "Epoch 5 | Step 3566900 | Avg Loss: 0.0156 | Grad Norm: 0.00928543\n",
      "Epoch 5 | Step 3567000 | Avg Loss: 0.0155 | Grad Norm: 0.00866803\n",
      "Epoch 5 | Step 3567100 | Avg Loss: 0.0152 | Grad Norm: 0.00850325\n",
      "Epoch 5 | Step 3567200 | Avg Loss: 0.0149 | Grad Norm: 0.00830327\n",
      "Epoch 5 | Step 3567300 | Avg Loss: 0.0151 | Grad Norm: 0.00833001\n",
      "Epoch 5 | Step 3567400 | Avg Loss: 0.0151 | Grad Norm: 0.00826674\n",
      "Epoch 5 | Step 3567500 | Avg Loss: 0.0154 | Grad Norm: 0.00939401\n",
      "Epoch 5 | Step 3567600 | Avg Loss: 0.0156 | Grad Norm: 0.00817593\n",
      "Epoch 5 | Step 3567700 | Avg Loss: 0.0157 | Grad Norm: 0.01449432\n",
      "Epoch 5 | Step 3567800 | Avg Loss: 0.0154 | Grad Norm: 0.00771944\n",
      "Epoch 5 | Step 3567900 | Avg Loss: 0.0155 | Grad Norm: 0.00939458\n",
      "Epoch 5 | Step 3568000 | Avg Loss: 0.0147 | Grad Norm: 0.00829073\n",
      "Epoch 5 | Step 3568100 | Avg Loss: 0.0149 | Grad Norm: 0.00802937\n",
      "Epoch 5 | Step 3568200 | Avg Loss: 0.0152 | Grad Norm: 0.00793064\n",
      "Epoch 5 | Step 3568300 | Avg Loss: 0.0157 | Grad Norm: 0.00908304\n",
      "Epoch 5 | Step 3568400 | Avg Loss: 0.0159 | Grad Norm: 0.01340087\n",
      "Epoch 5 | Step 3568500 | Avg Loss: 0.0158 | Grad Norm: 0.00867205\n",
      "Epoch 5 | Step 3568600 | Avg Loss: 0.0158 | Grad Norm: 0.00837311\n",
      "Epoch 5 | Step 3568700 | Avg Loss: 0.0156 | Grad Norm: 0.00942151\n",
      "Epoch 5 | Step 3568800 | Avg Loss: 0.0156 | Grad Norm: 0.00932699\n",
      "Epoch 5 | Step 3568900 | Avg Loss: 0.0159 | Grad Norm: 0.00875263\n",
      "Epoch 5 | Step 3569000 | Avg Loss: 0.0161 | Grad Norm: 0.01103443\n",
      "Epoch 5 | Step 3569100 | Avg Loss: 0.0160 | Grad Norm: 0.01023443\n",
      "Epoch 5 | Step 3569200 | Avg Loss: 0.0159 | Grad Norm: 0.01127912\n",
      "Epoch 5 | Step 3569300 | Avg Loss: 0.0161 | Grad Norm: 0.00986623\n",
      "Epoch 5 | Step 3569400 | Avg Loss: 0.0159 | Grad Norm: 0.00907289\n",
      "Epoch 5 | Step 3569500 | Avg Loss: 0.0159 | Grad Norm: 0.00949816\n",
      "Epoch 5 | Step 3569600 | Avg Loss: 0.0158 | Grad Norm: 0.00903102\n",
      "Epoch 5 | Step 3569700 | Avg Loss: 0.0157 | Grad Norm: 0.01005180\n",
      "Epoch 5 | Step 3569800 | Avg Loss: 0.0158 | Grad Norm: 0.00884141\n",
      "Epoch 5 | Step 3569900 | Avg Loss: 0.0161 | Grad Norm: 0.00807685\n",
      "Epoch 5 | Step 3570000 | Avg Loss: 0.0162 | Grad Norm: 0.00919034\n",
      "Epoch 5 | Step 3570100 | Avg Loss: 0.0158 | Grad Norm: 0.00896450\n",
      "Epoch 5 | Step 3570200 | Avg Loss: 0.0157 | Grad Norm: 0.00779270\n",
      "Epoch 5 | Step 3570300 | Avg Loss: 0.0155 | Grad Norm: 0.00816575\n",
      "Epoch 5 | Step 3570400 | Avg Loss: 0.0156 | Grad Norm: 0.00996166\n",
      "Epoch 5 | Step 3570500 | Avg Loss: 0.0160 | Grad Norm: 0.00913019\n",
      "Epoch 5 | Step 3570600 | Avg Loss: 0.0158 | Grad Norm: 0.00749500\n",
      "Epoch 5 | Step 3570700 | Avg Loss: 0.0160 | Grad Norm: 0.00980521\n",
      "Epoch 5 | Step 3570800 | Avg Loss: 0.0155 | Grad Norm: 0.00937860\n",
      "Epoch 5 | Step 3570900 | Avg Loss: 0.0156 | Grad Norm: 0.01118395\n",
      "Epoch 5 | Step 3571000 | Avg Loss: 0.0157 | Grad Norm: 0.00927790\n",
      "Epoch 5 | Step 3571100 | Avg Loss: 0.0154 | Grad Norm: 0.00991932\n",
      "Epoch 5 | Step 3571200 | Avg Loss: 0.0152 | Grad Norm: 0.00859148\n",
      "Epoch 5 | Step 3571300 | Avg Loss: 0.0154 | Grad Norm: 0.00918054\n",
      "Epoch 5 | Step 3571400 | Avg Loss: 0.0153 | Grad Norm: 0.00822249\n",
      "Epoch 5 | Step 3571500 | Avg Loss: 0.0154 | Grad Norm: 0.00825054\n",
      "Epoch 5 | Step 3571600 | Avg Loss: 0.0155 | Grad Norm: 0.00973552\n",
      "Epoch 5 | Step 3571700 | Avg Loss: 0.0157 | Grad Norm: 0.00878544\n",
      "Epoch 5 | Step 3571800 | Avg Loss: 0.0155 | Grad Norm: 0.00874860\n",
      "Epoch 5 | Step 3571900 | Avg Loss: 0.0156 | Grad Norm: 0.00996709\n",
      "Epoch 5 | Step 3572000 | Avg Loss: 0.0162 | Grad Norm: 0.01128797\n",
      "Epoch 5 | Step 3572100 | Avg Loss: 0.0161 | Grad Norm: 0.00955251\n",
      "Epoch 5 | Step 3572200 | Avg Loss: 0.0160 | Grad Norm: 0.01025933\n",
      "Epoch 5 | Step 3572300 | Avg Loss: 0.0159 | Grad Norm: 0.00800790\n",
      "Epoch 5 | Step 3572400 | Avg Loss: 0.0160 | Grad Norm: 0.01028956\n",
      "Epoch 5 | Step 3572500 | Avg Loss: 0.0156 | Grad Norm: 0.00871721\n",
      "Epoch 5 | Step 3572600 | Avg Loss: 0.0153 | Grad Norm: 0.00803397\n",
      "Epoch 5 | Step 3572700 | Avg Loss: 0.0157 | Grad Norm: 0.01080087\n",
      "Epoch 5 | Step 3572800 | Avg Loss: 0.0157 | Grad Norm: 0.01158078\n",
      "Epoch 5 | Step 3572900 | Avg Loss: 0.0162 | Grad Norm: 0.01003684\n",
      "Epoch 5 | Step 3573000 | Avg Loss: 0.0160 | Grad Norm: 0.00969573\n",
      "Epoch 5 | Step 3573100 | Avg Loss: 0.0161 | Grad Norm: 0.00922295\n",
      "Epoch 5 | Step 3573200 | Avg Loss: 0.0163 | Grad Norm: 0.01051360\n",
      "Epoch 5 | Step 3573300 | Avg Loss: 0.0159 | Grad Norm: 0.00906677\n",
      "Epoch 5 | Step 3573400 | Avg Loss: 0.0156 | Grad Norm: 0.00989365\n",
      "Epoch 5 | Step 3573500 | Avg Loss: 0.0155 | Grad Norm: 0.00953221\n",
      "Epoch 5 | Step 3573600 | Avg Loss: 0.0156 | Grad Norm: 0.01000754\n",
      "Epoch 5 | Step 3573700 | Avg Loss: 0.0152 | Grad Norm: 0.00917143\n",
      "Epoch 5 | Step 3573800 | Avg Loss: 0.0158 | Grad Norm: 0.00951928\n",
      "Epoch 5 | Step 3573900 | Avg Loss: 0.0158 | Grad Norm: 0.00887323\n",
      "Epoch 5 | Step 3574000 | Avg Loss: 0.0153 | Grad Norm: 0.01085686\n",
      "Epoch 5 | Step 3574100 | Avg Loss: 0.0156 | Grad Norm: 0.00819058\n",
      "Epoch 5 | Step 3574200 | Avg Loss: 0.0157 | Grad Norm: 0.00907617\n",
      "Epoch 5 | Step 3574300 | Avg Loss: 0.0157 | Grad Norm: 0.00865439\n",
      "Epoch 5 | Step 3574400 | Avg Loss: 0.0154 | Grad Norm: 0.00921706\n",
      "Epoch 5 | Step 3574500 | Avg Loss: 0.0154 | Grad Norm: 0.00907351\n",
      "Epoch 5 | Step 3574600 | Avg Loss: 0.0155 | Grad Norm: 0.00852295\n",
      "Epoch 5 | Step 3574700 | Avg Loss: 0.0157 | Grad Norm: 0.00834845\n",
      "Epoch 5 | Step 3574800 | Avg Loss: 0.0158 | Grad Norm: 0.01183582\n",
      "Epoch 5 | Step 3574900 | Avg Loss: 0.0154 | Grad Norm: 0.01051108\n",
      "Epoch 5 | Step 3575000 | Avg Loss: 0.0153 | Grad Norm: 0.00916098\n",
      "Epoch 5 | Step 3575100 | Avg Loss: 0.0153 | Grad Norm: 0.00882070\n",
      "Epoch 5 | Step 3575200 | Avg Loss: 0.0150 | Grad Norm: 0.00933996\n",
      "Epoch 5 | Step 3575300 | Avg Loss: 0.0152 | Grad Norm: 0.01171928\n",
      "Epoch 5 | Step 3575400 | Avg Loss: 0.0152 | Grad Norm: 0.00813118\n",
      "Epoch 5 | Step 3575500 | Avg Loss: 0.0152 | Grad Norm: 0.00849358\n",
      "Epoch 5 | Step 3575600 | Avg Loss: 0.0151 | Grad Norm: 0.01076101\n",
      "Epoch 5 | Step 3575700 | Avg Loss: 0.0152 | Grad Norm: 0.00910768\n",
      "Epoch 5 | Step 3575800 | Avg Loss: 0.0150 | Grad Norm: 0.00883562\n",
      "Epoch 5 | Step 3575900 | Avg Loss: 0.0153 | Grad Norm: 0.00899284\n",
      "Epoch 5 | Step 3576000 | Avg Loss: 0.0153 | Grad Norm: 0.01033709\n",
      "Epoch 5 | Step 3576100 | Avg Loss: 0.0154 | Grad Norm: 0.00792659\n",
      "Epoch 5 | Step 3576200 | Avg Loss: 0.0158 | Grad Norm: 0.01283674\n",
      "Epoch 5 | Step 3576300 | Avg Loss: 0.0158 | Grad Norm: 0.01205623\n",
      "Epoch 5 | Step 3576400 | Avg Loss: 0.0158 | Grad Norm: 0.01095402\n",
      "Epoch 5 | Step 3576500 | Avg Loss: 0.0157 | Grad Norm: 0.01040353\n",
      "Epoch 5 | Step 3576600 | Avg Loss: 0.0155 | Grad Norm: 0.00895153\n",
      "Epoch 5 | Step 3576700 | Avg Loss: 0.0154 | Grad Norm: 0.00877288\n",
      "Epoch 5 | Step 3576800 | Avg Loss: 0.0161 | Grad Norm: 0.01011437\n",
      "Epoch 5 | Step 3576900 | Avg Loss: 0.0158 | Grad Norm: 0.00884553\n",
      "Epoch 5 | Step 3577000 | Avg Loss: 0.0161 | Grad Norm: 0.01052518\n",
      "Epoch 5 | Step 3577100 | Avg Loss: 0.0160 | Grad Norm: 0.01040365\n",
      "Epoch 5 | Step 3577200 | Avg Loss: 0.0163 | Grad Norm: 0.00923736\n",
      "Epoch 5 | Step 3577300 | Avg Loss: 0.0163 | Grad Norm: 0.00982859\n",
      "Epoch 5 | Step 3577400 | Avg Loss: 0.0162 | Grad Norm: 0.00915796\n",
      "Epoch 5 | Step 3577500 | Avg Loss: 0.0159 | Grad Norm: 0.00862245\n",
      "Epoch 5 | Step 3577600 | Avg Loss: 0.0158 | Grad Norm: 0.00978818\n",
      "Epoch 5 | Step 3577700 | Avg Loss: 0.0158 | Grad Norm: 0.00780757\n",
      "Epoch 5 | Step 3577800 | Avg Loss: 0.0157 | Grad Norm: 0.01041429\n",
      "Epoch 5 | Step 3577900 | Avg Loss: 0.0155 | Grad Norm: 0.00900119\n",
      "Epoch 5 | Step 3578000 | Avg Loss: 0.0156 | Grad Norm: 0.00815380\n",
      "Epoch 5 | Step 3578100 | Avg Loss: 0.0153 | Grad Norm: 0.01043094\n",
      "Epoch 5 | Step 3578200 | Avg Loss: 0.0159 | Grad Norm: 0.01018537\n",
      "Epoch 5 | Step 3578300 | Avg Loss: 0.0157 | Grad Norm: 0.00927844\n",
      "Epoch 5 | Step 3578400 | Avg Loss: 0.0153 | Grad Norm: 0.00907539\n",
      "Epoch 5 | Step 3578500 | Avg Loss: 0.0155 | Grad Norm: 0.00853257\n",
      "Epoch 5 | Step 3578600 | Avg Loss: 0.0154 | Grad Norm: 0.00913521\n",
      "Epoch 5 | Step 3578700 | Avg Loss: 0.0154 | Grad Norm: 0.00936360\n",
      "Epoch 5 | Step 3578800 | Avg Loss: 0.0155 | Grad Norm: 0.00893795\n",
      "Epoch 5 | Step 3578900 | Avg Loss: 0.0152 | Grad Norm: 0.00809795\n",
      "Epoch 5 | Step 3579000 | Avg Loss: 0.0155 | Grad Norm: 0.00851571\n",
      "Epoch 5 | Step 3579100 | Avg Loss: 0.0155 | Grad Norm: 0.01046355\n",
      "Epoch 5 | Step 3579200 | Avg Loss: 0.0158 | Grad Norm: 0.00909960\n",
      "Epoch 5 | Step 3579300 | Avg Loss: 0.0160 | Grad Norm: 0.00961237\n",
      "Epoch 5 | Step 3579400 | Avg Loss: 0.0161 | Grad Norm: 0.00949992\n",
      "Epoch 5 | Step 3579500 | Avg Loss: 0.0161 | Grad Norm: 0.00909892\n",
      "Epoch 5 | Step 3579600 | Avg Loss: 0.0160 | Grad Norm: 0.01037442\n",
      "Epoch 5 | Step 3579700 | Avg Loss: 0.0162 | Grad Norm: 0.00852223\n",
      "Epoch 5 | Step 3579800 | Avg Loss: 0.0163 | Grad Norm: 0.00932511\n",
      "Epoch 5 | Step 3579900 | Avg Loss: 0.0160 | Grad Norm: 0.00935512\n",
      "Epoch 5 | Step 3580000 | Avg Loss: 0.0164 | Grad Norm: 0.00902084\n",
      "Epoch 5 | Step 3580100 | Avg Loss: 0.0163 | Grad Norm: 0.00930760\n",
      "Epoch 5 | Step 3580200 | Avg Loss: 0.0161 | Grad Norm: 0.00924080\n",
      "Epoch 5 | Step 3580300 | Avg Loss: 0.0154 | Grad Norm: 0.00857526\n",
      "Epoch 5 | Step 3580400 | Avg Loss: 0.0157 | Grad Norm: 0.01106871\n",
      "Epoch 5 | Step 3580500 | Avg Loss: 0.0160 | Grad Norm: 0.00878808\n",
      "Epoch 5 | Step 3580600 | Avg Loss: 0.0155 | Grad Norm: 0.00864093\n",
      "Epoch 5 | Step 3580700 | Avg Loss: 0.0156 | Grad Norm: 0.00933881\n",
      "Epoch 5 | Step 3580800 | Avg Loss: 0.0152 | Grad Norm: 0.00867586\n",
      "Epoch 5 | Step 3580900 | Avg Loss: 0.0151 | Grad Norm: 0.00966457\n",
      "Epoch 5 | Step 3581000 | Avg Loss: 0.0147 | Grad Norm: 0.01013818\n",
      "Epoch 5 | Step 3581100 | Avg Loss: 0.0148 | Grad Norm: 0.00988300\n",
      "Epoch 5 | Step 3581200 | Avg Loss: 0.0148 | Grad Norm: 0.00796819\n",
      "Epoch 5 | Step 3581300 | Avg Loss: 0.0152 | Grad Norm: 0.00902259\n",
      "Epoch 5 | Step 3581400 | Avg Loss: 0.0150 | Grad Norm: 0.00847914\n",
      "Epoch 5 | Step 3581500 | Avg Loss: 0.0151 | Grad Norm: 0.00875113\n",
      "Epoch 5 | Step 3581600 | Avg Loss: 0.0151 | Grad Norm: 0.01010393\n",
      "Epoch 5 | Step 3581700 | Avg Loss: 0.0151 | Grad Norm: 0.00919085\n",
      "Epoch 5 | Step 3581800 | Avg Loss: 0.0153 | Grad Norm: 0.00921612\n",
      "Epoch 5 | Step 3581900 | Avg Loss: 0.0152 | Grad Norm: 0.00719561\n",
      "Epoch 5 | Step 3582000 | Avg Loss: 0.0151 | Grad Norm: 0.00921495\n",
      "Epoch 5 | Step 3582100 | Avg Loss: 0.0149 | Grad Norm: 0.00937673\n",
      "Epoch 5 | Step 3582200 | Avg Loss: 0.0148 | Grad Norm: 0.00841885\n",
      "Epoch 5 | Step 3582300 | Avg Loss: 0.0147 | Grad Norm: 0.00884118\n",
      "Epoch 5 | Step 3582400 | Avg Loss: 0.0150 | Grad Norm: 0.00933260\n",
      "Epoch 5 | Step 3582500 | Avg Loss: 0.0156 | Grad Norm: 0.00886620\n",
      "Epoch 5 | Step 3582600 | Avg Loss: 0.0156 | Grad Norm: 0.00900444\n",
      "Epoch 5 | Step 3582700 | Avg Loss: 0.0152 | Grad Norm: 0.00880639\n",
      "Epoch 5 | Step 3582800 | Avg Loss: 0.0153 | Grad Norm: 0.00920702\n",
      "Epoch 5 | Step 3582900 | Avg Loss: 0.0151 | Grad Norm: 0.00826093\n",
      "Epoch 5 | Step 3583000 | Avg Loss: 0.0154 | Grad Norm: 0.00858889\n",
      "Epoch 5 | Step 3583100 | Avg Loss: 0.0153 | Grad Norm: 0.00780610\n",
      "Epoch 5 | Step 3583200 | Avg Loss: 0.0155 | Grad Norm: 0.00930034\n",
      "Epoch 5 | Step 3583300 | Avg Loss: 0.0153 | Grad Norm: 0.00857580\n",
      "Epoch 5 | Step 3583400 | Avg Loss: 0.0155 | Grad Norm: 0.00805489\n",
      "Epoch 5 | Step 3583500 | Avg Loss: 0.0152 | Grad Norm: 0.00952277\n",
      "Epoch 5 | Step 3583600 | Avg Loss: 0.0154 | Grad Norm: 0.00836182\n",
      "Epoch 5 | Step 3583700 | Avg Loss: 0.0160 | Grad Norm: 0.00854508\n",
      "Epoch 5 | Step 3583800 | Avg Loss: 0.0160 | Grad Norm: 0.00901457\n",
      "Epoch 5 | Step 3583900 | Avg Loss: 0.0161 | Grad Norm: 0.00982322\n",
      "Epoch 5 | Step 3584000 | Avg Loss: 0.0156 | Grad Norm: 0.01054139\n",
      "Epoch 5 | Step 3584100 | Avg Loss: 0.0156 | Grad Norm: 0.01024500\n",
      "Epoch 5 | Step 3584200 | Avg Loss: 0.0157 | Grad Norm: 0.00870137\n",
      "Epoch 5 | Step 3584300 | Avg Loss: 0.0156 | Grad Norm: 0.00926329\n",
      "Epoch 5 | Step 3584400 | Avg Loss: 0.0152 | Grad Norm: 0.00944532\n",
      "Epoch 5 | Step 3584500 | Avg Loss: 0.0152 | Grad Norm: 0.00975200\n",
      "Epoch 5 | Step 3584600 | Avg Loss: 0.0151 | Grad Norm: 0.00854965\n",
      "Epoch 5 | Step 3584700 | Avg Loss: 0.0155 | Grad Norm: 0.00894516\n",
      "Epoch 5 | Step 3584800 | Avg Loss: 0.0153 | Grad Norm: 0.00798211\n",
      "Epoch 5 | Step 3584900 | Avg Loss: 0.0153 | Grad Norm: 0.00945101\n",
      "Epoch 5 | Step 3585000 | Avg Loss: 0.0151 | Grad Norm: 0.00883684\n",
      "Epoch 5 | Step 3585100 | Avg Loss: 0.0150 | Grad Norm: 0.00858348\n",
      "Epoch 5 | Step 3585200 | Avg Loss: 0.0151 | Grad Norm: 0.00949359\n",
      "Epoch 5 | Step 3585300 | Avg Loss: 0.0151 | Grad Norm: 0.00958843\n",
      "Epoch 5 | Step 3585400 | Avg Loss: 0.0154 | Grad Norm: 0.00842835\n",
      "Epoch 5 | Step 3585500 | Avg Loss: 0.0151 | Grad Norm: 0.01024804\n",
      "Epoch 5 | Step 3585600 | Avg Loss: 0.0148 | Grad Norm: 0.00929883\n",
      "Epoch 5 | Step 3585700 | Avg Loss: 0.0148 | Grad Norm: 0.00922245\n",
      "Epoch 5 | Step 3585800 | Avg Loss: 0.0149 | Grad Norm: 0.00940093\n",
      "Epoch 5 | Step 3585900 | Avg Loss: 0.0148 | Grad Norm: 0.00984726\n",
      "Epoch 5 | Step 3586000 | Avg Loss: 0.0151 | Grad Norm: 0.00854972\n",
      "Epoch 5 | Step 3586100 | Avg Loss: 0.0150 | Grad Norm: 0.00830092\n",
      "Epoch 5 | Step 3586200 | Avg Loss: 0.0150 | Grad Norm: 0.00786749\n",
      "Epoch 5 | Step 3586300 | Avg Loss: 0.0148 | Grad Norm: 0.01091289\n",
      "Epoch 5 | Step 3586400 | Avg Loss: 0.0151 | Grad Norm: 0.00896210\n",
      "Epoch 5 | Step 3586500 | Avg Loss: 0.0147 | Grad Norm: 0.00888185\n",
      "Epoch 5 | Step 3586600 | Avg Loss: 0.0148 | Grad Norm: 0.00836986\n",
      "Epoch 5 | Step 3586700 | Avg Loss: 0.0150 | Grad Norm: 0.00980570\n",
      "Epoch 5 | Step 3586800 | Avg Loss: 0.0151 | Grad Norm: 0.00853759\n",
      "Epoch 5 | Step 3586900 | Avg Loss: 0.0148 | Grad Norm: 0.00841350\n",
      "Epoch 5 | Step 3587000 | Avg Loss: 0.0148 | Grad Norm: 0.01078923\n",
      "Epoch 5 | Step 3587100 | Avg Loss: 0.0149 | Grad Norm: 0.00891137\n",
      "Epoch 5 | Step 3587200 | Avg Loss: 0.0155 | Grad Norm: 0.00864089\n",
      "Epoch 5 | Step 3587300 | Avg Loss: 0.0151 | Grad Norm: 0.01115166\n",
      "Epoch 5 | Step 3587400 | Avg Loss: 0.0148 | Grad Norm: 0.00867245\n",
      "Epoch 5 | Step 3587500 | Avg Loss: 0.0152 | Grad Norm: 0.00875261\n",
      "Epoch 5 | Step 3587600 | Avg Loss: 0.0152 | Grad Norm: 0.00911183\n",
      "Epoch 5 | Step 3587700 | Avg Loss: 0.0153 | Grad Norm: 0.00881216\n",
      "Epoch 5 | Step 3587800 | Avg Loss: 0.0150 | Grad Norm: 0.00736919\n",
      "Epoch 5 | Step 3587900 | Avg Loss: 0.0155 | Grad Norm: 0.00942492\n",
      "Epoch 5 | Step 3588000 | Avg Loss: 0.0157 | Grad Norm: 0.00901970\n",
      "Epoch 5 | Step 3588100 | Avg Loss: 0.0156 | Grad Norm: 0.00960913\n",
      "Epoch 5 | Step 3588200 | Avg Loss: 0.0156 | Grad Norm: 0.00911394\n",
      "Epoch 5 | Step 3588300 | Avg Loss: 0.0157 | Grad Norm: 0.00983395\n",
      "Epoch 5 | Step 3588400 | Avg Loss: 0.0157 | Grad Norm: 0.00939811\n",
      "Epoch 5 | Step 3588500 | Avg Loss: 0.0159 | Grad Norm: 0.01053185\n",
      "Epoch 5 | Step 3588600 | Avg Loss: 0.0155 | Grad Norm: 0.00924032\n",
      "Epoch 5 | Step 3588700 | Avg Loss: 0.0154 | Grad Norm: 0.00824880\n",
      "Epoch 5 | Step 3588800 | Avg Loss: 0.0157 | Grad Norm: 0.00976816\n",
      "Epoch 5 | Step 3588900 | Avg Loss: 0.0153 | Grad Norm: 0.00767624\n",
      "Epoch 5 | Step 3589000 | Avg Loss: 0.0149 | Grad Norm: 0.00857854\n",
      "Epoch 5 | Step 3589100 | Avg Loss: 0.0153 | Grad Norm: 0.00890592\n",
      "Epoch 5 | Step 3589200 | Avg Loss: 0.0156 | Grad Norm: 0.00990760\n",
      "Epoch 5 | Step 3589300 | Avg Loss: 0.0156 | Grad Norm: 0.00826040\n",
      "Epoch 5 | Step 3589400 | Avg Loss: 0.0157 | Grad Norm: 0.00817639\n",
      "Epoch 5 | Step 3589500 | Avg Loss: 0.0154 | Grad Norm: 0.00816174\n",
      "Epoch 5 | Step 3589600 | Avg Loss: 0.0153 | Grad Norm: 0.00800640\n",
      "Epoch 5 | Step 3589700 | Avg Loss: 0.0155 | Grad Norm: 0.00921373\n",
      "Epoch 5 | Step 3589800 | Avg Loss: 0.0153 | Grad Norm: 0.01139412\n",
      "Epoch 5 | Step 3589900 | Avg Loss: 0.0155 | Grad Norm: 0.00975388\n",
      "Epoch 5 | Step 3590000 | Avg Loss: 0.0158 | Grad Norm: 0.00952618\n",
      "Epoch 5 | Step 3590100 | Avg Loss: 0.0156 | Grad Norm: 0.01039199\n",
      "Epoch 5 | Step 3590200 | Avg Loss: 0.0156 | Grad Norm: 0.00872347\n",
      "Epoch 5 | Step 3590300 | Avg Loss: 0.0156 | Grad Norm: 0.00864856\n",
      "Epoch 5 | Step 3590400 | Avg Loss: 0.0156 | Grad Norm: 0.00926181\n",
      "Epoch 5 | Step 3590500 | Avg Loss: 0.0157 | Grad Norm: 0.00867643\n",
      "Epoch 5 | Step 3590600 | Avg Loss: 0.0157 | Grad Norm: 0.01003786\n",
      "Epoch 5 | Step 3590700 | Avg Loss: 0.0157 | Grad Norm: 0.01171153\n",
      "Epoch 5 | Step 3590800 | Avg Loss: 0.0156 | Grad Norm: 0.01098588\n",
      "Epoch 5 | Step 3590900 | Avg Loss: 0.0155 | Grad Norm: 0.00956177\n",
      "Epoch 5 | Step 3591000 | Avg Loss: 0.0159 | Grad Norm: 0.00900378\n",
      "Epoch 5 | Step 3591100 | Avg Loss: 0.0159 | Grad Norm: 0.00894680\n",
      "Epoch 5 | Step 3591200 | Avg Loss: 0.0159 | Grad Norm: 0.00868543\n",
      "Epoch 5 | Step 3591300 | Avg Loss: 0.0156 | Grad Norm: 0.00871965\n",
      "Epoch 5 | Step 3591400 | Avg Loss: 0.0158 | Grad Norm: 0.01021173\n",
      "Epoch 5 | Step 3591500 | Avg Loss: 0.0155 | Grad Norm: 0.00978323\n",
      "Epoch 5 | Step 3591600 | Avg Loss: 0.0155 | Grad Norm: 0.00948187\n",
      "Epoch 5 | Step 3591700 | Avg Loss: 0.0154 | Grad Norm: 0.00877461\n",
      "Epoch 5 | Step 3591800 | Avg Loss: 0.0150 | Grad Norm: 0.00981414\n",
      "Epoch 5 | Step 3591900 | Avg Loss: 0.0155 | Grad Norm: 0.00889561\n",
      "Epoch 5 | Step 3592000 | Avg Loss: 0.0155 | Grad Norm: 0.00895198\n",
      "Epoch 5 | Step 3592100 | Avg Loss: 0.0153 | Grad Norm: 0.00867335\n",
      "Epoch 5 | Step 3592200 | Avg Loss: 0.0155 | Grad Norm: 0.01057853\n",
      "Epoch 5 | Step 3592300 | Avg Loss: 0.0156 | Grad Norm: 0.00928642\n",
      "Epoch 5 | Step 3592400 | Avg Loss: 0.0154 | Grad Norm: 0.00789794\n",
      "Epoch 5 | Step 3592500 | Avg Loss: 0.0155 | Grad Norm: 0.00909191\n",
      "Epoch 5 | Step 3592600 | Avg Loss: 0.0154 | Grad Norm: 0.00886521\n",
      "Epoch 5 | Step 3592700 | Avg Loss: 0.0155 | Grad Norm: 0.00933995\n",
      "Epoch 5 | Step 3592800 | Avg Loss: 0.0155 | Grad Norm: 0.00969812\n",
      "Epoch 5 | Step 3592900 | Avg Loss: 0.0157 | Grad Norm: 0.00821902\n",
      "Epoch 5 | Step 3593000 | Avg Loss: 0.0154 | Grad Norm: 0.00966678\n",
      "Epoch 5 | Step 3593100 | Avg Loss: 0.0153 | Grad Norm: 0.00908864\n",
      "Epoch 5 | Step 3593200 | Avg Loss: 0.0153 | Grad Norm: 0.00848776\n",
      "Epoch 5 | Step 3593300 | Avg Loss: 0.0154 | Grad Norm: 0.00960539\n",
      "Epoch 5 | Step 3593400 | Avg Loss: 0.0156 | Grad Norm: 0.00923549\n",
      "Epoch 5 | Step 3593500 | Avg Loss: 0.0157 | Grad Norm: 0.01064770\n",
      "Epoch 5 | Step 3593600 | Avg Loss: 0.0155 | Grad Norm: 0.00784924\n",
      "Epoch 5 | Step 3593700 | Avg Loss: 0.0155 | Grad Norm: 0.01047792\n",
      "Epoch 5 | Step 3593800 | Avg Loss: 0.0152 | Grad Norm: 0.00908020\n",
      "Epoch 5 | Step 3593900 | Avg Loss: 0.0153 | Grad Norm: 0.00947839\n",
      "Epoch 5 | Step 3594000 | Avg Loss: 0.0151 | Grad Norm: 0.00880873\n",
      "Epoch 5 | Step 3594100 | Avg Loss: 0.0156 | Grad Norm: 0.00907102\n",
      "Epoch 5 | Step 3594200 | Avg Loss: 0.0154 | Grad Norm: 0.00865640\n",
      "Epoch 5 | Step 3594300 | Avg Loss: 0.0155 | Grad Norm: 0.00850374\n",
      "Epoch 5 | Step 3594400 | Avg Loss: 0.0157 | Grad Norm: 0.00909756\n",
      "Epoch 5 | Step 3594500 | Avg Loss: 0.0155 | Grad Norm: 0.00934182\n",
      "Epoch 5 | Step 3594600 | Avg Loss: 0.0153 | Grad Norm: 0.01092235\n",
      "Epoch 5 | Step 3594700 | Avg Loss: 0.0157 | Grad Norm: 0.01041363\n",
      "Epoch 5 | Step 3594800 | Avg Loss: 0.0156 | Grad Norm: 0.00991269\n",
      "Epoch 5 | Step 3594900 | Avg Loss: 0.0156 | Grad Norm: 0.00895859\n",
      "Epoch 5 | Step 3595000 | Avg Loss: 0.0153 | Grad Norm: 0.01338482\n",
      "Epoch 5 | Step 3595100 | Avg Loss: 0.0158 | Grad Norm: 0.00956627\n",
      "Epoch 5 | Step 3595200 | Avg Loss: 0.0155 | Grad Norm: 0.00839454\n",
      "Epoch 5 | Step 3595300 | Avg Loss: 0.0154 | Grad Norm: 0.01037989\n",
      "Epoch 5 | Step 3595400 | Avg Loss: 0.0154 | Grad Norm: 0.00952873\n",
      "Epoch 5 | Step 3595500 | Avg Loss: 0.0153 | Grad Norm: 0.00856461\n",
      "Epoch 5 | Step 3595600 | Avg Loss: 0.0152 | Grad Norm: 0.01026042\n",
      "Epoch 5 | Step 3595700 | Avg Loss: 0.0153 | Grad Norm: 0.01194486\n",
      "Epoch 5 | Step 3595800 | Avg Loss: 0.0152 | Grad Norm: 0.00913079\n",
      "Epoch 5 | Step 3595900 | Avg Loss: 0.0149 | Grad Norm: 0.00990626\n",
      "Epoch 5 | Step 3596000 | Avg Loss: 0.0150 | Grad Norm: 0.00829234\n",
      "Epoch 5 | Step 3596100 | Avg Loss: 0.0150 | Grad Norm: 0.00905780\n",
      "Epoch 5 | Step 3596200 | Avg Loss: 0.0153 | Grad Norm: 0.00815194\n",
      "Epoch 5 | Step 3596300 | Avg Loss: 0.0153 | Grad Norm: 0.00869078\n",
      "Epoch 5 | Step 3596400 | Avg Loss: 0.0154 | Grad Norm: 0.00926521\n",
      "Epoch 5 | Step 3596500 | Avg Loss: 0.0153 | Grad Norm: 0.00878828\n",
      "Epoch 5 | Step 3596600 | Avg Loss: 0.0153 | Grad Norm: 0.00825721\n",
      "Epoch 5 | Step 3596700 | Avg Loss: 0.0152 | Grad Norm: 0.00880957\n",
      "Epoch 5 | Step 3596800 | Avg Loss: 0.0155 | Grad Norm: 0.01154640\n",
      "Epoch 5 | Step 3596900 | Avg Loss: 0.0159 | Grad Norm: 0.00844649\n",
      "Epoch 5 | Step 3597000 | Avg Loss: 0.0157 | Grad Norm: 0.00949964\n",
      "Epoch 5 | Step 3597100 | Avg Loss: 0.0152 | Grad Norm: 0.00837129\n",
      "Epoch 5 | Step 3597200 | Avg Loss: 0.0151 | Grad Norm: 0.01479042\n",
      "Epoch 5 | Step 3597300 | Avg Loss: 0.0151 | Grad Norm: 0.00885304\n",
      "Epoch 5 | Step 3597400 | Avg Loss: 0.0151 | Grad Norm: 0.00768443\n",
      "Epoch 5 | Step 3597500 | Avg Loss: 0.0154 | Grad Norm: 0.00914252\n",
      "Epoch 5 | Step 3597600 | Avg Loss: 0.0157 | Grad Norm: 0.00809468\n",
      "Epoch 5 | Step 3597700 | Avg Loss: 0.0155 | Grad Norm: 0.00864350\n",
      "Epoch 5 | Step 3597800 | Avg Loss: 0.0154 | Grad Norm: 0.00863101\n",
      "Epoch 5 | Step 3597900 | Avg Loss: 0.0156 | Grad Norm: 0.00856027\n",
      "Epoch 5 | Step 3598000 | Avg Loss: 0.0156 | Grad Norm: 0.00862656\n",
      "Epoch 5 | Step 3598100 | Avg Loss: 0.0154 | Grad Norm: 0.00798168\n",
      "Epoch 5 | Step 3598200 | Avg Loss: 0.0151 | Grad Norm: 0.00842863\n",
      "Epoch 5 | Step 3598300 | Avg Loss: 0.0152 | Grad Norm: 0.00941367\n",
      "Epoch 5 | Step 3598400 | Avg Loss: 0.0153 | Grad Norm: 0.01001605\n",
      "Epoch 5 | Step 3598500 | Avg Loss: 0.0155 | Grad Norm: 0.00907848\n",
      "Epoch 5 | Step 3598600 | Avg Loss: 0.0151 | Grad Norm: 0.00772227\n",
      "Epoch 5 | Step 3598700 | Avg Loss: 0.0155 | Grad Norm: 0.01013359\n",
      "Epoch 5 | Step 3598800 | Avg Loss: 0.0155 | Grad Norm: 0.00843533\n",
      "Epoch 5 | Step 3598900 | Avg Loss: 0.0158 | Grad Norm: 0.00824785\n",
      "Epoch 5 | Step 3599000 | Avg Loss: 0.0156 | Grad Norm: 0.00927871\n",
      "Epoch 5 | Step 3599100 | Avg Loss: 0.0156 | Grad Norm: 0.00815196\n",
      "Epoch 5 | Step 3599200 | Avg Loss: 0.0158 | Grad Norm: 0.00795624\n",
      "Epoch 5 | Step 3599300 | Avg Loss: 0.0157 | Grad Norm: 0.00819487\n",
      "Epoch 5 | Step 3599400 | Avg Loss: 0.0162 | Grad Norm: 0.00954707\n",
      "Epoch 5 | Step 3599500 | Avg Loss: 0.0162 | Grad Norm: 0.00935512\n",
      "Epoch 5 | Step 3599600 | Avg Loss: 0.0159 | Grad Norm: 0.00978198\n",
      "Epoch 5 | Step 3599700 | Avg Loss: 0.0160 | Grad Norm: 0.00893517\n",
      "Epoch 5 | Step 3599800 | Avg Loss: 0.0159 | Grad Norm: 0.00898373\n",
      "Epoch 5 | Step 3599900 | Avg Loss: 0.0158 | Grad Norm: 0.00980591\n",
      "Epoch 5 | Step 3600000 | Avg Loss: 0.0159 | Grad Norm: 0.00854384\n",
      "Saving model at step3600000\n",
      "Epoch 5 | Step 3600100 | Avg Loss: 0.0160 | Grad Norm: 0.01013920\n",
      "Epoch 5 | Step 3600200 | Avg Loss: 0.0158 | Grad Norm: 0.00969588\n",
      "Epoch 5 | Step 3600300 | Avg Loss: 0.0157 | Grad Norm: 0.01045720\n",
      "Epoch 5 | Step 3600400 | Avg Loss: 0.0153 | Grad Norm: 0.01016145\n",
      "Epoch 5 | Step 3600500 | Avg Loss: 0.0150 | Grad Norm: 0.00859675\n",
      "Epoch 5 | Step 3600600 | Avg Loss: 0.0149 | Grad Norm: 0.01148931\n",
      "Epoch 5 | Step 3600700 | Avg Loss: 0.0150 | Grad Norm: 0.00905592\n",
      "Epoch 5 | Step 3600800 | Avg Loss: 0.0147 | Grad Norm: 0.00949497\n",
      "Epoch 5 | Step 3600900 | Avg Loss: 0.0151 | Grad Norm: 0.00845956\n",
      "Epoch 5 | Step 3601000 | Avg Loss: 0.0150 | Grad Norm: 0.00812423\n",
      "Epoch 5 | Step 3601100 | Avg Loss: 0.0149 | Grad Norm: 0.00823135\n",
      "Epoch 5 | Step 3601200 | Avg Loss: 0.0150 | Grad Norm: 0.00895461\n",
      "Epoch 5 | Step 3601300 | Avg Loss: 0.0152 | Grad Norm: 0.00738055\n",
      "Epoch 5 | Step 3601400 | Avg Loss: 0.0156 | Grad Norm: 0.00929501\n",
      "Epoch 5 | Step 3601500 | Avg Loss: 0.0157 | Grad Norm: 0.01182891\n",
      "Epoch 5 | Step 3601600 | Avg Loss: 0.0159 | Grad Norm: 0.00928024\n",
      "Epoch 5 | Step 3601700 | Avg Loss: 0.0155 | Grad Norm: 0.00791255\n",
      "Epoch 5 | Step 3601800 | Avg Loss: 0.0156 | Grad Norm: 0.00951575\n",
      "Epoch 5 | Step 3601900 | Avg Loss: 0.0155 | Grad Norm: 0.00893285\n",
      "Epoch 5 | Step 3602000 | Avg Loss: 0.0158 | Grad Norm: 0.00926692\n",
      "Epoch 5 | Step 3602100 | Avg Loss: 0.0151 | Grad Norm: 0.00860501\n",
      "Epoch 5 | Step 3602200 | Avg Loss: 0.0154 | Grad Norm: 0.00800613\n",
      "Epoch 5 | Step 3602300 | Avg Loss: 0.0154 | Grad Norm: 0.00825480\n",
      "Epoch 5 | Step 3602400 | Avg Loss: 0.0156 | Grad Norm: 0.00987696\n",
      "Epoch 5 | Step 3602500 | Avg Loss: 0.0157 | Grad Norm: 0.00871374\n",
      "Epoch 5 | Step 3602600 | Avg Loss: 0.0158 | Grad Norm: 0.00797420\n",
      "Epoch 5 | Step 3602700 | Avg Loss: 0.0158 | Grad Norm: 0.00828775\n",
      "Epoch 5 | Step 3602800 | Avg Loss: 0.0157 | Grad Norm: 0.00984453\n",
      "Epoch 5 | Step 3602900 | Avg Loss: 0.0157 | Grad Norm: 0.00957164\n",
      "Epoch 5 | Step 3603000 | Avg Loss: 0.0160 | Grad Norm: 0.01102176\n",
      "Epoch 5 | Step 3603100 | Avg Loss: 0.0161 | Grad Norm: 0.00967516\n",
      "Epoch 5 | Step 3603200 | Avg Loss: 0.0159 | Grad Norm: 0.00853457\n",
      "Epoch 5 | Step 3603300 | Avg Loss: 0.0160 | Grad Norm: 0.00940124\n",
      "Epoch 5 | Step 3603400 | Avg Loss: 0.0158 | Grad Norm: 0.01048507\n",
      "Epoch 5 | Step 3603500 | Avg Loss: 0.0160 | Grad Norm: 0.00988577\n",
      "Epoch 5 | Step 3603600 | Avg Loss: 0.0160 | Grad Norm: 0.00888997\n",
      "Epoch 5 | Step 3603700 | Avg Loss: 0.0157 | Grad Norm: 0.00954781\n",
      "Epoch 5 | Step 3603800 | Avg Loss: 0.0155 | Grad Norm: 0.00904962\n",
      "Epoch 5 | Step 3603900 | Avg Loss: 0.0153 | Grad Norm: 0.00873892\n",
      "Epoch 5 | Step 3604000 | Avg Loss: 0.0155 | Grad Norm: 0.00949025\n",
      "Epoch 5 | Step 3604100 | Avg Loss: 0.0155 | Grad Norm: 0.00858614\n",
      "Epoch 5 | Step 3604200 | Avg Loss: 0.0152 | Grad Norm: 0.00820044\n",
      "Epoch 5 | Step 3604300 | Avg Loss: 0.0151 | Grad Norm: 0.01054942\n",
      "Epoch 5 | Step 3604400 | Avg Loss: 0.0154 | Grad Norm: 0.00874439\n",
      "Epoch 5 | Step 3604500 | Avg Loss: 0.0159 | Grad Norm: 0.00958158\n",
      "Epoch 5 | Step 3604600 | Avg Loss: 0.0156 | Grad Norm: 0.00911628\n",
      "Epoch 5 | Step 3604700 | Avg Loss: 0.0160 | Grad Norm: 0.01010191\n",
      "Epoch 5 | Step 3604800 | Avg Loss: 0.0159 | Grad Norm: 0.01071475\n",
      "Epoch 5 | Step 3604900 | Avg Loss: 0.0160 | Grad Norm: 0.00947377\n",
      "Epoch 5 | Step 3605000 | Avg Loss: 0.0163 | Grad Norm: 0.01001058\n",
      "Epoch 5 | Step 3605100 | Avg Loss: 0.0163 | Grad Norm: 0.00877911\n",
      "Epoch 5 | Step 3605200 | Avg Loss: 0.0161 | Grad Norm: 0.01211578\n",
      "Epoch 5 | Step 3605300 | Avg Loss: 0.0159 | Grad Norm: 0.01073302\n",
      "Epoch 5 | Step 3605400 | Avg Loss: 0.0157 | Grad Norm: 0.01074782\n",
      "Epoch 5 | Step 3605500 | Avg Loss: 0.0155 | Grad Norm: 0.00784226\n",
      "Epoch 5 | Step 3605600 | Avg Loss: 0.0153 | Grad Norm: 0.01029112\n",
      "Epoch 5 | Step 3605700 | Avg Loss: 0.0150 | Grad Norm: 0.00996410\n",
      "Epoch 5 | Step 3605800 | Avg Loss: 0.0153 | Grad Norm: 0.00850896\n",
      "Epoch 5 | Step 3605900 | Avg Loss: 0.0154 | Grad Norm: 0.00817866\n",
      "Epoch 5 | Step 3606000 | Avg Loss: 0.0155 | Grad Norm: 0.01089573\n",
      "Epoch 5 | Step 3606100 | Avg Loss: 0.0158 | Grad Norm: 0.00940584\n",
      "Epoch 5 | Step 3606200 | Avg Loss: 0.0160 | Grad Norm: 0.00892822\n",
      "Epoch 5 | Step 3606300 | Avg Loss: 0.0162 | Grad Norm: 0.01135177\n",
      "Epoch 5 | Step 3606400 | Avg Loss: 0.0162 | Grad Norm: 0.00984155\n",
      "Epoch 5 | Step 3606500 | Avg Loss: 0.0160 | Grad Norm: 0.00993364\n",
      "Epoch 5 | Step 3606600 | Avg Loss: 0.0158 | Grad Norm: 0.00913882\n",
      "Epoch 5 | Step 3606700 | Avg Loss: 0.0156 | Grad Norm: 0.00880666\n",
      "Epoch 5 | Step 3606800 | Avg Loss: 0.0158 | Grad Norm: 0.00895868\n",
      "Epoch 5 | Step 3606900 | Avg Loss: 0.0153 | Grad Norm: 0.00907768\n",
      "Epoch 5 | Step 3607000 | Avg Loss: 0.0149 | Grad Norm: 0.00734851\n",
      "Epoch 5 | Step 3607100 | Avg Loss: 0.0153 | Grad Norm: 0.00953316\n",
      "Epoch 5 | Step 3607200 | Avg Loss: 0.0154 | Grad Norm: 0.00951430\n",
      "Epoch 5 | Step 3607300 | Avg Loss: 0.0149 | Grad Norm: 0.00927313\n",
      "Epoch 5 | Step 3607400 | Avg Loss: 0.0153 | Grad Norm: 0.00837356\n",
      "Epoch 5 | Step 3607500 | Avg Loss: 0.0157 | Grad Norm: 0.00881611\n",
      "Epoch 5 | Step 3607600 | Avg Loss: 0.0157 | Grad Norm: 0.01092370\n",
      "Epoch 5 | Step 3607700 | Avg Loss: 0.0160 | Grad Norm: 0.00895329\n",
      "Epoch 5 | Step 3607800 | Avg Loss: 0.0160 | Grad Norm: 0.00810442\n",
      "Epoch 5 | Step 3607900 | Avg Loss: 0.0158 | Grad Norm: 0.00779924\n",
      "Epoch 5 | Step 3608000 | Avg Loss: 0.0157 | Grad Norm: 0.01020055\n",
      "Epoch 5 | Step 3608100 | Avg Loss: 0.0155 | Grad Norm: 0.00881438\n",
      "Epoch 5 | Step 3608200 | Avg Loss: 0.0157 | Grad Norm: 0.00999104\n",
      "Epoch 5 | Step 3608300 | Avg Loss: 0.0157 | Grad Norm: 0.00873300\n",
      "Epoch 5 | Step 3608400 | Avg Loss: 0.0155 | Grad Norm: 0.00815347\n",
      "Epoch 5 | Step 3608500 | Avg Loss: 0.0153 | Grad Norm: 0.00978234\n",
      "Epoch 5 | Step 3608600 | Avg Loss: 0.0153 | Grad Norm: 0.00879100\n",
      "Epoch 5 | Step 3608700 | Avg Loss: 0.0151 | Grad Norm: 0.00774645\n",
      "Epoch 5 | Step 3608800 | Avg Loss: 0.0153 | Grad Norm: 0.00908569\n",
      "Epoch 5 | Step 3608900 | Avg Loss: 0.0151 | Grad Norm: 0.01169930\n",
      "Epoch 5 | Step 3609000 | Avg Loss: 0.0155 | Grad Norm: 0.00890887\n",
      "Epoch 5 | Step 3609100 | Avg Loss: 0.0155 | Grad Norm: 0.00883414\n",
      "Epoch 5 | Step 3609200 | Avg Loss: 0.0155 | Grad Norm: 0.00873850\n",
      "Epoch 5 | Step 3609300 | Avg Loss: 0.0155 | Grad Norm: 0.00980108\n",
      "Epoch 5 | Step 3609400 | Avg Loss: 0.0156 | Grad Norm: 0.00889656\n",
      "Epoch 5 | Step 3609500 | Avg Loss: 0.0156 | Grad Norm: 0.00875154\n",
      "Epoch 5 | Step 3609600 | Avg Loss: 0.0154 | Grad Norm: 0.01174842\n",
      "Epoch 5 | Step 3609700 | Avg Loss: 0.0154 | Grad Norm: 0.00863511\n",
      "Epoch 5 | Step 3609800 | Avg Loss: 0.0155 | Grad Norm: 0.01003613\n",
      "Epoch 5 | Step 3609900 | Avg Loss: 0.0155 | Grad Norm: 0.00939712\n",
      "Epoch 5 | Step 3610000 | Avg Loss: 0.0157 | Grad Norm: 0.00960967\n",
      "Epoch 5 | Step 3610100 | Avg Loss: 0.0154 | Grad Norm: 0.00942767\n",
      "Epoch 5 | Step 3610200 | Avg Loss: 0.0151 | Grad Norm: 0.00992200\n",
      "Epoch 5 | Step 3610300 | Avg Loss: 0.0150 | Grad Norm: 0.00936332\n",
      "Epoch 5 | Step 3610400 | Avg Loss: 0.0149 | Grad Norm: 0.00877918\n",
      "Epoch 5 | Step 3610500 | Avg Loss: 0.0154 | Grad Norm: 0.00891019\n",
      "Epoch 5 | Step 3610600 | Avg Loss: 0.0153 | Grad Norm: 0.00937693\n",
      "Epoch 5 | Step 3610700 | Avg Loss: 0.0150 | Grad Norm: 0.00857565\n",
      "Epoch 5 | Step 3610800 | Avg Loss: 0.0153 | Grad Norm: 0.00830031\n",
      "Epoch 5 | Step 3610900 | Avg Loss: 0.0151 | Grad Norm: 0.00940453\n",
      "Epoch 5 | Step 3611000 | Avg Loss: 0.0152 | Grad Norm: 0.00870564\n",
      "Epoch 5 | Step 3611100 | Avg Loss: 0.0153 | Grad Norm: 0.00788841\n",
      "Epoch 5 | Step 3611200 | Avg Loss: 0.0156 | Grad Norm: 0.00933447\n",
      "Epoch 5 | Step 3611300 | Avg Loss: 0.0155 | Grad Norm: 0.00938442\n",
      "Epoch 5 | Step 3611400 | Avg Loss: 0.0161 | Grad Norm: 0.00953428\n",
      "Epoch 5 | Step 3611500 | Avg Loss: 0.0156 | Grad Norm: 0.00936888\n",
      "Epoch 5 | Step 3611600 | Avg Loss: 0.0155 | Grad Norm: 0.00940880\n",
      "Epoch 5 | Step 3611700 | Avg Loss: 0.0154 | Grad Norm: 0.00862853\n",
      "Epoch 5 | Step 3611800 | Avg Loss: 0.0155 | Grad Norm: 0.00855078\n",
      "Epoch 5 | Step 3611900 | Avg Loss: 0.0154 | Grad Norm: 0.00860090\n",
      "Epoch 5 | Step 3612000 | Avg Loss: 0.0154 | Grad Norm: 0.00836390\n",
      "Epoch 5 | Step 3612100 | Avg Loss: 0.0158 | Grad Norm: 0.00840859\n",
      "Epoch 5 | Step 3612200 | Avg Loss: 0.0154 | Grad Norm: 0.00993422\n",
      "Epoch 5 | Step 3612300 | Avg Loss: 0.0155 | Grad Norm: 0.01017786\n",
      "Epoch 5 | Step 3612400 | Avg Loss: 0.0153 | Grad Norm: 0.00909444\n",
      "Epoch 5 | Step 3612500 | Avg Loss: 0.0150 | Grad Norm: 0.00894276\n",
      "Epoch 5 | Step 3612600 | Avg Loss: 0.0148 | Grad Norm: 0.01034698\n",
      "Epoch 5 | Step 3612700 | Avg Loss: 0.0146 | Grad Norm: 0.00882520\n",
      "Epoch 5 | Step 3612800 | Avg Loss: 0.0149 | Grad Norm: 0.00890677\n",
      "Epoch 5 | Step 3612900 | Avg Loss: 0.0151 | Grad Norm: 0.00876131\n",
      "Epoch 5 | Step 3613000 | Avg Loss: 0.0148 | Grad Norm: 0.00883043\n",
      "Epoch 5 | Step 3613100 | Avg Loss: 0.0151 | Grad Norm: 0.00924748\n",
      "Epoch 5 | Step 3613200 | Avg Loss: 0.0151 | Grad Norm: 0.00949544\n",
      "Epoch 5 | Step 3613300 | Avg Loss: 0.0152 | Grad Norm: 0.00939884\n",
      "Epoch 5 | Step 3613400 | Avg Loss: 0.0154 | Grad Norm: 0.01062282\n",
      "Epoch 5 | Step 3613500 | Avg Loss: 0.0154 | Grad Norm: 0.01070489\n",
      "Epoch 5 | Step 3613600 | Avg Loss: 0.0154 | Grad Norm: 0.00819170\n",
      "Epoch 5 | Step 3613700 | Avg Loss: 0.0156 | Grad Norm: 0.01165092\n",
      "Epoch 5 | Step 3613800 | Avg Loss: 0.0157 | Grad Norm: 0.00999965\n",
      "Epoch 5 | Step 3613900 | Avg Loss: 0.0161 | Grad Norm: 0.00934384\n",
      "Epoch 5 | Step 3614000 | Avg Loss: 0.0159 | Grad Norm: 0.00963382\n",
      "Epoch 5 | Step 3614100 | Avg Loss: 0.0158 | Grad Norm: 0.00869742\n",
      "Epoch 5 | Step 3614200 | Avg Loss: 0.0155 | Grad Norm: 0.00979868\n",
      "Epoch 5 | Step 3614300 | Avg Loss: 0.0155 | Grad Norm: 0.01115024\n",
      "Epoch 5 | Step 3614400 | Avg Loss: 0.0155 | Grad Norm: 0.01037227\n",
      "Epoch 5 | Step 3614500 | Avg Loss: 0.0156 | Grad Norm: 0.00813913\n",
      "Epoch 5 | Step 3614600 | Avg Loss: 0.0155 | Grad Norm: 0.00940376\n",
      "Epoch 5 | Step 3614700 | Avg Loss: 0.0158 | Grad Norm: 0.00885641\n",
      "Epoch 5 | Step 3614800 | Avg Loss: 0.0157 | Grad Norm: 0.00922533\n",
      "Epoch 5 | Step 3614900 | Avg Loss: 0.0158 | Grad Norm: 0.01073926\n",
      "Epoch 5 | Step 3615000 | Avg Loss: 0.0159 | Grad Norm: 0.00812182\n",
      "Epoch 5 | Step 3615100 | Avg Loss: 0.0162 | Grad Norm: 0.01016704\n",
      "Epoch 5 | Step 3615200 | Avg Loss: 0.0162 | Grad Norm: 0.01166776\n",
      "Epoch 5 | Step 3615300 | Avg Loss: 0.0163 | Grad Norm: 0.00996074\n",
      "Epoch 5 | Step 3615400 | Avg Loss: 0.0162 | Grad Norm: 0.00908281\n",
      "Epoch 5 | Step 3615500 | Avg Loss: 0.0160 | Grad Norm: 0.00845572\n",
      "Epoch 5 | Step 3615600 | Avg Loss: 0.0161 | Grad Norm: 0.00944012\n",
      "Epoch 5 | Step 3615700 | Avg Loss: 0.0159 | Grad Norm: 0.00927484\n",
      "Epoch 5 | Step 3615800 | Avg Loss: 0.0157 | Grad Norm: 0.00864089\n",
      "Epoch 5 | Step 3615900 | Avg Loss: 0.0155 | Grad Norm: 0.01127653\n",
      "Epoch 5 | Step 3616000 | Avg Loss: 0.0160 | Grad Norm: 0.00950414\n",
      "Epoch 5 | Step 3616100 | Avg Loss: 0.0157 | Grad Norm: 0.01113744\n",
      "Epoch 5 | Step 3616200 | Avg Loss: 0.0155 | Grad Norm: 0.00797373\n",
      "Epoch 5 | Step 3616300 | Avg Loss: 0.0153 | Grad Norm: 0.00962198\n",
      "Epoch 5 | Step 3616400 | Avg Loss: 0.0151 | Grad Norm: 0.00850480\n",
      "Epoch 5 | Step 3616500 | Avg Loss: 0.0156 | Grad Norm: 0.00855880\n",
      "Epoch 5 | Step 3616600 | Avg Loss: 0.0155 | Grad Norm: 0.00863866\n",
      "Epoch 5 | Step 3616700 | Avg Loss: 0.0157 | Grad Norm: 0.00938540\n",
      "Epoch 5 | Step 3616800 | Avg Loss: 0.0153 | Grad Norm: 0.00974969\n",
      "Epoch 5 | Step 3616900 | Avg Loss: 0.0152 | Grad Norm: 0.00773523\n",
      "Epoch 5 | Step 3617000 | Avg Loss: 0.0148 | Grad Norm: 0.00819059\n",
      "Epoch 5 | Step 3617100 | Avg Loss: 0.0153 | Grad Norm: 0.00979427\n",
      "Epoch 5 | Step 3617200 | Avg Loss: 0.0151 | Grad Norm: 0.01023210\n",
      "Epoch 5 | Step 3617300 | Avg Loss: 0.0151 | Grad Norm: 0.00977351\n",
      "Epoch 5 | Step 3617400 | Avg Loss: 0.0153 | Grad Norm: 0.00918227\n",
      "Epoch 5 | Step 3617500 | Avg Loss: 0.0154 | Grad Norm: 0.00848588\n",
      "Epoch 5 | Step 3617600 | Avg Loss: 0.0160 | Grad Norm: 0.00909654\n",
      "Epoch 5 | Step 3617700 | Avg Loss: 0.0158 | Grad Norm: 0.00875555\n",
      "Epoch 5 | Step 3617800 | Avg Loss: 0.0155 | Grad Norm: 0.00816468\n",
      "Epoch 5 | Step 3617900 | Avg Loss: 0.0154 | Grad Norm: 0.00917144\n",
      "Epoch 5 | Step 3618000 | Avg Loss: 0.0155 | Grad Norm: 0.00958604\n",
      "Epoch 5 | Step 3618100 | Avg Loss: 0.0157 | Grad Norm: 0.00949058\n",
      "Epoch 5 | Step 3618200 | Avg Loss: 0.0157 | Grad Norm: 0.01208883\n",
      "Epoch 5 | Step 3618300 | Avg Loss: 0.0157 | Grad Norm: 0.00891222\n",
      "Epoch 5 | Step 3618400 | Avg Loss: 0.0159 | Grad Norm: 0.00931448\n",
      "Epoch 5 | Step 3618500 | Avg Loss: 0.0160 | Grad Norm: 0.00896829\n",
      "Epoch 5 | Step 3618600 | Avg Loss: 0.0159 | Grad Norm: 0.00851766\n",
      "Epoch 5 | Step 3618700 | Avg Loss: 0.0158 | Grad Norm: 0.01022078\n",
      "Epoch 5 | Step 3618800 | Avg Loss: 0.0159 | Grad Norm: 0.01027401\n",
      "Epoch 5 | Step 3618900 | Avg Loss: 0.0156 | Grad Norm: 0.01064173\n",
      "Epoch 5 | Step 3619000 | Avg Loss: 0.0155 | Grad Norm: 0.00915242\n",
      "Epoch 5 | Step 3619100 | Avg Loss: 0.0153 | Grad Norm: 0.00993720\n",
      "Epoch 5 | Step 3619200 | Avg Loss: 0.0152 | Grad Norm: 0.00843858\n",
      "Epoch 5 | Step 3619300 | Avg Loss: 0.0154 | Grad Norm: 0.01079924\n",
      "Epoch 5 | Step 3619400 | Avg Loss: 0.0152 | Grad Norm: 0.01010490\n",
      "Epoch 5 | Step 3619500 | Avg Loss: 0.0152 | Grad Norm: 0.00869425\n",
      "Epoch 5 | Step 3619600 | Avg Loss: 0.0152 | Grad Norm: 0.00971244\n",
      "Epoch 5 | Step 3619700 | Avg Loss: 0.0151 | Grad Norm: 0.00930499\n",
      "Epoch 5 | Step 3619800 | Avg Loss: 0.0152 | Grad Norm: 0.00919934\n",
      "Epoch 5 | Step 3619900 | Avg Loss: 0.0159 | Grad Norm: 0.00999618\n",
      "Epoch 5 | Step 3620000 | Avg Loss: 0.0156 | Grad Norm: 0.00963787\n",
      "Epoch 5 | Step 3620100 | Avg Loss: 0.0159 | Grad Norm: 0.00963292\n",
      "Epoch 5 | Step 3620200 | Avg Loss: 0.0159 | Grad Norm: 0.01001452\n",
      "Epoch 5 | Step 3620300 | Avg Loss: 0.0158 | Grad Norm: 0.00883319\n",
      "Epoch 5 | Step 3620400 | Avg Loss: 0.0159 | Grad Norm: 0.01002718\n",
      "Epoch 5 | Step 3620500 | Avg Loss: 0.0159 | Grad Norm: 0.00830243\n",
      "Epoch 5 | Step 3620600 | Avg Loss: 0.0158 | Grad Norm: 0.00928849\n",
      "Epoch 5 | Step 3620700 | Avg Loss: 0.0159 | Grad Norm: 0.00778461\n",
      "Epoch 5 | Step 3620800 | Avg Loss: 0.0154 | Grad Norm: 0.00886998\n",
      "Epoch 5 | Step 3620900 | Avg Loss: 0.0154 | Grad Norm: 0.00855161\n",
      "Epoch 5 | Step 3621000 | Avg Loss: 0.0153 | Grad Norm: 0.00901455\n",
      "Epoch 5 | Step 3621100 | Avg Loss: 0.0152 | Grad Norm: 0.00894538\n",
      "Epoch 5 | Step 3621200 | Avg Loss: 0.0155 | Grad Norm: 0.00865541\n",
      "Epoch 5 | Step 3621300 | Avg Loss: 0.0152 | Grad Norm: 0.00853462\n",
      "Epoch 5 | Step 3621400 | Avg Loss: 0.0151 | Grad Norm: 0.00920546\n",
      "Epoch 5 | Step 3621500 | Avg Loss: 0.0149 | Grad Norm: 0.00995269\n",
      "Epoch 5 | Step 3621600 | Avg Loss: 0.0153 | Grad Norm: 0.00846103\n",
      "Epoch 5 | Step 3621700 | Avg Loss: 0.0159 | Grad Norm: 0.00898606\n",
      "Epoch 5 | Step 3621800 | Avg Loss: 0.0160 | Grad Norm: 0.00956896\n",
      "Epoch 5 | Step 3621900 | Avg Loss: 0.0160 | Grad Norm: 0.00786084\n",
      "Epoch 5 | Step 3622000 | Avg Loss: 0.0153 | Grad Norm: 0.00832426\n",
      "Epoch 5 | Step 3622100 | Avg Loss: 0.0151 | Grad Norm: 0.00870101\n",
      "Epoch 5 | Step 3622200 | Avg Loss: 0.0156 | Grad Norm: 0.00928673\n",
      "Epoch 5 | Step 3622300 | Avg Loss: 0.0153 | Grad Norm: 0.00974864\n",
      "Epoch 5 | Step 3622400 | Avg Loss: 0.0153 | Grad Norm: 0.01012721\n",
      "Epoch 5 | Step 3622500 | Avg Loss: 0.0152 | Grad Norm: 0.01000024\n",
      "Epoch 5 | Step 3622600 | Avg Loss: 0.0154 | Grad Norm: 0.00881786\n",
      "Epoch 5 | Step 3622700 | Avg Loss: 0.0150 | Grad Norm: 0.00878099\n",
      "Epoch 5 | Step 3622800 | Avg Loss: 0.0149 | Grad Norm: 0.00921062\n",
      "Epoch 5 | Step 3622900 | Avg Loss: 0.0148 | Grad Norm: 0.00997363\n",
      "Epoch 5 | Step 3623000 | Avg Loss: 0.0148 | Grad Norm: 0.00824512\n",
      "Epoch 5 | Step 3623100 | Avg Loss: 0.0149 | Grad Norm: 0.00753286\n",
      "Epoch 5 | Step 3623200 | Avg Loss: 0.0149 | Grad Norm: 0.00975695\n",
      "Epoch 5 | Step 3623300 | Avg Loss: 0.0148 | Grad Norm: 0.00957795\n",
      "Epoch 5 | Step 3623400 | Avg Loss: 0.0151 | Grad Norm: 0.00862704\n",
      "Epoch 5 | Step 3623500 | Avg Loss: 0.0151 | Grad Norm: 0.00790791\n",
      "Epoch 5 | Step 3623600 | Avg Loss: 0.0153 | Grad Norm: 0.00825948\n",
      "Epoch 5 | Step 3623700 | Avg Loss: 0.0156 | Grad Norm: 0.00881157\n",
      "Epoch 5 | Step 3623800 | Avg Loss: 0.0160 | Grad Norm: 0.00994136\n",
      "Epoch 5 | Step 3623900 | Avg Loss: 0.0155 | Grad Norm: 0.01153373\n",
      "Epoch 5 | Step 3624000 | Avg Loss: 0.0155 | Grad Norm: 0.00951343\n",
      "Epoch 5 | Step 3624100 | Avg Loss: 0.0158 | Grad Norm: 0.01035452\n",
      "Epoch 5 | Step 3624200 | Avg Loss: 0.0160 | Grad Norm: 0.00900260\n",
      "Epoch 5 | Step 3624300 | Avg Loss: 0.0157 | Grad Norm: 0.00934366\n",
      "Epoch 5 | Step 3624400 | Avg Loss: 0.0155 | Grad Norm: 0.00815342\n",
      "Epoch 5 | Step 3624500 | Avg Loss: 0.0157 | Grad Norm: 0.00962653\n",
      "Epoch 5 | Step 3624600 | Avg Loss: 0.0157 | Grad Norm: 0.00864022\n",
      "Epoch 5 | Step 3624700 | Avg Loss: 0.0156 | Grad Norm: 0.00889997\n",
      "Epoch 5 | Step 3624800 | Avg Loss: 0.0160 | Grad Norm: 0.00894926\n",
      "Epoch 5 | Step 3624900 | Avg Loss: 0.0160 | Grad Norm: 0.00914089\n",
      "Epoch 5 | Step 3625000 | Avg Loss: 0.0165 | Grad Norm: 0.00901663\n",
      "Epoch 5 | Step 3625100 | Avg Loss: 0.0159 | Grad Norm: 0.00873885\n",
      "Epoch 5 | Step 3625200 | Avg Loss: 0.0157 | Grad Norm: 0.01065568\n",
      "Epoch 5 | Step 3625300 | Avg Loss: 0.0158 | Grad Norm: 0.00949627\n",
      "Epoch 5 | Step 3625400 | Avg Loss: 0.0159 | Grad Norm: 0.00964981\n",
      "Epoch 5 | Step 3625500 | Avg Loss: 0.0156 | Grad Norm: 0.00901109\n",
      "Epoch 5 | Step 3625600 | Avg Loss: 0.0155 | Grad Norm: 0.00946622\n",
      "Epoch 5 | Step 3625700 | Avg Loss: 0.0157 | Grad Norm: 0.01026351\n",
      "Epoch 5 | Step 3625800 | Avg Loss: 0.0155 | Grad Norm: 0.00988650\n",
      "Epoch 5 | Step 3625900 | Avg Loss: 0.0152 | Grad Norm: 0.01094270\n",
      "Epoch 5 | Step 3626000 | Avg Loss: 0.0155 | Grad Norm: 0.01027643\n",
      "Epoch 5 | Step 3626100 | Avg Loss: 0.0155 | Grad Norm: 0.00932648\n",
      "Epoch 5 | Step 3626200 | Avg Loss: 0.0155 | Grad Norm: 0.00914078\n",
      "Epoch 5 | Step 3626300 | Avg Loss: 0.0155 | Grad Norm: 0.01092743\n",
      "Epoch 5 | Step 3626400 | Avg Loss: 0.0156 | Grad Norm: 0.01362103\n",
      "Epoch 5 | Step 3626500 | Avg Loss: 0.0154 | Grad Norm: 0.00897586\n",
      "Epoch 5 | Step 3626600 | Avg Loss: 0.0152 | Grad Norm: 0.00796747\n",
      "Epoch 5 | Step 3626700 | Avg Loss: 0.0149 | Grad Norm: 0.00858150\n",
      "Epoch 5 | Step 3626800 | Avg Loss: 0.0144 | Grad Norm: 0.01091124\n",
      "Epoch 5 | Step 3626900 | Avg Loss: 0.0142 | Grad Norm: 0.00800350\n",
      "Epoch 5 | Step 3627000 | Avg Loss: 0.0145 | Grad Norm: 0.00846034\n",
      "Epoch 5 | Step 3627100 | Avg Loss: 0.0144 | Grad Norm: 0.01007063\n",
      "Epoch 5 | Step 3627200 | Avg Loss: 0.0145 | Grad Norm: 0.00989653\n",
      "Epoch 5 | Step 3627300 | Avg Loss: 0.0148 | Grad Norm: 0.00820055\n",
      "Epoch 5 | Step 3627400 | Avg Loss: 0.0151 | Grad Norm: 0.01032345\n",
      "Epoch 5 | Step 3627500 | Avg Loss: 0.0151 | Grad Norm: 0.00933357\n",
      "Epoch 5 | Step 3627600 | Avg Loss: 0.0150 | Grad Norm: 0.00985849\n",
      "Epoch 5 | Step 3627700 | Avg Loss: 0.0144 | Grad Norm: 0.00910080\n",
      "Epoch 5 | Step 3627800 | Avg Loss: 0.0144 | Grad Norm: 0.00748558\n",
      "Epoch 5 | Step 3627900 | Avg Loss: 0.0145 | Grad Norm: 0.01103491\n",
      "Epoch 5 | Step 3628000 | Avg Loss: 0.0148 | Grad Norm: 0.00871971\n",
      "Epoch 5 | Step 3628100 | Avg Loss: 0.0149 | Grad Norm: 0.00824095\n",
      "Epoch 5 | Step 3628200 | Avg Loss: 0.0150 | Grad Norm: 0.00952997\n",
      "Epoch 5 | Step 3628300 | Avg Loss: 0.0152 | Grad Norm: 0.00949134\n",
      "Epoch 5 | Step 3628400 | Avg Loss: 0.0149 | Grad Norm: 0.00957615\n",
      "Epoch 5 | Step 3628500 | Avg Loss: 0.0153 | Grad Norm: 0.00969148\n",
      "Epoch 5 | Step 3628600 | Avg Loss: 0.0156 | Grad Norm: 0.01071032\n",
      "Epoch 5 | Step 3628700 | Avg Loss: 0.0157 | Grad Norm: 0.00914162\n",
      "Epoch 5 | Step 3628800 | Avg Loss: 0.0158 | Grad Norm: 0.00876394\n",
      "Epoch 5 | Step 3628900 | Avg Loss: 0.0156 | Grad Norm: 0.00868090\n",
      "Epoch 5 | Step 3629000 | Avg Loss: 0.0156 | Grad Norm: 0.01080291\n",
      "Epoch 5 | Step 3629100 | Avg Loss: 0.0154 | Grad Norm: 0.00840009\n",
      "Epoch 5 | Step 3629200 | Avg Loss: 0.0157 | Grad Norm: 0.00917794\n",
      "Epoch 5 | Step 3629300 | Avg Loss: 0.0157 | Grad Norm: 0.00990415\n",
      "Epoch 5 | Step 3629400 | Avg Loss: 0.0154 | Grad Norm: 0.01020052\n",
      "Epoch 5 | Step 3629500 | Avg Loss: 0.0157 | Grad Norm: 0.01063265\n",
      "Epoch 5 | Step 3629600 | Avg Loss: 0.0156 | Grad Norm: 0.01001602\n",
      "Epoch 5 | Step 3629700 | Avg Loss: 0.0156 | Grad Norm: 0.00988695\n",
      "Epoch 5 | Step 3629800 | Avg Loss: 0.0161 | Grad Norm: 0.00897946\n",
      "Epoch 5 | Step 3629900 | Avg Loss: 0.0161 | Grad Norm: 0.00891797\n",
      "Epoch 5 | Step 3630000 | Avg Loss: 0.0157 | Grad Norm: 0.00981570\n",
      "Epoch 5 | Step 3630100 | Avg Loss: 0.0151 | Grad Norm: 0.00925975\n",
      "Epoch 5 | Step 3630200 | Avg Loss: 0.0153 | Grad Norm: 0.00903110\n",
      "Epoch 5 | Step 3630300 | Avg Loss: 0.0153 | Grad Norm: 0.00856639\n",
      "Epoch 5 | Step 3630400 | Avg Loss: 0.0155 | Grad Norm: 0.00791042\n",
      "Epoch 5 | Step 3630500 | Avg Loss: 0.0158 | Grad Norm: 0.00832806\n",
      "Epoch 5 | Step 3630600 | Avg Loss: 0.0160 | Grad Norm: 0.00961150\n",
      "Epoch 5 | Step 3630700 | Avg Loss: 0.0161 | Grad Norm: 0.01068499\n",
      "Epoch 5 | Step 3630800 | Avg Loss: 0.0164 | Grad Norm: 0.00841155\n",
      "Epoch 5 | Step 3630900 | Avg Loss: 0.0163 | Grad Norm: 0.00889308\n",
      "Epoch 5 | Step 3631000 | Avg Loss: 0.0163 | Grad Norm: 0.00974692\n",
      "Epoch 5 | Step 3631100 | Avg Loss: 0.0161 | Grad Norm: 0.01066664\n",
      "Epoch 5 | Step 3631200 | Avg Loss: 0.0161 | Grad Norm: 0.00911691\n",
      "Epoch 5 | Step 3631300 | Avg Loss: 0.0159 | Grad Norm: 0.00941142\n",
      "Epoch 5 | Step 3631400 | Avg Loss: 0.0157 | Grad Norm: 0.00931084\n",
      "Epoch 5 | Step 3631500 | Avg Loss: 0.0155 | Grad Norm: 0.00974603\n",
      "Epoch 5 | Step 3631600 | Avg Loss: 0.0154 | Grad Norm: 0.00921025\n",
      "Epoch 5 | Step 3631700 | Avg Loss: 0.0156 | Grad Norm: 0.01100288\n",
      "Epoch 5 | Step 3631800 | Avg Loss: 0.0157 | Grad Norm: 0.01296258\n",
      "Epoch 5 | Step 3631900 | Avg Loss: 0.0159 | Grad Norm: 0.00943164\n",
      "Epoch 5 | Step 3632000 | Avg Loss: 0.0162 | Grad Norm: 0.00989103\n",
      "Epoch 5 | Step 3632100 | Avg Loss: 0.0162 | Grad Norm: 0.00927971\n",
      "Epoch 5 | Step 3632200 | Avg Loss: 0.0153 | Grad Norm: 0.00829944\n",
      "Epoch 5 | Step 3632300 | Avg Loss: 0.0154 | Grad Norm: 0.00843503\n",
      "Epoch 5 | Step 3632400 | Avg Loss: 0.0151 | Grad Norm: 0.01027930\n",
      "Epoch 5 | Step 3632500 | Avg Loss: 0.0156 | Grad Norm: 0.01060147\n",
      "Epoch 5 | Step 3632600 | Avg Loss: 0.0157 | Grad Norm: 0.01094814\n",
      "Epoch 5 | Step 3632700 | Avg Loss: 0.0159 | Grad Norm: 0.00962232\n",
      "Epoch 5 | Step 3632800 | Avg Loss: 0.0161 | Grad Norm: 0.01062808\n",
      "Epoch 5 | Step 3632900 | Avg Loss: 0.0158 | Grad Norm: 0.00972624\n",
      "Epoch 5 | Step 3633000 | Avg Loss: 0.0161 | Grad Norm: 0.00969537\n",
      "Epoch 5 | Step 3633100 | Avg Loss: 0.0155 | Grad Norm: 0.00880880\n",
      "Epoch 5 | Step 3633200 | Avg Loss: 0.0156 | Grad Norm: 0.00938431\n",
      "Epoch 5 | Step 3633300 | Avg Loss: 0.0154 | Grad Norm: 0.00811680\n",
      "Epoch 5 | Step 3633400 | Avg Loss: 0.0155 | Grad Norm: 0.00920928\n",
      "Epoch 5 | Step 3633500 | Avg Loss: 0.0158 | Grad Norm: 0.00906006\n",
      "Epoch 5 | Step 3633600 | Avg Loss: 0.0158 | Grad Norm: 0.00953100\n",
      "Epoch 5 | Step 3633700 | Avg Loss: 0.0154 | Grad Norm: 0.00861461\n",
      "Epoch 5 | Step 3633800 | Avg Loss: 0.0157 | Grad Norm: 0.00851914\n",
      "Epoch 5 | Step 3633900 | Avg Loss: 0.0153 | Grad Norm: 0.00928183\n",
      "Epoch 5 | Step 3634000 | Avg Loss: 0.0153 | Grad Norm: 0.00976404\n",
      "Epoch 5 | Step 3634100 | Avg Loss: 0.0151 | Grad Norm: 0.01004085\n",
      "Epoch 5 | Step 3634200 | Avg Loss: 0.0149 | Grad Norm: 0.00778082\n",
      "Epoch 5 | Step 3634300 | Avg Loss: 0.0153 | Grad Norm: 0.00837067\n",
      "Epoch 5 | Step 3634400 | Avg Loss: 0.0150 | Grad Norm: 0.00967569\n",
      "Epoch 5 | Step 3634500 | Avg Loss: 0.0147 | Grad Norm: 0.00901282\n",
      "Epoch 5 | Step 3634600 | Avg Loss: 0.0145 | Grad Norm: 0.00821303\n",
      "Epoch 5 | Step 3634700 | Avg Loss: 0.0145 | Grad Norm: 0.00832104\n",
      "Epoch 5 | Step 3634800 | Avg Loss: 0.0145 | Grad Norm: 0.00946967\n",
      "Epoch 5 | Step 3634900 | Avg Loss: 0.0145 | Grad Norm: 0.00848424\n",
      "Epoch 5 | Step 3635000 | Avg Loss: 0.0141 | Grad Norm: 0.00804946\n",
      "Epoch 5 | Step 3635100 | Avg Loss: 0.0143 | Grad Norm: 0.00870063\n",
      "Epoch 5 | Step 3635200 | Avg Loss: 0.0147 | Grad Norm: 0.00794583\n",
      "Epoch 5 | Step 3635300 | Avg Loss: 0.0150 | Grad Norm: 0.00909904\n",
      "Epoch 5 | Step 3635400 | Avg Loss: 0.0152 | Grad Norm: 0.00815666\n",
      "Epoch 5 | Step 3635500 | Avg Loss: 0.0151 | Grad Norm: 0.00967671\n",
      "Epoch 5 | Step 3635600 | Avg Loss: 0.0155 | Grad Norm: 0.01030482\n",
      "Epoch 5 | Step 3635700 | Avg Loss: 0.0158 | Grad Norm: 0.00851229\n",
      "Epoch 5 | Step 3635800 | Avg Loss: 0.0156 | Grad Norm: 0.00900480\n",
      "Epoch 5 | Step 3635900 | Avg Loss: 0.0156 | Grad Norm: 0.00835089\n",
      "Epoch 5 | Step 3636000 | Avg Loss: 0.0159 | Grad Norm: 0.00854817\n",
      "Epoch 5 | Step 3636100 | Avg Loss: 0.0155 | Grad Norm: 0.00899462\n",
      "Epoch 5 | Step 3636200 | Avg Loss: 0.0157 | Grad Norm: 0.01079249\n",
      "Epoch 5 | Step 3636300 | Avg Loss: 0.0158 | Grad Norm: 0.00883731\n",
      "Epoch 5 | Step 3636400 | Avg Loss: 0.0161 | Grad Norm: 0.00956572\n",
      "Epoch 5 | Step 3636500 | Avg Loss: 0.0158 | Grad Norm: 0.00979811\n",
      "Epoch 5 | Step 3636600 | Avg Loss: 0.0159 | Grad Norm: 0.00932031\n",
      "Epoch 5 | Step 3636700 | Avg Loss: 0.0157 | Grad Norm: 0.00876287\n",
      "Epoch 5 | Step 3636800 | Avg Loss: 0.0154 | Grad Norm: 0.01141262\n",
      "Epoch 5 | Step 3636900 | Avg Loss: 0.0156 | Grad Norm: 0.00838180\n",
      "Epoch 5 | Step 3637000 | Avg Loss: 0.0157 | Grad Norm: 0.00848059\n",
      "Epoch 5 | Step 3637100 | Avg Loss: 0.0154 | Grad Norm: 0.00999652\n",
      "Epoch 5 | Step 3637200 | Avg Loss: 0.0155 | Grad Norm: 0.00840386\n",
      "Epoch 5 | Step 3637300 | Avg Loss: 0.0159 | Grad Norm: 0.00950605\n",
      "Epoch 5 | Step 3637400 | Avg Loss: 0.0162 | Grad Norm: 0.00965674\n",
      "Epoch 5 | Step 3637500 | Avg Loss: 0.0158 | Grad Norm: 0.01035880\n",
      "Epoch 5 | Step 3637600 | Avg Loss: 0.0159 | Grad Norm: 0.00881960\n",
      "Epoch 5 | Step 3637700 | Avg Loss: 0.0156 | Grad Norm: 0.00844328\n",
      "Epoch 5 | Step 3637800 | Avg Loss: 0.0154 | Grad Norm: 0.00903607\n",
      "Epoch 5 | Step 3637900 | Avg Loss: 0.0153 | Grad Norm: 0.00973773\n",
      "Epoch 5 | Step 3638000 | Avg Loss: 0.0156 | Grad Norm: 0.00897128\n",
      "Epoch 5 | Step 3638100 | Avg Loss: 0.0157 | Grad Norm: 0.00867568\n",
      "Epoch 5 | Step 3638200 | Avg Loss: 0.0154 | Grad Norm: 0.00852093\n",
      "Epoch 5 | Step 3638300 | Avg Loss: 0.0156 | Grad Norm: 0.00986067\n",
      "Epoch 5 | Step 3638400 | Avg Loss: 0.0151 | Grad Norm: 0.00964269\n",
      "Epoch 5 | Step 3638500 | Avg Loss: 0.0150 | Grad Norm: 0.00917966\n",
      "Epoch 5 | Step 3638600 | Avg Loss: 0.0150 | Grad Norm: 0.00841277\n",
      "Epoch 5 | Step 3638700 | Avg Loss: 0.0151 | Grad Norm: 0.00989225\n",
      "Epoch 5 | Step 3638800 | Avg Loss: 0.0155 | Grad Norm: 0.00831983\n",
      "Epoch 5 | Step 3638900 | Avg Loss: 0.0157 | Grad Norm: 0.00993224\n",
      "Epoch 5 | Step 3639000 | Avg Loss: 0.0158 | Grad Norm: 0.00829360\n",
      "Epoch 5 | Step 3639100 | Avg Loss: 0.0156 | Grad Norm: 0.00864370\n",
      "Epoch 5 | Step 3639200 | Avg Loss: 0.0155 | Grad Norm: 0.00860553\n",
      "Epoch 5 | Step 3639300 | Avg Loss: 0.0156 | Grad Norm: 0.00897092\n",
      "Epoch 5 | Step 3639400 | Avg Loss: 0.0160 | Grad Norm: 0.00874974\n",
      "Epoch 5 | Step 3639500 | Avg Loss: 0.0159 | Grad Norm: 0.00943119\n",
      "Epoch 5 | Step 3639600 | Avg Loss: 0.0158 | Grad Norm: 0.01030690\n",
      "Epoch 5 | Step 3639700 | Avg Loss: 0.0160 | Grad Norm: 0.01037265\n",
      "Epoch 5 | Step 3639800 | Avg Loss: 0.0161 | Grad Norm: 0.00916868\n",
      "Epoch 5 | Step 3639900 | Avg Loss: 0.0160 | Grad Norm: 0.01017801\n",
      "Epoch 5 | Step 3640000 | Avg Loss: 0.0162 | Grad Norm: 0.00901105\n",
      "Epoch 5 | Step 3640100 | Avg Loss: 0.0159 | Grad Norm: 0.00944082\n",
      "Epoch 5 | Step 3640200 | Avg Loss: 0.0160 | Grad Norm: 0.00851653\n",
      "Epoch 5 | Step 3640300 | Avg Loss: 0.0157 | Grad Norm: 0.00954123\n",
      "Epoch 5 | Step 3640400 | Avg Loss: 0.0158 | Grad Norm: 0.00903492\n",
      "Epoch 5 | Step 3640500 | Avg Loss: 0.0157 | Grad Norm: 0.00916043\n",
      "Epoch 5 | Step 3640600 | Avg Loss: 0.0160 | Grad Norm: 0.00939353\n",
      "Epoch 5 | Step 3640700 | Avg Loss: 0.0160 | Grad Norm: 0.01009935\n",
      "Epoch 5 | Step 3640800 | Avg Loss: 0.0160 | Grad Norm: 0.00996555\n",
      "Epoch 5 | Step 3640900 | Avg Loss: 0.0162 | Grad Norm: 0.01226429\n",
      "Epoch 5 | Step 3641000 | Avg Loss: 0.0158 | Grad Norm: 0.00764645\n",
      "Epoch 5 | Step 3641100 | Avg Loss: 0.0156 | Grad Norm: 0.01021535\n",
      "Epoch 5 | Step 3641200 | Avg Loss: 0.0156 | Grad Norm: 0.01213919\n",
      "Epoch 5 | Step 3641300 | Avg Loss: 0.0157 | Grad Norm: 0.00944681\n",
      "Epoch 5 | Step 3641400 | Avg Loss: 0.0156 | Grad Norm: 0.01051882\n",
      "Epoch 5 | Step 3641500 | Avg Loss: 0.0157 | Grad Norm: 0.00967406\n",
      "Epoch 5 | Step 3641600 | Avg Loss: 0.0158 | Grad Norm: 0.00820889\n",
      "Epoch 5 | Step 3641700 | Avg Loss: 0.0159 | Grad Norm: 0.00842550\n",
      "Epoch 5 | Step 3641800 | Avg Loss: 0.0160 | Grad Norm: 0.00848888\n",
      "Epoch 5 | Step 3641900 | Avg Loss: 0.0158 | Grad Norm: 0.00921537\n",
      "Epoch 5 | Step 3642000 | Avg Loss: 0.0158 | Grad Norm: 0.00847288\n",
      "Epoch 5 | Step 3642100 | Avg Loss: 0.0157 | Grad Norm: 0.00909680\n",
      "Epoch 5 | Step 3642200 | Avg Loss: 0.0153 | Grad Norm: 0.00959797\n",
      "Epoch 5 | Step 3642300 | Avg Loss: 0.0154 | Grad Norm: 0.00988312\n",
      "Epoch 5 | Step 3642400 | Avg Loss: 0.0151 | Grad Norm: 0.00892100\n",
      "Epoch 5 | Step 3642500 | Avg Loss: 0.0151 | Grad Norm: 0.00821987\n",
      "Epoch 5 | Step 3642600 | Avg Loss: 0.0154 | Grad Norm: 0.00962843\n",
      "Epoch 5 | Step 3642700 | Avg Loss: 0.0155 | Grad Norm: 0.01110439\n",
      "Epoch 5 | Step 3642800 | Avg Loss: 0.0152 | Grad Norm: 0.01209270\n",
      "Epoch 5 | Step 3642900 | Avg Loss: 0.0150 | Grad Norm: 0.00984599\n",
      "Epoch 5 | Step 3643000 | Avg Loss: 0.0150 | Grad Norm: 0.00951361\n",
      "Epoch 5 | Step 3643100 | Avg Loss: 0.0154 | Grad Norm: 0.00921006\n",
      "Epoch 5 | Step 3643200 | Avg Loss: 0.0156 | Grad Norm: 0.00929663\n",
      "Epoch 5 | Step 3643300 | Avg Loss: 0.0153 | Grad Norm: 0.00954075\n",
      "Epoch 5 | Step 3643400 | Avg Loss: 0.0159 | Grad Norm: 0.00958782\n",
      "Epoch 5 | Step 3643500 | Avg Loss: 0.0158 | Grad Norm: 0.01060807\n",
      "Epoch 5 | Step 3643600 | Avg Loss: 0.0154 | Grad Norm: 0.00973949\n",
      "Epoch 5 | Step 3643700 | Avg Loss: 0.0154 | Grad Norm: 0.00848227\n",
      "Epoch 5 | Step 3643800 | Avg Loss: 0.0152 | Grad Norm: 0.00942539\n",
      "Epoch 5 | Step 3643900 | Avg Loss: 0.0151 | Grad Norm: 0.01062021\n",
      "Epoch 5 | Step 3644000 | Avg Loss: 0.0155 | Grad Norm: 0.00829091\n",
      "Epoch 5 | Step 3644100 | Avg Loss: 0.0154 | Grad Norm: 0.00970327\n",
      "Epoch 5 | Step 3644200 | Avg Loss: 0.0153 | Grad Norm: 0.00794084\n",
      "Epoch 5 | Step 3644300 | Avg Loss: 0.0151 | Grad Norm: 0.00850647\n",
      "Epoch 5 | Step 3644400 | Avg Loss: 0.0151 | Grad Norm: 0.00896872\n",
      "Epoch 5 | Step 3644500 | Avg Loss: 0.0150 | Grad Norm: 0.00844375\n",
      "Epoch 5 | Step 3644600 | Avg Loss: 0.0150 | Grad Norm: 0.00978107\n",
      "Epoch 5 | Step 3644700 | Avg Loss: 0.0149 | Grad Norm: 0.00829957\n",
      "Epoch 5 | Step 3644800 | Avg Loss: 0.0149 | Grad Norm: 0.00929884\n",
      "Epoch 5 | Step 3644900 | Avg Loss: 0.0153 | Grad Norm: 0.01037772\n",
      "Epoch 5 | Step 3645000 | Avg Loss: 0.0157 | Grad Norm: 0.01053522\n",
      "Epoch 5 | Step 3645100 | Avg Loss: 0.0154 | Grad Norm: 0.00946865\n",
      "Epoch 5 | Step 3645200 | Avg Loss: 0.0154 | Grad Norm: 0.00816698\n",
      "Epoch 5 | Step 3645300 | Avg Loss: 0.0155 | Grad Norm: 0.01113049\n",
      "Epoch 5 | Step 3645400 | Avg Loss: 0.0154 | Grad Norm: 0.00932151\n",
      "Epoch 5 | Step 3645500 | Avg Loss: 0.0156 | Grad Norm: 0.00873764\n",
      "Epoch 5 | Step 3645600 | Avg Loss: 0.0155 | Grad Norm: 0.00855570\n",
      "Epoch 5 | Step 3645700 | Avg Loss: 0.0156 | Grad Norm: 0.00932630\n",
      "Epoch 5 | Step 3645800 | Avg Loss: 0.0159 | Grad Norm: 0.00972351\n",
      "Epoch 5 | Step 3645900 | Avg Loss: 0.0156 | Grad Norm: 0.00925667\n",
      "Epoch 5 | Step 3646000 | Avg Loss: 0.0153 | Grad Norm: 0.00960147\n",
      "Epoch 5 | Step 3646100 | Avg Loss: 0.0149 | Grad Norm: 0.00883100\n",
      "Epoch 5 | Step 3646200 | Avg Loss: 0.0148 | Grad Norm: 0.00897624\n",
      "Epoch 5 | Step 3646300 | Avg Loss: 0.0152 | Grad Norm: 0.01036967\n",
      "Epoch 5 | Step 3646400 | Avg Loss: 0.0151 | Grad Norm: 0.01026662\n",
      "Epoch 5 | Step 3646500 | Avg Loss: 0.0152 | Grad Norm: 0.00748592\n",
      "Epoch 5 | Step 3646600 | Avg Loss: 0.0153 | Grad Norm: 0.01008382\n",
      "Epoch 5 | Step 3646700 | Avg Loss: 0.0156 | Grad Norm: 0.00956945\n",
      "Epoch 5 | Step 3646800 | Avg Loss: 0.0155 | Grad Norm: 0.00870100\n",
      "Epoch 5 | Step 3646900 | Avg Loss: 0.0158 | Grad Norm: 0.00924868\n",
      "Epoch 5 | Step 3647000 | Avg Loss: 0.0158 | Grad Norm: 0.00987299\n",
      "Epoch 5 | Step 3647100 | Avg Loss: 0.0157 | Grad Norm: 0.00847207\n",
      "Epoch 5 | Step 3647200 | Avg Loss: 0.0151 | Grad Norm: 0.00848739\n",
      "Epoch 5 | Step 3647300 | Avg Loss: 0.0150 | Grad Norm: 0.00861518\n",
      "Epoch 5 | Step 3647400 | Avg Loss: 0.0153 | Grad Norm: 0.00910046\n",
      "Epoch 5 | Step 3647500 | Avg Loss: 0.0156 | Grad Norm: 0.01008223\n",
      "Epoch 5 | Step 3647600 | Avg Loss: 0.0153 | Grad Norm: 0.00937815\n",
      "Epoch 5 | Step 3647700 | Avg Loss: 0.0154 | Grad Norm: 0.01020601\n",
      "Epoch 5 | Step 3647800 | Avg Loss: 0.0152 | Grad Norm: 0.00930128\n",
      "Epoch 5 | Step 3647900 | Avg Loss: 0.0156 | Grad Norm: 0.00821459\n",
      "Epoch 5 | Step 3648000 | Avg Loss: 0.0156 | Grad Norm: 0.01035708\n",
      "Epoch 5 | Step 3648100 | Avg Loss: 0.0155 | Grad Norm: 0.00838735\n",
      "Epoch 5 | Step 3648200 | Avg Loss: 0.0157 | Grad Norm: 0.00875334\n",
      "Epoch 5 | Step 3648300 | Avg Loss: 0.0157 | Grad Norm: 0.00785311\n",
      "Epoch 5 | Step 3648400 | Avg Loss: 0.0157 | Grad Norm: 0.00988342\n",
      "Epoch 5 | Step 3648500 | Avg Loss: 0.0160 | Grad Norm: 0.00856098\n",
      "Epoch 5 | Step 3648600 | Avg Loss: 0.0159 | Grad Norm: 0.00807280\n",
      "Epoch 5 | Step 3648700 | Avg Loss: 0.0155 | Grad Norm: 0.01129346\n",
      "Epoch 5 | Step 3648800 | Avg Loss: 0.0153 | Grad Norm: 0.00915371\n",
      "Epoch 5 | Step 3648900 | Avg Loss: 0.0150 | Grad Norm: 0.00823518\n",
      "Epoch 5 | Step 3649000 | Avg Loss: 0.0150 | Grad Norm: 0.01192463\n",
      "Epoch 5 | Step 3649100 | Avg Loss: 0.0150 | Grad Norm: 0.01028074\n",
      "Epoch 5 | Step 3649200 | Avg Loss: 0.0150 | Grad Norm: 0.01173797\n",
      "Epoch 5 | Step 3649300 | Avg Loss: 0.0153 | Grad Norm: 0.00789348\n",
      "Epoch 5 | Step 3649400 | Avg Loss: 0.0153 | Grad Norm: 0.00962856\n",
      "Epoch 5 | Step 3649500 | Avg Loss: 0.0152 | Grad Norm: 0.00828214\n",
      "Epoch 5 | Step 3649600 | Avg Loss: 0.0151 | Grad Norm: 0.01079289\n",
      "Epoch 5 | Step 3649700 | Avg Loss: 0.0151 | Grad Norm: 0.00937655\n",
      "Epoch 5 | Step 3649800 | Avg Loss: 0.0154 | Grad Norm: 0.00842788\n",
      "Epoch 5 | Step 3649900 | Avg Loss: 0.0154 | Grad Norm: 0.00918862\n",
      "Epoch 5 | Step 3650000 | Avg Loss: 0.0157 | Grad Norm: 0.01091102\n",
      "Epoch 5 | Step 3650100 | Avg Loss: 0.0157 | Grad Norm: 0.01049615\n",
      "Epoch 5 | Step 3650200 | Avg Loss: 0.0157 | Grad Norm: 0.00937174\n",
      "Epoch 5 | Step 3650300 | Avg Loss: 0.0156 | Grad Norm: 0.00925336\n",
      "Epoch 5 | Step 3650400 | Avg Loss: 0.0157 | Grad Norm: 0.00877850\n",
      "Epoch 5 | Step 3650500 | Avg Loss: 0.0158 | Grad Norm: 0.00780000\n",
      "Epoch 5 | Step 3650600 | Avg Loss: 0.0158 | Grad Norm: 0.00914168\n",
      "Epoch 5 | Step 3650700 | Avg Loss: 0.0156 | Grad Norm: 0.00868714\n",
      "Epoch 5 | Step 3650800 | Avg Loss: 0.0158 | Grad Norm: 0.00916966\n",
      "Epoch 5 | Step 3650900 | Avg Loss: 0.0156 | Grad Norm: 0.00965039\n",
      "Epoch 5 | Step 3651000 | Avg Loss: 0.0149 | Grad Norm: 0.00818838\n",
      "Epoch 5 | Step 3651100 | Avg Loss: 0.0154 | Grad Norm: 0.00945913\n",
      "Epoch 5 | Step 3651200 | Avg Loss: 0.0151 | Grad Norm: 0.00870001\n",
      "Epoch 5 | Step 3651300 | Avg Loss: 0.0150 | Grad Norm: 0.00889338\n",
      "Epoch 5 | Step 3651400 | Avg Loss: 0.0149 | Grad Norm: 0.00989975\n",
      "Epoch 5 | Step 3651500 | Avg Loss: 0.0151 | Grad Norm: 0.00888029\n",
      "Epoch 5 | Step 3651600 | Avg Loss: 0.0153 | Grad Norm: 0.01137359\n",
      "Epoch 5 | Step 3651700 | Avg Loss: 0.0151 | Grad Norm: 0.00844187\n",
      "Epoch 5 | Step 3651800 | Avg Loss: 0.0151 | Grad Norm: 0.00807887\n",
      "Epoch 5 | Step 3651900 | Avg Loss: 0.0152 | Grad Norm: 0.00972856\n",
      "Epoch 5 | Step 3652000 | Avg Loss: 0.0156 | Grad Norm: 0.01065521\n",
      "Epoch 5 | Step 3652100 | Avg Loss: 0.0155 | Grad Norm: 0.00975797\n",
      "Epoch 5 | Step 3652200 | Avg Loss: 0.0158 | Grad Norm: 0.00936608\n",
      "Epoch 5 | Step 3652300 | Avg Loss: 0.0158 | Grad Norm: 0.00883785\n",
      "Epoch 5 | Step 3652400 | Avg Loss: 0.0162 | Grad Norm: 0.00989017\n",
      "Epoch 5 | Step 3652500 | Avg Loss: 0.0161 | Grad Norm: 0.00916308\n",
      "Epoch 5 | Step 3652600 | Avg Loss: 0.0159 | Grad Norm: 0.01025668\n",
      "Epoch 5 | Step 3652700 | Avg Loss: 0.0159 | Grad Norm: 0.00971794\n",
      "Epoch 5 | Step 3652800 | Avg Loss: 0.0160 | Grad Norm: 0.00957329\n",
      "Epoch 5 | Step 3652900 | Avg Loss: 0.0155 | Grad Norm: 0.00931667\n",
      "Epoch 5 | Step 3653000 | Avg Loss: 0.0155 | Grad Norm: 0.00942690\n",
      "Epoch 5 | Step 3653100 | Avg Loss: 0.0154 | Grad Norm: 0.01468888\n",
      "Epoch 5 | Step 3653200 | Avg Loss: 0.0150 | Grad Norm: 0.01110850\n",
      "Epoch 5 | Step 3653300 | Avg Loss: 0.0148 | Grad Norm: 0.00857861\n",
      "Epoch 5 | Step 3653400 | Avg Loss: 0.0147 | Grad Norm: 0.00781667\n",
      "Epoch 5 | Step 3653500 | Avg Loss: 0.0150 | Grad Norm: 0.00768714\n",
      "Epoch 5 | Step 3653600 | Avg Loss: 0.0152 | Grad Norm: 0.00843952\n",
      "Epoch 5 | Step 3653700 | Avg Loss: 0.0149 | Grad Norm: 0.00821855\n",
      "Epoch 5 | Step 3653800 | Avg Loss: 0.0155 | Grad Norm: 0.00832166\n",
      "Epoch 5 | Step 3653900 | Avg Loss: 0.0154 | Grad Norm: 0.00864266\n",
      "Epoch 5 | Step 3654000 | Avg Loss: 0.0155 | Grad Norm: 0.00964025\n",
      "Epoch 5 | Step 3654100 | Avg Loss: 0.0155 | Grad Norm: 0.00933896\n",
      "Epoch 5 | Step 3654200 | Avg Loss: 0.0158 | Grad Norm: 0.00993694\n",
      "Epoch 5 | Step 3654300 | Avg Loss: 0.0156 | Grad Norm: 0.00832151\n",
      "Epoch 5 | Step 3654400 | Avg Loss: 0.0155 | Grad Norm: 0.00938502\n",
      "Epoch 5 | Step 3654500 | Avg Loss: 0.0159 | Grad Norm: 0.00830225\n",
      "Epoch 5 | Step 3654600 | Avg Loss: 0.0155 | Grad Norm: 0.01036358\n",
      "Epoch 5 | Step 3654700 | Avg Loss: 0.0158 | Grad Norm: 0.00907606\n",
      "Epoch 5 | Step 3654800 | Avg Loss: 0.0158 | Grad Norm: 0.00976916\n",
      "Epoch 5 | Step 3654900 | Avg Loss: 0.0154 | Grad Norm: 0.00938973\n",
      "Epoch 5 | Step 3655000 | Avg Loss: 0.0154 | Grad Norm: 0.00829678\n",
      "Epoch 5 | Step 3655100 | Avg Loss: 0.0156 | Grad Norm: 0.00894049\n",
      "Epoch 5 | Step 3655200 | Avg Loss: 0.0157 | Grad Norm: 0.00955661\n",
      "Epoch 5 | Step 3655300 | Avg Loss: 0.0157 | Grad Norm: 0.01182823\n",
      "Epoch 5 | Step 3655400 | Avg Loss: 0.0154 | Grad Norm: 0.01051508\n",
      "Epoch 5 | Step 3655500 | Avg Loss: 0.0151 | Grad Norm: 0.00860211\n",
      "Epoch 5 | Step 3655600 | Avg Loss: 0.0155 | Grad Norm: 0.01005493\n",
      "Epoch 5 | Step 3655700 | Avg Loss: 0.0154 | Grad Norm: 0.00805141\n",
      "Epoch 5 | Step 3655800 | Avg Loss: 0.0152 | Grad Norm: 0.00912289\n",
      "Epoch 5 | Step 3655900 | Avg Loss: 0.0150 | Grad Norm: 0.00803284\n",
      "Epoch 5 | Step 3656000 | Avg Loss: 0.0150 | Grad Norm: 0.00973701\n",
      "Epoch 5 | Step 3656100 | Avg Loss: 0.0150 | Grad Norm: 0.00764128\n",
      "Epoch 5 | Step 3656200 | Avg Loss: 0.0148 | Grad Norm: 0.00988511\n",
      "Epoch 5 | Step 3656300 | Avg Loss: 0.0151 | Grad Norm: 0.00775391\n",
      "Epoch 5 | Step 3656400 | Avg Loss: 0.0150 | Grad Norm: 0.00764220\n",
      "Epoch 5 | Step 3656500 | Avg Loss: 0.0150 | Grad Norm: 0.00797040\n",
      "Epoch 5 | Step 3656600 | Avg Loss: 0.0151 | Grad Norm: 0.00970291\n",
      "Epoch 5 | Step 3656700 | Avg Loss: 0.0150 | Grad Norm: 0.00917542\n",
      "Epoch 5 | Step 3656800 | Avg Loss: 0.0152 | Grad Norm: 0.00836656\n",
      "Epoch 5 | Step 3656900 | Avg Loss: 0.0154 | Grad Norm: 0.01084962\n",
      "Epoch 5 | Step 3657000 | Avg Loss: 0.0155 | Grad Norm: 0.00870893\n",
      "Epoch 5 | Step 3657100 | Avg Loss: 0.0156 | Grad Norm: 0.00907390\n",
      "Epoch 5 | Step 3657200 | Avg Loss: 0.0157 | Grad Norm: 0.00850472\n",
      "Epoch 5 | Step 3657300 | Avg Loss: 0.0158 | Grad Norm: 0.00980430\n",
      "Epoch 5 | Step 3657400 | Avg Loss: 0.0156 | Grad Norm: 0.00773204\n",
      "Epoch 5 | Step 3657500 | Avg Loss: 0.0152 | Grad Norm: 0.00944550\n",
      "Epoch 5 | Step 3657600 | Avg Loss: 0.0154 | Grad Norm: 0.00850575\n",
      "Epoch 5 | Step 3657700 | Avg Loss: 0.0153 | Grad Norm: 0.01266980\n",
      "Epoch 5 | Step 3657800 | Avg Loss: 0.0152 | Grad Norm: 0.00815904\n",
      "Epoch 5 | Step 3657900 | Avg Loss: 0.0148 | Grad Norm: 0.00886827\n",
      "Epoch 5 | Step 3658000 | Avg Loss: 0.0146 | Grad Norm: 0.00733762\n",
      "Epoch 5 | Step 3658100 | Avg Loss: 0.0145 | Grad Norm: 0.00913853\n",
      "Epoch 5 | Step 3658200 | Avg Loss: 0.0150 | Grad Norm: 0.00790419\n",
      "Epoch 5 | Step 3658300 | Avg Loss: 0.0151 | Grad Norm: 0.00841334\n",
      "Epoch 5 | Step 3658400 | Avg Loss: 0.0155 | Grad Norm: 0.01144757\n",
      "Epoch 5 | Step 3658500 | Avg Loss: 0.0154 | Grad Norm: 0.00914659\n",
      "Epoch 5 | Step 3658600 | Avg Loss: 0.0152 | Grad Norm: 0.00859345\n",
      "Epoch 5 | Step 3658700 | Avg Loss: 0.0151 | Grad Norm: 0.01038502\n",
      "Epoch 5 | Step 3658800 | Avg Loss: 0.0154 | Grad Norm: 0.01035842\n",
      "Epoch 5 | Step 3658900 | Avg Loss: 0.0154 | Grad Norm: 0.00975388\n",
      "Epoch 5 | Step 3659000 | Avg Loss: 0.0152 | Grad Norm: 0.00919754\n",
      "Epoch 5 | Step 3659100 | Avg Loss: 0.0152 | Grad Norm: 0.00844903\n",
      "Epoch 5 | Step 3659200 | Avg Loss: 0.0149 | Grad Norm: 0.00804132\n",
      "Epoch 5 | Step 3659300 | Avg Loss: 0.0152 | Grad Norm: 0.00980439\n",
      "Epoch 5 | Step 3659400 | Avg Loss: 0.0152 | Grad Norm: 0.00764168\n",
      "Epoch 5 | Step 3659500 | Avg Loss: 0.0152 | Grad Norm: 0.00804657\n",
      "Epoch 5 | Step 3659600 | Avg Loss: 0.0153 | Grad Norm: 0.00925987\n",
      "Epoch 5 | Step 3659700 | Avg Loss: 0.0151 | Grad Norm: 0.00848177\n",
      "Epoch 5 | Step 3659800 | Avg Loss: 0.0150 | Grad Norm: 0.01082922\n",
      "Epoch 5 | Step 3659900 | Avg Loss: 0.0151 | Grad Norm: 0.00934068\n",
      "Epoch 5 | Step 3660000 | Avg Loss: 0.0152 | Grad Norm: 0.00936157\n",
      "Epoch 5 | Step 3660100 | Avg Loss: 0.0151 | Grad Norm: 0.01064228\n",
      "Epoch 5 | Step 3660200 | Avg Loss: 0.0153 | Grad Norm: 0.00996347\n",
      "Epoch 5 | Step 3660300 | Avg Loss: 0.0152 | Grad Norm: 0.00825933\n",
      "Epoch 5 | Step 3660400 | Avg Loss: 0.0151 | Grad Norm: 0.00969467\n",
      "Epoch 5 | Step 3660500 | Avg Loss: 0.0147 | Grad Norm: 0.01044798\n",
      "Epoch 5 | Step 3660600 | Avg Loss: 0.0146 | Grad Norm: 0.00843458\n",
      "Epoch 5 | Step 3660700 | Avg Loss: 0.0149 | Grad Norm: 0.00942871\n",
      "Epoch 5 | Step 3660800 | Avg Loss: 0.0150 | Grad Norm: 0.00919187\n",
      "Epoch 5 | Step 3660900 | Avg Loss: 0.0151 | Grad Norm: 0.00789704\n",
      "Epoch 5 | Step 3661000 | Avg Loss: 0.0152 | Grad Norm: 0.00850976\n",
      "Epoch 5 | Step 3661100 | Avg Loss: 0.0149 | Grad Norm: 0.00824394\n",
      "Epoch 5 | Step 3661200 | Avg Loss: 0.0149 | Grad Norm: 0.00897531\n",
      "Epoch 5 | Step 3661300 | Avg Loss: 0.0150 | Grad Norm: 0.00903497\n",
      "Epoch 5 | Step 3661400 | Avg Loss: 0.0148 | Grad Norm: 0.00886477\n",
      "Epoch 5 | Step 3661500 | Avg Loss: 0.0152 | Grad Norm: 0.00913216\n",
      "Epoch 5 | Step 3661600 | Avg Loss: 0.0151 | Grad Norm: 0.01050862\n",
      "Epoch 5 | Step 3661700 | Avg Loss: 0.0150 | Grad Norm: 0.00782685\n",
      "Epoch 5 | Step 3661800 | Avg Loss: 0.0147 | Grad Norm: 0.00909109\n",
      "Epoch 5 | Step 3661900 | Avg Loss: 0.0147 | Grad Norm: 0.01004899\n",
      "Epoch 5 | Step 3662000 | Avg Loss: 0.0146 | Grad Norm: 0.00810851\n",
      "Epoch 5 | Step 3662100 | Avg Loss: 0.0146 | Grad Norm: 0.00857635\n",
      "Epoch 5 | Step 3662200 | Avg Loss: 0.0148 | Grad Norm: 0.00765479\n",
      "Epoch 5 | Step 3662300 | Avg Loss: 0.0147 | Grad Norm: 0.00925746\n",
      "Epoch 5 | Step 3662400 | Avg Loss: 0.0151 | Grad Norm: 0.00897619\n",
      "Epoch 5 | Step 3662500 | Avg Loss: 0.0151 | Grad Norm: 0.00910501\n",
      "Epoch 5 | Step 3662600 | Avg Loss: 0.0150 | Grad Norm: 0.00870965\n",
      "Epoch 5 | Step 3662700 | Avg Loss: 0.0151 | Grad Norm: 0.00897024\n",
      "Epoch 5 | Step 3662800 | Avg Loss: 0.0151 | Grad Norm: 0.01063597\n",
      "Epoch 5 | Step 3662900 | Avg Loss: 0.0154 | Grad Norm: 0.00936313\n",
      "Epoch 5 | Step 3663000 | Avg Loss: 0.0156 | Grad Norm: 0.01002135\n",
      "Epoch 5 | Step 3663100 | Avg Loss: 0.0159 | Grad Norm: 0.01265577\n",
      "Epoch 5 | Step 3663200 | Avg Loss: 0.0157 | Grad Norm: 0.00996612\n",
      "Epoch 5 | Step 3663300 | Avg Loss: 0.0157 | Grad Norm: 0.01042623\n",
      "Epoch 5 | Step 3663400 | Avg Loss: 0.0159 | Grad Norm: 0.00902201\n",
      "Epoch 5 | Step 3663500 | Avg Loss: 0.0156 | Grad Norm: 0.00795468\n",
      "Epoch 5 | Step 3663600 | Avg Loss: 0.0159 | Grad Norm: 0.01065700\n",
      "Epoch 5 | Step 3663700 | Avg Loss: 0.0159 | Grad Norm: 0.00846877\n",
      "Epoch 5 | Step 3663800 | Avg Loss: 0.0158 | Grad Norm: 0.00985834\n",
      "Epoch 5 | Step 3663900 | Avg Loss: 0.0159 | Grad Norm: 0.00861048\n",
      "Epoch 5 | Step 3664000 | Avg Loss: 0.0158 | Grad Norm: 0.00871879\n",
      "Epoch 5 | Step 3664100 | Avg Loss: 0.0159 | Grad Norm: 0.00853843\n",
      "Epoch 5 | Step 3664200 | Avg Loss: 0.0161 | Grad Norm: 0.00832524\n",
      "Epoch 5 | Step 3664300 | Avg Loss: 0.0161 | Grad Norm: 0.00858771\n",
      "Epoch 5 | Step 3664400 | Avg Loss: 0.0158 | Grad Norm: 0.00984657\n",
      "Epoch 5 | Step 3664500 | Avg Loss: 0.0155 | Grad Norm: 0.00890010\n",
      "Epoch 5 | Step 3664600 | Avg Loss: 0.0156 | Grad Norm: 0.00863374\n",
      "Epoch 5 | Step 3664700 | Avg Loss: 0.0154 | Grad Norm: 0.00966769\n",
      "Epoch 5 | Step 3664800 | Avg Loss: 0.0153 | Grad Norm: 0.00921093\n",
      "Epoch 5 | Step 3664900 | Avg Loss: 0.0152 | Grad Norm: 0.00917181\n",
      "Epoch 5 | Step 3665000 | Avg Loss: 0.0158 | Grad Norm: 0.00841700\n",
      "Epoch 5 | Step 3665100 | Avg Loss: 0.0160 | Grad Norm: 0.01078655\n",
      "Epoch 5 | Step 3665200 | Avg Loss: 0.0160 | Grad Norm: 0.00937526\n",
      "Epoch 5 | Step 3665300 | Avg Loss: 0.0161 | Grad Norm: 0.00911172\n",
      "Epoch 5 | Step 3665400 | Avg Loss: 0.0160 | Grad Norm: 0.00807374\n",
      "Epoch 5 | Step 3665500 | Avg Loss: 0.0163 | Grad Norm: 0.01032999\n",
      "Epoch 5 | Step 3665600 | Avg Loss: 0.0161 | Grad Norm: 0.00888773\n",
      "Epoch 5 | Step 3665700 | Avg Loss: 0.0163 | Grad Norm: 0.00821680\n",
      "Epoch 5 | Step 3665800 | Avg Loss: 0.0163 | Grad Norm: 0.01024389\n",
      "Epoch 5 | Step 3665900 | Avg Loss: 0.0161 | Grad Norm: 0.00933475\n",
      "Epoch 5 | Step 3666000 | Avg Loss: 0.0162 | Grad Norm: 0.00966663\n",
      "Epoch 5 | Step 3666100 | Avg Loss: 0.0158 | Grad Norm: 0.01096986\n",
      "Epoch 5 | Step 3666200 | Avg Loss: 0.0159 | Grad Norm: 0.01056559\n",
      "Epoch 5 | Step 3666300 | Avg Loss: 0.0163 | Grad Norm: 0.01096312\n",
      "Epoch 5 | Step 3666400 | Avg Loss: 0.0161 | Grad Norm: 0.00783372\n",
      "Epoch 5 | Step 3666500 | Avg Loss: 0.0158 | Grad Norm: 0.00890424\n",
      "Epoch 5 | Step 3666600 | Avg Loss: 0.0159 | Grad Norm: 0.00858381\n",
      "Epoch 5 | Step 3666700 | Avg Loss: 0.0156 | Grad Norm: 0.00926446\n",
      "Epoch 5 | Step 3666800 | Avg Loss: 0.0157 | Grad Norm: 0.00991722\n",
      "Epoch 5 | Step 3666900 | Avg Loss: 0.0154 | Grad Norm: 0.00881694\n",
      "Epoch 5 | Step 3667000 | Avg Loss: 0.0155 | Grad Norm: 0.00886913\n",
      "Epoch 5 | Step 3667100 | Avg Loss: 0.0160 | Grad Norm: 0.01037845\n",
      "Epoch 5 | Step 3667200 | Avg Loss: 0.0158 | Grad Norm: 0.01038340\n",
      "Epoch 5 | Step 3667300 | Avg Loss: 0.0157 | Grad Norm: 0.00992180\n",
      "Epoch 5 | Step 3667400 | Avg Loss: 0.0157 | Grad Norm: 0.00970382\n",
      "Epoch 5 | Step 3667500 | Avg Loss: 0.0155 | Grad Norm: 0.00869329\n",
      "Epoch 5 | Step 3667600 | Avg Loss: 0.0157 | Grad Norm: 0.01006489\n",
      "Epoch 5 | Step 3667700 | Avg Loss: 0.0158 | Grad Norm: 0.00952480\n",
      "Epoch 5 | Step 3667800 | Avg Loss: 0.0156 | Grad Norm: 0.01177447\n",
      "Epoch 5 | Step 3667900 | Avg Loss: 0.0161 | Grad Norm: 0.01006239\n",
      "Epoch 5 | Step 3668000 | Avg Loss: 0.0157 | Grad Norm: 0.00903810\n",
      "Epoch 5 | Step 3668100 | Avg Loss: 0.0157 | Grad Norm: 0.00936503\n",
      "Epoch 5 | Step 3668200 | Avg Loss: 0.0160 | Grad Norm: 0.00891521\n",
      "Epoch 5 | Step 3668300 | Avg Loss: 0.0162 | Grad Norm: 0.01104871\n",
      "Epoch 5 | Step 3668400 | Avg Loss: 0.0159 | Grad Norm: 0.00973231\n",
      "Epoch 5 | Step 3668500 | Avg Loss: 0.0158 | Grad Norm: 0.00933157\n",
      "Epoch 5 | Step 3668600 | Avg Loss: 0.0157 | Grad Norm: 0.00872329\n",
      "Epoch 5 | Step 3668700 | Avg Loss: 0.0155 | Grad Norm: 0.00854425\n",
      "Epoch 5 | Step 3668800 | Avg Loss: 0.0153 | Grad Norm: 0.00839040\n",
      "Epoch 5 | Step 3668900 | Avg Loss: 0.0154 | Grad Norm: 0.00887039\n",
      "Epoch 5 | Step 3669000 | Avg Loss: 0.0156 | Grad Norm: 0.00889904\n",
      "Epoch 5 | Step 3669100 | Avg Loss: 0.0157 | Grad Norm: 0.00975573\n",
      "Epoch 5 | Step 3669200 | Avg Loss: 0.0154 | Grad Norm: 0.00876512\n",
      "Epoch 5 | Step 3669300 | Avg Loss: 0.0159 | Grad Norm: 0.00873666\n",
      "Epoch 5 | Step 3669400 | Avg Loss: 0.0156 | Grad Norm: 0.00922253\n",
      "Epoch 5 | Step 3669500 | Avg Loss: 0.0157 | Grad Norm: 0.00930402\n",
      "Epoch 5 | Step 3669600 | Avg Loss: 0.0155 | Grad Norm: 0.00965284\n",
      "Epoch 5 | Step 3669700 | Avg Loss: 0.0155 | Grad Norm: 0.00827009\n",
      "Epoch 5 | Step 3669800 | Avg Loss: 0.0154 | Grad Norm: 0.00847620\n",
      "Epoch 5 | Step 3669900 | Avg Loss: 0.0154 | Grad Norm: 0.00964619\n",
      "Epoch 5 | Step 3670000 | Avg Loss: 0.0151 | Grad Norm: 0.00878178\n",
      "Epoch 5 | Step 3670100 | Avg Loss: 0.0157 | Grad Norm: 0.00830101\n",
      "Epoch 5 | Step 3670200 | Avg Loss: 0.0156 | Grad Norm: 0.00911618\n",
      "Epoch 5 | Step 3670300 | Avg Loss: 0.0160 | Grad Norm: 0.00931529\n",
      "Epoch 5 | Step 3670400 | Avg Loss: 0.0158 | Grad Norm: 0.00826262\n",
      "Epoch 5 | Step 3670500 | Avg Loss: 0.0154 | Grad Norm: 0.00773730\n",
      "Epoch 5 | Step 3670600 | Avg Loss: 0.0153 | Grad Norm: 0.00924081\n",
      "Epoch 5 | Step 3670700 | Avg Loss: 0.0150 | Grad Norm: 0.00863363\n",
      "Epoch 5 | Step 3670800 | Avg Loss: 0.0147 | Grad Norm: 0.00841258\n",
      "Epoch 5 | Step 3670900 | Avg Loss: 0.0149 | Grad Norm: 0.00915839\n",
      "Epoch 5 | Step 3671000 | Avg Loss: 0.0151 | Grad Norm: 0.00888676\n",
      "Epoch 5 | Step 3671100 | Avg Loss: 0.0151 | Grad Norm: 0.00884708\n",
      "Epoch 5 | Step 3671200 | Avg Loss: 0.0153 | Grad Norm: 0.00856216\n",
      "Epoch 5 | Step 3671300 | Avg Loss: 0.0154 | Grad Norm: 0.00989998\n",
      "Epoch 5 | Step 3671400 | Avg Loss: 0.0151 | Grad Norm: 0.00840319\n",
      "Epoch 5 | Step 3671500 | Avg Loss: 0.0147 | Grad Norm: 0.00900265\n",
      "Epoch 5 | Step 3671600 | Avg Loss: 0.0148 | Grad Norm: 0.00984464\n",
      "Epoch 5 | Step 3671700 | Avg Loss: 0.0151 | Grad Norm: 0.00987276\n",
      "Epoch 5 | Step 3671800 | Avg Loss: 0.0152 | Grad Norm: 0.00949652\n",
      "Epoch 5 | Step 3671900 | Avg Loss: 0.0151 | Grad Norm: 0.00881836\n",
      "Epoch 5 | Step 3672000 | Avg Loss: 0.0151 | Grad Norm: 0.00823376\n",
      "Epoch 5 | Step 3672100 | Avg Loss: 0.0151 | Grad Norm: 0.00928168\n",
      "Epoch 5 | Step 3672200 | Avg Loss: 0.0152 | Grad Norm: 0.00843620\n",
      "Epoch 5 | Step 3672300 | Avg Loss: 0.0154 | Grad Norm: 0.00852592\n",
      "Epoch 5 | Step 3672400 | Avg Loss: 0.0156 | Grad Norm: 0.01044390\n",
      "Epoch 5 | Step 3672500 | Avg Loss: 0.0154 | Grad Norm: 0.00845865\n",
      "Epoch 5 | Step 3672600 | Avg Loss: 0.0152 | Grad Norm: 0.00892287\n",
      "Epoch 5 | Step 3672700 | Avg Loss: 0.0151 | Grad Norm: 0.01267401\n",
      "Epoch 5 | Step 3672800 | Avg Loss: 0.0151 | Grad Norm: 0.01013528\n",
      "Epoch 5 | Step 3672900 | Avg Loss: 0.0154 | Grad Norm: 0.01015749\n",
      "Epoch 5 | Step 3673000 | Avg Loss: 0.0156 | Grad Norm: 0.00907889\n",
      "Epoch 5 | Step 3673100 | Avg Loss: 0.0154 | Grad Norm: 0.01009064\n",
      "Epoch 5 | Step 3673200 | Avg Loss: 0.0157 | Grad Norm: 0.01041724\n",
      "Epoch 5 | Step 3673300 | Avg Loss: 0.0152 | Grad Norm: 0.00796929\n",
      "Epoch 5 | Step 3673400 | Avg Loss: 0.0155 | Grad Norm: 0.00896284\n",
      "Epoch 5 | Step 3673500 | Avg Loss: 0.0155 | Grad Norm: 0.00830222\n",
      "Epoch 5 | Step 3673600 | Avg Loss: 0.0154 | Grad Norm: 0.00931991\n",
      "Epoch 5 | Step 3673700 | Avg Loss: 0.0154 | Grad Norm: 0.00921521\n",
      "Epoch 5 | Step 3673800 | Avg Loss: 0.0159 | Grad Norm: 0.00856571\n",
      "Epoch 5 | Step 3673900 | Avg Loss: 0.0159 | Grad Norm: 0.01009944\n",
      "Epoch 5 | Step 3674000 | Avg Loss: 0.0160 | Grad Norm: 0.00875167\n",
      "Epoch 5 | Step 3674100 | Avg Loss: 0.0160 | Grad Norm: 0.00823003\n",
      "Epoch 5 | Step 3674200 | Avg Loss: 0.0160 | Grad Norm: 0.00951576\n",
      "Epoch 5 | Step 3674300 | Avg Loss: 0.0157 | Grad Norm: 0.00894474\n",
      "Epoch 5 | Step 3674400 | Avg Loss: 0.0157 | Grad Norm: 0.00904606\n",
      "Epoch 5 | Step 3674500 | Avg Loss: 0.0157 | Grad Norm: 0.01019983\n",
      "Epoch 5 | Step 3674600 | Avg Loss: 0.0157 | Grad Norm: 0.00911267\n",
      "Epoch 5 | Step 3674700 | Avg Loss: 0.0153 | Grad Norm: 0.00928470\n",
      "Epoch 5 | Step 3674800 | Avg Loss: 0.0155 | Grad Norm: 0.00870305\n",
      "Epoch 5 | Step 3674900 | Avg Loss: 0.0152 | Grad Norm: 0.00957004\n",
      "Epoch 5 | Step 3675000 | Avg Loss: 0.0154 | Grad Norm: 0.00881426\n",
      "Epoch 5 | Step 3675100 | Avg Loss: 0.0152 | Grad Norm: 0.00831828\n",
      "Epoch 5 | Step 3675200 | Avg Loss: 0.0151 | Grad Norm: 0.00858388\n",
      "Epoch 5 | Step 3675300 | Avg Loss: 0.0149 | Grad Norm: 0.00779279\n",
      "Epoch 5 | Step 3675400 | Avg Loss: 0.0150 | Grad Norm: 0.00878557\n",
      "Epoch 5 | Step 3675500 | Avg Loss: 0.0155 | Grad Norm: 0.00983106\n",
      "Epoch 5 | Step 3675600 | Avg Loss: 0.0155 | Grad Norm: 0.00955538\n",
      "Epoch 5 | Step 3675700 | Avg Loss: 0.0157 | Grad Norm: 0.00935700\n",
      "Epoch 5 | Step 3675800 | Avg Loss: 0.0155 | Grad Norm: 0.00855605\n",
      "Epoch 5 | Step 3675900 | Avg Loss: 0.0156 | Grad Norm: 0.01114332\n",
      "Epoch 5 | Step 3676000 | Avg Loss: 0.0156 | Grad Norm: 0.00964929\n",
      "Epoch 5 | Step 3676100 | Avg Loss: 0.0156 | Grad Norm: 0.00939494\n",
      "Epoch 5 | Step 3676200 | Avg Loss: 0.0154 | Grad Norm: 0.00804444\n",
      "Epoch 5 | Step 3676300 | Avg Loss: 0.0154 | Grad Norm: 0.01048497\n",
      "Epoch 5 | Step 3676400 | Avg Loss: 0.0155 | Grad Norm: 0.00928611\n",
      "Epoch 5 | Step 3676500 | Avg Loss: 0.0151 | Grad Norm: 0.00803711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: : <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fa8dea39ee0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#criterion = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#for _ in range(1000000):\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get data from the dataloader\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/nnue_training_split_model_200M_5_\")\n",
    "\n",
    "# hyperparameters\n",
    "nr_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "scaling_factor = 400\n",
    "ground_truth_scaling_factor = 400\n",
    "lambda_ = 0.2\n",
    "log_interval = 100\n",
    "save_interval = 100000\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "epsilon = 1e-10\n",
    "\n",
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_200M_4_3600000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "\n",
    "        #for _ in range(1000000):\n",
    "    \n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y, result = batch\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        result = result.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = lambda_*result + (1 - lambda_) * torch.sigmoid(batch_y / ground_truth_scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        #loss = (wdl_batch_y * torch.log(wdl_batch_y + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_batch_y + epsilon)) -(wdl_batch_y * torch.log(wdl_pred   + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_pred   + epsilon))\n",
    "        #loss = loss.mean()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(wdl_pred, wdl_batch_y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # make a step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        # calculate the gradient norm\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm of this parameter's gradient\n",
    "                total_norm_sq += param_norm.item() ** 2\n",
    "        total_grad_norm = total_norm_sq ** 0.5\n",
    "        # Now total_grad_norm is the L2 norm of all gradients combined.\n",
    "        #print(f\"Step {step}  Grad Norm = {total_grad_norm:.8f}\")\n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Avg Loss: {avg_loss:.4f} | Grad Norm: {total_grad_norm:.8f}\")\n",
    "            running_loss = 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "            writer.add_scalar(\"Grad Norm\", total_grad_norm, step)\n",
    "            writer.add_scalar(\"WDL Pred\", torch.mean(wdl_pred).item(), step)\n",
    "            writer.add_scalar(\"WDL BatchY\", torch.mean(wdl_batch_y).item(), step)\n",
    "            writer.add_scalar(\"Pred\", torch.median(pred).item(), step)\n",
    "            writer.add_scalar(\"BatchY\", torch.median(batch_y).item(), step)\n",
    "\n",
    "\n",
    "            # log separate grad norms\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    writer.add_scalar(f'GradNorm/{name}', grad_norm, step)\n",
    "\n",
    "        # Save the model every `save_interval` steps\n",
    "        if step % save_interval == 0:\n",
    "            model_name = 'saved_models/split_model_200M_5_' + str(step) + \".pth\"\n",
    "            print(\"Saving model at step\" + str(step))\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,}, model_name)\n",
    "            \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d09a",
   "metadata": {},
   "source": [
    "### Postprocessing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_200M_2100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def save_layer(layer, name):\n",
    "    w = layer.weight.detach().numpy()\n",
    "    b = layer.bias.detach().numpy()\n",
    "    with open(f\"{name}_weights.txt\", \"w\") as f:\n",
    "        for row in w:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    with open(f\"{name}_biases.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(map(str, b)))\n",
    "\n",
    "save_layer(model.fc1, \"model/layer1\")\n",
    "save_layer(model.fc2, \"model/layer2\")\n",
    "save_layer(model.fc3, \"model/layer3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "fen2 = 'rnbqkbnr/pppppppp/8/8/8/7N/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "fen3 = 'rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "\n",
    "start_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "\n",
    "#torch.tensor(FEN_to_input(fen1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1, b1, stm1 = FEN_to_inputs(fen1)\n",
    "    w1 = w1.unsqueeze(0)\n",
    "    b1 = b1.unsqueeze(0)\n",
    "    stm1 = stm1.unsqueeze(0)\n",
    "    print(w1.shape, b1.shape, stm1.shape)\n",
    "    #in_1 = np.argwhere(input1.numpy() == 1)\n",
    "    #print(sum(input1.numpy()))\n",
    "    w2, b2, stm2 = FEN_to_inputs(fen2)\n",
    "    w2 = w2.unsqueeze(0)\n",
    "    b2 = b2.unsqueeze(0)\n",
    "    stm2 = stm2.unsqueeze(0)\n",
    "    #print(np.argwhere(input2.numpy() == 1))\n",
    "    w3, b3, stm3 = FEN_to_inputs(fen3)\n",
    "    w3 = w3.unsqueeze(0)\n",
    "    b3 = b3.unsqueeze(0)\n",
    "    stm3 = stm3.unsqueeze(0)\n",
    "    #print(np.argwhere(input3.numpy() == 1))\n",
    "\n",
    "    #in_start = np.argwhere(FEN_to_inputs(start_fen).numpy() == 1)\n",
    "\n",
    "    pred1 = model(w1, b1, stm1)\n",
    "    pred2 = model(w2, b2, stm2)\n",
    "    pred3 = model(w3, b3, stm3)\n",
    "\n",
    "    print(pred1.item())\n",
    "    print(pred2.item())\n",
    "    print(pred3.item())\n",
    "\n",
    "    #accumulator = model.fc1(input1)\n",
    "    #ws, bs, stms = FEN_to_inputs(start_fen)\n",
    "    #ws = ws.unsqueeze(0)\n",
    "    #bs = bs.unsqueeze(0)\n",
    "    #stms = stms.unsqueeze(0)\n",
    "    w_accumulator = model.fc1(w1)\n",
    "    b_accumulator = model.fc1(b1)\n",
    "    #print(w_accumulator)\n",
    "    #print(b_accumulator)\n",
    "\n",
    "\n",
    "    cat_wb = torch.cat([w_accumulator, b_accumulator], dim=1)\n",
    "    cat_bw = torch.cat([b_accumulator, w_accumulator], dim=1)\n",
    "\n",
    "    #stm1 = stm1.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "    #print(cat_bw)\n",
    "\n",
    "    accumulator = (1 - stm1) * cat_wb + stm1 * cat_bw\n",
    "    #print(accumulator)\n",
    "\n",
    "    x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "    x = model.fc2(x)\n",
    "\n",
    "    #print(x)\n",
    "\n",
    "    print(model.fc2.bias.detach().numpy())\n",
    "    #print(model.fc2.weight[0][:10])\n",
    "\n",
    "    #print(\"weights[0][0]\")\n",
    "    #print(model.fc1.weight[0][0])\n",
    "    #print(\"weights[1][0]\")\n",
    "    #print(model.fc1.weight[1][0])\n",
    "    #print(model.fc1.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of bins\n",
    "num_bins = 64\n",
    "bin_edges = np.linspace(-32003, 32003, num_bins + 1)\n",
    "counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "filename = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/scores.txt'\n",
    "\n",
    "# Re-read the file and bin values\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            val = float(line.strip())\n",
    "            # Determine bin index\n",
    "            bin_idx = np.searchsorted(bin_edges, val, side='right') - 1\n",
    "            if 0 <= bin_idx < num_bins:\n",
    "                counts[bin_idx] += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Find empty bins\n",
    "empty_bins = []\n",
    "for i, count in enumerate(counts):\n",
    "    if count == 0:\n",
    "        left_edge = bin_edges[i]\n",
    "        right_edge = bin_edges[i + 1]\n",
    "        empty_bins.append((i, left_edge, right_edge))\n",
    "\n",
    "# Print empty bin ranges\n",
    "print(\"Empty bins:\")\n",
    "for i, left, right in empty_bins:\n",
    "    print(f\"Bin {i}: [{left}, {right})\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge')\n",
    "plt.title(\"Histogram (streamed)\")\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP to WDL conversion\n",
    "scaling_factor = 400\n",
    "score = torch.tensor(32000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(-1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(0, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41259",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_2 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 13, 14, 15, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "\n",
    "print(np.sort(in_start.reshape(1, 32)))\n",
    "print(np.sort(in_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481ab8",
   "metadata": {},
   "source": [
    "### HalfKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "\n",
    "def FEN_to_HalfKP(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    position = 0\n",
    "    white_king_position = 0\n",
    "    black_king_position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            elif char == 'K':\n",
    "                white_king_position = position\n",
    "                position += 1\n",
    "            elif char == 'k':\n",
    "                black_king_position = position\n",
    "                position += 1\n",
    "            else:\n",
    "                position += 1\n",
    "\n",
    "    white_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    black_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                if (char != 'K') & (char != 'k'):\n",
    "                    piece_index = (piece_dict[char] % 6) * 2 + (piece_dict[char] > 5)\n",
    "                    white_input_layer[position + (piece_index + white_king_position*10)*64] = 1\n",
    "                    black_input_layer[position + (piece_index + black_king_position*10)*64] = 1\n",
    "                    position += 1\n",
    "                else:\n",
    "                    position += 1\n",
    "\n",
    "    return torch.tensor(white_input_layer, dtype=torch.float32), torch.tensor(black_input_layer, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6378bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class HalfKP_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_HalfKP(row[0].strip())\n",
    "                score = float(row[1].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                buf.append((w_in, b_in, stm, score))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                yield w_in, b_in, stm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_FEATURES = 40960\n",
    "M = 1024\n",
    "N = 32\n",
    "K = 1\n",
    "\n",
    "class HalfKPNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HalfKPNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, M)\n",
    "        self.fc2 = nn.Linear(2*M, N)\n",
    "        self.fc3 = nn.Linear(N, K)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)  # [B, 2*M]\n",
    "        cat_bw = torch.cat([b, w], dim=1)  # [B, 2*M]\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = stm * cat_wb + (1 - stm) * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0.0, max = 1.0)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data.csv'\n",
    "dataset = HalfKP_Dataset(csv_path, shuffle_buffer=1000)\n",
    "loader = DataLoader(dataset, batch_size = 128, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/halfKP\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nr_epochs = 500\n",
    "model = HalfKPNNUE().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.05)\n",
    "total_size = 200000000\n",
    "batch_size = 128\n",
    "steps_per_epoch = total_size // batch_size\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "MAE_loss = nn.L1Loss()\n",
    "lowest_MAE = 10000\n",
    "\n",
    "# Transform the CP scores to the WDL space\n",
    "scaling_factor = 400\n",
    "\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "log_interval = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "        #for _ in range(100000):\n",
    "\n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y = batch\n",
    "\n",
    "        # move data to GPU\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)  # remove the last dimension\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = torch.sigmoid(batch_y / scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        # calculate the MSE loss\n",
    "        loss = criterion(wdl_batch_y, wdl_pred)\n",
    "        MAE = MAE_loss(wdl_batch_y, wdl_pred)\n",
    "        running_loss += loss.item()\n",
    "        running_mae += MAE\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            avg_mae = running_mae / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{steps_per_epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"MAE/train\", avg_mae, step)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "\n",
    "        # calculate MAE\n",
    "        if MAE < 0.0002:\n",
    "            lowest_MAE = MAE\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with MAE: {lowest_MAE.item():.4f}, loss: {loss.item():.4f}\")\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, MAE: {MAE.item():.4f}, lowest MAE: {lowest_MAE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def FEN_to_input(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                input_layer[position + piece_dict[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        #self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.clamp(self.fc1(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc3(x), min = 0, max = 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                fen, score, result = FEN_to_input(row[0].strip()), float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield fen, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for fen, score, result in stream:\n",
    "                buf.append((fen, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for fen, score, result in stream:\n",
    "                yield fen, score, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
