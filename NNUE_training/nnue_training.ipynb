{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5d7453",
   "metadata": {},
   "source": [
    "# NNUE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821225",
   "metadata": {},
   "source": [
    "Great source on NNUE: https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead68bd",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Stockfish has a lot of data available for NNUE training in the .binpack format. They have a repo for training NNUEs (nnue-pytorch) that enables efficient dataloading with this format. I don't want to use nnue-pytorch, i want to make my own NNUE training setup.\n",
    "\n",
    "The nnue-pytorch repo also has information on training datasets for NNUEs: https://github.com/official-stockfish/nnue-pytorch/wiki/Training-datasets. They explain how to make your own dataset and link some of the datasets they generated. I will use some of this data, because generating the data myself would be too time-consuming on my hardware.\n",
    "\n",
    "Currently using training data: test80-2024-01-jan-2tb7p.min-v2.v6.binpack.zst from https://huggingface.co/datasets/linrock/test80-2024/tree/main\n",
    "\n",
    "This file contains billions of positions with evaluations in the .binpack format. The stockfish tools branch has a tool to covert the .binpack data into .plain data (https://github.com/official-stockfish/Stockfish/blob/tools/docs/convert.md). I used this tool and stored the first 200M evaluated positions.\n",
    "\n",
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1c61",
   "metadata": {},
   "source": [
    "### Turn FEN into input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict_w = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "piece_dict_b = {'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K':11, 'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "def FEN_to_inputs(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer_w = np.zeros(768, dtype = np.float32)\n",
    "    input_layer_b = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                alt_pos = 63 - (position ^ 7)\n",
    "                input_layer_w[position + piece_dict_w[char]*64] = 1\n",
    "                input_layer_b[alt_pos + piece_dict_b[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer_w, dtype=torch.float32), torch.tensor(input_layer_b, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1938bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Black Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Side to Move: tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1091/2390657366.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(w_features)))\n",
      "/tmp/ipykernel_1091/2390657366.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(b_features)))\n"
     ]
    }
   ],
   "source": [
    "# testing encoding\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "\n",
    "w_features, b_features, stm = FEN_to_inputs(fen1)\n",
    "print(\"White Features:\", sum(w_features))\n",
    "print(np.nonzero(np.array(w_features)))\n",
    "print(\"Black Features:\", sum(b_features))\n",
    "print(np.nonzero(np.array(b_features)))\n",
    "print(\"Side to Move:\", stm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cdf67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test1 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 14, 15, 21, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "test2 = [ 8, 9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192, 199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510, 570, 573, 632, 639, 699, 764]\n",
    "\n",
    "np.sort(test1)\n",
    "np.sort(test2)\n",
    "print(np.array_equal(np.sort(test1), np.sort(test2)))\n",
    "\n",
    "test3 = [  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192, 199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,  570, 573, 632, 639, 699, 764]\n",
    "test4 = [632, 505, 570, 699, 764, 573, 510, 639, 432, 433, 434, 435, 436, 438, 439, 429, 8, 9, 10, 11, 12, 13, 14, 15, 192, 65, 130, 259, 324, 133, 70, 199]\n",
    "print(np.array_equal(np.sort(test3), np.sort(test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c0263",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Input: a sparse, binary array of length 768. Each element of the array represents a possible combination of piece type (6), piece_color (2) and position (64) (6*2*64 = 768).\n",
    "\n",
    "This is a very simple input feature (P feature set) set that will be improved upon later (HalfKP).\n",
    "\n",
    "The fully connected feedfoward network has 4 hidden layers: 768 -> 1024, 1024 -> 8, 8 -> 32 and 32 -> 1.\n",
    "\n",
    "The output is a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01de9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Split_NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split_NNUE, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)\n",
    "        cat_bw = torch.cat([b, w], dim=1)\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = (1 - stm) * cat_wb + stm * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Split_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_inputs(row[0].strip())\n",
    "                score, result = float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                buf.append((w_in, b_in, stm, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                yield w_in, b_in, stm, score, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data_full_10M.csv'\n",
    "dataset = Custom_Split_Dataset(csv_path, shuffle_buffer = 100000)\n",
    "loader = DataLoader(dataset, batch_size = 1024, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa7cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Kaiming uniform for piecewise-linear (ReLU-like) activations:\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0.0, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5bc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | Avg Loss: 0.0390 | Grad Norm: 0.00022215\n",
      "Epoch 1 | Step 200 | Avg Loss: 0.0396 | Grad Norm: 0.00029383\n",
      "Epoch 1 | Step 300 | Avg Loss: 0.0389 | Grad Norm: 0.00034422\n",
      "Epoch 1 | Step 400 | Avg Loss: 0.0397 | Grad Norm: 0.00046375\n",
      "Epoch 1 | Step 500 | Avg Loss: 0.0403 | Grad Norm: 0.00055304\n",
      "Epoch 1 | Step 600 | Avg Loss: 0.0399 | Grad Norm: 0.00066189\n",
      "Epoch 1 | Step 700 | Avg Loss: 0.0408 | Grad Norm: 0.00074175\n",
      "Epoch 1 | Step 800 | Avg Loss: 0.0415 | Grad Norm: 0.00083143\n",
      "Epoch 1 | Step 900 | Avg Loss: 0.0416 | Grad Norm: 0.00104168\n",
      "Epoch 1 | Step 1000 | Avg Loss: 0.0418 | Grad Norm: 0.00116530\n",
      "Epoch 1 | Step 1100 | Avg Loss: 0.0405 | Grad Norm: 0.00117744\n",
      "Epoch 1 | Step 1200 | Avg Loss: 0.0387 | Grad Norm: 0.00125537\n",
      "Epoch 1 | Step 1300 | Avg Loss: 0.0400 | Grad Norm: 0.00132723\n",
      "Epoch 1 | Step 1400 | Avg Loss: 0.0399 | Grad Norm: 0.00138729\n",
      "Epoch 1 | Step 1500 | Avg Loss: 0.0397 | Grad Norm: 0.00158825\n",
      "Epoch 1 | Step 1600 | Avg Loss: 0.0391 | Grad Norm: 0.00146230\n",
      "Epoch 1 | Step 1700 | Avg Loss: 0.0383 | Grad Norm: 0.00154807\n",
      "Epoch 1 | Step 1800 | Avg Loss: 0.0383 | Grad Norm: 0.00147962\n",
      "Epoch 1 | Step 1900 | Avg Loss: 0.0385 | Grad Norm: 0.00167994\n",
      "Epoch 1 | Step 2000 | Avg Loss: 0.0387 | Grad Norm: 0.00204805\n",
      "Epoch 1 | Step 2100 | Avg Loss: 0.0373 | Grad Norm: 0.00186845\n",
      "Epoch 1 | Step 2200 | Avg Loss: 0.0377 | Grad Norm: 0.00190126\n",
      "Epoch 1 | Step 2300 | Avg Loss: 0.0388 | Grad Norm: 0.00196325\n",
      "Epoch 1 | Step 2400 | Avg Loss: 0.0372 | Grad Norm: 0.00235915\n",
      "Epoch 1 | Step 2500 | Avg Loss: 0.0375 | Grad Norm: 0.00245858\n",
      "Epoch 1 | Step 2600 | Avg Loss: 0.0388 | Grad Norm: 0.00305269\n",
      "Epoch 1 | Step 2700 | Avg Loss: 0.0385 | Grad Norm: 0.00246821\n",
      "Epoch 1 | Step 2800 | Avg Loss: 0.0374 | Grad Norm: 0.00252267\n",
      "Epoch 1 | Step 2900 | Avg Loss: 0.0379 | Grad Norm: 0.00273941\n",
      "Epoch 1 | Step 3000 | Avg Loss: 0.0370 | Grad Norm: 0.00314732\n",
      "Epoch 1 | Step 3100 | Avg Loss: 0.0363 | Grad Norm: 0.00339374\n",
      "Epoch 1 | Step 3200 | Avg Loss: 0.0360 | Grad Norm: 0.00250393\n",
      "Epoch 1 | Step 3300 | Avg Loss: 0.0351 | Grad Norm: 0.00295365\n",
      "Epoch 1 | Step 3400 | Avg Loss: 0.0345 | Grad Norm: 0.00272696\n",
      "Epoch 1 | Step 3500 | Avg Loss: 0.0336 | Grad Norm: 0.00366415\n",
      "Epoch 1 | Step 3600 | Avg Loss: 0.0338 | Grad Norm: 0.00343492\n",
      "Epoch 1 | Step 3700 | Avg Loss: 0.0348 | Grad Norm: 0.00289874\n",
      "Epoch 1 | Step 3800 | Avg Loss: 0.0345 | Grad Norm: 0.00341087\n",
      "Epoch 1 | Step 3900 | Avg Loss: 0.0348 | Grad Norm: 0.00266750\n",
      "Epoch 1 | Step 4000 | Avg Loss: 0.0346 | Grad Norm: 0.00290260\n",
      "Epoch 1 | Step 4100 | Avg Loss: 0.0339 | Grad Norm: 0.00320303\n",
      "Epoch 1 | Step 4200 | Avg Loss: 0.0334 | Grad Norm: 0.00309476\n",
      "Epoch 1 | Step 4300 | Avg Loss: 0.0335 | Grad Norm: 0.00394366\n",
      "Epoch 1 | Step 4400 | Avg Loss: 0.0345 | Grad Norm: 0.00373345\n",
      "Epoch 1 | Step 4500 | Avg Loss: 0.0348 | Grad Norm: 0.00393682\n",
      "Epoch 1 | Step 4600 | Avg Loss: 0.0339 | Grad Norm: 0.00411570\n",
      "Epoch 1 | Step 4700 | Avg Loss: 0.0334 | Grad Norm: 0.00450937\n",
      "Epoch 1 | Step 4800 | Avg Loss: 0.0331 | Grad Norm: 0.00391787\n",
      "Epoch 1 | Step 4900 | Avg Loss: 0.0329 | Grad Norm: 0.00455364\n",
      "Epoch 1 | Step 5000 | Avg Loss: 0.0324 | Grad Norm: 0.00349696\n",
      "Epoch 1 | Step 5100 | Avg Loss: 0.0326 | Grad Norm: 0.00389551\n",
      "Epoch 1 | Step 5200 | Avg Loss: 0.0309 | Grad Norm: 0.00465784\n",
      "Epoch 1 | Step 5300 | Avg Loss: 0.0331 | Grad Norm: 0.00405815\n",
      "Epoch 1 | Step 5400 | Avg Loss: 0.0350 | Grad Norm: 0.00383595\n",
      "Epoch 1 | Step 5500 | Avg Loss: 0.0355 | Grad Norm: 0.00473587\n",
      "Epoch 1 | Step 5600 | Avg Loss: 0.0343 | Grad Norm: 0.00402124\n",
      "Epoch 1 | Step 5700 | Avg Loss: 0.0337 | Grad Norm: 0.00371292\n",
      "Epoch 1 | Step 5800 | Avg Loss: 0.0338 | Grad Norm: 0.00372790\n",
      "Epoch 1 | Step 5900 | Avg Loss: 0.0324 | Grad Norm: 0.00438758\n",
      "Epoch 1 | Step 6000 | Avg Loss: 0.0319 | Grad Norm: 0.00473034\n",
      "Epoch 1 | Step 6100 | Avg Loss: 0.0309 | Grad Norm: 0.00525196\n",
      "Epoch 1 | Step 6200 | Avg Loss: 0.0304 | Grad Norm: 0.00437385\n",
      "Epoch 1 | Step 6300 | Avg Loss: 0.0315 | Grad Norm: 0.00471353\n",
      "Epoch 1 | Step 6400 | Avg Loss: 0.0315 | Grad Norm: 0.00423210\n",
      "Epoch 1 | Step 6500 | Avg Loss: 0.0321 | Grad Norm: 0.00566750\n",
      "Epoch 1 | Step 6600 | Avg Loss: 0.0319 | Grad Norm: 0.00435662\n",
      "Epoch 1 | Step 6700 | Avg Loss: 0.0317 | Grad Norm: 0.00442627\n",
      "Epoch 1 | Step 6800 | Avg Loss: 0.0312 | Grad Norm: 0.00479508\n",
      "Epoch 1 | Step 6900 | Avg Loss: 0.0314 | Grad Norm: 0.00482080\n",
      "Epoch 1 | Step 7000 | Avg Loss: 0.0312 | Grad Norm: 0.00426951\n",
      "Epoch 1 | Step 7100 | Avg Loss: 0.0307 | Grad Norm: 0.00414459\n",
      "Epoch 1 | Step 7200 | Avg Loss: 0.0291 | Grad Norm: 0.00429654\n",
      "Epoch 1 | Step 7300 | Avg Loss: 0.0295 | Grad Norm: 0.00401499\n",
      "Epoch 1 | Step 7400 | Avg Loss: 0.0307 | Grad Norm: 0.00469569\n",
      "Epoch 1 | Step 7500 | Avg Loss: 0.0306 | Grad Norm: 0.00442870\n",
      "Epoch 1 | Step 7600 | Avg Loss: 0.0306 | Grad Norm: 0.00548631\n",
      "Epoch 1 | Step 7700 | Avg Loss: 0.0292 | Grad Norm: 0.00483994\n",
      "Epoch 1 | Step 7800 | Avg Loss: 0.0290 | Grad Norm: 0.00533236\n",
      "Epoch 1 | Step 7900 | Avg Loss: 0.0291 | Grad Norm: 0.00546792\n",
      "Epoch 1 | Step 8000 | Avg Loss: 0.0303 | Grad Norm: 0.00531426\n",
      "Epoch 1 | Step 8100 | Avg Loss: 0.0300 | Grad Norm: 0.00504869\n",
      "Epoch 1 | Step 8200 | Avg Loss: 0.0295 | Grad Norm: 0.00439174\n",
      "Epoch 1 | Step 8300 | Avg Loss: 0.0295 | Grad Norm: 0.00453561\n",
      "Epoch 1 | Step 8400 | Avg Loss: 0.0290 | Grad Norm: 0.00527447\n",
      "Epoch 1 | Step 8500 | Avg Loss: 0.0286 | Grad Norm: 0.00489576\n",
      "Epoch 1 | Step 8600 | Avg Loss: 0.0294 | Grad Norm: 0.00579924\n",
      "Epoch 1 | Step 8700 | Avg Loss: 0.0296 | Grad Norm: 0.00628339\n",
      "Epoch 1 | Step 8800 | Avg Loss: 0.0294 | Grad Norm: 0.00531387\n",
      "Epoch 1 | Step 8900 | Avg Loss: 0.0293 | Grad Norm: 0.00495559\n",
      "Epoch 1 | Step 9000 | Avg Loss: 0.0293 | Grad Norm: 0.00519378\n",
      "Epoch 1 | Step 9100 | Avg Loss: 0.0297 | Grad Norm: 0.00569525\n",
      "Epoch 1 | Step 9200 | Avg Loss: 0.0295 | Grad Norm: 0.00671028\n",
      "Epoch 1 | Step 9300 | Avg Loss: 0.0285 | Grad Norm: 0.00606307\n",
      "Epoch 1 | Step 9400 | Avg Loss: 0.0277 | Grad Norm: 0.00596718\n",
      "Epoch 1 | Step 9500 | Avg Loss: 0.0287 | Grad Norm: 0.00536015\n",
      "Epoch 1 | Step 9600 | Avg Loss: 0.0284 | Grad Norm: 0.00502031\n",
      "Epoch 1 | Step 9700 | Avg Loss: 0.0289 | Grad Norm: 0.00620694\n",
      "Epoch 1 | Step 9800 | Avg Loss: 0.0290 | Grad Norm: 0.00548434\n",
      "Epoch 1 | Step 9900 | Avg Loss: 0.0286 | Grad Norm: 0.00501044\n",
      "Epoch 1 | Step 10000 | Avg Loss: 0.0289 | Grad Norm: 0.00579969\n",
      "Epoch 1 | Step 10100 | Avg Loss: 0.0290 | Grad Norm: 0.00578089\n",
      "Epoch 1 | Step 10200 | Avg Loss: 0.0293 | Grad Norm: 0.00652248\n",
      "Epoch 1 | Step 10300 | Avg Loss: 0.0296 | Grad Norm: 0.00614516\n",
      "Epoch 1 | Step 10400 | Avg Loss: 0.0292 | Grad Norm: 0.00574244\n",
      "Epoch 1 | Step 10500 | Avg Loss: 0.0288 | Grad Norm: 0.00630168\n",
      "Epoch 1 | Step 10600 | Avg Loss: 0.0287 | Grad Norm: 0.00638766\n",
      "Epoch 1 | Step 10700 | Avg Loss: 0.0279 | Grad Norm: 0.00593039\n",
      "Epoch 1 | Step 10800 | Avg Loss: 0.0285 | Grad Norm: 0.00570010\n",
      "Epoch 1 | Step 10900 | Avg Loss: 0.0272 | Grad Norm: 0.00678245\n",
      "Epoch 1 | Step 11000 | Avg Loss: 0.0268 | Grad Norm: 0.00589906\n",
      "Epoch 1 | Step 11100 | Avg Loss: 0.0277 | Grad Norm: 0.00529073\n",
      "Epoch 1 | Step 11200 | Avg Loss: 0.0274 | Grad Norm: 0.00573765\n",
      "Epoch 1 | Step 11300 | Avg Loss: 0.0278 | Grad Norm: 0.00654501\n",
      "Epoch 1 | Step 11400 | Avg Loss: 0.0273 | Grad Norm: 0.00608205\n",
      "Epoch 1 | Step 11500 | Avg Loss: 0.0271 | Grad Norm: 0.00602996\n",
      "Epoch 1 | Step 11600 | Avg Loss: 0.0278 | Grad Norm: 0.00624975\n",
      "Epoch 1 | Step 11700 | Avg Loss: 0.0273 | Grad Norm: 0.00559611\n",
      "Epoch 1 | Step 11800 | Avg Loss: 0.0275 | Grad Norm: 0.00589962\n",
      "Epoch 1 | Step 11900 | Avg Loss: 0.0261 | Grad Norm: 0.00634175\n",
      "Epoch 1 | Step 12000 | Avg Loss: 0.0261 | Grad Norm: 0.00634819\n",
      "Epoch 1 | Step 12100 | Avg Loss: 0.0259 | Grad Norm: 0.00551433\n",
      "Epoch 1 | Step 12200 | Avg Loss: 0.0266 | Grad Norm: 0.00570754\n",
      "Epoch 1 | Step 12300 | Avg Loss: 0.0263 | Grad Norm: 0.00523001\n",
      "Epoch 1 | Step 12400 | Avg Loss: 0.0276 | Grad Norm: 0.00728266\n",
      "Epoch 1 | Step 12500 | Avg Loss: 0.0278 | Grad Norm: 0.00640164\n",
      "Epoch 1 | Step 12600 | Avg Loss: 0.0268 | Grad Norm: 0.00641481\n",
      "Epoch 1 | Step 12700 | Avg Loss: 0.0269 | Grad Norm: 0.00696006\n",
      "Epoch 1 | Step 12800 | Avg Loss: 0.0258 | Grad Norm: 0.00685642\n",
      "Epoch 1 | Step 12900 | Avg Loss: 0.0266 | Grad Norm: 0.00688888\n",
      "Epoch 1 | Step 13000 | Avg Loss: 0.0264 | Grad Norm: 0.00753281\n",
      "Epoch 1 | Step 13100 | Avg Loss: 0.0257 | Grad Norm: 0.00735303\n",
      "Epoch 1 | Step 13200 | Avg Loss: 0.0250 | Grad Norm: 0.00688707\n",
      "Epoch 1 | Step 13300 | Avg Loss: 0.0250 | Grad Norm: 0.00572930\n",
      "Epoch 1 | Step 13400 | Avg Loss: 0.0253 | Grad Norm: 0.00649382\n",
      "Epoch 1 | Step 13500 | Avg Loss: 0.0253 | Grad Norm: 0.00683555\n",
      "Epoch 1 | Step 13600 | Avg Loss: 0.0258 | Grad Norm: 0.00622151\n",
      "Epoch 1 | Step 13700 | Avg Loss: 0.0243 | Grad Norm: 0.00735737\n",
      "Epoch 1 | Step 13800 | Avg Loss: 0.0256 | Grad Norm: 0.00735826\n",
      "Epoch 1 | Step 13900 | Avg Loss: 0.0273 | Grad Norm: 0.00597636\n",
      "Epoch 1 | Step 14000 | Avg Loss: 0.0260 | Grad Norm: 0.00650490\n",
      "Epoch 1 | Step 14100 | Avg Loss: 0.0255 | Grad Norm: 0.00644691\n",
      "Epoch 1 | Step 14200 | Avg Loss: 0.0252 | Grad Norm: 0.00696153\n",
      "Epoch 1 | Step 14300 | Avg Loss: 0.0250 | Grad Norm: 0.00541409\n",
      "Epoch 1 | Step 14400 | Avg Loss: 0.0245 | Grad Norm: 0.00644113\n",
      "Epoch 1 | Step 14500 | Avg Loss: 0.0245 | Grad Norm: 0.00673927\n",
      "Epoch 1 | Step 14600 | Avg Loss: 0.0242 | Grad Norm: 0.00694655\n",
      "Epoch 1 | Step 14700 | Avg Loss: 0.0242 | Grad Norm: 0.00666587\n",
      "Epoch 1 | Step 14800 | Avg Loss: 0.0239 | Grad Norm: 0.00743313\n",
      "Epoch 1 | Step 14900 | Avg Loss: 0.0241 | Grad Norm: 0.00736218\n",
      "Epoch 1 | Step 15000 | Avg Loss: 0.0246 | Grad Norm: 0.00684101\n",
      "Epoch 1 | Step 15100 | Avg Loss: 0.0244 | Grad Norm: 0.00665646\n",
      "Epoch 1 | Step 15200 | Avg Loss: 0.0244 | Grad Norm: 0.00721232\n",
      "Epoch 1 | Step 15300 | Avg Loss: 0.0243 | Grad Norm: 0.00728694\n",
      "Epoch 1 | Step 15400 | Avg Loss: 0.0243 | Grad Norm: 0.00679735\n",
      "Epoch 1 | Step 15500 | Avg Loss: 0.0248 | Grad Norm: 0.00842546\n",
      "Epoch 1 | Step 15600 | Avg Loss: 0.0248 | Grad Norm: 0.00645318\n",
      "Epoch 1 | Step 15700 | Avg Loss: 0.0247 | Grad Norm: 0.00874816\n",
      "Epoch 1 | Step 15800 | Avg Loss: 0.0255 | Grad Norm: 0.00719155\n",
      "Epoch 1 | Step 15900 | Avg Loss: 0.0257 | Grad Norm: 0.00732973\n",
      "Epoch 1 | Step 16000 | Avg Loss: 0.0259 | Grad Norm: 0.00719701\n",
      "Epoch 1 | Step 16100 | Avg Loss: 0.0258 | Grad Norm: 0.00833588\n",
      "Epoch 1 | Step 16200 | Avg Loss: 0.0259 | Grad Norm: 0.00800342\n",
      "Epoch 1 | Step 16300 | Avg Loss: 0.0251 | Grad Norm: 0.00752682\n",
      "Epoch 1 | Step 16400 | Avg Loss: 0.0246 | Grad Norm: 0.00642726\n",
      "Epoch 1 | Step 16500 | Avg Loss: 0.0249 | Grad Norm: 0.00802989\n",
      "Epoch 1 | Step 16600 | Avg Loss: 0.0237 | Grad Norm: 0.00706483\n",
      "Epoch 1 | Step 16700 | Avg Loss: 0.0232 | Grad Norm: 0.00736505\n",
      "Epoch 1 | Step 16800 | Avg Loss: 0.0233 | Grad Norm: 0.00674310\n",
      "Epoch 1 | Step 16900 | Avg Loss: 0.0236 | Grad Norm: 0.00704213\n",
      "Epoch 1 | Step 17000 | Avg Loss: 0.0229 | Grad Norm: 0.00675823\n",
      "Epoch 1 | Step 17100 | Avg Loss: 0.0225 | Grad Norm: 0.00707369\n",
      "Epoch 1 | Step 17200 | Avg Loss: 0.0222 | Grad Norm: 0.00843930\n",
      "Epoch 1 | Step 17300 | Avg Loss: 0.0223 | Grad Norm: 0.00743899\n",
      "Epoch 1 | Step 17400 | Avg Loss: 0.0225 | Grad Norm: 0.00676203\n",
      "Epoch 1 | Step 17500 | Avg Loss: 0.0225 | Grad Norm: 0.00897977\n",
      "Epoch 1 | Step 17600 | Avg Loss: 0.0216 | Grad Norm: 0.00795957\n",
      "Epoch 1 | Step 17700 | Avg Loss: 0.0221 | Grad Norm: 0.00698724\n",
      "Epoch 1 | Step 17800 | Avg Loss: 0.0224 | Grad Norm: 0.00697646\n",
      "Epoch 1 | Step 17900 | Avg Loss: 0.0228 | Grad Norm: 0.00717433\n",
      "Epoch 1 | Step 18000 | Avg Loss: 0.0233 | Grad Norm: 0.00675009\n",
      "Epoch 1 | Step 18100 | Avg Loss: 0.0231 | Grad Norm: 0.00767551\n",
      "Epoch 1 | Step 18200 | Avg Loss: 0.0225 | Grad Norm: 0.00653376\n",
      "Epoch 1 | Step 18300 | Avg Loss: 0.0227 | Grad Norm: 0.00694059\n",
      "Epoch 1 | Step 18400 | Avg Loss: 0.0231 | Grad Norm: 0.00726872\n",
      "Epoch 1 | Step 18500 | Avg Loss: 0.0230 | Grad Norm: 0.00765425\n",
      "Epoch 1 | Step 18600 | Avg Loss: 0.0233 | Grad Norm: 0.00778334\n",
      "Epoch 1 | Step 18700 | Avg Loss: 0.0232 | Grad Norm: 0.00799049\n",
      "Epoch 1 | Step 18800 | Avg Loss: 0.0228 | Grad Norm: 0.00794269\n",
      "Epoch 1 | Step 18900 | Avg Loss: 0.0233 | Grad Norm: 0.00740184\n",
      "Epoch 1 | Step 19000 | Avg Loss: 0.0239 | Grad Norm: 0.00785529\n",
      "Epoch 1 | Step 19100 | Avg Loss: 0.0240 | Grad Norm: 0.00716117\n",
      "Epoch 1 | Step 19200 | Avg Loss: 0.0241 | Grad Norm: 0.00709852\n",
      "Epoch 1 | Step 19300 | Avg Loss: 0.0232 | Grad Norm: 0.00774950\n",
      "Epoch 1 | Step 19400 | Avg Loss: 0.0219 | Grad Norm: 0.00693571\n",
      "Epoch 1 | Step 19500 | Avg Loss: 0.0218 | Grad Norm: 0.00767624\n",
      "Epoch 1 | Step 19600 | Avg Loss: 0.0210 | Grad Norm: 0.00633934\n",
      "Epoch 1 | Step 19700 | Avg Loss: 0.0213 | Grad Norm: 0.00740926\n",
      "Epoch 1 | Step 19800 | Avg Loss: 0.0206 | Grad Norm: 0.00727157\n",
      "Epoch 1 | Step 19900 | Avg Loss: 0.0207 | Grad Norm: 0.00685449\n",
      "Epoch 1 | Step 20000 | Avg Loss: 0.0211 | Grad Norm: 0.00707619\n",
      "Epoch 1 | Step 20100 | Avg Loss: 0.0211 | Grad Norm: 0.00778404\n",
      "Epoch 1 | Step 20200 | Avg Loss: 0.0209 | Grad Norm: 0.00788237\n",
      "Epoch 1 | Step 20300 | Avg Loss: 0.0212 | Grad Norm: 0.00797476\n",
      "Epoch 1 | Step 20400 | Avg Loss: 0.0215 | Grad Norm: 0.00803257\n",
      "Epoch 1 | Step 20500 | Avg Loss: 0.0219 | Grad Norm: 0.00681260\n",
      "Epoch 1 | Step 20600 | Avg Loss: 0.0222 | Grad Norm: 0.00826194\n",
      "Epoch 1 | Step 20700 | Avg Loss: 0.0217 | Grad Norm: 0.00766616\n",
      "Epoch 1 | Step 20800 | Avg Loss: 0.0216 | Grad Norm: 0.00679398\n",
      "Epoch 1 | Step 20900 | Avg Loss: 0.0214 | Grad Norm: 0.00719994\n",
      "Epoch 1 | Step 21000 | Avg Loss: 0.0220 | Grad Norm: 0.00661840\n",
      "Epoch 1 | Step 21100 | Avg Loss: 0.0212 | Grad Norm: 0.00798903\n",
      "Epoch 1 | Step 21200 | Avg Loss: 0.0220 | Grad Norm: 0.00710750\n",
      "Epoch 1 | Step 21300 | Avg Loss: 0.0216 | Grad Norm: 0.00718496\n",
      "Epoch 1 | Step 21400 | Avg Loss: 0.0219 | Grad Norm: 0.00958919\n",
      "Epoch 1 | Step 21500 | Avg Loss: 0.0218 | Grad Norm: 0.00855612\n",
      "Epoch 1 | Step 21600 | Avg Loss: 0.0222 | Grad Norm: 0.00722461\n",
      "Epoch 1 | Step 21700 | Avg Loss: 0.0217 | Grad Norm: 0.00835047\n",
      "Epoch 1 | Step 21800 | Avg Loss: 0.0218 | Grad Norm: 0.00761204\n",
      "Epoch 1 | Step 21900 | Avg Loss: 0.0215 | Grad Norm: 0.00690861\n",
      "Epoch 1 | Step 22000 | Avg Loss: 0.0212 | Grad Norm: 0.00756221\n",
      "Epoch 1 | Step 22100 | Avg Loss: 0.0209 | Grad Norm: 0.00740805\n",
      "Epoch 1 | Step 22200 | Avg Loss: 0.0208 | Grad Norm: 0.00795848\n",
      "Epoch 1 | Step 22300 | Avg Loss: 0.0210 | Grad Norm: 0.00769875\n",
      "Epoch 1 | Step 22400 | Avg Loss: 0.0216 | Grad Norm: 0.00786821\n",
      "Epoch 1 | Step 22500 | Avg Loss: 0.0217 | Grad Norm: 0.00869771\n",
      "Epoch 1 | Step 22600 | Avg Loss: 0.0212 | Grad Norm: 0.00747865\n",
      "Epoch 1 | Step 22700 | Avg Loss: 0.0215 | Grad Norm: 0.00832066\n",
      "Epoch 1 | Step 22800 | Avg Loss: 0.0212 | Grad Norm: 0.00810042\n",
      "Epoch 1 | Step 22900 | Avg Loss: 0.0212 | Grad Norm: 0.00726744\n",
      "Epoch 1 | Step 23000 | Avg Loss: 0.0205 | Grad Norm: 0.00714615\n",
      "Epoch 1 | Step 23100 | Avg Loss: 0.0200 | Grad Norm: 0.00835968\n",
      "Epoch 1 | Step 23200 | Avg Loss: 0.0197 | Grad Norm: 0.00859018\n",
      "Epoch 1 | Step 23300 | Avg Loss: 0.0198 | Grad Norm: 0.00744036\n",
      "Epoch 1 | Step 23400 | Avg Loss: 0.0203 | Grad Norm: 0.00671579\n",
      "Epoch 1 | Step 23500 | Avg Loss: 0.0204 | Grad Norm: 0.00824384\n",
      "Epoch 1 | Step 23600 | Avg Loss: 0.0211 | Grad Norm: 0.00798700\n",
      "Epoch 1 | Step 23700 | Avg Loss: 0.0211 | Grad Norm: 0.00858378\n",
      "Epoch 1 | Step 23800 | Avg Loss: 0.0209 | Grad Norm: 0.00840716\n",
      "Epoch 1 | Step 23900 | Avg Loss: 0.0203 | Grad Norm: 0.00812620\n",
      "Epoch 1 | Step 24000 | Avg Loss: 0.0206 | Grad Norm: 0.00819108\n",
      "Epoch 1 | Step 24100 | Avg Loss: 0.0206 | Grad Norm: 0.00773413\n",
      "Epoch 1 | Step 24200 | Avg Loss: 0.0207 | Grad Norm: 0.00797016\n",
      "Epoch 1 | Step 24300 | Avg Loss: 0.0207 | Grad Norm: 0.00731843\n",
      "Epoch 1 | Step 24400 | Avg Loss: 0.0204 | Grad Norm: 0.00774866\n",
      "Epoch 1 | Step 24500 | Avg Loss: 0.0204 | Grad Norm: 0.00800893\n",
      "Epoch 1 | Step 24600 | Avg Loss: 0.0214 | Grad Norm: 0.00877407\n",
      "Epoch 1 | Step 24700 | Avg Loss: 0.0218 | Grad Norm: 0.00857716\n",
      "Epoch 1 | Step 24800 | Avg Loss: 0.0208 | Grad Norm: 0.00693723\n",
      "Epoch 1 | Step 24900 | Avg Loss: 0.0201 | Grad Norm: 0.00821497\n",
      "Epoch 1 | Step 25000 | Avg Loss: 0.0203 | Grad Norm: 0.00788779\n",
      "Epoch 1 | Step 25100 | Avg Loss: 0.0209 | Grad Norm: 0.00855537\n",
      "Epoch 1 | Step 25200 | Avg Loss: 0.0214 | Grad Norm: 0.00917291\n",
      "Epoch 1 | Step 25300 | Avg Loss: 0.0216 | Grad Norm: 0.00831761\n",
      "Epoch 1 | Step 25400 | Avg Loss: 0.0216 | Grad Norm: 0.00868104\n",
      "Epoch 1 | Step 25500 | Avg Loss: 0.0216 | Grad Norm: 0.00800454\n",
      "Epoch 1 | Step 25600 | Avg Loss: 0.0211 | Grad Norm: 0.00728388\n",
      "Epoch 1 | Step 25700 | Avg Loss: 0.0214 | Grad Norm: 0.00768316\n",
      "Epoch 1 | Step 25800 | Avg Loss: 0.0211 | Grad Norm: 0.00966778\n",
      "Epoch 1 | Step 25900 | Avg Loss: 0.0212 | Grad Norm: 0.00872462\n",
      "Epoch 1 | Step 26000 | Avg Loss: 0.0205 | Grad Norm: 0.00897588\n",
      "Epoch 1 | Step 26100 | Avg Loss: 0.0201 | Grad Norm: 0.00761650\n",
      "Epoch 1 | Step 26200 | Avg Loss: 0.0196 | Grad Norm: 0.00793462\n",
      "Epoch 1 | Step 26300 | Avg Loss: 0.0203 | Grad Norm: 0.00794725\n",
      "Epoch 1 | Step 26400 | Avg Loss: 0.0192 | Grad Norm: 0.00796590\n",
      "Epoch 1 | Step 26500 | Avg Loss: 0.0199 | Grad Norm: 0.00754342\n",
      "Epoch 1 | Step 26600 | Avg Loss: 0.0195 | Grad Norm: 0.00876559\n",
      "Epoch 1 | Step 26700 | Avg Loss: 0.0202 | Grad Norm: 0.00789175\n",
      "Epoch 1 | Step 26800 | Avg Loss: 0.0198 | Grad Norm: 0.00746164\n",
      "Epoch 1 | Step 26900 | Avg Loss: 0.0203 | Grad Norm: 0.00719180\n",
      "Epoch 1 | Step 27000 | Avg Loss: 0.0201 | Grad Norm: 0.00860600\n",
      "Epoch 1 | Step 27100 | Avg Loss: 0.0199 | Grad Norm: 0.00855478\n",
      "Epoch 1 | Step 27200 | Avg Loss: 0.0191 | Grad Norm: 0.00752807\n",
      "Epoch 1 | Step 27300 | Avg Loss: 0.0190 | Grad Norm: 0.00771437\n",
      "Epoch 1 | Step 27400 | Avg Loss: 0.0199 | Grad Norm: 0.00823776\n",
      "Epoch 1 | Step 27500 | Avg Loss: 0.0203 | Grad Norm: 0.00813305\n",
      "Epoch 1 | Step 27600 | Avg Loss: 0.0201 | Grad Norm: 0.00837897\n",
      "Epoch 1 | Step 27700 | Avg Loss: 0.0196 | Grad Norm: 0.00779779\n",
      "Epoch 1 | Step 27800 | Avg Loss: 0.0201 | Grad Norm: 0.00851310\n",
      "Epoch 1 | Step 27900 | Avg Loss: 0.0198 | Grad Norm: 0.00840168\n",
      "Epoch 1 | Step 28000 | Avg Loss: 0.0192 | Grad Norm: 0.00730318\n",
      "Epoch 1 | Step 28100 | Avg Loss: 0.0194 | Grad Norm: 0.00791231\n",
      "Epoch 1 | Step 28200 | Avg Loss: 0.0194 | Grad Norm: 0.00802943\n",
      "Epoch 1 | Step 28300 | Avg Loss: 0.0189 | Grad Norm: 0.00961849\n",
      "Epoch 1 | Step 28400 | Avg Loss: 0.0188 | Grad Norm: 0.00768281\n",
      "Epoch 1 | Step 28500 | Avg Loss: 0.0194 | Grad Norm: 0.00888371\n",
      "Epoch 1 | Step 28600 | Avg Loss: 0.0192 | Grad Norm: 0.00810253\n",
      "Epoch 1 | Step 28700 | Avg Loss: 0.0185 | Grad Norm: 0.00745552\n",
      "Epoch 1 | Step 28800 | Avg Loss: 0.0186 | Grad Norm: 0.00832176\n",
      "Epoch 1 | Step 28900 | Avg Loss: 0.0188 | Grad Norm: 0.00846379\n",
      "Epoch 1 | Step 29000 | Avg Loss: 0.0180 | Grad Norm: 0.00827708\n",
      "Epoch 1 | Step 29100 | Avg Loss: 0.0188 | Grad Norm: 0.00734331\n",
      "Epoch 1 | Step 29200 | Avg Loss: 0.0190 | Grad Norm: 0.00807855\n",
      "Epoch 1 | Step 29300 | Avg Loss: 0.0182 | Grad Norm: 0.00905950\n",
      "Epoch 1 | Step 29400 | Avg Loss: 0.0181 | Grad Norm: 0.00843621\n",
      "Epoch 1 | Step 29500 | Avg Loss: 0.0187 | Grad Norm: 0.00852879\n",
      "Epoch 1 | Step 29600 | Avg Loss: 0.0184 | Grad Norm: 0.00803911\n",
      "Epoch 1 | Step 29700 | Avg Loss: 0.0195 | Grad Norm: 0.00756269\n",
      "Epoch 1 | Step 29800 | Avg Loss: 0.0197 | Grad Norm: 0.00888261\n",
      "Epoch 1 | Step 29900 | Avg Loss: 0.0199 | Grad Norm: 0.00777675\n",
      "Epoch 1 | Step 30000 | Avg Loss: 0.0198 | Grad Norm: 0.00823848\n",
      "Epoch 1 | Step 30100 | Avg Loss: 0.0200 | Grad Norm: 0.00844563\n",
      "Epoch 1 | Step 30200 | Avg Loss: 0.0203 | Grad Norm: 0.00856592\n",
      "Epoch 1 | Step 30300 | Avg Loss: 0.0200 | Grad Norm: 0.00761752\n",
      "Epoch 1 | Step 30400 | Avg Loss: 0.0201 | Grad Norm: 0.00769259\n",
      "Epoch 1 | Step 30500 | Avg Loss: 0.0191 | Grad Norm: 0.00875229\n",
      "Epoch 1 | Step 30600 | Avg Loss: 0.0185 | Grad Norm: 0.00723055\n",
      "Epoch 1 | Step 30700 | Avg Loss: 0.0186 | Grad Norm: 0.00770420\n",
      "Epoch 1 | Step 30800 | Avg Loss: 0.0183 | Grad Norm: 0.00768108\n",
      "Epoch 1 | Step 30900 | Avg Loss: 0.0185 | Grad Norm: 0.00849674\n",
      "Epoch 1 | Step 31000 | Avg Loss: 0.0183 | Grad Norm: 0.00895875\n",
      "Epoch 1 | Step 31100 | Avg Loss: 0.0187 | Grad Norm: 0.00851201\n",
      "Epoch 1 | Step 31200 | Avg Loss: 0.0189 | Grad Norm: 0.00883674\n",
      "Epoch 1 | Step 31300 | Avg Loss: 0.0183 | Grad Norm: 0.00691229\n",
      "Epoch 1 | Step 31400 | Avg Loss: 0.0188 | Grad Norm: 0.00851694\n",
      "Epoch 1 | Step 31500 | Avg Loss: 0.0187 | Grad Norm: 0.00781379\n",
      "Epoch 1 | Step 31600 | Avg Loss: 0.0187 | Grad Norm: 0.00802950\n",
      "Epoch 1 | Step 31700 | Avg Loss: 0.0183 | Grad Norm: 0.00881003\n",
      "Epoch 1 | Step 31800 | Avg Loss: 0.0192 | Grad Norm: 0.00801914\n",
      "Epoch 1 | Step 31900 | Avg Loss: 0.0191 | Grad Norm: 0.00730744\n",
      "Epoch 1 | Step 32000 | Avg Loss: 0.0190 | Grad Norm: 0.00785528\n",
      "Epoch 1 | Step 32100 | Avg Loss: 0.0191 | Grad Norm: 0.00898467\n",
      "Epoch 1 | Step 32200 | Avg Loss: 0.0192 | Grad Norm: 0.00834433\n",
      "Epoch 1 | Step 32300 | Avg Loss: 0.0191 | Grad Norm: 0.00792150\n",
      "Epoch 1 | Step 32400 | Avg Loss: 0.0192 | Grad Norm: 0.00909766\n",
      "Epoch 1 | Step 32500 | Avg Loss: 0.0193 | Grad Norm: 0.00874254\n",
      "Epoch 1 | Step 32600 | Avg Loss: 0.0188 | Grad Norm: 0.00787958\n",
      "Epoch 1 | Step 32700 | Avg Loss: 0.0188 | Grad Norm: 0.00893121\n",
      "Epoch 1 | Step 32800 | Avg Loss: 0.0188 | Grad Norm: 0.00823846\n",
      "Epoch 1 | Step 32900 | Avg Loss: 0.0181 | Grad Norm: 0.00824695\n",
      "Epoch 1 | Step 33000 | Avg Loss: 0.0185 | Grad Norm: 0.00908691\n",
      "Epoch 1 | Step 33100 | Avg Loss: 0.0180 | Grad Norm: 0.00938260\n",
      "Epoch 1 | Step 33200 | Avg Loss: 0.0180 | Grad Norm: 0.00832716\n",
      "Epoch 1 | Step 33300 | Avg Loss: 0.0177 | Grad Norm: 0.00789588\n",
      "Epoch 1 | Step 33400 | Avg Loss: 0.0179 | Grad Norm: 0.00884550\n",
      "Epoch 1 | Step 33500 | Avg Loss: 0.0180 | Grad Norm: 0.00805681\n",
      "Epoch 1 | Step 33600 | Avg Loss: 0.0181 | Grad Norm: 0.00894473\n",
      "Epoch 1 | Step 33700 | Avg Loss: 0.0179 | Grad Norm: 0.00710514\n",
      "Epoch 1 | Step 33800 | Avg Loss: 0.0180 | Grad Norm: 0.00808192\n",
      "Epoch 1 | Step 33900 | Avg Loss: 0.0179 | Grad Norm: 0.00809311\n",
      "Epoch 1 | Step 34000 | Avg Loss: 0.0173 | Grad Norm: 0.00844052\n",
      "Epoch 1 | Step 34100 | Avg Loss: 0.0166 | Grad Norm: 0.00762084\n",
      "Epoch 1 | Step 34200 | Avg Loss: 0.0167 | Grad Norm: 0.00788608\n",
      "Epoch 1 | Step 34300 | Avg Loss: 0.0173 | Grad Norm: 0.00760550\n",
      "Epoch 1 | Step 34400 | Avg Loss: 0.0177 | Grad Norm: 0.00760686\n",
      "Epoch 1 | Step 34500 | Avg Loss: 0.0177 | Grad Norm: 0.00840414\n",
      "Epoch 1 | Step 34600 | Avg Loss: 0.0181 | Grad Norm: 0.00950668\n",
      "Epoch 1 | Step 34700 | Avg Loss: 0.0175 | Grad Norm: 0.00926958\n",
      "Epoch 1 | Step 34800 | Avg Loss: 0.0173 | Grad Norm: 0.00815859\n",
      "Epoch 1 | Step 34900 | Avg Loss: 0.0175 | Grad Norm: 0.00901584\n",
      "Epoch 1 | Step 35000 | Avg Loss: 0.0172 | Grad Norm: 0.00875145\n",
      "Epoch 1 | Step 35100 | Avg Loss: 0.0175 | Grad Norm: 0.00934926\n",
      "Epoch 1 | Step 35200 | Avg Loss: 0.0180 | Grad Norm: 0.00818718\n",
      "Epoch 1 | Step 35300 | Avg Loss: 0.0180 | Grad Norm: 0.00850524\n",
      "Epoch 1 | Step 35400 | Avg Loss: 0.0177 | Grad Norm: 0.00792021\n",
      "Epoch 1 | Step 35500 | Avg Loss: 0.0180 | Grad Norm: 0.01017225\n",
      "Epoch 1 | Step 35600 | Avg Loss: 0.0179 | Grad Norm: 0.00852768\n",
      "Epoch 1 | Step 35700 | Avg Loss: 0.0178 | Grad Norm: 0.00847748\n",
      "Epoch 1 | Step 35800 | Avg Loss: 0.0176 | Grad Norm: 0.00883858\n",
      "Epoch 1 | Step 35900 | Avg Loss: 0.0180 | Grad Norm: 0.00858901\n",
      "Epoch 1 | Step 36000 | Avg Loss: 0.0172 | Grad Norm: 0.00864383\n",
      "Epoch 1 | Step 36100 | Avg Loss: 0.0169 | Grad Norm: 0.00789975\n",
      "Epoch 1 | Step 36200 | Avg Loss: 0.0179 | Grad Norm: 0.00861689\n",
      "Epoch 1 | Step 36300 | Avg Loss: 0.0182 | Grad Norm: 0.01011043\n",
      "Epoch 1 | Step 36400 | Avg Loss: 0.0184 | Grad Norm: 0.00816352\n",
      "Epoch 1 | Step 36500 | Avg Loss: 0.0182 | Grad Norm: 0.00946338\n",
      "Epoch 1 | Step 36600 | Avg Loss: 0.0178 | Grad Norm: 0.00935306\n",
      "Epoch 1 | Step 36700 | Avg Loss: 0.0180 | Grad Norm: 0.00783207\n",
      "Epoch 1 | Step 36800 | Avg Loss: 0.0182 | Grad Norm: 0.00868107\n",
      "Epoch 1 | Step 36900 | Avg Loss: 0.0184 | Grad Norm: 0.00879168\n",
      "Epoch 1 | Step 37000 | Avg Loss: 0.0189 | Grad Norm: 0.00894156\n",
      "Epoch 1 | Step 37100 | Avg Loss: 0.0183 | Grad Norm: 0.00940155\n",
      "Epoch 1 | Step 37200 | Avg Loss: 0.0177 | Grad Norm: 0.00877131\n",
      "Epoch 1 | Step 37300 | Avg Loss: 0.0171 | Grad Norm: 0.00801415\n",
      "Epoch 1 | Step 37400 | Avg Loss: 0.0172 | Grad Norm: 0.00869886\n",
      "Epoch 1 | Step 37500 | Avg Loss: 0.0167 | Grad Norm: 0.00829882\n",
      "Epoch 1 | Step 37600 | Avg Loss: 0.0168 | Grad Norm: 0.00901126\n",
      "Epoch 1 | Step 37700 | Avg Loss: 0.0171 | Grad Norm: 0.00918549\n",
      "Epoch 1 | Step 37800 | Avg Loss: 0.0172 | Grad Norm: 0.00788218\n",
      "Epoch 1 | Step 37900 | Avg Loss: 0.0174 | Grad Norm: 0.00883448\n",
      "Epoch 1 | Step 38000 | Avg Loss: 0.0175 | Grad Norm: 0.00873717\n",
      "Epoch 1 | Step 38100 | Avg Loss: 0.0173 | Grad Norm: 0.00962246\n",
      "Epoch 1 | Step 38200 | Avg Loss: 0.0173 | Grad Norm: 0.00789924\n",
      "Epoch 1 | Step 38300 | Avg Loss: 0.0175 | Grad Norm: 0.00917273\n",
      "Epoch 1 | Step 38400 | Avg Loss: 0.0175 | Grad Norm: 0.00828436\n",
      "Epoch 1 | Step 38500 | Avg Loss: 0.0175 | Grad Norm: 0.00850021\n",
      "Epoch 1 | Step 38600 | Avg Loss: 0.0173 | Grad Norm: 0.00849940\n",
      "Epoch 1 | Step 38700 | Avg Loss: 0.0172 | Grad Norm: 0.00932075\n",
      "Epoch 1 | Step 38800 | Avg Loss: 0.0158 | Grad Norm: 0.00807661\n",
      "Epoch 1 | Step 38900 | Avg Loss: 0.0147 | Grad Norm: 0.00783317\n",
      "Epoch 1 | Step 39000 | Avg Loss: 0.0140 | Grad Norm: 0.00844557\n",
      "Epoch 1, Loss: 0.0150\n",
      "Epoch 2 | Step 39100 | Avg Loss: 0.0167 | Grad Norm: 0.00917903\n",
      "Epoch 2 | Step 39200 | Avg Loss: 0.0190 | Grad Norm: 0.00771917\n",
      "Epoch 2 | Step 39300 | Avg Loss: 0.0175 | Grad Norm: 0.00801356\n",
      "Epoch 2 | Step 39400 | Avg Loss: 0.0166 | Grad Norm: 0.00781601\n",
      "Epoch 2 | Step 39500 | Avg Loss: 0.0165 | Grad Norm: 0.00811970\n",
      "Epoch 2 | Step 39600 | Avg Loss: 0.0171 | Grad Norm: 0.01031810\n",
      "Epoch 2 | Step 39700 | Avg Loss: 0.0170 | Grad Norm: 0.00840827\n",
      "Epoch 2 | Step 39800 | Avg Loss: 0.0174 | Grad Norm: 0.00906940\n",
      "Epoch 2 | Step 39900 | Avg Loss: 0.0176 | Grad Norm: 0.00968395\n",
      "Epoch 2 | Step 40000 | Avg Loss: 0.0176 | Grad Norm: 0.00936391\n",
      "Epoch 2 | Step 40100 | Avg Loss: 0.0170 | Grad Norm: 0.00864092\n",
      "Epoch 2 | Step 40200 | Avg Loss: 0.0165 | Grad Norm: 0.00905504\n",
      "Epoch 2 | Step 40300 | Avg Loss: 0.0163 | Grad Norm: 0.00867757\n",
      "Epoch 2 | Step 40400 | Avg Loss: 0.0167 | Grad Norm: 0.00799041\n",
      "Epoch 2 | Step 40500 | Avg Loss: 0.0167 | Grad Norm: 0.00784192\n",
      "Epoch 2 | Step 40600 | Avg Loss: 0.0168 | Grad Norm: 0.00858307\n",
      "Epoch 2 | Step 40700 | Avg Loss: 0.0164 | Grad Norm: 0.00919393\n",
      "Epoch 2 | Step 40800 | Avg Loss: 0.0166 | Grad Norm: 0.00835951\n",
      "Epoch 2 | Step 40900 | Avg Loss: 0.0167 | Grad Norm: 0.00804734\n",
      "Epoch 2 | Step 41000 | Avg Loss: 0.0172 | Grad Norm: 0.00829833\n",
      "Epoch 2 | Step 41100 | Avg Loss: 0.0166 | Grad Norm: 0.00793948\n",
      "Epoch 2 | Step 41200 | Avg Loss: 0.0162 | Grad Norm: 0.00981439\n",
      "Epoch 2 | Step 41300 | Avg Loss: 0.0167 | Grad Norm: 0.00837188\n",
      "Epoch 2 | Step 41400 | Avg Loss: 0.0166 | Grad Norm: 0.00802123\n",
      "Epoch 2 | Step 41500 | Avg Loss: 0.0167 | Grad Norm: 0.01022678\n",
      "Epoch 2 | Step 41600 | Avg Loss: 0.0167 | Grad Norm: 0.00811248\n",
      "Epoch 2 | Step 41700 | Avg Loss: 0.0172 | Grad Norm: 0.00840953\n",
      "Epoch 2 | Step 41800 | Avg Loss: 0.0169 | Grad Norm: 0.00929368\n",
      "Epoch 2 | Step 41900 | Avg Loss: 0.0167 | Grad Norm: 0.00867429\n",
      "Epoch 2 | Step 42000 | Avg Loss: 0.0168 | Grad Norm: 0.00945513\n",
      "Epoch 2 | Step 42100 | Avg Loss: 0.0168 | Grad Norm: 0.00824161\n",
      "Epoch 2 | Step 42200 | Avg Loss: 0.0161 | Grad Norm: 0.00784469\n",
      "Epoch 2 | Step 42300 | Avg Loss: 0.0163 | Grad Norm: 0.00794914\n",
      "Epoch 2 | Step 42400 | Avg Loss: 0.0162 | Grad Norm: 0.00845853\n",
      "Epoch 2 | Step 42500 | Avg Loss: 0.0159 | Grad Norm: 0.00735218\n",
      "Epoch 2 | Step 42600 | Avg Loss: 0.0157 | Grad Norm: 0.00853196\n",
      "Epoch 2 | Step 42700 | Avg Loss: 0.0160 | Grad Norm: 0.00838328\n",
      "Epoch 2 | Step 42800 | Avg Loss: 0.0162 | Grad Norm: 0.00811063\n",
      "Epoch 2 | Step 42900 | Avg Loss: 0.0162 | Grad Norm: 0.00810543\n",
      "Epoch 2 | Step 43000 | Avg Loss: 0.0160 | Grad Norm: 0.00924593\n",
      "Epoch 2 | Step 43100 | Avg Loss: 0.0158 | Grad Norm: 0.00865315\n",
      "Epoch 2 | Step 43200 | Avg Loss: 0.0157 | Grad Norm: 0.00753944\n",
      "Epoch 2 | Step 43300 | Avg Loss: 0.0157 | Grad Norm: 0.00804987\n",
      "Epoch 2 | Step 43400 | Avg Loss: 0.0159 | Grad Norm: 0.00830406\n",
      "Epoch 2 | Step 43500 | Avg Loss: 0.0161 | Grad Norm: 0.00883557\n",
      "Epoch 2 | Step 43600 | Avg Loss: 0.0160 | Grad Norm: 0.00887362\n",
      "Epoch 2 | Step 43700 | Avg Loss: 0.0159 | Grad Norm: 0.00880438\n",
      "Epoch 2 | Step 43800 | Avg Loss: 0.0158 | Grad Norm: 0.00740714\n",
      "Epoch 2 | Step 43900 | Avg Loss: 0.0159 | Grad Norm: 0.01041084\n",
      "Epoch 2 | Step 44000 | Avg Loss: 0.0158 | Grad Norm: 0.00824027\n",
      "Epoch 2 | Step 44100 | Avg Loss: 0.0157 | Grad Norm: 0.00773796\n",
      "Epoch 2 | Step 44200 | Avg Loss: 0.0156 | Grad Norm: 0.00736937\n",
      "Epoch 2 | Step 44300 | Avg Loss: 0.0155 | Grad Norm: 0.00824999\n",
      "Epoch 2 | Step 44400 | Avg Loss: 0.0162 | Grad Norm: 0.00843282\n",
      "Epoch 2 | Step 44500 | Avg Loss: 0.0169 | Grad Norm: 0.00876713\n",
      "Epoch 2 | Step 44600 | Avg Loss: 0.0165 | Grad Norm: 0.00875414\n",
      "Epoch 2 | Step 44700 | Avg Loss: 0.0162 | Grad Norm: 0.00845184\n",
      "Epoch 2 | Step 44800 | Avg Loss: 0.0163 | Grad Norm: 0.00925784\n",
      "Epoch 2 | Step 44900 | Avg Loss: 0.0162 | Grad Norm: 0.00825337\n",
      "Epoch 2 | Step 45000 | Avg Loss: 0.0160 | Grad Norm: 0.00907293\n",
      "Epoch 2 | Step 45100 | Avg Loss: 0.0158 | Grad Norm: 0.00997514\n",
      "Epoch 2 | Step 45200 | Avg Loss: 0.0152 | Grad Norm: 0.00810650\n",
      "Epoch 2 | Step 45300 | Avg Loss: 0.0158 | Grad Norm: 0.00939524\n",
      "Epoch 2 | Step 45400 | Avg Loss: 0.0162 | Grad Norm: 0.00951391\n",
      "Epoch 2 | Step 45500 | Avg Loss: 0.0161 | Grad Norm: 0.00861375\n",
      "Epoch 2 | Step 45600 | Avg Loss: 0.0161 | Grad Norm: 0.00844076\n",
      "Epoch 2 | Step 45700 | Avg Loss: 0.0161 | Grad Norm: 0.00922637\n",
      "Epoch 2 | Step 45800 | Avg Loss: 0.0162 | Grad Norm: 0.00909942\n",
      "Epoch 2 | Step 45900 | Avg Loss: 0.0160 | Grad Norm: 0.00850258\n",
      "Epoch 2 | Step 46000 | Avg Loss: 0.0161 | Grad Norm: 0.00856848\n",
      "Epoch 2 | Step 46100 | Avg Loss: 0.0160 | Grad Norm: 0.00890950\n",
      "Epoch 2 | Step 46200 | Avg Loss: 0.0155 | Grad Norm: 0.00784454\n",
      "Epoch 2 | Step 46300 | Avg Loss: 0.0152 | Grad Norm: 0.00887032\n",
      "Epoch 2 | Step 46400 | Avg Loss: 0.0156 | Grad Norm: 0.00897894\n",
      "Epoch 2 | Step 46500 | Avg Loss: 0.0154 | Grad Norm: 0.00858562\n",
      "Epoch 2 | Step 46600 | Avg Loss: 0.0159 | Grad Norm: 0.00808343\n",
      "Epoch 2 | Step 46700 | Avg Loss: 0.0158 | Grad Norm: 0.00775156\n",
      "Epoch 2 | Step 46800 | Avg Loss: 0.0154 | Grad Norm: 0.00849283\n",
      "Epoch 2 | Step 46900 | Avg Loss: 0.0156 | Grad Norm: 0.00802521\n",
      "Epoch 2 | Step 47000 | Avg Loss: 0.0159 | Grad Norm: 0.00922918\n",
      "Epoch 2 | Step 47100 | Avg Loss: 0.0161 | Grad Norm: 0.00841204\n",
      "Epoch 2 | Step 47200 | Avg Loss: 0.0157 | Grad Norm: 0.00823850\n",
      "Epoch 2 | Step 47300 | Avg Loss: 0.0155 | Grad Norm: 0.00951169\n",
      "Epoch 2 | Step 47400 | Avg Loss: 0.0157 | Grad Norm: 0.00924793\n",
      "Epoch 2 | Step 47500 | Avg Loss: 0.0156 | Grad Norm: 0.00858999\n",
      "Epoch 2 | Step 47600 | Avg Loss: 0.0156 | Grad Norm: 0.00878876\n",
      "Epoch 2 | Step 47700 | Avg Loss: 0.0159 | Grad Norm: 0.00810107\n",
      "Epoch 2 | Step 47800 | Avg Loss: 0.0158 | Grad Norm: 0.00860973\n",
      "Epoch 2 | Step 47900 | Avg Loss: 0.0154 | Grad Norm: 0.00862431\n",
      "Epoch 2 | Step 48000 | Avg Loss: 0.0158 | Grad Norm: 0.00793221\n",
      "Epoch 2 | Step 48100 | Avg Loss: 0.0157 | Grad Norm: 0.00864432\n",
      "Epoch 2 | Step 48200 | Avg Loss: 0.0158 | Grad Norm: 0.00833500\n",
      "Epoch 2 | Step 48300 | Avg Loss: 0.0156 | Grad Norm: 0.00912099\n",
      "Epoch 2 | Step 48400 | Avg Loss: 0.0154 | Grad Norm: 0.00767510\n",
      "Epoch 2 | Step 48500 | Avg Loss: 0.0155 | Grad Norm: 0.00961029\n",
      "Epoch 2 | Step 48600 | Avg Loss: 0.0157 | Grad Norm: 0.00861933\n",
      "Epoch 2 | Step 48700 | Avg Loss: 0.0159 | Grad Norm: 0.01005906\n",
      "Epoch 2 | Step 48800 | Avg Loss: 0.0158 | Grad Norm: 0.00916498\n",
      "Epoch 2 | Step 48900 | Avg Loss: 0.0157 | Grad Norm: 0.00831717\n",
      "Epoch 2 | Step 49000 | Avg Loss: 0.0158 | Grad Norm: 0.00962063\n",
      "Epoch 2 | Step 49100 | Avg Loss: 0.0159 | Grad Norm: 0.00879769\n",
      "Epoch 2 | Step 49200 | Avg Loss: 0.0164 | Grad Norm: 0.00920921\n",
      "Epoch 2 | Step 49300 | Avg Loss: 0.0162 | Grad Norm: 0.01229249\n",
      "Epoch 2 | Step 49400 | Avg Loss: 0.0160 | Grad Norm: 0.00922746\n",
      "Epoch 2 | Step 49500 | Avg Loss: 0.0162 | Grad Norm: 0.00951751\n",
      "Epoch 2 | Step 49600 | Avg Loss: 0.0163 | Grad Norm: 0.00913304\n",
      "Epoch 2 | Step 49700 | Avg Loss: 0.0160 | Grad Norm: 0.01059112\n",
      "Epoch 2 | Step 49800 | Avg Loss: 0.0161 | Grad Norm: 0.00896031\n",
      "Epoch 2 | Step 49900 | Avg Loss: 0.0160 | Grad Norm: 0.00896237\n",
      "Epoch 2 | Step 50000 | Avg Loss: 0.0154 | Grad Norm: 0.00863676\n",
      "Epoch 2 | Step 50100 | Avg Loss: 0.0159 | Grad Norm: 0.00908818\n",
      "Epoch 2 | Step 50200 | Avg Loss: 0.0161 | Grad Norm: 0.01097407\n",
      "Epoch 2 | Step 50300 | Avg Loss: 0.0160 | Grad Norm: 0.00927240\n",
      "Epoch 2 | Step 50400 | Avg Loss: 0.0156 | Grad Norm: 0.00891108\n",
      "Epoch 2 | Step 50500 | Avg Loss: 0.0155 | Grad Norm: 0.00996495\n",
      "Epoch 2 | Step 50600 | Avg Loss: 0.0159 | Grad Norm: 0.00893173\n",
      "Epoch 2 | Step 50700 | Avg Loss: 0.0155 | Grad Norm: 0.01049051\n",
      "Epoch 2 | Step 50800 | Avg Loss: 0.0155 | Grad Norm: 0.00849859\n",
      "Epoch 2 | Step 50900 | Avg Loss: 0.0153 | Grad Norm: 0.00810107\n",
      "Epoch 2 | Step 51000 | Avg Loss: 0.0152 | Grad Norm: 0.01001524\n",
      "Epoch 2 | Step 51100 | Avg Loss: 0.0152 | Grad Norm: 0.00805629\n",
      "Epoch 2 | Step 51200 | Avg Loss: 0.0152 | Grad Norm: 0.00795672\n",
      "Epoch 2 | Step 51300 | Avg Loss: 0.0152 | Grad Norm: 0.00920720\n",
      "Epoch 2 | Step 51400 | Avg Loss: 0.0156 | Grad Norm: 0.01061265\n",
      "Epoch 2 | Step 51500 | Avg Loss: 0.0157 | Grad Norm: 0.00910562\n",
      "Epoch 2 | Step 51600 | Avg Loss: 0.0158 | Grad Norm: 0.00834370\n",
      "Epoch 2 | Step 51700 | Avg Loss: 0.0154 | Grad Norm: 0.00851687\n",
      "Epoch 2 | Step 51800 | Avg Loss: 0.0154 | Grad Norm: 0.01117054\n",
      "Epoch 2 | Step 51900 | Avg Loss: 0.0153 | Grad Norm: 0.00933833\n",
      "Epoch 2 | Step 52000 | Avg Loss: 0.0157 | Grad Norm: 0.00815199\n",
      "Epoch 2 | Step 52100 | Avg Loss: 0.0156 | Grad Norm: 0.00959346\n",
      "Epoch 2 | Step 52200 | Avg Loss: 0.0152 | Grad Norm: 0.00997797\n",
      "Epoch 2 | Step 52300 | Avg Loss: 0.0150 | Grad Norm: 0.00881092\n",
      "Epoch 2 | Step 52400 | Avg Loss: 0.0151 | Grad Norm: 0.00805313\n",
      "Epoch 2 | Step 52500 | Avg Loss: 0.0156 | Grad Norm: 0.00830101\n",
      "Epoch 2 | Step 52600 | Avg Loss: 0.0154 | Grad Norm: 0.00790958\n",
      "Epoch 2 | Step 52700 | Avg Loss: 0.0151 | Grad Norm: 0.01036041\n",
      "Epoch 2 | Step 52800 | Avg Loss: 0.0149 | Grad Norm: 0.00877202\n",
      "Epoch 2 | Step 52900 | Avg Loss: 0.0155 | Grad Norm: 0.00788151\n",
      "Epoch 2 | Step 53000 | Avg Loss: 0.0161 | Grad Norm: 0.00974433\n",
      "Epoch 2 | Step 53100 | Avg Loss: 0.0154 | Grad Norm: 0.00945349\n",
      "Epoch 2 | Step 53200 | Avg Loss: 0.0153 | Grad Norm: 0.00958572\n",
      "Epoch 2 | Step 53300 | Avg Loss: 0.0153 | Grad Norm: 0.00886980\n",
      "Epoch 2 | Step 53400 | Avg Loss: 0.0150 | Grad Norm: 0.00906975\n",
      "Epoch 2 | Step 53500 | Avg Loss: 0.0154 | Grad Norm: 0.00853928\n",
      "Epoch 2 | Step 53600 | Avg Loss: 0.0151 | Grad Norm: 0.00901153\n",
      "Epoch 2 | Step 53700 | Avg Loss: 0.0154 | Grad Norm: 0.00932848\n",
      "Epoch 2 | Step 53800 | Avg Loss: 0.0152 | Grad Norm: 0.00794627\n",
      "Epoch 2 | Step 53900 | Avg Loss: 0.0148 | Grad Norm: 0.01022639\n",
      "Epoch 2 | Step 54000 | Avg Loss: 0.0152 | Grad Norm: 0.01007066\n",
      "Epoch 2 | Step 54100 | Avg Loss: 0.0151 | Grad Norm: 0.00962680\n",
      "Epoch 2 | Step 54200 | Avg Loss: 0.0150 | Grad Norm: 0.00928724\n",
      "Epoch 2 | Step 54300 | Avg Loss: 0.0149 | Grad Norm: 0.00960454\n",
      "Epoch 2 | Step 54400 | Avg Loss: 0.0149 | Grad Norm: 0.00922355\n",
      "Epoch 2 | Step 54500 | Avg Loss: 0.0148 | Grad Norm: 0.00915525\n",
      "Epoch 2 | Step 54600 | Avg Loss: 0.0154 | Grad Norm: 0.00881850\n",
      "Epoch 2 | Step 54700 | Avg Loss: 0.0154 | Grad Norm: 0.00992738\n",
      "Epoch 2 | Step 54800 | Avg Loss: 0.0159 | Grad Norm: 0.01034816\n",
      "Epoch 2 | Step 54900 | Avg Loss: 0.0160 | Grad Norm: 0.00963864\n",
      "Epoch 2 | Step 55000 | Avg Loss: 0.0162 | Grad Norm: 0.00916052\n",
      "Epoch 2 | Step 55100 | Avg Loss: 0.0156 | Grad Norm: 0.01150520\n",
      "Epoch 2 | Step 55200 | Avg Loss: 0.0158 | Grad Norm: 0.00881448\n",
      "Epoch 2 | Step 55300 | Avg Loss: 0.0160 | Grad Norm: 0.00920262\n",
      "Epoch 2 | Step 55400 | Avg Loss: 0.0155 | Grad Norm: 0.00931421\n",
      "Epoch 2 | Step 55500 | Avg Loss: 0.0154 | Grad Norm: 0.00930742\n",
      "Epoch 2 | Step 55600 | Avg Loss: 0.0156 | Grad Norm: 0.00829858\n",
      "Epoch 2 | Step 55700 | Avg Loss: 0.0148 | Grad Norm: 0.01080682\n",
      "Epoch 2 | Step 55800 | Avg Loss: 0.0150 | Grad Norm: 0.00870954\n",
      "Epoch 2 | Step 55900 | Avg Loss: 0.0154 | Grad Norm: 0.00953076\n",
      "Epoch 2 | Step 56000 | Avg Loss: 0.0156 | Grad Norm: 0.00835644\n",
      "Epoch 2 | Step 56100 | Avg Loss: 0.0152 | Grad Norm: 0.00947720\n",
      "Epoch 2 | Step 56200 | Avg Loss: 0.0151 | Grad Norm: 0.00841788\n",
      "Epoch 2 | Step 56300 | Avg Loss: 0.0148 | Grad Norm: 0.00896983\n",
      "Epoch 2 | Step 56400 | Avg Loss: 0.0148 | Grad Norm: 0.00800179\n",
      "Epoch 2 | Step 56500 | Avg Loss: 0.0148 | Grad Norm: 0.00882890\n",
      "Epoch 2 | Step 56600 | Avg Loss: 0.0146 | Grad Norm: 0.00915857\n",
      "Epoch 2 | Step 56700 | Avg Loss: 0.0143 | Grad Norm: 0.00744704\n",
      "Epoch 2 | Step 56800 | Avg Loss: 0.0146 | Grad Norm: 0.00860796\n",
      "Epoch 2 | Step 56900 | Avg Loss: 0.0147 | Grad Norm: 0.01076050\n",
      "Epoch 2 | Step 57000 | Avg Loss: 0.0150 | Grad Norm: 0.00914167\n",
      "Epoch 2 | Step 57100 | Avg Loss: 0.0149 | Grad Norm: 0.00891204\n",
      "Epoch 2 | Step 57200 | Avg Loss: 0.0147 | Grad Norm: 0.00855456\n",
      "Epoch 2 | Step 57300 | Avg Loss: 0.0145 | Grad Norm: 0.00986460\n",
      "Epoch 2 | Step 57400 | Avg Loss: 0.0150 | Grad Norm: 0.00999718\n",
      "Epoch 2 | Step 57500 | Avg Loss: 0.0149 | Grad Norm: 0.01102806\n",
      "Epoch 2 | Step 57600 | Avg Loss: 0.0151 | Grad Norm: 0.00960389\n",
      "Epoch 2 | Step 57700 | Avg Loss: 0.0150 | Grad Norm: 0.00875447\n",
      "Epoch 2 | Step 57800 | Avg Loss: 0.0151 | Grad Norm: 0.00838740\n",
      "Epoch 2 | Step 57900 | Avg Loss: 0.0153 | Grad Norm: 0.01004792\n",
      "Epoch 2 | Step 58000 | Avg Loss: 0.0153 | Grad Norm: 0.00867505\n",
      "Epoch 2 | Step 58100 | Avg Loss: 0.0156 | Grad Norm: 0.00960213\n",
      "Epoch 2 | Step 58200 | Avg Loss: 0.0155 | Grad Norm: 0.00935195\n",
      "Epoch 2 | Step 58300 | Avg Loss: 0.0155 | Grad Norm: 0.00874483\n",
      "Epoch 2 | Step 58400 | Avg Loss: 0.0149 | Grad Norm: 0.00803307\n",
      "Epoch 2 | Step 58500 | Avg Loss: 0.0146 | Grad Norm: 0.00853944\n",
      "Epoch 2 | Step 58600 | Avg Loss: 0.0143 | Grad Norm: 0.00791359\n",
      "Epoch 2 | Step 58700 | Avg Loss: 0.0143 | Grad Norm: 0.00927831\n",
      "Epoch 2 | Step 58800 | Avg Loss: 0.0144 | Grad Norm: 0.00823290\n",
      "Epoch 2 | Step 58900 | Avg Loss: 0.0142 | Grad Norm: 0.00918695\n",
      "Epoch 2 | Step 59000 | Avg Loss: 0.0145 | Grad Norm: 0.00916349\n",
      "Epoch 2 | Step 59100 | Avg Loss: 0.0144 | Grad Norm: 0.00903801\n",
      "Epoch 2 | Step 59200 | Avg Loss: 0.0141 | Grad Norm: 0.00746693\n",
      "Epoch 2 | Step 59300 | Avg Loss: 0.0142 | Grad Norm: 0.00875163\n",
      "Epoch 2 | Step 59400 | Avg Loss: 0.0146 | Grad Norm: 0.00903853\n",
      "Epoch 2 | Step 59500 | Avg Loss: 0.0148 | Grad Norm: 0.01080049\n",
      "Epoch 2 | Step 59600 | Avg Loss: 0.0151 | Grad Norm: 0.00995759\n",
      "Epoch 2 | Step 59700 | Avg Loss: 0.0150 | Grad Norm: 0.00928519\n",
      "Epoch 2 | Step 59800 | Avg Loss: 0.0149 | Grad Norm: 0.00870809\n",
      "Epoch 2 | Step 59900 | Avg Loss: 0.0145 | Grad Norm: 0.01005230\n",
      "Epoch 2 | Step 60000 | Avg Loss: 0.0146 | Grad Norm: 0.01016548\n",
      "Epoch 2 | Step 60100 | Avg Loss: 0.0147 | Grad Norm: 0.00906456\n",
      "Epoch 2 | Step 60200 | Avg Loss: 0.0148 | Grad Norm: 0.00886543\n",
      "Epoch 2 | Step 60300 | Avg Loss: 0.0146 | Grad Norm: 0.00816867\n",
      "Epoch 2 | Step 60400 | Avg Loss: 0.0146 | Grad Norm: 0.00861780\n",
      "Epoch 2 | Step 60500 | Avg Loss: 0.0151 | Grad Norm: 0.01049699\n",
      "Epoch 2 | Step 60600 | Avg Loss: 0.0150 | Grad Norm: 0.00974476\n",
      "Epoch 2 | Step 60700 | Avg Loss: 0.0151 | Grad Norm: 0.00903964\n",
      "Epoch 2 | Step 60800 | Avg Loss: 0.0150 | Grad Norm: 0.01032355\n",
      "Epoch 2 | Step 60900 | Avg Loss: 0.0154 | Grad Norm: 0.00907038\n",
      "Epoch 2 | Step 61000 | Avg Loss: 0.0150 | Grad Norm: 0.00913198\n",
      "Epoch 2 | Step 61100 | Avg Loss: 0.0145 | Grad Norm: 0.00976813\n",
      "Epoch 2 | Step 61200 | Avg Loss: 0.0145 | Grad Norm: 0.00927403\n",
      "Epoch 2 | Step 61300 | Avg Loss: 0.0144 | Grad Norm: 0.00837195\n",
      "Epoch 2 | Step 61400 | Avg Loss: 0.0148 | Grad Norm: 0.00929507\n",
      "Epoch 2 | Step 61500 | Avg Loss: 0.0148 | Grad Norm: 0.00933946\n",
      "Epoch 2 | Step 61600 | Avg Loss: 0.0147 | Grad Norm: 0.00916590\n",
      "Epoch 2 | Step 61700 | Avg Loss: 0.0145 | Grad Norm: 0.00903520\n",
      "Epoch 2 | Step 61800 | Avg Loss: 0.0147 | Grad Norm: 0.00972510\n",
      "Epoch 2 | Step 61900 | Avg Loss: 0.0151 | Grad Norm: 0.01007890\n",
      "Epoch 2 | Step 62000 | Avg Loss: 0.0147 | Grad Norm: 0.00960158\n",
      "Epoch 2 | Step 62100 | Avg Loss: 0.0145 | Grad Norm: 0.00845748\n",
      "Epoch 2 | Step 62200 | Avg Loss: 0.0144 | Grad Norm: 0.00882767\n",
      "Epoch 2 | Step 62300 | Avg Loss: 0.0141 | Grad Norm: 0.00892978\n",
      "Epoch 2 | Step 62400 | Avg Loss: 0.0144 | Grad Norm: 0.00913791\n",
      "Epoch 2 | Step 62500 | Avg Loss: 0.0147 | Grad Norm: 0.00884172\n",
      "Epoch 2 | Step 62600 | Avg Loss: 0.0149 | Grad Norm: 0.01038415\n",
      "Epoch 2 | Step 62700 | Avg Loss: 0.0147 | Grad Norm: 0.00999346\n",
      "Epoch 2 | Step 62800 | Avg Loss: 0.0149 | Grad Norm: 0.00928502\n",
      "Epoch 2 | Step 62900 | Avg Loss: 0.0147 | Grad Norm: 0.01084725\n",
      "Epoch 2 | Step 63000 | Avg Loss: 0.0147 | Grad Norm: 0.00917123\n",
      "Epoch 2 | Step 63100 | Avg Loss: 0.0149 | Grad Norm: 0.00814241\n",
      "Epoch 2 | Step 63200 | Avg Loss: 0.0151 | Grad Norm: 0.01028933\n",
      "Epoch 2 | Step 63300 | Avg Loss: 0.0151 | Grad Norm: 0.00942284\n",
      "Epoch 2 | Step 63400 | Avg Loss: 0.0147 | Grad Norm: 0.00857330\n",
      "Epoch 2 | Step 63500 | Avg Loss: 0.0147 | Grad Norm: 0.00873007\n",
      "Epoch 2 | Step 63600 | Avg Loss: 0.0148 | Grad Norm: 0.00925026\n",
      "Epoch 2 | Step 63700 | Avg Loss: 0.0154 | Grad Norm: 0.01026334\n",
      "Epoch 2 | Step 63800 | Avg Loss: 0.0153 | Grad Norm: 0.00915114\n",
      "Epoch 2 | Step 63900 | Avg Loss: 0.0146 | Grad Norm: 0.00976250\n",
      "Epoch 2 | Step 64000 | Avg Loss: 0.0146 | Grad Norm: 0.00870124\n",
      "Epoch 2 | Step 64100 | Avg Loss: 0.0147 | Grad Norm: 0.00998995\n",
      "Epoch 2 | Step 64200 | Avg Loss: 0.0151 | Grad Norm: 0.01074249\n",
      "Epoch 2 | Step 64300 | Avg Loss: 0.0153 | Grad Norm: 0.00916690\n",
      "Epoch 2 | Step 64400 | Avg Loss: 0.0153 | Grad Norm: 0.00938828\n",
      "Epoch 2 | Step 64500 | Avg Loss: 0.0153 | Grad Norm: 0.00960274\n",
      "Epoch 2 | Step 64600 | Avg Loss: 0.0152 | Grad Norm: 0.00988725\n",
      "Epoch 2 | Step 64700 | Avg Loss: 0.0152 | Grad Norm: 0.01141843\n",
      "Epoch 2 | Step 64800 | Avg Loss: 0.0150 | Grad Norm: 0.00928414\n",
      "Epoch 2 | Step 64900 | Avg Loss: 0.0152 | Grad Norm: 0.00904346\n",
      "Epoch 2 | Step 65000 | Avg Loss: 0.0151 | Grad Norm: 0.00925118\n",
      "Epoch 2 | Step 65100 | Avg Loss: 0.0146 | Grad Norm: 0.00840594\n",
      "Epoch 2 | Step 65200 | Avg Loss: 0.0143 | Grad Norm: 0.00907053\n",
      "Epoch 2 | Step 65300 | Avg Loss: 0.0147 | Grad Norm: 0.00960464\n",
      "Epoch 2 | Step 65400 | Avg Loss: 0.0149 | Grad Norm: 0.00948644\n",
      "Epoch 2 | Step 65500 | Avg Loss: 0.0143 | Grad Norm: 0.00804067\n",
      "Epoch 2 | Step 65600 | Avg Loss: 0.0144 | Grad Norm: 0.00887444\n",
      "Epoch 2 | Step 65700 | Avg Loss: 0.0144 | Grad Norm: 0.01007712\n",
      "Epoch 2 | Step 65800 | Avg Loss: 0.0145 | Grad Norm: 0.00897880\n",
      "Epoch 2 | Step 65900 | Avg Loss: 0.0147 | Grad Norm: 0.00882626\n",
      "Epoch 2 | Step 66000 | Avg Loss: 0.0147 | Grad Norm: 0.00929657\n",
      "Epoch 2 | Step 66100 | Avg Loss: 0.0146 | Grad Norm: 0.00947511\n",
      "Epoch 2 | Step 66200 | Avg Loss: 0.0146 | Grad Norm: 0.01016891\n",
      "Epoch 2 | Step 66300 | Avg Loss: 0.0140 | Grad Norm: 0.00791018\n",
      "Epoch 2 | Step 66400 | Avg Loss: 0.0143 | Grad Norm: 0.00917291\n",
      "Epoch 2 | Step 66500 | Avg Loss: 0.0150 | Grad Norm: 0.00922893\n",
      "Epoch 2 | Step 66600 | Avg Loss: 0.0150 | Grad Norm: 0.00902707\n",
      "Epoch 2 | Step 66700 | Avg Loss: 0.0146 | Grad Norm: 0.00760374\n",
      "Epoch 2 | Step 66800 | Avg Loss: 0.0148 | Grad Norm: 0.00843742\n",
      "Epoch 2 | Step 66900 | Avg Loss: 0.0149 | Grad Norm: 0.01001387\n",
      "Epoch 2 | Step 67000 | Avg Loss: 0.0145 | Grad Norm: 0.00912525\n",
      "Epoch 2 | Step 67100 | Avg Loss: 0.0146 | Grad Norm: 0.00934094\n",
      "Epoch 2 | Step 67200 | Avg Loss: 0.0148 | Grad Norm: 0.01029414\n",
      "Epoch 2 | Step 67300 | Avg Loss: 0.0143 | Grad Norm: 0.00848165\n",
      "Epoch 2 | Step 67400 | Avg Loss: 0.0142 | Grad Norm: 0.00953913\n",
      "Epoch 2 | Step 67500 | Avg Loss: 0.0145 | Grad Norm: 0.00945665\n",
      "Epoch 2 | Step 67600 | Avg Loss: 0.0150 | Grad Norm: 0.01119063\n",
      "Epoch 2 | Step 67700 | Avg Loss: 0.0146 | Grad Norm: 0.00997680\n",
      "Epoch 2 | Step 67800 | Avg Loss: 0.0141 | Grad Norm: 0.00820040\n",
      "Epoch 2 | Step 67900 | Avg Loss: 0.0142 | Grad Norm: 0.00966040\n",
      "Epoch 2 | Step 68000 | Avg Loss: 0.0142 | Grad Norm: 0.00985758\n",
      "Epoch 2 | Step 68100 | Avg Loss: 0.0141 | Grad Norm: 0.00969121\n",
      "Epoch 2 | Step 68200 | Avg Loss: 0.0147 | Grad Norm: 0.00933575\n",
      "Epoch 2 | Step 68300 | Avg Loss: 0.0144 | Grad Norm: 0.00913075\n",
      "Epoch 2 | Step 68400 | Avg Loss: 0.0140 | Grad Norm: 0.00988868\n",
      "Epoch 2 | Step 68500 | Avg Loss: 0.0140 | Grad Norm: 0.00931604\n",
      "Epoch 2 | Step 68600 | Avg Loss: 0.0143 | Grad Norm: 0.00813827\n",
      "Epoch 2 | Step 68700 | Avg Loss: 0.0142 | Grad Norm: 0.01028443\n",
      "Epoch 2 | Step 68800 | Avg Loss: 0.0145 | Grad Norm: 0.01002515\n",
      "Epoch 2 | Step 68900 | Avg Loss: 0.0147 | Grad Norm: 0.00867677\n",
      "Epoch 2 | Step 69000 | Avg Loss: 0.0148 | Grad Norm: 0.00924308\n",
      "Epoch 2 | Step 69100 | Avg Loss: 0.0148 | Grad Norm: 0.00925136\n",
      "Epoch 2 | Step 69200 | Avg Loss: 0.0151 | Grad Norm: 0.00927083\n",
      "Epoch 2 | Step 69300 | Avg Loss: 0.0151 | Grad Norm: 0.00970104\n",
      "Epoch 2 | Step 69400 | Avg Loss: 0.0151 | Grad Norm: 0.00809696\n",
      "Epoch 2 | Step 69500 | Avg Loss: 0.0148 | Grad Norm: 0.01009707\n",
      "Epoch 2 | Step 69600 | Avg Loss: 0.0145 | Grad Norm: 0.00934544\n",
      "Epoch 2 | Step 69700 | Avg Loss: 0.0142 | Grad Norm: 0.00880773\n",
      "Epoch 2 | Step 69800 | Avg Loss: 0.0144 | Grad Norm: 0.01019751\n",
      "Epoch 2 | Step 69900 | Avg Loss: 0.0143 | Grad Norm: 0.00872804\n",
      "Epoch 2 | Step 70000 | Avg Loss: 0.0146 | Grad Norm: 0.01010630\n",
      "Epoch 2 | Step 70100 | Avg Loss: 0.0142 | Grad Norm: 0.00963443\n",
      "Epoch 2 | Step 70200 | Avg Loss: 0.0144 | Grad Norm: 0.00898240\n",
      "Epoch 2 | Step 70300 | Avg Loss: 0.0144 | Grad Norm: 0.00976801\n",
      "Epoch 2 | Step 70400 | Avg Loss: 0.0143 | Grad Norm: 0.00886410\n",
      "Epoch 2 | Step 70500 | Avg Loss: 0.0147 | Grad Norm: 0.00900874\n",
      "Epoch 2 | Step 70600 | Avg Loss: 0.0142 | Grad Norm: 0.00887422\n",
      "Epoch 2 | Step 70700 | Avg Loss: 0.0143 | Grad Norm: 0.00916456\n",
      "Epoch 2 | Step 70800 | Avg Loss: 0.0142 | Grad Norm: 0.00973046\n",
      "Epoch 2 | Step 70900 | Avg Loss: 0.0148 | Grad Norm: 0.00876384\n",
      "Epoch 2 | Step 71000 | Avg Loss: 0.0147 | Grad Norm: 0.01258718\n",
      "Epoch 2 | Step 71100 | Avg Loss: 0.0148 | Grad Norm: 0.01001469\n",
      "Epoch 2 | Step 71200 | Avg Loss: 0.0147 | Grad Norm: 0.01116932\n",
      "Epoch 2 | Step 71300 | Avg Loss: 0.0148 | Grad Norm: 0.01037316\n",
      "Epoch 2 | Step 71400 | Avg Loss: 0.0149 | Grad Norm: 0.00794616\n",
      "Epoch 2 | Step 71500 | Avg Loss: 0.0147 | Grad Norm: 0.00861818\n",
      "Epoch 2 | Step 71600 | Avg Loss: 0.0146 | Grad Norm: 0.00815339\n",
      "Epoch 2 | Step 71700 | Avg Loss: 0.0147 | Grad Norm: 0.01024185\n",
      "Epoch 2 | Step 71800 | Avg Loss: 0.0147 | Grad Norm: 0.01118541\n",
      "Epoch 2 | Step 71900 | Avg Loss: 0.0144 | Grad Norm: 0.00926245\n",
      "Epoch 2 | Step 72000 | Avg Loss: 0.0142 | Grad Norm: 0.01013860\n",
      "Epoch 2 | Step 72100 | Avg Loss: 0.0144 | Grad Norm: 0.00993842\n",
      "Epoch 2 | Step 72200 | Avg Loss: 0.0141 | Grad Norm: 0.01056268\n",
      "Epoch 2 | Step 72300 | Avg Loss: 0.0139 | Grad Norm: 0.00896924\n",
      "Epoch 2 | Step 72400 | Avg Loss: 0.0141 | Grad Norm: 0.00903092\n",
      "Epoch 2 | Step 72500 | Avg Loss: 0.0144 | Grad Norm: 0.00890521\n",
      "Epoch 2 | Step 72600 | Avg Loss: 0.0141 | Grad Norm: 0.00939192\n",
      "Epoch 2 | Step 72700 | Avg Loss: 0.0143 | Grad Norm: 0.00775672\n",
      "Epoch 2 | Step 72800 | Avg Loss: 0.0142 | Grad Norm: 0.00865155\n",
      "Epoch 2 | Step 72900 | Avg Loss: 0.0143 | Grad Norm: 0.00926819\n",
      "Epoch 2 | Step 73000 | Avg Loss: 0.0142 | Grad Norm: 0.00915499\n",
      "Epoch 2 | Step 73100 | Avg Loss: 0.0136 | Grad Norm: 0.00880045\n",
      "Epoch 2 | Step 73200 | Avg Loss: 0.0135 | Grad Norm: 0.00876108\n",
      "Epoch 2 | Step 73300 | Avg Loss: 0.0136 | Grad Norm: 0.00937354\n",
      "Epoch 2 | Step 73400 | Avg Loss: 0.0139 | Grad Norm: 0.00969497\n",
      "Epoch 2 | Step 73500 | Avg Loss: 0.0143 | Grad Norm: 0.00907883\n",
      "Epoch 2 | Step 73600 | Avg Loss: 0.0146 | Grad Norm: 0.00891724\n",
      "Epoch 2 | Step 73700 | Avg Loss: 0.0142 | Grad Norm: 0.00942955\n",
      "Epoch 2 | Step 73800 | Avg Loss: 0.0138 | Grad Norm: 0.00921983\n",
      "Epoch 2 | Step 73900 | Avg Loss: 0.0139 | Grad Norm: 0.00913059\n",
      "Epoch 2 | Step 74000 | Avg Loss: 0.0143 | Grad Norm: 0.00999807\n",
      "Epoch 2 | Step 74100 | Avg Loss: 0.0138 | Grad Norm: 0.00853499\n",
      "Epoch 2 | Step 74200 | Avg Loss: 0.0144 | Grad Norm: 0.01081333\n",
      "Epoch 2 | Step 74300 | Avg Loss: 0.0147 | Grad Norm: 0.00795658\n",
      "Epoch 2 | Step 74400 | Avg Loss: 0.0145 | Grad Norm: 0.01142960\n",
      "Epoch 2 | Step 74500 | Avg Loss: 0.0146 | Grad Norm: 0.00999739\n",
      "Epoch 2 | Step 74600 | Avg Loss: 0.0146 | Grad Norm: 0.01279510\n",
      "Epoch 2 | Step 74700 | Avg Loss: 0.0146 | Grad Norm: 0.00867641\n",
      "Epoch 2 | Step 74800 | Avg Loss: 0.0141 | Grad Norm: 0.00961871\n",
      "Epoch 2 | Step 74900 | Avg Loss: 0.0144 | Grad Norm: 0.00959007\n",
      "Epoch 2 | Step 75000 | Avg Loss: 0.0142 | Grad Norm: 0.00899058\n",
      "Epoch 2 | Step 75100 | Avg Loss: 0.0139 | Grad Norm: 0.00824791\n",
      "Epoch 2 | Step 75200 | Avg Loss: 0.0142 | Grad Norm: 0.00820714\n",
      "Epoch 2 | Step 75300 | Avg Loss: 0.0149 | Grad Norm: 0.00862528\n",
      "Epoch 2 | Step 75400 | Avg Loss: 0.0149 | Grad Norm: 0.01044792\n",
      "Epoch 2 | Step 75500 | Avg Loss: 0.0150 | Grad Norm: 0.01070081\n",
      "Epoch 2 | Step 75600 | Avg Loss: 0.0145 | Grad Norm: 0.00912601\n",
      "Epoch 2 | Step 75700 | Avg Loss: 0.0147 | Grad Norm: 0.01023482\n",
      "Epoch 2 | Step 75800 | Avg Loss: 0.0146 | Grad Norm: 0.00999180\n",
      "Epoch 2 | Step 75900 | Avg Loss: 0.0148 | Grad Norm: 0.00903579\n",
      "Epoch 2 | Step 76000 | Avg Loss: 0.0150 | Grad Norm: 0.00922408\n",
      "Epoch 2 | Step 76100 | Avg Loss: 0.0150 | Grad Norm: 0.01018467\n",
      "Epoch 2 | Step 76200 | Avg Loss: 0.0145 | Grad Norm: 0.00958055\n",
      "Epoch 2 | Step 76300 | Avg Loss: 0.0144 | Grad Norm: 0.01040935\n",
      "Epoch 2 | Step 76400 | Avg Loss: 0.0144 | Grad Norm: 0.00936337\n",
      "Epoch 2 | Step 76500 | Avg Loss: 0.0141 | Grad Norm: 0.00991758\n",
      "Epoch 2 | Step 76600 | Avg Loss: 0.0144 | Grad Norm: 0.01123125\n",
      "Epoch 2 | Step 76700 | Avg Loss: 0.0142 | Grad Norm: 0.01003806\n",
      "Epoch 2 | Step 76800 | Avg Loss: 0.0144 | Grad Norm: 0.00944370\n",
      "Epoch 2 | Step 76900 | Avg Loss: 0.0144 | Grad Norm: 0.01040748\n",
      "Epoch 2 | Step 77000 | Avg Loss: 0.0145 | Grad Norm: 0.01008748\n",
      "Epoch 2 | Step 77100 | Avg Loss: 0.0147 | Grad Norm: 0.00971000\n",
      "Epoch 2 | Step 77200 | Avg Loss: 0.0146 | Grad Norm: 0.01028076\n",
      "Epoch 2 | Step 77300 | Avg Loss: 0.0147 | Grad Norm: 0.01143087\n",
      "Epoch 2 | Step 77400 | Avg Loss: 0.0145 | Grad Norm: 0.01084831\n",
      "Epoch 2 | Step 77500 | Avg Loss: 0.0148 | Grad Norm: 0.00962335\n",
      "Epoch 2 | Step 77600 | Avg Loss: 0.0145 | Grad Norm: 0.00926942\n",
      "Epoch 2 | Step 77700 | Avg Loss: 0.0145 | Grad Norm: 0.01071084\n",
      "Epoch 2 | Step 77800 | Avg Loss: 0.0143 | Grad Norm: 0.00834500\n",
      "Epoch 2 | Step 77900 | Avg Loss: 0.0127 | Grad Norm: 0.00848686\n",
      "Epoch 2 | Step 78000 | Avg Loss: 0.0119 | Grad Norm: 0.00895383\n",
      "Epoch 2 | Step 78100 | Avg Loss: 0.0113 | Grad Norm: 0.00839104\n",
      "Epoch 2, Loss: 0.0100\n",
      "Epoch 3 | Step 78200 | Avg Loss: 0.0164 | Grad Norm: 0.00946792\n",
      "Epoch 3 | Step 78300 | Avg Loss: 0.0159 | Grad Norm: 0.00831724\n",
      "Epoch 3 | Step 78400 | Avg Loss: 0.0144 | Grad Norm: 0.00927046\n",
      "Epoch 3 | Step 78500 | Avg Loss: 0.0142 | Grad Norm: 0.00944932\n",
      "Epoch 3 | Step 78600 | Avg Loss: 0.0140 | Grad Norm: 0.00891470\n",
      "Epoch 3 | Step 78700 | Avg Loss: 0.0141 | Grad Norm: 0.00862747\n",
      "Epoch 3 | Step 78800 | Avg Loss: 0.0144 | Grad Norm: 0.00832094\n",
      "Epoch 3 | Step 78900 | Avg Loss: 0.0146 | Grad Norm: 0.01057451\n",
      "Epoch 3 | Step 79000 | Avg Loss: 0.0147 | Grad Norm: 0.00982217\n",
      "Epoch 3 | Step 79100 | Avg Loss: 0.0149 | Grad Norm: 0.00931110\n",
      "Epoch 3 | Step 79200 | Avg Loss: 0.0142 | Grad Norm: 0.01080910\n",
      "Epoch 3 | Step 79300 | Avg Loss: 0.0139 | Grad Norm: 0.00939526\n",
      "Epoch 3 | Step 79400 | Avg Loss: 0.0141 | Grad Norm: 0.00994770\n",
      "Epoch 3 | Step 79500 | Avg Loss: 0.0140 | Grad Norm: 0.00970073\n",
      "Epoch 3 | Step 79600 | Avg Loss: 0.0140 | Grad Norm: 0.00825708\n",
      "Epoch 3 | Step 79700 | Avg Loss: 0.0138 | Grad Norm: 0.01027235\n",
      "Epoch 3 | Step 79800 | Avg Loss: 0.0139 | Grad Norm: 0.00894384\n",
      "Epoch 3 | Step 79900 | Avg Loss: 0.0138 | Grad Norm: 0.01067968\n",
      "Epoch 3 | Step 80000 | Avg Loss: 0.0140 | Grad Norm: 0.00903940\n",
      "Epoch 3 | Step 80100 | Avg Loss: 0.0142 | Grad Norm: 0.01010219\n",
      "Epoch 3 | Step 80200 | Avg Loss: 0.0139 | Grad Norm: 0.00874737\n",
      "Epoch 3 | Step 80300 | Avg Loss: 0.0142 | Grad Norm: 0.00947601\n",
      "Epoch 3 | Step 80400 | Avg Loss: 0.0143 | Grad Norm: 0.00956433\n",
      "Epoch 3 | Step 80500 | Avg Loss: 0.0140 | Grad Norm: 0.00931007\n",
      "Epoch 3 | Step 80600 | Avg Loss: 0.0145 | Grad Norm: 0.00862720\n",
      "Epoch 3 | Step 80700 | Avg Loss: 0.0143 | Grad Norm: 0.00908424\n",
      "Epoch 3 | Step 80800 | Avg Loss: 0.0144 | Grad Norm: 0.01028000\n",
      "Epoch 3 | Step 80900 | Avg Loss: 0.0142 | Grad Norm: 0.01193416\n",
      "Epoch 3 | Step 81000 | Avg Loss: 0.0143 | Grad Norm: 0.01014863\n",
      "Epoch 3 | Step 81100 | Avg Loss: 0.0142 | Grad Norm: 0.00893745\n",
      "Epoch 3 | Step 81200 | Avg Loss: 0.0143 | Grad Norm: 0.01024163\n",
      "Epoch 3 | Step 81300 | Avg Loss: 0.0136 | Grad Norm: 0.00935451\n",
      "Epoch 3 | Step 81400 | Avg Loss: 0.0136 | Grad Norm: 0.00918516\n",
      "Epoch 3 | Step 81500 | Avg Loss: 0.0136 | Grad Norm: 0.00951384\n",
      "Epoch 3 | Step 81600 | Avg Loss: 0.0131 | Grad Norm: 0.00966147\n",
      "Epoch 3 | Step 81700 | Avg Loss: 0.0134 | Grad Norm: 0.01123663\n",
      "Epoch 3 | Step 81800 | Avg Loss: 0.0138 | Grad Norm: 0.01084448\n",
      "Epoch 3 | Step 81900 | Avg Loss: 0.0138 | Grad Norm: 0.00970297\n",
      "Epoch 3 | Step 82000 | Avg Loss: 0.0137 | Grad Norm: 0.01086320\n",
      "Epoch 3 | Step 82100 | Avg Loss: 0.0140 | Grad Norm: 0.00844250\n",
      "Epoch 3 | Step 82200 | Avg Loss: 0.0136 | Grad Norm: 0.00864713\n",
      "Epoch 3 | Step 82300 | Avg Loss: 0.0136 | Grad Norm: 0.00891892\n",
      "Epoch 3 | Step 82400 | Avg Loss: 0.0134 | Grad Norm: 0.00955787\n",
      "Epoch 3 | Step 82500 | Avg Loss: 0.0139 | Grad Norm: 0.00954726\n",
      "Epoch 3 | Step 82600 | Avg Loss: 0.0138 | Grad Norm: 0.00973615\n",
      "Epoch 3 | Step 82700 | Avg Loss: 0.0135 | Grad Norm: 0.00890728\n",
      "Epoch 3 | Step 82800 | Avg Loss: 0.0135 | Grad Norm: 0.00832376\n",
      "Epoch 3 | Step 82900 | Avg Loss: 0.0136 | Grad Norm: 0.01098883\n",
      "Epoch 3 | Step 83000 | Avg Loss: 0.0139 | Grad Norm: 0.01046470\n",
      "Epoch 3 | Step 83100 | Avg Loss: 0.0138 | Grad Norm: 0.00953372\n",
      "Epoch 3 | Step 83200 | Avg Loss: 0.0137 | Grad Norm: 0.00837760\n",
      "Epoch 3 | Step 83300 | Avg Loss: 0.0135 | Grad Norm: 0.00872795\n",
      "Epoch 3 | Step 83400 | Avg Loss: 0.0139 | Grad Norm: 0.00870878\n",
      "Epoch 3 | Step 83500 | Avg Loss: 0.0142 | Grad Norm: 0.00810510\n",
      "Epoch 3 | Step 83600 | Avg Loss: 0.0142 | Grad Norm: 0.00983745\n",
      "Epoch 3 | Step 83700 | Avg Loss: 0.0137 | Grad Norm: 0.00981364\n",
      "Epoch 3 | Step 83800 | Avg Loss: 0.0140 | Grad Norm: 0.00858961\n",
      "Epoch 3 | Step 83900 | Avg Loss: 0.0140 | Grad Norm: 0.00973495\n",
      "Epoch 3 | Step 84000 | Avg Loss: 0.0138 | Grad Norm: 0.00965404\n",
      "Epoch 3 | Step 84100 | Avg Loss: 0.0139 | Grad Norm: 0.01009930\n",
      "Epoch 3 | Step 84200 | Avg Loss: 0.0135 | Grad Norm: 0.00918564\n",
      "Epoch 3 | Step 84300 | Avg Loss: 0.0136 | Grad Norm: 0.00821822\n",
      "Epoch 3 | Step 84400 | Avg Loss: 0.0142 | Grad Norm: 0.01027722\n",
      "Epoch 3 | Step 84500 | Avg Loss: 0.0143 | Grad Norm: 0.00825741\n",
      "Epoch 3 | Step 84600 | Avg Loss: 0.0142 | Grad Norm: 0.00951413\n",
      "Epoch 3 | Step 84700 | Avg Loss: 0.0140 | Grad Norm: 0.00804576\n",
      "Epoch 3 | Step 84800 | Avg Loss: 0.0140 | Grad Norm: 0.00978235\n",
      "Epoch 3 | Step 84900 | Avg Loss: 0.0141 | Grad Norm: 0.00968312\n",
      "Epoch 3 | Step 85000 | Avg Loss: 0.0142 | Grad Norm: 0.00969800\n",
      "Epoch 3 | Step 85100 | Avg Loss: 0.0140 | Grad Norm: 0.00979210\n",
      "Epoch 3 | Step 85200 | Avg Loss: 0.0138 | Grad Norm: 0.00912742\n",
      "Epoch 3 | Step 85300 | Avg Loss: 0.0136 | Grad Norm: 0.00879972\n",
      "Epoch 3 | Step 85400 | Avg Loss: 0.0135 | Grad Norm: 0.00928947\n",
      "Epoch 3 | Step 85500 | Avg Loss: 0.0137 | Grad Norm: 0.00956018\n",
      "Epoch 3 | Step 85600 | Avg Loss: 0.0137 | Grad Norm: 0.00849969\n",
      "Epoch 3 | Step 85700 | Avg Loss: 0.0139 | Grad Norm: 0.01004262\n",
      "Epoch 3 | Step 85800 | Avg Loss: 0.0137 | Grad Norm: 0.00835415\n",
      "Epoch 3 | Step 85900 | Avg Loss: 0.0137 | Grad Norm: 0.01025005\n",
      "Epoch 3 | Step 86000 | Avg Loss: 0.0137 | Grad Norm: 0.00999962\n",
      "Epoch 3 | Step 86100 | Avg Loss: 0.0141 | Grad Norm: 0.00929159\n",
      "Epoch 3 | Step 86200 | Avg Loss: 0.0141 | Grad Norm: 0.01011810\n",
      "Epoch 3 | Step 86300 | Avg Loss: 0.0141 | Grad Norm: 0.00982687\n",
      "Epoch 3 | Step 86400 | Avg Loss: 0.0138 | Grad Norm: 0.00899152\n",
      "Epoch 3 | Step 86500 | Avg Loss: 0.0139 | Grad Norm: 0.00941978\n",
      "Epoch 3 | Step 86600 | Avg Loss: 0.0138 | Grad Norm: 0.00868887\n",
      "Epoch 3 | Step 86700 | Avg Loss: 0.0141 | Grad Norm: 0.01017762\n",
      "Epoch 3 | Step 86800 | Avg Loss: 0.0143 | Grad Norm: 0.00958598\n",
      "Epoch 3 | Step 86900 | Avg Loss: 0.0141 | Grad Norm: 0.01018555\n",
      "Epoch 3 | Step 87000 | Avg Loss: 0.0140 | Grad Norm: 0.00955362\n",
      "Epoch 3 | Step 87100 | Avg Loss: 0.0141 | Grad Norm: 0.00925467\n",
      "Epoch 3 | Step 87200 | Avg Loss: 0.0142 | Grad Norm: 0.00864502\n",
      "Epoch 3 | Step 87300 | Avg Loss: 0.0142 | Grad Norm: 0.00949694\n",
      "Epoch 3 | Step 87400 | Avg Loss: 0.0138 | Grad Norm: 0.00871845\n",
      "Epoch 3 | Step 87500 | Avg Loss: 0.0138 | Grad Norm: 0.00896767\n",
      "Epoch 3 | Step 87600 | Avg Loss: 0.0140 | Grad Norm: 0.01068483\n",
      "Epoch 3 | Step 87700 | Avg Loss: 0.0140 | Grad Norm: 0.01101763\n",
      "Epoch 3 | Step 87800 | Avg Loss: 0.0142 | Grad Norm: 0.01063846\n",
      "Epoch 3 | Step 87900 | Avg Loss: 0.0140 | Grad Norm: 0.00949279\n",
      "Epoch 3 | Step 88000 | Avg Loss: 0.0140 | Grad Norm: 0.00963285\n",
      "Epoch 3 | Step 88100 | Avg Loss: 0.0142 | Grad Norm: 0.01033288\n",
      "Epoch 3 | Step 88200 | Avg Loss: 0.0143 | Grad Norm: 0.00976432\n",
      "Epoch 3 | Step 88300 | Avg Loss: 0.0144 | Grad Norm: 0.00928760\n",
      "Epoch 3 | Step 88400 | Avg Loss: 0.0141 | Grad Norm: 0.01031078\n",
      "Epoch 3 | Step 88500 | Avg Loss: 0.0143 | Grad Norm: 0.01106181\n",
      "Epoch 3 | Step 88600 | Avg Loss: 0.0146 | Grad Norm: 0.00964924\n",
      "Epoch 3 | Step 88700 | Avg Loss: 0.0145 | Grad Norm: 0.01070026\n",
      "Epoch 3 | Step 88800 | Avg Loss: 0.0144 | Grad Norm: 0.01012482\n",
      "Epoch 3 | Step 88900 | Avg Loss: 0.0144 | Grad Norm: 0.01010359\n",
      "Epoch 3 | Step 89000 | Avg Loss: 0.0141 | Grad Norm: 0.00992970\n",
      "Epoch 3 | Step 89100 | Avg Loss: 0.0140 | Grad Norm: 0.01157432\n",
      "Epoch 3 | Step 89200 | Avg Loss: 0.0145 | Grad Norm: 0.01022810\n",
      "Epoch 3 | Step 89300 | Avg Loss: 0.0143 | Grad Norm: 0.01084344\n",
      "Epoch 3 | Step 89400 | Avg Loss: 0.0145 | Grad Norm: 0.00979579\n",
      "Epoch 3 | Step 89500 | Avg Loss: 0.0139 | Grad Norm: 0.00986323\n",
      "Epoch 3 | Step 89600 | Avg Loss: 0.0139 | Grad Norm: 0.01087966\n",
      "Epoch 3 | Step 89700 | Avg Loss: 0.0141 | Grad Norm: 0.01080900\n",
      "Epoch 3 | Step 89800 | Avg Loss: 0.0139 | Grad Norm: 0.01020579\n",
      "Epoch 3 | Step 89900 | Avg Loss: 0.0142 | Grad Norm: 0.00986652\n",
      "Epoch 3 | Step 90000 | Avg Loss: 0.0138 | Grad Norm: 0.00969349\n",
      "Epoch 3 | Step 90100 | Avg Loss: 0.0135 | Grad Norm: 0.00935964\n",
      "Epoch 3 | Step 90200 | Avg Loss: 0.0136 | Grad Norm: 0.00966905\n",
      "Epoch 3 | Step 90300 | Avg Loss: 0.0137 | Grad Norm: 0.00894052\n",
      "Epoch 3 | Step 90400 | Avg Loss: 0.0135 | Grad Norm: 0.00998274\n",
      "Epoch 3 | Step 90500 | Avg Loss: 0.0141 | Grad Norm: 0.00861217\n",
      "Epoch 3 | Step 90600 | Avg Loss: 0.0142 | Grad Norm: 0.00973581\n",
      "Epoch 3 | Step 90700 | Avg Loss: 0.0140 | Grad Norm: 0.00828234\n",
      "Epoch 3 | Step 90800 | Avg Loss: 0.0138 | Grad Norm: 0.01017885\n",
      "Epoch 3 | Step 90900 | Avg Loss: 0.0137 | Grad Norm: 0.01138378\n",
      "Epoch 3 | Step 91000 | Avg Loss: 0.0140 | Grad Norm: 0.00915765\n",
      "Epoch 3 | Step 91100 | Avg Loss: 0.0142 | Grad Norm: 0.00908407\n",
      "Epoch 3 | Step 91200 | Avg Loss: 0.0138 | Grad Norm: 0.00913975\n",
      "Epoch 3 | Step 91300 | Avg Loss: 0.0138 | Grad Norm: 0.01018280\n",
      "Epoch 3 | Step 91400 | Avg Loss: 0.0138 | Grad Norm: 0.00824715\n",
      "Epoch 3 | Step 91500 | Avg Loss: 0.0138 | Grad Norm: 0.00912685\n",
      "Epoch 3 | Step 91600 | Avg Loss: 0.0141 | Grad Norm: 0.01003221\n",
      "Epoch 3 | Step 91700 | Avg Loss: 0.0138 | Grad Norm: 0.01014805\n",
      "Epoch 3 | Step 91800 | Avg Loss: 0.0135 | Grad Norm: 0.00978177\n",
      "Epoch 3 | Step 91900 | Avg Loss: 0.0136 | Grad Norm: 0.00993662\n",
      "Epoch 3 | Step 92000 | Avg Loss: 0.0142 | Grad Norm: 0.00980176\n",
      "Epoch 3 | Step 92100 | Avg Loss: 0.0142 | Grad Norm: 0.01021548\n",
      "Epoch 3 | Step 92200 | Avg Loss: 0.0139 | Grad Norm: 0.01045713\n",
      "Epoch 3 | Step 92300 | Avg Loss: 0.0140 | Grad Norm: 0.01009371\n",
      "Epoch 3 | Step 92400 | Avg Loss: 0.0138 | Grad Norm: 0.00942992\n",
      "Epoch 3 | Step 92500 | Avg Loss: 0.0137 | Grad Norm: 0.00873045\n",
      "Epoch 3 | Step 92600 | Avg Loss: 0.0138 | Grad Norm: 0.00966752\n",
      "Epoch 3 | Step 92700 | Avg Loss: 0.0136 | Grad Norm: 0.01062871\n",
      "Epoch 3 | Step 92800 | Avg Loss: 0.0137 | Grad Norm: 0.01098806\n",
      "Epoch 3 | Step 92900 | Avg Loss: 0.0136 | Grad Norm: 0.01031788\n",
      "Epoch 3 | Step 93000 | Avg Loss: 0.0137 | Grad Norm: 0.01060556\n",
      "Epoch 3 | Step 93100 | Avg Loss: 0.0138 | Grad Norm: 0.00924621\n",
      "Epoch 3 | Step 93200 | Avg Loss: 0.0137 | Grad Norm: 0.00920014\n",
      "Epoch 3 | Step 93300 | Avg Loss: 0.0135 | Grad Norm: 0.01020127\n",
      "Epoch 3 | Step 93400 | Avg Loss: 0.0137 | Grad Norm: 0.01054358\n",
      "Epoch 3 | Step 93500 | Avg Loss: 0.0136 | Grad Norm: 0.00913877\n",
      "Epoch 3 | Step 93600 | Avg Loss: 0.0138 | Grad Norm: 0.01023873\n",
      "Epoch 3 | Step 93700 | Avg Loss: 0.0142 | Grad Norm: 0.00935124\n",
      "Epoch 3 | Step 93800 | Avg Loss: 0.0144 | Grad Norm: 0.00996179\n",
      "Epoch 3 | Step 93900 | Avg Loss: 0.0145 | Grad Norm: 0.00936433\n",
      "Epoch 3 | Step 94000 | Avg Loss: 0.0147 | Grad Norm: 0.01002874\n",
      "Epoch 3 | Step 94100 | Avg Loss: 0.0144 | Grad Norm: 0.01013368\n",
      "Epoch 3 | Step 94200 | Avg Loss: 0.0145 | Grad Norm: 0.00950421\n",
      "Epoch 3 | Step 94300 | Avg Loss: 0.0148 | Grad Norm: 0.00987211\n",
      "Epoch 3 | Step 94400 | Avg Loss: 0.0148 | Grad Norm: 0.01011066\n",
      "Epoch 3 | Step 94500 | Avg Loss: 0.0143 | Grad Norm: 0.00958910\n",
      "Epoch 3 | Step 94600 | Avg Loss: 0.0144 | Grad Norm: 0.00937298\n",
      "Epoch 3 | Step 94700 | Avg Loss: 0.0139 | Grad Norm: 0.00866323\n",
      "Epoch 3 | Step 94800 | Avg Loss: 0.0137 | Grad Norm: 0.00958630\n",
      "Epoch 3 | Step 94900 | Avg Loss: 0.0138 | Grad Norm: 0.00977335\n",
      "Epoch 3 | Step 95000 | Avg Loss: 0.0142 | Grad Norm: 0.01117689\n",
      "Epoch 3 | Step 95100 | Avg Loss: 0.0142 | Grad Norm: 0.00924572\n",
      "Epoch 3 | Step 95200 | Avg Loss: 0.0138 | Grad Norm: 0.01061941\n",
      "Epoch 3 | Step 95300 | Avg Loss: 0.0139 | Grad Norm: 0.00892952\n",
      "Epoch 3 | Step 95400 | Avg Loss: 0.0140 | Grad Norm: 0.00998919\n",
      "Epoch 3 | Step 95500 | Avg Loss: 0.0138 | Grad Norm: 0.01047764\n",
      "Epoch 3 | Step 95600 | Avg Loss: 0.0139 | Grad Norm: 0.00970323\n",
      "Epoch 3 | Step 95700 | Avg Loss: 0.0137 | Grad Norm: 0.01074221\n",
      "Epoch 3 | Step 95800 | Avg Loss: 0.0133 | Grad Norm: 0.00984484\n",
      "Epoch 3 | Step 95900 | Avg Loss: 0.0136 | Grad Norm: 0.00961614\n",
      "Epoch 3 | Step 96000 | Avg Loss: 0.0140 | Grad Norm: 0.00928621\n",
      "Epoch 3 | Step 96100 | Avg Loss: 0.0138 | Grad Norm: 0.00966716\n",
      "Epoch 3 | Step 96200 | Avg Loss: 0.0136 | Grad Norm: 0.00918449\n",
      "Epoch 3 | Step 96300 | Avg Loss: 0.0137 | Grad Norm: 0.00791291\n",
      "Epoch 3 | Step 96400 | Avg Loss: 0.0136 | Grad Norm: 0.00994561\n",
      "Epoch 3 | Step 96500 | Avg Loss: 0.0137 | Grad Norm: 0.00914835\n",
      "Epoch 3 | Step 96600 | Avg Loss: 0.0139 | Grad Norm: 0.00953696\n",
      "Epoch 3 | Step 96700 | Avg Loss: 0.0139 | Grad Norm: 0.00978589\n",
      "Epoch 3 | Step 96800 | Avg Loss: 0.0139 | Grad Norm: 0.00941402\n",
      "Epoch 3 | Step 96900 | Avg Loss: 0.0140 | Grad Norm: 0.01076782\n",
      "Epoch 3 | Step 97000 | Avg Loss: 0.0144 | Grad Norm: 0.01022654\n",
      "Epoch 3 | Step 97100 | Avg Loss: 0.0141 | Grad Norm: 0.01096141\n",
      "Epoch 3 | Step 97200 | Avg Loss: 0.0142 | Grad Norm: 0.00979216\n",
      "Epoch 3 | Step 97300 | Avg Loss: 0.0142 | Grad Norm: 0.00908058\n",
      "Epoch 3 | Step 97400 | Avg Loss: 0.0142 | Grad Norm: 0.00933150\n",
      "Epoch 3 | Step 97500 | Avg Loss: 0.0138 | Grad Norm: 0.01157200\n",
      "Epoch 3 | Step 97600 | Avg Loss: 0.0134 | Grad Norm: 0.01373339\n",
      "Epoch 3 | Step 97700 | Avg Loss: 0.0131 | Grad Norm: 0.01008459\n",
      "Epoch 3 | Step 97800 | Avg Loss: 0.0138 | Grad Norm: 0.00978896\n",
      "Epoch 3 | Step 97900 | Avg Loss: 0.0135 | Grad Norm: 0.00965170\n",
      "Epoch 3 | Step 98000 | Avg Loss: 0.0134 | Grad Norm: 0.01151282\n",
      "Epoch 3 | Step 98100 | Avg Loss: 0.0134 | Grad Norm: 0.00951653\n",
      "Epoch 3 | Step 98200 | Avg Loss: 0.0134 | Grad Norm: 0.00973439\n",
      "Epoch 3 | Step 98300 | Avg Loss: 0.0131 | Grad Norm: 0.00944397\n",
      "Epoch 3 | Step 98400 | Avg Loss: 0.0136 | Grad Norm: 0.00851282\n",
      "Epoch 3 | Step 98500 | Avg Loss: 0.0137 | Grad Norm: 0.00993229\n",
      "Epoch 3 | Step 98600 | Avg Loss: 0.0139 | Grad Norm: 0.01010492\n",
      "Epoch 3 | Step 98700 | Avg Loss: 0.0142 | Grad Norm: 0.01039748\n",
      "Epoch 3 | Step 98800 | Avg Loss: 0.0139 | Grad Norm: 0.01084061\n",
      "Epoch 3 | Step 98900 | Avg Loss: 0.0137 | Grad Norm: 0.00865089\n",
      "Epoch 3 | Step 99000 | Avg Loss: 0.0138 | Grad Norm: 0.00848789\n",
      "Epoch 3 | Step 99100 | Avg Loss: 0.0139 | Grad Norm: 0.01020036\n",
      "Epoch 3 | Step 99200 | Avg Loss: 0.0135 | Grad Norm: 0.01077860\n",
      "Epoch 3 | Step 99300 | Avg Loss: 0.0139 | Grad Norm: 0.01036775\n",
      "Epoch 3 | Step 99400 | Avg Loss: 0.0136 | Grad Norm: 0.00852084\n",
      "Epoch 3 | Step 99500 | Avg Loss: 0.0137 | Grad Norm: 0.00879894\n",
      "Epoch 3 | Step 99600 | Avg Loss: 0.0140 | Grad Norm: 0.01016500\n",
      "Epoch 3 | Step 99700 | Avg Loss: 0.0140 | Grad Norm: 0.01047879\n",
      "Epoch 3 | Step 99800 | Avg Loss: 0.0139 | Grad Norm: 0.01042833\n",
      "Epoch 3 | Step 99900 | Avg Loss: 0.0142 | Grad Norm: 0.00893506\n",
      "Epoch 3 | Step 100000 | Avg Loss: 0.0138 | Grad Norm: 0.00944644\n",
      "Saving model at step100000\n",
      "Epoch 3 | Step 100100 | Avg Loss: 0.0138 | Grad Norm: 0.01032201\n",
      "Epoch 3 | Step 100200 | Avg Loss: 0.0136 | Grad Norm: 0.01207018\n",
      "Epoch 3 | Step 100300 | Avg Loss: 0.0136 | Grad Norm: 0.01096246\n",
      "Epoch 3 | Step 100400 | Avg Loss: 0.0137 | Grad Norm: 0.00895222\n",
      "Epoch 3 | Step 100500 | Avg Loss: 0.0137 | Grad Norm: 0.00832280\n",
      "Epoch 3 | Step 100600 | Avg Loss: 0.0135 | Grad Norm: 0.00961790\n",
      "Epoch 3 | Step 100700 | Avg Loss: 0.0135 | Grad Norm: 0.00994941\n",
      "Epoch 3 | Step 100800 | Avg Loss: 0.0137 | Grad Norm: 0.00923194\n",
      "Epoch 3 | Step 100900 | Avg Loss: 0.0137 | Grad Norm: 0.00945695\n",
      "Epoch 3 | Step 101000 | Avg Loss: 0.0140 | Grad Norm: 0.00992034\n",
      "Epoch 3 | Step 101100 | Avg Loss: 0.0136 | Grad Norm: 0.00881292\n",
      "Epoch 3 | Step 101200 | Avg Loss: 0.0138 | Grad Norm: 0.00892522\n",
      "Epoch 3 | Step 101300 | Avg Loss: 0.0133 | Grad Norm: 0.00818076\n",
      "Epoch 3 | Step 101400 | Avg Loss: 0.0135 | Grad Norm: 0.01038140\n",
      "Epoch 3 | Step 101500 | Avg Loss: 0.0138 | Grad Norm: 0.00905033\n",
      "Epoch 3 | Step 101600 | Avg Loss: 0.0140 | Grad Norm: 0.01139237\n",
      "Epoch 3 | Step 101700 | Avg Loss: 0.0145 | Grad Norm: 0.01120823\n",
      "Epoch 3 | Step 101800 | Avg Loss: 0.0140 | Grad Norm: 0.00883911\n",
      "Epoch 3 | Step 101900 | Avg Loss: 0.0142 | Grad Norm: 0.01041920\n",
      "Epoch 3 | Step 102000 | Avg Loss: 0.0140 | Grad Norm: 0.01103782\n",
      "Epoch 3 | Step 102100 | Avg Loss: 0.0140 | Grad Norm: 0.00927249\n",
      "Epoch 3 | Step 102200 | Avg Loss: 0.0140 | Grad Norm: 0.01101584\n",
      "Epoch 3 | Step 102300 | Avg Loss: 0.0143 | Grad Norm: 0.01321696\n",
      "Epoch 3 | Step 102400 | Avg Loss: 0.0141 | Grad Norm: 0.01153747\n",
      "Epoch 3 | Step 102500 | Avg Loss: 0.0137 | Grad Norm: 0.00885808\n",
      "Epoch 3 | Step 102600 | Avg Loss: 0.0138 | Grad Norm: 0.00975375\n",
      "Epoch 3 | Step 102700 | Avg Loss: 0.0138 | Grad Norm: 0.00973945\n",
      "Epoch 3 | Step 102800 | Avg Loss: 0.0146 | Grad Norm: 0.01141990\n",
      "Epoch 3 | Step 102900 | Avg Loss: 0.0141 | Grad Norm: 0.01072663\n",
      "Epoch 3 | Step 103000 | Avg Loss: 0.0138 | Grad Norm: 0.00913023\n",
      "Epoch 3 | Step 103100 | Avg Loss: 0.0138 | Grad Norm: 0.00959215\n",
      "Epoch 3 | Step 103200 | Avg Loss: 0.0138 | Grad Norm: 0.01067927\n",
      "Epoch 3 | Step 103300 | Avg Loss: 0.0141 | Grad Norm: 0.00953644\n",
      "Epoch 3 | Step 103400 | Avg Loss: 0.0145 | Grad Norm: 0.00960316\n",
      "Epoch 3 | Step 103500 | Avg Loss: 0.0144 | Grad Norm: 0.01027298\n",
      "Epoch 3 | Step 103600 | Avg Loss: 0.0143 | Grad Norm: 0.01008588\n",
      "Epoch 3 | Step 103700 | Avg Loss: 0.0142 | Grad Norm: 0.01018530\n",
      "Epoch 3 | Step 103800 | Avg Loss: 0.0142 | Grad Norm: 0.01099303\n",
      "Epoch 3 | Step 103900 | Avg Loss: 0.0142 | Grad Norm: 0.01094265\n",
      "Epoch 3 | Step 104000 | Avg Loss: 0.0144 | Grad Norm: 0.01126813\n",
      "Epoch 3 | Step 104100 | Avg Loss: 0.0142 | Grad Norm: 0.00901124\n",
      "Epoch 3 | Step 104200 | Avg Loss: 0.0135 | Grad Norm: 0.00965331\n",
      "Epoch 3 | Step 104300 | Avg Loss: 0.0135 | Grad Norm: 0.00923721\n",
      "Epoch 3 | Step 104400 | Avg Loss: 0.0143 | Grad Norm: 0.00998874\n",
      "Epoch 3 | Step 104500 | Avg Loss: 0.0139 | Grad Norm: 0.00815210\n",
      "Epoch 3 | Step 104600 | Avg Loss: 0.0135 | Grad Norm: 0.00991683\n",
      "Epoch 3 | Step 104700 | Avg Loss: 0.0137 | Grad Norm: 0.01016479\n",
      "Epoch 3 | Step 104800 | Avg Loss: 0.0140 | Grad Norm: 0.01261134\n",
      "Epoch 3 | Step 104900 | Avg Loss: 0.0139 | Grad Norm: 0.00984743\n",
      "Epoch 3 | Step 105000 | Avg Loss: 0.0139 | Grad Norm: 0.01177448\n",
      "Epoch 3 | Step 105100 | Avg Loss: 0.0138 | Grad Norm: 0.01005534\n",
      "Epoch 3 | Step 105200 | Avg Loss: 0.0139 | Grad Norm: 0.00878158\n",
      "Epoch 3 | Step 105300 | Avg Loss: 0.0138 | Grad Norm: 0.00953240\n",
      "Epoch 3 | Step 105400 | Avg Loss: 0.0136 | Grad Norm: 0.01066002\n",
      "Epoch 3 | Step 105500 | Avg Loss: 0.0139 | Grad Norm: 0.01068543\n",
      "Epoch 3 | Step 105600 | Avg Loss: 0.0143 | Grad Norm: 0.01159416\n",
      "Epoch 3 | Step 105700 | Avg Loss: 0.0140 | Grad Norm: 0.00916808\n",
      "Epoch 3 | Step 105800 | Avg Loss: 0.0138 | Grad Norm: 0.00862823\n",
      "Epoch 3 | Step 105900 | Avg Loss: 0.0143 | Grad Norm: 0.01089198\n",
      "Epoch 3 | Step 106000 | Avg Loss: 0.0142 | Grad Norm: 0.00943584\n",
      "Epoch 3 | Step 106100 | Avg Loss: 0.0140 | Grad Norm: 0.00832333\n",
      "Epoch 3 | Step 106200 | Avg Loss: 0.0139 | Grad Norm: 0.01021819\n",
      "Epoch 3 | Step 106300 | Avg Loss: 0.0138 | Grad Norm: 0.01121266\n",
      "Epoch 3 | Step 106400 | Avg Loss: 0.0135 | Grad Norm: 0.00958743\n",
      "Epoch 3 | Step 106500 | Avg Loss: 0.0134 | Grad Norm: 0.00985368\n",
      "Epoch 3 | Step 106600 | Avg Loss: 0.0139 | Grad Norm: 0.01134146\n",
      "Epoch 3 | Step 106700 | Avg Loss: 0.0140 | Grad Norm: 0.00895483\n",
      "Epoch 3 | Step 106800 | Avg Loss: 0.0137 | Grad Norm: 0.01123205\n",
      "Epoch 3 | Step 106900 | Avg Loss: 0.0135 | Grad Norm: 0.01189790\n",
      "Epoch 3 | Step 107000 | Avg Loss: 0.0135 | Grad Norm: 0.00951703\n",
      "Epoch 3 | Step 107100 | Avg Loss: 0.0136 | Grad Norm: 0.00929801\n",
      "Epoch 3 | Step 107200 | Avg Loss: 0.0136 | Grad Norm: 0.01071309\n",
      "Epoch 3 | Step 107300 | Avg Loss: 0.0138 | Grad Norm: 0.01029168\n",
      "Epoch 3 | Step 107400 | Avg Loss: 0.0135 | Grad Norm: 0.01129012\n",
      "Epoch 3 | Step 107500 | Avg Loss: 0.0134 | Grad Norm: 0.00925341\n",
      "Epoch 3 | Step 107600 | Avg Loss: 0.0137 | Grad Norm: 0.00880272\n",
      "Epoch 3 | Step 107700 | Avg Loss: 0.0136 | Grad Norm: 0.00984109\n",
      "Epoch 3 | Step 107800 | Avg Loss: 0.0138 | Grad Norm: 0.01080853\n",
      "Epoch 3 | Step 107900 | Avg Loss: 0.0140 | Grad Norm: 0.01098723\n",
      "Epoch 3 | Step 108000 | Avg Loss: 0.0141 | Grad Norm: 0.00922854\n",
      "Epoch 3 | Step 108100 | Avg Loss: 0.0140 | Grad Norm: 0.01101541\n",
      "Epoch 3 | Step 108200 | Avg Loss: 0.0139 | Grad Norm: 0.01164436\n",
      "Epoch 3 | Step 108300 | Avg Loss: 0.0141 | Grad Norm: 0.01090999\n",
      "Epoch 3 | Step 108400 | Avg Loss: 0.0141 | Grad Norm: 0.00968590\n",
      "Epoch 3 | Step 108500 | Avg Loss: 0.0143 | Grad Norm: 0.00986512\n",
      "Epoch 3 | Step 108600 | Avg Loss: 0.0139 | Grad Norm: 0.00974416\n",
      "Epoch 3 | Step 108700 | Avg Loss: 0.0136 | Grad Norm: 0.00983638\n",
      "Epoch 3 | Step 108800 | Avg Loss: 0.0137 | Grad Norm: 0.00923447\n",
      "Epoch 3 | Step 108900 | Avg Loss: 0.0137 | Grad Norm: 0.00964255\n",
      "Epoch 3 | Step 109000 | Avg Loss: 0.0137 | Grad Norm: 0.00956719\n",
      "Epoch 3 | Step 109100 | Avg Loss: 0.0138 | Grad Norm: 0.01033927\n",
      "Epoch 3 | Step 109200 | Avg Loss: 0.0140 | Grad Norm: 0.01068250\n",
      "Epoch 3 | Step 109300 | Avg Loss: 0.0137 | Grad Norm: 0.01005381\n",
      "Epoch 3 | Step 109400 | Avg Loss: 0.0135 | Grad Norm: 0.01051677\n",
      "Epoch 3 | Step 109500 | Avg Loss: 0.0139 | Grad Norm: 0.00973116\n",
      "Epoch 3 | Step 109600 | Avg Loss: 0.0138 | Grad Norm: 0.00890154\n",
      "Epoch 3 | Step 109700 | Avg Loss: 0.0135 | Grad Norm: 0.00902347\n",
      "Epoch 3 | Step 109800 | Avg Loss: 0.0137 | Grad Norm: 0.00928987\n",
      "Epoch 3 | Step 109900 | Avg Loss: 0.0139 | Grad Norm: 0.01011476\n",
      "Epoch 3 | Step 110000 | Avg Loss: 0.0139 | Grad Norm: 0.00979776\n",
      "Epoch 3 | Step 110100 | Avg Loss: 0.0139 | Grad Norm: 0.00934264\n",
      "Epoch 3 | Step 110200 | Avg Loss: 0.0139 | Grad Norm: 0.00939929\n",
      "Epoch 3 | Step 110300 | Avg Loss: 0.0137 | Grad Norm: 0.01227217\n",
      "Epoch 3 | Step 110400 | Avg Loss: 0.0141 | Grad Norm: 0.01038159\n",
      "Epoch 3 | Step 110500 | Avg Loss: 0.0141 | Grad Norm: 0.00900085\n",
      "Epoch 3 | Step 110600 | Avg Loss: 0.0141 | Grad Norm: 0.01157849\n",
      "Epoch 3 | Step 110700 | Avg Loss: 0.0138 | Grad Norm: 0.01002886\n",
      "Epoch 3 | Step 110800 | Avg Loss: 0.0140 | Grad Norm: 0.00922895\n",
      "Epoch 3 | Step 110900 | Avg Loss: 0.0140 | Grad Norm: 0.00927345\n",
      "Epoch 3 | Step 111000 | Avg Loss: 0.0137 | Grad Norm: 0.00950435\n",
      "Epoch 3 | Step 111100 | Avg Loss: 0.0139 | Grad Norm: 0.00908000\n",
      "Epoch 3 | Step 111200 | Avg Loss: 0.0137 | Grad Norm: 0.01067572\n",
      "Epoch 3 | Step 111300 | Avg Loss: 0.0135 | Grad Norm: 0.00982507\n",
      "Epoch 3 | Step 111400 | Avg Loss: 0.0135 | Grad Norm: 0.01141893\n",
      "Epoch 3 | Step 111500 | Avg Loss: 0.0139 | Grad Norm: 0.00965314\n",
      "Epoch 3 | Step 111600 | Avg Loss: 0.0136 | Grad Norm: 0.01055729\n",
      "Epoch 3 | Step 111700 | Avg Loss: 0.0136 | Grad Norm: 0.01035895\n",
      "Epoch 3 | Step 111800 | Avg Loss: 0.0135 | Grad Norm: 0.00892009\n",
      "Epoch 3 | Step 111900 | Avg Loss: 0.0135 | Grad Norm: 0.00865217\n",
      "Epoch 3 | Step 112000 | Avg Loss: 0.0133 | Grad Norm: 0.01074907\n",
      "Epoch 3 | Step 112100 | Avg Loss: 0.0132 | Grad Norm: 0.01103873\n",
      "Epoch 3 | Step 112200 | Avg Loss: 0.0128 | Grad Norm: 0.00753212\n",
      "Epoch 3 | Step 112300 | Avg Loss: 0.0128 | Grad Norm: 0.00937905\n",
      "Epoch 3 | Step 112400 | Avg Loss: 0.0129 | Grad Norm: 0.01099203\n",
      "Epoch 3 | Step 112500 | Avg Loss: 0.0132 | Grad Norm: 0.00917238\n",
      "Epoch 3 | Step 112600 | Avg Loss: 0.0134 | Grad Norm: 0.00955095\n",
      "Epoch 3 | Step 112700 | Avg Loss: 0.0139 | Grad Norm: 0.00953018\n",
      "Epoch 3 | Step 112800 | Avg Loss: 0.0134 | Grad Norm: 0.00924134\n",
      "Epoch 3 | Step 112900 | Avg Loss: 0.0135 | Grad Norm: 0.00898953\n",
      "Epoch 3 | Step 113000 | Avg Loss: 0.0136 | Grad Norm: 0.00937908\n",
      "Epoch 3 | Step 113100 | Avg Loss: 0.0137 | Grad Norm: 0.01122398\n",
      "Epoch 3 | Step 113200 | Avg Loss: 0.0136 | Grad Norm: 0.00813969\n",
      "Epoch 3 | Step 113300 | Avg Loss: 0.0141 | Grad Norm: 0.01071823\n",
      "Epoch 3 | Step 113400 | Avg Loss: 0.0138 | Grad Norm: 0.01033193\n",
      "Epoch 3 | Step 113500 | Avg Loss: 0.0135 | Grad Norm: 0.00988759\n",
      "Epoch 3 | Step 113600 | Avg Loss: 0.0138 | Grad Norm: 0.01096631\n",
      "Epoch 3 | Step 113700 | Avg Loss: 0.0141 | Grad Norm: 0.01107952\n",
      "Epoch 3 | Step 113800 | Avg Loss: 0.0138 | Grad Norm: 0.00968080\n",
      "Epoch 3 | Step 113900 | Avg Loss: 0.0135 | Grad Norm: 0.00946731\n",
      "Epoch 3 | Step 114000 | Avg Loss: 0.0135 | Grad Norm: 0.01068787\n",
      "Epoch 3 | Step 114100 | Avg Loss: 0.0133 | Grad Norm: 0.01039853\n",
      "Epoch 3 | Step 114200 | Avg Loss: 0.0134 | Grad Norm: 0.01221796\n",
      "Epoch 3 | Step 114300 | Avg Loss: 0.0137 | Grad Norm: 0.00990110\n",
      "Epoch 3 | Step 114400 | Avg Loss: 0.0143 | Grad Norm: 0.01079467\n",
      "Epoch 3 | Step 114500 | Avg Loss: 0.0144 | Grad Norm: 0.01174013\n",
      "Epoch 3 | Step 114600 | Avg Loss: 0.0141 | Grad Norm: 0.01001933\n",
      "Epoch 3 | Step 114700 | Avg Loss: 0.0141 | Grad Norm: 0.01161127\n",
      "Epoch 3 | Step 114800 | Avg Loss: 0.0139 | Grad Norm: 0.01047526\n",
      "Epoch 3 | Step 114900 | Avg Loss: 0.0140 | Grad Norm: 0.00999731\n",
      "Epoch 3 | Step 115000 | Avg Loss: 0.0140 | Grad Norm: 0.01184188\n",
      "Epoch 3 | Step 115100 | Avg Loss: 0.0146 | Grad Norm: 0.01037386\n",
      "Epoch 3 | Step 115200 | Avg Loss: 0.0141 | Grad Norm: 0.00980472\n",
      "Epoch 3 | Step 115300 | Avg Loss: 0.0139 | Grad Norm: 0.00929541\n",
      "Epoch 3 | Step 115400 | Avg Loss: 0.0138 | Grad Norm: 0.00978414\n",
      "Epoch 3 | Step 115500 | Avg Loss: 0.0138 | Grad Norm: 0.00989823\n",
      "Epoch 3 | Step 115600 | Avg Loss: 0.0137 | Grad Norm: 0.00875031\n",
      "Epoch 3 | Step 115700 | Avg Loss: 0.0136 | Grad Norm: 0.00940343\n",
      "Epoch 3 | Step 115800 | Avg Loss: 0.0138 | Grad Norm: 0.01084950\n",
      "Epoch 3 | Step 115900 | Avg Loss: 0.0141 | Grad Norm: 0.01166607\n",
      "Epoch 3 | Step 116000 | Avg Loss: 0.0137 | Grad Norm: 0.01325355\n",
      "Epoch 3 | Step 116100 | Avg Loss: 0.0140 | Grad Norm: 0.01053059\n",
      "Epoch 3 | Step 116200 | Avg Loss: 0.0142 | Grad Norm: 0.01032215\n",
      "Epoch 3 | Step 116300 | Avg Loss: 0.0139 | Grad Norm: 0.01033249\n",
      "Epoch 3 | Step 116400 | Avg Loss: 0.0142 | Grad Norm: 0.01088608\n",
      "Epoch 3 | Step 116500 | Avg Loss: 0.0142 | Grad Norm: 0.00999590\n",
      "Epoch 3 | Step 116600 | Avg Loss: 0.0141 | Grad Norm: 0.01018840\n",
      "Epoch 3 | Step 116700 | Avg Loss: 0.0141 | Grad Norm: 0.00939091\n",
      "Epoch 3 | Step 116800 | Avg Loss: 0.0143 | Grad Norm: 0.00970789\n",
      "Epoch 3 | Step 116900 | Avg Loss: 0.0135 | Grad Norm: 0.01394858\n",
      "Epoch 3 | Step 117000 | Avg Loss: 0.0121 | Grad Norm: 0.00941002\n",
      "Epoch 3 | Step 117100 | Avg Loss: 0.0114 | Grad Norm: 0.01095260\n",
      "Epoch 3, Loss: 0.0089\n",
      "Epoch 4 | Step 117200 | Avg Loss: 0.0116 | Grad Norm: 0.01358217\n",
      "Epoch 4 | Step 117300 | Avg Loss: 0.0169 | Grad Norm: 0.01000497\n",
      "Epoch 4 | Step 117400 | Avg Loss: 0.0148 | Grad Norm: 0.00836916\n",
      "Epoch 4 | Step 117500 | Avg Loss: 0.0139 | Grad Norm: 0.00983412\n",
      "Epoch 4 | Step 117600 | Avg Loss: 0.0135 | Grad Norm: 0.00915996\n",
      "Epoch 4 | Step 117700 | Avg Loss: 0.0134 | Grad Norm: 0.00973336\n",
      "Epoch 4 | Step 117800 | Avg Loss: 0.0135 | Grad Norm: 0.01167355\n",
      "Epoch 4 | Step 117900 | Avg Loss: 0.0141 | Grad Norm: 0.01002637\n",
      "Epoch 4 | Step 118000 | Avg Loss: 0.0141 | Grad Norm: 0.01217490\n",
      "Epoch 4 | Step 118100 | Avg Loss: 0.0145 | Grad Norm: 0.01033504\n",
      "Epoch 4 | Step 118200 | Avg Loss: 0.0141 | Grad Norm: 0.01044163\n",
      "Epoch 4 | Step 118300 | Avg Loss: 0.0136 | Grad Norm: 0.00976250\n",
      "Epoch 4 | Step 118400 | Avg Loss: 0.0135 | Grad Norm: 0.00956112\n",
      "Epoch 4 | Step 118500 | Avg Loss: 0.0136 | Grad Norm: 0.00903176\n",
      "Epoch 4 | Step 118600 | Avg Loss: 0.0135 | Grad Norm: 0.00881212\n",
      "Epoch 4 | Step 118700 | Avg Loss: 0.0134 | Grad Norm: 0.01166103\n",
      "Epoch 4 | Step 118800 | Avg Loss: 0.0131 | Grad Norm: 0.01055639\n",
      "Epoch 4 | Step 118900 | Avg Loss: 0.0133 | Grad Norm: 0.01030375\n",
      "Epoch 4 | Step 119000 | Avg Loss: 0.0133 | Grad Norm: 0.01050599\n",
      "Epoch 4 | Step 119100 | Avg Loss: 0.0135 | Grad Norm: 0.00972271\n",
      "Epoch 4 | Step 119200 | Avg Loss: 0.0137 | Grad Norm: 0.01013175\n",
      "Epoch 4 | Step 119300 | Avg Loss: 0.0137 | Grad Norm: 0.01066479\n",
      "Epoch 4 | Step 119400 | Avg Loss: 0.0137 | Grad Norm: 0.01084890\n",
      "Epoch 4 | Step 119500 | Avg Loss: 0.0135 | Grad Norm: 0.01185951\n",
      "Epoch 4 | Step 119600 | Avg Loss: 0.0136 | Grad Norm: 0.01102867\n",
      "Epoch 4 | Step 119700 | Avg Loss: 0.0137 | Grad Norm: 0.01022361\n",
      "Epoch 4 | Step 119800 | Avg Loss: 0.0137 | Grad Norm: 0.00985252\n",
      "Epoch 4 | Step 119900 | Avg Loss: 0.0138 | Grad Norm: 0.01173668\n",
      "Epoch 4 | Step 120000 | Avg Loss: 0.0137 | Grad Norm: 0.01034445\n",
      "Epoch 4 | Step 120100 | Avg Loss: 0.0138 | Grad Norm: 0.01126861\n",
      "Epoch 4 | Step 120200 | Avg Loss: 0.0139 | Grad Norm: 0.01000591\n",
      "Epoch 4 | Step 120300 | Avg Loss: 0.0133 | Grad Norm: 0.01028640\n",
      "Epoch 4 | Step 120400 | Avg Loss: 0.0132 | Grad Norm: 0.01038131\n",
      "Epoch 4 | Step 120500 | Avg Loss: 0.0132 | Grad Norm: 0.00980716\n",
      "Epoch 4 | Step 120600 | Avg Loss: 0.0133 | Grad Norm: 0.01051250\n",
      "Epoch 4 | Step 120700 | Avg Loss: 0.0129 | Grad Norm: 0.00899268\n",
      "Epoch 4 | Step 120800 | Avg Loss: 0.0132 | Grad Norm: 0.00992488\n",
      "Epoch 4 | Step 120900 | Avg Loss: 0.0135 | Grad Norm: 0.01212593\n",
      "Epoch 4 | Step 121000 | Avg Loss: 0.0137 | Grad Norm: 0.01047476\n",
      "Epoch 4 | Step 121100 | Avg Loss: 0.0135 | Grad Norm: 0.01067671\n",
      "Epoch 4 | Step 121200 | Avg Loss: 0.0135 | Grad Norm: 0.00882831\n",
      "Epoch 4 | Step 121300 | Avg Loss: 0.0134 | Grad Norm: 0.00915935\n",
      "Epoch 4 | Step 121400 | Avg Loss: 0.0133 | Grad Norm: 0.01008300\n",
      "Epoch 4 | Step 121500 | Avg Loss: 0.0132 | Grad Norm: 0.01225796\n",
      "Epoch 4 | Step 121600 | Avg Loss: 0.0134 | Grad Norm: 0.01000806\n",
      "Epoch 4 | Step 121700 | Avg Loss: 0.0135 | Grad Norm: 0.00934565\n",
      "Epoch 4 | Step 121800 | Avg Loss: 0.0129 | Grad Norm: 0.01015947\n",
      "Epoch 4 | Step 121900 | Avg Loss: 0.0131 | Grad Norm: 0.00990112\n",
      "Epoch 4 | Step 122000 | Avg Loss: 0.0135 | Grad Norm: 0.01070880\n",
      "Epoch 4 | Step 122100 | Avg Loss: 0.0135 | Grad Norm: 0.00971339\n",
      "Epoch 4 | Step 122200 | Avg Loss: 0.0134 | Grad Norm: 0.00840229\n",
      "Epoch 4 | Step 122300 | Avg Loss: 0.0135 | Grad Norm: 0.01097448\n",
      "Epoch 4 | Step 122400 | Avg Loss: 0.0132 | Grad Norm: 0.00993428\n",
      "Epoch 4 | Step 122500 | Avg Loss: 0.0134 | Grad Norm: 0.01043087\n",
      "Epoch 4 | Step 122600 | Avg Loss: 0.0138 | Grad Norm: 0.01026164\n",
      "Epoch 4 | Step 122700 | Avg Loss: 0.0137 | Grad Norm: 0.00868708\n",
      "Epoch 4 | Step 122800 | Avg Loss: 0.0135 | Grad Norm: 0.01120732\n",
      "Epoch 4 | Step 122900 | Avg Loss: 0.0136 | Grad Norm: 0.00960254\n",
      "Epoch 4 | Step 123000 | Avg Loss: 0.0137 | Grad Norm: 0.00884309\n",
      "Epoch 4 | Step 123100 | Avg Loss: 0.0134 | Grad Norm: 0.01197659\n",
      "Epoch 4 | Step 123200 | Avg Loss: 0.0135 | Grad Norm: 0.00898400\n",
      "Epoch 4 | Step 123300 | Avg Loss: 0.0131 | Grad Norm: 0.00945874\n",
      "Epoch 4 | Step 123400 | Avg Loss: 0.0134 | Grad Norm: 0.00960346\n",
      "Epoch 4 | Step 123500 | Avg Loss: 0.0138 | Grad Norm: 0.00934607\n",
      "Epoch 4 | Step 123600 | Avg Loss: 0.0135 | Grad Norm: 0.00980101\n",
      "Epoch 4 | Step 123700 | Avg Loss: 0.0136 | Grad Norm: 0.01063556\n",
      "Epoch 4 | Step 123800 | Avg Loss: 0.0136 | Grad Norm: 0.01045530\n",
      "Epoch 4 | Step 123900 | Avg Loss: 0.0136 | Grad Norm: 0.00948153\n",
      "Epoch 4 | Step 124000 | Avg Loss: 0.0136 | Grad Norm: 0.01095516\n",
      "Epoch 4 | Step 124100 | Avg Loss: 0.0135 | Grad Norm: 0.01077826\n",
      "Epoch 4 | Step 124200 | Avg Loss: 0.0134 | Grad Norm: 0.01038846\n",
      "Epoch 4 | Step 124300 | Avg Loss: 0.0133 | Grad Norm: 0.01087914\n",
      "Epoch 4 | Step 124400 | Avg Loss: 0.0129 | Grad Norm: 0.01038021\n",
      "Epoch 4 | Step 124500 | Avg Loss: 0.0133 | Grad Norm: 0.01002936\n",
      "Epoch 4 | Step 124600 | Avg Loss: 0.0130 | Grad Norm: 0.00947817\n",
      "Epoch 4 | Step 124700 | Avg Loss: 0.0132 | Grad Norm: 0.00953403\n",
      "Epoch 4 | Step 124800 | Avg Loss: 0.0136 | Grad Norm: 0.00924399\n",
      "Epoch 4 | Step 124900 | Avg Loss: 0.0136 | Grad Norm: 0.01088842\n",
      "Epoch 4 | Step 125000 | Avg Loss: 0.0136 | Grad Norm: 0.01016657\n",
      "Epoch 4 | Step 125100 | Avg Loss: 0.0136 | Grad Norm: 0.01064966\n",
      "Epoch 4 | Step 125200 | Avg Loss: 0.0137 | Grad Norm: 0.01119352\n",
      "Epoch 4 | Step 125300 | Avg Loss: 0.0137 | Grad Norm: 0.00914227\n",
      "Epoch 4 | Step 125400 | Avg Loss: 0.0135 | Grad Norm: 0.00948476\n",
      "Epoch 4 | Step 125500 | Avg Loss: 0.0136 | Grad Norm: 0.00946673\n",
      "Epoch 4 | Step 125600 | Avg Loss: 0.0135 | Grad Norm: 0.01154604\n",
      "Epoch 4 | Step 125700 | Avg Loss: 0.0137 | Grad Norm: 0.01140391\n",
      "Epoch 4 | Step 125800 | Avg Loss: 0.0140 | Grad Norm: 0.01108051\n",
      "Epoch 4 | Step 125900 | Avg Loss: 0.0138 | Grad Norm: 0.00921483\n",
      "Epoch 4 | Step 126000 | Avg Loss: 0.0136 | Grad Norm: 0.00967481\n",
      "Epoch 4 | Step 126100 | Avg Loss: 0.0137 | Grad Norm: 0.01022568\n",
      "Epoch 4 | Step 126200 | Avg Loss: 0.0138 | Grad Norm: 0.01050471\n",
      "Epoch 4 | Step 126300 | Avg Loss: 0.0137 | Grad Norm: 0.01226404\n",
      "Epoch 4 | Step 126400 | Avg Loss: 0.0137 | Grad Norm: 0.01120131\n",
      "Epoch 4 | Step 126500 | Avg Loss: 0.0134 | Grad Norm: 0.01028717\n",
      "Epoch 4 | Step 126600 | Avg Loss: 0.0135 | Grad Norm: 0.01145414\n",
      "Epoch 4 | Step 126700 | Avg Loss: 0.0135 | Grad Norm: 0.00945368\n",
      "Epoch 4 | Step 126800 | Avg Loss: 0.0138 | Grad Norm: 0.00995826\n",
      "Epoch 4 | Step 126900 | Avg Loss: 0.0137 | Grad Norm: 0.01197369\n",
      "Epoch 4 | Step 127000 | Avg Loss: 0.0136 | Grad Norm: 0.01012296\n",
      "Epoch 4 | Step 127100 | Avg Loss: 0.0139 | Grad Norm: 0.01005430\n",
      "Epoch 4 | Step 127200 | Avg Loss: 0.0139 | Grad Norm: 0.01419222\n",
      "Epoch 4 | Step 127300 | Avg Loss: 0.0141 | Grad Norm: 0.00945037\n",
      "Epoch 4 | Step 127400 | Avg Loss: 0.0139 | Grad Norm: 0.01397867\n",
      "Epoch 4 | Step 127500 | Avg Loss: 0.0140 | Grad Norm: 0.01085609\n",
      "Epoch 4 | Step 127600 | Avg Loss: 0.0139 | Grad Norm: 0.00995489\n",
      "Epoch 4 | Step 127700 | Avg Loss: 0.0142 | Grad Norm: 0.01113346\n",
      "Epoch 4 | Step 127800 | Avg Loss: 0.0140 | Grad Norm: 0.00900683\n",
      "Epoch 4 | Step 127900 | Avg Loss: 0.0137 | Grad Norm: 0.01041836\n",
      "Epoch 4 | Step 128000 | Avg Loss: 0.0141 | Grad Norm: 0.01054822\n",
      "Epoch 4 | Step 128100 | Avg Loss: 0.0137 | Grad Norm: 0.01009857\n",
      "Epoch 4 | Step 128200 | Avg Loss: 0.0136 | Grad Norm: 0.01355621\n",
      "Epoch 4 | Step 128300 | Avg Loss: 0.0142 | Grad Norm: 0.01077926\n",
      "Epoch 4 | Step 128400 | Avg Loss: 0.0139 | Grad Norm: 0.01148992\n",
      "Epoch 4 | Step 128500 | Avg Loss: 0.0140 | Grad Norm: 0.01042187\n",
      "Epoch 4 | Step 128600 | Avg Loss: 0.0135 | Grad Norm: 0.01137778\n",
      "Epoch 4 | Step 128700 | Avg Loss: 0.0140 | Grad Norm: 0.00992739\n",
      "Epoch 4 | Step 128800 | Avg Loss: 0.0137 | Grad Norm: 0.00931083\n",
      "Epoch 4 | Step 128900 | Avg Loss: 0.0138 | Grad Norm: 0.01078368\n",
      "Epoch 4 | Step 129000 | Avg Loss: 0.0136 | Grad Norm: 0.01053598\n",
      "Epoch 4 | Step 129100 | Avg Loss: 0.0133 | Grad Norm: 0.01037700\n",
      "Epoch 4 | Step 129200 | Avg Loss: 0.0134 | Grad Norm: 0.01095717\n",
      "Epoch 4 | Step 129300 | Avg Loss: 0.0132 | Grad Norm: 0.00938887\n",
      "Epoch 4 | Step 129400 | Avg Loss: 0.0132 | Grad Norm: 0.01038874\n",
      "Epoch 4 | Step 129500 | Avg Loss: 0.0133 | Grad Norm: 0.01104780\n",
      "Epoch 4 | Step 129600 | Avg Loss: 0.0137 | Grad Norm: 0.00834785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#criterion = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#for _ in range(1000000):\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get data from the dataloader\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/nnue_training_split_model_10M\")\n",
    "\n",
    "# hyperparameters\n",
    "nr_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "scaling_factor = 400\n",
    "ground_truth_scaling_factor = 400\n",
    "lambda_ = 0.2\n",
    "log_interval = 100\n",
    "save_interval = 100000\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "epsilon = 1e-10\n",
    "\n",
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Split_NNUE().to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "\n",
    "        #for _ in range(1000000):\n",
    "    \n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y, result = batch\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        result = result.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = lambda_*result + (1 - lambda_) * torch.sigmoid(batch_y / ground_truth_scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        #loss = (wdl_batch_y * torch.log(wdl_batch_y + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_batch_y + epsilon)) -(wdl_batch_y * torch.log(wdl_pred   + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_pred   + epsilon))\n",
    "        #loss = loss.mean()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(wdl_pred, wdl_batch_y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # make a step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        # calculate the gradient norm\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm of this parameter's gradient\n",
    "                total_norm_sq += param_norm.item() ** 2\n",
    "        total_grad_norm = total_norm_sq ** 0.5\n",
    "        # Now total_grad_norm is the L2 norm of all gradients combined.\n",
    "        #print(f\"Step {step}  Grad Norm = {total_grad_norm:.8f}\")\n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Avg Loss: {avg_loss:.4f} | Grad Norm: {total_grad_norm:.8f}\")\n",
    "            running_loss = 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "            writer.add_scalar(\"Grad Norm\", total_grad_norm, step)\n",
    "            writer.add_scalar(\"WDL Pred\", torch.mean(wdl_pred).item(), step)\n",
    "            writer.add_scalar(\"WDL BatchY\", torch.mean(wdl_batch_y).item(), step)\n",
    "            writer.add_scalar(\"Pred\", torch.median(pred).item(), step)\n",
    "            writer.add_scalar(\"BatchY\", torch.median(batch_y).item(), step)\n",
    "\n",
    "\n",
    "            # log separate grad norms\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    writer.add_scalar(f'GradNorm/{name}', grad_norm, step)\n",
    "\n",
    "        # Save the model every `save_interval` steps\n",
    "        if step % save_interval == 0:\n",
    "            model_name = 'saved_models/split_model_10M_' + str(step) + \".pth\"\n",
    "            print(\"Saving model at step\" + str(step))\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,}, model_name)\n",
    "            \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d09a",
   "metadata": {},
   "source": [
    "### Postprocessing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def save_layer(layer, name):\n",
    "    w = layer.weight.detach().numpy()\n",
    "    b = layer.bias.detach().numpy()\n",
    "    with open(f\"{name}_weights.txt\", \"w\") as f:\n",
    "        for row in w:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    with open(f\"{name}_biases.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(map(str, b)))\n",
    "\n",
    "save_layer(model.fc1, \"model/layer1\")\n",
    "save_layer(model.fc2, \"model/layer2\")\n",
    "save_layer(model.fc3, \"model/layer3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22e35ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768]) torch.Size([1, 768]) torch.Size([1])\n",
      "75.44014739990234\n",
      "60.845481872558594\n",
      "-1.3742446899414062\n",
      "[-0.10270691 -0.7508942  -0.73212284 -0.6711883   0.4690156  -0.8976217\n",
      " -0.97113603 -0.7924493  -0.5964821  -0.75407517 -0.8800155  -0.2378027\n",
      "  0.70388985 -0.6717296  -0.9132408  -0.8965203  -0.89196694 -0.35501885\n",
      "  0.00581718  1.0301418  -0.73850054 -0.30603865 -0.21337932 -1.2286891\n",
      " -0.00251864 -1.0047029  -0.3110618  -0.640365   -0.5807997  -0.14240061\n",
      "  0.07644044 -0.7504747 ]\n"
     ]
    }
   ],
   "source": [
    "model = Split_NNUE()\n",
    "checkpoint = torch.load('saved_models/split_model_10M_100000.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "fen2 = 'rnbqkbnr/pppppppp/8/8/8/7N/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "fen3 = 'rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R b KQkq - 1 1'\n",
    "\n",
    "start_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "\n",
    "#torch.tensor(FEN_to_input(fen1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1, b1, stm1 = FEN_to_inputs(fen1)\n",
    "    w1 = w1.unsqueeze(0)\n",
    "    b1 = b1.unsqueeze(0)\n",
    "    stm1 = stm1.unsqueeze(0)\n",
    "    print(w1.shape, b1.shape, stm1.shape)\n",
    "    #in_1 = np.argwhere(input1.numpy() == 1)\n",
    "    #print(sum(input1.numpy()))\n",
    "    w2, b2, stm2 = FEN_to_inputs(fen2)\n",
    "    w2 = w2.unsqueeze(0)\n",
    "    b2 = b2.unsqueeze(0)\n",
    "    stm2 = stm2.unsqueeze(0)\n",
    "    #print(np.argwhere(input2.numpy() == 1))\n",
    "    w3, b3, stm3 = FEN_to_inputs(fen3)\n",
    "    w3 = w3.unsqueeze(0)\n",
    "    b3 = b3.unsqueeze(0)\n",
    "    stm3 = stm3.unsqueeze(0)\n",
    "    #print(np.argwhere(input3.numpy() == 1))\n",
    "\n",
    "    #in_start = np.argwhere(FEN_to_inputs(start_fen).numpy() == 1)\n",
    "\n",
    "    pred1 = model(w1, b1, stm1)\n",
    "    pred2 = model(w2, b2, stm2)\n",
    "    pred3 = model(w3, b3, stm3)\n",
    "\n",
    "    print(pred1.item())\n",
    "    print(pred2.item())\n",
    "    print(pred3.item())\n",
    "\n",
    "    #accumulator = model.fc1(input1)\n",
    "    #ws, bs, stms = FEN_to_inputs(start_fen)\n",
    "    #ws = ws.unsqueeze(0)\n",
    "    #bs = bs.unsqueeze(0)\n",
    "    #stms = stms.unsqueeze(0)\n",
    "    w_accumulator = model.fc1(w1)\n",
    "    b_accumulator = model.fc1(b1)\n",
    "    #print(w_accumulator)\n",
    "    #print(b_accumulator)\n",
    "\n",
    "\n",
    "    cat_wb = torch.cat([w_accumulator, b_accumulator], dim=1)\n",
    "    cat_bw = torch.cat([b_accumulator, w_accumulator], dim=1)\n",
    "\n",
    "    #stm1 = stm1.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "    #print(cat_bw)\n",
    "\n",
    "    accumulator = (1 - stm1) * cat_wb + stm1 * cat_bw\n",
    "    #print(accumulator)\n",
    "\n",
    "    x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "    x = model.fc2(x)\n",
    "\n",
    "    #print(x)\n",
    "\n",
    "    print(model.fc2.bias.detach().numpy())\n",
    "    #print(model.fc2.weight[0][:10])\n",
    "\n",
    "    #print(\"weights[0][0]\")\n",
    "    #print(model.fc1.weight[0][0])\n",
    "    #print(\"weights[1][0]\")\n",
    "    #print(model.fc1.weight[1][0])\n",
    "    #print(model.fc1.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of bins\n",
    "num_bins = 64\n",
    "bin_edges = np.linspace(-32003, 32003, num_bins + 1)\n",
    "counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "filename = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/scores.txt'\n",
    "\n",
    "# Re-read the file and bin values\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            val = float(line.strip())\n",
    "            # Determine bin index\n",
    "            bin_idx = np.searchsorted(bin_edges, val, side='right') - 1\n",
    "            if 0 <= bin_idx < num_bins:\n",
    "                counts[bin_idx] += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Find empty bins\n",
    "empty_bins = []\n",
    "for i, count in enumerate(counts):\n",
    "    if count == 0:\n",
    "        left_edge = bin_edges[i]\n",
    "        right_edge = bin_edges[i + 1]\n",
    "        empty_bins.append((i, left_edge, right_edge))\n",
    "\n",
    "# Print empty bin ranges\n",
    "print(\"Empty bins:\")\n",
    "for i, left, right in empty_bins:\n",
    "    print(f\"Bin {i}: [{left}, {right})\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge')\n",
    "plt.title(\"Histogram (streamed)\")\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP to WDL conversion\n",
    "scaling_factor = 400\n",
    "score = torch.tensor(32000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(-1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(0, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41259",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_2 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 13, 14, 15, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "\n",
    "print(np.sort(in_start.reshape(1, 32)))\n",
    "print(np.sort(in_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481ab8",
   "metadata": {},
   "source": [
    "### HalfKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "\n",
    "def FEN_to_HalfKP(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    position = 0\n",
    "    white_king_position = 0\n",
    "    black_king_position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            elif char == 'K':\n",
    "                white_king_position = position\n",
    "                position += 1\n",
    "            elif char == 'k':\n",
    "                black_king_position = position\n",
    "                position += 1\n",
    "            else:\n",
    "                position += 1\n",
    "\n",
    "    white_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    black_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                if (char != 'K') & (char != 'k'):\n",
    "                    piece_index = (piece_dict[char] % 6) * 2 + (piece_dict[char] > 5)\n",
    "                    white_input_layer[position + (piece_index + white_king_position*10)*64] = 1\n",
    "                    black_input_layer[position + (piece_index + black_king_position*10)*64] = 1\n",
    "                    position += 1\n",
    "                else:\n",
    "                    position += 1\n",
    "\n",
    "    return torch.tensor(white_input_layer, dtype=torch.float32), torch.tensor(black_input_layer, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6378bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class HalfKP_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_HalfKP(row[0].strip())\n",
    "                score = float(row[1].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                buf.append((w_in, b_in, stm, score))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                yield w_in, b_in, stm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_FEATURES = 40960\n",
    "M = 1024\n",
    "N = 32\n",
    "K = 1\n",
    "\n",
    "class HalfKPNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HalfKPNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, M)\n",
    "        self.fc2 = nn.Linear(2*M, N)\n",
    "        self.fc3 = nn.Linear(N, K)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)  # [B, 2*M]\n",
    "        cat_bw = torch.cat([b, w], dim=1)  # [B, 2*M]\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = stm * cat_wb + (1 - stm) * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0.0, max = 1.0)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data.csv'\n",
    "dataset = HalfKP_Dataset(csv_path, shuffle_buffer=1000)\n",
    "loader = DataLoader(dataset, batch_size = 128, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/halfKP\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nr_epochs = 500\n",
    "model = HalfKPNNUE().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.05)\n",
    "total_size = 200000000\n",
    "batch_size = 128\n",
    "steps_per_epoch = total_size // batch_size\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "MAE_loss = nn.L1Loss()\n",
    "lowest_MAE = 10000\n",
    "\n",
    "# Transform the CP scores to the WDL space\n",
    "scaling_factor = 400\n",
    "\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "log_interval = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "        #for _ in range(100000):\n",
    "\n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y = batch\n",
    "\n",
    "        # move data to GPU\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)  # remove the last dimension\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = torch.sigmoid(batch_y / scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        # calculate the MSE loss\n",
    "        loss = criterion(wdl_batch_y, wdl_pred)\n",
    "        MAE = MAE_loss(wdl_batch_y, wdl_pred)\n",
    "        running_loss += loss.item()\n",
    "        running_mae += MAE\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            avg_mae = running_mae / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{steps_per_epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"MAE/train\", avg_mae, step)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "\n",
    "        # calculate MAE\n",
    "        if MAE < 0.0002:\n",
    "            lowest_MAE = MAE\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with MAE: {lowest_MAE.item():.4f}, loss: {loss.item():.4f}\")\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, MAE: {MAE.item():.4f}, lowest MAE: {lowest_MAE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def FEN_to_input(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                input_layer[position + piece_dict[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        #self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.clamp(self.fc1(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc3(x), min = 0, max = 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                fen, score, result = FEN_to_input(row[0].strip()), float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield fen, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for fen, score, result in stream:\n",
    "                buf.append((fen, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for fen, score, result in stream:\n",
    "                yield fen, score, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
