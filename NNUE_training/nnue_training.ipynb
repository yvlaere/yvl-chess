{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5d7453",
   "metadata": {},
   "source": [
    "# NNUE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821225",
   "metadata": {},
   "source": [
    "Great source on NNUE: https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead68bd",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "Stockfish has a lot of data available for NNUE training in the .binpack format. They have a repo for training NNUEs (nnue-pytorch) that enables efficient dataloading with this format. I don't want to use nnue-pytorch, i want to make my own NNUE training setup.\n",
    "\n",
    "The nnue-pytorch repo also has information on training datasets for NNUEs: https://github.com/official-stockfish/nnue-pytorch/wiki/Training-datasets. They explain how to make your own dataset and link some of the datasets they generated. I will use some of this data, because generating the data myself would be too time-consuming on my hardware.\n",
    "\n",
    "Currently using training data: test80-2024-01-jan-2tb7p.min-v2.v6.binpack.zst from https://huggingface.co/datasets/linrock/test80-2024/tree/main\n",
    "\n",
    "This file contains billions of positions with evaluations in the .binpack format. The stockfish tools branch has a tool to covert the .binpack data into .plain data (https://github.com/official-stockfish/Stockfish/blob/tools/docs/convert.md). I used this tool and stored the first 200M evaluated positions.\n",
    "\n",
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1c61",
   "metadata": {},
   "source": [
    "### Turn FEN into input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict_w = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "piece_dict_b = {'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K':11, 'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "def FEN_to_inputs(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer_w = np.zeros(768, dtype = np.float32)\n",
    "    input_layer_b = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                alt_pos = 63 - (position ^ 7)\n",
    "                input_layer_w[position + piece_dict_w[char]*64] = 1\n",
    "                input_layer_b[alt_pos + piece_dict_b[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer_w, dtype=torch.float32), torch.tensor(input_layer_b, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1938bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  14,  15,  21,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 432, 433, 434, 435, 436, 437, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Black Features: tensor(32.)\n",
      "(array([  8,   9,  10,  11,  12,  13,  14,  15,  65,  70, 130, 133, 192,\n",
      "       199, 259, 324, 429, 432, 433, 434, 435, 436, 438, 439, 505, 510,\n",
      "       570, 573, 632, 639, 699, 764]),)\n",
      "Side to Move: tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28792/2390657366.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(w_features)))\n",
      "/tmp/ipykernel_28792/2390657366.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  print(np.nonzero(np.array(b_features)))\n"
     ]
    }
   ],
   "source": [
    "# testing encoding\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR b KQkq - 0 1'\n",
    "\n",
    "w_features, b_features, stm = FEN_to_inputs(fen1)\n",
    "print(\"White Features:\", sum(w_features))\n",
    "print(np.nonzero(np.array(w_features)))\n",
    "print(\"Black Features:\", sum(b_features))\n",
    "print(np.nonzero(np.array(b_features)))\n",
    "print(\"Side to Move:\", stm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c0263",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Input: a sparse, binary array of length 768. Each element of the array represents a possible combination of piece type (6), piece_color (2) and position (64) (6*2*64 = 768).\n",
    "\n",
    "This is a very simple input feature (P feature set) set that will be improved upon later (HalfKP).\n",
    "\n",
    "The fully connected feedfoward network has 4 hidden layers: 768 -> 1024, 1024 -> 8, 8 -> 32 and 32 -> 1.\n",
    "\n",
    "The output is a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Split_NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Split_NNUE, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)\n",
    "        cat_bw = torch.cat([b, w], dim=1)\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = (1 - stm) * cat_wb + stm * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0, max = 1)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f32318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Split_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_inputs(row[0].strip())\n",
    "                score, result = float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                buf.append((w_in, b_in, stm, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score, result in stream:\n",
    "                yield w_in, b_in, stm, score, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data_full_10M.csv'\n",
    "dataset = Custom_Split_Dataset(csv_path, shuffle_buffer = 100000)\n",
    "loader = DataLoader(dataset, batch_size = 1024, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa7cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Kaiming uniform for piecewise-linear (ReLU-like) activations:\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0.0, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | Avg Loss: 0.0382 | Grad Norm: 0.00119138\n",
      "Epoch 1 | Step 200 | Avg Loss: 0.0354 | Grad Norm: 0.00238500\n",
      "Epoch 1 | Step 300 | Avg Loss: 0.0308 | Grad Norm: 0.00291370\n",
      "Epoch 1 | Step 400 | Avg Loss: 0.0286 | Grad Norm: 0.00202588\n",
      "Epoch 1 | Step 500 | Avg Loss: 0.0272 | Grad Norm: 0.00341537\n",
      "Epoch 1 | Step 600 | Avg Loss: 0.0253 | Grad Norm: 0.00291860\n",
      "Epoch 1 | Step 700 | Avg Loss: 0.0250 | Grad Norm: 0.00486202\n",
      "Epoch 1 | Step 800 | Avg Loss: 0.0248 | Grad Norm: 0.00464848\n",
      "Epoch 1 | Step 900 | Avg Loss: 0.0246 | Grad Norm: 0.00361684\n",
      "Epoch 1 | Step 1000 | Avg Loss: 0.0242 | Grad Norm: 0.00343988\n",
      "Epoch 1 | Step 1100 | Avg Loss: 0.0234 | Grad Norm: 0.00377381\n",
      "Epoch 1 | Step 1200 | Avg Loss: 0.0229 | Grad Norm: 0.00366508\n",
      "Epoch 1 | Step 1300 | Avg Loss: 0.0232 | Grad Norm: 0.00390320\n",
      "Epoch 1 | Step 1400 | Avg Loss: 0.0231 | Grad Norm: 0.00467352\n",
      "Epoch 1 | Step 1500 | Avg Loss: 0.0229 | Grad Norm: 0.00476455\n",
      "Epoch 1 | Step 1600 | Avg Loss: 0.0223 | Grad Norm: 0.00501567\n",
      "Epoch 1 | Step 1700 | Avg Loss: 0.0228 | Grad Norm: 0.00340971\n",
      "Epoch 1 | Step 1800 | Avg Loss: 0.0225 | Grad Norm: 0.00711189\n",
      "Epoch 1 | Step 1900 | Avg Loss: 0.0227 | Grad Norm: 0.00356596\n",
      "Epoch 1 | Step 2000 | Avg Loss: 0.0228 | Grad Norm: 0.00504543\n",
      "Epoch 1 | Step 2100 | Avg Loss: 0.0223 | Grad Norm: 0.00416707\n",
      "Epoch 1 | Step 2200 | Avg Loss: 0.0227 | Grad Norm: 0.00521294\n",
      "Epoch 1 | Step 2300 | Avg Loss: 0.0232 | Grad Norm: 0.00383661\n",
      "Epoch 1 | Step 2400 | Avg Loss: 0.0228 | Grad Norm: 0.00682917\n",
      "Epoch 1 | Step 2500 | Avg Loss: 0.0233 | Grad Norm: 0.00362504\n",
      "Epoch 1 | Step 2600 | Avg Loss: 0.0235 | Grad Norm: 0.00382839\n",
      "Epoch 1 | Step 2700 | Avg Loss: 0.0234 | Grad Norm: 0.00418006\n",
      "Epoch 1 | Step 2800 | Avg Loss: 0.0230 | Grad Norm: 0.00405087\n",
      "Epoch 1 | Step 2900 | Avg Loss: 0.0234 | Grad Norm: 0.00365858\n",
      "Epoch 1 | Step 3000 | Avg Loss: 0.0235 | Grad Norm: 0.00478817\n",
      "Epoch 1 | Step 3100 | Avg Loss: 0.0231 | Grad Norm: 0.00305964\n",
      "Epoch 1 | Step 3200 | Avg Loss: 0.0229 | Grad Norm: 0.00335238\n",
      "Epoch 1 | Step 3300 | Avg Loss: 0.0226 | Grad Norm: 0.00561880\n",
      "Epoch 1 | Step 3400 | Avg Loss: 0.0225 | Grad Norm: 0.00421534\n",
      "Epoch 1 | Step 3500 | Avg Loss: 0.0219 | Grad Norm: 0.00432262\n",
      "Epoch 1 | Step 3600 | Avg Loss: 0.0221 | Grad Norm: 0.00447763\n",
      "Epoch 1 | Step 3700 | Avg Loss: 0.0226 | Grad Norm: 0.00295415\n",
      "Epoch 1 | Step 3800 | Avg Loss: 0.0227 | Grad Norm: 0.00367672\n",
      "Epoch 1 | Step 3900 | Avg Loss: 0.0228 | Grad Norm: 0.00535281\n",
      "Epoch 1 | Step 4000 | Avg Loss: 0.0226 | Grad Norm: 0.00352242\n",
      "Epoch 1 | Step 4100 | Avg Loss: 0.0225 | Grad Norm: 0.00407711\n",
      "Epoch 1 | Step 4200 | Avg Loss: 0.0226 | Grad Norm: 0.00356992\n",
      "Epoch 1 | Step 4300 | Avg Loss: 0.0229 | Grad Norm: 0.00423359\n",
      "Epoch 1 | Step 4400 | Avg Loss: 0.0232 | Grad Norm: 0.00395787\n",
      "Epoch 1 | Step 4500 | Avg Loss: 0.0228 | Grad Norm: 0.00513232\n",
      "Epoch 1 | Step 4600 | Avg Loss: 0.0225 | Grad Norm: 0.00669558\n",
      "Epoch 1 | Step 4700 | Avg Loss: 0.0223 | Grad Norm: 0.00401856\n",
      "Epoch 1 | Step 4800 | Avg Loss: 0.0222 | Grad Norm: 0.00295702\n",
      "Epoch 1 | Step 4900 | Avg Loss: 0.0222 | Grad Norm: 0.00359562\n",
      "Epoch 1 | Step 5000 | Avg Loss: 0.0226 | Grad Norm: 0.00350493\n",
      "Epoch 1 | Step 5100 | Avg Loss: 0.0225 | Grad Norm: 0.00402757\n",
      "Epoch 1 | Step 5200 | Avg Loss: 0.0222 | Grad Norm: 0.00319859\n",
      "Epoch 1 | Step 5300 | Avg Loss: 0.0228 | Grad Norm: 0.00373209\n",
      "Epoch 1 | Step 5400 | Avg Loss: 0.0238 | Grad Norm: 0.00321731\n",
      "Epoch 1 | Step 5500 | Avg Loss: 0.0237 | Grad Norm: 0.00409671\n",
      "Epoch 1 | Step 5600 | Avg Loss: 0.0232 | Grad Norm: 0.00645686\n",
      "Epoch 1 | Step 5700 | Avg Loss: 0.0230 | Grad Norm: 0.00533586\n",
      "Epoch 1 | Step 5800 | Avg Loss: 0.0235 | Grad Norm: 0.00388400\n",
      "Epoch 1 | Step 5900 | Avg Loss: 0.0230 | Grad Norm: 0.00350170\n",
      "Epoch 1 | Step 6000 | Avg Loss: 0.0229 | Grad Norm: 0.00570531\n",
      "Epoch 1 | Step 6100 | Avg Loss: 0.0224 | Grad Norm: 0.00305774\n",
      "Epoch 1 | Step 6200 | Avg Loss: 0.0224 | Grad Norm: 0.00398029\n",
      "Epoch 1 | Step 6300 | Avg Loss: 0.0234 | Grad Norm: 0.00297903\n",
      "Epoch 1 | Step 6400 | Avg Loss: 0.0231 | Grad Norm: 0.00303653\n",
      "Epoch 1 | Step 6500 | Avg Loss: 0.0232 | Grad Norm: 0.00370636\n",
      "Epoch 1 | Step 6600 | Avg Loss: 0.0231 | Grad Norm: 0.00770484\n",
      "Epoch 1 | Step 6700 | Avg Loss: 0.0228 | Grad Norm: 0.00374572\n",
      "Epoch 1 | Step 6800 | Avg Loss: 0.0229 | Grad Norm: 0.00385634\n",
      "Epoch 1 | Step 6900 | Avg Loss: 0.0226 | Grad Norm: 0.00360925\n",
      "Epoch 1 | Step 7000 | Avg Loss: 0.0227 | Grad Norm: 0.00405231\n",
      "Epoch 1 | Step 7100 | Avg Loss: 0.0226 | Grad Norm: 0.00462277\n",
      "Epoch 1 | Step 7200 | Avg Loss: 0.0220 | Grad Norm: 0.00357804\n",
      "Epoch 1 | Step 7300 | Avg Loss: 0.0220 | Grad Norm: 0.00359478\n",
      "Epoch 1 | Step 7400 | Avg Loss: 0.0224 | Grad Norm: 0.00305178\n",
      "Epoch 1 | Step 7500 | Avg Loss: 0.0221 | Grad Norm: 0.00373425\n",
      "Epoch 1 | Step 7600 | Avg Loss: 0.0227 | Grad Norm: 0.00313168\n",
      "Epoch 1 | Step 7700 | Avg Loss: 0.0223 | Grad Norm: 0.00519156\n",
      "Epoch 1 | Step 7800 | Avg Loss: 0.0223 | Grad Norm: 0.00358868\n",
      "Epoch 1 | Step 7900 | Avg Loss: 0.0222 | Grad Norm: 0.00465352\n",
      "Epoch 1 | Step 8000 | Avg Loss: 0.0229 | Grad Norm: 0.00313765\n",
      "Epoch 1 | Step 8100 | Avg Loss: 0.0231 | Grad Norm: 0.00289767\n",
      "Epoch 1 | Step 8200 | Avg Loss: 0.0230 | Grad Norm: 0.00282449\n",
      "Epoch 1 | Step 8300 | Avg Loss: 0.0231 | Grad Norm: 0.00354166\n",
      "Epoch 1 | Step 8400 | Avg Loss: 0.0230 | Grad Norm: 0.00321854\n",
      "Epoch 1 | Step 8500 | Avg Loss: 0.0231 | Grad Norm: 0.00310249\n",
      "Epoch 1 | Step 8600 | Avg Loss: 0.0236 | Grad Norm: 0.00593029\n",
      "Epoch 1 | Step 8700 | Avg Loss: 0.0240 | Grad Norm: 0.00366477\n",
      "Epoch 1 | Step 8800 | Avg Loss: 0.0236 | Grad Norm: 0.00312043\n",
      "Epoch 1 | Step 8900 | Avg Loss: 0.0233 | Grad Norm: 0.00292819\n",
      "Epoch 1 | Step 9000 | Avg Loss: 0.0234 | Grad Norm: 0.00290587\n",
      "Epoch 1 | Step 9100 | Avg Loss: 0.0233 | Grad Norm: 0.00344379\n",
      "Epoch 1 | Step 9200 | Avg Loss: 0.0229 | Grad Norm: 0.00331581\n",
      "Epoch 1 | Step 9300 | Avg Loss: 0.0226 | Grad Norm: 0.00385201\n",
      "Epoch 1 | Step 9400 | Avg Loss: 0.0222 | Grad Norm: 0.00350293\n",
      "Epoch 1 | Step 9500 | Avg Loss: 0.0227 | Grad Norm: 0.00289054\n",
      "Epoch 1 | Step 9600 | Avg Loss: 0.0228 | Grad Norm: 0.00393472\n",
      "Epoch 1 | Step 9700 | Avg Loss: 0.0230 | Grad Norm: 0.00328670\n",
      "Epoch 1 | Step 9800 | Avg Loss: 0.0230 | Grad Norm: 0.00423895\n",
      "Epoch 1 | Step 9900 | Avg Loss: 0.0228 | Grad Norm: 0.00351399\n",
      "Epoch 1 | Step 10000 | Avg Loss: 0.0228 | Grad Norm: 0.00362922\n",
      "Epoch 1 | Step 10100 | Avg Loss: 0.0231 | Grad Norm: 0.00465905\n",
      "Epoch 1 | Step 10200 | Avg Loss: 0.0231 | Grad Norm: 0.00486675\n",
      "Epoch 1 | Step 10300 | Avg Loss: 0.0229 | Grad Norm: 0.00364907\n",
      "Epoch 1 | Step 10400 | Avg Loss: 0.0233 | Grad Norm: 0.00521221\n",
      "Epoch 1 | Step 10500 | Avg Loss: 0.0235 | Grad Norm: 0.00417401\n",
      "Epoch 1 | Step 10600 | Avg Loss: 0.0237 | Grad Norm: 0.00528071\n",
      "Epoch 1 | Step 10700 | Avg Loss: 0.0237 | Grad Norm: 0.00322761\n",
      "Epoch 1 | Step 10800 | Avg Loss: 0.0240 | Grad Norm: 0.00575975\n",
      "Epoch 1 | Step 10900 | Avg Loss: 0.0234 | Grad Norm: 0.00728228\n",
      "Epoch 1 | Step 11000 | Avg Loss: 0.0234 | Grad Norm: 0.00413437\n",
      "Epoch 1 | Step 11100 | Avg Loss: 0.0237 | Grad Norm: 0.00393528\n",
      "Epoch 1 | Step 11200 | Avg Loss: 0.0235 | Grad Norm: 0.00311342\n",
      "Epoch 1 | Step 11300 | Avg Loss: 0.0236 | Grad Norm: 0.00286188\n",
      "Epoch 1 | Step 11400 | Avg Loss: 0.0231 | Grad Norm: 0.00313110\n",
      "Epoch 1 | Step 11500 | Avg Loss: 0.0238 | Grad Norm: 0.00339516\n",
      "Epoch 1 | Step 11600 | Avg Loss: 0.0236 | Grad Norm: 0.00334775\n",
      "Epoch 1 | Step 11700 | Avg Loss: 0.0232 | Grad Norm: 0.00339397\n",
      "Epoch 1 | Step 11800 | Avg Loss: 0.0230 | Grad Norm: 0.00306994\n",
      "Epoch 1 | Step 11900 | Avg Loss: 0.0223 | Grad Norm: 0.00275915\n",
      "Epoch 1 | Step 12000 | Avg Loss: 0.0219 | Grad Norm: 0.00298048\n",
      "Epoch 1 | Step 12100 | Avg Loss: 0.0219 | Grad Norm: 0.00254070\n",
      "Epoch 1 | Step 12200 | Avg Loss: 0.0220 | Grad Norm: 0.00345137\n",
      "Epoch 1 | Step 12300 | Avg Loss: 0.0217 | Grad Norm: 0.00278134\n",
      "Epoch 1 | Step 12400 | Avg Loss: 0.0224 | Grad Norm: 0.00313986\n",
      "Epoch 1 | Step 12500 | Avg Loss: 0.0223 | Grad Norm: 0.00311380\n",
      "Epoch 1 | Step 12600 | Avg Loss: 0.0222 | Grad Norm: 0.00361152\n",
      "Epoch 1 | Step 12700 | Avg Loss: 0.0219 | Grad Norm: 0.00318131\n",
      "Epoch 1 | Step 12800 | Avg Loss: 0.0217 | Grad Norm: 0.00314840\n",
      "Epoch 1 | Step 12900 | Avg Loss: 0.0219 | Grad Norm: 0.00292355\n",
      "Epoch 1 | Step 13000 | Avg Loss: 0.0219 | Grad Norm: 0.00322360\n",
      "Epoch 1 | Step 13100 | Avg Loss: 0.0216 | Grad Norm: 0.00271959\n",
      "Epoch 1 | Step 13200 | Avg Loss: 0.0211 | Grad Norm: 0.00293162\n",
      "Epoch 1 | Step 13300 | Avg Loss: 0.0215 | Grad Norm: 0.00313759\n",
      "Epoch 1 | Step 13400 | Avg Loss: 0.0214 | Grad Norm: 0.00317362\n",
      "Epoch 1 | Step 13500 | Avg Loss: 0.0214 | Grad Norm: 0.00354530\n",
      "Epoch 1 | Step 13600 | Avg Loss: 0.0212 | Grad Norm: 0.00316416\n",
      "Epoch 1 | Step 13700 | Avg Loss: 0.0206 | Grad Norm: 0.00308920\n",
      "Epoch 1 | Step 13800 | Avg Loss: 0.0212 | Grad Norm: 0.00298626\n",
      "Epoch 1 | Step 13900 | Avg Loss: 0.0222 | Grad Norm: 0.00324168\n",
      "Epoch 1 | Step 14000 | Avg Loss: 0.0218 | Grad Norm: 0.00311201\n",
      "Epoch 1 | Step 14100 | Avg Loss: 0.0212 | Grad Norm: 0.00333842\n",
      "Epoch 1 | Step 14200 | Avg Loss: 0.0211 | Grad Norm: 0.00286032\n",
      "Epoch 1 | Step 14300 | Avg Loss: 0.0214 | Grad Norm: 0.00289794\n",
      "Epoch 1 | Step 14400 | Avg Loss: 0.0213 | Grad Norm: 0.00321836\n",
      "Epoch 1 | Step 14500 | Avg Loss: 0.0216 | Grad Norm: 0.00301202\n",
      "Epoch 1 | Step 14600 | Avg Loss: 0.0216 | Grad Norm: 0.00288752\n",
      "Epoch 1 | Step 14700 | Avg Loss: 0.0218 | Grad Norm: 0.00342222\n",
      "Epoch 1 | Step 14800 | Avg Loss: 0.0215 | Grad Norm: 0.00342320\n",
      "Epoch 1 | Step 14900 | Avg Loss: 0.0216 | Grad Norm: 0.00309398\n",
      "Epoch 1 | Step 15000 | Avg Loss: 0.0215 | Grad Norm: 0.00315202\n",
      "Epoch 1 | Step 15100 | Avg Loss: 0.0213 | Grad Norm: 0.00321102\n",
      "Epoch 1 | Step 15200 | Avg Loss: 0.0214 | Grad Norm: 0.00341611\n",
      "Epoch 1 | Step 15300 | Avg Loss: 0.0215 | Grad Norm: 0.00418409\n",
      "Epoch 1 | Step 15400 | Avg Loss: 0.0212 | Grad Norm: 0.00397131\n",
      "Epoch 1 | Step 15500 | Avg Loss: 0.0217 | Grad Norm: 0.00315746\n",
      "Epoch 1 | Step 15600 | Avg Loss: 0.0220 | Grad Norm: 0.00365024\n",
      "Epoch 1 | Step 15700 | Avg Loss: 0.0220 | Grad Norm: 0.00313885\n",
      "Epoch 1 | Step 15800 | Avg Loss: 0.0226 | Grad Norm: 0.00328083\n",
      "Epoch 1 | Step 15900 | Avg Loss: 0.0226 | Grad Norm: 0.00344152\n",
      "Epoch 1 | Step 16000 | Avg Loss: 0.0223 | Grad Norm: 0.00407413\n",
      "Epoch 1 | Step 16100 | Avg Loss: 0.0222 | Grad Norm: 0.00330171\n",
      "Epoch 1 | Step 16200 | Avg Loss: 0.0224 | Grad Norm: 0.00364385\n",
      "Epoch 1 | Step 16300 | Avg Loss: 0.0222 | Grad Norm: 0.00397727\n",
      "Epoch 1 | Step 16400 | Avg Loss: 0.0218 | Grad Norm: 0.00339145\n",
      "Epoch 1 | Step 16500 | Avg Loss: 0.0220 | Grad Norm: 0.00266107\n",
      "Epoch 1 | Step 16600 | Avg Loss: 0.0212 | Grad Norm: 0.00386139\n",
      "Epoch 1 | Step 16700 | Avg Loss: 0.0211 | Grad Norm: 0.00332294\n",
      "Epoch 1 | Step 16800 | Avg Loss: 0.0215 | Grad Norm: 0.00319968\n",
      "Epoch 1 | Step 16900 | Avg Loss: 0.0215 | Grad Norm: 0.00330972\n",
      "Epoch 1 | Step 17000 | Avg Loss: 0.0215 | Grad Norm: 0.00343039\n",
      "Epoch 1 | Step 17100 | Avg Loss: 0.0211 | Grad Norm: 0.00352637\n",
      "Epoch 1 | Step 17200 | Avg Loss: 0.0212 | Grad Norm: 0.00284616\n",
      "Epoch 1 | Step 17300 | Avg Loss: 0.0210 | Grad Norm: 0.00295559\n",
      "Epoch 1 | Step 17400 | Avg Loss: 0.0210 | Grad Norm: 0.00339705\n",
      "Epoch 1 | Step 17500 | Avg Loss: 0.0210 | Grad Norm: 0.00336939\n",
      "Epoch 1 | Step 17600 | Avg Loss: 0.0207 | Grad Norm: 0.00296806\n",
      "Epoch 1 | Step 17700 | Avg Loss: 0.0208 | Grad Norm: 0.00312893\n",
      "Epoch 1 | Step 17800 | Avg Loss: 0.0206 | Grad Norm: 0.00315987\n",
      "Epoch 1 | Step 17900 | Avg Loss: 0.0209 | Grad Norm: 0.00278180\n",
      "Epoch 1 | Step 18000 | Avg Loss: 0.0210 | Grad Norm: 0.00295720\n",
      "Epoch 1 | Step 18100 | Avg Loss: 0.0208 | Grad Norm: 0.00360361\n",
      "Epoch 1 | Step 18200 | Avg Loss: 0.0206 | Grad Norm: 0.00303323\n",
      "Epoch 1 | Step 18300 | Avg Loss: 0.0213 | Grad Norm: 0.00313217\n",
      "Epoch 1 | Step 18400 | Avg Loss: 0.0210 | Grad Norm: 0.00319094\n",
      "Epoch 1 | Step 18500 | Avg Loss: 0.0209 | Grad Norm: 0.00312017\n",
      "Epoch 1 | Step 18600 | Avg Loss: 0.0213 | Grad Norm: 0.00268073\n",
      "Epoch 1 | Step 18700 | Avg Loss: 0.0213 | Grad Norm: 0.00336020\n",
      "Epoch 1 | Step 18800 | Avg Loss: 0.0212 | Grad Norm: 0.00301873\n",
      "Epoch 1 | Step 18900 | Avg Loss: 0.0215 | Grad Norm: 0.00367857\n",
      "Epoch 1 | Step 19000 | Avg Loss: 0.0216 | Grad Norm: 0.00384924\n",
      "Epoch 1 | Step 19100 | Avg Loss: 0.0217 | Grad Norm: 0.00298526\n",
      "Epoch 1 | Step 19200 | Avg Loss: 0.0219 | Grad Norm: 0.00300922\n",
      "Epoch 1 | Step 19300 | Avg Loss: 0.0213 | Grad Norm: 0.00299601\n",
      "Epoch 1 | Step 19400 | Avg Loss: 0.0210 | Grad Norm: 0.00285222\n",
      "Epoch 1 | Step 19500 | Avg Loss: 0.0207 | Grad Norm: 0.00258678\n",
      "Epoch 1 | Step 19600 | Avg Loss: 0.0206 | Grad Norm: 0.00307576\n",
      "Epoch 1 | Step 19700 | Avg Loss: 0.0204 | Grad Norm: 0.00314232\n",
      "Epoch 1 | Step 19800 | Avg Loss: 0.0203 | Grad Norm: 0.00269791\n",
      "Epoch 1 | Step 19900 | Avg Loss: 0.0203 | Grad Norm: 0.00292643\n",
      "Epoch 1 | Step 20000 | Avg Loss: 0.0205 | Grad Norm: 0.00327991\n",
      "Epoch 1 | Step 20100 | Avg Loss: 0.0205 | Grad Norm: 0.00286219\n",
      "Epoch 1 | Step 20200 | Avg Loss: 0.0199 | Grad Norm: 0.00291780\n",
      "Epoch 1 | Step 20300 | Avg Loss: 0.0205 | Grad Norm: 0.00324254\n",
      "Epoch 1 | Step 20400 | Avg Loss: 0.0207 | Grad Norm: 0.00283642\n",
      "Epoch 1 | Step 20500 | Avg Loss: 0.0208 | Grad Norm: 0.00327519\n",
      "Epoch 1 | Step 20600 | Avg Loss: 0.0214 | Grad Norm: 0.00299042\n",
      "Epoch 1 | Step 20700 | Avg Loss: 0.0208 | Grad Norm: 0.00263620\n",
      "Epoch 1 | Step 20800 | Avg Loss: 0.0207 | Grad Norm: 0.00269403\n",
      "Epoch 1 | Step 20900 | Avg Loss: 0.0205 | Grad Norm: 0.00274816\n",
      "Epoch 1 | Step 21000 | Avg Loss: 0.0205 | Grad Norm: 0.00369941\n",
      "Epoch 1 | Step 21100 | Avg Loss: 0.0204 | Grad Norm: 0.00326195\n",
      "Epoch 1 | Step 21200 | Avg Loss: 0.0206 | Grad Norm: 0.00294211\n",
      "Epoch 1 | Step 21300 | Avg Loss: 0.0206 | Grad Norm: 0.00293513\n",
      "Epoch 1 | Step 21400 | Avg Loss: 0.0209 | Grad Norm: 0.00276852\n",
      "Epoch 1 | Step 21500 | Avg Loss: 0.0212 | Grad Norm: 0.00266424\n",
      "Epoch 1 | Step 21600 | Avg Loss: 0.0215 | Grad Norm: 0.00327415\n",
      "Epoch 1 | Step 21700 | Avg Loss: 0.0212 | Grad Norm: 0.00346732\n",
      "Epoch 1 | Step 21800 | Avg Loss: 0.0219 | Grad Norm: 0.00282954\n",
      "Epoch 1 | Step 21900 | Avg Loss: 0.0213 | Grad Norm: 0.00308212\n",
      "Epoch 1 | Step 22000 | Avg Loss: 0.0212 | Grad Norm: 0.00330945\n",
      "Epoch 1 | Step 22100 | Avg Loss: 0.0210 | Grad Norm: 0.00329540\n",
      "Epoch 1 | Step 22200 | Avg Loss: 0.0210 | Grad Norm: 0.00344424\n",
      "Epoch 1 | Step 22300 | Avg Loss: 0.0207 | Grad Norm: 0.00315475\n",
      "Epoch 1 | Step 22400 | Avg Loss: 0.0208 | Grad Norm: 0.00300071\n",
      "Epoch 1 | Step 22500 | Avg Loss: 0.0209 | Grad Norm: 0.00295079\n",
      "Epoch 1 | Step 22600 | Avg Loss: 0.0208 | Grad Norm: 0.00358817\n",
      "Epoch 1 | Step 22700 | Avg Loss: 0.0207 | Grad Norm: 0.00329645\n",
      "Epoch 1 | Step 22800 | Avg Loss: 0.0208 | Grad Norm: 0.00314485\n",
      "Epoch 1 | Step 22900 | Avg Loss: 0.0211 | Grad Norm: 0.00393395\n",
      "Epoch 1 | Step 23000 | Avg Loss: 0.0205 | Grad Norm: 0.00302967\n",
      "Epoch 1 | Step 23100 | Avg Loss: 0.0203 | Grad Norm: 0.00294380\n",
      "Epoch 1 | Step 23200 | Avg Loss: 0.0198 | Grad Norm: 0.00314470\n",
      "Epoch 1 | Step 23300 | Avg Loss: 0.0201 | Grad Norm: 0.00321321\n",
      "Epoch 1 | Step 23400 | Avg Loss: 0.0203 | Grad Norm: 0.00298448\n",
      "Epoch 1 | Step 23500 | Avg Loss: 0.0207 | Grad Norm: 0.00316822\n",
      "Epoch 1 | Step 23600 | Avg Loss: 0.0213 | Grad Norm: 0.00283234\n",
      "Epoch 1 | Step 23700 | Avg Loss: 0.0209 | Grad Norm: 0.00319021\n",
      "Epoch 1 | Step 23800 | Avg Loss: 0.0211 | Grad Norm: 0.00311900\n",
      "Epoch 1 | Step 23900 | Avg Loss: 0.0211 | Grad Norm: 0.00332550\n",
      "Epoch 1 | Step 24000 | Avg Loss: 0.0208 | Grad Norm: 0.00295873\n",
      "Epoch 1 | Step 24100 | Avg Loss: 0.0206 | Grad Norm: 0.00293796\n",
      "Epoch 1 | Step 24200 | Avg Loss: 0.0210 | Grad Norm: 0.00342131\n",
      "Epoch 1 | Step 24300 | Avg Loss: 0.0205 | Grad Norm: 0.00269869\n",
      "Epoch 1 | Step 24400 | Avg Loss: 0.0202 | Grad Norm: 0.00400008\n",
      "Epoch 1 | Step 24500 | Avg Loss: 0.0205 | Grad Norm: 0.00312712\n",
      "Epoch 1 | Step 24600 | Avg Loss: 0.0205 | Grad Norm: 0.00367792\n",
      "Epoch 1 | Step 24700 | Avg Loss: 0.0213 | Grad Norm: 0.00276438\n",
      "Epoch 1 | Step 24800 | Avg Loss: 0.0209 | Grad Norm: 0.00350899\n",
      "Epoch 1 | Step 24900 | Avg Loss: 0.0205 | Grad Norm: 0.00326425\n",
      "Epoch 1 | Step 25000 | Avg Loss: 0.0208 | Grad Norm: 0.00350628\n",
      "Epoch 1 | Step 25100 | Avg Loss: 0.0211 | Grad Norm: 0.00367349\n",
      "Epoch 1 | Step 25200 | Avg Loss: 0.0212 | Grad Norm: 0.00322466\n",
      "Epoch 1 | Step 25300 | Avg Loss: 0.0215 | Grad Norm: 0.00334568\n",
      "Epoch 1 | Step 25400 | Avg Loss: 0.0217 | Grad Norm: 0.00313966\n",
      "Epoch 1 | Step 25500 | Avg Loss: 0.0216 | Grad Norm: 0.00309117\n",
      "Epoch 1 | Step 25600 | Avg Loss: 0.0217 | Grad Norm: 0.00357945\n",
      "Epoch 1 | Step 25700 | Avg Loss: 0.0214 | Grad Norm: 0.00327677\n",
      "Epoch 1 | Step 25800 | Avg Loss: 0.0214 | Grad Norm: 0.00322237\n",
      "Epoch 1 | Step 25900 | Avg Loss: 0.0216 | Grad Norm: 0.00314507\n",
      "Epoch 1 | Step 26000 | Avg Loss: 0.0212 | Grad Norm: 0.00320160\n",
      "Epoch 1 | Step 26100 | Avg Loss: 0.0206 | Grad Norm: 0.00322604\n",
      "Epoch 1 | Step 26200 | Avg Loss: 0.0204 | Grad Norm: 0.00323548\n",
      "Epoch 1 | Step 26300 | Avg Loss: 0.0207 | Grad Norm: 0.00339914\n",
      "Epoch 1 | Step 26400 | Avg Loss: 0.0204 | Grad Norm: 0.00335693\n",
      "Epoch 1 | Step 26500 | Avg Loss: 0.0203 | Grad Norm: 0.00302710\n",
      "Epoch 1 | Step 26600 | Avg Loss: 0.0204 | Grad Norm: 0.00307870\n",
      "Epoch 1 | Step 26700 | Avg Loss: 0.0207 | Grad Norm: 0.00309334\n",
      "Epoch 1 | Step 26800 | Avg Loss: 0.0209 | Grad Norm: 0.00341778\n",
      "Epoch 1 | Step 26900 | Avg Loss: 0.0209 | Grad Norm: 0.00345524\n",
      "Epoch 1 | Step 27000 | Avg Loss: 0.0211 | Grad Norm: 0.00343218\n",
      "Epoch 1 | Step 27100 | Avg Loss: 0.0211 | Grad Norm: 0.00296566\n",
      "Epoch 1 | Step 27200 | Avg Loss: 0.0204 | Grad Norm: 0.00331713\n",
      "Epoch 1 | Step 27300 | Avg Loss: 0.0202 | Grad Norm: 0.00307020\n",
      "Epoch 1 | Step 27400 | Avg Loss: 0.0208 | Grad Norm: 0.00310642\n",
      "Epoch 1 | Step 27500 | Avg Loss: 0.0209 | Grad Norm: 0.00309386\n",
      "Epoch 1 | Step 27600 | Avg Loss: 0.0208 | Grad Norm: 0.00334280\n",
      "Epoch 1 | Step 27700 | Avg Loss: 0.0207 | Grad Norm: 0.00345544\n",
      "Epoch 1 | Step 27800 | Avg Loss: 0.0209 | Grad Norm: 0.00313290\n",
      "Epoch 1 | Step 27900 | Avg Loss: 0.0208 | Grad Norm: 0.00267781\n",
      "Epoch 1 | Step 28000 | Avg Loss: 0.0208 | Grad Norm: 0.00333786\n",
      "Epoch 1 | Step 28100 | Avg Loss: 0.0212 | Grad Norm: 0.00292040\n",
      "Epoch 1 | Step 28200 | Avg Loss: 0.0210 | Grad Norm: 0.00304454\n",
      "Epoch 1 | Step 28300 | Avg Loss: 0.0205 | Grad Norm: 0.00375006\n",
      "Epoch 1 | Step 28400 | Avg Loss: 0.0205 | Grad Norm: 0.00340558\n",
      "Epoch 1 | Step 28500 | Avg Loss: 0.0216 | Grad Norm: 0.00410935\n",
      "Epoch 1 | Step 28600 | Avg Loss: 0.0212 | Grad Norm: 0.00360287\n",
      "Epoch 1 | Step 28700 | Avg Loss: 0.0210 | Grad Norm: 0.00289288\n",
      "Epoch 1 | Step 28800 | Avg Loss: 0.0206 | Grad Norm: 0.00368984\n",
      "Epoch 1 | Step 28900 | Avg Loss: 0.0209 | Grad Norm: 0.00330917\n",
      "Epoch 1 | Step 29000 | Avg Loss: 0.0203 | Grad Norm: 0.00308976\n",
      "Epoch 1 | Step 29100 | Avg Loss: 0.0205 | Grad Norm: 0.00291336\n",
      "Epoch 1 | Step 29200 | Avg Loss: 0.0208 | Grad Norm: 0.00345466\n",
      "Epoch 1 | Step 29300 | Avg Loss: 0.0206 | Grad Norm: 0.00327065\n",
      "Epoch 1 | Step 29400 | Avg Loss: 0.0208 | Grad Norm: 0.00279523\n",
      "Epoch 1 | Step 29500 | Avg Loss: 0.0206 | Grad Norm: 0.00295918\n",
      "Epoch 1 | Step 29600 | Avg Loss: 0.0206 | Grad Norm: 0.00309714\n",
      "Epoch 1 | Step 29700 | Avg Loss: 0.0210 | Grad Norm: 0.00346179\n",
      "Epoch 1 | Step 29800 | Avg Loss: 0.0211 | Grad Norm: 0.00278248\n",
      "Epoch 1 | Step 29900 | Avg Loss: 0.0211 | Grad Norm: 0.00287266\n",
      "Epoch 1 | Step 30000 | Avg Loss: 0.0212 | Grad Norm: 0.00312956\n",
      "Epoch 1 | Step 30100 | Avg Loss: 0.0211 | Grad Norm: 0.00269053\n",
      "Epoch 1 | Step 30200 | Avg Loss: 0.0215 | Grad Norm: 0.00326909\n",
      "Epoch 1 | Step 30300 | Avg Loss: 0.0217 | Grad Norm: 0.00358141\n",
      "Epoch 1 | Step 30400 | Avg Loss: 0.0218 | Grad Norm: 0.00309533\n",
      "Epoch 1 | Step 30500 | Avg Loss: 0.0210 | Grad Norm: 0.00273969\n",
      "Epoch 1 | Step 30600 | Avg Loss: 0.0205 | Grad Norm: 0.00295167\n",
      "Epoch 1 | Step 30700 | Avg Loss: 0.0205 | Grad Norm: 0.00369973\n",
      "Epoch 1 | Step 30800 | Avg Loss: 0.0207 | Grad Norm: 0.00317193\n",
      "Epoch 1 | Step 30900 | Avg Loss: 0.0205 | Grad Norm: 0.00294003\n",
      "Epoch 1 | Step 31000 | Avg Loss: 0.0205 | Grad Norm: 0.00298355\n",
      "Epoch 1 | Step 31100 | Avg Loss: 0.0206 | Grad Norm: 0.00275055\n",
      "Epoch 1 | Step 31200 | Avg Loss: 0.0203 | Grad Norm: 0.00305872\n",
      "Epoch 1 | Step 31300 | Avg Loss: 0.0202 | Grad Norm: 0.00276623\n",
      "Epoch 1 | Step 31400 | Avg Loss: 0.0208 | Grad Norm: 0.00270825\n",
      "Epoch 1 | Step 31500 | Avg Loss: 0.0204 | Grad Norm: 0.00320009\n",
      "Epoch 1 | Step 31600 | Avg Loss: 0.0203 | Grad Norm: 0.00356726\n",
      "Epoch 1 | Step 31700 | Avg Loss: 0.0199 | Grad Norm: 0.00327388\n",
      "Epoch 1 | Step 31800 | Avg Loss: 0.0201 | Grad Norm: 0.00385381\n",
      "Epoch 1 | Step 31900 | Avg Loss: 0.0207 | Grad Norm: 0.00385046\n",
      "Epoch 1 | Step 32000 | Avg Loss: 0.0208 | Grad Norm: 0.00289321\n",
      "Epoch 1 | Step 32100 | Avg Loss: 0.0208 | Grad Norm: 0.00337047\n",
      "Epoch 1 | Step 32200 | Avg Loss: 0.0207 | Grad Norm: 0.00337876\n",
      "Epoch 1 | Step 32300 | Avg Loss: 0.0211 | Grad Norm: 0.00371114\n",
      "Epoch 1 | Step 32400 | Avg Loss: 0.0212 | Grad Norm: 0.00293223\n",
      "Epoch 1 | Step 32500 | Avg Loss: 0.0211 | Grad Norm: 0.00306188\n",
      "Epoch 1 | Step 32600 | Avg Loss: 0.0212 | Grad Norm: 0.00321130\n",
      "Epoch 1 | Step 32700 | Avg Loss: 0.0208 | Grad Norm: 0.00271446\n",
      "Epoch 1 | Step 32800 | Avg Loss: 0.0211 | Grad Norm: 0.00273832\n",
      "Epoch 1 | Step 32900 | Avg Loss: 0.0209 | Grad Norm: 0.00327758\n",
      "Epoch 1 | Step 33000 | Avg Loss: 0.0208 | Grad Norm: 0.00301177\n",
      "Epoch 1 | Step 33100 | Avg Loss: 0.0207 | Grad Norm: 0.00329678\n",
      "Epoch 1 | Step 33200 | Avg Loss: 0.0201 | Grad Norm: 0.00379618\n",
      "Epoch 1 | Step 33300 | Avg Loss: 0.0203 | Grad Norm: 0.00347482\n",
      "Epoch 1 | Step 33400 | Avg Loss: 0.0206 | Grad Norm: 0.00259150\n",
      "Epoch 1 | Step 33500 | Avg Loss: 0.0206 | Grad Norm: 0.00358916\n",
      "Epoch 1 | Step 33600 | Avg Loss: 0.0205 | Grad Norm: 0.00312619\n",
      "Epoch 1 | Step 33700 | Avg Loss: 0.0205 | Grad Norm: 0.00288608\n",
      "Epoch 1 | Step 33800 | Avg Loss: 0.0205 | Grad Norm: 0.00302797\n",
      "Epoch 1 | Step 33900 | Avg Loss: 0.0202 | Grad Norm: 0.00303792\n",
      "Epoch 1 | Step 34000 | Avg Loss: 0.0195 | Grad Norm: 0.00273170\n",
      "Epoch 1 | Step 34100 | Avg Loss: 0.0191 | Grad Norm: 0.00270723\n",
      "Epoch 1 | Step 34200 | Avg Loss: 0.0189 | Grad Norm: 0.00253628\n",
      "Epoch 1 | Step 34300 | Avg Loss: 0.0194 | Grad Norm: 0.00251256\n",
      "Epoch 1 | Step 34400 | Avg Loss: 0.0201 | Grad Norm: 0.00306873\n",
      "Epoch 1 | Step 34500 | Avg Loss: 0.0202 | Grad Norm: 0.00291351\n",
      "Epoch 1 | Step 34600 | Avg Loss: 0.0209 | Grad Norm: 0.00268999\n",
      "Epoch 1 | Step 34700 | Avg Loss: 0.0203 | Grad Norm: 0.00282022\n",
      "Epoch 1 | Step 34800 | Avg Loss: 0.0202 | Grad Norm: 0.00288210\n",
      "Epoch 1 | Step 34900 | Avg Loss: 0.0203 | Grad Norm: 0.00262720\n",
      "Epoch 1 | Step 35000 | Avg Loss: 0.0201 | Grad Norm: 0.00286254\n",
      "Epoch 1 | Step 35100 | Avg Loss: 0.0203 | Grad Norm: 0.00281143\n",
      "Epoch 1 | Step 35200 | Avg Loss: 0.0208 | Grad Norm: 0.00316725\n",
      "Epoch 1 | Step 35300 | Avg Loss: 0.0207 | Grad Norm: 0.00264482\n",
      "Epoch 1 | Step 35400 | Avg Loss: 0.0208 | Grad Norm: 0.00296837\n",
      "Epoch 1 | Step 35500 | Avg Loss: 0.0208 | Grad Norm: 0.00311106\n",
      "Epoch 1 | Step 35600 | Avg Loss: 0.0212 | Grad Norm: 0.00291547\n",
      "Epoch 1 | Step 35700 | Avg Loss: 0.0206 | Grad Norm: 0.00319127\n",
      "Epoch 1 | Step 35800 | Avg Loss: 0.0202 | Grad Norm: 0.00292279\n",
      "Epoch 1 | Step 35900 | Avg Loss: 0.0203 | Grad Norm: 0.00285975\n",
      "Epoch 1 | Step 36000 | Avg Loss: 0.0198 | Grad Norm: 0.00345348\n",
      "Epoch 1 | Step 36100 | Avg Loss: 0.0198 | Grad Norm: 0.00312965\n",
      "Epoch 1 | Step 36200 | Avg Loss: 0.0204 | Grad Norm: 0.00350734\n",
      "Epoch 1 | Step 36300 | Avg Loss: 0.0208 | Grad Norm: 0.00310710\n",
      "Epoch 1 | Step 36400 | Avg Loss: 0.0213 | Grad Norm: 0.00322453\n",
      "Epoch 1 | Step 36500 | Avg Loss: 0.0208 | Grad Norm: 0.00288496\n",
      "Epoch 1 | Step 36600 | Avg Loss: 0.0211 | Grad Norm: 0.00337474\n",
      "Epoch 1 | Step 36700 | Avg Loss: 0.0209 | Grad Norm: 0.00274550\n",
      "Epoch 1 | Step 36800 | Avg Loss: 0.0209 | Grad Norm: 0.00308354\n",
      "Epoch 1 | Step 36900 | Avg Loss: 0.0210 | Grad Norm: 0.00308838\n",
      "Epoch 1 | Step 37000 | Avg Loss: 0.0214 | Grad Norm: 0.00289211\n",
      "Epoch 1 | Step 37100 | Avg Loss: 0.0214 | Grad Norm: 0.00282582\n",
      "Epoch 1 | Step 37200 | Avg Loss: 0.0208 | Grad Norm: 0.00260806\n",
      "Epoch 1 | Step 37300 | Avg Loss: 0.0203 | Grad Norm: 0.00322821\n",
      "Epoch 1 | Step 37400 | Avg Loss: 0.0206 | Grad Norm: 0.00253522\n",
      "Epoch 1 | Step 37500 | Avg Loss: 0.0206 | Grad Norm: 0.00265674\n",
      "Epoch 1 | Step 37600 | Avg Loss: 0.0204 | Grad Norm: 0.00311570\n",
      "Epoch 1 | Step 37700 | Avg Loss: 0.0204 | Grad Norm: 0.00266900\n",
      "Epoch 1 | Step 37800 | Avg Loss: 0.0204 | Grad Norm: 0.00335396\n",
      "Epoch 1 | Step 37900 | Avg Loss: 0.0205 | Grad Norm: 0.00312843\n",
      "Epoch 1 | Step 38000 | Avg Loss: 0.0209 | Grad Norm: 0.00303692\n",
      "Epoch 1 | Step 38100 | Avg Loss: 0.0209 | Grad Norm: 0.00357484\n",
      "Epoch 1 | Step 38200 | Avg Loss: 0.0212 | Grad Norm: 0.00270169\n",
      "Epoch 1 | Step 38300 | Avg Loss: 0.0209 | Grad Norm: 0.00345291\n",
      "Epoch 1 | Step 38400 | Avg Loss: 0.0213 | Grad Norm: 0.00281891\n",
      "Epoch 1 | Step 38500 | Avg Loss: 0.0209 | Grad Norm: 0.00332003\n",
      "Epoch 1 | Step 38600 | Avg Loss: 0.0209 | Grad Norm: 0.00300526\n",
      "Epoch 1 | Step 38700 | Avg Loss: 0.0210 | Grad Norm: 0.00300508\n",
      "Epoch 1 | Step 38800 | Avg Loss: 0.0209 | Grad Norm: 0.00265704\n",
      "Epoch 1 | Step 38900 | Avg Loss: 0.0209 | Grad Norm: 0.00300857\n",
      "Epoch 1 | Step 39000 | Avg Loss: 0.0208 | Grad Norm: 0.00270691\n",
      "Epoch 1, Loss: 0.0208\n",
      "Epoch 2 | Step 39100 | Avg Loss: 0.0205 | Grad Norm: 0.00289597\n",
      "Epoch 2 | Step 39200 | Avg Loss: 0.0196 | Grad Norm: 0.00242386\n",
      "Epoch 2 | Step 39300 | Avg Loss: 0.0197 | Grad Norm: 0.00330953\n",
      "Epoch 2 | Step 39400 | Avg Loss: 0.0195 | Grad Norm: 0.00274942\n",
      "Epoch 2 | Step 39500 | Avg Loss: 0.0196 | Grad Norm: 0.00303084\n",
      "Epoch 2 | Step 39600 | Avg Loss: 0.0198 | Grad Norm: 0.00317417\n",
      "Epoch 2 | Step 39700 | Avg Loss: 0.0201 | Grad Norm: 0.00248671\n",
      "Epoch 2 | Step 39800 | Avg Loss: 0.0209 | Grad Norm: 0.00352889\n",
      "Epoch 2 | Step 39900 | Avg Loss: 0.0213 | Grad Norm: 0.00340566\n",
      "Epoch 2 | Step 40000 | Avg Loss: 0.0212 | Grad Norm: 0.00338927\n",
      "Epoch 2 | Step 40100 | Avg Loss: 0.0210 | Grad Norm: 0.00308502\n",
      "Epoch 2 | Step 40200 | Avg Loss: 0.0204 | Grad Norm: 0.00334626\n",
      "Epoch 2 | Step 40300 | Avg Loss: 0.0203 | Grad Norm: 0.00319557\n",
      "Epoch 2 | Step 40400 | Avg Loss: 0.0203 | Grad Norm: 0.00319814\n",
      "Epoch 2 | Step 40500 | Avg Loss: 0.0204 | Grad Norm: 0.00293243\n",
      "Epoch 2 | Step 40600 | Avg Loss: 0.0202 | Grad Norm: 0.00269895\n",
      "Epoch 2 | Step 40700 | Avg Loss: 0.0201 | Grad Norm: 0.00315429\n",
      "Epoch 2 | Step 40800 | Avg Loss: 0.0200 | Grad Norm: 0.00253434\n",
      "Epoch 2 | Step 40900 | Avg Loss: 0.0201 | Grad Norm: 0.00325039\n",
      "Epoch 2 | Step 41000 | Avg Loss: 0.0203 | Grad Norm: 0.00291536\n",
      "Epoch 2 | Step 41100 | Avg Loss: 0.0202 | Grad Norm: 0.00293592\n",
      "Epoch 2 | Step 41200 | Avg Loss: 0.0200 | Grad Norm: 0.00283968\n",
      "Epoch 2 | Step 41300 | Avg Loss: 0.0203 | Grad Norm: 0.00352492\n",
      "Epoch 2 | Step 41400 | Avg Loss: 0.0203 | Grad Norm: 0.00283897\n",
      "Epoch 2 | Step 41500 | Avg Loss: 0.0202 | Grad Norm: 0.00271937\n",
      "Epoch 2 | Step 41600 | Avg Loss: 0.0204 | Grad Norm: 0.00273617\n",
      "Epoch 2 | Step 41700 | Avg Loss: 0.0208 | Grad Norm: 0.00319762\n",
      "Epoch 2 | Step 41800 | Avg Loss: 0.0206 | Grad Norm: 0.00338580\n",
      "Epoch 2 | Step 41900 | Avg Loss: 0.0204 | Grad Norm: 0.00310119\n",
      "Epoch 2 | Step 42000 | Avg Loss: 0.0206 | Grad Norm: 0.00318206\n",
      "Epoch 2 | Step 42100 | Avg Loss: 0.0208 | Grad Norm: 0.00261953\n",
      "Epoch 2 | Step 42200 | Avg Loss: 0.0204 | Grad Norm: 0.00257452\n",
      "Epoch 2 | Step 42300 | Avg Loss: 0.0203 | Grad Norm: 0.00256002\n",
      "Epoch 2 | Step 42400 | Avg Loss: 0.0201 | Grad Norm: 0.00262550\n",
      "Epoch 2 | Step 42500 | Avg Loss: 0.0196 | Grad Norm: 0.00289924\n",
      "Epoch 2 | Step 42600 | Avg Loss: 0.0194 | Grad Norm: 0.00277301\n",
      "Epoch 2 | Step 42700 | Avg Loss: 0.0197 | Grad Norm: 0.00319386\n",
      "Epoch 2 | Step 42800 | Avg Loss: 0.0200 | Grad Norm: 0.00262870\n",
      "Epoch 2 | Step 42900 | Avg Loss: 0.0200 | Grad Norm: 0.00287223\n",
      "Epoch 2 | Step 43000 | Avg Loss: 0.0201 | Grad Norm: 0.00277200\n",
      "Epoch 2 | Step 43100 | Avg Loss: 0.0202 | Grad Norm: 0.00297491\n",
      "Epoch 2 | Step 43200 | Avg Loss: 0.0200 | Grad Norm: 0.00278012\n",
      "Epoch 2 | Step 43300 | Avg Loss: 0.0201 | Grad Norm: 0.00292417\n",
      "Epoch 2 | Step 43400 | Avg Loss: 0.0202 | Grad Norm: 0.00262558\n",
      "Epoch 2 | Step 43500 | Avg Loss: 0.0202 | Grad Norm: 0.00306179\n",
      "Epoch 2 | Step 43600 | Avg Loss: 0.0199 | Grad Norm: 0.00329342\n",
      "Epoch 2 | Step 43700 | Avg Loss: 0.0195 | Grad Norm: 0.00295595\n",
      "Epoch 2 | Step 43800 | Avg Loss: 0.0194 | Grad Norm: 0.00329006\n",
      "Epoch 2 | Step 43900 | Avg Loss: 0.0197 | Grad Norm: 0.00319401\n",
      "Epoch 2 | Step 44000 | Avg Loss: 0.0199 | Grad Norm: 0.00256508\n",
      "Epoch 2 | Step 44100 | Avg Loss: 0.0200 | Grad Norm: 0.00291753\n",
      "Epoch 2 | Step 44200 | Avg Loss: 0.0198 | Grad Norm: 0.00260315\n",
      "Epoch 2 | Step 44300 | Avg Loss: 0.0197 | Grad Norm: 0.00277047\n",
      "Epoch 2 | Step 44400 | Avg Loss: 0.0202 | Grad Norm: 0.00318198\n",
      "Epoch 2 | Step 44500 | Avg Loss: 0.0207 | Grad Norm: 0.00252433\n",
      "Epoch 2 | Step 44600 | Avg Loss: 0.0206 | Grad Norm: 0.00279143\n",
      "Epoch 2 | Step 44700 | Avg Loss: 0.0205 | Grad Norm: 0.00338113\n",
      "Epoch 2 | Step 44800 | Avg Loss: 0.0205 | Grad Norm: 0.00308671\n",
      "Epoch 2 | Step 44900 | Avg Loss: 0.0206 | Grad Norm: 0.00284981\n",
      "Epoch 2 | Step 45000 | Avg Loss: 0.0204 | Grad Norm: 0.00314955\n",
      "Epoch 2 | Step 45100 | Avg Loss: 0.0200 | Grad Norm: 0.00284678\n",
      "Epoch 2 | Step 45200 | Avg Loss: 0.0198 | Grad Norm: 0.00279721\n",
      "Epoch 2 | Step 45300 | Avg Loss: 0.0201 | Grad Norm: 0.00299834\n",
      "Epoch 2 | Step 45400 | Avg Loss: 0.0205 | Grad Norm: 0.00367211\n",
      "Epoch 2 | Step 45500 | Avg Loss: 0.0198 | Grad Norm: 0.00290494\n",
      "Epoch 2 | Step 45600 | Avg Loss: 0.0201 | Grad Norm: 0.00292991\n",
      "Epoch 2 | Step 45700 | Avg Loss: 0.0200 | Grad Norm: 0.00344937\n",
      "Epoch 2 | Step 45800 | Avg Loss: 0.0198 | Grad Norm: 0.00332743\n",
      "Epoch 2 | Step 45900 | Avg Loss: 0.0200 | Grad Norm: 0.00317124\n",
      "Epoch 2 | Step 46000 | Avg Loss: 0.0197 | Grad Norm: 0.00279213\n",
      "Epoch 2 | Step 46100 | Avg Loss: 0.0198 | Grad Norm: 0.00295577\n",
      "Epoch 2 | Step 46200 | Avg Loss: 0.0195 | Grad Norm: 0.00278771\n",
      "Epoch 2 | Step 46300 | Avg Loss: 0.0192 | Grad Norm: 0.00297177\n",
      "Epoch 2 | Step 46400 | Avg Loss: 0.0194 | Grad Norm: 0.00281901\n",
      "Epoch 2 | Step 46500 | Avg Loss: 0.0194 | Grad Norm: 0.00268803\n",
      "Epoch 2 | Step 46600 | Avg Loss: 0.0196 | Grad Norm: 0.00255545\n",
      "Epoch 2 | Step 46700 | Avg Loss: 0.0199 | Grad Norm: 0.00283015\n",
      "Epoch 2 | Step 46800 | Avg Loss: 0.0194 | Grad Norm: 0.00313882\n",
      "Epoch 2 | Step 46900 | Avg Loss: 0.0194 | Grad Norm: 0.00230967\n",
      "Epoch 2 | Step 47000 | Avg Loss: 0.0198 | Grad Norm: 0.00372531\n",
      "Epoch 2 | Step 47100 | Avg Loss: 0.0203 | Grad Norm: 0.00306763\n",
      "Epoch 2 | Step 47200 | Avg Loss: 0.0200 | Grad Norm: 0.00324437\n",
      "Epoch 2 | Step 47300 | Avg Loss: 0.0203 | Grad Norm: 0.00318613\n",
      "Epoch 2 | Step 47400 | Avg Loss: 0.0203 | Grad Norm: 0.00318214\n",
      "Epoch 2 | Step 47500 | Avg Loss: 0.0202 | Grad Norm: 0.00314826\n",
      "Epoch 2 | Step 47600 | Avg Loss: 0.0203 | Grad Norm: 0.00362698\n",
      "Epoch 2 | Step 47700 | Avg Loss: 0.0207 | Grad Norm: 0.00263413\n",
      "Epoch 2 | Step 47800 | Avg Loss: 0.0206 | Grad Norm: 0.00324749\n",
      "Epoch 2 | Step 47900 | Avg Loss: 0.0205 | Grad Norm: 0.00294230\n",
      "Epoch 2 | Step 48000 | Avg Loss: 0.0203 | Grad Norm: 0.00277061\n",
      "Epoch 2 | Step 48100 | Avg Loss: 0.0204 | Grad Norm: 0.00337311\n",
      "Epoch 2 | Step 48200 | Avg Loss: 0.0204 | Grad Norm: 0.00296259\n",
      "Epoch 2 | Step 48300 | Avg Loss: 0.0198 | Grad Norm: 0.00278411\n",
      "Epoch 2 | Step 48400 | Avg Loss: 0.0195 | Grad Norm: 0.00277504\n",
      "Epoch 2 | Step 48500 | Avg Loss: 0.0195 | Grad Norm: 0.00261017\n",
      "Epoch 2 | Step 48600 | Avg Loss: 0.0195 | Grad Norm: 0.00340049\n",
      "Epoch 2 | Step 48700 | Avg Loss: 0.0197 | Grad Norm: 0.00275759\n",
      "Epoch 2 | Step 48800 | Avg Loss: 0.0199 | Grad Norm: 0.00295156\n",
      "Epoch 2 | Step 48900 | Avg Loss: 0.0199 | Grad Norm: 0.00261768\n",
      "Epoch 2 | Step 49000 | Avg Loss: 0.0200 | Grad Norm: 0.00303288\n",
      "Epoch 2 | Step 49100 | Avg Loss: 0.0200 | Grad Norm: 0.00298498\n",
      "Epoch 2 | Step 49200 | Avg Loss: 0.0206 | Grad Norm: 0.00271959\n",
      "Epoch 2 | Step 49300 | Avg Loss: 0.0202 | Grad Norm: 0.00275200\n",
      "Epoch 2 | Step 49400 | Avg Loss: 0.0201 | Grad Norm: 0.00306838\n",
      "Epoch 2 | Step 49500 | Avg Loss: 0.0202 | Grad Norm: 0.00283701\n",
      "Epoch 2 | Step 49600 | Avg Loss: 0.0203 | Grad Norm: 0.00304866\n",
      "Epoch 2 | Step 49700 | Avg Loss: 0.0204 | Grad Norm: 0.00302007\n",
      "Epoch 2 | Step 49800 | Avg Loss: 0.0204 | Grad Norm: 0.00339922\n",
      "Epoch 2 | Step 49900 | Avg Loss: 0.0203 | Grad Norm: 0.00326185\n",
      "Epoch 2 | Step 50000 | Avg Loss: 0.0198 | Grad Norm: 0.00302442\n",
      "Epoch 2 | Step 50100 | Avg Loss: 0.0205 | Grad Norm: 0.00273849\n",
      "Epoch 2 | Step 50200 | Avg Loss: 0.0208 | Grad Norm: 0.00353550\n",
      "Epoch 2 | Step 50300 | Avg Loss: 0.0206 | Grad Norm: 0.00287276\n",
      "Epoch 2 | Step 50400 | Avg Loss: 0.0202 | Grad Norm: 0.00345577\n",
      "Epoch 2 | Step 50500 | Avg Loss: 0.0204 | Grad Norm: 0.00364559\n",
      "Epoch 2 | Step 50600 | Avg Loss: 0.0205 | Grad Norm: 0.00307129\n",
      "Epoch 2 | Step 50700 | Avg Loss: 0.0202 | Grad Norm: 0.00277311\n",
      "Epoch 2 | Step 50800 | Avg Loss: 0.0201 | Grad Norm: 0.00358752\n",
      "Epoch 2 | Step 50900 | Avg Loss: 0.0200 | Grad Norm: 0.00310729\n",
      "Epoch 2 | Step 51000 | Avg Loss: 0.0194 | Grad Norm: 0.00286192\n",
      "Epoch 2 | Step 51100 | Avg Loss: 0.0191 | Grad Norm: 0.00307201\n",
      "Epoch 2 | Step 51200 | Avg Loss: 0.0194 | Grad Norm: 0.00331857\n",
      "Epoch 2 | Step 51300 | Avg Loss: 0.0194 | Grad Norm: 0.00264485\n",
      "Epoch 2 | Step 51400 | Avg Loss: 0.0198 | Grad Norm: 0.00383011\n",
      "Epoch 2 | Step 51500 | Avg Loss: 0.0202 | Grad Norm: 0.00356671\n",
      "Epoch 2 | Step 51600 | Avg Loss: 0.0203 | Grad Norm: 0.00332288\n",
      "Epoch 2 | Step 51700 | Avg Loss: 0.0199 | Grad Norm: 0.00332425\n",
      "Epoch 2 | Step 51800 | Avg Loss: 0.0197 | Grad Norm: 0.00312054\n",
      "Epoch 2 | Step 51900 | Avg Loss: 0.0199 | Grad Norm: 0.00322864\n",
      "Epoch 2 | Step 52000 | Avg Loss: 0.0202 | Grad Norm: 0.00391118\n",
      "Epoch 2 | Step 52100 | Avg Loss: 0.0198 | Grad Norm: 0.00290599\n",
      "Epoch 2 | Step 52200 | Avg Loss: 0.0197 | Grad Norm: 0.00328253\n",
      "Epoch 2 | Step 52300 | Avg Loss: 0.0195 | Grad Norm: 0.00306825\n",
      "Epoch 2 | Step 52400 | Avg Loss: 0.0194 | Grad Norm: 0.00284188\n",
      "Epoch 2 | Step 52500 | Avg Loss: 0.0197 | Grad Norm: 0.00310082\n",
      "Epoch 2 | Step 52600 | Avg Loss: 0.0196 | Grad Norm: 0.00384875\n",
      "Epoch 2 | Step 52700 | Avg Loss: 0.0192 | Grad Norm: 0.00291172\n",
      "Epoch 2 | Step 52800 | Avg Loss: 0.0192 | Grad Norm: 0.00266225\n",
      "Epoch 2 | Step 52900 | Avg Loss: 0.0197 | Grad Norm: 0.00287686\n",
      "Epoch 2 | Step 53000 | Avg Loss: 0.0200 | Grad Norm: 0.00329858\n",
      "Epoch 2 | Step 53100 | Avg Loss: 0.0195 | Grad Norm: 0.00303480\n",
      "Epoch 2 | Step 53200 | Avg Loss: 0.0195 | Grad Norm: 0.00253435\n",
      "Epoch 2 | Step 53300 | Avg Loss: 0.0198 | Grad Norm: 0.00291309\n",
      "Epoch 2 | Step 53400 | Avg Loss: 0.0196 | Grad Norm: 0.00362148\n",
      "Epoch 2 | Step 53500 | Avg Loss: 0.0199 | Grad Norm: 0.00407144\n",
      "Epoch 2 | Step 53600 | Avg Loss: 0.0197 | Grad Norm: 0.00375500\n",
      "Epoch 2 | Step 53700 | Avg Loss: 0.0200 | Grad Norm: 0.00352691\n",
      "Epoch 2 | Step 53800 | Avg Loss: 0.0200 | Grad Norm: 0.00282238\n",
      "Epoch 2 | Step 53900 | Avg Loss: 0.0200 | Grad Norm: 0.00301601\n",
      "Epoch 2 | Step 54000 | Avg Loss: 0.0199 | Grad Norm: 0.00361848\n",
      "Epoch 2 | Step 54100 | Avg Loss: 0.0198 | Grad Norm: 0.00389238\n",
      "Epoch 2 | Step 54200 | Avg Loss: 0.0198 | Grad Norm: 0.00270947\n",
      "Epoch 2 | Step 54300 | Avg Loss: 0.0198 | Grad Norm: 0.00303388\n",
      "Epoch 2 | Step 54400 | Avg Loss: 0.0198 | Grad Norm: 0.00288957\n",
      "Epoch 2 | Step 54500 | Avg Loss: 0.0197 | Grad Norm: 0.00327611\n",
      "Epoch 2 | Step 54600 | Avg Loss: 0.0201 | Grad Norm: 0.00414259\n",
      "Epoch 2 | Step 54700 | Avg Loss: 0.0203 | Grad Norm: 0.00297921\n",
      "Epoch 2 | Step 54800 | Avg Loss: 0.0207 | Grad Norm: 0.00309751\n",
      "Epoch 2 | Step 54900 | Avg Loss: 0.0210 | Grad Norm: 0.00417944\n",
      "Epoch 2 | Step 55000 | Avg Loss: 0.0211 | Grad Norm: 0.00265027\n",
      "Epoch 2 | Step 55100 | Avg Loss: 0.0207 | Grad Norm: 0.00404799\n",
      "Epoch 2 | Step 55200 | Avg Loss: 0.0211 | Grad Norm: 0.00328682\n",
      "Epoch 2 | Step 55300 | Avg Loss: 0.0210 | Grad Norm: 0.00346973\n",
      "Epoch 2 | Step 55400 | Avg Loss: 0.0205 | Grad Norm: 0.00267442\n",
      "Epoch 2 | Step 55500 | Avg Loss: 0.0205 | Grad Norm: 0.00323133\n",
      "Epoch 2 | Step 55600 | Avg Loss: 0.0201 | Grad Norm: 0.00424946\n",
      "Epoch 2 | Step 55700 | Avg Loss: 0.0195 | Grad Norm: 0.00319487\n",
      "Epoch 2 | Step 55800 | Avg Loss: 0.0198 | Grad Norm: 0.00395865\n",
      "Epoch 2 | Step 55900 | Avg Loss: 0.0202 | Grad Norm: 0.00307307\n",
      "Epoch 2 | Step 56000 | Avg Loss: 0.0200 | Grad Norm: 0.00270510\n",
      "Epoch 2 | Step 56100 | Avg Loss: 0.0198 | Grad Norm: 0.00270118\n",
      "Epoch 2 | Step 56200 | Avg Loss: 0.0199 | Grad Norm: 0.00295258\n",
      "Epoch 2 | Step 56300 | Avg Loss: 0.0197 | Grad Norm: 0.00351864\n",
      "Epoch 2 | Step 56400 | Avg Loss: 0.0197 | Grad Norm: 0.00273715\n",
      "Epoch 2 | Step 56500 | Avg Loss: 0.0198 | Grad Norm: 0.00341668\n",
      "Epoch 2 | Step 56600 | Avg Loss: 0.0195 | Grad Norm: 0.00296195\n",
      "Epoch 2 | Step 56700 | Avg Loss: 0.0190 | Grad Norm: 0.00327402\n",
      "Epoch 2 | Step 56800 | Avg Loss: 0.0193 | Grad Norm: 0.00325637\n",
      "Epoch 2 | Step 56900 | Avg Loss: 0.0192 | Grad Norm: 0.00320287\n",
      "Epoch 2 | Step 57000 | Avg Loss: 0.0196 | Grad Norm: 0.00381769\n",
      "Epoch 2 | Step 57100 | Avg Loss: 0.0198 | Grad Norm: 0.00317695\n",
      "Epoch 2 | Step 57200 | Avg Loss: 0.0193 | Grad Norm: 0.00309531\n",
      "Epoch 2 | Step 57300 | Avg Loss: 0.0194 | Grad Norm: 0.00273285\n",
      "Epoch 2 | Step 57400 | Avg Loss: 0.0198 | Grad Norm: 0.00284388\n",
      "Epoch 2 | Step 57500 | Avg Loss: 0.0198 | Grad Norm: 0.00309720\n",
      "Epoch 2 | Step 57600 | Avg Loss: 0.0198 | Grad Norm: 0.00267753\n",
      "Epoch 2 | Step 57700 | Avg Loss: 0.0201 | Grad Norm: 0.00287467\n",
      "Epoch 2 | Step 57800 | Avg Loss: 0.0200 | Grad Norm: 0.00261203\n",
      "Epoch 2 | Step 57900 | Avg Loss: 0.0200 | Grad Norm: 0.00299358\n",
      "Epoch 2 | Step 58000 | Avg Loss: 0.0202 | Grad Norm: 0.00311991\n",
      "Epoch 2 | Step 58100 | Avg Loss: 0.0203 | Grad Norm: 0.00284190\n",
      "Epoch 2 | Step 58200 | Avg Loss: 0.0203 | Grad Norm: 0.00312010\n",
      "Epoch 2 | Step 58300 | Avg Loss: 0.0205 | Grad Norm: 0.00304094\n",
      "Epoch 2 | Step 58400 | Avg Loss: 0.0198 | Grad Norm: 0.00327811\n",
      "Epoch 2 | Step 58500 | Avg Loss: 0.0197 | Grad Norm: 0.00312106\n",
      "Epoch 2 | Step 58600 | Avg Loss: 0.0191 | Grad Norm: 0.00303624\n",
      "Epoch 2 | Step 58700 | Avg Loss: 0.0191 | Grad Norm: 0.00311051\n",
      "Epoch 2 | Step 58800 | Avg Loss: 0.0192 | Grad Norm: 0.00281844\n",
      "Epoch 2 | Step 58900 | Avg Loss: 0.0189 | Grad Norm: 0.00326876\n",
      "Epoch 2 | Step 59000 | Avg Loss: 0.0192 | Grad Norm: 0.00288286\n",
      "Epoch 2 | Step 59100 | Avg Loss: 0.0193 | Grad Norm: 0.00292439\n",
      "Epoch 2 | Step 59200 | Avg Loss: 0.0191 | Grad Norm: 0.00313772\n",
      "Epoch 2 | Step 59300 | Avg Loss: 0.0190 | Grad Norm: 0.00361094\n",
      "Epoch 2 | Step 59400 | Avg Loss: 0.0194 | Grad Norm: 0.00355188\n",
      "Epoch 2 | Step 59500 | Avg Loss: 0.0195 | Grad Norm: 0.00296548\n",
      "Epoch 2 | Step 59600 | Avg Loss: 0.0194 | Grad Norm: 0.00388142\n",
      "Epoch 2 | Step 59700 | Avg Loss: 0.0196 | Grad Norm: 0.00350557\n",
      "Epoch 2 | Step 59800 | Avg Loss: 0.0195 | Grad Norm: 0.00354534\n",
      "Epoch 2 | Step 59900 | Avg Loss: 0.0193 | Grad Norm: 0.00272125\n",
      "Epoch 2 | Step 60000 | Avg Loss: 0.0193 | Grad Norm: 0.00309928\n",
      "Epoch 2 | Step 60100 | Avg Loss: 0.0192 | Grad Norm: 0.00368030\n",
      "Epoch 2 | Step 60200 | Avg Loss: 0.0193 | Grad Norm: 0.00320005\n",
      "Epoch 2 | Step 60300 | Avg Loss: 0.0195 | Grad Norm: 0.00285117\n",
      "Epoch 2 | Step 60400 | Avg Loss: 0.0194 | Grad Norm: 0.00280443\n",
      "Epoch 2 | Step 60500 | Avg Loss: 0.0199 | Grad Norm: 0.00296544\n",
      "Epoch 2 | Step 60600 | Avg Loss: 0.0201 | Grad Norm: 0.00336481\n",
      "Epoch 2 | Step 60700 | Avg Loss: 0.0198 | Grad Norm: 0.00291330\n",
      "Epoch 2 | Step 60800 | Avg Loss: 0.0201 | Grad Norm: 0.00329705\n",
      "Epoch 2 | Step 60900 | Avg Loss: 0.0200 | Grad Norm: 0.00333857\n",
      "Epoch 2 | Step 61000 | Avg Loss: 0.0200 | Grad Norm: 0.00340020\n",
      "Epoch 2 | Step 61100 | Avg Loss: 0.0198 | Grad Norm: 0.00329118\n",
      "Epoch 2 | Step 61200 | Avg Loss: 0.0194 | Grad Norm: 0.00269732\n",
      "Epoch 2 | Step 61300 | Avg Loss: 0.0193 | Grad Norm: 0.00304114\n",
      "Epoch 2 | Step 61400 | Avg Loss: 0.0194 | Grad Norm: 0.00323480\n",
      "Epoch 2 | Step 61500 | Avg Loss: 0.0197 | Grad Norm: 0.00264628\n",
      "Epoch 2 | Step 61600 | Avg Loss: 0.0196 | Grad Norm: 0.00306621\n",
      "Epoch 2 | Step 61700 | Avg Loss: 0.0193 | Grad Norm: 0.00345363\n",
      "Epoch 2 | Step 61800 | Avg Loss: 0.0194 | Grad Norm: 0.00303974\n",
      "Epoch 2 | Step 61900 | Avg Loss: 0.0198 | Grad Norm: 0.00279577\n",
      "Epoch 2 | Step 62000 | Avg Loss: 0.0197 | Grad Norm: 0.00345029\n",
      "Epoch 2 | Step 62100 | Avg Loss: 0.0193 | Grad Norm: 0.00414613\n",
      "Epoch 2 | Step 62200 | Avg Loss: 0.0191 | Grad Norm: 0.00281954\n",
      "Epoch 2 | Step 62300 | Avg Loss: 0.0189 | Grad Norm: 0.00328348\n",
      "Epoch 2 | Step 62400 | Avg Loss: 0.0190 | Grad Norm: 0.00346389\n",
      "Epoch 2 | Step 62500 | Avg Loss: 0.0190 | Grad Norm: 0.00301431\n",
      "Epoch 2 | Step 62600 | Avg Loss: 0.0198 | Grad Norm: 0.00308055\n",
      "Epoch 2 | Step 62700 | Avg Loss: 0.0199 | Grad Norm: 0.00348958\n",
      "Epoch 2 | Step 62800 | Avg Loss: 0.0198 | Grad Norm: 0.00295152\n",
      "Epoch 2 | Step 62900 | Avg Loss: 0.0198 | Grad Norm: 0.00309268\n",
      "Epoch 2 | Step 63000 | Avg Loss: 0.0195 | Grad Norm: 0.00311134\n",
      "Epoch 2 | Step 63100 | Avg Loss: 0.0197 | Grad Norm: 0.00340894\n",
      "Epoch 2 | Step 63200 | Avg Loss: 0.0198 | Grad Norm: 0.00289990\n",
      "Epoch 2 | Step 63300 | Avg Loss: 0.0196 | Grad Norm: 0.00301293\n",
      "Epoch 2 | Step 63400 | Avg Loss: 0.0193 | Grad Norm: 0.00386883\n",
      "Epoch 2 | Step 63500 | Avg Loss: 0.0191 | Grad Norm: 0.00344921\n",
      "Epoch 2 | Step 63600 | Avg Loss: 0.0192 | Grad Norm: 0.00334208\n",
      "Epoch 2 | Step 63700 | Avg Loss: 0.0199 | Grad Norm: 0.00311502\n",
      "Epoch 2 | Step 63800 | Avg Loss: 0.0200 | Grad Norm: 0.00356394\n",
      "Epoch 2 | Step 63900 | Avg Loss: 0.0195 | Grad Norm: 0.00411686\n",
      "Epoch 2 | Step 64000 | Avg Loss: 0.0195 | Grad Norm: 0.00307093\n",
      "Epoch 2 | Step 64100 | Avg Loss: 0.0195 | Grad Norm: 0.00350248\n",
      "Epoch 2 | Step 64200 | Avg Loss: 0.0199 | Grad Norm: 0.00422565\n",
      "Epoch 2 | Step 64300 | Avg Loss: 0.0199 | Grad Norm: 0.00383082\n",
      "Epoch 2 | Step 64400 | Avg Loss: 0.0203 | Grad Norm: 0.00313648\n",
      "Epoch 2 | Step 64500 | Avg Loss: 0.0206 | Grad Norm: 0.00321435\n",
      "Epoch 2 | Step 64600 | Avg Loss: 0.0201 | Grad Norm: 0.00296414\n",
      "Epoch 2 | Step 64700 | Avg Loss: 0.0202 | Grad Norm: 0.00263494\n",
      "Epoch 2 | Step 64800 | Avg Loss: 0.0202 | Grad Norm: 0.00350183\n",
      "Epoch 2 | Step 64900 | Avg Loss: 0.0202 | Grad Norm: 0.00332058\n",
      "Epoch 2 | Step 65000 | Avg Loss: 0.0201 | Grad Norm: 0.00350219\n",
      "Epoch 2 | Step 65100 | Avg Loss: 0.0196 | Grad Norm: 0.00292029\n",
      "Epoch 2 | Step 65200 | Avg Loss: 0.0192 | Grad Norm: 0.00379900\n",
      "Epoch 2 | Step 65300 | Avg Loss: 0.0195 | Grad Norm: 0.00319567\n",
      "Epoch 2 | Step 65400 | Avg Loss: 0.0196 | Grad Norm: 0.00273040\n",
      "Epoch 2 | Step 65500 | Avg Loss: 0.0190 | Grad Norm: 0.00291044\n",
      "Epoch 2 | Step 65600 | Avg Loss: 0.0190 | Grad Norm: 0.00325217\n",
      "Epoch 2 | Step 65700 | Avg Loss: 0.0191 | Grad Norm: 0.00274015\n",
      "Epoch 2 | Step 65800 | Avg Loss: 0.0194 | Grad Norm: 0.00301933\n",
      "Epoch 2 | Step 65900 | Avg Loss: 0.0196 | Grad Norm: 0.00337628\n",
      "Epoch 2 | Step 66000 | Avg Loss: 0.0197 | Grad Norm: 0.00307158\n",
      "Epoch 2 | Step 66100 | Avg Loss: 0.0196 | Grad Norm: 0.00408037\n",
      "Epoch 2 | Step 66200 | Avg Loss: 0.0196 | Grad Norm: 0.00305614\n",
      "Epoch 2 | Step 66300 | Avg Loss: 0.0191 | Grad Norm: 0.00281871\n",
      "Epoch 2 | Step 66400 | Avg Loss: 0.0194 | Grad Norm: 0.00273690\n",
      "Epoch 2 | Step 66500 | Avg Loss: 0.0197 | Grad Norm: 0.00247764\n",
      "Epoch 2 | Step 66600 | Avg Loss: 0.0198 | Grad Norm: 0.00328036\n",
      "Epoch 2 | Step 66700 | Avg Loss: 0.0196 | Grad Norm: 0.00251531\n",
      "Epoch 2 | Step 66800 | Avg Loss: 0.0198 | Grad Norm: 0.00405330\n",
      "Epoch 2 | Step 66900 | Avg Loss: 0.0199 | Grad Norm: 0.00301380\n",
      "Epoch 2 | Step 67000 | Avg Loss: 0.0196 | Grad Norm: 0.00329446\n",
      "Epoch 2 | Step 67100 | Avg Loss: 0.0197 | Grad Norm: 0.00338699\n",
      "Epoch 2 | Step 67200 | Avg Loss: 0.0199 | Grad Norm: 0.00279791\n",
      "Epoch 2 | Step 67300 | Avg Loss: 0.0193 | Grad Norm: 0.00307970\n",
      "Epoch 2 | Step 67400 | Avg Loss: 0.0190 | Grad Norm: 0.00331012\n",
      "Epoch 2 | Step 67500 | Avg Loss: 0.0196 | Grad Norm: 0.00403518\n",
      "Epoch 2 | Step 67600 | Avg Loss: 0.0199 | Grad Norm: 0.00279181\n",
      "Epoch 2 | Step 67700 | Avg Loss: 0.0197 | Grad Norm: 0.00303128\n",
      "Epoch 2 | Step 67800 | Avg Loss: 0.0193 | Grad Norm: 0.00292302\n",
      "Epoch 2 | Step 67900 | Avg Loss: 0.0194 | Grad Norm: 0.00286473\n",
      "Epoch 2 | Step 68000 | Avg Loss: 0.0192 | Grad Norm: 0.00283648\n",
      "Epoch 2 | Step 68100 | Avg Loss: 0.0192 | Grad Norm: 0.00344558\n",
      "Epoch 2 | Step 68200 | Avg Loss: 0.0195 | Grad Norm: 0.00276270\n",
      "Epoch 2 | Step 68300 | Avg Loss: 0.0194 | Grad Norm: 0.00299980\n",
      "Epoch 2 | Step 68400 | Avg Loss: 0.0193 | Grad Norm: 0.00375293\n",
      "Epoch 2 | Step 68500 | Avg Loss: 0.0194 | Grad Norm: 0.00335298\n",
      "Epoch 2 | Step 68600 | Avg Loss: 0.0194 | Grad Norm: 0.00298854\n",
      "Epoch 2 | Step 68700 | Avg Loss: 0.0194 | Grad Norm: 0.00397516\n",
      "Epoch 2 | Step 68800 | Avg Loss: 0.0198 | Grad Norm: 0.00348294\n",
      "Epoch 2 | Step 68900 | Avg Loss: 0.0200 | Grad Norm: 0.00332718\n",
      "Epoch 2 | Step 69000 | Avg Loss: 0.0201 | Grad Norm: 0.00332138\n",
      "Epoch 2 | Step 69100 | Avg Loss: 0.0200 | Grad Norm: 0.00315910\n",
      "Epoch 2 | Step 69200 | Avg Loss: 0.0199 | Grad Norm: 0.00327892\n",
      "Epoch 2 | Step 69300 | Avg Loss: 0.0204 | Grad Norm: 0.00335105\n",
      "Epoch 2 | Step 69400 | Avg Loss: 0.0202 | Grad Norm: 0.00287885\n",
      "Epoch 2 | Step 69500 | Avg Loss: 0.0200 | Grad Norm: 0.00308918\n",
      "Epoch 2 | Step 69600 | Avg Loss: 0.0195 | Grad Norm: 0.00295980\n",
      "Epoch 2 | Step 69700 | Avg Loss: 0.0190 | Grad Norm: 0.00324819\n",
      "Epoch 2 | Step 69800 | Avg Loss: 0.0194 | Grad Norm: 0.00281084\n",
      "Epoch 2 | Step 69900 | Avg Loss: 0.0192 | Grad Norm: 0.00422380\n",
      "Epoch 2 | Step 70000 | Avg Loss: 0.0192 | Grad Norm: 0.00293993\n",
      "Epoch 2 | Step 70100 | Avg Loss: 0.0193 | Grad Norm: 0.00346267\n",
      "Epoch 2 | Step 70200 | Avg Loss: 0.0192 | Grad Norm: 0.00309598\n",
      "Epoch 2 | Step 70300 | Avg Loss: 0.0189 | Grad Norm: 0.00309802\n",
      "Epoch 2 | Step 70400 | Avg Loss: 0.0191 | Grad Norm: 0.00393348\n",
      "Epoch 2 | Step 70500 | Avg Loss: 0.0195 | Grad Norm: 0.00315106\n",
      "Epoch 2 | Step 70600 | Avg Loss: 0.0192 | Grad Norm: 0.00336838\n",
      "Epoch 2 | Step 70700 | Avg Loss: 0.0190 | Grad Norm: 0.00271420\n",
      "Epoch 2 | Step 70800 | Avg Loss: 0.0188 | Grad Norm: 0.00322111\n",
      "Epoch 2 | Step 70900 | Avg Loss: 0.0197 | Grad Norm: 0.00313264\n",
      "Epoch 2 | Step 71000 | Avg Loss: 0.0197 | Grad Norm: 0.00307663\n",
      "Epoch 2 | Step 71100 | Avg Loss: 0.0198 | Grad Norm: 0.00345772\n",
      "Epoch 2 | Step 71200 | Avg Loss: 0.0195 | Grad Norm: 0.00320011\n",
      "Epoch 2 | Step 71300 | Avg Loss: 0.0197 | Grad Norm: 0.00300157\n",
      "Epoch 2 | Step 71400 | Avg Loss: 0.0198 | Grad Norm: 0.00307023\n",
      "Epoch 2 | Step 71500 | Avg Loss: 0.0199 | Grad Norm: 0.00306941\n",
      "Epoch 2 | Step 71600 | Avg Loss: 0.0198 | Grad Norm: 0.00307629\n",
      "Epoch 2 | Step 71700 | Avg Loss: 0.0198 | Grad Norm: 0.00320526\n",
      "Epoch 2 | Step 71800 | Avg Loss: 0.0199 | Grad Norm: 0.00291359\n",
      "Epoch 2 | Step 71900 | Avg Loss: 0.0196 | Grad Norm: 0.00355300\n",
      "Epoch 2 | Step 72000 | Avg Loss: 0.0194 | Grad Norm: 0.00314902\n",
      "Epoch 2 | Step 72100 | Avg Loss: 0.0193 | Grad Norm: 0.00285784\n",
      "Epoch 2 | Step 72200 | Avg Loss: 0.0191 | Grad Norm: 0.00367683\n",
      "Epoch 2 | Step 72300 | Avg Loss: 0.0188 | Grad Norm: 0.00268114\n",
      "Epoch 2 | Step 72400 | Avg Loss: 0.0194 | Grad Norm: 0.00291435\n",
      "Epoch 2 | Step 72500 | Avg Loss: 0.0197 | Grad Norm: 0.00259193\n",
      "Epoch 2 | Step 72600 | Avg Loss: 0.0194 | Grad Norm: 0.00376335\n",
      "Epoch 2 | Step 72700 | Avg Loss: 0.0196 | Grad Norm: 0.00364904\n",
      "Epoch 2 | Step 72800 | Avg Loss: 0.0193 | Grad Norm: 0.00281646\n",
      "Epoch 2 | Step 72900 | Avg Loss: 0.0191 | Grad Norm: 0.00266467\n",
      "Epoch 2 | Step 73000 | Avg Loss: 0.0189 | Grad Norm: 0.00316647\n",
      "Epoch 2 | Step 73100 | Avg Loss: 0.0185 | Grad Norm: 0.00362783\n",
      "Epoch 2 | Step 73200 | Avg Loss: 0.0180 | Grad Norm: 0.00295263\n",
      "Epoch 2 | Step 73300 | Avg Loss: 0.0181 | Grad Norm: 0.00260392\n",
      "Epoch 2 | Step 73400 | Avg Loss: 0.0185 | Grad Norm: 0.00309244\n",
      "Epoch 2 | Step 73500 | Avg Loss: 0.0189 | Grad Norm: 0.00275085\n",
      "Epoch 2 | Step 73600 | Avg Loss: 0.0192 | Grad Norm: 0.00274272\n",
      "Epoch 2 | Step 73700 | Avg Loss: 0.0193 | Grad Norm: 0.00310671\n",
      "Epoch 2 | Step 73800 | Avg Loss: 0.0191 | Grad Norm: 0.00308956\n",
      "Epoch 2 | Step 73900 | Avg Loss: 0.0190 | Grad Norm: 0.00309028\n",
      "Epoch 2 | Step 74000 | Avg Loss: 0.0192 | Grad Norm: 0.00291834\n",
      "Epoch 2 | Step 74100 | Avg Loss: 0.0190 | Grad Norm: 0.00281292\n",
      "Epoch 2 | Step 74200 | Avg Loss: 0.0193 | Grad Norm: 0.00387809\n",
      "Epoch 2 | Step 74300 | Avg Loss: 0.0197 | Grad Norm: 0.00294121\n",
      "Epoch 2 | Step 74400 | Avg Loss: 0.0194 | Grad Norm: 0.00369493\n",
      "Epoch 2 | Step 74500 | Avg Loss: 0.0194 | Grad Norm: 0.00308216\n",
      "Epoch 2 | Step 74600 | Avg Loss: 0.0198 | Grad Norm: 0.00351704\n",
      "Epoch 2 | Step 74700 | Avg Loss: 0.0196 | Grad Norm: 0.00294575\n",
      "Epoch 2 | Step 74800 | Avg Loss: 0.0195 | Grad Norm: 0.00446548\n",
      "Epoch 2 | Step 74900 | Avg Loss: 0.0192 | Grad Norm: 0.00264375\n",
      "Epoch 2 | Step 75000 | Avg Loss: 0.0192 | Grad Norm: 0.00288919\n",
      "Epoch 2 | Step 75100 | Avg Loss: 0.0186 | Grad Norm: 0.00318381\n",
      "Epoch 2 | Step 75200 | Avg Loss: 0.0190 | Grad Norm: 0.00321766\n",
      "Epoch 2 | Step 75300 | Avg Loss: 0.0195 | Grad Norm: 0.00264122\n",
      "Epoch 2 | Step 75400 | Avg Loss: 0.0200 | Grad Norm: 0.00374395\n",
      "Epoch 2 | Step 75500 | Avg Loss: 0.0200 | Grad Norm: 0.00311106\n",
      "Epoch 2 | Step 75600 | Avg Loss: 0.0195 | Grad Norm: 0.00347438\n",
      "Epoch 2 | Step 75700 | Avg Loss: 0.0196 | Grad Norm: 0.00324623\n",
      "Epoch 2 | Step 75800 | Avg Loss: 0.0198 | Grad Norm: 0.00345734\n",
      "Epoch 2 | Step 75900 | Avg Loss: 0.0198 | Grad Norm: 0.00285141\n",
      "Epoch 2 | Step 76000 | Avg Loss: 0.0200 | Grad Norm: 0.00282649\n",
      "Epoch 2 | Step 76100 | Avg Loss: 0.0201 | Grad Norm: 0.00296722\n",
      "Epoch 2 | Step 76200 | Avg Loss: 0.0199 | Grad Norm: 0.00365000\n",
      "Epoch 2 | Step 76300 | Avg Loss: 0.0195 | Grad Norm: 0.00307575\n",
      "Epoch 2 | Step 76400 | Avg Loss: 0.0193 | Grad Norm: 0.00295909\n",
      "Epoch 2 | Step 76500 | Avg Loss: 0.0192 | Grad Norm: 0.00382788\n",
      "Epoch 2 | Step 76600 | Avg Loss: 0.0193 | Grad Norm: 0.00284183\n",
      "Epoch 2 | Step 76700 | Avg Loss: 0.0191 | Grad Norm: 0.00316615\n",
      "Epoch 2 | Step 76800 | Avg Loss: 0.0193 | Grad Norm: 0.00393573\n",
      "Epoch 2 | Step 76900 | Avg Loss: 0.0192 | Grad Norm: 0.00296675\n",
      "Epoch 2 | Step 77000 | Avg Loss: 0.0194 | Grad Norm: 0.00288847\n",
      "Epoch 2 | Step 77100 | Avg Loss: 0.0199 | Grad Norm: 0.00297274\n",
      "Epoch 2 | Step 77200 | Avg Loss: 0.0198 | Grad Norm: 0.00321240\n",
      "Epoch 2 | Step 77300 | Avg Loss: 0.0199 | Grad Norm: 0.00281008\n",
      "Epoch 2 | Step 77400 | Avg Loss: 0.0198 | Grad Norm: 0.00372301\n",
      "Epoch 2 | Step 77500 | Avg Loss: 0.0199 | Grad Norm: 0.00371495\n",
      "Epoch 2 | Step 77600 | Avg Loss: 0.0196 | Grad Norm: 0.00324642\n",
      "Epoch 2 | Step 77700 | Avg Loss: 0.0196 | Grad Norm: 0.00257852\n",
      "Epoch 2 | Step 77800 | Avg Loss: 0.0197 | Grad Norm: 0.00324645\n",
      "Epoch 2 | Step 77900 | Avg Loss: 0.0195 | Grad Norm: 0.00393739\n",
      "Epoch 2 | Step 78000 | Avg Loss: 0.0192 | Grad Norm: 0.00323709\n",
      "Epoch 2 | Step 78100 | Avg Loss: 0.0189 | Grad Norm: 0.00306462\n",
      "Epoch 2, Loss: 0.0195\n",
      "Epoch 3 | Step 78200 | Avg Loss: 0.0193 | Grad Norm: 0.00295265\n",
      "Epoch 3 | Step 78300 | Avg Loss: 0.0189 | Grad Norm: 0.00267992\n",
      "Epoch 3 | Step 78400 | Avg Loss: 0.0187 | Grad Norm: 0.00283387\n",
      "Epoch 3 | Step 78500 | Avg Loss: 0.0185 | Grad Norm: 0.00299838\n",
      "Epoch 3 | Step 78600 | Avg Loss: 0.0187 | Grad Norm: 0.00347270\n",
      "Epoch 3 | Step 78700 | Avg Loss: 0.0186 | Grad Norm: 0.00365227\n",
      "Epoch 3 | Step 78800 | Avg Loss: 0.0195 | Grad Norm: 0.00297668\n",
      "Epoch 3 | Step 78900 | Avg Loss: 0.0197 | Grad Norm: 0.00420502\n",
      "Epoch 3 | Step 79000 | Avg Loss: 0.0201 | Grad Norm: 0.00326690\n",
      "Epoch 3 | Step 79100 | Avg Loss: 0.0197 | Grad Norm: 0.00294970\n",
      "Epoch 3 | Step 79200 | Avg Loss: 0.0194 | Grad Norm: 0.00365258\n",
      "Epoch 3 | Step 79300 | Avg Loss: 0.0192 | Grad Norm: 0.00300064\n",
      "Epoch 3 | Step 79400 | Avg Loss: 0.0191 | Grad Norm: 0.00322336\n",
      "Epoch 3 | Step 79500 | Avg Loss: 0.0192 | Grad Norm: 0.00276461\n",
      "Epoch 3 | Step 79600 | Avg Loss: 0.0192 | Grad Norm: 0.00418441\n",
      "Epoch 3 | Step 79700 | Avg Loss: 0.0189 | Grad Norm: 0.00253123\n",
      "Epoch 3 | Step 79800 | Avg Loss: 0.0185 | Grad Norm: 0.00280555\n",
      "Epoch 3 | Step 79900 | Avg Loss: 0.0187 | Grad Norm: 0.00349812\n",
      "Epoch 3 | Step 80000 | Avg Loss: 0.0187 | Grad Norm: 0.00382196\n",
      "Epoch 3 | Step 80100 | Avg Loss: 0.0191 | Grad Norm: 0.00384803\n",
      "Epoch 3 | Step 80200 | Avg Loss: 0.0187 | Grad Norm: 0.00379207\n",
      "Epoch 3 | Step 80300 | Avg Loss: 0.0191 | Grad Norm: 0.00492213\n",
      "Epoch 3 | Step 80400 | Avg Loss: 0.0190 | Grad Norm: 0.00315747\n",
      "Epoch 3 | Step 80500 | Avg Loss: 0.0189 | Grad Norm: 0.00278177\n",
      "Epoch 3 | Step 80600 | Avg Loss: 0.0193 | Grad Norm: 0.00370198\n",
      "Epoch 3 | Step 80700 | Avg Loss: 0.0193 | Grad Norm: 0.00332092\n",
      "Epoch 3 | Step 80800 | Avg Loss: 0.0195 | Grad Norm: 0.00262696\n",
      "Epoch 3 | Step 80900 | Avg Loss: 0.0195 | Grad Norm: 0.00336356\n",
      "Epoch 3 | Step 81000 | Avg Loss: 0.0193 | Grad Norm: 0.00315744\n",
      "Epoch 3 | Step 81100 | Avg Loss: 0.0194 | Grad Norm: 0.00347344\n",
      "Epoch 3 | Step 81200 | Avg Loss: 0.0194 | Grad Norm: 0.00315277\n",
      "Epoch 3 | Step 81300 | Avg Loss: 0.0190 | Grad Norm: 0.00340972\n",
      "Epoch 3 | Step 81400 | Avg Loss: 0.0190 | Grad Norm: 0.00374952\n",
      "Epoch 3 | Step 81500 | Avg Loss: 0.0186 | Grad Norm: 0.00335201\n",
      "Epoch 3 | Step 81600 | Avg Loss: 0.0183 | Grad Norm: 0.00391194\n",
      "Epoch 3 | Step 81700 | Avg Loss: 0.0183 | Grad Norm: 0.00347413\n",
      "Epoch 3 | Step 81800 | Avg Loss: 0.0186 | Grad Norm: 0.00373160\n",
      "Epoch 3 | Step 81900 | Avg Loss: 0.0188 | Grad Norm: 0.00381430\n",
      "Epoch 3 | Step 82000 | Avg Loss: 0.0191 | Grad Norm: 0.00331716\n",
      "Epoch 3 | Step 82100 | Avg Loss: 0.0190 | Grad Norm: 0.00273664\n",
      "Epoch 3 | Step 82200 | Avg Loss: 0.0189 | Grad Norm: 0.00289213\n",
      "Epoch 3 | Step 82300 | Avg Loss: 0.0188 | Grad Norm: 0.00403988\n",
      "Epoch 3 | Step 82400 | Avg Loss: 0.0191 | Grad Norm: 0.00300989\n",
      "Epoch 3 | Step 82500 | Avg Loss: 0.0193 | Grad Norm: 0.00271402\n",
      "Epoch 3 | Step 82600 | Avg Loss: 0.0191 | Grad Norm: 0.00275883\n",
      "Epoch 3 | Step 82700 | Avg Loss: 0.0184 | Grad Norm: 0.00309759\n",
      "Epoch 3 | Step 82800 | Avg Loss: 0.0185 | Grad Norm: 0.00279798\n",
      "Epoch 3 | Step 82900 | Avg Loss: 0.0186 | Grad Norm: 0.00302726\n",
      "Epoch 3 | Step 83000 | Avg Loss: 0.0187 | Grad Norm: 0.00341892\n",
      "Epoch 3 | Step 83100 | Avg Loss: 0.0187 | Grad Norm: 0.00281780\n",
      "Epoch 3 | Step 83200 | Avg Loss: 0.0187 | Grad Norm: 0.00259884\n",
      "Epoch 3 | Step 83300 | Avg Loss: 0.0187 | Grad Norm: 0.00347669\n",
      "Epoch 3 | Step 83400 | Avg Loss: 0.0189 | Grad Norm: 0.00389937\n",
      "Epoch 3 | Step 83500 | Avg Loss: 0.0194 | Grad Norm: 0.00281830\n",
      "Epoch 3 | Step 83600 | Avg Loss: 0.0196 | Grad Norm: 0.00339375\n",
      "Epoch 3 | Step 83700 | Avg Loss: 0.0197 | Grad Norm: 0.00293098\n",
      "Epoch 3 | Step 83800 | Avg Loss: 0.0191 | Grad Norm: 0.00292232\n",
      "Epoch 3 | Step 83900 | Avg Loss: 0.0195 | Grad Norm: 0.00320174\n",
      "Epoch 3 | Step 84000 | Avg Loss: 0.0191 | Grad Norm: 0.00340428\n",
      "Epoch 3 | Step 84100 | Avg Loss: 0.0190 | Grad Norm: 0.00283996\n",
      "Epoch 3 | Step 84200 | Avg Loss: 0.0187 | Grad Norm: 0.00274854\n",
      "Epoch 3 | Step 84300 | Avg Loss: 0.0186 | Grad Norm: 0.00383107\n",
      "Epoch 3 | Step 84400 | Avg Loss: 0.0190 | Grad Norm: 0.00335544\n",
      "Epoch 3 | Step 84500 | Avg Loss: 0.0192 | Grad Norm: 0.00306715\n",
      "Epoch 3 | Step 84600 | Avg Loss: 0.0189 | Grad Norm: 0.00246450\n",
      "Epoch 3 | Step 84700 | Avg Loss: 0.0191 | Grad Norm: 0.00382037\n",
      "Epoch 3 | Step 84800 | Avg Loss: 0.0189 | Grad Norm: 0.00282830\n",
      "Epoch 3 | Step 84900 | Avg Loss: 0.0187 | Grad Norm: 0.00366462\n",
      "Epoch 3 | Step 85000 | Avg Loss: 0.0187 | Grad Norm: 0.00276141\n",
      "Epoch 3 | Step 85100 | Avg Loss: 0.0188 | Grad Norm: 0.00344502\n",
      "Epoch 3 | Step 85200 | Avg Loss: 0.0187 | Grad Norm: 0.00253952\n",
      "Epoch 3 | Step 85300 | Avg Loss: 0.0184 | Grad Norm: 0.00314587\n",
      "Epoch 3 | Step 85400 | Avg Loss: 0.0181 | Grad Norm: 0.00265310\n",
      "Epoch 3 | Step 85500 | Avg Loss: 0.0184 | Grad Norm: 0.00287629\n",
      "Epoch 3 | Step 85600 | Avg Loss: 0.0183 | Grad Norm: 0.00377765\n",
      "Epoch 3 | Step 85700 | Avg Loss: 0.0187 | Grad Norm: 0.00269244\n",
      "Epoch 3 | Step 85800 | Avg Loss: 0.0185 | Grad Norm: 0.00280743\n",
      "Epoch 3 | Step 85900 | Avg Loss: 0.0183 | Grad Norm: 0.00322637\n",
      "Epoch 3 | Step 86000 | Avg Loss: 0.0185 | Grad Norm: 0.00290984\n",
      "Epoch 3 | Step 86100 | Avg Loss: 0.0190 | Grad Norm: 0.00273693\n",
      "Epoch 3 | Step 86200 | Avg Loss: 0.0193 | Grad Norm: 0.00367461\n",
      "Epoch 3 | Step 86300 | Avg Loss: 0.0192 | Grad Norm: 0.00341538\n",
      "Epoch 3 | Step 86400 | Avg Loss: 0.0193 | Grad Norm: 0.00458106\n",
      "Epoch 3 | Step 86500 | Avg Loss: 0.0190 | Grad Norm: 0.00317963\n",
      "Epoch 3 | Step 86600 | Avg Loss: 0.0193 | Grad Norm: 0.00364358\n",
      "Epoch 3 | Step 86700 | Avg Loss: 0.0196 | Grad Norm: 0.00266910\n",
      "Epoch 3 | Step 86800 | Avg Loss: 0.0198 | Grad Norm: 0.00350438\n",
      "Epoch 3 | Step 86900 | Avg Loss: 0.0196 | Grad Norm: 0.00296934\n",
      "Epoch 3 | Step 87000 | Avg Loss: 0.0195 | Grad Norm: 0.00293071\n",
      "Epoch 3 | Step 87100 | Avg Loss: 0.0193 | Grad Norm: 0.00284008\n",
      "Epoch 3 | Step 87200 | Avg Loss: 0.0193 | Grad Norm: 0.00349386\n",
      "Epoch 3 | Step 87300 | Avg Loss: 0.0192 | Grad Norm: 0.00293350\n",
      "Epoch 3 | Step 87400 | Avg Loss: 0.0187 | Grad Norm: 0.00292755\n",
      "Epoch 3 | Step 87500 | Avg Loss: 0.0185 | Grad Norm: 0.00274319\n",
      "Epoch 3 | Step 87600 | Avg Loss: 0.0187 | Grad Norm: 0.00279262\n",
      "Epoch 3 | Step 87700 | Avg Loss: 0.0188 | Grad Norm: 0.00446463\n",
      "Epoch 3 | Step 87800 | Avg Loss: 0.0190 | Grad Norm: 0.00282590\n",
      "Epoch 3 | Step 87900 | Avg Loss: 0.0191 | Grad Norm: 0.00299406\n",
      "Epoch 3 | Step 88000 | Avg Loss: 0.0191 | Grad Norm: 0.00307499\n",
      "Epoch 3 | Step 88100 | Avg Loss: 0.0192 | Grad Norm: 0.00291710\n",
      "Epoch 3 | Step 88200 | Avg Loss: 0.0192 | Grad Norm: 0.00333595\n",
      "Epoch 3 | Step 88300 | Avg Loss: 0.0194 | Grad Norm: 0.00288359\n",
      "Epoch 3 | Step 88400 | Avg Loss: 0.0192 | Grad Norm: 0.00288584\n",
      "Epoch 3 | Step 88500 | Avg Loss: 0.0190 | Grad Norm: 0.00283399\n",
      "Epoch 3 | Step 88600 | Avg Loss: 0.0193 | Grad Norm: 0.00315270\n",
      "Epoch 3 | Step 88700 | Avg Loss: 0.0196 | Grad Norm: 0.00283297\n",
      "Epoch 3 | Step 88800 | Avg Loss: 0.0193 | Grad Norm: 0.00430966\n",
      "Epoch 3 | Step 88900 | Avg Loss: 0.0196 | Grad Norm: 0.00336787\n",
      "Epoch 3 | Step 89000 | Avg Loss: 0.0192 | Grad Norm: 0.00303849\n",
      "Epoch 3 | Step 89100 | Avg Loss: 0.0192 | Grad Norm: 0.00319306\n",
      "Epoch 3 | Step 89200 | Avg Loss: 0.0199 | Grad Norm: 0.00341322\n",
      "Epoch 3 | Step 89300 | Avg Loss: 0.0199 | Grad Norm: 0.00311326\n",
      "Epoch 3 | Step 89400 | Avg Loss: 0.0197 | Grad Norm: 0.00350923\n",
      "Epoch 3 | Step 89500 | Avg Loss: 0.0194 | Grad Norm: 0.00308054\n",
      "Epoch 3 | Step 89600 | Avg Loss: 0.0194 | Grad Norm: 0.00346281\n",
      "Epoch 3 | Step 89700 | Avg Loss: 0.0194 | Grad Norm: 0.00402409\n",
      "Epoch 3 | Step 89800 | Avg Loss: 0.0192 | Grad Norm: 0.00295147\n",
      "Epoch 3 | Step 89900 | Avg Loss: 0.0189 | Grad Norm: 0.00313256\n",
      "Epoch 3 | Step 90000 | Avg Loss: 0.0187 | Grad Norm: 0.00298551\n",
      "Epoch 3 | Step 90100 | Avg Loss: 0.0185 | Grad Norm: 0.00308175\n",
      "Epoch 3 | Step 90200 | Avg Loss: 0.0183 | Grad Norm: 0.00244219\n",
      "Epoch 3 | Step 90300 | Avg Loss: 0.0184 | Grad Norm: 0.00286546\n",
      "Epoch 3 | Step 90400 | Avg Loss: 0.0185 | Grad Norm: 0.00283837\n",
      "Epoch 3 | Step 90500 | Avg Loss: 0.0191 | Grad Norm: 0.00304829\n",
      "Epoch 3 | Step 90600 | Avg Loss: 0.0196 | Grad Norm: 0.00390921\n",
      "Epoch 3 | Step 90700 | Avg Loss: 0.0195 | Grad Norm: 0.00342692\n",
      "Epoch 3 | Step 90800 | Avg Loss: 0.0190 | Grad Norm: 0.00340350\n",
      "Epoch 3 | Step 90900 | Avg Loss: 0.0189 | Grad Norm: 0.00419688\n",
      "Epoch 3 | Step 91000 | Avg Loss: 0.0189 | Grad Norm: 0.00289303\n",
      "Epoch 3 | Step 91100 | Avg Loss: 0.0189 | Grad Norm: 0.00397661\n",
      "Epoch 3 | Step 91200 | Avg Loss: 0.0190 | Grad Norm: 0.00291934\n",
      "Epoch 3 | Step 91300 | Avg Loss: 0.0187 | Grad Norm: 0.00273222\n",
      "Epoch 3 | Step 91400 | Avg Loss: 0.0188 | Grad Norm: 0.00295874\n",
      "Epoch 3 | Step 91500 | Avg Loss: 0.0186 | Grad Norm: 0.00290952\n",
      "Epoch 3 | Step 91600 | Avg Loss: 0.0188 | Grad Norm: 0.00265948\n",
      "Epoch 3 | Step 91700 | Avg Loss: 0.0188 | Grad Norm: 0.00330227\n",
      "Epoch 3 | Step 91800 | Avg Loss: 0.0183 | Grad Norm: 0.00283625\n",
      "Epoch 3 | Step 91900 | Avg Loss: 0.0183 | Grad Norm: 0.00338009\n",
      "Epoch 3 | Step 92000 | Avg Loss: 0.0192 | Grad Norm: 0.00356698\n",
      "Epoch 3 | Step 92100 | Avg Loss: 0.0192 | Grad Norm: 0.00252101\n",
      "Epoch 3 | Step 92200 | Avg Loss: 0.0188 | Grad Norm: 0.00274730\n",
      "Epoch 3 | Step 92300 | Avg Loss: 0.0190 | Grad Norm: 0.00364060\n",
      "Epoch 3 | Step 92400 | Avg Loss: 0.0189 | Grad Norm: 0.00338564\n",
      "Epoch 3 | Step 92500 | Avg Loss: 0.0190 | Grad Norm: 0.00250206\n",
      "Epoch 3 | Step 92600 | Avg Loss: 0.0191 | Grad Norm: 0.00270461\n",
      "Epoch 3 | Step 92700 | Avg Loss: 0.0194 | Grad Norm: 0.00312155\n",
      "Epoch 3 | Step 92800 | Avg Loss: 0.0194 | Grad Norm: 0.00357387\n",
      "Epoch 3 | Step 92900 | Avg Loss: 0.0190 | Grad Norm: 0.00287449\n",
      "Epoch 3 | Step 93000 | Avg Loss: 0.0189 | Grad Norm: 0.00302243\n",
      "Epoch 3 | Step 93100 | Avg Loss: 0.0191 | Grad Norm: 0.00432828\n",
      "Epoch 3 | Step 93200 | Avg Loss: 0.0190 | Grad Norm: 0.00306884\n",
      "Epoch 3 | Step 93300 | Avg Loss: 0.0189 | Grad Norm: 0.00303487\n",
      "Epoch 3 | Step 93400 | Avg Loss: 0.0189 | Grad Norm: 0.00334861\n",
      "Epoch 3 | Step 93500 | Avg Loss: 0.0190 | Grad Norm: 0.00304827\n",
      "Epoch 3 | Step 93600 | Avg Loss: 0.0188 | Grad Norm: 0.00310406\n",
      "Epoch 3 | Step 93700 | Avg Loss: 0.0193 | Grad Norm: 0.00326451\n",
      "Epoch 3 | Step 93800 | Avg Loss: 0.0195 | Grad Norm: 0.00353877\n",
      "Epoch 3 | Step 93900 | Avg Loss: 0.0199 | Grad Norm: 0.00321626\n",
      "Epoch 3 | Step 94000 | Avg Loss: 0.0201 | Grad Norm: 0.00373067\n",
      "Epoch 3 | Step 94100 | Avg Loss: 0.0201 | Grad Norm: 0.00326561\n",
      "Epoch 3 | Step 94200 | Avg Loss: 0.0199 | Grad Norm: 0.00337711\n",
      "Epoch 3 | Step 94300 | Avg Loss: 0.0199 | Grad Norm: 0.00363929\n",
      "Epoch 3 | Step 94400 | Avg Loss: 0.0199 | Grad Norm: 0.00251827\n",
      "Epoch 3 | Step 94500 | Avg Loss: 0.0197 | Grad Norm: 0.00282332\n",
      "Epoch 3 | Step 94600 | Avg Loss: 0.0197 | Grad Norm: 0.00369500\n",
      "Epoch 3 | Step 94700 | Avg Loss: 0.0194 | Grad Norm: 0.00344814\n",
      "Epoch 3 | Step 94800 | Avg Loss: 0.0189 | Grad Norm: 0.00332159\n",
      "Epoch 3 | Step 94900 | Avg Loss: 0.0191 | Grad Norm: 0.00262615\n",
      "Epoch 3 | Step 95000 | Avg Loss: 0.0193 | Grad Norm: 0.00359608\n",
      "Epoch 3 | Step 95100 | Avg Loss: 0.0193 | Grad Norm: 0.00307337\n",
      "Epoch 3 | Step 95200 | Avg Loss: 0.0190 | Grad Norm: 0.00362961\n",
      "Epoch 3 | Step 95300 | Avg Loss: 0.0188 | Grad Norm: 0.00300258\n",
      "Epoch 3 | Step 95400 | Avg Loss: 0.0187 | Grad Norm: 0.00358592\n",
      "Epoch 3 | Step 95500 | Avg Loss: 0.0186 | Grad Norm: 0.00296729\n",
      "Epoch 3 | Step 95600 | Avg Loss: 0.0187 | Grad Norm: 0.00329180\n",
      "Epoch 3 | Step 95700 | Avg Loss: 0.0183 | Grad Norm: 0.00272783\n",
      "Epoch 3 | Step 95800 | Avg Loss: 0.0182 | Grad Norm: 0.00307888\n",
      "Epoch 3 | Step 95900 | Avg Loss: 0.0185 | Grad Norm: 0.00508463\n",
      "Epoch 3 | Step 96000 | Avg Loss: 0.0186 | Grad Norm: 0.00303936\n",
      "Epoch 3 | Step 96100 | Avg Loss: 0.0189 | Grad Norm: 0.00317960\n",
      "Epoch 3 | Step 96200 | Avg Loss: 0.0186 | Grad Norm: 0.00434950\n",
      "Epoch 3 | Step 96300 | Avg Loss: 0.0184 | Grad Norm: 0.00320039\n",
      "Epoch 3 | Step 96400 | Avg Loss: 0.0188 | Grad Norm: 0.00297819\n",
      "Epoch 3 | Step 96500 | Avg Loss: 0.0187 | Grad Norm: 0.00306412\n",
      "Epoch 3 | Step 96600 | Avg Loss: 0.0187 | Grad Norm: 0.00321844\n",
      "Epoch 3 | Step 96700 | Avg Loss: 0.0189 | Grad Norm: 0.00381484\n",
      "Epoch 3 | Step 96800 | Avg Loss: 0.0191 | Grad Norm: 0.00260869\n",
      "Epoch 3 | Step 96900 | Avg Loss: 0.0192 | Grad Norm: 0.00321109\n",
      "Epoch 3 | Step 97000 | Avg Loss: 0.0193 | Grad Norm: 0.00348060\n",
      "Epoch 3 | Step 97100 | Avg Loss: 0.0194 | Grad Norm: 0.00271023\n",
      "Epoch 3 | Step 97200 | Avg Loss: 0.0196 | Grad Norm: 0.00390274\n",
      "Epoch 3 | Step 97300 | Avg Loss: 0.0195 | Grad Norm: 0.00330099\n",
      "Epoch 3 | Step 97400 | Avg Loss: 0.0193 | Grad Norm: 0.00330525\n",
      "Epoch 3 | Step 97500 | Avg Loss: 0.0187 | Grad Norm: 0.00364390\n",
      "Epoch 3 | Step 97600 | Avg Loss: 0.0185 | Grad Norm: 0.00490623\n",
      "Epoch 3 | Step 97700 | Avg Loss: 0.0181 | Grad Norm: 0.00301325\n",
      "Epoch 3 | Step 97800 | Avg Loss: 0.0185 | Grad Norm: 0.00307843\n",
      "Epoch 3 | Step 97900 | Avg Loss: 0.0181 | Grad Norm: 0.00254709\n",
      "Epoch 3 | Step 98000 | Avg Loss: 0.0181 | Grad Norm: 0.00286503\n",
      "Epoch 3 | Step 98100 | Avg Loss: 0.0184 | Grad Norm: 0.00323278\n",
      "Epoch 3 | Step 98200 | Avg Loss: 0.0186 | Grad Norm: 0.00339237\n",
      "Epoch 3 | Step 98300 | Avg Loss: 0.0178 | Grad Norm: 0.00272378\n",
      "Epoch 3 | Step 98400 | Avg Loss: 0.0182 | Grad Norm: 0.00357114\n",
      "Epoch 3 | Step 98500 | Avg Loss: 0.0186 | Grad Norm: 0.00367964\n",
      "Epoch 3 | Step 98600 | Avg Loss: 0.0188 | Grad Norm: 0.00354398\n",
      "Epoch 3 | Step 98700 | Avg Loss: 0.0189 | Grad Norm: 0.00246245\n",
      "Epoch 3 | Step 98800 | Avg Loss: 0.0189 | Grad Norm: 0.00336762\n",
      "Epoch 3 | Step 98900 | Avg Loss: 0.0189 | Grad Norm: 0.00256616\n",
      "Epoch 3 | Step 99000 | Avg Loss: 0.0186 | Grad Norm: 0.00367562\n",
      "Epoch 3 | Step 99100 | Avg Loss: 0.0186 | Grad Norm: 0.00251505\n",
      "Epoch 3 | Step 99200 | Avg Loss: 0.0185 | Grad Norm: 0.00297947\n",
      "Epoch 3 | Step 99300 | Avg Loss: 0.0187 | Grad Norm: 0.00356430\n",
      "Epoch 3 | Step 99400 | Avg Loss: 0.0185 | Grad Norm: 0.00423404\n",
      "Epoch 3 | Step 99500 | Avg Loss: 0.0188 | Grad Norm: 0.00263569\n",
      "Epoch 3 | Step 99600 | Avg Loss: 0.0192 | Grad Norm: 0.00379115\n",
      "Epoch 3 | Step 99700 | Avg Loss: 0.0192 | Grad Norm: 0.00398134\n",
      "Epoch 3 | Step 99800 | Avg Loss: 0.0189 | Grad Norm: 0.00302187\n",
      "Epoch 3 | Step 99900 | Avg Loss: 0.0193 | Grad Norm: 0.00279299\n",
      "Epoch 3 | Step 100000 | Avg Loss: 0.0193 | Grad Norm: 0.00293122\n",
      "Saving model at step100000\n",
      "Epoch 3 | Step 100100 | Avg Loss: 0.0190 | Grad Norm: 0.00282178\n",
      "Epoch 3 | Step 100200 | Avg Loss: 0.0186 | Grad Norm: 0.00285144\n",
      "Epoch 3 | Step 100300 | Avg Loss: 0.0187 | Grad Norm: 0.00350722\n",
      "Epoch 3 | Step 100400 | Avg Loss: 0.0188 | Grad Norm: 0.00276937\n",
      "Epoch 3 | Step 100500 | Avg Loss: 0.0189 | Grad Norm: 0.00302380\n",
      "Epoch 3 | Step 100600 | Avg Loss: 0.0188 | Grad Norm: 0.00349061\n",
      "Epoch 3 | Step 100700 | Avg Loss: 0.0184 | Grad Norm: 0.00276380\n",
      "Epoch 3 | Step 100800 | Avg Loss: 0.0186 | Grad Norm: 0.00282487\n",
      "Epoch 3 | Step 100900 | Avg Loss: 0.0189 | Grad Norm: 0.00359897\n",
      "Epoch 3 | Step 101000 | Avg Loss: 0.0191 | Grad Norm: 0.00337955\n",
      "Epoch 3 | Step 101100 | Avg Loss: 0.0187 | Grad Norm: 0.00362963\n",
      "Epoch 3 | Step 101200 | Avg Loss: 0.0185 | Grad Norm: 0.00285923\n",
      "Epoch 3 | Step 101300 | Avg Loss: 0.0183 | Grad Norm: 0.00279058\n",
      "Epoch 3 | Step 101400 | Avg Loss: 0.0182 | Grad Norm: 0.00320794\n",
      "Epoch 3 | Step 101500 | Avg Loss: 0.0184 | Grad Norm: 0.00295673\n",
      "Epoch 3 | Step 101600 | Avg Loss: 0.0188 | Grad Norm: 0.00284572\n",
      "Epoch 3 | Step 101700 | Avg Loss: 0.0193 | Grad Norm: 0.00300123\n",
      "Epoch 3 | Step 101800 | Avg Loss: 0.0192 | Grad Norm: 0.00294100\n",
      "Epoch 3 | Step 101900 | Avg Loss: 0.0191 | Grad Norm: 0.00315968\n",
      "Epoch 3 | Step 102000 | Avg Loss: 0.0190 | Grad Norm: 0.00411201\n",
      "Epoch 3 | Step 102100 | Avg Loss: 0.0189 | Grad Norm: 0.00323266\n",
      "Epoch 3 | Step 102200 | Avg Loss: 0.0187 | Grad Norm: 0.00294096\n",
      "Epoch 3 | Step 102300 | Avg Loss: 0.0192 | Grad Norm: 0.00284719\n",
      "Epoch 3 | Step 102400 | Avg Loss: 0.0185 | Grad Norm: 0.00308577\n",
      "Epoch 3 | Step 102500 | Avg Loss: 0.0184 | Grad Norm: 0.00388368\n",
      "Epoch 3 | Step 102600 | Avg Loss: 0.0184 | Grad Norm: 0.00320334\n",
      "Epoch 3 | Step 102700 | Avg Loss: 0.0185 | Grad Norm: 0.00276551\n",
      "Epoch 3 | Step 102800 | Avg Loss: 0.0192 | Grad Norm: 0.00260321\n",
      "Epoch 3 | Step 102900 | Avg Loss: 0.0193 | Grad Norm: 0.00309664\n",
      "Epoch 3 | Step 103000 | Avg Loss: 0.0187 | Grad Norm: 0.00301435\n",
      "Epoch 3 | Step 103100 | Avg Loss: 0.0189 | Grad Norm: 0.00269146\n",
      "Epoch 3 | Step 103200 | Avg Loss: 0.0190 | Grad Norm: 0.00266575\n",
      "Epoch 3 | Step 103300 | Avg Loss: 0.0193 | Grad Norm: 0.00445551\n",
      "Epoch 3 | Step 103400 | Avg Loss: 0.0196 | Grad Norm: 0.00285598\n",
      "Epoch 3 | Step 103500 | Avg Loss: 0.0195 | Grad Norm: 0.00366655\n",
      "Epoch 3 | Step 103600 | Avg Loss: 0.0196 | Grad Norm: 0.00288715\n",
      "Epoch 3 | Step 103700 | Avg Loss: 0.0193 | Grad Norm: 0.00323525\n",
      "Epoch 3 | Step 103800 | Avg Loss: 0.0191 | Grad Norm: 0.00638228\n",
      "Epoch 3 | Step 103900 | Avg Loss: 0.0195 | Grad Norm: 0.00402655\n",
      "Epoch 3 | Step 104000 | Avg Loss: 0.0196 | Grad Norm: 0.00504506\n",
      "Epoch 3 | Step 104100 | Avg Loss: 0.0193 | Grad Norm: 0.00464755\n",
      "Epoch 3 | Step 104200 | Avg Loss: 0.0188 | Grad Norm: 0.00252073\n",
      "Epoch 3 | Step 104300 | Avg Loss: 0.0184 | Grad Norm: 0.00314256\n",
      "Epoch 3 | Step 104400 | Avg Loss: 0.0189 | Grad Norm: 0.00291905\n",
      "Epoch 3 | Step 104500 | Avg Loss: 0.0185 | Grad Norm: 0.00228045\n",
      "Epoch 3 | Step 104600 | Avg Loss: 0.0185 | Grad Norm: 0.00355731\n",
      "Epoch 3 | Step 104700 | Avg Loss: 0.0183 | Grad Norm: 0.00413025\n",
      "Epoch 3 | Step 104800 | Avg Loss: 0.0188 | Grad Norm: 0.00382256\n",
      "Epoch 3 | Step 104900 | Avg Loss: 0.0188 | Grad Norm: 0.00383577\n",
      "Epoch 3 | Step 105000 | Avg Loss: 0.0189 | Grad Norm: 0.00419103\n",
      "Epoch 3 | Step 105100 | Avg Loss: 0.0189 | Grad Norm: 0.00260639\n",
      "Epoch 3 | Step 105200 | Avg Loss: 0.0189 | Grad Norm: 0.00370323\n",
      "Epoch 3 | Step 105300 | Avg Loss: 0.0188 | Grad Norm: 0.00280391\n",
      "Epoch 3 | Step 105400 | Avg Loss: 0.0183 | Grad Norm: 0.00305493\n",
      "Epoch 3 | Step 105500 | Avg Loss: 0.0188 | Grad Norm: 0.00352339\n",
      "Epoch 3 | Step 105600 | Avg Loss: 0.0195 | Grad Norm: 0.00378271\n",
      "Epoch 3 | Step 105700 | Avg Loss: 0.0192 | Grad Norm: 0.00297496\n",
      "Epoch 3 | Step 105800 | Avg Loss: 0.0189 | Grad Norm: 0.00302657\n",
      "Epoch 3 | Step 105900 | Avg Loss: 0.0191 | Grad Norm: 0.00336607\n",
      "Epoch 3 | Step 106000 | Avg Loss: 0.0191 | Grad Norm: 0.00374583\n",
      "Epoch 3 | Step 106100 | Avg Loss: 0.0189 | Grad Norm: 0.00438345\n",
      "Epoch 3 | Step 106200 | Avg Loss: 0.0190 | Grad Norm: 0.00340238\n",
      "Epoch 3 | Step 106300 | Avg Loss: 0.0192 | Grad Norm: 0.00391672\n",
      "Epoch 3 | Step 106400 | Avg Loss: 0.0183 | Grad Norm: 0.00324000\n",
      "Epoch 3 | Step 106500 | Avg Loss: 0.0185 | Grad Norm: 0.00540568\n",
      "Epoch 3 | Step 106600 | Avg Loss: 0.0189 | Grad Norm: 0.00285022\n",
      "Epoch 3 | Step 106700 | Avg Loss: 0.0195 | Grad Norm: 0.00278109\n",
      "Epoch 3 | Step 106800 | Avg Loss: 0.0189 | Grad Norm: 0.00295519\n",
      "Epoch 3 | Step 106900 | Avg Loss: 0.0185 | Grad Norm: 0.00314486\n",
      "Epoch 3 | Step 107000 | Avg Loss: 0.0184 | Grad Norm: 0.00293391\n",
      "Epoch 3 | Step 107100 | Avg Loss: 0.0185 | Grad Norm: 0.00313284\n",
      "Epoch 3 | Step 107200 | Avg Loss: 0.0190 | Grad Norm: 0.00353718\n",
      "Epoch 3 | Step 107300 | Avg Loss: 0.0190 | Grad Norm: 0.00333397\n",
      "Epoch 3 | Step 107400 | Avg Loss: 0.0188 | Grad Norm: 0.00319619\n",
      "Epoch 3 | Step 107500 | Avg Loss: 0.0186 | Grad Norm: 0.00320936\n",
      "Epoch 3 | Step 107600 | Avg Loss: 0.0187 | Grad Norm: 0.00259965\n",
      "Epoch 3 | Step 107700 | Avg Loss: 0.0186 | Grad Norm: 0.00485526\n",
      "Epoch 3 | Step 107800 | Avg Loss: 0.0188 | Grad Norm: 0.00283543\n",
      "Epoch 3 | Step 107900 | Avg Loss: 0.0188 | Grad Norm: 0.00335307\n",
      "Epoch 3 | Step 108000 | Avg Loss: 0.0190 | Grad Norm: 0.00296510\n",
      "Epoch 3 | Step 108100 | Avg Loss: 0.0192 | Grad Norm: 0.00358039\n",
      "Epoch 3 | Step 108200 | Avg Loss: 0.0192 | Grad Norm: 0.00349906\n",
      "Epoch 3 | Step 108300 | Avg Loss: 0.0195 | Grad Norm: 0.00279099\n",
      "Epoch 3 | Step 108400 | Avg Loss: 0.0194 | Grad Norm: 0.00305672\n",
      "Epoch 3 | Step 108500 | Avg Loss: 0.0196 | Grad Norm: 0.00300207\n",
      "Epoch 3 | Step 108600 | Avg Loss: 0.0189 | Grad Norm: 0.00297826\n",
      "Epoch 3 | Step 108700 | Avg Loss: 0.0185 | Grad Norm: 0.00292476\n",
      "Epoch 3 | Step 108800 | Avg Loss: 0.0183 | Grad Norm: 0.00350319\n",
      "Epoch 3 | Step 108900 | Avg Loss: 0.0188 | Grad Norm: 0.00301560\n",
      "Epoch 3 | Step 109000 | Avg Loss: 0.0188 | Grad Norm: 0.00361607\n",
      "Epoch 3 | Step 109100 | Avg Loss: 0.0185 | Grad Norm: 0.00365929\n",
      "Epoch 3 | Step 109200 | Avg Loss: 0.0189 | Grad Norm: 0.00331310\n",
      "Epoch 3 | Step 109300 | Avg Loss: 0.0183 | Grad Norm: 0.00362185\n",
      "Epoch 3 | Step 109400 | Avg Loss: 0.0182 | Grad Norm: 0.00473839\n",
      "Epoch 3 | Step 109500 | Avg Loss: 0.0186 | Grad Norm: 0.00325886\n",
      "Epoch 3 | Step 109600 | Avg Loss: 0.0188 | Grad Norm: 0.00392841\n",
      "Epoch 3 | Step 109700 | Avg Loss: 0.0182 | Grad Norm: 0.00364590\n",
      "Epoch 3 | Step 109800 | Avg Loss: 0.0181 | Grad Norm: 0.00330775\n",
      "Epoch 3 | Step 109900 | Avg Loss: 0.0181 | Grad Norm: 0.00289887\n",
      "Epoch 3 | Step 110000 | Avg Loss: 0.0191 | Grad Norm: 0.00356385\n",
      "Epoch 3 | Step 110100 | Avg Loss: 0.0191 | Grad Norm: 0.00446530\n",
      "Epoch 3 | Step 110200 | Avg Loss: 0.0190 | Grad Norm: 0.00287604\n",
      "Epoch 3 | Step 110300 | Avg Loss: 0.0188 | Grad Norm: 0.00351840\n",
      "Epoch 3 | Step 110400 | Avg Loss: 0.0193 | Grad Norm: 0.00309247\n",
      "Epoch 3 | Step 110500 | Avg Loss: 0.0194 | Grad Norm: 0.00356107\n",
      "Epoch 3 | Step 110600 | Avg Loss: 0.0193 | Grad Norm: 0.00301824\n",
      "Epoch 3 | Step 110700 | Avg Loss: 0.0189 | Grad Norm: 0.00287472\n",
      "Epoch 3 | Step 110800 | Avg Loss: 0.0191 | Grad Norm: 0.00393853\n",
      "Epoch 3 | Step 110900 | Avg Loss: 0.0191 | Grad Norm: 0.00299659\n",
      "Epoch 3 | Step 111000 | Avg Loss: 0.0188 | Grad Norm: 0.00415930\n",
      "Epoch 3 | Step 111100 | Avg Loss: 0.0191 | Grad Norm: 0.00339150\n",
      "Epoch 3 | Step 111200 | Avg Loss: 0.0184 | Grad Norm: 0.00365483\n",
      "Epoch 3 | Step 111300 | Avg Loss: 0.0184 | Grad Norm: 0.00368199\n",
      "Epoch 3 | Step 111400 | Avg Loss: 0.0182 | Grad Norm: 0.00284175\n",
      "Epoch 3 | Step 111500 | Avg Loss: 0.0186 | Grad Norm: 0.00304017\n",
      "Epoch 3 | Step 111600 | Avg Loss: 0.0187 | Grad Norm: 0.00427311\n",
      "Epoch 3 | Step 111700 | Avg Loss: 0.0189 | Grad Norm: 0.00312345\n",
      "Epoch 3 | Step 111800 | Avg Loss: 0.0185 | Grad Norm: 0.00333246\n",
      "Epoch 3 | Step 111900 | Avg Loss: 0.0187 | Grad Norm: 0.00314138\n",
      "Epoch 3 | Step 112000 | Avg Loss: 0.0184 | Grad Norm: 0.00460653\n",
      "Epoch 3 | Step 112100 | Avg Loss: 0.0182 | Grad Norm: 0.00325275\n",
      "Epoch 3 | Step 112200 | Avg Loss: 0.0176 | Grad Norm: 0.00347238\n",
      "Epoch 3 | Step 112300 | Avg Loss: 0.0177 | Grad Norm: 0.00275835\n",
      "Epoch 3 | Step 112400 | Avg Loss: 0.0175 | Grad Norm: 0.00286585\n",
      "Epoch 3 | Step 112500 | Avg Loss: 0.0183 | Grad Norm: 0.00279710\n",
      "Epoch 3 | Step 112600 | Avg Loss: 0.0183 | Grad Norm: 0.00261706\n",
      "Epoch 3 | Step 112700 | Avg Loss: 0.0189 | Grad Norm: 0.00242708\n",
      "Epoch 3 | Step 112800 | Avg Loss: 0.0186 | Grad Norm: 0.00328024\n",
      "Epoch 3 | Step 112900 | Avg Loss: 0.0184 | Grad Norm: 0.00321527\n",
      "Epoch 3 | Step 113000 | Avg Loss: 0.0184 | Grad Norm: 0.00364833\n",
      "Epoch 3 | Step 113100 | Avg Loss: 0.0184 | Grad Norm: 0.00392976\n",
      "Epoch 3 | Step 113200 | Avg Loss: 0.0183 | Grad Norm: 0.00316207\n",
      "Epoch 3 | Step 113300 | Avg Loss: 0.0190 | Grad Norm: 0.00287740\n",
      "Epoch 3 | Step 113400 | Avg Loss: 0.0189 | Grad Norm: 0.00340414\n",
      "Epoch 3 | Step 113500 | Avg Loss: 0.0187 | Grad Norm: 0.00302676\n",
      "Epoch 3 | Step 113600 | Avg Loss: 0.0190 | Grad Norm: 0.00353347\n",
      "Epoch 3 | Step 113700 | Avg Loss: 0.0194 | Grad Norm: 0.00376511\n",
      "Epoch 3 | Step 113800 | Avg Loss: 0.0193 | Grad Norm: 0.00325013\n",
      "Epoch 3 | Step 113900 | Avg Loss: 0.0188 | Grad Norm: 0.00378196\n",
      "Epoch 3 | Step 114000 | Avg Loss: 0.0186 | Grad Norm: 0.00407421\n",
      "Epoch 3 | Step 114100 | Avg Loss: 0.0183 | Grad Norm: 0.00629476\n",
      "Epoch 3 | Step 114200 | Avg Loss: 0.0182 | Grad Norm: 0.00673075\n",
      "Epoch 3 | Step 114300 | Avg Loss: 0.0187 | Grad Norm: 0.00319446\n",
      "Epoch 3 | Step 114400 | Avg Loss: 0.0191 | Grad Norm: 0.00288690\n",
      "Epoch 3 | Step 114500 | Avg Loss: 0.0193 | Grad Norm: 0.00332509\n",
      "Epoch 3 | Step 114600 | Avg Loss: 0.0192 | Grad Norm: 0.00332604\n",
      "Epoch 3 | Step 114700 | Avg Loss: 0.0191 | Grad Norm: 0.00790425\n",
      "Epoch 3 | Step 114800 | Avg Loss: 0.0191 | Grad Norm: 0.00291895\n",
      "Epoch 3 | Step 114900 | Avg Loss: 0.0190 | Grad Norm: 0.00299472\n",
      "Epoch 3 | Step 115000 | Avg Loss: 0.0189 | Grad Norm: 0.00499992\n",
      "Epoch 3 | Step 115100 | Avg Loss: 0.0196 | Grad Norm: 0.00299492\n",
      "Epoch 3 | Step 115200 | Avg Loss: 0.0194 | Grad Norm: 0.00292120\n",
      "Epoch 3 | Step 115300 | Avg Loss: 0.0191 | Grad Norm: 0.00298711\n",
      "Epoch 3 | Step 115400 | Avg Loss: 0.0187 | Grad Norm: 0.00349926\n",
      "Epoch 3 | Step 115500 | Avg Loss: 0.0189 | Grad Norm: 0.00285920\n",
      "Epoch 3 | Step 115600 | Avg Loss: 0.0188 | Grad Norm: 0.00296316\n",
      "Epoch 3 | Step 115700 | Avg Loss: 0.0186 | Grad Norm: 0.00604977\n",
      "Epoch 3 | Step 115800 | Avg Loss: 0.0186 | Grad Norm: 0.00313794\n",
      "Epoch 3 | Step 115900 | Avg Loss: 0.0188 | Grad Norm: 0.00372840\n",
      "Epoch 3 | Step 116000 | Avg Loss: 0.0185 | Grad Norm: 0.00320738\n",
      "Epoch 3 | Step 116100 | Avg Loss: 0.0192 | Grad Norm: 0.00294336\n",
      "Epoch 3 | Step 116200 | Avg Loss: 0.0191 | Grad Norm: 0.00548590\n",
      "Epoch 3 | Step 116300 | Avg Loss: 0.0193 | Grad Norm: 0.00315027\n",
      "Epoch 3 | Step 116400 | Avg Loss: 0.0193 | Grad Norm: 0.00356121\n",
      "Epoch 3 | Step 116500 | Avg Loss: 0.0194 | Grad Norm: 0.00282924\n",
      "Epoch 3 | Step 116600 | Avg Loss: 0.0194 | Grad Norm: 0.00303314\n",
      "Epoch 3 | Step 116700 | Avg Loss: 0.0191 | Grad Norm: 0.00252694\n",
      "Epoch 3 | Step 116800 | Avg Loss: 0.0191 | Grad Norm: 0.00386035\n",
      "Epoch 3 | Step 116900 | Avg Loss: 0.0191 | Grad Norm: 0.00294767\n",
      "Epoch 3 | Step 117000 | Avg Loss: 0.0188 | Grad Norm: 0.00266785\n",
      "Epoch 3 | Step 117100 | Avg Loss: 0.0185 | Grad Norm: 0.00370773\n",
      "Epoch 3, Loss: 0.0179\n",
      "Epoch 4 | Step 117200 | Avg Loss: 0.0184 | Grad Norm: 0.00375355\n",
      "Epoch 4 | Step 117300 | Avg Loss: 0.0188 | Grad Norm: 0.00375438\n",
      "Epoch 4 | Step 117400 | Avg Loss: 0.0181 | Grad Norm: 0.00383377\n",
      "Epoch 4 | Step 117500 | Avg Loss: 0.0180 | Grad Norm: 0.00440867\n",
      "Epoch 4 | Step 117600 | Avg Loss: 0.0180 | Grad Norm: 0.00423415\n",
      "Epoch 4 | Step 117700 | Avg Loss: 0.0181 | Grad Norm: 0.00319946\n",
      "Epoch 4 | Step 117800 | Avg Loss: 0.0186 | Grad Norm: 0.00332125\n",
      "Epoch 4 | Step 117900 | Avg Loss: 0.0189 | Grad Norm: 0.00289187\n",
      "Epoch 4 | Step 118000 | Avg Loss: 0.0194 | Grad Norm: 0.00449409\n",
      "Epoch 4 | Step 118100 | Avg Loss: 0.0196 | Grad Norm: 0.00304029\n",
      "Epoch 4 | Step 118200 | Avg Loss: 0.0192 | Grad Norm: 0.00346175\n",
      "Epoch 4 | Step 118300 | Avg Loss: 0.0188 | Grad Norm: 0.00264607\n",
      "Epoch 4 | Step 118400 | Avg Loss: 0.0187 | Grad Norm: 0.00362580\n",
      "Epoch 4 | Step 118500 | Avg Loss: 0.0186 | Grad Norm: 0.00321563\n",
      "Epoch 4 | Step 118600 | Avg Loss: 0.0187 | Grad Norm: 0.00309179\n",
      "Epoch 4 | Step 118700 | Avg Loss: 0.0185 | Grad Norm: 0.00308048\n",
      "Epoch 4 | Step 118800 | Avg Loss: 0.0182 | Grad Norm: 0.00445103\n",
      "Epoch 4 | Step 118900 | Avg Loss: 0.0182 | Grad Norm: 0.00298151\n",
      "Epoch 4 | Step 119000 | Avg Loss: 0.0181 | Grad Norm: 0.00276684\n",
      "Epoch 4 | Step 119100 | Avg Loss: 0.0184 | Grad Norm: 0.00331962\n",
      "Epoch 4 | Step 119200 | Avg Loss: 0.0183 | Grad Norm: 0.00299531\n",
      "Epoch 4 | Step 119300 | Avg Loss: 0.0182 | Grad Norm: 0.00283449\n",
      "Epoch 4 | Step 119400 | Avg Loss: 0.0184 | Grad Norm: 0.00500226\n",
      "Epoch 4 | Step 119500 | Avg Loss: 0.0185 | Grad Norm: 0.00365997\n",
      "Epoch 4 | Step 119600 | Avg Loss: 0.0183 | Grad Norm: 0.00269702\n",
      "Epoch 4 | Step 119700 | Avg Loss: 0.0187 | Grad Norm: 0.00291425\n",
      "Epoch 4 | Step 119800 | Avg Loss: 0.0189 | Grad Norm: 0.00280068\n",
      "Epoch 4 | Step 119900 | Avg Loss: 0.0190 | Grad Norm: 0.00439026\n",
      "Epoch 4 | Step 120000 | Avg Loss: 0.0188 | Grad Norm: 0.00273566\n",
      "Epoch 4 | Step 120100 | Avg Loss: 0.0189 | Grad Norm: 0.00284970\n",
      "Epoch 4 | Step 120200 | Avg Loss: 0.0189 | Grad Norm: 0.00268664\n",
      "Epoch 4 | Step 120300 | Avg Loss: 0.0187 | Grad Norm: 0.00371197\n",
      "Epoch 4 | Step 120400 | Avg Loss: 0.0185 | Grad Norm: 0.00314402\n",
      "Epoch 4 | Step 120500 | Avg Loss: 0.0184 | Grad Norm: 0.00412663\n",
      "Epoch 4 | Step 120600 | Avg Loss: 0.0181 | Grad Norm: 0.00455995\n",
      "Epoch 4 | Step 120700 | Avg Loss: 0.0176 | Grad Norm: 0.00293977\n",
      "Epoch 4 | Step 120800 | Avg Loss: 0.0176 | Grad Norm: 0.00316221\n",
      "Epoch 4 | Step 120900 | Avg Loss: 0.0181 | Grad Norm: 0.00630682\n",
      "Epoch 4 | Step 121000 | Avg Loss: 0.0186 | Grad Norm: 0.00362569\n",
      "Epoch 4 | Step 121100 | Avg Loss: 0.0184 | Grad Norm: 0.00275119\n",
      "Epoch 4 | Step 121200 | Avg Loss: 0.0189 | Grad Norm: 0.00249653\n",
      "Epoch 4 | Step 121300 | Avg Loss: 0.0183 | Grad Norm: 0.00554996\n",
      "Epoch 4 | Step 121400 | Avg Loss: 0.0183 | Grad Norm: 0.00325541\n",
      "Epoch 4 | Step 121500 | Avg Loss: 0.0184 | Grad Norm: 0.00251466\n",
      "Epoch 4 | Step 121600 | Avg Loss: 0.0186 | Grad Norm: 0.00327644\n",
      "Epoch 4 | Step 121700 | Avg Loss: 0.0184 | Grad Norm: 0.00348952\n",
      "Epoch 4 | Step 121800 | Avg Loss: 0.0177 | Grad Norm: 0.00279034\n",
      "Epoch 4 | Step 121900 | Avg Loss: 0.0178 | Grad Norm: 0.00336724\n",
      "Epoch 4 | Step 122000 | Avg Loss: 0.0179 | Grad Norm: 0.00417784\n",
      "Epoch 4 | Step 122100 | Avg Loss: 0.0182 | Grad Norm: 0.00355928\n",
      "Epoch 4 | Step 122200 | Avg Loss: 0.0182 | Grad Norm: 0.00272980\n",
      "Epoch 4 | Step 122300 | Avg Loss: 0.0181 | Grad Norm: 0.00320641\n",
      "Epoch 4 | Step 122400 | Avg Loss: 0.0182 | Grad Norm: 0.00301278\n",
      "Epoch 4 | Step 122500 | Avg Loss: 0.0186 | Grad Norm: 0.00313168\n",
      "Epoch 4 | Step 122600 | Avg Loss: 0.0193 | Grad Norm: 0.00387989\n",
      "Epoch 4 | Step 122700 | Avg Loss: 0.0191 | Grad Norm: 0.00440070\n",
      "Epoch 4 | Step 122800 | Avg Loss: 0.0191 | Grad Norm: 0.00324941\n",
      "Epoch 4 | Step 122900 | Avg Loss: 0.0186 | Grad Norm: 0.00609076\n",
      "Epoch 4 | Step 123000 | Avg Loss: 0.0190 | Grad Norm: 0.00316503\n",
      "Epoch 4 | Step 123100 | Avg Loss: 0.0185 | Grad Norm: 0.00294133\n",
      "Epoch 4 | Step 123200 | Avg Loss: 0.0187 | Grad Norm: 0.00296374\n",
      "Epoch 4 | Step 123300 | Avg Loss: 0.0182 | Grad Norm: 0.00363711\n",
      "Epoch 4 | Step 123400 | Avg Loss: 0.0183 | Grad Norm: 0.00436048\n",
      "Epoch 4 | Step 123500 | Avg Loss: 0.0189 | Grad Norm: 0.00264017\n",
      "Epoch 4 | Step 123600 | Avg Loss: 0.0186 | Grad Norm: 0.00326314\n",
      "Epoch 4 | Step 123700 | Avg Loss: 0.0187 | Grad Norm: 0.00312610\n",
      "Epoch 4 | Step 123800 | Avg Loss: 0.0183 | Grad Norm: 0.00387719\n",
      "Epoch 4 | Step 123900 | Avg Loss: 0.0186 | Grad Norm: 0.00551321\n",
      "Epoch 4 | Step 124000 | Avg Loss: 0.0185 | Grad Norm: 0.00297848\n",
      "Epoch 4 | Step 124100 | Avg Loss: 0.0183 | Grad Norm: 0.00421854\n",
      "Epoch 4 | Step 124200 | Avg Loss: 0.0184 | Grad Norm: 0.00314304\n",
      "Epoch 4 | Step 124300 | Avg Loss: 0.0185 | Grad Norm: 0.00282261\n",
      "Epoch 4 | Step 124400 | Avg Loss: 0.0178 | Grad Norm: 0.00420384\n",
      "Epoch 4 | Step 124500 | Avg Loss: 0.0178 | Grad Norm: 0.00652443\n",
      "Epoch 4 | Step 124600 | Avg Loss: 0.0180 | Grad Norm: 0.00336200\n",
      "Epoch 4 | Step 124700 | Avg Loss: 0.0179 | Grad Norm: 0.00317009\n",
      "Epoch 4 | Step 124800 | Avg Loss: 0.0182 | Grad Norm: 0.00288605\n",
      "Epoch 4 | Step 124900 | Avg Loss: 0.0181 | Grad Norm: 0.00297046\n",
      "Epoch 4 | Step 125000 | Avg Loss: 0.0178 | Grad Norm: 0.00290727\n",
      "Epoch 4 | Step 125100 | Avg Loss: 0.0183 | Grad Norm: 0.00301263\n",
      "Epoch 4 | Step 125200 | Avg Loss: 0.0186 | Grad Norm: 0.00258934\n",
      "Epoch 4 | Step 125300 | Avg Loss: 0.0187 | Grad Norm: 0.00321020\n",
      "Epoch 4 | Step 125400 | Avg Loss: 0.0188 | Grad Norm: 0.00326421\n",
      "Epoch 4 | Step 125500 | Avg Loss: 0.0189 | Grad Norm: 0.00255679\n",
      "Epoch 4 | Step 125600 | Avg Loss: 0.0187 | Grad Norm: 0.00344910\n",
      "Epoch 4 | Step 125700 | Avg Loss: 0.0187 | Grad Norm: 0.00399215\n",
      "Epoch 4 | Step 125800 | Avg Loss: 0.0192 | Grad Norm: 0.00379170\n",
      "Epoch 4 | Step 125900 | Avg Loss: 0.0192 | Grad Norm: 0.00328972\n",
      "Epoch 4 | Step 126000 | Avg Loss: 0.0189 | Grad Norm: 0.00398663\n",
      "Epoch 4 | Step 126100 | Avg Loss: 0.0188 | Grad Norm: 0.00241256\n",
      "Epoch 4 | Step 126200 | Avg Loss: 0.0190 | Grad Norm: 0.00301182\n",
      "Epoch 4 | Step 126300 | Avg Loss: 0.0187 | Grad Norm: 0.00263051\n",
      "Epoch 4 | Step 126400 | Avg Loss: 0.0186 | Grad Norm: 0.00275082\n",
      "Epoch 4 | Step 126500 | Avg Loss: 0.0181 | Grad Norm: 0.00282857\n",
      "Epoch 4 | Step 126600 | Avg Loss: 0.0182 | Grad Norm: 0.00293023\n",
      "Epoch 4 | Step 126700 | Avg Loss: 0.0184 | Grad Norm: 0.00339278\n",
      "Epoch 4 | Step 126800 | Avg Loss: 0.0185 | Grad Norm: 0.00484332\n",
      "Epoch 4 | Step 126900 | Avg Loss: 0.0184 | Grad Norm: 0.00363106\n",
      "Epoch 4 | Step 127000 | Avg Loss: 0.0185 | Grad Norm: 0.00420662\n",
      "Epoch 4 | Step 127100 | Avg Loss: 0.0186 | Grad Norm: 0.00307350\n",
      "Epoch 4 | Step 127200 | Avg Loss: 0.0188 | Grad Norm: 0.00303250\n",
      "Epoch 4 | Step 127300 | Avg Loss: 0.0190 | Grad Norm: 0.00290436\n",
      "Epoch 4 | Step 127400 | Avg Loss: 0.0188 | Grad Norm: 0.00377043\n",
      "Epoch 4 | Step 127500 | Avg Loss: 0.0186 | Grad Norm: 0.00321810\n",
      "Epoch 4 | Step 127600 | Avg Loss: 0.0187 | Grad Norm: 0.00443496\n",
      "Epoch 4 | Step 127700 | Avg Loss: 0.0191 | Grad Norm: 0.00464877\n",
      "Epoch 4 | Step 127800 | Avg Loss: 0.0190 | Grad Norm: 0.00313954\n",
      "Epoch 4 | Step 127900 | Avg Loss: 0.0189 | Grad Norm: 0.00512621\n",
      "Epoch 4 | Step 128000 | Avg Loss: 0.0192 | Grad Norm: 0.00253932\n",
      "Epoch 4 | Step 128100 | Avg Loss: 0.0188 | Grad Norm: 0.00658593\n",
      "Epoch 4 | Step 128200 | Avg Loss: 0.0189 | Grad Norm: 0.00358729\n",
      "Epoch 4 | Step 128300 | Avg Loss: 0.0194 | Grad Norm: 0.00344866\n",
      "Epoch 4 | Step 128400 | Avg Loss: 0.0189 | Grad Norm: 0.00365669\n",
      "Epoch 4 | Step 128500 | Avg Loss: 0.0190 | Grad Norm: 0.00282000\n",
      "Epoch 4 | Step 128600 | Avg Loss: 0.0188 | Grad Norm: 0.00357314\n",
      "Epoch 4 | Step 128700 | Avg Loss: 0.0190 | Grad Norm: 0.00363108\n",
      "Epoch 4 | Step 128800 | Avg Loss: 0.0187 | Grad Norm: 0.00283698\n",
      "Epoch 4 | Step 128900 | Avg Loss: 0.0186 | Grad Norm: 0.00358661\n",
      "Epoch 4 | Step 129000 | Avg Loss: 0.0184 | Grad Norm: 0.00304292\n",
      "Epoch 4 | Step 129100 | Avg Loss: 0.0180 | Grad Norm: 0.00269878\n",
      "Epoch 4 | Step 129200 | Avg Loss: 0.0179 | Grad Norm: 0.00540802\n",
      "Epoch 4 | Step 129300 | Avg Loss: 0.0183 | Grad Norm: 0.00295968\n",
      "Epoch 4 | Step 129400 | Avg Loss: 0.0181 | Grad Norm: 0.00343809\n",
      "Epoch 4 | Step 129500 | Avg Loss: 0.0185 | Grad Norm: 0.00287335\n",
      "Epoch 4 | Step 129600 | Avg Loss: 0.0190 | Grad Norm: 0.00321736\n",
      "Epoch 4 | Step 129700 | Avg Loss: 0.0190 | Grad Norm: 0.00276440\n",
      "Epoch 4 | Step 129800 | Avg Loss: 0.0185 | Grad Norm: 0.00290536\n",
      "Epoch 4 | Step 129900 | Avg Loss: 0.0184 | Grad Norm: 0.00279708\n",
      "Epoch 4 | Step 130000 | Avg Loss: 0.0183 | Grad Norm: 0.00385829\n",
      "Epoch 4 | Step 130100 | Avg Loss: 0.0187 | Grad Norm: 0.00319415\n",
      "Epoch 4 | Step 130200 | Avg Loss: 0.0184 | Grad Norm: 0.00426563\n",
      "Epoch 4 | Step 130300 | Avg Loss: 0.0185 | Grad Norm: 0.00271278\n",
      "Epoch 4 | Step 130400 | Avg Loss: 0.0182 | Grad Norm: 0.00480781\n",
      "Epoch 4 | Step 130500 | Avg Loss: 0.0184 | Grad Norm: 0.00279774\n",
      "Epoch 4 | Step 130600 | Avg Loss: 0.0182 | Grad Norm: 0.00286773\n",
      "Epoch 4 | Step 130700 | Avg Loss: 0.0184 | Grad Norm: 0.00412484\n",
      "Epoch 4 | Step 130800 | Avg Loss: 0.0185 | Grad Norm: 0.00365893\n",
      "Epoch 4 | Step 130900 | Avg Loss: 0.0181 | Grad Norm: 0.00274153\n",
      "Epoch 4 | Step 131000 | Avg Loss: 0.0184 | Grad Norm: 0.00289533\n",
      "Epoch 4 | Step 131100 | Avg Loss: 0.0190 | Grad Norm: 0.00291953\n",
      "Epoch 4 | Step 131200 | Avg Loss: 0.0186 | Grad Norm: 0.00336454\n",
      "Epoch 4 | Step 131300 | Avg Loss: 0.0182 | Grad Norm: 0.00402415\n",
      "Epoch 4 | Step 131400 | Avg Loss: 0.0184 | Grad Norm: 0.00422583\n",
      "Epoch 4 | Step 131500 | Avg Loss: 0.0184 | Grad Norm: 0.00292156\n",
      "Epoch 4 | Step 131600 | Avg Loss: 0.0186 | Grad Norm: 0.00344473\n",
      "Epoch 4 | Step 131700 | Avg Loss: 0.0186 | Grad Norm: 0.00292215\n",
      "Epoch 4 | Step 131800 | Avg Loss: 0.0187 | Grad Norm: 0.00351598\n",
      "Epoch 4 | Step 131900 | Avg Loss: 0.0186 | Grad Norm: 0.00284404\n",
      "Epoch 4 | Step 132000 | Avg Loss: 0.0184 | Grad Norm: 0.00466089\n",
      "Epoch 4 | Step 132100 | Avg Loss: 0.0185 | Grad Norm: 0.00288895\n",
      "Epoch 4 | Step 132200 | Avg Loss: 0.0186 | Grad Norm: 0.00617441\n",
      "Epoch 4 | Step 132300 | Avg Loss: 0.0187 | Grad Norm: 0.00301631\n",
      "Epoch 4 | Step 132400 | Avg Loss: 0.0185 | Grad Norm: 0.00314882\n",
      "Epoch 4 | Step 132500 | Avg Loss: 0.0184 | Grad Norm: 0.00306497\n",
      "Epoch 4 | Step 132600 | Avg Loss: 0.0185 | Grad Norm: 0.00327219\n",
      "Epoch 4 | Step 132700 | Avg Loss: 0.0186 | Grad Norm: 0.00399473\n",
      "Epoch 4 | Step 132800 | Avg Loss: 0.0190 | Grad Norm: 0.00355414\n",
      "Epoch 4 | Step 132900 | Avg Loss: 0.0192 | Grad Norm: 0.00407477\n",
      "Epoch 4 | Step 133000 | Avg Loss: 0.0197 | Grad Norm: 0.00363597\n",
      "Epoch 4 | Step 133100 | Avg Loss: 0.0198 | Grad Norm: 0.00281811\n",
      "Epoch 4 | Step 133200 | Avg Loss: 0.0196 | Grad Norm: 0.00475359\n",
      "Epoch 4 | Step 133300 | Avg Loss: 0.0197 | Grad Norm: 0.00370359\n",
      "Epoch 4 | Step 133400 | Avg Loss: 0.0196 | Grad Norm: 0.00465566\n",
      "Epoch 4 | Step 133500 | Avg Loss: 0.0195 | Grad Norm: 0.00407556\n",
      "Epoch 4 | Step 133600 | Avg Loss: 0.0190 | Grad Norm: 0.00284573\n",
      "Epoch 4 | Step 133700 | Avg Loss: 0.0193 | Grad Norm: 0.00328560\n",
      "Epoch 4 | Step 133800 | Avg Loss: 0.0185 | Grad Norm: 0.00343303\n",
      "Epoch 4 | Step 133900 | Avg Loss: 0.0186 | Grad Norm: 0.00308062\n",
      "Epoch 4 | Step 134000 | Avg Loss: 0.0186 | Grad Norm: 0.00466233\n",
      "Epoch 4 | Step 134100 | Avg Loss: 0.0190 | Grad Norm: 0.00297318\n",
      "Epoch 4 | Step 134200 | Avg Loss: 0.0188 | Grad Norm: 0.00308202\n",
      "Epoch 4 | Step 134300 | Avg Loss: 0.0185 | Grad Norm: 0.00374449\n",
      "Epoch 4 | Step 134400 | Avg Loss: 0.0184 | Grad Norm: 0.00345514\n",
      "Epoch 4 | Step 134500 | Avg Loss: 0.0184 | Grad Norm: 0.00299262\n",
      "Epoch 4 | Step 134600 | Avg Loss: 0.0184 | Grad Norm: 0.00377130\n",
      "Epoch 4 | Step 134700 | Avg Loss: 0.0181 | Grad Norm: 0.00351987\n",
      "Epoch 4 | Step 134800 | Avg Loss: 0.0179 | Grad Norm: 0.00302259\n",
      "Epoch 4 | Step 134900 | Avg Loss: 0.0181 | Grad Norm: 0.00295024\n",
      "Epoch 4 | Step 135000 | Avg Loss: 0.0180 | Grad Norm: 0.00300820\n",
      "Epoch 4 | Step 135100 | Avg Loss: 0.0183 | Grad Norm: 0.00300641\n",
      "Epoch 4 | Step 135200 | Avg Loss: 0.0183 | Grad Norm: 0.00256461\n",
      "Epoch 4 | Step 135300 | Avg Loss: 0.0182 | Grad Norm: 0.00266900\n",
      "Epoch 4 | Step 135400 | Avg Loss: 0.0181 | Grad Norm: 0.00322697\n",
      "Epoch 4 | Step 135500 | Avg Loss: 0.0184 | Grad Norm: 0.00302435\n",
      "Epoch 4 | Step 135600 | Avg Loss: 0.0185 | Grad Norm: 0.00359095\n",
      "Epoch 4 | Step 135700 | Avg Loss: 0.0185 | Grad Norm: 0.00333771\n",
      "Epoch 4 | Step 135800 | Avg Loss: 0.0187 | Grad Norm: 0.00365129\n",
      "Epoch 4 | Step 135900 | Avg Loss: 0.0185 | Grad Norm: 0.00359696\n",
      "Epoch 4 | Step 136000 | Avg Loss: 0.0186 | Grad Norm: 0.00399640\n",
      "Epoch 4 | Step 136100 | Avg Loss: 0.0190 | Grad Norm: 0.00264331\n",
      "Epoch 4 | Step 136200 | Avg Loss: 0.0190 | Grad Norm: 0.00360718\n",
      "Epoch 4 | Step 136300 | Avg Loss: 0.0191 | Grad Norm: 0.00284498\n",
      "Epoch 4 | Step 136400 | Avg Loss: 0.0190 | Grad Norm: 0.00287714\n",
      "Epoch 4 | Step 136500 | Avg Loss: 0.0187 | Grad Norm: 0.00340067\n",
      "Epoch 4 | Step 136600 | Avg Loss: 0.0182 | Grad Norm: 0.00304598\n",
      "Epoch 4 | Step 136700 | Avg Loss: 0.0180 | Grad Norm: 0.00435807\n",
      "Epoch 4 | Step 136800 | Avg Loss: 0.0179 | Grad Norm: 0.00337800\n",
      "Epoch 4 | Step 136900 | Avg Loss: 0.0179 | Grad Norm: 0.00279528\n",
      "Epoch 4 | Step 137000 | Avg Loss: 0.0177 | Grad Norm: 0.00316502\n",
      "Epoch 4 | Step 137100 | Avg Loss: 0.0178 | Grad Norm: 0.00268271\n",
      "Epoch 4 | Step 137200 | Avg Loss: 0.0182 | Grad Norm: 0.00266962\n",
      "Epoch 4 | Step 137300 | Avg Loss: 0.0180 | Grad Norm: 0.00279528\n",
      "Epoch 4 | Step 137400 | Avg Loss: 0.0175 | Grad Norm: 0.00312023\n",
      "Epoch 4 | Step 137500 | Avg Loss: 0.0180 | Grad Norm: 0.00380114\n",
      "Epoch 4 | Step 137600 | Avg Loss: 0.0185 | Grad Norm: 0.00375347\n",
      "Epoch 4 | Step 137700 | Avg Loss: 0.0186 | Grad Norm: 0.00429504\n",
      "Epoch 4 | Step 137800 | Avg Loss: 0.0185 | Grad Norm: 0.00352671\n",
      "Epoch 4 | Step 137900 | Avg Loss: 0.0187 | Grad Norm: 0.00260478\n",
      "Epoch 4 | Step 138000 | Avg Loss: 0.0182 | Grad Norm: 0.00255743\n",
      "Epoch 4 | Step 138100 | Avg Loss: 0.0180 | Grad Norm: 0.00315472\n",
      "Epoch 4 | Step 138200 | Avg Loss: 0.0182 | Grad Norm: 0.00280194\n",
      "Epoch 4 | Step 138300 | Avg Loss: 0.0181 | Grad Norm: 0.00315104\n",
      "Epoch 4 | Step 138400 | Avg Loss: 0.0184 | Grad Norm: 0.00305954\n",
      "Epoch 4 | Step 138500 | Avg Loss: 0.0183 | Grad Norm: 0.00341703\n",
      "Epoch 4 | Step 138600 | Avg Loss: 0.0185 | Grad Norm: 0.00361235\n",
      "Epoch 4 | Step 138700 | Avg Loss: 0.0186 | Grad Norm: 0.00282942\n",
      "Epoch 4 | Step 138800 | Avg Loss: 0.0188 | Grad Norm: 0.00310493\n",
      "Epoch 4 | Step 138900 | Avg Loss: 0.0187 | Grad Norm: 0.00624593\n",
      "Epoch 4 | Step 139000 | Avg Loss: 0.0189 | Grad Norm: 0.00379718\n",
      "Epoch 4 | Step 139100 | Avg Loss: 0.0188 | Grad Norm: 0.00309175\n",
      "Epoch 4 | Step 139200 | Avg Loss: 0.0186 | Grad Norm: 0.00323433\n",
      "Epoch 4 | Step 139300 | Avg Loss: 0.0183 | Grad Norm: 0.00330891\n",
      "Epoch 4 | Step 139400 | Avg Loss: 0.0181 | Grad Norm: 0.00580434\n",
      "Epoch 4 | Step 139500 | Avg Loss: 0.0184 | Grad Norm: 0.00320354\n",
      "Epoch 4 | Step 139600 | Avg Loss: 0.0185 | Grad Norm: 0.00558324\n",
      "Epoch 4 | Step 139700 | Avg Loss: 0.0185 | Grad Norm: 0.00346123\n",
      "Epoch 4 | Step 139800 | Avg Loss: 0.0181 | Grad Norm: 0.00270409\n",
      "Epoch 4 | Step 139900 | Avg Loss: 0.0183 | Grad Norm: 0.00473293\n",
      "Epoch 4 | Step 140000 | Avg Loss: 0.0187 | Grad Norm: 0.00387256\n",
      "Epoch 4 | Step 140100 | Avg Loss: 0.0186 | Grad Norm: 0.00251997\n",
      "Epoch 4 | Step 140200 | Avg Loss: 0.0185 | Grad Norm: 0.00413716\n",
      "Epoch 4 | Step 140300 | Avg Loss: 0.0183 | Grad Norm: 0.00268441\n",
      "Epoch 4 | Step 140400 | Avg Loss: 0.0178 | Grad Norm: 0.00299065\n",
      "Epoch 4 | Step 140500 | Avg Loss: 0.0176 | Grad Norm: 0.00327622\n",
      "Epoch 4 | Step 140600 | Avg Loss: 0.0184 | Grad Norm: 0.00244655\n",
      "Epoch 4 | Step 140700 | Avg Loss: 0.0184 | Grad Norm: 0.00398358\n",
      "Epoch 4 | Step 140800 | Avg Loss: 0.0190 | Grad Norm: 0.00290506\n",
      "Epoch 4 | Step 140900 | Avg Loss: 0.0187 | Grad Norm: 0.00329950\n",
      "Epoch 4 | Step 141000 | Avg Loss: 0.0190 | Grad Norm: 0.00310022\n",
      "Epoch 4 | Step 141100 | Avg Loss: 0.0184 | Grad Norm: 0.00500137\n",
      "Epoch 4 | Step 141200 | Avg Loss: 0.0185 | Grad Norm: 0.00268794\n",
      "Epoch 4 | Step 141300 | Avg Loss: 0.0185 | Grad Norm: 0.00428578\n",
      "Epoch 4 | Step 141400 | Avg Loss: 0.0184 | Grad Norm: 0.00394518\n",
      "Epoch 4 | Step 141500 | Avg Loss: 0.0183 | Grad Norm: 0.00288492\n",
      "Epoch 4 | Step 141600 | Avg Loss: 0.0181 | Grad Norm: 0.00469597\n",
      "Epoch 4 | Step 141700 | Avg Loss: 0.0180 | Grad Norm: 0.00450662\n",
      "Epoch 4 | Step 141800 | Avg Loss: 0.0186 | Grad Norm: 0.00279418\n",
      "Epoch 4 | Step 141900 | Avg Loss: 0.0190 | Grad Norm: 0.00260355\n",
      "Epoch 4 | Step 142000 | Avg Loss: 0.0186 | Grad Norm: 0.00339748\n",
      "Epoch 4 | Step 142100 | Avg Loss: 0.0182 | Grad Norm: 0.00591115\n",
      "Epoch 4 | Step 142200 | Avg Loss: 0.0182 | Grad Norm: 0.00457042\n",
      "Epoch 4 | Step 142300 | Avg Loss: 0.0188 | Grad Norm: 0.00546386\n",
      "Epoch 4 | Step 142400 | Avg Loss: 0.0190 | Grad Norm: 0.00308469\n",
      "Epoch 4 | Step 142500 | Avg Loss: 0.0194 | Grad Norm: 0.00428684\n",
      "Epoch 4 | Step 142600 | Avg Loss: 0.0191 | Grad Norm: 0.00264035\n",
      "Epoch 4 | Step 142700 | Avg Loss: 0.0192 | Grad Norm: 0.00309368\n",
      "Epoch 4 | Step 142800 | Avg Loss: 0.0190 | Grad Norm: 0.00331579\n",
      "Epoch 4 | Step 142900 | Avg Loss: 0.0190 | Grad Norm: 0.00454955\n",
      "Epoch 4 | Step 143000 | Avg Loss: 0.0193 | Grad Norm: 0.00374251\n",
      "Epoch 4 | Step 143100 | Avg Loss: 0.0192 | Grad Norm: 0.00412516\n",
      "Epoch 4 | Step 143200 | Avg Loss: 0.0188 | Grad Norm: 0.00279112\n",
      "Epoch 4 | Step 143300 | Avg Loss: 0.0181 | Grad Norm: 0.00293886\n",
      "Epoch 4 | Step 143400 | Avg Loss: 0.0183 | Grad Norm: 0.00387232\n",
      "Epoch 4 | Step 143500 | Avg Loss: 0.0184 | Grad Norm: 0.00311972\n",
      "Epoch 4 | Step 143600 | Avg Loss: 0.0180 | Grad Norm: 0.00318254\n",
      "Epoch 4 | Step 143700 | Avg Loss: 0.0182 | Grad Norm: 0.00580859\n",
      "Epoch 4 | Step 143800 | Avg Loss: 0.0180 | Grad Norm: 0.00250769\n",
      "Epoch 4 | Step 143900 | Avg Loss: 0.0183 | Grad Norm: 0.00307629\n",
      "Epoch 4 | Step 144000 | Avg Loss: 0.0185 | Grad Norm: 0.00312069\n",
      "Epoch 4 | Step 144100 | Avg Loss: 0.0187 | Grad Norm: 0.00373636\n",
      "Epoch 4 | Step 144200 | Avg Loss: 0.0186 | Grad Norm: 0.00461177\n",
      "Epoch 4 | Step 144300 | Avg Loss: 0.0189 | Grad Norm: 0.00343784\n",
      "Epoch 4 | Step 144400 | Avg Loss: 0.0184 | Grad Norm: 0.00463751\n",
      "Epoch 4 | Step 144500 | Avg Loss: 0.0184 | Grad Norm: 0.00293908\n",
      "Epoch 4 | Step 144600 | Avg Loss: 0.0187 | Grad Norm: 0.00519460\n",
      "Epoch 4 | Step 144700 | Avg Loss: 0.0188 | Grad Norm: 0.00290532\n",
      "Epoch 4 | Step 144800 | Avg Loss: 0.0187 | Grad Norm: 0.00569900\n",
      "Epoch 4 | Step 144900 | Avg Loss: 0.0186 | Grad Norm: 0.00292459\n",
      "Epoch 4 | Step 145000 | Avg Loss: 0.0187 | Grad Norm: 0.00397326\n",
      "Epoch 4 | Step 145100 | Avg Loss: 0.0190 | Grad Norm: 0.00309302\n",
      "Epoch 4 | Step 145200 | Avg Loss: 0.0185 | Grad Norm: 0.00282192\n",
      "Epoch 4 | Step 145300 | Avg Loss: 0.0188 | Grad Norm: 0.00290802\n",
      "Epoch 4 | Step 145400 | Avg Loss: 0.0185 | Grad Norm: 0.00262325\n",
      "Epoch 4 | Step 145500 | Avg Loss: 0.0182 | Grad Norm: 0.00299491\n",
      "Epoch 4 | Step 145600 | Avg Loss: 0.0184 | Grad Norm: 0.00429353\n",
      "Epoch 4 | Step 145700 | Avg Loss: 0.0189 | Grad Norm: 0.00295930\n",
      "Epoch 4 | Step 145800 | Avg Loss: 0.0190 | Grad Norm: 0.00279799\n",
      "Epoch 4 | Step 145900 | Avg Loss: 0.0184 | Grad Norm: 0.00392832\n",
      "Epoch 4 | Step 146000 | Avg Loss: 0.0182 | Grad Norm: 0.00452282\n",
      "Epoch 4 | Step 146100 | Avg Loss: 0.0183 | Grad Norm: 0.00480885\n",
      "Epoch 4 | Step 146200 | Avg Loss: 0.0182 | Grad Norm: 0.00313059\n",
      "Epoch 4 | Step 146300 | Avg Loss: 0.0186 | Grad Norm: 0.00324798\n",
      "Epoch 4 | Step 146400 | Avg Loss: 0.0187 | Grad Norm: 0.00354882\n",
      "Epoch 4 | Step 146500 | Avg Loss: 0.0185 | Grad Norm: 0.00379741\n",
      "Epoch 4 | Step 146600 | Avg Loss: 0.0183 | Grad Norm: 0.00327957\n",
      "Epoch 4 | Step 146700 | Avg Loss: 0.0185 | Grad Norm: 0.00281526\n",
      "Epoch 4 | Step 146800 | Avg Loss: 0.0182 | Grad Norm: 0.00423381\n",
      "Epoch 4 | Step 146900 | Avg Loss: 0.0184 | Grad Norm: 0.00352455\n",
      "Epoch 4 | Step 147000 | Avg Loss: 0.0189 | Grad Norm: 0.00312500\n",
      "Epoch 4 | Step 147100 | Avg Loss: 0.0189 | Grad Norm: 0.00428548\n",
      "Epoch 4 | Step 147200 | Avg Loss: 0.0188 | Grad Norm: 0.00293306\n",
      "Epoch 4 | Step 147300 | Avg Loss: 0.0188 | Grad Norm: 0.00379389\n",
      "Epoch 4 | Step 147400 | Avg Loss: 0.0190 | Grad Norm: 0.00269407\n",
      "Epoch 4 | Step 147500 | Avg Loss: 0.0190 | Grad Norm: 0.00300574\n",
      "Epoch 4 | Step 147600 | Avg Loss: 0.0192 | Grad Norm: 0.00610381\n",
      "Epoch 4 | Step 147700 | Avg Loss: 0.0183 | Grad Norm: 0.00272089\n",
      "Epoch 4 | Step 147800 | Avg Loss: 0.0182 | Grad Norm: 0.00356408\n",
      "Epoch 4 | Step 147900 | Avg Loss: 0.0185 | Grad Norm: 0.00337511\n",
      "Epoch 4 | Step 148000 | Avg Loss: 0.0183 | Grad Norm: 0.00280198\n",
      "Epoch 4 | Step 148100 | Avg Loss: 0.0185 | Grad Norm: 0.00515402\n",
      "Epoch 4 | Step 148200 | Avg Loss: 0.0185 | Grad Norm: 0.00343275\n",
      "Epoch 4 | Step 148300 | Avg Loss: 0.0182 | Grad Norm: 0.00336104\n",
      "Epoch 4 | Step 148400 | Avg Loss: 0.0182 | Grad Norm: 0.00340286\n",
      "Epoch 4 | Step 148500 | Avg Loss: 0.0180 | Grad Norm: 0.00318985\n",
      "Epoch 4 | Step 148600 | Avg Loss: 0.0185 | Grad Norm: 0.00473918\n",
      "Epoch 4 | Step 148700 | Avg Loss: 0.0182 | Grad Norm: 0.00468647\n",
      "Epoch 4 | Step 148800 | Avg Loss: 0.0179 | Grad Norm: 0.00516635\n",
      "Epoch 4 | Step 148900 | Avg Loss: 0.0177 | Grad Norm: 0.00265924\n",
      "Epoch 4 | Step 149000 | Avg Loss: 0.0184 | Grad Norm: 0.00483922\n",
      "Epoch 4 | Step 149100 | Avg Loss: 0.0186 | Grad Norm: 0.00300671\n",
      "Epoch 4 | Step 149200 | Avg Loss: 0.0189 | Grad Norm: 0.00309446\n",
      "Epoch 4 | Step 149300 | Avg Loss: 0.0187 | Grad Norm: 0.00342610\n",
      "Epoch 4 | Step 149400 | Avg Loss: 0.0189 | Grad Norm: 0.00337885\n",
      "Epoch 4 | Step 149500 | Avg Loss: 0.0191 | Grad Norm: 0.00282520\n",
      "Epoch 4 | Step 149600 | Avg Loss: 0.0190 | Grad Norm: 0.00329188\n",
      "Epoch 4 | Step 149700 | Avg Loss: 0.0189 | Grad Norm: 0.00390847\n",
      "Epoch 4 | Step 149800 | Avg Loss: 0.0187 | Grad Norm: 0.00262397\n",
      "Epoch 4 | Step 149900 | Avg Loss: 0.0189 | Grad Norm: 0.00275127\n",
      "Epoch 4 | Step 150000 | Avg Loss: 0.0186 | Grad Norm: 0.00496155\n",
      "Epoch 4 | Step 150100 | Avg Loss: 0.0186 | Grad Norm: 0.00363820\n",
      "Epoch 4 | Step 150200 | Avg Loss: 0.0185 | Grad Norm: 0.00305759\n",
      "Epoch 4 | Step 150300 | Avg Loss: 0.0182 | Grad Norm: 0.00340244\n",
      "Epoch 4 | Step 150400 | Avg Loss: 0.0180 | Grad Norm: 0.00312314\n",
      "Epoch 4 | Step 150500 | Avg Loss: 0.0183 | Grad Norm: 0.00341804\n",
      "Epoch 4 | Step 150600 | Avg Loss: 0.0184 | Grad Norm: 0.00404500\n",
      "Epoch 4 | Step 150700 | Avg Loss: 0.0185 | Grad Norm: 0.00371176\n",
      "Epoch 4 | Step 150800 | Avg Loss: 0.0186 | Grad Norm: 0.00366699\n",
      "Epoch 4 | Step 150900 | Avg Loss: 0.0184 | Grad Norm: 0.00338176\n",
      "Epoch 4 | Step 151000 | Avg Loss: 0.0184 | Grad Norm: 0.00317291\n",
      "Epoch 4 | Step 151100 | Avg Loss: 0.0181 | Grad Norm: 0.00273931\n",
      "Epoch 4 | Step 151200 | Avg Loss: 0.0174 | Grad Norm: 0.00348018\n",
      "Epoch 4 | Step 151300 | Avg Loss: 0.0173 | Grad Norm: 0.00266669\n",
      "Epoch 4 | Step 151400 | Avg Loss: 0.0173 | Grad Norm: 0.00317984\n",
      "Epoch 4 | Step 151500 | Avg Loss: 0.0177 | Grad Norm: 0.00363831\n",
      "Epoch 4 | Step 151600 | Avg Loss: 0.0179 | Grad Norm: 0.00403114\n",
      "Epoch 4 | Step 151700 | Avg Loss: 0.0183 | Grad Norm: 0.00560289\n",
      "Epoch 4 | Step 151800 | Avg Loss: 0.0184 | Grad Norm: 0.00287387\n",
      "Epoch 4 | Step 151900 | Avg Loss: 0.0182 | Grad Norm: 0.00315902\n",
      "Epoch 4 | Step 152000 | Avg Loss: 0.0181 | Grad Norm: 0.00254914\n",
      "Epoch 4 | Step 152100 | Avg Loss: 0.0184 | Grad Norm: 0.00279000\n",
      "Epoch 4 | Step 152200 | Avg Loss: 0.0182 | Grad Norm: 0.00311488\n",
      "Epoch 4 | Step 152300 | Avg Loss: 0.0182 | Grad Norm: 0.00329899\n",
      "Epoch 4 | Step 152400 | Avg Loss: 0.0186 | Grad Norm: 0.00271173\n",
      "Epoch 4 | Step 152500 | Avg Loss: 0.0186 | Grad Norm: 0.00405504\n",
      "Epoch 4 | Step 152600 | Avg Loss: 0.0185 | Grad Norm: 0.00341122\n",
      "Epoch 4 | Step 152700 | Avg Loss: 0.0187 | Grad Norm: 0.00366542\n",
      "Epoch 4 | Step 152800 | Avg Loss: 0.0192 | Grad Norm: 0.00494806\n",
      "Epoch 4 | Step 152900 | Avg Loss: 0.0186 | Grad Norm: 0.00364859\n",
      "Epoch 4 | Step 153000 | Avg Loss: 0.0184 | Grad Norm: 0.00423320\n",
      "Epoch 4 | Step 153100 | Avg Loss: 0.0182 | Grad Norm: 0.00333829\n",
      "Epoch 4 | Step 153200 | Avg Loss: 0.0180 | Grad Norm: 0.00321099\n",
      "Epoch 4 | Step 153300 | Avg Loss: 0.0179 | Grad Norm: 0.00380414\n",
      "Epoch 4 | Step 153400 | Avg Loss: 0.0186 | Grad Norm: 0.00384945\n",
      "Epoch 4 | Step 153500 | Avg Loss: 0.0189 | Grad Norm: 0.00296188\n",
      "Epoch 4 | Step 153600 | Avg Loss: 0.0188 | Grad Norm: 0.00392726\n",
      "Epoch 4 | Step 153700 | Avg Loss: 0.0188 | Grad Norm: 0.00480181\n",
      "Epoch 4 | Step 153800 | Avg Loss: 0.0189 | Grad Norm: 0.00325035\n",
      "Epoch 4 | Step 153900 | Avg Loss: 0.0187 | Grad Norm: 0.00296055\n",
      "Epoch 4 | Step 154000 | Avg Loss: 0.0188 | Grad Norm: 0.00298832\n",
      "Epoch 4 | Step 154100 | Avg Loss: 0.0190 | Grad Norm: 0.00312475\n",
      "Epoch 4 | Step 154200 | Avg Loss: 0.0193 | Grad Norm: 0.00263529\n",
      "Epoch 4 | Step 154300 | Avg Loss: 0.0190 | Grad Norm: 0.00380581\n",
      "Epoch 4 | Step 154400 | Avg Loss: 0.0189 | Grad Norm: 0.00397151\n",
      "Epoch 4 | Step 154500 | Avg Loss: 0.0183 | Grad Norm: 0.00290960\n",
      "Epoch 4 | Step 154600 | Avg Loss: 0.0186 | Grad Norm: 0.00396319\n",
      "Epoch 4 | Step 154700 | Avg Loss: 0.0184 | Grad Norm: 0.00359964\n",
      "Epoch 4 | Step 154800 | Avg Loss: 0.0182 | Grad Norm: 0.00355812\n",
      "Epoch 4 | Step 154900 | Avg Loss: 0.0185 | Grad Norm: 0.00321370\n",
      "Epoch 4 | Step 155000 | Avg Loss: 0.0185 | Grad Norm: 0.00653941\n",
      "Epoch 4 | Step 155100 | Avg Loss: 0.0184 | Grad Norm: 0.00306596\n",
      "Epoch 4 | Step 155200 | Avg Loss: 0.0190 | Grad Norm: 0.00353201\n",
      "Epoch 4 | Step 155300 | Avg Loss: 0.0192 | Grad Norm: 0.00404699\n",
      "Epoch 4 | Step 155400 | Avg Loss: 0.0188 | Grad Norm: 0.00314003\n",
      "Epoch 4 | Step 155500 | Avg Loss: 0.0192 | Grad Norm: 0.00364045\n",
      "Epoch 4 | Step 155600 | Avg Loss: 0.0191 | Grad Norm: 0.00457207\n",
      "Epoch 4 | Step 155700 | Avg Loss: 0.0192 | Grad Norm: 0.00333190\n",
      "Epoch 4 | Step 155800 | Avg Loss: 0.0189 | Grad Norm: 0.00358229\n",
      "Epoch 4 | Step 155900 | Avg Loss: 0.0189 | Grad Norm: 0.00470664\n",
      "Epoch 4 | Step 156000 | Avg Loss: 0.0187 | Grad Norm: 0.00366238\n",
      "Epoch 4 | Step 156100 | Avg Loss: 0.0186 | Grad Norm: 0.00576409\n",
      "Epoch 4 | Step 156200 | Avg Loss: 0.0180 | Grad Norm: 0.00372932\n",
      "Epoch 4, Loss: 0.0181\n",
      "Epoch 5 | Step 156300 | Avg Loss: 0.0185 | Grad Norm: 0.00351548\n",
      "Epoch 5 | Step 156400 | Avg Loss: 0.0181 | Grad Norm: 0.00294279\n",
      "Epoch 5 | Step 156500 | Avg Loss: 0.0179 | Grad Norm: 0.00306908\n",
      "Epoch 5 | Step 156600 | Avg Loss: 0.0179 | Grad Norm: 0.00322922\n",
      "Epoch 5 | Step 156700 | Avg Loss: 0.0177 | Grad Norm: 0.00301206\n",
      "Epoch 5 | Step 156800 | Avg Loss: 0.0180 | Grad Norm: 0.00357254\n",
      "Epoch 5 | Step 156900 | Avg Loss: 0.0185 | Grad Norm: 0.00282595\n",
      "Epoch 5 | Step 157000 | Avg Loss: 0.0188 | Grad Norm: 0.00300131\n",
      "Epoch 5 | Step 157100 | Avg Loss: 0.0193 | Grad Norm: 0.00398309\n",
      "Epoch 5 | Step 157200 | Avg Loss: 0.0192 | Grad Norm: 0.00403051\n",
      "Epoch 5 | Step 157300 | Avg Loss: 0.0189 | Grad Norm: 0.00296890\n",
      "Epoch 5 | Step 157400 | Avg Loss: 0.0186 | Grad Norm: 0.00320097\n",
      "Epoch 5 | Step 157500 | Avg Loss: 0.0184 | Grad Norm: 0.00262748\n",
      "Epoch 5 | Step 157600 | Avg Loss: 0.0185 | Grad Norm: 0.00338357\n",
      "Epoch 5 | Step 157700 | Avg Loss: 0.0183 | Grad Norm: 0.00509431\n",
      "Epoch 5 | Step 157800 | Avg Loss: 0.0183 | Grad Norm: 0.00295746\n",
      "Epoch 5 | Step 157900 | Avg Loss: 0.0178 | Grad Norm: 0.00252069\n",
      "Epoch 5 | Step 158000 | Avg Loss: 0.0182 | Grad Norm: 0.00273044\n",
      "Epoch 5 | Step 158100 | Avg Loss: 0.0180 | Grad Norm: 0.00266721\n",
      "Epoch 5 | Step 158200 | Avg Loss: 0.0182 | Grad Norm: 0.00331391\n",
      "Epoch 5 | Step 158300 | Avg Loss: 0.0180 | Grad Norm: 0.00323011\n",
      "Epoch 5 | Step 158400 | Avg Loss: 0.0181 | Grad Norm: 0.00275427\n",
      "Epoch 5 | Step 158500 | Avg Loss: 0.0184 | Grad Norm: 0.00308108\n",
      "Epoch 5 | Step 158600 | Avg Loss: 0.0182 | Grad Norm: 0.00363820\n",
      "Epoch 5 | Step 158700 | Avg Loss: 0.0183 | Grad Norm: 0.00282997\n",
      "Epoch 5 | Step 158800 | Avg Loss: 0.0184 | Grad Norm: 0.00340594\n",
      "Epoch 5 | Step 158900 | Avg Loss: 0.0187 | Grad Norm: 0.00317736\n",
      "Epoch 5 | Step 159000 | Avg Loss: 0.0186 | Grad Norm: 0.00394180\n",
      "Epoch 5 | Step 159100 | Avg Loss: 0.0185 | Grad Norm: 0.00273411\n",
      "Epoch 5 | Step 159200 | Avg Loss: 0.0187 | Grad Norm: 0.00409349\n",
      "Epoch 5 | Step 159300 | Avg Loss: 0.0189 | Grad Norm: 0.00363250\n",
      "Epoch 5 | Step 159400 | Avg Loss: 0.0183 | Grad Norm: 0.00289426\n",
      "Epoch 5 | Step 159500 | Avg Loss: 0.0182 | Grad Norm: 0.00296835\n",
      "Epoch 5 | Step 159600 | Avg Loss: 0.0182 | Grad Norm: 0.00264980\n",
      "Epoch 5 | Step 159700 | Avg Loss: 0.0177 | Grad Norm: 0.00399493\n",
      "Epoch 5 | Step 159800 | Avg Loss: 0.0174 | Grad Norm: 0.00319232\n",
      "Epoch 5 | Step 159900 | Avg Loss: 0.0177 | Grad Norm: 0.00292851\n",
      "Epoch 5 | Step 160000 | Avg Loss: 0.0179 | Grad Norm: 0.00357435\n",
      "Epoch 5 | Step 160100 | Avg Loss: 0.0181 | Grad Norm: 0.00371177\n",
      "Epoch 5 | Step 160200 | Avg Loss: 0.0184 | Grad Norm: 0.00424256\n",
      "Epoch 5 | Step 160300 | Avg Loss: 0.0181 | Grad Norm: 0.00499445\n",
      "Epoch 5 | Step 160400 | Avg Loss: 0.0181 | Grad Norm: 0.00593311\n",
      "Epoch 5 | Step 160500 | Avg Loss: 0.0181 | Grad Norm: 0.00604672\n",
      "Epoch 5 | Step 160600 | Avg Loss: 0.0182 | Grad Norm: 0.00304674\n",
      "Epoch 5 | Step 160700 | Avg Loss: 0.0183 | Grad Norm: 0.00326541\n",
      "Epoch 5 | Step 160800 | Avg Loss: 0.0179 | Grad Norm: 0.00321009\n",
      "Epoch 5 | Step 160900 | Avg Loss: 0.0177 | Grad Norm: 0.00506778\n",
      "Epoch 5 | Step 161000 | Avg Loss: 0.0176 | Grad Norm: 0.00385984\n",
      "Epoch 5 | Step 161100 | Avg Loss: 0.0179 | Grad Norm: 0.00327561\n",
      "Epoch 5 | Step 161200 | Avg Loss: 0.0180 | Grad Norm: 0.00317686\n",
      "Epoch 5 | Step 161300 | Avg Loss: 0.0179 | Grad Norm: 0.00449707\n",
      "Epoch 5 | Step 161400 | Avg Loss: 0.0180 | Grad Norm: 0.00454857\n",
      "Epoch 5 | Step 161500 | Avg Loss: 0.0181 | Grad Norm: 0.00288084\n",
      "Epoch 5 | Step 161600 | Avg Loss: 0.0186 | Grad Norm: 0.00385927\n",
      "Epoch 5 | Step 161700 | Avg Loss: 0.0190 | Grad Norm: 0.00623319\n",
      "Epoch 5 | Step 161800 | Avg Loss: 0.0189 | Grad Norm: 0.00274092\n",
      "Epoch 5 | Step 161900 | Avg Loss: 0.0187 | Grad Norm: 0.00474279\n",
      "Epoch 5 | Step 162000 | Avg Loss: 0.0186 | Grad Norm: 0.00483652\n",
      "Epoch 5 | Step 162100 | Avg Loss: 0.0186 | Grad Norm: 0.00277782\n",
      "Epoch 5 | Step 162200 | Avg Loss: 0.0186 | Grad Norm: 0.00298698\n",
      "Epoch 5 | Step 162300 | Avg Loss: 0.0181 | Grad Norm: 0.00342736\n",
      "Epoch 5 | Step 162400 | Avg Loss: 0.0178 | Grad Norm: 0.00384174\n",
      "Epoch 5 | Step 162500 | Avg Loss: 0.0184 | Grad Norm: 0.00401357\n",
      "Epoch 5 | Step 162600 | Avg Loss: 0.0185 | Grad Norm: 0.00281274\n",
      "Epoch 5 | Step 162700 | Avg Loss: 0.0185 | Grad Norm: 0.00274685\n",
      "Epoch 5 | Step 162800 | Avg Loss: 0.0185 | Grad Norm: 0.00307063\n",
      "Epoch 5 | Step 162900 | Avg Loss: 0.0183 | Grad Norm: 0.00330938\n",
      "Epoch 5 | Step 163000 | Avg Loss: 0.0181 | Grad Norm: 0.00655877\n",
      "Epoch 5 | Step 163100 | Avg Loss: 0.0181 | Grad Norm: 0.00326732\n",
      "Epoch 5 | Step 163200 | Avg Loss: 0.0181 | Grad Norm: 0.00323906\n",
      "Epoch 5 | Step 163300 | Avg Loss: 0.0181 | Grad Norm: 0.00368386\n",
      "Epoch 5 | Step 163400 | Avg Loss: 0.0179 | Grad Norm: 0.00238780\n",
      "Epoch 5 | Step 163500 | Avg Loss: 0.0174 | Grad Norm: 0.00278017\n",
      "Epoch 5 | Step 163600 | Avg Loss: 0.0176 | Grad Norm: 0.00266122\n",
      "Epoch 5 | Step 163700 | Avg Loss: 0.0176 | Grad Norm: 0.00521081\n",
      "Epoch 5 | Step 163800 | Avg Loss: 0.0180 | Grad Norm: 0.00271908\n",
      "Epoch 5 | Step 163900 | Avg Loss: 0.0181 | Grad Norm: 0.00383997\n",
      "Epoch 5 | Step 164000 | Avg Loss: 0.0179 | Grad Norm: 0.00286491\n",
      "Epoch 5 | Step 164100 | Avg Loss: 0.0177 | Grad Norm: 0.00333018\n",
      "Epoch 5 | Step 164200 | Avg Loss: 0.0183 | Grad Norm: 0.00299402\n",
      "Epoch 5 | Step 164300 | Avg Loss: 0.0185 | Grad Norm: 0.00355395\n",
      "Epoch 5 | Step 164400 | Avg Loss: 0.0184 | Grad Norm: 0.00422622\n",
      "Epoch 5 | Step 164500 | Avg Loss: 0.0184 | Grad Norm: 0.00394933\n",
      "Epoch 5 | Step 164600 | Avg Loss: 0.0183 | Grad Norm: 0.00306164\n",
      "Epoch 5 | Step 164700 | Avg Loss: 0.0183 | Grad Norm: 0.00247426\n",
      "Epoch 5 | Step 164800 | Avg Loss: 0.0188 | Grad Norm: 0.00271465\n",
      "Epoch 5 | Step 164900 | Avg Loss: 0.0193 | Grad Norm: 0.00348587\n",
      "Epoch 5 | Step 165000 | Avg Loss: 0.0187 | Grad Norm: 0.00496780\n",
      "Epoch 5 | Step 165100 | Avg Loss: 0.0188 | Grad Norm: 0.00594565\n",
      "Epoch 5 | Step 165200 | Avg Loss: 0.0186 | Grad Norm: 0.00286070\n",
      "Epoch 5 | Step 165300 | Avg Loss: 0.0186 | Grad Norm: 0.00402431\n",
      "Epoch 5 | Step 165400 | Avg Loss: 0.0183 | Grad Norm: 0.00663932\n",
      "Epoch 5 | Step 165500 | Avg Loss: 0.0184 | Grad Norm: 0.00300919\n",
      "Epoch 5 | Step 165600 | Avg Loss: 0.0180 | Grad Norm: 0.00518803\n",
      "Epoch 5 | Step 165700 | Avg Loss: 0.0182 | Grad Norm: 0.00292521\n",
      "Epoch 5 | Step 165800 | Avg Loss: 0.0182 | Grad Norm: 0.00300840\n",
      "Epoch 5 | Step 165900 | Avg Loss: 0.0182 | Grad Norm: 0.00401373\n",
      "Epoch 5 | Step 166000 | Avg Loss: 0.0183 | Grad Norm: 0.00265681\n",
      "Epoch 5 | Step 166100 | Avg Loss: 0.0183 | Grad Norm: 0.00355468\n",
      "Epoch 5 | Step 166200 | Avg Loss: 0.0184 | Grad Norm: 0.00496696\n",
      "Epoch 5 | Step 166300 | Avg Loss: 0.0186 | Grad Norm: 0.00228983\n",
      "Epoch 5 | Step 166400 | Avg Loss: 0.0188 | Grad Norm: 0.00370105\n",
      "Epoch 5 | Step 166500 | Avg Loss: 0.0186 | Grad Norm: 0.00270693\n",
      "Epoch 5 | Step 166600 | Avg Loss: 0.0186 | Grad Norm: 0.00274725\n",
      "Epoch 5 | Step 166700 | Avg Loss: 0.0185 | Grad Norm: 0.00327973\n",
      "Epoch 5 | Step 166800 | Avg Loss: 0.0188 | Grad Norm: 0.00320962\n",
      "Epoch 5 | Step 166900 | Avg Loss: 0.0185 | Grad Norm: 0.00413421\n",
      "Epoch 5 | Step 167000 | Avg Loss: 0.0190 | Grad Norm: 0.00271423\n",
      "Epoch 5 | Step 167100 | Avg Loss: 0.0191 | Grad Norm: 0.00333832\n",
      "Epoch 5 | Step 167200 | Avg Loss: 0.0185 | Grad Norm: 0.00488996\n",
      "Epoch 5 | Step 167300 | Avg Loss: 0.0189 | Grad Norm: 0.00440643\n",
      "Epoch 5 | Step 167400 | Avg Loss: 0.0193 | Grad Norm: 0.00347523\n",
      "Epoch 5 | Step 167500 | Avg Loss: 0.0190 | Grad Norm: 0.00466971\n",
      "Epoch 5 | Step 167600 | Avg Loss: 0.0189 | Grad Norm: 0.00297755\n",
      "Epoch 5 | Step 167700 | Avg Loss: 0.0188 | Grad Norm: 0.00308749\n",
      "Epoch 5 | Step 167800 | Avg Loss: 0.0190 | Grad Norm: 0.00324159\n",
      "Epoch 5 | Step 167900 | Avg Loss: 0.0185 | Grad Norm: 0.00476205\n",
      "Epoch 5 | Step 168000 | Avg Loss: 0.0185 | Grad Norm: 0.00454610\n",
      "Epoch 5 | Step 168100 | Avg Loss: 0.0182 | Grad Norm: 0.00566668\n",
      "Epoch 5 | Step 168200 | Avg Loss: 0.0181 | Grad Norm: 0.00298724\n",
      "Epoch 5 | Step 168300 | Avg Loss: 0.0177 | Grad Norm: 0.00340107\n",
      "Epoch 5 | Step 168400 | Avg Loss: 0.0178 | Grad Norm: 0.00518108\n",
      "Epoch 5 | Step 168500 | Avg Loss: 0.0180 | Grad Norm: 0.00262945\n",
      "Epoch 5 | Step 168600 | Avg Loss: 0.0183 | Grad Norm: 0.00642180\n",
      "Epoch 5 | Step 168700 | Avg Loss: 0.0187 | Grad Norm: 0.00322349\n",
      "Epoch 5 | Step 168800 | Avg Loss: 0.0189 | Grad Norm: 0.00362032\n",
      "Epoch 5 | Step 168900 | Avg Loss: 0.0183 | Grad Norm: 0.00338874\n",
      "Epoch 5 | Step 169000 | Avg Loss: 0.0181 | Grad Norm: 0.00343511\n",
      "Epoch 5 | Step 169100 | Avg Loss: 0.0181 | Grad Norm: 0.00309545\n",
      "Epoch 5 | Step 169200 | Avg Loss: 0.0184 | Grad Norm: 0.00394417\n",
      "Epoch 5 | Step 169300 | Avg Loss: 0.0182 | Grad Norm: 0.00346464\n",
      "Epoch 5 | Step 169400 | Avg Loss: 0.0181 | Grad Norm: 0.00317722\n",
      "Epoch 5 | Step 169500 | Avg Loss: 0.0180 | Grad Norm: 0.00545935\n",
      "Epoch 5 | Step 169600 | Avg Loss: 0.0181 | Grad Norm: 0.00397676\n",
      "Epoch 5 | Step 169700 | Avg Loss: 0.0183 | Grad Norm: 0.00330098\n",
      "Epoch 5 | Step 169800 | Avg Loss: 0.0181 | Grad Norm: 0.00367750\n",
      "Epoch 5 | Step 169900 | Avg Loss: 0.0182 | Grad Norm: 0.00277755\n",
      "Epoch 5 | Step 170000 | Avg Loss: 0.0179 | Grad Norm: 0.00298919\n",
      "Epoch 5 | Step 170100 | Avg Loss: 0.0184 | Grad Norm: 0.00325919\n",
      "Epoch 5 | Step 170200 | Avg Loss: 0.0186 | Grad Norm: 0.00294364\n",
      "Epoch 5 | Step 170300 | Avg Loss: 0.0184 | Grad Norm: 0.00449141\n",
      "Epoch 5 | Step 170400 | Avg Loss: 0.0181 | Grad Norm: 0.00461303\n",
      "Epoch 5 | Step 170500 | Avg Loss: 0.0185 | Grad Norm: 0.00279752\n",
      "Epoch 5 | Step 170600 | Avg Loss: 0.0183 | Grad Norm: 0.00309340\n",
      "Epoch 5 | Step 170700 | Avg Loss: 0.0185 | Grad Norm: 0.00256735\n",
      "Epoch 5 | Step 170800 | Avg Loss: 0.0185 | Grad Norm: 0.00364547\n",
      "Epoch 5 | Step 170900 | Avg Loss: 0.0184 | Grad Norm: 0.00397485\n",
      "Epoch 5 | Step 171000 | Avg Loss: 0.0183 | Grad Norm: 0.00317671\n",
      "Epoch 5 | Step 171100 | Avg Loss: 0.0182 | Grad Norm: 0.00301233\n",
      "Epoch 5 | Step 171200 | Avg Loss: 0.0184 | Grad Norm: 0.00468421\n",
      "Epoch 5 | Step 171300 | Avg Loss: 0.0185 | Grad Norm: 0.00271849\n",
      "Epoch 5 | Step 171400 | Avg Loss: 0.0185 | Grad Norm: 0.00544273\n",
      "Epoch 5 | Step 171500 | Avg Loss: 0.0182 | Grad Norm: 0.00343685\n",
      "Epoch 5 | Step 171600 | Avg Loss: 0.0186 | Grad Norm: 0.00510314\n",
      "Epoch 5 | Step 171700 | Avg Loss: 0.0181 | Grad Norm: 0.00303561\n",
      "Epoch 5 | Step 171800 | Avg Loss: 0.0186 | Grad Norm: 0.00407177\n",
      "Epoch 5 | Step 171900 | Avg Loss: 0.0188 | Grad Norm: 0.00305754\n",
      "Epoch 5 | Step 172000 | Avg Loss: 0.0194 | Grad Norm: 0.00322665\n",
      "Epoch 5 | Step 172100 | Avg Loss: 0.0194 | Grad Norm: 0.00355225\n",
      "Epoch 5 | Step 172200 | Avg Loss: 0.0195 | Grad Norm: 0.00369086\n",
      "Epoch 5 | Step 172300 | Avg Loss: 0.0191 | Grad Norm: 0.00702710\n",
      "Epoch 5 | Step 172400 | Avg Loss: 0.0195 | Grad Norm: 0.00351732\n",
      "Epoch 5 | Step 172500 | Avg Loss: 0.0196 | Grad Norm: 0.00310900\n",
      "Epoch 5 | Step 172600 | Avg Loss: 0.0190 | Grad Norm: 0.00425355\n",
      "Epoch 5 | Step 172700 | Avg Loss: 0.0191 | Grad Norm: 0.00373951\n",
      "Epoch 5 | Step 172800 | Avg Loss: 0.0189 | Grad Norm: 0.00522091\n",
      "Epoch 5 | Step 172900 | Avg Loss: 0.0182 | Grad Norm: 0.00639434\n",
      "Epoch 5 | Step 173000 | Avg Loss: 0.0183 | Grad Norm: 0.00325041\n",
      "Epoch 5 | Step 173100 | Avg Loss: 0.0187 | Grad Norm: 0.00317203\n",
      "Epoch 5 | Step 173200 | Avg Loss: 0.0187 | Grad Norm: 0.00329008\n",
      "Epoch 5 | Step 173300 | Avg Loss: 0.0184 | Grad Norm: 0.00402661\n",
      "Epoch 5 | Step 173400 | Avg Loss: 0.0186 | Grad Norm: 0.00457972\n",
      "Epoch 5 | Step 173500 | Avg Loss: 0.0182 | Grad Norm: 0.00298956\n",
      "Epoch 5 | Step 173600 | Avg Loss: 0.0181 | Grad Norm: 0.00274894\n",
      "Epoch 5 | Step 173700 | Avg Loss: 0.0183 | Grad Norm: 0.00367871\n",
      "Epoch 5 | Step 173800 | Avg Loss: 0.0179 | Grad Norm: 0.00280899\n",
      "Epoch 5 | Step 173900 | Avg Loss: 0.0178 | Grad Norm: 0.00277542\n",
      "Epoch 5 | Step 174000 | Avg Loss: 0.0178 | Grad Norm: 0.00415909\n",
      "Epoch 5 | Step 174100 | Avg Loss: 0.0179 | Grad Norm: 0.00364185\n",
      "Epoch 5 | Step 174200 | Avg Loss: 0.0181 | Grad Norm: 0.00242921\n",
      "Epoch 5 | Step 174300 | Avg Loss: 0.0182 | Grad Norm: 0.00312207\n",
      "Epoch 5 | Step 174400 | Avg Loss: 0.0181 | Grad Norm: 0.00306530\n",
      "Epoch 5 | Step 174500 | Avg Loss: 0.0180 | Grad Norm: 0.00423629\n",
      "Epoch 5 | Step 174600 | Avg Loss: 0.0182 | Grad Norm: 0.00354928\n",
      "Epoch 5 | Step 174700 | Avg Loss: 0.0182 | Grad Norm: 0.00402324\n",
      "Epoch 5 | Step 174800 | Avg Loss: 0.0184 | Grad Norm: 0.00391595\n",
      "Epoch 5 | Step 174900 | Avg Loss: 0.0184 | Grad Norm: 0.00361868\n",
      "Epoch 5 | Step 175000 | Avg Loss: 0.0186 | Grad Norm: 0.00353696\n",
      "Epoch 5 | Step 175100 | Avg Loss: 0.0185 | Grad Norm: 0.00260238\n",
      "Epoch 5 | Step 175200 | Avg Loss: 0.0186 | Grad Norm: 0.00350098\n",
      "Epoch 5 | Step 175300 | Avg Loss: 0.0189 | Grad Norm: 0.00448312\n",
      "Epoch 5 | Step 175400 | Avg Loss: 0.0188 | Grad Norm: 0.00363539\n",
      "Epoch 5 | Step 175500 | Avg Loss: 0.0186 | Grad Norm: 0.00285122\n",
      "Epoch 5 | Step 175600 | Avg Loss: 0.0182 | Grad Norm: 0.00366051\n",
      "Epoch 5 | Step 175700 | Avg Loss: 0.0180 | Grad Norm: 0.00575673\n",
      "Epoch 5 | Step 175800 | Avg Loss: 0.0176 | Grad Norm: 0.00280433\n",
      "Epoch 5 | Step 175900 | Avg Loss: 0.0177 | Grad Norm: 0.00477235\n",
      "Epoch 5 | Step 176000 | Avg Loss: 0.0177 | Grad Norm: 0.00277517\n",
      "Epoch 5 | Step 176100 | Avg Loss: 0.0176 | Grad Norm: 0.00379454\n",
      "Epoch 5 | Step 176200 | Avg Loss: 0.0177 | Grad Norm: 0.00374032\n",
      "Epoch 5 | Step 176300 | Avg Loss: 0.0178 | Grad Norm: 0.00329969\n",
      "Epoch 5 | Step 176400 | Avg Loss: 0.0177 | Grad Norm: 0.00305183\n",
      "Epoch 5 | Step 176500 | Avg Loss: 0.0173 | Grad Norm: 0.00277024\n",
      "Epoch 5 | Step 176600 | Avg Loss: 0.0180 | Grad Norm: 0.00296905\n",
      "Epoch 5 | Step 176700 | Avg Loss: 0.0182 | Grad Norm: 0.00362941\n",
      "Epoch 5 | Step 176800 | Avg Loss: 0.0183 | Grad Norm: 0.00305301\n",
      "Epoch 5 | Step 176900 | Avg Loss: 0.0184 | Grad Norm: 0.00292267\n",
      "Epoch 5 | Step 177000 | Avg Loss: 0.0183 | Grad Norm: 0.00392859\n",
      "Epoch 5 | Step 177100 | Avg Loss: 0.0180 | Grad Norm: 0.00242633\n",
      "Epoch 5 | Step 177200 | Avg Loss: 0.0182 | Grad Norm: 0.00298001\n",
      "Epoch 5 | Step 177300 | Avg Loss: 0.0178 | Grad Norm: 0.00433025\n",
      "Epoch 5 | Step 177400 | Avg Loss: 0.0180 | Grad Norm: 0.00465347\n",
      "Epoch 5 | Step 177500 | Avg Loss: 0.0182 | Grad Norm: 0.00301807\n",
      "Epoch 5 | Step 177600 | Avg Loss: 0.0180 | Grad Norm: 0.00293835\n",
      "Epoch 5 | Step 177700 | Avg Loss: 0.0184 | Grad Norm: 0.00300057\n",
      "Epoch 5 | Step 177800 | Avg Loss: 0.0185 | Grad Norm: 0.00405891\n",
      "Epoch 5 | Step 177900 | Avg Loss: 0.0185 | Grad Norm: 0.00534495\n",
      "Epoch 5 | Step 178000 | Avg Loss: 0.0187 | Grad Norm: 0.00441737\n",
      "Epoch 5 | Step 178100 | Avg Loss: 0.0187 | Grad Norm: 0.00370802\n",
      "Epoch 5 | Step 178200 | Avg Loss: 0.0184 | Grad Norm: 0.00611988\n",
      "Epoch 5 | Step 178300 | Avg Loss: 0.0182 | Grad Norm: 0.00429582\n",
      "Epoch 5 | Step 178400 | Avg Loss: 0.0181 | Grad Norm: 0.00441423\n",
      "Epoch 5 | Step 178500 | Avg Loss: 0.0181 | Grad Norm: 0.00405648\n",
      "Epoch 5 | Step 178600 | Avg Loss: 0.0181 | Grad Norm: 0.00561956\n",
      "Epoch 5 | Step 178700 | Avg Loss: 0.0183 | Grad Norm: 0.00334090\n",
      "Epoch 5 | Step 178800 | Avg Loss: 0.0181 | Grad Norm: 0.00343589\n",
      "Epoch 5 | Step 178900 | Avg Loss: 0.0181 | Grad Norm: 0.00349959\n",
      "Epoch 5 | Step 179000 | Avg Loss: 0.0180 | Grad Norm: 0.00269806\n",
      "Epoch 5 | Step 179100 | Avg Loss: 0.0185 | Grad Norm: 0.00341923\n",
      "Epoch 5 | Step 179200 | Avg Loss: 0.0183 | Grad Norm: 0.00451864\n",
      "Epoch 5 | Step 179300 | Avg Loss: 0.0179 | Grad Norm: 0.00343702\n",
      "Epoch 5 | Step 179400 | Avg Loss: 0.0179 | Grad Norm: 0.00322526\n",
      "Epoch 5 | Step 179500 | Avg Loss: 0.0177 | Grad Norm: 0.00260523\n",
      "Epoch 5 | Step 179600 | Avg Loss: 0.0178 | Grad Norm: 0.00318911\n",
      "Epoch 5 | Step 179700 | Avg Loss: 0.0179 | Grad Norm: 0.00299768\n",
      "Epoch 5 | Step 179800 | Avg Loss: 0.0187 | Grad Norm: 0.00308091\n",
      "Epoch 5 | Step 179900 | Avg Loss: 0.0186 | Grad Norm: 0.00389763\n",
      "Epoch 5 | Step 180000 | Avg Loss: 0.0185 | Grad Norm: 0.00309052\n",
      "Epoch 5 | Step 180100 | Avg Loss: 0.0188 | Grad Norm: 0.00403787\n",
      "Epoch 5 | Step 180200 | Avg Loss: 0.0185 | Grad Norm: 0.00287438\n",
      "Epoch 5 | Step 180300 | Avg Loss: 0.0184 | Grad Norm: 0.00253180\n",
      "Epoch 5 | Step 180400 | Avg Loss: 0.0186 | Grad Norm: 0.00291185\n",
      "Epoch 5 | Step 180500 | Avg Loss: 0.0183 | Grad Norm: 0.00351536\n",
      "Epoch 5 | Step 180600 | Avg Loss: 0.0181 | Grad Norm: 0.00354619\n",
      "Epoch 5 | Step 180700 | Avg Loss: 0.0179 | Grad Norm: 0.00277522\n",
      "Epoch 5 | Step 180800 | Avg Loss: 0.0179 | Grad Norm: 0.00357770\n",
      "Epoch 5 | Step 180900 | Avg Loss: 0.0186 | Grad Norm: 0.00415753\n",
      "Epoch 5 | Step 181000 | Avg Loss: 0.0187 | Grad Norm: 0.00445848\n",
      "Epoch 5 | Step 181100 | Avg Loss: 0.0184 | Grad Norm: 0.00312146\n",
      "Epoch 5 | Step 181200 | Avg Loss: 0.0183 | Grad Norm: 0.00418691\n",
      "Epoch 5 | Step 181300 | Avg Loss: 0.0181 | Grad Norm: 0.00566068\n",
      "Epoch 5 | Step 181400 | Avg Loss: 0.0188 | Grad Norm: 0.00424422\n",
      "Epoch 5 | Step 181500 | Avg Loss: 0.0187 | Grad Norm: 0.00434570\n",
      "Epoch 5 | Step 181600 | Avg Loss: 0.0188 | Grad Norm: 0.00319413\n",
      "Epoch 5 | Step 181700 | Avg Loss: 0.0188 | Grad Norm: 0.00334147\n",
      "Epoch 5 | Step 181800 | Avg Loss: 0.0188 | Grad Norm: 0.00343145\n",
      "Epoch 5 | Step 181900 | Avg Loss: 0.0189 | Grad Norm: 0.00352896\n",
      "Epoch 5 | Step 182000 | Avg Loss: 0.0190 | Grad Norm: 0.00338227\n",
      "Epoch 5 | Step 182100 | Avg Loss: 0.0190 | Grad Norm: 0.00385268\n",
      "Epoch 5 | Step 182200 | Avg Loss: 0.0188 | Grad Norm: 0.00351677\n",
      "Epoch 5 | Step 182300 | Avg Loss: 0.0183 | Grad Norm: 0.00661270\n",
      "Epoch 5 | Step 182400 | Avg Loss: 0.0177 | Grad Norm: 0.00348479\n",
      "Epoch 5 | Step 182500 | Avg Loss: 0.0181 | Grad Norm: 0.00333111\n",
      "Epoch 5 | Step 182600 | Avg Loss: 0.0183 | Grad Norm: 0.00249550\n",
      "Epoch 5 | Step 182700 | Avg Loss: 0.0178 | Grad Norm: 0.00391100\n",
      "Epoch 5 | Step 182800 | Avg Loss: 0.0181 | Grad Norm: 0.00586857\n",
      "Epoch 5 | Step 182900 | Avg Loss: 0.0178 | Grad Norm: 0.00323114\n",
      "Epoch 5 | Step 183000 | Avg Loss: 0.0181 | Grad Norm: 0.00284054\n",
      "Epoch 5 | Step 183100 | Avg Loss: 0.0183 | Grad Norm: 0.00379034\n",
      "Epoch 5 | Step 183200 | Avg Loss: 0.0186 | Grad Norm: 0.00365840\n",
      "Epoch 5 | Step 183300 | Avg Loss: 0.0186 | Grad Norm: 0.00479802\n",
      "Epoch 5 | Step 183400 | Avg Loss: 0.0185 | Grad Norm: 0.00480307\n",
      "Epoch 5 | Step 183500 | Avg Loss: 0.0182 | Grad Norm: 0.00279396\n",
      "Epoch 5 | Step 183600 | Avg Loss: 0.0185 | Grad Norm: 0.00272322\n",
      "Epoch 5 | Step 183700 | Avg Loss: 0.0186 | Grad Norm: 0.00316789\n",
      "Epoch 5 | Step 183800 | Avg Loss: 0.0187 | Grad Norm: 0.00364091\n",
      "Epoch 5 | Step 183900 | Avg Loss: 0.0185 | Grad Norm: 0.00540053\n",
      "Epoch 5 | Step 184000 | Avg Loss: 0.0186 | Grad Norm: 0.00405584\n",
      "Epoch 5 | Step 184100 | Avg Loss: 0.0186 | Grad Norm: 0.00666045\n",
      "Epoch 5 | Step 184200 | Avg Loss: 0.0186 | Grad Norm: 0.00488960\n",
      "Epoch 5 | Step 184300 | Avg Loss: 0.0184 | Grad Norm: 0.00376427\n",
      "Epoch 5 | Step 184400 | Avg Loss: 0.0187 | Grad Norm: 0.00326041\n",
      "Epoch 5 | Step 184500 | Avg Loss: 0.0180 | Grad Norm: 0.00477579\n",
      "Epoch 5 | Step 184600 | Avg Loss: 0.0181 | Grad Norm: 0.00309280\n",
      "Epoch 5 | Step 184700 | Avg Loss: 0.0184 | Grad Norm: 0.00307375\n",
      "Epoch 5 | Step 184800 | Avg Loss: 0.0188 | Grad Norm: 0.00497627\n",
      "Epoch 5 | Step 184900 | Avg Loss: 0.0187 | Grad Norm: 0.00366399\n",
      "Epoch 5 | Step 185000 | Avg Loss: 0.0182 | Grad Norm: 0.00438237\n",
      "Epoch 5 | Step 185100 | Avg Loss: 0.0182 | Grad Norm: 0.00369492\n",
      "Epoch 5 | Step 185200 | Avg Loss: 0.0180 | Grad Norm: 0.00323515\n",
      "Epoch 5 | Step 185300 | Avg Loss: 0.0182 | Grad Norm: 0.00345216\n",
      "Epoch 5 | Step 185400 | Avg Loss: 0.0185 | Grad Norm: 0.00370867\n",
      "Epoch 5 | Step 185500 | Avg Loss: 0.0184 | Grad Norm: 0.00288393\n",
      "Epoch 5 | Step 185600 | Avg Loss: 0.0182 | Grad Norm: 0.00334958\n",
      "Epoch 5 | Step 185700 | Avg Loss: 0.0182 | Grad Norm: 0.00573387\n",
      "Epoch 5 | Step 185800 | Avg Loss: 0.0185 | Grad Norm: 0.00304149\n",
      "Epoch 5 | Step 185900 | Avg Loss: 0.0183 | Grad Norm: 0.00272144\n",
      "Epoch 5 | Step 186000 | Avg Loss: 0.0183 | Grad Norm: 0.00307945\n",
      "Epoch 5 | Step 186100 | Avg Loss: 0.0185 | Grad Norm: 0.00308579\n",
      "Epoch 5 | Step 186200 | Avg Loss: 0.0187 | Grad Norm: 0.00343281\n",
      "Epoch 5 | Step 186300 | Avg Loss: 0.0186 | Grad Norm: 0.00296157\n",
      "Epoch 5 | Step 186400 | Avg Loss: 0.0187 | Grad Norm: 0.00517805\n",
      "Epoch 5 | Step 186500 | Avg Loss: 0.0189 | Grad Norm: 0.00276497\n",
      "Epoch 5 | Step 186600 | Avg Loss: 0.0191 | Grad Norm: 0.00459878\n",
      "Epoch 5 | Step 186700 | Avg Loss: 0.0186 | Grad Norm: 0.00312119\n",
      "Epoch 5 | Step 186800 | Avg Loss: 0.0182 | Grad Norm: 0.00240650\n",
      "Epoch 5 | Step 186900 | Avg Loss: 0.0179 | Grad Norm: 0.00394158\n",
      "Epoch 5 | Step 187000 | Avg Loss: 0.0181 | Grad Norm: 0.00408612\n",
      "Epoch 5 | Step 187100 | Avg Loss: 0.0182 | Grad Norm: 0.00415370\n",
      "Epoch 5 | Step 187200 | Avg Loss: 0.0183 | Grad Norm: 0.00332310\n",
      "Epoch 5 | Step 187300 | Avg Loss: 0.0183 | Grad Norm: 0.00313333\n",
      "Epoch 5 | Step 187400 | Avg Loss: 0.0181 | Grad Norm: 0.00298464\n",
      "Epoch 5 | Step 187500 | Avg Loss: 0.0179 | Grad Norm: 0.00281070\n",
      "Epoch 5 | Step 187600 | Avg Loss: 0.0180 | Grad Norm: 0.00786335\n",
      "Epoch 5 | Step 187700 | Avg Loss: 0.0183 | Grad Norm: 0.00346365\n",
      "Epoch 5 | Step 187800 | Avg Loss: 0.0181 | Grad Norm: 0.00293777\n",
      "Epoch 5 | Step 187900 | Avg Loss: 0.0178 | Grad Norm: 0.00390881\n",
      "Epoch 5 | Step 188000 | Avg Loss: 0.0179 | Grad Norm: 0.00348077\n",
      "Epoch 5 | Step 188100 | Avg Loss: 0.0184 | Grad Norm: 0.00248106\n",
      "Epoch 5 | Step 188200 | Avg Loss: 0.0184 | Grad Norm: 0.00286112\n",
      "Epoch 5 | Step 188300 | Avg Loss: 0.0184 | Grad Norm: 0.00308244\n",
      "Epoch 5 | Step 188400 | Avg Loss: 0.0185 | Grad Norm: 0.00283546\n",
      "Epoch 5 | Step 188500 | Avg Loss: 0.0186 | Grad Norm: 0.00630123\n",
      "Epoch 5 | Step 188600 | Avg Loss: 0.0189 | Grad Norm: 0.00303601\n",
      "Epoch 5 | Step 188700 | Avg Loss: 0.0186 | Grad Norm: 0.00346951\n",
      "Epoch 5 | Step 188800 | Avg Loss: 0.0188 | Grad Norm: 0.00283198\n",
      "Epoch 5 | Step 188900 | Avg Loss: 0.0185 | Grad Norm: 0.00358130\n",
      "Epoch 5 | Step 189000 | Avg Loss: 0.0187 | Grad Norm: 0.00398564\n",
      "Epoch 5 | Step 189100 | Avg Loss: 0.0185 | Grad Norm: 0.00359750\n",
      "Epoch 5 | Step 189200 | Avg Loss: 0.0183 | Grad Norm: 0.00558161\n",
      "Epoch 5 | Step 189300 | Avg Loss: 0.0179 | Grad Norm: 0.00317538\n",
      "Epoch 5 | Step 189400 | Avg Loss: 0.0182 | Grad Norm: 0.00393116\n",
      "Epoch 5 | Step 189500 | Avg Loss: 0.0178 | Grad Norm: 0.00314303\n",
      "Epoch 5 | Step 189600 | Avg Loss: 0.0181 | Grad Norm: 0.00286736\n",
      "Epoch 5 | Step 189700 | Avg Loss: 0.0182 | Grad Norm: 0.00700657\n",
      "Epoch 5 | Step 189800 | Avg Loss: 0.0185 | Grad Norm: 0.00511914\n",
      "Epoch 5 | Step 189900 | Avg Loss: 0.0185 | Grad Norm: 0.00501005\n",
      "Epoch 5 | Step 190000 | Avg Loss: 0.0182 | Grad Norm: 0.00383472\n",
      "Epoch 5 | Step 190100 | Avg Loss: 0.0178 | Grad Norm: 0.00398406\n",
      "Epoch 5 | Step 190200 | Avg Loss: 0.0180 | Grad Norm: 0.00268344\n",
      "Epoch 5 | Step 190300 | Avg Loss: 0.0173 | Grad Norm: 0.00267571\n",
      "Epoch 5 | Step 190400 | Avg Loss: 0.0169 | Grad Norm: 0.00511794\n",
      "Epoch 5 | Step 190500 | Avg Loss: 0.0168 | Grad Norm: 0.00379294\n",
      "Epoch 5 | Step 190600 | Avg Loss: 0.0175 | Grad Norm: 0.00542087\n",
      "Epoch 5 | Step 190700 | Avg Loss: 0.0179 | Grad Norm: 0.00266366\n",
      "Epoch 5 | Step 190800 | Avg Loss: 0.0181 | Grad Norm: 0.00509850\n",
      "Epoch 5 | Step 190900 | Avg Loss: 0.0183 | Grad Norm: 0.00259331\n",
      "Epoch 5 | Step 191000 | Avg Loss: 0.0179 | Grad Norm: 0.00370334\n",
      "Epoch 5 | Step 191100 | Avg Loss: 0.0180 | Grad Norm: 0.00256413\n",
      "Epoch 5 | Step 191200 | Avg Loss: 0.0182 | Grad Norm: 0.00438088\n",
      "Epoch 5 | Step 191300 | Avg Loss: 0.0177 | Grad Norm: 0.00314546\n",
      "Epoch 5 | Step 191400 | Avg Loss: 0.0181 | Grad Norm: 0.00617803\n",
      "Epoch 5 | Step 191500 | Avg Loss: 0.0185 | Grad Norm: 0.00283471\n",
      "Epoch 5 | Step 191600 | Avg Loss: 0.0186 | Grad Norm: 0.00616204\n",
      "Epoch 5 | Step 191700 | Avg Loss: 0.0183 | Grad Norm: 0.00292146\n",
      "Epoch 5 | Step 191800 | Avg Loss: 0.0188 | Grad Norm: 0.00287743\n",
      "Epoch 5 | Step 191900 | Avg Loss: 0.0188 | Grad Norm: 0.00439631\n",
      "Epoch 5 | Step 192000 | Avg Loss: 0.0184 | Grad Norm: 0.00387570\n",
      "Epoch 5 | Step 192100 | Avg Loss: 0.0182 | Grad Norm: 0.00468706\n",
      "Epoch 5 | Step 192200 | Avg Loss: 0.0178 | Grad Norm: 0.00528081\n",
      "Epoch 5 | Step 192300 | Avg Loss: 0.0177 | Grad Norm: 0.00489180\n",
      "Epoch 5 | Step 192400 | Avg Loss: 0.0180 | Grad Norm: 0.00430515\n",
      "Epoch 5 | Step 192500 | Avg Loss: 0.0186 | Grad Norm: 0.00323622\n",
      "Epoch 5 | Step 192600 | Avg Loss: 0.0187 | Grad Norm: 0.00300706\n",
      "Epoch 5 | Step 192700 | Avg Loss: 0.0188 | Grad Norm: 0.00328871\n",
      "Epoch 5 | Step 192800 | Avg Loss: 0.0185 | Grad Norm: 0.00257883\n",
      "Epoch 5 | Step 192900 | Avg Loss: 0.0185 | Grad Norm: 0.00297578\n",
      "Epoch 5 | Step 193000 | Avg Loss: 0.0185 | Grad Norm: 0.00526007\n",
      "Epoch 5 | Step 193100 | Avg Loss: 0.0187 | Grad Norm: 0.00323611\n",
      "Epoch 5 | Step 193200 | Avg Loss: 0.0188 | Grad Norm: 0.00324890\n",
      "Epoch 5 | Step 193300 | Avg Loss: 0.0191 | Grad Norm: 0.00293766\n",
      "Epoch 5 | Step 193400 | Avg Loss: 0.0188 | Grad Norm: 0.00298713\n",
      "Epoch 5 | Step 193500 | Avg Loss: 0.0182 | Grad Norm: 0.00320526\n",
      "Epoch 5 | Step 193600 | Avg Loss: 0.0183 | Grad Norm: 0.00313815\n",
      "Epoch 5 | Step 193700 | Avg Loss: 0.0183 | Grad Norm: 0.00285772\n",
      "Epoch 5 | Step 193800 | Avg Loss: 0.0183 | Grad Norm: 0.00276158\n",
      "Epoch 5 | Step 193900 | Avg Loss: 0.0182 | Grad Norm: 0.00288806\n",
      "Epoch 5 | Step 194000 | Avg Loss: 0.0184 | Grad Norm: 0.00389744\n",
      "Epoch 5 | Step 194100 | Avg Loss: 0.0182 | Grad Norm: 0.00329518\n",
      "Epoch 5 | Step 194200 | Avg Loss: 0.0184 | Grad Norm: 0.00366421\n",
      "Epoch 5 | Step 194300 | Avg Loss: 0.0189 | Grad Norm: 0.00335670\n",
      "Epoch 5 | Step 194400 | Avg Loss: 0.0188 | Grad Norm: 0.00353357\n",
      "Epoch 5 | Step 194500 | Avg Loss: 0.0189 | Grad Norm: 0.00302259\n",
      "Epoch 5 | Step 194600 | Avg Loss: 0.0187 | Grad Norm: 0.00319236\n",
      "Epoch 5 | Step 194700 | Avg Loss: 0.0189 | Grad Norm: 0.00266371\n",
      "Epoch 5 | Step 194800 | Avg Loss: 0.0188 | Grad Norm: 0.00306634\n",
      "Epoch 5 | Step 194900 | Avg Loss: 0.0189 | Grad Norm: 0.00364665\n",
      "Epoch 5 | Step 195000 | Avg Loss: 0.0189 | Grad Norm: 0.00564954\n",
      "Epoch 5 | Step 195100 | Avg Loss: 0.0185 | Grad Norm: 0.00289231\n",
      "Epoch 5 | Step 195200 | Avg Loss: 0.0181 | Grad Norm: 0.00329859\n",
      "Epoch 5 | Step 195300 | Avg Loss: 0.0178 | Grad Norm: 0.00355541\n",
      "Epoch 5, Loss: 0.0165\n",
      "Epoch 6 | Step 195400 | Avg Loss: 0.0185 | Grad Norm: 0.00318119\n",
      "Epoch 6 | Step 195500 | Avg Loss: 0.0179 | Grad Norm: 0.00485514\n",
      "Epoch 6 | Step 195600 | Avg Loss: 0.0177 | Grad Norm: 0.00270985\n",
      "Epoch 6 | Step 195700 | Avg Loss: 0.0176 | Grad Norm: 0.00324338\n",
      "Epoch 6 | Step 195800 | Avg Loss: 0.0178 | Grad Norm: 0.00380041\n",
      "Epoch 6 | Step 195900 | Avg Loss: 0.0181 | Grad Norm: 0.00353728\n",
      "Epoch 6 | Step 196000 | Avg Loss: 0.0186 | Grad Norm: 0.00671299\n",
      "Epoch 6 | Step 196100 | Avg Loss: 0.0188 | Grad Norm: 0.00337845\n",
      "Epoch 6 | Step 196200 | Avg Loss: 0.0193 | Grad Norm: 0.00361598\n",
      "Epoch 6 | Step 196300 | Avg Loss: 0.0191 | Grad Norm: 0.00297348\n",
      "Epoch 6 | Step 196400 | Avg Loss: 0.0185 | Grad Norm: 0.00411087\n",
      "Epoch 6 | Step 196500 | Avg Loss: 0.0185 | Grad Norm: 0.00393258\n",
      "Epoch 6 | Step 196600 | Avg Loss: 0.0183 | Grad Norm: 0.00412130\n",
      "Epoch 6 | Step 196700 | Avg Loss: 0.0183 | Grad Norm: 0.00272863\n",
      "Epoch 6 | Step 196800 | Avg Loss: 0.0181 | Grad Norm: 0.00400523\n",
      "Epoch 6 | Step 196900 | Avg Loss: 0.0180 | Grad Norm: 0.00369707\n",
      "Epoch 6 | Step 197000 | Avg Loss: 0.0177 | Grad Norm: 0.00310409\n",
      "Epoch 6 | Step 197100 | Avg Loss: 0.0179 | Grad Norm: 0.00295636\n",
      "Epoch 6 | Step 197200 | Avg Loss: 0.0178 | Grad Norm: 0.00464207\n",
      "Epoch 6 | Step 197300 | Avg Loss: 0.0179 | Grad Norm: 0.00668968\n",
      "Epoch 6 | Step 197400 | Avg Loss: 0.0179 | Grad Norm: 0.00463746\n",
      "Epoch 6 | Step 197500 | Avg Loss: 0.0179 | Grad Norm: 0.00310491\n",
      "Epoch 6 | Step 197600 | Avg Loss: 0.0182 | Grad Norm: 0.00309072\n",
      "Epoch 6 | Step 197700 | Avg Loss: 0.0180 | Grad Norm: 0.00258465\n",
      "Epoch 6 | Step 197800 | Avg Loss: 0.0182 | Grad Norm: 0.00313902\n",
      "Epoch 6 | Step 197900 | Avg Loss: 0.0184 | Grad Norm: 0.00336428\n",
      "Epoch 6 | Step 198000 | Avg Loss: 0.0185 | Grad Norm: 0.00365268\n",
      "Epoch 6 | Step 198100 | Avg Loss: 0.0185 | Grad Norm: 0.00318755\n",
      "Epoch 6 | Step 198200 | Avg Loss: 0.0185 | Grad Norm: 0.00274193\n",
      "Epoch 6 | Step 198300 | Avg Loss: 0.0184 | Grad Norm: 0.00261921\n",
      "Epoch 6 | Step 198400 | Avg Loss: 0.0184 | Grad Norm: 0.00557757\n",
      "Epoch 6 | Step 198500 | Avg Loss: 0.0181 | Grad Norm: 0.00320039\n",
      "Epoch 6 | Step 198600 | Avg Loss: 0.0177 | Grad Norm: 0.00289515\n",
      "Epoch 6 | Step 198700 | Avg Loss: 0.0176 | Grad Norm: 0.00363934\n",
      "Epoch 6 | Step 198800 | Avg Loss: 0.0174 | Grad Norm: 0.00590811\n",
      "Epoch 6 | Step 198900 | Avg Loss: 0.0174 | Grad Norm: 0.00412791\n",
      "Epoch 6 | Step 199000 | Avg Loss: 0.0177 | Grad Norm: 0.00511617\n",
      "Epoch 6 | Step 199100 | Avg Loss: 0.0179 | Grad Norm: 0.00396028\n",
      "Epoch 6 | Step 199200 | Avg Loss: 0.0180 | Grad Norm: 0.00297011\n",
      "Epoch 6 | Step 199300 | Avg Loss: 0.0184 | Grad Norm: 0.00317552\n",
      "Epoch 6 | Step 199400 | Avg Loss: 0.0179 | Grad Norm: 0.00288576\n",
      "Epoch 6 | Step 199500 | Avg Loss: 0.0178 | Grad Norm: 0.00288398\n",
      "Epoch 6 | Step 199600 | Avg Loss: 0.0178 | Grad Norm: 0.00277422\n",
      "Epoch 6 | Step 199700 | Avg Loss: 0.0182 | Grad Norm: 0.00355009\n",
      "Epoch 6 | Step 199800 | Avg Loss: 0.0179 | Grad Norm: 0.00392237\n",
      "Epoch 6 | Step 199900 | Avg Loss: 0.0177 | Grad Norm: 0.00470775\n",
      "Epoch 6 | Step 200000 | Avg Loss: 0.0175 | Grad Norm: 0.00421320\n",
      "Saving model at step200000\n",
      "Epoch 6 | Step 200100 | Avg Loss: 0.0175 | Grad Norm: 0.00377977\n",
      "Epoch 6 | Step 200200 | Avg Loss: 0.0177 | Grad Norm: 0.00360347\n",
      "Epoch 6 | Step 200300 | Avg Loss: 0.0179 | Grad Norm: 0.00358894\n",
      "Epoch 6 | Step 200400 | Avg Loss: 0.0179 | Grad Norm: 0.00466101\n",
      "Epoch 6 | Step 200500 | Avg Loss: 0.0177 | Grad Norm: 0.00371866\n",
      "Epoch 6 | Step 200600 | Avg Loss: 0.0179 | Grad Norm: 0.00307288\n",
      "Epoch 6 | Step 200700 | Avg Loss: 0.0186 | Grad Norm: 0.00483894\n",
      "Epoch 6 | Step 200800 | Avg Loss: 0.0190 | Grad Norm: 0.00352890\n",
      "Epoch 6 | Step 200900 | Avg Loss: 0.0185 | Grad Norm: 0.00289182\n",
      "Epoch 6 | Step 201000 | Avg Loss: 0.0184 | Grad Norm: 0.00374238\n",
      "Epoch 6 | Step 201100 | Avg Loss: 0.0184 | Grad Norm: 0.00568285\n",
      "Epoch 6 | Step 201200 | Avg Loss: 0.0180 | Grad Norm: 0.00284960\n",
      "Epoch 6 | Step 201300 | Avg Loss: 0.0182 | Grad Norm: 0.00408510\n",
      "Epoch 6 | Step 201400 | Avg Loss: 0.0180 | Grad Norm: 0.00326643\n",
      "Epoch 6 | Step 201500 | Avg Loss: 0.0178 | Grad Norm: 0.00308083\n",
      "Epoch 6 | Step 201600 | Avg Loss: 0.0182 | Grad Norm: 0.00268898\n",
      "Epoch 6 | Step 201700 | Avg Loss: 0.0182 | Grad Norm: 0.00312266\n",
      "Epoch 6 | Step 201800 | Avg Loss: 0.0181 | Grad Norm: 0.00304670\n",
      "Epoch 6 | Step 201900 | Avg Loss: 0.0183 | Grad Norm: 0.00314732\n",
      "Epoch 6 | Step 202000 | Avg Loss: 0.0182 | Grad Norm: 0.00573571\n",
      "Epoch 6 | Step 202100 | Avg Loss: 0.0180 | Grad Norm: 0.00292460\n",
      "Epoch 6 | Step 202200 | Avg Loss: 0.0180 | Grad Norm: 0.00419788\n",
      "Epoch 6 | Step 202300 | Avg Loss: 0.0179 | Grad Norm: 0.00271931\n",
      "Epoch 6 | Step 202400 | Avg Loss: 0.0180 | Grad Norm: 0.00598840\n",
      "Epoch 6 | Step 202500 | Avg Loss: 0.0174 | Grad Norm: 0.00283696\n",
      "Epoch 6 | Step 202600 | Avg Loss: 0.0173 | Grad Norm: 0.00517011\n",
      "Epoch 6 | Step 202700 | Avg Loss: 0.0175 | Grad Norm: 0.00405588\n",
      "Epoch 6 | Step 202800 | Avg Loss: 0.0176 | Grad Norm: 0.00316476\n",
      "Epoch 6 | Step 202900 | Avg Loss: 0.0178 | Grad Norm: 0.00326591\n",
      "Epoch 6 | Step 203000 | Avg Loss: 0.0176 | Grad Norm: 0.00341064\n",
      "Epoch 6 | Step 203100 | Avg Loss: 0.0177 | Grad Norm: 0.00372827\n",
      "Epoch 6 | Step 203200 | Avg Loss: 0.0176 | Grad Norm: 0.00293625\n",
      "Epoch 6 | Step 203300 | Avg Loss: 0.0181 | Grad Norm: 0.00358148\n",
      "Epoch 6 | Step 203400 | Avg Loss: 0.0181 | Grad Norm: 0.00513726\n",
      "Epoch 6 | Step 203500 | Avg Loss: 0.0181 | Grad Norm: 0.00664780\n",
      "Epoch 6 | Step 203600 | Avg Loss: 0.0186 | Grad Norm: 0.00356642\n",
      "Epoch 6 | Step 203700 | Avg Loss: 0.0182 | Grad Norm: 0.00354337\n",
      "Epoch 6 | Step 203800 | Avg Loss: 0.0181 | Grad Norm: 0.00302830\n",
      "Epoch 6 | Step 203900 | Avg Loss: 0.0188 | Grad Norm: 0.00320869\n",
      "Epoch 6 | Step 204000 | Avg Loss: 0.0187 | Grad Norm: 0.00276744\n",
      "Epoch 6 | Step 204100 | Avg Loss: 0.0190 | Grad Norm: 0.00493591\n",
      "Epoch 6 | Step 204200 | Avg Loss: 0.0185 | Grad Norm: 0.00299715\n",
      "Epoch 6 | Step 204300 | Avg Loss: 0.0185 | Grad Norm: 0.00368691\n",
      "Epoch 6 | Step 204400 | Avg Loss: 0.0183 | Grad Norm: 0.00258068\n",
      "Epoch 6 | Step 204500 | Avg Loss: 0.0183 | Grad Norm: 0.00484011\n",
      "Epoch 6 | Step 204600 | Avg Loss: 0.0177 | Grad Norm: 0.00466090\n",
      "Epoch 6 | Step 204700 | Avg Loss: 0.0177 | Grad Norm: 0.00290238\n",
      "Epoch 6 | Step 204800 | Avg Loss: 0.0181 | Grad Norm: 0.00333713\n",
      "Epoch 6 | Step 204900 | Avg Loss: 0.0179 | Grad Norm: 0.00391889\n",
      "Epoch 6 | Step 205000 | Avg Loss: 0.0181 | Grad Norm: 0.00334243\n",
      "Epoch 6 | Step 205100 | Avg Loss: 0.0180 | Grad Norm: 0.00315537\n",
      "Epoch 6 | Step 205200 | Avg Loss: 0.0183 | Grad Norm: 0.00353341\n",
      "Epoch 6 | Step 205300 | Avg Loss: 0.0182 | Grad Norm: 0.00395206\n",
      "Epoch 6 | Step 205400 | Avg Loss: 0.0185 | Grad Norm: 0.00368218\n",
      "Epoch 6 | Step 205500 | Avg Loss: 0.0189 | Grad Norm: 0.00402816\n",
      "Epoch 6 | Step 205600 | Avg Loss: 0.0183 | Grad Norm: 0.00305858\n",
      "Epoch 6 | Step 205700 | Avg Loss: 0.0183 | Grad Norm: 0.00283064\n",
      "Epoch 6 | Step 205800 | Avg Loss: 0.0185 | Grad Norm: 0.00324079\n",
      "Epoch 6 | Step 205900 | Avg Loss: 0.0188 | Grad Norm: 0.00279492\n",
      "Epoch 6 | Step 206000 | Avg Loss: 0.0184 | Grad Norm: 0.00564187\n",
      "Epoch 6 | Step 206100 | Avg Loss: 0.0186 | Grad Norm: 0.00328550\n",
      "Epoch 6 | Step 206200 | Avg Loss: 0.0187 | Grad Norm: 0.00296490\n",
      "Epoch 6 | Step 206300 | Avg Loss: 0.0186 | Grad Norm: 0.00451110\n",
      "Epoch 6 | Step 206400 | Avg Loss: 0.0190 | Grad Norm: 0.00487240\n",
      "Epoch 6 | Step 206500 | Avg Loss: 0.0189 | Grad Norm: 0.00391021\n",
      "Epoch 6 | Step 206600 | Avg Loss: 0.0188 | Grad Norm: 0.00343676\n",
      "Epoch 6 | Step 206700 | Avg Loss: 0.0183 | Grad Norm: 0.00335118\n",
      "Epoch 6 | Step 206800 | Avg Loss: 0.0188 | Grad Norm: 0.00362633\n",
      "Epoch 6 | Step 206900 | Avg Loss: 0.0186 | Grad Norm: 0.00434656\n",
      "Epoch 6 | Step 207000 | Avg Loss: 0.0181 | Grad Norm: 0.00381554\n",
      "Epoch 6 | Step 207100 | Avg Loss: 0.0183 | Grad Norm: 0.00715173\n",
      "Epoch 6 | Step 207200 | Avg Loss: 0.0179 | Grad Norm: 0.00268261\n",
      "Epoch 6 | Step 207300 | Avg Loss: 0.0180 | Grad Norm: 0.00284802\n",
      "Epoch 6 | Step 207400 | Avg Loss: 0.0176 | Grad Norm: 0.00505009\n",
      "Epoch 6 | Step 207500 | Avg Loss: 0.0179 | Grad Norm: 0.00293849\n",
      "Epoch 6 | Step 207600 | Avg Loss: 0.0177 | Grad Norm: 0.00289220\n",
      "Epoch 6 | Step 207700 | Avg Loss: 0.0187 | Grad Norm: 0.00624900\n",
      "Epoch 6 | Step 207800 | Avg Loss: 0.0187 | Grad Norm: 0.00328559\n",
      "Epoch 6 | Step 207900 | Avg Loss: 0.0181 | Grad Norm: 0.00312055\n",
      "Epoch 6 | Step 208000 | Avg Loss: 0.0183 | Grad Norm: 0.00373423\n",
      "Epoch 6 | Step 208100 | Avg Loss: 0.0178 | Grad Norm: 0.00616875\n",
      "Epoch 6 | Step 208200 | Avg Loss: 0.0180 | Grad Norm: 0.00300030\n",
      "Epoch 6 | Step 208300 | Avg Loss: 0.0182 | Grad Norm: 0.00269901\n",
      "Epoch 6 | Step 208400 | Avg Loss: 0.0181 | Grad Norm: 0.00444259\n",
      "Epoch 6 | Step 208500 | Avg Loss: 0.0179 | Grad Norm: 0.00301894\n",
      "Epoch 6 | Step 208600 | Avg Loss: 0.0180 | Grad Norm: 0.00318740\n",
      "Epoch 6 | Step 208700 | Avg Loss: 0.0180 | Grad Norm: 0.00339292\n",
      "Epoch 6 | Step 208800 | Avg Loss: 0.0182 | Grad Norm: 0.00504642\n",
      "Epoch 6 | Step 208900 | Avg Loss: 0.0179 | Grad Norm: 0.00385099\n",
      "Epoch 6 | Step 209000 | Avg Loss: 0.0177 | Grad Norm: 0.00332913\n",
      "Epoch 6 | Step 209100 | Avg Loss: 0.0178 | Grad Norm: 0.00253925\n",
      "Epoch 6 | Step 209200 | Avg Loss: 0.0185 | Grad Norm: 0.00331772\n",
      "Epoch 6 | Step 209300 | Avg Loss: 0.0184 | Grad Norm: 0.00526857\n",
      "Epoch 6 | Step 209400 | Avg Loss: 0.0179 | Grad Norm: 0.00357637\n",
      "Epoch 6 | Step 209500 | Avg Loss: 0.0180 | Grad Norm: 0.00256902\n",
      "Epoch 6 | Step 209600 | Avg Loss: 0.0181 | Grad Norm: 0.00270774\n",
      "Epoch 6 | Step 209700 | Avg Loss: 0.0179 | Grad Norm: 0.00381409\n",
      "Epoch 6 | Step 209800 | Avg Loss: 0.0182 | Grad Norm: 0.00480032\n",
      "Epoch 6 | Step 209900 | Avg Loss: 0.0184 | Grad Norm: 0.00573070\n",
      "Epoch 6 | Step 210000 | Avg Loss: 0.0185 | Grad Norm: 0.00298169\n",
      "Epoch 6 | Step 210100 | Avg Loss: 0.0179 | Grad Norm: 0.00293516\n",
      "Epoch 6 | Step 210200 | Avg Loss: 0.0181 | Grad Norm: 0.00318762\n",
      "Epoch 6 | Step 210300 | Avg Loss: 0.0183 | Grad Norm: 0.00442030\n",
      "Epoch 6 | Step 210400 | Avg Loss: 0.0183 | Grad Norm: 0.00374191\n",
      "Epoch 6 | Step 210500 | Avg Loss: 0.0183 | Grad Norm: 0.00343073\n",
      "Epoch 6 | Step 210600 | Avg Loss: 0.0183 | Grad Norm: 0.00284626\n",
      "Epoch 6 | Step 210700 | Avg Loss: 0.0181 | Grad Norm: 0.00376743\n",
      "Epoch 6 | Step 210800 | Avg Loss: 0.0181 | Grad Norm: 0.00318549\n",
      "Epoch 6 | Step 210900 | Avg Loss: 0.0184 | Grad Norm: 0.00300930\n",
      "Epoch 6 | Step 211000 | Avg Loss: 0.0187 | Grad Norm: 0.00345739\n",
      "Epoch 6 | Step 211100 | Avg Loss: 0.0192 | Grad Norm: 0.00467600\n",
      "Epoch 6 | Step 211200 | Avg Loss: 0.0194 | Grad Norm: 0.00499713\n",
      "Epoch 6 | Step 211300 | Avg Loss: 0.0192 | Grad Norm: 0.00353844\n",
      "Epoch 6 | Step 211400 | Avg Loss: 0.0192 | Grad Norm: 0.00435246\n",
      "Epoch 6 | Step 211500 | Avg Loss: 0.0192 | Grad Norm: 0.00367228\n",
      "Epoch 6 | Step 211600 | Avg Loss: 0.0190 | Grad Norm: 0.00365042\n",
      "Epoch 6 | Step 211700 | Avg Loss: 0.0187 | Grad Norm: 0.00369943\n",
      "Epoch 6 | Step 211800 | Avg Loss: 0.0191 | Grad Norm: 0.00609843\n",
      "Epoch 6 | Step 211900 | Avg Loss: 0.0183 | Grad Norm: 0.00772519\n",
      "Epoch 6 | Step 212000 | Avg Loss: 0.0182 | Grad Norm: 0.00324464\n",
      "Epoch 6 | Step 212100 | Avg Loss: 0.0181 | Grad Norm: 0.00305271\n",
      "Epoch 6 | Step 212200 | Avg Loss: 0.0185 | Grad Norm: 0.00318614\n",
      "Epoch 6 | Step 212300 | Avg Loss: 0.0185 | Grad Norm: 0.00694876\n",
      "Epoch 6 | Step 212400 | Avg Loss: 0.0182 | Grad Norm: 0.00316729\n",
      "Epoch 6 | Step 212500 | Avg Loss: 0.0180 | Grad Norm: 0.00422002\n",
      "Epoch 6 | Step 212600 | Avg Loss: 0.0181 | Grad Norm: 0.00287829\n",
      "Epoch 6 | Step 212700 | Avg Loss: 0.0179 | Grad Norm: 0.00491059\n",
      "Epoch 6 | Step 212800 | Avg Loss: 0.0179 | Grad Norm: 0.00352577\n",
      "Epoch 6 | Step 212900 | Avg Loss: 0.0178 | Grad Norm: 0.00299457\n",
      "Epoch 6 | Step 213000 | Avg Loss: 0.0176 | Grad Norm: 0.00554923\n",
      "Epoch 6 | Step 213100 | Avg Loss: 0.0177 | Grad Norm: 0.00426208\n",
      "Epoch 6 | Step 213200 | Avg Loss: 0.0178 | Grad Norm: 0.00588015\n",
      "Epoch 6 | Step 213300 | Avg Loss: 0.0181 | Grad Norm: 0.00384218\n",
      "Epoch 6 | Step 213400 | Avg Loss: 0.0176 | Grad Norm: 0.00314566\n",
      "Epoch 6 | Step 213500 | Avg Loss: 0.0178 | Grad Norm: 0.00352790\n",
      "Epoch 6 | Step 213600 | Avg Loss: 0.0180 | Grad Norm: 0.00302886\n",
      "Epoch 6 | Step 213700 | Avg Loss: 0.0182 | Grad Norm: 0.00296445\n",
      "Epoch 6 | Step 213800 | Avg Loss: 0.0180 | Grad Norm: 0.00445523\n",
      "Epoch 6 | Step 213900 | Avg Loss: 0.0181 | Grad Norm: 0.00346781\n",
      "Epoch 6 | Step 214000 | Avg Loss: 0.0184 | Grad Norm: 0.00311239\n",
      "Epoch 6 | Step 214100 | Avg Loss: 0.0185 | Grad Norm: 0.00278088\n",
      "Epoch 6 | Step 214200 | Avg Loss: 0.0186 | Grad Norm: 0.00410213\n",
      "Epoch 6 | Step 214300 | Avg Loss: 0.0185 | Grad Norm: 0.00292322\n",
      "Epoch 6 | Step 214400 | Avg Loss: 0.0187 | Grad Norm: 0.00368236\n",
      "Epoch 6 | Step 214500 | Avg Loss: 0.0186 | Grad Norm: 0.00389522\n",
      "Epoch 6 | Step 214600 | Avg Loss: 0.0184 | Grad Norm: 0.00327671\n",
      "Epoch 6 | Step 214700 | Avg Loss: 0.0180 | Grad Norm: 0.00371098\n",
      "Epoch 6 | Step 214800 | Avg Loss: 0.0177 | Grad Norm: 0.00325720\n",
      "Epoch 6 | Step 214900 | Avg Loss: 0.0174 | Grad Norm: 0.00562566\n",
      "Epoch 6 | Step 215000 | Avg Loss: 0.0177 | Grad Norm: 0.00300753\n",
      "Epoch 6 | Step 215100 | Avg Loss: 0.0174 | Grad Norm: 0.00272351\n",
      "Epoch 6 | Step 215200 | Avg Loss: 0.0175 | Grad Norm: 0.00420825\n",
      "Epoch 6 | Step 215300 | Avg Loss: 0.0177 | Grad Norm: 0.00341332\n",
      "Epoch 6 | Step 215400 | Avg Loss: 0.0179 | Grad Norm: 0.00336866\n",
      "Epoch 6 | Step 215500 | Avg Loss: 0.0171 | Grad Norm: 0.00237249\n",
      "Epoch 6 | Step 215600 | Avg Loss: 0.0175 | Grad Norm: 0.00365045\n",
      "Epoch 6 | Step 215700 | Avg Loss: 0.0179 | Grad Norm: 0.00275271\n",
      "Epoch 6 | Step 215800 | Avg Loss: 0.0182 | Grad Norm: 0.00329615\n",
      "Epoch 6 | Step 215900 | Avg Loss: 0.0183 | Grad Norm: 0.00325119\n",
      "Epoch 6 | Step 216000 | Avg Loss: 0.0180 | Grad Norm: 0.00287216\n",
      "Epoch 6 | Step 216100 | Avg Loss: 0.0179 | Grad Norm: 0.00531794\n",
      "Epoch 6 | Step 216200 | Avg Loss: 0.0179 | Grad Norm: 0.00433037\n",
      "Epoch 6 | Step 216300 | Avg Loss: 0.0180 | Grad Norm: 0.00253279\n",
      "Epoch 6 | Step 216400 | Avg Loss: 0.0178 | Grad Norm: 0.00296053\n",
      "Epoch 6 | Step 216500 | Avg Loss: 0.0180 | Grad Norm: 0.00287967\n",
      "Epoch 6 | Step 216600 | Avg Loss: 0.0178 | Grad Norm: 0.00345550\n",
      "Epoch 6 | Step 216700 | Avg Loss: 0.0178 | Grad Norm: 0.00578477\n",
      "Epoch 6 | Step 216800 | Avg Loss: 0.0185 | Grad Norm: 0.00561089\n",
      "Epoch 6 | Step 216900 | Avg Loss: 0.0183 | Grad Norm: 0.00452652\n",
      "Epoch 6 | Step 217000 | Avg Loss: 0.0182 | Grad Norm: 0.00359283\n",
      "Epoch 6 | Step 217100 | Avg Loss: 0.0188 | Grad Norm: 0.00310859\n",
      "Epoch 6 | Step 217200 | Avg Loss: 0.0184 | Grad Norm: 0.00407749\n",
      "Epoch 6 | Step 217300 | Avg Loss: 0.0184 | Grad Norm: 0.00335609\n",
      "Epoch 6 | Step 217400 | Avg Loss: 0.0182 | Grad Norm: 0.00251907\n",
      "Epoch 6 | Step 217500 | Avg Loss: 0.0179 | Grad Norm: 0.00310917\n",
      "Epoch 6 | Step 217600 | Avg Loss: 0.0180 | Grad Norm: 0.00526657\n",
      "Epoch 6 | Step 217700 | Avg Loss: 0.0180 | Grad Norm: 0.00415704\n",
      "Epoch 6 | Step 217800 | Avg Loss: 0.0178 | Grad Norm: 0.00577820\n",
      "Epoch 6 | Step 217900 | Avg Loss: 0.0179 | Grad Norm: 0.00426204\n",
      "Epoch 6 | Step 218000 | Avg Loss: 0.0179 | Grad Norm: 0.00377676\n",
      "Epoch 6 | Step 218100 | Avg Loss: 0.0181 | Grad Norm: 0.00355327\n",
      "Epoch 6 | Step 218200 | Avg Loss: 0.0183 | Grad Norm: 0.00388730\n",
      "Epoch 6 | Step 218300 | Avg Loss: 0.0179 | Grad Norm: 0.00434446\n",
      "Epoch 6 | Step 218400 | Avg Loss: 0.0179 | Grad Norm: 0.00347431\n",
      "Epoch 6 | Step 218500 | Avg Loss: 0.0176 | Grad Norm: 0.00639948\n",
      "Epoch 6 | Step 218600 | Avg Loss: 0.0177 | Grad Norm: 0.00311314\n",
      "Epoch 6 | Step 218700 | Avg Loss: 0.0178 | Grad Norm: 0.00282352\n",
      "Epoch 6 | Step 218800 | Avg Loss: 0.0180 | Grad Norm: 0.00304497\n",
      "Epoch 6 | Step 218900 | Avg Loss: 0.0185 | Grad Norm: 0.00353736\n",
      "Epoch 6 | Step 219000 | Avg Loss: 0.0184 | Grad Norm: 0.00332902\n",
      "Epoch 6 | Step 219100 | Avg Loss: 0.0186 | Grad Norm: 0.00349244\n",
      "Epoch 6 | Step 219200 | Avg Loss: 0.0184 | Grad Norm: 0.00261846\n",
      "Epoch 6 | Step 219300 | Avg Loss: 0.0186 | Grad Norm: 0.00318348\n",
      "Epoch 6 | Step 219400 | Avg Loss: 0.0180 | Grad Norm: 0.00286918\n",
      "Epoch 6 | Step 219500 | Avg Loss: 0.0184 | Grad Norm: 0.00490665\n",
      "Epoch 6 | Step 219600 | Avg Loss: 0.0182 | Grad Norm: 0.00397416\n",
      "Epoch 6 | Step 219700 | Avg Loss: 0.0177 | Grad Norm: 0.00297742\n",
      "Epoch 6 | Step 219800 | Avg Loss: 0.0178 | Grad Norm: 0.00470454\n",
      "Epoch 6 | Step 219900 | Avg Loss: 0.0182 | Grad Norm: 0.00363596\n",
      "Epoch 6 | Step 220000 | Avg Loss: 0.0186 | Grad Norm: 0.00395284\n",
      "Epoch 6 | Step 220100 | Avg Loss: 0.0183 | Grad Norm: 0.00452549\n",
      "Epoch 6 | Step 220200 | Avg Loss: 0.0180 | Grad Norm: 0.00357314\n",
      "Epoch 6 | Step 220300 | Avg Loss: 0.0180 | Grad Norm: 0.00375344\n",
      "Epoch 6 | Step 220400 | Avg Loss: 0.0183 | Grad Norm: 0.00307231\n",
      "Epoch 6 | Step 220500 | Avg Loss: 0.0183 | Grad Norm: 0.00254557\n",
      "Epoch 6 | Step 220600 | Avg Loss: 0.0187 | Grad Norm: 0.00663876\n",
      "Epoch 6 | Step 220700 | Avg Loss: 0.0189 | Grad Norm: 0.00367018\n",
      "Epoch 6 | Step 220800 | Avg Loss: 0.0188 | Grad Norm: 0.00536880\n",
      "Epoch 6 | Step 220900 | Avg Loss: 0.0188 | Grad Norm: 0.00299985\n",
      "Epoch 6 | Step 221000 | Avg Loss: 0.0186 | Grad Norm: 0.00454382\n",
      "Epoch 6 | Step 221100 | Avg Loss: 0.0186 | Grad Norm: 0.00297727\n",
      "Epoch 6 | Step 221200 | Avg Loss: 0.0186 | Grad Norm: 0.00312922\n",
      "Epoch 6 | Step 221300 | Avg Loss: 0.0186 | Grad Norm: 0.00331101\n",
      "Epoch 6 | Step 221400 | Avg Loss: 0.0179 | Grad Norm: 0.00304438\n",
      "Epoch 6 | Step 221500 | Avg Loss: 0.0178 | Grad Norm: 0.00305878\n",
      "Epoch 6 | Step 221600 | Avg Loss: 0.0183 | Grad Norm: 0.00408744\n",
      "Epoch 6 | Step 221700 | Avg Loss: 0.0179 | Grad Norm: 0.00298910\n",
      "Epoch 6 | Step 221800 | Avg Loss: 0.0179 | Grad Norm: 0.00331270\n",
      "Epoch 6 | Step 221900 | Avg Loss: 0.0177 | Grad Norm: 0.00603770\n",
      "Epoch 6 | Step 222000 | Avg Loss: 0.0180 | Grad Norm: 0.00393036\n",
      "Epoch 6 | Step 222100 | Avg Loss: 0.0181 | Grad Norm: 0.00553788\n",
      "Epoch 6 | Step 222200 | Avg Loss: 0.0184 | Grad Norm: 0.00266099\n",
      "Epoch 6 | Step 222300 | Avg Loss: 0.0184 | Grad Norm: 0.00328063\n",
      "Epoch 6 | Step 222400 | Avg Loss: 0.0183 | Grad Norm: 0.00329088\n",
      "Epoch 6 | Step 222500 | Avg Loss: 0.0181 | Grad Norm: 0.00416820\n",
      "Epoch 6 | Step 222600 | Avg Loss: 0.0178 | Grad Norm: 0.00259891\n",
      "Epoch 6 | Step 222700 | Avg Loss: 0.0183 | Grad Norm: 0.00414487\n",
      "Epoch 6 | Step 222800 | Avg Loss: 0.0186 | Grad Norm: 0.00460141\n",
      "Epoch 6 | Step 222900 | Avg Loss: 0.0185 | Grad Norm: 0.00247461\n",
      "Epoch 6 | Step 223000 | Avg Loss: 0.0184 | Grad Norm: 0.00405656\n",
      "Epoch 6 | Step 223100 | Avg Loss: 0.0186 | Grad Norm: 0.00388361\n",
      "Epoch 6 | Step 223200 | Avg Loss: 0.0185 | Grad Norm: 0.00310954\n",
      "Epoch 6 | Step 223300 | Avg Loss: 0.0184 | Grad Norm: 0.00302071\n",
      "Epoch 6 | Step 223400 | Avg Loss: 0.0185 | Grad Norm: 0.00295056\n",
      "Epoch 6 | Step 223500 | Avg Loss: 0.0185 | Grad Norm: 0.00307940\n",
      "Epoch 6 | Step 223600 | Avg Loss: 0.0178 | Grad Norm: 0.00368567\n",
      "Epoch 6 | Step 223700 | Avg Loss: 0.0180 | Grad Norm: 0.00293459\n",
      "Epoch 6 | Step 223800 | Avg Loss: 0.0185 | Grad Norm: 0.00558090\n",
      "Epoch 6 | Step 223900 | Avg Loss: 0.0186 | Grad Norm: 0.00271392\n",
      "Epoch 6 | Step 224000 | Avg Loss: 0.0183 | Grad Norm: 0.00290734\n",
      "Epoch 6 | Step 224100 | Avg Loss: 0.0180 | Grad Norm: 0.00596984\n",
      "Epoch 6 | Step 224200 | Avg Loss: 0.0182 | Grad Norm: 0.00453657\n",
      "Epoch 6 | Step 224300 | Avg Loss: 0.0179 | Grad Norm: 0.00305365\n",
      "Epoch 6 | Step 224400 | Avg Loss: 0.0181 | Grad Norm: 0.00230610\n",
      "Epoch 6 | Step 224500 | Avg Loss: 0.0183 | Grad Norm: 0.00529581\n",
      "Epoch 6 | Step 224600 | Avg Loss: 0.0182 | Grad Norm: 0.00319503\n",
      "Epoch 6 | Step 224700 | Avg Loss: 0.0181 | Grad Norm: 0.00381904\n",
      "Epoch 6 | Step 224800 | Avg Loss: 0.0182 | Grad Norm: 0.00357173\n",
      "Epoch 6 | Step 224900 | Avg Loss: 0.0182 | Grad Norm: 0.00277347\n",
      "Epoch 6 | Step 225000 | Avg Loss: 0.0183 | Grad Norm: 0.00462509\n",
      "Epoch 6 | Step 225100 | Avg Loss: 0.0183 | Grad Norm: 0.00310833\n",
      "Epoch 6 | Step 225200 | Avg Loss: 0.0186 | Grad Norm: 0.00432985\n",
      "Epoch 6 | Step 225300 | Avg Loss: 0.0183 | Grad Norm: 0.00383045\n",
      "Epoch 6 | Step 225400 | Avg Loss: 0.0184 | Grad Norm: 0.00564011\n",
      "Epoch 6 | Step 225500 | Avg Loss: 0.0188 | Grad Norm: 0.00292784\n",
      "Epoch 6 | Step 225600 | Avg Loss: 0.0188 | Grad Norm: 0.00410011\n",
      "Epoch 6 | Step 225700 | Avg Loss: 0.0190 | Grad Norm: 0.00361449\n",
      "Epoch 6 | Step 225800 | Avg Loss: 0.0184 | Grad Norm: 0.00382756\n",
      "Epoch 6 | Step 225900 | Avg Loss: 0.0181 | Grad Norm: 0.00292283\n",
      "Epoch 6 | Step 226000 | Avg Loss: 0.0180 | Grad Norm: 0.00455501\n",
      "Epoch 6 | Step 226100 | Avg Loss: 0.0181 | Grad Norm: 0.00707524\n",
      "Epoch 6 | Step 226200 | Avg Loss: 0.0181 | Grad Norm: 0.00341454\n",
      "Epoch 6 | Step 226300 | Avg Loss: 0.0182 | Grad Norm: 0.00517927\n",
      "Epoch 6 | Step 226400 | Avg Loss: 0.0182 | Grad Norm: 0.00257119\n",
      "Epoch 6 | Step 226500 | Avg Loss: 0.0179 | Grad Norm: 0.00502943\n",
      "Epoch 6 | Step 226600 | Avg Loss: 0.0175 | Grad Norm: 0.00549273\n",
      "Epoch 6 | Step 226700 | Avg Loss: 0.0181 | Grad Norm: 0.00400090\n",
      "Epoch 6 | Step 226800 | Avg Loss: 0.0182 | Grad Norm: 0.00410977\n",
      "Epoch 6 | Step 226900 | Avg Loss: 0.0176 | Grad Norm: 0.00351811\n",
      "Epoch 6 | Step 227000 | Avg Loss: 0.0179 | Grad Norm: 0.00256712\n",
      "Epoch 6 | Step 227100 | Avg Loss: 0.0179 | Grad Norm: 0.00303654\n",
      "Epoch 6 | Step 227200 | Avg Loss: 0.0184 | Grad Norm: 0.00393225\n",
      "Epoch 6 | Step 227300 | Avg Loss: 0.0184 | Grad Norm: 0.00279731\n",
      "Epoch 6 | Step 227400 | Avg Loss: 0.0183 | Grad Norm: 0.00365076\n",
      "Epoch 6 | Step 227500 | Avg Loss: 0.0185 | Grad Norm: 0.00356388\n",
      "Epoch 6 | Step 227600 | Avg Loss: 0.0188 | Grad Norm: 0.00282964\n",
      "Epoch 6 | Step 227700 | Avg Loss: 0.0187 | Grad Norm: 0.00301832\n",
      "Epoch 6 | Step 227800 | Avg Loss: 0.0186 | Grad Norm: 0.00458564\n",
      "Epoch 6 | Step 227900 | Avg Loss: 0.0186 | Grad Norm: 0.00246377\n",
      "Epoch 6 | Step 228000 | Avg Loss: 0.0185 | Grad Norm: 0.00316198\n",
      "Epoch 6 | Step 228100 | Avg Loss: 0.0185 | Grad Norm: 0.00326336\n",
      "Epoch 6 | Step 228200 | Avg Loss: 0.0180 | Grad Norm: 0.00252696\n",
      "Epoch 6 | Step 228300 | Avg Loss: 0.0183 | Grad Norm: 0.00275626\n",
      "Epoch 6 | Step 228400 | Avg Loss: 0.0178 | Grad Norm: 0.00278329\n",
      "Epoch 6 | Step 228500 | Avg Loss: 0.0179 | Grad Norm: 0.00276208\n",
      "Epoch 6 | Step 228600 | Avg Loss: 0.0180 | Grad Norm: 0.00307654\n",
      "Epoch 6 | Step 228700 | Avg Loss: 0.0180 | Grad Norm: 0.00509464\n",
      "Epoch 6 | Step 228800 | Avg Loss: 0.0181 | Grad Norm: 0.00540562\n",
      "Epoch 6 | Step 228900 | Avg Loss: 0.0183 | Grad Norm: 0.00413851\n",
      "Epoch 6 | Step 229000 | Avg Loss: 0.0181 | Grad Norm: 0.00336209\n",
      "Epoch 6 | Step 229100 | Avg Loss: 0.0180 | Grad Norm: 0.00406602\n",
      "Epoch 6 | Step 229200 | Avg Loss: 0.0177 | Grad Norm: 0.00465980\n",
      "Epoch 6 | Step 229300 | Avg Loss: 0.0176 | Grad Norm: 0.00420856\n",
      "Epoch 6 | Step 229400 | Avg Loss: 0.0170 | Grad Norm: 0.00237292\n",
      "Epoch 6 | Step 229500 | Avg Loss: 0.0169 | Grad Norm: 0.00313696\n",
      "Epoch 6 | Step 229600 | Avg Loss: 0.0169 | Grad Norm: 0.00659093\n",
      "Epoch 6 | Step 229700 | Avg Loss: 0.0176 | Grad Norm: 0.00410168\n",
      "Epoch 6 | Step 229800 | Avg Loss: 0.0178 | Grad Norm: 0.00285925\n",
      "Epoch 6 | Step 229900 | Avg Loss: 0.0181 | Grad Norm: 0.00299893\n",
      "Epoch 6 | Step 230000 | Avg Loss: 0.0178 | Grad Norm: 0.00306802\n",
      "Epoch 6 | Step 230100 | Avg Loss: 0.0177 | Grad Norm: 0.00297707\n",
      "Epoch 6 | Step 230200 | Avg Loss: 0.0180 | Grad Norm: 0.00286062\n",
      "Epoch 6 | Step 230300 | Avg Loss: 0.0178 | Grad Norm: 0.00271468\n",
      "Epoch 6 | Step 230400 | Avg Loss: 0.0176 | Grad Norm: 0.00249195\n",
      "Epoch 6 | Step 230500 | Avg Loss: 0.0183 | Grad Norm: 0.00305090\n",
      "Epoch 6 | Step 230600 | Avg Loss: 0.0184 | Grad Norm: 0.00412888\n",
      "Epoch 6 | Step 230700 | Avg Loss: 0.0184 | Grad Norm: 0.00413632\n",
      "Epoch 6 | Step 230800 | Avg Loss: 0.0183 | Grad Norm: 0.00412813\n",
      "Epoch 6 | Step 230900 | Avg Loss: 0.0187 | Grad Norm: 0.00226150\n",
      "Epoch 6 | Step 231000 | Avg Loss: 0.0185 | Grad Norm: 0.00358151\n",
      "Epoch 6 | Step 231100 | Avg Loss: 0.0180 | Grad Norm: 0.00234380\n",
      "Epoch 6 | Step 231200 | Avg Loss: 0.0182 | Grad Norm: 0.00308406\n",
      "Epoch 6 | Step 231300 | Avg Loss: 0.0178 | Grad Norm: 0.00341703\n",
      "Epoch 6 | Step 231400 | Avg Loss: 0.0173 | Grad Norm: 0.00305359\n",
      "Epoch 6 | Step 231500 | Avg Loss: 0.0183 | Grad Norm: 0.00274309\n",
      "Epoch 6 | Step 231600 | Avg Loss: 0.0185 | Grad Norm: 0.00701068\n",
      "Epoch 6 | Step 231700 | Avg Loss: 0.0186 | Grad Norm: 0.00297125\n",
      "Epoch 6 | Step 231800 | Avg Loss: 0.0184 | Grad Norm: 0.00281089\n",
      "Epoch 6 | Step 231900 | Avg Loss: 0.0185 | Grad Norm: 0.00358352\n",
      "Epoch 6 | Step 232000 | Avg Loss: 0.0185 | Grad Norm: 0.00288317\n",
      "Epoch 6 | Step 232100 | Avg Loss: 0.0184 | Grad Norm: 0.00508734\n",
      "Epoch 6 | Step 232200 | Avg Loss: 0.0184 | Grad Norm: 0.00301188\n",
      "Epoch 6 | Step 232300 | Avg Loss: 0.0188 | Grad Norm: 0.00326073\n",
      "Epoch 6 | Step 232400 | Avg Loss: 0.0189 | Grad Norm: 0.00526731\n",
      "Epoch 6 | Step 232500 | Avg Loss: 0.0184 | Grad Norm: 0.00493369\n",
      "Epoch 6 | Step 232600 | Avg Loss: 0.0183 | Grad Norm: 0.00346425\n",
      "Epoch 6 | Step 232700 | Avg Loss: 0.0184 | Grad Norm: 0.00355144\n",
      "Epoch 6 | Step 232800 | Avg Loss: 0.0184 | Grad Norm: 0.00382879\n",
      "Epoch 6 | Step 232900 | Avg Loss: 0.0181 | Grad Norm: 0.00402291\n",
      "Epoch 6 | Step 233000 | Avg Loss: 0.0181 | Grad Norm: 0.00282345\n",
      "Epoch 6 | Step 233100 | Avg Loss: 0.0182 | Grad Norm: 0.00288313\n",
      "Epoch 6 | Step 233200 | Avg Loss: 0.0182 | Grad Norm: 0.00346088\n",
      "Epoch 6 | Step 233300 | Avg Loss: 0.0185 | Grad Norm: 0.00304194\n",
      "Epoch 6 | Step 233400 | Avg Loss: 0.0189 | Grad Norm: 0.00302814\n",
      "Epoch 6 | Step 233500 | Avg Loss: 0.0188 | Grad Norm: 0.00292751\n",
      "Epoch 6 | Step 233600 | Avg Loss: 0.0186 | Grad Norm: 0.00347308\n",
      "Epoch 6 | Step 233700 | Avg Loss: 0.0187 | Grad Norm: 0.00427353\n",
      "Epoch 6 | Step 233800 | Avg Loss: 0.0189 | Grad Norm: 0.00294057\n",
      "Epoch 6 | Step 233900 | Avg Loss: 0.0187 | Grad Norm: 0.00426778\n",
      "Epoch 6 | Step 234000 | Avg Loss: 0.0184 | Grad Norm: 0.00549706\n",
      "Epoch 6 | Step 234100 | Avg Loss: 0.0187 | Grad Norm: 0.00325021\n",
      "Epoch 6 | Step 234200 | Avg Loss: 0.0182 | Grad Norm: 0.00487055\n",
      "Epoch 6 | Step 234300 | Avg Loss: 0.0180 | Grad Norm: 0.00442978\n",
      "Epoch 6, Loss: 0.0188\n",
      "Epoch 7 | Step 234400 | Avg Loss: 0.0181 | Grad Norm: 0.00290608\n",
      "Epoch 7 | Step 234500 | Avg Loss: 0.0183 | Grad Norm: 0.00531728\n",
      "Epoch 7 | Step 234600 | Avg Loss: 0.0177 | Grad Norm: 0.00266762\n",
      "Epoch 7 | Step 234700 | Avg Loss: 0.0176 | Grad Norm: 0.00331848\n",
      "Epoch 7 | Step 234800 | Avg Loss: 0.0175 | Grad Norm: 0.00370074\n",
      "Epoch 7 | Step 234900 | Avg Loss: 0.0175 | Grad Norm: 0.00356484\n",
      "Epoch 7 | Step 235000 | Avg Loss: 0.0181 | Grad Norm: 0.00349400\n",
      "Epoch 7 | Step 235100 | Avg Loss: 0.0186 | Grad Norm: 0.00695280\n",
      "Epoch 7 | Step 235200 | Avg Loss: 0.0188 | Grad Norm: 0.00306152\n",
      "Epoch 7 | Step 235300 | Avg Loss: 0.0190 | Grad Norm: 0.00275836\n",
      "Epoch 7 | Step 235400 | Avg Loss: 0.0188 | Grad Norm: 0.00281411\n",
      "Epoch 7 | Step 235500 | Avg Loss: 0.0183 | Grad Norm: 0.00431828\n",
      "Epoch 7 | Step 235600 | Avg Loss: 0.0181 | Grad Norm: 0.00398511\n",
      "Epoch 7 | Step 235700 | Avg Loss: 0.0181 | Grad Norm: 0.00422350\n",
      "Epoch 7 | Step 235800 | Avg Loss: 0.0180 | Grad Norm: 0.00280340\n",
      "Epoch 7 | Step 235900 | Avg Loss: 0.0180 | Grad Norm: 0.00283766\n",
      "Epoch 7 | Step 236000 | Avg Loss: 0.0177 | Grad Norm: 0.00358871\n",
      "Epoch 7 | Step 236100 | Avg Loss: 0.0177 | Grad Norm: 0.00279720\n",
      "Epoch 7 | Step 236200 | Avg Loss: 0.0179 | Grad Norm: 0.00681223\n",
      "Epoch 7 | Step 236300 | Avg Loss: 0.0178 | Grad Norm: 0.00277025\n",
      "Epoch 7 | Step 236400 | Avg Loss: 0.0178 | Grad Norm: 0.00353760\n",
      "Epoch 7 | Step 236500 | Avg Loss: 0.0178 | Grad Norm: 0.00265040\n",
      "Epoch 7 | Step 236600 | Avg Loss: 0.0181 | Grad Norm: 0.00595660\n",
      "Epoch 7 | Step 236700 | Avg Loss: 0.0179 | Grad Norm: 0.00288883\n",
      "Epoch 7 | Step 236800 | Avg Loss: 0.0179 | Grad Norm: 0.00332918\n",
      "Epoch 7 | Step 236900 | Avg Loss: 0.0182 | Grad Norm: 0.00355908\n",
      "Epoch 7 | Step 237000 | Avg Loss: 0.0182 | Grad Norm: 0.00301294\n",
      "Epoch 7 | Step 237100 | Avg Loss: 0.0185 | Grad Norm: 0.00311761\n",
      "Epoch 7 | Step 237200 | Avg Loss: 0.0183 | Grad Norm: 0.00503671\n",
      "Epoch 7 | Step 237300 | Avg Loss: 0.0185 | Grad Norm: 0.00550231\n",
      "Epoch 7 | Step 237400 | Avg Loss: 0.0186 | Grad Norm: 0.00622225\n",
      "Epoch 7 | Step 237500 | Avg Loss: 0.0180 | Grad Norm: 0.00425461\n",
      "Epoch 7 | Step 237600 | Avg Loss: 0.0178 | Grad Norm: 0.00252619\n",
      "Epoch 7 | Step 237700 | Avg Loss: 0.0177 | Grad Norm: 0.00251315\n",
      "Epoch 7 | Step 237800 | Avg Loss: 0.0175 | Grad Norm: 0.00465564\n",
      "Epoch 7 | Step 237900 | Avg Loss: 0.0170 | Grad Norm: 0.00437294\n",
      "Epoch 7 | Step 238000 | Avg Loss: 0.0171 | Grad Norm: 0.00347129\n",
      "Epoch 7 | Step 238100 | Avg Loss: 0.0177 | Grad Norm: 0.00291112\n",
      "Epoch 7 | Step 238200 | Avg Loss: 0.0179 | Grad Norm: 0.00463941\n",
      "Epoch 7 | Step 238300 | Avg Loss: 0.0181 | Grad Norm: 0.00288743\n",
      "Epoch 7 | Step 238400 | Avg Loss: 0.0180 | Grad Norm: 0.00419197\n",
      "Epoch 7 | Step 238500 | Avg Loss: 0.0178 | Grad Norm: 0.00277481\n",
      "Epoch 7 | Step 238600 | Avg Loss: 0.0178 | Grad Norm: 0.00333499\n",
      "Epoch 7 | Step 238700 | Avg Loss: 0.0179 | Grad Norm: 0.00276242\n",
      "Epoch 7 | Step 238800 | Avg Loss: 0.0179 | Grad Norm: 0.00423531\n",
      "Epoch 7 | Step 238900 | Avg Loss: 0.0180 | Grad Norm: 0.00350586\n",
      "Epoch 7 | Step 239000 | Avg Loss: 0.0176 | Grad Norm: 0.00294580\n",
      "Epoch 7 | Step 239100 | Avg Loss: 0.0173 | Grad Norm: 0.00308805\n",
      "Epoch 7 | Step 239200 | Avg Loss: 0.0174 | Grad Norm: 0.00416270\n",
      "Epoch 7 | Step 239300 | Avg Loss: 0.0178 | Grad Norm: 0.00311536\n",
      "Epoch 7 | Step 239400 | Avg Loss: 0.0178 | Grad Norm: 0.00397394\n",
      "Epoch 7 | Step 239500 | Avg Loss: 0.0178 | Grad Norm: 0.00243560\n",
      "Epoch 7 | Step 239600 | Avg Loss: 0.0176 | Grad Norm: 0.00273247\n",
      "Epoch 7 | Step 239700 | Avg Loss: 0.0182 | Grad Norm: 0.00452809\n",
      "Epoch 7 | Step 239800 | Avg Loss: 0.0188 | Grad Norm: 0.00309413\n",
      "Epoch 7 | Step 239900 | Avg Loss: 0.0185 | Grad Norm: 0.00367416\n",
      "Epoch 7 | Step 240000 | Avg Loss: 0.0183 | Grad Norm: 0.00323957\n",
      "Epoch 7 | Step 240100 | Avg Loss: 0.0182 | Grad Norm: 0.00378319\n",
      "Epoch 7 | Step 240200 | Avg Loss: 0.0183 | Grad Norm: 0.00359157\n",
      "Epoch 7 | Step 240300 | Avg Loss: 0.0180 | Grad Norm: 0.00344811\n",
      "Epoch 7 | Step 240400 | Avg Loss: 0.0179 | Grad Norm: 0.00375664\n",
      "Epoch 7 | Step 240500 | Avg Loss: 0.0176 | Grad Norm: 0.00561354\n",
      "Epoch 7 | Step 240600 | Avg Loss: 0.0176 | Grad Norm: 0.00272579\n",
      "Epoch 7 | Step 240700 | Avg Loss: 0.0183 | Grad Norm: 0.00268364\n",
      "Epoch 7 | Step 240800 | Avg Loss: 0.0180 | Grad Norm: 0.00401728\n",
      "Epoch 7 | Step 240900 | Avg Loss: 0.0182 | Grad Norm: 0.00400425\n",
      "Epoch 7 | Step 241000 | Avg Loss: 0.0180 | Grad Norm: 0.00379849\n",
      "Epoch 7 | Step 241100 | Avg Loss: 0.0180 | Grad Norm: 0.00259105\n",
      "Epoch 7 | Step 241200 | Avg Loss: 0.0175 | Grad Norm: 0.00275126\n",
      "Epoch 7 | Step 241300 | Avg Loss: 0.0179 | Grad Norm: 0.00292979\n",
      "Epoch 7 | Step 241400 | Avg Loss: 0.0178 | Grad Norm: 0.00349965\n",
      "Epoch 7 | Step 241500 | Avg Loss: 0.0176 | Grad Norm: 0.00561091\n",
      "Epoch 7 | Step 241600 | Avg Loss: 0.0173 | Grad Norm: 0.00488658\n",
      "Epoch 7 | Step 241700 | Avg Loss: 0.0173 | Grad Norm: 0.00340261\n",
      "Epoch 7 | Step 241800 | Avg Loss: 0.0174 | Grad Norm: 0.00396407\n",
      "Epoch 7 | Step 241900 | Avg Loss: 0.0176 | Grad Norm: 0.00362490\n",
      "Epoch 7 | Step 242000 | Avg Loss: 0.0177 | Grad Norm: 0.00712522\n",
      "Epoch 7 | Step 242100 | Avg Loss: 0.0174 | Grad Norm: 0.00241153\n",
      "Epoch 7 | Step 242200 | Avg Loss: 0.0177 | Grad Norm: 0.00635035\n",
      "Epoch 7 | Step 242300 | Avg Loss: 0.0178 | Grad Norm: 0.00632669\n",
      "Epoch 7 | Step 242400 | Avg Loss: 0.0182 | Grad Norm: 0.00554589\n",
      "Epoch 7 | Step 242500 | Avg Loss: 0.0179 | Grad Norm: 0.00283627\n",
      "Epoch 7 | Step 242600 | Avg Loss: 0.0179 | Grad Norm: 0.00340682\n",
      "Epoch 7 | Step 242700 | Avg Loss: 0.0181 | Grad Norm: 0.00352980\n",
      "Epoch 7 | Step 242800 | Avg Loss: 0.0182 | Grad Norm: 0.00259812\n",
      "Epoch 7 | Step 242900 | Avg Loss: 0.0183 | Grad Norm: 0.00257087\n",
      "Epoch 7 | Step 243000 | Avg Loss: 0.0186 | Grad Norm: 0.00361542\n",
      "Epoch 7 | Step 243100 | Avg Loss: 0.0186 | Grad Norm: 0.00320642\n",
      "Epoch 7 | Step 243200 | Avg Loss: 0.0185 | Grad Norm: 0.00280258\n",
      "Epoch 7 | Step 243300 | Avg Loss: 0.0184 | Grad Norm: 0.00442092\n",
      "Epoch 7 | Step 243400 | Avg Loss: 0.0185 | Grad Norm: 0.00260793\n",
      "Epoch 7 | Step 243500 | Avg Loss: 0.0181 | Grad Norm: 0.00389734\n",
      "Epoch 7 | Step 243600 | Avg Loss: 0.0180 | Grad Norm: 0.00495302\n",
      "Epoch 7 | Step 243700 | Avg Loss: 0.0177 | Grad Norm: 0.00270152\n",
      "Epoch 7 | Step 243800 | Avg Loss: 0.0179 | Grad Norm: 0.00276959\n",
      "Epoch 7 | Step 243900 | Avg Loss: 0.0179 | Grad Norm: 0.00728563\n",
      "Epoch 7 | Step 244000 | Avg Loss: 0.0179 | Grad Norm: 0.00509738\n",
      "Epoch 7 | Step 244100 | Avg Loss: 0.0179 | Grad Norm: 0.00600852\n",
      "Epoch 7 | Step 244200 | Avg Loss: 0.0180 | Grad Norm: 0.00517103\n",
      "Epoch 7 | Step 244300 | Avg Loss: 0.0180 | Grad Norm: 0.00721622\n",
      "Epoch 7 | Step 244400 | Avg Loss: 0.0183 | Grad Norm: 0.00370259\n",
      "Epoch 7 | Step 244500 | Avg Loss: 0.0186 | Grad Norm: 0.00631021\n",
      "Epoch 7 | Step 244600 | Avg Loss: 0.0184 | Grad Norm: 0.00558357\n",
      "Epoch 7 | Step 244700 | Avg Loss: 0.0183 | Grad Norm: 0.00553722\n",
      "Epoch 7 | Step 244800 | Avg Loss: 0.0184 | Grad Norm: 0.00323757\n",
      "Epoch 7 | Step 244900 | Avg Loss: 0.0186 | Grad Norm: 0.00256898\n",
      "Epoch 7 | Step 245000 | Avg Loss: 0.0188 | Grad Norm: 0.00291909\n",
      "Epoch 7 | Step 245100 | Avg Loss: 0.0185 | Grad Norm: 0.00451506\n",
      "Epoch 7 | Step 245200 | Avg Loss: 0.0187 | Grad Norm: 0.00316271\n",
      "Epoch 7 | Step 245300 | Avg Loss: 0.0181 | Grad Norm: 0.00459526\n",
      "Epoch 7 | Step 245400 | Avg Loss: 0.0184 | Grad Norm: 0.00295846\n",
      "Epoch 7 | Step 245500 | Avg Loss: 0.0189 | Grad Norm: 0.00349125\n",
      "Epoch 7 | Step 245600 | Avg Loss: 0.0186 | Grad Norm: 0.00377233\n",
      "Epoch 7 | Step 245700 | Avg Loss: 0.0190 | Grad Norm: 0.00449314\n",
      "Epoch 7 | Step 245800 | Avg Loss: 0.0185 | Grad Norm: 0.00344840\n",
      "Epoch 7 | Step 245900 | Avg Loss: 0.0185 | Grad Norm: 0.00278159\n",
      "Epoch 7 | Step 246000 | Avg Loss: 0.0185 | Grad Norm: 0.00342976\n",
      "Epoch 7 | Step 246100 | Avg Loss: 0.0182 | Grad Norm: 0.00277338\n",
      "Epoch 7 | Step 246200 | Avg Loss: 0.0180 | Grad Norm: 0.00354940\n",
      "Epoch 7 | Step 246300 | Avg Loss: 0.0177 | Grad Norm: 0.00267381\n",
      "Epoch 7 | Step 246400 | Avg Loss: 0.0177 | Grad Norm: 0.00327496\n",
      "Epoch 7 | Step 246500 | Avg Loss: 0.0176 | Grad Norm: 0.00289932\n",
      "Epoch 7 | Step 246600 | Avg Loss: 0.0176 | Grad Norm: 0.00331544\n",
      "Epoch 7 | Step 246700 | Avg Loss: 0.0180 | Grad Norm: 0.00388853\n",
      "Epoch 7 | Step 246800 | Avg Loss: 0.0185 | Grad Norm: 0.00262014\n",
      "Epoch 7 | Step 246900 | Avg Loss: 0.0185 | Grad Norm: 0.00698711\n",
      "Epoch 7 | Step 247000 | Avg Loss: 0.0181 | Grad Norm: 0.00322015\n",
      "Epoch 7 | Step 247100 | Avg Loss: 0.0180 | Grad Norm: 0.00301496\n",
      "Epoch 7 | Step 247200 | Avg Loss: 0.0179 | Grad Norm: 0.00455819\n",
      "Epoch 7 | Step 247300 | Avg Loss: 0.0181 | Grad Norm: 0.00241316\n",
      "Epoch 7 | Step 247400 | Avg Loss: 0.0180 | Grad Norm: 0.00322293\n",
      "Epoch 7 | Step 247500 | Avg Loss: 0.0180 | Grad Norm: 0.00295000\n",
      "Epoch 7 | Step 247600 | Avg Loss: 0.0178 | Grad Norm: 0.00324361\n",
      "Epoch 7 | Step 247700 | Avg Loss: 0.0179 | Grad Norm: 0.00263832\n",
      "Epoch 7 | Step 247800 | Avg Loss: 0.0179 | Grad Norm: 0.00377491\n",
      "Epoch 7 | Step 247900 | Avg Loss: 0.0180 | Grad Norm: 0.00386306\n",
      "Epoch 7 | Step 248000 | Avg Loss: 0.0179 | Grad Norm: 0.00318311\n",
      "Epoch 7 | Step 248100 | Avg Loss: 0.0175 | Grad Norm: 0.00383383\n",
      "Epoch 7 | Step 248200 | Avg Loss: 0.0177 | Grad Norm: 0.00473158\n",
      "Epoch 7 | Step 248300 | Avg Loss: 0.0185 | Grad Norm: 0.00534400\n",
      "Epoch 7 | Step 248400 | Avg Loss: 0.0180 | Grad Norm: 0.00295328\n",
      "Epoch 7 | Step 248500 | Avg Loss: 0.0176 | Grad Norm: 0.00411944\n",
      "Epoch 7 | Step 248600 | Avg Loss: 0.0182 | Grad Norm: 0.00250615\n",
      "Epoch 7 | Step 248700 | Avg Loss: 0.0178 | Grad Norm: 0.00345171\n",
      "Epoch 7 | Step 248800 | Avg Loss: 0.0182 | Grad Norm: 0.00468509\n",
      "Epoch 7 | Step 248900 | Avg Loss: 0.0180 | Grad Norm: 0.00289596\n",
      "Epoch 7 | Step 249000 | Avg Loss: 0.0184 | Grad Norm: 0.00311667\n",
      "Epoch 7 | Step 249100 | Avg Loss: 0.0181 | Grad Norm: 0.00463666\n",
      "Epoch 7 | Step 249200 | Avg Loss: 0.0181 | Grad Norm: 0.00497575\n",
      "Epoch 7 | Step 249300 | Avg Loss: 0.0183 | Grad Norm: 0.00473422\n",
      "Epoch 7 | Step 249400 | Avg Loss: 0.0181 | Grad Norm: 0.00563551\n",
      "Epoch 7 | Step 249500 | Avg Loss: 0.0178 | Grad Norm: 0.00482928\n",
      "Epoch 7 | Step 249600 | Avg Loss: 0.0181 | Grad Norm: 0.00512935\n",
      "Epoch 7 | Step 249700 | Avg Loss: 0.0182 | Grad Norm: 0.00419004\n",
      "Epoch 7 | Step 249800 | Avg Loss: 0.0180 | Grad Norm: 0.00346997\n",
      "Epoch 7 | Step 249900 | Avg Loss: 0.0183 | Grad Norm: 0.00313139\n",
      "Epoch 7 | Step 250000 | Avg Loss: 0.0186 | Grad Norm: 0.00334266\n",
      "Epoch 7 | Step 250100 | Avg Loss: 0.0188 | Grad Norm: 0.00373782\n",
      "Epoch 7 | Step 250200 | Avg Loss: 0.0190 | Grad Norm: 0.00576057\n",
      "Epoch 7 | Step 250300 | Avg Loss: 0.0193 | Grad Norm: 0.00500424\n",
      "Epoch 7 | Step 250400 | Avg Loss: 0.0193 | Grad Norm: 0.00484781\n",
      "Epoch 7 | Step 250500 | Avg Loss: 0.0191 | Grad Norm: 0.00453030\n",
      "Epoch 7 | Step 250600 | Avg Loss: 0.0191 | Grad Norm: 0.00495811\n",
      "Epoch 7 | Step 250700 | Avg Loss: 0.0188 | Grad Norm: 0.00668536\n",
      "Epoch 7 | Step 250800 | Avg Loss: 0.0185 | Grad Norm: 0.00411125\n",
      "Epoch 7 | Step 250900 | Avg Loss: 0.0187 | Grad Norm: 0.00360884\n",
      "Epoch 7 | Step 251000 | Avg Loss: 0.0180 | Grad Norm: 0.00302452\n",
      "Epoch 7 | Step 251100 | Avg Loss: 0.0178 | Grad Norm: 0.00378906\n",
      "Epoch 7 | Step 251200 | Avg Loss: 0.0184 | Grad Norm: 0.00360409\n",
      "Epoch 7 | Step 251300 | Avg Loss: 0.0182 | Grad Norm: 0.00271615\n",
      "Epoch 7 | Step 251400 | Avg Loss: 0.0182 | Grad Norm: 0.00462123\n",
      "Epoch 7 | Step 251500 | Avg Loss: 0.0184 | Grad Norm: 0.00325309\n",
      "Epoch 7 | Step 251600 | Avg Loss: 0.0179 | Grad Norm: 0.00326109\n",
      "Epoch 7 | Step 251700 | Avg Loss: 0.0178 | Grad Norm: 0.00552040\n",
      "Epoch 7 | Step 251800 | Avg Loss: 0.0180 | Grad Norm: 0.00507198\n",
      "Epoch 7 | Step 251900 | Avg Loss: 0.0178 | Grad Norm: 0.00482825\n",
      "Epoch 7 | Step 252000 | Avg Loss: 0.0176 | Grad Norm: 0.00278739\n",
      "Epoch 7 | Step 252100 | Avg Loss: 0.0177 | Grad Norm: 0.00235246\n",
      "Epoch 7 | Step 252200 | Avg Loss: 0.0177 | Grad Norm: 0.00268374\n",
      "Epoch 7 | Step 252300 | Avg Loss: 0.0180 | Grad Norm: 0.00380665\n",
      "Epoch 7 | Step 252400 | Avg Loss: 0.0178 | Grad Norm: 0.00383579\n",
      "Epoch 7 | Step 252500 | Avg Loss: 0.0177 | Grad Norm: 0.00347811\n",
      "Epoch 7 | Step 252600 | Avg Loss: 0.0178 | Grad Norm: 0.00305108\n",
      "Epoch 7 | Step 252700 | Avg Loss: 0.0179 | Grad Norm: 0.00267968\n",
      "Epoch 7 | Step 252800 | Avg Loss: 0.0179 | Grad Norm: 0.00287030\n",
      "Epoch 7 | Step 252900 | Avg Loss: 0.0178 | Grad Norm: 0.00257347\n",
      "Epoch 7 | Step 253000 | Avg Loss: 0.0180 | Grad Norm: 0.00530658\n",
      "Epoch 7 | Step 253100 | Avg Loss: 0.0182 | Grad Norm: 0.00279081\n",
      "Epoch 7 | Step 253200 | Avg Loss: 0.0184 | Grad Norm: 0.00517571\n",
      "Epoch 7 | Step 253300 | Avg Loss: 0.0186 | Grad Norm: 0.00331315\n",
      "Epoch 7 | Step 253400 | Avg Loss: 0.0186 | Grad Norm: 0.00450019\n",
      "Epoch 7 | Step 253500 | Avg Loss: 0.0185 | Grad Norm: 0.00350554\n",
      "Epoch 7 | Step 253600 | Avg Loss: 0.0189 | Grad Norm: 0.00331958\n",
      "Epoch 7 | Step 253700 | Avg Loss: 0.0181 | Grad Norm: 0.00448207\n",
      "Epoch 7 | Step 253800 | Avg Loss: 0.0178 | Grad Norm: 0.00322760\n",
      "Epoch 7 | Step 253900 | Avg Loss: 0.0174 | Grad Norm: 0.00293223\n",
      "Epoch 7 | Step 254000 | Avg Loss: 0.0173 | Grad Norm: 0.00373124\n",
      "Epoch 7 | Step 254100 | Avg Loss: 0.0175 | Grad Norm: 0.00261663\n",
      "Epoch 7 | Step 254200 | Avg Loss: 0.0174 | Grad Norm: 0.00396596\n",
      "Epoch 7 | Step 254300 | Avg Loss: 0.0175 | Grad Norm: 0.00516086\n",
      "Epoch 7 | Step 254400 | Avg Loss: 0.0176 | Grad Norm: 0.00294490\n",
      "Epoch 7 | Step 254500 | Avg Loss: 0.0175 | Grad Norm: 0.00581366\n",
      "Epoch 7 | Step 254600 | Avg Loss: 0.0171 | Grad Norm: 0.00343149\n",
      "Epoch 7 | Step 254700 | Avg Loss: 0.0175 | Grad Norm: 0.00511494\n",
      "Epoch 7 | Step 254800 | Avg Loss: 0.0180 | Grad Norm: 0.00275855\n",
      "Epoch 7 | Step 254900 | Avg Loss: 0.0181 | Grad Norm: 0.00495730\n",
      "Epoch 7 | Step 255000 | Avg Loss: 0.0182 | Grad Norm: 0.00297947\n",
      "Epoch 7 | Step 255100 | Avg Loss: 0.0181 | Grad Norm: 0.00400041\n",
      "Epoch 7 | Step 255200 | Avg Loss: 0.0178 | Grad Norm: 0.00339529\n",
      "Epoch 7 | Step 255300 | Avg Loss: 0.0176 | Grad Norm: 0.00291010\n",
      "Epoch 7 | Step 255400 | Avg Loss: 0.0177 | Grad Norm: 0.00347369\n",
      "Epoch 7 | Step 255500 | Avg Loss: 0.0176 | Grad Norm: 0.00247752\n",
      "Epoch 7 | Step 255600 | Avg Loss: 0.0179 | Grad Norm: 0.00391353\n",
      "Epoch 7 | Step 255700 | Avg Loss: 0.0178 | Grad Norm: 0.00351409\n",
      "Epoch 7 | Step 255800 | Avg Loss: 0.0181 | Grad Norm: 0.00314872\n",
      "Epoch 7 | Step 255900 | Avg Loss: 0.0181 | Grad Norm: 0.00448518\n",
      "Epoch 7 | Step 256000 | Avg Loss: 0.0183 | Grad Norm: 0.00286365\n",
      "Epoch 7 | Step 256100 | Avg Loss: 0.0185 | Grad Norm: 0.00390734\n",
      "Epoch 7 | Step 256200 | Avg Loss: 0.0186 | Grad Norm: 0.00606846\n",
      "Epoch 7 | Step 256300 | Avg Loss: 0.0186 | Grad Norm: 0.00282526\n",
      "Epoch 7 | Step 256400 | Avg Loss: 0.0184 | Grad Norm: 0.00526844\n",
      "Epoch 7 | Step 256500 | Avg Loss: 0.0178 | Grad Norm: 0.00351883\n",
      "Epoch 7 | Step 256600 | Avg Loss: 0.0178 | Grad Norm: 0.00336692\n",
      "Epoch 7 | Step 256700 | Avg Loss: 0.0178 | Grad Norm: 0.00330697\n",
      "Epoch 7 | Step 256800 | Avg Loss: 0.0180 | Grad Norm: 0.00526575\n",
      "Epoch 7 | Step 256900 | Avg Loss: 0.0179 | Grad Norm: 0.00255252\n",
      "Epoch 7 | Step 257000 | Avg Loss: 0.0179 | Grad Norm: 0.00362786\n",
      "Epoch 7 | Step 257100 | Avg Loss: 0.0179 | Grad Norm: 0.00506710\n",
      "Epoch 7 | Step 257200 | Avg Loss: 0.0182 | Grad Norm: 0.00308451\n",
      "Epoch 7 | Step 257300 | Avg Loss: 0.0180 | Grad Norm: 0.00277230\n",
      "Epoch 7 | Step 257400 | Avg Loss: 0.0179 | Grad Norm: 0.00469153\n",
      "Epoch 7 | Step 257500 | Avg Loss: 0.0177 | Grad Norm: 0.00566677\n",
      "Epoch 7 | Step 257600 | Avg Loss: 0.0175 | Grad Norm: 0.00267994\n",
      "Epoch 7 | Step 257700 | Avg Loss: 0.0174 | Grad Norm: 0.00729561\n",
      "Epoch 7 | Step 257800 | Avg Loss: 0.0179 | Grad Norm: 0.00344670\n",
      "Epoch 7 | Step 257900 | Avg Loss: 0.0184 | Grad Norm: 0.00380277\n",
      "Epoch 7 | Step 258000 | Avg Loss: 0.0185 | Grad Norm: 0.00259564\n",
      "Epoch 7 | Step 258100 | Avg Loss: 0.0185 | Grad Norm: 0.00348294\n",
      "Epoch 7 | Step 258200 | Avg Loss: 0.0186 | Grad Norm: 0.00559468\n",
      "Epoch 7 | Step 258300 | Avg Loss: 0.0181 | Grad Norm: 0.00613866\n",
      "Epoch 7 | Step 258400 | Avg Loss: 0.0183 | Grad Norm: 0.00446257\n",
      "Epoch 7 | Step 258500 | Avg Loss: 0.0183 | Grad Norm: 0.00511309\n",
      "Epoch 7 | Step 258600 | Avg Loss: 0.0182 | Grad Norm: 0.00280607\n",
      "Epoch 7 | Step 258700 | Avg Loss: 0.0181 | Grad Norm: 0.00257225\n",
      "Epoch 7 | Step 258800 | Avg Loss: 0.0177 | Grad Norm: 0.00367286\n",
      "Epoch 7 | Step 258900 | Avg Loss: 0.0176 | Grad Norm: 0.00300492\n",
      "Epoch 7 | Step 259000 | Avg Loss: 0.0183 | Grad Norm: 0.00282015\n",
      "Epoch 7 | Step 259100 | Avg Loss: 0.0187 | Grad Norm: 0.00269595\n",
      "Epoch 7 | Step 259200 | Avg Loss: 0.0182 | Grad Norm: 0.00372230\n",
      "Epoch 7 | Step 259300 | Avg Loss: 0.0179 | Grad Norm: 0.00765889\n",
      "Epoch 7 | Step 259400 | Avg Loss: 0.0181 | Grad Norm: 0.00413668\n",
      "Epoch 7 | Step 259500 | Avg Loss: 0.0182 | Grad Norm: 0.00467296\n",
      "Epoch 7 | Step 259600 | Avg Loss: 0.0186 | Grad Norm: 0.00768466\n",
      "Epoch 7 | Step 259700 | Avg Loss: 0.0188 | Grad Norm: 0.00502457\n",
      "Epoch 7 | Step 259800 | Avg Loss: 0.0188 | Grad Norm: 0.00298002\n",
      "Epoch 7 | Step 259900 | Avg Loss: 0.0187 | Grad Norm: 0.00403140\n",
      "Epoch 7 | Step 260000 | Avg Loss: 0.0186 | Grad Norm: 0.00322712\n",
      "Epoch 7 | Step 260100 | Avg Loss: 0.0186 | Grad Norm: 0.00387900\n",
      "Epoch 7 | Step 260200 | Avg Loss: 0.0186 | Grad Norm: 0.00329240\n",
      "Epoch 7 | Step 260300 | Avg Loss: 0.0187 | Grad Norm: 0.00447243\n",
      "Epoch 7 | Step 260400 | Avg Loss: 0.0180 | Grad Norm: 0.00302694\n",
      "Epoch 7 | Step 260500 | Avg Loss: 0.0179 | Grad Norm: 0.00588999\n",
      "Epoch 7 | Step 260600 | Avg Loss: 0.0179 | Grad Norm: 0.00280695\n",
      "Epoch 7 | Step 260700 | Avg Loss: 0.0183 | Grad Norm: 0.00327081\n",
      "Epoch 7 | Step 260800 | Avg Loss: 0.0179 | Grad Norm: 0.00375976\n",
      "Epoch 7 | Step 260900 | Avg Loss: 0.0175 | Grad Norm: 0.00370533\n",
      "Epoch 7 | Step 261000 | Avg Loss: 0.0174 | Grad Norm: 0.00420045\n",
      "Epoch 7 | Step 261100 | Avg Loss: 0.0180 | Grad Norm: 0.00274900\n",
      "Epoch 7 | Step 261200 | Avg Loss: 0.0180 | Grad Norm: 0.00351412\n",
      "Epoch 7 | Step 261300 | Avg Loss: 0.0181 | Grad Norm: 0.00305254\n",
      "Epoch 7 | Step 261400 | Avg Loss: 0.0184 | Grad Norm: 0.00334970\n",
      "Epoch 7 | Step 261500 | Avg Loss: 0.0185 | Grad Norm: 0.00376689\n",
      "Epoch 7 | Step 261600 | Avg Loss: 0.0179 | Grad Norm: 0.00487415\n",
      "Epoch 7 | Step 261700 | Avg Loss: 0.0178 | Grad Norm: 0.00306275\n",
      "Epoch 7 | Step 261800 | Avg Loss: 0.0183 | Grad Norm: 0.00663925\n",
      "Epoch 7 | Step 261900 | Avg Loss: 0.0185 | Grad Norm: 0.00351457\n",
      "Epoch 7 | Step 262000 | Avg Loss: 0.0183 | Grad Norm: 0.00355685\n",
      "Epoch 7 | Step 262100 | Avg Loss: 0.0182 | Grad Norm: 0.00295969\n",
      "Epoch 7 | Step 262200 | Avg Loss: 0.0184 | Grad Norm: 0.00459980\n",
      "Epoch 7 | Step 262300 | Avg Loss: 0.0187 | Grad Norm: 0.00524602\n",
      "Epoch 7 | Step 262400 | Avg Loss: 0.0183 | Grad Norm: 0.00391680\n",
      "Epoch 7 | Step 262500 | Avg Loss: 0.0185 | Grad Norm: 0.00287725\n",
      "Epoch 7 | Step 262600 | Avg Loss: 0.0183 | Grad Norm: 0.00337669\n",
      "Epoch 7 | Step 262700 | Avg Loss: 0.0176 | Grad Norm: 0.00822963\n",
      "Epoch 7 | Step 262800 | Avg Loss: 0.0183 | Grad Norm: 0.00280531\n",
      "Epoch 7 | Step 262900 | Avg Loss: 0.0184 | Grad Norm: 0.00383573\n",
      "Epoch 7 | Step 263000 | Avg Loss: 0.0185 | Grad Norm: 0.00272183\n",
      "Epoch 7 | Step 263100 | Avg Loss: 0.0182 | Grad Norm: 0.00400005\n",
      "Epoch 7 | Step 263200 | Avg Loss: 0.0180 | Grad Norm: 0.00454844\n",
      "Epoch 7 | Step 263300 | Avg Loss: 0.0182 | Grad Norm: 0.00292112\n",
      "Epoch 7 | Step 263400 | Avg Loss: 0.0179 | Grad Norm: 0.00397880\n",
      "Epoch 7 | Step 263500 | Avg Loss: 0.0183 | Grad Norm: 0.00611090\n",
      "Epoch 7 | Step 263600 | Avg Loss: 0.0182 | Grad Norm: 0.00361181\n",
      "Epoch 7 | Step 263700 | Avg Loss: 0.0181 | Grad Norm: 0.00279469\n",
      "Epoch 7 | Step 263800 | Avg Loss: 0.0181 | Grad Norm: 0.00346155\n",
      "Epoch 7 | Step 263900 | Avg Loss: 0.0183 | Grad Norm: 0.00269112\n",
      "Epoch 7 | Step 264000 | Avg Loss: 0.0180 | Grad Norm: 0.00328496\n",
      "Epoch 7 | Step 264100 | Avg Loss: 0.0184 | Grad Norm: 0.00308486\n",
      "Epoch 7 | Step 264200 | Avg Loss: 0.0184 | Grad Norm: 0.00327161\n",
      "Epoch 7 | Step 264300 | Avg Loss: 0.0186 | Grad Norm: 0.00373325\n",
      "Epoch 7 | Step 264400 | Avg Loss: 0.0185 | Grad Norm: 0.00668486\n",
      "Epoch 7 | Step 264500 | Avg Loss: 0.0186 | Grad Norm: 0.00308031\n",
      "Epoch 7 | Step 264600 | Avg Loss: 0.0187 | Grad Norm: 0.00492031\n",
      "Epoch 7 | Step 264700 | Avg Loss: 0.0186 | Grad Norm: 0.00364916\n",
      "Epoch 7 | Step 264800 | Avg Loss: 0.0187 | Grad Norm: 0.00315533\n",
      "Epoch 7 | Step 264900 | Avg Loss: 0.0181 | Grad Norm: 0.00392924\n",
      "Epoch 7 | Step 265000 | Avg Loss: 0.0178 | Grad Norm: 0.00245621\n",
      "Epoch 7 | Step 265100 | Avg Loss: 0.0183 | Grad Norm: 0.00441011\n",
      "Epoch 7 | Step 265200 | Avg Loss: 0.0181 | Grad Norm: 0.00285880\n",
      "Epoch 7 | Step 265300 | Avg Loss: 0.0181 | Grad Norm: 0.00314118\n",
      "Epoch 7 | Step 265400 | Avg Loss: 0.0180 | Grad Norm: 0.00292593\n",
      "Epoch 7 | Step 265500 | Avg Loss: 0.0181 | Grad Norm: 0.00276274\n",
      "Epoch 7 | Step 265600 | Avg Loss: 0.0179 | Grad Norm: 0.00279974\n",
      "Epoch 7 | Step 265700 | Avg Loss: 0.0177 | Grad Norm: 0.00420301\n",
      "Epoch 7 | Step 265800 | Avg Loss: 0.0181 | Grad Norm: 0.00386537\n",
      "Epoch 7 | Step 265900 | Avg Loss: 0.0179 | Grad Norm: 0.00364264\n",
      "Epoch 7 | Step 266000 | Avg Loss: 0.0176 | Grad Norm: 0.00431786\n",
      "Epoch 7 | Step 266100 | Avg Loss: 0.0174 | Grad Norm: 0.00362880\n",
      "Epoch 7 | Step 266200 | Avg Loss: 0.0182 | Grad Norm: 0.00456605\n",
      "Epoch 7 | Step 266300 | Avg Loss: 0.0183 | Grad Norm: 0.00273616\n",
      "Epoch 7 | Step 266400 | Avg Loss: 0.0182 | Grad Norm: 0.00887596\n",
      "Epoch 7 | Step 266500 | Avg Loss: 0.0182 | Grad Norm: 0.00451522\n",
      "Epoch 7 | Step 266600 | Avg Loss: 0.0185 | Grad Norm: 0.00465489\n",
      "Epoch 7 | Step 266700 | Avg Loss: 0.0188 | Grad Norm: 0.00382908\n",
      "Epoch 7 | Step 266800 | Avg Loss: 0.0189 | Grad Norm: 0.00701767\n",
      "Epoch 7 | Step 266900 | Avg Loss: 0.0184 | Grad Norm: 0.00420965\n",
      "Epoch 7 | Step 267000 | Avg Loss: 0.0184 | Grad Norm: 0.00301409\n",
      "Epoch 7 | Step 267100 | Avg Loss: 0.0186 | Grad Norm: 0.00329797\n",
      "Epoch 7 | Step 267200 | Avg Loss: 0.0184 | Grad Norm: 0.00358618\n",
      "Epoch 7 | Step 267300 | Avg Loss: 0.0182 | Grad Norm: 0.00349087\n",
      "Epoch 7 | Step 267400 | Avg Loss: 0.0178 | Grad Norm: 0.00344025\n",
      "Epoch 7 | Step 267500 | Avg Loss: 0.0177 | Grad Norm: 0.00429267\n",
      "Epoch 7 | Step 267600 | Avg Loss: 0.0177 | Grad Norm: 0.00352611\n",
      "Epoch 7 | Step 267700 | Avg Loss: 0.0180 | Grad Norm: 0.00372868\n",
      "Epoch 7 | Step 267800 | Avg Loss: 0.0180 | Grad Norm: 0.00466517\n",
      "Epoch 7 | Step 267900 | Avg Loss: 0.0181 | Grad Norm: 0.00369656\n",
      "Epoch 7 | Step 268000 | Avg Loss: 0.0181 | Grad Norm: 0.00496021\n",
      "Epoch 7 | Step 268100 | Avg Loss: 0.0180 | Grad Norm: 0.00417815\n",
      "Epoch 7 | Step 268200 | Avg Loss: 0.0178 | Grad Norm: 0.00330485\n",
      "Epoch 7 | Step 268300 | Avg Loss: 0.0176 | Grad Norm: 0.00412909\n",
      "Epoch 7 | Step 268400 | Avg Loss: 0.0174 | Grad Norm: 0.00336878\n",
      "Epoch 7 | Step 268500 | Avg Loss: 0.0169 | Grad Norm: 0.00595025\n",
      "Epoch 7 | Step 268600 | Avg Loss: 0.0168 | Grad Norm: 0.00546023\n",
      "Epoch 7 | Step 268700 | Avg Loss: 0.0171 | Grad Norm: 0.00365102\n",
      "Epoch 7 | Step 268800 | Avg Loss: 0.0174 | Grad Norm: 0.00294717\n",
      "Epoch 7 | Step 268900 | Avg Loss: 0.0178 | Grad Norm: 0.00362310\n",
      "Epoch 7 | Step 269000 | Avg Loss: 0.0182 | Grad Norm: 0.00295812\n",
      "Epoch 7 | Step 269100 | Avg Loss: 0.0176 | Grad Norm: 0.00581810\n",
      "Epoch 7 | Step 269200 | Avg Loss: 0.0177 | Grad Norm: 0.00349974\n",
      "Epoch 7 | Step 269300 | Avg Loss: 0.0180 | Grad Norm: 0.00551348\n",
      "Epoch 7 | Step 269400 | Avg Loss: 0.0176 | Grad Norm: 0.00505136\n",
      "Epoch 7 | Step 269500 | Avg Loss: 0.0178 | Grad Norm: 0.00414367\n",
      "Epoch 7 | Step 269600 | Avg Loss: 0.0182 | Grad Norm: 0.00568224\n",
      "Epoch 7 | Step 269700 | Avg Loss: 0.0182 | Grad Norm: 0.00366275\n",
      "Epoch 7 | Step 269800 | Avg Loss: 0.0182 | Grad Norm: 0.00452533\n",
      "Epoch 7 | Step 269900 | Avg Loss: 0.0184 | Grad Norm: 0.00770678\n",
      "Epoch 7 | Step 270000 | Avg Loss: 0.0187 | Grad Norm: 0.00309106\n",
      "Epoch 7 | Step 270100 | Avg Loss: 0.0182 | Grad Norm: 0.00815856\n",
      "Epoch 7 | Step 270200 | Avg Loss: 0.0180 | Grad Norm: 0.00567327\n",
      "Epoch 7 | Step 270300 | Avg Loss: 0.0180 | Grad Norm: 0.00293195\n",
      "Epoch 7 | Step 270400 | Avg Loss: 0.0175 | Grad Norm: 0.00375529\n",
      "Epoch 7 | Step 270500 | Avg Loss: 0.0176 | Grad Norm: 0.00738328\n",
      "Epoch 7 | Step 270600 | Avg Loss: 0.0182 | Grad Norm: 0.00309169\n",
      "Epoch 7 | Step 270700 | Avg Loss: 0.0184 | Grad Norm: 0.00346302\n",
      "Epoch 7 | Step 270800 | Avg Loss: 0.0189 | Grad Norm: 0.00253797\n",
      "Epoch 7 | Step 270900 | Avg Loss: 0.0184 | Grad Norm: 0.00380110\n",
      "Epoch 7 | Step 271000 | Avg Loss: 0.0185 | Grad Norm: 0.00270044\n",
      "Epoch 7 | Step 271100 | Avg Loss: 0.0184 | Grad Norm: 0.00289130\n",
      "Epoch 7 | Step 271200 | Avg Loss: 0.0184 | Grad Norm: 0.00898384\n",
      "Epoch 7 | Step 271300 | Avg Loss: 0.0183 | Grad Norm: 0.00658367\n",
      "Epoch 7 | Step 271400 | Avg Loss: 0.0188 | Grad Norm: 0.00448380\n",
      "Epoch 7 | Step 271500 | Avg Loss: 0.0186 | Grad Norm: 0.00478720\n",
      "Epoch 7 | Step 271600 | Avg Loss: 0.0185 | Grad Norm: 0.00258627\n",
      "Epoch 7 | Step 271700 | Avg Loss: 0.0181 | Grad Norm: 0.00290586\n",
      "Epoch 7 | Step 271800 | Avg Loss: 0.0182 | Grad Norm: 0.00307719\n",
      "Epoch 7 | Step 271900 | Avg Loss: 0.0182 | Grad Norm: 0.00371312\n",
      "Epoch 7 | Step 272000 | Avg Loss: 0.0181 | Grad Norm: 0.00373223\n",
      "Epoch 7 | Step 272100 | Avg Loss: 0.0181 | Grad Norm: 0.00319937\n",
      "Epoch 7 | Step 272200 | Avg Loss: 0.0183 | Grad Norm: 0.00356088\n",
      "Epoch 7 | Step 272300 | Avg Loss: 0.0184 | Grad Norm: 0.00402008\n",
      "Epoch 7 | Step 272400 | Avg Loss: 0.0185 | Grad Norm: 0.00360209\n",
      "Epoch 7 | Step 272500 | Avg Loss: 0.0186 | Grad Norm: 0.00268799\n",
      "Epoch 7 | Step 272600 | Avg Loss: 0.0186 | Grad Norm: 0.00802558\n",
      "Epoch 7 | Step 272700 | Avg Loss: 0.0189 | Grad Norm: 0.00745926\n",
      "Epoch 7 | Step 272800 | Avg Loss: 0.0190 | Grad Norm: 0.00317227\n",
      "Epoch 7 | Step 272900 | Avg Loss: 0.0187 | Grad Norm: 0.00598449\n",
      "Epoch 7 | Step 273000 | Avg Loss: 0.0186 | Grad Norm: 0.00278266\n",
      "Epoch 7 | Step 273100 | Avg Loss: 0.0186 | Grad Norm: 0.00322154\n",
      "Epoch 7 | Step 273200 | Avg Loss: 0.0183 | Grad Norm: 0.00263730\n",
      "Epoch 7 | Step 273300 | Avg Loss: 0.0181 | Grad Norm: 0.00291994\n",
      "Epoch 7 | Step 273400 | Avg Loss: 0.0179 | Grad Norm: 0.00326568\n",
      "Epoch 7, Loss: 0.0155\n",
      "Epoch 8 | Step 273500 | Avg Loss: 0.0183 | Grad Norm: 0.00319523\n",
      "Epoch 8 | Step 273600 | Avg Loss: 0.0179 | Grad Norm: 0.00274328\n",
      "Epoch 8 | Step 273700 | Avg Loss: 0.0176 | Grad Norm: 0.00273972\n",
      "Epoch 8 | Step 273800 | Avg Loss: 0.0175 | Grad Norm: 0.00281048\n",
      "Epoch 8 | Step 273900 | Avg Loss: 0.0176 | Grad Norm: 0.00426967\n",
      "Epoch 8 | Step 274000 | Avg Loss: 0.0176 | Grad Norm: 0.00454094\n",
      "Epoch 8 | Step 274100 | Avg Loss: 0.0181 | Grad Norm: 0.00362846\n",
      "Epoch 8 | Step 274200 | Avg Loss: 0.0187 | Grad Norm: 0.00502103\n",
      "Epoch 8 | Step 274300 | Avg Loss: 0.0191 | Grad Norm: 0.00302392\n",
      "Epoch 8 | Step 274400 | Avg Loss: 0.0188 | Grad Norm: 0.00422725\n",
      "Epoch 8 | Step 274500 | Avg Loss: 0.0184 | Grad Norm: 0.00341297\n",
      "Epoch 8 | Step 274600 | Avg Loss: 0.0180 | Grad Norm: 0.00337920\n",
      "Epoch 8 | Step 274700 | Avg Loss: 0.0181 | Grad Norm: 0.00324509\n",
      "Epoch 8 | Step 274800 | Avg Loss: 0.0181 | Grad Norm: 0.00567178\n",
      "Epoch 8 | Step 274900 | Avg Loss: 0.0178 | Grad Norm: 0.00356814\n",
      "Epoch 8 | Step 275000 | Avg Loss: 0.0177 | Grad Norm: 0.00287078\n",
      "Epoch 8 | Step 275100 | Avg Loss: 0.0175 | Grad Norm: 0.00480282\n",
      "Epoch 8 | Step 275200 | Avg Loss: 0.0180 | Grad Norm: 0.00297361\n",
      "Epoch 8 | Step 275300 | Avg Loss: 0.0176 | Grad Norm: 0.00306215\n",
      "Epoch 8 | Step 275400 | Avg Loss: 0.0179 | Grad Norm: 0.00269281\n",
      "Epoch 8 | Step 275500 | Avg Loss: 0.0177 | Grad Norm: 0.00287257\n",
      "Epoch 8 | Step 275600 | Avg Loss: 0.0179 | Grad Norm: 0.00342940\n",
      "Epoch 8 | Step 275700 | Avg Loss: 0.0179 | Grad Norm: 0.00353971\n",
      "Epoch 8 | Step 275800 | Avg Loss: 0.0180 | Grad Norm: 0.00476459\n",
      "Epoch 8 | Step 275900 | Avg Loss: 0.0182 | Grad Norm: 0.00435811\n",
      "Epoch 8 | Step 276000 | Avg Loss: 0.0181 | Grad Norm: 0.00526532\n",
      "Epoch 8 | Step 276100 | Avg Loss: 0.0182 | Grad Norm: 0.00346307\n",
      "Epoch 8 | Step 276200 | Avg Loss: 0.0183 | Grad Norm: 0.00505540\n",
      "Epoch 8 | Step 276300 | Avg Loss: 0.0183 | Grad Norm: 0.00441012\n",
      "Epoch 8 | Step 276400 | Avg Loss: 0.0183 | Grad Norm: 0.00319322\n",
      "Epoch 8 | Step 276500 | Avg Loss: 0.0183 | Grad Norm: 0.00357638\n",
      "Epoch 8 | Step 276600 | Avg Loss: 0.0179 | Grad Norm: 0.00296492\n",
      "Epoch 8 | Step 276700 | Avg Loss: 0.0177 | Grad Norm: 0.00511651\n",
      "Epoch 8 | Step 276800 | Avg Loss: 0.0178 | Grad Norm: 0.00690387\n",
      "Epoch 8 | Step 276900 | Avg Loss: 0.0173 | Grad Norm: 0.00287820\n",
      "Epoch 8 | Step 277000 | Avg Loss: 0.0173 | Grad Norm: 0.00391292\n",
      "Epoch 8 | Step 277100 | Avg Loss: 0.0173 | Grad Norm: 0.00286439\n",
      "Epoch 8 | Step 277200 | Avg Loss: 0.0176 | Grad Norm: 0.00413618\n",
      "Epoch 8 | Step 277300 | Avg Loss: 0.0179 | Grad Norm: 0.00263250\n",
      "Epoch 8 | Step 277400 | Avg Loss: 0.0180 | Grad Norm: 0.00379898\n",
      "Epoch 8 | Step 277500 | Avg Loss: 0.0178 | Grad Norm: 0.00527597\n",
      "Epoch 8 | Step 277600 | Avg Loss: 0.0178 | Grad Norm: 0.00251125\n",
      "Epoch 8 | Step 277700 | Avg Loss: 0.0177 | Grad Norm: 0.00425796\n",
      "Epoch 8 | Step 277800 | Avg Loss: 0.0180 | Grad Norm: 0.00322403\n",
      "Epoch 8 | Step 277900 | Avg Loss: 0.0179 | Grad Norm: 0.00438387\n",
      "Epoch 8 | Step 278000 | Avg Loss: 0.0178 | Grad Norm: 0.00452679\n",
      "Epoch 8 | Step 278100 | Avg Loss: 0.0173 | Grad Norm: 0.00550684\n",
      "Epoch 8 | Step 278200 | Avg Loss: 0.0170 | Grad Norm: 0.00353492\n",
      "Epoch 8 | Step 278300 | Avg Loss: 0.0175 | Grad Norm: 0.00275230\n",
      "Epoch 8 | Step 278400 | Avg Loss: 0.0177 | Grad Norm: 0.00272761\n",
      "Epoch 8 | Step 278500 | Avg Loss: 0.0176 | Grad Norm: 0.00291013\n",
      "Epoch 8 | Step 278600 | Avg Loss: 0.0177 | Grad Norm: 0.00276206\n",
      "Epoch 8 | Step 278700 | Avg Loss: 0.0177 | Grad Norm: 0.00314413\n",
      "Epoch 8 | Step 278800 | Avg Loss: 0.0182 | Grad Norm: 0.00325695\n",
      "Epoch 8 | Step 278900 | Avg Loss: 0.0187 | Grad Norm: 0.00502789\n",
      "Epoch 8 | Step 279000 | Avg Loss: 0.0185 | Grad Norm: 0.00351884\n",
      "Epoch 8 | Step 279100 | Avg Loss: 0.0181 | Grad Norm: 0.00267832\n",
      "Epoch 8 | Step 279200 | Avg Loss: 0.0183 | Grad Norm: 0.00329141\n",
      "Epoch 8 | Step 279300 | Avg Loss: 0.0180 | Grad Norm: 0.00548728\n",
      "Epoch 8 | Step 279400 | Avg Loss: 0.0181 | Grad Norm: 0.00299208\n",
      "Epoch 8 | Step 279500 | Avg Loss: 0.0177 | Grad Norm: 0.00340732\n",
      "Epoch 8 | Step 279600 | Avg Loss: 0.0177 | Grad Norm: 0.00269477\n",
      "Epoch 8 | Step 279700 | Avg Loss: 0.0179 | Grad Norm: 0.00321755\n",
      "Epoch 8 | Step 279800 | Avg Loss: 0.0180 | Grad Norm: 0.00332626\n",
      "Epoch 8 | Step 279900 | Avg Loss: 0.0178 | Grad Norm: 0.00344637\n",
      "Epoch 8 | Step 280000 | Avg Loss: 0.0180 | Grad Norm: 0.00318432\n",
      "Epoch 8 | Step 280100 | Avg Loss: 0.0180 | Grad Norm: 0.00507859\n",
      "Epoch 8 | Step 280200 | Avg Loss: 0.0179 | Grad Norm: 0.00847218\n",
      "Epoch 8 | Step 280300 | Avg Loss: 0.0179 | Grad Norm: 0.00347138\n",
      "Epoch 8 | Step 280400 | Avg Loss: 0.0179 | Grad Norm: 0.00329100\n",
      "Epoch 8 | Step 280500 | Avg Loss: 0.0177 | Grad Norm: 0.00522293\n",
      "Epoch 8 | Step 280600 | Avg Loss: 0.0175 | Grad Norm: 0.00326212\n",
      "Epoch 8 | Step 280700 | Avg Loss: 0.0171 | Grad Norm: 0.00604645\n",
      "Epoch 8 | Step 280800 | Avg Loss: 0.0174 | Grad Norm: 0.00351716\n",
      "Epoch 8 | Step 280900 | Avg Loss: 0.0175 | Grad Norm: 0.00472343\n",
      "Epoch 8 | Step 281000 | Avg Loss: 0.0177 | Grad Norm: 0.00553965\n",
      "Epoch 8 | Step 281100 | Avg Loss: 0.0174 | Grad Norm: 0.00559078\n",
      "Epoch 8 | Step 281200 | Avg Loss: 0.0174 | Grad Norm: 0.00518951\n",
      "Epoch 8 | Step 281300 | Avg Loss: 0.0174 | Grad Norm: 0.00317975\n",
      "Epoch 8 | Step 281400 | Avg Loss: 0.0178 | Grad Norm: 0.00276970\n",
      "Epoch 8 | Step 281500 | Avg Loss: 0.0180 | Grad Norm: 0.00290857\n",
      "Epoch 8 | Step 281600 | Avg Loss: 0.0176 | Grad Norm: 0.00342400\n",
      "Epoch 8 | Step 281700 | Avg Loss: 0.0182 | Grad Norm: 0.00324120\n",
      "Epoch 8 | Step 281800 | Avg Loss: 0.0182 | Grad Norm: 0.00624972\n",
      "Epoch 8 | Step 281900 | Avg Loss: 0.0181 | Grad Norm: 0.00380786\n",
      "Epoch 8 | Step 282000 | Avg Loss: 0.0185 | Grad Norm: 0.00259092\n",
      "Epoch 8 | Step 282100 | Avg Loss: 0.0185 | Grad Norm: 0.00824250\n",
      "Epoch 8 | Step 282200 | Avg Loss: 0.0185 | Grad Norm: 0.00253099\n",
      "Epoch 8 | Step 282300 | Avg Loss: 0.0184 | Grad Norm: 0.00444207\n",
      "Epoch 8 | Step 282400 | Avg Loss: 0.0181 | Grad Norm: 0.00321029\n",
      "Epoch 8 | Step 282500 | Avg Loss: 0.0183 | Grad Norm: 0.00412349\n",
      "Epoch 8 | Step 282600 | Avg Loss: 0.0183 | Grad Norm: 0.00330392\n",
      "Epoch 8 | Step 282700 | Avg Loss: 0.0176 | Grad Norm: 0.00360227\n",
      "Epoch 8 | Step 282800 | Avg Loss: 0.0177 | Grad Norm: 0.00279919\n",
      "Epoch 8 | Step 282900 | Avg Loss: 0.0181 | Grad Norm: 0.00283784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f48a97d1f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yvlaere/projects/yvl-chess/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#criterion = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#for _ in range(1000000):\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get data from the dataloader\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_w\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/yvl-chess/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/nnue_training_split_model_10M\")\n",
    "\n",
    "# hyperparameters\n",
    "nr_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "scaling_factor = 400\n",
    "ground_truth_scaling_factor = 400\n",
    "lambda_ = 0.2\n",
    "log_interval = 100\n",
    "save_interval = 100000\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "epsilon = 1e-10\n",
    "\n",
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Split_NNUE().to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "\n",
    "        #for _ in range(1000000):\n",
    "    \n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y, result = batch\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        result = result.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = lambda_*result + (1 - lambda_) * torch.sigmoid(batch_y / ground_truth_scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        #loss = (wdl_batch_y * torch.log(wdl_batch_y + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_batch_y + epsilon)) -(wdl_batch_y * torch.log(wdl_pred   + epsilon) + (1 - wdl_batch_y) * torch.log(1 - wdl_pred   + epsilon))\n",
    "        #loss = loss.mean()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(wdl_pred, wdl_batch_y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # make a step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        # calculate the gradient norm\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm of this parameter's gradient\n",
    "                total_norm_sq += param_norm.item() ** 2\n",
    "        total_grad_norm = total_norm_sq ** 0.5\n",
    "        # Now total_grad_norm is the L2 norm of all gradients combined.\n",
    "        #print(f\"Step {step}  Grad Norm = {total_grad_norm:.8f}\")\n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Avg Loss: {avg_loss:.4f} | Grad Norm: {total_grad_norm:.8f}\")\n",
    "            running_loss = 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "            writer.add_scalar(\"Grad Norm\", total_grad_norm, step)\n",
    "            writer.add_scalar(\"WDL Pred\", torch.mean(wdl_pred).item(), step)\n",
    "            writer.add_scalar(\"WDL BatchY\", torch.mean(wdl_batch_y).item(), step)\n",
    "            writer.add_scalar(\"Pred\", torch.median(pred).item(), step)\n",
    "            writer.add_scalar(\"BatchY\", torch.median(batch_y).item(), step)\n",
    "\n",
    "\n",
    "            # log separate grad norms\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    writer.add_scalar(f'GradNorm/{name}', grad_norm, step)\n",
    "\n",
    "        # Save the model every `save_interval` steps\n",
    "        if step % save_interval == 0:\n",
    "            model_name = 'saved_models/split_model_10M_' + str(step) + \".pth\"\n",
    "            print(\"Saving model at step\" + str(step))\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,}, model_name)\n",
    "            \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d09a",
   "metadata": {},
   "source": [
    "### Postprocessing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede75645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "model = SimpleNNUE()\n",
    "checkpoint = torch.load('saved_models/step_1M_ds_1M.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#model.load_state_dict(torch.load(\"best_model.pth\", map_location=\"cpu\"))\n",
    "\n",
    "def save_layer(layer, name):\n",
    "    w = layer.weight.detach().numpy()\n",
    "    b = layer.bias.detach().numpy()\n",
    "    with open(f\"{name}_weights.txt\", \"w\") as f:\n",
    "        for row in w:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    with open(f\"{name}_biases.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(map(str, b)))\n",
    "\n",
    "save_layer(model.fc1, \"model/layer1\")\n",
    "save_layer(model.fc2, \"model/layer2\")\n",
    "save_layer(model.fc3, \"model/layer3\")\n",
    "save_layer(model.fc4, \"model/layer4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of bins\n",
    "num_bins = 64\n",
    "bin_edges = np.linspace(-32003, 32003, num_bins + 1)\n",
    "counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "filename = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/scores.txt'\n",
    "\n",
    "# Re-read the file and bin values\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            val = float(line.strip())\n",
    "            # Determine bin index\n",
    "            bin_idx = np.searchsorted(bin_edges, val, side='right') - 1\n",
    "            if 0 <= bin_idx < num_bins:\n",
    "                counts[bin_idx] += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Find empty bins\n",
    "empty_bins = []\n",
    "for i, count in enumerate(counts):\n",
    "    if count == 0:\n",
    "        left_edge = bin_edges[i]\n",
    "        right_edge = bin_edges[i + 1]\n",
    "        empty_bins.append((i, left_edge, right_edge))\n",
    "\n",
    "# Print empty bin ranges\n",
    "print(\"Empty bins:\")\n",
    "for i, left, right in empty_bins:\n",
    "    print(f\"Bin {i}: [{left}, {right})\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', align='edge')\n",
    "plt.title(\"Histogram (streamed)\")\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP to WDL conversion\n",
    "scaling_factor = 400\n",
    "score = torch.tensor(32000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(-1000, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))\n",
    "score = torch.tensor(0, dtype=torch.float32)\n",
    "print(torch.sigmoid(score / scaling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNNUE()\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "fen1 = 'rnbqkbnr/pppppppp/8/8/8/5P2/PPPPP1PP/RNBQKBNR w KQkq - 0 1'\n",
    "fen2 = 'rnbqkbnr/pppppppp/8/8/8/7N/PPPPPPPP/RNBQKB1R w KQkq - 0 1'\n",
    "fen3 = 'rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R w KQkq - 0 1'\n",
    "\n",
    "start_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "\n",
    "#torch.tensor(FEN_to_input(fen1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    input1 = FEN_to_input(fen1)\n",
    "    in_1 = np.argwhere(input1.numpy() == 1)\n",
    "    print(sum(input1.numpy()))\n",
    "    input2 = FEN_to_input(fen2)\n",
    "    #print(np.argwhere(input2.numpy() == 1))\n",
    "    input3 = FEN_to_input(fen3)\n",
    "    #print(np.argwhere(input3.numpy() == 1))\n",
    "\n",
    "    in_start = np.argwhere(FEN_to_input(start_fen).numpy() == 1)\n",
    "\n",
    "    pred1 = model(input1)\n",
    "    pred2 = model(input2)\n",
    "    pred3 = model(input3)\n",
    "\n",
    "    print(pred1.item())\n",
    "    print(pred2.item())\n",
    "    print(pred3.item())\n",
    "\n",
    "    accumulator = model.fc1(input1)\n",
    "    start_accumulator = model.fc1(FEN_to_input(start_fen))\n",
    "    print(accumulator)\n",
    "\n",
    "    #print(\"weights[0][0]\")\n",
    "    #print(model.fc1.weight[0][0])\n",
    "    #print(\"weights[1][0]\")\n",
    "    #print(model.fc1.weight[1][0])\n",
    "    #print(model.fc1.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41259",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_2 = [192, 65, 130, 259, 324, 133, 70, 199, 8, 9, 10, 11, 12, 13, 14, 15, 432, 433, 434, 435, 436, 437, 438, 439, 632, 505, 570, 699, 764, 573, 510, 639]\n",
    "\n",
    "print(np.sort(in_start.reshape(1, 32)))\n",
    "print(np.sort(in_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481ab8",
   "metadata": {},
   "source": [
    "### HalfKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "stm_dict = {'w': 0, 'b': 1}\n",
    "\n",
    "\n",
    "def FEN_to_HalfKP(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    stm = stm_dict[sub_FEN[1]]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    position = 0\n",
    "    white_king_position = 0\n",
    "    black_king_position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            elif char == 'K':\n",
    "                white_king_position = position\n",
    "                position += 1\n",
    "            elif char == 'k':\n",
    "                black_king_position = position\n",
    "                position += 1\n",
    "            else:\n",
    "                position += 1\n",
    "\n",
    "    white_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "    black_input_layer = np.zeros(40960, dtype = np.float32)\n",
    "\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                if (char != 'K') & (char != 'k'):\n",
    "                    piece_index = (piece_dict[char] % 6) * 2 + (piece_dict[char] > 5)\n",
    "                    white_input_layer[position + (piece_index + white_king_position*10)*64] = 1\n",
    "                    black_input_layer[position + (piece_index + black_king_position*10)*64] = 1\n",
    "                    position += 1\n",
    "                else:\n",
    "                    position += 1\n",
    "\n",
    "    return torch.tensor(white_input_layer, dtype=torch.float32), torch.tensor(black_input_layer, dtype=torch.float32), torch.tensor(stm, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6378bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class HalfKP_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                w_in, b_in, stm = FEN_to_HalfKP(row[0].strip())\n",
    "                score = float(row[1].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                yield w_in, b_in, stm, torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                buf.append((w_in, b_in, stm, score))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for w_in, b_in, stm, score in stream:\n",
    "                yield w_in, b_in, stm, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_FEATURES = 40960\n",
    "M = 1024\n",
    "N = 32\n",
    "K = 1\n",
    "\n",
    "class HalfKPNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HalfKPNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, M)\n",
    "        self.fc2 = nn.Linear(2*M, N)\n",
    "        self.fc3 = nn.Linear(N, K)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):\n",
    "        w = self.fc1(white_features)\n",
    "        b = self.fc1(black_features)\n",
    "        cat_wb = torch.cat([w, b], dim=1)  # [B, 2*M]\n",
    "        cat_bw = torch.cat([b, w], dim=1)  # [B, 2*M]\n",
    "\n",
    "        stm = stm.to(dtype=cat_wb.dtype).view(-1, 1)\n",
    "\n",
    "        accumulator = stm * cat_wb + (1 - stm) * cat_bw\n",
    "\n",
    "        x = torch.clamp(accumulator, min = 0.0, max = 1.0)\n",
    "        x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = '/home/yvlaere/projects/yvl-chess/NNUE_training/training_data/sf_training_data.csv'\n",
    "dataset = HalfKP_Dataset(csv_path, shuffle_buffer=1000)\n",
    "loader = DataLoader(dataset, batch_size = 128, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/halfKP\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nr_epochs = 500\n",
    "model = HalfKPNNUE().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.05)\n",
    "total_size = 200000000\n",
    "batch_size = 128\n",
    "steps_per_epoch = total_size // batch_size\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, min_lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100000, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "MAE_loss = nn.L1Loss()\n",
    "lowest_MAE = 10000\n",
    "\n",
    "# Transform the CP scores to the WDL space\n",
    "scaling_factor = 400\n",
    "\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "log_interval = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch in loader:\n",
    "        #for _ in range(100000):\n",
    "\n",
    "        # get data from the dataloader\n",
    "        batch_x_w, batch_x_b, stm, batch_y = batch\n",
    "\n",
    "        # move data to GPU\n",
    "        batch_x_w = batch_x_w.to(device, non_blocking = True)\n",
    "        batch_x_b = batch_x_b.to(device, non_blocking = True)\n",
    "        batch_y = batch_y.to(device, non_blocking = True)\n",
    "        stm = stm.to(device, non_blocking = True)\n",
    "        pred = model(batch_x_w, batch_x_b, stm).squeeze(1)  # remove the last dimension\n",
    "\n",
    "        # Transform the CP scores to the WDL space\n",
    "        wdl_batch_y = torch.sigmoid(batch_y / scaling_factor)\n",
    "        wdl_pred = torch.sigmoid(pred / scaling_factor)\n",
    "\n",
    "        # calculate the MSE loss\n",
    "        loss = criterion(wdl_batch_y, wdl_pred)\n",
    "        MAE = MAE_loss(wdl_batch_y, wdl_pred)\n",
    "        running_loss += loss.item()\n",
    "        running_mae += MAE\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Log every `log_interval` steps\n",
    "        if step % log_interval == 0 and step != 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            avg_mae = running_mae / log_interval\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{steps_per_epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, step)\n",
    "            writer.add_scalar(\"MAE/train\", avg_mae, step)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar(\"LR\", current_lr, step)\n",
    "\n",
    "        # calculate MAE\n",
    "        if MAE < 0.0002:\n",
    "            lowest_MAE = MAE\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with MAE: {lowest_MAE.item():.4f}, loss: {loss.item():.4f}\")\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, MAE: {MAE.item():.4f}, lowest MAE: {lowest_MAE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def FEN_to_input(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string to an NNUE input vector.\n",
    "    \"\"\"\n",
    "    # Split the FEN string into its components\n",
    "    sub_FEN = fen.split(' ')\n",
    "    board = sub_FEN[0]\n",
    "    ranks = board.split('/')\n",
    "\n",
    "    # Convert the board to a 1D boolean array\n",
    "    # in the chess engine, position 0 corresponds to a1, so the ranks in the FEN string will need to be reversed\n",
    "    input_layer = np.zeros(768, dtype = np.float32)\n",
    "    position = 0\n",
    "    for rank in ranks[::-1]:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                position += int(char)\n",
    "            else:\n",
    "                input_layer[position + piece_dict[char]*64] = 1\n",
    "                position += 1\n",
    "\n",
    "    return torch.tensor(input_layer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd47b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNUE, self).__init__()\n",
    "        # three fully connected layers\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        #self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = torch.clamp(self.fc1(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc2(x), min = 0, max = 1)\n",
    "        #x = torch.clamp(self.fc3(x), min = 0, max = 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class Custom_Dataset(IterableDataset):\n",
    "    def __init__(self, csv_path, shuffle_buffer=0):\n",
    "        \"\"\"\n",
    "        csv_path: path to CSV file with two columns: fen, score\n",
    "        fen_to_tensor: function(str) -> torch.Tensor\n",
    "        shuffle_buffer: size of in-memory shuffle buffer; 0 = no shuffle\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "    def _row_stream(self):\n",
    "        \"\"\"\n",
    "        Generator that yields (fen, score) tuples from the CSV file.\n",
    "        \"\"\"\n",
    "        with open(self.csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if not row or row[0].startswith('#'):\n",
    "                    continue\n",
    "                fen, score, result = FEN_to_input(row[0].strip()), float(row[1].strip()), float(row[2].strip())\n",
    "                if score == 32002:\n",
    "                    score = 0\n",
    "                if result == -1:\n",
    "                    result = 0\n",
    "                elif result == 0:\n",
    "                    result = 0.5\n",
    "                yield fen, torch.tensor(score, dtype=torch.float32), torch.tensor(result, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = self._row_stream()\n",
    "        if self.shuffle_buffer > 1:\n",
    "\n",
    "            # reservoir-style shuffle buffer\n",
    "            buf = []\n",
    "            for fen, score, result in stream:\n",
    "                buf.append((fen, score, result))\n",
    "                if len(buf) >= self.shuffle_buffer:\n",
    "                    idx = torch.randint(len(buf), (1,)).item()\n",
    "                    yield buf.pop(idx)\n",
    "                    \n",
    "            # drain remaining buffer\n",
    "            while buf:\n",
    "                idx = torch.randint(len(buf), (1,)).item()\n",
    "                yield buf.pop(idx)\n",
    "        else:\n",
    "            for fen, score, result in stream:\n",
    "                yield fen, score, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
